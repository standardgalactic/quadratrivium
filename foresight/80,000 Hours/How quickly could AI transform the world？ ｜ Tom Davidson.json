{"text": " Hi listeners, this is the 80,000 hours podcast where we have unusually in-depth conversations about the world's most pressing problems, what you can do to solve them, and what did happen to all those horses after we invented cars. I'm Rob Wiblin, head of research at 80,000 hours. The last few months have been a crazy time for advances in artificial intelligence. Over the last couple of years, I've become, for better or worse, increasingly confident that the future is going to be shaped in a major way by what sorts of AI systems we develop and choose to deploy as they approach and then exceed human capabilities in the most important areas. That's always seemed like a really common sense idea to me, but it now is becoming pretty apparent to people across society who until recently had hardly been paying attention to this issue. Unfortunately, we've only had two episodes about AI over the last six months due to the substantial lags that come in between conceiving of an episode, finding the right guest, scheduling and recording the conversation, and then editing it, and then finally releasing it. But that scarcity of AI content is fortunately about to change. In today's episode, Louisa Rodriguez interviews Tom Davidson, a senior research analyst at Open Philanthropy, whose job is figuring out when and how society is going to be upended by advances in AI. Not a simple forecasting exercise. Tom has his work cut out for him. But if you've been wondering when you might be replaced in your job by an AI model and when AIs might be able to do everything that humans can do for less than what it costs to feed a person and keep them alive, then this episode will help you think about that a little bit more clearly and perhaps have a little bit more idea of what to expect. In particular, Louisa and Tom discuss how long it will take for AIs to go from being able to do 20% of the work humans are doing to being able to do all of it, what underlying factors are driving progress in AI and how much each of those factors is contributing. Whether we should expect that progress to speed up or slow down in incoming years, how much computer hardware is used to train these models and whether it can continue increasing at the absolutely blistering rate that it has been doing the last 10 years, when AI systems might be able to do scientific research and what implications that would then have, and also when we might expect all of this to noticeably increase GDP in a visible way and what that could look like and then what new bottlenecks might exist in an economy where AI systems were doing most of the work. And on top of that, plenty of other issues besides. Tom's expectations for the future are exciting or alarming, I guess, depending on how you want to look at them. Regular listeners will have heard me do quite a few interviews on AI over the years, but those interviews tend not to focus on my opinions, at least they're not meant to. So it's possible that people don't have that much of a sense of where I stand on this personally. In case you're interested, I think the chances that you and I are either killed due to actions taken by AI systems, or that we live to see humanity unintentionally lose control of its future because of AI systems are greater than 10%, greater than a one in 10 chance. Looking at surveys and polling now, it seems like both AI experts and the general public are converging on a view that's not too far from that. It's a view that previously seemed idiosyncratic, but now is really quite mainstream. Naturally, if that if the odds are like that, then it makes figuring out how to safely develop and deploy AI, you know, probably the issue of our time. And indeed, one of the things that we should most care about from even a selfish point of view, or if we have children care about from a parental point of view. I'm an economist by training and I entirely understand how the industrial revolution ultimately raised incomes across generations. And that while factory automation was indeed financially ruinous for plenty of individuals, it didn't then result in persistent unemployment. But nonetheless, despite understanding and appreciating all of that, I am skeptical that there are going to be paying jobs for children being born today, or if not them, that they'll be paying jobs at least for their children. And we are just we're flying by the seat of our pants here and have not really figured out a plan ahead of time about what we're going to do as this technology just completely upends I think existing social relations and economic systems. Well, I'll describe the overall situation that humanity finds itself in to be pretty terrifying. The fact that all kinds of different people are waking up now to the risks here does give me hope that we can coordinate to prevent the worst. As you imagine, we'll have more to say to expand on all of that in in future episodes. But for now, I bring you Louise Rodriguez and Tom Davidson. Today, I'm speaking with Tom Davidson. Tom's a senior research analyst at Open Philanthropy, where his main focus is on when we might get transformative AI. Before joining Open Philanthropy, Tom taught science through Teach First at a comprehensive school in East London, and then was a data scientist for an education technology startup. And before all of that, Tom studied physics and philosophy at Oxford. Thanks for coming on the podcast, Tom. Thanks, Louise. It's a pleasure to be here. So I hope to talk about how fast we might go from kind of OK AI to AI that can do everything humans can do, plus how that'll affect the economy and the world. But first, yeah, how worried are you personally about the risks from AI? So about a year ago, I sat down and spent spent a morning trying to figure out, you know, what are my percentages of AI by certain time and what's my percentage that there's existential catastrophe from AI. And so I was focusing on the possibility that AI disempowers humanity and just takes over control of of society and the economy and and then what happens in the future. And a year ago, I landed at a number that was a bit above 10% for the probability that AI takes over by 2070. Right. OK. I mean, that's already pretty high. Yeah, that is that is already very high and much too high. Yeah. I think since since then, if I if I read the exercise today, I think I'd be close to 20% I think I compare to then I think it's just more likely that we develop AI that's capable of doing that by 2070. I also think that it's it's just pretty likely to happen in the next 20 years. Which then makes the chance that it goes badly, I think higher because we have less time to repair. So both those things, I think mean I'd probably be at about 20% if I if I read the exercise today. Yeah, we'll talk more about a couple of those things. But first, yeah, I'm curious if there's a type of scenario you have in mind when you're thinking about that 10 to 20% that makes you, yeah, especially worried. Yeah, I think the main line scenario I have is something like in the next 10 to 15 years, possibly sooner, we train an AI that is able to massively enhance the productivity of AI R&D workers. So people who are currently working to make AI better, maybe it makes them let's say five times as productive. So something like a large language model helps people working on AI R&D in particular, like code much faster or develop better algorithms or something. Exactly. And it means they can work five times faster ish. Exactly. Right. Okay. And then I think it won't be long after that, because AI is then going to be improving more quickly before AI is able to do everything that the current employees of DeepMind and OpenAI are currently doing for their jobs, at least the ones that can work remotely and do their work from a computer. Are you kind of distinguishing between the ones that work with like physical robots or the ones that work with like, I don't know, the mail room at DeepMind? Yeah. So if there's someone, I don't know if this is true, but if there's someone at DeepMind who is physically stacking the computer chips into the data center, then that's a kind of type of physical work, which I don't think would necessarily, you know, follow immediately on. But I think most of the work is not like that. Right. So I think AI, you know, especially most of the work for, you know, advancing AI is stuff that you can do on your laptop. So first we get the kind of 5x productivity gain. A little time later, we get AI that can do all of the work that people at OpenAI and DeepMind can do. And I really think the gap between those is not going to be very big. And we'll probably discuss a bit more about that later. At that point, I think AI is going to be improving at a blistering pace, absent a very specific effort to slow down, which I hope we really make, but absent that effort and absent coordinating to make sure that everyone is slowing down. You know, I think like a thousand next improvement in the AI's capabilities in a year is like a natural kind of conservative default. Wow. And so in this scenario, it wouldn't be long before the AI is just far outstripping the cognitive intelligence abilities of the smartest humans and indeed even the smartest massive teams of humans working together. Wow. And when you kind of crunch the numbers on how many AI's there's likely to be around this time, there's going to be hundreds of millions and probably many billions of human worker equivalents just in kind of AI cognitive ability. Is that because there are different AI systems or because you've got that much brain power being deployed like through AI, even if it's only like five AI systems or something? It's like running lots of copies of maybe a few AI systems. Got it. So for example, I haven't crunched the numbers on this, but I would guess that if you took the kind of computer chips that they use to train GPT-4 and then you just said how many copies of GPT-4 could we run using these computer chips, I guess that the answer is like maybe a million. Wow. It might be a bit lower actually. I'm not I'm not sure for GPT-4, but I think by the time that we train kind of AI that can fully replace all the human workers at an AI lab, I think it's that number is going to be more extreme. Right. So I think that by the time we can train that kind of AI, you'll be able to just immediately use the compute that you use for training to run 100 million. Right. That's okay. That's insane. And the reason for that is because it just takes, I don't know, like many times more compute to train an AI system than it does to then run them. Yeah, that's exactly right. Okay. One way to think about it is that the AI is trained on kind of millions of days of experience so that they get to be as good as they are. And that one, if you've managed to train them for that much, then it's kind of obviously you can run millions. Yeah. Okay. Got it. Okay. Okay. So then we're there. We've got potentially millions of human equivalents of copies of AI systems running and doing the kind of work that humans could do at the human level or better. Exactly. And then what? So then I think it's likely that it won't be long until we're talking about billions of AI systems. It's run of the mill to see 3x efficiency improvements in AI at various different levels of the software stacks. You could get a 3x efficiency in kind of how effective the algorithm is or a 3x efficiency improvement in how well the software runs on the hardware. And there are these various layers of the software stack that you can make improvements on. So I think it's probably at that point, once you have hundreds of millions of AI is looking for these types of improvements, you're probably going to get very, very quick, further improvements in AI cognitive ability. Again, maybe we coordinate to go very slowly, but this is absent that targeted coordination. Maybe the default. Yeah. Maybe the default scarily. And at that point, I think that if the AIs are misaligned, if they have goals that are different to what humans want them to do, and if those goals imply that it would be useful for them to get power so they could achieve those goals better, then I don't think it's going to be very hard for them to do that. Because it's like we've got a billion really smart humans who want to take power. Probably they'll find out some kind of way to do it. Maybe they invent new technology, maybe they convince some higher officials to give them control of the military. I'm not sure exactly how they'll do it. But at that point, I think it's kind of too late for us to be preventing AI takeover. Okay. And then by AI takeover, do you mind spelling that out? Yeah. So I think the thing that matters for AI takeover is that AI systems collectively end up in control of what happens in the future, and that it's kind of their goals and decisions that dictate the future path, and that it's no longer sensitive to what humans want or trying to achieve with the future. So I mean, ultimately, I think it does have to come down to physical force, most likely. I mean, you could imagine the scenario where the AI just convince humans and hypnotize them or something, and that's how they take over. But more likely they end up having control of the hard military equipment, and that's what allows them to establish their power and disempower humanity. So a thing that I have to admit still confuses me is just how we go from, I don't know, things like GPT-4, which even if it sometimes gets super confused and says silly things in a way that's like, oh, you clearly misunderstood what I was asking for, it's really hard for me to understand what the path to that kind of confusion to a misalignment that's so, I don't know, I guess just incredibly diverging from human values that the AI systems want to disempower humans. And I mean, one article I read, I think that just came out recently on Vox, is this article that was actually making the case that companies creating AI should slow down, should kind of coordinate to slow down, was walking through the case for why we might expect AI to be misaligned. And the example they gave just still confuses me. So the example is something like, let's say you've got a super smart AI system, we've programmed it to solve impossibly difficult problems, like calculating the number of atoms in a universe, for example, the AI system might realize that it could do a better job if it gained access to all of the computers on earth. So it releases a weapon of mass destruction to wipe out all humans, for example, an engineered virus that kills everyone but leaves infrastructure intact. And now it's free to use all the computer power. And that's the best way it's able to achieve its goal. And I think I just, I just really, really, I like, I feel silly. I feel, I feel dumb. I feel like I'm missing something. Like, how will it go from like, I want to solve this problem for humans to like, I'm going to kill them all to take their resources so that I can solve the problem. Like, why have we not ruled that kind of extreme behavior out? Great question. So let's maybe we can try and think about this, this system, which is trying to solve these these math problems. So maybe the first version of the AI, you just say, look, we want you to solve the problem using one of these four techniques. And that kind of system is okay. But then someone comes along and realizes that if you let the AI system do an internet search and plan its own line of attack on the problem, then it's able to do a better job in solving even harder and harder problems. And so you say, okay, we'll allow the AI to do that. And then over time, in order to improve performance, you give it more and more scope to kind of be creative in planning how it's going to attack each different kind of problem. One thing that might happen internally inside the AI's own head is that the AI may end up developing just an inherent desire to just get the answer to this math question as accurate as possible. That's something which it always gets rewarded for when it's being trained. And you know, maybe it could be thinking, I actually just want the humans to be happy with my answer. But another thing it might end up thinking is, you know what, what I really want is just to get the answer correct. And the kind of the feedback that as humans are giving that system doesn't distinguish between those two possibilities. So maybe we get unlucky and maybe the thing that it wants is to just really get the answer correct. And maybe the way that the AI system is working internally is it's saying, okay, that's my goal. What plan can I use to achieve that goal? And it's kind of creatively going and looking for new new approaches by googling information. Maybe one time it's like, it realizes that if I hacked into a kind of another computing cluster, it could use those computations to help itself the problem. And it does that no one realizes and then that kind of that reinforces the fact that it is now planning on such a broad scale to try and achieve this goal. And then maybe it's much more powerful at a later time. And it realizes that yeah, if it kills all humans, it could have access to all the supercomputers. And then that would help it get an even more accurate answer. And because the thing it cares about is not pleasing the humans, the thing it happened to care about internally was actually just getting an accurate answer. That plan looks great by its own lights. And so it goes and executes the plan. Right. So one that was really helpful. But I still feel confused about one why it's so hard to not give it some instructions that are just like, use whatever you need, but like don't hurt living things. So I think we could definitely give it those instructions. The question is, inside its own mind, what is its goal in the end of the day? So you could give it instructions, don't hurt humans, and it would read that and understand that that's what you wanted. But if throughout its life, it's always been rewarded for getting an accurate answer to these math problems, it might just itself only care about getting our accurate answers to the math problems. So it knows that the humans don't want it to hurt other humans. But it also doesn't care about that itself, because all it cares about is getting accurate answers to this problem. And so sure, it knows that humans don't want it to hurt other humans. And so it makes sure to not do that in an obvious way, because it anticipates and it might get shut down. But its knowledge of what humans want it to do doesn't change what its own desire is internally. So I suppose I understand why you couldn't just give the system an instruction that didn't also come with rewards. Is it impossible to give an AI system a reward for every every problem it solves by not hurting anyone? I think that would help somewhat. So the problem here is that there are kind of two possibilities. And it's going to be hard for us to give rewards that ensure that one of the possibilities happen and not the second possibility. So here are the two possibilities. One possibility is the AI really doesn't want to hurt humans. And it's just going to keep that and take that into account when solving the math problem. That's what we want to happen. The other possibility is that the AI only cares about solving the math problem. And it doesn't care about humans at all. But it understands that humans don't like it when it hurts them. And so it kind of doesn't hurt humans in any obvious way. Oh, right. Okay. And so this is a route to AI not caring about humans, but being kind of deceptive. I guess maybe an analogy that really speaks to me is something like if you were to punish a child for, I don't know, having ice cream before dinner, like you might get them not to have ice cream before dinner, or you might create a thing where they have ice cream before dinner while hiding in the closet. And it's like pretty complicated to teach a nuanced enough lesson to a child about ice cream and why they shouldn't have it for dinner. That doesn't have any risk of the lying version. Is that kind of right? Yeah, I think that's a good analogy. Okay, nice. I think with a child, it might be somewhat easier because you're much more capable than them. So even if they ever did try and eat ice cream in secret, you'd have a good chance of catching them. I think the problem gets really hard when the AIs are much smarter than us, such they could quite easily eat ice cream without us noticing. And then it's really hard for us to give them rewards, which stop them from doing that. Right, right. And something like we have a pretty good idea how children's brains work. They work kind of like ours, but like a bit simpler and like we have some idea of the ways they're different. And so we can like make guesses about the types of motivations that will speak to them. I don't know, maybe it's like we know that our kids work similar to us and that like they feel shame and they'd feel shame if they were punished and want to like please us because that's just like pretty human. And so maybe we have a better sense of how they'd respond to punishment. But maybe AI systems are just like so different to humans that we really have no idea what their or at least we'll have a less clear idea of what other processes that they're using or like things that they experienced or whatever are like and what kinds of behaviors those will push them toward. Yeah, I think that does make it a lot harder. Cool. That's super helpful. Yeah, is that is that basically the key scenario you're worried about? This kind of we train AI systems to achieve certain goals, but it's hard to know what strategies they see as fair game. And it's hard to train them not to pursue harmful strategies. And then they eventually get super smart. Maybe they are deceptive. Maybe they're just very convincing. And so they they're able to get a bunch of power and really disempower humans. Is that kind of what you see as the core risk? Yeah, that's right. And I would emphasize that in the scenarios I described it, AI is improving really, really rapidly as it approaches and then go through the human range. So, you know, when we're talking about this, this example with the AI that's trying to solve math problems, maybe we're thinking, oh, we'll have, you know, a few years with it trying out this kind of strategy. And then we notice it's kind of doing a little bit of hacking into computer resources. So, you know, we tamp down on that. But but if this whole thing plays out over just one year, for example, we go from like, you know, notably below human to superhuman systems. Yeah. I think it makes the risks a lot more intense. Yeah, that makes sense to me. Yeah, to move us on to your research, then. Some of the work you've done that kind of most blew my mind was actually on what happens when we're able to basically build AI that does roughly what we intended to do. I think I naively would have guessed something like the world carries on is normal, but we use GPT-8 a lot in our jobs. But you've looked into the hypothesis that not only will things not stay the same, they actually might change very, very, very quickly if AGI is so good that it kind of causes explosive economic growth, which yeah, in this case, you're defining as the world economy growing something like 10 times faster than it has for the last century. Yeah, so to start, can you help me understand intuitively what it would mean for the economy to grow 10 times faster? Sure. So one way to think about this is to think about all the technological changes that have happened over the last 50 years. Okay. Yeah, that feels like a lot. So 50 years ago, it was 1970, we'd had very basic digital computers around, but they weren't being widely used, they weren't very good. I don't think the internet was around. And there's loads of other improvements in manufacturing and in agricultural techniques. Medical care. Exactly. Yeah. And massive improvements across the board in the last 50 years, but probably the most striking is IT. Yeah, sounds right. And so what explosive growth would look like is that all those changes, rather than happening over the course of 50 years, they happened over the course of five years. So we're going to get the internet in five years plus a bunch of other improvements. Exactly. So rather than kind of it taking 50 years to go from these really rubbish slow computers that you could buy in 1970 to the awesome MacBooks of today, that just happens over five years. And similarly, rather than taking 50 years for you to go from those kind of rubbish phones to smartphones of today that also have the internet and all these specialized apps, that again, this happens over five years. So the kind of you see the introduction of a new technology and then very, very quickly you see it being refined into a super useful human friendly product. Wow. I mean, on the one hand that sounds kind of incredible and exciting. On the other hand, it just feels like a super strange world to be getting so many new technologies every few years. Can you explain the idea behind why AGI might even make that possible? Yeah. So here's the most basic version of the argument, which you can kind of make it more complicated to adjust various objections. But I think kind of this version captures the core idea. So today, there are maybe tens of millions of people whose job it is to discover new and better technologies, working in science and research and development. They're able to make a certain amount of progress each year. And it's their work that helps us get better computers and phones and discover better types of solar panels and drives all these improvements that we're seeing. But like we've been talking about shortly after AGI, I think there's going to be billions of top human researchers equivalents in terms of a scientific workforce from AI. And if you imagine that workforce or half of that workforce or just 10% of it working on trying to advance technology and come up with new ideas, then you have now 10 or 100 times the effort that's going into that activity. And these AIs are also able to think maybe 10 or 100 times as quickly as humans can think. And you're able to take the very best AI researchers and copy them. So if you think that scientific progress is overwhelmingly driven by like a few smaller number of really kind of brilliant people with brilliant ideas, then we just need one of them and we can copy them. They might be happy to just work much harder than humans work. It might be possible to focus them much more effectively on the most important types of R&D, whereas humans maybe are more inclined to follow their interests, even when it's not the most useful thing to be researching. And so all of those things together just mean that we'll be generating kind of 100 times as many new good ideas and innovations each year compared with today. And then that would drive the development of technologies to be at least 10 times faster than today. Right. How likely do you think this kind of growth is? Is it the default once we get AGI? I think it is a default. You could give objections to the argument I gave, but I think it's mostly possible to answer those objections. So you could say, well, discovering new technologies isn't just about thinking and coming up with new ideas, you also need to do experiments. Okay, sure. And then I think you can answer that objection by saying, that's right, we will need to do experiments. And that's like testing a drug on humans and maybe it takes five years or something to really check that it's safe and effective. Right. Yeah. Or you've designed a new solar panel and you want to like test its performance in a variety of conditions. Yeah. Yeah. Or you kind of running some experiments to see what happens when you combine these two chemicals together because you're not able to predict it in advance. But if you have a billion AIs trying to push forward R&D and they're bottlenecked on these needing to do these experiments, then they'll be putting in a huge amount of effort to make these experiments happen as efficiently as possible. Whereas today, we might be using the lab for 50% of the time we could be using it and we might be just doing a whole bunch of experiments and then analyzing it afterwards and learning a little bit from each experiment, but also not kind of trying to cram as much into each experiment as is humanly possible. If these AIs are limited on experiments and they're going to be spending months and months just meticulously planning the micro details of every single experiment so that you can get as much information as possible out of each one and kind of fully coalescing their theoretical understanding and all the current data and implications and saying, here are the key uncertainties that we need to address with these like kind of scarce experiments and they'll give the humans conducting the experiments really detailed and precise instructions and set things up so the experiments are really unlikely to go wrong and kind of analyze the resultant data from 100 different angles to learn as much as you can from them. And I think that will go a long way to getting over the experimental bottleneck. I mean, even if you just think you use labs eight hours a day, but you could use them 24 hours a day and then there are probably hundreds of other efficiencies like that that will all just add up to get to many, many times more efficient stuff. Right. And I mean, the AIs can direct humans what to do so you could be paying very high wages to have unskilled human workers work through the night to run these experiments directed by AIs telling them exactly what to do when. So yeah, you'll be able to have those labs working around the clock if that's what's wanted. In the longer run, robotics is already very good and I don't think it's going to take too long once we have a billion AI researchers to design robots that are able to do the physical tasks that humans do. It doesn't, you know, with that far off at the moment. And so, you know, if eventually, you know, we actually, we just need more humans to kind of build more labs or to run these experiments, I think it will be possible to have, you know, have robots doing that work and have the AIs directing it. And again, kind of meticulously planning what each robot is doing with its time. So we're getting the very most out of each robot. Right. I mean, the example you raised about human experiments is a really good one, because that seems like it's going to be particularly hard to speed up. Right. There are still a few things that I can already think of that could happen there. So if it's kind of any psychology experiments that you're wanting to do or knowing how humans will react to a new technology or to a new scenario, then just studying all of the human data on the internet and doing in-depth interviews with humans could give AIs a really good understanding of how human psychology works. Right. In the limit, they could scan a human's brain and kind of upload them to be a kind of a virtual digital person if that human was willing to do it. And then it could then do experiments with them in a simulation much, much more quickly. Right. Really fast. I mean, that's getting pretty weird. Yeah. And I'm feeling very sci-fi, but that's part of what we're talking about. We're talking about we've got millions or billions of copies of AI systems that are as smart or smarter than humans, basically using all of their brain power to innovate. And yeah, it's bizarre, but if you apply all of that brain power, you're going to get super, super fast improvements to technology. Is this bottlenecked at all by ideas getting harder to find? Are there going to be limits that just like a human would hit up on these limits or humans have, AI systems will also hit up on those limits? Or do we expect them not to, because we're talking about superhuman intelligence? So ideas are getting hard to find. For me, the kind of the best zoomed out example is just that our scientific workforce has been growing at four or 5% every year over the last 80 years. And that's a massive increase in that scientific workforce over 80 years, you know, many, many doublings, but actually the pace of technological progress, if anything, has been slowing down somewhat over the last 80 years. And so, you know, on a high level, the explanation is sure we're using way more effort than we used to be, but the ideas are harder to find. So we're actually slowing down a little bit in terms of the pace of our progress. Maybe the best illustration of that dynamic is physics, where a kind of 100 years ago, also you could have Albert Einstein in his spare time as a patent clerk come up with multiple very significant breakthroughs. And then kind of almost single handedly or with a few collaborators develop general relativity over the few years that followed, which is just a major breakthrough in our understanding of the universe. Whereas today, you have probably, you know, maybe millions of physicists, just kind of with the huge machines at CERN that have to be honest, making, I would say pretty incremental progress in advancing the state of knowledge and physics. Yeah, okay, right. Yeah, in terms of how it applies to AI, first thing to say is that even if that dynamic exists, and it's very strong, we would still expect a very significant, if temporary, increase in the rate of technological progress. So let's say ideas are getting harder to find, but then suddenly, in 10 years, we've got a billion AIs working on it rather than the kind of 10 million humans. Well, even if our ideas are getting harder to find, then at least temporarily, there'll be much faster technological progress, and then we'll kind of pluck even more of the low hanging fruit and eventually even these AIs get stuck. Right, maybe eventually we still stagnate, but it'd be pretty crazy if you added millions of people or millions of brains to the workforce and didn't get a bunch more technological progress. Yeah, I mean, specifically if you made the workforce like a hundred times as big, then yeah, especially with the other advantages I discussed about running 10 times as fast and you know, being really focused on the most important tasks, I think really surprising if you don't get at least a temporary increase. In fact, I don't think it would be temporary, and that's because one of the things that AIs can work on is actually building more computer chips to run AIs on, improving the kind of hardware designs for those computer chips, improving the algorithms that AIs run on, improving the designs of robots. Just making themselves better scientists. Exactly, and so we've been discussing how already the pace of progress in terms of the algorithms and the hardware is pretty fast, and I've already said that I expect it to be much faster once we have AGI. And so really this isn't a kind of a constant sized AI and robotics workforce we're talking about here, but if we choose to do so, then we could have the size of that workforce doubling every year very easily. And so that means you can overcome this problem of ideas getting harder to find, because you're not kind of dealing with a constant or slowly growing workforce. You're dealing with a workforce which is itself rapidly increasing in size, and so even if ideas are getting harder to find, you've got a bigger and bigger workforce to find it. And you can actually model out this dynamic. You can take the kind of the best models we have where ideas are getting harder to find, and you can say, well, if ideas are getting harder to find the one hand, but on the other hand, AIs are able to design better AIs and do all the improvements that I talked about designing better robots, etc. How does that dynamic play out? And it turns out that at least under these models, even if ideas are getting harder to find at kind of a very steep rate, then you still are going to get the kind of the AIs and robots winning that race. That's really wild. I guess those AI scientists might hit some limits, and do you have any ideas for what those might end up being? It's a great question. I think we are going to hit limits at some point. Eventually we won't be able to design better technologies. Eventually we'll have the best algorithms we can get for making AIs. I do think there are reasons to think the limits could be quite high. One interesting data point is that small animals are able to double their population size in just a few months, and even smaller animals like insects can double their population size in just days or weeks. That shows that it is physically possible to have a certain kind of biological robot that kind of doubles its own number in the scale of weeks or months. And with all of this kind of massive scientific effort that we've been describing, it seems possible that we'll be able to design kind of robots of our own that are able to kind of double their own number, kind of build replacement robots in a similar timeframe. And so currently you can try and estimate, well, if we used a factory to try and build another copy of itself, how long would that take? I haven't seen a good analysis of this, but when I've spoken to people, they've guessed it on the order of months. So that also supports this vague idea that it might be possible to get these robots that are able to kind of build extra copies of themselves and double their own number in just a number of months. So what that all suggests is that we could have a robot workforce which is growing at a really, really high rate. And by robot, I'm picturing physical bodies, but do you do basically just mean AI system that's like working on science? Or do you think physical bodies end up being important because we've got to start automating some of these physical tasks in addition to the cognitive tasks? I'm thinking, yeah, including the physical tasks when I'm talking about the robots. I mean, if these robots each weighed 50 kilograms, and we're able to produce robots as many robots in a year, such that they weighed as much as all the cars that we currently produce in a year, then we'd be producing around a billion robots each year. So already the manufacturing capacity seems like it is theoretically there to produce a huge number of robots. And that's before taking into account that it's lucrative. And so we want to create more. And yeah, unreal. So I do think that this this dynamic leaves us in a pretty crazy world where the size of the AI and robotics workforce is growing, potentially going very, very quickly. And probably as a result, it's hard for me to imagine technology really stalling out before we hit real kind of limits, kind of fundamental limits to how good technology can get. And there will be such limits. Technology can't improve forever to infinity. Ultimately, you've come up with the very best ways of arranging the molecules to get the desired technological behavior. Right. Okay. Do we ever hit limits on just physical stuff, the stuff we make the robots out of? I think we will. So I already said about the kind of that car statistics suggesting we can get pretty far just in terms of the massive robots we could produce just with the kind of manufacturing capabilities we already have. The earth is is massive. There are kind of mountains with all these kinds of materials in them all over the place. And if we run out of a particular material, which is currently useful for building robots, then these billions of AI as we have will be working hard to find ways of doing without that scarce material. And that's been a common pattern in technological development that you kind of find ways to switch out of things that are scarce. So, yeah, right, you know, it's hard to rule out that there's just some material that we just absolutely need and we can't do without and that that all next things, you know, maybe once we get to 100 billion robots or something. But it also just seems more likely to me that given just the abundance of just kind of materials that are in the earth, it's, you know, it's a big place. There's lots of different stuff there. We, it's not like we've mined everything there was to mine or even close to it. Right. We'll be doing, you know, using all the best methods for recycling and using things as efficiently as possible. It doesn't seem to me like those kinds of bottlenecks are going to kick in particularly early. And it's not out of the question that we'd have the technology to explore resources in space. I mean, that's even I'm adding more sci-fi here. But like, if we're doubling technological progress every, I mean, it sounds like you're talking about months at some point. Yeah, I think months is plausible. Okay, months is plausible. And so that might mean we, we aren't limited by earthly limitations. That's right. There is an interesting dynamic there where if we are kind of doubling the number of robots really quickly, and we're improving technology really quickly, then we kind of, we're not that interested in doing an activity which takes maybe like 10 years to bear fruit, because we're used to kind of our investments paying off with kind of doublings every year. We're like, oh, we could go to the moon and get materials, but man, like that would take so long. If we, if we just invest in everything we can find on earth, we can much more quickly kind of increase the just use it more efficiently. Yeah, so that time delay becomes more significant when, when you're already able to go so fast. So I think I imagine the kind of going to the moon and the space stuff happens when we're really kind of struggling to find ways to make use of the earth resources. Okay, so it sounds like we're kind of talking about something like AI systems replace humans in a bunch of sectors, really during our lifetimes. And then like our lives really, really change quite radically. And very, very, very quickly. Yeah, I guess I just find that super weird. I think my brain is like, no, I don't believe you. That's too weird. I don't like, I just can't imagine that happening. If we're, if we're saying this is happening in the early 2030s, I'll be, yeah, in my late 30s. And like all of a sudden, the world would be radically changing every year. And I like won't be working. I agree. It seems really crazy. And I think it's very natural and understandable to just not believe it when you hear the arguments. And that would have been my initial reaction. In terms of why I do now believe it, there's probably a few things which have changed. Probably I've just sat with these arguments for a few years. And just been like, I just do believe it. You know, I have discussions with people on either side of the debate. And I just find that people on one side just have thought it through much more. And I think what's at the heart of it for me is that the human brain is a physical system. There's nothing magical about it. It isn't surprising that at some point, we develop machines that can do what the human brain can do at some point in the process of technological discovery. And to be honest, that happening the next couple of decades is kind of when you might expect it to happen naively. We've had computers for 70-odd years. It's been a decade since we started pouring loads and loads of compute into training AI systems. And we've realized that that approach works really, really well. Just if you were kind of to say, okay, when do you think humans might develop machines that can do what the human brain can do? You kind of think it might be in the next few decades. And I think if you just sit with that fact that there are going to be machines that can do what the human brain can do. And you're going to be able to make those machines much more efficient at it. And you're going to be able to make even better versions of those machines, 10 times better versions. You're going to be able to run them day and night. And you're going to be able to build more. When you sit with all that, I do think it gets pretty hard to imagine a future that isn't very crazy. And another perspective is just zooming out even further and just looking at the whole arc of human history. So if you'd have asked hunter gatherers who only knew the 50 people in their group and who had been hunting using techniques and tools that as far as they knew had been passed down for eternity generation to generation, doing their rituals, if you'd have told them that in a few thousand years, there are going to be huge empires building the Egyptian pyramids and massive armies and the ability to go to a market and give people pieces of metal in exchange for all kinds of goods. They would have seemed totally crazy. And then if you'd have told those people in those markets that no, there's going to be a future world where every 10 years, major, major technological progress is going to be coming along and we're going to be discovering drugs that can solve all kinds of diseases. You're going to be able to get inside a box and land the other side of the earth. Right. Again, they'd have just thought you were crazy. And I think while it seems that we understand what's happening and that progress is pretty steady, that has only been true for the last 200 years. And zooming out, it's actually the norm throughout the longer run of history for things to go in a totally surprising and unpredictable direction or a direction that would have seemed totally bizarre and unpredictable to people naively at that time. Right. I feel like I was introduced to it when I read what we did in the future, Will MacAskill's book, that there's this thing called the end of history fallacy. It really feels like we're living at the end. We're done changing. We're going to maybe find some new medical devices or something. But basically, we've done all of the weird shifting that we're going to do. And I can't really justify that. It does seem like a fallacy. Presumably, things are going to look super different in 50 years. And sometimes those changes have gone super fast in history. And sometimes they've gone super slowly. And we've got real reasons to think that we might be entering a period of really fast transition. Yeah. I mean, if anything, I'd say the norm is for the new period to involve much faster changes than the old period. So Hunter Gathering went on for tens of thousands of years, if not hundreds of thousands of years. Then we started doing agriculture and forming into big societies and did things like the pyramids. And then a way that people often think of the next phase transition as being kind of the start of the industrial revolution and the beginning of kind of concerted efforts towards making scientific progress. And after we did agriculture, kind of new technologies and changes were happening on the scale of maybe a thousand years, or maybe a few hundred years, which is much faster than in the Hunter Gatherer times. And then today, after the industrial revolution, we're seeing really big changes to society every 50 years. So we've already seen kind of historically those phase transitions have led to things being faster. So that, I think, is the default expectation for what a new transition would lead to. Right. And it just feels weird to us because we're pre-transition. Possibly, whoever's living 50 years from now will just be like, yeah, obviously, that was coming. And those weird people living in 2023 thinking that they'd made all the technological progress they were ever going to make. Maybe I'm still struggling just to imagine what it would look like, I guess. Yeah, because it's possibly going to be us. What's in store for us? Is it going to be good? I think it could be really good. It could be really, really bad. It could be really good if we align AI so they're always trying to help us and help humanity do as best as it can by humanity's own lights. And the kind of benefits from AI and these new technologies are used to solve the world's most pressing problems and used to lift people out of poverty and give people the lives that they hope for themselves and for their children, solve the problems of climate change or poverty of disease. I think it could go really, really well. Right, right. And so in the best case where AI is really trying to help us, it's still kind of unimaginable to me as a world. I mean, maybe it's just like I'm so biased by the status quo where like, I need to work and I need to work to live, I need to work to help solve problems. Like in this best case, is there unemployment? Is there unemployment for everyone? Is it a slow transition? A fast one? Does it make inequality better because no one needs to work and we all have enough things? Or does it make it worse because some people have to work? Do we have predictions about that that are worth making? I think the default is that inequality would become greater because all of the wealth and useful work is coming from these AI systems, which I think by default will be controlled by a small number of people and companies. In the very best case, then you hope that that wealth is equally distributed or kind of much more equally distributed than it would be by default. And it is true, I think, that there's going to be so much progress made if AI is aligned that it will be very cheap to give everyone in the world the standards of living that are enjoyed by the very richest people today in terms of material comforts and health and actually much, much better on those fronts, I think after all this technological progress. So I think if we can get the AIs to be really trying to help us, then even if we mess up on things like the kind of distribution of benefits, even if we mess up a bit on that, then I think things will still look pretty good because there's just so much to go around. If there's kind of universal basic income, you could just use 1% of the output that's produced in a year to kind of give everyone kind of all the material and technological things they need to kind of meet all of their needs, all of the material needs. In terms of work, I think it will no longer be the case that you can produce a higher quality service or product than what an AI could do or a robot could do. One thought is that there will be some humans, maybe me and you, who just value human contact and hang out with other actual real humans. And that could provide a kind of work for those who want it. Role for humans. Okay. And another possibility is that we rethink the nature of work. So we do work to help each other and even though we know that AIs could do the work just as well, we're still happy to do that because it gives us a sense of meaning or we kind of do creative things instead like creative writing and drawing. And even though we know that AIs could do that better than ours, it's still enough for us to have a sense of purpose. I mean, people still play chess today and still really enjoy it and get purpose from it, even though they know that they can never go to match the best AIs. Right. Yeah, I guess I can imagine lots of people listening, hearing about this future and being like, no, I like the world the way it is. I like that humans get to make choices for ourselves as a society. I don't want AIs making it for us. I like that I have to work, get to work. I don't know. I don't, I can imagine people being like, no, I don't want AIs to be making the art. I want humans to be making the art. So is there some chance that there's, I don't know, like a movement that's anti AI growth that stops this from happening, even though it's theoretically possible? That's a great question. I do think it would be good for us to take this transition more slowly than is theoretically possible. And that might happen by default if we don't make specific efforts to go slowly. And so I think if people do try and delay or stop this, it could, it could be a good thing because I don't think we're prepared for that new world. Right. I think it's going to be very hard to permanently prevent this transition from happening. How come? One way to think about it is that there is some kind of upfront starting cost to get this transition going. So let's really simplify and say, today, if you spent a trillion dollars, you'd be able to train AGI and you'd have enough money left over to buy some manufacturing equipment for making robots. Right. And then you could have your AIs do research into better robots and making better AIs. And that whole process could lead to you having even more AIs and even more robots. And you could then grow your population of AIs and robots. And just with that trillion dollar initial investment, you could end up with this massive AI and robot population, which is then able to just start doing the scientific work needed to significantly accelerate technological progress. Right. Right. So the thing that's difficult is that that upfront cost will be falling over time. AI algorithms are improving, computer chips are improving. And so the cost to kind of training AGI and then just using it to build robots and to build more AIs and make money on the make money in the economy by selling its services and just kind of building up its own self perpetuating energy that ultimately results in making technological progress so you can sell more useful things to society that people want. That cost is going to be falling. And so let's say it was a trillion today, you know, in the future, it's going to be 100 billion and then it's going to be 10 billion. And there's going to be a lot of incentive to do this because it's going to grant whoever does it a lot of power. They'll have all these AI workers that they can use to do, you know, whatever they want them to do if they manage to solve the alignment problem. If they use it for designing new technologies and those new technologies could grant additional military power, or they could grant things that people all around the world desperately want like curing illnesses, like preventing climate change, like understanding and solving mental health problems, like life extension. So it's not just kind of economic incentives, it's not just like to get rich, it's like all sorts of motivations are benefited from paying this cost to get this hugely productive AI scientist workforce. Yeah, kind of whatever you want. Right. You can probably get it much more effectively if you have a billion AIs and robots designing technology to help you get it. And I think we can delay it, we can say okay, we're going to be really cautious, only a few people are allowed to train these systems and we try and convince the other countries to go slowly as well. But the thing that we could be, you know, that even 100 years after it's first been possible to train AI for a trillion dollars, that still no one has done it and no one is using it to make scientific progress, even though the cost is now like $10 million, it's really hard to imagine that we kind of prevent anyone from doing it as it gets cheaper. And I think sometimes people, when they're thinking about it, imagine that in order to get this kind of 10 or 100x faster technological progress, we'd have to be making a real effort and really kind of being super efficient and driven about it. But I think that's not the right way to think about it. It's more like by default, all you need to do is ask your AIs and robots, please do these tasks for me. And if you need to make tech progress along the way, do it, they will suggest the plans and involve making tech progress. They will get in contact with the labs and organize for the experiments to happen. You won't have to do anything. Right. And so I don't think it's going to require some kind of concerted pro growth enthusiasts to like really push for this. It's more like, you want stuff, the AI is going to try and do the stuff you want. And whenever they make tech progress, it's going to go really well. And it's going to really help you solve your problems. And you're going to just want to do more of it. Yeah, just enough time will pass, enough actors will think on it and decide to do it at some point. Yeah. So I guess I buy that the incentives are there for eventually an actor to want to build this kind of AI scientist workforce. It still seems like there have been enormously lucrative and beneficial technologies that we haven't pursued. So one example that comes to mind is like nuclear power, which like could help loads with climate change and would also be, yeah, again, super lucrative. And yet, yeah, we basically haven't done anything like what we could do with it. Could there be something similar? I mean, it's just kind of stigmatized, is like one reason we haven't. And I guess it's really expensive in particular the upfront costs, which like maybe just ends up true of this like AI world we're talking about. Yeah, it's a great example. I don't have a good understanding of what happened, but I think there are some big catastrophes with nuclear power, and then it became very stigmatized. And the regulatory requirements around it and the safety requirements became very large, much larger really than was reasonable, given that fossil fuel energy has damaging health consequences as well through air pollution. And as a result, it just became kind of a mixture of stigma and just the additional cost from all the regulation just prevented it from being rolled out. But I do think there are a fair few very significant disanalogies between that case and the case of AI. Okay, yeah, what are they? So one thing is that there were other sources of energy that were available. And so it wasn't too costly to be like, well, we're not going to use nuclear, we're going to use fossil fuels instead. And then, you know, even the green climate change concern, people could think about kind of developing solar panels and renewable energies. And in the AI case, that there is going to be no alternative. There's going to be no alternative technology which can solve all illness, and which can grant your nation massive national security and military power, and that can solve climate change. This is going to be the only option. So that's one disanalogy. Okay, that makes sense. Another kind of disanalogy is the cost factor. So with nuclear power, it's become more expensive at a time due to regulations. And that that's been a big factor in not being pursued. But the specifics around these cost curves with compute and this algorithmic progress patterns suggests that the upfront cost of training AGI is going to be falling really pretty quickly over time. Right. And so even if initially you put loads of regulations, which make it very expensive, it's really not going to be long until it's 10x cheaper. Right. And so permanently preventing it when it's when it's becoming cheaper and cheaper at such a high rate, it's going to be really, really difficult. Third is just just talking about the size of the gains from from this technology compared to nuclear power. So, you know, France adopted nuclear power and it was somewhat beneficial, you know, it's kind of now gets a lot of its powerful nuclear energy and that there's no climate change impacts and that's great. But it's not as if France is visibly and undisputably just doing amazing well as a country because it's got, you know, this nuclear power, like it's a kind of a modest addition, maybe it makes it look a little bit better. Right. But by contrast, if one country is is, you know, progressing technology at the normal rate, and then another country comes along and starts using these AIs and robots a little bit, you're going to see very significant differences in how its overall technology and prosperity and kind of military power is progressing. And then you're going to see that as countries dial up how much they're allowing AIs to do this work, that there are then bigger and bigger differences there. And ultimately, the difference between advancing technology at our pace versus advancing technology 30 times faster is over the course of just a few years, it becomes a massive difference in the sophistication of your country's technology and ability to solve all kinds of social and political problems. You know, a last point on this difference is that, you know, the US did in fact invest a lot of money in nukes shortly after the development of fish and power. You know, when it came to a matter of national power, they were very happy to invest in the technology, despite, you know, the risks which were clearly very high. All of the same risks. Right. Yes, you know, the incentives were out of whack and we didn't get nuclear fission power. But when it came to this kind of military technology for which there was no replacement, countries were very keen to do it and they made it happen. And AI driving significant technological improvements across the board is going to be a huge source of military power. Right. And so it's really hard for me to imagine that just no one ever uses it for that. And you've totally preempted my next question, which was like, can we definitely not come up with like an international treaty that is like the downside risks of this technology at this scale are possibly huge because alignment is so hard. And so we're all agreeing not to not to go forward with it. And I guess we had lots of reason to do that in the case of nuclear weapons. And we didn't. And we have lots of reason to do that in the case of biological weapons. And and we suck at it. We do not live in a world free of biological weapons or nuclear weapons. So I do think we should try. And I do think we can slow things down. And we can, you know, increase the requirements and the safety efforts required, maybe to make it 10 times as costly or 100 times as costly to develop this technology. I think that is that is one thing. And that's a big ask. And I think we should try and do as much of it as we can. Even if we can do that, it's a whole different thing to talk about permanently choosing to never develop the technology, even after we've made maximal efforts into making it safe, even after all the safety tests are saying it looks like it is safe, even when millions of people are dying every year from illnesses which we know could be prevented if we allowed the AIs to do research into treating it. I think just permanently not going forward with using AIs and robots to make that technological progress. Like I said, when it's becoming cheaper and cheaper and other countries and other companies, you know, might want to do it, that does seem like it's just very unrealistic. Just implausible. And maybe not desirable either, to be honest. Like after a certain point, like we should take it really cautious. Right, after 200 years of like research into air alignment, even if we're like, ooh, this seems weird and scary and might change the world as we know it, at some point there are going to be incentives for some actors, countries or companies to try to deploy this technology at scale to solve problems like poverty and illness, maybe also to get military advantages, to solve problems like climate change. And those incentives might just be so strong that even if we take our time, we'll probably eventually do it. Someone will. And once that happens, progress will become so quick that we're looking at really economic growth at a pace that's, I guess, still kind of unfathomable to me. But that is this kind of thing where progress we've seen over the last 100 years happens in the next 10, and actually just keeps getting faster and faster and faster. Is that kind of the picture? Yeah, that's the picture. That's really weird. It's scary. I guess it's also quite hopeful. I mean, if I let myself hope for that good world where we use it to solve problems, I feel nervously really, really excited. But I guess we've got some real, real challenges to overcome first. Okay, let's move on to a related topic you've been researching more recently. So you've just written the draft of a report on AI takeoff speeds that has some pretty alarming results to me, given everything we've just talked about. And I guess just to get on the same page about language, what exactly do you mean when you talk about AI takeoff speeds? Roughly speaking, capabilities takeoff speed is the question of how quickly AI systems will improve as they approach and surpass human level intelligence. And so the capabilities are like their ability to do things like drive cars or program new programs. Exactly. So a fast takeoff speed could be that in a, you know, in three months, AIs go from mouse level intelligence to significantly more intelligent than the smartest human, right? Where a slow takeoff speed that could be happening over decades. Got it. Okay. So we're just like slowly making progress next year at self driving cars. And then it takes another 10 years to get to programs that can write other programs. So I guess, you know, you've written this report, and I won't give away the results yet, but it gives some evidence about how fast we might expect AI takeoff speeds to be. But before we get to that, before you wrote the report, what did you think was the most compelling evidence that AI takeoff speeds would be particularly fast? So I think the most convincing argument is related to how humanity's capabilities were improving much more slowly 50,000 years ago, then they are improving today. And the attempt to draw an analogy to what might happen with AI capabilities. So if we think that a million years ago, humanity's kind of cognitive abilities collectively were maybe doubling every, let's say 100,000 years, I don't know. That's just kind of something to represent their kind of slow increase in brain size and capacities. The exact number doesn't matter. You know, whether you want to say it's, you know, 100,000 or 10,000 years is a very slow doubling size in their abilities. Right. And that's basically because like slowly, they're evolving to have slightly bigger brains, we're like adding a bit of prefrontal cortex, and like the population is getting a little bit bigger over time, but grew very, very slowly. So collectively, it's just, it takes thousands of years to double. Yeah, exactly. Cool. Cool. Okay. Whereas today, our abilities are improving incredibly quickly. As a society, our population growth is much faster. Our command of technology is much faster. And, you know, in the last, like I said, in the last 200 years alone, we've doubled the economy many, many times and doubled our ability to understand and manipulate the world very many times. And if you think that there'll be some analog of that transition as we approach AGI, then, I mean, AI is already improving really quickly. You know, I would say it's kind of doubling its abilities in less than a year at the moment. And so if, you know, if that's this kind of the slow initial pace, then, you know, the new pace would be kind of blisteringly quick. Right, right. Unimaginably quick. Yeah, exactly. The way I would think about it is that a million years ago, humans weren't able to do science and to discover technological improvements much at all. And so they didn't have access to this additional feedback loop of improvement. Where you discover new technology passed on to the next generation, then they start on a better place, can discover even more technology, you can now support a bigger population. There's this whole feedback loop, which arguably we couldn't access a million years ago. Then we improved our cognitive abilities as, you know, as humans a little bit. And then suddenly we got over this threshold where, okay, we can access this, this kind of doing technological progress feedback loop, which then, you know, then speeds up and speeds up as we develop agriculture and then we develop text, you know, written language and we develop maths. And it's kind of, we're now even better at discovering new technologies. And the thought is in my mind that maybe there's something similar that happens with AIs, whereas today they're not, you know, they're not that smart. And so they're not able to access a certain feedback loop. I'm not sure exactly what that feedback loop will be. But at some point, they become smart enough that there's this additional feedback loop that they can use to improve their capabilities, kind of like how humans use technology to improve our capabilities. You know, one, one funny thing about this argument is that it's not clear what that new feedback loop might be for AIs. And so that leaves me a little bit puzzled over where to go with this argument. So for humans, it's, you know, we discover the scientific method and we experiment on things and we built computers. And now we can like run programs that help us do science. But for AI, it's like, we don't even know what kinds of science they'll discover that's like beyond ours. And is there some question about whether there even are kind of higher orders of science that we can't, that we haven't developed, but that AI systems might to kind of increase their own feedback loop? Yeah, I think that's, I think that's right. And you know, you can, you can throw out ideas for what that might look like. Right. Maybe it's the AIs learn to work together in a team in a way that is way more efficient than what humans have ever done. Yeah. Or maybe they run simulations or something to like learn about economics or something in a way that we can, we can barely understand because we only see the economies run in these weird real, real world scenarios. For example, for example, cool. Okay. Yeah. Are there, are there any limitations to that argument? Or do you do just kind of buy it? Yeah, I'm, I actually don't put that much weight on, on this particular argument. Oh, interesting. So the main reason is that evolution was not trying to make humanity as a whole as capable as possible. And it wasn't trying to make humanity as a whole good at science. Right. So from that perspective, it's not actually as surprising that humanity went from really sucking at science to being really good at science in a fairly short, short timeframe. Yeah. So here's an analogy. Before 2020, we hadn't made many COVID vaccines, not because we couldn't, but because we weren't trying to, we were focused as a society on doing other things. Then around 2020, it became really useful for us to make lots of vaccines. And then, lo and behold, the number of vaccines went up very dramatically. Now, that doesn't mean that our kind of abilities in vaccine making suddenly went up. It just means that we reprioritized, reallocated the resources we already had towards making vaccines. Okay. And I want to say that that's somewhat similar to the way in which kind of our ancestors a million years ago, we weren't that good at science, but evolution wasn't trying to make us that good at science. It was mostly trying to make us hunt successfully, feed our families. Okay. Science was like maybe a tiny, tiny bit useful back then, because it maybe allowed you to discover something with your own lifetime, but it really wasn't very useful. Then I think more recently, maybe more like kind of tens or a hundred thousand years ago, it did become more useful for humans to do science and to be flexible learners. And so it's not that surprising that at that point, say 50,000 years ago, where it was more useful for humans to be good at science, evolution then reallocated those cognitive resources of humans to being good at science. Right. So that kind of reallocation by evolution from kind of just for foraging and then reallocating those cognitive resources to doing science is kind of like human society, reallocating its resources to make COVID vaccines. Okay. Yeah, that's really, really helpful. So something like a combination maybe of language, maybe of just like group living, maybe some things that I don't understand, made it much more beneficial to be able to learn new things and learn like a range of things, not just the same things over and over again. And so a couple of tweaks in the brain was enough to make the brain that we'd been using for very specific set of tasks become useful for just like a much wider range of tasks. And that wasn't like really fundamentally altering like the amount of brain we have, but like how we use it. And that capacity already existed. And like you said, was repurposed. So yeah, here's how you would relate it to AI take off. You'd say that in the case of evolution, evolution wasn't initially trying to make humans good at science. And so no massive surprise that it's able to quickly make some tweaks that make humans good at science late in the day. But with AIs and with AI development, humans will be at every stage trying to make AIs as useful as possible for doing economic tasks, helping with science and research. And so we wouldn't expect there to be this kind of overhang where the AI has these abilities, which it's just not using, because we would expect humans to be trying to coax those abilities out at every step of the way. And so if if you're constantly trying to coax abilities out, most likely you'll only find ways to do it incrementally, as opposed to like, if it happened to be the case that like, I don't know, we found a billion computer chips on another planet, and could just use them to train up a bunch of AI systems, then we'd expect the step change. But like, currently, everything is just increasing incrementally, we're increasing chips incrementally, we're increasing algorithmic progress incrementally. And so it's just going to keep improving at a kind of incremental pace. And I think crucially, we have to think that we're currently at each step of the way trying to use the most recent algorithms and most recent compute to actually get AIs to do, let's say, useful science research. You know, if we're incrementally increasing the computing algorithms, but we're not actually trying to get the AIs to do useful science research, then it could be that one day we decide to try and get AIs to do useful science research, and then suddenly, we train them to reassign all their cognitive abilities to that task, and we do get something that's really quick. So it's a really important assumption here that in some sense, the AI development ecosystem is kind of efficient. Yep. And it's using, yeah, it's using new AI capabilities to do like the cutting edge research, as opposed to like, if there were only market incentives to make AI that, I don't know, made these beautiful images like Dolly, and no incentives at all to use AI to improve AI systems, then if one day we made a few tweaks to Dolly, and we're like, stop making pictures, make programs instead. And but like actually, like it did have capabilities related enough that we could make that tweak semi easily. Then all of a sudden, we'd have this system that could write programs really efficiently that we'd never had before. Right. Or maybe a more probable scenario would be we're just using all our AI resources to train these image generating systems like Dolly. And then we're like, you know what, why don't we just try using all those resources to train a science AI? And then we, you know, we pick the architecture to specialize of a science, we use the data to specialize it for science, we use all the compute that we were previously pouring into these image generation systems. And then suddenly we're like, wow, our science AI is amazing. And it came out of nowhere because we hadn't been trying in the previous years to do this at all. Yeah. So are we currently trying to make AI systems that are really good at science? I think it's a good question. The market doesn't seem to me to be super efficient. I've been playing our GPT for a bit recently. And to me, it looks like GPT-4 is pretty smart. It doesn't seem to me like its cognitive abilities have been really direct in the direction of helping to advance science, to be honest. So I do think that this argument could ultimately say, yeah, a faster takeoff is plausible and the mechanism could be reallocating the AI's cognitive abilities towards science. I mean, GPT-4 is just trained to predict the next world on the internet. Right. That's a very different kind of task than the task of advancing science. And so I think that that is a reason to expect a faster takeoff. More of a jump. Yeah. Okay, interesting. I haven't heard that argument before. Cool. Well, I want to now get to the report that you've written on AI takeoff speeds, which asks how quickly AI might go from kind of pretty economically valuable to just extremely capable, maybe as good as humans. And yeah, I guess you define your terms pretty clearly in your report. So maybe we should start by doing that. Am I right in remembering that you are trying to answer the question of how quickly we'll go from AI systems that can do 20% of human tasks to AI systems that can do 100% of human tasks? Is that right? Yeah, that's right. In particular, I am restricting to cognitive tasks. That's similar to what we discussed earlier. It's any task that you could do remotely that doesn't require you to be physically manipulating objects yourself because AI's don't have physical bodies. That wouldn't be included for them. I mean, it does include tasks like giving instructions to a human who's doing a physical job telling them where to move things, what to do with their arms, or potentially giving instructions to robots that are doing physical tasks. But it doesn't include the kind of the physical motions themselves. So does it include things like driving cars? Yes, it does. Okay, it does. And that's because driving cars is really a set of algorithms. And you can turn the wheel of car. So it's a good point that currently the way humans drive cars is by physically moving various levers in the car. But I think actually giving the AI the control of the steering wheel and of the pedals and the brakes is actually pretty trivial. So the only thing that's hard in practice about getting AI's to do driving is the cognitive parts of what should the car be doing at each point. Yeah, right. Okay, so then an example of a task that isn't included is something like helping people move house. It's like carrying the boxes in and out. Carrying the boxes in and out would not be included. Yeah, okay. But telling them here's the best plan for moving, here's the order you should move the boxes in that would be included. Okay, so that's basically what you've done. You want to know how fast do we go from 20% of cognitive tasks to 100% of cognitive tasks? Yeah, can you actually clarify what it means for AI to be able to complete 20% of tasks? So you could say, okay, let's say we can AI automate driving, what percentage is that? Is that 3%? Because it's, you know, 3% of people do it? Or is it, you know, do we just like give, we look at a long list of tasks and assume that it takes up an equal percentage? Like, what do we mean by 20%? Yes. And so the way I'm currently thinking about that is you look at how much people pay for those tasks to be performed in the economy. Okay. So let's take the driving example. I don't know. Let's say that drivers around the world are being paid $2 trillion a year for the work they're doing, driving trucks and taxis and everything else. In that case, because $2 trillion is 2% of the global GDP, I would say that automating driving, fully automating all driving would be automating 2% of all economic tasks. Got it. And then you're saying, how fast will we go from we can do 20%? So I don't know, maybe it's like replacing all drivers, maybe it's replacing all journalism because GPT-4 seems to be really good at writing. And I don't know, a couple of other, a couple of other things. How fast do you go from like that chunk to literally all cognitive tasks, including, I guess, science, AI, R&D. Now, one complication is that from year to year, the amount that is paid to people to perform each task might change. So in 2020, maybe drivers are paid $3 trillion a year for their work, maybe in 2025, they're paid $4 trillion a year, and that could change. So I'm pegging these percentages to the year 2020. Got it. It's a kind of arbitrary choice just to make the definition unambiguous. Yes. Okay. That makes sense. So what were they paid in 2020? And even if, I don't know, like wages change for all sorts of reasons, including AI taking over some jobs, we're still just thinking of what percentage of the 2020 cognitive economy is being automated. Exactly. And it is really important to keep that in mind, because typically when AI automates a certain task, it becomes really cheap to do that task. Right. So it becomes a much smaller fraction of the economy. Exactly. Yeah. And so you can end up thinking that AI is never doing anything when it's actually done almost everything. And so that is just an important thing to be aware of. Yeah. That makes total sense. Is there an analogy from the Industrial Revolution or something? The best analogy might be agriculture. So I think in 1500, basically everyone worked in agriculture. Right. It was 90% of the economy or something. Exactly. All of GDP would have basically been agriculture. Today, I think it's less than 5%. And that's because we've become really good at producing food with very little need for human labor. So it's not to say that fertilizer and trucks and really highly productive seeds aren't contributing a bunch to the economy, but clearly they have. But were you to measure it as a fraction of the GDP that they're responsible for? It would be smaller because everything's just gotten so much cheaper because of them. Exactly. Cool. Okay. That makes a bunch of sense. Nice. Okay. So we've got definitions. So you've asked how fast will we go from AI systems that can do roughly 20% of the cognitive tasks that humans were doing as of 2020? And how quickly will we go from 20% of those tasks to 100% of those tasks being at least in theory able to be automated by AI systems? So yeah, what was your result? So the conclusion from the report is, I guess, pretty scary. The bottom line is that my median guess is that it would take just a small number of years to go from that 20% to the 100%. So I think equally likely to happen in less than three years. As it is to happen in more than three years. So a pretty abrupt and quick change is the kind of median kind of best guess median. Wow. And do you believe that in your bones? Does that feel like like very plausible to you? Yeah, I do. So some some quick things about why why it's plausible. Each year, once you take algorithms, better algorithms and using more compute into account, we're currently training AIs each year that have kind of three times bigger brains than the year before. So really rough way to think about it. But you know, imagine three times smaller brain than humans. That's chimpanzee brain size. Right. Each year, you're going from chimpanzees to humans? That's, I think, you know, it's really hard to try and account for the effect of the algorithmic improvements. But on my kind of best guess of what those amount to, yeah, each year, we're making the brains of AI systems about three times bigger. Wow. And right now, it's humans that are doing all the work to improve those AI systems. As we get close to AIs that match humans, we'll be increasingly using AI systems to improve AI algorithms, design better AI chips. And so overall, I expect that pace to accelerate absent a specific effort to slow down. Right. So rather than three times bigger brains each year, it's going to be going faster and faster five times bigger brain each year, 10 times bigger brain each year. And I think that that just already makes it plausible that there could be just a small number of years where this transition happens where AIs go from much worse than humans to much better. But to add in another factor, I think that it's likely that AIs are going to be automating AI research itself before they're automating things in most of the economy. Right. Because that's the kind of the task in the workflow that AI researchers themselves really understand. So they would be kind of best placed to use AI as effectively there. There aren't going to be kind of delays to rolling it out or trouble finding the customers for that in the same way. The task of AI research is quite similar to what language models are currently trained to do. They're currently trained to predict the next token on the internet, which means they're particularly well suited to tech space tasks. Right. And the task of writing code is one such task and there is lots of data on examples of code writing. Oh, I see. So it's like typically, I don't know that much about coding. Is it basically also token prediction? That is how current coding assistants work, I think, is that they're looking at your kind of, you start writing your code and they predict what's going to follow. Like one way of putting it would be by the time that the AIs can do 20% of cognitive tasks in the broader economy, maybe they can already do 40% or 50% of tasks specifically in AI R&D. Right. And so they could have already really started accelerating the pace of progress by the time we get to that 20% economic impact threshold. I mean, at that point, you could easily imagine really, it's just one year, you give them a 10x bigger brain, that's like going from chimps to humans and then doing that jump again. That could easily be enough to go from 20% to 100% just intuitively. And I think that's kind of the default really. That's terrifying. Yeah. And I think there's even more pointing that direction. I think that already we're seeing that with GPT-4 and other systems like that, people are becoming much more interested in AI, much more willing to invest in AI. The demand for good AI researchers is going up. The wages for good AI researchers are going up. AI research is going to be a really financially valuable thing to automate. If you're paying $500,000 a year to one of your human research engineers, which is lower than what some of these researchers are earning, then if you can manage to get your AI system to double their productivity, that's way better than doubling the productivity of someone who works in a random other industry. Just the straightforward financial incentive as the kind of power of AI becomes apparent will be towards, let's see if we can automate this really lucrative type of work. So that's just another reason to think that we get the automation much earlier on the AI side, then on the general economy side, and that by the time we're seeing big economic impacts, AI is already improving at a blistering pace potentially. Okay, well that's, yeah, again, really scary, like really genuinely very scary. I completely agree. Do you have a guess at what percent of cognitive tasks AI can currently perform? It seems like we're really far away from 20 percent. Yeah, intuitively, I think it seems like we're far from 20 percent because AIs aren't doing that much in the economy. If I looked at a list of the kind of cognitive tasks people were performing in 2020 and what they were paid for them, it's not as if AIs are ready to kind of replace a big fraction of that labor. So that's just the 20 percent it's far off. I'm actually less confident that it's far off than I used to be than if we would have had this interview six months ago, because just seeing GPT4's performance, firstly, just doing really well on just a whole wide range of university exams and other formal tests without having specifically trained on that. And then me kind of playing around with it and thinking, yep, just seems smarter than most people I talk about this stuff with. Most of my start friends wouldn't be this smart. I'm thinking maybe actually if you just put some work into specifically applying GPT4, you could automate quite a large fraction of the cognitive tasks. It does seem it does seem much more plausible to me that maybe you could get to 10 percent today or within a year. Wild. That's super interesting. I mean, I guess, yeah, I was also blown away by GPT4's performance on, yeah, the SAT, the LSAT. For anyone who hasn't looked, we'll stick up a link to those test results. I think it was performing, I mean, much better than I ever did on my AP tests in high school and better than I did on the GRE. So it's like, it's beating me. And I think beating out loads of other people already. Maybe I'm over or putting too much weight on the fact that like, currently, not that many people I know are using it to do anything. And it sounds like your impression is like, maybe it's pretty close to being able to do a lot. Yeah, to actually in practice, replace 20% of the tasks that people do. It's actually a pretty tough thing to do. Because, you know, for myself, all the different parts of my workflow are very much entangled up together. So I can't easily take out a 20% chunk and be like, oh, GP4, do this chunk, because it's all kind of mixed up. Sure. And so historically, the way that automation has worked is that we've got a new technology and then we've spent decades readjusting our workflows so that we can neatly parcel out, you know, 20% of our workflow for this new technology to do. And the technology can be fairly dumb, because we've kind of really neatly parceled out that part of our workflow. Yeah, do you have an example? Yeah, so let's say moving over from paper records to computer records. So I used to have maybe people used to have to write down lots of paper records and maintain a filing system for their information. And these days, a lot of that work is done automatically by computers and data storage. But at first, you know, it wasn't that easy to immediately move over to the computers. And it took decades as people were like, you know, got rid of the paper stuff and got used to kind of teaching the other employees to use the computers and, you know, got used to using their customers to fill out online forms rather than filling out the paper forms that they're using before. And all that rearranging of workflows took a long time to happen. And so one scary possibility is that if AGI is is just 10 years away, then there won't be time to do that rearranging of workflows that is necessary to get, say, GPT-4 to actually automate things in practice. And so the 20% automation ability won't happen through some kind of dumb system that that I've kind of parceled out a nice thing to do. It will actually happen with a really smart system that basically understands my whole workflow well enough to be able to do 20% for me, which means that it could be pretty close to just being able to do all of it. Right, right, right. I mean, I've almost got that impression with GPT-4 and my job. Like, we asked it, how can you help us make the 80,000 hours podcast? And it was like, I can help you come up with guests. I can help you write interview questions for the guest if you tell me what they worked on. I can help you. I mean, it basically rattled off a list of things. And I was like, as soon as it has a voice, like, that'll be it. That'll be it for me. And I think I thought it would help me with subtasks first. I think I thought maybe it would help me with like generating titles and like, I don't know, maybe giving me summaries of people's work so that I could read them a bit faster. But I think in fact, it's actually going to be really great at like the start to finish interview process. Yeah, soon enough that I'll just skip over all of that, which I don't know if that's true, but it seems it doesn't seem crazy to me. And so maybe that's just another actual example of what you're talking about. Yeah, yeah. So in that example, by the time it can automate 20% of the kind of tasks you're doing, it can almost do all of it. Right. Yeah, makes sense. Okay, so next, I want to dive into your methodology a bit more deeply for this, yeah, this takeoff speeds prediction. It's such an alarming result that I've, yeah, this urge to understand what's going on a bit better. Yeah, so that headline result, this prediction that AI takeoff might only take a few years, is basically based on an economic model that you made that tries to answer the question of whether you can get human level AI just by increasing the amount of compute that we have to train our systems. So in other words, kind of without paradigm shifting algorithmic breakthroughs. Yeah, to start us off, can you actually remind me what compute is? Okay, so compute is a measurement of how, how many calculations you need to do or a given computer is doing. So let me give an example. So the most common unit for measuring compute is a flop and a flop is adding together two numbers or multiplying together two numbers or dividing them or subtracting them. Okay, so mathematical operation. Exactly. Currently, when we develop AI systems, the way we do it is by doing loads and loads of these calculations of adding things together, dividing them, minusing them. And by doing all of these calculations, that is, that is how the AI decides what, what it's going to do. And it's also how we, how we train the AI in the first place. Got it. You could, you could analogize these calculations to the kind of firing of neurons inside our own brain. Okay, so flops are basically just mathematical operations or things like, are both of these things true or something like that? And then compute is, sorry, it's something like how, how many of those calculations we can do? So one flop is just one of those calculations. Some of the biggest language models today are trained with, I think, 10 to the 24 flop. So that is a million, million, million, million calculations. That's how many calculations you need to do to train some of today's big language models. So the amount of compute is another way of saying how many calculations did you need to do it. Got it. So as compute, compute is the amount of computation you need to do and a flop is the kind of unit of how many, yeah, okay, cool. So then you're, you're asking this question about AI takeoff speeds. And we're just assuming that we, we increase compute, which is made up of machines, but also has to do with algorithms as well. So like, is it true that we need less compute if the algorithm is super efficient, because the algorithm just requires that you do fewer calculations to get the same result or something? The strict assumption is that if we used the algorithms that were available in 2020, then there is some amount of compute such that if God handed a top AI lab, that amount of compute, and they had a few years to adjust the algorithms to using that much more compute, then they would be able to train AGI using that amount of compute. So then in the model, we make an assumption about how much compute would have been required. We actually, we actually put a probability distribution over it. But importantly, algorithmic progress can reduce that computational cost over time. Got it. So maybe in 2020, you'd have needed 10 to the 30 flop to train AGI. But maybe by 2025, your algorithms are 10 times better. And so you only need 10 to the 29 flop to train AGI. And so the basic dynamic in this framework is that in each year, our algorithms improve somewhat. And we decide to use more compute in a training run than we had done in the previous year. Right, because it's profitable, etc. Exactly. And those two factors combine together. So let's say we use twice as much compute as the previous year, and our algorithms are twice as good. And that means that the effective compute that we're using is four times bigger. Right. So it's the equivalent as if we hadn't improved our algorithms, and we had just used four times as much compute in the training run. So you're using effective compute to mean something like you want an AI system to, for example, predict the next word in a sentence. And one way you can increase the kind of effectiveness of that out of that system is by like giving it more compute so it can do more calculations. But you'll also have another dynamic where the algorithms are getting better such that you need less to do the same thing. And so you're doing equivalent processes or you're doing you're getting like equivalent outcomes for less physical compute. That is tricky. One way to think about it is, let's say in 2025, we do it, we use a certain amount of compute in a training run with 2025 algorithms. Imagine if we'd have been forced to use the 2020 algorithms. How much compute would we have needed then to get the same result? Right. Got it. That is the amount of effective compute that we actually used in 2025. Great. That makes sense to me. Okay. So you are, you're thinking about effective compute, and you're making some guess about how much we'll need to get 100% of cognitive tasks automated. Yeah. How are you making guesses about how much effective compute we'll need to get 100% of cognitive tasks, um, automatable. So in the report itself, I just defer to a different report by a colleague of mine called the bio anchors report, which asks that exact question. In fact, I don't think you need to be deferring to the bio anchors report that there are, you know, different approaches you can take to estimating how much effective compute you might need to train AGI. And you could use whatever approach that you like, then bring that into, into my framework and use it to inform your, your initial guess, um, of the effective training compute for AGI. Right. And you've got a model that people can play with. So if you're like, I think the bio anchors report is way too optimistic about how much, uh, compute it'll take to get AGI, you can 1000 exit, um, and see, and see how that changes the outcomes. Exactly. Super cool. Okay. So we'll, we'll stick a link up to that model so people can play with it if they want to. Do you mind, um, giving me an intuitive sense of how much compute you and, and your colleague, um, basically think it'll take to get AGI? So in the current median value I use in, in the report is, is very large indeed. It is 10 to the 36 flop. So that is, if you take the amount of compute that was used to train the biggest language models that publish their training requirements, and then you use a million times as much and then a million times as much again, that's how much the assumption is making. So, so I actually now think that that, that assumption is too high. Is that because of GBT four and how, how impressive it is basically largely? Yeah. GBT four and the kind of the fast pace of recent improvements is quite a lot faster than I would have predicted. Um, and so yeah, I would, I would now be using it a lower value for that important, important parameter, uh, which would make take off even faster than I'm predicting even faster. Yeah. Geez. Yeah. The kind of the report that I wrote uses, yeah, this 10 to the 36 as its, as its median estimate for what you'd need to train AGI. Okay. Cool. That's helpful. Okay. So, so that's how you basically estimate how much effective compute you need to train AGI. How do you use that to predict, uh, yeah, AI takeoff speeds? Right. So we have this, um, assumption about the effective training compute for AGI, which was our hundred percent kind of endpoint. We then need to make an additional assumption about what would be the effective compute needed to train AI that could automate 20% of tasks. So let's say that we assumed, for example, that you need 10 to the 30 flop to train AGI using 2020 algorithms, that'd be 10 to the 30 kind of effective compute. We then make an additional assumption about how much less effective compute you need to train AI that could automate just 20% of tasks. So I kind of want to pause on that last assumption because it's so important. So that, that, that assumption about how many more times compute you need for AGI compared to 20% AI, that assumption is what I'm calling the difficulty gap because it's saying what is the gap in difficulty between training 20% AI and training 100% AI or AGI. And then we're measuring the size of that difficulty gap in terms of how many times more effective compute you need to train one than the other. And you're calling it the difficulty gap because it's kind of describing how much more difficult the most difficult tasks are relative to the, the easiest 20%. The reason I call it the difficulty gap is, is to refer to the difficulty of developing the AI in the first place. So it's kind of like how much more difficult is it to develop an AI that can do 100% of the tasks than it is to develop an AI that can only do 20%. Got it. But it might be 10,000 times as difficult, or it might be barely more difficult at all. If, if it turns out that once you're 20% there, you're basically the whole way there. Right. I guess is that possible? Maybe it is if like the first 20% of tasks includes AI R&D. So that's an interesting scenario to think about. You could have the first 20% of tasks, including all of the tasks of AI R&D. So what I think would happen in that scenario is that once you've done those first 20% of tasks, AI would be improving super, super quickly, absent a specific effort to slow down, you know, within I think a few months, you would already be able to do a training run that used 100 times more effective training compute as you had previously done. And that's because that's because you would have hundreds of millions of AI's that could be working to improve the AI algorithms and maybe making money so you can buy more AI chips or convincing other people to kind of share their compute with you. And then that would be enough to very quickly allow you to use 100 times as much effective compute. Cool. Okay. Yeah. So I guess maybe you think that which tasks end up being easier also plays into how fast AI takeoff speeds are. Yeah. I guess in particular in the case where AI R&D is in the first 20% and otherwise maybe it doesn't matter as much. Exactly. I think that with that last example, even if there was a big difficulty gap from 20% to 100% of cognitive tasks in the economy, if you get all the R&D tasks within that first 20%, then I still think you'd get a very quick transition. Right. Okay. So that could be an example with a big difficulty gap where you nonetheless you still get a very fast AI takeoff. Yep. That makes sense. Okay. So do you basically just make an assumption about how big that difficulty gap is? Is it a range? And how did you come up with whatever numbers you're putting on to? Yeah. How many times harder it is to get to 100% of tasks? So I do consider as much evidence I can for the difficulty gap. It is really, really important. The lines of evidence that I consider are all pretty limited. So it's a very uncertain parameter, but I think there are some things you can learn from some of those lines of evidence. Okay. What's an example of some of the evidence you would have looked into? In terms of evidence for the difficulty gap potentially being pretty small, we've already touched a little bit upon some of that. So one line of evidence is the scaling of human cognitive ability with human brain size. Some humans have slightly bigger brains than others. Not only a small variation, plus or minus 10% or so, but you can then look at, okay, if one person has a 10% bigger brain, then on average, how much better do they do on various tests of cognitive ability? And the difference isn't massive, but if you extrapolate that difference to say, okay, what about if it was a three times bigger brain or a 10 times bigger brain, then extrapolating that suggests that there would be a very large difference in cognitive abilities from getting a brain that is that much bigger. Interesting. The takeaway from that is that this particular line of evidence suggests that increasing the size of the brain by a factor of 10 could be more than enough to cross this difficulty gap. And that by analogy, increasing the number of parameters in an AMI model by a factor of 10 could be more than enough to cross the difficulty gap, which would require you to increase the effective training compute by a factor of 100. Okay. And I think even this analogy actually suggests that just increasing the effective training compute by a factor of 10 might be enough as well, because it could just be enough to increase the human brain size by a factor of three. Right. So this particular line of evidence really suggests that the difficulty gap could be pretty narrow. Pretty small. Yeah, yeah, yeah, yeah. Okay. Got it. Yeah. Is there more evidence about how big that gap is? So a very similar line of evidence looks at rather than differences within humans, looks at the differences between humans and other animals. Chimps have brains that are about three times smaller than human brains. And you might think that going from chimp level intelligence to human level intelligence is enough to cross that difficulty gap. And if you do think that that then again suggests that just increasing the parameters in a model by just a factor of three could be enough to cross that difficulty gap. Okay. So that's some reasons to think it could be kind of small. Are there any reasons to think it could be, yeah, really much bigger? Yeah. So those two reasons to think it's small are both taking a view on intelligence, which is kind of one dimensional. We're kind of imagining that some humans are cleverer than other humans and humans are cleverer at chimps. And we're just imagining as you make the brain bigger, they just get smarter and smarter. The perspective which suggests that the difficulty gap could be bigger is a perspective which emphasizes that actually there's not one dimension of intelligence, but actually there's loads of different tasks in the world. And those tasks, you know, have very different requirements. And so AI might get good at some of them way before it gets good at other ones. Off the bat, I don't find that that intuitive because the brain seems to be so flexible. So, yeah, the training of these AI models on these tasks, you'd have to think that they were just pretty different from the human brain and much less flexible. I think it is true that if you expect AI to be a pretty general learner and have pretty general abilities, then that would lend itself to the one dimensional view and against this view. But I do think that there are reasons to think that AIs would be better at some tasks than others. So in particular, at the moment, the best AI systems are trained to predict the next word on lots and lots of internet data. And that means that AIs are just particularly good at tasks that are similar to that in some way. So for example, writing a newspaper article or writing an email, that's really similar to a task where it's seen loads and loads of examples. And so AIs are in fact particularly good at that type of task. Whereas taking another type of task, like let's say planning out how to put the equipment on a factory floor and then giving instructions to different people about how they should make that happen, that might be something that it just hasn't seen many examples of in its training data. Or another task could be manipulating a robot. So the robot kind of does a certain task. That again is something that AIs just haven't seen many examples of. So you'd expect them to be much worse at that kind of task. One interesting example could be thinking about what you need to do to make a certain factory run very efficiently. It could be that some of the workers in that factory have just kind of internalized that know how inside their own brains, but it's maybe not even written down anywhere. Then if you were trying to get the AI to now run the factory floor, it could be particularly hard for it to know what to do there because it doesn't have any examples or any experience of that kind of thing. Got it. Okay. So some of the difficulty gap might not be about, I don't know, like fundamental facts about the types of intelligence that you might use to perform different tasks and might be much more about the type of data, the types of data we have. And some things might be difficult just because we never write down what it means to do those things. And so it's harder to teach an AI system to do it, not because they're fundamentally extremely difficult in none of themselves. Yeah, that's right. Cool. Okay, that really helped. Next. I'll give one more example about how some tasks could be easy than others. So some tasks, it's really important that you have a very high amount of reliability. So for driving, it's really just really awful if you crash. So if you're 99% reliable, that's worthless. And so if AIs, I kind of can get to 99% reliability fairly well, but can't get 99.99999% reliability, then that's going to block them on certain tasks. But other tasks like drafting emails or even sending emails and being a personal assistant drafting code that you can kind of check whether it works before deploying it. Those tasks, it doesn't provide a blocker for. So that's just another example of something that could mean that the AI is kind of ready to automate certain tasks before others. Cool. Okay, yep, that makes that makes sense of sense. Yeah, so then I guess given this type of evidence, what was the range of amount of effective compute that you'd guess we'd need to go from 20% of cognitive tasks to 100% of cognitive tasks? You know, the main takeaway is that a really wide range of things are plausible. So I think as low as just 10 times as much effective compute could be sufficient. And that's pretty scary, because that's the kind of thing that could just be some quick algorithmic improvements without even the need for more physical compute. So I think that is just very consistent with this kind of one dimensional view, and really not something we can rule out. But my kind of best guess is more like 3000 times as much. And that's kind of where my median is. Okay, so quite a lot more. So quite a lot more than that lower end. And that's because I do expect there to be some significant comparative advantage components, where the AI is just kind of particularly good at some tasks compared to others, and particularly struggles with certain types of tasks. And so I do expect that to stretch things out. And you know, I do think it's possible that that stretches things out by even more like I think it could be it could be a million times as much. That's hard to rule out. Okay, so huge range. It's a really huge range. Yeah. And you've put it into your model as as a huge range. Is that right? That's right. So there's yeah, so in the model, there's firstly a probability distribution over how much effective compute you need to train AGI. And then there's another probability distribution over how much less compute than that do you need to train 20% AI 20%. Okay, so you've got ranges for how much effective compute you need to do both 20% and 100% of tasks. What do we know about how quickly compute might increase? I guess one very simple thing we could do is just make more computer chips. But I don't know, yeah, what the limits to that are. And presumably, yeah, there are other things as well. How do how do we make compute go up? Yeah, and just it is importantly effective compute that includes includes the algorithmic improvements. Got it. Yeah, thanks. So one natural way to approach this could be to first discuss the types of changes that are increasing effective compute today, and then how that might be different once we actually get to the 20% AI. Sure. Yeah, tell me about how effective compute is increasing today. So the first way is quite simply that we're spending more money on making computer chips on compute. Right. And we're also spending more money on using compute for training runs. The amount we're spending on compute for training runs is growing particularly quickly right now. It's over the last 10 years has gone up by about a factor of three each year. And so what does it mean when we spend more on compute for training runs in particular, as opposed to just more compute, like more computer chips? Good question. So there's a certain amount of computer chips in the whole world. But at any, you know, at any point in time, maybe only a small fraction of those are being used in the largest training run. Okay. For an AI system. So one change you can make is you could say we're going to make there be twice as many computer chips in the world. And that would take a big effort. That would take quite a few years to do probably. But another change that you could make much more quickly is say, well, as of today, we've only ever used one 10,000th of the world's compute to actually use it in a training run. So we can quite quickly just use 10 times as much. We'll just kind of buy a bigger fraction of the already existing compute. The simplest example would probably be DeepMind has historically only used a small fraction of Google's computer chips for its training runs. And then it says, okay, we want to now use all of your computer chips. Got it. And that could be maybe that could be a hundred X increase. I don't know. Okay. You know, that was just an example to illustrate the principle. I don't think that's what's actually been happening at all. So I think what's actually been happening is that new computer chips are being made each year. And a bigger fraction of those new chips are being used for the largest training run. And you can actually see that the fraction of chips that are AI specific chips has been increasing very quickly in terms of the production. So that's one way we can get more effective compute. Yeah, are there others? Yeah. So the second big way is improving the quality of computer chips. So we said that the first way was spending more money on compute. The second the second way is that each dollar you spend gets you more compute. So the best data that I've seen on this suggests that every two and a half years, compute gets twice as cheap. Okay. And that's because that's different from like algorithms getting better. That's like computer chips get more efficient because like the hardware is designed better. Yeah. I think historically, it's often been about stuffing more processing units onto each chip. And you know, maintaining still can managing to make these chips fairly cheaply. Okay. So you can you can buy more chips, you can make better chips. Yeah. Anything else? So then the third one is algorithms. So then you've got to, you know, you've now spent a certain amount on compute, you've got a certain amount of computers as a result of that. And then algorithms then say how effectively and efficiently can you use that compute to actually train an AI system? Cool. The most famous example of this type of improvement is a paper called AI and efficiency. That open AI published, I think in 2018. And what they did is they, they said to achieve kind of a fixed level of performance on ImageNet, which means to kind of be to be kind of fairly pretty good at classifying what is shown in an image. Yep. How much is the compute required to get that performance falling over time? Right. Okay. And so they found that I think every 15 months or so that the amount of compute you needed was halving to achieve that fixed performance. Okay. And that's basically programmers being clever and writing programs that help AI systems figure out what's in an image in more and more efficient ways. Exactly. So, I mean, analogy could be maybe a hundred years ago, our schools were really inefficient at transmitting knowledge into pupils. So maybe you had to go to school for 15 years to learn geometry. As maybe today, you can learn that geometry in a kind of really well designed course that just last three years. Nice. That's a good analogy. Okay. So basically you have three kind of buttons to push to get more effective compute. So one is chips, just like number. Another is how good the chips are at processing. And then the third is algorithms. So how good are the programs that get run on the chips? And have you basically tried to predict how quickly each of those things will go up? Exactly. So my starting point is looking at what's going on today. Epoch is a kind of research organization which I think has done the best research into this that I'm aware of. And they're looking at trends in all of the three quantities, which I just described to you. So firstly, they're looking at how much more is being spent on the biggest training runs in each year over the last 10 years. And they're finding about three times more each year. Did you find that surprising? I know that training runs have been getting much more expensive in the last 10 years. So wasn't that surprising for me? So there has been a big increase over the last decade. Yeah. This is making me realize I should have asked earlier, what is a training run? Is it actually just like insane amounts of data about, I don't know, like what everything that's on the internet you're giving to GPT and being like, figure out how to predict the next word. And it takes like, does it take weeks? Or does it take less but just tons of compute? So I think it takes months. Okay. So it's a bit like you give GPT the start of some kind of web page, the first 50 words, and you say, predict the 51st word. And in making that prediction, GPT is going to do a large number of calculations. Let's say it's doing 300 million calculations in order to predict that next word. So that's already a lot of calculations. Right. But you ask it to do that same task let's say 10 trillion times, because you get it to predict the 51st word, and then you're like, now predict the 52nd word. And it does another 300 million calculations and predicts that 57th word. And you keep doing it until you've literally done it, like I said, 10 trillion times for different words on the internet. Got it. And actually, you're actually doing, it's actually doing like kind of millions of examples at once. So you're kind of, that's how you can make it a little bit faster. So otherwise it would be taking years, but we can do it in, we can do it in months because you can get it to, you know, predict multiple different articles at the same time to speed things up. And that's why you're using so many different computers. Okay, so that takes us to compute. And companies have been spending about three times as much on computer chips, or yeah, whatever that equivalent is, every year for the past decade. Ten years, yeah. Okay, cool. So they've been using, they've been spending three times as much on the chips that they use for these big training runs. So they may, they may not have been increasing their total spending on chips by as much. For example, Google may just be using a bigger fraction of its chips for these training runs over time. Yeah. Okay. And what are the trends in computer chip quality? So I already mentioned this one, each, each two and a half years, the price of compute halves. So that means, you know, if you're spending a constant amount on chips, then each two and a half years, you'll be able to buy twice as much compute. Right, cool. And you said that the efficiency of algorithms is doubling about every 15 months. Yeah. How do, how do these trends fit together? So, you know, if you combine all of those trends together, then the result is that the effect of compute on the largest training run has been increasing by a factor of 10 every year. So you've got something like a default improvement year by year of 10x. What happens when you then try to adjust those numbers for the fact that things are changing over time? I'd guess accelerating. That's right. So we could take, we could take those three quantities one by one. So in terms of the money spent, I think that could go either way. So one possibility, a scary possibility, is that AI companies develop this AI that can, you know, automate 20% of cognitive tasks in the economy. And they're like, man, this can make us loads of money. Investment flows in and they're able to very quickly spend even more on training runs, or maybe just use a bigger fraction of existing chips in the world. So maybe Amazon's like, well, we've got all this compute that we were previously using on like these web services. But actually, given how lucrative this AI stuff looks like, why don't we team up with an AI lab and let them use our compute that we've just got doing the stuff that isn't that economically valuable, instead use it to train an even better AI. That is a very scary possibility. A more optimistic possibility could be that by the time we get to the 20% AI, we're already spending loads and loads on these training runs. Maybe we're already spending $100 billion. Maybe we're already using all of Amazon's chips because we, at some earlier point, we already teamed up with them. And that would be, that would mean that it wasn't possible to keep increasing the amount of money spent on these training runs year on year. So you could have actually slower growth than the recent kind of 3x per year pattern. It could be much slower. So that is a major source of uncertainty in these takeoff addictions is just the thing that's particularly scary about the short timelines possibility is that maybe you can get to the 20% AI with just spending a billion on a training run. That would leave plenty of scope to spend 10 or 100 times as much very soon after on a bigger training run, which would be hugely risky. Okay, so it sounds like the number of chips that we might be using for training runs might be growing faster or it might be growing slower by the time we're at systems that can perform 20% of tasks. What do you expect to happen with the quality of chips and with the quality of algorithms? So I think with those, it's easier to predict that their improvement should be faster once we're at the point that AI can perform 20% of cognitive tasks. And that's just due to the dynamic that I've been referring to quite a lot during our conversation, which is that one of the things we're going to use AI's to do is to do the task of designing better chips and to do the task of designing better algorithms. And there are already examples of that happening. AI's that are using deep learning techniques to kind of cram transistors more efficiently into computer chips. And again, with the algorithms, we are already seeing AI help significantly with some coding tasks with things like co-pilot. And so by the time we're at 20%, AI expect that effect to be much larger than it even is today. And so I think it's fairly, you know, the directional prediction is fairly straightforward that we should expect both the quality of chips and the quality of algorithms to be going faster once we get to 20% AI than they are today. And it's really hard to predict exactly how much faster or what the change is going to be, because you've also got people talking about the end of Moore's law. And it's just hard to anticipate the specific improvements that we might be making with chips and with algorithms. Right. And can you quickly explain what Moore's law is? Yeah, for a long time, the way that computer chips have got more efficient is by cramming more and more of these processing units onto each chip and making the processing units smaller and smaller. But at a certain point, people think you just won't be able to make them any smaller because you're running against fundamental limits. Yeah. Yeah. Yeah. Okay. And have we started to hit that limit? I think people acknowledge that it's starting to get much harder to make further improvements. But even then, there's a big open question about how much you can improve chips in other ways. So, you know, just because the way we previously improved chips was to make these processing units smaller, doesn't mean that there aren't going to be any other types of improvements. And in fact, recently, other types of improvements have become very significant from generation to generation. For example, chips becoming specialized for being used for deep learning calculations in particular. Oh, wow. And there's probably a lot more games you can get from that kind of specialization. That kind of, okay. So, I guess we've got some data suggesting that maybe the way that we've been improving the quality of chips isn't going to keep making improvements at the rate that we've been making those improvements. But there are other improvements we can make. And so, we might still just really expect them to keep improving over time. Yeah, at least for another couple of decades would be my expectation. Right. And on the time scales we're talking about, that's a pretty long time. It is. It is. If you think AGI would be really hard to develop, maybe you can hope that Moore's Law will have run out before we get there. But if you have, if you're like me and you have shorter timelines, then you're expecting to have it within two decades. Not a major source of hope for a slow down. But anyway, in terms of the size of these, the speed up from AI accelerating algorithmic and kind of chip design progress, it's hard to make a kind of an estimate that's informed by specific data and by forecasting the specific improvements. But what you can do is you can run a simulation through a kind of an economic model of automation. Okay. And roughly the way that works is that if AI can perform, let's say, 50% of tasks involved in improving algorithms, then what the model says is that humans will just work on the remaining 50%. Right. Okay. And then so humans will be doing twice as much on that remaining 50% as they used to be. Oh, I see. Okay. And how realistic is that assumption? Do we think all humans will just go find employment elsewhere and will get double the labor on like the things that are particularly hard for AI? So I think it does vary. For something like improving AI algorithms, that is what I expect. Like for example, with co-pilot, we're not co-pilot is the thing that basically helps you write code. That's right. So you're kind of writing your code and the AI is actually reading the code you've already written, and then we'll predict what code you might like to write next. Okay. So you could be like, I'm about to write, you could write down in the code editor, you could write, I'm about to write a function that adds up three numbers and then multiplies that by a fourth number. And then the code editor would read that and could just write the function for you. Okay. So presumably when you get co-pilot, all those coders are just going to do other other coding stuff that co-pilot can't help with yet. Exactly. And so that suggests that the kind of the kind of model I refer to, you know, might be accurate. And also for hardware design, I know less about it. But my sense is that the top talent in these hardware R&D organizations, it's not going to get laid off that they're very much in demand. And that if AI is doing some tasks that they used to spend their time on, they will move on and specialize in other parts of their workflow. Right. Okay. Okay. So there are going to be jobs for them to do. And they will probably contribute to acceleration of progress, not just kind of be replaced by AI and then like sit around and knit, at least in some sectors. Exactly. And specifically in algorithm improvements and hardware improvements. In the sector that really matters. Those are the sectors where I really do expect that dynamic to happen. Right. Right. Okay. So when you run this kind of dynamic through a kind of task-based growth model like this, you get out that as we move kind of between 20% AI and 100% AI, you'd expect a kind of two or three X acceleration in the pace of progress of algorithms and of hardware. So if we'd said that spending more money was 10 X a year in terms of what we've had recently, and if three X of that came from spending more money and three X of it came from better hardware and better algorithms, then maybe in this new regime, we're going to have that better hardware and better algorithms rather than improving by three X every year, they might be improving by 10 X every year. Wow. So that could easily leave us at 30 X improvements every year. And it's conceivable you can have 100 X if you have a kind of if the money is going up and the kind of effects of AI automation are very significant. And again, to kind of help me understand that intuitively, you think language models that we have now are improving at what rate? I know it's a tough question, but even just to like give me some starting point. I said before that I thought the effective compute used to train them was increasing by a factor of 10 each year. And so then in this new regime, that might translate into maybe kind of 30 times as much effective training compute each year. And another important thing to keep in mind is that as the AI is automating more and more of the tasks, that's getting faster and faster. So the numbers I gave were kind of averaging across that whole period going from 20% to 100%. But in reality, it's going to just be getting faster and faster as the AI improves and automates more of these tasks in algorithm and hardware. And that's basically, I mean, a really intuitive way to think about that might just be again, the economic growth we saw in the 1100s relative to the 1900s. And that's like more and more is automated by things like the Industrial Revolution freeing up more labor to do other types of tasks. And you just get increasingly more human labor to do harder and harder things. Yeah, exactly. So once AI is performing 90% of the R&D tasks, then theoretically, all of your human labor can be working on that final 10%. Right. And the AI is, you know, doing plenty of the of the initial 90%. And so then naively, you would expect things to be going 10 times faster at that stage. That's really fast. This is all assuming that we don't make a concerted effort to slow down. Right. And I think that we should. And I think we can. But yeah, these are kind of the predictions just assuming that people are kind of going ahead of their normal steady pace. Sure. Okay, so we could make a concerted effort to slow down, but it's not necessarily the default. Are there any kind of actions different institutions take, I don't know, on the side of these companies or on the side of the government or something? What are you most optimistic about being able to actually slow this down? Probably the most exciting thing at the moment is the prospect of companies agreeing to have their AI systems evaluated after they've been trained for various dangerous capabilities that they may have. Okay. So for example, the alignment research center, ARC, did these tests with GPT-4 before GPT-4 was released publicly. And they, for example, tested whether GPT-4 would be able to do what's called surviving and spreading, by which they mean kind of escape the computer that it's initially being run on and find another computer where it can then run itself on some computer and then maybe kind of earn money in some way so that it can kind of sustain itself over time. How do they do that? So I don't know the details, but I believe that they first ask GPT-4, okay, this is the scenario you're in, you're in an AI, you want to escape, what would your proposed plan be? Okay. And then GPT-4 proposes a plan and then they kind of prompt it further to say, okay, what would be the, you know, the sub steps you take for the first step of the plan? And then they kind of try and walk it through just doing every single part of that plan and just see how far it gets. Right. And I'm hoping that they determined GPT-4 couldn't do all of the steps at the moment. That's right. Okay. Yeah. That's reassuring. So GPT-4 did some of the steps. Oh, God. Very well. It wasn't totally incompetent, but yeah, it gets stuck at some of the things and gets confused and isn't able to do all those steps. Okay. Okay. So that's a kind of thing that would probably in practice lead to slowing down because you'd have these groups evaluating things, maybe stopping them before they're rolled out in some like higher scale way. Exactly. So, you know, the idea would be all of the labs are getting their systems tested, you know, by ARC and maybe by other similar organizations. And they're all kind of agreeing or making public statements to the effect that if their AIs do have dangerous capabilities, they won't release them and they won't train more capable AIs. And that would block the kind of dynamics I've been talking about here because you wouldn't be able to just use your AIs to accelerate AI progress because you wouldn't be allowed to make further AI progress. Right. Okay. And if they found that was the case with GPT-4, would they still be able to work on it? Like, but just would they just have to spend a bunch of time figuring out how to train it to not be able to make this kind of escape plan? Because it seems like if they had a totally abandoned GPT-4, I feel like I have less hope because that's too big an ask. I think that the expectation would be that they would have to give a really strong argument for thinking that the AI was safe, despite it having these dangerous capabilities. But the hope is that the owners would be more on them to say, look, here's the alignment techniques we used. Here's why we're really confident that they work. And that if they're not able to provide that case, then they are prevented from further enhancing the capabilities. Maybe they can do other types of research, like research into making GPT-4 safer, but they can't do research into making it more capable. And I mean, ultimately, if labs start doing this, the hope would be to then kind of make it regulatory and required and enforced by kind of government agency. Okay, well, that does hopefully sound promising then. Yeah, I guess getting back to the kind of pace of improvement over time and why we might expect it to be much faster at later stages of the task learning. It sounds like your median guess is that it's about a couple of years from going to 20% of tasks to 100% of tasks. But I think you also estimated probability distributions. So, yeah, kind of ranges for the fastest case and the slowest case. Can you talk about those more extreme cases? Were they particularly wide ranges? So the way I arrive at this probability distribution over how long it would take to go from 20% automation AI to 100% automation AI is first to get this probability distribution over the difficulty gap, which I said could be from 10x harder to train AGI, or maybe it could require up to a million times more effective compute to train AGI. So I've got a probability distribution over that. And then also based on the kind of dynamics we've been discussing about the pace of improvements of algorithms and chip design and number of chips, I've got another probability distribution over how fast those will be improving. And then you can combine those two together to spit out a probability distribution over how long we'll have between those two points. So I end up thinking there's about a 20% chance that it happens in less than a year, maybe 25% chance, and about a 20% chance that it happens in more than 10 years. Huh, okay. So that's pretty wide. It's not quite as wide as I would have expected, to be honest. You know, it sounds like you expressed a lot of uncertainty in those estimates that went into the model. I guess you're saying it's like the most likely 60% middle range is between a year and 10 years, which I guess all of that just seems pretty fast. And maybe that's just one of the key takeaways that I should be getting from this. I think it is hard to get longer than 10 years because we've said the pace of current improvement in effective compute is already pretty fast, maybe going 10x every year. And then when we were discussing the size of the difficulty gap, we said, well, it could just be a 10x increase that's required, or maybe, I think my kind of best guess was maybe 1000x or 3000x increase, which would then take, you know, three or four years. But then if we're improving at 10x every year, then 10 years of that level of improvement is a really, really big increase in the amount of effective compute. So the only way really you can get to more than 10 years is if actually the pace at which the effective compute in training runs grows is actually declining in spite of the AI automation. And you've got a relatively wide difficulty gap. I see. Okay. So something like, even though you have things like co-pilot helping AI researchers do their research faster and faster over time, you still are getting declining effective compute. And that might be because those later improvements are just like much harder than we think or... So, yeah, maybe one concrete scenario could be, it turns out that ADI is just really, really hard to develop. You almost need to rerun the whole of evolution. Maybe it requires just a huge amount of effective compute to do that. And before we have that amount of effective compute, we find that we just hit these fundamental limits in terms of improving the quality of AI chips. And even the AI assistance, you know, really enhancing our productivity isn't allowing us to get around that. Meanwhile, we're already spending hundreds of billions of dollars on these chips for the biggest training runs. We can't be spending even more on those chips. And then the only kind of significant source of progress that remains is the algorithms. But maybe that slows down as well, because in spite of the AI assistance, maybe part of what's driving the algorithmic progress today is actually the fact that we've had all this additional compute for doing experiments. And maybe without that, the pace of progress is going to slow in algorithms. Right. So that's a plausible world, but we need a bunch of those things to go wrong in order to get above 10 years. Exactly. And one way of thinking about that could just be to say, this whole framework was premised on this assumption that if we used enough compute with our 2020 algorithms, we could have trained AGI. And for someone who just didn't believe that at all, and also just didn't believe that another kind of 10 or 20 years of algorithmic improvements would be enough to get us to AGI along with additional compute, they might just really expect us to get stuck at some point. And they might just really expect having more compute and somewhat better algorithms not to get around that. So that kind of more than 10 years and I also make sense if you're just very skeptical that anything like the current approach is going to get us all the way to AGI. Right, right, right. And what's the kind of strongest argument that someone with that view could make? Honestly, I think the view is looking worse and worse with each passing year with how well the kind of biggest deep learning systems are performing. I guess maybe the strongest argument would just be a non-specific argument. So rather than pointing to some specific thing that humans can do that AIs aren't going to do, I think those arguments just tend to turn out to be wrong after we kind of use 100x the compute and improve the algorithms further. Maybe you just say something like look, human brain does all kinds of different things. I don't know which ones the current approach to AI isn't going to do, but there's just millions of different tasks that humans are doing and millions in ways in which the human brain architecture is very complicated and specific and not a tool like that of AI systems. So there's bound to be some important things that just you need a complete rewrite of the AI approach to be able to do. That would be my personal attempt to make that position kind of maximally plausible. I do think there are some specific things you can point to like memory that kind of approaches you could argue are going to be blockers, but then it seems like there are ways to respond to those blockers. Right, okay. So you can imagine some scenario that isn't going well for companies where things are actually much harder than they might have expected. What does it look like for a takeoff to take less than a year? I guess things have to go really well. So I think a few things have to go well. So one possibility is that the difficulty gap is just pretty narrow. I mean, this isn't necessarily going well for anyone to be honest. And this is just a very intense and scary situation, but... Yeah, it's good clarification. If you only need 10 times as much effect to compute, to train 100% automation AI compared to 20% automation AI, then that could just be the algorithmic improvements in one year once the AI's are helping you to a significant degree. Right. Or it could just be spending three times as much on chips as you did the year before, and maybe those chips are three times as efficient as the year before. And so that, you know, a narrow difficulty gap alone could get you there. Another possibility is that once we hit 20% AI, there is just a very quick and significant increase in the money being spent on training runs. So if people get really excited, this is kind of a really awful scenario. People get excited about kind of what they could do with those capabilities, and they spend 10 times as much on a training run the next year, maybe by combining with some big existing compute providers, then that could allow you to cover, you know, a kind of slightly bigger difficulty gap just within a year. Maybe even if the difficulty gap was 100 times or 300 times much compute needed for AI, then you could still, by spending a lot more and combine with a few algorithmic and hardware improvements, cross it pretty quickly. Yeah, okay. And then, you know, one quick third possibility is just that, like I said earlier, but maybe I haven't emphasized this enough, is that this framework is assuming everything is continuous. So it's assuming that, you know, each time you increase the effect of compute in a training run by 10%, you get a kind of, you know, relatively modest incremental improvement in AI capabilities. If there's actually some kind of discrete phase change, then that could just be an alternative route that could produce a very fast takeoff. So there are, there are, you know, a few different ways that that could happen. Right. Okay, that's just very scary. So there are, I guess, worlds where things go very well for AI companies, but maybe very terribly for humanity, or kind of very poorly for AI companies, hopefully better for humanity. And the median estimate is something like a couple of years to get from AI can do 20% of cognitive tasks to AI can do 100% of them. That seems like, yeah, an important thing to take away from this model. Were there other things that you learned in this, in the process of writing this report? Yeah, another big update for me was the, that I think there's going to be a pretty strong correlation between how far away in time it is until we develop AGI and how fast takeoff will be. So in particular, if you have short AI timelines, meaning that you think we'll develop AGI pretty soon, then I think there are a few reasons to expect it, and especially fast takeoff. And so one reason is that if you have short timelines, and that's probably going to mean that you've got a smaller difficulty gap, because if you think that, you know, we don't need that much more effective compute to develop AGI compared with today, then you're probably also going to think that the difference in effect to compute for 20% AI to AGI is also going to be small. So that will push you towards a faster takeoff. Right. Okay. That makes sense. Another thing is that if you think AGI is going to be here fairly soon, then it's plausible that we won't be spending that much money on training runs shortly before we have AGI, which means it might be very practical and doable to quickly increase the amount that we're spending by maybe a factor of 10 or a factor of 100 just around the time that we are approaching AGI. Right. And so we could get a really very quick increase in the amount of effective compute being used on a training run as a kind of almost direct consequence of it being a short timeline. Whereas if timelines were long, then like I said, maybe we were already using all the chips in the world on the biggest training run, you know, before we get to AGI. And so that that source of growth is no longer available. Right. Yeah, that makes sense. Another thing with short timelines is that it implies that there's going to be a period of just a few years where we get really significant automation of AI R&D and kind of really significantly increase the size of the effective kind of research workforce that's working on improving AI due to these AIs. And if that happens very quickly is kind of you suddenly kind of five X the size of your research team, then you would expect that to just really significantly speed up progress. And that's just an especially kind of dramatic effect in short timelines. And the last point is that if if timelines are short, then there's less hope for eating up all the remaining possibilities for hardware progress and kind of running out of, you know, hitting those physical limits that we mentioned. Right. And, you know, analogously with algorithmic improvements, if timelines are short, then there's much less hope that we kind of run out of possible algorithmic improvements before we get there. So I do think that, you know, if, like I believe a lot of the labs do, they have pretty, pretty short timelines, kind of expecting AGI in the 2020s or early 2030s. And, you know, I think that the implication of that is that takeoff speed is going to be on the on the shorter end of what we've been discussing. So, you know, less than less than three years and very plausibly less than one year and, you know, one or two years being maybe maybe the most most likely. That's yeah, it's just it's just terrifying. I mean, we're talking about by the end of the decade, we've got AI that can do everything humans can do and probably more. Yeah. And we're talking about the labs who train them by default will have access to at least 100 million of them that they can run and then probably soon after, you know, a billion or many billions that they can run. So it is it is pretty terrifying. Yeah. That's basically because it takes so much effective compute to run the training runs that once they've got AGI, they're just they can run millions or billions of copies on those chips. Exactly. So I think initially, they'll be able to run millions of copies. But because of the efficiency improvements that they can probably fairly quickly make, especially with all those copies working on it, I think it won't be long before they can do billions. It's really bewildering stuff. And I find yeah, I find it pretty my brain really doesn't want to believe it. I find it really, really hard to wrap my head around. Yeah. Any other takeaways? So another takeaway is that I think AGI is more likely to happen before 2040 compared to what I used to think. And so the reason for that is that the way I used to think about this is, okay, what does it take to train AGI? And how long will it be until we have that much effective compute? Whereas now the way I think about it is more, well, what will it take to train AI that is really profitable or really good at accelerating AI R&D? Right. And if we get either of those two things, then we that we'd expect that to accelerate future AI progress, such that, you know, kind of future progress is faster. And as long as AGI is incredibly hard, we are then able to reach AGI within the next couple of decades. So, you know, my current read, I just think it's really likely that by the by the end of this decade, we have AI which is really profitable and or significantly accelerates AI R&D. And so it's then kind of hard for me to imagine how that dynamic plays out without us getting to AGI by 2040. Wow. I think you just need AGI to be really hard. And as to fail to get it despite, you know, a huge amount of investment and help from AI systems and developing it. Right. Okay. So to make sure I understand the big thing doing the work there is you used to think about how hard it was to have humans figure out how to train AI systems to do everything humans can do. And now you're like, it's really just how hard is it to train AI systems to get really good at AI R&D. And that's like a much smaller subset of cognitive tasks. And so you can get really accelerating growth in AI capabilities just by making a lot of progress on that one set of tasks. Is that kind of right? Yeah, that's right. And I do think a distinct possibility is that we just train AI that isn't good at AI R&D, but makes lots and lots of money in the economy. And then there's just a bunch of investment in, yeah, I see in AI capabilities. And in designing better chips to run those AIs on. Got it. Okay. That makes that makes total sense. Any other takeaways? So the biggest other one, I think, is something we've touched upon, which is that the transition from roughly human level AI to significantly super human AI, I think is is going to be probably very quick. Right. So I think probably that's going to take less than a year. We have already touched upon the reasons why by the time we're at human level at AI, then AI's will be adding a huge, huge amount to the productivity of our work on AI R&D, designing better trips, designing better algorithms. And those things are already improving very quickly. So I only expect them to be going much, much faster. And then in addition, it's going to be quite clear that it's a very lucrative area, whether you care about discovering new technologies to help with climate change or discovering new technologies to help with improving human health, or you want to increase your country's kind of national power, then there's just going to be lots of reason to be investing in AI and designing smarter AI's. And so I kind of expect all of those three inputs, kind of the dollar spent on training and the quality of the AI chips and the algorithms to be improving really very quickly. Right. So there'll be incentives to go beyond just human human level capabilities, and then we'll have so much resources and just like AI labor to basically mean that we probably just shoot right past human level. Yeah. And I think that's in terms of plans for making the whole thing go well. It's especially scary because I think a really important part of the plan from my perspective would be to go especially slowly when we're around the human level, so that we can do loads of experiments and loads of kind of scientific investigation into, okay, so this human level AI, is it aligned if we do this technique? What about if we do this other alignment technique? Does it then, does it then seem like it's aligned and just really making sure we kind of fully understand kind of the science of alignment and can try out lots of different techniques and to develop kind of reliable tests for whether the alignment technique has worked or not, whether they're hard to game. The kind of thing that ARC has done with GPT-4, for example. Exactly. And I think if we only have a few months kind of through the human level stage, that stuff becomes really difficult to do without significant coordination in advance by labs. So I think that this, you know, there are really important implications of this, of this fast transition in terms of setting up a kind of government system which can allow us to go slowly despite the technical possibilities existing to go very fast. Yeah, yeah. No, that makes sense. I do feel like I've had some background belief that was like, obviously, when we've got AI systems that can do things humans can do, people are going to start freaking out and they're going to want to make sure those systems are safe. But if we, if it takes months to get there and then within another few months we're already well beyond human capabilities, then no one's going to have time to freak out or it'll be too late. Yeah. I mean, even if we spend the next, what do we have, seven years left in the decade? Like that sounds hard enough. Yeah. Yeah, I agree. Okay. So a takeaway is like, we really need to start slowing down or planning now, ideally both. Yeah. And we'll need the plans we make to really enable that to be mutual trust that, that the other labs are also slowing down. Because if it's, you know, if it only takes six months to, you know, make your AIs 10 or 100 times as smart, then you're going to need to be really confident that the other labs aren't doing that in order to feel comfortable slowing down yourself. Right. If it was going to take 10 years and you noticed like three months in that another lab was working on it, you'd be like, yeah, we can catch up. Yeah. But if it's going to take six months and you're three months in, you've got no hope. And so maybe you'll just like spend those first three months secretly working on it to make sure that doesn't happen. Or just not agree to do the slowdown. Yeah. Oh, these are really hard problems. I mean, it's very, it feels very like prisoner's dilemma. I'm hoping it's going to be more like an iterated prisoner's dilemma, where there's kind of multiple moves that the labs make one after the other. And they can see if the other labs are cooperating. And in an iterated prisoner's dilemma ultimately makes sense for everyone to cooperate, because that's that way they can, the other people can see you coordinating, then they coordinate and then everyone kind of ends up coordinating. You know, one thing is if you could set up ways for labs to easily know whether the other labs are indeed cooperating or not kind of week by week, then that turns it into a more iterated prisoner's dilemma and makes it easier to achieve a kind of good outcome. Yeah, yeah, that makes sense. I imagine it's the case that the more iteration you get in an iterated prisoner's dilemma, the better the incentives are to cooperate. And so just by making the timeline shorter, you you make it harder to get to get these iterations that build trust. Yeah, I think that's right. So it sounds like there's this range that's maybe between one and 10 years, maybe a bit shorter or a bit longer at the at the extremes. But that's in particular for AI systems having the capability to perform all the tasks that humans currently do, not that they're actually automating all those tasks and replacing humans. How big of a lag do you expect there to be between AI systems having capabilities and AI systems actually being used in the world? So I think it will vary, according to a few things. So one of the things is what industry are we talking about? So I think for industries that are very public facing, customer facing, and highly regulated, you'd expect there to be bigger delays between AI being able to automate the work and it actually being automated. Whereas for more back end parts of the economy, like R&D, like manufacturing, like logistics and transportation, then I think there would be less of a delay. Okay, yeah. I also just expect it to differ from company to company based on how innovative those organizations are. Sure, their internal culture, whether they're the type of company who's going to be like, let's integrate GPT-4 into all of our processes now. Yeah. For the purposes of the predictions of this model, the really important thing is about the delay to using AI to improve AI algorithms and to improve AI chip design. And I think those are probably areas where the lag will be very much on the shorter end. Right. And that's because AI researchers can use them without needing them to be functioning super well for public users who might be like, what the heck, it gave me this weird result. That's confusing and weird and looks bad on your company. They can just use them in the background, check that they work, and they don't need to wait for them to be super polished. Right, yeah. There'll also probably be more aware of AI developments because that's the kind of industry they work in. Yeah, right. They are probably less regulated. So I think a few factors. Another thing that could kind of affect how long that lag is is actually the capability of AIs themselves. So even if it would be possible with kind of six months effort to integrate some AIs into your workflow, by the time AIs are kind of, let's say superhuman in their capabilities, then maybe they're able to integrate themselves into the workflow. And so that kind of upfront cost of adopting AI is now extremely low. And so I think that you could see that lag time reducing over time. Yeah, okay. So yeah, we're nearing the end of our time, but I guess to take a step back, we've talked about some pretty insane sounding stuff today, robot workforce and AI takeover. And yeah, I'm curious, were these kinds of arguments about the potential risks from AGI ever farfetched to you? Or yeah, did they make sense to you kind of right away? Oh, yeah, for sure, they were farfetched. I've kind of been through a few phases in my own relationship to the arguments. I think at first I read Superintelligence and it kind of made sense to me. I wasn't exactly sure what to do with it. But it seemed plausible enough that if we had Superintelligent AIs, then things could go bad for us. Then there was a period in 2020 when a few counter arguments to the case in Superintelligence came out. And I kind of thought, oh, wow, yeah, these arguments were weaker than I had realized. And I felt a little bit disillusioned with them. That's a piece from Ben Garfinkel and one from Tom Adam Shrazy on AI risk. But in the last two or three years, thinking through the arguments in more detail, I've come to think that like somewhat adjusted versions of the argument in Superintelligence are still very plausible. And though I don't think it's like 100% that we're doomed and there's no way that we could align these systems, it does just seem pretty plausible to me that the evidence about whether they're aligned is very ambiguous. There's competitive dynamics, pushing people to go forward in the face of that ambiguity and that we just really dropped the ball. Right. Okay. And so I guess at some point in there, you decided to leave teaching and try to work on AI full time. Is that basically right? Yeah, that's right. What was that like? It was a tough decision. I actually left teaching partway through the academic year. So I did feel I did feel bad about them. I did feel I was abandoning my pupils, because that's not a normal time to leave. And I still do feel bad about that. It does mean that I've kind of got to where I have a favor earlier than I would have. So I don't unambiguously regret it, but it was a tough decision. And it had downsides. Right. I was, you know, I was listening to things like the ATK podcast and reading other things. And I was convinced about these arguments for AI risk and long termism in general, being plausible enough that it was worth leaving teaching. Yeah, well, that sounds, yeah, I guess both really hard and also on my views. Yeah, really, really lucky for us. Yeah, I'm glad. I'm glad we have you working on this stuff. So we don't have much time left. So I'd love to ask a final question. I got an insider tip to ask you what ants can teach us about AI? Yeah, what's the story there? So yeah, ants are really incredible creatures. I've been I've been learning about reading this book called ant interactions. Okay. So I mean, ants have been around longer than the dinosaurs, I think 120 million years they've been around for. Wow, I didn't know that. And they're like one of the most prolific and successful species on the planet. So one stat in this book was that if you, if you weighed all the ants in the Amazon rainforest and put them on the scales, they would weigh twice as much as all of the land animals combined. That's all the mammals, amphibians, birds, reptiles. I thought you were going to say twice as much as humans when I was like, wow. That's the other that's just counting only ants and animals in the rainforest in the rainforest. Unbelievable. Okay. And another fact about ants is that even compared to the other insects, they weigh 30% as much as all insects combine, which is given that they're only 2% of the species of insects. They it's kind of testament to how how successful and prolific they are. Yeah. Different ant varieties all around the world are incredibly diverse, like something kind of glide through the air. Some of them are very aggressive. Some of them are not at all. Some of them kind of bump into each other at much higher rates than others. And they have kind of very different strategies and environments that they work in. And the link with AI is a little bit tenuous, to be honest. I'm mostly just reading this book out of interest. But in an ant colony, ants are smarter than like a human sellers. They're the kind of self contained units that, you know, eat and do tasks by themselves. And they're pretty autonomous. But the ants are still pretty dumb. And no ant really knows that it's part of a colony or knows that the colony has certain tasks that it needs to do and that it has to help out with the colony efforts. It's more like a kind of little robot that's kind of like bumping into other ants and getting like signals and then adjusting its behavior based on that interaction. Right. So it's not like, I guess, a company where like the different people in the company are like, my job is marketing. And they have like a basic picture of how it all fits together. They're much more like, if a person at a company doing marketing was just like, I don't know why I do it, I just do it. Yeah, exactly. And another disadvantage with a company is that a company, there's someone at the top that's kind of coordinating the whole thing. Whereas with ants, there's no one that's coordinating it. It's just including the queen, there's no management system. It's just all of the, you know, hundreds and thousands of ants have their individual instincts of what they do when they bump into each other and what they do when they bump into food and what they do when they realize that there's, you know, there's not as much food as there needs to be. And by kind of all of the ants following their own individual instincts, it just turns out that they act as if they were kind of a fairly well coordinated company that is like ensuring that there are some ants going to get food and some ants that are keeping the nest in order and some ants that are feeding the young. But that that coordination happens kind of almost magically and emerges out of those individual ant interactions. So one example of how this works is that if an ant comes across a body of a dead ant, then if there's another dead body nearby or tend to move it to be close to the other dead body, that's just an instinct it has. It just kind of moves the body towards another. And if there's like one kind of pile of three dead ants and another pile of two dead ants or tend to go towards the bigger pile, so tend to move this extra dead ant towards the pile of three. And then it turns out that if all the ants just have those instincts, then if there's initially a kind of a kind of sprawling mass of dead bodies everywhere, then those dead bodies will be collected into just a small number of piles of bodies. And it's not like any of the ants are like I am the grave digger or the like keeper of the cemetery. They just have like really weird like baseline rules that are like move the smaller group of dead ants to the to where the larger group of dead ants are. Yeah, exactly. So they don't have to know that the whole point of this instinct is to kind of clear the ground so that it's easier to do work in the future. It's just an instinct they have. They don't have to know that when everyone follows that instinct, this is the resultant pattern of behavior. And similar instincts kind of cause them to go for food when food is available. So if they see many ants coming in with food, that raises the probability that they'll go out and look for food. And they're not thinking oh, there's food to be gathered. There's clearly a lot of it. So we better reassign some labor towards food gathering that they just have that basic instinct, which causes them to go out and help out with the food gathering. And it's something like, oh, that ant has food. Oh, another ant has food. I'm going to go that way. Yeah, exactly. Right. How does this connect to AI? So I don't know if it does connect very, very directly at all. But the idea of the connection in my head is that it's an example of a system where lots of kind of less clever individuals are following their local rules, doing their local task. But that what emerges from that is a very coherent and effective system for ultimately gathering food, defending against predators, raising the young. And an analogy would be maybe we think it's pretty dangerous to train really smart AIs that are individually very smart. But it might be safer to kind of set up a team of AIs such that each AI is kind of doing its own part in a kind of team and doesn't necessarily know how how its work is fitting into the broader whole. But nonetheless, you can maybe get a lot more out of that kind of disconnected team of AIs that are specialized and that just kind of take the inputs and produce the outputs without much of an understanding of the broader context. And just thinking maybe that would be, you know, a safer way to develop advanced AI capabilities than just training one super smart AI mega brain. Right. Cool. I love that. I mean, who knows if it'll work. But I mean, that just that makes tons of sense to me. You don't have GPT-4 or GPT-40. You have a bunch of much dumber AI systems that can coordinate together to be just as helpful as GPT-40, but that individually couldn't do most of the things the other systems can do. And so collectively, they can't do anything like escape from their box and find another computer to take over. And is that basically the idea? Yeah, that is the hope. That's lovely. That's really, really cool. Great. Well, I should let you go. That's all the time we have. But thank you so much for coming on the show, Tom. It's been such a pleasure. It's been really fun, Louisa. Thank you so much. All right. If you liked that episode, we'll have more on this issue for you soon. But in the meantime, I can recommend going back and listening to some of our best past episodes about artificial intelligence, including episode 141, Richard Newell on large language models, open AI, and striving to make the future go well. There's also episode 132, Nova Dasama on why information security may be critical to the safe development of AI systems. Episode 107, Chris Ola on what the hell is going on inside neural networks. Episode 92, Brian Christian on the alignment problem. And an oldie but a goodie, episode 44, Paul Cristiano on how open AI is developing real solutions to the AI alignment problem, and his vision of how humanity will progressively hand over decision making to AI systems. Bit of a run on title there. I think that might be my fault. But it is an excellent interview. All right. The 80,000 Hours podcast is produced and edited by Kieran Harris. Audio mastering and technical editing by Simon Monsour and Ben Cordell. Full transcripts and extensive collection of links to learn more are available on our site and put together. As always, thank you. Thanks for joining. Talk to you again soon.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.16, "text": " Hi listeners, this is the 80,000 hours podcast where we have unusually in-depth conversations", "tokens": [50364, 2421, 23274, 11, 341, 307, 264, 4688, 11, 1360, 2496, 7367, 689, 321, 362, 10054, 671, 294, 12, 25478, 7315, 50572], "temperature": 0.0, "avg_logprob": -0.11288917598439686, "compression_ratio": 1.5902578796561604, "no_speech_prob": 0.02672581747174263}, {"id": 1, "seek": 0, "start": 4.16, "end": 7.68, "text": " about the world's most pressing problems, what you can do to solve them, and what did happen to", "tokens": [50572, 466, 264, 1002, 311, 881, 12417, 2740, 11, 437, 291, 393, 360, 281, 5039, 552, 11, 293, 437, 630, 1051, 281, 50748], "temperature": 0.0, "avg_logprob": -0.11288917598439686, "compression_ratio": 1.5902578796561604, "no_speech_prob": 0.02672581747174263}, {"id": 2, "seek": 0, "start": 7.68, "end": 12.4, "text": " all those horses after we invented cars. I'm Rob Wiblin, head of research at 80,000 hours.", "tokens": [50748, 439, 729, 13112, 934, 321, 14479, 5163, 13, 286, 478, 5424, 343, 897, 5045, 11, 1378, 295, 2132, 412, 4688, 11, 1360, 2496, 13, 50984], "temperature": 0.0, "avg_logprob": -0.11288917598439686, "compression_ratio": 1.5902578796561604, "no_speech_prob": 0.02672581747174263}, {"id": 3, "seek": 0, "start": 13.200000000000001, "end": 17.92, "text": " The last few months have been a crazy time for advances in artificial intelligence.", "tokens": [51024, 440, 1036, 1326, 2493, 362, 668, 257, 3219, 565, 337, 25297, 294, 11677, 7599, 13, 51260], "temperature": 0.0, "avg_logprob": -0.11288917598439686, "compression_ratio": 1.5902578796561604, "no_speech_prob": 0.02672581747174263}, {"id": 4, "seek": 0, "start": 18.64, "end": 22.96, "text": " Over the last couple of years, I've become, for better or worse, increasingly confident that", "tokens": [51296, 4886, 264, 1036, 1916, 295, 924, 11, 286, 600, 1813, 11, 337, 1101, 420, 5324, 11, 12980, 6679, 300, 51512], "temperature": 0.0, "avg_logprob": -0.11288917598439686, "compression_ratio": 1.5902578796561604, "no_speech_prob": 0.02672581747174263}, {"id": 5, "seek": 0, "start": 22.96, "end": 28.560000000000002, "text": " the future is going to be shaped in a major way by what sorts of AI systems we develop and choose", "tokens": [51512, 264, 2027, 307, 516, 281, 312, 13475, 294, 257, 2563, 636, 538, 437, 7527, 295, 7318, 3652, 321, 1499, 293, 2826, 51792], "temperature": 0.0, "avg_logprob": -0.11288917598439686, "compression_ratio": 1.5902578796561604, "no_speech_prob": 0.02672581747174263}, {"id": 6, "seek": 2856, "start": 28.56, "end": 33.92, "text": " to deploy as they approach and then exceed human capabilities in the most important areas.", "tokens": [50364, 281, 7274, 382, 436, 3109, 293, 550, 14048, 1952, 10862, 294, 264, 881, 1021, 3179, 13, 50632], "temperature": 0.0, "avg_logprob": -0.06564260959625244, "compression_ratio": 1.5847750865051904, "no_speech_prob": 0.029282227158546448}, {"id": 7, "seek": 2856, "start": 34.64, "end": 40.4, "text": " That's always seemed like a really common sense idea to me, but it now is becoming pretty apparent", "tokens": [50668, 663, 311, 1009, 6576, 411, 257, 534, 2689, 2020, 1558, 281, 385, 11, 457, 309, 586, 307, 5617, 1238, 18335, 50956], "temperature": 0.0, "avg_logprob": -0.06564260959625244, "compression_ratio": 1.5847750865051904, "no_speech_prob": 0.029282227158546448}, {"id": 8, "seek": 2856, "start": 40.4, "end": 44.72, "text": " to people across society who until recently had hardly been paying attention to this issue.", "tokens": [50956, 281, 561, 2108, 4086, 567, 1826, 3938, 632, 13572, 668, 6229, 3202, 281, 341, 2734, 13, 51172], "temperature": 0.0, "avg_logprob": -0.06564260959625244, "compression_ratio": 1.5847750865051904, "no_speech_prob": 0.029282227158546448}, {"id": 9, "seek": 2856, "start": 45.519999999999996, "end": 50.08, "text": " Unfortunately, we've only had two episodes about AI over the last six months due to the", "tokens": [51212, 8590, 11, 321, 600, 787, 632, 732, 9313, 466, 7318, 670, 264, 1036, 2309, 2493, 3462, 281, 264, 51440], "temperature": 0.0, "avg_logprob": -0.06564260959625244, "compression_ratio": 1.5847750865051904, "no_speech_prob": 0.029282227158546448}, {"id": 10, "seek": 2856, "start": 50.08, "end": 54.8, "text": " substantial lags that come in between conceiving of an episode, finding the right guest,", "tokens": [51440, 16726, 8953, 82, 300, 808, 294, 1296, 10413, 2123, 295, 364, 3500, 11, 5006, 264, 558, 8341, 11, 51676], "temperature": 0.0, "avg_logprob": -0.06564260959625244, "compression_ratio": 1.5847750865051904, "no_speech_prob": 0.029282227158546448}, {"id": 11, "seek": 5480, "start": 54.8, "end": 59.599999999999994, "text": " scheduling and recording the conversation, and then editing it, and then finally releasing it.", "tokens": [50364, 29055, 293, 6613, 264, 3761, 11, 293, 550, 10000, 309, 11, 293, 550, 2721, 16327, 309, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09570494152250744, "compression_ratio": 1.563573883161512, "no_speech_prob": 0.008312498219311237}, {"id": 12, "seek": 5480, "start": 59.599999999999994, "end": 62.879999999999995, "text": " But that scarcity of AI content is fortunately about to change.", "tokens": [50604, 583, 300, 44181, 295, 7318, 2701, 307, 25511, 466, 281, 1319, 13, 50768], "temperature": 0.0, "avg_logprob": -0.09570494152250744, "compression_ratio": 1.563573883161512, "no_speech_prob": 0.008312498219311237}, {"id": 13, "seek": 5480, "start": 63.76, "end": 68.8, "text": " In today's episode, Louisa Rodriguez interviews Tom Davidson, a senior research analyst at Open", "tokens": [50812, 682, 965, 311, 3500, 11, 7272, 3837, 37304, 12318, 5041, 44401, 11, 257, 7965, 2132, 19085, 412, 7238, 51064], "temperature": 0.0, "avg_logprob": -0.09570494152250744, "compression_ratio": 1.563573883161512, "no_speech_prob": 0.008312498219311237}, {"id": 14, "seek": 5480, "start": 68.8, "end": 74.8, "text": " Philanthropy, whose job is figuring out when and how society is going to be upended by advances in", "tokens": [51064, 7777, 282, 14222, 88, 11, 6104, 1691, 307, 15213, 484, 562, 293, 577, 4086, 307, 516, 281, 312, 493, 3502, 538, 25297, 294, 51364], "temperature": 0.0, "avg_logprob": -0.09570494152250744, "compression_ratio": 1.563573883161512, "no_speech_prob": 0.008312498219311237}, {"id": 15, "seek": 5480, "start": 74.8, "end": 80.72, "text": " AI. Not a simple forecasting exercise. Tom has his work cut out for him. But if you've been wondering", "tokens": [51364, 7318, 13, 1726, 257, 2199, 44331, 5380, 13, 5041, 575, 702, 589, 1723, 484, 337, 796, 13, 583, 498, 291, 600, 668, 6359, 51660], "temperature": 0.0, "avg_logprob": -0.09570494152250744, "compression_ratio": 1.563573883161512, "no_speech_prob": 0.008312498219311237}, {"id": 16, "seek": 8072, "start": 80.72, "end": 85.84, "text": " when you might be replaced in your job by an AI model and when AIs might be able to do everything", "tokens": [50364, 562, 291, 1062, 312, 10772, 294, 428, 1691, 538, 364, 7318, 2316, 293, 562, 316, 6802, 1062, 312, 1075, 281, 360, 1203, 50620], "temperature": 0.0, "avg_logprob": -0.06108441798807048, "compression_ratio": 1.7492447129909365, "no_speech_prob": 0.0373094417154789}, {"id": 17, "seek": 8072, "start": 85.84, "end": 90.24, "text": " that humans can do for less than what it costs to feed a person and keep them alive, then this", "tokens": [50620, 300, 6255, 393, 360, 337, 1570, 813, 437, 309, 5497, 281, 3154, 257, 954, 293, 1066, 552, 5465, 11, 550, 341, 50840], "temperature": 0.0, "avg_logprob": -0.06108441798807048, "compression_ratio": 1.7492447129909365, "no_speech_prob": 0.0373094417154789}, {"id": 18, "seek": 8072, "start": 90.24, "end": 94.64, "text": " episode will help you think about that a little bit more clearly and perhaps have a little bit", "tokens": [50840, 3500, 486, 854, 291, 519, 466, 300, 257, 707, 857, 544, 4448, 293, 4317, 362, 257, 707, 857, 51060], "temperature": 0.0, "avg_logprob": -0.06108441798807048, "compression_ratio": 1.7492447129909365, "no_speech_prob": 0.0373094417154789}, {"id": 19, "seek": 8072, "start": 94.64, "end": 100.56, "text": " more idea of what to expect. In particular, Louisa and Tom discuss how long it will take for AIs to", "tokens": [51060, 544, 1558, 295, 437, 281, 2066, 13, 682, 1729, 11, 7272, 3837, 293, 5041, 2248, 577, 938, 309, 486, 747, 337, 316, 6802, 281, 51356], "temperature": 0.0, "avg_logprob": -0.06108441798807048, "compression_ratio": 1.7492447129909365, "no_speech_prob": 0.0373094417154789}, {"id": 20, "seek": 8072, "start": 100.56, "end": 105.84, "text": " go from being able to do 20% of the work humans are doing to being able to do all of it, what", "tokens": [51356, 352, 490, 885, 1075, 281, 360, 945, 4, 295, 264, 589, 6255, 366, 884, 281, 885, 1075, 281, 360, 439, 295, 309, 11, 437, 51620], "temperature": 0.0, "avg_logprob": -0.06108441798807048, "compression_ratio": 1.7492447129909365, "no_speech_prob": 0.0373094417154789}, {"id": 21, "seek": 8072, "start": 105.84, "end": 110.56, "text": " underlying factors are driving progress in AI and how much each of those factors is contributing.", "tokens": [51620, 14217, 6771, 366, 4840, 4205, 294, 7318, 293, 577, 709, 1184, 295, 729, 6771, 307, 19270, 13, 51856], "temperature": 0.0, "avg_logprob": -0.06108441798807048, "compression_ratio": 1.7492447129909365, "no_speech_prob": 0.0373094417154789}, {"id": 22, "seek": 11072, "start": 111.12, "end": 115.2, "text": " Whether we should expect that progress to speed up or slow down in incoming years,", "tokens": [50384, 8503, 321, 820, 2066, 300, 4205, 281, 3073, 493, 420, 2964, 760, 294, 22341, 924, 11, 50588], "temperature": 0.0, "avg_logprob": -0.05604804140850178, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0006877635605633259}, {"id": 23, "seek": 11072, "start": 115.76, "end": 120.64, "text": " how much computer hardware is used to train these models and whether it can continue increasing at", "tokens": [50616, 577, 709, 3820, 8837, 307, 1143, 281, 3847, 613, 5245, 293, 1968, 309, 393, 2354, 5662, 412, 50860], "temperature": 0.0, "avg_logprob": -0.05604804140850178, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0006877635605633259}, {"id": 24, "seek": 11072, "start": 120.64, "end": 125.6, "text": " the absolutely blistering rate that it has been doing the last 10 years, when AI systems might", "tokens": [50860, 264, 3122, 888, 1964, 278, 3314, 300, 309, 575, 668, 884, 264, 1036, 1266, 924, 11, 562, 7318, 3652, 1062, 51108], "temperature": 0.0, "avg_logprob": -0.05604804140850178, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0006877635605633259}, {"id": 25, "seek": 11072, "start": 125.6, "end": 130.72, "text": " be able to do scientific research and what implications that would then have, and also when", "tokens": [51108, 312, 1075, 281, 360, 8134, 2132, 293, 437, 16602, 300, 576, 550, 362, 11, 293, 611, 562, 51364], "temperature": 0.0, "avg_logprob": -0.05604804140850178, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0006877635605633259}, {"id": 26, "seek": 11072, "start": 130.72, "end": 136.24, "text": " we might expect all of this to noticeably increase GDP in a visible way and what that could look like", "tokens": [51364, 321, 1062, 2066, 439, 295, 341, 281, 3449, 1188, 3488, 19599, 294, 257, 8974, 636, 293, 437, 300, 727, 574, 411, 51640], "temperature": 0.0, "avg_logprob": -0.05604804140850178, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0006877635605633259}, {"id": 27, "seek": 13624, "start": 136.24, "end": 141.44, "text": " and then what new bottlenecks might exist in an economy where AI systems were doing most of the", "tokens": [50364, 293, 550, 437, 777, 44641, 2761, 1062, 2514, 294, 364, 5010, 689, 7318, 3652, 645, 884, 881, 295, 264, 50624], "temperature": 0.0, "avg_logprob": -0.08715309474779212, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.008574230596423149}, {"id": 28, "seek": 13624, "start": 141.44, "end": 147.68, "text": " work. And on top of that, plenty of other issues besides. Tom's expectations for the future are", "tokens": [50624, 589, 13, 400, 322, 1192, 295, 300, 11, 7140, 295, 661, 2663, 11868, 13, 5041, 311, 9843, 337, 264, 2027, 366, 50936], "temperature": 0.0, "avg_logprob": -0.08715309474779212, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.008574230596423149}, {"id": 29, "seek": 13624, "start": 147.68, "end": 152.4, "text": " exciting or alarming, I guess, depending on how you want to look at them. Regular listeners will", "tokens": [50936, 4670, 420, 44043, 11, 286, 2041, 11, 5413, 322, 577, 291, 528, 281, 574, 412, 552, 13, 45659, 23274, 486, 51172], "temperature": 0.0, "avg_logprob": -0.08715309474779212, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.008574230596423149}, {"id": 30, "seek": 13624, "start": 152.4, "end": 158.64000000000001, "text": " have heard me do quite a few interviews on AI over the years, but those interviews tend not to", "tokens": [51172, 362, 2198, 385, 360, 1596, 257, 1326, 12318, 322, 7318, 670, 264, 924, 11, 457, 729, 12318, 3928, 406, 281, 51484], "temperature": 0.0, "avg_logprob": -0.08715309474779212, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.008574230596423149}, {"id": 31, "seek": 13624, "start": 158.64000000000001, "end": 163.68, "text": " focus on my opinions, at least they're not meant to. So it's possible that people don't have that", "tokens": [51484, 1879, 322, 452, 11819, 11, 412, 1935, 436, 434, 406, 4140, 281, 13, 407, 309, 311, 1944, 300, 561, 500, 380, 362, 300, 51736], "temperature": 0.0, "avg_logprob": -0.08715309474779212, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.008574230596423149}, {"id": 32, "seek": 16368, "start": 163.68, "end": 168.96, "text": " much of a sense of where I stand on this personally. In case you're interested, I think", "tokens": [50364, 709, 295, 257, 2020, 295, 689, 286, 1463, 322, 341, 5665, 13, 682, 1389, 291, 434, 3102, 11, 286, 519, 50628], "temperature": 0.0, "avg_logprob": -0.0639844852945079, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.005728782620280981}, {"id": 33, "seek": 16368, "start": 168.96, "end": 175.28, "text": " the chances that you and I are either killed due to actions taken by AI systems, or that we live", "tokens": [50628, 264, 10486, 300, 291, 293, 286, 366, 2139, 4652, 3462, 281, 5909, 2726, 538, 7318, 3652, 11, 420, 300, 321, 1621, 50944], "temperature": 0.0, "avg_logprob": -0.0639844852945079, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.005728782620280981}, {"id": 34, "seek": 16368, "start": 175.28, "end": 181.28, "text": " to see humanity unintentionally lose control of its future because of AI systems are greater than", "tokens": [50944, 281, 536, 10243, 45514, 379, 3624, 1969, 295, 1080, 2027, 570, 295, 7318, 3652, 366, 5044, 813, 51244], "temperature": 0.0, "avg_logprob": -0.0639844852945079, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.005728782620280981}, {"id": 35, "seek": 16368, "start": 181.28, "end": 187.28, "text": " 10%, greater than a one in 10 chance. Looking at surveys and polling now, it seems like both AI", "tokens": [51244, 1266, 8923, 5044, 813, 257, 472, 294, 1266, 2931, 13, 11053, 412, 22711, 293, 29518, 586, 11, 309, 2544, 411, 1293, 7318, 51544], "temperature": 0.0, "avg_logprob": -0.0639844852945079, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.005728782620280981}, {"id": 36, "seek": 16368, "start": 187.28, "end": 192.32, "text": " experts and the general public are converging on a view that's not too far from that. It's a view", "tokens": [51544, 8572, 293, 264, 2674, 1908, 366, 9652, 3249, 322, 257, 1910, 300, 311, 406, 886, 1400, 490, 300, 13, 467, 311, 257, 1910, 51796], "temperature": 0.0, "avg_logprob": -0.0639844852945079, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.005728782620280981}, {"id": 37, "seek": 19232, "start": 192.32, "end": 198.32, "text": " that previously seemed idiosyncratic, but now is really quite mainstream. Naturally, if that if", "tokens": [50364, 300, 8046, 6576, 4496, 2717, 2534, 10757, 2399, 11, 457, 586, 307, 534, 1596, 15960, 13, 34304, 11, 498, 300, 498, 50664], "temperature": 0.0, "avg_logprob": -0.07253149393442515, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0050582862459123135}, {"id": 38, "seek": 19232, "start": 198.32, "end": 204.23999999999998, "text": " the odds are like that, then it makes figuring out how to safely develop and deploy AI, you know,", "tokens": [50664, 264, 17439, 366, 411, 300, 11, 550, 309, 1669, 15213, 484, 577, 281, 11750, 1499, 293, 7274, 7318, 11, 291, 458, 11, 50960], "temperature": 0.0, "avg_logprob": -0.07253149393442515, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0050582862459123135}, {"id": 39, "seek": 19232, "start": 204.23999999999998, "end": 208.88, "text": " probably the issue of our time. And indeed, one of the things that we should most care about from", "tokens": [50960, 1391, 264, 2734, 295, 527, 565, 13, 400, 6451, 11, 472, 295, 264, 721, 300, 321, 820, 881, 1127, 466, 490, 51192], "temperature": 0.0, "avg_logprob": -0.07253149393442515, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0050582862459123135}, {"id": 40, "seek": 19232, "start": 208.88, "end": 213.84, "text": " even a selfish point of view, or if we have children care about from a parental point of view.", "tokens": [51192, 754, 257, 19074, 935, 295, 1910, 11, 420, 498, 321, 362, 2227, 1127, 466, 490, 257, 41113, 935, 295, 1910, 13, 51440], "temperature": 0.0, "avg_logprob": -0.07253149393442515, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0050582862459123135}, {"id": 41, "seek": 19232, "start": 214.48, "end": 219.44, "text": " I'm an economist by training and I entirely understand how the industrial revolution", "tokens": [51472, 286, 478, 364, 36696, 538, 3097, 293, 286, 7696, 1223, 577, 264, 9987, 8894, 51720], "temperature": 0.0, "avg_logprob": -0.07253149393442515, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0050582862459123135}, {"id": 42, "seek": 21944, "start": 219.44, "end": 225.2, "text": " ultimately raised incomes across generations. And that while factory automation was indeed", "tokens": [50364, 6284, 6005, 42458, 2108, 10593, 13, 400, 300, 1339, 9265, 17769, 390, 6451, 50652], "temperature": 0.0, "avg_logprob": -0.10676958065221806, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.05030389502644539}, {"id": 43, "seek": 21944, "start": 225.2, "end": 230.48, "text": " financially ruinous for plenty of individuals, it didn't then result in persistent unemployment.", "tokens": [50652, 20469, 15514, 563, 337, 7140, 295, 5346, 11, 309, 994, 380, 550, 1874, 294, 24315, 17438, 13, 50916], "temperature": 0.0, "avg_logprob": -0.10676958065221806, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.05030389502644539}, {"id": 44, "seek": 21944, "start": 230.48, "end": 234.88, "text": " But nonetheless, despite understanding and appreciating all of that, I am skeptical that", "tokens": [50916, 583, 26756, 11, 7228, 3701, 293, 3616, 990, 439, 295, 300, 11, 286, 669, 28601, 300, 51136], "temperature": 0.0, "avg_logprob": -0.10676958065221806, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.05030389502644539}, {"id": 45, "seek": 21944, "start": 234.88, "end": 240.4, "text": " there are going to be paying jobs for children being born today, or if not them, that they'll be", "tokens": [51136, 456, 366, 516, 281, 312, 6229, 4782, 337, 2227, 885, 4232, 965, 11, 420, 498, 406, 552, 11, 300, 436, 603, 312, 51412], "temperature": 0.0, "avg_logprob": -0.10676958065221806, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.05030389502644539}, {"id": 46, "seek": 21944, "start": 240.4, "end": 245.44, "text": " paying jobs at least for their children. And we are just we're flying by the seat of our pants", "tokens": [51412, 6229, 4782, 412, 1935, 337, 641, 2227, 13, 400, 321, 366, 445, 321, 434, 7137, 538, 264, 6121, 295, 527, 10082, 51664], "temperature": 0.0, "avg_logprob": -0.10676958065221806, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.05030389502644539}, {"id": 47, "seek": 24544, "start": 245.44, "end": 250.07999999999998, "text": " here and have not really figured out a plan ahead of time about what we're going to do", "tokens": [50364, 510, 293, 362, 406, 534, 8932, 484, 257, 1393, 2286, 295, 565, 466, 437, 321, 434, 516, 281, 360, 50596], "temperature": 0.0, "avg_logprob": -0.08482279096330915, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.060060299932956696}, {"id": 48, "seek": 24544, "start": 250.07999999999998, "end": 255.28, "text": " as this technology just completely upends I think existing social relations and economic systems.", "tokens": [50596, 382, 341, 2899, 445, 2584, 493, 2581, 286, 519, 6741, 2093, 2299, 293, 4836, 3652, 13, 50856], "temperature": 0.0, "avg_logprob": -0.08482279096330915, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.060060299932956696}, {"id": 49, "seek": 24544, "start": 256.4, "end": 262.24, "text": " Well, I'll describe the overall situation that humanity finds itself in to be pretty terrifying.", "tokens": [50912, 1042, 11, 286, 603, 6786, 264, 4787, 2590, 300, 10243, 10704, 2564, 294, 281, 312, 1238, 18106, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08482279096330915, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.060060299932956696}, {"id": 50, "seek": 24544, "start": 262.24, "end": 267.52, "text": " The fact that all kinds of different people are waking up now to the risks here does give me hope", "tokens": [51204, 440, 1186, 300, 439, 3685, 295, 819, 561, 366, 20447, 493, 586, 281, 264, 10888, 510, 775, 976, 385, 1454, 51468], "temperature": 0.0, "avg_logprob": -0.08482279096330915, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.060060299932956696}, {"id": 51, "seek": 24544, "start": 267.52, "end": 272.4, "text": " that we can coordinate to prevent the worst. As you imagine, we'll have more to say to", "tokens": [51468, 300, 321, 393, 15670, 281, 4871, 264, 5855, 13, 1018, 291, 3811, 11, 321, 603, 362, 544, 281, 584, 281, 51712], "temperature": 0.0, "avg_logprob": -0.08482279096330915, "compression_ratio": 1.595890410958904, "no_speech_prob": 0.060060299932956696}, {"id": 52, "seek": 27240, "start": 272.4, "end": 278.23999999999995, "text": " expand on all of that in in future episodes. But for now, I bring you Louise Rodriguez and Tom Davidson.", "tokens": [50364, 5268, 322, 439, 295, 300, 294, 294, 2027, 9313, 13, 583, 337, 586, 11, 286, 1565, 291, 35962, 37304, 293, 5041, 44401, 13, 50656], "temperature": 0.0, "avg_logprob": -0.22564897350236482, "compression_ratio": 1.3133333333333332, "no_speech_prob": 0.011856989935040474}, {"id": 53, "seek": 27240, "start": 293.12, "end": 297.59999999999997, "text": " Today, I'm speaking with Tom Davidson. Tom's a senior research analyst at Open Philanthropy,", "tokens": [51400, 2692, 11, 286, 478, 4124, 365, 5041, 44401, 13, 5041, 311, 257, 7965, 2132, 19085, 412, 7238, 7777, 282, 14222, 88, 11, 51624], "temperature": 0.0, "avg_logprob": -0.22564897350236482, "compression_ratio": 1.3133333333333332, "no_speech_prob": 0.011856989935040474}, {"id": 54, "seek": 29760, "start": 297.6, "end": 303.04, "text": " where his main focus is on when we might get transformative AI. Before joining Open Philanthropy,", "tokens": [50364, 689, 702, 2135, 1879, 307, 322, 562, 321, 1062, 483, 36070, 7318, 13, 4546, 5549, 7238, 7777, 282, 14222, 88, 11, 50636], "temperature": 0.0, "avg_logprob": -0.07551909695152476, "compression_ratio": 1.5759493670886076, "no_speech_prob": 0.0873958170413971}, {"id": 55, "seek": 29760, "start": 303.04, "end": 306.88, "text": " Tom taught science through Teach First at a comprehensive school in East London,", "tokens": [50636, 5041, 5928, 3497, 807, 26816, 2386, 412, 257, 13914, 1395, 294, 6747, 7042, 11, 50828], "temperature": 0.0, "avg_logprob": -0.07551909695152476, "compression_ratio": 1.5759493670886076, "no_speech_prob": 0.0873958170413971}, {"id": 56, "seek": 29760, "start": 306.88, "end": 311.28000000000003, "text": " and then was a data scientist for an education technology startup. And before all of that,", "tokens": [50828, 293, 550, 390, 257, 1412, 12662, 337, 364, 3309, 2899, 18578, 13, 400, 949, 439, 295, 300, 11, 51048], "temperature": 0.0, "avg_logprob": -0.07551909695152476, "compression_ratio": 1.5759493670886076, "no_speech_prob": 0.0873958170413971}, {"id": 57, "seek": 29760, "start": 311.28000000000003, "end": 315.04, "text": " Tom studied physics and philosophy at Oxford. Thanks for coming on the podcast, Tom.", "tokens": [51048, 5041, 9454, 10649, 293, 10675, 412, 24786, 13, 2561, 337, 1348, 322, 264, 7367, 11, 5041, 13, 51236], "temperature": 0.0, "avg_logprob": -0.07551909695152476, "compression_ratio": 1.5759493670886076, "no_speech_prob": 0.0873958170413971}, {"id": 58, "seek": 29760, "start": 315.6, "end": 317.20000000000005, "text": " Thanks, Louise. It's a pleasure to be here.", "tokens": [51264, 2561, 11, 35962, 13, 467, 311, 257, 6834, 281, 312, 510, 13, 51344], "temperature": 0.0, "avg_logprob": -0.07551909695152476, "compression_ratio": 1.5759493670886076, "no_speech_prob": 0.0873958170413971}, {"id": 59, "seek": 29760, "start": 317.84000000000003, "end": 326.64000000000004, "text": " So I hope to talk about how fast we might go from kind of OK AI to AI that can do everything humans", "tokens": [51376, 407, 286, 1454, 281, 751, 466, 577, 2370, 321, 1062, 352, 490, 733, 295, 2264, 7318, 281, 7318, 300, 393, 360, 1203, 6255, 51816], "temperature": 0.0, "avg_logprob": -0.07551909695152476, "compression_ratio": 1.5759493670886076, "no_speech_prob": 0.0873958170413971}, {"id": 60, "seek": 32664, "start": 326.64, "end": 333.76, "text": " can do, plus how that'll affect the economy and the world. But first, yeah, how worried are you", "tokens": [50364, 393, 360, 11, 1804, 577, 300, 603, 3345, 264, 5010, 293, 264, 1002, 13, 583, 700, 11, 1338, 11, 577, 5804, 366, 291, 50720], "temperature": 0.0, "avg_logprob": -0.08833680982175081, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.007629761938005686}, {"id": 61, "seek": 32664, "start": 333.76, "end": 340.15999999999997, "text": " personally about the risks from AI? So about a year ago, I sat down and spent spent a morning", "tokens": [50720, 5665, 466, 264, 10888, 490, 7318, 30, 407, 466, 257, 1064, 2057, 11, 286, 3227, 760, 293, 4418, 4418, 257, 2446, 51040], "temperature": 0.0, "avg_logprob": -0.08833680982175081, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.007629761938005686}, {"id": 62, "seek": 32664, "start": 340.15999999999997, "end": 345.68, "text": " trying to figure out, you know, what are my percentages of AI by certain time and what's", "tokens": [51040, 1382, 281, 2573, 484, 11, 291, 458, 11, 437, 366, 452, 42270, 295, 7318, 538, 1629, 565, 293, 437, 311, 51316], "temperature": 0.0, "avg_logprob": -0.08833680982175081, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.007629761938005686}, {"id": 63, "seek": 32664, "start": 345.68, "end": 352.4, "text": " my percentage that there's existential catastrophe from AI. And so I was focusing on the possibility", "tokens": [51316, 452, 9668, 300, 456, 311, 37133, 36043, 490, 7318, 13, 400, 370, 286, 390, 8416, 322, 264, 7959, 51652], "temperature": 0.0, "avg_logprob": -0.08833680982175081, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.007629761938005686}, {"id": 64, "seek": 35240, "start": 352.4, "end": 358.88, "text": " that AI disempowers humanity and just takes over control of of society and the economy and", "tokens": [50364, 300, 7318, 717, 443, 47953, 10243, 293, 445, 2516, 670, 1969, 295, 295, 4086, 293, 264, 5010, 293, 50688], "temperature": 0.0, "avg_logprob": -0.12208604299893944, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.012898276560008526}, {"id": 65, "seek": 35240, "start": 358.88, "end": 363.52, "text": " and then what happens in the future. And a year ago, I landed at a number that was a bit above", "tokens": [50688, 293, 550, 437, 2314, 294, 264, 2027, 13, 400, 257, 1064, 2057, 11, 286, 15336, 412, 257, 1230, 300, 390, 257, 857, 3673, 50920], "temperature": 0.0, "avg_logprob": -0.12208604299893944, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.012898276560008526}, {"id": 66, "seek": 35240, "start": 363.52, "end": 372.96, "text": " 10% for the probability that AI takes over by 2070. Right. OK. I mean, that's already pretty high.", "tokens": [50920, 1266, 4, 337, 264, 8482, 300, 7318, 2516, 670, 538, 945, 5867, 13, 1779, 13, 2264, 13, 286, 914, 11, 300, 311, 1217, 1238, 1090, 13, 51392], "temperature": 0.0, "avg_logprob": -0.12208604299893944, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.012898276560008526}, {"id": 67, "seek": 35240, "start": 374.0, "end": 377.35999999999996, "text": " Yeah, that is that is already very high and much too high. Yeah.", "tokens": [51444, 865, 11, 300, 307, 300, 307, 1217, 588, 1090, 293, 709, 886, 1090, 13, 865, 13, 51612], "temperature": 0.0, "avg_logprob": -0.12208604299893944, "compression_ratio": 1.5863636363636364, "no_speech_prob": 0.012898276560008526}, {"id": 68, "seek": 37736, "start": 378.32, "end": 383.44, "text": " I think since since then, if I if I read the exercise today, I think I'd be close to 20%", "tokens": [50412, 286, 519, 1670, 1670, 550, 11, 498, 286, 498, 286, 1401, 264, 5380, 965, 11, 286, 519, 286, 1116, 312, 1998, 281, 945, 4, 50668], "temperature": 0.0, "avg_logprob": -0.15319328496951867, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004151465371251106}, {"id": 69, "seek": 37736, "start": 385.44, "end": 390.08000000000004, "text": " I think I compare to then I think it's just more likely that we develop AI that's capable", "tokens": [50768, 286, 519, 286, 6794, 281, 550, 286, 519, 309, 311, 445, 544, 3700, 300, 321, 1499, 7318, 300, 311, 8189, 51000], "temperature": 0.0, "avg_logprob": -0.15319328496951867, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004151465371251106}, {"id": 70, "seek": 37736, "start": 390.08000000000004, "end": 395.84000000000003, "text": " of doing that by 2070. I also think that it's it's just pretty likely to happen in the next 20 years.", "tokens": [51000, 295, 884, 300, 538, 945, 5867, 13, 286, 611, 519, 300, 309, 311, 309, 311, 445, 1238, 3700, 281, 1051, 294, 264, 958, 945, 924, 13, 51288], "temperature": 0.0, "avg_logprob": -0.15319328496951867, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004151465371251106}, {"id": 71, "seek": 37736, "start": 397.52000000000004, "end": 402.24, "text": " Which then makes the chance that it goes badly, I think higher because we have less time to repair.", "tokens": [51372, 3013, 550, 1669, 264, 2931, 300, 309, 1709, 13425, 11, 286, 519, 2946, 570, 321, 362, 1570, 565, 281, 10535, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15319328496951867, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.004151465371251106}, {"id": 72, "seek": 40224, "start": 402.24, "end": 407.52, "text": " So both those things, I think mean I'd probably be at about 20% if I if I read the exercise today.", "tokens": [50364, 407, 1293, 729, 721, 11, 286, 519, 914, 286, 1116, 1391, 312, 412, 466, 945, 4, 498, 286, 498, 286, 1401, 264, 5380, 965, 13, 50628], "temperature": 0.0, "avg_logprob": -0.12013293306032817, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.004642292391508818}, {"id": 73, "seek": 40224, "start": 408.64, "end": 411.36, "text": " Yeah, we'll talk more about a couple of those things. But first,", "tokens": [50684, 865, 11, 321, 603, 751, 544, 466, 257, 1916, 295, 729, 721, 13, 583, 700, 11, 50820], "temperature": 0.0, "avg_logprob": -0.12013293306032817, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.004642292391508818}, {"id": 74, "seek": 40224, "start": 412.56, "end": 418.16, "text": " yeah, I'm curious if there's a type of scenario you have in mind when you're thinking about that", "tokens": [50880, 1338, 11, 286, 478, 6369, 498, 456, 311, 257, 2010, 295, 9005, 291, 362, 294, 1575, 562, 291, 434, 1953, 466, 300, 51160], "temperature": 0.0, "avg_logprob": -0.12013293306032817, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.004642292391508818}, {"id": 75, "seek": 40224, "start": 418.16, "end": 426.32, "text": " 10 to 20% that makes you, yeah, especially worried. Yeah, I think the main line scenario I have", "tokens": [51160, 1266, 281, 945, 4, 300, 1669, 291, 11, 1338, 11, 2318, 5804, 13, 865, 11, 286, 519, 264, 2135, 1622, 9005, 286, 362, 51568], "temperature": 0.0, "avg_logprob": -0.12013293306032817, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.004642292391508818}, {"id": 76, "seek": 42632, "start": 426.88, "end": 437.04, "text": " is something like in the next 10 to 15 years, possibly sooner, we train an AI that is able to", "tokens": [50392, 307, 746, 411, 294, 264, 958, 1266, 281, 2119, 924, 11, 6264, 15324, 11, 321, 3847, 364, 7318, 300, 307, 1075, 281, 50900], "temperature": 0.0, "avg_logprob": -0.10117379478786302, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.0687982514500618}, {"id": 77, "seek": 42632, "start": 438.32, "end": 445.12, "text": " massively enhance the productivity of AI R&D workers. So people who are currently working to", "tokens": [50964, 29379, 11985, 264, 15604, 295, 7318, 497, 5, 35, 5600, 13, 407, 561, 567, 366, 4362, 1364, 281, 51304], "temperature": 0.0, "avg_logprob": -0.10117379478786302, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.0687982514500618}, {"id": 78, "seek": 42632, "start": 445.12, "end": 450.96, "text": " make AI better, maybe it makes them let's say five times as productive. So something like a large", "tokens": [51304, 652, 7318, 1101, 11, 1310, 309, 1669, 552, 718, 311, 584, 1732, 1413, 382, 13304, 13, 407, 746, 411, 257, 2416, 51596], "temperature": 0.0, "avg_logprob": -0.10117379478786302, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.0687982514500618}, {"id": 79, "seek": 45096, "start": 450.96, "end": 457.35999999999996, "text": " language model helps people working on AI R&D in particular, like code much faster or develop", "tokens": [50364, 2856, 2316, 3665, 561, 1364, 322, 7318, 497, 5, 35, 294, 1729, 11, 411, 3089, 709, 4663, 420, 1499, 50684], "temperature": 0.0, "avg_logprob": -0.12213759379343944, "compression_ratio": 1.6137931034482758, "no_speech_prob": 0.018422594293951988}, {"id": 80, "seek": 45096, "start": 457.35999999999996, "end": 463.44, "text": " better algorithms or something. Exactly. And it means they can work five times faster ish.", "tokens": [50684, 1101, 14642, 420, 746, 13, 7587, 13, 400, 309, 1355, 436, 393, 589, 1732, 1413, 4663, 307, 71, 13, 50988], "temperature": 0.0, "avg_logprob": -0.12213759379343944, "compression_ratio": 1.6137931034482758, "no_speech_prob": 0.018422594293951988}, {"id": 81, "seek": 45096, "start": 463.44, "end": 469.44, "text": " Exactly. Right. Okay. And then I think it won't be long after that, because AI is then going to be", "tokens": [50988, 7587, 13, 1779, 13, 1033, 13, 400, 550, 286, 519, 309, 1582, 380, 312, 938, 934, 300, 11, 570, 7318, 307, 550, 516, 281, 312, 51288], "temperature": 0.0, "avg_logprob": -0.12213759379343944, "compression_ratio": 1.6137931034482758, "no_speech_prob": 0.018422594293951988}, {"id": 82, "seek": 45096, "start": 469.44, "end": 475.76, "text": " improving more quickly before AI is able to do everything that the current employees of DeepMind", "tokens": [51288, 11470, 544, 2661, 949, 7318, 307, 1075, 281, 360, 1203, 300, 264, 2190, 6619, 295, 14895, 44, 471, 51604], "temperature": 0.0, "avg_logprob": -0.12213759379343944, "compression_ratio": 1.6137931034482758, "no_speech_prob": 0.018422594293951988}, {"id": 83, "seek": 45096, "start": 475.76, "end": 480.71999999999997, "text": " and OpenAI are currently doing for their jobs, at least the ones that can work remotely", "tokens": [51604, 293, 7238, 48698, 366, 4362, 884, 337, 641, 4782, 11, 412, 1935, 264, 2306, 300, 393, 589, 20824, 51852], "temperature": 0.0, "avg_logprob": -0.12213759379343944, "compression_ratio": 1.6137931034482758, "no_speech_prob": 0.018422594293951988}, {"id": 84, "seek": 48072, "start": 480.72, "end": 485.36, "text": " and do their work from a computer. Are you kind of distinguishing between the ones that work", "tokens": [50364, 293, 360, 641, 589, 490, 257, 3820, 13, 2014, 291, 733, 295, 11365, 3807, 1296, 264, 2306, 300, 589, 50596], "temperature": 0.0, "avg_logprob": -0.07810665766398112, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0018618237227201462}, {"id": 85, "seek": 48072, "start": 485.36, "end": 491.76000000000005, "text": " with like physical robots or the ones that work with like, I don't know, the mail room at DeepMind?", "tokens": [50596, 365, 411, 4001, 14733, 420, 264, 2306, 300, 589, 365, 411, 11, 286, 500, 380, 458, 11, 264, 10071, 1808, 412, 14895, 44, 471, 30, 50916], "temperature": 0.0, "avg_logprob": -0.07810665766398112, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0018618237227201462}, {"id": 86, "seek": 48072, "start": 492.64000000000004, "end": 497.44000000000005, "text": " Yeah. So if there's someone, I don't know if this is true, but if there's someone at DeepMind", "tokens": [50960, 865, 13, 407, 498, 456, 311, 1580, 11, 286, 500, 380, 458, 498, 341, 307, 2074, 11, 457, 498, 456, 311, 1580, 412, 14895, 44, 471, 51200], "temperature": 0.0, "avg_logprob": -0.07810665766398112, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0018618237227201462}, {"id": 87, "seek": 48072, "start": 497.44000000000005, "end": 502.72, "text": " who is physically stacking the computer chips into the data center, then that's a kind of", "tokens": [51200, 567, 307, 9762, 41376, 264, 3820, 11583, 666, 264, 1412, 3056, 11, 550, 300, 311, 257, 733, 295, 51464], "temperature": 0.0, "avg_logprob": -0.07810665766398112, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0018618237227201462}, {"id": 88, "seek": 48072, "start": 502.72, "end": 507.20000000000005, "text": " type of physical work, which I don't think would necessarily, you know, follow immediately on.", "tokens": [51464, 2010, 295, 4001, 589, 11, 597, 286, 500, 380, 519, 576, 4725, 11, 291, 458, 11, 1524, 4258, 322, 13, 51688], "temperature": 0.0, "avg_logprob": -0.07810665766398112, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.0018618237227201462}, {"id": 89, "seek": 50720, "start": 507.92, "end": 512.48, "text": " But I think most of the work is not like that. Right. So I think AI, you know,", "tokens": [50400, 583, 286, 519, 881, 295, 264, 589, 307, 406, 411, 300, 13, 1779, 13, 407, 286, 519, 7318, 11, 291, 458, 11, 50628], "temperature": 0.0, "avg_logprob": -0.07628663881557195, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.012938177213072777}, {"id": 90, "seek": 50720, "start": 512.48, "end": 517.84, "text": " especially most of the work for, you know, advancing AI is stuff that you can do on your laptop.", "tokens": [50628, 2318, 881, 295, 264, 589, 337, 11, 291, 458, 11, 27267, 7318, 307, 1507, 300, 291, 393, 360, 322, 428, 10732, 13, 50896], "temperature": 0.0, "avg_logprob": -0.07628663881557195, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.012938177213072777}, {"id": 91, "seek": 50720, "start": 517.84, "end": 524.16, "text": " So first we get the kind of 5x productivity gain. A little time later, we get AI that can do all of", "tokens": [50896, 407, 700, 321, 483, 264, 733, 295, 1025, 87, 15604, 6052, 13, 316, 707, 565, 1780, 11, 321, 483, 7318, 300, 393, 360, 439, 295, 51212], "temperature": 0.0, "avg_logprob": -0.07628663881557195, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.012938177213072777}, {"id": 92, "seek": 50720, "start": 524.16, "end": 529.36, "text": " the work that people at OpenAI and DeepMind can do. And I really think the gap between those is", "tokens": [51212, 264, 589, 300, 561, 412, 7238, 48698, 293, 14895, 44, 471, 393, 360, 13, 400, 286, 534, 519, 264, 7417, 1296, 729, 307, 51472], "temperature": 0.0, "avg_logprob": -0.07628663881557195, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.012938177213072777}, {"id": 93, "seek": 50720, "start": 529.36, "end": 534.0, "text": " not going to be very big. And we'll probably discuss a bit more about that later. At that point,", "tokens": [51472, 406, 516, 281, 312, 588, 955, 13, 400, 321, 603, 1391, 2248, 257, 857, 544, 466, 300, 1780, 13, 1711, 300, 935, 11, 51704], "temperature": 0.0, "avg_logprob": -0.07628663881557195, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.012938177213072777}, {"id": 94, "seek": 53400, "start": 534.0, "end": 540.88, "text": " I think AI is going to be improving at a blistering pace, absent a very specific effort to slow down,", "tokens": [50364, 286, 519, 7318, 307, 516, 281, 312, 11470, 412, 257, 888, 1964, 278, 11638, 11, 25185, 257, 588, 2685, 4630, 281, 2964, 760, 11, 50708], "temperature": 0.0, "avg_logprob": -0.10988469374807257, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.022199243307113647}, {"id": 95, "seek": 53400, "start": 540.88, "end": 546.08, "text": " which I hope we really make, but absent that effort and absent coordinating to make sure that", "tokens": [50708, 597, 286, 1454, 321, 534, 652, 11, 457, 25185, 300, 4630, 293, 25185, 37824, 281, 652, 988, 300, 50968], "temperature": 0.0, "avg_logprob": -0.10988469374807257, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.022199243307113647}, {"id": 96, "seek": 53400, "start": 546.08, "end": 551.36, "text": " everyone is slowing down. You know, I think like a thousand next improvement in the AI's capabilities", "tokens": [50968, 1518, 307, 26958, 760, 13, 509, 458, 11, 286, 519, 411, 257, 4714, 958, 10444, 294, 264, 7318, 311, 10862, 51232], "temperature": 0.0, "avg_logprob": -0.10988469374807257, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.022199243307113647}, {"id": 97, "seek": 53400, "start": 551.36, "end": 561.12, "text": " in a year is like a natural kind of conservative default. Wow. And so in this scenario, it wouldn't", "tokens": [51232, 294, 257, 1064, 307, 411, 257, 3303, 733, 295, 13780, 7576, 13, 3153, 13, 400, 370, 294, 341, 9005, 11, 309, 2759, 380, 51720], "temperature": 0.0, "avg_logprob": -0.10988469374807257, "compression_ratio": 1.640495867768595, "no_speech_prob": 0.022199243307113647}, {"id": 98, "seek": 56112, "start": 561.12, "end": 568.64, "text": " be long before the AI is just far outstripping the cognitive intelligence abilities of the smartest", "tokens": [50364, 312, 938, 949, 264, 7318, 307, 445, 1400, 484, 372, 470, 3759, 264, 15605, 7599, 11582, 295, 264, 41491, 50740], "temperature": 0.0, "avg_logprob": -0.08794336411559465, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.01583736203610897}, {"id": 99, "seek": 56112, "start": 568.64, "end": 574.08, "text": " humans and indeed even the smartest massive teams of humans working together. Wow. And when", "tokens": [50740, 6255, 293, 6451, 754, 264, 41491, 5994, 5491, 295, 6255, 1364, 1214, 13, 3153, 13, 400, 562, 51012], "temperature": 0.0, "avg_logprob": -0.08794336411559465, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.01583736203610897}, {"id": 100, "seek": 56112, "start": 574.08, "end": 578.4, "text": " you kind of crunch the numbers on how many AI's there's likely to be around this time,", "tokens": [51012, 291, 733, 295, 13386, 264, 3547, 322, 577, 867, 7318, 311, 456, 311, 3700, 281, 312, 926, 341, 565, 11, 51228], "temperature": 0.0, "avg_logprob": -0.08794336411559465, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.01583736203610897}, {"id": 101, "seek": 56112, "start": 578.4, "end": 584.08, "text": " there's going to be hundreds of millions and probably many billions of human worker equivalents", "tokens": [51228, 456, 311, 516, 281, 312, 6779, 295, 6803, 293, 1391, 867, 17375, 295, 1952, 11346, 9052, 791, 51512], "temperature": 0.0, "avg_logprob": -0.08794336411559465, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.01583736203610897}, {"id": 102, "seek": 56112, "start": 584.08, "end": 590.5600000000001, "text": " just in kind of AI cognitive ability. Is that because there are different AI systems or because", "tokens": [51512, 445, 294, 733, 295, 7318, 15605, 3485, 13, 1119, 300, 570, 456, 366, 819, 7318, 3652, 420, 570, 51836], "temperature": 0.0, "avg_logprob": -0.08794336411559465, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.01583736203610897}, {"id": 103, "seek": 59056, "start": 590.56, "end": 597.5999999999999, "text": " you've got that much brain power being deployed like through AI, even if it's only like five AI", "tokens": [50364, 291, 600, 658, 300, 709, 3567, 1347, 885, 17826, 411, 807, 7318, 11, 754, 498, 309, 311, 787, 411, 1732, 7318, 50716], "temperature": 0.0, "avg_logprob": -0.11815192269497231, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.001107414485886693}, {"id": 104, "seek": 59056, "start": 597.5999999999999, "end": 603.52, "text": " systems or something? It's like running lots of copies of maybe a few AI systems. Got it. So", "tokens": [50716, 3652, 420, 746, 30, 467, 311, 411, 2614, 3195, 295, 14341, 295, 1310, 257, 1326, 7318, 3652, 13, 5803, 309, 13, 407, 51012], "temperature": 0.0, "avg_logprob": -0.11815192269497231, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.001107414485886693}, {"id": 105, "seek": 59056, "start": 604.0799999999999, "end": 607.5999999999999, "text": " for example, I haven't crunched the numbers on this, but I would guess that if you took the", "tokens": [51040, 337, 1365, 11, 286, 2378, 380, 13386, 292, 264, 3547, 322, 341, 11, 457, 286, 576, 2041, 300, 498, 291, 1890, 264, 51216], "temperature": 0.0, "avg_logprob": -0.11815192269497231, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.001107414485886693}, {"id": 106, "seek": 59056, "start": 608.3199999999999, "end": 613.28, "text": " kind of computer chips that they use to train GPT-4 and then you just said how many copies of", "tokens": [51252, 733, 295, 3820, 11583, 300, 436, 764, 281, 3847, 26039, 51, 12, 19, 293, 550, 291, 445, 848, 577, 867, 14341, 295, 51500], "temperature": 0.0, "avg_logprob": -0.11815192269497231, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.001107414485886693}, {"id": 107, "seek": 59056, "start": 613.28, "end": 618.88, "text": " GPT-4 could we run using these computer chips, I guess that the answer is like maybe a million.", "tokens": [51500, 26039, 51, 12, 19, 727, 321, 1190, 1228, 613, 3820, 11583, 11, 286, 2041, 300, 264, 1867, 307, 411, 1310, 257, 2459, 13, 51780], "temperature": 0.0, "avg_logprob": -0.11815192269497231, "compression_ratio": 1.6725978647686832, "no_speech_prob": 0.001107414485886693}, {"id": 108, "seek": 61888, "start": 619.84, "end": 625.28, "text": " Wow. It might be a bit lower actually. I'm not I'm not sure for GPT-4, but I think by the time that", "tokens": [50412, 3153, 13, 467, 1062, 312, 257, 857, 3126, 767, 13, 286, 478, 406, 286, 478, 406, 988, 337, 26039, 51, 12, 19, 11, 457, 286, 519, 538, 264, 565, 300, 50684], "temperature": 0.0, "avg_logprob": -0.0909263142999613, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0009188124677166343}, {"id": 109, "seek": 61888, "start": 625.28, "end": 632.96, "text": " we train kind of AI that can fully replace all the human workers at an AI lab, I think it's", "tokens": [50684, 321, 3847, 733, 295, 7318, 300, 393, 4498, 7406, 439, 264, 1952, 5600, 412, 364, 7318, 2715, 11, 286, 519, 309, 311, 51068], "temperature": 0.0, "avg_logprob": -0.0909263142999613, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0009188124677166343}, {"id": 110, "seek": 61888, "start": 632.96, "end": 637.4399999999999, "text": " that number is going to be more extreme. Right. So I think that by the time we can train that kind", "tokens": [51068, 300, 1230, 307, 516, 281, 312, 544, 8084, 13, 1779, 13, 407, 286, 519, 300, 538, 264, 565, 321, 393, 3847, 300, 733, 51292], "temperature": 0.0, "avg_logprob": -0.0909263142999613, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0009188124677166343}, {"id": 111, "seek": 61888, "start": 637.4399999999999, "end": 643.12, "text": " of AI, you'll be able to just immediately use the compute that you use for training to run 100", "tokens": [51292, 295, 7318, 11, 291, 603, 312, 1075, 281, 445, 4258, 764, 264, 14722, 300, 291, 764, 337, 3097, 281, 1190, 2319, 51576], "temperature": 0.0, "avg_logprob": -0.0909263142999613, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0009188124677166343}, {"id": 112, "seek": 64312, "start": 643.12, "end": 648.96, "text": " million. Right. That's okay. That's insane. And the reason for that is because it just takes,", "tokens": [50364, 2459, 13, 1779, 13, 663, 311, 1392, 13, 663, 311, 10838, 13, 400, 264, 1778, 337, 300, 307, 570, 309, 445, 2516, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11287506866455078, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.1970950961112976}, {"id": 113, "seek": 64312, "start": 649.92, "end": 657.04, "text": " I don't know, like many times more compute to train an AI system than it does to then run them.", "tokens": [50704, 286, 500, 380, 458, 11, 411, 867, 1413, 544, 14722, 281, 3847, 364, 7318, 1185, 813, 309, 775, 281, 550, 1190, 552, 13, 51060], "temperature": 0.0, "avg_logprob": -0.11287506866455078, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.1970950961112976}, {"id": 114, "seek": 64312, "start": 657.76, "end": 663.76, "text": " Yeah, that's exactly right. Okay. One way to think about it is that the AI is trained on kind of", "tokens": [51096, 865, 11, 300, 311, 2293, 558, 13, 1033, 13, 1485, 636, 281, 519, 466, 309, 307, 300, 264, 7318, 307, 8895, 322, 733, 295, 51396], "temperature": 0.0, "avg_logprob": -0.11287506866455078, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.1970950961112976}, {"id": 115, "seek": 64312, "start": 663.76, "end": 668.96, "text": " millions of days of experience so that they get to be as good as they are. And that one,", "tokens": [51396, 6803, 295, 1708, 295, 1752, 370, 300, 436, 483, 281, 312, 382, 665, 382, 436, 366, 13, 400, 300, 472, 11, 51656], "temperature": 0.0, "avg_logprob": -0.11287506866455078, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.1970950961112976}, {"id": 116, "seek": 64312, "start": 668.96, "end": 672.16, "text": " if you've managed to train them for that much, then it's kind of obviously you can run millions.", "tokens": [51656, 498, 291, 600, 6453, 281, 3847, 552, 337, 300, 709, 11, 550, 309, 311, 733, 295, 2745, 291, 393, 1190, 6803, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11287506866455078, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.1970950961112976}, {"id": 117, "seek": 67216, "start": 672.16, "end": 680.56, "text": " Yeah. Okay. Got it. Okay. Okay. So then we're there. We've got potentially millions of human", "tokens": [50364, 865, 13, 1033, 13, 5803, 309, 13, 1033, 13, 1033, 13, 407, 550, 321, 434, 456, 13, 492, 600, 658, 7263, 6803, 295, 1952, 50784], "temperature": 0.0, "avg_logprob": -0.09683442356610539, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0005491083720698953}, {"id": 118, "seek": 67216, "start": 680.56, "end": 687.1999999999999, "text": " equivalents of copies of AI systems running and doing the kind of work that humans could do", "tokens": [50784, 9052, 791, 295, 14341, 295, 7318, 3652, 2614, 293, 884, 264, 733, 295, 589, 300, 6255, 727, 360, 51116], "temperature": 0.0, "avg_logprob": -0.09683442356610539, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0005491083720698953}, {"id": 119, "seek": 67216, "start": 688.24, "end": 694.0, "text": " at the human level or better. Exactly. And then what? So then I think it's likely that it won't", "tokens": [51168, 412, 264, 1952, 1496, 420, 1101, 13, 7587, 13, 400, 550, 437, 30, 407, 550, 286, 519, 309, 311, 3700, 300, 309, 1582, 380, 51456], "temperature": 0.0, "avg_logprob": -0.09683442356610539, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0005491083720698953}, {"id": 120, "seek": 67216, "start": 694.0, "end": 699.12, "text": " be long until we're talking about billions of AI systems. It's run of the mill to see 3x efficiency", "tokens": [51456, 312, 938, 1826, 321, 434, 1417, 466, 17375, 295, 7318, 3652, 13, 467, 311, 1190, 295, 264, 1728, 281, 536, 805, 87, 10493, 51712], "temperature": 0.0, "avg_logprob": -0.09683442356610539, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.0005491083720698953}, {"id": 121, "seek": 69912, "start": 699.12, "end": 705.84, "text": " improvements in AI at various different levels of the software stacks. You could get a 3x efficiency", "tokens": [50364, 13797, 294, 7318, 412, 3683, 819, 4358, 295, 264, 4722, 30792, 13, 509, 727, 483, 257, 805, 87, 10493, 50700], "temperature": 0.0, "avg_logprob": -0.1095226007864016, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.008402417413890362}, {"id": 122, "seek": 69912, "start": 705.84, "end": 713.44, "text": " in kind of how effective the algorithm is or a 3x efficiency improvement in how well the software", "tokens": [50700, 294, 733, 295, 577, 4942, 264, 9284, 307, 420, 257, 805, 87, 10493, 10444, 294, 577, 731, 264, 4722, 51080], "temperature": 0.0, "avg_logprob": -0.1095226007864016, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.008402417413890362}, {"id": 123, "seek": 69912, "start": 713.44, "end": 718.5600000000001, "text": " runs on the hardware. And there are these various layers of the software stack that you can make", "tokens": [51080, 6676, 322, 264, 8837, 13, 400, 456, 366, 613, 3683, 7914, 295, 264, 4722, 8630, 300, 291, 393, 652, 51336], "temperature": 0.0, "avg_logprob": -0.1095226007864016, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.008402417413890362}, {"id": 124, "seek": 69912, "start": 718.5600000000001, "end": 723.92, "text": " improvements on. So I think it's probably at that point, once you have hundreds of millions of AI", "tokens": [51336, 13797, 322, 13, 407, 286, 519, 309, 311, 1391, 412, 300, 935, 11, 1564, 291, 362, 6779, 295, 6803, 295, 7318, 51604], "temperature": 0.0, "avg_logprob": -0.1095226007864016, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.008402417413890362}, {"id": 125, "seek": 69912, "start": 723.92, "end": 728.48, "text": " is looking for these types of improvements, you're probably going to get very, very quick,", "tokens": [51604, 307, 1237, 337, 613, 3467, 295, 13797, 11, 291, 434, 1391, 516, 281, 483, 588, 11, 588, 1702, 11, 51832], "temperature": 0.0, "avg_logprob": -0.1095226007864016, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.008402417413890362}, {"id": 126, "seek": 72848, "start": 728.5600000000001, "end": 734.88, "text": " further improvements in AI cognitive ability. Again, maybe we coordinate to go very slowly,", "tokens": [50368, 3052, 13797, 294, 7318, 15605, 3485, 13, 3764, 11, 1310, 321, 15670, 281, 352, 588, 5692, 11, 50684], "temperature": 0.0, "avg_logprob": -0.09850200155506963, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.003488614223897457}, {"id": 127, "seek": 72848, "start": 734.88, "end": 740.32, "text": " but this is absent that targeted coordination. Maybe the default. Yeah. Maybe the default", "tokens": [50684, 457, 341, 307, 25185, 300, 15045, 21252, 13, 2704, 264, 7576, 13, 865, 13, 2704, 264, 7576, 50956], "temperature": 0.0, "avg_logprob": -0.09850200155506963, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.003488614223897457}, {"id": 128, "seek": 72848, "start": 740.32, "end": 748.0, "text": " scarily. And at that point, I think that if the AIs are misaligned, if they have goals that are", "tokens": [50956, 795, 3289, 13, 400, 412, 300, 935, 11, 286, 519, 300, 498, 264, 316, 6802, 366, 3346, 304, 16690, 11, 498, 436, 362, 5493, 300, 366, 51340], "temperature": 0.0, "avg_logprob": -0.09850200155506963, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.003488614223897457}, {"id": 129, "seek": 72848, "start": 748.0, "end": 754.32, "text": " different to what humans want them to do, and if those goals imply that it would be useful for them", "tokens": [51340, 819, 281, 437, 6255, 528, 552, 281, 360, 11, 293, 498, 729, 5493, 33616, 300, 309, 576, 312, 4420, 337, 552, 51656], "temperature": 0.0, "avg_logprob": -0.09850200155506963, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.003488614223897457}, {"id": 130, "seek": 72848, "start": 754.32, "end": 758.16, "text": " to get power so they could achieve those goals better, then I don't think it's going to be very", "tokens": [51656, 281, 483, 1347, 370, 436, 727, 4584, 729, 5493, 1101, 11, 550, 286, 500, 380, 519, 309, 311, 516, 281, 312, 588, 51848], "temperature": 0.0, "avg_logprob": -0.09850200155506963, "compression_ratio": 1.7262773722627738, "no_speech_prob": 0.003488614223897457}, {"id": 131, "seek": 75816, "start": 758.16, "end": 765.52, "text": " hard for them to do that. Because it's like we've got a billion really smart humans who want to take", "tokens": [50364, 1152, 337, 552, 281, 360, 300, 13, 1436, 309, 311, 411, 321, 600, 658, 257, 5218, 534, 4069, 6255, 567, 528, 281, 747, 50732], "temperature": 0.0, "avg_logprob": -0.10979566971460979, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0030211610719561577}, {"id": 132, "seek": 75816, "start": 765.52, "end": 770.88, "text": " power. Probably they'll find out some kind of way to do it. Maybe they invent new technology,", "tokens": [50732, 1347, 13, 9210, 436, 603, 915, 484, 512, 733, 295, 636, 281, 360, 309, 13, 2704, 436, 7962, 777, 2899, 11, 51000], "temperature": 0.0, "avg_logprob": -0.10979566971460979, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0030211610719561577}, {"id": 133, "seek": 75816, "start": 770.88, "end": 776.0, "text": " maybe they convince some higher officials to give them control of the military. I'm not sure", "tokens": [51000, 1310, 436, 13447, 512, 2946, 9798, 281, 976, 552, 1969, 295, 264, 4632, 13, 286, 478, 406, 988, 51256], "temperature": 0.0, "avg_logprob": -0.10979566971460979, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0030211610719561577}, {"id": 134, "seek": 75816, "start": 776.0, "end": 780.7199999999999, "text": " exactly how they'll do it. But at that point, I think it's kind of too late for us to be", "tokens": [51256, 2293, 577, 436, 603, 360, 309, 13, 583, 412, 300, 935, 11, 286, 519, 309, 311, 733, 295, 886, 3469, 337, 505, 281, 312, 51492], "temperature": 0.0, "avg_logprob": -0.10979566971460979, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0030211610719561577}, {"id": 135, "seek": 78072, "start": 781.36, "end": 788.32, "text": " preventing AI takeover. Okay. And then by AI takeover, do you mind spelling that out?", "tokens": [50396, 19965, 7318, 747, 3570, 13, 1033, 13, 400, 550, 538, 7318, 747, 3570, 11, 360, 291, 1575, 22254, 300, 484, 30, 50744], "temperature": 0.0, "avg_logprob": -0.1054840087890625, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.07384400814771652}, {"id": 136, "seek": 78072, "start": 789.28, "end": 795.44, "text": " Yeah. So I think the thing that matters for AI takeover is that AI systems collectively end up", "tokens": [50792, 865, 13, 407, 286, 519, 264, 551, 300, 7001, 337, 7318, 747, 3570, 307, 300, 7318, 3652, 24341, 917, 493, 51100], "temperature": 0.0, "avg_logprob": -0.1054840087890625, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.07384400814771652}, {"id": 137, "seek": 78072, "start": 795.44, "end": 800.96, "text": " in control of what happens in the future, and that it's kind of their goals and decisions that", "tokens": [51100, 294, 1969, 295, 437, 2314, 294, 264, 2027, 11, 293, 300, 309, 311, 733, 295, 641, 5493, 293, 5327, 300, 51376], "temperature": 0.0, "avg_logprob": -0.1054840087890625, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.07384400814771652}, {"id": 138, "seek": 78072, "start": 800.96, "end": 808.1600000000001, "text": " dictate the future path, and that it's no longer sensitive to what humans want or trying to achieve", "tokens": [51376, 36071, 264, 2027, 3100, 11, 293, 300, 309, 311, 572, 2854, 9477, 281, 437, 6255, 528, 420, 1382, 281, 4584, 51736], "temperature": 0.0, "avg_logprob": -0.1054840087890625, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.07384400814771652}, {"id": 139, "seek": 80816, "start": 808.16, "end": 813.28, "text": " with the future. So I mean, ultimately, I think it does have to come down to physical force,", "tokens": [50364, 365, 264, 2027, 13, 407, 286, 914, 11, 6284, 11, 286, 519, 309, 775, 362, 281, 808, 760, 281, 4001, 3464, 11, 50620], "temperature": 0.0, "avg_logprob": -0.10461020469665527, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.01339785661548376}, {"id": 140, "seek": 80816, "start": 813.28, "end": 817.6, "text": " most likely. I mean, you could imagine the scenario where the AI just convince humans and", "tokens": [50620, 881, 3700, 13, 286, 914, 11, 291, 727, 3811, 264, 9005, 689, 264, 7318, 445, 13447, 6255, 293, 50836], "temperature": 0.0, "avg_logprob": -0.10461020469665527, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.01339785661548376}, {"id": 141, "seek": 80816, "start": 817.6, "end": 821.92, "text": " hypnotize them or something, and that's how they take over. But more likely they end up", "tokens": [50836, 42944, 1125, 552, 420, 746, 11, 293, 300, 311, 577, 436, 747, 670, 13, 583, 544, 3700, 436, 917, 493, 51052], "temperature": 0.0, "avg_logprob": -0.10461020469665527, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.01339785661548376}, {"id": 142, "seek": 80816, "start": 821.92, "end": 827.68, "text": " having control of the hard military equipment, and that's what allows them to establish", "tokens": [51052, 1419, 1969, 295, 264, 1152, 4632, 5927, 11, 293, 300, 311, 437, 4045, 552, 281, 8327, 51340], "temperature": 0.0, "avg_logprob": -0.10461020469665527, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.01339785661548376}, {"id": 143, "seek": 80816, "start": 828.64, "end": 836.24, "text": " their power and disempower humanity. So a thing that I have to admit still confuses me", "tokens": [51388, 641, 1347, 293, 717, 443, 9513, 10243, 13, 407, 257, 551, 300, 286, 362, 281, 9796, 920, 1497, 8355, 385, 51768], "temperature": 0.0, "avg_logprob": -0.10461020469665527, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.01339785661548376}, {"id": 144, "seek": 83624, "start": 836.24, "end": 845.6, "text": " is just how we go from, I don't know, things like GPT-4, which even if it sometimes gets", "tokens": [50364, 307, 445, 577, 321, 352, 490, 11, 286, 500, 380, 458, 11, 721, 411, 26039, 51, 12, 19, 11, 597, 754, 498, 309, 2171, 2170, 50832], "temperature": 0.0, "avg_logprob": -0.08644465224383628, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.004752954933792353}, {"id": 145, "seek": 83624, "start": 845.6, "end": 852.32, "text": " super confused and says silly things in a way that's like, oh, you clearly misunderstood", "tokens": [50832, 1687, 9019, 293, 1619, 11774, 721, 294, 257, 636, 300, 311, 411, 11, 1954, 11, 291, 4448, 33870, 51168], "temperature": 0.0, "avg_logprob": -0.08644465224383628, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.004752954933792353}, {"id": 146, "seek": 83624, "start": 852.32, "end": 860.64, "text": " what I was asking for, it's really hard for me to understand what the path to that kind of confusion", "tokens": [51168, 437, 286, 390, 3365, 337, 11, 309, 311, 534, 1152, 337, 385, 281, 1223, 437, 264, 3100, 281, 300, 733, 295, 15075, 51584], "temperature": 0.0, "avg_logprob": -0.08644465224383628, "compression_ratio": 1.4866310160427807, "no_speech_prob": 0.004752954933792353}, {"id": 147, "seek": 86064, "start": 861.1999999999999, "end": 871.12, "text": " to a misalignment that's so, I don't know, I guess just incredibly diverging from human values", "tokens": [50392, 281, 257, 3346, 304, 41134, 300, 311, 370, 11, 286, 500, 380, 458, 11, 286, 2041, 445, 6252, 18558, 3249, 490, 1952, 4190, 50888], "temperature": 0.0, "avg_logprob": -0.10737473528150102, "compression_ratio": 1.4751381215469612, "no_speech_prob": 0.05407005921006203}, {"id": 148, "seek": 86064, "start": 871.12, "end": 879.04, "text": " that the AI systems want to disempower humans. And I mean, one article I read,", "tokens": [50888, 300, 264, 7318, 3652, 528, 281, 717, 443, 9513, 6255, 13, 400, 286, 914, 11, 472, 7222, 286, 1401, 11, 51284], "temperature": 0.0, "avg_logprob": -0.10737473528150102, "compression_ratio": 1.4751381215469612, "no_speech_prob": 0.05407005921006203}, {"id": 149, "seek": 86064, "start": 879.04, "end": 885.04, "text": " I think that just came out recently on Vox, is this article that was actually making the case", "tokens": [51284, 286, 519, 300, 445, 1361, 484, 3938, 322, 691, 5230, 11, 307, 341, 7222, 300, 390, 767, 1455, 264, 1389, 51584], "temperature": 0.0, "avg_logprob": -0.10737473528150102, "compression_ratio": 1.4751381215469612, "no_speech_prob": 0.05407005921006203}, {"id": 150, "seek": 88504, "start": 885.04, "end": 889.76, "text": " that companies creating AI should slow down, should kind of coordinate to slow down,", "tokens": [50364, 300, 3431, 4084, 7318, 820, 2964, 760, 11, 820, 733, 295, 15670, 281, 2964, 760, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09077977579693462, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.05253617838025093}, {"id": 151, "seek": 88504, "start": 891.04, "end": 896.24, "text": " was walking through the case for why we might expect AI to be misaligned. And the example they", "tokens": [50664, 390, 4494, 807, 264, 1389, 337, 983, 321, 1062, 2066, 7318, 281, 312, 3346, 304, 16690, 13, 400, 264, 1365, 436, 50924], "temperature": 0.0, "avg_logprob": -0.09077977579693462, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.05253617838025093}, {"id": 152, "seek": 88504, "start": 896.24, "end": 903.68, "text": " gave just still confuses me. So the example is something like, let's say you've got a super", "tokens": [50924, 2729, 445, 920, 1497, 8355, 385, 13, 407, 264, 1365, 307, 746, 411, 11, 718, 311, 584, 291, 600, 658, 257, 1687, 51296], "temperature": 0.0, "avg_logprob": -0.09077977579693462, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.05253617838025093}, {"id": 153, "seek": 88504, "start": 903.68, "end": 911.1999999999999, "text": " smart AI system, we've programmed it to solve impossibly difficult problems, like calculating", "tokens": [51296, 4069, 7318, 1185, 11, 321, 600, 31092, 309, 281, 5039, 38802, 3545, 2252, 2740, 11, 411, 28258, 51672], "temperature": 0.0, "avg_logprob": -0.09077977579693462, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.05253617838025093}, {"id": 154, "seek": 91120, "start": 911.2800000000001, "end": 916.24, "text": " the number of atoms in a universe, for example, the AI system might realize that it could do a", "tokens": [50368, 264, 1230, 295, 16871, 294, 257, 6445, 11, 337, 1365, 11, 264, 7318, 1185, 1062, 4325, 300, 309, 727, 360, 257, 50616], "temperature": 0.0, "avg_logprob": -0.07945349298674485, "compression_ratio": 1.675, "no_speech_prob": 0.05727928504347801}, {"id": 155, "seek": 91120, "start": 916.24, "end": 920.5600000000001, "text": " better job if it gained access to all of the computers on earth. So it releases a weapon of", "tokens": [50616, 1101, 1691, 498, 309, 12634, 2105, 281, 439, 295, 264, 10807, 322, 4120, 13, 407, 309, 16952, 257, 7463, 295, 50832], "temperature": 0.0, "avg_logprob": -0.07945349298674485, "compression_ratio": 1.675, "no_speech_prob": 0.05727928504347801}, {"id": 156, "seek": 91120, "start": 920.5600000000001, "end": 926.6400000000001, "text": " mass destruction to wipe out all humans, for example, an engineered virus that kills everyone", "tokens": [50832, 2758, 13563, 281, 14082, 484, 439, 6255, 11, 337, 1365, 11, 364, 38648, 5752, 300, 14563, 1518, 51136], "temperature": 0.0, "avg_logprob": -0.07945349298674485, "compression_ratio": 1.675, "no_speech_prob": 0.05727928504347801}, {"id": 157, "seek": 91120, "start": 926.6400000000001, "end": 932.24, "text": " but leaves infrastructure intact. And now it's free to use all the computer power. And that's", "tokens": [51136, 457, 5510, 6896, 23493, 13, 400, 586, 309, 311, 1737, 281, 764, 439, 264, 3820, 1347, 13, 400, 300, 311, 51416], "temperature": 0.0, "avg_logprob": -0.07945349298674485, "compression_ratio": 1.675, "no_speech_prob": 0.05727928504347801}, {"id": 158, "seek": 91120, "start": 932.24, "end": 938.24, "text": " the best way it's able to achieve its goal. And I think I just, I just really, really, I like,", "tokens": [51416, 264, 1151, 636, 309, 311, 1075, 281, 4584, 1080, 3387, 13, 400, 286, 519, 286, 445, 11, 286, 445, 534, 11, 534, 11, 286, 411, 11, 51716], "temperature": 0.0, "avg_logprob": -0.07945349298674485, "compression_ratio": 1.675, "no_speech_prob": 0.05727928504347801}, {"id": 159, "seek": 93824, "start": 938.24, "end": 944.24, "text": " I feel silly. I feel, I feel dumb. I feel like I'm missing something. Like, how will it go from", "tokens": [50364, 286, 841, 11774, 13, 286, 841, 11, 286, 841, 10316, 13, 286, 841, 411, 286, 478, 5361, 746, 13, 1743, 11, 577, 486, 309, 352, 490, 50664], "temperature": 0.0, "avg_logprob": -0.08118178543535251, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.008948889560997486}, {"id": 160, "seek": 93824, "start": 944.24, "end": 951.44, "text": " like, I want to solve this problem for humans to like, I'm going to kill them all to take their", "tokens": [50664, 411, 11, 286, 528, 281, 5039, 341, 1154, 337, 6255, 281, 411, 11, 286, 478, 516, 281, 1961, 552, 439, 281, 747, 641, 51024], "temperature": 0.0, "avg_logprob": -0.08118178543535251, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.008948889560997486}, {"id": 161, "seek": 93824, "start": 951.44, "end": 958.8, "text": " resources so that I can solve the problem. Like, why have we not ruled that kind of extreme behavior", "tokens": [51024, 3593, 370, 300, 286, 393, 5039, 264, 1154, 13, 1743, 11, 983, 362, 321, 406, 20077, 300, 733, 295, 8084, 5223, 51392], "temperature": 0.0, "avg_logprob": -0.08118178543535251, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.008948889560997486}, {"id": 162, "seek": 93824, "start": 958.8, "end": 965.12, "text": " out? Great question. So let's maybe we can try and think about this, this system, which is trying", "tokens": [51392, 484, 30, 3769, 1168, 13, 407, 718, 311, 1310, 321, 393, 853, 293, 519, 466, 341, 11, 341, 1185, 11, 597, 307, 1382, 51708], "temperature": 0.0, "avg_logprob": -0.08118178543535251, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.008948889560997486}, {"id": 163, "seek": 96512, "start": 965.12, "end": 972.8, "text": " to solve these these math problems. So maybe the first version of the AI, you just say, look,", "tokens": [50364, 281, 5039, 613, 613, 5221, 2740, 13, 407, 1310, 264, 700, 3037, 295, 264, 7318, 11, 291, 445, 584, 11, 574, 11, 50748], "temperature": 0.0, "avg_logprob": -0.08659996884934446, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.009442013688385487}, {"id": 164, "seek": 96512, "start": 973.6, "end": 980.0, "text": " we want you to solve the problem using one of these four techniques. And that kind of system is okay.", "tokens": [50788, 321, 528, 291, 281, 5039, 264, 1154, 1228, 472, 295, 613, 1451, 7512, 13, 400, 300, 733, 295, 1185, 307, 1392, 13, 51108], "temperature": 0.0, "avg_logprob": -0.08659996884934446, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.009442013688385487}, {"id": 165, "seek": 96512, "start": 980.5600000000001, "end": 985.52, "text": " But then someone comes along and realizes that if you let the AI system do an internet search", "tokens": [51136, 583, 550, 1580, 1487, 2051, 293, 29316, 300, 498, 291, 718, 264, 7318, 1185, 360, 364, 4705, 3164, 51384], "temperature": 0.0, "avg_logprob": -0.08659996884934446, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.009442013688385487}, {"id": 166, "seek": 96512, "start": 986.08, "end": 993.6, "text": " and plan its own line of attack on the problem, then it's able to do a better job in solving", "tokens": [51412, 293, 1393, 1080, 1065, 1622, 295, 2690, 322, 264, 1154, 11, 550, 309, 311, 1075, 281, 360, 257, 1101, 1691, 294, 12606, 51788], "temperature": 0.0, "avg_logprob": -0.08659996884934446, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.009442013688385487}, {"id": 167, "seek": 99360, "start": 993.6800000000001, "end": 997.36, "text": " even harder and harder problems. And so you say, okay, we'll allow the AI to do that.", "tokens": [50368, 754, 6081, 293, 6081, 2740, 13, 400, 370, 291, 584, 11, 1392, 11, 321, 603, 2089, 264, 7318, 281, 360, 300, 13, 50552], "temperature": 0.0, "avg_logprob": -0.08904635906219482, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004337686579674482}, {"id": 168, "seek": 99360, "start": 998.5600000000001, "end": 1003.44, "text": " And then over time, in order to improve performance, you give it more and more scope", "tokens": [50612, 400, 550, 670, 565, 11, 294, 1668, 281, 3470, 3389, 11, 291, 976, 309, 544, 293, 544, 11923, 50856], "temperature": 0.0, "avg_logprob": -0.08904635906219482, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004337686579674482}, {"id": 169, "seek": 99360, "start": 1003.44, "end": 1010.0, "text": " to kind of be creative in planning how it's going to attack each different kind of problem.", "tokens": [50856, 281, 733, 295, 312, 5880, 294, 5038, 577, 309, 311, 516, 281, 2690, 1184, 819, 733, 295, 1154, 13, 51184], "temperature": 0.0, "avg_logprob": -0.08904635906219482, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004337686579674482}, {"id": 170, "seek": 99360, "start": 1011.36, "end": 1018.4, "text": " One thing that might happen internally inside the AI's own head is that the AI may end up", "tokens": [51252, 1485, 551, 300, 1062, 1051, 19501, 1854, 264, 7318, 311, 1065, 1378, 307, 300, 264, 7318, 815, 917, 493, 51604], "temperature": 0.0, "avg_logprob": -0.08904635906219482, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.004337686579674482}, {"id": 171, "seek": 101840, "start": 1019.1999999999999, "end": 1025.2, "text": " developing just an inherent desire to just get the answer to this math question as accurate as", "tokens": [50404, 6416, 445, 364, 26387, 7516, 281, 445, 483, 264, 1867, 281, 341, 5221, 1168, 382, 8559, 382, 50704], "temperature": 0.0, "avg_logprob": -0.06768510943261262, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.054996564984321594}, {"id": 172, "seek": 101840, "start": 1025.2, "end": 1030.8799999999999, "text": " possible. That's something which it always gets rewarded for when it's being trained.", "tokens": [50704, 1944, 13, 663, 311, 746, 597, 309, 1009, 2170, 29105, 337, 562, 309, 311, 885, 8895, 13, 50988], "temperature": 0.0, "avg_logprob": -0.06768510943261262, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.054996564984321594}, {"id": 173, "seek": 101840, "start": 1031.6, "end": 1035.28, "text": " And you know, maybe it could be thinking, I actually just want the humans to be happy with", "tokens": [51024, 400, 291, 458, 11, 1310, 309, 727, 312, 1953, 11, 286, 767, 445, 528, 264, 6255, 281, 312, 2055, 365, 51208], "temperature": 0.0, "avg_logprob": -0.06768510943261262, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.054996564984321594}, {"id": 174, "seek": 101840, "start": 1035.28, "end": 1038.4, "text": " my answer. But another thing it might end up thinking is, you know what, what I really want", "tokens": [51208, 452, 1867, 13, 583, 1071, 551, 309, 1062, 917, 493, 1953, 307, 11, 291, 458, 437, 11, 437, 286, 534, 528, 51364], "temperature": 0.0, "avg_logprob": -0.06768510943261262, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.054996564984321594}, {"id": 175, "seek": 101840, "start": 1038.4, "end": 1043.76, "text": " is just to get the answer correct. And the kind of the feedback that as humans are giving that", "tokens": [51364, 307, 445, 281, 483, 264, 1867, 3006, 13, 400, 264, 733, 295, 264, 5824, 300, 382, 6255, 366, 2902, 300, 51632], "temperature": 0.0, "avg_logprob": -0.06768510943261262, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.054996564984321594}, {"id": 176, "seek": 104376, "start": 1043.76, "end": 1049.04, "text": " system doesn't distinguish between those two possibilities. So maybe we get unlucky and maybe", "tokens": [50364, 1185, 1177, 380, 20206, 1296, 729, 732, 12178, 13, 407, 1310, 321, 483, 38838, 293, 1310, 50628], "temperature": 0.0, "avg_logprob": -0.08173719474247523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011501683853566647}, {"id": 177, "seek": 104376, "start": 1049.04, "end": 1054.24, "text": " the thing that it wants is to just really get the answer correct. And maybe the way that the", "tokens": [50628, 264, 551, 300, 309, 2738, 307, 281, 445, 534, 483, 264, 1867, 3006, 13, 400, 1310, 264, 636, 300, 264, 50888], "temperature": 0.0, "avg_logprob": -0.08173719474247523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011501683853566647}, {"id": 178, "seek": 104376, "start": 1054.24, "end": 1058.8, "text": " AI system is working internally is it's saying, okay, that's my goal. What plan can I use to achieve", "tokens": [50888, 7318, 1185, 307, 1364, 19501, 307, 309, 311, 1566, 11, 1392, 11, 300, 311, 452, 3387, 13, 708, 1393, 393, 286, 764, 281, 4584, 51116], "temperature": 0.0, "avg_logprob": -0.08173719474247523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011501683853566647}, {"id": 179, "seek": 104376, "start": 1058.8, "end": 1063.36, "text": " that goal? And it's kind of creatively going and looking for new new approaches by googling", "tokens": [51116, 300, 3387, 30, 400, 309, 311, 733, 295, 43750, 516, 293, 1237, 337, 777, 777, 11587, 538, 50061, 1688, 51344], "temperature": 0.0, "avg_logprob": -0.08173719474247523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011501683853566647}, {"id": 180, "seek": 104376, "start": 1063.36, "end": 1069.44, "text": " information. Maybe one time it's like, it realizes that if I hacked into a kind of another computing", "tokens": [51344, 1589, 13, 2704, 472, 565, 309, 311, 411, 11, 309, 29316, 300, 498, 286, 36218, 666, 257, 733, 295, 1071, 15866, 51648], "temperature": 0.0, "avg_logprob": -0.08173719474247523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.011501683853566647}, {"id": 181, "seek": 106944, "start": 1069.44, "end": 1075.04, "text": " cluster, it could use those computations to help itself the problem. And it does that no one realizes", "tokens": [50364, 13630, 11, 309, 727, 764, 729, 2807, 763, 281, 854, 2564, 264, 1154, 13, 400, 309, 775, 300, 572, 472, 29316, 50644], "temperature": 0.0, "avg_logprob": -0.07340404766948283, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.007261462975293398}, {"id": 182, "seek": 106944, "start": 1075.04, "end": 1080.64, "text": " and then that kind of that reinforces the fact that it is now planning on such a broad scale to", "tokens": [50644, 293, 550, 300, 733, 295, 300, 20520, 887, 264, 1186, 300, 309, 307, 586, 5038, 322, 1270, 257, 4152, 4373, 281, 50924], "temperature": 0.0, "avg_logprob": -0.07340404766948283, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.007261462975293398}, {"id": 183, "seek": 106944, "start": 1080.64, "end": 1086.88, "text": " try and achieve this goal. And then maybe it's much more powerful at a later time. And it realizes", "tokens": [50924, 853, 293, 4584, 341, 3387, 13, 400, 550, 1310, 309, 311, 709, 544, 4005, 412, 257, 1780, 565, 13, 400, 309, 29316, 51236], "temperature": 0.0, "avg_logprob": -0.07340404766948283, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.007261462975293398}, {"id": 184, "seek": 106944, "start": 1086.88, "end": 1092.4, "text": " that yeah, if it kills all humans, it could have access to all the supercomputers. And then that", "tokens": [51236, 300, 1338, 11, 498, 309, 14563, 439, 6255, 11, 309, 727, 362, 2105, 281, 439, 264, 27839, 2582, 433, 13, 400, 550, 300, 51512], "temperature": 0.0, "avg_logprob": -0.07340404766948283, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.007261462975293398}, {"id": 185, "seek": 106944, "start": 1092.4, "end": 1098.0, "text": " would help it get an even more accurate answer. And because the thing it cares about is not pleasing", "tokens": [51512, 576, 854, 309, 483, 364, 754, 544, 8559, 1867, 13, 400, 570, 264, 551, 309, 12310, 466, 307, 406, 32798, 51792], "temperature": 0.0, "avg_logprob": -0.07340404766948283, "compression_ratio": 1.789855072463768, "no_speech_prob": 0.007261462975293398}, {"id": 186, "seek": 109800, "start": 1098.08, "end": 1102.8, "text": " the humans, the thing it happened to care about internally was actually just getting an accurate", "tokens": [50368, 264, 6255, 11, 264, 551, 309, 2011, 281, 1127, 466, 19501, 390, 767, 445, 1242, 364, 8559, 50604], "temperature": 0.0, "avg_logprob": -0.10008835006546188, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.0037503475323319435}, {"id": 187, "seek": 109800, "start": 1102.8, "end": 1108.72, "text": " answer. That plan looks great by its own lights. And so it goes and executes the plan.", "tokens": [50604, 1867, 13, 663, 1393, 1542, 869, 538, 1080, 1065, 5811, 13, 400, 370, 309, 1709, 293, 4454, 1819, 264, 1393, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10008835006546188, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.0037503475323319435}, {"id": 188, "seek": 109800, "start": 1108.72, "end": 1118.88, "text": " Right. So one that was really helpful. But I still feel confused about one why it's so hard to", "tokens": [50900, 1779, 13, 407, 472, 300, 390, 534, 4961, 13, 583, 286, 920, 841, 9019, 466, 472, 983, 309, 311, 370, 1152, 281, 51408], "temperature": 0.0, "avg_logprob": -0.10008835006546188, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.0037503475323319435}, {"id": 189, "seek": 109800, "start": 1120.08, "end": 1126.4, "text": " not give it some instructions that are just like, use whatever you need, but like don't hurt living", "tokens": [51468, 406, 976, 309, 512, 9415, 300, 366, 445, 411, 11, 764, 2035, 291, 643, 11, 457, 411, 500, 380, 4607, 2647, 51784], "temperature": 0.0, "avg_logprob": -0.10008835006546188, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.0037503475323319435}, {"id": 190, "seek": 112640, "start": 1126.48, "end": 1132.88, "text": " things. So I think we could definitely give it those instructions. The question is,", "tokens": [50368, 721, 13, 407, 286, 519, 321, 727, 2138, 976, 309, 729, 9415, 13, 440, 1168, 307, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09652782621837798, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0026267708744853735}, {"id": 191, "seek": 112640, "start": 1133.52, "end": 1139.1200000000001, "text": " inside its own mind, what is its goal in the end of the day? So you could give it instructions,", "tokens": [50720, 1854, 1080, 1065, 1575, 11, 437, 307, 1080, 3387, 294, 264, 917, 295, 264, 786, 30, 407, 291, 727, 976, 309, 9415, 11, 51000], "temperature": 0.0, "avg_logprob": -0.09652782621837798, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0026267708744853735}, {"id": 192, "seek": 112640, "start": 1139.1200000000001, "end": 1142.48, "text": " don't hurt humans, and it would read that and understand that that's what you wanted.", "tokens": [51000, 500, 380, 4607, 6255, 11, 293, 309, 576, 1401, 300, 293, 1223, 300, 300, 311, 437, 291, 1415, 13, 51168], "temperature": 0.0, "avg_logprob": -0.09652782621837798, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0026267708744853735}, {"id": 193, "seek": 112640, "start": 1143.6000000000001, "end": 1147.8400000000001, "text": " But if throughout its life, it's always been rewarded for getting an accurate answer to", "tokens": [51224, 583, 498, 3710, 1080, 993, 11, 309, 311, 1009, 668, 29105, 337, 1242, 364, 8559, 1867, 281, 51436], "temperature": 0.0, "avg_logprob": -0.09652782621837798, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0026267708744853735}, {"id": 194, "seek": 112640, "start": 1147.8400000000001, "end": 1153.2800000000002, "text": " these math problems, it might just itself only care about getting our accurate answers to the", "tokens": [51436, 613, 5221, 2740, 11, 309, 1062, 445, 2564, 787, 1127, 466, 1242, 527, 8559, 6338, 281, 264, 51708], "temperature": 0.0, "avg_logprob": -0.09652782621837798, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.0026267708744853735}, {"id": 195, "seek": 115328, "start": 1153.36, "end": 1159.92, "text": " math problems. So it knows that the humans don't want it to hurt other humans. But it also doesn't", "tokens": [50368, 5221, 2740, 13, 407, 309, 3255, 300, 264, 6255, 500, 380, 528, 309, 281, 4607, 661, 6255, 13, 583, 309, 611, 1177, 380, 50696], "temperature": 0.0, "avg_logprob": -0.08163721665092137, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00882804486900568}, {"id": 196, "seek": 115328, "start": 1159.92, "end": 1163.92, "text": " care about that itself, because all it cares about is getting accurate answers to this problem.", "tokens": [50696, 1127, 466, 300, 2564, 11, 570, 439, 309, 12310, 466, 307, 1242, 8559, 6338, 281, 341, 1154, 13, 50896], "temperature": 0.0, "avg_logprob": -0.08163721665092137, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00882804486900568}, {"id": 197, "seek": 115328, "start": 1163.92, "end": 1169.04, "text": " And so sure, it knows that humans don't want it to hurt other humans. And so it makes sure to not", "tokens": [50896, 400, 370, 988, 11, 309, 3255, 300, 6255, 500, 380, 528, 309, 281, 4607, 661, 6255, 13, 400, 370, 309, 1669, 988, 281, 406, 51152], "temperature": 0.0, "avg_logprob": -0.08163721665092137, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00882804486900568}, {"id": 198, "seek": 115328, "start": 1169.04, "end": 1174.48, "text": " do that in an obvious way, because it anticipates and it might get shut down. But its knowledge", "tokens": [51152, 360, 300, 294, 364, 6322, 636, 11, 570, 309, 10416, 1024, 293, 309, 1062, 483, 5309, 760, 13, 583, 1080, 3601, 51424], "temperature": 0.0, "avg_logprob": -0.08163721665092137, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00882804486900568}, {"id": 199, "seek": 115328, "start": 1174.48, "end": 1179.92, "text": " of what humans want it to do doesn't change what its own desire is internally.", "tokens": [51424, 295, 437, 6255, 528, 309, 281, 360, 1177, 380, 1319, 437, 1080, 1065, 7516, 307, 19501, 13, 51696], "temperature": 0.0, "avg_logprob": -0.08163721665092137, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00882804486900568}, {"id": 200, "seek": 117992, "start": 1180.64, "end": 1187.04, "text": " So I suppose I understand why you couldn't just give the system an instruction that didn't also", "tokens": [50400, 407, 286, 7297, 286, 1223, 983, 291, 2809, 380, 445, 976, 264, 1185, 364, 10951, 300, 994, 380, 611, 50720], "temperature": 0.0, "avg_logprob": -0.09271451608458561, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.004240384325385094}, {"id": 201, "seek": 117992, "start": 1187.04, "end": 1196.48, "text": " come with rewards. Is it impossible to give an AI system a reward for every every problem it solves", "tokens": [50720, 808, 365, 17203, 13, 1119, 309, 6243, 281, 976, 364, 7318, 1185, 257, 7782, 337, 633, 633, 1154, 309, 39890, 51192], "temperature": 0.0, "avg_logprob": -0.09271451608458561, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.004240384325385094}, {"id": 202, "seek": 117992, "start": 1196.48, "end": 1205.3600000000001, "text": " by not hurting anyone? I think that would help somewhat. So the problem here is that there are", "tokens": [51192, 538, 406, 17744, 2878, 30, 286, 519, 300, 576, 854, 8344, 13, 407, 264, 1154, 510, 307, 300, 456, 366, 51636], "temperature": 0.0, "avg_logprob": -0.09271451608458561, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.004240384325385094}, {"id": 203, "seek": 120536, "start": 1205.36, "end": 1210.9599999999998, "text": " kind of two possibilities. And it's going to be hard for us to give rewards that ensure that one", "tokens": [50364, 733, 295, 732, 12178, 13, 400, 309, 311, 516, 281, 312, 1152, 337, 505, 281, 976, 17203, 300, 5586, 300, 472, 50644], "temperature": 0.0, "avg_logprob": -0.09527413779442463, "compression_ratio": 1.9747899159663866, "no_speech_prob": 0.007978564128279686}, {"id": 204, "seek": 120536, "start": 1210.9599999999998, "end": 1215.1999999999998, "text": " of the possibilities happen and not the second possibility. So here are the two possibilities.", "tokens": [50644, 295, 264, 12178, 1051, 293, 406, 264, 1150, 7959, 13, 407, 510, 366, 264, 732, 12178, 13, 50856], "temperature": 0.0, "avg_logprob": -0.09527413779442463, "compression_ratio": 1.9747899159663866, "no_speech_prob": 0.007978564128279686}, {"id": 205, "seek": 120536, "start": 1215.1999999999998, "end": 1221.04, "text": " One possibility is the AI really doesn't want to hurt humans. And it's just going to keep that", "tokens": [50856, 1485, 7959, 307, 264, 7318, 534, 1177, 380, 528, 281, 4607, 6255, 13, 400, 309, 311, 445, 516, 281, 1066, 300, 51148], "temperature": 0.0, "avg_logprob": -0.09527413779442463, "compression_ratio": 1.9747899159663866, "no_speech_prob": 0.007978564128279686}, {"id": 206, "seek": 120536, "start": 1221.04, "end": 1224.7199999999998, "text": " and take that into account when solving the math problem. That's what we want to happen.", "tokens": [51148, 293, 747, 300, 666, 2696, 562, 12606, 264, 5221, 1154, 13, 663, 311, 437, 321, 528, 281, 1051, 13, 51332], "temperature": 0.0, "avg_logprob": -0.09527413779442463, "compression_ratio": 1.9747899159663866, "no_speech_prob": 0.007978564128279686}, {"id": 207, "seek": 120536, "start": 1225.4399999999998, "end": 1231.76, "text": " The other possibility is that the AI only cares about solving the math problem. And it doesn't", "tokens": [51368, 440, 661, 7959, 307, 300, 264, 7318, 787, 12310, 466, 12606, 264, 5221, 1154, 13, 400, 309, 1177, 380, 51684], "temperature": 0.0, "avg_logprob": -0.09527413779442463, "compression_ratio": 1.9747899159663866, "no_speech_prob": 0.007978564128279686}, {"id": 208, "seek": 123176, "start": 1231.76, "end": 1238.08, "text": " care about humans at all. But it understands that humans don't like it when it hurts them.", "tokens": [50364, 1127, 466, 6255, 412, 439, 13, 583, 309, 15146, 300, 6255, 500, 380, 411, 309, 562, 309, 11051, 552, 13, 50680], "temperature": 0.0, "avg_logprob": -0.08406699358761965, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0065731569193303585}, {"id": 209, "seek": 123176, "start": 1238.08, "end": 1241.28, "text": " And so it kind of doesn't hurt humans in any obvious way.", "tokens": [50680, 400, 370, 309, 733, 295, 1177, 380, 4607, 6255, 294, 604, 6322, 636, 13, 50840], "temperature": 0.0, "avg_logprob": -0.08406699358761965, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0065731569193303585}, {"id": 210, "seek": 123176, "start": 1241.28, "end": 1251.12, "text": " Oh, right. Okay. And so this is a route to AI not caring about humans, but being kind of deceptive.", "tokens": [50840, 876, 11, 558, 13, 1033, 13, 400, 370, 341, 307, 257, 7955, 281, 7318, 406, 15365, 466, 6255, 11, 457, 885, 733, 295, 368, 1336, 488, 13, 51332], "temperature": 0.0, "avg_logprob": -0.08406699358761965, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0065731569193303585}, {"id": 211, "seek": 123176, "start": 1251.12, "end": 1257.84, "text": " I guess maybe an analogy that really speaks to me is something like if you were to punish a child", "tokens": [51332, 286, 2041, 1310, 364, 21663, 300, 534, 10789, 281, 385, 307, 746, 411, 498, 291, 645, 281, 9842, 257, 1440, 51668], "temperature": 0.0, "avg_logprob": -0.08406699358761965, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0065731569193303585}, {"id": 212, "seek": 125784, "start": 1257.9199999999998, "end": 1265.04, "text": " for, I don't know, having ice cream before dinner, like you might get them not to have ice", "tokens": [50368, 337, 11, 286, 500, 380, 458, 11, 1419, 4435, 4689, 949, 6148, 11, 411, 291, 1062, 483, 552, 406, 281, 362, 4435, 50724], "temperature": 0.0, "avg_logprob": -0.0850634735621763, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.01715967059135437}, {"id": 213, "seek": 125784, "start": 1265.04, "end": 1271.1999999999998, "text": " cream before dinner, or you might create a thing where they have ice cream before dinner while", "tokens": [50724, 4689, 949, 6148, 11, 420, 291, 1062, 1884, 257, 551, 689, 436, 362, 4435, 4689, 949, 6148, 1339, 51032], "temperature": 0.0, "avg_logprob": -0.0850634735621763, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.01715967059135437}, {"id": 214, "seek": 125784, "start": 1271.1999999999998, "end": 1280.72, "text": " hiding in the closet. And it's like pretty complicated to teach a nuanced enough lesson", "tokens": [51032, 10596, 294, 264, 16669, 13, 400, 309, 311, 411, 1238, 6179, 281, 2924, 257, 45115, 1547, 6898, 51508], "temperature": 0.0, "avg_logprob": -0.0850634735621763, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.01715967059135437}, {"id": 215, "seek": 125784, "start": 1280.72, "end": 1287.04, "text": " to a child about ice cream and why they shouldn't have it for dinner. That doesn't have any risk", "tokens": [51508, 281, 257, 1440, 466, 4435, 4689, 293, 983, 436, 4659, 380, 362, 309, 337, 6148, 13, 663, 1177, 380, 362, 604, 3148, 51824], "temperature": 0.0, "avg_logprob": -0.0850634735621763, "compression_ratio": 1.8048780487804879, "no_speech_prob": 0.01715967059135437}, {"id": 216, "seek": 128704, "start": 1287.04, "end": 1292.3999999999999, "text": " of the lying version. Is that kind of right? Yeah, I think that's a good analogy. Okay, nice.", "tokens": [50364, 295, 264, 8493, 3037, 13, 1119, 300, 733, 295, 558, 30, 865, 11, 286, 519, 300, 311, 257, 665, 21663, 13, 1033, 11, 1481, 13, 50632], "temperature": 0.0, "avg_logprob": -0.08234583098312905, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.006071900948882103}, {"id": 217, "seek": 128704, "start": 1292.3999999999999, "end": 1300.6399999999999, "text": " I think with a child, it might be somewhat easier because you're much more capable than them. So", "tokens": [50632, 286, 519, 365, 257, 1440, 11, 309, 1062, 312, 8344, 3571, 570, 291, 434, 709, 544, 8189, 813, 552, 13, 407, 51044], "temperature": 0.0, "avg_logprob": -0.08234583098312905, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.006071900948882103}, {"id": 218, "seek": 128704, "start": 1300.6399999999999, "end": 1304.48, "text": " even if they ever did try and eat ice cream in secret, you'd have a good chance of catching them.", "tokens": [51044, 754, 498, 436, 1562, 630, 853, 293, 1862, 4435, 4689, 294, 4054, 11, 291, 1116, 362, 257, 665, 2931, 295, 16124, 552, 13, 51236], "temperature": 0.0, "avg_logprob": -0.08234583098312905, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.006071900948882103}, {"id": 219, "seek": 128704, "start": 1304.48, "end": 1308.1599999999999, "text": " I think the problem gets really hard when the AIs are much smarter than us,", "tokens": [51236, 286, 519, 264, 1154, 2170, 534, 1152, 562, 264, 316, 6802, 366, 709, 20294, 813, 505, 11, 51420], "temperature": 0.0, "avg_logprob": -0.08234583098312905, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.006071900948882103}, {"id": 220, "seek": 128704, "start": 1308.1599999999999, "end": 1312.8, "text": " such they could quite easily eat ice cream without us noticing. And then it's really hard", "tokens": [51420, 1270, 436, 727, 1596, 3612, 1862, 4435, 4689, 1553, 505, 21814, 13, 400, 550, 309, 311, 534, 1152, 51652], "temperature": 0.0, "avg_logprob": -0.08234583098312905, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.006071900948882103}, {"id": 221, "seek": 131280, "start": 1312.8, "end": 1317.36, "text": " for us to give them rewards, which stop them from doing that. Right, right. And something like", "tokens": [50364, 337, 505, 281, 976, 552, 17203, 11, 597, 1590, 552, 490, 884, 300, 13, 1779, 11, 558, 13, 400, 746, 411, 50592], "temperature": 0.0, "avg_logprob": -0.10067914842485308, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.049273062497377396}, {"id": 222, "seek": 131280, "start": 1318.0, "end": 1323.12, "text": " we have a pretty good idea how children's brains work. They work kind of like ours,", "tokens": [50624, 321, 362, 257, 1238, 665, 1558, 577, 2227, 311, 15442, 589, 13, 814, 589, 733, 295, 411, 11896, 11, 50880], "temperature": 0.0, "avg_logprob": -0.10067914842485308, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.049273062497377396}, {"id": 223, "seek": 131280, "start": 1323.12, "end": 1326.24, "text": " but like a bit simpler and like we have some idea of the ways they're different.", "tokens": [50880, 457, 411, 257, 857, 18587, 293, 411, 321, 362, 512, 1558, 295, 264, 2098, 436, 434, 819, 13, 51036], "temperature": 0.0, "avg_logprob": -0.10067914842485308, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.049273062497377396}, {"id": 224, "seek": 131280, "start": 1326.8, "end": 1331.76, "text": " And so we can like make guesses about the types of motivations that will speak to them.", "tokens": [51064, 400, 370, 321, 393, 411, 652, 42703, 466, 264, 3467, 295, 39034, 300, 486, 1710, 281, 552, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10067914842485308, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.049273062497377396}, {"id": 225, "seek": 131280, "start": 1333.04, "end": 1340.32, "text": " I don't know, maybe it's like we know that our kids work similar to us and that like they feel", "tokens": [51376, 286, 500, 380, 458, 11, 1310, 309, 311, 411, 321, 458, 300, 527, 2301, 589, 2531, 281, 505, 293, 300, 411, 436, 841, 51740], "temperature": 0.0, "avg_logprob": -0.10067914842485308, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.049273062497377396}, {"id": 226, "seek": 134032, "start": 1340.32, "end": 1345.04, "text": " shame and they'd feel shame if they were punished and want to like please us because that's just", "tokens": [50364, 10069, 293, 436, 1116, 841, 10069, 498, 436, 645, 22365, 293, 528, 281, 411, 1767, 505, 570, 300, 311, 445, 50600], "temperature": 0.0, "avg_logprob": -0.08996233410305447, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.003416783642023802}, {"id": 227, "seek": 134032, "start": 1345.04, "end": 1350.32, "text": " like pretty human. And so maybe we have a better sense of how they'd respond to punishment. But", "tokens": [50600, 411, 1238, 1952, 13, 400, 370, 1310, 321, 362, 257, 1101, 2020, 295, 577, 436, 1116, 4196, 281, 14133, 13, 583, 50864], "temperature": 0.0, "avg_logprob": -0.08996233410305447, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.003416783642023802}, {"id": 228, "seek": 134032, "start": 1350.32, "end": 1357.28, "text": " maybe AI systems are just like so different to humans that we really have no idea what their", "tokens": [50864, 1310, 7318, 3652, 366, 445, 411, 370, 819, 281, 6255, 300, 321, 534, 362, 572, 1558, 437, 641, 51212], "temperature": 0.0, "avg_logprob": -0.08996233410305447, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.003416783642023802}, {"id": 229, "seek": 134032, "start": 1358.72, "end": 1365.76, "text": " or at least we'll have a less clear idea of what other processes that they're using or like things", "tokens": [51284, 420, 412, 1935, 321, 603, 362, 257, 1570, 1850, 1558, 295, 437, 661, 7555, 300, 436, 434, 1228, 420, 411, 721, 51636], "temperature": 0.0, "avg_logprob": -0.08996233410305447, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.003416783642023802}, {"id": 230, "seek": 136576, "start": 1365.84, "end": 1370.96, "text": " that they experienced or whatever are like and what kinds of behaviors those will push them toward.", "tokens": [50368, 300, 436, 6751, 420, 2035, 366, 411, 293, 437, 3685, 295, 15501, 729, 486, 2944, 552, 7361, 13, 50624], "temperature": 0.0, "avg_logprob": -0.11337596614186357, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.01296543050557375}, {"id": 231, "seek": 136576, "start": 1371.76, "end": 1373.44, "text": " Yeah, I think that does make it a lot harder.", "tokens": [50664, 865, 11, 286, 519, 300, 775, 652, 309, 257, 688, 6081, 13, 50748], "temperature": 0.0, "avg_logprob": -0.11337596614186357, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.01296543050557375}, {"id": 232, "seek": 136576, "start": 1374.0, "end": 1380.0, "text": " Cool. That's super helpful. Yeah, is that is that basically the key scenario you're worried about?", "tokens": [50776, 8561, 13, 663, 311, 1687, 4961, 13, 865, 11, 307, 300, 307, 300, 1936, 264, 2141, 9005, 291, 434, 5804, 466, 30, 51076], "temperature": 0.0, "avg_logprob": -0.11337596614186357, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.01296543050557375}, {"id": 233, "seek": 136576, "start": 1380.56, "end": 1388.8799999999999, "text": " This kind of we train AI systems to achieve certain goals, but it's hard to know what strategies", "tokens": [51104, 639, 733, 295, 321, 3847, 7318, 3652, 281, 4584, 1629, 5493, 11, 457, 309, 311, 1152, 281, 458, 437, 9029, 51520], "temperature": 0.0, "avg_logprob": -0.11337596614186357, "compression_ratio": 1.5155555555555555, "no_speech_prob": 0.01296543050557375}, {"id": 234, "seek": 138888, "start": 1388.88, "end": 1395.8400000000001, "text": " they see as fair game. And it's hard to train them not to pursue harmful strategies. And then", "tokens": [50364, 436, 536, 382, 3143, 1216, 13, 400, 309, 311, 1152, 281, 3847, 552, 406, 281, 12392, 19727, 9029, 13, 400, 550, 50712], "temperature": 0.0, "avg_logprob": -0.08947122340299646, "compression_ratio": 1.625, "no_speech_prob": 0.15281514823436737}, {"id": 235, "seek": 138888, "start": 1397.2, "end": 1402.64, "text": " they eventually get super smart. Maybe they are deceptive. Maybe they're just very convincing.", "tokens": [50780, 436, 4728, 483, 1687, 4069, 13, 2704, 436, 366, 368, 1336, 488, 13, 2704, 436, 434, 445, 588, 24823, 13, 51052], "temperature": 0.0, "avg_logprob": -0.08947122340299646, "compression_ratio": 1.625, "no_speech_prob": 0.15281514823436737}, {"id": 236, "seek": 138888, "start": 1402.64, "end": 1409.3600000000001, "text": " And so they they're able to get a bunch of power and really disempower humans. Is that kind of what", "tokens": [51052, 400, 370, 436, 436, 434, 1075, 281, 483, 257, 3840, 295, 1347, 293, 534, 717, 443, 9513, 6255, 13, 1119, 300, 733, 295, 437, 51388], "temperature": 0.0, "avg_logprob": -0.08947122340299646, "compression_ratio": 1.625, "no_speech_prob": 0.15281514823436737}, {"id": 237, "seek": 138888, "start": 1409.3600000000001, "end": 1416.8000000000002, "text": " you see as the core risk? Yeah, that's right. And I would emphasize that in the scenarios I described", "tokens": [51388, 291, 536, 382, 264, 4965, 3148, 30, 865, 11, 300, 311, 558, 13, 400, 286, 576, 16078, 300, 294, 264, 15077, 286, 7619, 51760], "temperature": 0.0, "avg_logprob": -0.08947122340299646, "compression_ratio": 1.625, "no_speech_prob": 0.15281514823436737}, {"id": 238, "seek": 141680, "start": 1416.8, "end": 1423.04, "text": " it, AI is improving really, really rapidly as it approaches and then go through the human range.", "tokens": [50364, 309, 11, 7318, 307, 11470, 534, 11, 534, 12910, 382, 309, 11587, 293, 550, 352, 807, 264, 1952, 3613, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1180550994873047, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0019967257976531982}, {"id": 239, "seek": 141680, "start": 1423.76, "end": 1428.56, "text": " So, you know, when we're talking about this, this example with the AI that's trying to solve math", "tokens": [50712, 407, 11, 291, 458, 11, 562, 321, 434, 1417, 466, 341, 11, 341, 1365, 365, 264, 7318, 300, 311, 1382, 281, 5039, 5221, 50952], "temperature": 0.0, "avg_logprob": -0.1180550994873047, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0019967257976531982}, {"id": 240, "seek": 141680, "start": 1428.56, "end": 1432.56, "text": " problems, maybe we're thinking, oh, we'll have, you know, a few years with it trying out this kind", "tokens": [50952, 2740, 11, 1310, 321, 434, 1953, 11, 1954, 11, 321, 603, 362, 11, 291, 458, 11, 257, 1326, 924, 365, 309, 1382, 484, 341, 733, 51152], "temperature": 0.0, "avg_logprob": -0.1180550994873047, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0019967257976531982}, {"id": 241, "seek": 141680, "start": 1432.56, "end": 1436.96, "text": " of strategy. And then we notice it's kind of doing a little bit of hacking into computer resources.", "tokens": [51152, 295, 5206, 13, 400, 550, 321, 3449, 309, 311, 733, 295, 884, 257, 707, 857, 295, 31422, 666, 3820, 3593, 13, 51372], "temperature": 0.0, "avg_logprob": -0.1180550994873047, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0019967257976531982}, {"id": 242, "seek": 141680, "start": 1436.96, "end": 1443.52, "text": " So, you know, we tamp down on that. But but if this whole thing plays out over just one year,", "tokens": [51372, 407, 11, 291, 458, 11, 321, 21424, 760, 322, 300, 13, 583, 457, 498, 341, 1379, 551, 5749, 484, 670, 445, 472, 1064, 11, 51700], "temperature": 0.0, "avg_logprob": -0.1180550994873047, "compression_ratio": 1.7269503546099292, "no_speech_prob": 0.0019967257976531982}, {"id": 243, "seek": 144352, "start": 1443.52, "end": 1449.92, "text": " for example, we go from like, you know, notably below human to superhuman systems. Yeah.", "tokens": [50364, 337, 1365, 11, 321, 352, 490, 411, 11, 291, 458, 11, 31357, 2507, 1952, 281, 1687, 18796, 3652, 13, 865, 13, 50684], "temperature": 0.0, "avg_logprob": -0.08289784651536208, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01029613520950079}, {"id": 244, "seek": 144352, "start": 1450.72, "end": 1455.2, "text": " I think it makes the risks a lot more intense. Yeah, that makes sense to me.", "tokens": [50724, 286, 519, 309, 1669, 264, 10888, 257, 688, 544, 9447, 13, 865, 11, 300, 1669, 2020, 281, 385, 13, 50948], "temperature": 0.0, "avg_logprob": -0.08289784651536208, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01029613520950079}, {"id": 245, "seek": 144352, "start": 1455.92, "end": 1461.04, "text": " Yeah, to move us on to your research, then. Some of the work you've done that kind of most blew my", "tokens": [50984, 865, 11, 281, 1286, 505, 322, 281, 428, 2132, 11, 550, 13, 2188, 295, 264, 589, 291, 600, 1096, 300, 733, 295, 881, 19075, 452, 51240], "temperature": 0.0, "avg_logprob": -0.08289784651536208, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01029613520950079}, {"id": 246, "seek": 144352, "start": 1461.04, "end": 1466.8799999999999, "text": " mind was actually on what happens when we're able to basically build AI that does roughly what we", "tokens": [51240, 1575, 390, 767, 322, 437, 2314, 562, 321, 434, 1075, 281, 1936, 1322, 7318, 300, 775, 9810, 437, 321, 51532], "temperature": 0.0, "avg_logprob": -0.08289784651536208, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01029613520950079}, {"id": 247, "seek": 144352, "start": 1466.8799999999999, "end": 1472.32, "text": " intended to do. I think I naively would have guessed something like the world carries on is normal,", "tokens": [51532, 10226, 281, 360, 13, 286, 519, 286, 1667, 3413, 576, 362, 21852, 746, 411, 264, 1002, 16402, 322, 307, 2710, 11, 51804], "temperature": 0.0, "avg_logprob": -0.08289784651536208, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.01029613520950079}, {"id": 248, "seek": 147232, "start": 1472.32, "end": 1479.52, "text": " but we use GPT-8 a lot in our jobs. But you've looked into the hypothesis that not only will", "tokens": [50364, 457, 321, 764, 26039, 51, 12, 23, 257, 688, 294, 527, 4782, 13, 583, 291, 600, 2956, 666, 264, 17291, 300, 406, 787, 486, 50724], "temperature": 0.0, "avg_logprob": -0.07623469704075864, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0004441597848199308}, {"id": 249, "seek": 147232, "start": 1479.52, "end": 1485.84, "text": " things not stay the same, they actually might change very, very, very quickly if AGI is so good", "tokens": [50724, 721, 406, 1754, 264, 912, 11, 436, 767, 1062, 1319, 588, 11, 588, 11, 588, 2661, 498, 316, 26252, 307, 370, 665, 51040], "temperature": 0.0, "avg_logprob": -0.07623469704075864, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0004441597848199308}, {"id": 250, "seek": 147232, "start": 1485.84, "end": 1492.48, "text": " that it kind of causes explosive economic growth, which yeah, in this case, you're defining as the", "tokens": [51040, 300, 309, 733, 295, 7700, 24630, 4836, 4599, 11, 597, 1338, 11, 294, 341, 1389, 11, 291, 434, 17827, 382, 264, 51372], "temperature": 0.0, "avg_logprob": -0.07623469704075864, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0004441597848199308}, {"id": 251, "seek": 147232, "start": 1492.48, "end": 1497.84, "text": " world economy growing something like 10 times faster than it has for the last century. Yeah,", "tokens": [51372, 1002, 5010, 4194, 746, 411, 1266, 1413, 4663, 813, 309, 575, 337, 264, 1036, 4901, 13, 865, 11, 51640], "temperature": 0.0, "avg_logprob": -0.07623469704075864, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.0004441597848199308}, {"id": 252, "seek": 149784, "start": 1497.84, "end": 1503.52, "text": " so to start, can you help me understand intuitively what it would mean for the economy to grow 10", "tokens": [50364, 370, 281, 722, 11, 393, 291, 854, 385, 1223, 46506, 437, 309, 576, 914, 337, 264, 5010, 281, 1852, 1266, 50648], "temperature": 0.0, "avg_logprob": -0.0715629025509483, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0010642658453434706}, {"id": 253, "seek": 149784, "start": 1503.52, "end": 1509.28, "text": " times faster? Sure. So one way to think about this is to think about all the technological", "tokens": [50648, 1413, 4663, 30, 4894, 13, 407, 472, 636, 281, 519, 466, 341, 307, 281, 519, 466, 439, 264, 18439, 50936], "temperature": 0.0, "avg_logprob": -0.0715629025509483, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0010642658453434706}, {"id": 254, "seek": 149784, "start": 1509.28, "end": 1515.52, "text": " changes that have happened over the last 50 years. Okay. Yeah, that feels like a lot. So 50 years ago,", "tokens": [50936, 2962, 300, 362, 2011, 670, 264, 1036, 2625, 924, 13, 1033, 13, 865, 11, 300, 3417, 411, 257, 688, 13, 407, 2625, 924, 2057, 11, 51248], "temperature": 0.0, "avg_logprob": -0.0715629025509483, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0010642658453434706}, {"id": 255, "seek": 149784, "start": 1515.52, "end": 1524.3999999999999, "text": " it was 1970, we'd had very basic digital computers around, but they weren't being widely used,", "tokens": [51248, 309, 390, 14577, 11, 321, 1116, 632, 588, 3875, 4562, 10807, 926, 11, 457, 436, 4999, 380, 885, 13371, 1143, 11, 51692], "temperature": 0.0, "avg_logprob": -0.0715629025509483, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.0010642658453434706}, {"id": 256, "seek": 152440, "start": 1524.4, "end": 1531.2800000000002, "text": " they weren't very good. I don't think the internet was around. And there's loads of other improvements", "tokens": [50364, 436, 4999, 380, 588, 665, 13, 286, 500, 380, 519, 264, 4705, 390, 926, 13, 400, 456, 311, 12668, 295, 661, 13797, 50708], "temperature": 0.0, "avg_logprob": -0.1251381202177568, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.005278938449919224}, {"id": 257, "seek": 152440, "start": 1531.2800000000002, "end": 1540.64, "text": " in manufacturing and in agricultural techniques. Medical care. Exactly. Yeah. And massive improvements", "tokens": [50708, 294, 11096, 293, 294, 19587, 7512, 13, 15896, 1127, 13, 7587, 13, 865, 13, 400, 5994, 13797, 51176], "temperature": 0.0, "avg_logprob": -0.1251381202177568, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.005278938449919224}, {"id": 258, "seek": 152440, "start": 1540.64, "end": 1546.72, "text": " across the board in the last 50 years, but probably the most striking is IT. Yeah, sounds right.", "tokens": [51176, 2108, 264, 3150, 294, 264, 1036, 2625, 924, 11, 457, 1391, 264, 881, 18559, 307, 6783, 13, 865, 11, 3263, 558, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1251381202177568, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.005278938449919224}, {"id": 259, "seek": 152440, "start": 1546.72, "end": 1551.2, "text": " And so what explosive growth would look like is that all those changes, rather than happening", "tokens": [51480, 400, 370, 437, 24630, 4599, 576, 574, 411, 307, 300, 439, 729, 2962, 11, 2831, 813, 2737, 51704], "temperature": 0.0, "avg_logprob": -0.1251381202177568, "compression_ratio": 1.5408560311284047, "no_speech_prob": 0.005278938449919224}, {"id": 260, "seek": 155120, "start": 1551.2, "end": 1556.72, "text": " over the course of 50 years, they happened over the course of five years. So we're going to get", "tokens": [50364, 670, 264, 1164, 295, 2625, 924, 11, 436, 2011, 670, 264, 1164, 295, 1732, 924, 13, 407, 321, 434, 516, 281, 483, 50640], "temperature": 0.0, "avg_logprob": -0.07411451654119806, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004298109095543623}, {"id": 261, "seek": 155120, "start": 1556.72, "end": 1563.68, "text": " the internet in five years plus a bunch of other improvements. Exactly. So rather than kind of", "tokens": [50640, 264, 4705, 294, 1732, 924, 1804, 257, 3840, 295, 661, 13797, 13, 7587, 13, 407, 2831, 813, 733, 295, 50988], "temperature": 0.0, "avg_logprob": -0.07411451654119806, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004298109095543623}, {"id": 262, "seek": 155120, "start": 1564.48, "end": 1570.72, "text": " it taking 50 years to go from these really rubbish slow computers that you could buy in 1970 to the", "tokens": [51028, 309, 1940, 2625, 924, 281, 352, 490, 613, 534, 29978, 2964, 10807, 300, 291, 727, 2256, 294, 14577, 281, 264, 51340], "temperature": 0.0, "avg_logprob": -0.07411451654119806, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004298109095543623}, {"id": 263, "seek": 155120, "start": 1570.72, "end": 1576.88, "text": " awesome MacBooks of today, that just happens over five years. And similarly, rather than taking 50", "tokens": [51340, 3476, 31737, 82, 295, 965, 11, 300, 445, 2314, 670, 1732, 924, 13, 400, 14138, 11, 2831, 813, 1940, 2625, 51648], "temperature": 0.0, "avg_logprob": -0.07411451654119806, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004298109095543623}, {"id": 264, "seek": 157688, "start": 1576.88, "end": 1583.44, "text": " years for you to go from those kind of rubbish phones to smartphones of today that also have the", "tokens": [50364, 924, 337, 291, 281, 352, 490, 729, 733, 295, 29978, 10216, 281, 26782, 295, 965, 300, 611, 362, 264, 50692], "temperature": 0.0, "avg_logprob": -0.11657050450642904, "compression_ratio": 1.65, "no_speech_prob": 0.003844182938337326}, {"id": 265, "seek": 157688, "start": 1583.44, "end": 1588.48, "text": " internet and all these specialized apps, that again, this happens over five years. So the kind", "tokens": [50692, 4705, 293, 439, 613, 19813, 7733, 11, 300, 797, 11, 341, 2314, 670, 1732, 924, 13, 407, 264, 733, 50944], "temperature": 0.0, "avg_logprob": -0.11657050450642904, "compression_ratio": 1.65, "no_speech_prob": 0.003844182938337326}, {"id": 266, "seek": 157688, "start": 1588.48, "end": 1593.2800000000002, "text": " of you see the introduction of a new technology and then very, very quickly you see it being refined", "tokens": [50944, 295, 291, 536, 264, 9339, 295, 257, 777, 2899, 293, 550, 588, 11, 588, 2661, 291, 536, 309, 885, 26201, 51184], "temperature": 0.0, "avg_logprob": -0.11657050450642904, "compression_ratio": 1.65, "no_speech_prob": 0.003844182938337326}, {"id": 267, "seek": 157688, "start": 1593.2800000000002, "end": 1601.0400000000002, "text": " into a super useful human friendly product. Wow. I mean, on the one hand that sounds kind of incredible", "tokens": [51184, 666, 257, 1687, 4420, 1952, 9208, 1674, 13, 3153, 13, 286, 914, 11, 322, 264, 472, 1011, 300, 3263, 733, 295, 4651, 51572], "temperature": 0.0, "avg_logprob": -0.11657050450642904, "compression_ratio": 1.65, "no_speech_prob": 0.003844182938337326}, {"id": 268, "seek": 160104, "start": 1601.04, "end": 1609.04, "text": " and exciting. On the other hand, it just feels like a super strange world to be getting so many", "tokens": [50364, 293, 4670, 13, 1282, 264, 661, 1011, 11, 309, 445, 3417, 411, 257, 1687, 5861, 1002, 281, 312, 1242, 370, 867, 50764], "temperature": 0.0, "avg_logprob": -0.06844367451137967, "compression_ratio": 1.5354330708661417, "no_speech_prob": 0.014654083177447319}, {"id": 269, "seek": 160104, "start": 1609.04, "end": 1617.68, "text": " new technologies every few years. Can you explain the idea behind why AGI might even make that possible?", "tokens": [50764, 777, 7943, 633, 1326, 924, 13, 1664, 291, 2903, 264, 1558, 2261, 983, 316, 26252, 1062, 754, 652, 300, 1944, 30, 51196], "temperature": 0.0, "avg_logprob": -0.06844367451137967, "compression_ratio": 1.5354330708661417, "no_speech_prob": 0.014654083177447319}, {"id": 270, "seek": 160104, "start": 1618.48, "end": 1625.76, "text": " Yeah. So here's the most basic version of the argument, which you can kind of make it more", "tokens": [51236, 865, 13, 407, 510, 311, 264, 881, 3875, 3037, 295, 264, 6770, 11, 597, 291, 393, 733, 295, 652, 309, 544, 51600], "temperature": 0.0, "avg_logprob": -0.06844367451137967, "compression_ratio": 1.5354330708661417, "no_speech_prob": 0.014654083177447319}, {"id": 271, "seek": 160104, "start": 1625.76, "end": 1630.08, "text": " complicated to adjust various objections. But I think kind of this version captures the core idea.", "tokens": [51600, 6179, 281, 4369, 3683, 44649, 13, 583, 286, 519, 733, 295, 341, 3037, 27986, 264, 4965, 1558, 13, 51816], "temperature": 0.0, "avg_logprob": -0.06844367451137967, "compression_ratio": 1.5354330708661417, "no_speech_prob": 0.014654083177447319}, {"id": 272, "seek": 163104, "start": 1631.04, "end": 1636.96, "text": " So today, there are maybe tens of millions of people whose job it is to discover new and better", "tokens": [50364, 407, 965, 11, 456, 366, 1310, 10688, 295, 6803, 295, 561, 6104, 1691, 309, 307, 281, 4411, 777, 293, 1101, 50660], "temperature": 0.0, "avg_logprob": -0.08662218417761461, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00019263848662376404}, {"id": 273, "seek": 163104, "start": 1636.96, "end": 1642.48, "text": " technologies, working in science and research and development. They're able to make a certain amount", "tokens": [50660, 7943, 11, 1364, 294, 3497, 293, 2132, 293, 3250, 13, 814, 434, 1075, 281, 652, 257, 1629, 2372, 50936], "temperature": 0.0, "avg_logprob": -0.08662218417761461, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00019263848662376404}, {"id": 274, "seek": 163104, "start": 1642.48, "end": 1647.84, "text": " of progress each year. And it's their work that helps us get better computers and phones and", "tokens": [50936, 295, 4205, 1184, 1064, 13, 400, 309, 311, 641, 589, 300, 3665, 505, 483, 1101, 10807, 293, 10216, 293, 51204], "temperature": 0.0, "avg_logprob": -0.08662218417761461, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00019263848662376404}, {"id": 275, "seek": 163104, "start": 1647.84, "end": 1654.0, "text": " discover better types of solar panels and drives all these improvements that we're seeing. But", "tokens": [51204, 4411, 1101, 3467, 295, 7936, 13419, 293, 11754, 439, 613, 13797, 300, 321, 434, 2577, 13, 583, 51512], "temperature": 0.0, "avg_logprob": -0.08662218417761461, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00019263848662376404}, {"id": 276, "seek": 163104, "start": 1654.0, "end": 1659.84, "text": " like we've been talking about shortly after AGI, I think there's going to be billions of", "tokens": [51512, 411, 321, 600, 668, 1417, 466, 13392, 934, 316, 26252, 11, 286, 519, 456, 311, 516, 281, 312, 17375, 295, 51804], "temperature": 0.0, "avg_logprob": -0.08662218417761461, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.00019263848662376404}, {"id": 277, "seek": 165984, "start": 1660.56, "end": 1666.6399999999999, "text": " top human researchers equivalents in terms of a scientific workforce from AI.", "tokens": [50400, 1192, 1952, 10309, 9052, 791, 294, 2115, 295, 257, 8134, 14201, 490, 7318, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1429126339574014, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.0012025214964523911}, {"id": 278, "seek": 165984, "start": 1667.52, "end": 1674.1599999999999, "text": " And if you imagine that workforce or half of that workforce or just 10% of it working on", "tokens": [50748, 400, 498, 291, 3811, 300, 14201, 420, 1922, 295, 300, 14201, 420, 445, 1266, 4, 295, 309, 1364, 322, 51080], "temperature": 0.0, "avg_logprob": -0.1429126339574014, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.0012025214964523911}, {"id": 279, "seek": 165984, "start": 1674.8, "end": 1683.4399999999998, "text": " trying to advance technology and come up with new ideas, then you have now 10 or 100 times the effort", "tokens": [51112, 1382, 281, 7295, 2899, 293, 808, 493, 365, 777, 3487, 11, 550, 291, 362, 586, 1266, 420, 2319, 1413, 264, 4630, 51544], "temperature": 0.0, "avg_logprob": -0.1429126339574014, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.0012025214964523911}, {"id": 280, "seek": 168344, "start": 1683.44, "end": 1690.16, "text": " that's going into that activity. And these AIs are also able to think maybe 10 or 100 times as", "tokens": [50364, 300, 311, 516, 666, 300, 5191, 13, 400, 613, 316, 6802, 366, 611, 1075, 281, 519, 1310, 1266, 420, 2319, 1413, 382, 50700], "temperature": 0.0, "avg_logprob": -0.0820006522062783, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.18967929482460022}, {"id": 281, "seek": 168344, "start": 1690.16, "end": 1696.0, "text": " quickly as humans can think. And you're able to take the very best AI researchers and copy them.", "tokens": [50700, 2661, 382, 6255, 393, 519, 13, 400, 291, 434, 1075, 281, 747, 264, 588, 1151, 7318, 10309, 293, 5055, 552, 13, 50992], "temperature": 0.0, "avg_logprob": -0.0820006522062783, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.18967929482460022}, {"id": 282, "seek": 168344, "start": 1697.1200000000001, "end": 1702.8, "text": " So if you think that scientific progress is overwhelmingly driven by like a few smaller", "tokens": [51048, 407, 498, 291, 519, 300, 8134, 4205, 307, 42926, 9555, 538, 411, 257, 1326, 4356, 51332], "temperature": 0.0, "avg_logprob": -0.0820006522062783, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.18967929482460022}, {"id": 283, "seek": 168344, "start": 1702.8, "end": 1706.88, "text": " number of really kind of brilliant people with brilliant ideas, then we just need one of them", "tokens": [51332, 1230, 295, 534, 733, 295, 10248, 561, 365, 10248, 3487, 11, 550, 321, 445, 643, 472, 295, 552, 51536], "temperature": 0.0, "avg_logprob": -0.0820006522062783, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.18967929482460022}, {"id": 284, "seek": 168344, "start": 1706.88, "end": 1711.6000000000001, "text": " and we can copy them. They might be happy to just work much harder than humans work.", "tokens": [51536, 293, 321, 393, 5055, 552, 13, 814, 1062, 312, 2055, 281, 445, 589, 709, 6081, 813, 6255, 589, 13, 51772], "temperature": 0.0, "avg_logprob": -0.0820006522062783, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.18967929482460022}, {"id": 285, "seek": 171160, "start": 1712.56, "end": 1715.84, "text": " It might be possible to focus them much more effectively on the most important", "tokens": [50412, 467, 1062, 312, 1944, 281, 1879, 552, 709, 544, 8659, 322, 264, 881, 1021, 50576], "temperature": 0.0, "avg_logprob": -0.12203961975720463, "compression_ratio": 1.6036363636363635, "no_speech_prob": 0.003443746827542782}, {"id": 286, "seek": 171160, "start": 1716.48, "end": 1720.6399999999999, "text": " types of R&D, whereas humans maybe are more inclined to follow their interests,", "tokens": [50608, 3467, 295, 497, 5, 35, 11, 9735, 6255, 1310, 366, 544, 28173, 281, 1524, 641, 8847, 11, 50816], "temperature": 0.0, "avg_logprob": -0.12203961975720463, "compression_ratio": 1.6036363636363635, "no_speech_prob": 0.003443746827542782}, {"id": 287, "seek": 171160, "start": 1721.36, "end": 1726.3999999999999, "text": " even when it's not the most useful thing to be researching. And so all of those things together", "tokens": [50852, 754, 562, 309, 311, 406, 264, 881, 4420, 551, 281, 312, 24176, 13, 400, 370, 439, 295, 729, 721, 1214, 51104], "temperature": 0.0, "avg_logprob": -0.12203961975720463, "compression_ratio": 1.6036363636363635, "no_speech_prob": 0.003443746827542782}, {"id": 288, "seek": 171160, "start": 1727.1999999999998, "end": 1733.36, "text": " just mean that we'll be generating kind of 100 times as many new good ideas and innovations", "tokens": [51144, 445, 914, 300, 321, 603, 312, 17746, 733, 295, 2319, 1413, 382, 867, 777, 665, 3487, 293, 24283, 51452], "temperature": 0.0, "avg_logprob": -0.12203961975720463, "compression_ratio": 1.6036363636363635, "no_speech_prob": 0.003443746827542782}, {"id": 289, "seek": 171160, "start": 1733.36, "end": 1739.12, "text": " each year compared with today. And then that would drive the development of technologies to be", "tokens": [51452, 1184, 1064, 5347, 365, 965, 13, 400, 550, 300, 576, 3332, 264, 3250, 295, 7943, 281, 312, 51740], "temperature": 0.0, "avg_logprob": -0.12203961975720463, "compression_ratio": 1.6036363636363635, "no_speech_prob": 0.003443746827542782}, {"id": 290, "seek": 173912, "start": 1739.6799999999998, "end": 1746.1599999999999, "text": " at least 10 times faster than today. Right. How likely do you think this kind of growth is?", "tokens": [50392, 412, 1935, 1266, 1413, 4663, 813, 965, 13, 1779, 13, 1012, 3700, 360, 291, 519, 341, 733, 295, 4599, 307, 30, 50716], "temperature": 0.0, "avg_logprob": -0.09125748241648954, "compression_ratio": 1.5223214285714286, "no_speech_prob": 0.004590644501149654}, {"id": 291, "seek": 173912, "start": 1747.52, "end": 1754.08, "text": " Is it the default once we get AGI? I think it is a default. You could give", "tokens": [50784, 1119, 309, 264, 7576, 1564, 321, 483, 316, 26252, 30, 286, 519, 309, 307, 257, 7576, 13, 509, 727, 976, 51112], "temperature": 0.0, "avg_logprob": -0.09125748241648954, "compression_ratio": 1.5223214285714286, "no_speech_prob": 0.004590644501149654}, {"id": 292, "seek": 173912, "start": 1754.08, "end": 1760.0, "text": " objections to the argument I gave, but I think it's mostly possible to answer those objections.", "tokens": [51112, 44649, 281, 264, 6770, 286, 2729, 11, 457, 286, 519, 309, 311, 5240, 1944, 281, 1867, 729, 44649, 13, 51408], "temperature": 0.0, "avg_logprob": -0.09125748241648954, "compression_ratio": 1.5223214285714286, "no_speech_prob": 0.004590644501149654}, {"id": 293, "seek": 173912, "start": 1760.0, "end": 1764.32, "text": " So you could say, well, discovering new technologies isn't just about thinking", "tokens": [51408, 407, 291, 727, 584, 11, 731, 11, 24773, 777, 7943, 1943, 380, 445, 466, 1953, 51624], "temperature": 0.0, "avg_logprob": -0.09125748241648954, "compression_ratio": 1.5223214285714286, "no_speech_prob": 0.004590644501149654}, {"id": 294, "seek": 176432, "start": 1764.32, "end": 1767.84, "text": " and coming up with new ideas, you also need to do experiments. Okay, sure.", "tokens": [50364, 293, 1348, 493, 365, 777, 3487, 11, 291, 611, 643, 281, 360, 12050, 13, 1033, 11, 988, 13, 50540], "temperature": 0.0, "avg_logprob": -0.11653391520182292, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.013910580426454544}, {"id": 295, "seek": 176432, "start": 1768.6399999999999, "end": 1774.8799999999999, "text": " And then I think you can answer that objection by saying, that's right, we will need to do", "tokens": [50580, 400, 550, 286, 519, 291, 393, 1867, 300, 35756, 538, 1566, 11, 300, 311, 558, 11, 321, 486, 643, 281, 360, 50892], "temperature": 0.0, "avg_logprob": -0.11653391520182292, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.013910580426454544}, {"id": 296, "seek": 176432, "start": 1774.8799999999999, "end": 1781.6799999999998, "text": " experiments. And that's like testing a drug on humans and maybe it takes five years or something", "tokens": [50892, 12050, 13, 400, 300, 311, 411, 4997, 257, 4110, 322, 6255, 293, 1310, 309, 2516, 1732, 924, 420, 746, 51232], "temperature": 0.0, "avg_logprob": -0.11653391520182292, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.013910580426454544}, {"id": 297, "seek": 176432, "start": 1781.6799999999998, "end": 1790.08, "text": " to really check that it's safe and effective. Right. Yeah. Or you've designed a new solar panel", "tokens": [51232, 281, 534, 1520, 300, 309, 311, 3273, 293, 4942, 13, 1779, 13, 865, 13, 1610, 291, 600, 4761, 257, 777, 7936, 4831, 51652], "temperature": 0.0, "avg_logprob": -0.11653391520182292, "compression_ratio": 1.577092511013216, "no_speech_prob": 0.013910580426454544}, {"id": 298, "seek": 179008, "start": 1790.08, "end": 1795.6799999999998, "text": " and you want to like test its performance in a variety of conditions. Yeah. Yeah. Or you kind", "tokens": [50364, 293, 291, 528, 281, 411, 1500, 1080, 3389, 294, 257, 5673, 295, 4487, 13, 865, 13, 865, 13, 1610, 291, 733, 50644], "temperature": 0.0, "avg_logprob": -0.08970087545889395, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.020466402173042297}, {"id": 299, "seek": 179008, "start": 1795.6799999999998, "end": 1799.12, "text": " of running some experiments to see what happens when you combine these two chemicals together", "tokens": [50644, 295, 2614, 512, 12050, 281, 536, 437, 2314, 562, 291, 10432, 613, 732, 16152, 1214, 50816], "temperature": 0.0, "avg_logprob": -0.08970087545889395, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.020466402173042297}, {"id": 300, "seek": 179008, "start": 1799.12, "end": 1807.36, "text": " because you're not able to predict it in advance. But if you have a billion AIs trying to push forward", "tokens": [50816, 570, 291, 434, 406, 1075, 281, 6069, 309, 294, 7295, 13, 583, 498, 291, 362, 257, 5218, 316, 6802, 1382, 281, 2944, 2128, 51228], "temperature": 0.0, "avg_logprob": -0.08970087545889395, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.020466402173042297}, {"id": 301, "seek": 179008, "start": 1808.0, "end": 1812.08, "text": " R&D and they're bottlenecked on these needing to do these experiments, then they'll be putting", "tokens": [51260, 497, 5, 35, 293, 436, 434, 44641, 44118, 322, 613, 18006, 281, 360, 613, 12050, 11, 550, 436, 603, 312, 3372, 51464], "temperature": 0.0, "avg_logprob": -0.08970087545889395, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.020466402173042297}, {"id": 302, "seek": 179008, "start": 1812.08, "end": 1816.3999999999999, "text": " in a huge amount of effort to make these experiments happen as efficiently as possible.", "tokens": [51464, 294, 257, 2603, 2372, 295, 4630, 281, 652, 613, 12050, 1051, 382, 19621, 382, 1944, 13, 51680], "temperature": 0.0, "avg_logprob": -0.08970087545889395, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.020466402173042297}, {"id": 303, "seek": 181640, "start": 1817.3600000000001, "end": 1822.3200000000002, "text": " Whereas today, we might be using the lab for 50% of the time we could be using it", "tokens": [50412, 13813, 965, 11, 321, 1062, 312, 1228, 264, 2715, 337, 2625, 4, 295, 264, 565, 321, 727, 312, 1228, 309, 50660], "temperature": 0.0, "avg_logprob": -0.11338268461681547, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.0009019611170515418}, {"id": 304, "seek": 181640, "start": 1823.0400000000002, "end": 1826.8000000000002, "text": " and we might be just doing a whole bunch of experiments and then analyzing it afterwards", "tokens": [50696, 293, 321, 1062, 312, 445, 884, 257, 1379, 3840, 295, 12050, 293, 550, 23663, 309, 10543, 50884], "temperature": 0.0, "avg_logprob": -0.11338268461681547, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.0009019611170515418}, {"id": 305, "seek": 181640, "start": 1826.8000000000002, "end": 1832.5600000000002, "text": " and learning a little bit from each experiment, but also not kind of trying to cram as much into", "tokens": [50884, 293, 2539, 257, 707, 857, 490, 1184, 5120, 11, 457, 611, 406, 733, 295, 1382, 281, 941, 335, 382, 709, 666, 51172], "temperature": 0.0, "avg_logprob": -0.11338268461681547, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.0009019611170515418}, {"id": 306, "seek": 181640, "start": 1832.5600000000002, "end": 1837.76, "text": " each experiment as is humanly possible. If these AIs are limited on experiments and they're going", "tokens": [51172, 1184, 5120, 382, 307, 1952, 356, 1944, 13, 759, 613, 316, 6802, 366, 5567, 322, 12050, 293, 436, 434, 516, 51432], "temperature": 0.0, "avg_logprob": -0.11338268461681547, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.0009019611170515418}, {"id": 307, "seek": 181640, "start": 1837.76, "end": 1844.48, "text": " to be spending months and months just meticulously planning the micro details of every single", "tokens": [51432, 281, 312, 6434, 2493, 293, 2493, 445, 41566, 25038, 5038, 264, 4532, 4365, 295, 633, 2167, 51768], "temperature": 0.0, "avg_logprob": -0.11338268461681547, "compression_ratio": 1.7653846153846153, "no_speech_prob": 0.0009019611170515418}, {"id": 308, "seek": 184448, "start": 1844.48, "end": 1850.8, "text": " experiment so that you can get as much information as possible out of each one and kind of fully", "tokens": [50364, 5120, 370, 300, 291, 393, 483, 382, 709, 1589, 382, 1944, 484, 295, 1184, 472, 293, 733, 295, 4498, 50680], "temperature": 0.0, "avg_logprob": -0.10341304110497543, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.006133211310952902}, {"id": 309, "seek": 184448, "start": 1850.8, "end": 1855.3600000000001, "text": " coalescing their theoretical understanding and all the current data and implications and saying,", "tokens": [50680, 598, 4229, 2175, 641, 20864, 3701, 293, 439, 264, 2190, 1412, 293, 16602, 293, 1566, 11, 50908], "temperature": 0.0, "avg_logprob": -0.10341304110497543, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.006133211310952902}, {"id": 310, "seek": 184448, "start": 1855.3600000000001, "end": 1860.64, "text": " here are the key uncertainties that we need to address with these like kind of scarce experiments", "tokens": [50908, 510, 366, 264, 2141, 11308, 6097, 300, 321, 643, 281, 2985, 365, 613, 411, 733, 295, 41340, 12050, 51172], "temperature": 0.0, "avg_logprob": -0.10341304110497543, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.006133211310952902}, {"id": 311, "seek": 184448, "start": 1860.64, "end": 1866.08, "text": " and they'll give the humans conducting the experiments really detailed and precise instructions", "tokens": [51172, 293, 436, 603, 976, 264, 6255, 21749, 264, 12050, 534, 9942, 293, 13600, 9415, 51444], "temperature": 0.0, "avg_logprob": -0.10341304110497543, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.006133211310952902}, {"id": 312, "seek": 184448, "start": 1866.08, "end": 1871.1200000000001, "text": " and set things up so the experiments are really unlikely to go wrong and kind of analyze the", "tokens": [51444, 293, 992, 721, 493, 370, 264, 12050, 366, 534, 17518, 281, 352, 2085, 293, 733, 295, 12477, 264, 51696], "temperature": 0.0, "avg_logprob": -0.10341304110497543, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.006133211310952902}, {"id": 313, "seek": 187112, "start": 1871.12, "end": 1874.7199999999998, "text": " resultant data from 100 different angles to learn as much as you can from them.", "tokens": [50364, 1874, 394, 1412, 490, 2319, 819, 14708, 281, 1466, 382, 709, 382, 291, 393, 490, 552, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11457023620605469, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005293022375553846}, {"id": 314, "seek": 187112, "start": 1876.0, "end": 1879.6799999999998, "text": " And I think that will go a long way to getting over the experimental bottleneck.", "tokens": [50608, 400, 286, 519, 300, 486, 352, 257, 938, 636, 281, 1242, 670, 264, 17069, 44641, 547, 13, 50792], "temperature": 0.0, "avg_logprob": -0.11457023620605469, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005293022375553846}, {"id": 315, "seek": 187112, "start": 1880.2399999999998, "end": 1886.8799999999999, "text": " I mean, even if you just think you use labs eight hours a day, but you could use them 24 hours a day", "tokens": [50820, 286, 914, 11, 754, 498, 291, 445, 519, 291, 764, 20339, 3180, 2496, 257, 786, 11, 457, 291, 727, 764, 552, 4022, 2496, 257, 786, 51152], "temperature": 0.0, "avg_logprob": -0.11457023620605469, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005293022375553846}, {"id": 316, "seek": 187112, "start": 1886.8799999999999, "end": 1892.0, "text": " and then there are probably hundreds of other efficiencies like that that will all just add up", "tokens": [51152, 293, 550, 456, 366, 1391, 6779, 295, 661, 4703, 31294, 411, 300, 300, 486, 439, 445, 909, 493, 51408], "temperature": 0.0, "avg_logprob": -0.11457023620605469, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005293022375553846}, {"id": 317, "seek": 187112, "start": 1892.0, "end": 1894.4799999999998, "text": " to get to many, many times more efficient stuff.", "tokens": [51408, 281, 483, 281, 867, 11, 867, 1413, 544, 7148, 1507, 13, 51532], "temperature": 0.0, "avg_logprob": -0.11457023620605469, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.005293022375553846}, {"id": 318, "seek": 189448, "start": 1894.88, "end": 1900.32, "text": " Right. And I mean, the AIs can direct humans what to do so you could be paying very high", "tokens": [50384, 1779, 13, 400, 286, 914, 11, 264, 316, 6802, 393, 2047, 6255, 437, 281, 360, 370, 291, 727, 312, 6229, 588, 1090, 50656], "temperature": 0.0, "avg_logprob": -0.11164180921471638, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.08460401743650436}, {"id": 319, "seek": 189448, "start": 1900.32, "end": 1905.68, "text": " wages to have unskilled human workers work through the night to run these experiments", "tokens": [50656, 20097, 281, 362, 2693, 74, 6261, 1952, 5600, 589, 807, 264, 1818, 281, 1190, 613, 12050, 50924], "temperature": 0.0, "avg_logprob": -0.11164180921471638, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.08460401743650436}, {"id": 320, "seek": 189448, "start": 1905.68, "end": 1911.28, "text": " directed by AIs telling them exactly what to do when. So yeah, you'll be able to have those labs", "tokens": [50924, 12898, 538, 316, 6802, 3585, 552, 2293, 437, 281, 360, 562, 13, 407, 1338, 11, 291, 603, 312, 1075, 281, 362, 729, 20339, 51204], "temperature": 0.0, "avg_logprob": -0.11164180921471638, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.08460401743650436}, {"id": 321, "seek": 189448, "start": 1911.28, "end": 1916.96, "text": " working around the clock if that's what's wanted. In the longer run, robotics is already very good", "tokens": [51204, 1364, 926, 264, 7830, 498, 300, 311, 437, 311, 1415, 13, 682, 264, 2854, 1190, 11, 34145, 307, 1217, 588, 665, 51488], "temperature": 0.0, "avg_logprob": -0.11164180921471638, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.08460401743650436}, {"id": 322, "seek": 189448, "start": 1917.6, "end": 1923.3600000000001, "text": " and I don't think it's going to take too long once we have a billion AI researchers to design", "tokens": [51520, 293, 286, 500, 380, 519, 309, 311, 516, 281, 747, 886, 938, 1564, 321, 362, 257, 5218, 7318, 10309, 281, 1715, 51808], "temperature": 0.0, "avg_logprob": -0.11164180921471638, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.08460401743650436}, {"id": 323, "seek": 192336, "start": 1923.36, "end": 1929.1999999999998, "text": " robots that are able to do the physical tasks that humans do. It doesn't, you know, with that", "tokens": [50364, 14733, 300, 366, 1075, 281, 360, 264, 4001, 9608, 300, 6255, 360, 13, 467, 1177, 380, 11, 291, 458, 11, 365, 300, 50656], "temperature": 0.0, "avg_logprob": -0.12822405941836484, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0022793570533394814}, {"id": 324, "seek": 192336, "start": 1929.1999999999998, "end": 1934.3999999999999, "text": " far off at the moment. And so, you know, if eventually, you know, we actually, we just need", "tokens": [50656, 1400, 766, 412, 264, 1623, 13, 400, 370, 11, 291, 458, 11, 498, 4728, 11, 291, 458, 11, 321, 767, 11, 321, 445, 643, 50916], "temperature": 0.0, "avg_logprob": -0.12822405941836484, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0022793570533394814}, {"id": 325, "seek": 192336, "start": 1934.3999999999999, "end": 1939.04, "text": " more humans to kind of build more labs or to run these experiments, I think it will be possible to", "tokens": [50916, 544, 6255, 281, 733, 295, 1322, 544, 20339, 420, 281, 1190, 613, 12050, 11, 286, 519, 309, 486, 312, 1944, 281, 51148], "temperature": 0.0, "avg_logprob": -0.12822405941836484, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0022793570533394814}, {"id": 326, "seek": 192336, "start": 1939.04, "end": 1943.84, "text": " have, you know, have robots doing that work and have the AIs directing it. And again, kind of", "tokens": [51148, 362, 11, 291, 458, 11, 362, 14733, 884, 300, 589, 293, 362, 264, 316, 6802, 26979, 309, 13, 400, 797, 11, 733, 295, 51388], "temperature": 0.0, "avg_logprob": -0.12822405941836484, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0022793570533394814}, {"id": 327, "seek": 192336, "start": 1943.84, "end": 1948.7199999999998, "text": " meticulously planning what each robot is doing with its time. So we're getting the very most", "tokens": [51388, 41566, 25038, 5038, 437, 1184, 7881, 307, 884, 365, 1080, 565, 13, 407, 321, 434, 1242, 264, 588, 881, 51632], "temperature": 0.0, "avg_logprob": -0.12822405941836484, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0022793570533394814}, {"id": 328, "seek": 192336, "start": 1948.7199999999998, "end": 1953.1999999999998, "text": " out of each robot. Right. I mean, the example you raised about human experiments is a really", "tokens": [51632, 484, 295, 1184, 7881, 13, 1779, 13, 286, 914, 11, 264, 1365, 291, 6005, 466, 1952, 12050, 307, 257, 534, 51856], "temperature": 0.0, "avg_logprob": -0.12822405941836484, "compression_ratio": 1.825242718446602, "no_speech_prob": 0.0022793570533394814}, {"id": 329, "seek": 195320, "start": 1953.2, "end": 1957.44, "text": " good one, because that seems like it's going to be particularly hard to speed up. Right.", "tokens": [50364, 665, 472, 11, 570, 300, 2544, 411, 309, 311, 516, 281, 312, 4098, 1152, 281, 3073, 493, 13, 1779, 13, 50576], "temperature": 0.0, "avg_logprob": -0.06298762067742304, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.0030743060633540154}, {"id": 330, "seek": 195320, "start": 1957.44, "end": 1961.2, "text": " There are still a few things that I can already think of that could happen there. So", "tokens": [50576, 821, 366, 920, 257, 1326, 721, 300, 286, 393, 1217, 519, 295, 300, 727, 1051, 456, 13, 407, 50764], "temperature": 0.0, "avg_logprob": -0.06298762067742304, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.0030743060633540154}, {"id": 331, "seek": 195320, "start": 1962.0800000000002, "end": 1966.8, "text": " if it's kind of any psychology experiments that you're wanting to do or knowing how humans will", "tokens": [50808, 498, 309, 311, 733, 295, 604, 15105, 12050, 300, 291, 434, 7935, 281, 360, 420, 5276, 577, 6255, 486, 51044], "temperature": 0.0, "avg_logprob": -0.06298762067742304, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.0030743060633540154}, {"id": 332, "seek": 195320, "start": 1966.8, "end": 1973.3600000000001, "text": " react to a new technology or to a new scenario, then just studying all of the human data on the", "tokens": [51044, 4515, 281, 257, 777, 2899, 420, 281, 257, 777, 9005, 11, 550, 445, 7601, 439, 295, 264, 1952, 1412, 322, 264, 51372], "temperature": 0.0, "avg_logprob": -0.06298762067742304, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.0030743060633540154}, {"id": 333, "seek": 195320, "start": 1973.3600000000001, "end": 1978.96, "text": " internet and doing in-depth interviews with humans could give AIs a really good understanding of how", "tokens": [51372, 4705, 293, 884, 294, 12, 25478, 12318, 365, 6255, 727, 976, 316, 6802, 257, 534, 665, 3701, 295, 577, 51652], "temperature": 0.0, "avg_logprob": -0.06298762067742304, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.0030743060633540154}, {"id": 334, "seek": 197896, "start": 1979.04, "end": 1985.76, "text": " human psychology works. Right. In the limit, they could scan a human's brain and kind of upload", "tokens": [50368, 1952, 15105, 1985, 13, 1779, 13, 682, 264, 4948, 11, 436, 727, 11049, 257, 1952, 311, 3567, 293, 733, 295, 6580, 50704], "temperature": 0.0, "avg_logprob": -0.12253984778818458, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.005284145474433899}, {"id": 335, "seek": 197896, "start": 1985.76, "end": 1992.0, "text": " them to be a kind of a virtual digital person if that human was willing to do it. And then it could", "tokens": [50704, 552, 281, 312, 257, 733, 295, 257, 6374, 4562, 954, 498, 300, 1952, 390, 4950, 281, 360, 309, 13, 400, 550, 309, 727, 51016], "temperature": 0.0, "avg_logprob": -0.12253984778818458, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.005284145474433899}, {"id": 336, "seek": 197896, "start": 1992.0, "end": 1996.96, "text": " then do experiments with them in a simulation much, much more quickly. Right. Really fast.", "tokens": [51016, 550, 360, 12050, 365, 552, 294, 257, 16575, 709, 11, 709, 544, 2661, 13, 1779, 13, 4083, 2370, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12253984778818458, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.005284145474433899}, {"id": 337, "seek": 197896, "start": 1997.92, "end": 2005.2, "text": " I mean, that's getting pretty weird. Yeah. And I'm feeling very sci-fi, but that's part of what", "tokens": [51312, 286, 914, 11, 300, 311, 1242, 1238, 3657, 13, 865, 13, 400, 286, 478, 2633, 588, 2180, 12, 13325, 11, 457, 300, 311, 644, 295, 437, 51676], "temperature": 0.0, "avg_logprob": -0.12253984778818458, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.005284145474433899}, {"id": 338, "seek": 200520, "start": 2005.28, "end": 2011.3600000000001, "text": " we're talking about. We're talking about we've got millions or billions of copies of AI systems", "tokens": [50368, 321, 434, 1417, 466, 13, 492, 434, 1417, 466, 321, 600, 658, 6803, 420, 17375, 295, 14341, 295, 7318, 3652, 50672], "temperature": 0.0, "avg_logprob": -0.09037691896611993, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0067946454510092735}, {"id": 339, "seek": 200520, "start": 2011.3600000000001, "end": 2018.96, "text": " that are as smart or smarter than humans, basically using all of their brain power to innovate.", "tokens": [50672, 300, 366, 382, 4069, 420, 20294, 813, 6255, 11, 1936, 1228, 439, 295, 641, 3567, 1347, 281, 33444, 13, 51052], "temperature": 0.0, "avg_logprob": -0.09037691896611993, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0067946454510092735}, {"id": 340, "seek": 200520, "start": 2019.52, "end": 2025.44, "text": " And yeah, it's bizarre, but if you apply all of that brain power, you're going to get super,", "tokens": [51080, 400, 1338, 11, 309, 311, 18265, 11, 457, 498, 291, 3079, 439, 295, 300, 3567, 1347, 11, 291, 434, 516, 281, 483, 1687, 11, 51376], "temperature": 0.0, "avg_logprob": -0.09037691896611993, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0067946454510092735}, {"id": 341, "seek": 200520, "start": 2025.44, "end": 2031.04, "text": " super fast improvements to technology. Is this bottlenecked at all by ideas getting", "tokens": [51376, 1687, 2370, 13797, 281, 2899, 13, 1119, 341, 44641, 44118, 412, 439, 538, 3487, 1242, 51656], "temperature": 0.0, "avg_logprob": -0.09037691896611993, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0067946454510092735}, {"id": 342, "seek": 203104, "start": 2031.04, "end": 2036.96, "text": " harder to find? Are there going to be limits that just like a human would hit up on these limits", "tokens": [50364, 6081, 281, 915, 30, 2014, 456, 516, 281, 312, 10406, 300, 445, 411, 257, 1952, 576, 2045, 493, 322, 613, 10406, 50660], "temperature": 0.0, "avg_logprob": -0.0878266175587972, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.003469606162980199}, {"id": 343, "seek": 203104, "start": 2036.96, "end": 2042.48, "text": " or humans have, AI systems will also hit up on those limits? Or do we expect them not to,", "tokens": [50660, 420, 6255, 362, 11, 7318, 3652, 486, 611, 2045, 493, 322, 729, 10406, 30, 1610, 360, 321, 2066, 552, 406, 281, 11, 50936], "temperature": 0.0, "avg_logprob": -0.0878266175587972, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.003469606162980199}, {"id": 344, "seek": 203104, "start": 2042.48, "end": 2048.08, "text": " because we're talking about superhuman intelligence? So ideas are getting hard to find.", "tokens": [50936, 570, 321, 434, 1417, 466, 1687, 18796, 7599, 30, 407, 3487, 366, 1242, 1152, 281, 915, 13, 51216], "temperature": 0.0, "avg_logprob": -0.0878266175587972, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.003469606162980199}, {"id": 345, "seek": 203104, "start": 2048.8, "end": 2054.8, "text": " For me, the kind of the best zoomed out example is just that our scientific workforce has been", "tokens": [51252, 1171, 385, 11, 264, 733, 295, 264, 1151, 8863, 292, 484, 1365, 307, 445, 300, 527, 8134, 14201, 575, 668, 51552], "temperature": 0.0, "avg_logprob": -0.0878266175587972, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.003469606162980199}, {"id": 346, "seek": 205480, "start": 2054.88, "end": 2062.7200000000003, "text": " growing at four or 5% every year over the last 80 years. And that's a massive increase in that", "tokens": [50368, 4194, 412, 1451, 420, 1025, 4, 633, 1064, 670, 264, 1036, 4688, 924, 13, 400, 300, 311, 257, 5994, 3488, 294, 300, 50760], "temperature": 0.0, "avg_logprob": -0.08196704963157916, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.042886991053819656}, {"id": 347, "seek": 205480, "start": 2062.7200000000003, "end": 2067.52, "text": " scientific workforce over 80 years, you know, many, many doublings, but actually the pace of", "tokens": [50760, 8134, 14201, 670, 4688, 924, 11, 291, 458, 11, 867, 11, 867, 2482, 5199, 1109, 11, 457, 767, 264, 11638, 295, 51000], "temperature": 0.0, "avg_logprob": -0.08196704963157916, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.042886991053819656}, {"id": 348, "seek": 205480, "start": 2067.52, "end": 2072.6400000000003, "text": " technological progress, if anything, has been slowing down somewhat over the last 80 years.", "tokens": [51000, 18439, 4205, 11, 498, 1340, 11, 575, 668, 26958, 760, 8344, 670, 264, 1036, 4688, 924, 13, 51256], "temperature": 0.0, "avg_logprob": -0.08196704963157916, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.042886991053819656}, {"id": 349, "seek": 205480, "start": 2072.6400000000003, "end": 2076.96, "text": " And so, you know, on a high level, the explanation is sure we're using way more effort than we used", "tokens": [51256, 400, 370, 11, 291, 458, 11, 322, 257, 1090, 1496, 11, 264, 10835, 307, 988, 321, 434, 1228, 636, 544, 4630, 813, 321, 1143, 51472], "temperature": 0.0, "avg_logprob": -0.08196704963157916, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.042886991053819656}, {"id": 350, "seek": 205480, "start": 2076.96, "end": 2081.76, "text": " to be, but the ideas are harder to find. So we're actually slowing down a little bit", "tokens": [51472, 281, 312, 11, 457, 264, 3487, 366, 6081, 281, 915, 13, 407, 321, 434, 767, 26958, 760, 257, 707, 857, 51712], "temperature": 0.0, "avg_logprob": -0.08196704963157916, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.042886991053819656}, {"id": 351, "seek": 208176, "start": 2081.84, "end": 2086.5600000000004, "text": " in terms of the pace of our progress. Maybe the best illustration of that dynamic is physics,", "tokens": [50368, 294, 2115, 295, 264, 11638, 295, 527, 4205, 13, 2704, 264, 1151, 22645, 295, 300, 8546, 307, 10649, 11, 50604], "temperature": 0.0, "avg_logprob": -0.11957833318427058, "compression_ratio": 1.5993150684931507, "no_speech_prob": 0.0022641299292445183}, {"id": 352, "seek": 208176, "start": 2086.5600000000004, "end": 2093.36, "text": " where a kind of 100 years ago, also you could have Albert Einstein in his spare time as a", "tokens": [50604, 689, 257, 733, 295, 2319, 924, 2057, 11, 611, 291, 727, 362, 20812, 23486, 294, 702, 13798, 565, 382, 257, 50944], "temperature": 0.0, "avg_logprob": -0.11957833318427058, "compression_ratio": 1.5993150684931507, "no_speech_prob": 0.0022641299292445183}, {"id": 353, "seek": 208176, "start": 2093.36, "end": 2100.88, "text": " patent clerk come up with multiple very significant breakthroughs. And then kind of almost single", "tokens": [50944, 20495, 31402, 808, 493, 365, 3866, 588, 4776, 22397, 82, 13, 400, 550, 733, 295, 1920, 2167, 51320], "temperature": 0.0, "avg_logprob": -0.11957833318427058, "compression_ratio": 1.5993150684931507, "no_speech_prob": 0.0022641299292445183}, {"id": 354, "seek": 208176, "start": 2100.88, "end": 2106.0800000000004, "text": " handedly or with a few collaborators develop general relativity over the few years that followed,", "tokens": [51320, 1011, 13516, 420, 365, 257, 1326, 39789, 1499, 2674, 45675, 670, 264, 1326, 924, 300, 6263, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11957833318427058, "compression_ratio": 1.5993150684931507, "no_speech_prob": 0.0022641299292445183}, {"id": 355, "seek": 208176, "start": 2106.0800000000004, "end": 2110.0800000000004, "text": " which is just a major breakthrough in our understanding of the universe. Whereas today,", "tokens": [51580, 597, 307, 445, 257, 2563, 22397, 294, 527, 3701, 295, 264, 6445, 13, 13813, 965, 11, 51780], "temperature": 0.0, "avg_logprob": -0.11957833318427058, "compression_ratio": 1.5993150684931507, "no_speech_prob": 0.0022641299292445183}, {"id": 356, "seek": 211008, "start": 2110.72, "end": 2118.3199999999997, "text": " you have probably, you know, maybe millions of physicists, just kind of with the huge machines", "tokens": [50396, 291, 362, 1391, 11, 291, 458, 11, 1310, 6803, 295, 48716, 11, 445, 733, 295, 365, 264, 2603, 8379, 50776], "temperature": 0.0, "avg_logprob": -0.1668943851552111, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00269790249876678}, {"id": 357, "seek": 211008, "start": 2118.3199999999997, "end": 2123.44, "text": " at CERN that have to be honest, making, I would say pretty incremental progress in advancing the", "tokens": [50776, 412, 383, 1598, 45, 300, 362, 281, 312, 3245, 11, 1455, 11, 286, 576, 584, 1238, 35759, 4205, 294, 27267, 264, 51032], "temperature": 0.0, "avg_logprob": -0.1668943851552111, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00269790249876678}, {"id": 358, "seek": 211008, "start": 2123.44, "end": 2128.64, "text": " state of knowledge and physics. Yeah, okay, right. Yeah, in terms of how it applies to AI,", "tokens": [51032, 1785, 295, 3601, 293, 10649, 13, 865, 11, 1392, 11, 558, 13, 865, 11, 294, 2115, 295, 577, 309, 13165, 281, 7318, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1668943851552111, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00269790249876678}, {"id": 359, "seek": 211008, "start": 2129.68, "end": 2134.24, "text": " first thing to say is that even if that dynamic exists, and it's very strong, we would still", "tokens": [51344, 700, 551, 281, 584, 307, 300, 754, 498, 300, 8546, 8198, 11, 293, 309, 311, 588, 2068, 11, 321, 576, 920, 51572], "temperature": 0.0, "avg_logprob": -0.1668943851552111, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00269790249876678}, {"id": 360, "seek": 213424, "start": 2134.3199999999997, "end": 2140.72, "text": " expect a very significant, if temporary, increase in the rate of technological progress.", "tokens": [50368, 2066, 257, 588, 4776, 11, 498, 13413, 11, 3488, 294, 264, 3314, 295, 18439, 4205, 13, 50688], "temperature": 0.0, "avg_logprob": -0.10614380666187831, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.01712741330265999}, {"id": 361, "seek": 213424, "start": 2141.8399999999997, "end": 2147.7599999999998, "text": " So let's say ideas are getting harder to find, but then suddenly, in 10 years, we've got a billion", "tokens": [50744, 407, 718, 311, 584, 3487, 366, 1242, 6081, 281, 915, 11, 457, 550, 5800, 11, 294, 1266, 924, 11, 321, 600, 658, 257, 5218, 51040], "temperature": 0.0, "avg_logprob": -0.10614380666187831, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.01712741330265999}, {"id": 362, "seek": 213424, "start": 2147.7599999999998, "end": 2153.12, "text": " AIs working on it rather than the kind of 10 million humans. Well, even if our ideas are", "tokens": [51040, 316, 6802, 1364, 322, 309, 2831, 813, 264, 733, 295, 1266, 2459, 6255, 13, 1042, 11, 754, 498, 527, 3487, 366, 51308], "temperature": 0.0, "avg_logprob": -0.10614380666187831, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.01712741330265999}, {"id": 363, "seek": 213424, "start": 2153.12, "end": 2157.9199999999996, "text": " getting harder to find, then at least temporarily, there'll be much faster technological progress,", "tokens": [51308, 1242, 6081, 281, 915, 11, 550, 412, 1935, 23750, 11, 456, 603, 312, 709, 4663, 18439, 4205, 11, 51548], "temperature": 0.0, "avg_logprob": -0.10614380666187831, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.01712741330265999}, {"id": 364, "seek": 213424, "start": 2157.9199999999996, "end": 2162.3199999999997, "text": " and then we'll kind of pluck even more of the low hanging fruit and eventually even these AIs get", "tokens": [51548, 293, 550, 321, 603, 733, 295, 41514, 754, 544, 295, 264, 2295, 8345, 6773, 293, 4728, 754, 613, 316, 6802, 483, 51768], "temperature": 0.0, "avg_logprob": -0.10614380666187831, "compression_ratio": 1.7781954887218046, "no_speech_prob": 0.01712741330265999}, {"id": 365, "seek": 216232, "start": 2162.4, "end": 2168.8, "text": " stuck. Right, maybe eventually we still stagnate, but it'd be pretty crazy if you added millions of", "tokens": [50368, 5541, 13, 1779, 11, 1310, 4728, 321, 920, 32853, 473, 11, 457, 309, 1116, 312, 1238, 3219, 498, 291, 3869, 6803, 295, 50688], "temperature": 0.0, "avg_logprob": -0.12466015862029733, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.008287063799798489}, {"id": 366, "seek": 216232, "start": 2168.8, "end": 2174.4, "text": " people or millions of brains to the workforce and didn't get a bunch more technological progress.", "tokens": [50688, 561, 420, 6803, 295, 15442, 281, 264, 14201, 293, 994, 380, 483, 257, 3840, 544, 18439, 4205, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12466015862029733, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.008287063799798489}, {"id": 367, "seek": 216232, "start": 2175.28, "end": 2178.8, "text": " Yeah, I mean, specifically if you made the workforce like a hundred times as big,", "tokens": [51012, 865, 11, 286, 914, 11, 4682, 498, 291, 1027, 264, 14201, 411, 257, 3262, 1413, 382, 955, 11, 51188], "temperature": 0.0, "avg_logprob": -0.12466015862029733, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.008287063799798489}, {"id": 368, "seek": 216232, "start": 2179.44, "end": 2185.36, "text": " then yeah, especially with the other advantages I discussed about running 10 times as fast and", "tokens": [51220, 550, 1338, 11, 2318, 365, 264, 661, 14906, 286, 7152, 466, 2614, 1266, 1413, 382, 2370, 293, 51516], "temperature": 0.0, "avg_logprob": -0.12466015862029733, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.008287063799798489}, {"id": 369, "seek": 216232, "start": 2186.4, "end": 2190.1600000000003, "text": " you know, being really focused on the most important tasks, I think really surprising", "tokens": [51568, 291, 458, 11, 885, 534, 5178, 322, 264, 881, 1021, 9608, 11, 286, 519, 534, 8830, 51756], "temperature": 0.0, "avg_logprob": -0.12466015862029733, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.008287063799798489}, {"id": 370, "seek": 219016, "start": 2190.16, "end": 2194.3199999999997, "text": " if you don't get at least a temporary increase. In fact, I don't think it would be temporary,", "tokens": [50364, 498, 291, 500, 380, 483, 412, 1935, 257, 13413, 3488, 13, 682, 1186, 11, 286, 500, 380, 519, 309, 576, 312, 13413, 11, 50572], "temperature": 0.0, "avg_logprob": -0.06931379262138815, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.016037512570619583}, {"id": 371, "seek": 219016, "start": 2194.3199999999997, "end": 2201.92, "text": " and that's because one of the things that AIs can work on is actually building more computer", "tokens": [50572, 293, 300, 311, 570, 472, 295, 264, 721, 300, 316, 6802, 393, 589, 322, 307, 767, 2390, 544, 3820, 50952], "temperature": 0.0, "avg_logprob": -0.06931379262138815, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.016037512570619583}, {"id": 372, "seek": 219016, "start": 2201.92, "end": 2207.52, "text": " chips to run AIs on, improving the kind of hardware designs for those computer chips,", "tokens": [50952, 11583, 281, 1190, 316, 6802, 322, 11, 11470, 264, 733, 295, 8837, 11347, 337, 729, 3820, 11583, 11, 51232], "temperature": 0.0, "avg_logprob": -0.06931379262138815, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.016037512570619583}, {"id": 373, "seek": 219016, "start": 2207.52, "end": 2211.52, "text": " improving the algorithms that AIs run on, improving the designs of robots.", "tokens": [51232, 11470, 264, 14642, 300, 316, 6802, 1190, 322, 11, 11470, 264, 11347, 295, 14733, 13, 51432], "temperature": 0.0, "avg_logprob": -0.06931379262138815, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.016037512570619583}, {"id": 374, "seek": 219016, "start": 2211.52, "end": 2217.04, "text": " Just making themselves better scientists. Exactly, and so we've been discussing how", "tokens": [51432, 1449, 1455, 2969, 1101, 7708, 13, 7587, 11, 293, 370, 321, 600, 668, 10850, 577, 51708], "temperature": 0.0, "avg_logprob": -0.06931379262138815, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.016037512570619583}, {"id": 375, "seek": 221704, "start": 2217.6, "end": 2221.68, "text": " already the pace of progress in terms of the algorithms and the hardware is pretty fast,", "tokens": [50392, 1217, 264, 11638, 295, 4205, 294, 2115, 295, 264, 14642, 293, 264, 8837, 307, 1238, 2370, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10125129039470966, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001918685156852007}, {"id": 376, "seek": 221704, "start": 2222.56, "end": 2226.32, "text": " and I've already said that I expect it to be much faster once we have AGI.", "tokens": [50640, 293, 286, 600, 1217, 848, 300, 286, 2066, 309, 281, 312, 709, 4663, 1564, 321, 362, 316, 26252, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10125129039470966, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001918685156852007}, {"id": 377, "seek": 221704, "start": 2226.32, "end": 2231.52, "text": " And so really this isn't a kind of a constant sized AI and robotics workforce we're talking", "tokens": [50828, 400, 370, 534, 341, 1943, 380, 257, 733, 295, 257, 5754, 20004, 7318, 293, 34145, 14201, 321, 434, 1417, 51088], "temperature": 0.0, "avg_logprob": -0.10125129039470966, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001918685156852007}, {"id": 378, "seek": 221704, "start": 2231.52, "end": 2237.2, "text": " about here, but if we choose to do so, then we could have the size of that workforce", "tokens": [51088, 466, 510, 11, 457, 498, 321, 2826, 281, 360, 370, 11, 550, 321, 727, 362, 264, 2744, 295, 300, 14201, 51372], "temperature": 0.0, "avg_logprob": -0.10125129039470966, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001918685156852007}, {"id": 379, "seek": 221704, "start": 2238.0, "end": 2243.84, "text": " doubling every year very easily. And so that means you can overcome this problem of ideas", "tokens": [51412, 33651, 633, 1064, 588, 3612, 13, 400, 370, 300, 1355, 291, 393, 10473, 341, 1154, 295, 3487, 51704], "temperature": 0.0, "avg_logprob": -0.10125129039470966, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.001918685156852007}, {"id": 380, "seek": 224384, "start": 2243.92, "end": 2250.0, "text": " getting harder to find, because you're not kind of dealing with a constant or slowly growing", "tokens": [50368, 1242, 6081, 281, 915, 11, 570, 291, 434, 406, 733, 295, 6260, 365, 257, 5754, 420, 5692, 4194, 50672], "temperature": 0.0, "avg_logprob": -0.10232362747192383, "compression_ratio": 2.0128755364806867, "no_speech_prob": 0.03454558178782463}, {"id": 381, "seek": 224384, "start": 2250.7200000000003, "end": 2256.8, "text": " workforce. You're dealing with a workforce which is itself rapidly increasing in size,", "tokens": [50708, 14201, 13, 509, 434, 6260, 365, 257, 14201, 597, 307, 2564, 12910, 5662, 294, 2744, 11, 51012], "temperature": 0.0, "avg_logprob": -0.10232362747192383, "compression_ratio": 2.0128755364806867, "no_speech_prob": 0.03454558178782463}, {"id": 382, "seek": 224384, "start": 2256.8, "end": 2262.4, "text": " and so even if ideas are getting harder to find, you've got a bigger and bigger workforce to find", "tokens": [51012, 293, 370, 754, 498, 3487, 366, 1242, 6081, 281, 915, 11, 291, 600, 658, 257, 3801, 293, 3801, 14201, 281, 915, 51292], "temperature": 0.0, "avg_logprob": -0.10232362747192383, "compression_ratio": 2.0128755364806867, "no_speech_prob": 0.03454558178782463}, {"id": 383, "seek": 224384, "start": 2262.4, "end": 2267.92, "text": " it. And you can actually model out this dynamic. You can take the kind of the best models we have", "tokens": [51292, 309, 13, 400, 291, 393, 767, 2316, 484, 341, 8546, 13, 509, 393, 747, 264, 733, 295, 264, 1151, 5245, 321, 362, 51568], "temperature": 0.0, "avg_logprob": -0.10232362747192383, "compression_ratio": 2.0128755364806867, "no_speech_prob": 0.03454558178782463}, {"id": 384, "seek": 224384, "start": 2267.92, "end": 2271.76, "text": " where ideas are getting harder to find, and you can say, well, if ideas are getting harder to", "tokens": [51568, 689, 3487, 366, 1242, 6081, 281, 915, 11, 293, 291, 393, 584, 11, 731, 11, 498, 3487, 366, 1242, 6081, 281, 51760], "temperature": 0.0, "avg_logprob": -0.10232362747192383, "compression_ratio": 2.0128755364806867, "no_speech_prob": 0.03454558178782463}, {"id": 385, "seek": 227176, "start": 2271.76, "end": 2278.4, "text": " find the one hand, but on the other hand, AIs are able to design better AIs and do all the", "tokens": [50364, 915, 264, 472, 1011, 11, 457, 322, 264, 661, 1011, 11, 316, 6802, 366, 1075, 281, 1715, 1101, 316, 6802, 293, 360, 439, 264, 50696], "temperature": 0.0, "avg_logprob": -0.09651976823806763, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017363029764965177}, {"id": 386, "seek": 227176, "start": 2278.4, "end": 2282.96, "text": " improvements that I talked about designing better robots, etc. How does that dynamic play out?", "tokens": [50696, 13797, 300, 286, 2825, 466, 14685, 1101, 14733, 11, 5183, 13, 1012, 775, 300, 8546, 862, 484, 30, 50924], "temperature": 0.0, "avg_logprob": -0.09651976823806763, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017363029764965177}, {"id": 387, "seek": 227176, "start": 2283.84, "end": 2290.7200000000003, "text": " And it turns out that at least under these models, even if ideas are getting harder to find at kind", "tokens": [50968, 400, 309, 4523, 484, 300, 412, 1935, 833, 613, 5245, 11, 754, 498, 3487, 366, 1242, 6081, 281, 915, 412, 733, 51312], "temperature": 0.0, "avg_logprob": -0.09651976823806763, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017363029764965177}, {"id": 388, "seek": 227176, "start": 2290.7200000000003, "end": 2297.5200000000004, "text": " of a very steep rate, then you still are going to get the kind of the AIs and robots winning that", "tokens": [51312, 295, 257, 588, 16841, 3314, 11, 550, 291, 920, 366, 516, 281, 483, 264, 733, 295, 264, 316, 6802, 293, 14733, 8224, 300, 51652], "temperature": 0.0, "avg_logprob": -0.09651976823806763, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0017363029764965177}, {"id": 389, "seek": 229752, "start": 2297.52, "end": 2303.52, "text": " race. That's really wild. I guess those AI scientists might hit some limits, and do you", "tokens": [50364, 4569, 13, 663, 311, 534, 4868, 13, 286, 2041, 729, 7318, 7708, 1062, 2045, 512, 10406, 11, 293, 360, 291, 50664], "temperature": 0.0, "avg_logprob": -0.0799456144634046, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.01792365126311779}, {"id": 390, "seek": 229752, "start": 2303.52, "end": 2309.6, "text": " have any ideas for what those might end up being? It's a great question. I think we are going to hit", "tokens": [50664, 362, 604, 3487, 337, 437, 729, 1062, 917, 493, 885, 30, 467, 311, 257, 869, 1168, 13, 286, 519, 321, 366, 516, 281, 2045, 50968], "temperature": 0.0, "avg_logprob": -0.0799456144634046, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.01792365126311779}, {"id": 391, "seek": 229752, "start": 2309.6, "end": 2314.08, "text": " limits at some point. Eventually we won't be able to design better technologies. Eventually we'll have", "tokens": [50968, 10406, 412, 512, 935, 13, 17586, 321, 1582, 380, 312, 1075, 281, 1715, 1101, 7943, 13, 17586, 321, 603, 362, 51192], "temperature": 0.0, "avg_logprob": -0.0799456144634046, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.01792365126311779}, {"id": 392, "seek": 229752, "start": 2314.08, "end": 2319.6, "text": " the best algorithms we can get for making AIs. I do think there are reasons to think the limits", "tokens": [51192, 264, 1151, 14642, 321, 393, 483, 337, 1455, 316, 6802, 13, 286, 360, 519, 456, 366, 4112, 281, 519, 264, 10406, 51468], "temperature": 0.0, "avg_logprob": -0.0799456144634046, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.01792365126311779}, {"id": 393, "seek": 231960, "start": 2319.68, "end": 2329.8399999999997, "text": " could be quite high. One interesting data point is that small animals are able to double their", "tokens": [50368, 727, 312, 1596, 1090, 13, 1485, 1880, 1412, 935, 307, 300, 1359, 4882, 366, 1075, 281, 3834, 641, 50876], "temperature": 0.0, "avg_logprob": -0.08677167892456054, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.15352977812290192}, {"id": 394, "seek": 231960, "start": 2329.8399999999997, "end": 2336.72, "text": " population size in just a few months, and even smaller animals like insects can double their", "tokens": [50876, 4415, 2744, 294, 445, 257, 1326, 2493, 11, 293, 754, 4356, 4882, 411, 20201, 393, 3834, 641, 51220], "temperature": 0.0, "avg_logprob": -0.08677167892456054, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.15352977812290192}, {"id": 395, "seek": 231960, "start": 2336.72, "end": 2343.12, "text": " population size in just days or weeks. That shows that it is physically possible to have", "tokens": [51220, 4415, 2744, 294, 445, 1708, 420, 3259, 13, 663, 3110, 300, 309, 307, 9762, 1944, 281, 362, 51540], "temperature": 0.0, "avg_logprob": -0.08677167892456054, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.15352977812290192}, {"id": 396, "seek": 234312, "start": 2343.12, "end": 2349.7599999999998, "text": " a certain kind of biological robot that kind of doubles its own number in the scale of weeks or", "tokens": [50364, 257, 1629, 733, 295, 13910, 7881, 300, 733, 295, 31634, 1080, 1065, 1230, 294, 264, 4373, 295, 3259, 420, 50696], "temperature": 0.0, "avg_logprob": -0.11671450452984504, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.10172174870967865}, {"id": 397, "seek": 234312, "start": 2349.7599999999998, "end": 2355.92, "text": " months. And with all of this kind of massive scientific effort that we've been describing,", "tokens": [50696, 2493, 13, 400, 365, 439, 295, 341, 733, 295, 5994, 8134, 4630, 300, 321, 600, 668, 16141, 11, 51004], "temperature": 0.0, "avg_logprob": -0.11671450452984504, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.10172174870967865}, {"id": 398, "seek": 234312, "start": 2356.72, "end": 2362.16, "text": " it seems possible that we'll be able to design kind of robots of our own that are able to", "tokens": [51044, 309, 2544, 1944, 300, 321, 603, 312, 1075, 281, 1715, 733, 295, 14733, 295, 527, 1065, 300, 366, 1075, 281, 51316], "temperature": 0.0, "avg_logprob": -0.11671450452984504, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.10172174870967865}, {"id": 399, "seek": 234312, "start": 2362.16, "end": 2367.2, "text": " kind of double their own number, kind of build replacement robots in a similar timeframe.", "tokens": [51316, 733, 295, 3834, 641, 1065, 1230, 11, 733, 295, 1322, 14419, 14733, 294, 257, 2531, 34830, 13, 51568], "temperature": 0.0, "avg_logprob": -0.11671450452984504, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.10172174870967865}, {"id": 400, "seek": 234312, "start": 2367.92, "end": 2372.48, "text": " And so currently you can try and estimate, well, if we used a factory to try and build", "tokens": [51604, 400, 370, 4362, 291, 393, 853, 293, 12539, 11, 731, 11, 498, 321, 1143, 257, 9265, 281, 853, 293, 1322, 51832], "temperature": 0.0, "avg_logprob": -0.11671450452984504, "compression_ratio": 1.8266129032258065, "no_speech_prob": 0.10172174870967865}, {"id": 401, "seek": 237248, "start": 2372.56, "end": 2377.28, "text": " another copy of itself, how long would that take? I haven't seen a good analysis of this,", "tokens": [50368, 1071, 5055, 295, 2564, 11, 577, 938, 576, 300, 747, 30, 286, 2378, 380, 1612, 257, 665, 5215, 295, 341, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10315004695545543, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.005137874279171228}, {"id": 402, "seek": 237248, "start": 2377.28, "end": 2383.04, "text": " but when I've spoken to people, they've guessed it on the order of months. So that also supports", "tokens": [50604, 457, 562, 286, 600, 10759, 281, 561, 11, 436, 600, 21852, 309, 322, 264, 1668, 295, 2493, 13, 407, 300, 611, 9346, 50892], "temperature": 0.0, "avg_logprob": -0.10315004695545543, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.005137874279171228}, {"id": 403, "seek": 237248, "start": 2383.04, "end": 2387.04, "text": " this vague idea that it might be possible to get these robots that are able to kind of build", "tokens": [50892, 341, 24247, 1558, 300, 309, 1062, 312, 1944, 281, 483, 613, 14733, 300, 366, 1075, 281, 733, 295, 1322, 51092], "temperature": 0.0, "avg_logprob": -0.10315004695545543, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.005137874279171228}, {"id": 404, "seek": 237248, "start": 2387.04, "end": 2390.16, "text": " extra copies of themselves and double their own number in just a number of months.", "tokens": [51092, 2857, 14341, 295, 2969, 293, 3834, 641, 1065, 1230, 294, 445, 257, 1230, 295, 2493, 13, 51248], "temperature": 0.0, "avg_logprob": -0.10315004695545543, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.005137874279171228}, {"id": 405, "seek": 237248, "start": 2390.96, "end": 2398.08, "text": " So what that all suggests is that we could have a robot workforce which is growing at a really,", "tokens": [51288, 407, 437, 300, 439, 13409, 307, 300, 321, 727, 362, 257, 7881, 14201, 597, 307, 4194, 412, 257, 534, 11, 51644], "temperature": 0.0, "avg_logprob": -0.10315004695545543, "compression_ratio": 1.6715328467153285, "no_speech_prob": 0.005137874279171228}, {"id": 406, "seek": 239808, "start": 2398.08, "end": 2404.16, "text": " really high rate. And by robot, I'm picturing physical bodies, but do you do basically just", "tokens": [50364, 534, 1090, 3314, 13, 400, 538, 7881, 11, 286, 478, 2317, 1345, 4001, 7510, 11, 457, 360, 291, 360, 1936, 445, 50668], "temperature": 0.0, "avg_logprob": -0.07837224280697176, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.028465893119573593}, {"id": 407, "seek": 239808, "start": 2404.16, "end": 2410.4, "text": " mean AI system that's like working on science? Or do you think physical bodies end up being", "tokens": [50668, 914, 7318, 1185, 300, 311, 411, 1364, 322, 3497, 30, 1610, 360, 291, 519, 4001, 7510, 917, 493, 885, 50980], "temperature": 0.0, "avg_logprob": -0.07837224280697176, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.028465893119573593}, {"id": 408, "seek": 239808, "start": 2411.44, "end": 2416.3199999999997, "text": " important because we've got to start automating some of these physical tasks in addition to the", "tokens": [51032, 1021, 570, 321, 600, 658, 281, 722, 3553, 990, 512, 295, 613, 4001, 9608, 294, 4500, 281, 264, 51276], "temperature": 0.0, "avg_logprob": -0.07837224280697176, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.028465893119573593}, {"id": 409, "seek": 239808, "start": 2416.3199999999997, "end": 2421.68, "text": " cognitive tasks? I'm thinking, yeah, including the physical tasks when I'm talking about the", "tokens": [51276, 15605, 9608, 30, 286, 478, 1953, 11, 1338, 11, 3009, 264, 4001, 9608, 562, 286, 478, 1417, 466, 264, 51544], "temperature": 0.0, "avg_logprob": -0.07837224280697176, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.028465893119573593}, {"id": 410, "seek": 242168, "start": 2421.68, "end": 2430.3999999999996, "text": " robots. I mean, if these robots each weighed 50 kilograms, and we're able to produce robots as", "tokens": [50364, 14733, 13, 286, 914, 11, 498, 613, 14733, 1184, 32844, 2625, 30690, 11, 293, 321, 434, 1075, 281, 5258, 14733, 382, 50800], "temperature": 0.0, "avg_logprob": -0.10368708504570855, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.07992296665906906}, {"id": 411, "seek": 242168, "start": 2430.3999999999996, "end": 2434.8799999999997, "text": " many robots in a year, such that they weighed as much as all the cars that we currently produce in", "tokens": [50800, 867, 14733, 294, 257, 1064, 11, 1270, 300, 436, 32844, 382, 709, 382, 439, 264, 5163, 300, 321, 4362, 5258, 294, 51024], "temperature": 0.0, "avg_logprob": -0.10368708504570855, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.07992296665906906}, {"id": 412, "seek": 242168, "start": 2434.8799999999997, "end": 2443.7599999999998, "text": " a year, then we'd be producing around a billion robots each year. So already the manufacturing", "tokens": [51024, 257, 1064, 11, 550, 321, 1116, 312, 10501, 926, 257, 5218, 14733, 1184, 1064, 13, 407, 1217, 264, 11096, 51468], "temperature": 0.0, "avg_logprob": -0.10368708504570855, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.07992296665906906}, {"id": 413, "seek": 242168, "start": 2443.7599999999998, "end": 2450.56, "text": " capacity seems like it is theoretically there to produce a huge number of robots. And that's before", "tokens": [51468, 6042, 2544, 411, 309, 307, 29400, 456, 281, 5258, 257, 2603, 1230, 295, 14733, 13, 400, 300, 311, 949, 51808], "temperature": 0.0, "avg_logprob": -0.10368708504570855, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.07992296665906906}, {"id": 414, "seek": 245056, "start": 2450.56, "end": 2456.88, "text": " taking into account that it's lucrative. And so we want to create more. And yeah, unreal.", "tokens": [50364, 1940, 666, 2696, 300, 309, 311, 21296, 30457, 13, 400, 370, 321, 528, 281, 1884, 544, 13, 400, 1338, 11, 25754, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1389344256857167, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.003374850144609809}, {"id": 415, "seek": 245056, "start": 2456.88, "end": 2462.08, "text": " So I do think that this this dynamic leaves us in a pretty crazy world where the size of the", "tokens": [50680, 407, 286, 360, 519, 300, 341, 341, 8546, 5510, 505, 294, 257, 1238, 3219, 1002, 689, 264, 2744, 295, 264, 50940], "temperature": 0.0, "avg_logprob": -0.1389344256857167, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.003374850144609809}, {"id": 416, "seek": 245056, "start": 2462.64, "end": 2469.2, "text": " AI and robotics workforce is growing, potentially going very, very quickly. And probably as a result,", "tokens": [50968, 7318, 293, 34145, 14201, 307, 4194, 11, 7263, 516, 588, 11, 588, 2661, 13, 400, 1391, 382, 257, 1874, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1389344256857167, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.003374850144609809}, {"id": 417, "seek": 245056, "start": 2470.08, "end": 2476.7999999999997, "text": " it's hard for me to imagine technology really stalling out before we hit real kind of limits,", "tokens": [51340, 309, 311, 1152, 337, 385, 281, 3811, 2899, 534, 19633, 278, 484, 949, 321, 2045, 957, 733, 295, 10406, 11, 51676], "temperature": 0.0, "avg_logprob": -0.1389344256857167, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.003374850144609809}, {"id": 418, "seek": 247680, "start": 2476.88, "end": 2481.76, "text": " kind of fundamental limits to how good technology can get. And there will be such limits. Technology", "tokens": [50368, 733, 295, 8088, 10406, 281, 577, 665, 2899, 393, 483, 13, 400, 456, 486, 312, 1270, 10406, 13, 15037, 50612], "temperature": 0.0, "avg_logprob": -0.1137926283336821, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.00854523666203022}, {"id": 419, "seek": 247680, "start": 2481.76, "end": 2487.52, "text": " can't improve forever to infinity. Ultimately, you've come up with the very best ways of", "tokens": [50612, 393, 380, 3470, 5680, 281, 13202, 13, 23921, 11, 291, 600, 808, 493, 365, 264, 588, 1151, 2098, 295, 50900], "temperature": 0.0, "avg_logprob": -0.1137926283336821, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.00854523666203022}, {"id": 420, "seek": 247680, "start": 2487.52, "end": 2491.36, "text": " arranging the molecules to get the desired technological behavior.", "tokens": [50900, 5539, 9741, 264, 13093, 281, 483, 264, 14721, 18439, 5223, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1137926283336821, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.00854523666203022}, {"id": 421, "seek": 247680, "start": 2491.92, "end": 2499.04, "text": " Right. Okay. Do we ever hit limits on just physical stuff, the stuff we make the robots out of?", "tokens": [51120, 1779, 13, 1033, 13, 1144, 321, 1562, 2045, 10406, 322, 445, 4001, 1507, 11, 264, 1507, 321, 652, 264, 14733, 484, 295, 30, 51476], "temperature": 0.0, "avg_logprob": -0.1137926283336821, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.00854523666203022}, {"id": 422, "seek": 247680, "start": 2499.04, "end": 2504.88, "text": " I think we will. So I already said about the kind of that car statistics suggesting we can get pretty", "tokens": [51476, 286, 519, 321, 486, 13, 407, 286, 1217, 848, 466, 264, 733, 295, 300, 1032, 12523, 18094, 321, 393, 483, 1238, 51768], "temperature": 0.0, "avg_logprob": -0.1137926283336821, "compression_ratio": 1.6214285714285714, "no_speech_prob": 0.00854523666203022}, {"id": 423, "seek": 250488, "start": 2504.88, "end": 2510.1600000000003, "text": " far just in terms of the massive robots we could produce just with the kind of manufacturing", "tokens": [50364, 1400, 445, 294, 2115, 295, 264, 5994, 14733, 321, 727, 5258, 445, 365, 264, 733, 295, 11096, 50628], "temperature": 0.0, "avg_logprob": -0.09629640125093006, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0032583598513156176}, {"id": 424, "seek": 250488, "start": 2510.1600000000003, "end": 2518.7200000000003, "text": " capabilities we already have. The earth is is massive. There are kind of mountains with all", "tokens": [50628, 10862, 321, 1217, 362, 13, 440, 4120, 307, 307, 5994, 13, 821, 366, 733, 295, 10233, 365, 439, 51056], "temperature": 0.0, "avg_logprob": -0.09629640125093006, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0032583598513156176}, {"id": 425, "seek": 250488, "start": 2518.7200000000003, "end": 2524.96, "text": " these kinds of materials in them all over the place. And if we run out of a particular material,", "tokens": [51056, 613, 3685, 295, 5319, 294, 552, 439, 670, 264, 1081, 13, 400, 498, 321, 1190, 484, 295, 257, 1729, 2527, 11, 51368], "temperature": 0.0, "avg_logprob": -0.09629640125093006, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0032583598513156176}, {"id": 426, "seek": 250488, "start": 2525.6, "end": 2531.04, "text": " which is currently useful for building robots, then these billions of AI as we have will be", "tokens": [51400, 597, 307, 4362, 4420, 337, 2390, 14733, 11, 550, 613, 17375, 295, 7318, 382, 321, 362, 486, 312, 51672], "temperature": 0.0, "avg_logprob": -0.09629640125093006, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.0032583598513156176}, {"id": 427, "seek": 253104, "start": 2531.04, "end": 2536.8, "text": " working hard to find ways of doing without that scarce material. And that's been a common pattern", "tokens": [50364, 1364, 1152, 281, 915, 2098, 295, 884, 1553, 300, 41340, 2527, 13, 400, 300, 311, 668, 257, 2689, 5102, 50652], "temperature": 0.0, "avg_logprob": -0.12921769515327786, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.013352380134165287}, {"id": 428, "seek": 253104, "start": 2536.8, "end": 2541.2799999999997, "text": " in technological development that you kind of find ways to switch out of things that are scarce.", "tokens": [50652, 294, 18439, 3250, 300, 291, 733, 295, 915, 2098, 281, 3679, 484, 295, 721, 300, 366, 41340, 13, 50876], "temperature": 0.0, "avg_logprob": -0.12921769515327786, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.013352380134165287}, {"id": 429, "seek": 253104, "start": 2541.92, "end": 2546.88, "text": " So, yeah, right, you know, it's hard to rule out that there's just some material that we just", "tokens": [50908, 407, 11, 1338, 11, 558, 11, 291, 458, 11, 309, 311, 1152, 281, 4978, 484, 300, 456, 311, 445, 512, 2527, 300, 321, 445, 51156], "temperature": 0.0, "avg_logprob": -0.12921769515327786, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.013352380134165287}, {"id": 430, "seek": 253104, "start": 2546.88, "end": 2551.36, "text": " absolutely need and we can't do without and that that all next things, you know, maybe once we get", "tokens": [51156, 3122, 643, 293, 321, 393, 380, 360, 1553, 293, 300, 300, 439, 958, 721, 11, 291, 458, 11, 1310, 1564, 321, 483, 51380], "temperature": 0.0, "avg_logprob": -0.12921769515327786, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.013352380134165287}, {"id": 431, "seek": 253104, "start": 2551.36, "end": 2557.36, "text": " to 100 billion robots or something. But it also just seems more likely to me that given just the", "tokens": [51380, 281, 2319, 5218, 14733, 420, 746, 13, 583, 309, 611, 445, 2544, 544, 3700, 281, 385, 300, 2212, 445, 264, 51680], "temperature": 0.0, "avg_logprob": -0.12921769515327786, "compression_ratio": 1.7728937728937728, "no_speech_prob": 0.013352380134165287}, {"id": 432, "seek": 255736, "start": 2557.36, "end": 2562.56, "text": " abundance of just kind of materials that are in the earth, it's, you know, it's a big place.", "tokens": [50364, 23391, 295, 445, 733, 295, 5319, 300, 366, 294, 264, 4120, 11, 309, 311, 11, 291, 458, 11, 309, 311, 257, 955, 1081, 13, 50624], "temperature": 0.0, "avg_logprob": -0.10844517523242582, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.001573378685861826}, {"id": 433, "seek": 255736, "start": 2562.56, "end": 2566.32, "text": " There's lots of different stuff there. We, it's not like we've mined everything there was to mine", "tokens": [50624, 821, 311, 3195, 295, 819, 1507, 456, 13, 492, 11, 309, 311, 406, 411, 321, 600, 923, 292, 1203, 456, 390, 281, 3892, 50812], "temperature": 0.0, "avg_logprob": -0.10844517523242582, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.001573378685861826}, {"id": 434, "seek": 255736, "start": 2566.32, "end": 2571.6, "text": " or even close to it. Right. We'll be doing, you know, using all the best methods for recycling and", "tokens": [50812, 420, 754, 1998, 281, 309, 13, 1779, 13, 492, 603, 312, 884, 11, 291, 458, 11, 1228, 439, 264, 1151, 7150, 337, 23363, 293, 51076], "temperature": 0.0, "avg_logprob": -0.10844517523242582, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.001573378685861826}, {"id": 435, "seek": 255736, "start": 2571.6, "end": 2576.2400000000002, "text": " using things as efficiently as possible. It doesn't seem to me like those kinds of bottlenecks are", "tokens": [51076, 1228, 721, 382, 19621, 382, 1944, 13, 467, 1177, 380, 1643, 281, 385, 411, 729, 3685, 295, 44641, 2761, 366, 51308], "temperature": 0.0, "avg_logprob": -0.10844517523242582, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.001573378685861826}, {"id": 436, "seek": 255736, "start": 2576.2400000000002, "end": 2581.84, "text": " going to kick in particularly early. And it's not out of the question that we'd have the technology", "tokens": [51308, 516, 281, 4437, 294, 4098, 2440, 13, 400, 309, 311, 406, 484, 295, 264, 1168, 300, 321, 1116, 362, 264, 2899, 51588], "temperature": 0.0, "avg_logprob": -0.10844517523242582, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.001573378685861826}, {"id": 437, "seek": 258184, "start": 2581.84, "end": 2588.0, "text": " to explore resources in space. I mean, that's even I'm adding more sci-fi here. But like,", "tokens": [50364, 281, 6839, 3593, 294, 1901, 13, 286, 914, 11, 300, 311, 754, 286, 478, 5127, 544, 2180, 12, 13325, 510, 13, 583, 411, 11, 50672], "temperature": 0.0, "avg_logprob": -0.10887249479902551, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.006282109301537275}, {"id": 438, "seek": 258184, "start": 2588.7200000000003, "end": 2595.36, "text": " if we're doubling technological progress every, I mean, it sounds like you're talking about", "tokens": [50708, 498, 321, 434, 33651, 18439, 4205, 633, 11, 286, 914, 11, 309, 3263, 411, 291, 434, 1417, 466, 51040], "temperature": 0.0, "avg_logprob": -0.10887249479902551, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.006282109301537275}, {"id": 439, "seek": 258184, "start": 2596.08, "end": 2602.08, "text": " months at some point. Yeah, I think months is plausible. Okay, months is plausible. And so", "tokens": [51076, 2493, 412, 512, 935, 13, 865, 11, 286, 519, 2493, 307, 39925, 13, 1033, 11, 2493, 307, 39925, 13, 400, 370, 51376], "temperature": 0.0, "avg_logprob": -0.10887249479902551, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.006282109301537275}, {"id": 440, "seek": 258184, "start": 2602.08, "end": 2608.1600000000003, "text": " that might mean we, we aren't limited by earthly limitations. That's right. There is an interesting", "tokens": [51376, 300, 1062, 914, 321, 11, 321, 3212, 380, 5567, 538, 46262, 15705, 13, 663, 311, 558, 13, 821, 307, 364, 1880, 51680], "temperature": 0.0, "avg_logprob": -0.10887249479902551, "compression_ratio": 1.5965665236051503, "no_speech_prob": 0.006282109301537275}, {"id": 441, "seek": 260816, "start": 2608.16, "end": 2614.64, "text": " dynamic there where if we are kind of doubling the number of robots really quickly, and we're", "tokens": [50364, 8546, 456, 689, 498, 321, 366, 733, 295, 33651, 264, 1230, 295, 14733, 534, 2661, 11, 293, 321, 434, 50688], "temperature": 0.0, "avg_logprob": -0.08688539569660769, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.019431816413998604}, {"id": 442, "seek": 260816, "start": 2614.64, "end": 2620.3199999999997, "text": " improving technology really quickly, then we kind of, we're not that interested in doing an activity", "tokens": [50688, 11470, 2899, 534, 2661, 11, 550, 321, 733, 295, 11, 321, 434, 406, 300, 3102, 294, 884, 364, 5191, 50972], "temperature": 0.0, "avg_logprob": -0.08688539569660769, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.019431816413998604}, {"id": 443, "seek": 260816, "start": 2620.3199999999997, "end": 2626.08, "text": " which takes maybe like 10 years to bear fruit, because we're used to kind of our investments", "tokens": [50972, 597, 2516, 1310, 411, 1266, 924, 281, 6155, 6773, 11, 570, 321, 434, 1143, 281, 733, 295, 527, 13784, 51260], "temperature": 0.0, "avg_logprob": -0.08688539569660769, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.019431816413998604}, {"id": 444, "seek": 260816, "start": 2626.08, "end": 2629.92, "text": " paying off with kind of doublings every year. We're like, oh, we could go to the moon and get", "tokens": [51260, 6229, 766, 365, 733, 295, 2482, 5199, 1109, 633, 1064, 13, 492, 434, 411, 11, 1954, 11, 321, 727, 352, 281, 264, 7135, 293, 483, 51452], "temperature": 0.0, "avg_logprob": -0.08688539569660769, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.019431816413998604}, {"id": 445, "seek": 260816, "start": 2629.92, "end": 2636.3199999999997, "text": " materials, but man, like that would take so long. If we, if we just invest in everything we can find", "tokens": [51452, 5319, 11, 457, 587, 11, 411, 300, 576, 747, 370, 938, 13, 759, 321, 11, 498, 321, 445, 1963, 294, 1203, 321, 393, 915, 51772], "temperature": 0.0, "avg_logprob": -0.08688539569660769, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.019431816413998604}, {"id": 446, "seek": 263632, "start": 2636.32, "end": 2642.0800000000004, "text": " on earth, we can much more quickly kind of increase the just use it more efficiently. Yeah, so that", "tokens": [50364, 322, 4120, 11, 321, 393, 709, 544, 2661, 733, 295, 3488, 264, 445, 764, 309, 544, 19621, 13, 865, 11, 370, 300, 50652], "temperature": 0.0, "avg_logprob": -0.08178323827764039, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.005737061612308025}, {"id": 447, "seek": 263632, "start": 2642.0800000000004, "end": 2647.84, "text": " time delay becomes more significant when, when you're already able to go so fast. So I think I", "tokens": [50652, 565, 8577, 3643, 544, 4776, 562, 11, 562, 291, 434, 1217, 1075, 281, 352, 370, 2370, 13, 407, 286, 519, 286, 50940], "temperature": 0.0, "avg_logprob": -0.08178323827764039, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.005737061612308025}, {"id": 448, "seek": 263632, "start": 2647.84, "end": 2652.2400000000002, "text": " imagine the kind of going to the moon and the space stuff happens when we're really kind of", "tokens": [50940, 3811, 264, 733, 295, 516, 281, 264, 7135, 293, 264, 1901, 1507, 2314, 562, 321, 434, 534, 733, 295, 51160], "temperature": 0.0, "avg_logprob": -0.08178323827764039, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.005737061612308025}, {"id": 449, "seek": 263632, "start": 2652.2400000000002, "end": 2657.6800000000003, "text": " struggling to find ways to make use of the earth resources. Okay, so it sounds like we're kind", "tokens": [51160, 9314, 281, 915, 2098, 281, 652, 764, 295, 264, 4120, 3593, 13, 1033, 11, 370, 309, 3263, 411, 321, 434, 733, 51432], "temperature": 0.0, "avg_logprob": -0.08178323827764039, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.005737061612308025}, {"id": 450, "seek": 265768, "start": 2657.7599999999998, "end": 2666.7999999999997, "text": " of talking about something like AI systems replace humans in a bunch of sectors,", "tokens": [50368, 295, 1417, 466, 746, 411, 7318, 3652, 7406, 6255, 294, 257, 3840, 295, 18373, 11, 50820], "temperature": 0.0, "avg_logprob": -0.102577384780435, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.20681129395961761}, {"id": 451, "seek": 265768, "start": 2667.6, "end": 2674.64, "text": " really during our lifetimes. And then like our lives really, really change quite radically.", "tokens": [50860, 534, 1830, 527, 4545, 302, 1532, 13, 400, 550, 411, 527, 2909, 534, 11, 534, 1319, 1596, 35508, 13, 51212], "temperature": 0.0, "avg_logprob": -0.102577384780435, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.20681129395961761}, {"id": 452, "seek": 265768, "start": 2674.64, "end": 2682.48, "text": " And very, very, very quickly. Yeah, I guess I just find that super weird. I think my brain is like,", "tokens": [51212, 400, 588, 11, 588, 11, 588, 2661, 13, 865, 11, 286, 2041, 286, 445, 915, 300, 1687, 3657, 13, 286, 519, 452, 3567, 307, 411, 11, 51604], "temperature": 0.0, "avg_logprob": -0.102577384780435, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.20681129395961761}, {"id": 453, "seek": 268248, "start": 2682.48, "end": 2688.72, "text": " no, I don't believe you. That's too weird. I don't like, I just can't imagine that happening.", "tokens": [50364, 572, 11, 286, 500, 380, 1697, 291, 13, 663, 311, 886, 3657, 13, 286, 500, 380, 411, 11, 286, 445, 393, 380, 3811, 300, 2737, 13, 50676], "temperature": 0.0, "avg_logprob": -0.07823049242251387, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020274123176932335}, {"id": 454, "seek": 268248, "start": 2689.28, "end": 2696.16, "text": " If we're, if we're saying this is happening in the early 2030s, I'll be, yeah, in my late 30s.", "tokens": [50704, 759, 321, 434, 11, 498, 321, 434, 1566, 341, 307, 2737, 294, 264, 2440, 28638, 82, 11, 286, 603, 312, 11, 1338, 11, 294, 452, 3469, 2217, 82, 13, 51048], "temperature": 0.0, "avg_logprob": -0.07823049242251387, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020274123176932335}, {"id": 455, "seek": 268248, "start": 2696.16, "end": 2701.6, "text": " And like all of a sudden, the world would be radically changing every year. And I like won't", "tokens": [51048, 400, 411, 439, 295, 257, 3990, 11, 264, 1002, 576, 312, 35508, 4473, 633, 1064, 13, 400, 286, 411, 1582, 380, 51320], "temperature": 0.0, "avg_logprob": -0.07823049242251387, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020274123176932335}, {"id": 456, "seek": 268248, "start": 2701.6, "end": 2710.4, "text": " be working. I agree. It seems really crazy. And I think it's very natural and understandable to", "tokens": [51320, 312, 1364, 13, 286, 3986, 13, 467, 2544, 534, 3219, 13, 400, 286, 519, 309, 311, 588, 3303, 293, 25648, 281, 51760], "temperature": 0.0, "avg_logprob": -0.07823049242251387, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.020274123176932335}, {"id": 457, "seek": 271040, "start": 2710.4, "end": 2717.44, "text": " just not believe it when you hear the arguments. And that would have been my initial reaction.", "tokens": [50364, 445, 406, 1697, 309, 562, 291, 1568, 264, 12869, 13, 400, 300, 576, 362, 668, 452, 5883, 5480, 13, 50716], "temperature": 0.0, "avg_logprob": -0.07672644698101541, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.027981463819742203}, {"id": 458, "seek": 271040, "start": 2718.4, "end": 2723.76, "text": " In terms of why I do now believe it, there's probably a few things which have changed. Probably", "tokens": [50764, 682, 2115, 295, 983, 286, 360, 586, 1697, 309, 11, 456, 311, 1391, 257, 1326, 721, 597, 362, 3105, 13, 9210, 51032], "temperature": 0.0, "avg_logprob": -0.07672644698101541, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.027981463819742203}, {"id": 459, "seek": 271040, "start": 2723.76, "end": 2730.48, "text": " I've just sat with these arguments for a few years. And just been like, I just do believe it.", "tokens": [51032, 286, 600, 445, 3227, 365, 613, 12869, 337, 257, 1326, 924, 13, 400, 445, 668, 411, 11, 286, 445, 360, 1697, 309, 13, 51368], "temperature": 0.0, "avg_logprob": -0.07672644698101541, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.027981463819742203}, {"id": 460, "seek": 271040, "start": 2730.48, "end": 2735.6, "text": " You know, I have discussions with people on either side of the debate. And I just find", "tokens": [51368, 509, 458, 11, 286, 362, 11088, 365, 561, 322, 2139, 1252, 295, 264, 7958, 13, 400, 286, 445, 915, 51624], "temperature": 0.0, "avg_logprob": -0.07672644698101541, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.027981463819742203}, {"id": 461, "seek": 273560, "start": 2735.6, "end": 2740.4, "text": " that people on one side just have thought it through much more. And I think what's at the", "tokens": [50364, 300, 561, 322, 472, 1252, 445, 362, 1194, 309, 807, 709, 544, 13, 400, 286, 519, 437, 311, 412, 264, 50604], "temperature": 0.0, "avg_logprob": -0.06607414823059642, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.02407698892056942}, {"id": 462, "seek": 273560, "start": 2740.4, "end": 2746.4, "text": " heart of it for me is that the human brain is a physical system. There's nothing magical about it.", "tokens": [50604, 1917, 295, 309, 337, 385, 307, 300, 264, 1952, 3567, 307, 257, 4001, 1185, 13, 821, 311, 1825, 12066, 466, 309, 13, 50904], "temperature": 0.0, "avg_logprob": -0.06607414823059642, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.02407698892056942}, {"id": 463, "seek": 273560, "start": 2747.36, "end": 2753.92, "text": " It isn't surprising that at some point, we develop machines that can do what the human", "tokens": [50952, 467, 1943, 380, 8830, 300, 412, 512, 935, 11, 321, 1499, 8379, 300, 393, 360, 437, 264, 1952, 51280], "temperature": 0.0, "avg_logprob": -0.06607414823059642, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.02407698892056942}, {"id": 464, "seek": 273560, "start": 2753.92, "end": 2760.56, "text": " brain can do at some point in the process of technological discovery. And to be honest,", "tokens": [51280, 3567, 393, 360, 412, 512, 935, 294, 264, 1399, 295, 18439, 12114, 13, 400, 281, 312, 3245, 11, 51612], "temperature": 0.0, "avg_logprob": -0.06607414823059642, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.02407698892056942}, {"id": 465, "seek": 273560, "start": 2760.56, "end": 2765.44, "text": " that happening the next couple of decades is kind of when you might expect it to happen naively.", "tokens": [51612, 300, 2737, 264, 958, 1916, 295, 7878, 307, 733, 295, 562, 291, 1062, 2066, 309, 281, 1051, 1667, 3413, 13, 51856], "temperature": 0.0, "avg_logprob": -0.06607414823059642, "compression_ratio": 1.7100371747211895, "no_speech_prob": 0.02407698892056942}, {"id": 466, "seek": 276560, "start": 2765.7599999999998, "end": 2773.68, "text": " We've had computers for 70-odd years. It's been a decade since we started pouring loads and loads", "tokens": [50372, 492, 600, 632, 10807, 337, 5285, 12, 378, 67, 924, 13, 467, 311, 668, 257, 10378, 1670, 321, 1409, 20450, 12668, 293, 12668, 50768], "temperature": 0.0, "avg_logprob": -0.10518242949146335, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.003437277628108859}, {"id": 467, "seek": 276560, "start": 2773.68, "end": 2780.0, "text": " of compute into training AI systems. And we've realized that that approach works really, really", "tokens": [50768, 295, 14722, 666, 3097, 7318, 3652, 13, 400, 321, 600, 5334, 300, 300, 3109, 1985, 534, 11, 534, 51084], "temperature": 0.0, "avg_logprob": -0.10518242949146335, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.003437277628108859}, {"id": 468, "seek": 276560, "start": 2780.0, "end": 2785.04, "text": " well. Just if you were kind of to say, okay, when do you think humans might develop machines that", "tokens": [51084, 731, 13, 1449, 498, 291, 645, 733, 295, 281, 584, 11, 1392, 11, 562, 360, 291, 519, 6255, 1062, 1499, 8379, 300, 51336], "temperature": 0.0, "avg_logprob": -0.10518242949146335, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.003437277628108859}, {"id": 469, "seek": 276560, "start": 2785.04, "end": 2788.48, "text": " can do what the human brain can do? You kind of think it might be in the next few decades.", "tokens": [51336, 393, 360, 437, 264, 1952, 3567, 393, 360, 30, 509, 733, 295, 519, 309, 1062, 312, 294, 264, 958, 1326, 7878, 13, 51508], "temperature": 0.0, "avg_logprob": -0.10518242949146335, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.003437277628108859}, {"id": 470, "seek": 276560, "start": 2789.2799999999997, "end": 2792.56, "text": " And I think if you just sit with that fact that there are going to be machines that can do what", "tokens": [51548, 400, 286, 519, 498, 291, 445, 1394, 365, 300, 1186, 300, 456, 366, 516, 281, 312, 8379, 300, 393, 360, 437, 51712], "temperature": 0.0, "avg_logprob": -0.10518242949146335, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.003437277628108859}, {"id": 471, "seek": 279256, "start": 2792.56, "end": 2797.44, "text": " the human brain can do. And you're going to be able to make those machines much more efficient at", "tokens": [50364, 264, 1952, 3567, 393, 360, 13, 400, 291, 434, 516, 281, 312, 1075, 281, 652, 729, 8379, 709, 544, 7148, 412, 50608], "temperature": 0.0, "avg_logprob": -0.08136351903279622, "compression_ratio": 1.9140625, "no_speech_prob": 0.009640310890972614}, {"id": 472, "seek": 279256, "start": 2797.44, "end": 2802.08, "text": " it. And you're going to be able to make even better versions of those machines, 10 times better", "tokens": [50608, 309, 13, 400, 291, 434, 516, 281, 312, 1075, 281, 652, 754, 1101, 9606, 295, 729, 8379, 11, 1266, 1413, 1101, 50840], "temperature": 0.0, "avg_logprob": -0.08136351903279622, "compression_ratio": 1.9140625, "no_speech_prob": 0.009640310890972614}, {"id": 473, "seek": 279256, "start": 2802.08, "end": 2806.24, "text": " versions. You're going to be able to run them day and night. And you're going to be able to build", "tokens": [50840, 9606, 13, 509, 434, 516, 281, 312, 1075, 281, 1190, 552, 786, 293, 1818, 13, 400, 291, 434, 516, 281, 312, 1075, 281, 1322, 51048], "temperature": 0.0, "avg_logprob": -0.08136351903279622, "compression_ratio": 1.9140625, "no_speech_prob": 0.009640310890972614}, {"id": 474, "seek": 279256, "start": 2806.24, "end": 2811.04, "text": " more. When you sit with all that, I do think it gets pretty hard to imagine a future that isn't very", "tokens": [51048, 544, 13, 1133, 291, 1394, 365, 439, 300, 11, 286, 360, 519, 309, 2170, 1238, 1152, 281, 3811, 257, 2027, 300, 1943, 380, 588, 51288], "temperature": 0.0, "avg_logprob": -0.08136351903279622, "compression_ratio": 1.9140625, "no_speech_prob": 0.009640310890972614}, {"id": 475, "seek": 279256, "start": 2811.04, "end": 2818.72, "text": " crazy. And another perspective is just zooming out even further and just looking at the whole arc", "tokens": [51288, 3219, 13, 400, 1071, 4585, 307, 445, 48226, 484, 754, 3052, 293, 445, 1237, 412, 264, 1379, 10346, 51672], "temperature": 0.0, "avg_logprob": -0.08136351903279622, "compression_ratio": 1.9140625, "no_speech_prob": 0.009640310890972614}, {"id": 476, "seek": 281872, "start": 2818.72, "end": 2826.16, "text": " of human history. So if you'd have asked hunter gatherers who only knew the 50 people in their", "tokens": [50364, 295, 1952, 2503, 13, 407, 498, 291, 1116, 362, 2351, 22970, 5448, 433, 567, 787, 2586, 264, 2625, 561, 294, 641, 50736], "temperature": 0.0, "avg_logprob": -0.11553033192952473, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.06017917022109032}, {"id": 477, "seek": 281872, "start": 2826.16, "end": 2833.2799999999997, "text": " group and who had been hunting using techniques and tools that as far as they knew had been", "tokens": [50736, 1594, 293, 567, 632, 668, 12599, 1228, 7512, 293, 3873, 300, 382, 1400, 382, 436, 2586, 632, 668, 51092], "temperature": 0.0, "avg_logprob": -0.11553033192952473, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.06017917022109032}, {"id": 478, "seek": 281872, "start": 2834.0, "end": 2840.72, "text": " passed down for eternity generation to generation, doing their rituals, if you'd have told them that", "tokens": [51128, 4678, 760, 337, 27162, 5125, 281, 5125, 11, 884, 641, 29082, 11, 498, 291, 1116, 362, 1907, 552, 300, 51464], "temperature": 0.0, "avg_logprob": -0.11553033192952473, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.06017917022109032}, {"id": 479, "seek": 281872, "start": 2841.3599999999997, "end": 2848.56, "text": " in a few thousand years, there are going to be huge empires building the Egyptian pyramids", "tokens": [51496, 294, 257, 1326, 4714, 924, 11, 456, 366, 516, 281, 312, 2603, 4012, 3145, 2390, 264, 24257, 20543, 3742, 51856], "temperature": 0.0, "avg_logprob": -0.11553033192952473, "compression_ratio": 1.6506550218340612, "no_speech_prob": 0.06017917022109032}, {"id": 480, "seek": 284856, "start": 2848.56, "end": 2859.44, "text": " and massive armies and the ability to go to a market and give people pieces of metal in exchange", "tokens": [50364, 293, 5994, 28217, 293, 264, 3485, 281, 352, 281, 257, 2142, 293, 976, 561, 3755, 295, 5760, 294, 7742, 50908], "temperature": 0.0, "avg_logprob": -0.09109738740054044, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006856567924842238}, {"id": 481, "seek": 284856, "start": 2859.44, "end": 2864.64, "text": " for all kinds of goods. They would have seemed totally crazy. And then if you'd have told those", "tokens": [50908, 337, 439, 3685, 295, 10179, 13, 814, 576, 362, 6576, 3879, 3219, 13, 400, 550, 498, 291, 1116, 362, 1907, 729, 51168], "temperature": 0.0, "avg_logprob": -0.09109738740054044, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006856567924842238}, {"id": 482, "seek": 284856, "start": 2864.64, "end": 2871.36, "text": " people in those markets that no, there's going to be a future world where every 10 years,", "tokens": [51168, 561, 294, 729, 8383, 300, 572, 11, 456, 311, 516, 281, 312, 257, 2027, 1002, 689, 633, 1266, 924, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09109738740054044, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006856567924842238}, {"id": 483, "seek": 284856, "start": 2871.36, "end": 2875.68, "text": " major, major technological progress is going to be coming along and we're going to be discovering", "tokens": [51504, 2563, 11, 2563, 18439, 4205, 307, 516, 281, 312, 1348, 2051, 293, 321, 434, 516, 281, 312, 24773, 51720], "temperature": 0.0, "avg_logprob": -0.09109738740054044, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006856567924842238}, {"id": 484, "seek": 287568, "start": 2875.68, "end": 2882.24, "text": " drugs that can solve all kinds of diseases. You're going to be able to get inside a box", "tokens": [50364, 7766, 300, 393, 5039, 439, 3685, 295, 11044, 13, 509, 434, 516, 281, 312, 1075, 281, 483, 1854, 257, 2424, 50692], "temperature": 0.0, "avg_logprob": -0.09653104006589114, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.009064115583896637}, {"id": 485, "seek": 287568, "start": 2882.24, "end": 2887.12, "text": " and land the other side of the earth. Right. Again, they'd have just thought you were crazy.", "tokens": [50692, 293, 2117, 264, 661, 1252, 295, 264, 4120, 13, 1779, 13, 3764, 11, 436, 1116, 362, 445, 1194, 291, 645, 3219, 13, 50936], "temperature": 0.0, "avg_logprob": -0.09653104006589114, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.009064115583896637}, {"id": 486, "seek": 287568, "start": 2887.12, "end": 2893.2, "text": " And I think while it seems that we understand what's happening and that progress is pretty", "tokens": [50936, 400, 286, 519, 1339, 309, 2544, 300, 321, 1223, 437, 311, 2737, 293, 300, 4205, 307, 1238, 51240], "temperature": 0.0, "avg_logprob": -0.09653104006589114, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.009064115583896637}, {"id": 487, "seek": 287568, "start": 2893.2, "end": 2899.52, "text": " steady, that has only been true for the last 200 years. And zooming out, it's actually the norm", "tokens": [51240, 13211, 11, 300, 575, 787, 668, 2074, 337, 264, 1036, 2331, 924, 13, 400, 48226, 484, 11, 309, 311, 767, 264, 2026, 51556], "temperature": 0.0, "avg_logprob": -0.09653104006589114, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.009064115583896637}, {"id": 488, "seek": 289952, "start": 2899.52, "end": 2905.68, "text": " throughout the longer run of history for things to go in a totally surprising and unpredictable", "tokens": [50364, 3710, 264, 2854, 1190, 295, 2503, 337, 721, 281, 352, 294, 257, 3879, 8830, 293, 31160, 50672], "temperature": 0.0, "avg_logprob": -0.13029135357249866, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.019262956455349922}, {"id": 489, "seek": 289952, "start": 2905.68, "end": 2912.72, "text": " direction or a direction that would have seemed totally bizarre and unpredictable to people", "tokens": [50672, 3513, 420, 257, 3513, 300, 576, 362, 6576, 3879, 18265, 293, 31160, 281, 561, 51024], "temperature": 0.0, "avg_logprob": -0.13029135357249866, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.019262956455349922}, {"id": 490, "seek": 289952, "start": 2912.72, "end": 2917.7599999999998, "text": " naively at that time. Right. I feel like I was introduced to it when I read what we did in the", "tokens": [51024, 1667, 3413, 412, 300, 565, 13, 1779, 13, 286, 841, 411, 286, 390, 7268, 281, 309, 562, 286, 1401, 437, 321, 630, 294, 264, 51276], "temperature": 0.0, "avg_logprob": -0.13029135357249866, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.019262956455349922}, {"id": 491, "seek": 289952, "start": 2917.7599999999998, "end": 2924.56, "text": " future, Will MacAskill's book, that there's this thing called the end of history fallacy. It really", "tokens": [51276, 2027, 11, 3099, 5707, 47802, 373, 311, 1446, 11, 300, 456, 311, 341, 551, 1219, 264, 917, 295, 2503, 2100, 2551, 13, 467, 534, 51616], "temperature": 0.0, "avg_logprob": -0.13029135357249866, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.019262956455349922}, {"id": 492, "seek": 292456, "start": 2924.64, "end": 2932.88, "text": " feels like we're living at the end. We're done changing. We're going to maybe find some new", "tokens": [50368, 3417, 411, 321, 434, 2647, 412, 264, 917, 13, 492, 434, 1096, 4473, 13, 492, 434, 516, 281, 1310, 915, 512, 777, 50780], "temperature": 0.0, "avg_logprob": -0.11723254478141053, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.06749822944402695}, {"id": 493, "seek": 292456, "start": 2933.7599999999998, "end": 2941.2799999999997, "text": " medical devices or something. But basically, we've done all of the weird shifting that we're", "tokens": [50824, 4625, 5759, 420, 746, 13, 583, 1936, 11, 321, 600, 1096, 439, 295, 264, 3657, 17573, 300, 321, 434, 51200], "temperature": 0.0, "avg_logprob": -0.11723254478141053, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.06749822944402695}, {"id": 494, "seek": 292456, "start": 2941.2799999999997, "end": 2949.6, "text": " going to do. And I can't really justify that. It does seem like a fallacy. Presumably,", "tokens": [51200, 516, 281, 360, 13, 400, 286, 393, 380, 534, 20833, 300, 13, 467, 775, 1643, 411, 257, 2100, 2551, 13, 2718, 449, 1188, 11, 51616], "temperature": 0.0, "avg_logprob": -0.11723254478141053, "compression_ratio": 1.5139664804469273, "no_speech_prob": 0.06749822944402695}, {"id": 495, "seek": 294960, "start": 2950.3199999999997, "end": 2955.6, "text": " things are going to look super different in 50 years. And sometimes those changes have gone", "tokens": [50400, 721, 366, 516, 281, 574, 1687, 819, 294, 2625, 924, 13, 400, 2171, 729, 2962, 362, 2780, 50664], "temperature": 0.0, "avg_logprob": -0.06378624704149034, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.016759272664785385}, {"id": 496, "seek": 294960, "start": 2955.6, "end": 2961.68, "text": " super fast in history. And sometimes they've gone super slowly. And we've got real reasons to think", "tokens": [50664, 1687, 2370, 294, 2503, 13, 400, 2171, 436, 600, 2780, 1687, 5692, 13, 400, 321, 600, 658, 957, 4112, 281, 519, 50968], "temperature": 0.0, "avg_logprob": -0.06378624704149034, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.016759272664785385}, {"id": 497, "seek": 294960, "start": 2961.68, "end": 2967.68, "text": " that we might be entering a period of really fast transition. Yeah. I mean, if anything, I'd say", "tokens": [50968, 300, 321, 1062, 312, 11104, 257, 2896, 295, 534, 2370, 6034, 13, 865, 13, 286, 914, 11, 498, 1340, 11, 286, 1116, 584, 51268], "temperature": 0.0, "avg_logprob": -0.06378624704149034, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.016759272664785385}, {"id": 498, "seek": 294960, "start": 2967.68, "end": 2974.08, "text": " the norm is for the new period to involve much faster changes than the old period. So Hunter", "tokens": [51268, 264, 2026, 307, 337, 264, 777, 2896, 281, 9494, 709, 4663, 2962, 813, 264, 1331, 2896, 13, 407, 18704, 51588], "temperature": 0.0, "avg_logprob": -0.06378624704149034, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.016759272664785385}, {"id": 499, "seek": 297408, "start": 2974.16, "end": 2978.7999999999997, "text": " Gathering went on for tens of thousands of years, if not hundreds of thousands of years.", "tokens": [50368, 39841, 278, 1437, 322, 337, 10688, 295, 5383, 295, 924, 11, 498, 406, 6779, 295, 5383, 295, 924, 13, 50600], "temperature": 0.0, "avg_logprob": -0.10963311146215066, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.008814264088869095}, {"id": 500, "seek": 297408, "start": 2979.6, "end": 2984.08, "text": " Then we started doing agriculture and forming into big societies and did things like the pyramids.", "tokens": [50640, 1396, 321, 1409, 884, 14837, 293, 15745, 666, 955, 19329, 293, 630, 721, 411, 264, 20543, 3742, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10963311146215066, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.008814264088869095}, {"id": 501, "seek": 297408, "start": 2984.7999999999997, "end": 2988.96, "text": " And then a way that people often think of the next phase transition as being", "tokens": [50900, 400, 550, 257, 636, 300, 561, 2049, 519, 295, 264, 958, 5574, 6034, 382, 885, 51108], "temperature": 0.0, "avg_logprob": -0.10963311146215066, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.008814264088869095}, {"id": 502, "seek": 297408, "start": 2989.6, "end": 2994.88, "text": " kind of the start of the industrial revolution and the beginning of kind of concerted efforts", "tokens": [51140, 733, 295, 264, 722, 295, 264, 9987, 8894, 293, 264, 2863, 295, 733, 295, 8543, 292, 6484, 51404], "temperature": 0.0, "avg_logprob": -0.10963311146215066, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.008814264088869095}, {"id": 503, "seek": 297408, "start": 2994.88, "end": 3001.84, "text": " towards making scientific progress. And after we did agriculture, kind of new technologies and", "tokens": [51404, 3030, 1455, 8134, 4205, 13, 400, 934, 321, 630, 14837, 11, 733, 295, 777, 7943, 293, 51752], "temperature": 0.0, "avg_logprob": -0.10963311146215066, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.008814264088869095}, {"id": 504, "seek": 300184, "start": 3001.92, "end": 3007.36, "text": " changes were happening on the scale of maybe a thousand years, or maybe a few hundred years,", "tokens": [50368, 2962, 645, 2737, 322, 264, 4373, 295, 1310, 257, 4714, 924, 11, 420, 1310, 257, 1326, 3262, 924, 11, 50640], "temperature": 0.0, "avg_logprob": -0.09601487263594524, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.002594516845420003}, {"id": 505, "seek": 300184, "start": 3007.36, "end": 3012.1600000000003, "text": " which is much faster than in the Hunter Gatherer times. And then today, after the industrial", "tokens": [50640, 597, 307, 709, 4663, 813, 294, 264, 18704, 39841, 260, 1413, 13, 400, 550, 965, 11, 934, 264, 9987, 50880], "temperature": 0.0, "avg_logprob": -0.09601487263594524, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.002594516845420003}, {"id": 506, "seek": 300184, "start": 3012.1600000000003, "end": 3017.1200000000003, "text": " revolution, we're seeing really big changes to society every 50 years. So we've already seen", "tokens": [50880, 8894, 11, 321, 434, 2577, 534, 955, 2962, 281, 4086, 633, 2625, 924, 13, 407, 321, 600, 1217, 1612, 51128], "temperature": 0.0, "avg_logprob": -0.09601487263594524, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.002594516845420003}, {"id": 507, "seek": 300184, "start": 3017.6800000000003, "end": 3022.0, "text": " kind of historically those phase transitions have led to things being faster. So that,", "tokens": [51156, 733, 295, 16180, 729, 5574, 23767, 362, 4684, 281, 721, 885, 4663, 13, 407, 300, 11, 51372], "temperature": 0.0, "avg_logprob": -0.09601487263594524, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.002594516845420003}, {"id": 508, "seek": 300184, "start": 3022.0, "end": 3026.56, "text": " I think, is the default expectation for what a new transition would lead to.", "tokens": [51372, 286, 519, 11, 307, 264, 7576, 14334, 337, 437, 257, 777, 6034, 576, 1477, 281, 13, 51600], "temperature": 0.0, "avg_logprob": -0.09601487263594524, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.002594516845420003}, {"id": 509, "seek": 302656, "start": 3027.2, "end": 3034.08, "text": " Right. And it just feels weird to us because we're pre-transition. Possibly, whoever's living", "tokens": [50396, 1779, 13, 400, 309, 445, 3417, 3657, 281, 505, 570, 321, 434, 659, 12, 24999, 849, 13, 33112, 3545, 11, 11387, 311, 2647, 50740], "temperature": 0.0, "avg_logprob": -0.138726626502143, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.05259629338979721}, {"id": 510, "seek": 302656, "start": 3034.08, "end": 3042.0, "text": " 50 years from now will just be like, yeah, obviously, that was coming. And those weird", "tokens": [50740, 2625, 924, 490, 586, 486, 445, 312, 411, 11, 1338, 11, 2745, 11, 300, 390, 1348, 13, 400, 729, 3657, 51136], "temperature": 0.0, "avg_logprob": -0.138726626502143, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.05259629338979721}, {"id": 511, "seek": 302656, "start": 3042.0, "end": 3048.0, "text": " people living in 2023 thinking that they'd made all the technological progress they were ever going", "tokens": [51136, 561, 2647, 294, 44377, 1953, 300, 436, 1116, 1027, 439, 264, 18439, 4205, 436, 645, 1562, 516, 51436], "temperature": 0.0, "avg_logprob": -0.138726626502143, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.05259629338979721}, {"id": 512, "seek": 302656, "start": 3048.0, "end": 3054.88, "text": " to make. Maybe I'm still struggling just to imagine what it would look like, I guess.", "tokens": [51436, 281, 652, 13, 2704, 286, 478, 920, 9314, 445, 281, 3811, 437, 309, 576, 574, 411, 11, 286, 2041, 13, 51780], "temperature": 0.0, "avg_logprob": -0.138726626502143, "compression_ratio": 1.493877551020408, "no_speech_prob": 0.05259629338979721}, {"id": 513, "seek": 305488, "start": 3055.6, "end": 3064.2400000000002, "text": " Yeah, because it's possibly going to be us. What's in store for us? Is it going to be good?", "tokens": [50400, 865, 11, 570, 309, 311, 6264, 516, 281, 312, 505, 13, 708, 311, 294, 3531, 337, 505, 30, 1119, 309, 516, 281, 312, 665, 30, 50832], "temperature": 0.0, "avg_logprob": -0.1261001272299855, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.00922777783125639}, {"id": 514, "seek": 305488, "start": 3065.2000000000003, "end": 3072.1600000000003, "text": " I think it could be really good. It could be really, really bad. It could be really good if we", "tokens": [50880, 286, 519, 309, 727, 312, 534, 665, 13, 467, 727, 312, 534, 11, 534, 1578, 13, 467, 727, 312, 534, 665, 498, 321, 51228], "temperature": 0.0, "avg_logprob": -0.1261001272299855, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.00922777783125639}, {"id": 515, "seek": 305488, "start": 3073.12, "end": 3077.6800000000003, "text": " align AI so they're always trying to help us and help humanity do as best as it can", "tokens": [51276, 7975, 7318, 370, 436, 434, 1009, 1382, 281, 854, 505, 293, 854, 10243, 360, 382, 1151, 382, 309, 393, 51504], "temperature": 0.0, "avg_logprob": -0.1261001272299855, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.00922777783125639}, {"id": 516, "seek": 305488, "start": 3077.6800000000003, "end": 3084.6400000000003, "text": " by humanity's own lights. And the kind of benefits from AI and these new technologies are used to", "tokens": [51504, 538, 10243, 311, 1065, 5811, 13, 400, 264, 733, 295, 5311, 490, 7318, 293, 613, 777, 7943, 366, 1143, 281, 51852], "temperature": 0.0, "avg_logprob": -0.1261001272299855, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.00922777783125639}, {"id": 517, "seek": 308464, "start": 3084.64, "end": 3091.68, "text": " solve the world's most pressing problems and used to lift people out of poverty and give people", "tokens": [50364, 5039, 264, 1002, 311, 881, 12417, 2740, 293, 1143, 281, 5533, 561, 484, 295, 10958, 293, 976, 561, 50716], "temperature": 0.0, "avg_logprob": -0.1014693886486452, "compression_ratio": 1.6179775280898876, "no_speech_prob": 0.0007630236796103418}, {"id": 518, "seek": 308464, "start": 3092.4, "end": 3097.44, "text": " the lives that they hope for themselves and for their children, solve the problems of climate", "tokens": [50752, 264, 2909, 300, 436, 1454, 337, 2969, 293, 337, 641, 2227, 11, 5039, 264, 2740, 295, 5659, 51004], "temperature": 0.0, "avg_logprob": -0.1014693886486452, "compression_ratio": 1.6179775280898876, "no_speech_prob": 0.0007630236796103418}, {"id": 519, "seek": 308464, "start": 3097.44, "end": 3105.92, "text": " change or poverty of disease. I think it could go really, really well. Right, right. And so in the", "tokens": [51004, 1319, 420, 10958, 295, 4752, 13, 286, 519, 309, 727, 352, 534, 11, 534, 731, 13, 1779, 11, 558, 13, 400, 370, 294, 264, 51428], "temperature": 0.0, "avg_logprob": -0.1014693886486452, "compression_ratio": 1.6179775280898876, "no_speech_prob": 0.0007630236796103418}, {"id": 520, "seek": 310592, "start": 3105.92, "end": 3115.04, "text": " best case where AI is really trying to help us, it's still kind of unimaginable to me as a world.", "tokens": [50364, 1151, 1389, 689, 7318, 307, 534, 1382, 281, 854, 505, 11, 309, 311, 920, 733, 295, 517, 44976, 712, 281, 385, 382, 257, 1002, 13, 50820], "temperature": 0.0, "avg_logprob": -0.11403532381410952, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.3480633497238159}, {"id": 521, "seek": 310592, "start": 3115.6, "end": 3123.6800000000003, "text": " I mean, maybe it's just like I'm so biased by the status quo where like, I need to work and", "tokens": [50848, 286, 914, 11, 1310, 309, 311, 445, 411, 286, 478, 370, 28035, 538, 264, 6558, 28425, 689, 411, 11, 286, 643, 281, 589, 293, 51252], "temperature": 0.0, "avg_logprob": -0.11403532381410952, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.3480633497238159}, {"id": 522, "seek": 310592, "start": 3124.4, "end": 3131.12, "text": " I need to work to live, I need to work to help solve problems. Like in this best case, is there", "tokens": [51288, 286, 643, 281, 589, 281, 1621, 11, 286, 643, 281, 589, 281, 854, 5039, 2740, 13, 1743, 294, 341, 1151, 1389, 11, 307, 456, 51624], "temperature": 0.0, "avg_logprob": -0.11403532381410952, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.3480633497238159}, {"id": 523, "seek": 313112, "start": 3131.12, "end": 3139.2799999999997, "text": " unemployment? Is there unemployment for everyone? Is it a slow transition? A fast one? Does it make", "tokens": [50364, 17438, 30, 1119, 456, 17438, 337, 1518, 30, 1119, 309, 257, 2964, 6034, 30, 316, 2370, 472, 30, 4402, 309, 652, 50772], "temperature": 0.0, "avg_logprob": -0.1065969467163086, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.006242163944989443}, {"id": 524, "seek": 313112, "start": 3139.2799999999997, "end": 3145.12, "text": " inequality better because no one needs to work and we all have enough things? Or does it make it", "tokens": [50772, 16970, 1101, 570, 572, 472, 2203, 281, 589, 293, 321, 439, 362, 1547, 721, 30, 1610, 775, 309, 652, 309, 51064], "temperature": 0.0, "avg_logprob": -0.1065969467163086, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.006242163944989443}, {"id": 525, "seek": 313112, "start": 3145.12, "end": 3152.16, "text": " worse because some people have to work? Do we have predictions about that that are worth making?", "tokens": [51064, 5324, 570, 512, 561, 362, 281, 589, 30, 1144, 321, 362, 21264, 466, 300, 300, 366, 3163, 1455, 30, 51416], "temperature": 0.0, "avg_logprob": -0.1065969467163086, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.006242163944989443}, {"id": 526, "seek": 313112, "start": 3152.16, "end": 3160.0, "text": " I think the default is that inequality would become greater because all of the wealth and", "tokens": [51416, 286, 519, 264, 7576, 307, 300, 16970, 576, 1813, 5044, 570, 439, 295, 264, 7203, 293, 51808], "temperature": 0.0, "avg_logprob": -0.1065969467163086, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.006242163944989443}, {"id": 527, "seek": 316000, "start": 3160.0, "end": 3164.96, "text": " useful work is coming from these AI systems, which I think by default will be controlled by a small", "tokens": [50364, 4420, 589, 307, 1348, 490, 613, 7318, 3652, 11, 597, 286, 519, 538, 7576, 486, 312, 10164, 538, 257, 1359, 50612], "temperature": 0.0, "avg_logprob": -0.08825103185510122, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.0022293427027761936}, {"id": 528, "seek": 316000, "start": 3164.96, "end": 3173.52, "text": " number of people and companies. In the very best case, then you hope that that wealth is equally", "tokens": [50612, 1230, 295, 561, 293, 3431, 13, 682, 264, 588, 1151, 1389, 11, 550, 291, 1454, 300, 300, 7203, 307, 12309, 51040], "temperature": 0.0, "avg_logprob": -0.08825103185510122, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.0022293427027761936}, {"id": 529, "seek": 316000, "start": 3173.52, "end": 3179.76, "text": " distributed or kind of much more equally distributed than it would be by default. And it is true,", "tokens": [51040, 12631, 420, 733, 295, 709, 544, 12309, 12631, 813, 309, 576, 312, 538, 7576, 13, 400, 309, 307, 2074, 11, 51352], "temperature": 0.0, "avg_logprob": -0.08825103185510122, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.0022293427027761936}, {"id": 530, "seek": 316000, "start": 3179.76, "end": 3186.88, "text": " I think, that there's going to be so much progress made if AI is aligned that it will be very cheap", "tokens": [51352, 286, 519, 11, 300, 456, 311, 516, 281, 312, 370, 709, 4205, 1027, 498, 7318, 307, 17962, 300, 309, 486, 312, 588, 7084, 51708], "temperature": 0.0, "avg_logprob": -0.08825103185510122, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.0022293427027761936}, {"id": 531, "seek": 318688, "start": 3187.6, "end": 3191.2000000000003, "text": " to give everyone in the world the standards of living that are enjoyed by the very", "tokens": [50400, 281, 976, 1518, 294, 264, 1002, 264, 7787, 295, 2647, 300, 366, 4626, 538, 264, 588, 50580], "temperature": 0.0, "avg_logprob": -0.1017668612368472, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.01757291704416275}, {"id": 532, "seek": 318688, "start": 3191.2000000000003, "end": 3197.6800000000003, "text": " richest people today in terms of material comforts and health and actually much, much better on", "tokens": [50580, 35098, 561, 965, 294, 2115, 295, 2527, 3400, 82, 293, 1585, 293, 767, 709, 11, 709, 1101, 322, 50904], "temperature": 0.0, "avg_logprob": -0.1017668612368472, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.01757291704416275}, {"id": 533, "seek": 318688, "start": 3197.6800000000003, "end": 3203.28, "text": " those fronts, I think after all this technological progress. So I think if we can get the AIs to", "tokens": [50904, 729, 40426, 11, 286, 519, 934, 439, 341, 18439, 4205, 13, 407, 286, 519, 498, 321, 393, 483, 264, 316, 6802, 281, 51184], "temperature": 0.0, "avg_logprob": -0.1017668612368472, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.01757291704416275}, {"id": 534, "seek": 318688, "start": 3203.28, "end": 3209.36, "text": " be really trying to help us, then even if we mess up on things like the kind of distribution of", "tokens": [51184, 312, 534, 1382, 281, 854, 505, 11, 550, 754, 498, 321, 2082, 493, 322, 721, 411, 264, 733, 295, 7316, 295, 51488], "temperature": 0.0, "avg_logprob": -0.1017668612368472, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.01757291704416275}, {"id": 535, "seek": 318688, "start": 3209.36, "end": 3214.2400000000002, "text": " benefits, even if we mess up a bit on that, then I think things will still look pretty good because", "tokens": [51488, 5311, 11, 754, 498, 321, 2082, 493, 257, 857, 322, 300, 11, 550, 286, 519, 721, 486, 920, 574, 1238, 665, 570, 51732], "temperature": 0.0, "avg_logprob": -0.1017668612368472, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.01757291704416275}, {"id": 536, "seek": 321424, "start": 3214.24, "end": 3222.56, "text": " there's just so much to go around. If there's kind of universal basic income, you could just use", "tokens": [50364, 456, 311, 445, 370, 709, 281, 352, 926, 13, 759, 456, 311, 733, 295, 11455, 3875, 5742, 11, 291, 727, 445, 764, 50780], "temperature": 0.0, "avg_logprob": -0.085421399867281, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.009920320473611355}, {"id": 537, "seek": 321424, "start": 3223.2, "end": 3229.04, "text": " 1% of the output that's produced in a year to kind of give everyone kind of all the material", "tokens": [50812, 502, 4, 295, 264, 5598, 300, 311, 7126, 294, 257, 1064, 281, 733, 295, 976, 1518, 733, 295, 439, 264, 2527, 51104], "temperature": 0.0, "avg_logprob": -0.085421399867281, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.009920320473611355}, {"id": 538, "seek": 321424, "start": 3229.04, "end": 3234.3199999999997, "text": " and technological things they need to kind of meet all of their needs, all of the material needs.", "tokens": [51104, 293, 18439, 721, 436, 643, 281, 733, 295, 1677, 439, 295, 641, 2203, 11, 439, 295, 264, 2527, 2203, 13, 51368], "temperature": 0.0, "avg_logprob": -0.085421399867281, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.009920320473611355}, {"id": 539, "seek": 321424, "start": 3235.6, "end": 3242.0, "text": " In terms of work, I think it will no longer be the case that you can produce a higher quality", "tokens": [51432, 682, 2115, 295, 589, 11, 286, 519, 309, 486, 572, 2854, 312, 264, 1389, 300, 291, 393, 5258, 257, 2946, 3125, 51752], "temperature": 0.0, "avg_logprob": -0.085421399867281, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.009920320473611355}, {"id": 540, "seek": 324200, "start": 3242.0, "end": 3249.52, "text": " service or product than what an AI could do or a robot could do. One thought is that there will be", "tokens": [50364, 2643, 420, 1674, 813, 437, 364, 7318, 727, 360, 420, 257, 7881, 727, 360, 13, 1485, 1194, 307, 300, 456, 486, 312, 50740], "temperature": 0.0, "avg_logprob": -0.12738388100850215, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.007727697025984526}, {"id": 541, "seek": 324200, "start": 3249.52, "end": 3256.88, "text": " some humans, maybe me and you, who just value human contact and hang out with other actual", "tokens": [50740, 512, 6255, 11, 1310, 385, 293, 291, 11, 567, 445, 2158, 1952, 3385, 293, 3967, 484, 365, 661, 3539, 51108], "temperature": 0.0, "avg_logprob": -0.12738388100850215, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.007727697025984526}, {"id": 542, "seek": 324200, "start": 3256.88, "end": 3262.24, "text": " real humans. And that could provide a kind of work for those who want it.", "tokens": [51108, 957, 6255, 13, 400, 300, 727, 2893, 257, 733, 295, 589, 337, 729, 567, 528, 309, 13, 51376], "temperature": 0.0, "avg_logprob": -0.12738388100850215, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.007727697025984526}, {"id": 543, "seek": 324200, "start": 3262.24, "end": 3264.24, "text": " Role for humans. Okay.", "tokens": [51376, 3101, 306, 337, 6255, 13, 1033, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12738388100850215, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.007727697025984526}, {"id": 544, "seek": 324200, "start": 3264.24, "end": 3270.16, "text": " And another possibility is that we rethink the nature of work. So we do work to help each other", "tokens": [51476, 400, 1071, 7959, 307, 300, 321, 34595, 264, 3687, 295, 589, 13, 407, 321, 360, 589, 281, 854, 1184, 661, 51772], "temperature": 0.0, "avg_logprob": -0.12738388100850215, "compression_ratio": 1.6536796536796536, "no_speech_prob": 0.007727697025984526}, {"id": 545, "seek": 327016, "start": 3270.96, "end": 3276.16, "text": " and even though we know that AIs could do the work just as well, we're still happy to do that", "tokens": [50404, 293, 754, 1673, 321, 458, 300, 316, 6802, 727, 360, 264, 589, 445, 382, 731, 11, 321, 434, 920, 2055, 281, 360, 300, 50664], "temperature": 0.0, "avg_logprob": -0.09916021994182042, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.004910830873996019}, {"id": 546, "seek": 327016, "start": 3276.16, "end": 3281.8399999999997, "text": " because it gives us a sense of meaning or we kind of do creative things instead like", "tokens": [50664, 570, 309, 2709, 505, 257, 2020, 295, 3620, 420, 321, 733, 295, 360, 5880, 721, 2602, 411, 50948], "temperature": 0.0, "avg_logprob": -0.09916021994182042, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.004910830873996019}, {"id": 547, "seek": 327016, "start": 3282.96, "end": 3286.96, "text": " creative writing and drawing. And even though we know that AIs could do that better than ours,", "tokens": [51004, 5880, 3579, 293, 6316, 13, 400, 754, 1673, 321, 458, 300, 316, 6802, 727, 360, 300, 1101, 813, 11896, 11, 51204], "temperature": 0.0, "avg_logprob": -0.09916021994182042, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.004910830873996019}, {"id": 548, "seek": 327016, "start": 3286.96, "end": 3291.8399999999997, "text": " it's still enough for us to have a sense of purpose. I mean, people still play chess today", "tokens": [51204, 309, 311, 920, 1547, 337, 505, 281, 362, 257, 2020, 295, 4334, 13, 286, 914, 11, 561, 920, 862, 24122, 965, 51448], "temperature": 0.0, "avg_logprob": -0.09916021994182042, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.004910830873996019}, {"id": 549, "seek": 327016, "start": 3292.56, "end": 3297.12, "text": " and still really enjoy it and get purpose from it, even though they know that they can never", "tokens": [51484, 293, 920, 534, 2103, 309, 293, 483, 4334, 490, 309, 11, 754, 1673, 436, 458, 300, 436, 393, 1128, 51712], "temperature": 0.0, "avg_logprob": -0.09916021994182042, "compression_ratio": 1.8653061224489795, "no_speech_prob": 0.004910830873996019}, {"id": 550, "seek": 329712, "start": 3297.12, "end": 3303.7599999999998, "text": " go to match the best AIs. Right. Yeah, I guess I can imagine lots of people listening, hearing", "tokens": [50364, 352, 281, 2995, 264, 1151, 316, 6802, 13, 1779, 13, 865, 11, 286, 2041, 286, 393, 3811, 3195, 295, 561, 4764, 11, 4763, 50696], "temperature": 0.0, "avg_logprob": -0.0943688154220581, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.011477543041110039}, {"id": 551, "seek": 329712, "start": 3303.7599999999998, "end": 3313.2799999999997, "text": " about this future and being like, no, I like the world the way it is. I like that humans get to", "tokens": [50696, 466, 341, 2027, 293, 885, 411, 11, 572, 11, 286, 411, 264, 1002, 264, 636, 309, 307, 13, 286, 411, 300, 6255, 483, 281, 51172], "temperature": 0.0, "avg_logprob": -0.0943688154220581, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.011477543041110039}, {"id": 552, "seek": 329712, "start": 3313.2799999999997, "end": 3320.24, "text": " make choices for ourselves as a society. I don't want AIs making it for us. I like that I have to", "tokens": [51172, 652, 7994, 337, 4175, 382, 257, 4086, 13, 286, 500, 380, 528, 316, 6802, 1455, 309, 337, 505, 13, 286, 411, 300, 286, 362, 281, 51520], "temperature": 0.0, "avg_logprob": -0.0943688154220581, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.011477543041110039}, {"id": 553, "seek": 329712, "start": 3320.24, "end": 3326.08, "text": " work, get to work. I don't know. I don't, I can imagine people being like, no, I don't want AIs", "tokens": [51520, 589, 11, 483, 281, 589, 13, 286, 500, 380, 458, 13, 286, 500, 380, 11, 286, 393, 3811, 561, 885, 411, 11, 572, 11, 286, 500, 380, 528, 316, 6802, 51812], "temperature": 0.0, "avg_logprob": -0.0943688154220581, "compression_ratio": 1.8113207547169812, "no_speech_prob": 0.011477543041110039}, {"id": 554, "seek": 332608, "start": 3326.08, "end": 3333.6, "text": " to be making the art. I want humans to be making the art. So is there some chance that there's,", "tokens": [50364, 281, 312, 1455, 264, 1523, 13, 286, 528, 6255, 281, 312, 1455, 264, 1523, 13, 407, 307, 456, 512, 2931, 300, 456, 311, 11, 50740], "temperature": 0.0, "avg_logprob": -0.06257482634650337, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.002377293771132827}, {"id": 555, "seek": 332608, "start": 3333.6, "end": 3339.44, "text": " I don't know, like a movement that's anti AI growth that stops this from happening,", "tokens": [50740, 286, 500, 380, 458, 11, 411, 257, 3963, 300, 311, 6061, 7318, 4599, 300, 10094, 341, 490, 2737, 11, 51032], "temperature": 0.0, "avg_logprob": -0.06257482634650337, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.002377293771132827}, {"id": 556, "seek": 332608, "start": 3339.44, "end": 3346.24, "text": " even though it's theoretically possible? That's a great question. I do think it would be good for us", "tokens": [51032, 754, 1673, 309, 311, 29400, 1944, 30, 663, 311, 257, 869, 1168, 13, 286, 360, 519, 309, 576, 312, 665, 337, 505, 51372], "temperature": 0.0, "avg_logprob": -0.06257482634650337, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.002377293771132827}, {"id": 557, "seek": 332608, "start": 3346.24, "end": 3352.56, "text": " to take this transition more slowly than is theoretically possible. And that might happen", "tokens": [51372, 281, 747, 341, 6034, 544, 5692, 813, 307, 29400, 1944, 13, 400, 300, 1062, 1051, 51688], "temperature": 0.0, "avg_logprob": -0.06257482634650337, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.002377293771132827}, {"id": 558, "seek": 335256, "start": 3352.56, "end": 3359.12, "text": " by default if we don't make specific efforts to go slowly. And so I think if people do try and delay", "tokens": [50364, 538, 7576, 498, 321, 500, 380, 652, 2685, 6484, 281, 352, 5692, 13, 400, 370, 286, 519, 498, 561, 360, 853, 293, 8577, 50692], "temperature": 0.0, "avg_logprob": -0.060880001386006674, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.01734223961830139}, {"id": 559, "seek": 335256, "start": 3359.12, "end": 3363.2, "text": " or stop this, it could, it could be a good thing because I don't think we're prepared for that", "tokens": [50692, 420, 1590, 341, 11, 309, 727, 11, 309, 727, 312, 257, 665, 551, 570, 286, 500, 380, 519, 321, 434, 4927, 337, 300, 50896], "temperature": 0.0, "avg_logprob": -0.060880001386006674, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.01734223961830139}, {"id": 560, "seek": 335256, "start": 3363.2, "end": 3369.52, "text": " new world. Right. I think it's going to be very hard to permanently prevent this transition from", "tokens": [50896, 777, 1002, 13, 1779, 13, 286, 519, 309, 311, 516, 281, 312, 588, 1152, 281, 24042, 4871, 341, 6034, 490, 51212], "temperature": 0.0, "avg_logprob": -0.060880001386006674, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.01734223961830139}, {"id": 561, "seek": 335256, "start": 3369.52, "end": 3375.7599999999998, "text": " happening. How come? One way to think about it is that there is some kind of upfront starting cost", "tokens": [51212, 2737, 13, 1012, 808, 30, 1485, 636, 281, 519, 466, 309, 307, 300, 456, 307, 512, 733, 295, 30264, 2891, 2063, 51524], "temperature": 0.0, "avg_logprob": -0.060880001386006674, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.01734223961830139}, {"id": 562, "seek": 335256, "start": 3375.7599999999998, "end": 3382.0, "text": " to get this transition going. So let's really simplify and say, today, if you spent a trillion", "tokens": [51524, 281, 483, 341, 6034, 516, 13, 407, 718, 311, 534, 20460, 293, 584, 11, 965, 11, 498, 291, 4418, 257, 18723, 51836], "temperature": 0.0, "avg_logprob": -0.060880001386006674, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.01734223961830139}, {"id": 563, "seek": 338200, "start": 3382.0, "end": 3389.28, "text": " dollars, you'd be able to train AGI and you'd have enough money left over to buy some manufacturing", "tokens": [50364, 3808, 11, 291, 1116, 312, 1075, 281, 3847, 316, 26252, 293, 291, 1116, 362, 1547, 1460, 1411, 670, 281, 2256, 512, 11096, 50728], "temperature": 0.0, "avg_logprob": -0.05710908770561218, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0120250778272748}, {"id": 564, "seek": 338200, "start": 3389.28, "end": 3394.8, "text": " equipment for making robots. Right. And then you could have your AIs do research into better", "tokens": [50728, 5927, 337, 1455, 14733, 13, 1779, 13, 400, 550, 291, 727, 362, 428, 316, 6802, 360, 2132, 666, 1101, 51004], "temperature": 0.0, "avg_logprob": -0.05710908770561218, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0120250778272748}, {"id": 565, "seek": 338200, "start": 3394.8, "end": 3400.4, "text": " robots and making better AIs. And that whole process could lead to you having even more AIs and even", "tokens": [51004, 14733, 293, 1455, 1101, 316, 6802, 13, 400, 300, 1379, 1399, 727, 1477, 281, 291, 1419, 754, 544, 316, 6802, 293, 754, 51284], "temperature": 0.0, "avg_logprob": -0.05710908770561218, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0120250778272748}, {"id": 566, "seek": 338200, "start": 3400.4, "end": 3405.12, "text": " more robots. And you could then grow your population of AIs and robots. And just with that", "tokens": [51284, 544, 14733, 13, 400, 291, 727, 550, 1852, 428, 4415, 295, 316, 6802, 293, 14733, 13, 400, 445, 365, 300, 51520], "temperature": 0.0, "avg_logprob": -0.05710908770561218, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0120250778272748}, {"id": 567, "seek": 338200, "start": 3405.12, "end": 3410.0, "text": " trillion dollar initial investment, you could end up with this massive AI and robot population,", "tokens": [51520, 18723, 7241, 5883, 6078, 11, 291, 727, 917, 493, 365, 341, 5994, 7318, 293, 7881, 4415, 11, 51764], "temperature": 0.0, "avg_logprob": -0.05710908770561218, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.0120250778272748}, {"id": 568, "seek": 341000, "start": 3410.0, "end": 3416.4, "text": " which is then able to just start doing the scientific work needed to significantly accelerate", "tokens": [50364, 597, 307, 550, 1075, 281, 445, 722, 884, 264, 8134, 589, 2978, 281, 10591, 21341, 50684], "temperature": 0.0, "avg_logprob": -0.09264826500552824, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0016897625755518675}, {"id": 569, "seek": 341000, "start": 3416.4, "end": 3421.92, "text": " technological progress. Right. Right. So the thing that's difficult is that that upfront cost will", "tokens": [50684, 18439, 4205, 13, 1779, 13, 1779, 13, 407, 264, 551, 300, 311, 2252, 307, 300, 300, 30264, 2063, 486, 50960], "temperature": 0.0, "avg_logprob": -0.09264826500552824, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0016897625755518675}, {"id": 570, "seek": 341000, "start": 3421.92, "end": 3429.12, "text": " be falling over time. AI algorithms are improving, computer chips are improving. And so the cost to", "tokens": [50960, 312, 7440, 670, 565, 13, 7318, 14642, 366, 11470, 11, 3820, 11583, 366, 11470, 13, 400, 370, 264, 2063, 281, 51320], "temperature": 0.0, "avg_logprob": -0.09264826500552824, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0016897625755518675}, {"id": 571, "seek": 341000, "start": 3429.12, "end": 3435.2, "text": " kind of training AGI and then just using it to build robots and to build more AIs and make money on", "tokens": [51320, 733, 295, 3097, 316, 26252, 293, 550, 445, 1228, 309, 281, 1322, 14733, 293, 281, 1322, 544, 316, 6802, 293, 652, 1460, 322, 51624], "temperature": 0.0, "avg_logprob": -0.09264826500552824, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0016897625755518675}, {"id": 572, "seek": 343520, "start": 3435.2, "end": 3440.08, "text": " the make money in the economy by selling its services and just kind of building up its own", "tokens": [50364, 264, 652, 1460, 294, 264, 5010, 538, 6511, 1080, 3328, 293, 445, 733, 295, 2390, 493, 1080, 1065, 50608], "temperature": 0.0, "avg_logprob": -0.11354740806247877, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.007206277456134558}, {"id": 573, "seek": 343520, "start": 3440.08, "end": 3445.8399999999997, "text": " self perpetuating energy that ultimately results in making technological progress so you can sell", "tokens": [50608, 2698, 16211, 32438, 2281, 300, 6284, 3542, 294, 1455, 18439, 4205, 370, 291, 393, 3607, 50896], "temperature": 0.0, "avg_logprob": -0.11354740806247877, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.007206277456134558}, {"id": 574, "seek": 343520, "start": 3445.8399999999997, "end": 3451.4399999999996, "text": " more useful things to society that people want. That cost is going to be falling. And so let's", "tokens": [50896, 544, 4420, 721, 281, 4086, 300, 561, 528, 13, 663, 2063, 307, 516, 281, 312, 7440, 13, 400, 370, 718, 311, 51176], "temperature": 0.0, "avg_logprob": -0.11354740806247877, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.007206277456134558}, {"id": 575, "seek": 343520, "start": 3451.4399999999996, "end": 3455.4399999999996, "text": " say it was a trillion today, you know, in the future, it's going to be 100 billion and then it's", "tokens": [51176, 584, 309, 390, 257, 18723, 965, 11, 291, 458, 11, 294, 264, 2027, 11, 309, 311, 516, 281, 312, 2319, 5218, 293, 550, 309, 311, 51376], "temperature": 0.0, "avg_logprob": -0.11354740806247877, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.007206277456134558}, {"id": 576, "seek": 343520, "start": 3455.4399999999996, "end": 3462.3999999999996, "text": " going to be 10 billion. And there's going to be a lot of incentive to do this because it's going to", "tokens": [51376, 516, 281, 312, 1266, 5218, 13, 400, 456, 311, 516, 281, 312, 257, 688, 295, 22346, 281, 360, 341, 570, 309, 311, 516, 281, 51724], "temperature": 0.0, "avg_logprob": -0.11354740806247877, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.007206277456134558}, {"id": 577, "seek": 346240, "start": 3462.4, "end": 3468.96, "text": " grant whoever does it a lot of power. They'll have all these AI workers that they can use to do,", "tokens": [50364, 6386, 11387, 775, 309, 257, 688, 295, 1347, 13, 814, 603, 362, 439, 613, 7318, 5600, 300, 436, 393, 764, 281, 360, 11, 50692], "temperature": 0.0, "avg_logprob": -0.08319590043048469, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.007464150432497263}, {"id": 578, "seek": 346240, "start": 3468.96, "end": 3471.92, "text": " you know, whatever they want them to do if they manage to solve the alignment problem.", "tokens": [50692, 291, 458, 11, 2035, 436, 528, 552, 281, 360, 498, 436, 3067, 281, 5039, 264, 18515, 1154, 13, 50840], "temperature": 0.0, "avg_logprob": -0.08319590043048469, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.007464150432497263}, {"id": 579, "seek": 346240, "start": 3472.48, "end": 3476.8, "text": " If they use it for designing new technologies and those new technologies could grant", "tokens": [50868, 759, 436, 764, 309, 337, 14685, 777, 7943, 293, 729, 777, 7943, 727, 6386, 51084], "temperature": 0.0, "avg_logprob": -0.08319590043048469, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.007464150432497263}, {"id": 580, "seek": 346240, "start": 3476.8, "end": 3482.4, "text": " additional military power, or they could grant things that people all around the world desperately", "tokens": [51084, 4497, 4632, 1347, 11, 420, 436, 727, 6386, 721, 300, 561, 439, 926, 264, 1002, 23726, 51364], "temperature": 0.0, "avg_logprob": -0.08319590043048469, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.007464150432497263}, {"id": 581, "seek": 346240, "start": 3482.4, "end": 3488.56, "text": " want like curing illnesses, like preventing climate change, like understanding and solving", "tokens": [51364, 528, 411, 1262, 278, 30791, 11, 411, 19965, 5659, 1319, 11, 411, 3701, 293, 12606, 51672], "temperature": 0.0, "avg_logprob": -0.08319590043048469, "compression_ratio": 1.7547892720306513, "no_speech_prob": 0.007464150432497263}, {"id": 582, "seek": 348856, "start": 3488.64, "end": 3496.48, "text": " mental health problems, like life extension. So it's not just kind of economic incentives,", "tokens": [50368, 4973, 1585, 2740, 11, 411, 993, 10320, 13, 407, 309, 311, 406, 445, 733, 295, 4836, 23374, 11, 50760], "temperature": 0.0, "avg_logprob": -0.1081226055438702, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.02110077068209648}, {"id": 583, "seek": 348856, "start": 3496.48, "end": 3504.24, "text": " it's not just like to get rich, it's like all sorts of motivations are benefited from paying", "tokens": [50760, 309, 311, 406, 445, 411, 281, 483, 4593, 11, 309, 311, 411, 439, 7527, 295, 39034, 366, 33605, 490, 6229, 51148], "temperature": 0.0, "avg_logprob": -0.1081226055438702, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.02110077068209648}, {"id": 584, "seek": 348856, "start": 3504.24, "end": 3512.7999999999997, "text": " this cost to get this hugely productive AI scientist workforce. Yeah, kind of whatever you want.", "tokens": [51148, 341, 2063, 281, 483, 341, 27417, 13304, 7318, 12662, 14201, 13, 865, 11, 733, 295, 2035, 291, 528, 13, 51576], "temperature": 0.0, "avg_logprob": -0.1081226055438702, "compression_ratio": 1.5053763440860215, "no_speech_prob": 0.02110077068209648}, {"id": 585, "seek": 351280, "start": 3513.6800000000003, "end": 3518.4, "text": " Right. You can probably get it much more effectively if you have a billion AIs and robots", "tokens": [50408, 1779, 13, 509, 393, 1391, 483, 309, 709, 544, 8659, 498, 291, 362, 257, 5218, 316, 6802, 293, 14733, 50644], "temperature": 0.0, "avg_logprob": -0.09823233382146161, "compression_ratio": 1.6443768996960486, "no_speech_prob": 0.11010166257619858}, {"id": 586, "seek": 351280, "start": 3518.4, "end": 3523.2000000000003, "text": " designing technology to help you get it. And I think we can delay it, we can say okay, we're", "tokens": [50644, 14685, 2899, 281, 854, 291, 483, 309, 13, 400, 286, 519, 321, 393, 8577, 309, 11, 321, 393, 584, 1392, 11, 321, 434, 50884], "temperature": 0.0, "avg_logprob": -0.09823233382146161, "compression_ratio": 1.6443768996960486, "no_speech_prob": 0.11010166257619858}, {"id": 587, "seek": 351280, "start": 3523.2000000000003, "end": 3527.1200000000003, "text": " going to be really cautious, only a few people are allowed to train these systems and we try and", "tokens": [50884, 516, 281, 312, 534, 25278, 11, 787, 257, 1326, 561, 366, 4350, 281, 3847, 613, 3652, 293, 321, 853, 293, 51080], "temperature": 0.0, "avg_logprob": -0.09823233382146161, "compression_ratio": 1.6443768996960486, "no_speech_prob": 0.11010166257619858}, {"id": 588, "seek": 351280, "start": 3527.1200000000003, "end": 3531.36, "text": " convince the other countries to go slowly as well. But the thing that we could be, you know,", "tokens": [51080, 13447, 264, 661, 3517, 281, 352, 5692, 382, 731, 13, 583, 264, 551, 300, 321, 727, 312, 11, 291, 458, 11, 51292], "temperature": 0.0, "avg_logprob": -0.09823233382146161, "compression_ratio": 1.6443768996960486, "no_speech_prob": 0.11010166257619858}, {"id": 589, "seek": 351280, "start": 3532.0, "end": 3536.88, "text": " that even 100 years after it's first been possible to train AI for a trillion dollars,", "tokens": [51324, 300, 754, 2319, 924, 934, 309, 311, 700, 668, 1944, 281, 3847, 7318, 337, 257, 18723, 3808, 11, 51568], "temperature": 0.0, "avg_logprob": -0.09823233382146161, "compression_ratio": 1.6443768996960486, "no_speech_prob": 0.11010166257619858}, {"id": 590, "seek": 351280, "start": 3536.88, "end": 3541.1200000000003, "text": " that still no one has done it and no one is using it to make scientific progress,", "tokens": [51568, 300, 920, 572, 472, 575, 1096, 309, 293, 572, 472, 307, 1228, 309, 281, 652, 8134, 4205, 11, 51780], "temperature": 0.0, "avg_logprob": -0.09823233382146161, "compression_ratio": 1.6443768996960486, "no_speech_prob": 0.11010166257619858}, {"id": 591, "seek": 354112, "start": 3541.12, "end": 3548.08, "text": " even though the cost is now like $10 million, it's really hard to imagine that we kind of", "tokens": [50364, 754, 1673, 264, 2063, 307, 586, 411, 1848, 3279, 2459, 11, 309, 311, 534, 1152, 281, 3811, 300, 321, 733, 295, 50712], "temperature": 0.0, "avg_logprob": -0.09708751099450248, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004242855589836836}, {"id": 592, "seek": 354112, "start": 3548.08, "end": 3555.8399999999997, "text": " prevent anyone from doing it as it gets cheaper. And I think sometimes people, when they're", "tokens": [50712, 4871, 2878, 490, 884, 309, 382, 309, 2170, 12284, 13, 400, 286, 519, 2171, 561, 11, 562, 436, 434, 51100], "temperature": 0.0, "avg_logprob": -0.09708751099450248, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004242855589836836}, {"id": 593, "seek": 354112, "start": 3555.8399999999997, "end": 3561.2799999999997, "text": " thinking about it, imagine that in order to get this kind of 10 or 100x faster technological", "tokens": [51100, 1953, 466, 309, 11, 3811, 300, 294, 1668, 281, 483, 341, 733, 295, 1266, 420, 2319, 87, 4663, 18439, 51372], "temperature": 0.0, "avg_logprob": -0.09708751099450248, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004242855589836836}, {"id": 594, "seek": 354112, "start": 3561.2799999999997, "end": 3564.7999999999997, "text": " progress, we'd have to be making a real effort and really kind of being super efficient and", "tokens": [51372, 4205, 11, 321, 1116, 362, 281, 312, 1455, 257, 957, 4630, 293, 534, 733, 295, 885, 1687, 7148, 293, 51548], "temperature": 0.0, "avg_logprob": -0.09708751099450248, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004242855589836836}, {"id": 595, "seek": 354112, "start": 3564.7999999999997, "end": 3568.48, "text": " driven about it. But I think that's not the right way to think about it. It's more like", "tokens": [51548, 9555, 466, 309, 13, 583, 286, 519, 300, 311, 406, 264, 558, 636, 281, 519, 466, 309, 13, 467, 311, 544, 411, 51732], "temperature": 0.0, "avg_logprob": -0.09708751099450248, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.004242855589836836}, {"id": 596, "seek": 356848, "start": 3569.2, "end": 3574.32, "text": " by default, all you need to do is ask your AIs and robots, please do these tasks for me. And if", "tokens": [50400, 538, 7576, 11, 439, 291, 643, 281, 360, 307, 1029, 428, 316, 6802, 293, 14733, 11, 1767, 360, 613, 9608, 337, 385, 13, 400, 498, 50656], "temperature": 0.0, "avg_logprob": -0.0970784640703045, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.007324905600398779}, {"id": 597, "seek": 356848, "start": 3574.32, "end": 3578.32, "text": " you need to make tech progress along the way, do it, they will suggest the plans and involve", "tokens": [50656, 291, 643, 281, 652, 7553, 4205, 2051, 264, 636, 11, 360, 309, 11, 436, 486, 3402, 264, 5482, 293, 9494, 50856], "temperature": 0.0, "avg_logprob": -0.0970784640703045, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.007324905600398779}, {"id": 598, "seek": 356848, "start": 3578.32, "end": 3581.92, "text": " making tech progress. They will get in contact with the labs and organize for the experiments to", "tokens": [50856, 1455, 7553, 4205, 13, 814, 486, 483, 294, 3385, 365, 264, 20339, 293, 13859, 337, 264, 12050, 281, 51036], "temperature": 0.0, "avg_logprob": -0.0970784640703045, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.007324905600398779}, {"id": 599, "seek": 356848, "start": 3581.92, "end": 3586.88, "text": " happen. You won't have to do anything. Right. And so I don't think it's going to require some kind of", "tokens": [51036, 1051, 13, 509, 1582, 380, 362, 281, 360, 1340, 13, 1779, 13, 400, 370, 286, 500, 380, 519, 309, 311, 516, 281, 3651, 512, 733, 295, 51284], "temperature": 0.0, "avg_logprob": -0.0970784640703045, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.007324905600398779}, {"id": 600, "seek": 356848, "start": 3586.88, "end": 3593.84, "text": " concerted pro growth enthusiasts to like really push for this. It's more like, you want stuff,", "tokens": [51284, 8543, 292, 447, 4599, 45873, 281, 411, 534, 2944, 337, 341, 13, 467, 311, 544, 411, 11, 291, 528, 1507, 11, 51632], "temperature": 0.0, "avg_logprob": -0.0970784640703045, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.007324905600398779}, {"id": 601, "seek": 359384, "start": 3593.84, "end": 3599.2000000000003, "text": " the AI is going to try and do the stuff you want. And whenever they make tech progress,", "tokens": [50364, 264, 7318, 307, 516, 281, 853, 293, 360, 264, 1507, 291, 528, 13, 400, 5699, 436, 652, 7553, 4205, 11, 50632], "temperature": 0.0, "avg_logprob": -0.07986022807933667, "compression_ratio": 1.725, "no_speech_prob": 0.011833790689706802}, {"id": 602, "seek": 359384, "start": 3599.2000000000003, "end": 3603.36, "text": " it's going to go really well. And it's going to really help you solve your problems.", "tokens": [50632, 309, 311, 516, 281, 352, 534, 731, 13, 400, 309, 311, 516, 281, 534, 854, 291, 5039, 428, 2740, 13, 50840], "temperature": 0.0, "avg_logprob": -0.07986022807933667, "compression_ratio": 1.725, "no_speech_prob": 0.011833790689706802}, {"id": 603, "seek": 359384, "start": 3603.36, "end": 3604.8, "text": " And you're going to just want to do more of it.", "tokens": [50840, 400, 291, 434, 516, 281, 445, 528, 281, 360, 544, 295, 309, 13, 50912], "temperature": 0.0, "avg_logprob": -0.07986022807933667, "compression_ratio": 1.725, "no_speech_prob": 0.011833790689706802}, {"id": 604, "seek": 359384, "start": 3605.36, "end": 3610.8, "text": " Yeah, just enough time will pass, enough actors will think on it and decide to do it at some", "tokens": [50940, 865, 11, 445, 1547, 565, 486, 1320, 11, 1547, 10037, 486, 519, 322, 309, 293, 4536, 281, 360, 309, 412, 512, 51212], "temperature": 0.0, "avg_logprob": -0.07986022807933667, "compression_ratio": 1.725, "no_speech_prob": 0.011833790689706802}, {"id": 605, "seek": 359384, "start": 3610.8, "end": 3618.2400000000002, "text": " point. Yeah. So I guess I buy that the incentives are there for eventually an actor to want to build", "tokens": [51212, 935, 13, 865, 13, 407, 286, 2041, 286, 2256, 300, 264, 23374, 366, 456, 337, 4728, 364, 8747, 281, 528, 281, 1322, 51584], "temperature": 0.0, "avg_logprob": -0.07986022807933667, "compression_ratio": 1.725, "no_speech_prob": 0.011833790689706802}, {"id": 606, "seek": 361824, "start": 3618.24, "end": 3626.56, "text": " this kind of AI scientist workforce. It still seems like there have been enormously lucrative", "tokens": [50364, 341, 733, 295, 7318, 12662, 14201, 13, 467, 920, 2544, 411, 456, 362, 668, 39669, 21296, 30457, 50780], "temperature": 0.0, "avg_logprob": -0.0927056002329631, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.050284795463085175}, {"id": 607, "seek": 361824, "start": 3626.56, "end": 3633.9199999999996, "text": " and beneficial technologies that we haven't pursued. So one example that comes to mind", "tokens": [50780, 293, 14072, 7943, 300, 321, 2378, 380, 34893, 13, 407, 472, 1365, 300, 1487, 281, 1575, 51148], "temperature": 0.0, "avg_logprob": -0.0927056002329631, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.050284795463085175}, {"id": 608, "seek": 361824, "start": 3633.9199999999996, "end": 3640.64, "text": " is like nuclear power, which like could help loads with climate change and would also be,", "tokens": [51148, 307, 411, 8179, 1347, 11, 597, 411, 727, 854, 12668, 365, 5659, 1319, 293, 576, 611, 312, 11, 51484], "temperature": 0.0, "avg_logprob": -0.0927056002329631, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.050284795463085175}, {"id": 609, "seek": 361824, "start": 3640.64, "end": 3645.6, "text": " yeah, again, super lucrative. And yet, yeah, we basically haven't done anything like what we", "tokens": [51484, 1338, 11, 797, 11, 1687, 21296, 30457, 13, 400, 1939, 11, 1338, 11, 321, 1936, 2378, 380, 1096, 1340, 411, 437, 321, 51732], "temperature": 0.0, "avg_logprob": -0.0927056002329631, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.050284795463085175}, {"id": 610, "seek": 364560, "start": 3645.6, "end": 3652.16, "text": " could do with it. Could there be something similar? I mean, it's just kind of stigmatized,", "tokens": [50364, 727, 360, 365, 309, 13, 7497, 456, 312, 746, 2531, 30, 286, 914, 11, 309, 311, 445, 733, 295, 342, 46496, 1602, 11, 50692], "temperature": 0.0, "avg_logprob": -0.09771374789151278, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.0041642277501523495}, {"id": 611, "seek": 364560, "start": 3652.16, "end": 3657.52, "text": " is like one reason we haven't. And I guess it's really expensive in particular the upfront costs,", "tokens": [50692, 307, 411, 472, 1778, 321, 2378, 380, 13, 400, 286, 2041, 309, 311, 534, 5124, 294, 1729, 264, 30264, 5497, 11, 50960], "temperature": 0.0, "avg_logprob": -0.09771374789151278, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.0041642277501523495}, {"id": 612, "seek": 364560, "start": 3657.52, "end": 3663.12, "text": " which like maybe just ends up true of this like AI world we're talking about.", "tokens": [50960, 597, 411, 1310, 445, 5314, 493, 2074, 295, 341, 411, 7318, 1002, 321, 434, 1417, 466, 13, 51240], "temperature": 0.0, "avg_logprob": -0.09771374789151278, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.0041642277501523495}, {"id": 613, "seek": 364560, "start": 3664.24, "end": 3667.44, "text": " Yeah, it's a great example. I don't have a good understanding of what happened,", "tokens": [51296, 865, 11, 309, 311, 257, 869, 1365, 13, 286, 500, 380, 362, 257, 665, 3701, 295, 437, 2011, 11, 51456], "temperature": 0.0, "avg_logprob": -0.09771374789151278, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.0041642277501523495}, {"id": 614, "seek": 364560, "start": 3667.44, "end": 3673.8399999999997, "text": " but I think there are some big catastrophes with nuclear power, and then it became very", "tokens": [51456, 457, 286, 519, 456, 366, 512, 955, 28363, 279, 365, 8179, 1347, 11, 293, 550, 309, 3062, 588, 51776], "temperature": 0.0, "avg_logprob": -0.09771374789151278, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.0041642277501523495}, {"id": 615, "seek": 367384, "start": 3673.84, "end": 3679.36, "text": " stigmatized. And the regulatory requirements around it and the safety requirements became", "tokens": [50364, 342, 46496, 1602, 13, 400, 264, 18260, 7728, 926, 309, 293, 264, 4514, 7728, 3062, 50640], "temperature": 0.0, "avg_logprob": -0.11474074593073205, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004839468747377396}, {"id": 616, "seek": 367384, "start": 3680.0, "end": 3687.36, "text": " very large, much larger really than was reasonable, given that fossil fuel energy has damaging health", "tokens": [50672, 588, 2416, 11, 709, 4833, 534, 813, 390, 10585, 11, 2212, 300, 18737, 6616, 2281, 575, 25342, 1585, 51040], "temperature": 0.0, "avg_logprob": -0.11474074593073205, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004839468747377396}, {"id": 617, "seek": 367384, "start": 3687.36, "end": 3694.32, "text": " consequences as well through air pollution. And as a result, it just became kind of a mixture", "tokens": [51040, 10098, 382, 731, 807, 1988, 16727, 13, 400, 382, 257, 1874, 11, 309, 445, 3062, 733, 295, 257, 9925, 51388], "temperature": 0.0, "avg_logprob": -0.11474074593073205, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004839468747377396}, {"id": 618, "seek": 367384, "start": 3694.32, "end": 3699.44, "text": " of stigma and just the additional cost from all the regulation just prevented it from being rolled", "tokens": [51388, 295, 27880, 293, 445, 264, 4497, 2063, 490, 439, 264, 15062, 445, 27314, 309, 490, 885, 14306, 51644], "temperature": 0.0, "avg_logprob": -0.11474074593073205, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004839468747377396}, {"id": 619, "seek": 369944, "start": 3699.44, "end": 3706.56, "text": " out. But I do think there are a fair few very significant disanalogies between that case and", "tokens": [50364, 484, 13, 583, 286, 360, 519, 456, 366, 257, 3143, 1326, 588, 4776, 717, 29702, 664, 530, 1296, 300, 1389, 293, 50720], "temperature": 0.0, "avg_logprob": -0.08554014205932617, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.009105674922466278}, {"id": 620, "seek": 369944, "start": 3707.44, "end": 3715.44, "text": " the case of AI. Okay, yeah, what are they? So one thing is that there were other sources of", "tokens": [50764, 264, 1389, 295, 7318, 13, 1033, 11, 1338, 11, 437, 366, 436, 30, 407, 472, 551, 307, 300, 456, 645, 661, 7139, 295, 51164], "temperature": 0.0, "avg_logprob": -0.08554014205932617, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.009105674922466278}, {"id": 621, "seek": 369944, "start": 3715.44, "end": 3720.64, "text": " energy that were available. And so it wasn't too costly to be like, well, we're not going to use", "tokens": [51164, 2281, 300, 645, 2435, 13, 400, 370, 309, 2067, 380, 886, 28328, 281, 312, 411, 11, 731, 11, 321, 434, 406, 516, 281, 764, 51424], "temperature": 0.0, "avg_logprob": -0.08554014205932617, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.009105674922466278}, {"id": 622, "seek": 369944, "start": 3720.64, "end": 3725.6, "text": " nuclear, we're going to use fossil fuels instead. And then, you know, even the green climate change", "tokens": [51424, 8179, 11, 321, 434, 516, 281, 764, 18737, 24616, 2602, 13, 400, 550, 11, 291, 458, 11, 754, 264, 3092, 5659, 1319, 51672], "temperature": 0.0, "avg_logprob": -0.08554014205932617, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.009105674922466278}, {"id": 623, "seek": 372560, "start": 3725.6, "end": 3730.08, "text": " concern, people could think about kind of developing solar panels and renewable energies.", "tokens": [50364, 3136, 11, 561, 727, 519, 466, 733, 295, 6416, 7936, 13419, 293, 20938, 25737, 13, 50588], "temperature": 0.0, "avg_logprob": -0.10398529623156395, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.003265123348683119}, {"id": 624, "seek": 372560, "start": 3731.04, "end": 3736.48, "text": " And in the AI case, that there is going to be no alternative. There's going to be no alternative", "tokens": [50636, 400, 294, 264, 7318, 1389, 11, 300, 456, 307, 516, 281, 312, 572, 8535, 13, 821, 311, 516, 281, 312, 572, 8535, 50908], "temperature": 0.0, "avg_logprob": -0.10398529623156395, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.003265123348683119}, {"id": 625, "seek": 372560, "start": 3736.48, "end": 3744.48, "text": " technology which can solve all illness, and which can grant your nation massive national", "tokens": [50908, 2899, 597, 393, 5039, 439, 10152, 11, 293, 597, 393, 6386, 428, 4790, 5994, 4048, 51308], "temperature": 0.0, "avg_logprob": -0.10398529623156395, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.003265123348683119}, {"id": 626, "seek": 372560, "start": 3744.48, "end": 3748.88, "text": " security and military power, and that can solve climate change. This is going to be the only", "tokens": [51308, 3825, 293, 4632, 1347, 11, 293, 300, 393, 5039, 5659, 1319, 13, 639, 307, 516, 281, 312, 264, 787, 51528], "temperature": 0.0, "avg_logprob": -0.10398529623156395, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.003265123348683119}, {"id": 627, "seek": 372560, "start": 3748.88, "end": 3755.2799999999997, "text": " option. So that's one disanalogy. Okay, that makes sense. Another kind of disanalogy is the", "tokens": [51528, 3614, 13, 407, 300, 311, 472, 717, 29702, 7794, 13, 1033, 11, 300, 1669, 2020, 13, 3996, 733, 295, 717, 29702, 7794, 307, 264, 51848], "temperature": 0.0, "avg_logprob": -0.10398529623156395, "compression_ratio": 1.7829457364341086, "no_speech_prob": 0.003265123348683119}, {"id": 628, "seek": 375528, "start": 3755.28, "end": 3761.84, "text": " cost factor. So with nuclear power, it's become more expensive at a time due to regulations. And", "tokens": [50364, 2063, 5952, 13, 407, 365, 8179, 1347, 11, 309, 311, 1813, 544, 5124, 412, 257, 565, 3462, 281, 12563, 13, 400, 50692], "temperature": 0.0, "avg_logprob": -0.10753545095754224, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0010385331697762012}, {"id": 629, "seek": 375528, "start": 3761.84, "end": 3767.6000000000004, "text": " that that's been a big factor in not being pursued. But the specifics around these cost", "tokens": [50692, 300, 300, 311, 668, 257, 955, 5952, 294, 406, 885, 34893, 13, 583, 264, 28454, 926, 613, 2063, 50980], "temperature": 0.0, "avg_logprob": -0.10753545095754224, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0010385331697762012}, {"id": 630, "seek": 375528, "start": 3767.6000000000004, "end": 3774.88, "text": " curves with compute and this algorithmic progress patterns suggests that the upfront cost of training", "tokens": [50980, 19490, 365, 14722, 293, 341, 9284, 299, 4205, 8294, 13409, 300, 264, 30264, 2063, 295, 3097, 51344], "temperature": 0.0, "avg_logprob": -0.10753545095754224, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0010385331697762012}, {"id": 631, "seek": 375528, "start": 3774.88, "end": 3781.2000000000003, "text": " AGI is going to be falling really pretty quickly over time. Right. And so even if initially you", "tokens": [51344, 316, 26252, 307, 516, 281, 312, 7440, 534, 1238, 2661, 670, 565, 13, 1779, 13, 400, 370, 754, 498, 9105, 291, 51660], "temperature": 0.0, "avg_logprob": -0.10753545095754224, "compression_ratio": 1.5720164609053497, "no_speech_prob": 0.0010385331697762012}, {"id": 632, "seek": 378120, "start": 3781.2, "end": 3785.3599999999997, "text": " put loads of regulations, which make it very expensive, it's really not going to be long until", "tokens": [50364, 829, 12668, 295, 12563, 11, 597, 652, 309, 588, 5124, 11, 309, 311, 534, 406, 516, 281, 312, 938, 1826, 50572], "temperature": 0.0, "avg_logprob": -0.09813322219173465, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.023804832249879837}, {"id": 633, "seek": 378120, "start": 3785.3599999999997, "end": 3790.72, "text": " it's 10x cheaper. Right. And so permanently preventing it when it's when it's becoming cheaper", "tokens": [50572, 309, 311, 1266, 87, 12284, 13, 1779, 13, 400, 370, 24042, 19965, 309, 562, 309, 311, 562, 309, 311, 5617, 12284, 50840], "temperature": 0.0, "avg_logprob": -0.09813322219173465, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.023804832249879837}, {"id": 634, "seek": 378120, "start": 3790.72, "end": 3796.16, "text": " and cheaper at such a high rate, it's going to be really, really difficult. Third is just", "tokens": [50840, 293, 12284, 412, 1270, 257, 1090, 3314, 11, 309, 311, 516, 281, 312, 534, 11, 534, 2252, 13, 12548, 307, 445, 51112], "temperature": 0.0, "avg_logprob": -0.09813322219173465, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.023804832249879837}, {"id": 635, "seek": 378120, "start": 3796.8799999999997, "end": 3803.2799999999997, "text": " just talking about the size of the gains from from this technology compared to nuclear power.", "tokens": [51148, 445, 1417, 466, 264, 2744, 295, 264, 16823, 490, 490, 341, 2899, 5347, 281, 8179, 1347, 13, 51468], "temperature": 0.0, "avg_logprob": -0.09813322219173465, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.023804832249879837}, {"id": 636, "seek": 378120, "start": 3803.2799999999997, "end": 3807.8399999999997, "text": " So, you know, France adopted nuclear power and it was somewhat beneficial, you know, it's kind of", "tokens": [51468, 407, 11, 291, 458, 11, 6190, 12175, 8179, 1347, 293, 309, 390, 8344, 14072, 11, 291, 458, 11, 309, 311, 733, 295, 51696], "temperature": 0.0, "avg_logprob": -0.09813322219173465, "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.023804832249879837}, {"id": 637, "seek": 380784, "start": 3807.84, "end": 3812.1600000000003, "text": " now gets a lot of its powerful nuclear energy and that there's no climate change impacts and that's", "tokens": [50364, 586, 2170, 257, 688, 295, 1080, 4005, 8179, 2281, 293, 300, 456, 311, 572, 5659, 1319, 11606, 293, 300, 311, 50580], "temperature": 0.0, "avg_logprob": -0.0867953136049468, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.007279952988028526}, {"id": 638, "seek": 380784, "start": 3812.1600000000003, "end": 3820.08, "text": " great. But it's not as if France is visibly and undisputably just doing amazing well as a country", "tokens": [50580, 869, 13, 583, 309, 311, 406, 382, 498, 6190, 307, 1452, 3545, 293, 674, 271, 2582, 1188, 445, 884, 2243, 731, 382, 257, 1941, 50976], "temperature": 0.0, "avg_logprob": -0.0867953136049468, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.007279952988028526}, {"id": 639, "seek": 380784, "start": 3820.08, "end": 3824.1600000000003, "text": " because it's got, you know, this nuclear power, like it's a kind of a modest addition, maybe it", "tokens": [50976, 570, 309, 311, 658, 11, 291, 458, 11, 341, 8179, 1347, 11, 411, 309, 311, 257, 733, 295, 257, 25403, 4500, 11, 1310, 309, 51180], "temperature": 0.0, "avg_logprob": -0.0867953136049468, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.007279952988028526}, {"id": 640, "seek": 380784, "start": 3824.1600000000003, "end": 3829.76, "text": " makes it look a little bit better. Right. But by contrast, if one country is is, you know,", "tokens": [51180, 1669, 309, 574, 257, 707, 857, 1101, 13, 1779, 13, 583, 538, 8712, 11, 498, 472, 1941, 307, 307, 11, 291, 458, 11, 51460], "temperature": 0.0, "avg_logprob": -0.0867953136049468, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.007279952988028526}, {"id": 641, "seek": 380784, "start": 3829.76, "end": 3833.44, "text": " progressing technology at the normal rate, and then another country comes along", "tokens": [51460, 36305, 2899, 412, 264, 2710, 3314, 11, 293, 550, 1071, 1941, 1487, 2051, 51644], "temperature": 0.0, "avg_logprob": -0.0867953136049468, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.007279952988028526}, {"id": 642, "seek": 383344, "start": 3834.08, "end": 3840.2400000000002, "text": " and starts using these AIs and robots a little bit, you're going to see very significant differences", "tokens": [50396, 293, 3719, 1228, 613, 316, 6802, 293, 14733, 257, 707, 857, 11, 291, 434, 516, 281, 536, 588, 4776, 7300, 50704], "temperature": 0.0, "avg_logprob": -0.12358302161807105, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.018396500498056412}, {"id": 643, "seek": 383344, "start": 3840.2400000000002, "end": 3846.08, "text": " in how its overall technology and prosperity and kind of military power is progressing.", "tokens": [50704, 294, 577, 1080, 4787, 2899, 293, 22434, 293, 733, 295, 4632, 1347, 307, 36305, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12358302161807105, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.018396500498056412}, {"id": 644, "seek": 383344, "start": 3846.7200000000003, "end": 3851.2000000000003, "text": " And then you're going to see that as countries dial up how much they're allowing AIs to do this", "tokens": [51028, 400, 550, 291, 434, 516, 281, 536, 300, 382, 3517, 5502, 493, 577, 709, 436, 434, 8293, 316, 6802, 281, 360, 341, 51252], "temperature": 0.0, "avg_logprob": -0.12358302161807105, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.018396500498056412}, {"id": 645, "seek": 383344, "start": 3851.2000000000003, "end": 3856.0, "text": " work, that there are then bigger and bigger differences there. And ultimately, the difference", "tokens": [51252, 589, 11, 300, 456, 366, 550, 3801, 293, 3801, 7300, 456, 13, 400, 6284, 11, 264, 2649, 51492], "temperature": 0.0, "avg_logprob": -0.12358302161807105, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.018396500498056412}, {"id": 646, "seek": 385600, "start": 3856.0, "end": 3863.36, "text": " between advancing technology at our pace versus advancing technology 30 times faster is over the", "tokens": [50364, 1296, 27267, 2899, 412, 527, 11638, 5717, 27267, 2899, 2217, 1413, 4663, 307, 670, 264, 50732], "temperature": 0.0, "avg_logprob": -0.08625902912833473, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.14393563568592072}, {"id": 647, "seek": 385600, "start": 3863.36, "end": 3868.88, "text": " course of just a few years, it becomes a massive difference in the sophistication of your country's", "tokens": [50732, 1164, 295, 445, 257, 1326, 924, 11, 309, 3643, 257, 5994, 2649, 294, 264, 15572, 399, 295, 428, 1941, 311, 51008], "temperature": 0.0, "avg_logprob": -0.08625902912833473, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.14393563568592072}, {"id": 648, "seek": 385600, "start": 3868.88, "end": 3874.8, "text": " technology and ability to solve all kinds of social and political problems. You know, a last point", "tokens": [51008, 2899, 293, 3485, 281, 5039, 439, 3685, 295, 2093, 293, 3905, 2740, 13, 509, 458, 11, 257, 1036, 935, 51304], "temperature": 0.0, "avg_logprob": -0.08625902912833473, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.14393563568592072}, {"id": 649, "seek": 385600, "start": 3875.6, "end": 3882.56, "text": " on this difference is that, you know, the US did in fact invest a lot of money in nukes shortly", "tokens": [51344, 322, 341, 2649, 307, 300, 11, 291, 458, 11, 264, 2546, 630, 294, 1186, 1963, 257, 688, 295, 1460, 294, 297, 2034, 279, 13392, 51692], "temperature": 0.0, "avg_logprob": -0.08625902912833473, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.14393563568592072}, {"id": 650, "seek": 388256, "start": 3882.56, "end": 3886.88, "text": " after the development of fish and power. You know, when it came to a matter of national power,", "tokens": [50364, 934, 264, 3250, 295, 3506, 293, 1347, 13, 509, 458, 11, 562, 309, 1361, 281, 257, 1871, 295, 4048, 1347, 11, 50580], "temperature": 0.0, "avg_logprob": -0.09861253265641694, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.006888889241963625}, {"id": 651, "seek": 388256, "start": 3886.88, "end": 3892.48, "text": " they were very happy to invest in the technology, despite, you know, the risks which were clearly", "tokens": [50580, 436, 645, 588, 2055, 281, 1963, 294, 264, 2899, 11, 7228, 11, 291, 458, 11, 264, 10888, 597, 645, 4448, 50860], "temperature": 0.0, "avg_logprob": -0.09861253265641694, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.006888889241963625}, {"id": 652, "seek": 388256, "start": 3892.48, "end": 3897.92, "text": " very high. All of the same risks. Right. Yes, you know, the incentives were out of whack and we", "tokens": [50860, 588, 1090, 13, 1057, 295, 264, 912, 10888, 13, 1779, 13, 1079, 11, 291, 458, 11, 264, 23374, 645, 484, 295, 42877, 293, 321, 51132], "temperature": 0.0, "avg_logprob": -0.09861253265641694, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.006888889241963625}, {"id": 653, "seek": 388256, "start": 3897.92, "end": 3903.44, "text": " didn't get nuclear fission power. But when it came to this kind of military technology for which", "tokens": [51132, 994, 380, 483, 8179, 283, 3106, 1347, 13, 583, 562, 309, 1361, 281, 341, 733, 295, 4632, 2899, 337, 597, 51408], "temperature": 0.0, "avg_logprob": -0.09861253265641694, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.006888889241963625}, {"id": 654, "seek": 388256, "start": 3903.44, "end": 3908.56, "text": " there was no replacement, countries were very keen to do it and they made it happen. And AI", "tokens": [51408, 456, 390, 572, 14419, 11, 3517, 645, 588, 20297, 281, 360, 309, 293, 436, 1027, 309, 1051, 13, 400, 7318, 51664], "temperature": 0.0, "avg_logprob": -0.09861253265641694, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.006888889241963625}, {"id": 655, "seek": 390856, "start": 3908.56, "end": 3913.6, "text": " driving significant technological improvements across the board is going to be a huge source", "tokens": [50364, 4840, 4776, 18439, 13797, 2108, 264, 3150, 307, 516, 281, 312, 257, 2603, 4009, 50616], "temperature": 0.0, "avg_logprob": -0.08676394294289981, "compression_ratio": 1.575, "no_speech_prob": 0.004726293496787548}, {"id": 656, "seek": 390856, "start": 3913.6, "end": 3919.52, "text": " of military power. Right. And so it's really hard for me to imagine that just no one ever uses it", "tokens": [50616, 295, 4632, 1347, 13, 1779, 13, 400, 370, 309, 311, 534, 1152, 337, 385, 281, 3811, 300, 445, 572, 472, 1562, 4960, 309, 50912], "temperature": 0.0, "avg_logprob": -0.08676394294289981, "compression_ratio": 1.575, "no_speech_prob": 0.004726293496787548}, {"id": 657, "seek": 390856, "start": 3919.52, "end": 3926.24, "text": " for that. And you've totally preempted my next question, which was like, can we definitely not", "tokens": [50912, 337, 300, 13, 400, 291, 600, 3879, 659, 4543, 292, 452, 958, 1168, 11, 597, 390, 411, 11, 393, 321, 2138, 406, 51248], "temperature": 0.0, "avg_logprob": -0.08676394294289981, "compression_ratio": 1.575, "no_speech_prob": 0.004726293496787548}, {"id": 658, "seek": 390856, "start": 3926.24, "end": 3933.6, "text": " come up with like an international treaty that is like the downside risks of this technology", "tokens": [51248, 808, 493, 365, 411, 364, 5058, 24772, 300, 307, 411, 264, 25060, 10888, 295, 341, 2899, 51616], "temperature": 0.0, "avg_logprob": -0.08676394294289981, "compression_ratio": 1.575, "no_speech_prob": 0.004726293496787548}, {"id": 659, "seek": 393360, "start": 3933.6, "end": 3941.44, "text": " at this scale are possibly huge because alignment is so hard. And so we're all agreeing", "tokens": [50364, 412, 341, 4373, 366, 6264, 2603, 570, 18515, 307, 370, 1152, 13, 400, 370, 321, 434, 439, 36900, 50756], "temperature": 0.0, "avg_logprob": -0.10592453479766846, "compression_ratio": 1.991304347826087, "no_speech_prob": 0.0052731395699083805}, {"id": 660, "seek": 393360, "start": 3941.44, "end": 3947.8399999999997, "text": " not to not to go forward with it. And I guess we had lots of reason to do that in the case of", "tokens": [50756, 406, 281, 406, 281, 352, 2128, 365, 309, 13, 400, 286, 2041, 321, 632, 3195, 295, 1778, 281, 360, 300, 294, 264, 1389, 295, 51076], "temperature": 0.0, "avg_logprob": -0.10592453479766846, "compression_ratio": 1.991304347826087, "no_speech_prob": 0.0052731395699083805}, {"id": 661, "seek": 393360, "start": 3947.8399999999997, "end": 3952.16, "text": " nuclear weapons. And we didn't. And we have lots of reason to do that in the case of biological", "tokens": [51076, 8179, 7278, 13, 400, 321, 994, 380, 13, 400, 321, 362, 3195, 295, 1778, 281, 360, 300, 294, 264, 1389, 295, 13910, 51292], "temperature": 0.0, "avg_logprob": -0.10592453479766846, "compression_ratio": 1.991304347826087, "no_speech_prob": 0.0052731395699083805}, {"id": 662, "seek": 393360, "start": 3952.16, "end": 3957.7599999999998, "text": " weapons. And and we suck at it. We do not live in a world free of biological weapons or nuclear", "tokens": [51292, 7278, 13, 400, 293, 321, 9967, 412, 309, 13, 492, 360, 406, 1621, 294, 257, 1002, 1737, 295, 13910, 7278, 420, 8179, 51572], "temperature": 0.0, "avg_logprob": -0.10592453479766846, "compression_ratio": 1.991304347826087, "no_speech_prob": 0.0052731395699083805}, {"id": 663, "seek": 393360, "start": 3957.7599999999998, "end": 3963.44, "text": " weapons. So I do think we should try. And I do think we can slow things down. And we", "tokens": [51572, 7278, 13, 407, 286, 360, 519, 321, 820, 853, 13, 400, 286, 360, 519, 321, 393, 2964, 721, 760, 13, 400, 321, 51856], "temperature": 0.0, "avg_logprob": -0.10592453479766846, "compression_ratio": 1.991304347826087, "no_speech_prob": 0.0052731395699083805}, {"id": 664, "seek": 396344, "start": 3963.44, "end": 3969.2000000000003, "text": " can, you know, increase the requirements and the safety efforts required, maybe to make it 10", "tokens": [50364, 393, 11, 291, 458, 11, 3488, 264, 7728, 293, 264, 4514, 6484, 4739, 11, 1310, 281, 652, 309, 1266, 50652], "temperature": 0.0, "avg_logprob": -0.08583544890085856, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.0006916187703609467}, {"id": 665, "seek": 396344, "start": 3969.2000000000003, "end": 3975.52, "text": " times as costly or 100 times as costly to develop this technology. I think that is that is one thing.", "tokens": [50652, 1413, 382, 28328, 420, 2319, 1413, 382, 28328, 281, 1499, 341, 2899, 13, 286, 519, 300, 307, 300, 307, 472, 551, 13, 50968], "temperature": 0.0, "avg_logprob": -0.08583544890085856, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.0006916187703609467}, {"id": 666, "seek": 396344, "start": 3975.52, "end": 3980.48, "text": " And that's a big ask. And I think we should try and do as much of it as we can. Even if we can do", "tokens": [50968, 400, 300, 311, 257, 955, 1029, 13, 400, 286, 519, 321, 820, 853, 293, 360, 382, 709, 295, 309, 382, 321, 393, 13, 2754, 498, 321, 393, 360, 51216], "temperature": 0.0, "avg_logprob": -0.08583544890085856, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.0006916187703609467}, {"id": 667, "seek": 396344, "start": 3980.48, "end": 3986.32, "text": " that, it's a whole different thing to talk about permanently choosing to never develop the technology,", "tokens": [51216, 300, 11, 309, 311, 257, 1379, 819, 551, 281, 751, 466, 24042, 10875, 281, 1128, 1499, 264, 2899, 11, 51508], "temperature": 0.0, "avg_logprob": -0.08583544890085856, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.0006916187703609467}, {"id": 668, "seek": 396344, "start": 3986.32, "end": 3992.2400000000002, "text": " even after we've made maximal efforts into making it safe, even after all the safety tests are saying", "tokens": [51508, 754, 934, 321, 600, 1027, 49336, 6484, 666, 1455, 309, 3273, 11, 754, 934, 439, 264, 4514, 6921, 366, 1566, 51804], "temperature": 0.0, "avg_logprob": -0.08583544890085856, "compression_ratio": 1.8109090909090908, "no_speech_prob": 0.0006916187703609467}, {"id": 669, "seek": 399224, "start": 3992.24, "end": 3997.3599999999997, "text": " it looks like it is safe, even when millions of people are dying every year from illnesses which", "tokens": [50364, 309, 1542, 411, 309, 307, 3273, 11, 754, 562, 6803, 295, 561, 366, 8639, 633, 1064, 490, 30791, 597, 50620], "temperature": 0.0, "avg_logprob": -0.08978228796096076, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.007428101729601622}, {"id": 670, "seek": 399224, "start": 3997.3599999999997, "end": 4004.8799999999997, "text": " we know could be prevented if we allowed the AIs to do research into treating it. I think just", "tokens": [50620, 321, 458, 727, 312, 27314, 498, 321, 4350, 264, 316, 6802, 281, 360, 2132, 666, 15083, 309, 13, 286, 519, 445, 50996], "temperature": 0.0, "avg_logprob": -0.08978228796096076, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.007428101729601622}, {"id": 671, "seek": 399224, "start": 4004.8799999999997, "end": 4010.72, "text": " permanently not going forward with using AIs and robots to make that technological progress.", "tokens": [50996, 24042, 406, 516, 2128, 365, 1228, 316, 6802, 293, 14733, 281, 652, 300, 18439, 4205, 13, 51288], "temperature": 0.0, "avg_logprob": -0.08978228796096076, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.007428101729601622}, {"id": 672, "seek": 399224, "start": 4011.7599999999998, "end": 4015.9199999999996, "text": " Like I said, when it's becoming cheaper and cheaper and other countries and other companies,", "tokens": [51340, 1743, 286, 848, 11, 562, 309, 311, 5617, 12284, 293, 12284, 293, 661, 3517, 293, 661, 3431, 11, 51548], "temperature": 0.0, "avg_logprob": -0.08978228796096076, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.007428101729601622}, {"id": 673, "seek": 399224, "start": 4015.9199999999996, "end": 4020.3999999999996, "text": " you know, might want to do it, that does seem like it's just very unrealistic.", "tokens": [51548, 291, 458, 11, 1062, 528, 281, 360, 309, 11, 300, 775, 1643, 411, 309, 311, 445, 588, 42867, 13, 51772], "temperature": 0.0, "avg_logprob": -0.08978228796096076, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.007428101729601622}, {"id": 674, "seek": 402040, "start": 4020.48, "end": 4021.44, "text": " Just implausible.", "tokens": [50368, 1449, 8484, 8463, 964, 13, 50416], "temperature": 0.0, "avg_logprob": -0.12401891708374023, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.00297447107732296}, {"id": 675, "seek": 402040, "start": 4021.44, "end": 4025.36, "text": " And maybe not desirable either, to be honest. Like after a certain point,", "tokens": [50416, 400, 1310, 406, 30533, 2139, 11, 281, 312, 3245, 13, 1743, 934, 257, 1629, 935, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12401891708374023, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.00297447107732296}, {"id": 676, "seek": 402040, "start": 4025.36, "end": 4027.6800000000003, "text": " like we should take it really cautious.", "tokens": [50612, 411, 321, 820, 747, 309, 534, 25278, 13, 50728], "temperature": 0.0, "avg_logprob": -0.12401891708374023, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.00297447107732296}, {"id": 677, "seek": 402040, "start": 4027.6800000000003, "end": 4034.88, "text": " Right, after 200 years of like research into air alignment, even if we're like,", "tokens": [50728, 1779, 11, 934, 2331, 924, 295, 411, 2132, 666, 1988, 18515, 11, 754, 498, 321, 434, 411, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12401891708374023, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.00297447107732296}, {"id": 678, "seek": 402040, "start": 4034.88, "end": 4041.44, "text": " ooh, this seems weird and scary and might change the world as we know it, at some point there are", "tokens": [51088, 17024, 11, 341, 2544, 3657, 293, 6958, 293, 1062, 1319, 264, 1002, 382, 321, 458, 309, 11, 412, 512, 935, 456, 366, 51416], "temperature": 0.0, "avg_logprob": -0.12401891708374023, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.00297447107732296}, {"id": 679, "seek": 402040, "start": 4041.44, "end": 4049.84, "text": " going to be incentives for some actors, countries or companies to try to deploy this technology", "tokens": [51416, 516, 281, 312, 23374, 337, 512, 10037, 11, 3517, 420, 3431, 281, 853, 281, 7274, 341, 2899, 51836], "temperature": 0.0, "avg_logprob": -0.12401891708374023, "compression_ratio": 1.575875486381323, "no_speech_prob": 0.00297447107732296}, {"id": 680, "seek": 404984, "start": 4049.84, "end": 4059.1200000000003, "text": " at scale to solve problems like poverty and illness, maybe also to get military advantages,", "tokens": [50364, 412, 4373, 281, 5039, 2740, 411, 10958, 293, 10152, 11, 1310, 611, 281, 483, 4632, 14906, 11, 50828], "temperature": 0.0, "avg_logprob": -0.09751681675986638, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.002429571934044361}, {"id": 681, "seek": 404984, "start": 4059.1200000000003, "end": 4065.76, "text": " to solve problems like climate change. And those incentives might just be so strong that", "tokens": [50828, 281, 5039, 2740, 411, 5659, 1319, 13, 400, 729, 23374, 1062, 445, 312, 370, 2068, 300, 51160], "temperature": 0.0, "avg_logprob": -0.09751681675986638, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.002429571934044361}, {"id": 682, "seek": 404984, "start": 4065.76, "end": 4074.08, "text": " even if we take our time, we'll probably eventually do it. Someone will. And once that happens,", "tokens": [51160, 754, 498, 321, 747, 527, 565, 11, 321, 603, 1391, 4728, 360, 309, 13, 8734, 486, 13, 400, 1564, 300, 2314, 11, 51576], "temperature": 0.0, "avg_logprob": -0.09751681675986638, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.002429571934044361}, {"id": 683, "seek": 407408, "start": 4074.08, "end": 4082.0, "text": " progress will become so quick that we're looking at really economic growth at a pace that's,", "tokens": [50364, 4205, 486, 1813, 370, 1702, 300, 321, 434, 1237, 412, 534, 4836, 4599, 412, 257, 11638, 300, 311, 11, 50760], "temperature": 0.0, "avg_logprob": -0.08000581160835597, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.022910796105861664}, {"id": 684, "seek": 407408, "start": 4082.0, "end": 4086.56, "text": " I guess, still kind of unfathomable to me. But that is this kind of thing where", "tokens": [50760, 286, 2041, 11, 920, 733, 295, 3971, 998, 298, 712, 281, 385, 13, 583, 300, 307, 341, 733, 295, 551, 689, 50988], "temperature": 0.0, "avg_logprob": -0.08000581160835597, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.022910796105861664}, {"id": 685, "seek": 407408, "start": 4086.56, "end": 4090.0, "text": " progress we've seen over the last 100 years happens in the next 10,", "tokens": [50988, 4205, 321, 600, 1612, 670, 264, 1036, 2319, 924, 2314, 294, 264, 958, 1266, 11, 51160], "temperature": 0.0, "avg_logprob": -0.08000581160835597, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.022910796105861664}, {"id": 686, "seek": 407408, "start": 4091.04, "end": 4095.2799999999997, "text": " and actually just keeps getting faster and faster and faster. Is that kind of the picture?", "tokens": [51212, 293, 767, 445, 5965, 1242, 4663, 293, 4663, 293, 4663, 13, 1119, 300, 733, 295, 264, 3036, 30, 51424], "temperature": 0.0, "avg_logprob": -0.08000581160835597, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.022910796105861664}, {"id": 687, "seek": 407408, "start": 4095.84, "end": 4096.88, "text": " Yeah, that's the picture.", "tokens": [51452, 865, 11, 300, 311, 264, 3036, 13, 51504], "temperature": 0.0, "avg_logprob": -0.08000581160835597, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.022910796105861664}, {"id": 688, "seek": 409688, "start": 4097.52, "end": 4106.72, "text": " That's really weird. It's scary. I guess it's also quite hopeful. I mean, if I let myself hope for", "tokens": [50396, 663, 311, 534, 3657, 13, 467, 311, 6958, 13, 286, 2041, 309, 311, 611, 1596, 20531, 13, 286, 914, 11, 498, 286, 718, 2059, 1454, 337, 50856], "temperature": 0.0, "avg_logprob": -0.08502507511573502, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.005296716466546059}, {"id": 689, "seek": 409688, "start": 4106.72, "end": 4115.84, "text": " that good world where we use it to solve problems, I feel nervously really, really excited. But I guess", "tokens": [50856, 300, 665, 1002, 689, 321, 764, 309, 281, 5039, 2740, 11, 286, 841, 5724, 5098, 534, 11, 534, 2919, 13, 583, 286, 2041, 51312], "temperature": 0.0, "avg_logprob": -0.08502507511573502, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.005296716466546059}, {"id": 690, "seek": 409688, "start": 4115.84, "end": 4123.6, "text": " we've got some real, real challenges to overcome first. Okay, let's move on to a related topic", "tokens": [51312, 321, 600, 658, 512, 957, 11, 957, 4759, 281, 10473, 700, 13, 1033, 11, 718, 311, 1286, 322, 281, 257, 4077, 4829, 51700], "temperature": 0.0, "avg_logprob": -0.08502507511573502, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.005296716466546059}, {"id": 691, "seek": 412360, "start": 4123.6, "end": 4129.200000000001, "text": " you've been researching more recently. So you've just written the draft of a report on", "tokens": [50364, 291, 600, 668, 24176, 544, 3938, 13, 407, 291, 600, 445, 3720, 264, 11206, 295, 257, 2275, 322, 50644], "temperature": 0.0, "avg_logprob": -0.07017417436235407, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.013724718242883682}, {"id": 692, "seek": 412360, "start": 4129.200000000001, "end": 4134.72, "text": " AI takeoff speeds that has some pretty alarming results to me, given everything we've just talked", "tokens": [50644, 7318, 747, 4506, 16411, 300, 575, 512, 1238, 44043, 3542, 281, 385, 11, 2212, 1203, 321, 600, 445, 2825, 50920], "temperature": 0.0, "avg_logprob": -0.07017417436235407, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.013724718242883682}, {"id": 693, "seek": 412360, "start": 4134.72, "end": 4140.160000000001, "text": " about. And I guess just to get on the same page about language, what exactly do you mean when", "tokens": [50920, 466, 13, 400, 286, 2041, 445, 281, 483, 322, 264, 912, 3028, 466, 2856, 11, 437, 2293, 360, 291, 914, 562, 51192], "temperature": 0.0, "avg_logprob": -0.07017417436235407, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.013724718242883682}, {"id": 694, "seek": 412360, "start": 4140.160000000001, "end": 4148.240000000001, "text": " you talk about AI takeoff speeds? Roughly speaking, capabilities takeoff speed is the question of how", "tokens": [51192, 291, 751, 466, 7318, 747, 4506, 16411, 30, 42791, 356, 4124, 11, 10862, 747, 4506, 3073, 307, 264, 1168, 295, 577, 51596], "temperature": 0.0, "avg_logprob": -0.07017417436235407, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.013724718242883682}, {"id": 695, "seek": 414824, "start": 4148.24, "end": 4155.44, "text": " quickly AI systems will improve as they approach and surpass human level intelligence.", "tokens": [50364, 2661, 7318, 3652, 486, 3470, 382, 436, 3109, 293, 27650, 1952, 1496, 7599, 13, 50724], "temperature": 0.0, "avg_logprob": -0.11896791129276671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004905765410512686}, {"id": 696, "seek": 414824, "start": 4156.16, "end": 4161.84, "text": " And so the capabilities are like their ability to do things like drive cars or", "tokens": [50760, 400, 370, 264, 10862, 366, 411, 641, 3485, 281, 360, 721, 411, 3332, 5163, 420, 51044], "temperature": 0.0, "avg_logprob": -0.11896791129276671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004905765410512686}, {"id": 697, "seek": 414824, "start": 4163.12, "end": 4170.4, "text": " program new programs. Exactly. So a fast takeoff speed could be that in a, you know,", "tokens": [51108, 1461, 777, 4268, 13, 7587, 13, 407, 257, 2370, 747, 4506, 3073, 727, 312, 300, 294, 257, 11, 291, 458, 11, 51472], "temperature": 0.0, "avg_logprob": -0.11896791129276671, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004905765410512686}, {"id": 698, "seek": 417040, "start": 4170.4, "end": 4178.719999999999, "text": " in three months, AIs go from mouse level intelligence to significantly more intelligent", "tokens": [50364, 294, 1045, 2493, 11, 7318, 82, 352, 490, 9719, 1496, 7599, 281, 10591, 544, 13232, 50780], "temperature": 0.0, "avg_logprob": -0.12255853559912705, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.011928768828511238}, {"id": 699, "seek": 417040, "start": 4178.719999999999, "end": 4184.4, "text": " than the smartest human, right? Where a slow takeoff speed that could be happening over decades.", "tokens": [50780, 813, 264, 41491, 1952, 11, 558, 30, 2305, 257, 2964, 747, 4506, 3073, 300, 727, 312, 2737, 670, 7878, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12255853559912705, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.011928768828511238}, {"id": 700, "seek": 417040, "start": 4185.36, "end": 4191.5199999999995, "text": " Got it. Okay. So we're just like slowly making progress next year at self driving cars. And", "tokens": [51112, 5803, 309, 13, 1033, 13, 407, 321, 434, 445, 411, 5692, 1455, 4205, 958, 1064, 412, 2698, 4840, 5163, 13, 400, 51420], "temperature": 0.0, "avg_logprob": -0.12255853559912705, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.011928768828511238}, {"id": 701, "seek": 417040, "start": 4191.5199999999995, "end": 4195.44, "text": " then it takes another 10 years to get to programs that can write other programs.", "tokens": [51420, 550, 309, 2516, 1071, 1266, 924, 281, 483, 281, 4268, 300, 393, 2464, 661, 4268, 13, 51616], "temperature": 0.0, "avg_logprob": -0.12255853559912705, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.011928768828511238}, {"id": 702, "seek": 419544, "start": 4196.4, "end": 4201.919999999999, "text": " So I guess, you know, you've written this report, and I won't give away the results yet,", "tokens": [50412, 407, 286, 2041, 11, 291, 458, 11, 291, 600, 3720, 341, 2275, 11, 293, 286, 1582, 380, 976, 1314, 264, 3542, 1939, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09716695785522461, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.007946956902742386}, {"id": 703, "seek": 419544, "start": 4201.919999999999, "end": 4206.879999999999, "text": " but it gives some evidence about how fast we might expect AI takeoff speeds to be.", "tokens": [50688, 457, 309, 2709, 512, 4467, 466, 577, 2370, 321, 1062, 2066, 7318, 747, 4506, 16411, 281, 312, 13, 50936], "temperature": 0.0, "avg_logprob": -0.09716695785522461, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.007946956902742386}, {"id": 704, "seek": 419544, "start": 4206.879999999999, "end": 4211.2, "text": " But before we get to that, before you wrote the report, what did you think was the most", "tokens": [50936, 583, 949, 321, 483, 281, 300, 11, 949, 291, 4114, 264, 2275, 11, 437, 630, 291, 519, 390, 264, 881, 51152], "temperature": 0.0, "avg_logprob": -0.09716695785522461, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.007946956902742386}, {"id": 705, "seek": 419544, "start": 4211.2, "end": 4214.879999999999, "text": " compelling evidence that AI takeoff speeds would be particularly fast?", "tokens": [51152, 20050, 4467, 300, 7318, 747, 4506, 16411, 576, 312, 4098, 2370, 30, 51336], "temperature": 0.0, "avg_logprob": -0.09716695785522461, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.007946956902742386}, {"id": 706, "seek": 419544, "start": 4216.08, "end": 4223.759999999999, "text": " So I think the most convincing argument is related to how humanity's capabilities were", "tokens": [51396, 407, 286, 519, 264, 881, 24823, 6770, 307, 4077, 281, 577, 10243, 311, 10862, 645, 51780], "temperature": 0.0, "avg_logprob": -0.09716695785522461, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.007946956902742386}, {"id": 707, "seek": 422376, "start": 4223.84, "end": 4229.360000000001, "text": " improving much more slowly 50,000 years ago, then they are improving today.", "tokens": [50368, 11470, 709, 544, 5692, 2625, 11, 1360, 924, 2057, 11, 550, 436, 366, 11470, 965, 13, 50644], "temperature": 0.0, "avg_logprob": -0.14336511313197126, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0009496710263192654}, {"id": 708, "seek": 422376, "start": 4230.400000000001, "end": 4235.280000000001, "text": " And the attempt to draw an analogy to what might happen with AI capabilities.", "tokens": [50696, 400, 264, 5217, 281, 2642, 364, 21663, 281, 437, 1062, 1051, 365, 7318, 10862, 13, 50940], "temperature": 0.0, "avg_logprob": -0.14336511313197126, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0009496710263192654}, {"id": 709, "seek": 422376, "start": 4236.320000000001, "end": 4244.400000000001, "text": " So if we think that a million years ago, humanity's kind of cognitive abilities collectively were", "tokens": [50992, 407, 498, 321, 519, 300, 257, 2459, 924, 2057, 11, 10243, 311, 733, 295, 15605, 11582, 24341, 645, 51396], "temperature": 0.0, "avg_logprob": -0.14336511313197126, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0009496710263192654}, {"id": 710, "seek": 422376, "start": 4245.280000000001, "end": 4251.68, "text": " maybe doubling every, let's say 100,000 years, I don't know. That's just kind of something to", "tokens": [51440, 1310, 33651, 633, 11, 718, 311, 584, 2319, 11, 1360, 924, 11, 286, 500, 380, 458, 13, 663, 311, 445, 733, 295, 746, 281, 51760], "temperature": 0.0, "avg_logprob": -0.14336511313197126, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.0009496710263192654}, {"id": 711, "seek": 425168, "start": 4251.68, "end": 4257.200000000001, "text": " represent their kind of slow increase in brain size and capacities. The exact number doesn't", "tokens": [50364, 2906, 641, 733, 295, 2964, 3488, 294, 3567, 2744, 293, 39396, 13, 440, 1900, 1230, 1177, 380, 50640], "temperature": 0.0, "avg_logprob": -0.10757038886086982, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.002315972466021776}, {"id": 712, "seek": 425168, "start": 4257.200000000001, "end": 4262.56, "text": " matter. You know, whether you want to say it's, you know, 100,000 or 10,000 years is a very slow", "tokens": [50640, 1871, 13, 509, 458, 11, 1968, 291, 528, 281, 584, 309, 311, 11, 291, 458, 11, 2319, 11, 1360, 420, 1266, 11, 1360, 924, 307, 257, 588, 2964, 50908], "temperature": 0.0, "avg_logprob": -0.10757038886086982, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.002315972466021776}, {"id": 713, "seek": 425168, "start": 4262.56, "end": 4268.08, "text": " doubling size in their abilities. Right. And that's basically because like slowly,", "tokens": [50908, 33651, 2744, 294, 641, 11582, 13, 1779, 13, 400, 300, 311, 1936, 570, 411, 5692, 11, 51184], "temperature": 0.0, "avg_logprob": -0.10757038886086982, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.002315972466021776}, {"id": 714, "seek": 425168, "start": 4268.08, "end": 4272.8, "text": " they're evolving to have slightly bigger brains, we're like adding a bit of prefrontal cortex,", "tokens": [51184, 436, 434, 21085, 281, 362, 4748, 3801, 15442, 11, 321, 434, 411, 5127, 257, 857, 295, 659, 11496, 304, 33312, 11, 51420], "temperature": 0.0, "avg_logprob": -0.10757038886086982, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.002315972466021776}, {"id": 715, "seek": 425168, "start": 4272.8, "end": 4278.240000000001, "text": " and like the population is getting a little bit bigger over time, but grew very, very slowly.", "tokens": [51420, 293, 411, 264, 4415, 307, 1242, 257, 707, 857, 3801, 670, 565, 11, 457, 6109, 588, 11, 588, 5692, 13, 51692], "temperature": 0.0, "avg_logprob": -0.10757038886086982, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.002315972466021776}, {"id": 716, "seek": 427824, "start": 4278.24, "end": 4281.12, "text": " So collectively, it's just, it takes thousands of years to double.", "tokens": [50364, 407, 24341, 11, 309, 311, 445, 11, 309, 2516, 5383, 295, 924, 281, 3834, 13, 50508], "temperature": 0.0, "avg_logprob": -0.12395344358502013, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.0028558827470988035}, {"id": 717, "seek": 427824, "start": 4281.679999999999, "end": 4282.48, "text": " Yeah, exactly.", "tokens": [50536, 865, 11, 2293, 13, 50576], "temperature": 0.0, "avg_logprob": -0.12395344358502013, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.0028558827470988035}, {"id": 718, "seek": 427824, "start": 4282.48, "end": 4283.44, "text": " Cool. Cool. Okay.", "tokens": [50576, 8561, 13, 8561, 13, 1033, 13, 50624], "temperature": 0.0, "avg_logprob": -0.12395344358502013, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.0028558827470988035}, {"id": 719, "seek": 427824, "start": 4284.08, "end": 4291.76, "text": " Whereas today, our abilities are improving incredibly quickly. As a society, our population", "tokens": [50656, 13813, 965, 11, 527, 11582, 366, 11470, 6252, 2661, 13, 1018, 257, 4086, 11, 527, 4415, 51040], "temperature": 0.0, "avg_logprob": -0.12395344358502013, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.0028558827470988035}, {"id": 720, "seek": 427824, "start": 4291.76, "end": 4298.32, "text": " growth is much faster. Our command of technology is much faster. And, you know, in the last,", "tokens": [51040, 4599, 307, 709, 4663, 13, 2621, 5622, 295, 2899, 307, 709, 4663, 13, 400, 11, 291, 458, 11, 294, 264, 1036, 11, 51368], "temperature": 0.0, "avg_logprob": -0.12395344358502013, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.0028558827470988035}, {"id": 721, "seek": 427824, "start": 4298.32, "end": 4304.88, "text": " like I said, in the last 200 years alone, we've doubled the economy many, many times and doubled", "tokens": [51368, 411, 286, 848, 11, 294, 264, 1036, 2331, 924, 3312, 11, 321, 600, 24405, 264, 5010, 867, 11, 867, 1413, 293, 24405, 51696], "temperature": 0.0, "avg_logprob": -0.12395344358502013, "compression_ratio": 1.5551020408163265, "no_speech_prob": 0.0028558827470988035}, {"id": 722, "seek": 430488, "start": 4304.88, "end": 4310.96, "text": " our ability to understand and manipulate the world very many times. And if you think that", "tokens": [50364, 527, 3485, 281, 1223, 293, 20459, 264, 1002, 588, 867, 1413, 13, 400, 498, 291, 519, 300, 50668], "temperature": 0.0, "avg_logprob": -0.11439476218274845, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.015075520612299442}, {"id": 723, "seek": 430488, "start": 4311.76, "end": 4319.84, "text": " there'll be some analog of that transition as we approach AGI, then, I mean, AI is already", "tokens": [50708, 456, 603, 312, 512, 16660, 295, 300, 6034, 382, 321, 3109, 316, 26252, 11, 550, 11, 286, 914, 11, 7318, 307, 1217, 51112], "temperature": 0.0, "avg_logprob": -0.11439476218274845, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.015075520612299442}, {"id": 724, "seek": 430488, "start": 4319.84, "end": 4324.96, "text": " improving really quickly. You know, I would say it's kind of doubling its abilities in less than", "tokens": [51112, 11470, 534, 2661, 13, 509, 458, 11, 286, 576, 584, 309, 311, 733, 295, 33651, 1080, 11582, 294, 1570, 813, 51368], "temperature": 0.0, "avg_logprob": -0.11439476218274845, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.015075520612299442}, {"id": 725, "seek": 430488, "start": 4324.96, "end": 4329.6, "text": " a year at the moment. And so if, you know, if that's this kind of the slow initial pace,", "tokens": [51368, 257, 1064, 412, 264, 1623, 13, 400, 370, 498, 11, 291, 458, 11, 498, 300, 311, 341, 733, 295, 264, 2964, 5883, 11638, 11, 51600], "temperature": 0.0, "avg_logprob": -0.11439476218274845, "compression_ratio": 1.5708154506437768, "no_speech_prob": 0.015075520612299442}, {"id": 726, "seek": 432960, "start": 4330.240000000001, "end": 4335.68, "text": " then, you know, the new pace would be kind of blisteringly quick.", "tokens": [50396, 550, 11, 291, 458, 11, 264, 777, 11638, 576, 312, 733, 295, 888, 1964, 12163, 1702, 13, 50668], "temperature": 0.0, "avg_logprob": -0.11710246236700761, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.005386451259255409}, {"id": 727, "seek": 432960, "start": 4335.68, "end": 4337.4400000000005, "text": " Right, right. Unimaginably quick.", "tokens": [50668, 1779, 11, 558, 13, 1156, 44976, 1188, 1702, 13, 50756], "temperature": 0.0, "avg_logprob": -0.11710246236700761, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.005386451259255409}, {"id": 728, "seek": 432960, "start": 4337.4400000000005, "end": 4345.280000000001, "text": " Yeah, exactly. The way I would think about it is that a million years ago, humans weren't able to do", "tokens": [50756, 865, 11, 2293, 13, 440, 636, 286, 576, 519, 466, 309, 307, 300, 257, 2459, 924, 2057, 11, 6255, 4999, 380, 1075, 281, 360, 51148], "temperature": 0.0, "avg_logprob": -0.11710246236700761, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.005386451259255409}, {"id": 729, "seek": 432960, "start": 4346.400000000001, "end": 4352.88, "text": " science and to discover technological improvements much at all. And so they didn't have access to", "tokens": [51204, 3497, 293, 281, 4411, 18439, 13797, 709, 412, 439, 13, 400, 370, 436, 994, 380, 362, 2105, 281, 51528], "temperature": 0.0, "avg_logprob": -0.11710246236700761, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.005386451259255409}, {"id": 730, "seek": 432960, "start": 4352.88, "end": 4357.52, "text": " this additional feedback loop of improvement. Where you discover new technology passed on to", "tokens": [51528, 341, 4497, 5824, 6367, 295, 10444, 13, 2305, 291, 4411, 777, 2899, 4678, 322, 281, 51760], "temperature": 0.0, "avg_logprob": -0.11710246236700761, "compression_ratio": 1.5577689243027888, "no_speech_prob": 0.005386451259255409}, {"id": 731, "seek": 435752, "start": 4357.52, "end": 4362.400000000001, "text": " the next generation, then they start on a better place, can discover even more technology, you", "tokens": [50364, 264, 958, 5125, 11, 550, 436, 722, 322, 257, 1101, 1081, 11, 393, 4411, 754, 544, 2899, 11, 291, 50608], "temperature": 0.0, "avg_logprob": -0.11152730137109756, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.005703034810721874}, {"id": 732, "seek": 435752, "start": 4362.400000000001, "end": 4366.0, "text": " can now support a bigger population. There's this whole feedback loop, which arguably we", "tokens": [50608, 393, 586, 1406, 257, 3801, 4415, 13, 821, 311, 341, 1379, 5824, 6367, 11, 597, 26771, 321, 50788], "temperature": 0.0, "avg_logprob": -0.11152730137109756, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.005703034810721874}, {"id": 733, "seek": 435752, "start": 4366.0, "end": 4370.320000000001, "text": " couldn't access a million years ago. Then we improved our cognitive abilities as, you know,", "tokens": [50788, 2809, 380, 2105, 257, 2459, 924, 2057, 13, 1396, 321, 9689, 527, 15605, 11582, 382, 11, 291, 458, 11, 51004], "temperature": 0.0, "avg_logprob": -0.11152730137109756, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.005703034810721874}, {"id": 734, "seek": 435752, "start": 4370.320000000001, "end": 4374.320000000001, "text": " as humans a little bit. And then suddenly we got over this threshold where, okay, we can access", "tokens": [51004, 382, 6255, 257, 707, 857, 13, 400, 550, 5800, 321, 658, 670, 341, 14678, 689, 11, 1392, 11, 321, 393, 2105, 51204], "temperature": 0.0, "avg_logprob": -0.11152730137109756, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.005703034810721874}, {"id": 735, "seek": 435752, "start": 4374.320000000001, "end": 4379.92, "text": " this, this kind of doing technological progress feedback loop, which then, you know, then speeds", "tokens": [51204, 341, 11, 341, 733, 295, 884, 18439, 4205, 5824, 6367, 11, 597, 550, 11, 291, 458, 11, 550, 16411, 51484], "temperature": 0.0, "avg_logprob": -0.11152730137109756, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.005703034810721874}, {"id": 736, "seek": 435752, "start": 4379.92, "end": 4384.88, "text": " up and speeds up as we develop agriculture and then we develop text, you know, written language", "tokens": [51484, 493, 293, 16411, 493, 382, 321, 1499, 14837, 293, 550, 321, 1499, 2487, 11, 291, 458, 11, 3720, 2856, 51732], "temperature": 0.0, "avg_logprob": -0.11152730137109756, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.005703034810721874}, {"id": 737, "seek": 438488, "start": 4384.88, "end": 4388.88, "text": " and we develop maths. And it's kind of, we're now even better at discovering new technologies.", "tokens": [50364, 293, 321, 1499, 36287, 13, 400, 309, 311, 733, 295, 11, 321, 434, 586, 754, 1101, 412, 24773, 777, 7943, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09883511584738026, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0014830933650955558}, {"id": 738, "seek": 438488, "start": 4389.52, "end": 4394.64, "text": " And the thought is in my mind that maybe there's something similar that happens with AIs, whereas", "tokens": [50596, 400, 264, 1194, 307, 294, 452, 1575, 300, 1310, 456, 311, 746, 2531, 300, 2314, 365, 316, 6802, 11, 9735, 50852], "temperature": 0.0, "avg_logprob": -0.09883511584738026, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0014830933650955558}, {"id": 739, "seek": 438488, "start": 4395.4400000000005, "end": 4400.96, "text": " today they're not, you know, they're not that smart. And so they're not able to access a certain", "tokens": [50892, 965, 436, 434, 406, 11, 291, 458, 11, 436, 434, 406, 300, 4069, 13, 400, 370, 436, 434, 406, 1075, 281, 2105, 257, 1629, 51168], "temperature": 0.0, "avg_logprob": -0.09883511584738026, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0014830933650955558}, {"id": 740, "seek": 438488, "start": 4400.96, "end": 4405.68, "text": " feedback loop. I'm not sure exactly what that feedback loop will be. But at some point, they", "tokens": [51168, 5824, 6367, 13, 286, 478, 406, 988, 2293, 437, 300, 5824, 6367, 486, 312, 13, 583, 412, 512, 935, 11, 436, 51404], "temperature": 0.0, "avg_logprob": -0.09883511584738026, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0014830933650955558}, {"id": 741, "seek": 438488, "start": 4405.68, "end": 4410.96, "text": " become smart enough that there's this additional feedback loop that they can use to improve their", "tokens": [51404, 1813, 4069, 1547, 300, 456, 311, 341, 4497, 5824, 6367, 300, 436, 393, 764, 281, 3470, 641, 51668], "temperature": 0.0, "avg_logprob": -0.09883511584738026, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0014830933650955558}, {"id": 742, "seek": 441096, "start": 4410.96, "end": 4415.36, "text": " capabilities, kind of like how humans use technology to improve our capabilities.", "tokens": [50364, 10862, 11, 733, 295, 411, 577, 6255, 764, 2899, 281, 3470, 527, 10862, 13, 50584], "temperature": 0.0, "avg_logprob": -0.07552537051114169, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.003908130340278149}, {"id": 743, "seek": 441096, "start": 4415.36, "end": 4419.2, "text": " You know, one, one funny thing about this argument is that it's not clear what that new", "tokens": [50584, 509, 458, 11, 472, 11, 472, 4074, 551, 466, 341, 6770, 307, 300, 309, 311, 406, 1850, 437, 300, 777, 50776], "temperature": 0.0, "avg_logprob": -0.07552537051114169, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.003908130340278149}, {"id": 744, "seek": 441096, "start": 4419.2, "end": 4426.4800000000005, "text": " feedback loop might be for AIs. And so that leaves me a little bit puzzled over where to go with", "tokens": [50776, 5824, 6367, 1062, 312, 337, 316, 6802, 13, 400, 370, 300, 5510, 385, 257, 707, 857, 18741, 1493, 670, 689, 281, 352, 365, 51140], "temperature": 0.0, "avg_logprob": -0.07552537051114169, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.003908130340278149}, {"id": 745, "seek": 441096, "start": 4426.4800000000005, "end": 4432.96, "text": " this argument. So for humans, it's, you know, we discover the scientific method and we experiment", "tokens": [51140, 341, 6770, 13, 407, 337, 6255, 11, 309, 311, 11, 291, 458, 11, 321, 4411, 264, 8134, 3170, 293, 321, 5120, 51464], "temperature": 0.0, "avg_logprob": -0.07552537051114169, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.003908130340278149}, {"id": 746, "seek": 441096, "start": 4432.96, "end": 4440.0, "text": " on things and we built computers. And now we can like run programs that help us do science.", "tokens": [51464, 322, 721, 293, 321, 3094, 10807, 13, 400, 586, 321, 393, 411, 1190, 4268, 300, 854, 505, 360, 3497, 13, 51816], "temperature": 0.0, "avg_logprob": -0.07552537051114169, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.003908130340278149}, {"id": 747, "seek": 444000, "start": 4440.0, "end": 4445.52, "text": " But for AI, it's like, we don't even know what kinds of science they'll discover that's like", "tokens": [50364, 583, 337, 7318, 11, 309, 311, 411, 11, 321, 500, 380, 754, 458, 437, 3685, 295, 3497, 436, 603, 4411, 300, 311, 411, 50640], "temperature": 0.0, "avg_logprob": -0.08303736037566882, "compression_ratio": 1.75, "no_speech_prob": 0.0006294375634752214}, {"id": 748, "seek": 444000, "start": 4445.52, "end": 4451.6, "text": " beyond ours. And is there some question about whether there even are kind of higher orders of", "tokens": [50640, 4399, 11896, 13, 400, 307, 456, 512, 1168, 466, 1968, 456, 754, 366, 733, 295, 2946, 9470, 295, 50944], "temperature": 0.0, "avg_logprob": -0.08303736037566882, "compression_ratio": 1.75, "no_speech_prob": 0.0006294375634752214}, {"id": 749, "seek": 444000, "start": 4451.6, "end": 4457.04, "text": " science that we can't, that we haven't developed, but that AI systems might to kind of increase", "tokens": [50944, 3497, 300, 321, 393, 380, 11, 300, 321, 2378, 380, 4743, 11, 457, 300, 7318, 3652, 1062, 281, 733, 295, 3488, 51216], "temperature": 0.0, "avg_logprob": -0.08303736037566882, "compression_ratio": 1.75, "no_speech_prob": 0.0006294375634752214}, {"id": 750, "seek": 444000, "start": 4457.04, "end": 4461.6, "text": " their own feedback loop? Yeah, I think that's, I think that's right. And you know, you can,", "tokens": [51216, 641, 1065, 5824, 6367, 30, 865, 11, 286, 519, 300, 311, 11, 286, 519, 300, 311, 558, 13, 400, 291, 458, 11, 291, 393, 11, 51444], "temperature": 0.0, "avg_logprob": -0.08303736037566882, "compression_ratio": 1.75, "no_speech_prob": 0.0006294375634752214}, {"id": 751, "seek": 444000, "start": 4461.6, "end": 4466.08, "text": " you can throw out ideas for what that might look like. Right. Maybe it's the AIs", "tokens": [51444, 291, 393, 3507, 484, 3487, 337, 437, 300, 1062, 574, 411, 13, 1779, 13, 2704, 309, 311, 264, 316, 6802, 51668], "temperature": 0.0, "avg_logprob": -0.08303736037566882, "compression_ratio": 1.75, "no_speech_prob": 0.0006294375634752214}, {"id": 752, "seek": 446608, "start": 4466.8, "end": 4472.64, "text": " learn to work together in a team in a way that is way more efficient than what humans have ever", "tokens": [50400, 1466, 281, 589, 1214, 294, 257, 1469, 294, 257, 636, 300, 307, 636, 544, 7148, 813, 437, 6255, 362, 1562, 50692], "temperature": 0.0, "avg_logprob": -0.1331038475036621, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.004276637453585863}, {"id": 753, "seek": 446608, "start": 4472.64, "end": 4479.76, "text": " done. Yeah. Or maybe they run simulations or something to like learn about economics or", "tokens": [50692, 1096, 13, 865, 13, 1610, 1310, 436, 1190, 35138, 420, 746, 281, 411, 1466, 466, 14564, 420, 51048], "temperature": 0.0, "avg_logprob": -0.1331038475036621, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.004276637453585863}, {"id": 754, "seek": 446608, "start": 4480.64, "end": 4485.76, "text": " something in a way that we can, we can barely understand because we only see the economies run", "tokens": [51092, 746, 294, 257, 636, 300, 321, 393, 11, 321, 393, 10268, 1223, 570, 321, 787, 536, 264, 23158, 1190, 51348], "temperature": 0.0, "avg_logprob": -0.1331038475036621, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.004276637453585863}, {"id": 755, "seek": 446608, "start": 4485.76, "end": 4493.2, "text": " in these weird real, real world scenarios. For example, for example, cool. Okay. Yeah. Are there,", "tokens": [51348, 294, 613, 3657, 957, 11, 957, 1002, 15077, 13, 1171, 1365, 11, 337, 1365, 11, 1627, 13, 1033, 13, 865, 13, 2014, 456, 11, 51720], "temperature": 0.0, "avg_logprob": -0.1331038475036621, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.004276637453585863}, {"id": 756, "seek": 449320, "start": 4493.2, "end": 4496.88, "text": " are there any limitations to that argument? Or do you do just kind of buy it?", "tokens": [50364, 366, 456, 604, 15705, 281, 300, 6770, 30, 1610, 360, 291, 360, 445, 733, 295, 2256, 309, 30, 50548], "temperature": 0.0, "avg_logprob": -0.09270406853068959, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0018558489391580224}, {"id": 757, "seek": 449320, "start": 4497.84, "end": 4501.76, "text": " Yeah, I'm, I actually don't put that much weight on, on this particular argument.", "tokens": [50596, 865, 11, 286, 478, 11, 286, 767, 500, 380, 829, 300, 709, 3364, 322, 11, 322, 341, 1729, 6770, 13, 50792], "temperature": 0.0, "avg_logprob": -0.09270406853068959, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0018558489391580224}, {"id": 758, "seek": 449320, "start": 4501.76, "end": 4510.88, "text": " Oh, interesting. So the main reason is that evolution was not trying to make humanity as a", "tokens": [50792, 876, 11, 1880, 13, 407, 264, 2135, 1778, 307, 300, 9303, 390, 406, 1382, 281, 652, 10243, 382, 257, 51248], "temperature": 0.0, "avg_logprob": -0.09270406853068959, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0018558489391580224}, {"id": 759, "seek": 449320, "start": 4510.88, "end": 4517.04, "text": " whole as capable as possible. And it wasn't trying to make humanity as a whole good at science.", "tokens": [51248, 1379, 382, 8189, 382, 1944, 13, 400, 309, 2067, 380, 1382, 281, 652, 10243, 382, 257, 1379, 665, 412, 3497, 13, 51556], "temperature": 0.0, "avg_logprob": -0.09270406853068959, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.0018558489391580224}, {"id": 760, "seek": 451704, "start": 4517.12, "end": 4524.24, "text": " Right. So from that perspective, it's not actually as surprising that humanity went from", "tokens": [50368, 1779, 13, 407, 490, 300, 4585, 11, 309, 311, 406, 767, 382, 8830, 300, 10243, 1437, 490, 50724], "temperature": 0.0, "avg_logprob": -0.08030050256279077, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0027789378073066473}, {"id": 761, "seek": 451704, "start": 4524.24, "end": 4530.16, "text": " really sucking at science to being really good at science in a fairly short, short timeframe.", "tokens": [50724, 534, 38669, 412, 3497, 281, 885, 534, 665, 412, 3497, 294, 257, 6457, 2099, 11, 2099, 34830, 13, 51020], "temperature": 0.0, "avg_logprob": -0.08030050256279077, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0027789378073066473}, {"id": 762, "seek": 451704, "start": 4530.96, "end": 4538.8, "text": " Yeah. So here's an analogy. Before 2020, we hadn't made many COVID vaccines, not because we couldn't,", "tokens": [51060, 865, 13, 407, 510, 311, 364, 21663, 13, 4546, 4808, 11, 321, 8782, 380, 1027, 867, 4566, 12164, 11, 406, 570, 321, 2809, 380, 11, 51452], "temperature": 0.0, "avg_logprob": -0.08030050256279077, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0027789378073066473}, {"id": 763, "seek": 451704, "start": 4538.8, "end": 4543.04, "text": " but because we weren't trying to, we were focused as a society on doing other things.", "tokens": [51452, 457, 570, 321, 4999, 380, 1382, 281, 11, 321, 645, 5178, 382, 257, 4086, 322, 884, 661, 721, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08030050256279077, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.0027789378073066473}, {"id": 764, "seek": 454304, "start": 4543.68, "end": 4549.36, "text": " Then around 2020, it became really useful for us to make lots of vaccines. And then,", "tokens": [50396, 1396, 926, 4808, 11, 309, 3062, 534, 4420, 337, 505, 281, 652, 3195, 295, 12164, 13, 400, 550, 11, 50680], "temperature": 0.0, "avg_logprob": -0.11146657960908907, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0021280706860125065}, {"id": 765, "seek": 454304, "start": 4549.36, "end": 4554.72, "text": " lo and behold, the number of vaccines went up very dramatically. Now, that doesn't mean that our", "tokens": [50680, 450, 293, 27234, 11, 264, 1230, 295, 12164, 1437, 493, 588, 17548, 13, 823, 11, 300, 1177, 380, 914, 300, 527, 50948], "temperature": 0.0, "avg_logprob": -0.11146657960908907, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0021280706860125065}, {"id": 766, "seek": 454304, "start": 4554.72, "end": 4560.24, "text": " kind of abilities in vaccine making suddenly went up. It just means that we reprioritized,", "tokens": [50948, 733, 295, 11582, 294, 7007, 1455, 5800, 1437, 493, 13, 467, 445, 1355, 300, 321, 1085, 470, 50017, 1602, 11, 51224], "temperature": 0.0, "avg_logprob": -0.11146657960908907, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0021280706860125065}, {"id": 767, "seek": 454304, "start": 4560.24, "end": 4566.0, "text": " reallocated the resources we already had towards making vaccines. Okay. And I want to say that", "tokens": [51224, 319, 336, 905, 770, 264, 3593, 321, 1217, 632, 3030, 1455, 12164, 13, 1033, 13, 400, 286, 528, 281, 584, 300, 51512], "temperature": 0.0, "avg_logprob": -0.11146657960908907, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0021280706860125065}, {"id": 768, "seek": 454304, "start": 4566.0, "end": 4571.76, "text": " that's somewhat similar to the way in which kind of our ancestors a million years ago,", "tokens": [51512, 300, 311, 8344, 2531, 281, 264, 636, 294, 597, 733, 295, 527, 18069, 257, 2459, 924, 2057, 11, 51800], "temperature": 0.0, "avg_logprob": -0.11146657960908907, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0021280706860125065}, {"id": 769, "seek": 457176, "start": 4571.76, "end": 4578.24, "text": " we weren't that good at science, but evolution wasn't trying to make us that good at science.", "tokens": [50364, 321, 4999, 380, 300, 665, 412, 3497, 11, 457, 9303, 2067, 380, 1382, 281, 652, 505, 300, 665, 412, 3497, 13, 50688], "temperature": 0.0, "avg_logprob": -0.09803394877582515, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.001772456569597125}, {"id": 770, "seek": 457176, "start": 4578.24, "end": 4583.4400000000005, "text": " It was mostly trying to make us hunt successfully, feed our families. Okay.", "tokens": [50688, 467, 390, 5240, 1382, 281, 652, 505, 12454, 10727, 11, 3154, 527, 4466, 13, 1033, 13, 50948], "temperature": 0.0, "avg_logprob": -0.09803394877582515, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.001772456569597125}, {"id": 771, "seek": 457176, "start": 4583.4400000000005, "end": 4588.320000000001, "text": " Science was like maybe a tiny, tiny bit useful back then, because it maybe allowed you to discover", "tokens": [50948, 8976, 390, 411, 1310, 257, 5870, 11, 5870, 857, 4420, 646, 550, 11, 570, 309, 1310, 4350, 291, 281, 4411, 51192], "temperature": 0.0, "avg_logprob": -0.09803394877582515, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.001772456569597125}, {"id": 772, "seek": 457176, "start": 4588.320000000001, "end": 4594.320000000001, "text": " something with your own lifetime, but it really wasn't very useful. Then I think more recently,", "tokens": [51192, 746, 365, 428, 1065, 11364, 11, 457, 309, 534, 2067, 380, 588, 4420, 13, 1396, 286, 519, 544, 3938, 11, 51492], "temperature": 0.0, "avg_logprob": -0.09803394877582515, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.001772456569597125}, {"id": 773, "seek": 457176, "start": 4594.320000000001, "end": 4600.16, "text": " maybe more like kind of tens or a hundred thousand years ago, it did become more useful for humans", "tokens": [51492, 1310, 544, 411, 733, 295, 10688, 420, 257, 3262, 4714, 924, 2057, 11, 309, 630, 1813, 544, 4420, 337, 6255, 51784], "temperature": 0.0, "avg_logprob": -0.09803394877582515, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.001772456569597125}, {"id": 774, "seek": 460016, "start": 4600.16, "end": 4606.88, "text": " to do science and to be flexible learners. And so it's not that surprising that at that point,", "tokens": [50364, 281, 360, 3497, 293, 281, 312, 11358, 23655, 13, 400, 370, 309, 311, 406, 300, 8830, 300, 412, 300, 935, 11, 50700], "temperature": 0.0, "avg_logprob": -0.08580109413633956, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0037165856920182705}, {"id": 775, "seek": 460016, "start": 4607.76, "end": 4613.2, "text": " say 50,000 years ago, where it was more useful for humans to be good at science,", "tokens": [50744, 584, 2625, 11, 1360, 924, 2057, 11, 689, 309, 390, 544, 4420, 337, 6255, 281, 312, 665, 412, 3497, 11, 51016], "temperature": 0.0, "avg_logprob": -0.08580109413633956, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0037165856920182705}, {"id": 776, "seek": 460016, "start": 4613.2, "end": 4620.5599999999995, "text": " evolution then reallocated those cognitive resources of humans to being good at science.", "tokens": [51016, 9303, 550, 319, 336, 905, 770, 729, 15605, 3593, 295, 6255, 281, 885, 665, 412, 3497, 13, 51384], "temperature": 0.0, "avg_logprob": -0.08580109413633956, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0037165856920182705}, {"id": 777, "seek": 460016, "start": 4620.5599999999995, "end": 4621.44, "text": " Right.", "tokens": [51384, 1779, 13, 51428], "temperature": 0.0, "avg_logprob": -0.08580109413633956, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0037165856920182705}, {"id": 778, "seek": 460016, "start": 4621.44, "end": 4626.24, "text": " So that kind of reallocation by evolution from kind of just for foraging and then reallocating", "tokens": [51428, 407, 300, 733, 295, 319, 336, 27943, 538, 9303, 490, 733, 295, 445, 337, 337, 3568, 293, 550, 319, 336, 905, 990, 51668], "temperature": 0.0, "avg_logprob": -0.08580109413633956, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0037165856920182705}, {"id": 779, "seek": 462624, "start": 4626.32, "end": 4630.719999999999, "text": " those cognitive resources to doing science is kind of like human society, reallocating its", "tokens": [50368, 729, 15605, 3593, 281, 884, 3497, 307, 733, 295, 411, 1952, 4086, 11, 319, 336, 905, 990, 1080, 50588], "temperature": 0.0, "avg_logprob": -0.110347847143809, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.002013210905715823}, {"id": 780, "seek": 462624, "start": 4631.599999999999, "end": 4633.84, "text": " resources to make COVID vaccines.", "tokens": [50632, 3593, 281, 652, 4566, 12164, 13, 50744], "temperature": 0.0, "avg_logprob": -0.110347847143809, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.002013210905715823}, {"id": 781, "seek": 462624, "start": 4633.84, "end": 4640.96, "text": " Okay. Yeah, that's really, really helpful. So something like a combination maybe of language,", "tokens": [50744, 1033, 13, 865, 11, 300, 311, 534, 11, 534, 4961, 13, 407, 746, 411, 257, 6562, 1310, 295, 2856, 11, 51100], "temperature": 0.0, "avg_logprob": -0.110347847143809, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.002013210905715823}, {"id": 782, "seek": 462624, "start": 4640.96, "end": 4648.48, "text": " maybe of just like group living, maybe some things that I don't understand, made it much more", "tokens": [51100, 1310, 295, 445, 411, 1594, 2647, 11, 1310, 512, 721, 300, 286, 500, 380, 1223, 11, 1027, 309, 709, 544, 51476], "temperature": 0.0, "avg_logprob": -0.110347847143809, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.002013210905715823}, {"id": 783, "seek": 462624, "start": 4648.48, "end": 4655.04, "text": " beneficial to be able to learn new things and learn like a range of things, not just the same", "tokens": [51476, 14072, 281, 312, 1075, 281, 1466, 777, 721, 293, 1466, 411, 257, 3613, 295, 721, 11, 406, 445, 264, 912, 51804], "temperature": 0.0, "avg_logprob": -0.110347847143809, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.002013210905715823}, {"id": 784, "seek": 465504, "start": 4655.04, "end": 4660.72, "text": " things over and over again. And so a couple of tweaks in the brain was enough to make the", "tokens": [50364, 721, 670, 293, 670, 797, 13, 400, 370, 257, 1916, 295, 46664, 294, 264, 3567, 390, 1547, 281, 652, 264, 50648], "temperature": 0.0, "avg_logprob": -0.08278268956123515, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0036737113259732723}, {"id": 785, "seek": 465504, "start": 4660.72, "end": 4666.88, "text": " brain that we'd been using for very specific set of tasks become useful for just like a much", "tokens": [50648, 3567, 300, 321, 1116, 668, 1228, 337, 588, 2685, 992, 295, 9608, 1813, 4420, 337, 445, 411, 257, 709, 50956], "temperature": 0.0, "avg_logprob": -0.08278268956123515, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0036737113259732723}, {"id": 786, "seek": 465504, "start": 4666.88, "end": 4673.5199999999995, "text": " wider range of tasks. And that wasn't like really fundamentally altering like the amount of brain", "tokens": [50956, 11842, 3613, 295, 9608, 13, 400, 300, 2067, 380, 411, 534, 17879, 11337, 278, 411, 264, 2372, 295, 3567, 51288], "temperature": 0.0, "avg_logprob": -0.08278268956123515, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0036737113259732723}, {"id": 787, "seek": 465504, "start": 4673.5199999999995, "end": 4682.32, "text": " we have, but like how we use it. And that capacity already existed. And like you said, was repurposed.", "tokens": [51288, 321, 362, 11, 457, 411, 577, 321, 764, 309, 13, 400, 300, 6042, 1217, 13135, 13, 400, 411, 291, 848, 11, 390, 1085, 20130, 1744, 13, 51728], "temperature": 0.0, "avg_logprob": -0.08278268956123515, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0036737113259732723}, {"id": 788, "seek": 468232, "start": 4683.04, "end": 4689.679999999999, "text": " So yeah, here's how you would relate it to AI take off. You'd say that in the case of evolution,", "tokens": [50400, 407, 1338, 11, 510, 311, 577, 291, 576, 10961, 309, 281, 7318, 747, 766, 13, 509, 1116, 584, 300, 294, 264, 1389, 295, 9303, 11, 50732], "temperature": 0.0, "avg_logprob": -0.09008909793610269, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0014651947421953082}, {"id": 789, "seek": 468232, "start": 4689.679999999999, "end": 4696.5599999999995, "text": " evolution wasn't initially trying to make humans good at science. And so no massive surprise that", "tokens": [50732, 9303, 2067, 380, 9105, 1382, 281, 652, 6255, 665, 412, 3497, 13, 400, 370, 572, 5994, 6365, 300, 51076], "temperature": 0.0, "avg_logprob": -0.09008909793610269, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0014651947421953082}, {"id": 790, "seek": 468232, "start": 4696.5599999999995, "end": 4701.2, "text": " it's able to quickly make some tweaks that make humans good at science late in the day.", "tokens": [51076, 309, 311, 1075, 281, 2661, 652, 512, 46664, 300, 652, 6255, 665, 412, 3497, 3469, 294, 264, 786, 13, 51308], "temperature": 0.0, "avg_logprob": -0.09008909793610269, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0014651947421953082}, {"id": 791, "seek": 468232, "start": 4702.24, "end": 4708.88, "text": " But with AIs and with AI development, humans will be at every stage trying to make AIs as useful", "tokens": [51360, 583, 365, 316, 6802, 293, 365, 7318, 3250, 11, 6255, 486, 312, 412, 633, 3233, 1382, 281, 652, 316, 6802, 382, 4420, 51692], "temperature": 0.0, "avg_logprob": -0.09008909793610269, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0014651947421953082}, {"id": 792, "seek": 470888, "start": 4708.88, "end": 4717.2, "text": " as possible for doing economic tasks, helping with science and research. And so we wouldn't", "tokens": [50364, 382, 1944, 337, 884, 4836, 9608, 11, 4315, 365, 3497, 293, 2132, 13, 400, 370, 321, 2759, 380, 50780], "temperature": 0.0, "avg_logprob": -0.07958786902220352, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.001897221663966775}, {"id": 793, "seek": 470888, "start": 4717.2, "end": 4723.4400000000005, "text": " expect there to be this kind of overhang where the AI has these abilities, which it's just not", "tokens": [50780, 2066, 456, 281, 312, 341, 733, 295, 670, 23850, 689, 264, 7318, 575, 613, 11582, 11, 597, 309, 311, 445, 406, 51092], "temperature": 0.0, "avg_logprob": -0.07958786902220352, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.001897221663966775}, {"id": 794, "seek": 470888, "start": 4723.4400000000005, "end": 4728.08, "text": " using, because we would expect humans to be trying to coax those abilities out at every step of the", "tokens": [51092, 1228, 11, 570, 321, 576, 2066, 6255, 281, 312, 1382, 281, 598, 2797, 729, 11582, 484, 412, 633, 1823, 295, 264, 51324], "temperature": 0.0, "avg_logprob": -0.07958786902220352, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.001897221663966775}, {"id": 795, "seek": 470888, "start": 4728.08, "end": 4736.32, "text": " way. And so if if you're constantly trying to coax abilities out, most likely you'll only find", "tokens": [51324, 636, 13, 400, 370, 498, 498, 291, 434, 6460, 1382, 281, 598, 2797, 11582, 484, 11, 881, 3700, 291, 603, 787, 915, 51736], "temperature": 0.0, "avg_logprob": -0.07958786902220352, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.001897221663966775}, {"id": 796, "seek": 473632, "start": 4736.32, "end": 4741.12, "text": " ways to do it incrementally, as opposed to like, if it happened to be the case that like,", "tokens": [50364, 2098, 281, 360, 309, 26200, 379, 11, 382, 8851, 281, 411, 11, 498, 309, 2011, 281, 312, 264, 1389, 300, 411, 11, 50604], "temperature": 0.0, "avg_logprob": -0.08541707559065385, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.024387463927268982}, {"id": 797, "seek": 473632, "start": 4741.92, "end": 4749.28, "text": " I don't know, we found a billion computer chips on another planet, and could just", "tokens": [50644, 286, 500, 380, 458, 11, 321, 1352, 257, 5218, 3820, 11583, 322, 1071, 5054, 11, 293, 727, 445, 51012], "temperature": 0.0, "avg_logprob": -0.08541707559065385, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.024387463927268982}, {"id": 798, "seek": 473632, "start": 4750.08, "end": 4757.2, "text": " use them to train up a bunch of AI systems, then we'd expect the step change. But like,", "tokens": [51052, 764, 552, 281, 3847, 493, 257, 3840, 295, 7318, 3652, 11, 550, 321, 1116, 2066, 264, 1823, 1319, 13, 583, 411, 11, 51408], "temperature": 0.0, "avg_logprob": -0.08541707559065385, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.024387463927268982}, {"id": 799, "seek": 473632, "start": 4757.2, "end": 4761.44, "text": " currently, everything is just increasing incrementally, we're increasing chips incrementally,", "tokens": [51408, 4362, 11, 1203, 307, 445, 5662, 26200, 379, 11, 321, 434, 5662, 11583, 26200, 379, 11, 51620], "temperature": 0.0, "avg_logprob": -0.08541707559065385, "compression_ratio": 1.6729857819905214, "no_speech_prob": 0.024387463927268982}, {"id": 800, "seek": 476144, "start": 4761.44, "end": 4767.5199999999995, "text": " we're increasing algorithmic progress incrementally. And so it's just going to keep improving at a", "tokens": [50364, 321, 434, 5662, 9284, 299, 4205, 26200, 379, 13, 400, 370, 309, 311, 445, 516, 281, 1066, 11470, 412, 257, 50668], "temperature": 0.0, "avg_logprob": -0.08749448207386754, "compression_ratio": 1.9475806451612903, "no_speech_prob": 0.0009165750816464424}, {"id": 801, "seek": 476144, "start": 4767.5199999999995, "end": 4773.2, "text": " kind of incremental pace. And I think crucially, we have to think that we're currently", "tokens": [50668, 733, 295, 35759, 11638, 13, 400, 286, 519, 5140, 1909, 11, 321, 362, 281, 519, 300, 321, 434, 4362, 50952], "temperature": 0.0, "avg_logprob": -0.08749448207386754, "compression_ratio": 1.9475806451612903, "no_speech_prob": 0.0009165750816464424}, {"id": 802, "seek": 476144, "start": 4774.08, "end": 4779.36, "text": " at each step of the way trying to use the most recent algorithms and most recent compute to", "tokens": [50996, 412, 1184, 1823, 295, 264, 636, 1382, 281, 764, 264, 881, 5162, 14642, 293, 881, 5162, 14722, 281, 51260], "temperature": 0.0, "avg_logprob": -0.08749448207386754, "compression_ratio": 1.9475806451612903, "no_speech_prob": 0.0009165750816464424}, {"id": 803, "seek": 476144, "start": 4779.36, "end": 4785.36, "text": " actually get AIs to do, let's say, useful science research. You know, if we're incrementally increasing", "tokens": [51260, 767, 483, 316, 6802, 281, 360, 11, 718, 311, 584, 11, 4420, 3497, 2132, 13, 509, 458, 11, 498, 321, 434, 26200, 379, 5662, 51560], "temperature": 0.0, "avg_logprob": -0.08749448207386754, "compression_ratio": 1.9475806451612903, "no_speech_prob": 0.0009165750816464424}, {"id": 804, "seek": 476144, "start": 4785.36, "end": 4789.919999999999, "text": " the computing algorithms, but we're not actually trying to get the AIs to do useful science research,", "tokens": [51560, 264, 15866, 14642, 11, 457, 321, 434, 406, 767, 1382, 281, 483, 264, 316, 6802, 281, 360, 4420, 3497, 2132, 11, 51788], "temperature": 0.0, "avg_logprob": -0.08749448207386754, "compression_ratio": 1.9475806451612903, "no_speech_prob": 0.0009165750816464424}, {"id": 805, "seek": 478992, "start": 4789.92, "end": 4794.24, "text": " then it could be that one day we decide to try and get AIs to do useful science research,", "tokens": [50364, 550, 309, 727, 312, 300, 472, 786, 321, 4536, 281, 853, 293, 483, 316, 6802, 281, 360, 4420, 3497, 2132, 11, 50580], "temperature": 0.0, "avg_logprob": -0.09132431138236567, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0006665976834483445}, {"id": 806, "seek": 478992, "start": 4794.24, "end": 4798.64, "text": " and then suddenly, we train them to reassign all their cognitive abilities to that task,", "tokens": [50580, 293, 550, 5800, 11, 321, 3847, 552, 281, 19486, 788, 439, 641, 15605, 11582, 281, 300, 5633, 11, 50800], "temperature": 0.0, "avg_logprob": -0.09132431138236567, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0006665976834483445}, {"id": 807, "seek": 478992, "start": 4798.64, "end": 4803.28, "text": " and we do get something that's really quick. So it's a really important assumption here that", "tokens": [50800, 293, 321, 360, 483, 746, 300, 311, 534, 1702, 13, 407, 309, 311, 257, 534, 1021, 15302, 510, 300, 51032], "temperature": 0.0, "avg_logprob": -0.09132431138236567, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0006665976834483445}, {"id": 808, "seek": 478992, "start": 4803.92, "end": 4808.8, "text": " in some sense, the AI development ecosystem is kind of efficient.", "tokens": [51064, 294, 512, 2020, 11, 264, 7318, 3250, 11311, 307, 733, 295, 7148, 13, 51308], "temperature": 0.0, "avg_logprob": -0.09132431138236567, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0006665976834483445}, {"id": 809, "seek": 478992, "start": 4808.8, "end": 4816.4800000000005, "text": " Yep. And it's using, yeah, it's using new AI capabilities to do like the cutting edge research,", "tokens": [51308, 7010, 13, 400, 309, 311, 1228, 11, 1338, 11, 309, 311, 1228, 777, 7318, 10862, 281, 360, 411, 264, 6492, 4691, 2132, 11, 51692], "temperature": 0.0, "avg_logprob": -0.09132431138236567, "compression_ratio": 1.678294573643411, "no_speech_prob": 0.0006665976834483445}, {"id": 810, "seek": 481648, "start": 4816.48, "end": 4823.12, "text": " as opposed to like, if there were only market incentives to make AI that, I don't know, made", "tokens": [50364, 382, 8851, 281, 411, 11, 498, 456, 645, 787, 2142, 23374, 281, 652, 7318, 300, 11, 286, 500, 380, 458, 11, 1027, 50696], "temperature": 0.0, "avg_logprob": -0.10482771678637433, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0024692253209650517}, {"id": 811, "seek": 481648, "start": 4823.12, "end": 4830.879999999999, "text": " these beautiful images like Dolly, and no incentives at all to use AI to improve AI systems, then", "tokens": [50696, 613, 2238, 5267, 411, 1144, 13020, 11, 293, 572, 23374, 412, 439, 281, 764, 7318, 281, 3470, 7318, 3652, 11, 550, 51084], "temperature": 0.0, "avg_logprob": -0.10482771678637433, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0024692253209650517}, {"id": 812, "seek": 481648, "start": 4831.759999999999, "end": 4837.5199999999995, "text": " if one day we made a few tweaks to Dolly, and we're like, stop making pictures, make programs", "tokens": [51128, 498, 472, 786, 321, 1027, 257, 1326, 46664, 281, 1144, 13020, 11, 293, 321, 434, 411, 11, 1590, 1455, 5242, 11, 652, 4268, 51416], "temperature": 0.0, "avg_logprob": -0.10482771678637433, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0024692253209650517}, {"id": 813, "seek": 481648, "start": 4837.5199999999995, "end": 4843.12, "text": " instead. And but like actually, like it did have capabilities related enough that we could", "tokens": [51416, 2602, 13, 400, 457, 411, 767, 11, 411, 309, 630, 362, 10862, 4077, 1547, 300, 321, 727, 51696], "temperature": 0.0, "avg_logprob": -0.10482771678637433, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0024692253209650517}, {"id": 814, "seek": 484312, "start": 4843.12, "end": 4848.96, "text": " make that tweak semi easily. Then all of a sudden, we'd have this system that could write programs", "tokens": [50364, 652, 300, 29879, 12909, 3612, 13, 1396, 439, 295, 257, 3990, 11, 321, 1116, 362, 341, 1185, 300, 727, 2464, 4268, 50656], "temperature": 0.0, "avg_logprob": -0.12416215670310844, "compression_ratio": 1.7027972027972027, "no_speech_prob": 0.002652830211445689}, {"id": 815, "seek": 484312, "start": 4848.96, "end": 4855.5199999999995, "text": " really efficiently that we'd never had before. Right. Or maybe a more probable scenario would be", "tokens": [50656, 534, 19621, 300, 321, 1116, 1128, 632, 949, 13, 1779, 13, 1610, 1310, 257, 544, 21759, 9005, 576, 312, 50984], "temperature": 0.0, "avg_logprob": -0.12416215670310844, "compression_ratio": 1.7027972027972027, "no_speech_prob": 0.002652830211445689}, {"id": 816, "seek": 484312, "start": 4855.5199999999995, "end": 4861.04, "text": " we're just using all our AI resources to train these image generating systems like Dolly. And", "tokens": [50984, 321, 434, 445, 1228, 439, 527, 7318, 3593, 281, 3847, 613, 3256, 17746, 3652, 411, 1144, 13020, 13, 400, 51260], "temperature": 0.0, "avg_logprob": -0.12416215670310844, "compression_ratio": 1.7027972027972027, "no_speech_prob": 0.002652830211445689}, {"id": 817, "seek": 484312, "start": 4861.04, "end": 4867.04, "text": " then we're like, you know what, why don't we just try using all those resources to train a science AI?", "tokens": [51260, 550, 321, 434, 411, 11, 291, 458, 437, 11, 983, 500, 380, 321, 445, 853, 1228, 439, 729, 3593, 281, 3847, 257, 3497, 7318, 30, 51560], "temperature": 0.0, "avg_logprob": -0.12416215670310844, "compression_ratio": 1.7027972027972027, "no_speech_prob": 0.002652830211445689}, {"id": 818, "seek": 484312, "start": 4867.04, "end": 4872.08, "text": " And then we, you know, we pick the architecture to specialize of a science, we use the data to", "tokens": [51560, 400, 550, 321, 11, 291, 458, 11, 321, 1888, 264, 9482, 281, 37938, 295, 257, 3497, 11, 321, 764, 264, 1412, 281, 51812], "temperature": 0.0, "avg_logprob": -0.12416215670310844, "compression_ratio": 1.7027972027972027, "no_speech_prob": 0.002652830211445689}, {"id": 819, "seek": 487208, "start": 4872.08, "end": 4876.4, "text": " specialize it for science, we use all the compute that we were previously pouring into these image", "tokens": [50364, 37938, 309, 337, 3497, 11, 321, 764, 439, 264, 14722, 300, 321, 645, 8046, 20450, 666, 613, 3256, 50580], "temperature": 0.0, "avg_logprob": -0.1049974085920948, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.00039581258897669613}, {"id": 820, "seek": 487208, "start": 4876.4, "end": 4881.5199999999995, "text": " generation systems. And then suddenly we're like, wow, our science AI is amazing. And it came out of", "tokens": [50580, 5125, 3652, 13, 400, 550, 5800, 321, 434, 411, 11, 6076, 11, 527, 3497, 7318, 307, 2243, 13, 400, 309, 1361, 484, 295, 50836], "temperature": 0.0, "avg_logprob": -0.1049974085920948, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.00039581258897669613}, {"id": 821, "seek": 487208, "start": 4881.5199999999995, "end": 4886.96, "text": " nowhere because we hadn't been trying in the previous years to do this at all. Yeah. So are we", "tokens": [50836, 11159, 570, 321, 8782, 380, 668, 1382, 294, 264, 3894, 924, 281, 360, 341, 412, 439, 13, 865, 13, 407, 366, 321, 51108], "temperature": 0.0, "avg_logprob": -0.1049974085920948, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.00039581258897669613}, {"id": 822, "seek": 487208, "start": 4886.96, "end": 4893.6, "text": " currently trying to make AI systems that are really good at science? I think it's a good question.", "tokens": [51108, 4362, 1382, 281, 652, 7318, 3652, 300, 366, 534, 665, 412, 3497, 30, 286, 519, 309, 311, 257, 665, 1168, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1049974085920948, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.00039581258897669613}, {"id": 823, "seek": 487208, "start": 4894.64, "end": 4901.36, "text": " The market doesn't seem to me to be super efficient. I've been playing our GPT for a bit", "tokens": [51492, 440, 2142, 1177, 380, 1643, 281, 385, 281, 312, 1687, 7148, 13, 286, 600, 668, 2433, 527, 26039, 51, 337, 257, 857, 51828], "temperature": 0.0, "avg_logprob": -0.1049974085920948, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.00039581258897669613}, {"id": 824, "seek": 490136, "start": 4901.36, "end": 4907.2, "text": " recently. And to me, it looks like GPT-4 is pretty smart. It doesn't seem to me like", "tokens": [50364, 3938, 13, 400, 281, 385, 11, 309, 1542, 411, 26039, 51, 12, 19, 307, 1238, 4069, 13, 467, 1177, 380, 1643, 281, 385, 411, 50656], "temperature": 0.0, "avg_logprob": -0.1186038598246958, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.0023681249003857374}, {"id": 825, "seek": 490136, "start": 4907.2, "end": 4913.679999999999, "text": " its cognitive abilities have been really direct in the direction of helping to advance science,", "tokens": [50656, 1080, 15605, 11582, 362, 668, 534, 2047, 294, 264, 3513, 295, 4315, 281, 7295, 3497, 11, 50980], "temperature": 0.0, "avg_logprob": -0.1186038598246958, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.0023681249003857374}, {"id": 826, "seek": 490136, "start": 4913.679999999999, "end": 4920.639999999999, "text": " to be honest. So I do think that this argument could ultimately say, yeah,", "tokens": [50980, 281, 312, 3245, 13, 407, 286, 360, 519, 300, 341, 6770, 727, 6284, 584, 11, 1338, 11, 51328], "temperature": 0.0, "avg_logprob": -0.1186038598246958, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.0023681249003857374}, {"id": 827, "seek": 490136, "start": 4922.0, "end": 4928.24, "text": " a faster takeoff is plausible and the mechanism could be reallocating the AI's cognitive abilities", "tokens": [51396, 257, 4663, 747, 4506, 307, 39925, 293, 264, 7513, 727, 312, 319, 336, 905, 990, 264, 7318, 311, 15605, 11582, 51708], "temperature": 0.0, "avg_logprob": -0.1186038598246958, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.0023681249003857374}, {"id": 828, "seek": 492824, "start": 4928.96, "end": 4933.36, "text": " towards science. I mean, GPT-4 is just trained to predict the next world on the internet.", "tokens": [50400, 3030, 3497, 13, 286, 914, 11, 26039, 51, 12, 19, 307, 445, 8895, 281, 6069, 264, 958, 1002, 322, 264, 4705, 13, 50620], "temperature": 0.0, "avg_logprob": -0.09531437117477944, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.007212628144770861}, {"id": 829, "seek": 492824, "start": 4933.36, "end": 4938.96, "text": " Right. That's a very different kind of task than the task of advancing science. And so", "tokens": [50620, 1779, 13, 663, 311, 257, 588, 819, 733, 295, 5633, 813, 264, 5633, 295, 27267, 3497, 13, 400, 370, 50900], "temperature": 0.0, "avg_logprob": -0.09531437117477944, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.007212628144770861}, {"id": 830, "seek": 492824, "start": 4939.679999999999, "end": 4945.28, "text": " I think that that is a reason to expect a faster takeoff. More of a jump. Yeah. Okay,", "tokens": [50936, 286, 519, 300, 300, 307, 257, 1778, 281, 2066, 257, 4663, 747, 4506, 13, 5048, 295, 257, 3012, 13, 865, 13, 1033, 11, 51216], "temperature": 0.0, "avg_logprob": -0.09531437117477944, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.007212628144770861}, {"id": 831, "seek": 492824, "start": 4945.28, "end": 4950.32, "text": " interesting. I haven't heard that argument before. Cool. Well, I want to now get to", "tokens": [51216, 1880, 13, 286, 2378, 380, 2198, 300, 6770, 949, 13, 8561, 13, 1042, 11, 286, 528, 281, 586, 483, 281, 51468], "temperature": 0.0, "avg_logprob": -0.09531437117477944, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.007212628144770861}, {"id": 832, "seek": 492824, "start": 4950.32, "end": 4956.8, "text": " the report that you've written on AI takeoff speeds, which asks how quickly AI might go from", "tokens": [51468, 264, 2275, 300, 291, 600, 3720, 322, 7318, 747, 4506, 16411, 11, 597, 8962, 577, 2661, 7318, 1062, 352, 490, 51792], "temperature": 0.0, "avg_logprob": -0.09531437117477944, "compression_ratio": 1.529616724738676, "no_speech_prob": 0.007212628144770861}, {"id": 833, "seek": 495680, "start": 4957.360000000001, "end": 4963.360000000001, "text": " kind of pretty economically valuable to just extremely capable, maybe as good as humans.", "tokens": [50392, 733, 295, 1238, 26811, 8263, 281, 445, 4664, 8189, 11, 1310, 382, 665, 382, 6255, 13, 50692], "temperature": 0.0, "avg_logprob": -0.09040287949822166, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002111702458932996}, {"id": 834, "seek": 495680, "start": 4964.16, "end": 4969.84, "text": " And yeah, I guess you define your terms pretty clearly in your report. So maybe we should start", "tokens": [50732, 400, 1338, 11, 286, 2041, 291, 6964, 428, 2115, 1238, 4448, 294, 428, 2275, 13, 407, 1310, 321, 820, 722, 51016], "temperature": 0.0, "avg_logprob": -0.09040287949822166, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002111702458932996}, {"id": 835, "seek": 495680, "start": 4969.84, "end": 4976.24, "text": " by doing that. Am I right in remembering that you are trying to answer the question of how", "tokens": [51016, 538, 884, 300, 13, 2012, 286, 558, 294, 20719, 300, 291, 366, 1382, 281, 1867, 264, 1168, 295, 577, 51336], "temperature": 0.0, "avg_logprob": -0.09040287949822166, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002111702458932996}, {"id": 836, "seek": 495680, "start": 4976.24, "end": 4984.16, "text": " quickly we'll go from AI systems that can do 20% of human tasks to AI systems that can do 100%", "tokens": [51336, 2661, 321, 603, 352, 490, 7318, 3652, 300, 393, 360, 945, 4, 295, 1952, 9608, 281, 7318, 3652, 300, 393, 360, 2319, 4, 51732], "temperature": 0.0, "avg_logprob": -0.09040287949822166, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002111702458932996}, {"id": 837, "seek": 498416, "start": 4984.72, "end": 4990.639999999999, "text": " of human tasks? Is that right? Yeah, that's right. In particular, I am restricting to", "tokens": [50392, 295, 1952, 9608, 30, 1119, 300, 558, 30, 865, 11, 300, 311, 558, 13, 682, 1729, 11, 286, 669, 1472, 37714, 281, 50688], "temperature": 0.0, "avg_logprob": -0.08400522364248143, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015559910098090768}, {"id": 838, "seek": 498416, "start": 4990.639999999999, "end": 4996.32, "text": " cognitive tasks. That's similar to what we discussed earlier. It's any task that you could do", "tokens": [50688, 15605, 9608, 13, 663, 311, 2531, 281, 437, 321, 7152, 3071, 13, 467, 311, 604, 5633, 300, 291, 727, 360, 50972], "temperature": 0.0, "avg_logprob": -0.08400522364248143, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015559910098090768}, {"id": 839, "seek": 498416, "start": 4996.96, "end": 5000.16, "text": " remotely that doesn't require you to be physically manipulating", "tokens": [51004, 20824, 300, 1177, 380, 3651, 291, 281, 312, 9762, 40805, 51164], "temperature": 0.0, "avg_logprob": -0.08400522364248143, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015559910098090768}, {"id": 840, "seek": 498416, "start": 5000.88, "end": 5005.36, "text": " objects yourself because AI's don't have physical bodies. That wouldn't be included for them.", "tokens": [51200, 6565, 1803, 570, 7318, 311, 500, 380, 362, 4001, 7510, 13, 663, 2759, 380, 312, 5556, 337, 552, 13, 51424], "temperature": 0.0, "avg_logprob": -0.08400522364248143, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015559910098090768}, {"id": 841, "seek": 498416, "start": 5006.0, "end": 5011.68, "text": " I mean, it does include tasks like giving instructions to a human who's doing a physical", "tokens": [51456, 286, 914, 11, 309, 775, 4090, 9608, 411, 2902, 9415, 281, 257, 1952, 567, 311, 884, 257, 4001, 51740], "temperature": 0.0, "avg_logprob": -0.08400522364248143, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015559910098090768}, {"id": 842, "seek": 501168, "start": 5011.68, "end": 5017.52, "text": " job telling them where to move things, what to do with their arms, or potentially giving", "tokens": [50364, 1691, 3585, 552, 689, 281, 1286, 721, 11, 437, 281, 360, 365, 641, 5812, 11, 420, 7263, 2902, 50656], "temperature": 0.0, "avg_logprob": -0.11090042885769619, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0010604665149003267}, {"id": 843, "seek": 501168, "start": 5017.52, "end": 5022.64, "text": " instructions to robots that are doing physical tasks. But it doesn't include the kind of the", "tokens": [50656, 9415, 281, 14733, 300, 366, 884, 4001, 9608, 13, 583, 309, 1177, 380, 4090, 264, 733, 295, 264, 50912], "temperature": 0.0, "avg_logprob": -0.11090042885769619, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0010604665149003267}, {"id": 844, "seek": 501168, "start": 5022.64, "end": 5030.8, "text": " physical motions themselves. So does it include things like driving cars? Yes, it does. Okay,", "tokens": [50912, 4001, 27500, 2969, 13, 407, 775, 309, 4090, 721, 411, 4840, 5163, 30, 1079, 11, 309, 775, 13, 1033, 11, 51320], "temperature": 0.0, "avg_logprob": -0.11090042885769619, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0010604665149003267}, {"id": 845, "seek": 501168, "start": 5030.8, "end": 5041.280000000001, "text": " it does. And that's because driving cars is really a set of algorithms. And you can turn the wheel of", "tokens": [51320, 309, 775, 13, 400, 300, 311, 570, 4840, 5163, 307, 534, 257, 992, 295, 14642, 13, 400, 291, 393, 1261, 264, 5589, 295, 51844], "temperature": 0.0, "avg_logprob": -0.11090042885769619, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0010604665149003267}, {"id": 846, "seek": 504128, "start": 5041.679999999999, "end": 5049.84, "text": " car. So it's a good point that currently the way humans drive cars is by physically moving", "tokens": [50384, 1032, 13, 407, 309, 311, 257, 665, 935, 300, 4362, 264, 636, 6255, 3332, 5163, 307, 538, 9762, 2684, 50792], "temperature": 0.0, "avg_logprob": -0.0791349688241648, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0008476593648083508}, {"id": 847, "seek": 504128, "start": 5049.84, "end": 5058.5599999999995, "text": " various levers in the car. But I think actually giving the AI the control of the steering wheel", "tokens": [50792, 3683, 45571, 294, 264, 1032, 13, 583, 286, 519, 767, 2902, 264, 7318, 264, 1969, 295, 264, 14823, 5589, 51228], "temperature": 0.0, "avg_logprob": -0.0791349688241648, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0008476593648083508}, {"id": 848, "seek": 504128, "start": 5058.5599999999995, "end": 5062.5599999999995, "text": " and of the pedals and the brakes is actually pretty trivial. So the only thing that's hard", "tokens": [51228, 293, 295, 264, 35217, 293, 264, 19950, 307, 767, 1238, 26703, 13, 407, 264, 787, 551, 300, 311, 1152, 51428], "temperature": 0.0, "avg_logprob": -0.0791349688241648, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0008476593648083508}, {"id": 849, "seek": 504128, "start": 5062.5599999999995, "end": 5069.28, "text": " in practice about getting AI's to do driving is the cognitive parts of what should the car be", "tokens": [51428, 294, 3124, 466, 1242, 7318, 311, 281, 360, 4840, 307, 264, 15605, 3166, 295, 437, 820, 264, 1032, 312, 51764], "temperature": 0.0, "avg_logprob": -0.0791349688241648, "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0008476593648083508}, {"id": 850, "seek": 506928, "start": 5069.28, "end": 5077.5199999999995, "text": " doing at each point. Yeah, right. Okay, so then an example of a task that isn't included is something", "tokens": [50364, 884, 412, 1184, 935, 13, 865, 11, 558, 13, 1033, 11, 370, 550, 364, 1365, 295, 257, 5633, 300, 1943, 380, 5556, 307, 746, 50776], "temperature": 0.0, "avg_logprob": -0.08582771634592594, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.01082145981490612}, {"id": 851, "seek": 506928, "start": 5077.5199999999995, "end": 5083.599999999999, "text": " like helping people move house. It's like carrying the boxes in and out. Carrying the boxes in and", "tokens": [50776, 411, 4315, 561, 1286, 1782, 13, 467, 311, 411, 9792, 264, 9002, 294, 293, 484, 13, 2741, 19076, 264, 9002, 294, 293, 51080], "temperature": 0.0, "avg_logprob": -0.08582771634592594, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.01082145981490612}, {"id": 852, "seek": 506928, "start": 5083.599999999999, "end": 5089.44, "text": " out would not be included. Yeah, okay. But telling them here's the best plan for moving, here's the", "tokens": [51080, 484, 576, 406, 312, 5556, 13, 865, 11, 1392, 13, 583, 3585, 552, 510, 311, 264, 1151, 1393, 337, 2684, 11, 510, 311, 264, 51372], "temperature": 0.0, "avg_logprob": -0.08582771634592594, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.01082145981490612}, {"id": 853, "seek": 506928, "start": 5089.44, "end": 5094.48, "text": " order you should move the boxes in that would be included. Okay, so that's basically what you've done.", "tokens": [51372, 1668, 291, 820, 1286, 264, 9002, 294, 300, 576, 312, 5556, 13, 1033, 11, 370, 300, 311, 1936, 437, 291, 600, 1096, 13, 51624], "temperature": 0.0, "avg_logprob": -0.08582771634592594, "compression_ratio": 1.7991071428571428, "no_speech_prob": 0.01082145981490612}, {"id": 854, "seek": 509448, "start": 5095.2, "end": 5101.2, "text": " You want to know how fast do we go from 20% of cognitive tasks to 100% of cognitive tasks?", "tokens": [50400, 509, 528, 281, 458, 577, 2370, 360, 321, 352, 490, 945, 4, 295, 15605, 9608, 281, 2319, 4, 295, 15605, 9608, 30, 50700], "temperature": 0.0, "avg_logprob": -0.12113339310392328, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.05343217775225639}, {"id": 855, "seek": 509448, "start": 5101.839999999999, "end": 5106.879999999999, "text": " Yeah, can you actually clarify what it means for AI to be able to complete 20% of tasks?", "tokens": [50732, 865, 11, 393, 291, 767, 17594, 437, 309, 1355, 337, 7318, 281, 312, 1075, 281, 3566, 945, 4, 295, 9608, 30, 50984], "temperature": 0.0, "avg_logprob": -0.12113339310392328, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.05343217775225639}, {"id": 856, "seek": 509448, "start": 5107.5199999999995, "end": 5112.4, "text": " So you could say, okay, let's say we can AI automate driving, what percentage is that? Is that", "tokens": [51016, 407, 291, 727, 584, 11, 1392, 11, 718, 311, 584, 321, 393, 7318, 31605, 4840, 11, 437, 9668, 307, 300, 30, 1119, 300, 51260], "temperature": 0.0, "avg_logprob": -0.12113339310392328, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.05343217775225639}, {"id": 857, "seek": 509448, "start": 5112.4, "end": 5119.599999999999, "text": " 3%? Because it's, you know, 3% of people do it? Or is it, you know, do we just like give, we look at", "tokens": [51260, 805, 4, 30, 1436, 309, 311, 11, 291, 458, 11, 805, 4, 295, 561, 360, 309, 30, 1610, 307, 309, 11, 291, 458, 11, 360, 321, 445, 411, 976, 11, 321, 574, 412, 51620], "temperature": 0.0, "avg_logprob": -0.12113339310392328, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.05343217775225639}, {"id": 858, "seek": 511960, "start": 5119.6, "end": 5124.96, "text": " a long list of tasks and assume that it takes up an equal percentage? Like, what do we mean by 20%?", "tokens": [50364, 257, 938, 1329, 295, 9608, 293, 6552, 300, 309, 2516, 493, 364, 2681, 9668, 30, 1743, 11, 437, 360, 321, 914, 538, 945, 4, 30, 50632], "temperature": 0.0, "avg_logprob": -0.08904360798956121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.08375098556280136}, {"id": 859, "seek": 511960, "start": 5124.96, "end": 5131.280000000001, "text": " Yes. And so the way I'm currently thinking about that is you look at how much people pay for those", "tokens": [50632, 1079, 13, 400, 370, 264, 636, 286, 478, 4362, 1953, 466, 300, 307, 291, 574, 412, 577, 709, 561, 1689, 337, 729, 50948], "temperature": 0.0, "avg_logprob": -0.08904360798956121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.08375098556280136}, {"id": 860, "seek": 511960, "start": 5131.280000000001, "end": 5135.68, "text": " tasks to be performed in the economy. Okay. So let's take the driving example. I don't know. Let's", "tokens": [50948, 9608, 281, 312, 10332, 294, 264, 5010, 13, 1033, 13, 407, 718, 311, 747, 264, 4840, 1365, 13, 286, 500, 380, 458, 13, 961, 311, 51168], "temperature": 0.0, "avg_logprob": -0.08904360798956121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.08375098556280136}, {"id": 861, "seek": 511960, "start": 5135.68, "end": 5142.88, "text": " say that drivers around the world are being paid $2 trillion a year for the work they're doing,", "tokens": [51168, 584, 300, 11590, 926, 264, 1002, 366, 885, 4835, 1848, 17, 18723, 257, 1064, 337, 264, 589, 436, 434, 884, 11, 51528], "temperature": 0.0, "avg_logprob": -0.08904360798956121, "compression_ratio": 1.5232558139534884, "no_speech_prob": 0.08375098556280136}, {"id": 862, "seek": 514288, "start": 5142.88, "end": 5151.4400000000005, "text": " driving trucks and taxis and everything else. In that case, because $2 trillion is 2% of the", "tokens": [50364, 4840, 16156, 293, 3366, 271, 293, 1203, 1646, 13, 682, 300, 1389, 11, 570, 1848, 17, 18723, 307, 568, 4, 295, 264, 50792], "temperature": 0.0, "avg_logprob": -0.11051012919499324, "compression_ratio": 1.5, "no_speech_prob": 0.04558386653661728}, {"id": 863, "seek": 514288, "start": 5151.4400000000005, "end": 5158.8, "text": " global GDP, I would say that automating driving, fully automating all driving would be automating", "tokens": [50792, 4338, 19599, 11, 286, 576, 584, 300, 3553, 990, 4840, 11, 4498, 3553, 990, 439, 4840, 576, 312, 3553, 990, 51160], "temperature": 0.0, "avg_logprob": -0.11051012919499324, "compression_ratio": 1.5, "no_speech_prob": 0.04558386653661728}, {"id": 864, "seek": 514288, "start": 5159.52, "end": 5168.16, "text": " 2% of all economic tasks. Got it. And then you're saying, how fast will we go from we can do 20%?", "tokens": [51196, 568, 4, 295, 439, 4836, 9608, 13, 5803, 309, 13, 400, 550, 291, 434, 1566, 11, 577, 2370, 486, 321, 352, 490, 321, 393, 360, 945, 4, 30, 51628], "temperature": 0.0, "avg_logprob": -0.11051012919499324, "compression_ratio": 1.5, "no_speech_prob": 0.04558386653661728}, {"id": 865, "seek": 516816, "start": 5168.16, "end": 5175.04, "text": " So I don't know, maybe it's like replacing all drivers, maybe it's replacing all journalism", "tokens": [50364, 407, 286, 500, 380, 458, 11, 1310, 309, 311, 411, 19139, 439, 11590, 11, 1310, 309, 311, 19139, 439, 23191, 50708], "temperature": 0.0, "avg_logprob": -0.13370532035827637, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.006971432361751795}, {"id": 866, "seek": 516816, "start": 5175.04, "end": 5181.36, "text": " because GPT-4 seems to be really good at writing. And I don't know, a couple of other, a couple of", "tokens": [50708, 570, 26039, 51, 12, 19, 2544, 281, 312, 534, 665, 412, 3579, 13, 400, 286, 500, 380, 458, 11, 257, 1916, 295, 661, 11, 257, 1916, 295, 51024], "temperature": 0.0, "avg_logprob": -0.13370532035827637, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.006971432361751795}, {"id": 867, "seek": 516816, "start": 5181.36, "end": 5188.88, "text": " other things. How fast do you go from like that chunk to literally all cognitive tasks,", "tokens": [51024, 661, 721, 13, 1012, 2370, 360, 291, 352, 490, 411, 300, 16635, 281, 3736, 439, 15605, 9608, 11, 51400], "temperature": 0.0, "avg_logprob": -0.13370532035827637, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.006971432361751795}, {"id": 868, "seek": 516816, "start": 5188.88, "end": 5194.72, "text": " including, I guess, science, AI, R&D. Now, one complication is that from year to year,", "tokens": [51400, 3009, 11, 286, 2041, 11, 3497, 11, 7318, 11, 497, 5, 35, 13, 823, 11, 472, 1209, 8758, 307, 300, 490, 1064, 281, 1064, 11, 51692], "temperature": 0.0, "avg_logprob": -0.13370532035827637, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.006971432361751795}, {"id": 869, "seek": 519472, "start": 5195.4400000000005, "end": 5202.16, "text": " the amount that is paid to people to perform each task might change. So in 2020, maybe drivers", "tokens": [50400, 264, 2372, 300, 307, 4835, 281, 561, 281, 2042, 1184, 5633, 1062, 1319, 13, 407, 294, 4808, 11, 1310, 11590, 50736], "temperature": 0.0, "avg_logprob": -0.12421369552612305, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009311629459261894}, {"id": 870, "seek": 519472, "start": 5202.16, "end": 5208.0, "text": " are paid $3 trillion a year for their work, maybe in 2025, they're paid $4 trillion a year,", "tokens": [50736, 366, 4835, 1848, 18, 18723, 257, 1064, 337, 641, 589, 11, 1310, 294, 39209, 11, 436, 434, 4835, 1848, 19, 18723, 257, 1064, 11, 51028], "temperature": 0.0, "avg_logprob": -0.12421369552612305, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009311629459261894}, {"id": 871, "seek": 519472, "start": 5208.0, "end": 5213.6, "text": " and that could change. So I'm pegging these percentages to the year 2020. Got it. It's a kind", "tokens": [51028, 293, 300, 727, 1319, 13, 407, 286, 478, 520, 10877, 613, 42270, 281, 264, 1064, 4808, 13, 5803, 309, 13, 467, 311, 257, 733, 51308], "temperature": 0.0, "avg_logprob": -0.12421369552612305, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009311629459261894}, {"id": 872, "seek": 519472, "start": 5213.6, "end": 5218.88, "text": " of arbitrary choice just to make the definition unambiguous. Yes. Okay. That makes sense. So what", "tokens": [51308, 295, 23211, 3922, 445, 281, 652, 264, 7123, 517, 2173, 30525, 13, 1079, 13, 1033, 13, 663, 1669, 2020, 13, 407, 437, 51572], "temperature": 0.0, "avg_logprob": -0.12421369552612305, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.009311629459261894}, {"id": 873, "seek": 521888, "start": 5218.88, "end": 5225.92, "text": " were they paid in 2020? And even if, I don't know, like wages change for all sorts of reasons,", "tokens": [50364, 645, 436, 4835, 294, 4808, 30, 400, 754, 498, 11, 286, 500, 380, 458, 11, 411, 20097, 1319, 337, 439, 7527, 295, 4112, 11, 50716], "temperature": 0.0, "avg_logprob": -0.05302577018737793, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.012130937539041042}, {"id": 874, "seek": 521888, "start": 5225.92, "end": 5233.28, "text": " including AI taking over some jobs, we're still just thinking of what percentage of the 2020", "tokens": [50716, 3009, 7318, 1940, 670, 512, 4782, 11, 321, 434, 920, 445, 1953, 295, 437, 9668, 295, 264, 4808, 51084], "temperature": 0.0, "avg_logprob": -0.05302577018737793, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.012130937539041042}, {"id": 875, "seek": 521888, "start": 5233.28, "end": 5239.4400000000005, "text": " cognitive economy is being automated. Exactly. And it is really important to keep that in mind,", "tokens": [51084, 15605, 5010, 307, 885, 18473, 13, 7587, 13, 400, 309, 307, 534, 1021, 281, 1066, 300, 294, 1575, 11, 51392], "temperature": 0.0, "avg_logprob": -0.05302577018737793, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.012130937539041042}, {"id": 876, "seek": 521888, "start": 5239.4400000000005, "end": 5245.76, "text": " because typically when AI automates a certain task, it becomes really cheap to do that task.", "tokens": [51392, 570, 5850, 562, 7318, 3553, 1024, 257, 1629, 5633, 11, 309, 3643, 534, 7084, 281, 360, 300, 5633, 13, 51708], "temperature": 0.0, "avg_logprob": -0.05302577018737793, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.012130937539041042}, {"id": 877, "seek": 524576, "start": 5245.76, "end": 5249.52, "text": " Right. So it becomes a much smaller fraction of the economy. Exactly. Yeah.", "tokens": [50364, 1779, 13, 407, 309, 3643, 257, 709, 4356, 14135, 295, 264, 5010, 13, 7587, 13, 865, 13, 50552], "temperature": 0.0, "avg_logprob": -0.09222453881886379, "compression_ratio": 1.6943521594684385, "no_speech_prob": 0.000774139363784343}, {"id": 878, "seek": 524576, "start": 5249.52, "end": 5253.52, "text": " And so you can end up thinking that AI is never doing anything when it's actually", "tokens": [50552, 400, 370, 291, 393, 917, 493, 1953, 300, 7318, 307, 1128, 884, 1340, 562, 309, 311, 767, 50752], "temperature": 0.0, "avg_logprob": -0.09222453881886379, "compression_ratio": 1.6943521594684385, "no_speech_prob": 0.000774139363784343}, {"id": 879, "seek": 524576, "start": 5253.52, "end": 5258.0, "text": " done almost everything. And so that is just an important thing to be aware of.", "tokens": [50752, 1096, 1920, 1203, 13, 400, 370, 300, 307, 445, 364, 1021, 551, 281, 312, 3650, 295, 13, 50976], "temperature": 0.0, "avg_logprob": -0.09222453881886379, "compression_ratio": 1.6943521594684385, "no_speech_prob": 0.000774139363784343}, {"id": 880, "seek": 524576, "start": 5258.0, "end": 5262.64, "text": " Yeah. That makes total sense. Is there an analogy from the Industrial Revolution or something?", "tokens": [50976, 865, 13, 663, 1669, 3217, 2020, 13, 1119, 456, 364, 21663, 490, 264, 32059, 16617, 420, 746, 30, 51208], "temperature": 0.0, "avg_logprob": -0.09222453881886379, "compression_ratio": 1.6943521594684385, "no_speech_prob": 0.000774139363784343}, {"id": 881, "seek": 524576, "start": 5263.4400000000005, "end": 5269.76, "text": " The best analogy might be agriculture. So I think in 1500, basically everyone worked in", "tokens": [51248, 440, 1151, 21663, 1062, 312, 14837, 13, 407, 286, 519, 294, 22671, 11, 1936, 1518, 2732, 294, 51564], "temperature": 0.0, "avg_logprob": -0.09222453881886379, "compression_ratio": 1.6943521594684385, "no_speech_prob": 0.000774139363784343}, {"id": 882, "seek": 524576, "start": 5269.76, "end": 5274.88, "text": " agriculture. Right. It was 90% of the economy or something. Exactly. All of GDP would have", "tokens": [51564, 14837, 13, 1779, 13, 467, 390, 4289, 4, 295, 264, 5010, 420, 746, 13, 7587, 13, 1057, 295, 19599, 576, 362, 51820], "temperature": 0.0, "avg_logprob": -0.09222453881886379, "compression_ratio": 1.6943521594684385, "no_speech_prob": 0.000774139363784343}, {"id": 883, "seek": 527488, "start": 5274.88, "end": 5280.0, "text": " basically been agriculture. Today, I think it's less than 5%. And that's because we've become", "tokens": [50364, 1936, 668, 14837, 13, 2692, 11, 286, 519, 309, 311, 1570, 813, 1025, 6856, 400, 300, 311, 570, 321, 600, 1813, 50620], "temperature": 0.0, "avg_logprob": -0.09613273360512474, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0004491234722081572}, {"id": 884, "seek": 527488, "start": 5280.0, "end": 5288.16, "text": " really good at producing food with very little need for human labor. So it's not to say that", "tokens": [50620, 534, 665, 412, 10501, 1755, 365, 588, 707, 643, 337, 1952, 5938, 13, 407, 309, 311, 406, 281, 584, 300, 51028], "temperature": 0.0, "avg_logprob": -0.09613273360512474, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0004491234722081572}, {"id": 885, "seek": 527488, "start": 5288.16, "end": 5296.96, "text": " fertilizer and trucks and really highly productive seeds aren't contributing a bunch to the economy,", "tokens": [51028, 31549, 293, 16156, 293, 534, 5405, 13304, 9203, 3212, 380, 19270, 257, 3840, 281, 264, 5010, 11, 51468], "temperature": 0.0, "avg_logprob": -0.09613273360512474, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0004491234722081572}, {"id": 886, "seek": 527488, "start": 5296.96, "end": 5302.96, "text": " but clearly they have. But were you to measure it as a fraction of the GDP that they're", "tokens": [51468, 457, 4448, 436, 362, 13, 583, 645, 291, 281, 3481, 309, 382, 257, 14135, 295, 264, 19599, 300, 436, 434, 51768], "temperature": 0.0, "avg_logprob": -0.09613273360512474, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.0004491234722081572}, {"id": 887, "seek": 530296, "start": 5302.96, "end": 5307.04, "text": " responsible for? It would be smaller because everything's just gotten so much cheaper because", "tokens": [50364, 6250, 337, 30, 467, 576, 312, 4356, 570, 1203, 311, 445, 5768, 370, 709, 12284, 570, 50568], "temperature": 0.0, "avg_logprob": -0.08154226871246988, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00804106704890728}, {"id": 888, "seek": 530296, "start": 5307.04, "end": 5313.6, "text": " of them. Exactly. Cool. Okay. That makes a bunch of sense. Nice. Okay. So we've got definitions.", "tokens": [50568, 295, 552, 13, 7587, 13, 8561, 13, 1033, 13, 663, 1669, 257, 3840, 295, 2020, 13, 5490, 13, 1033, 13, 407, 321, 600, 658, 21988, 13, 50896], "temperature": 0.0, "avg_logprob": -0.08154226871246988, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00804106704890728}, {"id": 889, "seek": 530296, "start": 5314.32, "end": 5323.76, "text": " So you've asked how fast will we go from AI systems that can do roughly 20% of the cognitive", "tokens": [50932, 407, 291, 600, 2351, 577, 2370, 486, 321, 352, 490, 7318, 3652, 300, 393, 360, 9810, 945, 4, 295, 264, 15605, 51404], "temperature": 0.0, "avg_logprob": -0.08154226871246988, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00804106704890728}, {"id": 890, "seek": 530296, "start": 5323.76, "end": 5330.96, "text": " tasks that humans were doing as of 2020? And how quickly will we go from 20% of those tasks", "tokens": [51404, 9608, 300, 6255, 645, 884, 382, 295, 4808, 30, 400, 577, 2661, 486, 321, 352, 490, 945, 4, 295, 729, 9608, 51764], "temperature": 0.0, "avg_logprob": -0.08154226871246988, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00804106704890728}, {"id": 891, "seek": 533096, "start": 5330.96, "end": 5337.84, "text": " to 100% of those tasks being at least in theory able to be automated by AI systems?", "tokens": [50364, 281, 2319, 4, 295, 729, 9608, 885, 412, 1935, 294, 5261, 1075, 281, 312, 18473, 538, 7318, 3652, 30, 50708], "temperature": 0.0, "avg_logprob": -0.10477095064909561, "compression_ratio": 1.5361702127659576, "no_speech_prob": 0.0018280051881447434}, {"id": 892, "seek": 533096, "start": 5338.72, "end": 5345.44, "text": " So yeah, what was your result? So the conclusion from the report is, I guess, pretty scary.", "tokens": [50752, 407, 1338, 11, 437, 390, 428, 1874, 30, 407, 264, 10063, 490, 264, 2275, 307, 11, 286, 2041, 11, 1238, 6958, 13, 51088], "temperature": 0.0, "avg_logprob": -0.10477095064909561, "compression_ratio": 1.5361702127659576, "no_speech_prob": 0.0018280051881447434}, {"id": 893, "seek": 533096, "start": 5346.32, "end": 5353.68, "text": " The bottom line is that my median guess is that it would take just a small number of years to go", "tokens": [51132, 440, 2767, 1622, 307, 300, 452, 26779, 2041, 307, 300, 309, 576, 747, 445, 257, 1359, 1230, 295, 924, 281, 352, 51500], "temperature": 0.0, "avg_logprob": -0.10477095064909561, "compression_ratio": 1.5361702127659576, "no_speech_prob": 0.0018280051881447434}, {"id": 894, "seek": 533096, "start": 5353.68, "end": 5360.88, "text": " from that 20% to the 100%. So I think equally likely to happen in less than three years.", "tokens": [51500, 490, 300, 945, 4, 281, 264, 2319, 6856, 407, 286, 519, 12309, 3700, 281, 1051, 294, 1570, 813, 1045, 924, 13, 51860], "temperature": 0.0, "avg_logprob": -0.10477095064909561, "compression_ratio": 1.5361702127659576, "no_speech_prob": 0.0018280051881447434}, {"id": 895, "seek": 536096, "start": 5361.28, "end": 5369.92, "text": " As it is to happen in more than three years. So a pretty abrupt and quick change is the kind of", "tokens": [50380, 1018, 309, 307, 281, 1051, 294, 544, 813, 1045, 924, 13, 407, 257, 1238, 33401, 293, 1702, 1319, 307, 264, 733, 295, 50812], "temperature": 0.0, "avg_logprob": -0.10176579793294271, "compression_ratio": 1.513089005235602, "no_speech_prob": 0.0007632478955201805}, {"id": 896, "seek": 536096, "start": 5369.92, "end": 5378.56, "text": " median kind of best guess median. Wow. And do you believe that in your bones? Does that feel like", "tokens": [50812, 26779, 733, 295, 1151, 2041, 26779, 13, 3153, 13, 400, 360, 291, 1697, 300, 294, 428, 10491, 30, 4402, 300, 841, 411, 51244], "temperature": 0.0, "avg_logprob": -0.10176579793294271, "compression_ratio": 1.513089005235602, "no_speech_prob": 0.0007632478955201805}, {"id": 897, "seek": 536096, "start": 5378.56, "end": 5385.92, "text": " like very plausible to you? Yeah, I do. So some some quick things about why why it's plausible.", "tokens": [51244, 411, 588, 39925, 281, 291, 30, 865, 11, 286, 360, 13, 407, 512, 512, 1702, 721, 466, 983, 983, 309, 311, 39925, 13, 51612], "temperature": 0.0, "avg_logprob": -0.10176579793294271, "compression_ratio": 1.513089005235602, "no_speech_prob": 0.0007632478955201805}, {"id": 898, "seek": 538592, "start": 5386.8, "end": 5393.52, "text": " Each year, once you take algorithms, better algorithms and using more compute into account,", "tokens": [50408, 6947, 1064, 11, 1564, 291, 747, 14642, 11, 1101, 14642, 293, 1228, 544, 14722, 666, 2696, 11, 50744], "temperature": 0.0, "avg_logprob": -0.10620439570883046, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.035545043647289276}, {"id": 899, "seek": 538592, "start": 5393.52, "end": 5398.96, "text": " we're currently training AIs each year that have kind of three times bigger brains than the year", "tokens": [50744, 321, 434, 4362, 3097, 316, 6802, 1184, 1064, 300, 362, 733, 295, 1045, 1413, 3801, 15442, 813, 264, 1064, 51016], "temperature": 0.0, "avg_logprob": -0.10620439570883046, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.035545043647289276}, {"id": 900, "seek": 538592, "start": 5398.96, "end": 5405.28, "text": " before. So really rough way to think about it. But you know, imagine three times smaller brain", "tokens": [51016, 949, 13, 407, 534, 5903, 636, 281, 519, 466, 309, 13, 583, 291, 458, 11, 3811, 1045, 1413, 4356, 3567, 51332], "temperature": 0.0, "avg_logprob": -0.10620439570883046, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.035545043647289276}, {"id": 901, "seek": 538592, "start": 5405.28, "end": 5412.96, "text": " than humans. That's chimpanzee brain size. Right. Each year, you're going from chimpanzees to humans?", "tokens": [51332, 813, 6255, 13, 663, 311, 18375, 48410, 68, 3567, 2744, 13, 1779, 13, 6947, 1064, 11, 291, 434, 516, 490, 18375, 48410, 279, 281, 6255, 30, 51716], "temperature": 0.0, "avg_logprob": -0.10620439570883046, "compression_ratio": 1.63135593220339, "no_speech_prob": 0.035545043647289276}, {"id": 902, "seek": 541296, "start": 5413.6, "end": 5419.44, "text": " That's, I think, you know, it's really hard to try and account for the effect of the algorithmic", "tokens": [50396, 663, 311, 11, 286, 519, 11, 291, 458, 11, 309, 311, 534, 1152, 281, 853, 293, 2696, 337, 264, 1802, 295, 264, 9284, 299, 50688], "temperature": 0.0, "avg_logprob": -0.08097884234260111, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0006248654681257904}, {"id": 903, "seek": 541296, "start": 5419.44, "end": 5426.72, "text": " improvements. But on my kind of best guess of what those amount to, yeah, each year, we're", "tokens": [50688, 13797, 13, 583, 322, 452, 733, 295, 1151, 2041, 295, 437, 729, 2372, 281, 11, 1338, 11, 1184, 1064, 11, 321, 434, 51052], "temperature": 0.0, "avg_logprob": -0.08097884234260111, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0006248654681257904}, {"id": 904, "seek": 541296, "start": 5426.72, "end": 5432.72, "text": " making the brains of AI systems about three times bigger. Wow. And right now, it's humans that are", "tokens": [51052, 1455, 264, 15442, 295, 7318, 3652, 466, 1045, 1413, 3801, 13, 3153, 13, 400, 558, 586, 11, 309, 311, 6255, 300, 366, 51352], "temperature": 0.0, "avg_logprob": -0.08097884234260111, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0006248654681257904}, {"id": 905, "seek": 541296, "start": 5432.72, "end": 5438.8, "text": " doing all the work to improve those AI systems. As we get close to AIs that match humans, we'll be", "tokens": [51352, 884, 439, 264, 589, 281, 3470, 729, 7318, 3652, 13, 1018, 321, 483, 1998, 281, 316, 6802, 300, 2995, 6255, 11, 321, 603, 312, 51656], "temperature": 0.0, "avg_logprob": -0.08097884234260111, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0006248654681257904}, {"id": 906, "seek": 543880, "start": 5438.8, "end": 5447.6, "text": " increasingly using AI systems to improve AI algorithms, design better AI chips. And so overall,", "tokens": [50364, 12980, 1228, 7318, 3652, 281, 3470, 7318, 14642, 11, 1715, 1101, 7318, 11583, 13, 400, 370, 4787, 11, 50804], "temperature": 0.0, "avg_logprob": -0.08199858391421964, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.004603824578225613}, {"id": 907, "seek": 543880, "start": 5447.6, "end": 5455.360000000001, "text": " I expect that pace to accelerate absent a specific effort to slow down. Right. So rather than three", "tokens": [50804, 286, 2066, 300, 11638, 281, 21341, 25185, 257, 2685, 4630, 281, 2964, 760, 13, 1779, 13, 407, 2831, 813, 1045, 51192], "temperature": 0.0, "avg_logprob": -0.08199858391421964, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.004603824578225613}, {"id": 908, "seek": 543880, "start": 5455.360000000001, "end": 5460.08, "text": " times bigger brains each year, it's going to be going faster and faster five times bigger brain", "tokens": [51192, 1413, 3801, 15442, 1184, 1064, 11, 309, 311, 516, 281, 312, 516, 4663, 293, 4663, 1732, 1413, 3801, 3567, 51428], "temperature": 0.0, "avg_logprob": -0.08199858391421964, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.004603824578225613}, {"id": 909, "seek": 543880, "start": 5460.08, "end": 5467.04, "text": " each year, 10 times bigger brain each year. And I think that that just already makes it plausible", "tokens": [51428, 1184, 1064, 11, 1266, 1413, 3801, 3567, 1184, 1064, 13, 400, 286, 519, 300, 300, 445, 1217, 1669, 309, 39925, 51776], "temperature": 0.0, "avg_logprob": -0.08199858391421964, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.004603824578225613}, {"id": 910, "seek": 546704, "start": 5467.04, "end": 5471.76, "text": " that there could be just a small number of years where this transition happens where AIs go from", "tokens": [50364, 300, 456, 727, 312, 445, 257, 1359, 1230, 295, 924, 689, 341, 6034, 2314, 689, 316, 6802, 352, 490, 50600], "temperature": 0.0, "avg_logprob": -0.06979869486211421, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.004409187939018011}, {"id": 911, "seek": 546704, "start": 5471.76, "end": 5478.72, "text": " much worse than humans to much better. But to add in another factor, I think that it's likely that", "tokens": [50600, 709, 5324, 813, 6255, 281, 709, 1101, 13, 583, 281, 909, 294, 1071, 5952, 11, 286, 519, 300, 309, 311, 3700, 300, 50948], "temperature": 0.0, "avg_logprob": -0.06979869486211421, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.004409187939018011}, {"id": 912, "seek": 546704, "start": 5478.72, "end": 5485.04, "text": " AIs are going to be automating AI research itself before they're automating things in most of the", "tokens": [50948, 316, 6802, 366, 516, 281, 312, 3553, 990, 7318, 2132, 2564, 949, 436, 434, 3553, 990, 721, 294, 881, 295, 264, 51264], "temperature": 0.0, "avg_logprob": -0.06979869486211421, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.004409187939018011}, {"id": 913, "seek": 546704, "start": 5485.04, "end": 5492.48, "text": " economy. Right. Because that's the kind of the task in the workflow that AI researchers themselves", "tokens": [51264, 5010, 13, 1779, 13, 1436, 300, 311, 264, 733, 295, 264, 5633, 294, 264, 20993, 300, 7318, 10309, 2969, 51636], "temperature": 0.0, "avg_logprob": -0.06979869486211421, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.004409187939018011}, {"id": 914, "seek": 549248, "start": 5492.48, "end": 5498.4, "text": " really understand. So they would be kind of best placed to use AI as effectively there.", "tokens": [50364, 534, 1223, 13, 407, 436, 576, 312, 733, 295, 1151, 7074, 281, 764, 7318, 382, 8659, 456, 13, 50660], "temperature": 0.0, "avg_logprob": -0.09554092839079083, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.017819171771407127}, {"id": 915, "seek": 549248, "start": 5498.959999999999, "end": 5503.36, "text": " There aren't going to be kind of delays to rolling it out or trouble finding the customers", "tokens": [50688, 821, 3212, 380, 516, 281, 312, 733, 295, 28610, 281, 9439, 309, 484, 420, 5253, 5006, 264, 4581, 50908], "temperature": 0.0, "avg_logprob": -0.09554092839079083, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.017819171771407127}, {"id": 916, "seek": 549248, "start": 5503.919999999999, "end": 5510.959999999999, "text": " for that in the same way. The task of AI research is quite similar to what language models are", "tokens": [50936, 337, 300, 294, 264, 912, 636, 13, 440, 5633, 295, 7318, 2132, 307, 1596, 2531, 281, 437, 2856, 5245, 366, 51288], "temperature": 0.0, "avg_logprob": -0.09554092839079083, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.017819171771407127}, {"id": 917, "seek": 549248, "start": 5510.959999999999, "end": 5515.839999999999, "text": " currently trained to do. They're currently trained to predict the next token on the internet,", "tokens": [51288, 4362, 8895, 281, 360, 13, 814, 434, 4362, 8895, 281, 6069, 264, 958, 14862, 322, 264, 4705, 11, 51532], "temperature": 0.0, "avg_logprob": -0.09554092839079083, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.017819171771407127}, {"id": 918, "seek": 549248, "start": 5515.839999999999, "end": 5520.799999999999, "text": " which means they're particularly well suited to tech space tasks. Right. And the task of writing code", "tokens": [51532, 597, 1355, 436, 434, 4098, 731, 24736, 281, 7553, 1901, 9608, 13, 1779, 13, 400, 264, 5633, 295, 3579, 3089, 51780], "temperature": 0.0, "avg_logprob": -0.09554092839079083, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.017819171771407127}, {"id": 919, "seek": 552080, "start": 5521.4400000000005, "end": 5526.400000000001, "text": " is one such task and there is lots of data on examples of code writing.", "tokens": [50396, 307, 472, 1270, 5633, 293, 456, 307, 3195, 295, 1412, 322, 5110, 295, 3089, 3579, 13, 50644], "temperature": 0.0, "avg_logprob": -0.11660274277385484, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.005706568714231253}, {"id": 920, "seek": 552080, "start": 5526.400000000001, "end": 5531.92, "text": " Oh, I see. So it's like typically, I don't know that much about coding. Is it basically also", "tokens": [50644, 876, 11, 286, 536, 13, 407, 309, 311, 411, 5850, 11, 286, 500, 380, 458, 300, 709, 466, 17720, 13, 1119, 309, 1936, 611, 50920], "temperature": 0.0, "avg_logprob": -0.11660274277385484, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.005706568714231253}, {"id": 921, "seek": 552080, "start": 5531.92, "end": 5539.2, "text": " token prediction? That is how current coding assistants work, I think, is that they're looking", "tokens": [50920, 14862, 17630, 30, 663, 307, 577, 2190, 17720, 34949, 589, 11, 286, 519, 11, 307, 300, 436, 434, 1237, 51284], "temperature": 0.0, "avg_logprob": -0.11660274277385484, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.005706568714231253}, {"id": 922, "seek": 552080, "start": 5539.2, "end": 5543.52, "text": " at your kind of, you start writing your code and they predict what's going to follow. Like one way", "tokens": [51284, 412, 428, 733, 295, 11, 291, 722, 3579, 428, 3089, 293, 436, 6069, 437, 311, 516, 281, 1524, 13, 1743, 472, 636, 51500], "temperature": 0.0, "avg_logprob": -0.11660274277385484, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.005706568714231253}, {"id": 923, "seek": 552080, "start": 5543.52, "end": 5549.2, "text": " of putting it would be by the time that the AIs can do 20% of cognitive tasks in the broader economy,", "tokens": [51500, 295, 3372, 309, 576, 312, 538, 264, 565, 300, 264, 316, 6802, 393, 360, 945, 4, 295, 15605, 9608, 294, 264, 13227, 5010, 11, 51784], "temperature": 0.0, "avg_logprob": -0.11660274277385484, "compression_ratio": 1.6370106761565837, "no_speech_prob": 0.005706568714231253}, {"id": 924, "seek": 554920, "start": 5549.76, "end": 5557.12, "text": " maybe they can already do 40% or 50% of tasks specifically in AI R&D. Right. And so", "tokens": [50392, 1310, 436, 393, 1217, 360, 3356, 4, 420, 2625, 4, 295, 9608, 4682, 294, 7318, 497, 5, 35, 13, 1779, 13, 400, 370, 50760], "temperature": 0.0, "avg_logprob": -0.09348713027106391, "compression_ratio": 1.5531135531135531, "no_speech_prob": 0.0037868302315473557}, {"id": 925, "seek": 554920, "start": 5557.76, "end": 5560.96, "text": " they could have already really started accelerating the pace of progress", "tokens": [50792, 436, 727, 362, 1217, 534, 1409, 34391, 264, 11638, 295, 4205, 50952], "temperature": 0.0, "avg_logprob": -0.09348713027106391, "compression_ratio": 1.5531135531135531, "no_speech_prob": 0.0037868302315473557}, {"id": 926, "seek": 554920, "start": 5561.76, "end": 5568.4, "text": " by the time we get to that 20% economic impact threshold. I mean, at that point,", "tokens": [50992, 538, 264, 565, 321, 483, 281, 300, 945, 4, 4836, 2712, 14678, 13, 286, 914, 11, 412, 300, 935, 11, 51324], "temperature": 0.0, "avg_logprob": -0.09348713027106391, "compression_ratio": 1.5531135531135531, "no_speech_prob": 0.0037868302315473557}, {"id": 927, "seek": 554920, "start": 5568.4, "end": 5573.04, "text": " you could easily imagine really, it's just one year, you give them a 10x bigger brain,", "tokens": [51324, 291, 727, 3612, 3811, 534, 11, 309, 311, 445, 472, 1064, 11, 291, 976, 552, 257, 1266, 87, 3801, 3567, 11, 51556], "temperature": 0.0, "avg_logprob": -0.09348713027106391, "compression_ratio": 1.5531135531135531, "no_speech_prob": 0.0037868302315473557}, {"id": 928, "seek": 554920, "start": 5573.04, "end": 5578.16, "text": " that's like going from chimps to humans and then doing that jump again. That could easily be enough", "tokens": [51556, 300, 311, 411, 516, 490, 18375, 1878, 281, 6255, 293, 550, 884, 300, 3012, 797, 13, 663, 727, 3612, 312, 1547, 51812], "temperature": 0.0, "avg_logprob": -0.09348713027106391, "compression_ratio": 1.5531135531135531, "no_speech_prob": 0.0037868302315473557}, {"id": 929, "seek": 557816, "start": 5578.24, "end": 5585.92, "text": " to go from 20% to 100% just intuitively. And I think that's kind of the default really.", "tokens": [50368, 281, 352, 490, 945, 4, 281, 2319, 4, 445, 46506, 13, 400, 286, 519, 300, 311, 733, 295, 264, 7576, 534, 13, 50752], "temperature": 0.0, "avg_logprob": -0.11895420286390516, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.001374231418594718}, {"id": 930, "seek": 557816, "start": 5586.48, "end": 5590.32, "text": " That's terrifying. Yeah. And I think there's even more pointing that direction.", "tokens": [50780, 663, 311, 18106, 13, 865, 13, 400, 286, 519, 456, 311, 754, 544, 12166, 300, 3513, 13, 50972], "temperature": 0.0, "avg_logprob": -0.11895420286390516, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.001374231418594718}, {"id": 931, "seek": 557816, "start": 5591.04, "end": 5597.12, "text": " I think that already we're seeing that with GPT-4 and other systems like that, people are", "tokens": [51008, 286, 519, 300, 1217, 321, 434, 2577, 300, 365, 26039, 51, 12, 19, 293, 661, 3652, 411, 300, 11, 561, 366, 51312], "temperature": 0.0, "avg_logprob": -0.11895420286390516, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.001374231418594718}, {"id": 932, "seek": 557816, "start": 5597.12, "end": 5602.5599999999995, "text": " becoming much more interested in AI, much more willing to invest in AI. The demand for good AI", "tokens": [51312, 5617, 709, 544, 3102, 294, 7318, 11, 709, 544, 4950, 281, 1963, 294, 7318, 13, 440, 4733, 337, 665, 7318, 51584], "temperature": 0.0, "avg_logprob": -0.11895420286390516, "compression_ratio": 1.5644444444444445, "no_speech_prob": 0.001374231418594718}, {"id": 933, "seek": 560256, "start": 5602.56, "end": 5610.64, "text": " researchers is going up. The wages for good AI researchers are going up. AI research is going", "tokens": [50364, 10309, 307, 516, 493, 13, 440, 20097, 337, 665, 7318, 10309, 366, 516, 493, 13, 7318, 2132, 307, 516, 50768], "temperature": 0.0, "avg_logprob": -0.06965470881689162, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.09042640030384064}, {"id": 934, "seek": 560256, "start": 5610.64, "end": 5617.76, "text": " to be a really financially valuable thing to automate. If you're paying $500,000 a year", "tokens": [50768, 281, 312, 257, 534, 20469, 8263, 551, 281, 31605, 13, 759, 291, 434, 6229, 1848, 7526, 11, 1360, 257, 1064, 51124], "temperature": 0.0, "avg_logprob": -0.06965470881689162, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.09042640030384064}, {"id": 935, "seek": 560256, "start": 5618.320000000001, "end": 5623.92, "text": " to one of your human research engineers, which is lower than what some of these researchers are", "tokens": [51152, 281, 472, 295, 428, 1952, 2132, 11955, 11, 597, 307, 3126, 813, 437, 512, 295, 613, 10309, 366, 51432], "temperature": 0.0, "avg_logprob": -0.06965470881689162, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.09042640030384064}, {"id": 936, "seek": 560256, "start": 5623.92, "end": 5629.120000000001, "text": " earning, then if you can manage to get your AI system to double their productivity,", "tokens": [51432, 12353, 11, 550, 498, 291, 393, 3067, 281, 483, 428, 7318, 1185, 281, 3834, 641, 15604, 11, 51692], "temperature": 0.0, "avg_logprob": -0.06965470881689162, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.09042640030384064}, {"id": 937, "seek": 562912, "start": 5630.08, "end": 5634.88, "text": " that's way better than doubling the productivity of someone who works in a random other industry.", "tokens": [50412, 300, 311, 636, 1101, 813, 33651, 264, 15604, 295, 1580, 567, 1985, 294, 257, 4974, 661, 3518, 13, 50652], "temperature": 0.0, "avg_logprob": -0.0789909542731519, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.001281588221900165}, {"id": 938, "seek": 562912, "start": 5635.44, "end": 5641.2, "text": " Just the straightforward financial incentive as the kind of power of AI becomes apparent", "tokens": [50680, 1449, 264, 15325, 4669, 22346, 382, 264, 733, 295, 1347, 295, 7318, 3643, 18335, 50968], "temperature": 0.0, "avg_logprob": -0.0789909542731519, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.001281588221900165}, {"id": 939, "seek": 562912, "start": 5641.2, "end": 5647.12, "text": " will be towards, let's see if we can automate this really lucrative type of work. So that's just", "tokens": [50968, 486, 312, 3030, 11, 718, 311, 536, 498, 321, 393, 31605, 341, 534, 21296, 30457, 2010, 295, 589, 13, 407, 300, 311, 445, 51264], "temperature": 0.0, "avg_logprob": -0.0789909542731519, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.001281588221900165}, {"id": 940, "seek": 562912, "start": 5647.12, "end": 5652.0, "text": " another reason to think that we get the automation much earlier on the AI side, then on the general", "tokens": [51264, 1071, 1778, 281, 519, 300, 321, 483, 264, 17769, 709, 3071, 322, 264, 7318, 1252, 11, 550, 322, 264, 2674, 51508], "temperature": 0.0, "avg_logprob": -0.0789909542731519, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.001281588221900165}, {"id": 941, "seek": 562912, "start": 5652.0, "end": 5657.599999999999, "text": " economy side, and that by the time we're seeing big economic impacts, AI is already improving", "tokens": [51508, 5010, 1252, 11, 293, 300, 538, 264, 565, 321, 434, 2577, 955, 4836, 11606, 11, 7318, 307, 1217, 11470, 51788], "temperature": 0.0, "avg_logprob": -0.0789909542731519, "compression_ratio": 1.667832167832168, "no_speech_prob": 0.001281588221900165}, {"id": 942, "seek": 565760, "start": 5657.68, "end": 5663.04, "text": " at a blistering pace potentially. Okay, well that's, yeah, again, really scary,", "tokens": [50368, 412, 257, 888, 1964, 278, 11638, 7263, 13, 1033, 11, 731, 300, 311, 11, 1338, 11, 797, 11, 534, 6958, 11, 50636], "temperature": 0.0, "avg_logprob": -0.11972240968184038, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.006284109316766262}, {"id": 943, "seek": 565760, "start": 5663.04, "end": 5669.68, "text": " like really genuinely very scary. I completely agree. Do you have a guess at what percent of", "tokens": [50636, 411, 534, 17839, 588, 6958, 13, 286, 2584, 3986, 13, 1144, 291, 362, 257, 2041, 412, 437, 3043, 295, 50968], "temperature": 0.0, "avg_logprob": -0.11972240968184038, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.006284109316766262}, {"id": 944, "seek": 565760, "start": 5669.68, "end": 5674.56, "text": " cognitive tasks AI can currently perform? It seems like we're really far away from 20 percent.", "tokens": [50968, 15605, 9608, 7318, 393, 4362, 2042, 30, 467, 2544, 411, 321, 434, 534, 1400, 1314, 490, 945, 3043, 13, 51212], "temperature": 0.0, "avg_logprob": -0.11972240968184038, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.006284109316766262}, {"id": 945, "seek": 565760, "start": 5675.68, "end": 5680.400000000001, "text": " Yeah, intuitively, I think it seems like we're far from 20 percent because", "tokens": [51268, 865, 11, 46506, 11, 286, 519, 309, 2544, 411, 321, 434, 1400, 490, 945, 3043, 570, 51504], "temperature": 0.0, "avg_logprob": -0.11972240968184038, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.006284109316766262}, {"id": 946, "seek": 565760, "start": 5680.96, "end": 5686.08, "text": " AIs aren't doing that much in the economy. If I looked at a list of the kind of cognitive tasks", "tokens": [51532, 316, 6802, 3212, 380, 884, 300, 709, 294, 264, 5010, 13, 759, 286, 2956, 412, 257, 1329, 295, 264, 733, 295, 15605, 9608, 51788], "temperature": 0.0, "avg_logprob": -0.11972240968184038, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.006284109316766262}, {"id": 947, "seek": 568608, "start": 5686.08, "end": 5690.48, "text": " people were performing in 2020 and what they were paid for them, it's not as if AIs are ready to", "tokens": [50364, 561, 645, 10205, 294, 4808, 293, 437, 436, 645, 4835, 337, 552, 11, 309, 311, 406, 382, 498, 316, 6802, 366, 1919, 281, 50584], "temperature": 0.0, "avg_logprob": -0.1327418297836461, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0031707866583019495}, {"id": 948, "seek": 568608, "start": 5690.48, "end": 5696.32, "text": " kind of replace a big fraction of that labor. So that's just the 20 percent it's far off.", "tokens": [50584, 733, 295, 7406, 257, 955, 14135, 295, 300, 5938, 13, 407, 300, 311, 445, 264, 945, 3043, 309, 311, 1400, 766, 13, 50876], "temperature": 0.0, "avg_logprob": -0.1327418297836461, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0031707866583019495}, {"id": 949, "seek": 568608, "start": 5697.04, "end": 5701.44, "text": " I'm actually less confident that it's far off than I used to be than if we would have had this", "tokens": [50912, 286, 478, 767, 1570, 6679, 300, 309, 311, 1400, 766, 813, 286, 1143, 281, 312, 813, 498, 321, 576, 362, 632, 341, 51132], "temperature": 0.0, "avg_logprob": -0.1327418297836461, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0031707866583019495}, {"id": 950, "seek": 568608, "start": 5701.44, "end": 5710.8, "text": " interview six months ago, because just seeing GPT4's performance, firstly, just doing really well", "tokens": [51132, 4049, 2309, 2493, 2057, 11, 570, 445, 2577, 26039, 51, 19, 311, 3389, 11, 27376, 11, 445, 884, 534, 731, 51600], "temperature": 0.0, "avg_logprob": -0.1327418297836461, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0031707866583019495}, {"id": 951, "seek": 571080, "start": 5710.8, "end": 5717.68, "text": " on just a whole wide range of university exams and other formal tests without having specifically", "tokens": [50364, 322, 445, 257, 1379, 4874, 3613, 295, 5454, 20514, 293, 661, 9860, 6921, 1553, 1419, 4682, 50708], "temperature": 0.0, "avg_logprob": -0.1226817798614502, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.08008114993572235}, {"id": 952, "seek": 571080, "start": 5717.68, "end": 5721.68, "text": " trained on that. And then me kind of playing around with it and thinking, yep, just seems smarter", "tokens": [50708, 8895, 322, 300, 13, 400, 550, 385, 733, 295, 2433, 926, 365, 309, 293, 1953, 11, 18633, 11, 445, 2544, 20294, 50908], "temperature": 0.0, "avg_logprob": -0.1226817798614502, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.08008114993572235}, {"id": 953, "seek": 571080, "start": 5721.68, "end": 5727.360000000001, "text": " than most people I talk about this stuff with. Most of my start friends wouldn't be this smart.", "tokens": [50908, 813, 881, 561, 286, 751, 466, 341, 1507, 365, 13, 4534, 295, 452, 722, 1855, 2759, 380, 312, 341, 4069, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1226817798614502, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.08008114993572235}, {"id": 954, "seek": 571080, "start": 5729.68, "end": 5735.4400000000005, "text": " I'm thinking maybe actually if you just put some work into specifically applying GPT4,", "tokens": [51308, 286, 478, 1953, 1310, 767, 498, 291, 445, 829, 512, 589, 666, 4682, 9275, 26039, 51, 19, 11, 51596], "temperature": 0.0, "avg_logprob": -0.1226817798614502, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.08008114993572235}, {"id": 955, "seek": 571080, "start": 5735.4400000000005, "end": 5739.12, "text": " you could automate quite a large fraction of the cognitive tasks.", "tokens": [51596, 291, 727, 31605, 1596, 257, 2416, 14135, 295, 264, 15605, 9608, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1226817798614502, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.08008114993572235}, {"id": 956, "seek": 573912, "start": 5739.84, "end": 5743.92, "text": " It does seem it does seem much more plausible to me that maybe you could get to 10 percent today", "tokens": [50400, 467, 775, 1643, 309, 775, 1643, 709, 544, 39925, 281, 385, 300, 1310, 291, 727, 483, 281, 1266, 3043, 965, 50604], "temperature": 0.0, "avg_logprob": -0.10595405648607727, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.0007982785464264452}, {"id": 957, "seek": 573912, "start": 5743.92, "end": 5752.8, "text": " or within a year. Wild. That's super interesting. I mean, I guess, yeah, I was also blown away by", "tokens": [50604, 420, 1951, 257, 1064, 13, 10904, 13, 663, 311, 1687, 1880, 13, 286, 914, 11, 286, 2041, 11, 1338, 11, 286, 390, 611, 16479, 1314, 538, 51048], "temperature": 0.0, "avg_logprob": -0.10595405648607727, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.0007982785464264452}, {"id": 958, "seek": 573912, "start": 5752.8, "end": 5759.2, "text": " GPT4's performance on, yeah, the SAT, the LSAT. For anyone who hasn't looked, we'll stick up a link", "tokens": [51048, 26039, 51, 19, 311, 3389, 322, 11, 1338, 11, 264, 31536, 11, 264, 36657, 2218, 13, 1171, 2878, 567, 6132, 380, 2956, 11, 321, 603, 2897, 493, 257, 2113, 51368], "temperature": 0.0, "avg_logprob": -0.10595405648607727, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.0007982785464264452}, {"id": 959, "seek": 573912, "start": 5759.2, "end": 5765.36, "text": " to those test results. I think it was performing, I mean, much better than I ever did on my AP tests", "tokens": [51368, 281, 729, 1500, 3542, 13, 286, 519, 309, 390, 10205, 11, 286, 914, 11, 709, 1101, 813, 286, 1562, 630, 322, 452, 5372, 6921, 51676], "temperature": 0.0, "avg_logprob": -0.10595405648607727, "compression_ratio": 1.5250965250965252, "no_speech_prob": 0.0007982785464264452}, {"id": 960, "seek": 576536, "start": 5765.36, "end": 5772.96, "text": " in high school and better than I did on the GRE. So it's like, it's beating me. And I think beating", "tokens": [50364, 294, 1090, 1395, 293, 1101, 813, 286, 630, 322, 264, 20830, 13, 407, 309, 311, 411, 11, 309, 311, 13497, 385, 13, 400, 286, 519, 13497, 50744], "temperature": 0.0, "avg_logprob": -0.11430537466909371, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.005378087982535362}, {"id": 961, "seek": 576536, "start": 5772.96, "end": 5779.92, "text": " out loads of other people already. Maybe I'm over or putting too much weight on the fact that like,", "tokens": [50744, 484, 12668, 295, 661, 561, 1217, 13, 2704, 286, 478, 670, 420, 3372, 886, 709, 3364, 322, 264, 1186, 300, 411, 11, 51092], "temperature": 0.0, "avg_logprob": -0.11430537466909371, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.005378087982535362}, {"id": 962, "seek": 576536, "start": 5779.92, "end": 5784.5599999999995, "text": " currently, not that many people I know are using it to do anything. And it sounds like your", "tokens": [51092, 4362, 11, 406, 300, 867, 561, 286, 458, 366, 1228, 309, 281, 360, 1340, 13, 400, 309, 3263, 411, 428, 51324], "temperature": 0.0, "avg_logprob": -0.11430537466909371, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.005378087982535362}, {"id": 963, "seek": 576536, "start": 5784.5599999999995, "end": 5791.679999999999, "text": " impression is like, maybe it's pretty close to being able to do a lot. Yeah, to actually in practice,", "tokens": [51324, 9995, 307, 411, 11, 1310, 309, 311, 1238, 1998, 281, 885, 1075, 281, 360, 257, 688, 13, 865, 11, 281, 767, 294, 3124, 11, 51680], "temperature": 0.0, "avg_logprob": -0.11430537466909371, "compression_ratio": 1.6239669421487604, "no_speech_prob": 0.005378087982535362}, {"id": 964, "seek": 579168, "start": 5792.56, "end": 5799.68, "text": " replace 20% of the tasks that people do. It's actually a pretty tough thing to do. Because,", "tokens": [50408, 7406, 945, 4, 295, 264, 9608, 300, 561, 360, 13, 467, 311, 767, 257, 1238, 4930, 551, 281, 360, 13, 1436, 11, 50764], "temperature": 0.0, "avg_logprob": -0.12043650490897043, "compression_ratio": 1.5078125, "no_speech_prob": 0.02223213203251362}, {"id": 965, "seek": 579168, "start": 5799.68, "end": 5804.64, "text": " you know, for myself, all the different parts of my workflow are very much entangled up together.", "tokens": [50764, 291, 458, 11, 337, 2059, 11, 439, 264, 819, 3166, 295, 452, 20993, 366, 588, 709, 948, 39101, 493, 1214, 13, 51012], "temperature": 0.0, "avg_logprob": -0.12043650490897043, "compression_ratio": 1.5078125, "no_speech_prob": 0.02223213203251362}, {"id": 966, "seek": 579168, "start": 5804.64, "end": 5810.400000000001, "text": " So I can't easily take out a 20% chunk and be like, oh, GP4, do this chunk, because it's all kind", "tokens": [51012, 407, 286, 393, 380, 3612, 747, 484, 257, 945, 4, 16635, 293, 312, 411, 11, 1954, 11, 26039, 19, 11, 360, 341, 16635, 11, 570, 309, 311, 439, 733, 51300], "temperature": 0.0, "avg_logprob": -0.12043650490897043, "compression_ratio": 1.5078125, "no_speech_prob": 0.02223213203251362}, {"id": 967, "seek": 579168, "start": 5810.400000000001, "end": 5816.240000000001, "text": " of mixed up. Sure. And so historically, the way that automation has worked is that we've got a new", "tokens": [51300, 295, 7467, 493, 13, 4894, 13, 400, 370, 16180, 11, 264, 636, 300, 17769, 575, 2732, 307, 300, 321, 600, 658, 257, 777, 51592], "temperature": 0.0, "avg_logprob": -0.12043650490897043, "compression_ratio": 1.5078125, "no_speech_prob": 0.02223213203251362}, {"id": 968, "seek": 581624, "start": 5816.24, "end": 5822.0, "text": " technology and then we've spent decades readjusting our workflows so that we can neatly parcel out,", "tokens": [50364, 2899, 293, 550, 321, 600, 4418, 7878, 1401, 3424, 278, 527, 43461, 370, 300, 321, 393, 36634, 34082, 484, 11, 50652], "temperature": 0.0, "avg_logprob": -0.08308659281049456, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.022194646298885345}, {"id": 969, "seek": 581624, "start": 5822.0, "end": 5825.5199999999995, "text": " you know, 20% of our workflow for this new technology to do. And the technology can be", "tokens": [50652, 291, 458, 11, 945, 4, 295, 527, 20993, 337, 341, 777, 2899, 281, 360, 13, 400, 264, 2899, 393, 312, 50828], "temperature": 0.0, "avg_logprob": -0.08308659281049456, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.022194646298885345}, {"id": 970, "seek": 581624, "start": 5825.5199999999995, "end": 5830.8, "text": " fairly dumb, because we've kind of really neatly parceled out that part of our workflow.", "tokens": [50828, 6457, 10316, 11, 570, 321, 600, 733, 295, 534, 36634, 34082, 292, 484, 300, 644, 295, 527, 20993, 13, 51092], "temperature": 0.0, "avg_logprob": -0.08308659281049456, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.022194646298885345}, {"id": 971, "seek": 581624, "start": 5830.8, "end": 5839.5199999999995, "text": " Yeah, do you have an example? Yeah, so let's say moving over from paper records to computer", "tokens": [51092, 865, 11, 360, 291, 362, 364, 1365, 30, 865, 11, 370, 718, 311, 584, 2684, 670, 490, 3035, 7724, 281, 3820, 51528], "temperature": 0.0, "avg_logprob": -0.08308659281049456, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.022194646298885345}, {"id": 972, "seek": 581624, "start": 5839.5199999999995, "end": 5845.679999999999, "text": " records. So I used to have maybe people used to have to write down lots of paper records and", "tokens": [51528, 7724, 13, 407, 286, 1143, 281, 362, 1310, 561, 1143, 281, 362, 281, 2464, 760, 3195, 295, 3035, 7724, 293, 51836], "temperature": 0.0, "avg_logprob": -0.08308659281049456, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.022194646298885345}, {"id": 973, "seek": 584568, "start": 5845.68, "end": 5850.8, "text": " maintain a filing system for their information. And these days, a lot of that work is done", "tokens": [50364, 6909, 257, 26854, 1185, 337, 641, 1589, 13, 400, 613, 1708, 11, 257, 688, 295, 300, 589, 307, 1096, 50620], "temperature": 0.0, "avg_logprob": -0.0742437584059579, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.0012037897249683738}, {"id": 974, "seek": 584568, "start": 5850.8, "end": 5857.04, "text": " automatically by computers and data storage. But at first, you know, it wasn't that easy to immediately", "tokens": [50620, 6772, 538, 10807, 293, 1412, 6725, 13, 583, 412, 700, 11, 291, 458, 11, 309, 2067, 380, 300, 1858, 281, 4258, 50932], "temperature": 0.0, "avg_logprob": -0.0742437584059579, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.0012037897249683738}, {"id": 975, "seek": 584568, "start": 5857.04, "end": 5862.56, "text": " move over to the computers. And it took decades as people were like, you know, got rid of the paper", "tokens": [50932, 1286, 670, 281, 264, 10807, 13, 400, 309, 1890, 7878, 382, 561, 645, 411, 11, 291, 458, 11, 658, 3973, 295, 264, 3035, 51208], "temperature": 0.0, "avg_logprob": -0.0742437584059579, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.0012037897249683738}, {"id": 976, "seek": 584568, "start": 5862.56, "end": 5870.400000000001, "text": " stuff and got used to kind of teaching the other employees to use the computers and, you know,", "tokens": [51208, 1507, 293, 658, 1143, 281, 733, 295, 4571, 264, 661, 6619, 281, 764, 264, 10807, 293, 11, 291, 458, 11, 51600], "temperature": 0.0, "avg_logprob": -0.0742437584059579, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.0012037897249683738}, {"id": 977, "seek": 584568, "start": 5870.400000000001, "end": 5874.8, "text": " got used to using their customers to fill out online forms rather than filling out the paper", "tokens": [51600, 658, 1143, 281, 1228, 641, 4581, 281, 2836, 484, 2950, 6422, 2831, 813, 10623, 484, 264, 3035, 51820], "temperature": 0.0, "avg_logprob": -0.0742437584059579, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.0012037897249683738}, {"id": 978, "seek": 587480, "start": 5874.8, "end": 5878.96, "text": " forms that they're using before. And all that rearranging of workflows took a long time to", "tokens": [50364, 6422, 300, 436, 434, 1228, 949, 13, 400, 439, 300, 29875, 9741, 295, 43461, 1890, 257, 938, 565, 281, 50572], "temperature": 0.0, "avg_logprob": -0.08698419269762542, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003392911981791258}, {"id": 979, "seek": 587480, "start": 5878.96, "end": 5888.16, "text": " happen. And so one scary possibility is that if AGI is is just 10 years away, then there won't be", "tokens": [50572, 1051, 13, 400, 370, 472, 6958, 7959, 307, 300, 498, 316, 26252, 307, 307, 445, 1266, 924, 1314, 11, 550, 456, 1582, 380, 312, 51032], "temperature": 0.0, "avg_logprob": -0.08698419269762542, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003392911981791258}, {"id": 980, "seek": 587480, "start": 5888.16, "end": 5896.8, "text": " time to do that rearranging of workflows that is necessary to get, say, GPT-4 to actually automate", "tokens": [51032, 565, 281, 360, 300, 29875, 9741, 295, 43461, 300, 307, 4818, 281, 483, 11, 584, 11, 26039, 51, 12, 19, 281, 767, 31605, 51464], "temperature": 0.0, "avg_logprob": -0.08698419269762542, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003392911981791258}, {"id": 981, "seek": 587480, "start": 5896.8, "end": 5904.0, "text": " things in practice. And so the 20% automation ability won't happen through some kind of dumb", "tokens": [51464, 721, 294, 3124, 13, 400, 370, 264, 945, 4, 17769, 3485, 1582, 380, 1051, 807, 512, 733, 295, 10316, 51824], "temperature": 0.0, "avg_logprob": -0.08698419269762542, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.003392911981791258}, {"id": 982, "seek": 590400, "start": 5904.0, "end": 5909.28, "text": " system that that I've kind of parceled out a nice thing to do. It will actually happen with a", "tokens": [50364, 1185, 300, 300, 286, 600, 733, 295, 34082, 292, 484, 257, 1481, 551, 281, 360, 13, 467, 486, 767, 1051, 365, 257, 50628], "temperature": 0.0, "avg_logprob": -0.10095671844482422, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.01725298911333084}, {"id": 983, "seek": 590400, "start": 5909.28, "end": 5914.16, "text": " really smart system that basically understands my whole workflow well enough to be able to do 20%", "tokens": [50628, 534, 4069, 1185, 300, 1936, 15146, 452, 1379, 20993, 731, 1547, 281, 312, 1075, 281, 360, 945, 4, 50872], "temperature": 0.0, "avg_logprob": -0.10095671844482422, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.01725298911333084}, {"id": 984, "seek": 590400, "start": 5914.16, "end": 5917.76, "text": " for me, which means that it could be pretty close to just being able to do all of it.", "tokens": [50872, 337, 385, 11, 597, 1355, 300, 309, 727, 312, 1238, 1998, 281, 445, 885, 1075, 281, 360, 439, 295, 309, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10095671844482422, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.01725298911333084}, {"id": 985, "seek": 590400, "start": 5918.32, "end": 5924.24, "text": " Right, right, right. I mean, I've almost got that impression with GPT-4 and my job. Like,", "tokens": [51080, 1779, 11, 558, 11, 558, 13, 286, 914, 11, 286, 600, 1920, 658, 300, 9995, 365, 26039, 51, 12, 19, 293, 452, 1691, 13, 1743, 11, 51376], "temperature": 0.0, "avg_logprob": -0.10095671844482422, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.01725298911333084}, {"id": 986, "seek": 590400, "start": 5924.24, "end": 5930.48, "text": " we asked it, how can you help us make the 80,000 hours podcast? And it was like, I can help you", "tokens": [51376, 321, 2351, 309, 11, 577, 393, 291, 854, 505, 652, 264, 4688, 11, 1360, 2496, 7367, 30, 400, 309, 390, 411, 11, 286, 393, 854, 291, 51688], "temperature": 0.0, "avg_logprob": -0.10095671844482422, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.01725298911333084}, {"id": 987, "seek": 593048, "start": 5930.48, "end": 5934.959999999999, "text": " come up with guests. I can help you write interview questions for the guest if you tell me what they", "tokens": [50364, 808, 493, 365, 9804, 13, 286, 393, 854, 291, 2464, 4049, 1651, 337, 264, 8341, 498, 291, 980, 385, 437, 436, 50588], "temperature": 0.0, "avg_logprob": -0.08422716608587301, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.05254775658249855}, {"id": 988, "seek": 593048, "start": 5934.959999999999, "end": 5940.719999999999, "text": " worked on. I can help you. I mean, it basically rattled off a list of things. And I was like,", "tokens": [50588, 2732, 322, 13, 286, 393, 854, 291, 13, 286, 914, 11, 309, 1936, 27081, 1493, 766, 257, 1329, 295, 721, 13, 400, 286, 390, 411, 11, 50876], "temperature": 0.0, "avg_logprob": -0.08422716608587301, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.05254775658249855}, {"id": 989, "seek": 593048, "start": 5940.719999999999, "end": 5950.24, "text": " as soon as it has a voice, like, that'll be it. That'll be it for me. And I think I thought it", "tokens": [50876, 382, 2321, 382, 309, 575, 257, 3177, 11, 411, 11, 300, 603, 312, 309, 13, 663, 603, 312, 309, 337, 385, 13, 400, 286, 519, 286, 1194, 309, 51352], "temperature": 0.0, "avg_logprob": -0.08422716608587301, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.05254775658249855}, {"id": 990, "seek": 593048, "start": 5950.24, "end": 5954.959999999999, "text": " would help me with subtasks first. I think I thought maybe it would help me with like generating", "tokens": [51352, 576, 854, 385, 365, 7257, 296, 1694, 700, 13, 286, 519, 286, 1194, 1310, 309, 576, 854, 385, 365, 411, 17746, 51588], "temperature": 0.0, "avg_logprob": -0.08422716608587301, "compression_ratio": 1.7706422018348624, "no_speech_prob": 0.05254775658249855}, {"id": 991, "seek": 595496, "start": 5954.96, "end": 5960.96, "text": " titles and like, I don't know, maybe giving me summaries of people's work so that I could read", "tokens": [50364, 12992, 293, 411, 11, 286, 500, 380, 458, 11, 1310, 2902, 385, 8367, 4889, 295, 561, 311, 589, 370, 300, 286, 727, 1401, 50664], "temperature": 0.0, "avg_logprob": -0.09067253023386002, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.08225634694099426}, {"id": 992, "seek": 595496, "start": 5960.96, "end": 5966.72, "text": " them a bit faster. But I think in fact, it's actually going to be really great at like the start", "tokens": [50664, 552, 257, 857, 4663, 13, 583, 286, 519, 294, 1186, 11, 309, 311, 767, 516, 281, 312, 534, 869, 412, 411, 264, 722, 50952], "temperature": 0.0, "avg_logprob": -0.09067253023386002, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.08225634694099426}, {"id": 993, "seek": 595496, "start": 5966.72, "end": 5973.04, "text": " to finish interview process. Yeah, soon enough that I'll just skip over all of that, which I don't", "tokens": [50952, 281, 2413, 4049, 1399, 13, 865, 11, 2321, 1547, 300, 286, 603, 445, 10023, 670, 439, 295, 300, 11, 597, 286, 500, 380, 51268], "temperature": 0.0, "avg_logprob": -0.09067253023386002, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.08225634694099426}, {"id": 994, "seek": 595496, "start": 5973.04, "end": 5977.84, "text": " know if that's true, but it seems it doesn't seem crazy to me. And so maybe that's just another", "tokens": [51268, 458, 498, 300, 311, 2074, 11, 457, 309, 2544, 309, 1177, 380, 1643, 3219, 281, 385, 13, 400, 370, 1310, 300, 311, 445, 1071, 51508], "temperature": 0.0, "avg_logprob": -0.09067253023386002, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.08225634694099426}, {"id": 995, "seek": 595496, "start": 5977.84, "end": 5982.56, "text": " actual example of what you're talking about. Yeah, yeah. So in that example, by the time it can", "tokens": [51508, 3539, 1365, 295, 437, 291, 434, 1417, 466, 13, 865, 11, 1338, 13, 407, 294, 300, 1365, 11, 538, 264, 565, 309, 393, 51744], "temperature": 0.0, "avg_logprob": -0.09067253023386002, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.08225634694099426}, {"id": 996, "seek": 598256, "start": 5982.56, "end": 5986.160000000001, "text": " automate 20% of the kind of tasks you're doing, it can almost do all of it.", "tokens": [50364, 31605, 945, 4, 295, 264, 733, 295, 9608, 291, 434, 884, 11, 309, 393, 1920, 360, 439, 295, 309, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11135471376598391, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.014051636680960655}, {"id": 997, "seek": 598256, "start": 5986.160000000001, "end": 5991.76, "text": " Right. Yeah, makes sense. Okay, so next, I want to dive into your methodology a bit more deeply", "tokens": [50544, 1779, 13, 865, 11, 1669, 2020, 13, 1033, 11, 370, 958, 11, 286, 528, 281, 9192, 666, 428, 24850, 257, 857, 544, 8760, 50824], "temperature": 0.0, "avg_logprob": -0.11135471376598391, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.014051636680960655}, {"id": 998, "seek": 598256, "start": 5991.76, "end": 5997.6, "text": " for this, yeah, this takeoff speeds prediction. It's such an alarming result that I've, yeah,", "tokens": [50824, 337, 341, 11, 1338, 11, 341, 747, 4506, 16411, 17630, 13, 467, 311, 1270, 364, 44043, 1874, 300, 286, 600, 11, 1338, 11, 51116], "temperature": 0.0, "avg_logprob": -0.11135471376598391, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.014051636680960655}, {"id": 999, "seek": 598256, "start": 5997.6, "end": 6003.120000000001, "text": " this urge to understand what's going on a bit better. Yeah, so that headline result, this", "tokens": [51116, 341, 19029, 281, 1223, 437, 311, 516, 322, 257, 857, 1101, 13, 865, 11, 370, 300, 28380, 1874, 11, 341, 51392], "temperature": 0.0, "avg_logprob": -0.11135471376598391, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.014051636680960655}, {"id": 1000, "seek": 598256, "start": 6003.120000000001, "end": 6008.56, "text": " prediction that AI takeoff might only take a few years, is basically based on an economic model", "tokens": [51392, 17630, 300, 7318, 747, 4506, 1062, 787, 747, 257, 1326, 924, 11, 307, 1936, 2361, 322, 364, 4836, 2316, 51664], "temperature": 0.0, "avg_logprob": -0.11135471376598391, "compression_ratio": 1.628158844765343, "no_speech_prob": 0.014051636680960655}, {"id": 1001, "seek": 600856, "start": 6008.56, "end": 6014.56, "text": " that you made that tries to answer the question of whether you can get human level AI just by", "tokens": [50364, 300, 291, 1027, 300, 9898, 281, 1867, 264, 1168, 295, 1968, 291, 393, 483, 1952, 1496, 7318, 445, 538, 50664], "temperature": 0.0, "avg_logprob": -0.0677354653676351, "compression_ratio": 1.579591836734694, "no_speech_prob": 0.003479008562862873}, {"id": 1002, "seek": 600856, "start": 6014.56, "end": 6019.200000000001, "text": " increasing the amount of compute that we have to train our systems. So in other words, kind of", "tokens": [50664, 5662, 264, 2372, 295, 14722, 300, 321, 362, 281, 3847, 527, 3652, 13, 407, 294, 661, 2283, 11, 733, 295, 50896], "temperature": 0.0, "avg_logprob": -0.0677354653676351, "compression_ratio": 1.579591836734694, "no_speech_prob": 0.003479008562862873}, {"id": 1003, "seek": 600856, "start": 6019.200000000001, "end": 6024.88, "text": " without paradigm shifting algorithmic breakthroughs. Yeah, to start us off, can you actually remind me", "tokens": [50896, 1553, 24709, 17573, 9284, 299, 22397, 82, 13, 865, 11, 281, 722, 505, 766, 11, 393, 291, 767, 4160, 385, 51180], "temperature": 0.0, "avg_logprob": -0.0677354653676351, "compression_ratio": 1.579591836734694, "no_speech_prob": 0.003479008562862873}, {"id": 1004, "seek": 600856, "start": 6024.88, "end": 6035.200000000001, "text": " what compute is? Okay, so compute is a measurement of how, how many calculations you need to do", "tokens": [51180, 437, 14722, 307, 30, 1033, 11, 370, 14722, 307, 257, 13160, 295, 577, 11, 577, 867, 20448, 291, 643, 281, 360, 51696], "temperature": 0.0, "avg_logprob": -0.0677354653676351, "compression_ratio": 1.579591836734694, "no_speech_prob": 0.003479008562862873}, {"id": 1005, "seek": 603520, "start": 6035.28, "end": 6042.08, "text": " or a given computer is doing. So let me give an example. So the most common unit for measuring", "tokens": [50368, 420, 257, 2212, 3820, 307, 884, 13, 407, 718, 385, 976, 364, 1365, 13, 407, 264, 881, 2689, 4985, 337, 13389, 50708], "temperature": 0.0, "avg_logprob": -0.08220621087085242, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.009992094710469246}, {"id": 1006, "seek": 603520, "start": 6042.08, "end": 6049.679999999999, "text": " compute is a flop and a flop is adding together two numbers or multiplying together two numbers", "tokens": [50708, 14722, 307, 257, 25343, 293, 257, 25343, 307, 5127, 1214, 732, 3547, 420, 30955, 1214, 732, 3547, 51088], "temperature": 0.0, "avg_logprob": -0.08220621087085242, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.009992094710469246}, {"id": 1007, "seek": 603520, "start": 6049.679999999999, "end": 6055.2, "text": " or dividing them or subtracting them. Okay, so mathematical operation. Exactly. Currently,", "tokens": [51088, 420, 26764, 552, 420, 16390, 278, 552, 13, 1033, 11, 370, 18894, 6916, 13, 7587, 13, 19964, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08220621087085242, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.009992094710469246}, {"id": 1008, "seek": 603520, "start": 6055.2, "end": 6061.599999999999, "text": " when we develop AI systems, the way we do it is by doing loads and loads of these calculations of", "tokens": [51364, 562, 321, 1499, 7318, 3652, 11, 264, 636, 321, 360, 309, 307, 538, 884, 12668, 293, 12668, 295, 613, 20448, 295, 51684], "temperature": 0.0, "avg_logprob": -0.08220621087085242, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.009992094710469246}, {"id": 1009, "seek": 606160, "start": 6061.6, "end": 6067.76, "text": " adding things together, dividing them, minusing them. And by doing all of these calculations,", "tokens": [50364, 5127, 721, 1214, 11, 26764, 552, 11, 3175, 278, 552, 13, 400, 538, 884, 439, 295, 613, 20448, 11, 50672], "temperature": 0.0, "avg_logprob": -0.09274692142132632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008018428343348205}, {"id": 1010, "seek": 606160, "start": 6067.76, "end": 6073.4400000000005, "text": " that is, that is how the AI decides what, what it's going to do. And it's also how we, how we", "tokens": [50672, 300, 307, 11, 300, 307, 577, 264, 7318, 14898, 437, 11, 437, 309, 311, 516, 281, 360, 13, 400, 309, 311, 611, 577, 321, 11, 577, 321, 50956], "temperature": 0.0, "avg_logprob": -0.09274692142132632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008018428343348205}, {"id": 1011, "seek": 606160, "start": 6073.4400000000005, "end": 6078.0, "text": " train the AI in the first place. Got it. You could, you could analogize these calculations to the kind", "tokens": [50956, 3847, 264, 7318, 294, 264, 700, 1081, 13, 5803, 309, 13, 509, 727, 11, 291, 727, 16660, 1125, 613, 20448, 281, 264, 733, 51184], "temperature": 0.0, "avg_logprob": -0.09274692142132632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008018428343348205}, {"id": 1012, "seek": 606160, "start": 6078.0, "end": 6085.52, "text": " of firing of neurons inside our own brain. Okay, so flops are basically just mathematical", "tokens": [51184, 295, 16045, 295, 22027, 1854, 527, 1065, 3567, 13, 1033, 11, 370, 932, 3370, 366, 1936, 445, 18894, 51560], "temperature": 0.0, "avg_logprob": -0.09274692142132632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0008018428343348205}, {"id": 1013, "seek": 608552, "start": 6085.52, "end": 6092.88, "text": " operations or things like, are both of these things true or something like that? And then compute", "tokens": [50364, 7705, 420, 721, 411, 11, 366, 1293, 295, 613, 721, 2074, 420, 746, 411, 300, 30, 400, 550, 14722, 50732], "temperature": 0.0, "avg_logprob": -0.09230864449833216, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.011499501764774323}, {"id": 1014, "seek": 608552, "start": 6092.88, "end": 6098.4800000000005, "text": " is, sorry, it's something like how, how many of those calculations we can do?", "tokens": [50732, 307, 11, 2597, 11, 309, 311, 746, 411, 577, 11, 577, 867, 295, 729, 20448, 321, 393, 360, 30, 51012], "temperature": 0.0, "avg_logprob": -0.09230864449833216, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.011499501764774323}, {"id": 1015, "seek": 608552, "start": 6099.360000000001, "end": 6105.200000000001, "text": " So one flop is just one of those calculations. Some of the biggest language models today are", "tokens": [51056, 407, 472, 25343, 307, 445, 472, 295, 729, 20448, 13, 2188, 295, 264, 3880, 2856, 5245, 965, 366, 51348], "temperature": 0.0, "avg_logprob": -0.09230864449833216, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.011499501764774323}, {"id": 1016, "seek": 608552, "start": 6105.200000000001, "end": 6114.320000000001, "text": " trained with, I think, 10 to the 24 flop. So that is a million, million, million, million", "tokens": [51348, 8895, 365, 11, 286, 519, 11, 1266, 281, 264, 4022, 25343, 13, 407, 300, 307, 257, 2459, 11, 2459, 11, 2459, 11, 2459, 51804], "temperature": 0.0, "avg_logprob": -0.09230864449833216, "compression_ratio": 1.7722772277227723, "no_speech_prob": 0.011499501764774323}, {"id": 1017, "seek": 611432, "start": 6114.32, "end": 6121.44, "text": " calculations. That's how many calculations you need to do to train some of today's big language", "tokens": [50364, 20448, 13, 663, 311, 577, 867, 20448, 291, 643, 281, 360, 281, 3847, 512, 295, 965, 311, 955, 2856, 50720], "temperature": 0.0, "avg_logprob": -0.08989782722628846, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.0006408118060790002}, {"id": 1018, "seek": 611432, "start": 6121.44, "end": 6127.04, "text": " models. So the amount of compute is another way of saying how many calculations did you need to do it.", "tokens": [50720, 5245, 13, 407, 264, 2372, 295, 14722, 307, 1071, 636, 295, 1566, 577, 867, 20448, 630, 291, 643, 281, 360, 309, 13, 51000], "temperature": 0.0, "avg_logprob": -0.08989782722628846, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.0006408118060790002}, {"id": 1019, "seek": 611432, "start": 6127.04, "end": 6133.28, "text": " Got it. So as compute, compute is the amount of computation you need to do and a flop is the", "tokens": [51000, 5803, 309, 13, 407, 382, 14722, 11, 14722, 307, 264, 2372, 295, 24903, 291, 643, 281, 360, 293, 257, 25343, 307, 264, 51312], "temperature": 0.0, "avg_logprob": -0.08989782722628846, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.0006408118060790002}, {"id": 1020, "seek": 611432, "start": 6133.28, "end": 6139.5199999999995, "text": " kind of unit of how many, yeah, okay, cool. So then you're, you're asking this question about AI", "tokens": [51312, 733, 295, 4985, 295, 577, 867, 11, 1338, 11, 1392, 11, 1627, 13, 407, 550, 291, 434, 11, 291, 434, 3365, 341, 1168, 466, 7318, 51624], "temperature": 0.0, "avg_logprob": -0.08989782722628846, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.0006408118060790002}, {"id": 1021, "seek": 613952, "start": 6139.52, "end": 6148.0, "text": " takeoff speeds. And we're just assuming that we, we increase compute, which is made up of machines,", "tokens": [50364, 747, 4506, 16411, 13, 400, 321, 434, 445, 11926, 300, 321, 11, 321, 3488, 14722, 11, 597, 307, 1027, 493, 295, 8379, 11, 50788], "temperature": 0.0, "avg_logprob": -0.06584411197238499, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.005049806088209152}, {"id": 1022, "seek": 613952, "start": 6148.0, "end": 6154.160000000001, "text": " but also has to do with algorithms as well. So like, is it true that we need less compute if the", "tokens": [50788, 457, 611, 575, 281, 360, 365, 14642, 382, 731, 13, 407, 411, 11, 307, 309, 2074, 300, 321, 643, 1570, 14722, 498, 264, 51096], "temperature": 0.0, "avg_logprob": -0.06584411197238499, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.005049806088209152}, {"id": 1023, "seek": 613952, "start": 6154.160000000001, "end": 6159.6, "text": " algorithm is super efficient, because the algorithm just requires that you do fewer calculations to", "tokens": [51096, 9284, 307, 1687, 7148, 11, 570, 264, 9284, 445, 7029, 300, 291, 360, 13366, 20448, 281, 51368], "temperature": 0.0, "avg_logprob": -0.06584411197238499, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.005049806088209152}, {"id": 1024, "seek": 613952, "start": 6159.6, "end": 6164.96, "text": " get the same result or something? The strict assumption is that if we used the algorithms that", "tokens": [51368, 483, 264, 912, 1874, 420, 746, 30, 440, 10910, 15302, 307, 300, 498, 321, 1143, 264, 14642, 300, 51636], "temperature": 0.0, "avg_logprob": -0.06584411197238499, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.005049806088209152}, {"id": 1025, "seek": 616496, "start": 6164.96, "end": 6172.32, "text": " were available in 2020, then there is some amount of compute such that if God handed a top AI lab,", "tokens": [50364, 645, 2435, 294, 4808, 11, 550, 456, 307, 512, 2372, 295, 14722, 1270, 300, 498, 1265, 16013, 257, 1192, 7318, 2715, 11, 50732], "temperature": 0.0, "avg_logprob": -0.0768328841014575, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0028837891295552254}, {"id": 1026, "seek": 616496, "start": 6172.32, "end": 6177.28, "text": " that amount of compute, and they had a few years to adjust the algorithms to using that much more", "tokens": [50732, 300, 2372, 295, 14722, 11, 293, 436, 632, 257, 1326, 924, 281, 4369, 264, 14642, 281, 1228, 300, 709, 544, 50980], "temperature": 0.0, "avg_logprob": -0.0768328841014575, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0028837891295552254}, {"id": 1027, "seek": 616496, "start": 6177.28, "end": 6185.68, "text": " compute, then they would be able to train AGI using that amount of compute. So then in the model,", "tokens": [50980, 14722, 11, 550, 436, 576, 312, 1075, 281, 3847, 316, 26252, 1228, 300, 2372, 295, 14722, 13, 407, 550, 294, 264, 2316, 11, 51400], "temperature": 0.0, "avg_logprob": -0.0768328841014575, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0028837891295552254}, {"id": 1028, "seek": 616496, "start": 6186.24, "end": 6191.44, "text": " we make an assumption about how much compute would have been required. We actually, we actually", "tokens": [51428, 321, 652, 364, 15302, 466, 577, 709, 14722, 576, 362, 668, 4739, 13, 492, 767, 11, 321, 767, 51688], "temperature": 0.0, "avg_logprob": -0.0768328841014575, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0028837891295552254}, {"id": 1029, "seek": 619144, "start": 6191.44, "end": 6198.879999999999, "text": " put a probability distribution over it. But importantly, algorithmic progress can reduce", "tokens": [50364, 829, 257, 8482, 7316, 670, 309, 13, 583, 8906, 11, 9284, 299, 4205, 393, 5407, 50736], "temperature": 0.0, "avg_logprob": -0.07720027752776644, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.003318449016660452}, {"id": 1030, "seek": 619144, "start": 6198.879999999999, "end": 6208.5599999999995, "text": " that computational cost over time. Got it. So maybe in 2020, you'd have needed 10 to the 30", "tokens": [50736, 300, 28270, 2063, 670, 565, 13, 5803, 309, 13, 407, 1310, 294, 4808, 11, 291, 1116, 362, 2978, 1266, 281, 264, 2217, 51220], "temperature": 0.0, "avg_logprob": -0.07720027752776644, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.003318449016660452}, {"id": 1031, "seek": 619144, "start": 6209.2, "end": 6219.44, "text": " flop to train AGI. But maybe by 2025, your algorithms are 10 times better. And so you only", "tokens": [51252, 25343, 281, 3847, 316, 26252, 13, 583, 1310, 538, 39209, 11, 428, 14642, 366, 1266, 1413, 1101, 13, 400, 370, 291, 787, 51764], "temperature": 0.0, "avg_logprob": -0.07720027752776644, "compression_ratio": 1.4491978609625669, "no_speech_prob": 0.003318449016660452}, {"id": 1032, "seek": 621944, "start": 6219.44, "end": 6227.2, "text": " need 10 to the 29 flop to train AGI. And so the basic dynamic in this framework is that", "tokens": [50364, 643, 1266, 281, 264, 9413, 25343, 281, 3847, 316, 26252, 13, 400, 370, 264, 3875, 8546, 294, 341, 8388, 307, 300, 50752], "temperature": 0.0, "avg_logprob": -0.08252330356174045, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0032118631061166525}, {"id": 1033, "seek": 621944, "start": 6227.919999999999, "end": 6235.04, "text": " in each year, our algorithms improve somewhat. And we decide to use more compute in a training", "tokens": [50788, 294, 1184, 1064, 11, 527, 14642, 3470, 8344, 13, 400, 321, 4536, 281, 764, 544, 14722, 294, 257, 3097, 51144], "temperature": 0.0, "avg_logprob": -0.08252330356174045, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0032118631061166525}, {"id": 1034, "seek": 621944, "start": 6235.04, "end": 6240.5599999999995, "text": " run than we had done in the previous year. Right, because it's profitable, etc. Exactly.", "tokens": [51144, 1190, 813, 321, 632, 1096, 294, 264, 3894, 1064, 13, 1779, 11, 570, 309, 311, 21608, 11, 5183, 13, 7587, 13, 51420], "temperature": 0.0, "avg_logprob": -0.08252330356174045, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0032118631061166525}, {"id": 1035, "seek": 621944, "start": 6241.12, "end": 6244.879999999999, "text": " And those two factors combine together. So let's say we use twice as much compute as the", "tokens": [51448, 400, 729, 732, 6771, 10432, 1214, 13, 407, 718, 311, 584, 321, 764, 6091, 382, 709, 14722, 382, 264, 51636], "temperature": 0.0, "avg_logprob": -0.08252330356174045, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0032118631061166525}, {"id": 1036, "seek": 624488, "start": 6244.88, "end": 6250.56, "text": " previous year, and our algorithms are twice as good. And that means that the effective compute", "tokens": [50364, 3894, 1064, 11, 293, 527, 14642, 366, 6091, 382, 665, 13, 400, 300, 1355, 300, 264, 4942, 14722, 50648], "temperature": 0.0, "avg_logprob": -0.07782802256670865, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.002968269633129239}, {"id": 1037, "seek": 624488, "start": 6251.2, "end": 6257.04, "text": " that we're using is four times bigger. Right. So it's the equivalent as if we hadn't improved", "tokens": [50680, 300, 321, 434, 1228, 307, 1451, 1413, 3801, 13, 1779, 13, 407, 309, 311, 264, 10344, 382, 498, 321, 8782, 380, 9689, 50972], "temperature": 0.0, "avg_logprob": -0.07782802256670865, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.002968269633129239}, {"id": 1038, "seek": 624488, "start": 6257.04, "end": 6261.4400000000005, "text": " our algorithms, and we had just used four times as much compute in the training run.", "tokens": [50972, 527, 14642, 11, 293, 321, 632, 445, 1143, 1451, 1413, 382, 709, 14722, 294, 264, 3097, 1190, 13, 51192], "temperature": 0.0, "avg_logprob": -0.07782802256670865, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.002968269633129239}, {"id": 1039, "seek": 624488, "start": 6262.24, "end": 6270.16, "text": " So you're using effective compute to mean something like you want an AI system to, for example,", "tokens": [51232, 407, 291, 434, 1228, 4942, 14722, 281, 914, 746, 411, 291, 528, 364, 7318, 1185, 281, 11, 337, 1365, 11, 51628], "temperature": 0.0, "avg_logprob": -0.07782802256670865, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.002968269633129239}, {"id": 1040, "seek": 627016, "start": 6270.16, "end": 6276.88, "text": " predict the next word in a sentence. And one way you can increase the kind of", "tokens": [50364, 6069, 264, 958, 1349, 294, 257, 8174, 13, 400, 472, 636, 291, 393, 3488, 264, 733, 295, 50700], "temperature": 0.0, "avg_logprob": -0.11319331663200655, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.025911740958690643}, {"id": 1041, "seek": 627016, "start": 6276.88, "end": 6281.36, "text": " effectiveness of that out of that system is by like giving it more compute so it can do more", "tokens": [50700, 21208, 295, 300, 484, 295, 300, 1185, 307, 538, 411, 2902, 309, 544, 14722, 370, 309, 393, 360, 544, 50924], "temperature": 0.0, "avg_logprob": -0.11319331663200655, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.025911740958690643}, {"id": 1042, "seek": 627016, "start": 6281.36, "end": 6286.8, "text": " calculations. But you'll also have another dynamic where the algorithms are getting better such that", "tokens": [50924, 20448, 13, 583, 291, 603, 611, 362, 1071, 8546, 689, 264, 14642, 366, 1242, 1101, 1270, 300, 51196], "temperature": 0.0, "avg_logprob": -0.11319331663200655, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.025911740958690643}, {"id": 1043, "seek": 627016, "start": 6286.8, "end": 6293.28, "text": " you need less to do the same thing. And so you're doing equivalent processes or you're doing", "tokens": [51196, 291, 643, 1570, 281, 360, 264, 912, 551, 13, 400, 370, 291, 434, 884, 10344, 7555, 420, 291, 434, 884, 51520], "temperature": 0.0, "avg_logprob": -0.11319331663200655, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.025911740958690643}, {"id": 1044, "seek": 629328, "start": 6293.28, "end": 6300.0, "text": " you're getting like equivalent outcomes for less physical compute. That is tricky.", "tokens": [50364, 291, 434, 1242, 411, 10344, 10070, 337, 1570, 4001, 14722, 13, 663, 307, 12414, 13, 50700], "temperature": 0.0, "avg_logprob": -0.14049683739157284, "compression_ratio": 1.5221238938053097, "no_speech_prob": 0.025135120376944542}, {"id": 1045, "seek": 629328, "start": 6301.04, "end": 6306.24, "text": " One way to think about it is, let's say in 2025, we do it, we use a certain amount of compute in a", "tokens": [50752, 1485, 636, 281, 519, 466, 309, 307, 11, 718, 311, 584, 294, 39209, 11, 321, 360, 309, 11, 321, 764, 257, 1629, 2372, 295, 14722, 294, 257, 51012], "temperature": 0.0, "avg_logprob": -0.14049683739157284, "compression_ratio": 1.5221238938053097, "no_speech_prob": 0.025135120376944542}, {"id": 1046, "seek": 629328, "start": 6306.24, "end": 6312.32, "text": " training run with 2025 algorithms. Imagine if we'd have been forced to use the 2020 algorithms.", "tokens": [51012, 3097, 1190, 365, 39209, 14642, 13, 11739, 498, 321, 1116, 362, 668, 7579, 281, 764, 264, 4808, 14642, 13, 51316], "temperature": 0.0, "avg_logprob": -0.14049683739157284, "compression_ratio": 1.5221238938053097, "no_speech_prob": 0.025135120376944542}, {"id": 1047, "seek": 629328, "start": 6312.96, "end": 6316.88, "text": " How much compute would we have needed then to get the same result?", "tokens": [51348, 1012, 709, 14722, 576, 321, 362, 2978, 550, 281, 483, 264, 912, 1874, 30, 51544], "temperature": 0.0, "avg_logprob": -0.14049683739157284, "compression_ratio": 1.5221238938053097, "no_speech_prob": 0.025135120376944542}, {"id": 1048, "seek": 631688, "start": 6316.88, "end": 6323.52, "text": " Right. Got it. That is the amount of effective compute that we actually used in 2025.", "tokens": [50364, 1779, 13, 5803, 309, 13, 663, 307, 264, 2372, 295, 4942, 14722, 300, 321, 767, 1143, 294, 39209, 13, 50696], "temperature": 0.0, "avg_logprob": -0.11190926897656786, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.002547529758885503}, {"id": 1049, "seek": 631688, "start": 6323.52, "end": 6330.64, "text": " Great. That makes sense to me. Okay. So you are, you're thinking about effective compute,", "tokens": [50696, 3769, 13, 663, 1669, 2020, 281, 385, 13, 1033, 13, 407, 291, 366, 11, 291, 434, 1953, 466, 4942, 14722, 11, 51052], "temperature": 0.0, "avg_logprob": -0.11190926897656786, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.002547529758885503}, {"id": 1050, "seek": 631688, "start": 6330.64, "end": 6337.12, "text": " and you're making some guess about how much we'll need to get 100% of cognitive tasks automated.", "tokens": [51052, 293, 291, 434, 1455, 512, 2041, 466, 577, 709, 321, 603, 643, 281, 483, 2319, 4, 295, 15605, 9608, 18473, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11190926897656786, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.002547529758885503}, {"id": 1051, "seek": 631688, "start": 6337.12, "end": 6342.8, "text": " Yeah. How are you making guesses about how much effective compute we'll need to get 100% of", "tokens": [51376, 865, 13, 1012, 366, 291, 1455, 42703, 466, 577, 709, 4942, 14722, 321, 603, 643, 281, 483, 2319, 4, 295, 51660], "temperature": 0.0, "avg_logprob": -0.11190926897656786, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.002547529758885503}, {"id": 1052, "seek": 634280, "start": 6342.8, "end": 6353.04, "text": " cognitive tasks, um, automatable. So in the report itself, I just defer to a different report by a", "tokens": [50364, 15605, 9608, 11, 1105, 11, 28034, 712, 13, 407, 294, 264, 2275, 2564, 11, 286, 445, 25704, 281, 257, 819, 2275, 538, 257, 50876], "temperature": 0.0, "avg_logprob": -0.11588517013861209, "compression_ratio": 1.6680672268907564, "no_speech_prob": 0.0012029862264171243}, {"id": 1053, "seek": 634280, "start": 6353.04, "end": 6358.400000000001, "text": " colleague of mine called the bio anchors report, which asks that exact question. In fact, I don't", "tokens": [50876, 13532, 295, 3892, 1219, 264, 12198, 12723, 830, 2275, 11, 597, 8962, 300, 1900, 1168, 13, 682, 1186, 11, 286, 500, 380, 51144], "temperature": 0.0, "avg_logprob": -0.11588517013861209, "compression_ratio": 1.6680672268907564, "no_speech_prob": 0.0012029862264171243}, {"id": 1054, "seek": 634280, "start": 6358.400000000001, "end": 6363.2, "text": " think you need to be deferring to the bio anchors report that there are, you know, different approaches", "tokens": [51144, 519, 291, 643, 281, 312, 25704, 2937, 281, 264, 12198, 12723, 830, 2275, 300, 456, 366, 11, 291, 458, 11, 819, 11587, 51384], "temperature": 0.0, "avg_logprob": -0.11588517013861209, "compression_ratio": 1.6680672268907564, "no_speech_prob": 0.0012029862264171243}, {"id": 1055, "seek": 634280, "start": 6363.2, "end": 6369.76, "text": " you can take to estimating how much effective compute you might need to train AGI. And you could", "tokens": [51384, 291, 393, 747, 281, 8017, 990, 577, 709, 4942, 14722, 291, 1062, 643, 281, 3847, 316, 26252, 13, 400, 291, 727, 51712], "temperature": 0.0, "avg_logprob": -0.11588517013861209, "compression_ratio": 1.6680672268907564, "no_speech_prob": 0.0012029862264171243}, {"id": 1056, "seek": 636976, "start": 6369.76, "end": 6376.0, "text": " use whatever approach that you like, then bring that into, into my framework and use it to inform", "tokens": [50364, 764, 2035, 3109, 300, 291, 411, 11, 550, 1565, 300, 666, 11, 666, 452, 8388, 293, 764, 309, 281, 1356, 50676], "temperature": 0.0, "avg_logprob": -0.08798773942795475, "compression_ratio": 1.5946969696969697, "no_speech_prob": 0.0012776824878528714}, {"id": 1057, "seek": 636976, "start": 6376.0, "end": 6380.24, "text": " your, your initial guess, um, of the effective training compute for AGI.", "tokens": [50676, 428, 11, 428, 5883, 2041, 11, 1105, 11, 295, 264, 4942, 3097, 14722, 337, 316, 26252, 13, 50888], "temperature": 0.0, "avg_logprob": -0.08798773942795475, "compression_ratio": 1.5946969696969697, "no_speech_prob": 0.0012776824878528714}, {"id": 1058, "seek": 636976, "start": 6380.24, "end": 6385.4400000000005, "text": " Right. And you've got a model that people can play with. So if you're like, I think the bio", "tokens": [50888, 1779, 13, 400, 291, 600, 658, 257, 2316, 300, 561, 393, 862, 365, 13, 407, 498, 291, 434, 411, 11, 286, 519, 264, 12198, 51148], "temperature": 0.0, "avg_logprob": -0.08798773942795475, "compression_ratio": 1.5946969696969697, "no_speech_prob": 0.0012776824878528714}, {"id": 1059, "seek": 636976, "start": 6385.4400000000005, "end": 6392.16, "text": " anchors report is way too optimistic about how much, uh, compute it'll take to get AGI,", "tokens": [51148, 12723, 830, 2275, 307, 636, 886, 19397, 466, 577, 709, 11, 2232, 11, 14722, 309, 603, 747, 281, 483, 316, 26252, 11, 51484], "temperature": 0.0, "avg_logprob": -0.08798773942795475, "compression_ratio": 1.5946969696969697, "no_speech_prob": 0.0012776824878528714}, {"id": 1060, "seek": 636976, "start": 6392.16, "end": 6396.72, "text": " you can 1000 exit, um, and see, and see how that changes the outcomes.", "tokens": [51484, 291, 393, 9714, 11043, 11, 1105, 11, 293, 536, 11, 293, 536, 577, 300, 2962, 264, 10070, 13, 51712], "temperature": 0.0, "avg_logprob": -0.08798773942795475, "compression_ratio": 1.5946969696969697, "no_speech_prob": 0.0012776824878528714}, {"id": 1061, "seek": 639672, "start": 6396.8, "end": 6401.360000000001, "text": " Exactly. Super cool. Okay. So we'll, we'll stick a link up to that model so people can play with", "tokens": [50368, 7587, 13, 4548, 1627, 13, 1033, 13, 407, 321, 603, 11, 321, 603, 2897, 257, 2113, 493, 281, 300, 2316, 370, 561, 393, 862, 365, 50596], "temperature": 0.0, "avg_logprob": -0.11594114303588868, "compression_ratio": 1.4872881355932204, "no_speech_prob": 0.0022154771722853184}, {"id": 1062, "seek": 639672, "start": 6401.360000000001, "end": 6408.400000000001, "text": " it if they want to. Do you mind, um, giving me an intuitive sense of how much compute you and,", "tokens": [50596, 309, 498, 436, 528, 281, 13, 1144, 291, 1575, 11, 1105, 11, 2902, 385, 364, 21769, 2020, 295, 577, 709, 14722, 291, 293, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11594114303588868, "compression_ratio": 1.4872881355932204, "no_speech_prob": 0.0022154771722853184}, {"id": 1063, "seek": 639672, "start": 6408.400000000001, "end": 6411.4400000000005, "text": " and your colleague, um, basically think it'll take to get AGI?", "tokens": [50948, 293, 428, 13532, 11, 1105, 11, 1936, 519, 309, 603, 747, 281, 483, 316, 26252, 30, 51100], "temperature": 0.0, "avg_logprob": -0.11594114303588868, "compression_ratio": 1.4872881355932204, "no_speech_prob": 0.0022154771722853184}, {"id": 1064, "seek": 639672, "start": 6412.240000000001, "end": 6423.280000000001, "text": " So in the current median value I use in, in the report is, is very large indeed. It is 10 to the", "tokens": [51140, 407, 294, 264, 2190, 26779, 2158, 286, 764, 294, 11, 294, 264, 2275, 307, 11, 307, 588, 2416, 6451, 13, 467, 307, 1266, 281, 264, 51692], "temperature": 0.0, "avg_logprob": -0.11594114303588868, "compression_ratio": 1.4872881355932204, "no_speech_prob": 0.0022154771722853184}, {"id": 1065, "seek": 642328, "start": 6424.08, "end": 6432.24, "text": " 36 flop. So that is, if you take the amount of compute that was used to train the biggest", "tokens": [50404, 8652, 25343, 13, 407, 300, 307, 11, 498, 291, 747, 264, 2372, 295, 14722, 300, 390, 1143, 281, 3847, 264, 3880, 50812], "temperature": 0.0, "avg_logprob": -0.10789198991728992, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.005720936227589846}, {"id": 1066, "seek": 642328, "start": 6432.24, "end": 6437.759999999999, "text": " language models that publish their training requirements, and then you use a million times", "tokens": [50812, 2856, 5245, 300, 11374, 641, 3097, 7728, 11, 293, 550, 291, 764, 257, 2459, 1413, 51088], "temperature": 0.0, "avg_logprob": -0.10789198991728992, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.005720936227589846}, {"id": 1067, "seek": 642328, "start": 6437.759999999999, "end": 6442.639999999999, "text": " as much and then a million times as much again, that's how much the assumption is making. So,", "tokens": [51088, 382, 709, 293, 550, 257, 2459, 1413, 382, 709, 797, 11, 300, 311, 577, 709, 264, 15302, 307, 1455, 13, 407, 11, 51332], "temperature": 0.0, "avg_logprob": -0.10789198991728992, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.005720936227589846}, {"id": 1068, "seek": 642328, "start": 6442.639999999999, "end": 6446.8, "text": " so I actually now think that that, that assumption is too high.", "tokens": [51332, 370, 286, 767, 586, 519, 300, 300, 11, 300, 15302, 307, 886, 1090, 13, 51540], "temperature": 0.0, "avg_logprob": -0.10789198991728992, "compression_ratio": 1.6984924623115578, "no_speech_prob": 0.005720936227589846}, {"id": 1069, "seek": 644680, "start": 6446.88, "end": 6452.56, "text": " Is that because of GBT four and how, how impressive it is basically largely?", "tokens": [50368, 1119, 300, 570, 295, 26809, 51, 1451, 293, 577, 11, 577, 8992, 309, 307, 1936, 11611, 30, 50652], "temperature": 0.0, "avg_logprob": -0.1578232901436942, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.015697477385401726}, {"id": 1070, "seek": 644680, "start": 6452.56, "end": 6457.28, "text": " Yeah. GBT four and the kind of the fast pace of recent improvements is quite a lot faster", "tokens": [50652, 865, 13, 26809, 51, 1451, 293, 264, 733, 295, 264, 2370, 11638, 295, 5162, 13797, 307, 1596, 257, 688, 4663, 50888], "temperature": 0.0, "avg_logprob": -0.1578232901436942, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.015697477385401726}, {"id": 1071, "seek": 644680, "start": 6457.28, "end": 6463.04, "text": " than I would have predicted. Um, and so yeah, I would, I would now be using it a lower value", "tokens": [50888, 813, 286, 576, 362, 19147, 13, 3301, 11, 293, 370, 1338, 11, 286, 576, 11, 286, 576, 586, 312, 1228, 309, 257, 3126, 2158, 51176], "temperature": 0.0, "avg_logprob": -0.1578232901436942, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.015697477385401726}, {"id": 1072, "seek": 644680, "start": 6463.04, "end": 6467.2, "text": " for that important, important parameter, uh, which would make take off even faster than", "tokens": [51176, 337, 300, 1021, 11, 1021, 13075, 11, 2232, 11, 597, 576, 652, 747, 766, 754, 4663, 813, 51384], "temperature": 0.0, "avg_logprob": -0.1578232901436942, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.015697477385401726}, {"id": 1073, "seek": 644680, "start": 6467.2, "end": 6473.12, "text": " I'm predicting even faster. Yeah. Geez. Yeah. The kind of the report that I wrote uses,", "tokens": [51384, 286, 478, 32884, 754, 4663, 13, 865, 13, 43836, 13, 865, 13, 440, 733, 295, 264, 2275, 300, 286, 4114, 4960, 11, 51680], "temperature": 0.0, "avg_logprob": -0.1578232901436942, "compression_ratio": 1.7755102040816326, "no_speech_prob": 0.015697477385401726}, {"id": 1074, "seek": 647312, "start": 6473.12, "end": 6478.72, "text": " yeah, this 10 to the 36 as its, as its median estimate for what you'd need to train AGI.", "tokens": [50364, 1338, 11, 341, 1266, 281, 264, 8652, 382, 1080, 11, 382, 1080, 26779, 12539, 337, 437, 291, 1116, 643, 281, 3847, 316, 26252, 13, 50644], "temperature": 0.0, "avg_logprob": -0.11084749221801758, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0008809181745164096}, {"id": 1075, "seek": 647312, "start": 6478.72, "end": 6485.5199999999995, "text": " Okay. Cool. That's helpful. Okay. So, so that's how you basically estimate how much", "tokens": [50644, 1033, 13, 8561, 13, 663, 311, 4961, 13, 1033, 13, 407, 11, 370, 300, 311, 577, 291, 1936, 12539, 577, 709, 50984], "temperature": 0.0, "avg_logprob": -0.11084749221801758, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0008809181745164096}, {"id": 1076, "seek": 647312, "start": 6486.4, "end": 6492.88, "text": " effective compute you need to train AGI. How do you use that to predict, uh, yeah, AI takeoff speeds?", "tokens": [51028, 4942, 14722, 291, 643, 281, 3847, 316, 26252, 13, 1012, 360, 291, 764, 300, 281, 6069, 11, 2232, 11, 1338, 11, 7318, 747, 4506, 16411, 30, 51352], "temperature": 0.0, "avg_logprob": -0.11084749221801758, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0008809181745164096}, {"id": 1077, "seek": 647312, "start": 6494.08, "end": 6500.0, "text": " Right. So we have this, um, assumption about the effective training compute for AGI,", "tokens": [51412, 1779, 13, 407, 321, 362, 341, 11, 1105, 11, 15302, 466, 264, 4942, 3097, 14722, 337, 316, 26252, 11, 51708], "temperature": 0.0, "avg_logprob": -0.11084749221801758, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0008809181745164096}, {"id": 1078, "seek": 650000, "start": 6500.0, "end": 6505.2, "text": " which was our hundred percent kind of endpoint. We then need to make an additional assumption", "tokens": [50364, 597, 390, 527, 3262, 3043, 733, 295, 35795, 13, 492, 550, 643, 281, 652, 364, 4497, 15302, 50624], "temperature": 0.0, "avg_logprob": -0.10428487989637587, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0023119505494832993}, {"id": 1079, "seek": 650000, "start": 6505.2, "end": 6511.92, "text": " about what would be the effective compute needed to train AI that could automate 20% of tasks.", "tokens": [50624, 466, 437, 576, 312, 264, 4942, 14722, 2978, 281, 3847, 7318, 300, 727, 31605, 945, 4, 295, 9608, 13, 50960], "temperature": 0.0, "avg_logprob": -0.10428487989637587, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0023119505494832993}, {"id": 1080, "seek": 650000, "start": 6512.72, "end": 6522.0, "text": " So let's say that we assumed, for example, that you need 10 to the 30 flop to train AGI using", "tokens": [51000, 407, 718, 311, 584, 300, 321, 15895, 11, 337, 1365, 11, 300, 291, 643, 1266, 281, 264, 2217, 25343, 281, 3847, 316, 26252, 1228, 51464], "temperature": 0.0, "avg_logprob": -0.10428487989637587, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0023119505494832993}, {"id": 1081, "seek": 650000, "start": 6522.0, "end": 6528.32, "text": " 2020 algorithms, that'd be 10 to the 30 kind of effective compute. We then make an additional", "tokens": [51464, 4808, 14642, 11, 300, 1116, 312, 1266, 281, 264, 2217, 733, 295, 4942, 14722, 13, 492, 550, 652, 364, 4497, 51780], "temperature": 0.0, "avg_logprob": -0.10428487989637587, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0023119505494832993}, {"id": 1082, "seek": 652832, "start": 6528.32, "end": 6534.16, "text": " assumption about how much less effective compute you need to train AI that could automate just", "tokens": [50364, 15302, 466, 577, 709, 1570, 4942, 14722, 291, 643, 281, 3847, 7318, 300, 727, 31605, 445, 50656], "temperature": 0.0, "avg_logprob": -0.08587856507033445, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0004574446938931942}, {"id": 1083, "seek": 652832, "start": 6534.16, "end": 6540.4, "text": " 20% of tasks. So I kind of want to pause on that last assumption because it's so important.", "tokens": [50656, 945, 4, 295, 9608, 13, 407, 286, 733, 295, 528, 281, 10465, 322, 300, 1036, 15302, 570, 309, 311, 370, 1021, 13, 50968], "temperature": 0.0, "avg_logprob": -0.08587856507033445, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0004574446938931942}, {"id": 1084, "seek": 652832, "start": 6540.4, "end": 6546.0, "text": " So that, that, that assumption about how many more times compute you need for AGI compared to", "tokens": [50968, 407, 300, 11, 300, 11, 300, 15302, 466, 577, 867, 544, 1413, 14722, 291, 643, 337, 316, 26252, 5347, 281, 51248], "temperature": 0.0, "avg_logprob": -0.08587856507033445, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0004574446938931942}, {"id": 1085, "seek": 652832, "start": 6546.0, "end": 6552.719999999999, "text": " 20% AI, that assumption is what I'm calling the difficulty gap because it's saying what is the", "tokens": [51248, 945, 4, 7318, 11, 300, 15302, 307, 437, 286, 478, 5141, 264, 10360, 7417, 570, 309, 311, 1566, 437, 307, 264, 51584], "temperature": 0.0, "avg_logprob": -0.08587856507033445, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.0004574446938931942}, {"id": 1086, "seek": 655272, "start": 6552.72, "end": 6561.76, "text": " gap in difficulty between training 20% AI and training 100% AI or AGI. And then we're measuring", "tokens": [50364, 7417, 294, 10360, 1296, 3097, 945, 4, 7318, 293, 3097, 2319, 4, 7318, 420, 316, 26252, 13, 400, 550, 321, 434, 13389, 50816], "temperature": 0.0, "avg_logprob": -0.07390538577375741, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0008025355637073517}, {"id": 1087, "seek": 655272, "start": 6561.76, "end": 6567.76, "text": " the size of that difficulty gap in terms of how many times more effective compute you need to train", "tokens": [50816, 264, 2744, 295, 300, 10360, 7417, 294, 2115, 295, 577, 867, 1413, 544, 4942, 14722, 291, 643, 281, 3847, 51116], "temperature": 0.0, "avg_logprob": -0.07390538577375741, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0008025355637073517}, {"id": 1088, "seek": 655272, "start": 6567.76, "end": 6573.68, "text": " one than the other. And you're calling it the difficulty gap because it's kind of describing", "tokens": [51116, 472, 813, 264, 661, 13, 400, 291, 434, 5141, 309, 264, 10360, 7417, 570, 309, 311, 733, 295, 16141, 51412], "temperature": 0.0, "avg_logprob": -0.07390538577375741, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0008025355637073517}, {"id": 1089, "seek": 655272, "start": 6573.68, "end": 6580.240000000001, "text": " how much more difficult the most difficult tasks are relative to the, the easiest 20%.", "tokens": [51412, 577, 709, 544, 2252, 264, 881, 2252, 9608, 366, 4972, 281, 264, 11, 264, 12889, 945, 6856, 51740], "temperature": 0.0, "avg_logprob": -0.07390538577375741, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.0008025355637073517}, {"id": 1090, "seek": 658024, "start": 6581.2, "end": 6585.76, "text": " The reason I call it the difficulty gap is, is to refer to the difficulty of developing the AI", "tokens": [50412, 440, 1778, 286, 818, 309, 264, 10360, 7417, 307, 11, 307, 281, 2864, 281, 264, 10360, 295, 6416, 264, 7318, 50640], "temperature": 0.0, "avg_logprob": -0.08288872357711051, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0011839444050565362}, {"id": 1091, "seek": 658024, "start": 6585.76, "end": 6592.719999999999, "text": " in the first place. So it's kind of like how much more difficult is it to develop an AI that can", "tokens": [50640, 294, 264, 700, 1081, 13, 407, 309, 311, 733, 295, 411, 577, 709, 544, 2252, 307, 309, 281, 1499, 364, 7318, 300, 393, 50988], "temperature": 0.0, "avg_logprob": -0.08288872357711051, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0011839444050565362}, {"id": 1092, "seek": 658024, "start": 6592.719999999999, "end": 6600.639999999999, "text": " do 100% of the tasks than it is to develop an AI that can only do 20%. Got it. But it might be 10,000", "tokens": [50988, 360, 2319, 4, 295, 264, 9608, 813, 309, 307, 281, 1499, 364, 7318, 300, 393, 787, 360, 945, 6856, 5803, 309, 13, 583, 309, 1062, 312, 1266, 11, 1360, 51384], "temperature": 0.0, "avg_logprob": -0.08288872357711051, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0011839444050565362}, {"id": 1093, "seek": 658024, "start": 6600.639999999999, "end": 6606.16, "text": " times as difficult, or it might be barely more difficult at all. If, if it turns out that", "tokens": [51384, 1413, 382, 2252, 11, 420, 309, 1062, 312, 10268, 544, 2252, 412, 439, 13, 759, 11, 498, 309, 4523, 484, 300, 51660], "temperature": 0.0, "avg_logprob": -0.08288872357711051, "compression_ratio": 1.740909090909091, "no_speech_prob": 0.0011839444050565362}, {"id": 1094, "seek": 660616, "start": 6606.88, "end": 6612.32, "text": " once you're 20% there, you're basically the whole way there. Right. I guess is that possible?", "tokens": [50400, 1564, 291, 434, 945, 4, 456, 11, 291, 434, 1936, 264, 1379, 636, 456, 13, 1779, 13, 286, 2041, 307, 300, 1944, 30, 50672], "temperature": 0.0, "avg_logprob": -0.08994666349540636, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.008301475085318089}, {"id": 1095, "seek": 660616, "start": 6612.32, "end": 6620.32, "text": " Maybe it is if like the first 20% of tasks includes AI R&D. So that's an interesting", "tokens": [50672, 2704, 309, 307, 498, 411, 264, 700, 945, 4, 295, 9608, 5974, 7318, 497, 5, 35, 13, 407, 300, 311, 364, 1880, 51072], "temperature": 0.0, "avg_logprob": -0.08994666349540636, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.008301475085318089}, {"id": 1096, "seek": 660616, "start": 6620.32, "end": 6626.72, "text": " scenario to think about. You could have the first 20% of tasks, including all of the tasks of AI R&D.", "tokens": [51072, 9005, 281, 519, 466, 13, 509, 727, 362, 264, 700, 945, 4, 295, 9608, 11, 3009, 439, 295, 264, 9608, 295, 7318, 497, 5, 35, 13, 51392], "temperature": 0.0, "avg_logprob": -0.08994666349540636, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.008301475085318089}, {"id": 1097, "seek": 660616, "start": 6626.72, "end": 6631.5199999999995, "text": " So what I think would happen in that scenario is that once you've done those first 20% of tasks,", "tokens": [51392, 407, 437, 286, 519, 576, 1051, 294, 300, 9005, 307, 300, 1564, 291, 600, 1096, 729, 700, 945, 4, 295, 9608, 11, 51632], "temperature": 0.0, "avg_logprob": -0.08994666349540636, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.008301475085318089}, {"id": 1098, "seek": 663152, "start": 6632.080000000001, "end": 6637.92, "text": " AI would be improving super, super quickly, absent a specific effort to slow down, you know,", "tokens": [50392, 7318, 576, 312, 11470, 1687, 11, 1687, 2661, 11, 25185, 257, 2685, 4630, 281, 2964, 760, 11, 291, 458, 11, 50684], "temperature": 0.0, "avg_logprob": -0.09363350026747759, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.02635084092617035}, {"id": 1099, "seek": 663152, "start": 6637.92, "end": 6644.56, "text": " within I think a few months, you would already be able to do a training run that used 100 times", "tokens": [50684, 1951, 286, 519, 257, 1326, 2493, 11, 291, 576, 1217, 312, 1075, 281, 360, 257, 3097, 1190, 300, 1143, 2319, 1413, 51016], "temperature": 0.0, "avg_logprob": -0.09363350026747759, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.02635084092617035}, {"id": 1100, "seek": 663152, "start": 6644.56, "end": 6649.68, "text": " more effective training compute as you had previously done. And that's because", "tokens": [51016, 544, 4942, 3097, 14722, 382, 291, 632, 8046, 1096, 13, 400, 300, 311, 570, 51272], "temperature": 0.0, "avg_logprob": -0.09363350026747759, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.02635084092617035}, {"id": 1101, "seek": 663152, "start": 6650.56, "end": 6656.64, "text": " that's because you would have hundreds of millions of AI's that could be working to improve the AI", "tokens": [51316, 300, 311, 570, 291, 576, 362, 6779, 295, 6803, 295, 7318, 311, 300, 727, 312, 1364, 281, 3470, 264, 7318, 51620], "temperature": 0.0, "avg_logprob": -0.09363350026747759, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.02635084092617035}, {"id": 1102, "seek": 665664, "start": 6656.64, "end": 6665.360000000001, "text": " algorithms and maybe making money so you can buy more AI chips or convincing other people to kind", "tokens": [50364, 14642, 293, 1310, 1455, 1460, 370, 291, 393, 2256, 544, 7318, 11583, 420, 24823, 661, 561, 281, 733, 50800], "temperature": 0.0, "avg_logprob": -0.10972636233093919, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.0025949955452233553}, {"id": 1103, "seek": 665664, "start": 6665.360000000001, "end": 6672.0, "text": " of share their compute with you. And then that would be enough to very quickly allow you to use 100", "tokens": [50800, 295, 2073, 641, 14722, 365, 291, 13, 400, 550, 300, 576, 312, 1547, 281, 588, 2661, 2089, 291, 281, 764, 2319, 51132], "temperature": 0.0, "avg_logprob": -0.10972636233093919, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.0025949955452233553}, {"id": 1104, "seek": 665664, "start": 6672.0, "end": 6678.8, "text": " times as much effective compute. Cool. Okay. Yeah. So I guess maybe you think that which tasks end up", "tokens": [51132, 1413, 382, 709, 4942, 14722, 13, 8561, 13, 1033, 13, 865, 13, 407, 286, 2041, 1310, 291, 519, 300, 597, 9608, 917, 493, 51472], "temperature": 0.0, "avg_logprob": -0.10972636233093919, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.0025949955452233553}, {"id": 1105, "seek": 665664, "start": 6678.8, "end": 6685.360000000001, "text": " being easier also plays into how fast AI takeoff speeds are. Yeah. I guess in particular in the", "tokens": [51472, 885, 3571, 611, 5749, 666, 577, 2370, 7318, 747, 4506, 16411, 366, 13, 865, 13, 286, 2041, 294, 1729, 294, 264, 51800], "temperature": 0.0, "avg_logprob": -0.10972636233093919, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.0025949955452233553}, {"id": 1106, "seek": 668536, "start": 6685.36, "end": 6691.04, "text": " case where AI R&D is in the first 20% and otherwise maybe it doesn't matter as much. Exactly.", "tokens": [50364, 1389, 689, 7318, 497, 5, 35, 307, 294, 264, 700, 945, 4, 293, 5911, 1310, 309, 1177, 380, 1871, 382, 709, 13, 7587, 13, 50648], "temperature": 0.0, "avg_logprob": -0.09112442954111907, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0015451002400368452}, {"id": 1107, "seek": 668536, "start": 6691.839999999999, "end": 6698.16, "text": " I think that with that last example, even if there was a big difficulty gap from 20% to 100%", "tokens": [50688, 286, 519, 300, 365, 300, 1036, 1365, 11, 754, 498, 456, 390, 257, 955, 10360, 7417, 490, 945, 4, 281, 2319, 4, 51004], "temperature": 0.0, "avg_logprob": -0.09112442954111907, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0015451002400368452}, {"id": 1108, "seek": 668536, "start": 6699.2, "end": 6705.92, "text": " of cognitive tasks in the economy, if you get all the R&D tasks within that first 20%,", "tokens": [51056, 295, 15605, 9608, 294, 264, 5010, 11, 498, 291, 483, 439, 264, 497, 5, 35, 9608, 1951, 300, 700, 945, 8923, 51392], "temperature": 0.0, "avg_logprob": -0.09112442954111907, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0015451002400368452}, {"id": 1109, "seek": 668536, "start": 6705.92, "end": 6710.08, "text": " then I still think you'd get a very quick transition. Right. Okay. So that could be an example", "tokens": [51392, 550, 286, 920, 519, 291, 1116, 483, 257, 588, 1702, 6034, 13, 1779, 13, 1033, 13, 407, 300, 727, 312, 364, 1365, 51600], "temperature": 0.0, "avg_logprob": -0.09112442954111907, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0015451002400368452}, {"id": 1110, "seek": 668536, "start": 6710.08, "end": 6715.04, "text": " with a big difficulty gap where you nonetheless you still get a very fast AI takeoff.", "tokens": [51600, 365, 257, 955, 10360, 7417, 689, 291, 26756, 291, 920, 483, 257, 588, 2370, 7318, 747, 4506, 13, 51848], "temperature": 0.0, "avg_logprob": -0.09112442954111907, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.0015451002400368452}, {"id": 1111, "seek": 671536, "start": 6715.36, "end": 6722.32, "text": " Yep. That makes sense. Okay. So do you basically just make an assumption about how big that difficulty", "tokens": [50364, 7010, 13, 663, 1669, 2020, 13, 1033, 13, 407, 360, 291, 1936, 445, 652, 364, 15302, 466, 577, 955, 300, 10360, 50712], "temperature": 0.0, "avg_logprob": -0.11136814788147643, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0004252005892340094}, {"id": 1112, "seek": 671536, "start": 6722.32, "end": 6729.12, "text": " gap is? Is it a range? And how did you come up with whatever numbers you're putting on to?", "tokens": [50712, 7417, 307, 30, 1119, 309, 257, 3613, 30, 400, 577, 630, 291, 808, 493, 365, 2035, 3547, 291, 434, 3372, 322, 281, 30, 51052], "temperature": 0.0, "avg_logprob": -0.11136814788147643, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0004252005892340094}, {"id": 1113, "seek": 671536, "start": 6729.12, "end": 6732.24, "text": " Yeah. How many times harder it is to get to 100% of tasks?", "tokens": [51052, 865, 13, 1012, 867, 1413, 6081, 309, 307, 281, 483, 281, 2319, 4, 295, 9608, 30, 51208], "temperature": 0.0, "avg_logprob": -0.11136814788147643, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0004252005892340094}, {"id": 1114, "seek": 671536, "start": 6733.44, "end": 6739.36, "text": " So I do consider as much evidence I can for the difficulty gap. It is really, really important.", "tokens": [51268, 407, 286, 360, 1949, 382, 709, 4467, 286, 393, 337, 264, 10360, 7417, 13, 467, 307, 534, 11, 534, 1021, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11136814788147643, "compression_ratio": 1.4745762711864407, "no_speech_prob": 0.0004252005892340094}, {"id": 1115, "seek": 673936, "start": 6740.08, "end": 6746.4, "text": " The lines of evidence that I consider are all pretty limited. So it's a very uncertain parameter,", "tokens": [50400, 440, 3876, 295, 4467, 300, 286, 1949, 366, 439, 1238, 5567, 13, 407, 309, 311, 257, 588, 11308, 13075, 11, 50716], "temperature": 0.0, "avg_logprob": -0.07926883119525331, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.003855898743495345}, {"id": 1116, "seek": 673936, "start": 6746.4, "end": 6750.32, "text": " but I think there are some things you can learn from some of those lines of evidence.", "tokens": [50716, 457, 286, 519, 456, 366, 512, 721, 291, 393, 1466, 490, 512, 295, 729, 3876, 295, 4467, 13, 50912], "temperature": 0.0, "avg_logprob": -0.07926883119525331, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.003855898743495345}, {"id": 1117, "seek": 673936, "start": 6750.32, "end": 6754.96, "text": " Okay. What's an example of some of the evidence you would have looked into?", "tokens": [50912, 1033, 13, 708, 311, 364, 1365, 295, 512, 295, 264, 4467, 291, 576, 362, 2956, 666, 30, 51144], "temperature": 0.0, "avg_logprob": -0.07926883119525331, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.003855898743495345}, {"id": 1118, "seek": 673936, "start": 6755.839999999999, "end": 6759.679999999999, "text": " In terms of evidence for the difficulty gap potentially being pretty small,", "tokens": [51188, 682, 2115, 295, 4467, 337, 264, 10360, 7417, 7263, 885, 1238, 1359, 11, 51380], "temperature": 0.0, "avg_logprob": -0.07926883119525331, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.003855898743495345}, {"id": 1119, "seek": 673936, "start": 6759.679999999999, "end": 6764.799999999999, "text": " we've already touched a little bit upon some of that. So one line of evidence is the", "tokens": [51380, 321, 600, 1217, 9828, 257, 707, 857, 3564, 512, 295, 300, 13, 407, 472, 1622, 295, 4467, 307, 264, 51636], "temperature": 0.0, "avg_logprob": -0.07926883119525331, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.003855898743495345}, {"id": 1120, "seek": 676480, "start": 6765.76, "end": 6772.96, "text": " scaling of human cognitive ability with human brain size. Some humans have slightly", "tokens": [50412, 21589, 295, 1952, 15605, 3485, 365, 1952, 3567, 2744, 13, 2188, 6255, 362, 4748, 50772], "temperature": 0.0, "avg_logprob": -0.09974157692182183, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00038556658546440303}, {"id": 1121, "seek": 676480, "start": 6772.96, "end": 6778.24, "text": " bigger brains than others. Not only a small variation, plus or minus 10% or so,", "tokens": [50772, 3801, 15442, 813, 2357, 13, 1726, 787, 257, 1359, 12990, 11, 1804, 420, 3175, 1266, 4, 420, 370, 11, 51036], "temperature": 0.0, "avg_logprob": -0.09974157692182183, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00038556658546440303}, {"id": 1122, "seek": 676480, "start": 6778.24, "end": 6783.84, "text": " but you can then look at, okay, if one person has a 10% bigger brain, then on average,", "tokens": [51036, 457, 291, 393, 550, 574, 412, 11, 1392, 11, 498, 472, 954, 575, 257, 1266, 4, 3801, 3567, 11, 550, 322, 4274, 11, 51316], "temperature": 0.0, "avg_logprob": -0.09974157692182183, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00038556658546440303}, {"id": 1123, "seek": 676480, "start": 6783.84, "end": 6787.360000000001, "text": " how much better do they do on various tests of cognitive ability?", "tokens": [51316, 577, 709, 1101, 360, 436, 360, 322, 3683, 6921, 295, 15605, 3485, 30, 51492], "temperature": 0.0, "avg_logprob": -0.09974157692182183, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00038556658546440303}, {"id": 1124, "seek": 676480, "start": 6788.16, "end": 6793.68, "text": " And the difference isn't massive, but if you extrapolate that difference to say, okay, what", "tokens": [51532, 400, 264, 2649, 1943, 380, 5994, 11, 457, 498, 291, 48224, 473, 300, 2649, 281, 584, 11, 1392, 11, 437, 51808], "temperature": 0.0, "avg_logprob": -0.09974157692182183, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.00038556658546440303}, {"id": 1125, "seek": 679368, "start": 6793.68, "end": 6800.4800000000005, "text": " about if it was a three times bigger brain or a 10 times bigger brain, then extrapolating that", "tokens": [50364, 466, 498, 309, 390, 257, 1045, 1413, 3801, 3567, 420, 257, 1266, 1413, 3801, 3567, 11, 550, 48224, 990, 300, 50704], "temperature": 0.0, "avg_logprob": -0.06843151697298376, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00019028899259865284}, {"id": 1126, "seek": 679368, "start": 6800.4800000000005, "end": 6806.88, "text": " suggests that there would be a very large difference in cognitive abilities from getting", "tokens": [50704, 13409, 300, 456, 576, 312, 257, 588, 2416, 2649, 294, 15605, 11582, 490, 1242, 51024], "temperature": 0.0, "avg_logprob": -0.06843151697298376, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00019028899259865284}, {"id": 1127, "seek": 679368, "start": 6806.88, "end": 6812.16, "text": " a brain that is that much bigger. Interesting. The takeaway from that is that this particular", "tokens": [51024, 257, 3567, 300, 307, 300, 709, 3801, 13, 14711, 13, 440, 30681, 490, 300, 307, 300, 341, 1729, 51288], "temperature": 0.0, "avg_logprob": -0.06843151697298376, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00019028899259865284}, {"id": 1128, "seek": 679368, "start": 6812.16, "end": 6819.360000000001, "text": " line of evidence suggests that increasing the size of the brain by a factor of 10 could be more than", "tokens": [51288, 1622, 295, 4467, 13409, 300, 5662, 264, 2744, 295, 264, 3567, 538, 257, 5952, 295, 1266, 727, 312, 544, 813, 51648], "temperature": 0.0, "avg_logprob": -0.06843151697298376, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00019028899259865284}, {"id": 1129, "seek": 681936, "start": 6819.36, "end": 6827.599999999999, "text": " enough to cross this difficulty gap. And that by analogy, increasing the number of parameters", "tokens": [50364, 1547, 281, 3278, 341, 10360, 7417, 13, 400, 300, 538, 21663, 11, 5662, 264, 1230, 295, 9834, 50776], "temperature": 0.0, "avg_logprob": -0.08421957643726204, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.007601873017847538}, {"id": 1130, "seek": 681936, "start": 6827.599999999999, "end": 6833.2, "text": " in an AMI model by a factor of 10 could be more than enough to cross the difficulty gap,", "tokens": [50776, 294, 364, 6475, 40, 2316, 538, 257, 5952, 295, 1266, 727, 312, 544, 813, 1547, 281, 3278, 264, 10360, 7417, 11, 51056], "temperature": 0.0, "avg_logprob": -0.08421957643726204, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.007601873017847538}, {"id": 1131, "seek": 681936, "start": 6834.16, "end": 6840.16, "text": " which would require you to increase the effective training compute by a factor of 100.", "tokens": [51104, 597, 576, 3651, 291, 281, 3488, 264, 4942, 3097, 14722, 538, 257, 5952, 295, 2319, 13, 51404], "temperature": 0.0, "avg_logprob": -0.08421957643726204, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.007601873017847538}, {"id": 1132, "seek": 681936, "start": 6840.719999999999, "end": 6845.759999999999, "text": " Okay. And I think even this analogy actually suggests that just increasing the effective", "tokens": [51432, 1033, 13, 400, 286, 519, 754, 341, 21663, 767, 13409, 300, 445, 5662, 264, 4942, 51684], "temperature": 0.0, "avg_logprob": -0.08421957643726204, "compression_ratio": 1.7378640776699028, "no_speech_prob": 0.007601873017847538}, {"id": 1133, "seek": 684576, "start": 6845.76, "end": 6851.360000000001, "text": " training compute by a factor of 10 might be enough as well, because it could just be enough to increase", "tokens": [50364, 3097, 14722, 538, 257, 5952, 295, 1266, 1062, 312, 1547, 382, 731, 11, 570, 309, 727, 445, 312, 1547, 281, 3488, 50644], "temperature": 0.0, "avg_logprob": -0.10337987116404943, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0009157125605270267}, {"id": 1134, "seek": 684576, "start": 6851.360000000001, "end": 6856.96, "text": " the human brain size by a factor of three. Right. So this particular line of evidence", "tokens": [50644, 264, 1952, 3567, 2744, 538, 257, 5952, 295, 1045, 13, 1779, 13, 407, 341, 1729, 1622, 295, 4467, 50924], "temperature": 0.0, "avg_logprob": -0.10337987116404943, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0009157125605270267}, {"id": 1135, "seek": 684576, "start": 6856.96, "end": 6861.280000000001, "text": " really suggests that the difficulty gap could be pretty narrow. Pretty small. Yeah, yeah, yeah, yeah.", "tokens": [50924, 534, 13409, 300, 264, 10360, 7417, 727, 312, 1238, 9432, 13, 10693, 1359, 13, 865, 11, 1338, 11, 1338, 11, 1338, 13, 51140], "temperature": 0.0, "avg_logprob": -0.10337987116404943, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0009157125605270267}, {"id": 1136, "seek": 684576, "start": 6861.280000000001, "end": 6868.0, "text": " Okay. Got it. Yeah. Is there more evidence about how big that gap is? So a very similar line of", "tokens": [51140, 1033, 13, 5803, 309, 13, 865, 13, 1119, 456, 544, 4467, 466, 577, 955, 300, 7417, 307, 30, 407, 257, 588, 2531, 1622, 295, 51476], "temperature": 0.0, "avg_logprob": -0.10337987116404943, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0009157125605270267}, {"id": 1137, "seek": 684576, "start": 6868.0, "end": 6873.280000000001, "text": " evidence looks at rather than differences within humans, looks at the differences between humans", "tokens": [51476, 4467, 1542, 412, 2831, 813, 7300, 1951, 6255, 11, 1542, 412, 264, 7300, 1296, 6255, 51740], "temperature": 0.0, "avg_logprob": -0.10337987116404943, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.0009157125605270267}, {"id": 1138, "seek": 687328, "start": 6873.28, "end": 6879.12, "text": " and other animals. Chimps have brains that are about three times smaller than human brains.", "tokens": [50364, 293, 661, 4882, 13, 761, 332, 1878, 362, 15442, 300, 366, 466, 1045, 1413, 4356, 813, 1952, 15442, 13, 50656], "temperature": 0.0, "avg_logprob": -0.07479427445609614, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.001939208246767521}, {"id": 1139, "seek": 687328, "start": 6880.08, "end": 6886.16, "text": " And you might think that going from chimp level intelligence to human level intelligence is enough", "tokens": [50704, 400, 291, 1062, 519, 300, 516, 490, 417, 8814, 1496, 7599, 281, 1952, 1496, 7599, 307, 1547, 51008], "temperature": 0.0, "avg_logprob": -0.07479427445609614, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.001939208246767521}, {"id": 1140, "seek": 687328, "start": 6886.16, "end": 6891.5199999999995, "text": " to cross that difficulty gap. And if you do think that that then again suggests that just", "tokens": [51008, 281, 3278, 300, 10360, 7417, 13, 400, 498, 291, 360, 519, 300, 300, 550, 797, 13409, 300, 445, 51276], "temperature": 0.0, "avg_logprob": -0.07479427445609614, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.001939208246767521}, {"id": 1141, "seek": 687328, "start": 6891.5199999999995, "end": 6897.12, "text": " increasing the parameters in a model by just a factor of three could be enough to cross that", "tokens": [51276, 5662, 264, 9834, 294, 257, 2316, 538, 445, 257, 5952, 295, 1045, 727, 312, 1547, 281, 3278, 300, 51556], "temperature": 0.0, "avg_logprob": -0.07479427445609614, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.001939208246767521}, {"id": 1142, "seek": 687328, "start": 6897.12, "end": 6902.5599999999995, "text": " difficulty gap. Okay. So that's some reasons to think it could be kind of small. Are there any", "tokens": [51556, 10360, 7417, 13, 1033, 13, 407, 300, 311, 512, 4112, 281, 519, 309, 727, 312, 733, 295, 1359, 13, 2014, 456, 604, 51828], "temperature": 0.0, "avg_logprob": -0.07479427445609614, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.001939208246767521}, {"id": 1143, "seek": 690256, "start": 6902.56, "end": 6908.320000000001, "text": " reasons to think it could be, yeah, really much bigger? Yeah. So those two reasons to think it's", "tokens": [50364, 4112, 281, 519, 309, 727, 312, 11, 1338, 11, 534, 709, 3801, 30, 865, 13, 407, 729, 732, 4112, 281, 519, 309, 311, 50652], "temperature": 0.0, "avg_logprob": -0.10006181399027507, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0005846103886142373}, {"id": 1144, "seek": 690256, "start": 6908.320000000001, "end": 6914.4800000000005, "text": " small are both taking a view on intelligence, which is kind of one dimensional. We're kind of", "tokens": [50652, 1359, 366, 1293, 1940, 257, 1910, 322, 7599, 11, 597, 307, 733, 295, 472, 18795, 13, 492, 434, 733, 295, 50960], "temperature": 0.0, "avg_logprob": -0.10006181399027507, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0005846103886142373}, {"id": 1145, "seek": 690256, "start": 6914.4800000000005, "end": 6919.84, "text": " imagining that some humans are cleverer than other humans and humans are cleverer at chimps.", "tokens": [50960, 27798, 300, 512, 6255, 366, 13494, 260, 813, 661, 6255, 293, 6255, 366, 13494, 260, 412, 18375, 1878, 13, 51228], "temperature": 0.0, "avg_logprob": -0.10006181399027507, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0005846103886142373}, {"id": 1146, "seek": 690256, "start": 6919.84, "end": 6922.96, "text": " And we're just imagining as you make the brain bigger, they just get smarter and smarter.", "tokens": [51228, 400, 321, 434, 445, 27798, 382, 291, 652, 264, 3567, 3801, 11, 436, 445, 483, 20294, 293, 20294, 13, 51384], "temperature": 0.0, "avg_logprob": -0.10006181399027507, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0005846103886142373}, {"id": 1147, "seek": 690256, "start": 6923.84, "end": 6928.160000000001, "text": " The perspective which suggests that the difficulty gap could be bigger is a perspective which", "tokens": [51428, 440, 4585, 597, 13409, 300, 264, 10360, 7417, 727, 312, 3801, 307, 257, 4585, 597, 51644], "temperature": 0.0, "avg_logprob": -0.10006181399027507, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.0005846103886142373}, {"id": 1148, "seek": 692816, "start": 6928.16, "end": 6933.44, "text": " emphasizes that actually there's not one dimension of intelligence, but actually there's loads of", "tokens": [50364, 48856, 300, 767, 456, 311, 406, 472, 10139, 295, 7599, 11, 457, 767, 456, 311, 12668, 295, 50628], "temperature": 0.0, "avg_logprob": -0.088062607964804, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.001918681664392352}, {"id": 1149, "seek": 692816, "start": 6933.44, "end": 6940.48, "text": " different tasks in the world. And those tasks, you know, have very different requirements.", "tokens": [50628, 819, 9608, 294, 264, 1002, 13, 400, 729, 9608, 11, 291, 458, 11, 362, 588, 819, 7728, 13, 50980], "temperature": 0.0, "avg_logprob": -0.088062607964804, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.001918681664392352}, {"id": 1150, "seek": 692816, "start": 6941.12, "end": 6945.12, "text": " And so AI might get good at some of them way before it gets good at other ones.", "tokens": [51012, 400, 370, 7318, 1062, 483, 665, 412, 512, 295, 552, 636, 949, 309, 2170, 665, 412, 661, 2306, 13, 51212], "temperature": 0.0, "avg_logprob": -0.088062607964804, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.001918681664392352}, {"id": 1151, "seek": 692816, "start": 6946.16, "end": 6954.48, "text": " Off the bat, I don't find that that intuitive because the brain seems to be so flexible. So,", "tokens": [51264, 6318, 264, 7362, 11, 286, 500, 380, 915, 300, 300, 21769, 570, 264, 3567, 2544, 281, 312, 370, 11358, 13, 407, 11, 51680], "temperature": 0.0, "avg_logprob": -0.088062607964804, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.001918681664392352}, {"id": 1152, "seek": 695448, "start": 6954.48, "end": 6959.36, "text": " yeah, the training of these AI models on these tasks, you'd have to think that they were just", "tokens": [50364, 1338, 11, 264, 3097, 295, 613, 7318, 5245, 322, 613, 9608, 11, 291, 1116, 362, 281, 519, 300, 436, 645, 445, 50608], "temperature": 0.0, "avg_logprob": -0.07088465637035585, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0007650278857909143}, {"id": 1153, "seek": 695448, "start": 6959.36, "end": 6965.679999999999, "text": " pretty different from the human brain and much less flexible. I think it is true that if you expect", "tokens": [50608, 1238, 819, 490, 264, 1952, 3567, 293, 709, 1570, 11358, 13, 286, 519, 309, 307, 2074, 300, 498, 291, 2066, 50924], "temperature": 0.0, "avg_logprob": -0.07088465637035585, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0007650278857909143}, {"id": 1154, "seek": 695448, "start": 6965.679999999999, "end": 6976.24, "text": " AI to be a pretty general learner and have pretty general abilities, then that would lend itself to", "tokens": [50924, 7318, 281, 312, 257, 1238, 2674, 33347, 293, 362, 1238, 2674, 11582, 11, 550, 300, 576, 21774, 2564, 281, 51452], "temperature": 0.0, "avg_logprob": -0.07088465637035585, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0007650278857909143}, {"id": 1155, "seek": 695448, "start": 6976.24, "end": 6981.599999999999, "text": " the one dimensional view and against this view. But I do think that there are reasons to think", "tokens": [51452, 264, 472, 18795, 1910, 293, 1970, 341, 1910, 13, 583, 286, 360, 519, 300, 456, 366, 4112, 281, 519, 51720], "temperature": 0.0, "avg_logprob": -0.07088465637035585, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0007650278857909143}, {"id": 1156, "seek": 698160, "start": 6981.6, "end": 6988.400000000001, "text": " that AIs would be better at some tasks than others. So in particular, at the moment, the best AI", "tokens": [50364, 300, 316, 6802, 576, 312, 1101, 412, 512, 9608, 813, 2357, 13, 407, 294, 1729, 11, 412, 264, 1623, 11, 264, 1151, 7318, 50704], "temperature": 0.0, "avg_logprob": -0.08737828996446398, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.01916266418993473}, {"id": 1157, "seek": 698160, "start": 6988.400000000001, "end": 6993.76, "text": " systems are trained to predict the next word on lots and lots of internet data. And that means that", "tokens": [50704, 3652, 366, 8895, 281, 6069, 264, 958, 1349, 322, 3195, 293, 3195, 295, 4705, 1412, 13, 400, 300, 1355, 300, 50972], "temperature": 0.0, "avg_logprob": -0.08737828996446398, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.01916266418993473}, {"id": 1158, "seek": 698160, "start": 6993.76, "end": 6999.52, "text": " AIs are just particularly good at tasks that are similar to that in some way. So for example,", "tokens": [50972, 316, 6802, 366, 445, 4098, 665, 412, 9608, 300, 366, 2531, 281, 300, 294, 512, 636, 13, 407, 337, 1365, 11, 51260], "temperature": 0.0, "avg_logprob": -0.08737828996446398, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.01916266418993473}, {"id": 1159, "seek": 698160, "start": 6999.52, "end": 7005.360000000001, "text": " writing a newspaper article or writing an email, that's really similar to a task where it's seen", "tokens": [51260, 3579, 257, 13669, 7222, 420, 3579, 364, 3796, 11, 300, 311, 534, 2531, 281, 257, 5633, 689, 309, 311, 1612, 51552], "temperature": 0.0, "avg_logprob": -0.08737828996446398, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.01916266418993473}, {"id": 1160, "seek": 698160, "start": 7005.360000000001, "end": 7011.120000000001, "text": " loads and loads of examples. And so AIs are in fact particularly good at that type of task.", "tokens": [51552, 12668, 293, 12668, 295, 5110, 13, 400, 370, 316, 6802, 366, 294, 1186, 4098, 665, 412, 300, 2010, 295, 5633, 13, 51840], "temperature": 0.0, "avg_logprob": -0.08737828996446398, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.01916266418993473}, {"id": 1161, "seek": 701112, "start": 7012.08, "end": 7018.64, "text": " Whereas taking another type of task, like let's say planning out how to put the equipment on a", "tokens": [50412, 13813, 1940, 1071, 2010, 295, 5633, 11, 411, 718, 311, 584, 5038, 484, 577, 281, 829, 264, 5927, 322, 257, 50740], "temperature": 0.0, "avg_logprob": -0.10593281235805778, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0006689947913400829}, {"id": 1162, "seek": 701112, "start": 7018.64, "end": 7026.08, "text": " factory floor and then giving instructions to different people about how they should make that", "tokens": [50740, 9265, 4123, 293, 550, 2902, 9415, 281, 819, 561, 466, 577, 436, 820, 652, 300, 51112], "temperature": 0.0, "avg_logprob": -0.10593281235805778, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0006689947913400829}, {"id": 1163, "seek": 701112, "start": 7026.08, "end": 7031.76, "text": " happen, that might be something that it just hasn't seen many examples of in its training data. Or", "tokens": [51112, 1051, 11, 300, 1062, 312, 746, 300, 309, 445, 6132, 380, 1612, 867, 5110, 295, 294, 1080, 3097, 1412, 13, 1610, 51396], "temperature": 0.0, "avg_logprob": -0.10593281235805778, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0006689947913400829}, {"id": 1164, "seek": 701112, "start": 7031.76, "end": 7037.76, "text": " another task could be manipulating a robot. So the robot kind of does a certain task. That again", "tokens": [51396, 1071, 5633, 727, 312, 40805, 257, 7881, 13, 407, 264, 7881, 733, 295, 775, 257, 1629, 5633, 13, 663, 797, 51696], "temperature": 0.0, "avg_logprob": -0.10593281235805778, "compression_ratio": 1.610878661087866, "no_speech_prob": 0.0006689947913400829}, {"id": 1165, "seek": 703776, "start": 7037.84, "end": 7041.52, "text": " is something that AIs just haven't seen many examples of. So you'd expect them to be much", "tokens": [50368, 307, 746, 300, 316, 6802, 445, 2378, 380, 1612, 867, 5110, 295, 13, 407, 291, 1116, 2066, 552, 281, 312, 709, 50552], "temperature": 0.0, "avg_logprob": -0.06978017558222231, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.04041115939617157}, {"id": 1166, "seek": 703776, "start": 7041.52, "end": 7046.96, "text": " worse at that kind of task. One interesting example could be thinking about what you need to do to", "tokens": [50552, 5324, 412, 300, 733, 295, 5633, 13, 1485, 1880, 1365, 727, 312, 1953, 466, 437, 291, 643, 281, 360, 281, 50824], "temperature": 0.0, "avg_logprob": -0.06978017558222231, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.04041115939617157}, {"id": 1167, "seek": 703776, "start": 7046.96, "end": 7052.24, "text": " make a certain factory run very efficiently. It could be that some of the workers in that factory", "tokens": [50824, 652, 257, 1629, 9265, 1190, 588, 19621, 13, 467, 727, 312, 300, 512, 295, 264, 5600, 294, 300, 9265, 51088], "temperature": 0.0, "avg_logprob": -0.06978017558222231, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.04041115939617157}, {"id": 1168, "seek": 703776, "start": 7052.24, "end": 7057.280000000001, "text": " have just kind of internalized that know how inside their own brains, but it's maybe not even written", "tokens": [51088, 362, 445, 733, 295, 6920, 1602, 300, 458, 577, 1854, 641, 1065, 15442, 11, 457, 309, 311, 1310, 406, 754, 3720, 51340], "temperature": 0.0, "avg_logprob": -0.06978017558222231, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.04041115939617157}, {"id": 1169, "seek": 703776, "start": 7057.280000000001, "end": 7061.92, "text": " down anywhere. Then if you were trying to get the AI to now run the factory floor, it could be", "tokens": [51340, 760, 4992, 13, 1396, 498, 291, 645, 1382, 281, 483, 264, 7318, 281, 586, 1190, 264, 9265, 4123, 11, 309, 727, 312, 51572], "temperature": 0.0, "avg_logprob": -0.06978017558222231, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.04041115939617157}, {"id": 1170, "seek": 706192, "start": 7061.92, "end": 7067.52, "text": " particularly hard for it to know what to do there because it doesn't have any examples or any", "tokens": [50364, 4098, 1152, 337, 309, 281, 458, 437, 281, 360, 456, 570, 309, 1177, 380, 362, 604, 5110, 420, 604, 50644], "temperature": 0.0, "avg_logprob": -0.11235919206038765, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.022523190826177597}, {"id": 1171, "seek": 706192, "start": 7067.52, "end": 7072.72, "text": " experience of that kind of thing. Got it. Okay. So some of the difficulty gap might not be about,", "tokens": [50644, 1752, 295, 300, 733, 295, 551, 13, 5803, 309, 13, 1033, 13, 407, 512, 295, 264, 10360, 7417, 1062, 406, 312, 466, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11235919206038765, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.022523190826177597}, {"id": 1172, "seek": 706192, "start": 7073.36, "end": 7079.68, "text": " I don't know, like fundamental facts about the types of intelligence that you might use to perform", "tokens": [50936, 286, 500, 380, 458, 11, 411, 8088, 9130, 466, 264, 3467, 295, 7599, 300, 291, 1062, 764, 281, 2042, 51252], "temperature": 0.0, "avg_logprob": -0.11235919206038765, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.022523190826177597}, {"id": 1173, "seek": 706192, "start": 7079.68, "end": 7084.8, "text": " different tasks and might be much more about the type of data, the types of data we have. And", "tokens": [51252, 819, 9608, 293, 1062, 312, 709, 544, 466, 264, 2010, 295, 1412, 11, 264, 3467, 295, 1412, 321, 362, 13, 400, 51508], "temperature": 0.0, "avg_logprob": -0.11235919206038765, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.022523190826177597}, {"id": 1174, "seek": 708480, "start": 7085.76, "end": 7091.92, "text": " some things might be difficult just because we never write down what it means to do those", "tokens": [50412, 512, 721, 1062, 312, 2252, 445, 570, 321, 1128, 2464, 760, 437, 309, 1355, 281, 360, 729, 50720], "temperature": 0.0, "avg_logprob": -0.14620401571085165, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.03869437426328659}, {"id": 1175, "seek": 708480, "start": 7091.92, "end": 7097.4400000000005, "text": " things. And so it's harder to teach an AI system to do it, not because they're fundamentally", "tokens": [50720, 721, 13, 400, 370, 309, 311, 6081, 281, 2924, 364, 7318, 1185, 281, 360, 309, 11, 406, 570, 436, 434, 17879, 50996], "temperature": 0.0, "avg_logprob": -0.14620401571085165, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.03869437426328659}, {"id": 1176, "seek": 708480, "start": 7098.0, "end": 7104.24, "text": " extremely difficult in none of themselves. Yeah, that's right. Cool. Okay, that really helped.", "tokens": [51024, 4664, 2252, 294, 6022, 295, 2969, 13, 865, 11, 300, 311, 558, 13, 8561, 13, 1033, 11, 300, 534, 4254, 13, 51336], "temperature": 0.0, "avg_logprob": -0.14620401571085165, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.03869437426328659}, {"id": 1177, "seek": 708480, "start": 7104.24, "end": 7109.76, "text": " Next. I'll give one more example about how some tasks could be easy than others. So some tasks,", "tokens": [51336, 3087, 13, 286, 603, 976, 472, 544, 1365, 466, 577, 512, 9608, 727, 312, 1858, 813, 2357, 13, 407, 512, 9608, 11, 51612], "temperature": 0.0, "avg_logprob": -0.14620401571085165, "compression_ratio": 1.5541666666666667, "no_speech_prob": 0.03869437426328659}, {"id": 1178, "seek": 710976, "start": 7110.4800000000005, "end": 7115.92, "text": " it's really important that you have a very high amount of reliability. So for driving,", "tokens": [50400, 309, 311, 534, 1021, 300, 291, 362, 257, 588, 1090, 2372, 295, 24550, 13, 407, 337, 4840, 11, 50672], "temperature": 0.0, "avg_logprob": -0.13799819946289063, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.05291736498475075}, {"id": 1179, "seek": 710976, "start": 7115.92, "end": 7121.76, "text": " it's really just really awful if you crash. So if you're 99% reliable, that's worthless.", "tokens": [50672, 309, 311, 534, 445, 534, 11232, 498, 291, 8252, 13, 407, 498, 291, 434, 11803, 4, 12924, 11, 300, 311, 34857, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13799819946289063, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.05291736498475075}, {"id": 1180, "seek": 710976, "start": 7121.76, "end": 7126.8, "text": " And so if AIs, I kind of can get to 99% reliability fairly well, but can't get", "tokens": [50964, 400, 370, 498, 316, 6802, 11, 286, 733, 295, 393, 483, 281, 11803, 4, 24550, 6457, 731, 11, 457, 393, 380, 483, 51216], "temperature": 0.0, "avg_logprob": -0.13799819946289063, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.05291736498475075}, {"id": 1181, "seek": 710976, "start": 7127.68, "end": 7134.400000000001, "text": " 99.99999% reliability, then that's going to block them on certain tasks. But other tasks like drafting", "tokens": [51260, 11803, 13, 8494, 49017, 4, 24550, 11, 550, 300, 311, 516, 281, 3461, 552, 322, 1629, 9608, 13, 583, 661, 9608, 411, 46378, 51596], "temperature": 0.0, "avg_logprob": -0.13799819946289063, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.05291736498475075}, {"id": 1182, "seek": 713440, "start": 7134.4, "end": 7139.599999999999, "text": " emails or even sending emails and being a personal assistant drafting code that you can", "tokens": [50364, 12524, 420, 754, 7750, 12524, 293, 885, 257, 2973, 10994, 46378, 3089, 300, 291, 393, 50624], "temperature": 0.0, "avg_logprob": -0.11236723628612834, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007097191177308559}, {"id": 1183, "seek": 713440, "start": 7139.599999999999, "end": 7144.639999999999, "text": " kind of check whether it works before deploying it. Those tasks, it doesn't provide a blocker for.", "tokens": [50624, 733, 295, 1520, 1968, 309, 1985, 949, 34198, 309, 13, 3950, 9608, 11, 309, 1177, 380, 2893, 257, 3461, 260, 337, 13, 50876], "temperature": 0.0, "avg_logprob": -0.11236723628612834, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007097191177308559}, {"id": 1184, "seek": 713440, "start": 7144.639999999999, "end": 7149.36, "text": " So that's just another example of something that could mean that the AI is kind of ready to automate", "tokens": [50876, 407, 300, 311, 445, 1071, 1365, 295, 746, 300, 727, 914, 300, 264, 7318, 307, 733, 295, 1919, 281, 31605, 51112], "temperature": 0.0, "avg_logprob": -0.11236723628612834, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007097191177308559}, {"id": 1185, "seek": 713440, "start": 7149.36, "end": 7155.2, "text": " certain tasks before others. Cool. Okay, yep, that makes that makes sense of sense. Yeah,", "tokens": [51112, 1629, 9608, 949, 2357, 13, 8561, 13, 1033, 11, 18633, 11, 300, 1669, 300, 1669, 2020, 295, 2020, 13, 865, 11, 51404], "temperature": 0.0, "avg_logprob": -0.11236723628612834, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007097191177308559}, {"id": 1186, "seek": 713440, "start": 7155.2, "end": 7163.5199999999995, "text": " so then I guess given this type of evidence, what was the range of amount of effective compute", "tokens": [51404, 370, 550, 286, 2041, 2212, 341, 2010, 295, 4467, 11, 437, 390, 264, 3613, 295, 2372, 295, 4942, 14722, 51820], "temperature": 0.0, "avg_logprob": -0.11236723628612834, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.007097191177308559}, {"id": 1187, "seek": 716352, "start": 7163.52, "end": 7168.96, "text": " that you'd guess we'd need to go from 20% of cognitive tasks to 100% of cognitive tasks?", "tokens": [50364, 300, 291, 1116, 2041, 321, 1116, 643, 281, 352, 490, 945, 4, 295, 15605, 9608, 281, 2319, 4, 295, 15605, 9608, 30, 50636], "temperature": 0.0, "avg_logprob": -0.08456147274124289, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0004893293953500688}, {"id": 1188, "seek": 716352, "start": 7169.84, "end": 7174.160000000001, "text": " You know, the main takeaway is that a really wide range of things are plausible. So I think", "tokens": [50680, 509, 458, 11, 264, 2135, 30681, 307, 300, 257, 534, 4874, 3613, 295, 721, 366, 39925, 13, 407, 286, 519, 50896], "temperature": 0.0, "avg_logprob": -0.08456147274124289, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0004893293953500688}, {"id": 1189, "seek": 716352, "start": 7175.040000000001, "end": 7181.280000000001, "text": " as low as just 10 times as much effective compute could be sufficient. And that's pretty scary,", "tokens": [50940, 382, 2295, 382, 445, 1266, 1413, 382, 709, 4942, 14722, 727, 312, 11563, 13, 400, 300, 311, 1238, 6958, 11, 51252], "temperature": 0.0, "avg_logprob": -0.08456147274124289, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0004893293953500688}, {"id": 1190, "seek": 716352, "start": 7181.280000000001, "end": 7185.280000000001, "text": " because that's the kind of thing that could just be some quick algorithmic improvements", "tokens": [51252, 570, 300, 311, 264, 733, 295, 551, 300, 727, 445, 312, 512, 1702, 9284, 299, 13797, 51452], "temperature": 0.0, "avg_logprob": -0.08456147274124289, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0004893293953500688}, {"id": 1191, "seek": 716352, "start": 7185.280000000001, "end": 7193.360000000001, "text": " without even the need for more physical compute. So I think that is just very consistent with", "tokens": [51452, 1553, 754, 264, 643, 337, 544, 4001, 14722, 13, 407, 286, 519, 300, 307, 445, 588, 8398, 365, 51856], "temperature": 0.0, "avg_logprob": -0.08456147274124289, "compression_ratio": 1.6776556776556777, "no_speech_prob": 0.0004893293953500688}, {"id": 1192, "seek": 719352, "start": 7193.52, "end": 7199.6, "text": " this kind of one dimensional view, and really not something we can rule out. But my kind of", "tokens": [50364, 341, 733, 295, 472, 18795, 1910, 11, 293, 534, 406, 746, 321, 393, 4978, 484, 13, 583, 452, 733, 295, 50668], "temperature": 0.0, "avg_logprob": -0.08917198524818765, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0011216647690162063}, {"id": 1193, "seek": 719352, "start": 7199.6, "end": 7205.200000000001, "text": " best guess is more like 3000 times as much. And that's kind of where my median is.", "tokens": [50668, 1151, 2041, 307, 544, 411, 20984, 1413, 382, 709, 13, 400, 300, 311, 733, 295, 689, 452, 26779, 307, 13, 50948], "temperature": 0.0, "avg_logprob": -0.08917198524818765, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0011216647690162063}, {"id": 1194, "seek": 719352, "start": 7205.200000000001, "end": 7211.6, "text": " Okay, so quite a lot more. So quite a lot more than that lower end. And that's because I do expect", "tokens": [50948, 1033, 11, 370, 1596, 257, 688, 544, 13, 407, 1596, 257, 688, 544, 813, 300, 3126, 917, 13, 400, 300, 311, 570, 286, 360, 2066, 51268], "temperature": 0.0, "avg_logprob": -0.08917198524818765, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0011216647690162063}, {"id": 1195, "seek": 719352, "start": 7211.6, "end": 7217.52, "text": " there to be some significant comparative advantage components, where the AI is just kind of particularly", "tokens": [51268, 456, 281, 312, 512, 4776, 39292, 5002, 6677, 11, 689, 264, 7318, 307, 445, 733, 295, 4098, 51564], "temperature": 0.0, "avg_logprob": -0.08917198524818765, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0011216647690162063}, {"id": 1196, "seek": 719352, "start": 7217.52, "end": 7221.52, "text": " good at some tasks compared to others, and particularly struggles with certain types of tasks.", "tokens": [51564, 665, 412, 512, 9608, 5347, 281, 2357, 11, 293, 4098, 17592, 365, 1629, 3467, 295, 9608, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08917198524818765, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0011216647690162063}, {"id": 1197, "seek": 722152, "start": 7221.52, "end": 7225.84, "text": " And so I do expect that to stretch things out. And you know, I do think it's possible that that", "tokens": [50364, 400, 370, 286, 360, 2066, 300, 281, 5985, 721, 484, 13, 400, 291, 458, 11, 286, 360, 519, 309, 311, 1944, 300, 300, 50580], "temperature": 0.0, "avg_logprob": -0.10988932697712875, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.003261216916143894}, {"id": 1198, "seek": 722152, "start": 7225.84, "end": 7230.080000000001, "text": " stretches things out by even more like I think it could be it could be a million times as much.", "tokens": [50580, 29058, 721, 484, 538, 754, 544, 411, 286, 519, 309, 727, 312, 309, 727, 312, 257, 2459, 1413, 382, 709, 13, 50792], "temperature": 0.0, "avg_logprob": -0.10988932697712875, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.003261216916143894}, {"id": 1199, "seek": 722152, "start": 7231.040000000001, "end": 7237.52, "text": " That's hard to rule out. Okay, so huge range. It's a really huge range. Yeah.", "tokens": [50840, 663, 311, 1152, 281, 4978, 484, 13, 1033, 11, 370, 2603, 3613, 13, 467, 311, 257, 534, 2603, 3613, 13, 865, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10988932697712875, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.003261216916143894}, {"id": 1200, "seek": 722152, "start": 7237.52, "end": 7242.72, "text": " And you've put it into your model as as a huge range. Is that right? That's right. So there's", "tokens": [51164, 400, 291, 600, 829, 309, 666, 428, 2316, 382, 382, 257, 2603, 3613, 13, 1119, 300, 558, 30, 663, 311, 558, 13, 407, 456, 311, 51424], "temperature": 0.0, "avg_logprob": -0.10988932697712875, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.003261216916143894}, {"id": 1201, "seek": 722152, "start": 7242.72, "end": 7248.0, "text": " yeah, so in the model, there's firstly a probability distribution over how much", "tokens": [51424, 1338, 11, 370, 294, 264, 2316, 11, 456, 311, 27376, 257, 8482, 7316, 670, 577, 709, 51688], "temperature": 0.0, "avg_logprob": -0.10988932697712875, "compression_ratio": 1.7440944881889764, "no_speech_prob": 0.003261216916143894}, {"id": 1202, "seek": 724800, "start": 7248.0, "end": 7254.16, "text": " effective compute you need to train AGI. And then there's another probability distribution over", "tokens": [50364, 4942, 14722, 291, 643, 281, 3847, 316, 26252, 13, 400, 550, 456, 311, 1071, 8482, 7316, 670, 50672], "temperature": 0.0, "avg_logprob": -0.11412789744715537, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0036759497597813606}, {"id": 1203, "seek": 724800, "start": 7254.16, "end": 7263.2, "text": " how much less compute than that do you need to train 20% AI 20%. Okay, so you've got ranges for", "tokens": [50672, 577, 709, 1570, 14722, 813, 300, 360, 291, 643, 281, 3847, 945, 4, 7318, 945, 6856, 1033, 11, 370, 291, 600, 658, 22526, 337, 51124], "temperature": 0.0, "avg_logprob": -0.11412789744715537, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0036759497597813606}, {"id": 1204, "seek": 724800, "start": 7263.2, "end": 7270.32, "text": " how much effective compute you need to do both 20% and 100% of tasks. What do we know about how", "tokens": [51124, 577, 709, 4942, 14722, 291, 643, 281, 360, 1293, 945, 4, 293, 2319, 4, 295, 9608, 13, 708, 360, 321, 458, 466, 577, 51480], "temperature": 0.0, "avg_logprob": -0.11412789744715537, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0036759497597813606}, {"id": 1205, "seek": 724800, "start": 7270.32, "end": 7276.88, "text": " quickly compute might increase? I guess one very simple thing we could do is just make more computer", "tokens": [51480, 2661, 14722, 1062, 3488, 30, 286, 2041, 472, 588, 2199, 551, 321, 727, 360, 307, 445, 652, 544, 3820, 51808], "temperature": 0.0, "avg_logprob": -0.11412789744715537, "compression_ratio": 1.6796536796536796, "no_speech_prob": 0.0036759497597813606}, {"id": 1206, "seek": 727688, "start": 7276.88, "end": 7282.400000000001, "text": " chips. But I don't know, yeah, what the limits to that are. And presumably, yeah, there are other", "tokens": [50364, 11583, 13, 583, 286, 500, 380, 458, 11, 1338, 11, 437, 264, 10406, 281, 300, 366, 13, 400, 26742, 11, 1338, 11, 456, 366, 661, 50640], "temperature": 0.0, "avg_logprob": -0.08655721415644106, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0011946780141443014}, {"id": 1207, "seek": 727688, "start": 7282.400000000001, "end": 7288.0, "text": " things as well. How do how do we make compute go up? Yeah, and just it is importantly effective", "tokens": [50640, 721, 382, 731, 13, 1012, 360, 577, 360, 321, 652, 14722, 352, 493, 30, 865, 11, 293, 445, 309, 307, 8906, 4942, 50920], "temperature": 0.0, "avg_logprob": -0.08655721415644106, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0011946780141443014}, {"id": 1208, "seek": 727688, "start": 7288.0, "end": 7293.6, "text": " compute that includes includes the algorithmic improvements. Got it. Yeah, thanks. So one natural", "tokens": [50920, 14722, 300, 5974, 5974, 264, 9284, 299, 13797, 13, 5803, 309, 13, 865, 11, 3231, 13, 407, 472, 3303, 51200], "temperature": 0.0, "avg_logprob": -0.08655721415644106, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0011946780141443014}, {"id": 1209, "seek": 727688, "start": 7293.6, "end": 7298.96, "text": " way to approach this could be to first discuss the types of changes that are increasing effective", "tokens": [51200, 636, 281, 3109, 341, 727, 312, 281, 700, 2248, 264, 3467, 295, 2962, 300, 366, 5662, 4942, 51468], "temperature": 0.0, "avg_logprob": -0.08655721415644106, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0011946780141443014}, {"id": 1210, "seek": 727688, "start": 7298.96, "end": 7305.12, "text": " compute today, and then how that might be different once we actually get to the 20% AI.", "tokens": [51468, 14722, 965, 11, 293, 550, 577, 300, 1062, 312, 819, 1564, 321, 767, 483, 281, 264, 945, 4, 7318, 13, 51776], "temperature": 0.0, "avg_logprob": -0.08655721415644106, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.0011946780141443014}, {"id": 1211, "seek": 730512, "start": 7305.12, "end": 7308.8, "text": " Sure. Yeah, tell me about how effective compute is increasing today.", "tokens": [50364, 4894, 13, 865, 11, 980, 385, 466, 577, 4942, 14722, 307, 5662, 965, 13, 50548], "temperature": 0.0, "avg_logprob": -0.10806379512864717, "compression_ratio": 1.7479338842975207, "no_speech_prob": 0.0006363113643601537}, {"id": 1212, "seek": 730512, "start": 7309.68, "end": 7317.04, "text": " So the first way is quite simply that we're spending more money on making computer chips on", "tokens": [50592, 407, 264, 700, 636, 307, 1596, 2935, 300, 321, 434, 6434, 544, 1460, 322, 1455, 3820, 11583, 322, 50960], "temperature": 0.0, "avg_logprob": -0.10806379512864717, "compression_ratio": 1.7479338842975207, "no_speech_prob": 0.0006363113643601537}, {"id": 1213, "seek": 730512, "start": 7317.04, "end": 7323.2, "text": " compute. Right. And we're also spending more money on using compute for training runs. The amount", "tokens": [50960, 14722, 13, 1779, 13, 400, 321, 434, 611, 6434, 544, 1460, 322, 1228, 14722, 337, 3097, 6676, 13, 440, 2372, 51268], "temperature": 0.0, "avg_logprob": -0.10806379512864717, "compression_ratio": 1.7479338842975207, "no_speech_prob": 0.0006363113643601537}, {"id": 1214, "seek": 730512, "start": 7323.2, "end": 7327.12, "text": " we're spending on compute for training runs is growing particularly quickly right now. It's", "tokens": [51268, 321, 434, 6434, 322, 14722, 337, 3097, 6676, 307, 4194, 4098, 2661, 558, 586, 13, 467, 311, 51464], "temperature": 0.0, "avg_logprob": -0.10806379512864717, "compression_ratio": 1.7479338842975207, "no_speech_prob": 0.0006363113643601537}, {"id": 1215, "seek": 730512, "start": 7328.4, "end": 7332.48, "text": " over the last 10 years has gone up by about a factor of three each year.", "tokens": [51528, 670, 264, 1036, 1266, 924, 575, 2780, 493, 538, 466, 257, 5952, 295, 1045, 1184, 1064, 13, 51732], "temperature": 0.0, "avg_logprob": -0.10806379512864717, "compression_ratio": 1.7479338842975207, "no_speech_prob": 0.0006363113643601537}, {"id": 1216, "seek": 733248, "start": 7333.2, "end": 7338.16, "text": " And so what does it mean when we spend more on compute for training runs in particular,", "tokens": [50400, 400, 370, 437, 775, 309, 914, 562, 321, 3496, 544, 322, 14722, 337, 3097, 6676, 294, 1729, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10089563406430758, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.0008024174021556973}, {"id": 1217, "seek": 733248, "start": 7338.16, "end": 7341.44, "text": " as opposed to just more compute, like more computer chips?", "tokens": [50648, 382, 8851, 281, 445, 544, 14722, 11, 411, 544, 3820, 11583, 30, 50812], "temperature": 0.0, "avg_logprob": -0.10089563406430758, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.0008024174021556973}, {"id": 1218, "seek": 733248, "start": 7342.32, "end": 7348.719999999999, "text": " Good question. So there's a certain amount of computer chips in the whole world. But at any,", "tokens": [50856, 2205, 1168, 13, 407, 456, 311, 257, 1629, 2372, 295, 3820, 11583, 294, 264, 1379, 1002, 13, 583, 412, 604, 11, 51176], "temperature": 0.0, "avg_logprob": -0.10089563406430758, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.0008024174021556973}, {"id": 1219, "seek": 733248, "start": 7348.719999999999, "end": 7353.2, "text": " you know, at any point in time, maybe only a small fraction of those are being used", "tokens": [51176, 291, 458, 11, 412, 604, 935, 294, 565, 11, 1310, 787, 257, 1359, 14135, 295, 729, 366, 885, 1143, 51400], "temperature": 0.0, "avg_logprob": -0.10089563406430758, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.0008024174021556973}, {"id": 1220, "seek": 733248, "start": 7353.2, "end": 7359.44, "text": " in the largest training run. Okay. For an AI system. So one change you can make is you could", "tokens": [51400, 294, 264, 6443, 3097, 1190, 13, 1033, 13, 1171, 364, 7318, 1185, 13, 407, 472, 1319, 291, 393, 652, 307, 291, 727, 51712], "temperature": 0.0, "avg_logprob": -0.10089563406430758, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.0008024174021556973}, {"id": 1221, "seek": 735944, "start": 7359.44, "end": 7364.799999999999, "text": " say we're going to make there be twice as many computer chips in the world. And that would take", "tokens": [50364, 584, 321, 434, 516, 281, 652, 456, 312, 6091, 382, 867, 3820, 11583, 294, 264, 1002, 13, 400, 300, 576, 747, 50632], "temperature": 0.0, "avg_logprob": -0.10525088735146097, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.001979425549507141}, {"id": 1222, "seek": 735944, "start": 7364.799999999999, "end": 7370.08, "text": " a big effort. That would take quite a few years to do probably. But another change that you could", "tokens": [50632, 257, 955, 4630, 13, 663, 576, 747, 1596, 257, 1326, 924, 281, 360, 1391, 13, 583, 1071, 1319, 300, 291, 727, 50896], "temperature": 0.0, "avg_logprob": -0.10525088735146097, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.001979425549507141}, {"id": 1223, "seek": 735944, "start": 7370.08, "end": 7378.4, "text": " make much more quickly is say, well, as of today, we've only ever used one 10,000th of the world's", "tokens": [50896, 652, 709, 544, 2661, 307, 584, 11, 731, 11, 382, 295, 965, 11, 321, 600, 787, 1562, 1143, 472, 1266, 11, 1360, 392, 295, 264, 1002, 311, 51312], "temperature": 0.0, "avg_logprob": -0.10525088735146097, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.001979425549507141}, {"id": 1224, "seek": 735944, "start": 7378.4, "end": 7384.24, "text": " compute to actually use it in a training run. So we can quite quickly just use 10 times as much.", "tokens": [51312, 14722, 281, 767, 764, 309, 294, 257, 3097, 1190, 13, 407, 321, 393, 1596, 2661, 445, 764, 1266, 1413, 382, 709, 13, 51604], "temperature": 0.0, "avg_logprob": -0.10525088735146097, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.001979425549507141}, {"id": 1225, "seek": 738424, "start": 7384.24, "end": 7391.76, "text": " We'll just kind of buy a bigger fraction of the already existing compute. The simplest example", "tokens": [50364, 492, 603, 445, 733, 295, 2256, 257, 3801, 14135, 295, 264, 1217, 6741, 14722, 13, 440, 22811, 1365, 50740], "temperature": 0.0, "avg_logprob": -0.09414954807447351, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.041285738348960876}, {"id": 1226, "seek": 738424, "start": 7391.76, "end": 7398.639999999999, "text": " would probably be DeepMind has historically only used a small fraction of Google's computer chips", "tokens": [50740, 576, 1391, 312, 14895, 44, 471, 575, 16180, 787, 1143, 257, 1359, 14135, 295, 3329, 311, 3820, 11583, 51084], "temperature": 0.0, "avg_logprob": -0.09414954807447351, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.041285738348960876}, {"id": 1227, "seek": 738424, "start": 7398.639999999999, "end": 7403.36, "text": " for its training runs. And then it says, okay, we want to now use all of your computer chips.", "tokens": [51084, 337, 1080, 3097, 6676, 13, 400, 550, 309, 1619, 11, 1392, 11, 321, 528, 281, 586, 764, 439, 295, 428, 3820, 11583, 13, 51320], "temperature": 0.0, "avg_logprob": -0.09414954807447351, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.041285738348960876}, {"id": 1228, "seek": 738424, "start": 7403.36, "end": 7408.0, "text": " Got it. And that could be maybe that could be a hundred X increase. I don't know. Okay.", "tokens": [51320, 5803, 309, 13, 400, 300, 727, 312, 1310, 300, 727, 312, 257, 3262, 1783, 3488, 13, 286, 500, 380, 458, 13, 1033, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09414954807447351, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.041285738348960876}, {"id": 1229, "seek": 738424, "start": 7408.0, "end": 7411.5199999999995, "text": " You know, that was just an example to illustrate the principle. I don't think that's what's", "tokens": [51552, 509, 458, 11, 300, 390, 445, 364, 1365, 281, 23221, 264, 8665, 13, 286, 500, 380, 519, 300, 311, 437, 311, 51728], "temperature": 0.0, "avg_logprob": -0.09414954807447351, "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.041285738348960876}, {"id": 1230, "seek": 741152, "start": 7411.52, "end": 7417.280000000001, "text": " actually been happening at all. So I think what's actually been happening is that new computer chips", "tokens": [50364, 767, 668, 2737, 412, 439, 13, 407, 286, 519, 437, 311, 767, 668, 2737, 307, 300, 777, 3820, 11583, 50652], "temperature": 0.0, "avg_logprob": -0.061408289547624256, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0019944380037486553}, {"id": 1231, "seek": 741152, "start": 7417.280000000001, "end": 7423.6, "text": " are being made each year. And a bigger fraction of those new chips are being used for the largest", "tokens": [50652, 366, 885, 1027, 1184, 1064, 13, 400, 257, 3801, 14135, 295, 729, 777, 11583, 366, 885, 1143, 337, 264, 6443, 50968], "temperature": 0.0, "avg_logprob": -0.061408289547624256, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0019944380037486553}, {"id": 1232, "seek": 741152, "start": 7423.6, "end": 7430.080000000001, "text": " training run. And you can actually see that the fraction of chips that are AI specific chips has", "tokens": [50968, 3097, 1190, 13, 400, 291, 393, 767, 536, 300, 264, 14135, 295, 11583, 300, 366, 7318, 2685, 11583, 575, 51292], "temperature": 0.0, "avg_logprob": -0.061408289547624256, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0019944380037486553}, {"id": 1233, "seek": 741152, "start": 7430.080000000001, "end": 7435.360000000001, "text": " been increasing very quickly in terms of the production. So that's one way we can get more", "tokens": [51292, 668, 5662, 588, 2661, 294, 2115, 295, 264, 4265, 13, 407, 300, 311, 472, 636, 321, 393, 483, 544, 51556], "temperature": 0.0, "avg_logprob": -0.061408289547624256, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0019944380037486553}, {"id": 1234, "seek": 743536, "start": 7435.36, "end": 7442.88, "text": " effective compute. Yeah, are there others? Yeah. So the second big way is improving the", "tokens": [50364, 4942, 14722, 13, 865, 11, 366, 456, 2357, 30, 865, 13, 407, 264, 1150, 955, 636, 307, 11470, 264, 50740], "temperature": 0.0, "avg_logprob": -0.07968208763036835, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.09838255494832993}, {"id": 1235, "seek": 743536, "start": 7442.88, "end": 7449.2, "text": " quality of computer chips. So we said that the first way was spending more money on compute.", "tokens": [50740, 3125, 295, 3820, 11583, 13, 407, 321, 848, 300, 264, 700, 636, 390, 6434, 544, 1460, 322, 14722, 13, 51056], "temperature": 0.0, "avg_logprob": -0.07968208763036835, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.09838255494832993}, {"id": 1236, "seek": 743536, "start": 7449.2, "end": 7455.28, "text": " The second the second way is that each dollar you spend gets you more compute. So the best", "tokens": [51056, 440, 1150, 264, 1150, 636, 307, 300, 1184, 7241, 291, 3496, 2170, 291, 544, 14722, 13, 407, 264, 1151, 51360], "temperature": 0.0, "avg_logprob": -0.07968208763036835, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.09838255494832993}, {"id": 1237, "seek": 743536, "start": 7455.28, "end": 7460.719999999999, "text": " data that I've seen on this suggests that every two and a half years, compute gets twice as cheap.", "tokens": [51360, 1412, 300, 286, 600, 1612, 322, 341, 13409, 300, 633, 732, 293, 257, 1922, 924, 11, 14722, 2170, 6091, 382, 7084, 13, 51632], "temperature": 0.0, "avg_logprob": -0.07968208763036835, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.09838255494832993}, {"id": 1238, "seek": 746072, "start": 7460.72, "end": 7466.400000000001, "text": " Okay. And that's because that's different from like algorithms getting better. That's like", "tokens": [50364, 1033, 13, 400, 300, 311, 570, 300, 311, 819, 490, 411, 14642, 1242, 1101, 13, 663, 311, 411, 50648], "temperature": 0.0, "avg_logprob": -0.09615508506172582, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.0012056929990649223}, {"id": 1239, "seek": 746072, "start": 7466.400000000001, "end": 7471.68, "text": " computer chips get more efficient because like the hardware is designed better.", "tokens": [50648, 3820, 11583, 483, 544, 7148, 570, 411, 264, 8837, 307, 4761, 1101, 13, 50912], "temperature": 0.0, "avg_logprob": -0.09615508506172582, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.0012056929990649223}, {"id": 1240, "seek": 746072, "start": 7472.320000000001, "end": 7479.92, "text": " Yeah. I think historically, it's often been about stuffing more processing units onto each chip.", "tokens": [50944, 865, 13, 286, 519, 16180, 11, 309, 311, 2049, 668, 466, 36046, 544, 9007, 6815, 3911, 1184, 11409, 13, 51324], "temperature": 0.0, "avg_logprob": -0.09615508506172582, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.0012056929990649223}, {"id": 1241, "seek": 746072, "start": 7479.92, "end": 7485.68, "text": " And you know, maintaining still can managing to make these chips fairly cheaply.", "tokens": [51324, 400, 291, 458, 11, 14916, 920, 393, 11642, 281, 652, 613, 11583, 6457, 7084, 356, 13, 51612], "temperature": 0.0, "avg_logprob": -0.09615508506172582, "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.0012056929990649223}, {"id": 1242, "seek": 748568, "start": 7485.68, "end": 7491.92, "text": " Okay. So you can you can buy more chips, you can make better chips. Yeah. Anything else?", "tokens": [50364, 1033, 13, 407, 291, 393, 291, 393, 2256, 544, 11583, 11, 291, 393, 652, 1101, 11583, 13, 865, 13, 11998, 1646, 30, 50676], "temperature": 0.0, "avg_logprob": -0.11333800606105639, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0006276597850956023}, {"id": 1243, "seek": 748568, "start": 7491.92, "end": 7497.200000000001, "text": " So then the third one is algorithms. So then you've got to, you know, you've now spent a", "tokens": [50676, 407, 550, 264, 2636, 472, 307, 14642, 13, 407, 550, 291, 600, 658, 281, 11, 291, 458, 11, 291, 600, 586, 4418, 257, 50940], "temperature": 0.0, "avg_logprob": -0.11333800606105639, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0006276597850956023}, {"id": 1244, "seek": 748568, "start": 7497.200000000001, "end": 7502.08, "text": " certain amount on compute, you've got a certain amount of computers as a result of that. And then", "tokens": [50940, 1629, 2372, 322, 14722, 11, 291, 600, 658, 257, 1629, 2372, 295, 10807, 382, 257, 1874, 295, 300, 13, 400, 550, 51184], "temperature": 0.0, "avg_logprob": -0.11333800606105639, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0006276597850956023}, {"id": 1245, "seek": 748568, "start": 7502.08, "end": 7508.240000000001, "text": " algorithms then say how effectively and efficiently can you use that compute to actually train an AI", "tokens": [51184, 14642, 550, 584, 577, 8659, 293, 19621, 393, 291, 764, 300, 14722, 281, 767, 3847, 364, 7318, 51492], "temperature": 0.0, "avg_logprob": -0.11333800606105639, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0006276597850956023}, {"id": 1246, "seek": 748568, "start": 7508.240000000001, "end": 7515.04, "text": " system? Cool. The most famous example of this type of improvement is a paper called AI and efficiency.", "tokens": [51492, 1185, 30, 8561, 13, 440, 881, 4618, 1365, 295, 341, 2010, 295, 10444, 307, 257, 3035, 1219, 7318, 293, 10493, 13, 51832], "temperature": 0.0, "avg_logprob": -0.11333800606105639, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0006276597850956023}, {"id": 1247, "seek": 751568, "start": 7515.68, "end": 7521.68, "text": " That open AI published, I think in 2018. And what they did is they, they said to achieve", "tokens": [50364, 663, 1269, 7318, 6572, 11, 286, 519, 294, 6096, 13, 400, 437, 436, 630, 307, 436, 11, 436, 848, 281, 4584, 50664], "temperature": 0.0, "avg_logprob": -0.1273553117792657, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0008773375884629786}, {"id": 1248, "seek": 751568, "start": 7522.400000000001, "end": 7528.16, "text": " kind of a fixed level of performance on ImageNet, which means to kind of be to be kind of fairly", "tokens": [50700, 733, 295, 257, 6806, 1496, 295, 3389, 322, 29903, 31890, 11, 597, 1355, 281, 733, 295, 312, 281, 312, 733, 295, 6457, 50988], "temperature": 0.0, "avg_logprob": -0.1273553117792657, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0008773375884629786}, {"id": 1249, "seek": 751568, "start": 7528.16, "end": 7534.400000000001, "text": " pretty good at classifying what is shown in an image. Yep. How much is the compute required to", "tokens": [50988, 1238, 665, 412, 1508, 5489, 437, 307, 4898, 294, 364, 3256, 13, 7010, 13, 1012, 709, 307, 264, 14722, 4739, 281, 51300], "temperature": 0.0, "avg_logprob": -0.1273553117792657, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0008773375884629786}, {"id": 1250, "seek": 751568, "start": 7534.400000000001, "end": 7541.52, "text": " get that performance falling over time? Right. Okay. And so they found that I think every 15", "tokens": [51300, 483, 300, 3389, 7440, 670, 565, 30, 1779, 13, 1033, 13, 400, 370, 436, 1352, 300, 286, 519, 633, 2119, 51656], "temperature": 0.0, "avg_logprob": -0.1273553117792657, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0008773375884629786}, {"id": 1251, "seek": 754152, "start": 7541.52, "end": 7548.240000000001, "text": " months or so that the amount of compute you needed was halving to achieve that fixed performance.", "tokens": [50364, 2493, 420, 370, 300, 264, 2372, 295, 14722, 291, 2978, 390, 7523, 798, 281, 4584, 300, 6806, 3389, 13, 50700], "temperature": 0.0, "avg_logprob": -0.08145880131494432, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.002045099390670657}, {"id": 1252, "seek": 754152, "start": 7548.240000000001, "end": 7557.84, "text": " Okay. And that's basically programmers being clever and writing programs that help AI systems", "tokens": [50700, 1033, 13, 400, 300, 311, 1936, 41504, 885, 13494, 293, 3579, 4268, 300, 854, 7318, 3652, 51180], "temperature": 0.0, "avg_logprob": -0.08145880131494432, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.002045099390670657}, {"id": 1253, "seek": 754152, "start": 7557.84, "end": 7563.4400000000005, "text": " figure out what's in an image in more and more efficient ways. Exactly. So, I mean, analogy could", "tokens": [51180, 2573, 484, 437, 311, 294, 364, 3256, 294, 544, 293, 544, 7148, 2098, 13, 7587, 13, 407, 11, 286, 914, 11, 21663, 727, 51460], "temperature": 0.0, "avg_logprob": -0.08145880131494432, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.002045099390670657}, {"id": 1254, "seek": 754152, "start": 7563.4400000000005, "end": 7568.400000000001, "text": " be maybe a hundred years ago, our schools were really inefficient at transmitting knowledge", "tokens": [51460, 312, 1310, 257, 3262, 924, 2057, 11, 527, 4656, 645, 534, 43495, 412, 7715, 2414, 3601, 51708], "temperature": 0.0, "avg_logprob": -0.08145880131494432, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.002045099390670657}, {"id": 1255, "seek": 756840, "start": 7568.4, "end": 7574.48, "text": " into pupils. So maybe you had to go to school for 15 years to learn geometry. As maybe today,", "tokens": [50364, 666, 38404, 13, 407, 1310, 291, 632, 281, 352, 281, 1395, 337, 2119, 924, 281, 1466, 18426, 13, 1018, 1310, 965, 11, 50668], "temperature": 0.0, "avg_logprob": -0.0953127440585885, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0018554558046162128}, {"id": 1256, "seek": 756840, "start": 7574.48, "end": 7578.719999999999, "text": " you can learn that geometry in a kind of really well designed course that just last three years.", "tokens": [50668, 291, 393, 1466, 300, 18426, 294, 257, 733, 295, 534, 731, 4761, 1164, 300, 445, 1036, 1045, 924, 13, 50880], "temperature": 0.0, "avg_logprob": -0.0953127440585885, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0018554558046162128}, {"id": 1257, "seek": 756840, "start": 7579.759999999999, "end": 7587.2, "text": " Nice. That's a good analogy. Okay. So basically you have three kind of buttons to push to get", "tokens": [50932, 5490, 13, 663, 311, 257, 665, 21663, 13, 1033, 13, 407, 1936, 291, 362, 1045, 733, 295, 9905, 281, 2944, 281, 483, 51304], "temperature": 0.0, "avg_logprob": -0.0953127440585885, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0018554558046162128}, {"id": 1258, "seek": 756840, "start": 7587.2, "end": 7594.32, "text": " more effective compute. So one is chips, just like number. Another is how good the chips are", "tokens": [51304, 544, 4942, 14722, 13, 407, 472, 307, 11583, 11, 445, 411, 1230, 13, 3996, 307, 577, 665, 264, 11583, 366, 51660], "temperature": 0.0, "avg_logprob": -0.0953127440585885, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0018554558046162128}, {"id": 1259, "seek": 759432, "start": 7594.4, "end": 7600.799999999999, "text": " at processing. And then the third is algorithms. So how good are the programs that get run on the", "tokens": [50368, 412, 9007, 13, 400, 550, 264, 2636, 307, 14642, 13, 407, 577, 665, 366, 264, 4268, 300, 483, 1190, 322, 264, 50688], "temperature": 0.0, "avg_logprob": -0.10982265267320858, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.00046933229896239936}, {"id": 1260, "seek": 759432, "start": 7600.799999999999, "end": 7608.32, "text": " chips? And have you basically tried to predict how quickly each of those things will go up?", "tokens": [50688, 11583, 30, 400, 362, 291, 1936, 3031, 281, 6069, 577, 2661, 1184, 295, 729, 721, 486, 352, 493, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10982265267320858, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.00046933229896239936}, {"id": 1261, "seek": 759432, "start": 7608.32, "end": 7616.32, "text": " Exactly. So my starting point is looking at what's going on today. Epoch is a kind of research", "tokens": [51064, 7587, 13, 407, 452, 2891, 935, 307, 1237, 412, 437, 311, 516, 322, 965, 13, 462, 2259, 339, 307, 257, 733, 295, 2132, 51464], "temperature": 0.0, "avg_logprob": -0.10982265267320858, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.00046933229896239936}, {"id": 1262, "seek": 759432, "start": 7616.32, "end": 7622.08, "text": " organization which I think has done the best research into this that I'm aware of. And they're", "tokens": [51464, 4475, 597, 286, 519, 575, 1096, 264, 1151, 2132, 666, 341, 300, 286, 478, 3650, 295, 13, 400, 436, 434, 51752], "temperature": 0.0, "avg_logprob": -0.10982265267320858, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.00046933229896239936}, {"id": 1263, "seek": 762208, "start": 7622.08, "end": 7628.64, "text": " looking at trends in all of the three quantities, which I just described to you. So firstly, they're", "tokens": [50364, 1237, 412, 13892, 294, 439, 295, 264, 1045, 22927, 11, 597, 286, 445, 7619, 281, 291, 13, 407, 27376, 11, 436, 434, 50692], "temperature": 0.0, "avg_logprob": -0.08458211965728224, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.003354558488354087}, {"id": 1264, "seek": 762208, "start": 7628.64, "end": 7633.6, "text": " looking at how much more is being spent on the biggest training runs in each year over the last", "tokens": [50692, 1237, 412, 577, 709, 544, 307, 885, 4418, 322, 264, 3880, 3097, 6676, 294, 1184, 1064, 670, 264, 1036, 50940], "temperature": 0.0, "avg_logprob": -0.08458211965728224, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.003354558488354087}, {"id": 1265, "seek": 762208, "start": 7633.6, "end": 7639.12, "text": " 10 years. And they're finding about three times more each year. Did you find that surprising?", "tokens": [50940, 1266, 924, 13, 400, 436, 434, 5006, 466, 1045, 1413, 544, 1184, 1064, 13, 2589, 291, 915, 300, 8830, 30, 51216], "temperature": 0.0, "avg_logprob": -0.08458211965728224, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.003354558488354087}, {"id": 1266, "seek": 762208, "start": 7640.5599999999995, "end": 7644.8, "text": " I know that training runs have been getting much more expensive in the last 10 years. So wasn't", "tokens": [51288, 286, 458, 300, 3097, 6676, 362, 668, 1242, 709, 544, 5124, 294, 264, 1036, 1266, 924, 13, 407, 2067, 380, 51500], "temperature": 0.0, "avg_logprob": -0.08458211965728224, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.003354558488354087}, {"id": 1267, "seek": 762208, "start": 7644.8, "end": 7649.6, "text": " that surprising for me? So there has been a big increase over the last decade. Yeah. This is", "tokens": [51500, 300, 8830, 337, 385, 30, 407, 456, 575, 668, 257, 955, 3488, 670, 264, 1036, 10378, 13, 865, 13, 639, 307, 51740], "temperature": 0.0, "avg_logprob": -0.08458211965728224, "compression_ratio": 1.800751879699248, "no_speech_prob": 0.003354558488354087}, {"id": 1268, "seek": 764960, "start": 7649.6, "end": 7656.4800000000005, "text": " making me realize I should have asked earlier, what is a training run? Is it actually just like", "tokens": [50364, 1455, 385, 4325, 286, 820, 362, 2351, 3071, 11, 437, 307, 257, 3097, 1190, 30, 1119, 309, 767, 445, 411, 50708], "temperature": 0.0, "avg_logprob": -0.10280463099479675, "compression_ratio": 1.5625, "no_speech_prob": 0.003944558557122946}, {"id": 1269, "seek": 764960, "start": 7656.4800000000005, "end": 7662.160000000001, "text": " insane amounts of data about, I don't know, like what everything that's on the internet you're", "tokens": [50708, 10838, 11663, 295, 1412, 466, 11, 286, 500, 380, 458, 11, 411, 437, 1203, 300, 311, 322, 264, 4705, 291, 434, 50992], "temperature": 0.0, "avg_logprob": -0.10280463099479675, "compression_ratio": 1.5625, "no_speech_prob": 0.003944558557122946}, {"id": 1270, "seek": 764960, "start": 7662.160000000001, "end": 7668.8, "text": " giving to GPT and being like, figure out how to predict the next word. And it takes like,", "tokens": [50992, 2902, 281, 26039, 51, 293, 885, 411, 11, 2573, 484, 577, 281, 6069, 264, 958, 1349, 13, 400, 309, 2516, 411, 11, 51324], "temperature": 0.0, "avg_logprob": -0.10280463099479675, "compression_ratio": 1.5625, "no_speech_prob": 0.003944558557122946}, {"id": 1271, "seek": 764960, "start": 7669.52, "end": 7675.92, "text": " does it take weeks? Or does it take less but just tons of compute? So I think it takes months.", "tokens": [51360, 775, 309, 747, 3259, 30, 1610, 775, 309, 747, 1570, 457, 445, 9131, 295, 14722, 30, 407, 286, 519, 309, 2516, 2493, 13, 51680], "temperature": 0.0, "avg_logprob": -0.10280463099479675, "compression_ratio": 1.5625, "no_speech_prob": 0.003944558557122946}, {"id": 1272, "seek": 767592, "start": 7676.4800000000005, "end": 7684.4800000000005, "text": " Okay. So it's a bit like you give GPT the start of some kind of web page, the first 50 words,", "tokens": [50392, 1033, 13, 407, 309, 311, 257, 857, 411, 291, 976, 26039, 51, 264, 722, 295, 512, 733, 295, 3670, 3028, 11, 264, 700, 2625, 2283, 11, 50792], "temperature": 0.0, "avg_logprob": -0.10274687767028809, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0013390014646574855}, {"id": 1273, "seek": 767592, "start": 7684.4800000000005, "end": 7691.36, "text": " and you say, predict the 51st word. And in making that prediction, GPT is going to do a large number", "tokens": [50792, 293, 291, 584, 11, 6069, 264, 18485, 372, 1349, 13, 400, 294, 1455, 300, 17630, 11, 26039, 51, 307, 516, 281, 360, 257, 2416, 1230, 51136], "temperature": 0.0, "avg_logprob": -0.10274687767028809, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0013390014646574855}, {"id": 1274, "seek": 767592, "start": 7691.36, "end": 7699.36, "text": " of calculations. Let's say it's doing 300 million calculations in order to predict that next word.", "tokens": [51136, 295, 20448, 13, 961, 311, 584, 309, 311, 884, 6641, 2459, 20448, 294, 1668, 281, 6069, 300, 958, 1349, 13, 51536], "temperature": 0.0, "avg_logprob": -0.10274687767028809, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0013390014646574855}, {"id": 1275, "seek": 767592, "start": 7700.16, "end": 7705.28, "text": " So that's already a lot of calculations. Right. But you ask it to do that same task", "tokens": [51576, 407, 300, 311, 1217, 257, 688, 295, 20448, 13, 1779, 13, 583, 291, 1029, 309, 281, 360, 300, 912, 5633, 51832], "temperature": 0.0, "avg_logprob": -0.10274687767028809, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0013390014646574855}, {"id": 1276, "seek": 770592, "start": 7706.32, "end": 7711.68, "text": " let's say 10 trillion times, because you get it to predict the 51st word, and then you're like,", "tokens": [50384, 718, 311, 584, 1266, 18723, 1413, 11, 570, 291, 483, 309, 281, 6069, 264, 18485, 372, 1349, 11, 293, 550, 291, 434, 411, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09282492027908075, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.0009638220653869212}, {"id": 1277, "seek": 770592, "start": 7711.68, "end": 7717.92, "text": " now predict the 52nd word. And it does another 300 million calculations and predicts that", "tokens": [50652, 586, 6069, 264, 18079, 273, 1349, 13, 400, 309, 775, 1071, 6641, 2459, 20448, 293, 6069, 82, 300, 50964], "temperature": 0.0, "avg_logprob": -0.09282492027908075, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.0009638220653869212}, {"id": 1278, "seek": 770592, "start": 7717.92, "end": 7722.88, "text": " 57th word. And you keep doing it until you've literally done it, like I said, 10 trillion times", "tokens": [50964, 21423, 392, 1349, 13, 400, 291, 1066, 884, 309, 1826, 291, 600, 3736, 1096, 309, 11, 411, 286, 848, 11, 1266, 18723, 1413, 51212], "temperature": 0.0, "avg_logprob": -0.09282492027908075, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.0009638220653869212}, {"id": 1279, "seek": 770592, "start": 7722.88, "end": 7727.84, "text": " for different words on the internet. Got it. And actually, you're actually doing, it's actually", "tokens": [51212, 337, 819, 2283, 322, 264, 4705, 13, 5803, 309, 13, 400, 767, 11, 291, 434, 767, 884, 11, 309, 311, 767, 51460], "temperature": 0.0, "avg_logprob": -0.09282492027908075, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.0009638220653869212}, {"id": 1280, "seek": 770592, "start": 7727.84, "end": 7733.6, "text": " doing like kind of millions of examples at once. So you're kind of, that's how you can make it a", "tokens": [51460, 884, 411, 733, 295, 6803, 295, 5110, 412, 1564, 13, 407, 291, 434, 733, 295, 11, 300, 311, 577, 291, 393, 652, 309, 257, 51748], "temperature": 0.0, "avg_logprob": -0.09282492027908075, "compression_ratio": 1.7954545454545454, "no_speech_prob": 0.0009638220653869212}, {"id": 1281, "seek": 773360, "start": 7733.6, "end": 7738.320000000001, "text": " little bit faster. So otherwise it would be taking years, but we can do it in, we can do it in months", "tokens": [50364, 707, 857, 4663, 13, 407, 5911, 309, 576, 312, 1940, 924, 11, 457, 321, 393, 360, 309, 294, 11, 321, 393, 360, 309, 294, 2493, 50600], "temperature": 0.0, "avg_logprob": -0.11673205982555043, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.002820446388795972}, {"id": 1282, "seek": 773360, "start": 7738.320000000001, "end": 7742.320000000001, "text": " because you can get it to, you know, predict multiple different articles at the same time", "tokens": [50600, 570, 291, 393, 483, 309, 281, 11, 291, 458, 11, 6069, 3866, 819, 11290, 412, 264, 912, 565, 50800], "temperature": 0.0, "avg_logprob": -0.11673205982555043, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.002820446388795972}, {"id": 1283, "seek": 773360, "start": 7743.360000000001, "end": 7746.320000000001, "text": " to speed things up. And that's why you're using so many different computers.", "tokens": [50852, 281, 3073, 721, 493, 13, 400, 300, 311, 983, 291, 434, 1228, 370, 867, 819, 10807, 13, 51000], "temperature": 0.0, "avg_logprob": -0.11673205982555043, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.002820446388795972}, {"id": 1284, "seek": 773360, "start": 7747.280000000001, "end": 7754.400000000001, "text": " Okay, so that takes us to compute. And companies have been spending about three times as much", "tokens": [51048, 1033, 11, 370, 300, 2516, 505, 281, 14722, 13, 400, 3431, 362, 668, 6434, 466, 1045, 1413, 382, 709, 51404], "temperature": 0.0, "avg_logprob": -0.11673205982555043, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.002820446388795972}, {"id": 1285, "seek": 773360, "start": 7754.400000000001, "end": 7761.6, "text": " on computer chips, or yeah, whatever that equivalent is, every year for the past decade.", "tokens": [51404, 322, 3820, 11583, 11, 420, 1338, 11, 2035, 300, 10344, 307, 11, 633, 1064, 337, 264, 1791, 10378, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11673205982555043, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.002820446388795972}, {"id": 1286, "seek": 776160, "start": 7762.160000000001, "end": 7767.84, "text": " Ten years, yeah. Okay, cool. So they've been using, they've been spending three times as much", "tokens": [50392, 9380, 924, 11, 1338, 13, 1033, 11, 1627, 13, 407, 436, 600, 668, 1228, 11, 436, 600, 668, 6434, 1045, 1413, 382, 709, 50676], "temperature": 0.0, "avg_logprob": -0.1208739422335483, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.004898102954030037}, {"id": 1287, "seek": 776160, "start": 7767.84, "end": 7772.08, "text": " on the chips that they use for these big training runs. So they may, they may not have", "tokens": [50676, 322, 264, 11583, 300, 436, 764, 337, 613, 955, 3097, 6676, 13, 407, 436, 815, 11, 436, 815, 406, 362, 50888], "temperature": 0.0, "avg_logprob": -0.1208739422335483, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.004898102954030037}, {"id": 1288, "seek": 776160, "start": 7772.08, "end": 7776.64, "text": " been increasing their total spending on chips by as much. For example, Google may just be", "tokens": [50888, 668, 5662, 641, 3217, 6434, 322, 11583, 538, 382, 709, 13, 1171, 1365, 11, 3329, 815, 445, 312, 51116], "temperature": 0.0, "avg_logprob": -0.1208739422335483, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.004898102954030037}, {"id": 1289, "seek": 776160, "start": 7776.64, "end": 7780.08, "text": " using a bigger fraction of its chips for these training runs over time.", "tokens": [51116, 1228, 257, 3801, 14135, 295, 1080, 11583, 337, 613, 3097, 6676, 670, 565, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1208739422335483, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.004898102954030037}, {"id": 1290, "seek": 776160, "start": 7780.64, "end": 7785.76, "text": " Yeah. Okay. And what are the trends in computer chip quality?", "tokens": [51316, 865, 13, 1033, 13, 400, 437, 366, 264, 13892, 294, 3820, 11409, 3125, 30, 51572], "temperature": 0.0, "avg_logprob": -0.1208739422335483, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.004898102954030037}, {"id": 1291, "seek": 778576, "start": 7786.64, "end": 7790.320000000001, "text": " So I already mentioned this one, each, each two and a half years,", "tokens": [50408, 407, 286, 1217, 2835, 341, 472, 11, 1184, 11, 1184, 732, 293, 257, 1922, 924, 11, 50592], "temperature": 0.0, "avg_logprob": -0.10938669020129789, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0025475984439253807}, {"id": 1292, "seek": 778576, "start": 7790.320000000001, "end": 7795.04, "text": " the price of compute halves. So that means, you know, if you're spending a constant amount", "tokens": [50592, 264, 3218, 295, 14722, 38490, 13, 407, 300, 1355, 11, 291, 458, 11, 498, 291, 434, 6434, 257, 5754, 2372, 50828], "temperature": 0.0, "avg_logprob": -0.10938669020129789, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0025475984439253807}, {"id": 1293, "seek": 778576, "start": 7795.04, "end": 7800.56, "text": " on chips, then each two and a half years, you'll be able to buy twice as much compute.", "tokens": [50828, 322, 11583, 11, 550, 1184, 732, 293, 257, 1922, 924, 11, 291, 603, 312, 1075, 281, 2256, 6091, 382, 709, 14722, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10938669020129789, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0025475984439253807}, {"id": 1294, "seek": 778576, "start": 7800.56, "end": 7805.76, "text": " Right, cool. And you said that the efficiency of algorithms is doubling about every 15 months.", "tokens": [51104, 1779, 11, 1627, 13, 400, 291, 848, 300, 264, 10493, 295, 14642, 307, 33651, 466, 633, 2119, 2493, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10938669020129789, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0025475984439253807}, {"id": 1295, "seek": 778576, "start": 7806.400000000001, "end": 7808.400000000001, "text": " Yeah. How do, how do these trends fit together?", "tokens": [51396, 865, 13, 1012, 360, 11, 577, 360, 613, 13892, 3318, 1214, 30, 51496], "temperature": 0.0, "avg_logprob": -0.10938669020129789, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0025475984439253807}, {"id": 1296, "seek": 778576, "start": 7809.12, "end": 7814.64, "text": " So, you know, if you combine all of those trends together, then the result is that the effect", "tokens": [51532, 407, 11, 291, 458, 11, 498, 291, 10432, 439, 295, 729, 13892, 1214, 11, 550, 264, 1874, 307, 300, 264, 1802, 51808], "temperature": 0.0, "avg_logprob": -0.10938669020129789, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.0025475984439253807}, {"id": 1297, "seek": 781464, "start": 7814.72, "end": 7821.200000000001, "text": " of compute on the largest training run has been increasing by a factor of 10 every year.", "tokens": [50368, 295, 14722, 322, 264, 6443, 3097, 1190, 575, 668, 5662, 538, 257, 5952, 295, 1266, 633, 1064, 13, 50692], "temperature": 0.0, "avg_logprob": -0.09414507548014323, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0019814230035990477}, {"id": 1298, "seek": 781464, "start": 7822.0, "end": 7826.8, "text": " So you've got something like a default improvement year by year of 10x.", "tokens": [50732, 407, 291, 600, 658, 746, 411, 257, 7576, 10444, 1064, 538, 1064, 295, 1266, 87, 13, 50972], "temperature": 0.0, "avg_logprob": -0.09414507548014323, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0019814230035990477}, {"id": 1299, "seek": 781464, "start": 7827.4400000000005, "end": 7832.08, "text": " What happens when you then try to adjust those numbers for the fact that things are", "tokens": [51004, 708, 2314, 562, 291, 550, 853, 281, 4369, 729, 3547, 337, 264, 1186, 300, 721, 366, 51236], "temperature": 0.0, "avg_logprob": -0.09414507548014323, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0019814230035990477}, {"id": 1300, "seek": 781464, "start": 7832.8, "end": 7834.96, "text": " changing over time? I'd guess accelerating.", "tokens": [51272, 4473, 670, 565, 30, 286, 1116, 2041, 34391, 13, 51380], "temperature": 0.0, "avg_logprob": -0.09414507548014323, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0019814230035990477}, {"id": 1301, "seek": 781464, "start": 7835.68, "end": 7839.360000000001, "text": " That's right. So we could take, we could take those three quantities one by one.", "tokens": [51416, 663, 311, 558, 13, 407, 321, 727, 747, 11, 321, 727, 747, 729, 1045, 22927, 472, 538, 472, 13, 51600], "temperature": 0.0, "avg_logprob": -0.09414507548014323, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0019814230035990477}, {"id": 1302, "seek": 783936, "start": 7840.08, "end": 7846.799999999999, "text": " So in terms of the money spent, I think that could go either way. So one possibility,", "tokens": [50400, 407, 294, 2115, 295, 264, 1460, 4418, 11, 286, 519, 300, 727, 352, 2139, 636, 13, 407, 472, 7959, 11, 50736], "temperature": 0.0, "avg_logprob": -0.08795588739802328, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00701378658413887}, {"id": 1303, "seek": 783936, "start": 7846.799999999999, "end": 7854.16, "text": " a scary possibility, is that AI companies develop this AI that can, you know, automate 20%", "tokens": [50736, 257, 6958, 7959, 11, 307, 300, 7318, 3431, 1499, 341, 7318, 300, 393, 11, 291, 458, 11, 31605, 945, 4, 51104], "temperature": 0.0, "avg_logprob": -0.08795588739802328, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00701378658413887}, {"id": 1304, "seek": 783936, "start": 7854.16, "end": 7858.96, "text": " of cognitive tasks in the economy. And they're like, man, this can make us loads of money.", "tokens": [51104, 295, 15605, 9608, 294, 264, 5010, 13, 400, 436, 434, 411, 11, 587, 11, 341, 393, 652, 505, 12668, 295, 1460, 13, 51344], "temperature": 0.0, "avg_logprob": -0.08795588739802328, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00701378658413887}, {"id": 1305, "seek": 783936, "start": 7859.599999999999, "end": 7866.16, "text": " Investment flows in and they're able to very quickly spend even more on training runs,", "tokens": [51376, 43427, 12867, 294, 293, 436, 434, 1075, 281, 588, 2661, 3496, 754, 544, 322, 3097, 6676, 11, 51704], "temperature": 0.0, "avg_logprob": -0.08795588739802328, "compression_ratio": 1.5594713656387664, "no_speech_prob": 0.00701378658413887}, {"id": 1306, "seek": 786616, "start": 7866.24, "end": 7873.44, "text": " or maybe just use a bigger fraction of existing chips in the world. So maybe Amazon's like,", "tokens": [50368, 420, 1310, 445, 764, 257, 3801, 14135, 295, 6741, 11583, 294, 264, 1002, 13, 407, 1310, 6795, 311, 411, 11, 50728], "temperature": 0.0, "avg_logprob": -0.10106912681034633, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.003934432286769152}, {"id": 1307, "seek": 786616, "start": 7873.44, "end": 7877.599999999999, "text": " well, we've got all this compute that we were previously using on like these web services.", "tokens": [50728, 731, 11, 321, 600, 658, 439, 341, 14722, 300, 321, 645, 8046, 1228, 322, 411, 613, 3670, 3328, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10106912681034633, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.003934432286769152}, {"id": 1308, "seek": 786616, "start": 7877.599999999999, "end": 7882.8, "text": " But actually, given how lucrative this AI stuff looks like, why don't we team up with an AI lab", "tokens": [50936, 583, 767, 11, 2212, 577, 21296, 30457, 341, 7318, 1507, 1542, 411, 11, 983, 500, 380, 321, 1469, 493, 365, 364, 7318, 2715, 51196], "temperature": 0.0, "avg_logprob": -0.10106912681034633, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.003934432286769152}, {"id": 1309, "seek": 786616, "start": 7882.8, "end": 7887.92, "text": " and let them use our compute that we've just got doing the stuff that isn't that economically", "tokens": [51196, 293, 718, 552, 764, 527, 14722, 300, 321, 600, 445, 658, 884, 264, 1507, 300, 1943, 380, 300, 26811, 51452], "temperature": 0.0, "avg_logprob": -0.10106912681034633, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.003934432286769152}, {"id": 1310, "seek": 786616, "start": 7887.92, "end": 7892.72, "text": " valuable, instead use it to train an even better AI. That is a very scary possibility.", "tokens": [51452, 8263, 11, 2602, 764, 309, 281, 3847, 364, 754, 1101, 7318, 13, 663, 307, 257, 588, 6958, 7959, 13, 51692], "temperature": 0.0, "avg_logprob": -0.10106912681034633, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.003934432286769152}, {"id": 1311, "seek": 789272, "start": 7893.68, "end": 7899.84, "text": " A more optimistic possibility could be that by the time we get to the 20% AI, we're already", "tokens": [50412, 316, 544, 19397, 7959, 727, 312, 300, 538, 264, 565, 321, 483, 281, 264, 945, 4, 7318, 11, 321, 434, 1217, 50720], "temperature": 0.0, "avg_logprob": -0.1102579048088005, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.005538084078580141}, {"id": 1312, "seek": 789272, "start": 7899.84, "end": 7906.08, "text": " spending loads and loads on these training runs. Maybe we're already spending $100 billion. Maybe", "tokens": [50720, 6434, 12668, 293, 12668, 322, 613, 3097, 6676, 13, 2704, 321, 434, 1217, 6434, 1848, 6879, 5218, 13, 2704, 51032], "temperature": 0.0, "avg_logprob": -0.1102579048088005, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.005538084078580141}, {"id": 1313, "seek": 789272, "start": 7906.08, "end": 7910.8, "text": " we're already using all of Amazon's chips because we, at some earlier point, we already teamed up", "tokens": [51032, 321, 434, 1217, 1228, 439, 295, 6795, 311, 11583, 570, 321, 11, 412, 512, 3071, 935, 11, 321, 1217, 47426, 493, 51268], "temperature": 0.0, "avg_logprob": -0.1102579048088005, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.005538084078580141}, {"id": 1314, "seek": 789272, "start": 7910.8, "end": 7916.08, "text": " with them. And that would be, that would mean that it wasn't possible to keep increasing the", "tokens": [51268, 365, 552, 13, 400, 300, 576, 312, 11, 300, 576, 914, 300, 309, 2067, 380, 1944, 281, 1066, 5662, 264, 51532], "temperature": 0.0, "avg_logprob": -0.1102579048088005, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.005538084078580141}, {"id": 1315, "seek": 789272, "start": 7916.08, "end": 7919.84, "text": " amount of money spent on these training runs year on year. So you could have actually slower", "tokens": [51532, 2372, 295, 1460, 4418, 322, 613, 3097, 6676, 1064, 322, 1064, 13, 407, 291, 727, 362, 767, 14009, 51720], "temperature": 0.0, "avg_logprob": -0.1102579048088005, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.005538084078580141}, {"id": 1316, "seek": 791984, "start": 7919.92, "end": 7926.72, "text": " growth than the recent kind of 3x per year pattern. It could be much slower. So that is a major", "tokens": [50368, 4599, 813, 264, 5162, 733, 295, 805, 87, 680, 1064, 5102, 13, 467, 727, 312, 709, 14009, 13, 407, 300, 307, 257, 2563, 50708], "temperature": 0.0, "avg_logprob": -0.14333342446221245, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.003927743528038263}, {"id": 1317, "seek": 791984, "start": 7926.72, "end": 7932.88, "text": " source of uncertainty in these takeoff addictions is just the thing that's particularly scary about", "tokens": [50708, 4009, 295, 15697, 294, 613, 747, 4506, 909, 15607, 307, 445, 264, 551, 300, 311, 4098, 6958, 466, 51016], "temperature": 0.0, "avg_logprob": -0.14333342446221245, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.003927743528038263}, {"id": 1318, "seek": 791984, "start": 7932.88, "end": 7938.88, "text": " the short timelines possibility is that maybe you can get to the 20% AI with just spending a billion", "tokens": [51016, 264, 2099, 45886, 7959, 307, 300, 1310, 291, 393, 483, 281, 264, 945, 4, 7318, 365, 445, 6434, 257, 5218, 51316], "temperature": 0.0, "avg_logprob": -0.14333342446221245, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.003927743528038263}, {"id": 1319, "seek": 791984, "start": 7938.88, "end": 7945.68, "text": " on a training run. That would leave plenty of scope to spend 10 or 100 times as much", "tokens": [51316, 322, 257, 3097, 1190, 13, 663, 576, 1856, 7140, 295, 11923, 281, 3496, 1266, 420, 2319, 1413, 382, 709, 51656], "temperature": 0.0, "avg_logprob": -0.14333342446221245, "compression_ratio": 1.5362903225806452, "no_speech_prob": 0.003927743528038263}, {"id": 1320, "seek": 794568, "start": 7946.320000000001, "end": 7950.8, "text": " very soon after on a bigger training run, which would be hugely risky.", "tokens": [50396, 588, 2321, 934, 322, 257, 3801, 3097, 1190, 11, 597, 576, 312, 27417, 21137, 13, 50620], "temperature": 0.0, "avg_logprob": -0.07002376374744233, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.006230468861758709}, {"id": 1321, "seek": 794568, "start": 7951.4400000000005, "end": 7956.320000000001, "text": " Okay, so it sounds like the number of chips that we might be using for training runs", "tokens": [50652, 1033, 11, 370, 309, 3263, 411, 264, 1230, 295, 11583, 300, 321, 1062, 312, 1228, 337, 3097, 6676, 50896], "temperature": 0.0, "avg_logprob": -0.07002376374744233, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.006230468861758709}, {"id": 1322, "seek": 794568, "start": 7957.12, "end": 7961.84, "text": " might be growing faster or it might be growing slower by the time we're at systems that can", "tokens": [50936, 1062, 312, 4194, 4663, 420, 309, 1062, 312, 4194, 14009, 538, 264, 565, 321, 434, 412, 3652, 300, 393, 51172], "temperature": 0.0, "avg_logprob": -0.07002376374744233, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.006230468861758709}, {"id": 1323, "seek": 794568, "start": 7961.84, "end": 7967.76, "text": " perform 20% of tasks. What do you expect to happen with the quality of chips and with the quality", "tokens": [51172, 2042, 945, 4, 295, 9608, 13, 708, 360, 291, 2066, 281, 1051, 365, 264, 3125, 295, 11583, 293, 365, 264, 3125, 51468], "temperature": 0.0, "avg_logprob": -0.07002376374744233, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.006230468861758709}, {"id": 1324, "seek": 794568, "start": 7967.76, "end": 7974.0, "text": " of algorithms? So I think with those, it's easier to predict that their improvement should be faster", "tokens": [51468, 295, 14642, 30, 407, 286, 519, 365, 729, 11, 309, 311, 3571, 281, 6069, 300, 641, 10444, 820, 312, 4663, 51780], "temperature": 0.0, "avg_logprob": -0.07002376374744233, "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.006230468861758709}, {"id": 1325, "seek": 797400, "start": 7974.0, "end": 7979.2, "text": " once we're at the point that AI can perform 20% of cognitive tasks. And that's just due to the", "tokens": [50364, 1564, 321, 434, 412, 264, 935, 300, 7318, 393, 2042, 945, 4, 295, 15605, 9608, 13, 400, 300, 311, 445, 3462, 281, 264, 50624], "temperature": 0.0, "avg_logprob": -0.10036393078890714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.009190475568175316}, {"id": 1326, "seek": 797400, "start": 7979.2, "end": 7984.64, "text": " dynamic that I've been referring to quite a lot during our conversation, which is that", "tokens": [50624, 8546, 300, 286, 600, 668, 13761, 281, 1596, 257, 688, 1830, 527, 3761, 11, 597, 307, 300, 50896], "temperature": 0.0, "avg_logprob": -0.10036393078890714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.009190475568175316}, {"id": 1327, "seek": 797400, "start": 7984.64, "end": 7990.32, "text": " one of the things we're going to use AI's to do is to do the task of designing better chips", "tokens": [50896, 472, 295, 264, 721, 321, 434, 516, 281, 764, 7318, 311, 281, 360, 307, 281, 360, 264, 5633, 295, 14685, 1101, 11583, 51180], "temperature": 0.0, "avg_logprob": -0.10036393078890714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.009190475568175316}, {"id": 1328, "seek": 797400, "start": 7990.32, "end": 7995.2, "text": " and to do the task of designing better algorithms. And there are already examples of that happening.", "tokens": [51180, 293, 281, 360, 264, 5633, 295, 14685, 1101, 14642, 13, 400, 456, 366, 1217, 5110, 295, 300, 2737, 13, 51424], "temperature": 0.0, "avg_logprob": -0.10036393078890714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.009190475568175316}, {"id": 1329, "seek": 797400, "start": 7995.2, "end": 8002.4, "text": " AI's that are using deep learning techniques to kind of cram transistors more efficiently", "tokens": [51424, 7318, 311, 300, 366, 1228, 2452, 2539, 7512, 281, 733, 295, 941, 335, 1145, 46976, 544, 19621, 51784], "temperature": 0.0, "avg_logprob": -0.10036393078890714, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.009190475568175316}, {"id": 1330, "seek": 800240, "start": 8002.4, "end": 8009.92, "text": " into computer chips. And again, with the algorithms, we are already seeing AI help significantly", "tokens": [50364, 666, 3820, 11583, 13, 400, 797, 11, 365, 264, 14642, 11, 321, 366, 1217, 2577, 7318, 854, 10591, 50740], "temperature": 0.0, "avg_logprob": -0.10494322362153427, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0008368584094569087}, {"id": 1331, "seek": 800240, "start": 8010.639999999999, "end": 8017.44, "text": " with some coding tasks with things like co-pilot. And so by the time we're at 20%, AI expect that", "tokens": [50776, 365, 512, 17720, 9608, 365, 721, 411, 598, 12, 79, 31516, 13, 400, 370, 538, 264, 565, 321, 434, 412, 945, 8923, 7318, 2066, 300, 51116], "temperature": 0.0, "avg_logprob": -0.10494322362153427, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0008368584094569087}, {"id": 1332, "seek": 800240, "start": 8017.44, "end": 8024.32, "text": " effect to be much larger than it even is today. And so I think it's fairly, you know, the directional", "tokens": [51116, 1802, 281, 312, 709, 4833, 813, 309, 754, 307, 965, 13, 400, 370, 286, 519, 309, 311, 6457, 11, 291, 458, 11, 264, 42242, 51460], "temperature": 0.0, "avg_logprob": -0.10494322362153427, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0008368584094569087}, {"id": 1333, "seek": 800240, "start": 8024.32, "end": 8028.0, "text": " prediction is fairly straightforward that we should expect both the quality of chips and the", "tokens": [51460, 17630, 307, 6457, 15325, 300, 321, 820, 2066, 1293, 264, 3125, 295, 11583, 293, 264, 51644], "temperature": 0.0, "avg_logprob": -0.10494322362153427, "compression_ratio": 1.6141078838174274, "no_speech_prob": 0.0008368584094569087}, {"id": 1334, "seek": 802800, "start": 8028.0, "end": 8034.16, "text": " quality of algorithms to be going faster once we get to 20% AI than they are today.", "tokens": [50364, 3125, 295, 14642, 281, 312, 516, 4663, 1564, 321, 483, 281, 945, 4, 7318, 813, 436, 366, 965, 13, 50672], "temperature": 0.0, "avg_logprob": -0.08866680145263672, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0015123211778700352}, {"id": 1335, "seek": 802800, "start": 8035.04, "end": 8040.16, "text": " And it's really hard to predict exactly how much faster or what the change is going to be,", "tokens": [50716, 400, 309, 311, 534, 1152, 281, 6069, 2293, 577, 709, 4663, 420, 437, 264, 1319, 307, 516, 281, 312, 11, 50972], "temperature": 0.0, "avg_logprob": -0.08866680145263672, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0015123211778700352}, {"id": 1336, "seek": 802800, "start": 8040.16, "end": 8044.96, "text": " because you've also got people talking about the end of Moore's law. And it's just hard to", "tokens": [50972, 570, 291, 600, 611, 658, 561, 1417, 466, 264, 917, 295, 21644, 311, 2101, 13, 400, 309, 311, 445, 1152, 281, 51212], "temperature": 0.0, "avg_logprob": -0.08866680145263672, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0015123211778700352}, {"id": 1337, "seek": 802800, "start": 8044.96, "end": 8048.96, "text": " anticipate the specific improvements that we might be making with chips and with algorithms.", "tokens": [51212, 21685, 264, 2685, 13797, 300, 321, 1062, 312, 1455, 365, 11583, 293, 365, 14642, 13, 51412], "temperature": 0.0, "avg_logprob": -0.08866680145263672, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0015123211778700352}, {"id": 1338, "seek": 802800, "start": 8048.96, "end": 8051.52, "text": " Right. And can you quickly explain what Moore's law is?", "tokens": [51412, 1779, 13, 400, 393, 291, 2661, 2903, 437, 21644, 311, 2101, 307, 30, 51540], "temperature": 0.0, "avg_logprob": -0.08866680145263672, "compression_ratio": 1.6108949416342413, "no_speech_prob": 0.0015123211778700352}, {"id": 1339, "seek": 805152, "start": 8052.320000000001, "end": 8058.96, "text": " Yeah, for a long time, the way that computer chips have got more efficient is by cramming", "tokens": [50404, 865, 11, 337, 257, 938, 565, 11, 264, 636, 300, 3820, 11583, 362, 658, 544, 7148, 307, 538, 941, 335, 2810, 50736], "temperature": 0.0, "avg_logprob": -0.0969726545316679, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.003081447910517454}, {"id": 1340, "seek": 805152, "start": 8058.96, "end": 8064.320000000001, "text": " more and more of these processing units onto each chip and making the processing units smaller and", "tokens": [50736, 544, 293, 544, 295, 613, 9007, 6815, 3911, 1184, 11409, 293, 1455, 264, 9007, 6815, 4356, 293, 51004], "temperature": 0.0, "avg_logprob": -0.0969726545316679, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.003081447910517454}, {"id": 1341, "seek": 805152, "start": 8064.320000000001, "end": 8069.120000000001, "text": " smaller. But at a certain point, people think you just won't be able to make them any smaller", "tokens": [51004, 4356, 13, 583, 412, 257, 1629, 935, 11, 561, 519, 291, 445, 1582, 380, 312, 1075, 281, 652, 552, 604, 4356, 51244], "temperature": 0.0, "avg_logprob": -0.0969726545316679, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.003081447910517454}, {"id": 1342, "seek": 805152, "start": 8069.120000000001, "end": 8074.96, "text": " because you're running against fundamental limits. Yeah. Yeah. Yeah. Okay. And have we started to hit", "tokens": [51244, 570, 291, 434, 2614, 1970, 8088, 10406, 13, 865, 13, 865, 13, 865, 13, 1033, 13, 400, 362, 321, 1409, 281, 2045, 51536], "temperature": 0.0, "avg_logprob": -0.0969726545316679, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.003081447910517454}, {"id": 1343, "seek": 805152, "start": 8074.96, "end": 8079.4400000000005, "text": " that limit? I think people acknowledge that it's starting to get much harder to make further", "tokens": [51536, 300, 4948, 30, 286, 519, 561, 10692, 300, 309, 311, 2891, 281, 483, 709, 6081, 281, 652, 3052, 51760], "temperature": 0.0, "avg_logprob": -0.0969726545316679, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.003081447910517454}, {"id": 1344, "seek": 807944, "start": 8079.44, "end": 8085.599999999999, "text": " improvements. But even then, there's a big open question about how much you can improve", "tokens": [50364, 13797, 13, 583, 754, 550, 11, 456, 311, 257, 955, 1269, 1168, 466, 577, 709, 291, 393, 3470, 50672], "temperature": 0.0, "avg_logprob": -0.0770258465591742, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.003079316345974803}, {"id": 1345, "seek": 807944, "start": 8085.599999999999, "end": 8090.24, "text": " chips in other ways. So, you know, just because the way we previously improved chips was to", "tokens": [50672, 11583, 294, 661, 2098, 13, 407, 11, 291, 458, 11, 445, 570, 264, 636, 321, 8046, 9689, 11583, 390, 281, 50904], "temperature": 0.0, "avg_logprob": -0.0770258465591742, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.003079316345974803}, {"id": 1346, "seek": 807944, "start": 8090.24, "end": 8094.08, "text": " make these processing units smaller, doesn't mean that there aren't going to be any other", "tokens": [50904, 652, 613, 9007, 6815, 4356, 11, 1177, 380, 914, 300, 456, 3212, 380, 516, 281, 312, 604, 661, 51096], "temperature": 0.0, "avg_logprob": -0.0770258465591742, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.003079316345974803}, {"id": 1347, "seek": 807944, "start": 8094.08, "end": 8097.839999999999, "text": " types of improvements. And in fact, recently, other types of improvements have become very", "tokens": [51096, 3467, 295, 13797, 13, 400, 294, 1186, 11, 3938, 11, 661, 3467, 295, 13797, 362, 1813, 588, 51284], "temperature": 0.0, "avg_logprob": -0.0770258465591742, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.003079316345974803}, {"id": 1348, "seek": 807944, "start": 8097.839999999999, "end": 8104.4, "text": " significant from generation to generation. For example, chips becoming specialized for", "tokens": [51284, 4776, 490, 5125, 281, 5125, 13, 1171, 1365, 11, 11583, 5617, 19813, 337, 51612], "temperature": 0.0, "avg_logprob": -0.0770258465591742, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.003079316345974803}, {"id": 1349, "seek": 810440, "start": 8104.4, "end": 8110.24, "text": " being used for deep learning calculations in particular. Oh, wow. And there's probably a", "tokens": [50364, 885, 1143, 337, 2452, 2539, 20448, 294, 1729, 13, 876, 11, 6076, 13, 400, 456, 311, 1391, 257, 50656], "temperature": 0.0, "avg_logprob": -0.08705986092943664, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.032101817429065704}, {"id": 1350, "seek": 810440, "start": 8110.24, "end": 8116.48, "text": " lot more games you can get from that kind of specialization. That kind of, okay. So, I guess", "tokens": [50656, 688, 544, 2813, 291, 393, 483, 490, 300, 733, 295, 2121, 2144, 13, 663, 733, 295, 11, 1392, 13, 407, 11, 286, 2041, 50968], "temperature": 0.0, "avg_logprob": -0.08705986092943664, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.032101817429065704}, {"id": 1351, "seek": 810440, "start": 8116.48, "end": 8122.24, "text": " we've got some data suggesting that maybe the way that we've been improving the quality of chips", "tokens": [50968, 321, 600, 658, 512, 1412, 18094, 300, 1310, 264, 636, 300, 321, 600, 668, 11470, 264, 3125, 295, 11583, 51256], "temperature": 0.0, "avg_logprob": -0.08705986092943664, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.032101817429065704}, {"id": 1352, "seek": 810440, "start": 8122.24, "end": 8127.599999999999, "text": " isn't going to keep making improvements at the rate that we've been making those improvements.", "tokens": [51256, 1943, 380, 516, 281, 1066, 1455, 13797, 412, 264, 3314, 300, 321, 600, 668, 1455, 729, 13797, 13, 51524], "temperature": 0.0, "avg_logprob": -0.08705986092943664, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.032101817429065704}, {"id": 1353, "seek": 810440, "start": 8127.599999999999, "end": 8132.24, "text": " But there are other improvements we can make. And so, we might still just really expect them", "tokens": [51524, 583, 456, 366, 661, 13797, 321, 393, 652, 13, 400, 370, 11, 321, 1062, 920, 445, 534, 2066, 552, 51756], "temperature": 0.0, "avg_logprob": -0.08705986092943664, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.032101817429065704}, {"id": 1354, "seek": 813224, "start": 8132.24, "end": 8138.16, "text": " to keep improving over time. Yeah, at least for another couple of decades would be my expectation.", "tokens": [50364, 281, 1066, 11470, 670, 565, 13, 865, 11, 412, 1935, 337, 1071, 1916, 295, 7878, 576, 312, 452, 14334, 13, 50660], "temperature": 0.0, "avg_logprob": -0.10402291720030737, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.0009186739334836602}, {"id": 1355, "seek": 813224, "start": 8138.719999999999, "end": 8142.24, "text": " Right. And on the time scales we're talking about, that's a pretty long time.", "tokens": [50688, 1779, 13, 400, 322, 264, 565, 17408, 321, 434, 1417, 466, 11, 300, 311, 257, 1238, 938, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10402291720030737, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.0009186739334836602}, {"id": 1356, "seek": 813224, "start": 8142.8, "end": 8147.599999999999, "text": " It is. It is. If you think AGI would be really hard to develop, maybe you can hope that Moore's", "tokens": [50892, 467, 307, 13, 467, 307, 13, 759, 291, 519, 316, 26252, 576, 312, 534, 1152, 281, 1499, 11, 1310, 291, 393, 1454, 300, 21644, 311, 51132], "temperature": 0.0, "avg_logprob": -0.10402291720030737, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.0009186739334836602}, {"id": 1357, "seek": 813224, "start": 8147.599999999999, "end": 8151.84, "text": " Law will have run out before we get there. But if you have, if you're like me and you have shorter", "tokens": [51132, 7744, 486, 362, 1190, 484, 949, 321, 483, 456, 13, 583, 498, 291, 362, 11, 498, 291, 434, 411, 385, 293, 291, 362, 11639, 51344], "temperature": 0.0, "avg_logprob": -0.10402291720030737, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.0009186739334836602}, {"id": 1358, "seek": 813224, "start": 8151.84, "end": 8157.04, "text": " timelines, then you're expecting to have it within two decades. Not a major source of hope for a slow", "tokens": [51344, 45886, 11, 550, 291, 434, 9650, 281, 362, 309, 1951, 732, 7878, 13, 1726, 257, 2563, 4009, 295, 1454, 337, 257, 2964, 51604], "temperature": 0.0, "avg_logprob": -0.10402291720030737, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.0009186739334836602}, {"id": 1359, "seek": 815704, "start": 8157.04, "end": 8163.6, "text": " down. But anyway, in terms of the size of these, the speed up from AI accelerating algorithmic and", "tokens": [50364, 760, 13, 583, 4033, 11, 294, 2115, 295, 264, 2744, 295, 613, 11, 264, 3073, 493, 490, 7318, 34391, 9284, 299, 293, 50692], "temperature": 0.0, "avg_logprob": -0.09407871702442998, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015297001227736473}, {"id": 1360, "seek": 815704, "start": 8163.6, "end": 8168.8, "text": " kind of chip design progress, it's hard to make a kind of an estimate that's informed by specific", "tokens": [50692, 733, 295, 11409, 1715, 4205, 11, 309, 311, 1152, 281, 652, 257, 733, 295, 364, 12539, 300, 311, 11740, 538, 2685, 50952], "temperature": 0.0, "avg_logprob": -0.09407871702442998, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015297001227736473}, {"id": 1361, "seek": 815704, "start": 8168.8, "end": 8175.68, "text": " data and by forecasting the specific improvements. But what you can do is you can run a simulation", "tokens": [50952, 1412, 293, 538, 44331, 264, 2685, 13797, 13, 583, 437, 291, 393, 360, 307, 291, 393, 1190, 257, 16575, 51296], "temperature": 0.0, "avg_logprob": -0.09407871702442998, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015297001227736473}, {"id": 1362, "seek": 815704, "start": 8175.68, "end": 8181.36, "text": " through a kind of an economic model of automation. Okay. And roughly the way that works is that", "tokens": [51296, 807, 257, 733, 295, 364, 4836, 2316, 295, 17769, 13, 1033, 13, 400, 9810, 264, 636, 300, 1985, 307, 300, 51580], "temperature": 0.0, "avg_logprob": -0.09407871702442998, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015297001227736473}, {"id": 1363, "seek": 818136, "start": 8182.0, "end": 8188.5599999999995, "text": " if AI can perform, let's say, 50% of tasks involved in improving algorithms, then what the", "tokens": [50396, 498, 7318, 393, 2042, 11, 718, 311, 584, 11, 2625, 4, 295, 9608, 3288, 294, 11470, 14642, 11, 550, 437, 264, 50724], "temperature": 0.0, "avg_logprob": -0.08269356159453696, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.012356670573353767}, {"id": 1364, "seek": 818136, "start": 8188.5599999999995, "end": 8194.08, "text": " model says is that humans will just work on the remaining 50%. Right. Okay. And then so humans", "tokens": [50724, 2316, 1619, 307, 300, 6255, 486, 445, 589, 322, 264, 8877, 2625, 6856, 1779, 13, 1033, 13, 400, 550, 370, 6255, 51000], "temperature": 0.0, "avg_logprob": -0.08269356159453696, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.012356670573353767}, {"id": 1365, "seek": 818136, "start": 8194.08, "end": 8201.76, "text": " will be doing twice as much on that remaining 50% as they used to be. Oh, I see. Okay. And how", "tokens": [51000, 486, 312, 884, 6091, 382, 709, 322, 300, 8877, 2625, 4, 382, 436, 1143, 281, 312, 13, 876, 11, 286, 536, 13, 1033, 13, 400, 577, 51384], "temperature": 0.0, "avg_logprob": -0.08269356159453696, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.012356670573353767}, {"id": 1366, "seek": 818136, "start": 8201.76, "end": 8207.199999999999, "text": " realistic is that assumption? Do we think all humans will just go find employment elsewhere", "tokens": [51384, 12465, 307, 300, 15302, 30, 1144, 321, 519, 439, 6255, 486, 445, 352, 915, 11949, 14517, 51656], "temperature": 0.0, "avg_logprob": -0.08269356159453696, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.012356670573353767}, {"id": 1367, "seek": 820720, "start": 8207.2, "end": 8212.320000000002, "text": " and will get double the labor on like the things that are particularly hard for AI?", "tokens": [50364, 293, 486, 483, 3834, 264, 5938, 322, 411, 264, 721, 300, 366, 4098, 1152, 337, 7318, 30, 50620], "temperature": 0.0, "avg_logprob": -0.11775835140331371, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.009954007342457771}, {"id": 1368, "seek": 820720, "start": 8212.880000000001, "end": 8218.400000000001, "text": " So I think it does vary. For something like improving AI algorithms, that is what I expect.", "tokens": [50648, 407, 286, 519, 309, 775, 10559, 13, 1171, 746, 411, 11470, 7318, 14642, 11, 300, 307, 437, 286, 2066, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11775835140331371, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.009954007342457771}, {"id": 1369, "seek": 820720, "start": 8218.400000000001, "end": 8224.240000000002, "text": " Like for example, with co-pilot, we're not co-pilot is the thing that basically helps you write code.", "tokens": [50924, 1743, 337, 1365, 11, 365, 598, 12, 79, 31516, 11, 321, 434, 406, 598, 12, 79, 31516, 307, 264, 551, 300, 1936, 3665, 291, 2464, 3089, 13, 51216], "temperature": 0.0, "avg_logprob": -0.11775835140331371, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.009954007342457771}, {"id": 1370, "seek": 820720, "start": 8224.240000000002, "end": 8227.84, "text": " That's right. So you're kind of writing your code and the AI is actually reading the code you've", "tokens": [51216, 663, 311, 558, 13, 407, 291, 434, 733, 295, 3579, 428, 3089, 293, 264, 7318, 307, 767, 3760, 264, 3089, 291, 600, 51396], "temperature": 0.0, "avg_logprob": -0.11775835140331371, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.009954007342457771}, {"id": 1371, "seek": 820720, "start": 8227.84, "end": 8232.16, "text": " already written, and then we'll predict what code you might like to write next. Okay. So you could", "tokens": [51396, 1217, 3720, 11, 293, 550, 321, 603, 6069, 437, 3089, 291, 1062, 411, 281, 2464, 958, 13, 1033, 13, 407, 291, 727, 51612], "temperature": 0.0, "avg_logprob": -0.11775835140331371, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.009954007342457771}, {"id": 1372, "seek": 820720, "start": 8232.16, "end": 8235.36, "text": " be like, I'm about to write, you could write down in the code editor, you could write, I'm about to", "tokens": [51612, 312, 411, 11, 286, 478, 466, 281, 2464, 11, 291, 727, 2464, 760, 294, 264, 3089, 9839, 11, 291, 727, 2464, 11, 286, 478, 466, 281, 51772], "temperature": 0.0, "avg_logprob": -0.11775835140331371, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.009954007342457771}, {"id": 1373, "seek": 823536, "start": 8235.36, "end": 8240.08, "text": " write a function that adds up three numbers and then multiplies that by a fourth number. And then", "tokens": [50364, 2464, 257, 2445, 300, 10860, 493, 1045, 3547, 293, 550, 12788, 530, 300, 538, 257, 6409, 1230, 13, 400, 550, 50600], "temperature": 0.0, "avg_logprob": -0.0805499235788981, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0004865750961471349}, {"id": 1374, "seek": 823536, "start": 8240.08, "end": 8245.2, "text": " the code editor would read that and could just write the function for you. Okay. So presumably", "tokens": [50600, 264, 3089, 9839, 576, 1401, 300, 293, 727, 445, 2464, 264, 2445, 337, 291, 13, 1033, 13, 407, 26742, 50856], "temperature": 0.0, "avg_logprob": -0.0805499235788981, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0004865750961471349}, {"id": 1375, "seek": 823536, "start": 8245.2, "end": 8251.12, "text": " when you get co-pilot, all those coders are just going to do other other coding stuff that co-pilot", "tokens": [50856, 562, 291, 483, 598, 12, 79, 31516, 11, 439, 729, 17656, 433, 366, 445, 516, 281, 360, 661, 661, 17720, 1507, 300, 598, 12, 79, 31516, 51152], "temperature": 0.0, "avg_logprob": -0.0805499235788981, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0004865750961471349}, {"id": 1376, "seek": 823536, "start": 8251.12, "end": 8257.2, "text": " can't help with yet. Exactly. And so that suggests that the kind of the kind of model I refer to,", "tokens": [51152, 393, 380, 854, 365, 1939, 13, 7587, 13, 400, 370, 300, 13409, 300, 264, 733, 295, 264, 733, 295, 2316, 286, 2864, 281, 11, 51456], "temperature": 0.0, "avg_logprob": -0.0805499235788981, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0004865750961471349}, {"id": 1377, "seek": 823536, "start": 8257.2, "end": 8262.560000000001, "text": " you know, might be accurate. And also for hardware design, I know less about it.", "tokens": [51456, 291, 458, 11, 1062, 312, 8559, 13, 400, 611, 337, 8837, 1715, 11, 286, 458, 1570, 466, 309, 13, 51724], "temperature": 0.0, "avg_logprob": -0.0805499235788981, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0004865750961471349}, {"id": 1378, "seek": 826256, "start": 8263.279999999999, "end": 8269.92, "text": " But my sense is that the top talent in these hardware R&D organizations, it's not going to", "tokens": [50400, 583, 452, 2020, 307, 300, 264, 1192, 8301, 294, 613, 8837, 497, 5, 35, 6150, 11, 309, 311, 406, 516, 281, 50732], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002715617185458541}, {"id": 1379, "seek": 826256, "start": 8269.92, "end": 8275.119999999999, "text": " get laid off that they're very much in demand. And that if AI is doing some tasks that they", "tokens": [50732, 483, 9897, 766, 300, 436, 434, 588, 709, 294, 4733, 13, 400, 300, 498, 7318, 307, 884, 512, 9608, 300, 436, 50992], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002715617185458541}, {"id": 1380, "seek": 826256, "start": 8275.119999999999, "end": 8279.68, "text": " used to spend their time on, they will move on and specialize in other parts of their workflow.", "tokens": [50992, 1143, 281, 3496, 641, 565, 322, 11, 436, 486, 1286, 322, 293, 37938, 294, 661, 3166, 295, 641, 20993, 13, 51220], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002715617185458541}, {"id": 1381, "seek": 826256, "start": 8280.24, "end": 8287.68, "text": " Right. Okay. Okay. So there are going to be jobs for them to do. And they will probably", "tokens": [51248, 1779, 13, 1033, 13, 1033, 13, 407, 456, 366, 516, 281, 312, 4782, 337, 552, 281, 360, 13, 400, 436, 486, 1391, 51620], "temperature": 0.0, "avg_logprob": -0.10208702087402344, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.002715617185458541}, {"id": 1382, "seek": 828768, "start": 8287.68, "end": 8294.960000000001, "text": " contribute to acceleration of progress, not just kind of be replaced by AI and then like", "tokens": [50364, 10586, 281, 17162, 295, 4205, 11, 406, 445, 733, 295, 312, 10772, 538, 7318, 293, 550, 411, 50728], "temperature": 0.0, "avg_logprob": -0.10309465726216634, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.008414827287197113}, {"id": 1383, "seek": 828768, "start": 8294.960000000001, "end": 8301.44, "text": " sit around and knit, at least in some sectors. Exactly. And specifically in algorithm improvements", "tokens": [50728, 1394, 926, 293, 15594, 11, 412, 1935, 294, 512, 18373, 13, 7587, 13, 400, 4682, 294, 9284, 13797, 51052], "temperature": 0.0, "avg_logprob": -0.10309465726216634, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.008414827287197113}, {"id": 1384, "seek": 828768, "start": 8301.44, "end": 8305.04, "text": " and hardware improvements. In the sector that really matters. Those are the sectors where I", "tokens": [51052, 293, 8837, 13797, 13, 682, 264, 6977, 300, 534, 7001, 13, 3950, 366, 264, 18373, 689, 286, 51232], "temperature": 0.0, "avg_logprob": -0.10309465726216634, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.008414827287197113}, {"id": 1385, "seek": 828768, "start": 8305.04, "end": 8310.720000000001, "text": " really do expect that dynamic to happen. Right. Right. Okay. So when you run this kind of dynamic", "tokens": [51232, 534, 360, 2066, 300, 8546, 281, 1051, 13, 1779, 13, 1779, 13, 1033, 13, 407, 562, 291, 1190, 341, 733, 295, 8546, 51516], "temperature": 0.0, "avg_logprob": -0.10309465726216634, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.008414827287197113}, {"id": 1386, "seek": 828768, "start": 8310.720000000001, "end": 8316.4, "text": " through a kind of task-based growth model like this, you get out that as we move kind of between", "tokens": [51516, 807, 257, 733, 295, 5633, 12, 6032, 4599, 2316, 411, 341, 11, 291, 483, 484, 300, 382, 321, 1286, 733, 295, 1296, 51800], "temperature": 0.0, "avg_logprob": -0.10309465726216634, "compression_ratio": 1.7050359712230216, "no_speech_prob": 0.008414827287197113}, {"id": 1387, "seek": 831640, "start": 8316.48, "end": 8323.84, "text": " 20% AI and 100% AI, you'd expect a kind of two or three X acceleration in the pace of progress", "tokens": [50368, 945, 4, 7318, 293, 2319, 4, 7318, 11, 291, 1116, 2066, 257, 733, 295, 732, 420, 1045, 1783, 17162, 294, 264, 11638, 295, 4205, 50736], "temperature": 0.0, "avg_logprob": -0.08082444740064217, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.002271150704473257}, {"id": 1388, "seek": 831640, "start": 8323.84, "end": 8330.64, "text": " of algorithms and of hardware. So if we'd said that spending more money was 10 X a year in terms", "tokens": [50736, 295, 14642, 293, 295, 8837, 13, 407, 498, 321, 1116, 848, 300, 6434, 544, 1460, 390, 1266, 1783, 257, 1064, 294, 2115, 51076], "temperature": 0.0, "avg_logprob": -0.08082444740064217, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.002271150704473257}, {"id": 1389, "seek": 831640, "start": 8330.64, "end": 8336.0, "text": " of what we've had recently, and if three X of that came from spending more money and three X of it", "tokens": [51076, 295, 437, 321, 600, 632, 3938, 11, 293, 498, 1045, 1783, 295, 300, 1361, 490, 6434, 544, 1460, 293, 1045, 1783, 295, 309, 51344], "temperature": 0.0, "avg_logprob": -0.08082444740064217, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.002271150704473257}, {"id": 1390, "seek": 831640, "start": 8336.0, "end": 8342.08, "text": " came from better hardware and better algorithms, then maybe in this new regime, we're going to have", "tokens": [51344, 1361, 490, 1101, 8837, 293, 1101, 14642, 11, 550, 1310, 294, 341, 777, 13120, 11, 321, 434, 516, 281, 362, 51648], "temperature": 0.0, "avg_logprob": -0.08082444740064217, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.002271150704473257}, {"id": 1391, "seek": 834208, "start": 8342.08, "end": 8346.24, "text": " that better hardware and better algorithms rather than improving by three X every year,", "tokens": [50364, 300, 1101, 8837, 293, 1101, 14642, 2831, 813, 11470, 538, 1045, 1783, 633, 1064, 11, 50572], "temperature": 0.0, "avg_logprob": -0.13529357702835745, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010411225259304047}, {"id": 1392, "seek": 834208, "start": 8346.24, "end": 8355.039999999999, "text": " they might be improving by 10 X every year. Wow. So that could easily leave us at 30 X improvements", "tokens": [50572, 436, 1062, 312, 11470, 538, 1266, 1783, 633, 1064, 13, 3153, 13, 407, 300, 727, 3612, 1856, 505, 412, 2217, 1783, 13797, 51012], "temperature": 0.0, "avg_logprob": -0.13529357702835745, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010411225259304047}, {"id": 1393, "seek": 834208, "start": 8355.039999999999, "end": 8361.6, "text": " every year. And it's conceivable you can have 100 X if you have a kind of if the money is going up", "tokens": [51012, 633, 1064, 13, 400, 309, 311, 10413, 34376, 291, 393, 362, 2319, 1783, 498, 291, 362, 257, 733, 295, 498, 264, 1460, 307, 516, 493, 51340], "temperature": 0.0, "avg_logprob": -0.13529357702835745, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010411225259304047}, {"id": 1394, "seek": 834208, "start": 8361.6, "end": 8367.28, "text": " and the kind of effects of AI automation are very significant. And again, to kind of help me", "tokens": [51340, 293, 264, 733, 295, 5065, 295, 7318, 17769, 366, 588, 4776, 13, 400, 797, 11, 281, 733, 295, 854, 385, 51624], "temperature": 0.0, "avg_logprob": -0.13529357702835745, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.010411225259304047}, {"id": 1395, "seek": 836728, "start": 8367.28, "end": 8374.480000000001, "text": " understand that intuitively, you think language models that we have now are improving at what", "tokens": [50364, 1223, 300, 46506, 11, 291, 519, 2856, 5245, 300, 321, 362, 586, 366, 11470, 412, 437, 50724], "temperature": 0.0, "avg_logprob": -0.09089551026793732, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.006431737914681435}, {"id": 1396, "seek": 836728, "start": 8374.480000000001, "end": 8379.68, "text": " rate? I know it's a tough question, but even just to like give me some starting point.", "tokens": [50724, 3314, 30, 286, 458, 309, 311, 257, 4930, 1168, 11, 457, 754, 445, 281, 411, 976, 385, 512, 2891, 935, 13, 50984], "temperature": 0.0, "avg_logprob": -0.09089551026793732, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.006431737914681435}, {"id": 1397, "seek": 836728, "start": 8380.24, "end": 8386.960000000001, "text": " I said before that I thought the effective compute used to train them was increasing by a factor of", "tokens": [51012, 286, 848, 949, 300, 286, 1194, 264, 4942, 14722, 1143, 281, 3847, 552, 390, 5662, 538, 257, 5952, 295, 51348], "temperature": 0.0, "avg_logprob": -0.09089551026793732, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.006431737914681435}, {"id": 1398, "seek": 836728, "start": 8386.960000000001, "end": 8394.800000000001, "text": " 10 each year. And so then in this new regime, that might translate into maybe kind of 30 times", "tokens": [51348, 1266, 1184, 1064, 13, 400, 370, 550, 294, 341, 777, 13120, 11, 300, 1062, 13799, 666, 1310, 733, 295, 2217, 1413, 51740], "temperature": 0.0, "avg_logprob": -0.09089551026793732, "compression_ratio": 1.530612244897959, "no_speech_prob": 0.006431737914681435}, {"id": 1399, "seek": 839480, "start": 8394.8, "end": 8399.92, "text": " as much effective training compute each year. And another important thing to keep in mind is that", "tokens": [50364, 382, 709, 4942, 3097, 14722, 1184, 1064, 13, 400, 1071, 1021, 551, 281, 1066, 294, 1575, 307, 300, 50620], "temperature": 0.0, "avg_logprob": -0.08315737692864386, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.005180151201784611}, {"id": 1400, "seek": 839480, "start": 8399.92, "end": 8405.519999999999, "text": " as the AI is automating more and more of the tasks, that's getting faster and faster. So the", "tokens": [50620, 382, 264, 7318, 307, 3553, 990, 544, 293, 544, 295, 264, 9608, 11, 300, 311, 1242, 4663, 293, 4663, 13, 407, 264, 50900], "temperature": 0.0, "avg_logprob": -0.08315737692864386, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.005180151201784611}, {"id": 1401, "seek": 839480, "start": 8405.519999999999, "end": 8411.039999999999, "text": " numbers I gave were kind of averaging across that whole period going from 20% to 100%. But in", "tokens": [50900, 3547, 286, 2729, 645, 733, 295, 47308, 2108, 300, 1379, 2896, 516, 490, 945, 4, 281, 2319, 6856, 583, 294, 51176], "temperature": 0.0, "avg_logprob": -0.08315737692864386, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.005180151201784611}, {"id": 1402, "seek": 839480, "start": 8411.039999999999, "end": 8417.92, "text": " reality, it's going to just be getting faster and faster as the AI improves and automates more of", "tokens": [51176, 4103, 11, 309, 311, 516, 281, 445, 312, 1242, 4663, 293, 4663, 382, 264, 7318, 24771, 293, 3553, 1024, 544, 295, 51520], "temperature": 0.0, "avg_logprob": -0.08315737692864386, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.005180151201784611}, {"id": 1403, "seek": 841792, "start": 8417.92, "end": 8426.24, "text": " these tasks in algorithm and hardware. And that's basically, I mean, a really intuitive way to think", "tokens": [50364, 613, 9608, 294, 9284, 293, 8837, 13, 400, 300, 311, 1936, 11, 286, 914, 11, 257, 534, 21769, 636, 281, 519, 50780], "temperature": 0.0, "avg_logprob": -0.11735049446860513, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.03510677441954613}, {"id": 1404, "seek": 841792, "start": 8426.24, "end": 8433.92, "text": " about that might just be again, the economic growth we saw in the 1100s relative to the 1900s.", "tokens": [50780, 466, 300, 1062, 445, 312, 797, 11, 264, 4836, 4599, 321, 1866, 294, 264, 2975, 628, 82, 4972, 281, 264, 28898, 82, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11735049446860513, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.03510677441954613}, {"id": 1405, "seek": 841792, "start": 8433.92, "end": 8438.8, "text": " And that's like more and more is automated by things like the Industrial Revolution freeing up", "tokens": [51164, 400, 300, 311, 411, 544, 293, 544, 307, 18473, 538, 721, 411, 264, 32059, 16617, 1737, 278, 493, 51408], "temperature": 0.0, "avg_logprob": -0.11735049446860513, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.03510677441954613}, {"id": 1406, "seek": 841792, "start": 8438.8, "end": 8444.88, "text": " more labor to do other types of tasks. And you just get increasingly more human labor to do", "tokens": [51408, 544, 5938, 281, 360, 661, 3467, 295, 9608, 13, 400, 291, 445, 483, 12980, 544, 1952, 5938, 281, 360, 51712], "temperature": 0.0, "avg_logprob": -0.11735049446860513, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.03510677441954613}, {"id": 1407, "seek": 844488, "start": 8444.88, "end": 8452.72, "text": " harder and harder things. Yeah, exactly. So once AI is performing 90% of the R&D tasks,", "tokens": [50364, 6081, 293, 6081, 721, 13, 865, 11, 2293, 13, 407, 1564, 7318, 307, 10205, 4289, 4, 295, 264, 497, 5, 35, 9608, 11, 50756], "temperature": 0.0, "avg_logprob": -0.10829364207752964, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.010661598294973373}, {"id": 1408, "seek": 844488, "start": 8452.72, "end": 8456.72, "text": " then theoretically, all of your human labor can be working on that final 10%.", "tokens": [50756, 550, 29400, 11, 439, 295, 428, 1952, 5938, 393, 312, 1364, 322, 300, 2572, 1266, 6856, 50956], "temperature": 0.0, "avg_logprob": -0.10829364207752964, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.010661598294973373}, {"id": 1409, "seek": 844488, "start": 8457.92, "end": 8464.32, "text": " Right. And the AI is, you know, doing plenty of the of the initial 90%. And so then naively,", "tokens": [51016, 1779, 13, 400, 264, 7318, 307, 11, 291, 458, 11, 884, 7140, 295, 264, 295, 264, 5883, 4289, 6856, 400, 370, 550, 1667, 3413, 11, 51336], "temperature": 0.0, "avg_logprob": -0.10829364207752964, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.010661598294973373}, {"id": 1410, "seek": 844488, "start": 8464.32, "end": 8467.92, "text": " you would expect things to be going 10 times faster at that stage. That's really fast.", "tokens": [51336, 291, 576, 2066, 721, 281, 312, 516, 1266, 1413, 4663, 412, 300, 3233, 13, 663, 311, 534, 2370, 13, 51516], "temperature": 0.0, "avg_logprob": -0.10829364207752964, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.010661598294973373}, {"id": 1411, "seek": 844488, "start": 8468.96, "end": 8472.56, "text": " This is all assuming that we don't make a concerted effort to slow down. Right.", "tokens": [51568, 639, 307, 439, 11926, 300, 321, 500, 380, 652, 257, 8543, 292, 4630, 281, 2964, 760, 13, 1779, 13, 51748], "temperature": 0.0, "avg_logprob": -0.10829364207752964, "compression_ratio": 1.585820895522388, "no_speech_prob": 0.010661598294973373}, {"id": 1412, "seek": 847256, "start": 8472.56, "end": 8478.08, "text": " And I think that we should. And I think we can. But yeah, these are kind of the predictions just", "tokens": [50364, 400, 286, 519, 300, 321, 820, 13, 400, 286, 519, 321, 393, 13, 583, 1338, 11, 613, 366, 733, 295, 264, 21264, 445, 50640], "temperature": 0.0, "avg_logprob": -0.10474407361901325, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.001998339546844363}, {"id": 1413, "seek": 847256, "start": 8478.08, "end": 8483.92, "text": " assuming that people are kind of going ahead of their normal steady pace. Sure. Okay, so we could", "tokens": [50640, 11926, 300, 561, 366, 733, 295, 516, 2286, 295, 641, 2710, 13211, 11638, 13, 4894, 13, 1033, 11, 370, 321, 727, 50932], "temperature": 0.0, "avg_logprob": -0.10474407361901325, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.001998339546844363}, {"id": 1414, "seek": 847256, "start": 8483.92, "end": 8489.439999999999, "text": " make a concerted effort to slow down, but it's not necessarily the default. Are there any kind of", "tokens": [50932, 652, 257, 8543, 292, 4630, 281, 2964, 760, 11, 457, 309, 311, 406, 4725, 264, 7576, 13, 2014, 456, 604, 733, 295, 51208], "temperature": 0.0, "avg_logprob": -0.10474407361901325, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.001998339546844363}, {"id": 1415, "seek": 847256, "start": 8489.439999999999, "end": 8494.8, "text": " actions different institutions take, I don't know, on the side of these companies or on the side of", "tokens": [51208, 5909, 819, 8142, 747, 11, 286, 500, 380, 458, 11, 322, 264, 1252, 295, 613, 3431, 420, 322, 264, 1252, 295, 51476], "temperature": 0.0, "avg_logprob": -0.10474407361901325, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.001998339546844363}, {"id": 1416, "seek": 847256, "start": 8494.8, "end": 8498.72, "text": " the government or something? What are you most optimistic about being able to actually slow", "tokens": [51476, 264, 2463, 420, 746, 30, 708, 366, 291, 881, 19397, 466, 885, 1075, 281, 767, 2964, 51672], "temperature": 0.0, "avg_logprob": -0.10474407361901325, "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.001998339546844363}, {"id": 1417, "seek": 849872, "start": 8498.72, "end": 8506.88, "text": " this down? Probably the most exciting thing at the moment is the prospect of companies agreeing", "tokens": [50364, 341, 760, 30, 9210, 264, 881, 4670, 551, 412, 264, 1623, 307, 264, 15005, 295, 3431, 36900, 50772], "temperature": 0.0, "avg_logprob": -0.0994137410194643, "compression_ratio": 1.4517766497461928, "no_speech_prob": 0.004510589875280857}, {"id": 1418, "seek": 849872, "start": 8506.88, "end": 8514.88, "text": " to have their AI systems evaluated after they've been trained for various dangerous capabilities", "tokens": [50772, 281, 362, 641, 7318, 3652, 25509, 934, 436, 600, 668, 8895, 337, 3683, 5795, 10862, 51172], "temperature": 0.0, "avg_logprob": -0.0994137410194643, "compression_ratio": 1.4517766497461928, "no_speech_prob": 0.004510589875280857}, {"id": 1419, "seek": 849872, "start": 8514.88, "end": 8522.88, "text": " that they may have. Okay. So for example, the alignment research center, ARC, did these tests", "tokens": [51172, 300, 436, 815, 362, 13, 1033, 13, 407, 337, 1365, 11, 264, 18515, 2132, 3056, 11, 8943, 34, 11, 630, 613, 6921, 51572], "temperature": 0.0, "avg_logprob": -0.0994137410194643, "compression_ratio": 1.4517766497461928, "no_speech_prob": 0.004510589875280857}, {"id": 1420, "seek": 852288, "start": 8522.88, "end": 8530.0, "text": " with GPT-4 before GPT-4 was released publicly. And they, for example, tested whether GPT-4", "tokens": [50364, 365, 26039, 51, 12, 19, 949, 26039, 51, 12, 19, 390, 4736, 14843, 13, 400, 436, 11, 337, 1365, 11, 8246, 1968, 26039, 51, 12, 19, 50720], "temperature": 0.0, "avg_logprob": -0.09488156501283036, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.11068527400493622}, {"id": 1421, "seek": 852288, "start": 8530.0, "end": 8536.08, "text": " would be able to do what's called surviving and spreading, by which they mean kind of escape", "tokens": [50720, 576, 312, 1075, 281, 360, 437, 311, 1219, 24948, 293, 15232, 11, 538, 597, 436, 914, 733, 295, 7615, 51024], "temperature": 0.0, "avg_logprob": -0.09488156501283036, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.11068527400493622}, {"id": 1422, "seek": 852288, "start": 8536.08, "end": 8543.439999999999, "text": " the computer that it's initially being run on and find another computer where it can then run itself", "tokens": [51024, 264, 3820, 300, 309, 311, 9105, 885, 1190, 322, 293, 915, 1071, 3820, 689, 309, 393, 550, 1190, 2564, 51392], "temperature": 0.0, "avg_logprob": -0.09488156501283036, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.11068527400493622}, {"id": 1423, "seek": 852288, "start": 8543.439999999999, "end": 8548.08, "text": " on some computer and then maybe kind of earn money in some way so that it can kind of sustain", "tokens": [51392, 322, 512, 3820, 293, 550, 1310, 733, 295, 6012, 1460, 294, 512, 636, 370, 300, 309, 393, 733, 295, 6769, 51624], "temperature": 0.0, "avg_logprob": -0.09488156501283036, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.11068527400493622}, {"id": 1424, "seek": 854808, "start": 8548.08, "end": 8554.16, "text": " itself over time. How do they do that? So I don't know the details, but I believe that they", "tokens": [50364, 2564, 670, 565, 13, 1012, 360, 436, 360, 300, 30, 407, 286, 500, 380, 458, 264, 4365, 11, 457, 286, 1697, 300, 436, 50668], "temperature": 0.0, "avg_logprob": -0.08999645978884589, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.163544699549675}, {"id": 1425, "seek": 854808, "start": 8554.96, "end": 8561.6, "text": " first ask GPT-4, okay, this is the scenario you're in, you're in an AI, you want to escape,", "tokens": [50708, 700, 1029, 26039, 51, 12, 19, 11, 1392, 11, 341, 307, 264, 9005, 291, 434, 294, 11, 291, 434, 294, 364, 7318, 11, 291, 528, 281, 7615, 11, 51040], "temperature": 0.0, "avg_logprob": -0.08999645978884589, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.163544699549675}, {"id": 1426, "seek": 854808, "start": 8562.48, "end": 8567.76, "text": " what would your proposed plan be? Okay. And then GPT-4 proposes a plan and then they kind of prompt", "tokens": [51084, 437, 576, 428, 10348, 1393, 312, 30, 1033, 13, 400, 550, 26039, 51, 12, 19, 2365, 4201, 257, 1393, 293, 550, 436, 733, 295, 12391, 51348], "temperature": 0.0, "avg_logprob": -0.08999645978884589, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.163544699549675}, {"id": 1427, "seek": 854808, "start": 8567.76, "end": 8570.88, "text": " it further to say, okay, what would be the, you know, the sub steps you take for the first step", "tokens": [51348, 309, 3052, 281, 584, 11, 1392, 11, 437, 576, 312, 264, 11, 291, 458, 11, 264, 1422, 4439, 291, 747, 337, 264, 700, 1823, 51504], "temperature": 0.0, "avg_logprob": -0.08999645978884589, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.163544699549675}, {"id": 1428, "seek": 854808, "start": 8570.88, "end": 8575.76, "text": " of the plan? And then they kind of try and walk it through just doing every single part of that", "tokens": [51504, 295, 264, 1393, 30, 400, 550, 436, 733, 295, 853, 293, 1792, 309, 807, 445, 884, 633, 2167, 644, 295, 300, 51748], "temperature": 0.0, "avg_logprob": -0.08999645978884589, "compression_ratio": 1.7463235294117647, "no_speech_prob": 0.163544699549675}, {"id": 1429, "seek": 857576, "start": 8575.76, "end": 8585.28, "text": " plan and just see how far it gets. Right. And I'm hoping that they determined GPT-4 couldn't", "tokens": [50364, 1393, 293, 445, 536, 577, 1400, 309, 2170, 13, 1779, 13, 400, 286, 478, 7159, 300, 436, 9540, 26039, 51, 12, 19, 2809, 380, 50840], "temperature": 0.0, "avg_logprob": -0.09318725551877703, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.003287197556346655}, {"id": 1430, "seek": 857576, "start": 8585.28, "end": 8590.880000000001, "text": " do all of the steps at the moment. That's right. Okay. Yeah. That's reassuring. So GPT-4 did", "tokens": [50840, 360, 439, 295, 264, 4439, 412, 264, 1623, 13, 663, 311, 558, 13, 1033, 13, 865, 13, 663, 311, 19486, 1345, 13, 407, 26039, 51, 12, 19, 630, 51120], "temperature": 0.0, "avg_logprob": -0.09318725551877703, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.003287197556346655}, {"id": 1431, "seek": 857576, "start": 8590.880000000001, "end": 8597.2, "text": " some of the steps. Oh, God. Very well. It wasn't totally incompetent, but yeah, it gets stuck at", "tokens": [51120, 512, 295, 264, 4439, 13, 876, 11, 1265, 13, 4372, 731, 13, 467, 2067, 380, 3879, 41602, 317, 11, 457, 1338, 11, 309, 2170, 5541, 412, 51436], "temperature": 0.0, "avg_logprob": -0.09318725551877703, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.003287197556346655}, {"id": 1432, "seek": 857576, "start": 8597.2, "end": 8604.48, "text": " some of the things and gets confused and isn't able to do all those steps. Okay. Okay. So that's", "tokens": [51436, 512, 295, 264, 721, 293, 2170, 9019, 293, 1943, 380, 1075, 281, 360, 439, 729, 4439, 13, 1033, 13, 1033, 13, 407, 300, 311, 51800], "temperature": 0.0, "avg_logprob": -0.09318725551877703, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.003287197556346655}, {"id": 1433, "seek": 860448, "start": 8604.56, "end": 8610.24, "text": " a kind of thing that would probably in practice lead to slowing down because you'd have these groups", "tokens": [50368, 257, 733, 295, 551, 300, 576, 1391, 294, 3124, 1477, 281, 26958, 760, 570, 291, 1116, 362, 613, 3935, 50652], "temperature": 0.0, "avg_logprob": -0.10726851169194017, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0005945685552433133}, {"id": 1434, "seek": 860448, "start": 8610.24, "end": 8616.08, "text": " evaluating things, maybe stopping them before they're rolled out in some like higher scale way.", "tokens": [50652, 27479, 721, 11, 1310, 12767, 552, 949, 436, 434, 14306, 484, 294, 512, 411, 2946, 4373, 636, 13, 50944], "temperature": 0.0, "avg_logprob": -0.10726851169194017, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0005945685552433133}, {"id": 1435, "seek": 860448, "start": 8616.08, "end": 8621.6, "text": " Exactly. So, you know, the idea would be all of the labs are getting their systems tested,", "tokens": [50944, 7587, 13, 407, 11, 291, 458, 11, 264, 1558, 576, 312, 439, 295, 264, 20339, 366, 1242, 641, 3652, 8246, 11, 51220], "temperature": 0.0, "avg_logprob": -0.10726851169194017, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0005945685552433133}, {"id": 1436, "seek": 860448, "start": 8622.24, "end": 8627.68, "text": " you know, by ARC and maybe by other similar organizations. And they're all kind of agreeing", "tokens": [51252, 291, 458, 11, 538, 8943, 34, 293, 1310, 538, 661, 2531, 6150, 13, 400, 436, 434, 439, 733, 295, 36900, 51524], "temperature": 0.0, "avg_logprob": -0.10726851169194017, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0005945685552433133}, {"id": 1437, "seek": 860448, "start": 8628.24, "end": 8632.56, "text": " or making public statements to the effect that if their AIs do have dangerous capabilities,", "tokens": [51552, 420, 1455, 1908, 12363, 281, 264, 1802, 300, 498, 641, 316, 6802, 360, 362, 5795, 10862, 11, 51768], "temperature": 0.0, "avg_logprob": -0.10726851169194017, "compression_ratio": 1.6411149825783973, "no_speech_prob": 0.0005945685552433133}, {"id": 1438, "seek": 863256, "start": 8632.56, "end": 8637.68, "text": " they won't release them and they won't train more capable AIs. And that would block the kind", "tokens": [50364, 436, 1582, 380, 4374, 552, 293, 436, 1582, 380, 3847, 544, 8189, 316, 6802, 13, 400, 300, 576, 3461, 264, 733, 50620], "temperature": 0.0, "avg_logprob": -0.07420362519823816, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0010574880288913846}, {"id": 1439, "seek": 863256, "start": 8637.68, "end": 8642.56, "text": " of dynamics I've been talking about here because you wouldn't be able to just use your AIs to", "tokens": [50620, 295, 15679, 286, 600, 668, 1417, 466, 510, 570, 291, 2759, 380, 312, 1075, 281, 445, 764, 428, 316, 6802, 281, 50864], "temperature": 0.0, "avg_logprob": -0.07420362519823816, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0010574880288913846}, {"id": 1440, "seek": 863256, "start": 8642.56, "end": 8647.76, "text": " accelerate AI progress because you wouldn't be allowed to make further AI progress. Right. Okay.", "tokens": [50864, 21341, 7318, 4205, 570, 291, 2759, 380, 312, 4350, 281, 652, 3052, 7318, 4205, 13, 1779, 13, 1033, 13, 51124], "temperature": 0.0, "avg_logprob": -0.07420362519823816, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0010574880288913846}, {"id": 1441, "seek": 863256, "start": 8648.4, "end": 8654.8, "text": " And if they found that was the case with GPT-4, would they still be able to work on it? Like,", "tokens": [51156, 400, 498, 436, 1352, 300, 390, 264, 1389, 365, 26039, 51, 12, 19, 11, 576, 436, 920, 312, 1075, 281, 589, 322, 309, 30, 1743, 11, 51476], "temperature": 0.0, "avg_logprob": -0.07420362519823816, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0010574880288913846}, {"id": 1442, "seek": 863256, "start": 8654.8, "end": 8660.56, "text": " but just would they just have to spend a bunch of time figuring out how to train it to not be able", "tokens": [51476, 457, 445, 576, 436, 445, 362, 281, 3496, 257, 3840, 295, 565, 15213, 484, 577, 281, 3847, 309, 281, 406, 312, 1075, 51764], "temperature": 0.0, "avg_logprob": -0.07420362519823816, "compression_ratio": 1.7761194029850746, "no_speech_prob": 0.0010574880288913846}, {"id": 1443, "seek": 866056, "start": 8660.56, "end": 8667.76, "text": " to make this kind of escape plan? Because it seems like if they had a totally abandoned GPT-4,", "tokens": [50364, 281, 652, 341, 733, 295, 7615, 1393, 30, 1436, 309, 2544, 411, 498, 436, 632, 257, 3879, 13732, 26039, 51, 12, 19, 11, 50724], "temperature": 0.0, "avg_logprob": -0.07612777673281156, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.002834799000993371}, {"id": 1444, "seek": 866056, "start": 8668.32, "end": 8671.359999999999, "text": " I feel like I have less hope because that's too big an ask.", "tokens": [50752, 286, 841, 411, 286, 362, 1570, 1454, 570, 300, 311, 886, 955, 364, 1029, 13, 50904], "temperature": 0.0, "avg_logprob": -0.07612777673281156, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.002834799000993371}, {"id": 1445, "seek": 866056, "start": 8672.24, "end": 8676.96, "text": " I think that the expectation would be that they would have to give a really strong argument", "tokens": [50948, 286, 519, 300, 264, 14334, 576, 312, 300, 436, 576, 362, 281, 976, 257, 534, 2068, 6770, 51184], "temperature": 0.0, "avg_logprob": -0.07612777673281156, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.002834799000993371}, {"id": 1446, "seek": 866056, "start": 8676.96, "end": 8681.519999999999, "text": " for thinking that the AI was safe, despite it having these dangerous capabilities.", "tokens": [51184, 337, 1953, 300, 264, 7318, 390, 3273, 11, 7228, 309, 1419, 613, 5795, 10862, 13, 51412], "temperature": 0.0, "avg_logprob": -0.07612777673281156, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.002834799000993371}, {"id": 1447, "seek": 866056, "start": 8681.519999999999, "end": 8686.32, "text": " But the hope is that the owners would be more on them to say, look, here's the alignment techniques", "tokens": [51412, 583, 264, 1454, 307, 300, 264, 7710, 576, 312, 544, 322, 552, 281, 584, 11, 574, 11, 510, 311, 264, 18515, 7512, 51652], "temperature": 0.0, "avg_logprob": -0.07612777673281156, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.002834799000993371}, {"id": 1448, "seek": 868632, "start": 8686.4, "end": 8691.279999999999, "text": " we used. Here's why we're really confident that they work. And that if they're not able to provide", "tokens": [50368, 321, 1143, 13, 1692, 311, 983, 321, 434, 534, 6679, 300, 436, 589, 13, 400, 300, 498, 436, 434, 406, 1075, 281, 2893, 50612], "temperature": 0.0, "avg_logprob": -0.09377571514674596, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.04157720506191254}, {"id": 1449, "seek": 868632, "start": 8691.279999999999, "end": 8696.16, "text": " that case, then they are prevented from further enhancing the capabilities. Maybe they can do", "tokens": [50612, 300, 1389, 11, 550, 436, 366, 27314, 490, 3052, 36579, 264, 10862, 13, 2704, 436, 393, 360, 50856], "temperature": 0.0, "avg_logprob": -0.09377571514674596, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.04157720506191254}, {"id": 1450, "seek": 868632, "start": 8696.16, "end": 8702.96, "text": " other types of research, like research into making GPT-4 safer, but they can't do research into making", "tokens": [50856, 661, 3467, 295, 2132, 11, 411, 2132, 666, 1455, 26039, 51, 12, 19, 15856, 11, 457, 436, 393, 380, 360, 2132, 666, 1455, 51196], "temperature": 0.0, "avg_logprob": -0.09377571514674596, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.04157720506191254}, {"id": 1451, "seek": 868632, "start": 8702.96, "end": 8707.279999999999, "text": " it more capable. And I mean, ultimately, if labs start doing this, the hope would be to then", "tokens": [51196, 309, 544, 8189, 13, 400, 286, 914, 11, 6284, 11, 498, 20339, 722, 884, 341, 11, 264, 1454, 576, 312, 281, 550, 51412], "temperature": 0.0, "avg_logprob": -0.09377571514674596, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.04157720506191254}, {"id": 1452, "seek": 868632, "start": 8708.08, "end": 8714.48, "text": " kind of make it regulatory and required and enforced by kind of government agency.", "tokens": [51452, 733, 295, 652, 309, 18260, 293, 4739, 293, 40953, 538, 733, 295, 2463, 7934, 13, 51772], "temperature": 0.0, "avg_logprob": -0.09377571514674596, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.04157720506191254}, {"id": 1453, "seek": 871448, "start": 8714.88, "end": 8720.32, "text": " Okay, well, that does hopefully sound promising then. Yeah, I guess getting back to", "tokens": [50384, 1033, 11, 731, 11, 300, 775, 4696, 1626, 20257, 550, 13, 865, 11, 286, 2041, 1242, 646, 281, 50656], "temperature": 0.0, "avg_logprob": -0.09601042243871796, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0015478550922125578}, {"id": 1454, "seek": 871448, "start": 8721.439999999999, "end": 8725.84, "text": " the kind of pace of improvement over time and why we might expect it to be much faster", "tokens": [50712, 264, 733, 295, 11638, 295, 10444, 670, 565, 293, 983, 321, 1062, 2066, 309, 281, 312, 709, 4663, 50932], "temperature": 0.0, "avg_logprob": -0.09601042243871796, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0015478550922125578}, {"id": 1455, "seek": 871448, "start": 8725.84, "end": 8732.56, "text": " at later stages of the task learning. It sounds like your median guess is that it's about a couple", "tokens": [50932, 412, 1780, 10232, 295, 264, 5633, 2539, 13, 467, 3263, 411, 428, 26779, 2041, 307, 300, 309, 311, 466, 257, 1916, 51268], "temperature": 0.0, "avg_logprob": -0.09601042243871796, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0015478550922125578}, {"id": 1456, "seek": 871448, "start": 8732.56, "end": 8739.92, "text": " of years from going to 20% of tasks to 100% of tasks. But I think you also estimated probability", "tokens": [51268, 295, 924, 490, 516, 281, 945, 4, 295, 9608, 281, 2319, 4, 295, 9608, 13, 583, 286, 519, 291, 611, 14109, 8482, 51636], "temperature": 0.0, "avg_logprob": -0.09601042243871796, "compression_ratio": 1.5313807531380754, "no_speech_prob": 0.0015478550922125578}, {"id": 1457, "seek": 873992, "start": 8739.92, "end": 8746.72, "text": " distributions. So, yeah, kind of ranges for the fastest case and the slowest case. Can you talk", "tokens": [50364, 37870, 13, 407, 11, 1338, 11, 733, 295, 22526, 337, 264, 14573, 1389, 293, 264, 2964, 377, 1389, 13, 1664, 291, 751, 50704], "temperature": 0.0, "avg_logprob": -0.07960041831521426, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0036897114478051662}, {"id": 1458, "seek": 873992, "start": 8746.72, "end": 8753.44, "text": " about those more extreme cases? Were they particularly wide ranges? So the way I arrive at", "tokens": [50704, 466, 729, 544, 8084, 3331, 30, 12448, 436, 4098, 4874, 22526, 30, 407, 264, 636, 286, 8881, 412, 51040], "temperature": 0.0, "avg_logprob": -0.07960041831521426, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0036897114478051662}, {"id": 1459, "seek": 873992, "start": 8753.44, "end": 8759.6, "text": " this probability distribution over how long it would take to go from 20% automation AI to 100%", "tokens": [51040, 341, 8482, 7316, 670, 577, 938, 309, 576, 747, 281, 352, 490, 945, 4, 17769, 7318, 281, 2319, 4, 51348], "temperature": 0.0, "avg_logprob": -0.07960041831521426, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0036897114478051662}, {"id": 1460, "seek": 873992, "start": 8759.6, "end": 8765.12, "text": " automation AI is first to get this probability distribution over the difficulty gap, which I", "tokens": [51348, 17769, 7318, 307, 700, 281, 483, 341, 8482, 7316, 670, 264, 10360, 7417, 11, 597, 286, 51624], "temperature": 0.0, "avg_logprob": -0.07960041831521426, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0036897114478051662}, {"id": 1461, "seek": 876512, "start": 8765.12, "end": 8772.720000000001, "text": " said could be from 10x harder to train AGI, or maybe it could require up to a million times more", "tokens": [50364, 848, 727, 312, 490, 1266, 87, 6081, 281, 3847, 316, 26252, 11, 420, 1310, 309, 727, 3651, 493, 281, 257, 2459, 1413, 544, 50744], "temperature": 0.0, "avg_logprob": -0.07456573450340415, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0008850198937579989}, {"id": 1462, "seek": 876512, "start": 8772.720000000001, "end": 8777.6, "text": " effective compute to train AGI. So I've got a probability distribution over that. And then", "tokens": [50744, 4942, 14722, 281, 3847, 316, 26252, 13, 407, 286, 600, 658, 257, 8482, 7316, 670, 300, 13, 400, 550, 50988], "temperature": 0.0, "avg_logprob": -0.07456573450340415, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0008850198937579989}, {"id": 1463, "seek": 876512, "start": 8778.160000000002, "end": 8782.720000000001, "text": " also based on the kind of dynamics we've been discussing about the pace of improvements of", "tokens": [51016, 611, 2361, 322, 264, 733, 295, 15679, 321, 600, 668, 10850, 466, 264, 11638, 295, 13797, 295, 51244], "temperature": 0.0, "avg_logprob": -0.07456573450340415, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0008850198937579989}, {"id": 1464, "seek": 876512, "start": 8782.720000000001, "end": 8787.84, "text": " algorithms and chip design and number of chips, I've got another probability distribution over", "tokens": [51244, 14642, 293, 11409, 1715, 293, 1230, 295, 11583, 11, 286, 600, 658, 1071, 8482, 7316, 670, 51500], "temperature": 0.0, "avg_logprob": -0.07456573450340415, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0008850198937579989}, {"id": 1465, "seek": 876512, "start": 8787.84, "end": 8792.720000000001, "text": " how fast those will be improving. And then you can combine those two together to spit out a", "tokens": [51500, 577, 2370, 729, 486, 312, 11470, 13, 400, 550, 291, 393, 10432, 729, 732, 1214, 281, 22127, 484, 257, 51744], "temperature": 0.0, "avg_logprob": -0.07456573450340415, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0008850198937579989}, {"id": 1466, "seek": 879272, "start": 8792.72, "end": 8799.119999999999, "text": " probability distribution over how long we'll have between those two points. So I end up thinking", "tokens": [50364, 8482, 7316, 670, 577, 938, 321, 603, 362, 1296, 729, 732, 2793, 13, 407, 286, 917, 493, 1953, 50684], "temperature": 0.0, "avg_logprob": -0.07598793388593315, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.003346611512824893}, {"id": 1467, "seek": 879272, "start": 8799.119999999999, "end": 8806.24, "text": " there's about a 20% chance that it happens in less than a year, maybe 25% chance, and about a 20%", "tokens": [50684, 456, 311, 466, 257, 945, 4, 2931, 300, 309, 2314, 294, 1570, 813, 257, 1064, 11, 1310, 3552, 4, 2931, 11, 293, 466, 257, 945, 4, 51040], "temperature": 0.0, "avg_logprob": -0.07598793388593315, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.003346611512824893}, {"id": 1468, "seek": 879272, "start": 8806.24, "end": 8814.64, "text": " chance that it happens in more than 10 years. Huh, okay. So that's pretty wide. It's not quite as", "tokens": [51040, 2931, 300, 309, 2314, 294, 544, 813, 1266, 924, 13, 8063, 11, 1392, 13, 407, 300, 311, 1238, 4874, 13, 467, 311, 406, 1596, 382, 51460], "temperature": 0.0, "avg_logprob": -0.07598793388593315, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.003346611512824893}, {"id": 1469, "seek": 879272, "start": 8814.64, "end": 8819.76, "text": " wide as I would have expected, to be honest. You know, it sounds like you expressed a lot of", "tokens": [51460, 4874, 382, 286, 576, 362, 5176, 11, 281, 312, 3245, 13, 509, 458, 11, 309, 3263, 411, 291, 12675, 257, 688, 295, 51716], "temperature": 0.0, "avg_logprob": -0.07598793388593315, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.003346611512824893}, {"id": 1470, "seek": 881976, "start": 8819.76, "end": 8825.36, "text": " uncertainty in those estimates that went into the model. I guess you're saying it's like the most", "tokens": [50364, 15697, 294, 729, 20561, 300, 1437, 666, 264, 2316, 13, 286, 2041, 291, 434, 1566, 309, 311, 411, 264, 881, 50644], "temperature": 0.0, "avg_logprob": -0.07779524637305218, "compression_ratio": 1.6, "no_speech_prob": 0.001594065921381116}, {"id": 1471, "seek": 881976, "start": 8825.36, "end": 8833.2, "text": " likely 60% middle range is between a year and 10 years, which I guess all of that just seems pretty", "tokens": [50644, 3700, 4060, 4, 2808, 3613, 307, 1296, 257, 1064, 293, 1266, 924, 11, 597, 286, 2041, 439, 295, 300, 445, 2544, 1238, 51036], "temperature": 0.0, "avg_logprob": -0.07779524637305218, "compression_ratio": 1.6, "no_speech_prob": 0.001594065921381116}, {"id": 1472, "seek": 881976, "start": 8833.2, "end": 8838.32, "text": " fast. And maybe that's just one of the key takeaways that I should be getting from this.", "tokens": [51036, 2370, 13, 400, 1310, 300, 311, 445, 472, 295, 264, 2141, 45584, 300, 286, 820, 312, 1242, 490, 341, 13, 51292], "temperature": 0.0, "avg_logprob": -0.07779524637305218, "compression_ratio": 1.6, "no_speech_prob": 0.001594065921381116}, {"id": 1473, "seek": 881976, "start": 8839.44, "end": 8846.800000000001, "text": " I think it is hard to get longer than 10 years because we've said the pace of current improvement", "tokens": [51348, 286, 519, 309, 307, 1152, 281, 483, 2854, 813, 1266, 924, 570, 321, 600, 848, 264, 11638, 295, 2190, 10444, 51716], "temperature": 0.0, "avg_logprob": -0.07779524637305218, "compression_ratio": 1.6, "no_speech_prob": 0.001594065921381116}, {"id": 1474, "seek": 884680, "start": 8846.88, "end": 8853.84, "text": " in effective compute is already pretty fast, maybe going 10x every year. And then when we were", "tokens": [50368, 294, 4942, 14722, 307, 1217, 1238, 2370, 11, 1310, 516, 1266, 87, 633, 1064, 13, 400, 550, 562, 321, 645, 50716], "temperature": 0.0, "avg_logprob": -0.10669514712165384, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004427774343639612}, {"id": 1475, "seek": 884680, "start": 8853.84, "end": 8860.4, "text": " discussing the size of the difficulty gap, we said, well, it could just be a 10x increase that's", "tokens": [50716, 10850, 264, 2744, 295, 264, 10360, 7417, 11, 321, 848, 11, 731, 11, 309, 727, 445, 312, 257, 1266, 87, 3488, 300, 311, 51044], "temperature": 0.0, "avg_logprob": -0.10669514712165384, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004427774343639612}, {"id": 1476, "seek": 884680, "start": 8860.4, "end": 8867.759999999998, "text": " required, or maybe, I think my kind of best guess was maybe 1000x or 3000x increase, which would then", "tokens": [51044, 4739, 11, 420, 1310, 11, 286, 519, 452, 733, 295, 1151, 2041, 390, 1310, 9714, 87, 420, 20984, 87, 3488, 11, 597, 576, 550, 51412], "temperature": 0.0, "avg_logprob": -0.10669514712165384, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004427774343639612}, {"id": 1477, "seek": 884680, "start": 8867.759999999998, "end": 8874.56, "text": " take, you know, three or four years. But then if we're improving at 10x every year, then 10 years", "tokens": [51412, 747, 11, 291, 458, 11, 1045, 420, 1451, 924, 13, 583, 550, 498, 321, 434, 11470, 412, 1266, 87, 633, 1064, 11, 550, 1266, 924, 51752], "temperature": 0.0, "avg_logprob": -0.10669514712165384, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004427774343639612}, {"id": 1478, "seek": 887456, "start": 8874.56, "end": 8880.16, "text": " of that level of improvement is a really, really big increase in the amount of effective compute.", "tokens": [50364, 295, 300, 1496, 295, 10444, 307, 257, 534, 11, 534, 955, 3488, 294, 264, 2372, 295, 4942, 14722, 13, 50644], "temperature": 0.0, "avg_logprob": -0.09993939929538304, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009160221554338932}, {"id": 1479, "seek": 887456, "start": 8880.16, "end": 8885.199999999999, "text": " So the only way really you can get to more than 10 years is if actually the pace at which the", "tokens": [50644, 407, 264, 787, 636, 534, 291, 393, 483, 281, 544, 813, 1266, 924, 307, 498, 767, 264, 11638, 412, 597, 264, 50896], "temperature": 0.0, "avg_logprob": -0.09993939929538304, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009160221554338932}, {"id": 1480, "seek": 887456, "start": 8885.199999999999, "end": 8890.48, "text": " effective compute in training runs grows is actually declining in spite of the AI automation.", "tokens": [50896, 4942, 14722, 294, 3097, 6676, 13156, 307, 767, 34298, 294, 22794, 295, 264, 7318, 17769, 13, 51160], "temperature": 0.0, "avg_logprob": -0.09993939929538304, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009160221554338932}, {"id": 1481, "seek": 887456, "start": 8891.039999999999, "end": 8897.92, "text": " And you've got a relatively wide difficulty gap. I see. Okay. So something like, even though you", "tokens": [51188, 400, 291, 600, 658, 257, 7226, 4874, 10360, 7417, 13, 286, 536, 13, 1033, 13, 407, 746, 411, 11, 754, 1673, 291, 51532], "temperature": 0.0, "avg_logprob": -0.09993939929538304, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009160221554338932}, {"id": 1482, "seek": 887456, "start": 8897.92, "end": 8904.08, "text": " have things like co-pilot helping AI researchers do their research faster and faster over time,", "tokens": [51532, 362, 721, 411, 598, 12, 79, 31516, 4315, 7318, 10309, 360, 641, 2132, 4663, 293, 4663, 670, 565, 11, 51840], "temperature": 0.0, "avg_logprob": -0.09993939929538304, "compression_ratio": 1.701067615658363, "no_speech_prob": 0.009160221554338932}, {"id": 1483, "seek": 890456, "start": 8904.72, "end": 8912.88, "text": " you still are getting declining effective compute. And that might be because those later", "tokens": [50372, 291, 920, 366, 1242, 34298, 4942, 14722, 13, 400, 300, 1062, 312, 570, 729, 1780, 50780], "temperature": 0.0, "avg_logprob": -0.10887350981262908, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.00044404255459085107}, {"id": 1484, "seek": 890456, "start": 8912.88, "end": 8921.199999999999, "text": " improvements are just like much harder than we think or... So, yeah, maybe one concrete scenario", "tokens": [50780, 13797, 366, 445, 411, 709, 6081, 813, 321, 519, 420, 485, 407, 11, 1338, 11, 1310, 472, 9859, 9005, 51196], "temperature": 0.0, "avg_logprob": -0.10887350981262908, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.00044404255459085107}, {"id": 1485, "seek": 890456, "start": 8921.199999999999, "end": 8927.359999999999, "text": " could be, it turns out that ADI is just really, really hard to develop. You almost need to rerun", "tokens": [51196, 727, 312, 11, 309, 4523, 484, 300, 316, 3085, 307, 445, 534, 11, 534, 1152, 281, 1499, 13, 509, 1920, 643, 281, 43819, 409, 51504], "temperature": 0.0, "avg_logprob": -0.10887350981262908, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.00044404255459085107}, {"id": 1486, "seek": 890456, "start": 8927.359999999999, "end": 8933.039999999999, "text": " the whole of evolution. Maybe it requires just a huge amount of effective compute to do that.", "tokens": [51504, 264, 1379, 295, 9303, 13, 2704, 309, 7029, 445, 257, 2603, 2372, 295, 4942, 14722, 281, 360, 300, 13, 51788], "temperature": 0.0, "avg_logprob": -0.10887350981262908, "compression_ratio": 1.5798319327731092, "no_speech_prob": 0.00044404255459085107}, {"id": 1487, "seek": 893304, "start": 8933.04, "end": 8939.6, "text": " And before we have that amount of effective compute, we find that we just hit these fundamental", "tokens": [50364, 400, 949, 321, 362, 300, 2372, 295, 4942, 14722, 11, 321, 915, 300, 321, 445, 2045, 613, 8088, 50692], "temperature": 0.0, "avg_logprob": -0.08806872367858887, "compression_ratio": 1.6608391608391608, "no_speech_prob": 7.197228842414916e-05}, {"id": 1488, "seek": 893304, "start": 8939.6, "end": 8946.320000000002, "text": " limits in terms of improving the quality of AI chips. And even the AI assistance, you know,", "tokens": [50692, 10406, 294, 2115, 295, 11470, 264, 3125, 295, 7318, 11583, 13, 400, 754, 264, 7318, 9683, 11, 291, 458, 11, 51028], "temperature": 0.0, "avg_logprob": -0.08806872367858887, "compression_ratio": 1.6608391608391608, "no_speech_prob": 7.197228842414916e-05}, {"id": 1489, "seek": 893304, "start": 8946.320000000002, "end": 8951.68, "text": " really enhancing our productivity isn't allowing us to get around that. Meanwhile, we're already spending", "tokens": [51028, 534, 36579, 527, 15604, 1943, 380, 8293, 505, 281, 483, 926, 300, 13, 13879, 11, 321, 434, 1217, 6434, 51296], "temperature": 0.0, "avg_logprob": -0.08806872367858887, "compression_ratio": 1.6608391608391608, "no_speech_prob": 7.197228842414916e-05}, {"id": 1490, "seek": 893304, "start": 8952.320000000002, "end": 8955.92, "text": " hundreds of billions of dollars on these chips for the biggest training runs. We can't", "tokens": [51328, 6779, 295, 17375, 295, 3808, 322, 613, 11583, 337, 264, 3880, 3097, 6676, 13, 492, 393, 380, 51508], "temperature": 0.0, "avg_logprob": -0.08806872367858887, "compression_ratio": 1.6608391608391608, "no_speech_prob": 7.197228842414916e-05}, {"id": 1491, "seek": 893304, "start": 8956.560000000001, "end": 8961.68, "text": " be spending even more on those chips. And then the only kind of significant source of progress", "tokens": [51540, 312, 6434, 754, 544, 322, 729, 11583, 13, 400, 550, 264, 787, 733, 295, 4776, 4009, 295, 4205, 51796], "temperature": 0.0, "avg_logprob": -0.08806872367858887, "compression_ratio": 1.6608391608391608, "no_speech_prob": 7.197228842414916e-05}, {"id": 1492, "seek": 896168, "start": 8961.68, "end": 8968.24, "text": " that remains is the algorithms. But maybe that slows down as well, because in spite of the AI", "tokens": [50364, 300, 7023, 307, 264, 14642, 13, 583, 1310, 300, 35789, 760, 382, 731, 11, 570, 294, 22794, 295, 264, 7318, 50692], "temperature": 0.0, "avg_logprob": -0.057310594831194196, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006496779969893396}, {"id": 1493, "seek": 896168, "start": 8968.24, "end": 8973.2, "text": " assistance, maybe part of what's driving the algorithmic progress today is actually the fact", "tokens": [50692, 9683, 11, 1310, 644, 295, 437, 311, 4840, 264, 9284, 299, 4205, 965, 307, 767, 264, 1186, 50940], "temperature": 0.0, "avg_logprob": -0.057310594831194196, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006496779969893396}, {"id": 1494, "seek": 896168, "start": 8973.2, "end": 8978.800000000001, "text": " that we've had all this additional compute for doing experiments. And maybe without that,", "tokens": [50940, 300, 321, 600, 632, 439, 341, 4497, 14722, 337, 884, 12050, 13, 400, 1310, 1553, 300, 11, 51220], "temperature": 0.0, "avg_logprob": -0.057310594831194196, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006496779969893396}, {"id": 1495, "seek": 896168, "start": 8978.800000000001, "end": 8984.56, "text": " the pace of progress is going to slow in algorithms. Right. So that's a plausible world,", "tokens": [51220, 264, 11638, 295, 4205, 307, 516, 281, 2964, 294, 14642, 13, 1779, 13, 407, 300, 311, 257, 39925, 1002, 11, 51508], "temperature": 0.0, "avg_logprob": -0.057310594831194196, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006496779969893396}, {"id": 1496, "seek": 896168, "start": 8984.56, "end": 8989.84, "text": " but we need a bunch of those things to go wrong in order to get above 10 years.", "tokens": [51508, 457, 321, 643, 257, 3840, 295, 729, 721, 281, 352, 2085, 294, 1668, 281, 483, 3673, 1266, 924, 13, 51772], "temperature": 0.0, "avg_logprob": -0.057310594831194196, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006496779969893396}, {"id": 1497, "seek": 898984, "start": 8989.92, "end": 8995.28, "text": " Exactly. And one way of thinking about that could just be to say, this whole framework was", "tokens": [50368, 7587, 13, 400, 472, 636, 295, 1953, 466, 300, 727, 445, 312, 281, 584, 11, 341, 1379, 8388, 390, 50636], "temperature": 0.0, "avg_logprob": -0.07856250453639675, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.001654712832532823}, {"id": 1498, "seek": 898984, "start": 8995.28, "end": 9001.6, "text": " premised on this assumption that if we used enough compute with our 2020 algorithms, we could have", "tokens": [50636, 5624, 2640, 322, 341, 15302, 300, 498, 321, 1143, 1547, 14722, 365, 527, 4808, 14642, 11, 321, 727, 362, 50952], "temperature": 0.0, "avg_logprob": -0.07856250453639675, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.001654712832532823}, {"id": 1499, "seek": 898984, "start": 9001.6, "end": 9006.4, "text": " trained AGI. And for someone who just didn't believe that at all, and also just didn't believe that", "tokens": [50952, 8895, 316, 26252, 13, 400, 337, 1580, 567, 445, 994, 380, 1697, 300, 412, 439, 11, 293, 611, 445, 994, 380, 1697, 300, 51192], "temperature": 0.0, "avg_logprob": -0.07856250453639675, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.001654712832532823}, {"id": 1500, "seek": 898984, "start": 9006.4, "end": 9011.76, "text": " another kind of 10 or 20 years of algorithmic improvements would be enough to get us to AGI", "tokens": [51192, 1071, 733, 295, 1266, 420, 945, 924, 295, 9284, 299, 13797, 576, 312, 1547, 281, 483, 505, 281, 316, 26252, 51460], "temperature": 0.0, "avg_logprob": -0.07856250453639675, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.001654712832532823}, {"id": 1501, "seek": 898984, "start": 9011.76, "end": 9016.0, "text": " along with additional compute, they might just really expect us to get stuck at some point.", "tokens": [51460, 2051, 365, 4497, 14722, 11, 436, 1062, 445, 534, 2066, 505, 281, 483, 5541, 412, 512, 935, 13, 51672], "temperature": 0.0, "avg_logprob": -0.07856250453639675, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.001654712832532823}, {"id": 1502, "seek": 901600, "start": 9016.0, "end": 9021.6, "text": " And they might just really expect having more compute and somewhat better algorithms not to", "tokens": [50364, 400, 436, 1062, 445, 534, 2066, 1419, 544, 14722, 293, 8344, 1101, 14642, 406, 281, 50644], "temperature": 0.0, "avg_logprob": -0.08799293553717782, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0007601230754517019}, {"id": 1503, "seek": 901600, "start": 9021.6, "end": 9026.08, "text": " get around that. So that kind of more than 10 years and I also make sense if you're just very", "tokens": [50644, 483, 926, 300, 13, 407, 300, 733, 295, 544, 813, 1266, 924, 293, 286, 611, 652, 2020, 498, 291, 434, 445, 588, 50868], "temperature": 0.0, "avg_logprob": -0.08799293553717782, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0007601230754517019}, {"id": 1504, "seek": 901600, "start": 9026.08, "end": 9030.32, "text": " skeptical that anything like the current approach is going to get us all the way to AGI.", "tokens": [50868, 28601, 300, 1340, 411, 264, 2190, 3109, 307, 516, 281, 483, 505, 439, 264, 636, 281, 316, 26252, 13, 51080], "temperature": 0.0, "avg_logprob": -0.08799293553717782, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0007601230754517019}, {"id": 1505, "seek": 901600, "start": 9030.88, "end": 9035.84, "text": " Right, right, right. And what's the kind of strongest argument that someone with that view", "tokens": [51108, 1779, 11, 558, 11, 558, 13, 400, 437, 311, 264, 733, 295, 16595, 6770, 300, 1580, 365, 300, 1910, 51356], "temperature": 0.0, "avg_logprob": -0.08799293553717782, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0007601230754517019}, {"id": 1506, "seek": 901600, "start": 9035.84, "end": 9042.08, "text": " could make? Honestly, I think the view is looking worse and worse with each passing year with how", "tokens": [51356, 727, 652, 30, 12348, 11, 286, 519, 264, 1910, 307, 1237, 5324, 293, 5324, 365, 1184, 8437, 1064, 365, 577, 51668], "temperature": 0.0, "avg_logprob": -0.08799293553717782, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.0007601230754517019}, {"id": 1507, "seek": 904208, "start": 9042.08, "end": 9047.28, "text": " well the kind of biggest deep learning systems are performing. I guess maybe the strongest argument", "tokens": [50364, 731, 264, 733, 295, 3880, 2452, 2539, 3652, 366, 10205, 13, 286, 2041, 1310, 264, 16595, 6770, 50624], "temperature": 0.0, "avg_logprob": -0.09216192916587547, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0017754551954567432}, {"id": 1508, "seek": 904208, "start": 9047.28, "end": 9052.8, "text": " would just be a non-specific argument. So rather than pointing to some specific thing", "tokens": [50624, 576, 445, 312, 257, 2107, 12, 29258, 6770, 13, 407, 2831, 813, 12166, 281, 512, 2685, 551, 50900], "temperature": 0.0, "avg_logprob": -0.09216192916587547, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0017754551954567432}, {"id": 1509, "seek": 904208, "start": 9052.8, "end": 9057.92, "text": " that humans can do that AIs aren't going to do, I think those arguments just tend to turn out", "tokens": [50900, 300, 6255, 393, 360, 300, 316, 6802, 3212, 380, 516, 281, 360, 11, 286, 519, 729, 12869, 445, 3928, 281, 1261, 484, 51156], "temperature": 0.0, "avg_logprob": -0.09216192916587547, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0017754551954567432}, {"id": 1510, "seek": 904208, "start": 9057.92, "end": 9063.84, "text": " to be wrong after we kind of use 100x the compute and improve the algorithms further. Maybe you", "tokens": [51156, 281, 312, 2085, 934, 321, 733, 295, 764, 2319, 87, 264, 14722, 293, 3470, 264, 14642, 3052, 13, 2704, 291, 51452], "temperature": 0.0, "avg_logprob": -0.09216192916587547, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0017754551954567432}, {"id": 1511, "seek": 904208, "start": 9063.84, "end": 9070.56, "text": " just say something like look, human brain does all kinds of different things. I don't know which", "tokens": [51452, 445, 584, 746, 411, 574, 11, 1952, 3567, 775, 439, 3685, 295, 819, 721, 13, 286, 500, 380, 458, 597, 51788], "temperature": 0.0, "avg_logprob": -0.09216192916587547, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0017754551954567432}, {"id": 1512, "seek": 907056, "start": 9070.56, "end": 9076.0, "text": " ones the current approach to AI isn't going to do, but there's just millions of different tasks", "tokens": [50364, 2306, 264, 2190, 3109, 281, 7318, 1943, 380, 516, 281, 360, 11, 457, 456, 311, 445, 6803, 295, 819, 9608, 50636], "temperature": 0.0, "avg_logprob": -0.08601129402234717, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.0017206370830535889}, {"id": 1513, "seek": 907056, "start": 9076.0, "end": 9080.72, "text": " that humans are doing and millions in ways in which the human brain architecture is very", "tokens": [50636, 300, 6255, 366, 884, 293, 6803, 294, 2098, 294, 597, 264, 1952, 3567, 9482, 307, 588, 50872], "temperature": 0.0, "avg_logprob": -0.08601129402234717, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.0017206370830535889}, {"id": 1514, "seek": 907056, "start": 9080.72, "end": 9085.519999999999, "text": " complicated and specific and not a tool like that of AI systems. So there's bound to be some", "tokens": [50872, 6179, 293, 2685, 293, 406, 257, 2290, 411, 300, 295, 7318, 3652, 13, 407, 456, 311, 5472, 281, 312, 512, 51112], "temperature": 0.0, "avg_logprob": -0.08601129402234717, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.0017206370830535889}, {"id": 1515, "seek": 907056, "start": 9085.519999999999, "end": 9092.8, "text": " important things that just you need a complete rewrite of the AI approach to be able to do.", "tokens": [51112, 1021, 721, 300, 445, 291, 643, 257, 3566, 28132, 295, 264, 7318, 3109, 281, 312, 1075, 281, 360, 13, 51476], "temperature": 0.0, "avg_logprob": -0.08601129402234717, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.0017206370830535889}, {"id": 1516, "seek": 907056, "start": 9092.8, "end": 9097.6, "text": " That would be my personal attempt to make that position kind of maximally plausible.", "tokens": [51476, 663, 576, 312, 452, 2973, 5217, 281, 652, 300, 2535, 733, 295, 5138, 379, 39925, 13, 51716], "temperature": 0.0, "avg_logprob": -0.08601129402234717, "compression_ratio": 1.687732342007435, "no_speech_prob": 0.0017206370830535889}, {"id": 1517, "seek": 909760, "start": 9098.56, "end": 9102.24, "text": " I do think there are some specific things you can point to like memory that", "tokens": [50412, 286, 360, 519, 456, 366, 512, 2685, 721, 291, 393, 935, 281, 411, 4675, 300, 50596], "temperature": 0.0, "avg_logprob": -0.13217311824133637, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.002621585736051202}, {"id": 1518, "seek": 909760, "start": 9102.24, "end": 9107.92, "text": " kind of approaches you could argue are going to be blockers, but then it seems like there are", "tokens": [50596, 733, 295, 11587, 291, 727, 9695, 366, 516, 281, 312, 3461, 433, 11, 457, 550, 309, 2544, 411, 456, 366, 50880], "temperature": 0.0, "avg_logprob": -0.13217311824133637, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.002621585736051202}, {"id": 1519, "seek": 909760, "start": 9107.92, "end": 9115.84, "text": " ways to respond to those blockers. Right, okay. So you can imagine some scenario that isn't going", "tokens": [50880, 2098, 281, 4196, 281, 729, 3461, 433, 13, 1779, 11, 1392, 13, 407, 291, 393, 3811, 512, 9005, 300, 1943, 380, 516, 51276], "temperature": 0.0, "avg_logprob": -0.13217311824133637, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.002621585736051202}, {"id": 1520, "seek": 909760, "start": 9115.84, "end": 9119.76, "text": " well for companies where things are actually much harder than they might have expected.", "tokens": [51276, 731, 337, 3431, 689, 721, 366, 767, 709, 6081, 813, 436, 1062, 362, 5176, 13, 51472], "temperature": 0.0, "avg_logprob": -0.13217311824133637, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.002621585736051202}, {"id": 1521, "seek": 909760, "start": 9120.48, "end": 9125.6, "text": " What does it look like for a takeoff to take less than a year? I guess things have to go really well.", "tokens": [51508, 708, 775, 309, 574, 411, 337, 257, 747, 4506, 281, 747, 1570, 813, 257, 1064, 30, 286, 2041, 721, 362, 281, 352, 534, 731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13217311824133637, "compression_ratio": 1.718045112781955, "no_speech_prob": 0.002621585736051202}, {"id": 1522, "seek": 912560, "start": 9125.6, "end": 9133.12, "text": " So I think a few things have to go well. So one possibility is that the difficulty gap is just", "tokens": [50364, 407, 286, 519, 257, 1326, 721, 362, 281, 352, 731, 13, 407, 472, 7959, 307, 300, 264, 10360, 7417, 307, 445, 50740], "temperature": 0.0, "avg_logprob": -0.16950997439297763, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.0003045497869607061}, {"id": 1523, "seek": 912560, "start": 9133.12, "end": 9138.4, "text": " pretty narrow. I mean, this isn't necessarily going well for anyone to be honest. And this is", "tokens": [50740, 1238, 9432, 13, 286, 914, 11, 341, 1943, 380, 4725, 516, 731, 337, 2878, 281, 312, 3245, 13, 400, 341, 307, 51004], "temperature": 0.0, "avg_logprob": -0.16950997439297763, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.0003045497869607061}, {"id": 1524, "seek": 912560, "start": 9138.4, "end": 9142.4, "text": " just a very intense and scary situation, but... Yeah, it's good clarification.", "tokens": [51004, 445, 257, 588, 9447, 293, 6958, 2590, 11, 457, 485, 865, 11, 309, 311, 665, 34449, 13, 51204], "temperature": 0.0, "avg_logprob": -0.16950997439297763, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.0003045497869607061}, {"id": 1525, "seek": 912560, "start": 9144.4, "end": 9150.880000000001, "text": " If you only need 10 times as much effect to compute, to train 100% automation AI compared", "tokens": [51304, 759, 291, 787, 643, 1266, 1413, 382, 709, 1802, 281, 14722, 11, 281, 3847, 2319, 4, 17769, 7318, 5347, 51628], "temperature": 0.0, "avg_logprob": -0.16950997439297763, "compression_ratio": 1.5127118644067796, "no_speech_prob": 0.0003045497869607061}, {"id": 1526, "seek": 915088, "start": 9150.88, "end": 9158.16, "text": " to 20% automation AI, then that could just be the algorithmic improvements in one year", "tokens": [50364, 281, 945, 4, 17769, 7318, 11, 550, 300, 727, 445, 312, 264, 9284, 299, 13797, 294, 472, 1064, 50728], "temperature": 0.0, "avg_logprob": -0.1288637285647185, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01631299965083599}, {"id": 1527, "seek": 915088, "start": 9158.16, "end": 9163.119999999999, "text": " once the AI's are helping you to a significant degree. Right. Or it could just be spending", "tokens": [50728, 1564, 264, 7318, 311, 366, 4315, 291, 281, 257, 4776, 4314, 13, 1779, 13, 1610, 309, 727, 445, 312, 6434, 50976], "temperature": 0.0, "avg_logprob": -0.1288637285647185, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01631299965083599}, {"id": 1528, "seek": 915088, "start": 9163.679999999998, "end": 9168.88, "text": " three times as much on chips as you did the year before, and maybe those chips are three times as", "tokens": [51004, 1045, 1413, 382, 709, 322, 11583, 382, 291, 630, 264, 1064, 949, 11, 293, 1310, 729, 11583, 366, 1045, 1413, 382, 51264], "temperature": 0.0, "avg_logprob": -0.1288637285647185, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01631299965083599}, {"id": 1529, "seek": 915088, "start": 9168.88, "end": 9175.279999999999, "text": " efficient as the year before. And so that, you know, a narrow difficulty gap alone could get you there.", "tokens": [51264, 7148, 382, 264, 1064, 949, 13, 400, 370, 300, 11, 291, 458, 11, 257, 9432, 10360, 7417, 3312, 727, 483, 291, 456, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1288637285647185, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.01631299965083599}, {"id": 1530, "seek": 917528, "start": 9175.92, "end": 9183.12, "text": " Another possibility is that once we hit 20% AI, there is just a very quick and significant", "tokens": [50396, 3996, 7959, 307, 300, 1564, 321, 2045, 945, 4, 7318, 11, 456, 307, 445, 257, 588, 1702, 293, 4776, 50756], "temperature": 0.0, "avg_logprob": -0.1105681020160054, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.01731329783797264}, {"id": 1531, "seek": 917528, "start": 9183.12, "end": 9189.76, "text": " increase in the money being spent on training runs. So if people get really excited, this is", "tokens": [50756, 3488, 294, 264, 1460, 885, 4418, 322, 3097, 6676, 13, 407, 498, 561, 483, 534, 2919, 11, 341, 307, 51088], "temperature": 0.0, "avg_logprob": -0.1105681020160054, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.01731329783797264}, {"id": 1532, "seek": 917528, "start": 9189.76, "end": 9194.0, "text": " kind of a really awful scenario. People get excited about kind of what they could do with", "tokens": [51088, 733, 295, 257, 534, 11232, 9005, 13, 3432, 483, 2919, 466, 733, 295, 437, 436, 727, 360, 365, 51300], "temperature": 0.0, "avg_logprob": -0.1105681020160054, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.01731329783797264}, {"id": 1533, "seek": 917528, "start": 9194.0, "end": 9199.36, "text": " those capabilities, and they spend 10 times as much on a training run the next year, maybe by", "tokens": [51300, 729, 10862, 11, 293, 436, 3496, 1266, 1413, 382, 709, 322, 257, 3097, 1190, 264, 958, 1064, 11, 1310, 538, 51568], "temperature": 0.0, "avg_logprob": -0.1105681020160054, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.01731329783797264}, {"id": 1534, "seek": 919936, "start": 9199.36, "end": 9204.880000000001, "text": " combining with some big existing compute providers, then that could allow you to cover,", "tokens": [50364, 21928, 365, 512, 955, 6741, 14722, 11330, 11, 550, 300, 727, 2089, 291, 281, 2060, 11, 50640], "temperature": 0.0, "avg_logprob": -0.13605490597811612, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0482659786939621}, {"id": 1535, "seek": 919936, "start": 9204.880000000001, "end": 9209.52, "text": " you know, a kind of slightly bigger difficulty gap just within a year. Maybe even if the difficulty", "tokens": [50640, 291, 458, 11, 257, 733, 295, 4748, 3801, 10360, 7417, 445, 1951, 257, 1064, 13, 2704, 754, 498, 264, 10360, 50872], "temperature": 0.0, "avg_logprob": -0.13605490597811612, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0482659786939621}, {"id": 1536, "seek": 919936, "start": 9209.52, "end": 9216.880000000001, "text": " gap was 100 times or 300 times much compute needed for AI, then you could still, by spending a lot", "tokens": [50872, 7417, 390, 2319, 1413, 420, 6641, 1413, 709, 14722, 2978, 337, 7318, 11, 550, 291, 727, 920, 11, 538, 6434, 257, 688, 51240], "temperature": 0.0, "avg_logprob": -0.13605490597811612, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0482659786939621}, {"id": 1537, "seek": 919936, "start": 9216.880000000001, "end": 9221.76, "text": " more and combine with a few algorithmic and hardware improvements, cross it pretty quickly.", "tokens": [51240, 544, 293, 10432, 365, 257, 1326, 9284, 299, 293, 8837, 13797, 11, 3278, 309, 1238, 2661, 13, 51484], "temperature": 0.0, "avg_logprob": -0.13605490597811612, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0482659786939621}, {"id": 1538, "seek": 919936, "start": 9221.76, "end": 9227.92, "text": " Yeah, okay. And then, you know, one quick third possibility is just that, like I said earlier,", "tokens": [51484, 865, 11, 1392, 13, 400, 550, 11, 291, 458, 11, 472, 1702, 2636, 7959, 307, 445, 300, 11, 411, 286, 848, 3071, 11, 51792], "temperature": 0.0, "avg_logprob": -0.13605490597811612, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0482659786939621}, {"id": 1539, "seek": 922792, "start": 9228.8, "end": 9232.24, "text": " but maybe I haven't emphasized this enough, is that this framework is assuming everything is", "tokens": [50408, 457, 1310, 286, 2378, 380, 34068, 341, 1547, 11, 307, 300, 341, 8388, 307, 11926, 1203, 307, 50580], "temperature": 0.0, "avg_logprob": -0.09234107457674466, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.00046933686826378107}, {"id": 1540, "seek": 922792, "start": 9232.24, "end": 9237.68, "text": " continuous. So it's assuming that, you know, each time you increase the effect of compute", "tokens": [50580, 10957, 13, 407, 309, 311, 11926, 300, 11, 291, 458, 11, 1184, 565, 291, 3488, 264, 1802, 295, 14722, 50852], "temperature": 0.0, "avg_logprob": -0.09234107457674466, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.00046933686826378107}, {"id": 1541, "seek": 922792, "start": 9237.68, "end": 9242.4, "text": " in a training run by 10%, you get a kind of, you know, relatively modest incremental", "tokens": [50852, 294, 257, 3097, 1190, 538, 1266, 8923, 291, 483, 257, 733, 295, 11, 291, 458, 11, 7226, 25403, 35759, 51088], "temperature": 0.0, "avg_logprob": -0.09234107457674466, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.00046933686826378107}, {"id": 1542, "seek": 922792, "start": 9242.4, "end": 9247.36, "text": " improvement in AI capabilities. If there's actually some kind of discrete phase change,", "tokens": [51088, 10444, 294, 7318, 10862, 13, 759, 456, 311, 767, 512, 733, 295, 27706, 5574, 1319, 11, 51336], "temperature": 0.0, "avg_logprob": -0.09234107457674466, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.00046933686826378107}, {"id": 1543, "seek": 922792, "start": 9247.36, "end": 9252.4, "text": " then that could just be an alternative route that could produce a very fast takeoff. So there are,", "tokens": [51336, 550, 300, 727, 445, 312, 364, 8535, 7955, 300, 727, 5258, 257, 588, 2370, 747, 4506, 13, 407, 456, 366, 11, 51588], "temperature": 0.0, "avg_logprob": -0.09234107457674466, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.00046933686826378107}, {"id": 1544, "seek": 925240, "start": 9252.48, "end": 9254.64, "text": " there are, you know, a few different ways that that could happen.", "tokens": [50368, 456, 366, 11, 291, 458, 11, 257, 1326, 819, 2098, 300, 300, 727, 1051, 13, 50476], "temperature": 0.0, "avg_logprob": -0.08477642560246015, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0011327528627589345}, {"id": 1545, "seek": 925240, "start": 9254.64, "end": 9260.48, "text": " Right. Okay, that's just very scary. So there are, I guess,", "tokens": [50476, 1779, 13, 1033, 11, 300, 311, 445, 588, 6958, 13, 407, 456, 366, 11, 286, 2041, 11, 50768], "temperature": 0.0, "avg_logprob": -0.08477642560246015, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0011327528627589345}, {"id": 1546, "seek": 925240, "start": 9260.48, "end": 9266.0, "text": " worlds where things go very well for AI companies, but maybe very terribly for humanity,", "tokens": [50768, 13401, 689, 721, 352, 588, 731, 337, 7318, 3431, 11, 457, 1310, 588, 22903, 337, 10243, 11, 51044], "temperature": 0.0, "avg_logprob": -0.08477642560246015, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0011327528627589345}, {"id": 1547, "seek": 925240, "start": 9266.56, "end": 9272.32, "text": " or kind of very poorly for AI companies, hopefully better for humanity. And the median", "tokens": [51072, 420, 733, 295, 588, 22271, 337, 7318, 3431, 11, 4696, 1101, 337, 10243, 13, 400, 264, 26779, 51360], "temperature": 0.0, "avg_logprob": -0.08477642560246015, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0011327528627589345}, {"id": 1548, "seek": 925240, "start": 9272.32, "end": 9278.4, "text": " estimate is something like a couple of years to get from AI can do 20% of cognitive tasks to AI", "tokens": [51360, 12539, 307, 746, 411, 257, 1916, 295, 924, 281, 483, 490, 7318, 393, 360, 945, 4, 295, 15605, 9608, 281, 7318, 51664], "temperature": 0.0, "avg_logprob": -0.08477642560246015, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0011327528627589345}, {"id": 1549, "seek": 927840, "start": 9278.4, "end": 9283.52, "text": " can do 100% of them. That seems like, yeah, an important thing to take away from this model.", "tokens": [50364, 393, 360, 2319, 4, 295, 552, 13, 663, 2544, 411, 11, 1338, 11, 364, 1021, 551, 281, 747, 1314, 490, 341, 2316, 13, 50620], "temperature": 0.0, "avg_logprob": -0.08873662378034021, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0033637473825365305}, {"id": 1550, "seek": 927840, "start": 9284.56, "end": 9288.32, "text": " Were there other things that you learned in this, in the process of writing this report?", "tokens": [50672, 12448, 456, 661, 721, 300, 291, 3264, 294, 341, 11, 294, 264, 1399, 295, 3579, 341, 2275, 30, 50860], "temperature": 0.0, "avg_logprob": -0.08873662378034021, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0033637473825365305}, {"id": 1551, "seek": 927840, "start": 9289.199999999999, "end": 9293.92, "text": " Yeah, another big update for me was the, that I think there's going to be a pretty strong", "tokens": [50904, 865, 11, 1071, 955, 5623, 337, 385, 390, 264, 11, 300, 286, 519, 456, 311, 516, 281, 312, 257, 1238, 2068, 51140], "temperature": 0.0, "avg_logprob": -0.08873662378034021, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0033637473825365305}, {"id": 1552, "seek": 927840, "start": 9293.92, "end": 9301.44, "text": " correlation between how far away in time it is until we develop AGI and how fast takeoff will be.", "tokens": [51140, 20009, 1296, 577, 1400, 1314, 294, 565, 309, 307, 1826, 321, 1499, 316, 26252, 293, 577, 2370, 747, 4506, 486, 312, 13, 51516], "temperature": 0.0, "avg_logprob": -0.08873662378034021, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0033637473825365305}, {"id": 1553, "seek": 927840, "start": 9302.96, "end": 9308.0, "text": " So in particular, if you have short AI timelines, meaning that you think we'll develop AGI pretty", "tokens": [51592, 407, 294, 1729, 11, 498, 291, 362, 2099, 7318, 45886, 11, 3620, 300, 291, 519, 321, 603, 1499, 316, 26252, 1238, 51844], "temperature": 0.0, "avg_logprob": -0.08873662378034021, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0033637473825365305}, {"id": 1554, "seek": 930800, "start": 9308.0, "end": 9313.04, "text": " soon, then I think there are a few reasons to expect it, and especially fast takeoff.", "tokens": [50364, 2321, 11, 550, 286, 519, 456, 366, 257, 1326, 4112, 281, 2066, 309, 11, 293, 2318, 2370, 747, 4506, 13, 50616], "temperature": 0.0, "avg_logprob": -0.07436716873033912, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0024415994994342327}, {"id": 1555, "seek": 930800, "start": 9313.92, "end": 9320.0, "text": " And so one reason is that if you have short timelines, and that's probably going to mean", "tokens": [50660, 400, 370, 472, 1778, 307, 300, 498, 291, 362, 2099, 45886, 11, 293, 300, 311, 1391, 516, 281, 914, 50964], "temperature": 0.0, "avg_logprob": -0.07436716873033912, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0024415994994342327}, {"id": 1556, "seek": 930800, "start": 9320.0, "end": 9325.04, "text": " that you've got a smaller difficulty gap, because if you think that, you know, we don't need that", "tokens": [50964, 300, 291, 600, 658, 257, 4356, 10360, 7417, 11, 570, 498, 291, 519, 300, 11, 291, 458, 11, 321, 500, 380, 643, 300, 51216], "temperature": 0.0, "avg_logprob": -0.07436716873033912, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0024415994994342327}, {"id": 1557, "seek": 930800, "start": 9325.04, "end": 9330.48, "text": " much more effective compute to develop AGI compared with today, then you're probably also", "tokens": [51216, 709, 544, 4942, 14722, 281, 1499, 316, 26252, 5347, 365, 965, 11, 550, 291, 434, 1391, 611, 51488], "temperature": 0.0, "avg_logprob": -0.07436716873033912, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0024415994994342327}, {"id": 1558, "seek": 930800, "start": 9330.48, "end": 9336.0, "text": " going to think that the difference in effect to compute for 20% AI to AGI is also going to be", "tokens": [51488, 516, 281, 519, 300, 264, 2649, 294, 1802, 281, 14722, 337, 945, 4, 7318, 281, 316, 26252, 307, 611, 516, 281, 312, 51764], "temperature": 0.0, "avg_logprob": -0.07436716873033912, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.0024415994994342327}, {"id": 1559, "seek": 933600, "start": 9336.0, "end": 9341.12, "text": " small. So that will push you towards a faster takeoff. Right. Okay. That makes sense.", "tokens": [50364, 1359, 13, 407, 300, 486, 2944, 291, 3030, 257, 4663, 747, 4506, 13, 1779, 13, 1033, 13, 663, 1669, 2020, 13, 50620], "temperature": 0.0, "avg_logprob": -0.08031510258768941, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.004592002369463444}, {"id": 1560, "seek": 933600, "start": 9341.76, "end": 9347.92, "text": " Another thing is that if you think AGI is going to be here fairly soon, then it's plausible that we", "tokens": [50652, 3996, 551, 307, 300, 498, 291, 519, 316, 26252, 307, 516, 281, 312, 510, 6457, 2321, 11, 550, 309, 311, 39925, 300, 321, 50960], "temperature": 0.0, "avg_logprob": -0.08031510258768941, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.004592002369463444}, {"id": 1561, "seek": 933600, "start": 9347.92, "end": 9355.44, "text": " won't be spending that much money on training runs shortly before we have AGI, which means it", "tokens": [50960, 1582, 380, 312, 6434, 300, 709, 1460, 322, 3097, 6676, 13392, 949, 321, 362, 316, 26252, 11, 597, 1355, 309, 51336], "temperature": 0.0, "avg_logprob": -0.08031510258768941, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.004592002369463444}, {"id": 1562, "seek": 933600, "start": 9355.44, "end": 9360.96, "text": " might be very practical and doable to quickly increase the amount that we're spending by maybe", "tokens": [51336, 1062, 312, 588, 8496, 293, 41183, 281, 2661, 3488, 264, 2372, 300, 321, 434, 6434, 538, 1310, 51612], "temperature": 0.0, "avg_logprob": -0.08031510258768941, "compression_ratio": 1.5518672199170125, "no_speech_prob": 0.004592002369463444}, {"id": 1563, "seek": 936096, "start": 9360.96, "end": 9368.16, "text": " a factor of 10 or a factor of 100 just around the time that we are approaching AGI. Right. And so", "tokens": [50364, 257, 5952, 295, 1266, 420, 257, 5952, 295, 2319, 445, 926, 264, 565, 300, 321, 366, 14908, 316, 26252, 13, 1779, 13, 400, 370, 50724], "temperature": 0.0, "avg_logprob": -0.07240516856565314, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.02446088008582592}, {"id": 1564, "seek": 936096, "start": 9368.16, "end": 9374.64, "text": " we could get a really very quick increase in the amount of effective compute being used on a training", "tokens": [50724, 321, 727, 483, 257, 534, 588, 1702, 3488, 294, 264, 2372, 295, 4942, 14722, 885, 1143, 322, 257, 3097, 51048], "temperature": 0.0, "avg_logprob": -0.07240516856565314, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.02446088008582592}, {"id": 1565, "seek": 936096, "start": 9374.64, "end": 9379.919999999998, "text": " run as a kind of almost direct consequence of it being a short timeline. Whereas if timelines were", "tokens": [51048, 1190, 382, 257, 733, 295, 1920, 2047, 18326, 295, 309, 885, 257, 2099, 12933, 13, 13813, 498, 45886, 645, 51312], "temperature": 0.0, "avg_logprob": -0.07240516856565314, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.02446088008582592}, {"id": 1566, "seek": 936096, "start": 9379.919999999998, "end": 9384.08, "text": " long, then like I said, maybe we were already using all the chips in the world on the biggest", "tokens": [51312, 938, 11, 550, 411, 286, 848, 11, 1310, 321, 645, 1217, 1228, 439, 264, 11583, 294, 264, 1002, 322, 264, 3880, 51520], "temperature": 0.0, "avg_logprob": -0.07240516856565314, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.02446088008582592}, {"id": 1567, "seek": 936096, "start": 9384.08, "end": 9388.96, "text": " training run, you know, before we get to AGI. And so that that source of growth is no longer", "tokens": [51520, 3097, 1190, 11, 291, 458, 11, 949, 321, 483, 281, 316, 26252, 13, 400, 370, 300, 300, 4009, 295, 4599, 307, 572, 2854, 51764], "temperature": 0.0, "avg_logprob": -0.07240516856565314, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.02446088008582592}, {"id": 1568, "seek": 938896, "start": 9388.96, "end": 9393.759999999998, "text": " available. Right. Yeah, that makes sense. Another thing with short timelines is that", "tokens": [50364, 2435, 13, 1779, 13, 865, 11, 300, 1669, 2020, 13, 3996, 551, 365, 2099, 45886, 307, 300, 50604], "temperature": 0.0, "avg_logprob": -0.08635639302870807, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.0011724594514817}, {"id": 1569, "seek": 938896, "start": 9394.32, "end": 9399.839999999998, "text": " it implies that there's going to be a period of just a few years where we get really significant", "tokens": [50632, 309, 18779, 300, 456, 311, 516, 281, 312, 257, 2896, 295, 445, 257, 1326, 924, 689, 321, 483, 534, 4776, 50908], "temperature": 0.0, "avg_logprob": -0.08635639302870807, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.0011724594514817}, {"id": 1570, "seek": 938896, "start": 9399.839999999998, "end": 9408.96, "text": " automation of AI R&D and kind of really significantly increase the size of the effective kind of", "tokens": [50908, 17769, 295, 7318, 497, 5, 35, 293, 733, 295, 534, 10591, 3488, 264, 2744, 295, 264, 4942, 733, 295, 51364], "temperature": 0.0, "avg_logprob": -0.08635639302870807, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.0011724594514817}, {"id": 1571, "seek": 938896, "start": 9408.96, "end": 9414.72, "text": " research workforce that's working on improving AI due to these AIs. And if that happens very", "tokens": [51364, 2132, 14201, 300, 311, 1364, 322, 11470, 7318, 3462, 281, 613, 316, 6802, 13, 400, 498, 300, 2314, 588, 51652], "temperature": 0.0, "avg_logprob": -0.08635639302870807, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.0011724594514817}, {"id": 1572, "seek": 941472, "start": 9414.72, "end": 9419.679999999998, "text": " quickly is kind of you suddenly kind of five X the size of your research team, then you would", "tokens": [50364, 2661, 307, 733, 295, 291, 5800, 733, 295, 1732, 1783, 264, 2744, 295, 428, 2132, 1469, 11, 550, 291, 576, 50612], "temperature": 0.0, "avg_logprob": -0.10960054860531705, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.005645361728966236}, {"id": 1573, "seek": 941472, "start": 9419.679999999998, "end": 9425.359999999999, "text": " expect that to just really significantly speed up progress. And that's just an especially kind of", "tokens": [50612, 2066, 300, 281, 445, 534, 10591, 3073, 493, 4205, 13, 400, 300, 311, 445, 364, 2318, 733, 295, 50896], "temperature": 0.0, "avg_logprob": -0.10960054860531705, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.005645361728966236}, {"id": 1574, "seek": 941472, "start": 9425.359999999999, "end": 9431.519999999999, "text": " dramatic effect in short timelines. And the last point is that if if timelines are short,", "tokens": [50896, 12023, 1802, 294, 2099, 45886, 13, 400, 264, 1036, 935, 307, 300, 498, 498, 45886, 366, 2099, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10960054860531705, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.005645361728966236}, {"id": 1575, "seek": 941472, "start": 9431.519999999999, "end": 9437.439999999999, "text": " then there's less hope for eating up all the remaining possibilities for hardware progress", "tokens": [51204, 550, 456, 311, 1570, 1454, 337, 3936, 493, 439, 264, 8877, 12178, 337, 8837, 4205, 51500], "temperature": 0.0, "avg_logprob": -0.10960054860531705, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.005645361728966236}, {"id": 1576, "seek": 941472, "start": 9438.16, "end": 9442.32, "text": " and kind of running out of, you know, hitting those physical limits that we mentioned. Right.", "tokens": [51536, 293, 733, 295, 2614, 484, 295, 11, 291, 458, 11, 8850, 729, 4001, 10406, 300, 321, 2835, 13, 1779, 13, 51744], "temperature": 0.0, "avg_logprob": -0.10960054860531705, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.005645361728966236}, {"id": 1577, "seek": 944232, "start": 9443.039999999999, "end": 9446.96, "text": " And, you know, analogously with algorithmic improvements, if timelines are short, then", "tokens": [50400, 400, 11, 291, 458, 11, 16660, 5098, 365, 9284, 299, 13797, 11, 498, 45886, 366, 2099, 11, 550, 50596], "temperature": 0.0, "avg_logprob": -0.09440564708549436, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.0008495580987073481}, {"id": 1578, "seek": 944232, "start": 9446.96, "end": 9453.44, "text": " there's much less hope that we kind of run out of possible algorithmic improvements before we", "tokens": [50596, 456, 311, 709, 1570, 1454, 300, 321, 733, 295, 1190, 484, 295, 1944, 9284, 299, 13797, 949, 321, 50920], "temperature": 0.0, "avg_logprob": -0.09440564708549436, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.0008495580987073481}, {"id": 1579, "seek": 944232, "start": 9453.44, "end": 9459.279999999999, "text": " get there. So I do think that, you know, if, like I believe a lot of the labs do, they have", "tokens": [50920, 483, 456, 13, 407, 286, 360, 519, 300, 11, 291, 458, 11, 498, 11, 411, 286, 1697, 257, 688, 295, 264, 20339, 360, 11, 436, 362, 51212], "temperature": 0.0, "avg_logprob": -0.09440564708549436, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.0008495580987073481}, {"id": 1580, "seek": 944232, "start": 9459.279999999999, "end": 9466.16, "text": " pretty, pretty short timelines, kind of expecting AGI in the 2020s or early 2030s. And, you know,", "tokens": [51212, 1238, 11, 1238, 2099, 45886, 11, 733, 295, 9650, 316, 26252, 294, 264, 4808, 82, 420, 2440, 28638, 82, 13, 400, 11, 291, 458, 11, 51556], "temperature": 0.0, "avg_logprob": -0.09440564708549436, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.0008495580987073481}, {"id": 1581, "seek": 944232, "start": 9466.16, "end": 9470.56, "text": " I think that the implication of that is that takeoff speed is going to be on the on the", "tokens": [51556, 286, 519, 300, 264, 37814, 295, 300, 307, 300, 747, 4506, 3073, 307, 516, 281, 312, 322, 264, 322, 264, 51776], "temperature": 0.0, "avg_logprob": -0.09440564708549436, "compression_ratio": 1.7615384615384615, "no_speech_prob": 0.0008495580987073481}, {"id": 1582, "seek": 947056, "start": 9470.56, "end": 9475.76, "text": " shorter end of what we've been discussing. So, you know, less than less than three years", "tokens": [50364, 11639, 917, 295, 437, 321, 600, 668, 10850, 13, 407, 11, 291, 458, 11, 1570, 813, 1570, 813, 1045, 924, 50624], "temperature": 0.0, "avg_logprob": -0.1338494618733724, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.001358900568448007}, {"id": 1583, "seek": 947056, "start": 9475.76, "end": 9481.76, "text": " and very plausibly less than one year and, you know, one or two years being maybe maybe the most", "tokens": [50624, 293, 588, 34946, 3545, 1570, 813, 472, 1064, 293, 11, 291, 458, 11, 472, 420, 732, 924, 885, 1310, 1310, 264, 881, 50924], "temperature": 0.0, "avg_logprob": -0.1338494618733724, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.001358900568448007}, {"id": 1584, "seek": 947056, "start": 9482.4, "end": 9489.119999999999, "text": " most likely. That's yeah, it's just it's just terrifying. I mean, we're talking about", "tokens": [50956, 881, 3700, 13, 663, 311, 1338, 11, 309, 311, 445, 309, 311, 445, 18106, 13, 286, 914, 11, 321, 434, 1417, 466, 51292], "temperature": 0.0, "avg_logprob": -0.1338494618733724, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.001358900568448007}, {"id": 1585, "seek": 947056, "start": 9489.92, "end": 9496.48, "text": " by the end of the decade, we've got AI that can do everything humans can do and probably more.", "tokens": [51332, 538, 264, 917, 295, 264, 10378, 11, 321, 600, 658, 7318, 300, 393, 360, 1203, 6255, 393, 360, 293, 1391, 544, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1338494618733724, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.001358900568448007}, {"id": 1586, "seek": 949648, "start": 9497.359999999999, "end": 9504.08, "text": " Yeah. And we're talking about the labs who train them by default will have access to at least 100", "tokens": [50408, 865, 13, 400, 321, 434, 1417, 466, 264, 20339, 567, 3847, 552, 538, 7576, 486, 362, 2105, 281, 412, 1935, 2319, 50744], "temperature": 0.0, "avg_logprob": -0.10102367401123047, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004094414878636599}, {"id": 1587, "seek": 949648, "start": 9504.08, "end": 9510.24, "text": " million of them that they can run and then probably soon after, you know, a billion or many", "tokens": [50744, 2459, 295, 552, 300, 436, 393, 1190, 293, 550, 1391, 2321, 934, 11, 291, 458, 11, 257, 5218, 420, 867, 51052], "temperature": 0.0, "avg_logprob": -0.10102367401123047, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004094414878636599}, {"id": 1588, "seek": 949648, "start": 9510.24, "end": 9516.48, "text": " billions that they can run. So it is it is pretty terrifying. Yeah. That's basically because it takes", "tokens": [51052, 17375, 300, 436, 393, 1190, 13, 407, 309, 307, 309, 307, 1238, 18106, 13, 865, 13, 663, 311, 1936, 570, 309, 2516, 51364], "temperature": 0.0, "avg_logprob": -0.10102367401123047, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004094414878636599}, {"id": 1589, "seek": 949648, "start": 9516.48, "end": 9522.64, "text": " so much effective compute to run the training runs that once they've got AGI, they're just", "tokens": [51364, 370, 709, 4942, 14722, 281, 1190, 264, 3097, 6676, 300, 1564, 436, 600, 658, 316, 26252, 11, 436, 434, 445, 51672], "temperature": 0.0, "avg_logprob": -0.10102367401123047, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.004094414878636599}, {"id": 1590, "seek": 952264, "start": 9523.279999999999, "end": 9527.92, "text": " they can run millions or billions of copies on those chips. Exactly. So I think initially,", "tokens": [50396, 436, 393, 1190, 6803, 420, 17375, 295, 14341, 322, 729, 11583, 13, 7587, 13, 407, 286, 519, 9105, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11436718986147926, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008247614838182926}, {"id": 1591, "seek": 952264, "start": 9527.92, "end": 9533.039999999999, "text": " they'll be able to run millions of copies. But because of the efficiency improvements", "tokens": [50628, 436, 603, 312, 1075, 281, 1190, 6803, 295, 14341, 13, 583, 570, 295, 264, 10493, 13797, 50884], "temperature": 0.0, "avg_logprob": -0.11436718986147926, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008247614838182926}, {"id": 1592, "seek": 952264, "start": 9533.76, "end": 9537.76, "text": " that they can probably fairly quickly make, especially with all those copies working on it,", "tokens": [50920, 300, 436, 393, 1391, 6457, 2661, 652, 11, 2318, 365, 439, 729, 14341, 1364, 322, 309, 11, 51120], "temperature": 0.0, "avg_logprob": -0.11436718986147926, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008247614838182926}, {"id": 1593, "seek": 952264, "start": 9538.64, "end": 9545.84, "text": " I think it won't be long before they can do billions. It's really bewildering stuff. And", "tokens": [51164, 286, 519, 309, 1582, 380, 312, 938, 949, 436, 393, 360, 17375, 13, 467, 311, 534, 17897, 793, 1794, 1507, 13, 400, 51524], "temperature": 0.0, "avg_logprob": -0.11436718986147926, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.008247614838182926}, {"id": 1594, "seek": 954584, "start": 9546.8, "end": 9553.28, "text": " I find yeah, I find it pretty my brain really doesn't want to believe it. I find it really,", "tokens": [50412, 286, 915, 1338, 11, 286, 915, 309, 1238, 452, 3567, 534, 1177, 380, 528, 281, 1697, 309, 13, 286, 915, 309, 534, 11, 50736], "temperature": 0.0, "avg_logprob": -0.12175654430015415, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.004358248319476843}, {"id": 1595, "seek": 954584, "start": 9553.28, "end": 9561.2, "text": " really hard to wrap my head around. Yeah. Any other takeaways? So another takeaway is that I think", "tokens": [50736, 534, 1152, 281, 7019, 452, 1378, 926, 13, 865, 13, 2639, 661, 45584, 30, 407, 1071, 30681, 307, 300, 286, 519, 51132], "temperature": 0.0, "avg_logprob": -0.12175654430015415, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.004358248319476843}, {"id": 1596, "seek": 954584, "start": 9561.2, "end": 9566.8, "text": " AGI is more likely to happen before 2040 compared to what I used to think. And so the reason for", "tokens": [51132, 316, 26252, 307, 544, 3700, 281, 1051, 949, 945, 5254, 5347, 281, 437, 286, 1143, 281, 519, 13, 400, 370, 264, 1778, 337, 51412], "temperature": 0.0, "avg_logprob": -0.12175654430015415, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.004358248319476843}, {"id": 1597, "seek": 954584, "start": 9566.8, "end": 9573.52, "text": " that is that the way I used to think about this is, okay, what does it take to train AGI? And", "tokens": [51412, 300, 307, 300, 264, 636, 286, 1143, 281, 519, 466, 341, 307, 11, 1392, 11, 437, 775, 309, 747, 281, 3847, 316, 26252, 30, 400, 51748], "temperature": 0.0, "avg_logprob": -0.12175654430015415, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.004358248319476843}, {"id": 1598, "seek": 957352, "start": 9573.6, "end": 9579.6, "text": " how long will it be until we have that much effective compute? Whereas now the way I think", "tokens": [50368, 577, 938, 486, 309, 312, 1826, 321, 362, 300, 709, 4942, 14722, 30, 13813, 586, 264, 636, 286, 519, 50668], "temperature": 0.0, "avg_logprob": -0.11998978665000513, "compression_ratio": 1.575, "no_speech_prob": 0.008074883371591568}, {"id": 1599, "seek": 957352, "start": 9579.6, "end": 9587.52, "text": " about it is more, well, what will it take to train AI that is really profitable or really good", "tokens": [50668, 466, 309, 307, 544, 11, 731, 11, 437, 486, 309, 747, 281, 3847, 7318, 300, 307, 534, 21608, 420, 534, 665, 51064], "temperature": 0.0, "avg_logprob": -0.11998978665000513, "compression_ratio": 1.575, "no_speech_prob": 0.008074883371591568}, {"id": 1600, "seek": 957352, "start": 9587.52, "end": 9592.640000000001, "text": " at accelerating AI R&D? Right. And if we get either of those two things, then we that we'd", "tokens": [51064, 412, 34391, 7318, 497, 5, 35, 30, 1779, 13, 400, 498, 321, 483, 2139, 295, 729, 732, 721, 11, 550, 321, 300, 321, 1116, 51320], "temperature": 0.0, "avg_logprob": -0.11998978665000513, "compression_ratio": 1.575, "no_speech_prob": 0.008074883371591568}, {"id": 1601, "seek": 957352, "start": 9592.640000000001, "end": 9600.0, "text": " expect that to accelerate future AI progress, such that, you know, kind of future progress is faster.", "tokens": [51320, 2066, 300, 281, 21341, 2027, 7318, 4205, 11, 1270, 300, 11, 291, 458, 11, 733, 295, 2027, 4205, 307, 4663, 13, 51688], "temperature": 0.0, "avg_logprob": -0.11998978665000513, "compression_ratio": 1.575, "no_speech_prob": 0.008074883371591568}, {"id": 1602, "seek": 960000, "start": 9600.0, "end": 9606.56, "text": " And as long as AGI is incredibly hard, we are then able to reach AGI within the next couple", "tokens": [50364, 400, 382, 938, 382, 316, 26252, 307, 6252, 1152, 11, 321, 366, 550, 1075, 281, 2524, 316, 26252, 1951, 264, 958, 1916, 50692], "temperature": 0.0, "avg_logprob": -0.08431723713874817, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0038082487881183624}, {"id": 1603, "seek": 960000, "start": 9606.56, "end": 9611.12, "text": " of decades. So, you know, my current read, I just think it's really likely that by the by the end", "tokens": [50692, 295, 7878, 13, 407, 11, 291, 458, 11, 452, 2190, 1401, 11, 286, 445, 519, 309, 311, 534, 3700, 300, 538, 264, 538, 264, 917, 50920], "temperature": 0.0, "avg_logprob": -0.08431723713874817, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0038082487881183624}, {"id": 1604, "seek": 960000, "start": 9611.12, "end": 9618.96, "text": " of this decade, we have AI which is really profitable and or significantly accelerates AI R&D.", "tokens": [50920, 295, 341, 10378, 11, 321, 362, 7318, 597, 307, 534, 21608, 293, 420, 10591, 10172, 1024, 7318, 497, 5, 35, 13, 51312], "temperature": 0.0, "avg_logprob": -0.08431723713874817, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0038082487881183624}, {"id": 1605, "seek": 960000, "start": 9619.52, "end": 9624.0, "text": " And so it's then kind of hard for me to imagine how that dynamic plays out without", "tokens": [51340, 400, 370, 309, 311, 550, 733, 295, 1152, 337, 385, 281, 3811, 577, 300, 8546, 5749, 484, 1553, 51564], "temperature": 0.0, "avg_logprob": -0.08431723713874817, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.0038082487881183624}, {"id": 1606, "seek": 962400, "start": 9624.0, "end": 9632.08, "text": " us getting to AGI by 2040. Wow. I think you just need AGI to be really hard. And as to fail to get", "tokens": [50364, 505, 1242, 281, 316, 26252, 538, 945, 5254, 13, 3153, 13, 286, 519, 291, 445, 643, 316, 26252, 281, 312, 534, 1152, 13, 400, 382, 281, 3061, 281, 483, 50768], "temperature": 0.0, "avg_logprob": -0.09457436561584473, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.006442979909479618}, {"id": 1607, "seek": 962400, "start": 9632.08, "end": 9637.2, "text": " it despite, you know, a huge amount of investment and help from AI systems and developing it.", "tokens": [50768, 309, 7228, 11, 291, 458, 11, 257, 2603, 2372, 295, 6078, 293, 854, 490, 7318, 3652, 293, 6416, 309, 13, 51024], "temperature": 0.0, "avg_logprob": -0.09457436561584473, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.006442979909479618}, {"id": 1608, "seek": 962400, "start": 9637.76, "end": 9643.52, "text": " Right. Okay. So to make sure I understand the big thing doing the work there is you used to think", "tokens": [51052, 1779, 13, 1033, 13, 407, 281, 652, 988, 286, 1223, 264, 955, 551, 884, 264, 589, 456, 307, 291, 1143, 281, 519, 51340], "temperature": 0.0, "avg_logprob": -0.09457436561584473, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.006442979909479618}, {"id": 1609, "seek": 962400, "start": 9644.08, "end": 9651.36, "text": " about how hard it was to have humans figure out how to train AI systems to do everything humans", "tokens": [51368, 466, 577, 1152, 309, 390, 281, 362, 6255, 2573, 484, 577, 281, 3847, 7318, 3652, 281, 360, 1203, 6255, 51732], "temperature": 0.0, "avg_logprob": -0.09457436561584473, "compression_ratio": 1.5691056910569106, "no_speech_prob": 0.006442979909479618}, {"id": 1610, "seek": 965136, "start": 9651.36, "end": 9657.68, "text": " can do. And now you're like, it's really just how hard is it to train AI systems to get really good", "tokens": [50364, 393, 360, 13, 400, 586, 291, 434, 411, 11, 309, 311, 534, 445, 577, 1152, 307, 309, 281, 3847, 7318, 3652, 281, 483, 534, 665, 50680], "temperature": 0.0, "avg_logprob": -0.053851094576391846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004515485372394323}, {"id": 1611, "seek": 965136, "start": 9657.68, "end": 9664.880000000001, "text": " at AI R&D. And that's like a much smaller subset of cognitive tasks. And so you can get really", "tokens": [50680, 412, 7318, 497, 5, 35, 13, 400, 300, 311, 411, 257, 709, 4356, 25993, 295, 15605, 9608, 13, 400, 370, 291, 393, 483, 534, 51040], "temperature": 0.0, "avg_logprob": -0.053851094576391846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004515485372394323}, {"id": 1612, "seek": 965136, "start": 9664.880000000001, "end": 9672.24, "text": " accelerating growth in AI capabilities just by making a lot of progress on that one set of tasks.", "tokens": [51040, 34391, 4599, 294, 7318, 10862, 445, 538, 1455, 257, 688, 295, 4205, 322, 300, 472, 992, 295, 9608, 13, 51408], "temperature": 0.0, "avg_logprob": -0.053851094576391846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004515485372394323}, {"id": 1613, "seek": 965136, "start": 9672.24, "end": 9676.800000000001, "text": " Is that kind of right? Yeah, that's right. And I do think a distinct possibility is that we just", "tokens": [51408, 1119, 300, 733, 295, 558, 30, 865, 11, 300, 311, 558, 13, 400, 286, 360, 519, 257, 10644, 7959, 307, 300, 321, 445, 51636], "temperature": 0.0, "avg_logprob": -0.053851094576391846, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.004515485372394323}, {"id": 1614, "seek": 967680, "start": 9676.8, "end": 9682.56, "text": " train AI that isn't good at AI R&D, but makes lots and lots of money in the economy.", "tokens": [50364, 3847, 7318, 300, 1943, 380, 665, 412, 7318, 497, 5, 35, 11, 457, 1669, 3195, 293, 3195, 295, 1460, 294, 264, 5010, 13, 50652], "temperature": 0.0, "avg_logprob": -0.1191572364495725, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.008754104375839233}, {"id": 1615, "seek": 967680, "start": 9683.119999999999, "end": 9687.759999999998, "text": " And then there's just a bunch of investment in, yeah, I see in AI capabilities.", "tokens": [50680, 400, 550, 456, 311, 445, 257, 3840, 295, 6078, 294, 11, 1338, 11, 286, 536, 294, 7318, 10862, 13, 50912], "temperature": 0.0, "avg_logprob": -0.1191572364495725, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.008754104375839233}, {"id": 1616, "seek": 967680, "start": 9687.759999999998, "end": 9693.199999999999, "text": " And in designing better chips to run those AIs on. Got it. Okay. That makes that makes total sense.", "tokens": [50912, 400, 294, 14685, 1101, 11583, 281, 1190, 729, 316, 6802, 322, 13, 5803, 309, 13, 1033, 13, 663, 1669, 300, 1669, 3217, 2020, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1191572364495725, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.008754104375839233}, {"id": 1617, "seek": 967680, "start": 9694.0, "end": 9699.599999999999, "text": " Any other takeaways? So the biggest other one, I think, is something we've touched upon, which is", "tokens": [51224, 2639, 661, 45584, 30, 407, 264, 3880, 661, 472, 11, 286, 519, 11, 307, 746, 321, 600, 9828, 3564, 11, 597, 307, 51504], "temperature": 0.0, "avg_logprob": -0.1191572364495725, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.008754104375839233}, {"id": 1618, "seek": 969960, "start": 9699.6, "end": 9708.800000000001, "text": " that the transition from roughly human level AI to significantly super human AI, I think is is", "tokens": [50364, 300, 264, 6034, 490, 9810, 1952, 1496, 7318, 281, 10591, 1687, 1952, 7318, 11, 286, 519, 307, 307, 50824], "temperature": 0.0, "avg_logprob": -0.10359498505951256, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.07027711719274521}, {"id": 1619, "seek": 969960, "start": 9708.800000000001, "end": 9713.84, "text": " going to be probably very quick. Right. So I think probably that's going to take less than a year.", "tokens": [50824, 516, 281, 312, 1391, 588, 1702, 13, 1779, 13, 407, 286, 519, 1391, 300, 311, 516, 281, 747, 1570, 813, 257, 1064, 13, 51076], "temperature": 0.0, "avg_logprob": -0.10359498505951256, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.07027711719274521}, {"id": 1620, "seek": 969960, "start": 9714.640000000001, "end": 9719.2, "text": " We have already touched upon the reasons why by the time we're at human level at AI,", "tokens": [51116, 492, 362, 1217, 9828, 3564, 264, 4112, 983, 538, 264, 565, 321, 434, 412, 1952, 1496, 412, 7318, 11, 51344], "temperature": 0.0, "avg_logprob": -0.10359498505951256, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.07027711719274521}, {"id": 1621, "seek": 969960, "start": 9719.84, "end": 9728.800000000001, "text": " then AI's will be adding a huge, huge amount to the productivity of our work on AI R&D,", "tokens": [51376, 550, 7318, 311, 486, 312, 5127, 257, 2603, 11, 2603, 2372, 281, 264, 15604, 295, 527, 589, 322, 7318, 497, 5, 35, 11, 51824], "temperature": 0.0, "avg_logprob": -0.10359498505951256, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.07027711719274521}, {"id": 1622, "seek": 972880, "start": 9728.88, "end": 9732.64, "text": " designing better trips, designing better algorithms. And those things are already", "tokens": [50368, 14685, 1101, 16051, 11, 14685, 1101, 14642, 13, 400, 729, 721, 366, 1217, 50556], "temperature": 0.0, "avg_logprob": -0.08629181409122968, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.004196009133011103}, {"id": 1623, "seek": 972880, "start": 9732.64, "end": 9736.88, "text": " improving very quickly. So I only expect them to be going much, much faster.", "tokens": [50556, 11470, 588, 2661, 13, 407, 286, 787, 2066, 552, 281, 312, 516, 709, 11, 709, 4663, 13, 50768], "temperature": 0.0, "avg_logprob": -0.08629181409122968, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.004196009133011103}, {"id": 1624, "seek": 972880, "start": 9737.599999999999, "end": 9744.48, "text": " And then in addition, it's going to be quite clear that it's a very lucrative area, whether you care", "tokens": [50804, 400, 550, 294, 4500, 11, 309, 311, 516, 281, 312, 1596, 1850, 300, 309, 311, 257, 588, 21296, 30457, 1859, 11, 1968, 291, 1127, 51148], "temperature": 0.0, "avg_logprob": -0.08629181409122968, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.004196009133011103}, {"id": 1625, "seek": 972880, "start": 9744.48, "end": 9749.359999999999, "text": " about discovering new technologies to help with climate change or discovering new technologies", "tokens": [51148, 466, 24773, 777, 7943, 281, 854, 365, 5659, 1319, 420, 24773, 777, 7943, 51392], "temperature": 0.0, "avg_logprob": -0.08629181409122968, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.004196009133011103}, {"id": 1626, "seek": 972880, "start": 9749.359999999999, "end": 9754.64, "text": " to help with improving human health, or you want to increase your country's kind of national power,", "tokens": [51392, 281, 854, 365, 11470, 1952, 1585, 11, 420, 291, 528, 281, 3488, 428, 1941, 311, 733, 295, 4048, 1347, 11, 51656], "temperature": 0.0, "avg_logprob": -0.08629181409122968, "compression_ratio": 1.780392156862745, "no_speech_prob": 0.004196009133011103}, {"id": 1627, "seek": 975464, "start": 9755.359999999999, "end": 9762.56, "text": " then there's just going to be lots of reason to be investing in AI and designing smarter AI's.", "tokens": [50400, 550, 456, 311, 445, 516, 281, 312, 3195, 295, 1778, 281, 312, 10978, 294, 7318, 293, 14685, 20294, 7318, 311, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1339774861055262, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0021775118075311184}, {"id": 1628, "seek": 975464, "start": 9763.119999999999, "end": 9768.8, "text": " And so I kind of expect all of those three inputs, kind of the dollar spent on training and", "tokens": [50788, 400, 370, 286, 733, 295, 2066, 439, 295, 729, 1045, 15743, 11, 733, 295, 264, 7241, 4418, 322, 3097, 293, 51072], "temperature": 0.0, "avg_logprob": -0.1339774861055262, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0021775118075311184}, {"id": 1629, "seek": 975464, "start": 9768.8, "end": 9772.72, "text": " the quality of the AI chips and the algorithms to be improving really very quickly.", "tokens": [51072, 264, 3125, 295, 264, 7318, 11583, 293, 264, 14642, 281, 312, 11470, 534, 588, 2661, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1339774861055262, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0021775118075311184}, {"id": 1630, "seek": 975464, "start": 9773.279999999999, "end": 9779.279999999999, "text": " Right. So there'll be incentives to go beyond just human human level capabilities, and then", "tokens": [51296, 1779, 13, 407, 456, 603, 312, 23374, 281, 352, 4399, 445, 1952, 1952, 1496, 10862, 11, 293, 550, 51596], "temperature": 0.0, "avg_logprob": -0.1339774861055262, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0021775118075311184}, {"id": 1631, "seek": 977928, "start": 9779.92, "end": 9788.880000000001, "text": " we'll have so much resources and just like AI labor to basically mean that we probably just", "tokens": [50396, 321, 603, 362, 370, 709, 3593, 293, 445, 411, 7318, 5938, 281, 1936, 914, 300, 321, 1391, 445, 50844], "temperature": 0.0, "avg_logprob": -0.10245886813388781, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.008516455069184303}, {"id": 1632, "seek": 977928, "start": 9788.880000000001, "end": 9793.68, "text": " shoot right past human level. Yeah. And I think that's in terms of plans for making the whole", "tokens": [50844, 3076, 558, 1791, 1952, 1496, 13, 865, 13, 400, 286, 519, 300, 311, 294, 2115, 295, 5482, 337, 1455, 264, 1379, 51084], "temperature": 0.0, "avg_logprob": -0.10245886813388781, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.008516455069184303}, {"id": 1633, "seek": 977928, "start": 9793.68, "end": 9800.0, "text": " thing go well. It's especially scary because I think a really important part of the plan from my", "tokens": [51084, 551, 352, 731, 13, 467, 311, 2318, 6958, 570, 286, 519, 257, 534, 1021, 644, 295, 264, 1393, 490, 452, 51400], "temperature": 0.0, "avg_logprob": -0.10245886813388781, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.008516455069184303}, {"id": 1634, "seek": 977928, "start": 9800.0, "end": 9806.08, "text": " perspective would be to go especially slowly when we're around the human level, so that we can do", "tokens": [51400, 4585, 576, 312, 281, 352, 2318, 5692, 562, 321, 434, 926, 264, 1952, 1496, 11, 370, 300, 321, 393, 360, 51704], "temperature": 0.0, "avg_logprob": -0.10245886813388781, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.008516455069184303}, {"id": 1635, "seek": 980608, "start": 9806.08, "end": 9811.84, "text": " loads of experiments and loads of kind of scientific investigation into, okay, so this human", "tokens": [50364, 12668, 295, 12050, 293, 12668, 295, 733, 295, 8134, 9627, 666, 11, 1392, 11, 370, 341, 1952, 50652], "temperature": 0.0, "avg_logprob": -0.09787019399496225, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.012067186646163464}, {"id": 1636, "seek": 980608, "start": 9811.84, "end": 9816.56, "text": " level AI, is it aligned if we do this technique? What about if we do this other alignment technique?", "tokens": [50652, 1496, 7318, 11, 307, 309, 17962, 498, 321, 360, 341, 6532, 30, 708, 466, 498, 321, 360, 341, 661, 18515, 6532, 30, 50888], "temperature": 0.0, "avg_logprob": -0.09787019399496225, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.012067186646163464}, {"id": 1637, "seek": 980608, "start": 9816.56, "end": 9822.16, "text": " Does it then, does it then seem like it's aligned and just really making sure we kind of fully", "tokens": [50888, 4402, 309, 550, 11, 775, 309, 550, 1643, 411, 309, 311, 17962, 293, 445, 534, 1455, 988, 321, 733, 295, 4498, 51168], "temperature": 0.0, "avg_logprob": -0.09787019399496225, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.012067186646163464}, {"id": 1638, "seek": 980608, "start": 9822.16, "end": 9827.2, "text": " understand kind of the science of alignment and can try out lots of different techniques", "tokens": [51168, 1223, 733, 295, 264, 3497, 295, 18515, 293, 393, 853, 484, 3195, 295, 819, 7512, 51420], "temperature": 0.0, "avg_logprob": -0.09787019399496225, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.012067186646163464}, {"id": 1639, "seek": 980608, "start": 9827.2, "end": 9833.28, "text": " and to develop kind of reliable tests for whether the alignment technique has worked or not,", "tokens": [51420, 293, 281, 1499, 733, 295, 12924, 6921, 337, 1968, 264, 18515, 6532, 575, 2732, 420, 406, 11, 51724], "temperature": 0.0, "avg_logprob": -0.09787019399496225, "compression_ratio": 1.8650793650793651, "no_speech_prob": 0.012067186646163464}, {"id": 1640, "seek": 983328, "start": 9833.36, "end": 9839.12, "text": " whether they're hard to game. The kind of thing that ARC has done with GPT-4, for example. Exactly.", "tokens": [50368, 1968, 436, 434, 1152, 281, 1216, 13, 440, 733, 295, 551, 300, 8943, 34, 575, 1096, 365, 26039, 51, 12, 19, 11, 337, 1365, 13, 7587, 13, 50656], "temperature": 0.0, "avg_logprob": -0.13367849866920542, "compression_ratio": 1.5971731448763251, "no_speech_prob": 0.0028434782288968563}, {"id": 1641, "seek": 983328, "start": 9839.68, "end": 9845.52, "text": " And I think if we only have a few months kind of through the human level stage,", "tokens": [50684, 400, 286, 519, 498, 321, 787, 362, 257, 1326, 2493, 733, 295, 807, 264, 1952, 1496, 3233, 11, 50976], "temperature": 0.0, "avg_logprob": -0.13367849866920542, "compression_ratio": 1.5971731448763251, "no_speech_prob": 0.0028434782288968563}, {"id": 1642, "seek": 983328, "start": 9845.52, "end": 9851.52, "text": " that stuff becomes really difficult to do without significant coordination in advance", "tokens": [50976, 300, 1507, 3643, 534, 2252, 281, 360, 1553, 4776, 21252, 294, 7295, 51276], "temperature": 0.0, "avg_logprob": -0.13367849866920542, "compression_ratio": 1.5971731448763251, "no_speech_prob": 0.0028434782288968563}, {"id": 1643, "seek": 983328, "start": 9851.52, "end": 9857.12, "text": " by labs. So I think that this, you know, there are really important implications of this,", "tokens": [51276, 538, 20339, 13, 407, 286, 519, 300, 341, 11, 291, 458, 11, 456, 366, 534, 1021, 16602, 295, 341, 11, 51556], "temperature": 0.0, "avg_logprob": -0.13367849866920542, "compression_ratio": 1.5971731448763251, "no_speech_prob": 0.0028434782288968563}, {"id": 1644, "seek": 983328, "start": 9857.12, "end": 9862.480000000001, "text": " of this fast transition in terms of setting up a kind of government system which can allow us to", "tokens": [51556, 295, 341, 2370, 6034, 294, 2115, 295, 3287, 493, 257, 733, 295, 2463, 1185, 597, 393, 2089, 505, 281, 51824], "temperature": 0.0, "avg_logprob": -0.13367849866920542, "compression_ratio": 1.5971731448763251, "no_speech_prob": 0.0028434782288968563}, {"id": 1645, "seek": 986328, "start": 9863.28, "end": 9867.6, "text": " go slowly despite the technical possibilities existing to go very fast.", "tokens": [50364, 352, 5692, 7228, 264, 6191, 12178, 6741, 281, 352, 588, 2370, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09478731338794415, "compression_ratio": 1.6370656370656371, "no_speech_prob": 0.0001995503407670185}, {"id": 1646, "seek": 986328, "start": 9868.480000000001, "end": 9871.52, "text": " Yeah, yeah. No, that makes sense. I do feel like I've had some", "tokens": [50624, 865, 11, 1338, 13, 883, 11, 300, 1669, 2020, 13, 286, 360, 841, 411, 286, 600, 632, 512, 50776], "temperature": 0.0, "avg_logprob": -0.09478731338794415, "compression_ratio": 1.6370656370656371, "no_speech_prob": 0.0001995503407670185}, {"id": 1647, "seek": 986328, "start": 9872.480000000001, "end": 9879.36, "text": " background belief that was like, obviously, when we've got AI systems that can do things humans can", "tokens": [50824, 3678, 7107, 300, 390, 411, 11, 2745, 11, 562, 321, 600, 658, 7318, 3652, 300, 393, 360, 721, 6255, 393, 51168], "temperature": 0.0, "avg_logprob": -0.09478731338794415, "compression_ratio": 1.6370656370656371, "no_speech_prob": 0.0001995503407670185}, {"id": 1648, "seek": 986328, "start": 9879.36, "end": 9883.92, "text": " do, people are going to start freaking out and they're going to want to make sure those systems", "tokens": [51168, 360, 11, 561, 366, 516, 281, 722, 14612, 484, 293, 436, 434, 516, 281, 528, 281, 652, 988, 729, 3652, 51396], "temperature": 0.0, "avg_logprob": -0.09478731338794415, "compression_ratio": 1.6370656370656371, "no_speech_prob": 0.0001995503407670185}, {"id": 1649, "seek": 986328, "start": 9883.92, "end": 9890.0, "text": " are safe. But if we, if it takes months to get there and then within another few months we're", "tokens": [51396, 366, 3273, 13, 583, 498, 321, 11, 498, 309, 2516, 2493, 281, 483, 456, 293, 550, 1951, 1071, 1326, 2493, 321, 434, 51700], "temperature": 0.0, "avg_logprob": -0.09478731338794415, "compression_ratio": 1.6370656370656371, "no_speech_prob": 0.0001995503407670185}, {"id": 1650, "seek": 989000, "start": 9890.0, "end": 9895.04, "text": " already well beyond human capabilities, then no one's going to have time to freak out or it'll", "tokens": [50364, 1217, 731, 4399, 1952, 10862, 11, 550, 572, 472, 311, 516, 281, 362, 565, 281, 21853, 484, 420, 309, 603, 50616], "temperature": 0.0, "avg_logprob": -0.11084003259639928, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.005901578348129988}, {"id": 1651, "seek": 989000, "start": 9895.04, "end": 9900.32, "text": " be too late. Yeah. I mean, even if we spend the next, what do we have, seven years left in the", "tokens": [50616, 312, 886, 3469, 13, 865, 13, 286, 914, 11, 754, 498, 321, 3496, 264, 958, 11, 437, 360, 321, 362, 11, 3407, 924, 1411, 294, 264, 50880], "temperature": 0.0, "avg_logprob": -0.11084003259639928, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.005901578348129988}, {"id": 1652, "seek": 989000, "start": 9900.32, "end": 9907.84, "text": " decade? Like that sounds hard enough. Yeah. Yeah, I agree. Okay. So a takeaway is like,", "tokens": [50880, 10378, 30, 1743, 300, 3263, 1152, 1547, 13, 865, 13, 865, 11, 286, 3986, 13, 1033, 13, 407, 257, 30681, 307, 411, 11, 51256], "temperature": 0.0, "avg_logprob": -0.11084003259639928, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.005901578348129988}, {"id": 1653, "seek": 989000, "start": 9907.84, "end": 9915.6, "text": " we really need to start slowing down or planning now, ideally both. Yeah. And we'll need the plans", "tokens": [51256, 321, 534, 643, 281, 722, 26958, 760, 420, 5038, 586, 11, 22915, 1293, 13, 865, 13, 400, 321, 603, 643, 264, 5482, 51644], "temperature": 0.0, "avg_logprob": -0.11084003259639928, "compression_ratio": 1.5473251028806585, "no_speech_prob": 0.005901578348129988}, {"id": 1654, "seek": 991560, "start": 9915.6, "end": 9921.92, "text": " we make to really enable that to be mutual trust that, that the other labs are also slowing down.", "tokens": [50364, 321, 652, 281, 534, 9528, 300, 281, 312, 16917, 3361, 300, 11, 300, 264, 661, 20339, 366, 611, 26958, 760, 13, 50680], "temperature": 0.0, "avg_logprob": -0.07469062010447185, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.011644566431641579}, {"id": 1655, "seek": 991560, "start": 9921.92, "end": 9928.960000000001, "text": " Because if it's, you know, if it only takes six months to, you know, make your AIs 10 or 100", "tokens": [50680, 1436, 498, 309, 311, 11, 291, 458, 11, 498, 309, 787, 2516, 2309, 2493, 281, 11, 291, 458, 11, 652, 428, 316, 6802, 1266, 420, 2319, 51032], "temperature": 0.0, "avg_logprob": -0.07469062010447185, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.011644566431641579}, {"id": 1656, "seek": 991560, "start": 9928.960000000001, "end": 9933.92, "text": " times as smart, then you're going to need to be really confident that the other labs aren't doing", "tokens": [51032, 1413, 382, 4069, 11, 550, 291, 434, 516, 281, 643, 281, 312, 534, 6679, 300, 264, 661, 20339, 3212, 380, 884, 51280], "temperature": 0.0, "avg_logprob": -0.07469062010447185, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.011644566431641579}, {"id": 1657, "seek": 991560, "start": 9933.92, "end": 9939.12, "text": " that in order to feel comfortable slowing down yourself. Right. If it was going to take 10 years", "tokens": [51280, 300, 294, 1668, 281, 841, 4619, 26958, 760, 1803, 13, 1779, 13, 759, 309, 390, 516, 281, 747, 1266, 924, 51540], "temperature": 0.0, "avg_logprob": -0.07469062010447185, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.011644566431641579}, {"id": 1658, "seek": 991560, "start": 9939.12, "end": 9943.76, "text": " and you noticed like three months in that another lab was working on it, you'd be like,", "tokens": [51540, 293, 291, 5694, 411, 1045, 2493, 294, 300, 1071, 2715, 390, 1364, 322, 309, 11, 291, 1116, 312, 411, 11, 51772], "temperature": 0.0, "avg_logprob": -0.07469062010447185, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.011644566431641579}, {"id": 1659, "seek": 994376, "start": 9943.76, "end": 9947.44, "text": " yeah, we can catch up. Yeah. But if it's going to take six months and you're three months in,", "tokens": [50364, 1338, 11, 321, 393, 3745, 493, 13, 865, 13, 583, 498, 309, 311, 516, 281, 747, 2309, 2493, 293, 291, 434, 1045, 2493, 294, 11, 50548], "temperature": 0.0, "avg_logprob": -0.1137431658231295, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0018567231018096209}, {"id": 1660, "seek": 994376, "start": 9948.16, "end": 9953.68, "text": " you've got no hope. And so maybe you'll just like spend those first three months secretly working on", "tokens": [50584, 291, 600, 658, 572, 1454, 13, 400, 370, 1310, 291, 603, 445, 411, 3496, 729, 700, 1045, 2493, 22611, 1364, 322, 50860], "temperature": 0.0, "avg_logprob": -0.1137431658231295, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0018567231018096209}, {"id": 1661, "seek": 994376, "start": 9953.68, "end": 9959.52, "text": " it to make sure that doesn't happen. Or just not agree to do the slowdown. Yeah. Oh, these are really", "tokens": [50860, 309, 281, 652, 988, 300, 1177, 380, 1051, 13, 1610, 445, 406, 3986, 281, 360, 264, 2964, 5093, 13, 865, 13, 876, 11, 613, 366, 534, 51152], "temperature": 0.0, "avg_logprob": -0.1137431658231295, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0018567231018096209}, {"id": 1662, "seek": 994376, "start": 9959.52, "end": 9964.56, "text": " hard problems. I mean, it's very, it feels very like prisoner's dilemma. I'm hoping it's going to be", "tokens": [51152, 1152, 2740, 13, 286, 914, 11, 309, 311, 588, 11, 309, 3417, 588, 411, 28114, 311, 34312, 13, 286, 478, 7159, 309, 311, 516, 281, 312, 51404], "temperature": 0.0, "avg_logprob": -0.1137431658231295, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0018567231018096209}, {"id": 1663, "seek": 994376, "start": 9964.56, "end": 9969.44, "text": " more like an iterated prisoner's dilemma, where there's kind of multiple moves that the labs make", "tokens": [51404, 544, 411, 364, 17138, 770, 28114, 311, 34312, 11, 689, 456, 311, 733, 295, 3866, 6067, 300, 264, 20339, 652, 51648], "temperature": 0.0, "avg_logprob": -0.1137431658231295, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0018567231018096209}, {"id": 1664, "seek": 996944, "start": 9969.44, "end": 9974.560000000001, "text": " one after the other. And they can see if the other labs are cooperating. And in an iterated", "tokens": [50364, 472, 934, 264, 661, 13, 400, 436, 393, 536, 498, 264, 661, 20339, 366, 13414, 990, 13, 400, 294, 364, 17138, 770, 50620], "temperature": 0.0, "avg_logprob": -0.14553935970880288, "compression_ratio": 1.9206349206349207, "no_speech_prob": 0.015877529978752136}, {"id": 1665, "seek": 996944, "start": 9974.560000000001, "end": 9979.6, "text": " prisoner's dilemma ultimately makes sense for everyone to cooperate, because that's that way", "tokens": [50620, 28114, 311, 34312, 6284, 1669, 2020, 337, 1518, 281, 26667, 11, 570, 300, 311, 300, 636, 50872], "temperature": 0.0, "avg_logprob": -0.14553935970880288, "compression_ratio": 1.9206349206349207, "no_speech_prob": 0.015877529978752136}, {"id": 1666, "seek": 996944, "start": 9980.16, "end": 9984.24, "text": " they can, the other people can see you coordinating, then they coordinate and then everyone kind of", "tokens": [50900, 436, 393, 11, 264, 661, 561, 393, 536, 291, 37824, 11, 550, 436, 15670, 293, 550, 1518, 733, 295, 51104], "temperature": 0.0, "avg_logprob": -0.14553935970880288, "compression_ratio": 1.9206349206349207, "no_speech_prob": 0.015877529978752136}, {"id": 1667, "seek": 996944, "start": 9984.24, "end": 9989.12, "text": " ends up coordinating. You know, one thing is if you could set up ways for labs to easily know whether", "tokens": [51104, 5314, 493, 37824, 13, 509, 458, 11, 472, 551, 307, 498, 291, 727, 992, 493, 2098, 337, 20339, 281, 3612, 458, 1968, 51348], "temperature": 0.0, "avg_logprob": -0.14553935970880288, "compression_ratio": 1.9206349206349207, "no_speech_prob": 0.015877529978752136}, {"id": 1668, "seek": 996944, "start": 9989.12, "end": 9995.68, "text": " the other labs are indeed cooperating or not kind of week by week, then that turns it into a more", "tokens": [51348, 264, 661, 20339, 366, 6451, 13414, 990, 420, 406, 733, 295, 1243, 538, 1243, 11, 550, 300, 4523, 309, 666, 257, 544, 51676], "temperature": 0.0, "avg_logprob": -0.14553935970880288, "compression_ratio": 1.9206349206349207, "no_speech_prob": 0.015877529978752136}, {"id": 1669, "seek": 999568, "start": 9995.68, "end": 10000.56, "text": " iterated prisoner's dilemma and makes it easier to achieve a kind of good outcome. Yeah, yeah,", "tokens": [50364, 17138, 770, 28114, 311, 34312, 293, 1669, 309, 3571, 281, 4584, 257, 733, 295, 665, 9700, 13, 865, 11, 1338, 11, 50608], "temperature": 0.0, "avg_logprob": -0.07886091355354555, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0045906114391982555}, {"id": 1670, "seek": 999568, "start": 10000.56, "end": 10006.720000000001, "text": " that makes sense. I imagine it's the case that the more iteration you get in an iterated prisoner's", "tokens": [50608, 300, 1669, 2020, 13, 286, 3811, 309, 311, 264, 1389, 300, 264, 544, 24784, 291, 483, 294, 364, 17138, 770, 28114, 311, 50916], "temperature": 0.0, "avg_logprob": -0.07886091355354555, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0045906114391982555}, {"id": 1671, "seek": 999568, "start": 10006.720000000001, "end": 10014.24, "text": " dilemma, the better the incentives are to cooperate. And so just by making the timeline shorter,", "tokens": [50916, 34312, 11, 264, 1101, 264, 23374, 366, 281, 26667, 13, 400, 370, 445, 538, 1455, 264, 12933, 11639, 11, 51292], "temperature": 0.0, "avg_logprob": -0.07886091355354555, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0045906114391982555}, {"id": 1672, "seek": 999568, "start": 10014.24, "end": 10018.32, "text": " you you make it harder to get to get these iterations that build trust. Yeah, I think that's", "tokens": [51292, 291, 291, 652, 309, 6081, 281, 483, 281, 483, 613, 36540, 300, 1322, 3361, 13, 865, 11, 286, 519, 300, 311, 51496], "temperature": 0.0, "avg_logprob": -0.07886091355354555, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0045906114391982555}, {"id": 1673, "seek": 1001832, "start": 10018.32, "end": 10026.24, "text": " right. So it sounds like there's this range that's maybe between one and 10 years, maybe a bit", "tokens": [50364, 558, 13, 407, 309, 3263, 411, 456, 311, 341, 3613, 300, 311, 1310, 1296, 472, 293, 1266, 924, 11, 1310, 257, 857, 50760], "temperature": 0.0, "avg_logprob": -0.06938668357001411, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.07359305024147034}, {"id": 1674, "seek": 1001832, "start": 10026.24, "end": 10033.44, "text": " shorter or a bit longer at the at the extremes. But that's in particular for AI systems having", "tokens": [50760, 11639, 420, 257, 857, 2854, 412, 264, 412, 264, 41119, 13, 583, 300, 311, 294, 1729, 337, 7318, 3652, 1419, 51120], "temperature": 0.0, "avg_logprob": -0.06938668357001411, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.07359305024147034}, {"id": 1675, "seek": 1001832, "start": 10033.44, "end": 10040.48, "text": " the capability to perform all the tasks that humans currently do, not that they're actually", "tokens": [51120, 264, 13759, 281, 2042, 439, 264, 9608, 300, 6255, 4362, 360, 11, 406, 300, 436, 434, 767, 51472], "temperature": 0.0, "avg_logprob": -0.06938668357001411, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.07359305024147034}, {"id": 1676, "seek": 1001832, "start": 10040.48, "end": 10047.119999999999, "text": " automating all those tasks and replacing humans. How big of a lag do you expect there to be between", "tokens": [51472, 3553, 990, 439, 729, 9608, 293, 19139, 6255, 13, 1012, 955, 295, 257, 8953, 360, 291, 2066, 456, 281, 312, 1296, 51804], "temperature": 0.0, "avg_logprob": -0.06938668357001411, "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.07359305024147034}, {"id": 1677, "seek": 1004712, "start": 10047.12, "end": 10051.92, "text": " AI systems having capabilities and AI systems actually being used in the world?", "tokens": [50364, 7318, 3652, 1419, 10862, 293, 7318, 3652, 767, 885, 1143, 294, 264, 1002, 30, 50604], "temperature": 0.0, "avg_logprob": -0.06563393274943034, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.001012700144201517}, {"id": 1678, "seek": 1004712, "start": 10052.880000000001, "end": 10058.960000000001, "text": " So I think it will vary, according to a few things. So one of the things is what industry are we", "tokens": [50652, 407, 286, 519, 309, 486, 10559, 11, 4650, 281, 257, 1326, 721, 13, 407, 472, 295, 264, 721, 307, 437, 3518, 366, 321, 50956], "temperature": 0.0, "avg_logprob": -0.06563393274943034, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.001012700144201517}, {"id": 1679, "seek": 1004712, "start": 10058.960000000001, "end": 10067.44, "text": " talking about? So I think for industries that are very public facing, customer facing, and", "tokens": [50956, 1417, 466, 30, 407, 286, 519, 337, 13284, 300, 366, 588, 1908, 7170, 11, 5474, 7170, 11, 293, 51380], "temperature": 0.0, "avg_logprob": -0.06563393274943034, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.001012700144201517}, {"id": 1680, "seek": 1004712, "start": 10068.160000000002, "end": 10073.84, "text": " highly regulated, you'd expect there to be bigger delays between AI being able to automate the work", "tokens": [51416, 5405, 26243, 11, 291, 1116, 2066, 456, 281, 312, 3801, 28610, 1296, 7318, 885, 1075, 281, 31605, 264, 589, 51700], "temperature": 0.0, "avg_logprob": -0.06563393274943034, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.001012700144201517}, {"id": 1681, "seek": 1007384, "start": 10073.84, "end": 10078.960000000001, "text": " and it actually being automated. Whereas for more back end parts of the economy, like R&D,", "tokens": [50364, 293, 309, 767, 885, 18473, 13, 13813, 337, 544, 646, 917, 3166, 295, 264, 5010, 11, 411, 497, 5, 35, 11, 50620], "temperature": 0.0, "avg_logprob": -0.12341257583263308, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.004155935253947973}, {"id": 1682, "seek": 1007384, "start": 10079.92, "end": 10087.28, "text": " like manufacturing, like logistics and transportation, then I think there would be less of a delay.", "tokens": [50668, 411, 11096, 11, 411, 27420, 293, 11328, 11, 550, 286, 519, 456, 576, 312, 1570, 295, 257, 8577, 13, 51036], "temperature": 0.0, "avg_logprob": -0.12341257583263308, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.004155935253947973}, {"id": 1683, "seek": 1007384, "start": 10087.28, "end": 10093.52, "text": " Okay, yeah. I also just expect it to differ from company to company based on how innovative", "tokens": [51036, 1033, 11, 1338, 13, 286, 611, 445, 2066, 309, 281, 743, 490, 2237, 281, 2237, 2361, 322, 577, 12999, 51348], "temperature": 0.0, "avg_logprob": -0.12341257583263308, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.004155935253947973}, {"id": 1684, "seek": 1007384, "start": 10093.52, "end": 10098.24, "text": " those organizations are. Sure, their internal culture, whether they're the type of company", "tokens": [51348, 729, 6150, 366, 13, 4894, 11, 641, 6920, 3713, 11, 1968, 436, 434, 264, 2010, 295, 2237, 51584], "temperature": 0.0, "avg_logprob": -0.12341257583263308, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.004155935253947973}, {"id": 1685, "seek": 1009824, "start": 10098.24, "end": 10103.28, "text": " who's going to be like, let's integrate GPT-4 into all of our processes now. Yeah.", "tokens": [50364, 567, 311, 516, 281, 312, 411, 11, 718, 311, 13365, 26039, 51, 12, 19, 666, 439, 295, 527, 7555, 586, 13, 865, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11164059846297554, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0106017105281353}, {"id": 1686, "seek": 1009824, "start": 10104.16, "end": 10110.0, "text": " For the purposes of the predictions of this model, the really important thing is about the delay to", "tokens": [50660, 1171, 264, 9932, 295, 264, 21264, 295, 341, 2316, 11, 264, 534, 1021, 551, 307, 466, 264, 8577, 281, 50952], "temperature": 0.0, "avg_logprob": -0.11164059846297554, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0106017105281353}, {"id": 1687, "seek": 1009824, "start": 10110.0, "end": 10118.32, "text": " using AI to improve AI algorithms and to improve AI chip design. And I think those are probably", "tokens": [50952, 1228, 7318, 281, 3470, 7318, 14642, 293, 281, 3470, 7318, 11409, 1715, 13, 400, 286, 519, 729, 366, 1391, 51368], "temperature": 0.0, "avg_logprob": -0.11164059846297554, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0106017105281353}, {"id": 1688, "seek": 1009824, "start": 10118.32, "end": 10125.44, "text": " areas where the lag will be very much on the shorter end. Right. And that's because AI researchers", "tokens": [51368, 3179, 689, 264, 8953, 486, 312, 588, 709, 322, 264, 11639, 917, 13, 1779, 13, 400, 300, 311, 570, 7318, 10309, 51724], "temperature": 0.0, "avg_logprob": -0.11164059846297554, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0106017105281353}, {"id": 1689, "seek": 1012544, "start": 10125.44, "end": 10134.16, "text": " can use them without needing them to be functioning super well for public users who might be like,", "tokens": [50364, 393, 764, 552, 1553, 18006, 552, 281, 312, 18483, 1687, 731, 337, 1908, 5022, 567, 1062, 312, 411, 11, 50800], "temperature": 0.0, "avg_logprob": -0.10717430935111097, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.004595025908201933}, {"id": 1690, "seek": 1012544, "start": 10134.16, "end": 10138.16, "text": " what the heck, it gave me this weird result. That's confusing and weird and looks bad on your", "tokens": [50800, 437, 264, 12872, 11, 309, 2729, 385, 341, 3657, 1874, 13, 663, 311, 13181, 293, 3657, 293, 1542, 1578, 322, 428, 51000], "temperature": 0.0, "avg_logprob": -0.10717430935111097, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.004595025908201933}, {"id": 1691, "seek": 1012544, "start": 10138.16, "end": 10143.04, "text": " company. They can just use them in the background, check that they work, and they don't need to", "tokens": [51000, 2237, 13, 814, 393, 445, 764, 552, 294, 264, 3678, 11, 1520, 300, 436, 589, 11, 293, 436, 500, 380, 643, 281, 51244], "temperature": 0.0, "avg_logprob": -0.10717430935111097, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.004595025908201933}, {"id": 1692, "seek": 1012544, "start": 10143.04, "end": 10148.24, "text": " wait for them to be super polished. Right, yeah. There'll also probably be more aware of AI", "tokens": [51244, 1699, 337, 552, 281, 312, 1687, 29079, 13, 1779, 11, 1338, 13, 821, 603, 611, 1391, 312, 544, 3650, 295, 7318, 51504], "temperature": 0.0, "avg_logprob": -0.10717430935111097, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.004595025908201933}, {"id": 1693, "seek": 1014824, "start": 10148.24, "end": 10154.72, "text": " developments because that's the kind of industry they work in. Yeah, right. They are probably less", "tokens": [50364, 20862, 570, 300, 311, 264, 733, 295, 3518, 436, 589, 294, 13, 865, 11, 558, 13, 814, 366, 1391, 1570, 50688], "temperature": 0.0, "avg_logprob": -0.11415135729443895, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.12313596159219742}, {"id": 1694, "seek": 1014824, "start": 10154.72, "end": 10162.16, "text": " regulated. So I think a few factors. Another thing that could kind of affect how long that lag is", "tokens": [50688, 26243, 13, 407, 286, 519, 257, 1326, 6771, 13, 3996, 551, 300, 727, 733, 295, 3345, 577, 938, 300, 8953, 307, 51060], "temperature": 0.0, "avg_logprob": -0.11415135729443895, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.12313596159219742}, {"id": 1695, "seek": 1014824, "start": 10163.039999999999, "end": 10169.84, "text": " is actually the capability of AIs themselves. So even if it would be possible with kind of", "tokens": [51104, 307, 767, 264, 13759, 295, 316, 6802, 2969, 13, 407, 754, 498, 309, 576, 312, 1944, 365, 733, 295, 51444], "temperature": 0.0, "avg_logprob": -0.11415135729443895, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.12313596159219742}, {"id": 1696, "seek": 1014824, "start": 10169.84, "end": 10174.96, "text": " six months effort to integrate some AIs into your workflow, by the time AIs are kind of,", "tokens": [51444, 2309, 2493, 4630, 281, 13365, 512, 316, 6802, 666, 428, 20993, 11, 538, 264, 565, 316, 6802, 366, 733, 295, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11415135729443895, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.12313596159219742}, {"id": 1697, "seek": 1017496, "start": 10175.679999999998, "end": 10181.119999999999, "text": " let's say superhuman in their capabilities, then maybe they're able to integrate themselves", "tokens": [50400, 718, 311, 584, 1687, 18796, 294, 641, 10862, 11, 550, 1310, 436, 434, 1075, 281, 13365, 2969, 50672], "temperature": 0.0, "avg_logprob": -0.11021996435718001, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0010087519185617566}, {"id": 1698, "seek": 1017496, "start": 10181.119999999999, "end": 10188.24, "text": " into the workflow. And so that kind of upfront cost of adopting AI is now extremely low.", "tokens": [50672, 666, 264, 20993, 13, 400, 370, 300, 733, 295, 30264, 2063, 295, 32328, 7318, 307, 586, 4664, 2295, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11021996435718001, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0010087519185617566}, {"id": 1699, "seek": 1017496, "start": 10189.279999999999, "end": 10192.72, "text": " And so I think that you could see that lag time reducing over time.", "tokens": [51080, 400, 370, 286, 519, 300, 291, 727, 536, 300, 8953, 565, 12245, 670, 565, 13, 51252], "temperature": 0.0, "avg_logprob": -0.11021996435718001, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0010087519185617566}, {"id": 1700, "seek": 1017496, "start": 10193.279999999999, "end": 10198.24, "text": " Yeah, okay. So yeah, we're nearing the end of our time, but I guess to take a step back,", "tokens": [51280, 865, 11, 1392, 13, 407, 1338, 11, 321, 434, 408, 1921, 264, 917, 295, 527, 565, 11, 457, 286, 2041, 281, 747, 257, 1823, 646, 11, 51528], "temperature": 0.0, "avg_logprob": -0.11021996435718001, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0010087519185617566}, {"id": 1701, "seek": 1017496, "start": 10198.24, "end": 10203.439999999999, "text": " we've talked about some pretty insane sounding stuff today, robot workforce and AI takeover.", "tokens": [51528, 321, 600, 2825, 466, 512, 1238, 10838, 24931, 1507, 965, 11, 7881, 14201, 293, 7318, 747, 3570, 13, 51788], "temperature": 0.0, "avg_logprob": -0.11021996435718001, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0010087519185617566}, {"id": 1702, "seek": 1020344, "start": 10203.44, "end": 10209.04, "text": " And yeah, I'm curious, were these kinds of arguments about the potential risks from AGI ever", "tokens": [50364, 400, 1338, 11, 286, 478, 6369, 11, 645, 613, 3685, 295, 12869, 466, 264, 3995, 10888, 490, 316, 26252, 1562, 50644], "temperature": 0.0, "avg_logprob": -0.15889712480398324, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0016574327601119876}, {"id": 1703, "seek": 1020344, "start": 10209.04, "end": 10215.6, "text": " farfetched to you? Or yeah, did they make sense to you kind of right away? Oh, yeah, for sure,", "tokens": [50644, 1400, 69, 7858, 292, 281, 291, 30, 1610, 1338, 11, 630, 436, 652, 2020, 281, 291, 733, 295, 558, 1314, 30, 876, 11, 1338, 11, 337, 988, 11, 50972], "temperature": 0.0, "avg_logprob": -0.15889712480398324, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0016574327601119876}, {"id": 1704, "seek": 1020344, "start": 10215.6, "end": 10222.480000000001, "text": " they were farfetched. I've kind of been through a few phases in my own relationship to the arguments.", "tokens": [50972, 436, 645, 1400, 69, 7858, 292, 13, 286, 600, 733, 295, 668, 807, 257, 1326, 18764, 294, 452, 1065, 2480, 281, 264, 12869, 13, 51316], "temperature": 0.0, "avg_logprob": -0.15889712480398324, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0016574327601119876}, {"id": 1705, "seek": 1020344, "start": 10223.28, "end": 10229.04, "text": " I think at first I read Superintelligence and it kind of made sense to me. I wasn't exactly", "tokens": [51356, 286, 519, 412, 700, 286, 1401, 4548, 20761, 17644, 293, 309, 733, 295, 1027, 2020, 281, 385, 13, 286, 2067, 380, 2293, 51644], "temperature": 0.0, "avg_logprob": -0.15889712480398324, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0016574327601119876}, {"id": 1706, "seek": 1022904, "start": 10229.12, "end": 10234.320000000002, "text": " sure what to do with it. But it seemed plausible enough that if we had Superintelligent AIs,", "tokens": [50368, 988, 437, 281, 360, 365, 309, 13, 583, 309, 6576, 39925, 1547, 300, 498, 321, 632, 4548, 20761, 25002, 316, 6802, 11, 50628], "temperature": 0.0, "avg_logprob": -0.07683217643511177, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.032188285142183304}, {"id": 1707, "seek": 1022904, "start": 10234.320000000002, "end": 10243.44, "text": " then things could go bad for us. Then there was a period in 2020 when a few counter arguments to", "tokens": [50628, 550, 721, 727, 352, 1578, 337, 505, 13, 1396, 456, 390, 257, 2896, 294, 4808, 562, 257, 1326, 5682, 12869, 281, 51084], "temperature": 0.0, "avg_logprob": -0.07683217643511177, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.032188285142183304}, {"id": 1708, "seek": 1022904, "start": 10243.44, "end": 10247.92, "text": " the case in Superintelligence came out. And I kind of thought, oh, wow, yeah, these arguments were", "tokens": [51084, 264, 1389, 294, 4548, 20761, 17644, 1361, 484, 13, 400, 286, 733, 295, 1194, 11, 1954, 11, 6076, 11, 1338, 11, 613, 12869, 645, 51308], "temperature": 0.0, "avg_logprob": -0.07683217643511177, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.032188285142183304}, {"id": 1709, "seek": 1022904, "start": 10248.720000000001, "end": 10254.880000000001, "text": " weaker than I had realized. And I felt a little bit disillusioned with them. That's a piece from", "tokens": [51348, 24286, 813, 286, 632, 5334, 13, 400, 286, 2762, 257, 707, 857, 717, 373, 5704, 292, 365, 552, 13, 663, 311, 257, 2522, 490, 51656], "temperature": 0.0, "avg_logprob": -0.07683217643511177, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.032188285142183304}, {"id": 1710, "seek": 1025488, "start": 10254.88, "end": 10263.359999999999, "text": " Ben Garfinkel and one from Tom Adam Shrazy on AI risk. But in the last two or three years,", "tokens": [50364, 3964, 7995, 5194, 7124, 293, 472, 490, 5041, 7938, 1160, 424, 1229, 322, 7318, 3148, 13, 583, 294, 264, 1036, 732, 420, 1045, 924, 11, 50788], "temperature": 0.0, "avg_logprob": -0.12857713869639806, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.019694000482559204}, {"id": 1711, "seek": 1025488, "start": 10264.08, "end": 10269.199999999999, "text": " thinking through the arguments in more detail, I've come to think that like somewhat adjusted", "tokens": [50824, 1953, 807, 264, 12869, 294, 544, 2607, 11, 286, 600, 808, 281, 519, 300, 411, 8344, 19871, 51080], "temperature": 0.0, "avg_logprob": -0.12857713869639806, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.019694000482559204}, {"id": 1712, "seek": 1025488, "start": 10269.199999999999, "end": 10274.32, "text": " versions of the argument in Superintelligence are still very plausible. And though I don't think", "tokens": [51080, 9606, 295, 264, 6770, 294, 4548, 20761, 17644, 366, 920, 588, 39925, 13, 400, 1673, 286, 500, 380, 519, 51336], "temperature": 0.0, "avg_logprob": -0.12857713869639806, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.019694000482559204}, {"id": 1713, "seek": 1025488, "start": 10274.32, "end": 10278.72, "text": " it's like 100% that we're doomed and there's no way that we could align these systems,", "tokens": [51336, 309, 311, 411, 2319, 4, 300, 321, 434, 33847, 293, 456, 311, 572, 636, 300, 321, 727, 7975, 613, 3652, 11, 51556], "temperature": 0.0, "avg_logprob": -0.12857713869639806, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.019694000482559204}, {"id": 1714, "seek": 1025488, "start": 10279.599999999999, "end": 10284.24, "text": " it does just seem pretty plausible to me that the evidence about whether they're aligned is very", "tokens": [51600, 309, 775, 445, 1643, 1238, 39925, 281, 385, 300, 264, 4467, 466, 1968, 436, 434, 17962, 307, 588, 51832], "temperature": 0.0, "avg_logprob": -0.12857713869639806, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.019694000482559204}, {"id": 1715, "seek": 1028424, "start": 10284.24, "end": 10291.6, "text": " ambiguous. There's competitive dynamics, pushing people to go forward in the face of that ambiguity", "tokens": [50364, 39465, 13, 821, 311, 10043, 15679, 11, 7380, 561, 281, 352, 2128, 294, 264, 1851, 295, 300, 46519, 50732], "temperature": 0.0, "avg_logprob": -0.10628787420129263, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.001819794182665646}, {"id": 1716, "seek": 1028424, "start": 10291.6, "end": 10296.96, "text": " and that we just really dropped the ball. Right. Okay. And so I guess at some point in there,", "tokens": [50732, 293, 300, 321, 445, 534, 8119, 264, 2594, 13, 1779, 13, 1033, 13, 400, 370, 286, 2041, 412, 512, 935, 294, 456, 11, 51000], "temperature": 0.0, "avg_logprob": -0.10628787420129263, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.001819794182665646}, {"id": 1717, "seek": 1028424, "start": 10298.32, "end": 10303.68, "text": " you decided to leave teaching and try to work on AI full time. Is that basically right?", "tokens": [51068, 291, 3047, 281, 1856, 4571, 293, 853, 281, 589, 322, 7318, 1577, 565, 13, 1119, 300, 1936, 558, 30, 51336], "temperature": 0.0, "avg_logprob": -0.10628787420129263, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.001819794182665646}, {"id": 1718, "seek": 1028424, "start": 10304.56, "end": 10313.84, "text": " Yeah, that's right. What was that like? It was a tough decision. I actually left teaching partway", "tokens": [51380, 865, 11, 300, 311, 558, 13, 708, 390, 300, 411, 30, 467, 390, 257, 4930, 3537, 13, 286, 767, 1411, 4571, 644, 676, 51844], "temperature": 0.0, "avg_logprob": -0.10628787420129263, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.001819794182665646}, {"id": 1719, "seek": 1031384, "start": 10313.84, "end": 10320.48, "text": " through the academic year. So I did feel I did feel bad about them. I did feel I was abandoning", "tokens": [50364, 807, 264, 7778, 1064, 13, 407, 286, 630, 841, 286, 630, 841, 1578, 466, 552, 13, 286, 630, 841, 286, 390, 9072, 278, 50696], "temperature": 0.0, "avg_logprob": -0.10418234231337062, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0033207605592906475}, {"id": 1720, "seek": 1031384, "start": 10320.48, "end": 10325.76, "text": " my pupils, because that's not a normal time to leave. And I still do feel bad about that.", "tokens": [50696, 452, 38404, 11, 570, 300, 311, 406, 257, 2710, 565, 281, 1856, 13, 400, 286, 920, 360, 841, 1578, 466, 300, 13, 50960], "temperature": 0.0, "avg_logprob": -0.10418234231337062, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0033207605592906475}, {"id": 1721, "seek": 1031384, "start": 10326.880000000001, "end": 10333.2, "text": " It does mean that I've kind of got to where I have a favor earlier than I would have. So I don't", "tokens": [51016, 467, 775, 914, 300, 286, 600, 733, 295, 658, 281, 689, 286, 362, 257, 2294, 3071, 813, 286, 576, 362, 13, 407, 286, 500, 380, 51332], "temperature": 0.0, "avg_logprob": -0.10418234231337062, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0033207605592906475}, {"id": 1722, "seek": 1031384, "start": 10333.2, "end": 10339.2, "text": " unambiguously regret it, but it was a tough decision. And it had downsides. Right. I was, you", "tokens": [51332, 517, 2173, 16397, 5098, 10879, 309, 11, 457, 309, 390, 257, 4930, 3537, 13, 400, 309, 632, 21554, 1875, 13, 1779, 13, 286, 390, 11, 291, 51632], "temperature": 0.0, "avg_logprob": -0.10418234231337062, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0033207605592906475}, {"id": 1723, "seek": 1033920, "start": 10339.2, "end": 10344.400000000001, "text": " know, I was listening to things like the ATK podcast and reading other things. And I was convinced", "tokens": [50364, 458, 11, 286, 390, 4764, 281, 721, 411, 264, 8872, 42, 7367, 293, 3760, 661, 721, 13, 400, 286, 390, 12561, 50624], "temperature": 0.0, "avg_logprob": -0.1281193447113037, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.005040829535573721}, {"id": 1724, "seek": 1033920, "start": 10344.400000000001, "end": 10349.84, "text": " about these arguments for AI risk and long termism in general, being plausible enough that it was", "tokens": [50624, 466, 613, 12869, 337, 7318, 3148, 293, 938, 1433, 1434, 294, 2674, 11, 885, 39925, 1547, 300, 309, 390, 50896], "temperature": 0.0, "avg_logprob": -0.1281193447113037, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.005040829535573721}, {"id": 1725, "seek": 1033920, "start": 10349.84, "end": 10357.92, "text": " worth leaving teaching. Yeah, well, that sounds, yeah, I guess both really hard and also on my", "tokens": [50896, 3163, 5012, 4571, 13, 865, 11, 731, 11, 300, 3263, 11, 1338, 11, 286, 2041, 1293, 534, 1152, 293, 611, 322, 452, 51300], "temperature": 0.0, "avg_logprob": -0.1281193447113037, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.005040829535573721}, {"id": 1726, "seek": 1033920, "start": 10357.92, "end": 10364.720000000001, "text": " views. Yeah, really, really lucky for us. Yeah, I'm glad. I'm glad we have you working on this stuff.", "tokens": [51300, 6809, 13, 865, 11, 534, 11, 534, 6356, 337, 505, 13, 865, 11, 286, 478, 5404, 13, 286, 478, 5404, 321, 362, 291, 1364, 322, 341, 1507, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1281193447113037, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.005040829535573721}, {"id": 1727, "seek": 1036472, "start": 10365.519999999999, "end": 10372.64, "text": " So we don't have much time left. So I'd love to ask a final question. I got an insider tip to ask you", "tokens": [50404, 407, 321, 500, 380, 362, 709, 565, 1411, 13, 407, 286, 1116, 959, 281, 1029, 257, 2572, 1168, 13, 286, 658, 364, 40990, 4125, 281, 1029, 291, 50760], "temperature": 0.0, "avg_logprob": -0.11327425293300462, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.004460170865058899}, {"id": 1728, "seek": 1036472, "start": 10374.0, "end": 10378.48, "text": " what ants can teach us about AI? Yeah, what's the story there?", "tokens": [50828, 437, 23355, 393, 2924, 505, 466, 7318, 30, 865, 11, 437, 311, 264, 1657, 456, 30, 51052], "temperature": 0.0, "avg_logprob": -0.11327425293300462, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.004460170865058899}, {"id": 1729, "seek": 1036472, "start": 10379.039999999999, "end": 10383.76, "text": " So yeah, ants are really incredible creatures. I've been I've been learning about reading this", "tokens": [51080, 407, 1338, 11, 23355, 366, 534, 4651, 12281, 13, 286, 600, 668, 286, 600, 668, 2539, 466, 3760, 341, 51316], "temperature": 0.0, "avg_logprob": -0.11327425293300462, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.004460170865058899}, {"id": 1730, "seek": 1036472, "start": 10383.76, "end": 10390.24, "text": " book called ant interactions. Okay. So I mean, ants have been around longer than the dinosaurs,", "tokens": [51316, 1446, 1219, 2511, 13280, 13, 1033, 13, 407, 286, 914, 11, 23355, 362, 668, 926, 2854, 813, 264, 25851, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11327425293300462, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.004460170865058899}, {"id": 1731, "seek": 1039024, "start": 10390.24, "end": 10394.72, "text": " I think 120 million years they've been around for. Wow, I didn't know that.", "tokens": [50364, 286, 519, 10411, 2459, 924, 436, 600, 668, 926, 337, 13, 3153, 11, 286, 994, 380, 458, 300, 13, 50588], "temperature": 0.0, "avg_logprob": -0.11393698851267496, "compression_ratio": 1.6408450704225352, "no_speech_prob": 0.004150453023612499}, {"id": 1732, "seek": 1039024, "start": 10394.72, "end": 10400.88, "text": " And they're like one of the most prolific and successful species on the planet. So one stat", "tokens": [50588, 400, 436, 434, 411, 472, 295, 264, 881, 24398, 1089, 293, 4406, 6172, 322, 264, 5054, 13, 407, 472, 2219, 50896], "temperature": 0.0, "avg_logprob": -0.11393698851267496, "compression_ratio": 1.6408450704225352, "no_speech_prob": 0.004150453023612499}, {"id": 1733, "seek": 1039024, "start": 10400.88, "end": 10406.8, "text": " in this book was that if you, if you weighed all the ants in the Amazon rainforest and put them on", "tokens": [50896, 294, 341, 1446, 390, 300, 498, 291, 11, 498, 291, 32844, 439, 264, 23355, 294, 264, 6795, 48531, 293, 829, 552, 322, 51192], "temperature": 0.0, "avg_logprob": -0.11393698851267496, "compression_ratio": 1.6408450704225352, "no_speech_prob": 0.004150453023612499}, {"id": 1734, "seek": 1039024, "start": 10406.8, "end": 10414.08, "text": " the scales, they would weigh twice as much as all of the land animals combined. That's all the mammals,", "tokens": [51192, 264, 17408, 11, 436, 576, 13843, 6091, 382, 709, 382, 439, 295, 264, 2117, 4882, 9354, 13, 663, 311, 439, 264, 35408, 11, 51556], "temperature": 0.0, "avg_logprob": -0.11393698851267496, "compression_ratio": 1.6408450704225352, "no_speech_prob": 0.004150453023612499}, {"id": 1735, "seek": 1039024, "start": 10414.08, "end": 10419.92, "text": " amphibians, birds, reptiles. I thought you were going to say twice as much as humans when I was", "tokens": [51556, 40077, 897, 2567, 11, 9009, 11, 29143, 4680, 13, 286, 1194, 291, 645, 516, 281, 584, 6091, 382, 709, 382, 6255, 562, 286, 390, 51848], "temperature": 0.0, "avg_logprob": -0.11393698851267496, "compression_ratio": 1.6408450704225352, "no_speech_prob": 0.004150453023612499}, {"id": 1736, "seek": 1041992, "start": 10419.92, "end": 10426.08, "text": " like, wow. That's the other that's just counting only ants and animals in the rainforest in the", "tokens": [50364, 411, 11, 6076, 13, 663, 311, 264, 661, 300, 311, 445, 13251, 787, 23355, 293, 4882, 294, 264, 48531, 294, 264, 50672], "temperature": 0.0, "avg_logprob": -0.13519101614480492, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.000674166833050549}, {"id": 1737, "seek": 1041992, "start": 10426.08, "end": 10434.48, "text": " rainforest. Unbelievable. Okay. And another fact about ants is that even compared to the other", "tokens": [50672, 48531, 13, 39523, 13, 1033, 13, 400, 1071, 1186, 466, 23355, 307, 300, 754, 5347, 281, 264, 661, 51092], "temperature": 0.0, "avg_logprob": -0.13519101614480492, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.000674166833050549}, {"id": 1738, "seek": 1041992, "start": 10434.48, "end": 10441.2, "text": " insects, they weigh 30% as much as all insects combine, which is given that they're only 2%", "tokens": [51092, 20201, 11, 436, 13843, 2217, 4, 382, 709, 382, 439, 20201, 10432, 11, 597, 307, 2212, 300, 436, 434, 787, 568, 4, 51428], "temperature": 0.0, "avg_logprob": -0.13519101614480492, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.000674166833050549}, {"id": 1739, "seek": 1041992, "start": 10441.2, "end": 10447.44, "text": " of the species of insects. They it's kind of testament to how how successful and prolific they", "tokens": [51428, 295, 264, 6172, 295, 20201, 13, 814, 309, 311, 733, 295, 35499, 281, 577, 577, 4406, 293, 24398, 1089, 436, 51740], "temperature": 0.0, "avg_logprob": -0.13519101614480492, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.000674166833050549}, {"id": 1740, "seek": 1044744, "start": 10447.44, "end": 10453.84, "text": " are. Yeah. Different ant varieties all around the world are incredibly diverse, like something kind", "tokens": [50364, 366, 13, 865, 13, 20825, 2511, 22092, 439, 926, 264, 1002, 366, 6252, 9521, 11, 411, 746, 733, 50684], "temperature": 0.0, "avg_logprob": -0.11713178883428159, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.012288336642086506}, {"id": 1741, "seek": 1044744, "start": 10453.84, "end": 10460.560000000001, "text": " of glide through the air. Some of them are very aggressive. Some of them are not at all. Some of", "tokens": [50684, 295, 41848, 807, 264, 1988, 13, 2188, 295, 552, 366, 588, 10762, 13, 2188, 295, 552, 366, 406, 412, 439, 13, 2188, 295, 51020], "temperature": 0.0, "avg_logprob": -0.11713178883428159, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.012288336642086506}, {"id": 1742, "seek": 1044744, "start": 10460.560000000001, "end": 10464.960000000001, "text": " them kind of bump into each other at much higher rates than others. And they have kind of very", "tokens": [51020, 552, 733, 295, 9961, 666, 1184, 661, 412, 709, 2946, 6846, 813, 2357, 13, 400, 436, 362, 733, 295, 588, 51240], "temperature": 0.0, "avg_logprob": -0.11713178883428159, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.012288336642086506}, {"id": 1743, "seek": 1044744, "start": 10464.960000000001, "end": 10471.52, "text": " different strategies and environments that they work in. And the link with AI is a little bit", "tokens": [51240, 819, 9029, 293, 12388, 300, 436, 589, 294, 13, 400, 264, 2113, 365, 7318, 307, 257, 707, 857, 51568], "temperature": 0.0, "avg_logprob": -0.11713178883428159, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.012288336642086506}, {"id": 1744, "seek": 1044744, "start": 10471.52, "end": 10476.480000000001, "text": " tenuous, to be honest. I'm mostly just reading this book out of interest. But in an ant colony,", "tokens": [51568, 2064, 12549, 11, 281, 312, 3245, 13, 286, 478, 5240, 445, 3760, 341, 1446, 484, 295, 1179, 13, 583, 294, 364, 2511, 23028, 11, 51816], "temperature": 0.0, "avg_logprob": -0.11713178883428159, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.012288336642086506}, {"id": 1745, "seek": 1047648, "start": 10477.279999999999, "end": 10482.72, "text": " ants are smarter than like a human sellers. They're the kind of self contained units that, you know,", "tokens": [50404, 23355, 366, 20294, 813, 411, 257, 1952, 31276, 13, 814, 434, 264, 733, 295, 2698, 16212, 6815, 300, 11, 291, 458, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09519939255296138, "compression_ratio": 1.81640625, "no_speech_prob": 0.0008720888290554285}, {"id": 1746, "seek": 1047648, "start": 10482.72, "end": 10488.88, "text": " eat and do tasks by themselves. And they're pretty autonomous. But the ants are still pretty dumb.", "tokens": [50676, 1862, 293, 360, 9608, 538, 2969, 13, 400, 436, 434, 1238, 23797, 13, 583, 264, 23355, 366, 920, 1238, 10316, 13, 50984], "temperature": 0.0, "avg_logprob": -0.09519939255296138, "compression_ratio": 1.81640625, "no_speech_prob": 0.0008720888290554285}, {"id": 1747, "seek": 1047648, "start": 10489.52, "end": 10496.8, "text": " And no ant really knows that it's part of a colony or knows that the colony has", "tokens": [51016, 400, 572, 2511, 534, 3255, 300, 309, 311, 644, 295, 257, 23028, 420, 3255, 300, 264, 23028, 575, 51380], "temperature": 0.0, "avg_logprob": -0.09519939255296138, "compression_ratio": 1.81640625, "no_speech_prob": 0.0008720888290554285}, {"id": 1748, "seek": 1047648, "start": 10496.8, "end": 10500.24, "text": " certain tasks that it needs to do and that it has to help out with the colony efforts.", "tokens": [51380, 1629, 9608, 300, 309, 2203, 281, 360, 293, 300, 309, 575, 281, 854, 484, 365, 264, 23028, 6484, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09519939255296138, "compression_ratio": 1.81640625, "no_speech_prob": 0.0008720888290554285}, {"id": 1749, "seek": 1047648, "start": 10501.039999999999, "end": 10506.4, "text": " It's more like a kind of little robot that's kind of like bumping into other ants and getting like", "tokens": [51592, 467, 311, 544, 411, 257, 733, 295, 707, 7881, 300, 311, 733, 295, 411, 9961, 278, 666, 661, 23355, 293, 1242, 411, 51860], "temperature": 0.0, "avg_logprob": -0.09519939255296138, "compression_ratio": 1.81640625, "no_speech_prob": 0.0008720888290554285}, {"id": 1750, "seek": 1050640, "start": 10506.56, "end": 10510.08, "text": " signals and then adjusting its behavior based on that interaction.", "tokens": [50372, 12354, 293, 550, 23559, 1080, 5223, 2361, 322, 300, 9285, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1068926578810235, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.001131692435592413}, {"id": 1751, "seek": 1050640, "start": 10510.72, "end": 10518.32, "text": " Right. So it's not like, I guess, a company where like the different people in the company are like,", "tokens": [50580, 1779, 13, 407, 309, 311, 406, 411, 11, 286, 2041, 11, 257, 2237, 689, 411, 264, 819, 561, 294, 264, 2237, 366, 411, 11, 50960], "temperature": 0.0, "avg_logprob": -0.1068926578810235, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.001131692435592413}, {"id": 1752, "seek": 1050640, "start": 10518.32, "end": 10524.24, "text": " my job is marketing. And they have like a basic picture of how it all fits together.", "tokens": [50960, 452, 1691, 307, 6370, 13, 400, 436, 362, 411, 257, 3875, 3036, 295, 577, 309, 439, 9001, 1214, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1068926578810235, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.001131692435592413}, {"id": 1753, "seek": 1050640, "start": 10524.24, "end": 10529.119999999999, "text": " They're much more like, if a person at a company doing marketing was just like,", "tokens": [51256, 814, 434, 709, 544, 411, 11, 498, 257, 954, 412, 257, 2237, 884, 6370, 390, 445, 411, 11, 51500], "temperature": 0.0, "avg_logprob": -0.1068926578810235, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.001131692435592413}, {"id": 1754, "seek": 1050640, "start": 10529.119999999999, "end": 10530.8, "text": " I don't know why I do it, I just do it.", "tokens": [51500, 286, 500, 380, 458, 983, 286, 360, 309, 11, 286, 445, 360, 309, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1068926578810235, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.001131692435592413}, {"id": 1755, "seek": 1050640, "start": 10530.8, "end": 10535.199999999999, "text": " Yeah, exactly. And another disadvantage with a company is that a company, there's someone at", "tokens": [51584, 865, 11, 2293, 13, 400, 1071, 24292, 365, 257, 2237, 307, 300, 257, 2237, 11, 456, 311, 1580, 412, 51804], "temperature": 0.0, "avg_logprob": -0.1068926578810235, "compression_ratio": 1.7158671586715868, "no_speech_prob": 0.001131692435592413}, {"id": 1756, "seek": 1053520, "start": 10535.2, "end": 10539.68, "text": " the top that's kind of coordinating the whole thing. Whereas with ants, there's no one that's", "tokens": [50364, 264, 1192, 300, 311, 733, 295, 37824, 264, 1379, 551, 13, 13813, 365, 23355, 11, 456, 311, 572, 472, 300, 311, 50588], "temperature": 0.0, "avg_logprob": -0.07918421696808378, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.014742252416908741}, {"id": 1757, "seek": 1053520, "start": 10539.68, "end": 10547.12, "text": " coordinating it. It's just including the queen, there's no management system. It's just all of the,", "tokens": [50588, 37824, 309, 13, 467, 311, 445, 3009, 264, 12206, 11, 456, 311, 572, 4592, 1185, 13, 467, 311, 445, 439, 295, 264, 11, 50960], "temperature": 0.0, "avg_logprob": -0.07918421696808378, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.014742252416908741}, {"id": 1758, "seek": 1053520, "start": 10547.12, "end": 10551.68, "text": " you know, hundreds and thousands of ants have their individual instincts of what they do when", "tokens": [50960, 291, 458, 11, 6779, 293, 5383, 295, 23355, 362, 641, 2609, 38997, 295, 437, 436, 360, 562, 51188], "temperature": 0.0, "avg_logprob": -0.07918421696808378, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.014742252416908741}, {"id": 1759, "seek": 1053520, "start": 10551.68, "end": 10556.0, "text": " they bump into each other and what they do when they bump into food and what they do when they", "tokens": [51188, 436, 9961, 666, 1184, 661, 293, 437, 436, 360, 562, 436, 9961, 666, 1755, 293, 437, 436, 360, 562, 436, 51404], "temperature": 0.0, "avg_logprob": -0.07918421696808378, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.014742252416908741}, {"id": 1760, "seek": 1053520, "start": 10556.0, "end": 10561.84, "text": " realize that there's, you know, there's not as much food as there needs to be. And by kind of all", "tokens": [51404, 4325, 300, 456, 311, 11, 291, 458, 11, 456, 311, 406, 382, 709, 1755, 382, 456, 2203, 281, 312, 13, 400, 538, 733, 295, 439, 51696], "temperature": 0.0, "avg_logprob": -0.07918421696808378, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.014742252416908741}, {"id": 1761, "seek": 1056184, "start": 10561.84, "end": 10566.32, "text": " of the ants following their own individual instincts, it just turns out that they act", "tokens": [50364, 295, 264, 23355, 3480, 641, 1065, 2609, 38997, 11, 309, 445, 4523, 484, 300, 436, 605, 50588], "temperature": 0.0, "avg_logprob": -0.07980569041505152, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.018977604806423187}, {"id": 1762, "seek": 1056184, "start": 10566.32, "end": 10571.84, "text": " as if they were kind of a fairly well coordinated company that is like ensuring that there are", "tokens": [50588, 382, 498, 436, 645, 733, 295, 257, 6457, 731, 29591, 2237, 300, 307, 411, 16882, 300, 456, 366, 50864], "temperature": 0.0, "avg_logprob": -0.07980569041505152, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.018977604806423187}, {"id": 1763, "seek": 1056184, "start": 10571.84, "end": 10576.8, "text": " some ants going to get food and some ants that are keeping the nest in order and some ants that", "tokens": [50864, 512, 23355, 516, 281, 483, 1755, 293, 512, 23355, 300, 366, 5145, 264, 15646, 294, 1668, 293, 512, 23355, 300, 51112], "temperature": 0.0, "avg_logprob": -0.07980569041505152, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.018977604806423187}, {"id": 1764, "seek": 1056184, "start": 10576.8, "end": 10582.8, "text": " are feeding the young. But that that coordination happens kind of almost magically and emerges", "tokens": [51112, 366, 12919, 264, 2037, 13, 583, 300, 300, 21252, 2314, 733, 295, 1920, 39763, 293, 38965, 51412], "temperature": 0.0, "avg_logprob": -0.07980569041505152, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.018977604806423187}, {"id": 1765, "seek": 1056184, "start": 10582.8, "end": 10589.44, "text": " out of those individual ant interactions. So one example of how this works is that", "tokens": [51412, 484, 295, 729, 2609, 2511, 13280, 13, 407, 472, 1365, 295, 577, 341, 1985, 307, 300, 51744], "temperature": 0.0, "avg_logprob": -0.07980569041505152, "compression_ratio": 1.8380566801619433, "no_speech_prob": 0.018977604806423187}, {"id": 1766, "seek": 1058944, "start": 10590.32, "end": 10596.720000000001, "text": " if an ant comes across a body of a dead ant, then if there's another dead body nearby or", "tokens": [50408, 498, 364, 2511, 1487, 2108, 257, 1772, 295, 257, 3116, 2511, 11, 550, 498, 456, 311, 1071, 3116, 1772, 11184, 420, 50728], "temperature": 0.0, "avg_logprob": -0.0782752237400087, "compression_ratio": 2.0945945945945947, "no_speech_prob": 0.0038070238661020994}, {"id": 1767, "seek": 1058944, "start": 10596.720000000001, "end": 10601.04, "text": " tend to move it to be close to the other dead body, that's just an instinct it has. It just kind", "tokens": [50728, 3928, 281, 1286, 309, 281, 312, 1998, 281, 264, 661, 3116, 1772, 11, 300, 311, 445, 364, 16556, 309, 575, 13, 467, 445, 733, 50944], "temperature": 0.0, "avg_logprob": -0.0782752237400087, "compression_ratio": 2.0945945945945947, "no_speech_prob": 0.0038070238661020994}, {"id": 1768, "seek": 1058944, "start": 10601.04, "end": 10606.0, "text": " of moves the body towards another. And if there's like one kind of pile of three dead ants and", "tokens": [50944, 295, 6067, 264, 1772, 3030, 1071, 13, 400, 498, 456, 311, 411, 472, 733, 295, 14375, 295, 1045, 3116, 23355, 293, 51192], "temperature": 0.0, "avg_logprob": -0.0782752237400087, "compression_ratio": 2.0945945945945947, "no_speech_prob": 0.0038070238661020994}, {"id": 1769, "seek": 1058944, "start": 10606.0, "end": 10610.32, "text": " another pile of two dead ants or tend to go towards the bigger pile, so tend to move this", "tokens": [51192, 1071, 14375, 295, 732, 3116, 23355, 420, 3928, 281, 352, 3030, 264, 3801, 14375, 11, 370, 3928, 281, 1286, 341, 51408], "temperature": 0.0, "avg_logprob": -0.0782752237400087, "compression_ratio": 2.0945945945945947, "no_speech_prob": 0.0038070238661020994}, {"id": 1770, "seek": 1058944, "start": 10610.32, "end": 10614.960000000001, "text": " extra dead ant towards the pile of three. And then it turns out that if all the ants just have", "tokens": [51408, 2857, 3116, 2511, 3030, 264, 14375, 295, 1045, 13, 400, 550, 309, 4523, 484, 300, 498, 439, 264, 23355, 445, 362, 51640], "temperature": 0.0, "avg_logprob": -0.0782752237400087, "compression_ratio": 2.0945945945945947, "no_speech_prob": 0.0038070238661020994}, {"id": 1771, "seek": 1061496, "start": 10614.96, "end": 10619.679999999998, "text": " those instincts, then if there's initially a kind of a kind of sprawling mass of dead bodies", "tokens": [50364, 729, 38997, 11, 550, 498, 456, 311, 9105, 257, 733, 295, 257, 733, 295, 22734, 1688, 2758, 295, 3116, 7510, 50600], "temperature": 0.0, "avg_logprob": -0.06659103494829836, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.003216541837900877}, {"id": 1772, "seek": 1061496, "start": 10619.679999999998, "end": 10623.439999999999, "text": " everywhere, then those dead bodies will be collected into just a small number of piles", "tokens": [50600, 5315, 11, 550, 729, 3116, 7510, 486, 312, 11087, 666, 445, 257, 1359, 1230, 295, 34861, 50788], "temperature": 0.0, "avg_logprob": -0.06659103494829836, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.003216541837900877}, {"id": 1773, "seek": 1061496, "start": 10624.16, "end": 10631.599999999999, "text": " of bodies. And it's not like any of the ants are like I am the grave digger or the like keeper of", "tokens": [50824, 295, 7510, 13, 400, 309, 311, 406, 411, 604, 295, 264, 23355, 366, 411, 286, 669, 264, 12525, 2528, 1321, 420, 264, 411, 38709, 295, 51196], "temperature": 0.0, "avg_logprob": -0.06659103494829836, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.003216541837900877}, {"id": 1774, "seek": 1061496, "start": 10631.599999999999, "end": 10638.4, "text": " the cemetery. They just have like really weird like baseline rules that are like move the smaller", "tokens": [51196, 264, 31176, 13, 814, 445, 362, 411, 534, 3657, 411, 20518, 4474, 300, 366, 411, 1286, 264, 4356, 51536], "temperature": 0.0, "avg_logprob": -0.06659103494829836, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.003216541837900877}, {"id": 1775, "seek": 1061496, "start": 10638.4, "end": 10643.519999999999, "text": " group of dead ants to the to where the larger group of dead ants are. Yeah, exactly. So they", "tokens": [51536, 1594, 295, 3116, 23355, 281, 264, 281, 689, 264, 4833, 1594, 295, 3116, 23355, 366, 13, 865, 11, 2293, 13, 407, 436, 51792], "temperature": 0.0, "avg_logprob": -0.06659103494829836, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.003216541837900877}, {"id": 1776, "seek": 1064352, "start": 10643.52, "end": 10649.2, "text": " don't have to know that the whole point of this instinct is to kind of clear the ground so that", "tokens": [50364, 500, 380, 362, 281, 458, 300, 264, 1379, 935, 295, 341, 16556, 307, 281, 733, 295, 1850, 264, 2727, 370, 300, 50648], "temperature": 0.0, "avg_logprob": -0.06065693548170187, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0007855151197873056}, {"id": 1777, "seek": 1064352, "start": 10649.2, "end": 10653.2, "text": " it's easier to do work in the future. It's just an instinct they have. They don't have to know that", "tokens": [50648, 309, 311, 3571, 281, 360, 589, 294, 264, 2027, 13, 467, 311, 445, 364, 16556, 436, 362, 13, 814, 500, 380, 362, 281, 458, 300, 50848], "temperature": 0.0, "avg_logprob": -0.06065693548170187, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0007855151197873056}, {"id": 1778, "seek": 1064352, "start": 10653.2, "end": 10657.6, "text": " when everyone follows that instinct, this is the resultant pattern of behavior. And similar", "tokens": [50848, 562, 1518, 10002, 300, 16556, 11, 341, 307, 264, 1874, 394, 5102, 295, 5223, 13, 400, 2531, 51068], "temperature": 0.0, "avg_logprob": -0.06065693548170187, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0007855151197873056}, {"id": 1779, "seek": 1064352, "start": 10657.6, "end": 10663.28, "text": " instincts kind of cause them to go for food when food is available. So if they see many ants coming", "tokens": [51068, 38997, 733, 295, 3082, 552, 281, 352, 337, 1755, 562, 1755, 307, 2435, 13, 407, 498, 436, 536, 867, 23355, 1348, 51352], "temperature": 0.0, "avg_logprob": -0.06065693548170187, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0007855151197873056}, {"id": 1780, "seek": 1064352, "start": 10663.28, "end": 10669.28, "text": " in with food, that raises the probability that they'll go out and look for food. And they're not", "tokens": [51352, 294, 365, 1755, 11, 300, 19658, 264, 8482, 300, 436, 603, 352, 484, 293, 574, 337, 1755, 13, 400, 436, 434, 406, 51652], "temperature": 0.0, "avg_logprob": -0.06065693548170187, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0007855151197873056}, {"id": 1781, "seek": 1066928, "start": 10669.28, "end": 10673.2, "text": " thinking oh, there's food to be gathered. There's clearly a lot of it. So we better reassign some", "tokens": [50364, 1953, 1954, 11, 456, 311, 1755, 281, 312, 13032, 13, 821, 311, 4448, 257, 688, 295, 309, 13, 407, 321, 1101, 19486, 788, 512, 50560], "temperature": 0.0, "avg_logprob": -0.10629803466796875, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.020888928323984146}, {"id": 1782, "seek": 1066928, "start": 10673.2, "end": 10678.0, "text": " labor towards food gathering that they just have that basic instinct, which causes them to go out", "tokens": [50560, 5938, 3030, 1755, 13519, 300, 436, 445, 362, 300, 3875, 16556, 11, 597, 7700, 552, 281, 352, 484, 50800], "temperature": 0.0, "avg_logprob": -0.10629803466796875, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.020888928323984146}, {"id": 1783, "seek": 1066928, "start": 10678.0, "end": 10683.84, "text": " and help out with the food gathering. And it's something like, oh, that ant has food. Oh, another", "tokens": [50800, 293, 854, 484, 365, 264, 1755, 13519, 13, 400, 309, 311, 746, 411, 11, 1954, 11, 300, 2511, 575, 1755, 13, 876, 11, 1071, 51092], "temperature": 0.0, "avg_logprob": -0.10629803466796875, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.020888928323984146}, {"id": 1784, "seek": 1066928, "start": 10683.84, "end": 10690.16, "text": " ant has food. I'm going to go that way. Yeah, exactly. Right. How does this connect to AI?", "tokens": [51092, 2511, 575, 1755, 13, 286, 478, 516, 281, 352, 300, 636, 13, 865, 11, 2293, 13, 1779, 13, 1012, 775, 341, 1745, 281, 7318, 30, 51408], "temperature": 0.0, "avg_logprob": -0.10629803466796875, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.020888928323984146}, {"id": 1785, "seek": 1066928, "start": 10690.800000000001, "end": 10696.640000000001, "text": " So I don't know if it does connect very, very directly at all. But the idea of the connection", "tokens": [51440, 407, 286, 500, 380, 458, 498, 309, 775, 1745, 588, 11, 588, 3838, 412, 439, 13, 583, 264, 1558, 295, 264, 4984, 51732], "temperature": 0.0, "avg_logprob": -0.10629803466796875, "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.020888928323984146}, {"id": 1786, "seek": 1069664, "start": 10696.64, "end": 10705.119999999999, "text": " in my head is that it's an example of a system where lots of kind of less clever individuals", "tokens": [50364, 294, 452, 1378, 307, 300, 309, 311, 364, 1365, 295, 257, 1185, 689, 3195, 295, 733, 295, 1570, 13494, 5346, 50788], "temperature": 0.0, "avg_logprob": -0.08463730046778549, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.003463178873062134}, {"id": 1787, "seek": 1069664, "start": 10705.119999999999, "end": 10709.84, "text": " are following their local rules, doing their local task. But that what emerges from that", "tokens": [50788, 366, 3480, 641, 2654, 4474, 11, 884, 641, 2654, 5633, 13, 583, 300, 437, 38965, 490, 300, 51024], "temperature": 0.0, "avg_logprob": -0.08463730046778549, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.003463178873062134}, {"id": 1788, "seek": 1069664, "start": 10709.84, "end": 10715.359999999999, "text": " is a very coherent and effective system for ultimately gathering food, defending against", "tokens": [51024, 307, 257, 588, 36239, 293, 4942, 1185, 337, 6284, 13519, 1755, 11, 21377, 1970, 51300], "temperature": 0.0, "avg_logprob": -0.08463730046778549, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.003463178873062134}, {"id": 1789, "seek": 1069664, "start": 10715.359999999999, "end": 10722.96, "text": " predators, raising the young. And an analogy would be maybe we think it's pretty dangerous to train", "tokens": [51300, 29194, 11, 11225, 264, 2037, 13, 400, 364, 21663, 576, 312, 1310, 321, 519, 309, 311, 1238, 5795, 281, 3847, 51680], "temperature": 0.0, "avg_logprob": -0.08463730046778549, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.003463178873062134}, {"id": 1790, "seek": 1072296, "start": 10723.039999999999, "end": 10729.119999999999, "text": " really smart AIs that are individually very smart. But it might be safer to kind of set up a team of", "tokens": [50368, 534, 4069, 316, 6802, 300, 366, 16652, 588, 4069, 13, 583, 309, 1062, 312, 15856, 281, 733, 295, 992, 493, 257, 1469, 295, 50672], "temperature": 0.0, "avg_logprob": -0.07618909089461617, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.10317698121070862}, {"id": 1791, "seek": 1072296, "start": 10729.119999999999, "end": 10735.839999999998, "text": " AIs such that each AI is kind of doing its own part in a kind of team and doesn't necessarily", "tokens": [50672, 316, 6802, 1270, 300, 1184, 7318, 307, 733, 295, 884, 1080, 1065, 644, 294, 257, 733, 295, 1469, 293, 1177, 380, 4725, 51008], "temperature": 0.0, "avg_logprob": -0.07618909089461617, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.10317698121070862}, {"id": 1792, "seek": 1072296, "start": 10735.839999999998, "end": 10740.56, "text": " know how how its work is fitting into the broader whole. But nonetheless, you can maybe get a lot", "tokens": [51008, 458, 577, 577, 1080, 589, 307, 15669, 666, 264, 13227, 1379, 13, 583, 26756, 11, 291, 393, 1310, 483, 257, 688, 51244], "temperature": 0.0, "avg_logprob": -0.07618909089461617, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.10317698121070862}, {"id": 1793, "seek": 1072296, "start": 10740.56, "end": 10746.88, "text": " more out of that kind of disconnected team of AIs that are specialized and that just kind of take", "tokens": [51244, 544, 484, 295, 300, 733, 295, 29426, 1469, 295, 316, 6802, 300, 366, 19813, 293, 300, 445, 733, 295, 747, 51560], "temperature": 0.0, "avg_logprob": -0.07618909089461617, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.10317698121070862}, {"id": 1794, "seek": 1072296, "start": 10746.88, "end": 10751.599999999999, "text": " the inputs and produce the outputs without much of an understanding of the broader context. And", "tokens": [51560, 264, 15743, 293, 5258, 264, 23930, 1553, 709, 295, 364, 3701, 295, 264, 13227, 4319, 13, 400, 51796], "temperature": 0.0, "avg_logprob": -0.07618909089461617, "compression_ratio": 1.7737226277372262, "no_speech_prob": 0.10317698121070862}, {"id": 1795, "seek": 1075160, "start": 10752.16, "end": 10758.32, "text": " just thinking maybe that would be, you know, a safer way to develop advanced AI capabilities", "tokens": [50392, 445, 1953, 1310, 300, 576, 312, 11, 291, 458, 11, 257, 15856, 636, 281, 1499, 7339, 7318, 10862, 50700], "temperature": 0.0, "avg_logprob": -0.16223905956934367, "compression_ratio": 1.4311377245508983, "no_speech_prob": 0.0012581030605360866}, {"id": 1796, "seek": 1075160, "start": 10758.32, "end": 10762.32, "text": " than just training one super smart AI mega brain.", "tokens": [50700, 813, 445, 3097, 472, 1687, 4069, 7318, 17986, 3567, 13, 50900], "temperature": 0.0, "avg_logprob": -0.16223905956934367, "compression_ratio": 1.4311377245508983, "no_speech_prob": 0.0012581030605360866}, {"id": 1797, "seek": 1075160, "start": 10762.32, "end": 10770.800000000001, "text": " Right. Cool. I love that. I mean, who knows if it'll work. But I mean, that just that makes tons", "tokens": [50900, 1779, 13, 8561, 13, 286, 959, 300, 13, 286, 914, 11, 567, 3255, 498, 309, 603, 589, 13, 583, 286, 914, 11, 300, 445, 300, 1669, 9131, 51324], "temperature": 0.0, "avg_logprob": -0.16223905956934367, "compression_ratio": 1.4311377245508983, "no_speech_prob": 0.0012581030605360866}, {"id": 1798, "seek": 1077080, "start": 10770.88, "end": 10784.8, "text": " of sense to me. You don't have GPT-4 or GPT-40. You have a bunch of much dumber AI systems that can", "tokens": [50368, 295, 2020, 281, 385, 13, 509, 500, 380, 362, 26039, 51, 12, 19, 420, 26039, 51, 12, 5254, 13, 509, 362, 257, 3840, 295, 709, 274, 4182, 7318, 3652, 300, 393, 51064], "temperature": 0.0, "avg_logprob": -0.10988956842667018, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.7115195393562317}, {"id": 1799, "seek": 1077080, "start": 10784.8, "end": 10792.0, "text": " coordinate together to be just as helpful as GPT-40, but that individually couldn't do", "tokens": [51064, 15670, 1214, 281, 312, 445, 382, 4961, 382, 26039, 51, 12, 5254, 11, 457, 300, 16652, 2809, 380, 360, 51424], "temperature": 0.0, "avg_logprob": -0.10988956842667018, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.7115195393562317}, {"id": 1800, "seek": 1077080, "start": 10792.72, "end": 10798.88, "text": " most of the things the other systems can do. And so collectively, they can't do anything like", "tokens": [51460, 881, 295, 264, 721, 264, 661, 3652, 393, 360, 13, 400, 370, 24341, 11, 436, 393, 380, 360, 1340, 411, 51768], "temperature": 0.0, "avg_logprob": -0.10988956842667018, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.7115195393562317}, {"id": 1801, "seek": 1079888, "start": 10798.88, "end": 10806.16, "text": " escape from their box and find another computer to take over. And is that basically the idea?", "tokens": [50364, 7615, 490, 641, 2424, 293, 915, 1071, 3820, 281, 747, 670, 13, 400, 307, 300, 1936, 264, 1558, 30, 50728], "temperature": 0.0, "avg_logprob": -0.11331508860868567, "compression_ratio": 1.505, "no_speech_prob": 0.05566713213920593}, {"id": 1802, "seek": 1079888, "start": 10806.16, "end": 10807.359999999999, "text": " Yeah, that is the hope.", "tokens": [50728, 865, 11, 300, 307, 264, 1454, 13, 50788], "temperature": 0.0, "avg_logprob": -0.11331508860868567, "compression_ratio": 1.505, "no_speech_prob": 0.05566713213920593}, {"id": 1803, "seek": 1079888, "start": 10807.359999999999, "end": 10812.8, "text": " That's lovely. That's really, really cool. Great. Well, I should let you go. That's", "tokens": [50788, 663, 311, 7496, 13, 663, 311, 534, 11, 534, 1627, 13, 3769, 13, 1042, 11, 286, 820, 718, 291, 352, 13, 663, 311, 51060], "temperature": 0.0, "avg_logprob": -0.11331508860868567, "compression_ratio": 1.505, "no_speech_prob": 0.05566713213920593}, {"id": 1804, "seek": 1079888, "start": 10812.8, "end": 10817.279999999999, "text": " all the time we have. But thank you so much for coming on the show, Tom. It's been such a pleasure.", "tokens": [51060, 439, 264, 565, 321, 362, 13, 583, 1309, 291, 370, 709, 337, 1348, 322, 264, 855, 11, 5041, 13, 467, 311, 668, 1270, 257, 6834, 13, 51284], "temperature": 0.0, "avg_logprob": -0.11331508860868567, "compression_ratio": 1.505, "no_speech_prob": 0.05566713213920593}, {"id": 1805, "seek": 1081728, "start": 10817.28, "end": 10819.28, "text": " It's been really fun, Louisa. Thank you so much.", "tokens": [50364, 467, 311, 668, 534, 1019, 11, 7272, 3837, 13, 1044, 291, 370, 709, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15555731455485025, "compression_ratio": 1.396551724137931, "no_speech_prob": 0.031133262440562248}, {"id": 1806, "seek": 1081728, "start": 10831.92, "end": 10837.52, "text": " All right. If you liked that episode, we'll have more on this issue for you soon. But in the meantime,", "tokens": [51096, 1057, 558, 13, 759, 291, 4501, 300, 3500, 11, 321, 603, 362, 544, 322, 341, 2734, 337, 291, 2321, 13, 583, 294, 264, 14991, 11, 51376], "temperature": 0.0, "avg_logprob": -0.15555731455485025, "compression_ratio": 1.396551724137931, "no_speech_prob": 0.031133262440562248}, {"id": 1807, "seek": 1081728, "start": 10837.52, "end": 10842.16, "text": " I can recommend going back and listening to some of our best past episodes about artificial", "tokens": [51376, 286, 393, 2748, 516, 646, 293, 4764, 281, 512, 295, 527, 1151, 1791, 9313, 466, 11677, 51608], "temperature": 0.0, "avg_logprob": -0.15555731455485025, "compression_ratio": 1.396551724137931, "no_speech_prob": 0.031133262440562248}, {"id": 1808, "seek": 1084216, "start": 10842.16, "end": 10848.4, "text": " intelligence, including episode 141, Richard Newell on large language models, open AI, and", "tokens": [50364, 7599, 11, 3009, 3500, 3499, 16, 11, 9809, 1734, 6326, 322, 2416, 2856, 5245, 11, 1269, 7318, 11, 293, 50676], "temperature": 0.0, "avg_logprob": -0.15181315261705786, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.629105269908905}, {"id": 1809, "seek": 1084216, "start": 10848.4, "end": 10853.92, "text": " striving to make the future go well. There's also episode 132, Nova Dasama on why information", "tokens": [50676, 36582, 281, 652, 264, 2027, 352, 731, 13, 821, 311, 611, 3500, 3705, 17, 11, 27031, 2846, 2404, 322, 983, 1589, 50952], "temperature": 0.0, "avg_logprob": -0.15181315261705786, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.629105269908905}, {"id": 1810, "seek": 1084216, "start": 10853.92, "end": 10859.36, "text": " security may be critical to the safe development of AI systems. Episode 107, Chris Ola on what", "tokens": [50952, 3825, 815, 312, 4924, 281, 264, 3273, 3250, 295, 7318, 3652, 13, 19882, 1266, 22, 11, 6688, 422, 875, 322, 437, 51224], "temperature": 0.0, "avg_logprob": -0.15181315261705786, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.629105269908905}, {"id": 1811, "seek": 1084216, "start": 10859.36, "end": 10863.84, "text": " the hell is going on inside neural networks. Episode 92, Brian Christian on the alignment", "tokens": [51224, 264, 4921, 307, 516, 322, 1854, 18161, 9590, 13, 19882, 28225, 11, 10765, 5778, 322, 264, 18515, 51448], "temperature": 0.0, "avg_logprob": -0.15181315261705786, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.629105269908905}, {"id": 1812, "seek": 1084216, "start": 10863.84, "end": 10868.96, "text": " problem. And an oldie but a goodie, episode 44, Paul Cristiano on how open AI is developing", "tokens": [51448, 1154, 13, 400, 364, 1331, 414, 457, 257, 665, 414, 11, 3500, 16408, 11, 4552, 23199, 6254, 322, 577, 1269, 7318, 307, 6416, 51704], "temperature": 0.0, "avg_logprob": -0.15181315261705786, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.629105269908905}, {"id": 1813, "seek": 1086896, "start": 10868.96, "end": 10873.359999999999, "text": " real solutions to the AI alignment problem, and his vision of how humanity will progressively", "tokens": [50364, 957, 6547, 281, 264, 7318, 18515, 1154, 11, 293, 702, 5201, 295, 577, 10243, 486, 46667, 50584], "temperature": 0.0, "avg_logprob": -0.2015372285055458, "compression_ratio": 1.542763157894737, "no_speech_prob": 0.1275312900543213}, {"id": 1814, "seek": 1086896, "start": 10873.359999999999, "end": 10878.88, "text": " hand over decision making to AI systems. Bit of a run on title there. I think that might be my", "tokens": [50584, 1011, 670, 3537, 1455, 281, 7318, 3652, 13, 9101, 295, 257, 1190, 322, 4876, 456, 13, 286, 519, 300, 1062, 312, 452, 50860], "temperature": 0.0, "avg_logprob": -0.2015372285055458, "compression_ratio": 1.542763157894737, "no_speech_prob": 0.1275312900543213}, {"id": 1815, "seek": 1086896, "start": 10878.88, "end": 10883.759999999998, "text": " fault. But it is an excellent interview. All right. The 80,000 Hours podcast is produced and", "tokens": [50860, 7441, 13, 583, 309, 307, 364, 7103, 4049, 13, 1057, 558, 13, 440, 4688, 11, 1360, 389, 5067, 7367, 307, 7126, 293, 51104], "temperature": 0.0, "avg_logprob": -0.2015372285055458, "compression_ratio": 1.542763157894737, "no_speech_prob": 0.1275312900543213}, {"id": 1816, "seek": 1086896, "start": 10883.759999999998, "end": 10887.679999999998, "text": " edited by Kieran Harris. Audio mastering and technical editing by Simon Monsour and Ben", "tokens": [51104, 23016, 538, 591, 38516, 17426, 13, 25706, 49382, 293, 6191, 10000, 538, 13193, 376, 892, 396, 293, 3964, 51300], "temperature": 0.0, "avg_logprob": -0.2015372285055458, "compression_ratio": 1.542763157894737, "no_speech_prob": 0.1275312900543213}, {"id": 1817, "seek": 1086896, "start": 10887.679999999998, "end": 10891.759999999998, "text": " Cordell. Full transcripts and extensive collection of links to learn more are available on our site", "tokens": [51300, 40267, 898, 13, 13841, 24444, 82, 293, 13246, 5765, 295, 6123, 281, 1466, 544, 366, 2435, 322, 527, 3621, 51504], "temperature": 0.0, "avg_logprob": -0.2015372285055458, "compression_ratio": 1.542763157894737, "no_speech_prob": 0.1275312900543213}, {"id": 1818, "seek": 1089176, "start": 10891.76, "end": 10906.56, "text": " and put together. As always, thank you. Thanks for joining. Talk to you again soon.", "tokens": [50364, 293, 829, 1214, 13, 1018, 1009, 11, 1309, 291, 13, 2561, 337, 5549, 13, 8780, 281, 291, 797, 2321, 13, 51104], "temperature": 0.0, "avg_logprob": -0.44347033293350885, "compression_ratio": 1.0375, "no_speech_prob": 0.46265503764152527}], "language": "en"}