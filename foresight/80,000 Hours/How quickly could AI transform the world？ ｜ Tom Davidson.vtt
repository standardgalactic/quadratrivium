WEBVTT

00:00.000 --> 00:04.160
Hi listeners, this is the 80,000 hours podcast where we have unusually in-depth conversations

00:04.160 --> 00:07.680
about the world's most pressing problems, what you can do to solve them, and what did happen to

00:07.680 --> 00:12.400
all those horses after we invented cars. I'm Rob Wiblin, head of research at 80,000 hours.

00:13.200 --> 00:17.920
The last few months have been a crazy time for advances in artificial intelligence.

00:18.640 --> 00:22.960
Over the last couple of years, I've become, for better or worse, increasingly confident that

00:22.960 --> 00:28.560
the future is going to be shaped in a major way by what sorts of AI systems we develop and choose

00:28.560 --> 00:33.920
to deploy as they approach and then exceed human capabilities in the most important areas.

00:34.640 --> 00:40.400
That's always seemed like a really common sense idea to me, but it now is becoming pretty apparent

00:40.400 --> 00:44.720
to people across society who until recently had hardly been paying attention to this issue.

00:45.520 --> 00:50.080
Unfortunately, we've only had two episodes about AI over the last six months due to the

00:50.080 --> 00:54.800
substantial lags that come in between conceiving of an episode, finding the right guest,

00:54.800 --> 00:59.600
scheduling and recording the conversation, and then editing it, and then finally releasing it.

00:59.600 --> 01:02.880
But that scarcity of AI content is fortunately about to change.

01:03.760 --> 01:08.800
In today's episode, Louisa Rodriguez interviews Tom Davidson, a senior research analyst at Open

01:08.800 --> 01:14.800
Philanthropy, whose job is figuring out when and how society is going to be upended by advances in

01:14.800 --> 01:20.720
AI. Not a simple forecasting exercise. Tom has his work cut out for him. But if you've been wondering

01:20.720 --> 01:25.840
when you might be replaced in your job by an AI model and when AIs might be able to do everything

01:25.840 --> 01:30.240
that humans can do for less than what it costs to feed a person and keep them alive, then this

01:30.240 --> 01:34.640
episode will help you think about that a little bit more clearly and perhaps have a little bit

01:34.640 --> 01:40.560
more idea of what to expect. In particular, Louisa and Tom discuss how long it will take for AIs to

01:40.560 --> 01:45.840
go from being able to do 20% of the work humans are doing to being able to do all of it, what

01:45.840 --> 01:50.560
underlying factors are driving progress in AI and how much each of those factors is contributing.

01:51.120 --> 01:55.200
Whether we should expect that progress to speed up or slow down in incoming years,

01:55.760 --> 02:00.640
how much computer hardware is used to train these models and whether it can continue increasing at

02:00.640 --> 02:05.600
the absolutely blistering rate that it has been doing the last 10 years, when AI systems might

02:05.600 --> 02:10.720
be able to do scientific research and what implications that would then have, and also when

02:10.720 --> 02:16.240
we might expect all of this to noticeably increase GDP in a visible way and what that could look like

02:16.240 --> 02:21.440
and then what new bottlenecks might exist in an economy where AI systems were doing most of the

02:21.440 --> 02:27.680
work. And on top of that, plenty of other issues besides. Tom's expectations for the future are

02:27.680 --> 02:32.400
exciting or alarming, I guess, depending on how you want to look at them. Regular listeners will

02:32.400 --> 02:38.640
have heard me do quite a few interviews on AI over the years, but those interviews tend not to

02:38.640 --> 02:43.680
focus on my opinions, at least they're not meant to. So it's possible that people don't have that

02:43.680 --> 02:48.960
much of a sense of where I stand on this personally. In case you're interested, I think

02:48.960 --> 02:55.280
the chances that you and I are either killed due to actions taken by AI systems, or that we live

02:55.280 --> 03:01.280
to see humanity unintentionally lose control of its future because of AI systems are greater than

03:01.280 --> 03:07.280
10%, greater than a one in 10 chance. Looking at surveys and polling now, it seems like both AI

03:07.280 --> 03:12.320
experts and the general public are converging on a view that's not too far from that. It's a view

03:12.320 --> 03:18.320
that previously seemed idiosyncratic, but now is really quite mainstream. Naturally, if that if

03:18.320 --> 03:24.240
the odds are like that, then it makes figuring out how to safely develop and deploy AI, you know,

03:24.240 --> 03:28.880
probably the issue of our time. And indeed, one of the things that we should most care about from

03:28.880 --> 03:33.840
even a selfish point of view, or if we have children care about from a parental point of view.

03:34.480 --> 03:39.440
I'm an economist by training and I entirely understand how the industrial revolution

03:39.440 --> 03:45.200
ultimately raised incomes across generations. And that while factory automation was indeed

03:45.200 --> 03:50.480
financially ruinous for plenty of individuals, it didn't then result in persistent unemployment.

03:50.480 --> 03:54.880
But nonetheless, despite understanding and appreciating all of that, I am skeptical that

03:54.880 --> 04:00.400
there are going to be paying jobs for children being born today, or if not them, that they'll be

04:00.400 --> 04:05.440
paying jobs at least for their children. And we are just we're flying by the seat of our pants

04:05.440 --> 04:10.080
here and have not really figured out a plan ahead of time about what we're going to do

04:10.080 --> 04:15.280
as this technology just completely upends I think existing social relations and economic systems.

04:16.400 --> 04:22.240
Well, I'll describe the overall situation that humanity finds itself in to be pretty terrifying.

04:22.240 --> 04:27.520
The fact that all kinds of different people are waking up now to the risks here does give me hope

04:27.520 --> 04:32.400
that we can coordinate to prevent the worst. As you imagine, we'll have more to say to

04:32.400 --> 04:38.240
expand on all of that in in future episodes. But for now, I bring you Louise Rodriguez and Tom Davidson.

04:53.120 --> 04:57.600
Today, I'm speaking with Tom Davidson. Tom's a senior research analyst at Open Philanthropy,

04:57.600 --> 05:03.040
where his main focus is on when we might get transformative AI. Before joining Open Philanthropy,

05:03.040 --> 05:06.880
Tom taught science through Teach First at a comprehensive school in East London,

05:06.880 --> 05:11.280
and then was a data scientist for an education technology startup. And before all of that,

05:11.280 --> 05:15.040
Tom studied physics and philosophy at Oxford. Thanks for coming on the podcast, Tom.

05:15.600 --> 05:17.200
Thanks, Louise. It's a pleasure to be here.

05:17.840 --> 05:26.640
So I hope to talk about how fast we might go from kind of OK AI to AI that can do everything humans

05:26.640 --> 05:33.760
can do, plus how that'll affect the economy and the world. But first, yeah, how worried are you

05:33.760 --> 05:40.160
personally about the risks from AI? So about a year ago, I sat down and spent spent a morning

05:40.160 --> 05:45.680
trying to figure out, you know, what are my percentages of AI by certain time and what's

05:45.680 --> 05:52.400
my percentage that there's existential catastrophe from AI. And so I was focusing on the possibility

05:52.400 --> 05:58.880
that AI disempowers humanity and just takes over control of of society and the economy and

05:58.880 --> 06:03.520
and then what happens in the future. And a year ago, I landed at a number that was a bit above

06:03.520 --> 06:12.960
10% for the probability that AI takes over by 2070. Right. OK. I mean, that's already pretty high.

06:14.000 --> 06:17.360
Yeah, that is that is already very high and much too high. Yeah.

06:18.320 --> 06:23.440
I think since since then, if I if I read the exercise today, I think I'd be close to 20%

06:25.440 --> 06:30.080
I think I compare to then I think it's just more likely that we develop AI that's capable

06:30.080 --> 06:35.840
of doing that by 2070. I also think that it's it's just pretty likely to happen in the next 20 years.

06:37.520 --> 06:42.240
Which then makes the chance that it goes badly, I think higher because we have less time to repair.

06:42.240 --> 06:47.520
So both those things, I think mean I'd probably be at about 20% if I if I read the exercise today.

06:48.640 --> 06:51.360
Yeah, we'll talk more about a couple of those things. But first,

06:52.560 --> 06:58.160
yeah, I'm curious if there's a type of scenario you have in mind when you're thinking about that

06:58.160 --> 07:06.320
10 to 20% that makes you, yeah, especially worried. Yeah, I think the main line scenario I have

07:06.880 --> 07:17.040
is something like in the next 10 to 15 years, possibly sooner, we train an AI that is able to

07:18.320 --> 07:25.120
massively enhance the productivity of AI R&D workers. So people who are currently working to

07:25.120 --> 07:30.960
make AI better, maybe it makes them let's say five times as productive. So something like a large

07:30.960 --> 07:37.360
language model helps people working on AI R&D in particular, like code much faster or develop

07:37.360 --> 07:43.440
better algorithms or something. Exactly. And it means they can work five times faster ish.

07:43.440 --> 07:49.440
Exactly. Right. Okay. And then I think it won't be long after that, because AI is then going to be

07:49.440 --> 07:55.760
improving more quickly before AI is able to do everything that the current employees of DeepMind

07:55.760 --> 08:00.720
and OpenAI are currently doing for their jobs, at least the ones that can work remotely

08:00.720 --> 08:05.360
and do their work from a computer. Are you kind of distinguishing between the ones that work

08:05.360 --> 08:11.760
with like physical robots or the ones that work with like, I don't know, the mail room at DeepMind?

08:12.640 --> 08:17.440
Yeah. So if there's someone, I don't know if this is true, but if there's someone at DeepMind

08:17.440 --> 08:22.720
who is physically stacking the computer chips into the data center, then that's a kind of

08:22.720 --> 08:27.200
type of physical work, which I don't think would necessarily, you know, follow immediately on.

08:27.920 --> 08:32.480
But I think most of the work is not like that. Right. So I think AI, you know,

08:32.480 --> 08:37.840
especially most of the work for, you know, advancing AI is stuff that you can do on your laptop.

08:37.840 --> 08:44.160
So first we get the kind of 5x productivity gain. A little time later, we get AI that can do all of

08:44.160 --> 08:49.360
the work that people at OpenAI and DeepMind can do. And I really think the gap between those is

08:49.360 --> 08:54.000
not going to be very big. And we'll probably discuss a bit more about that later. At that point,

08:54.000 --> 09:00.880
I think AI is going to be improving at a blistering pace, absent a very specific effort to slow down,

09:00.880 --> 09:06.080
which I hope we really make, but absent that effort and absent coordinating to make sure that

09:06.080 --> 09:11.360
everyone is slowing down. You know, I think like a thousand next improvement in the AI's capabilities

09:11.360 --> 09:21.120
in a year is like a natural kind of conservative default. Wow. And so in this scenario, it wouldn't

09:21.120 --> 09:28.640
be long before the AI is just far outstripping the cognitive intelligence abilities of the smartest

09:28.640 --> 09:34.080
humans and indeed even the smartest massive teams of humans working together. Wow. And when

09:34.080 --> 09:38.400
you kind of crunch the numbers on how many AI's there's likely to be around this time,

09:38.400 --> 09:44.080
there's going to be hundreds of millions and probably many billions of human worker equivalents

09:44.080 --> 09:50.560
just in kind of AI cognitive ability. Is that because there are different AI systems or because

09:50.560 --> 09:57.600
you've got that much brain power being deployed like through AI, even if it's only like five AI

09:57.600 --> 10:03.520
systems or something? It's like running lots of copies of maybe a few AI systems. Got it. So

10:04.080 --> 10:07.600
for example, I haven't crunched the numbers on this, but I would guess that if you took the

10:08.320 --> 10:13.280
kind of computer chips that they use to train GPT-4 and then you just said how many copies of

10:13.280 --> 10:18.880
GPT-4 could we run using these computer chips, I guess that the answer is like maybe a million.

10:19.840 --> 10:25.280
Wow. It might be a bit lower actually. I'm not I'm not sure for GPT-4, but I think by the time that

10:25.280 --> 10:32.960
we train kind of AI that can fully replace all the human workers at an AI lab, I think it's

10:32.960 --> 10:37.440
that number is going to be more extreme. Right. So I think that by the time we can train that kind

10:37.440 --> 10:43.120
of AI, you'll be able to just immediately use the compute that you use for training to run 100

10:43.120 --> 10:48.960
million. Right. That's okay. That's insane. And the reason for that is because it just takes,

10:49.920 --> 10:57.040
I don't know, like many times more compute to train an AI system than it does to then run them.

10:57.760 --> 11:03.760
Yeah, that's exactly right. Okay. One way to think about it is that the AI is trained on kind of

11:03.760 --> 11:08.960
millions of days of experience so that they get to be as good as they are. And that one,

11:08.960 --> 11:12.160
if you've managed to train them for that much, then it's kind of obviously you can run millions.

11:12.160 --> 11:20.560
Yeah. Okay. Got it. Okay. Okay. So then we're there. We've got potentially millions of human

11:20.560 --> 11:27.200
equivalents of copies of AI systems running and doing the kind of work that humans could do

11:28.240 --> 11:34.000
at the human level or better. Exactly. And then what? So then I think it's likely that it won't

11:34.000 --> 11:39.120
be long until we're talking about billions of AI systems. It's run of the mill to see 3x efficiency

11:39.120 --> 11:45.840
improvements in AI at various different levels of the software stacks. You could get a 3x efficiency

11:45.840 --> 11:53.440
in kind of how effective the algorithm is or a 3x efficiency improvement in how well the software

11:53.440 --> 11:58.560
runs on the hardware. And there are these various layers of the software stack that you can make

11:58.560 --> 12:03.920
improvements on. So I think it's probably at that point, once you have hundreds of millions of AI

12:03.920 --> 12:08.480
is looking for these types of improvements, you're probably going to get very, very quick,

12:08.560 --> 12:14.880
further improvements in AI cognitive ability. Again, maybe we coordinate to go very slowly,

12:14.880 --> 12:20.320
but this is absent that targeted coordination. Maybe the default. Yeah. Maybe the default

12:20.320 --> 12:28.000
scarily. And at that point, I think that if the AIs are misaligned, if they have goals that are

12:28.000 --> 12:34.320
different to what humans want them to do, and if those goals imply that it would be useful for them

12:34.320 --> 12:38.160
to get power so they could achieve those goals better, then I don't think it's going to be very

12:38.160 --> 12:45.520
hard for them to do that. Because it's like we've got a billion really smart humans who want to take

12:45.520 --> 12:50.880
power. Probably they'll find out some kind of way to do it. Maybe they invent new technology,

12:50.880 --> 12:56.000
maybe they convince some higher officials to give them control of the military. I'm not sure

12:56.000 --> 13:00.720
exactly how they'll do it. But at that point, I think it's kind of too late for us to be

13:01.360 --> 13:08.320
preventing AI takeover. Okay. And then by AI takeover, do you mind spelling that out?

13:09.280 --> 13:15.440
Yeah. So I think the thing that matters for AI takeover is that AI systems collectively end up

13:15.440 --> 13:20.960
in control of what happens in the future, and that it's kind of their goals and decisions that

13:20.960 --> 13:28.160
dictate the future path, and that it's no longer sensitive to what humans want or trying to achieve

13:28.160 --> 13:33.280
with the future. So I mean, ultimately, I think it does have to come down to physical force,

13:33.280 --> 13:37.600
most likely. I mean, you could imagine the scenario where the AI just convince humans and

13:37.600 --> 13:41.920
hypnotize them or something, and that's how they take over. But more likely they end up

13:41.920 --> 13:47.680
having control of the hard military equipment, and that's what allows them to establish

13:48.640 --> 13:56.240
their power and disempower humanity. So a thing that I have to admit still confuses me

13:56.240 --> 14:05.600
is just how we go from, I don't know, things like GPT-4, which even if it sometimes gets

14:05.600 --> 14:12.320
super confused and says silly things in a way that's like, oh, you clearly misunderstood

14:12.320 --> 14:20.640
what I was asking for, it's really hard for me to understand what the path to that kind of confusion

14:21.200 --> 14:31.120
to a misalignment that's so, I don't know, I guess just incredibly diverging from human values

14:31.120 --> 14:39.040
that the AI systems want to disempower humans. And I mean, one article I read,

14:39.040 --> 14:45.040
I think that just came out recently on Vox, is this article that was actually making the case

14:45.040 --> 14:49.760
that companies creating AI should slow down, should kind of coordinate to slow down,

14:51.040 --> 14:56.240
was walking through the case for why we might expect AI to be misaligned. And the example they

14:56.240 --> 15:03.680
gave just still confuses me. So the example is something like, let's say you've got a super

15:03.680 --> 15:11.200
smart AI system, we've programmed it to solve impossibly difficult problems, like calculating

15:11.280 --> 15:16.240
the number of atoms in a universe, for example, the AI system might realize that it could do a

15:16.240 --> 15:20.560
better job if it gained access to all of the computers on earth. So it releases a weapon of

15:20.560 --> 15:26.640
mass destruction to wipe out all humans, for example, an engineered virus that kills everyone

15:26.640 --> 15:32.240
but leaves infrastructure intact. And now it's free to use all the computer power. And that's

15:32.240 --> 15:38.240
the best way it's able to achieve its goal. And I think I just, I just really, really, I like,

15:38.240 --> 15:44.240
I feel silly. I feel, I feel dumb. I feel like I'm missing something. Like, how will it go from

15:44.240 --> 15:51.440
like, I want to solve this problem for humans to like, I'm going to kill them all to take their

15:51.440 --> 15:58.800
resources so that I can solve the problem. Like, why have we not ruled that kind of extreme behavior

15:58.800 --> 16:05.120
out? Great question. So let's maybe we can try and think about this, this system, which is trying

16:05.120 --> 16:12.800
to solve these these math problems. So maybe the first version of the AI, you just say, look,

16:13.600 --> 16:20.000
we want you to solve the problem using one of these four techniques. And that kind of system is okay.

16:20.560 --> 16:25.520
But then someone comes along and realizes that if you let the AI system do an internet search

16:26.080 --> 16:33.600
and plan its own line of attack on the problem, then it's able to do a better job in solving

16:33.680 --> 16:37.360
even harder and harder problems. And so you say, okay, we'll allow the AI to do that.

16:38.560 --> 16:43.440
And then over time, in order to improve performance, you give it more and more scope

16:43.440 --> 16:50.000
to kind of be creative in planning how it's going to attack each different kind of problem.

16:51.360 --> 16:58.400
One thing that might happen internally inside the AI's own head is that the AI may end up

16:59.200 --> 17:05.200
developing just an inherent desire to just get the answer to this math question as accurate as

17:05.200 --> 17:10.880
possible. That's something which it always gets rewarded for when it's being trained.

17:11.600 --> 17:15.280
And you know, maybe it could be thinking, I actually just want the humans to be happy with

17:15.280 --> 17:18.400
my answer. But another thing it might end up thinking is, you know what, what I really want

17:18.400 --> 17:23.760
is just to get the answer correct. And the kind of the feedback that as humans are giving that

17:23.760 --> 17:29.040
system doesn't distinguish between those two possibilities. So maybe we get unlucky and maybe

17:29.040 --> 17:34.240
the thing that it wants is to just really get the answer correct. And maybe the way that the

17:34.240 --> 17:38.800
AI system is working internally is it's saying, okay, that's my goal. What plan can I use to achieve

17:38.800 --> 17:43.360
that goal? And it's kind of creatively going and looking for new new approaches by googling

17:43.360 --> 17:49.440
information. Maybe one time it's like, it realizes that if I hacked into a kind of another computing

17:49.440 --> 17:55.040
cluster, it could use those computations to help itself the problem. And it does that no one realizes

17:55.040 --> 18:00.640
and then that kind of that reinforces the fact that it is now planning on such a broad scale to

18:00.640 --> 18:06.880
try and achieve this goal. And then maybe it's much more powerful at a later time. And it realizes

18:06.880 --> 18:12.400
that yeah, if it kills all humans, it could have access to all the supercomputers. And then that

18:12.400 --> 18:18.000
would help it get an even more accurate answer. And because the thing it cares about is not pleasing

18:18.080 --> 18:22.800
the humans, the thing it happened to care about internally was actually just getting an accurate

18:22.800 --> 18:28.720
answer. That plan looks great by its own lights. And so it goes and executes the plan.

18:28.720 --> 18:38.880
Right. So one that was really helpful. But I still feel confused about one why it's so hard to

18:40.080 --> 18:46.400
not give it some instructions that are just like, use whatever you need, but like don't hurt living

18:46.480 --> 18:52.880
things. So I think we could definitely give it those instructions. The question is,

18:53.520 --> 18:59.120
inside its own mind, what is its goal in the end of the day? So you could give it instructions,

18:59.120 --> 19:02.480
don't hurt humans, and it would read that and understand that that's what you wanted.

19:03.600 --> 19:07.840
But if throughout its life, it's always been rewarded for getting an accurate answer to

19:07.840 --> 19:13.280
these math problems, it might just itself only care about getting our accurate answers to the

19:13.360 --> 19:19.920
math problems. So it knows that the humans don't want it to hurt other humans. But it also doesn't

19:19.920 --> 19:23.920
care about that itself, because all it cares about is getting accurate answers to this problem.

19:23.920 --> 19:29.040
And so sure, it knows that humans don't want it to hurt other humans. And so it makes sure to not

19:29.040 --> 19:34.480
do that in an obvious way, because it anticipates and it might get shut down. But its knowledge

19:34.480 --> 19:39.920
of what humans want it to do doesn't change what its own desire is internally.

19:40.640 --> 19:47.040
So I suppose I understand why you couldn't just give the system an instruction that didn't also

19:47.040 --> 19:56.480
come with rewards. Is it impossible to give an AI system a reward for every every problem it solves

19:56.480 --> 20:05.360
by not hurting anyone? I think that would help somewhat. So the problem here is that there are

20:05.360 --> 20:10.960
kind of two possibilities. And it's going to be hard for us to give rewards that ensure that one

20:10.960 --> 20:15.200
of the possibilities happen and not the second possibility. So here are the two possibilities.

20:15.200 --> 20:21.040
One possibility is the AI really doesn't want to hurt humans. And it's just going to keep that

20:21.040 --> 20:24.720
and take that into account when solving the math problem. That's what we want to happen.

20:25.440 --> 20:31.760
The other possibility is that the AI only cares about solving the math problem. And it doesn't

20:31.760 --> 20:38.080
care about humans at all. But it understands that humans don't like it when it hurts them.

20:38.080 --> 20:41.280
And so it kind of doesn't hurt humans in any obvious way.

20:41.280 --> 20:51.120
Oh, right. Okay. And so this is a route to AI not caring about humans, but being kind of deceptive.

20:51.120 --> 20:57.840
I guess maybe an analogy that really speaks to me is something like if you were to punish a child

20:57.920 --> 21:05.040
for, I don't know, having ice cream before dinner, like you might get them not to have ice

21:05.040 --> 21:11.200
cream before dinner, or you might create a thing where they have ice cream before dinner while

21:11.200 --> 21:20.720
hiding in the closet. And it's like pretty complicated to teach a nuanced enough lesson

21:20.720 --> 21:27.040
to a child about ice cream and why they shouldn't have it for dinner. That doesn't have any risk

21:27.040 --> 21:32.400
of the lying version. Is that kind of right? Yeah, I think that's a good analogy. Okay, nice.

21:32.400 --> 21:40.640
I think with a child, it might be somewhat easier because you're much more capable than them. So

21:40.640 --> 21:44.480
even if they ever did try and eat ice cream in secret, you'd have a good chance of catching them.

21:44.480 --> 21:48.160
I think the problem gets really hard when the AIs are much smarter than us,

21:48.160 --> 21:52.800
such they could quite easily eat ice cream without us noticing. And then it's really hard

21:52.800 --> 21:57.360
for us to give them rewards, which stop them from doing that. Right, right. And something like

21:58.000 --> 22:03.120
we have a pretty good idea how children's brains work. They work kind of like ours,

22:03.120 --> 22:06.240
but like a bit simpler and like we have some idea of the ways they're different.

22:06.800 --> 22:11.760
And so we can like make guesses about the types of motivations that will speak to them.

22:13.040 --> 22:20.320
I don't know, maybe it's like we know that our kids work similar to us and that like they feel

22:20.320 --> 22:25.040
shame and they'd feel shame if they were punished and want to like please us because that's just

22:25.040 --> 22:30.320
like pretty human. And so maybe we have a better sense of how they'd respond to punishment. But

22:30.320 --> 22:37.280
maybe AI systems are just like so different to humans that we really have no idea what their

22:38.720 --> 22:45.760
or at least we'll have a less clear idea of what other processes that they're using or like things

22:45.840 --> 22:50.960
that they experienced or whatever are like and what kinds of behaviors those will push them toward.

22:51.760 --> 22:53.440
Yeah, I think that does make it a lot harder.

22:54.000 --> 23:00.000
Cool. That's super helpful. Yeah, is that is that basically the key scenario you're worried about?

23:00.560 --> 23:08.880
This kind of we train AI systems to achieve certain goals, but it's hard to know what strategies

23:08.880 --> 23:15.840
they see as fair game. And it's hard to train them not to pursue harmful strategies. And then

23:17.200 --> 23:22.640
they eventually get super smart. Maybe they are deceptive. Maybe they're just very convincing.

23:22.640 --> 23:29.360
And so they they're able to get a bunch of power and really disempower humans. Is that kind of what

23:29.360 --> 23:36.800
you see as the core risk? Yeah, that's right. And I would emphasize that in the scenarios I described

23:36.800 --> 23:43.040
it, AI is improving really, really rapidly as it approaches and then go through the human range.

23:43.760 --> 23:48.560
So, you know, when we're talking about this, this example with the AI that's trying to solve math

23:48.560 --> 23:52.560
problems, maybe we're thinking, oh, we'll have, you know, a few years with it trying out this kind

23:52.560 --> 23:56.960
of strategy. And then we notice it's kind of doing a little bit of hacking into computer resources.

23:56.960 --> 24:03.520
So, you know, we tamp down on that. But but if this whole thing plays out over just one year,

24:03.520 --> 24:09.920
for example, we go from like, you know, notably below human to superhuman systems. Yeah.

24:10.720 --> 24:15.200
I think it makes the risks a lot more intense. Yeah, that makes sense to me.

24:15.920 --> 24:21.040
Yeah, to move us on to your research, then. Some of the work you've done that kind of most blew my

24:21.040 --> 24:26.880
mind was actually on what happens when we're able to basically build AI that does roughly what we

24:26.880 --> 24:32.320
intended to do. I think I naively would have guessed something like the world carries on is normal,

24:32.320 --> 24:39.520
but we use GPT-8 a lot in our jobs. But you've looked into the hypothesis that not only will

24:39.520 --> 24:45.840
things not stay the same, they actually might change very, very, very quickly if AGI is so good

24:45.840 --> 24:52.480
that it kind of causes explosive economic growth, which yeah, in this case, you're defining as the

24:52.480 --> 24:57.840
world economy growing something like 10 times faster than it has for the last century. Yeah,

24:57.840 --> 25:03.520
so to start, can you help me understand intuitively what it would mean for the economy to grow 10

25:03.520 --> 25:09.280
times faster? Sure. So one way to think about this is to think about all the technological

25:09.280 --> 25:15.520
changes that have happened over the last 50 years. Okay. Yeah, that feels like a lot. So 50 years ago,

25:15.520 --> 25:24.400
it was 1970, we'd had very basic digital computers around, but they weren't being widely used,

25:24.400 --> 25:31.280
they weren't very good. I don't think the internet was around. And there's loads of other improvements

25:31.280 --> 25:40.640
in manufacturing and in agricultural techniques. Medical care. Exactly. Yeah. And massive improvements

25:40.640 --> 25:46.720
across the board in the last 50 years, but probably the most striking is IT. Yeah, sounds right.

25:46.720 --> 25:51.200
And so what explosive growth would look like is that all those changes, rather than happening

25:51.200 --> 25:56.720
over the course of 50 years, they happened over the course of five years. So we're going to get

25:56.720 --> 26:03.680
the internet in five years plus a bunch of other improvements. Exactly. So rather than kind of

26:04.480 --> 26:10.720
it taking 50 years to go from these really rubbish slow computers that you could buy in 1970 to the

26:10.720 --> 26:16.880
awesome MacBooks of today, that just happens over five years. And similarly, rather than taking 50

26:16.880 --> 26:23.440
years for you to go from those kind of rubbish phones to smartphones of today that also have the

26:23.440 --> 26:28.480
internet and all these specialized apps, that again, this happens over five years. So the kind

26:28.480 --> 26:33.280
of you see the introduction of a new technology and then very, very quickly you see it being refined

26:33.280 --> 26:41.040
into a super useful human friendly product. Wow. I mean, on the one hand that sounds kind of incredible

26:41.040 --> 26:49.040
and exciting. On the other hand, it just feels like a super strange world to be getting so many

26:49.040 --> 26:57.680
new technologies every few years. Can you explain the idea behind why AGI might even make that possible?

26:58.480 --> 27:05.760
Yeah. So here's the most basic version of the argument, which you can kind of make it more

27:05.760 --> 27:10.080
complicated to adjust various objections. But I think kind of this version captures the core idea.

27:11.040 --> 27:16.960
So today, there are maybe tens of millions of people whose job it is to discover new and better

27:16.960 --> 27:22.480
technologies, working in science and research and development. They're able to make a certain amount

27:22.480 --> 27:27.840
of progress each year. And it's their work that helps us get better computers and phones and

27:27.840 --> 27:34.000
discover better types of solar panels and drives all these improvements that we're seeing. But

27:34.000 --> 27:39.840
like we've been talking about shortly after AGI, I think there's going to be billions of

27:40.560 --> 27:46.640
top human researchers equivalents in terms of a scientific workforce from AI.

27:47.520 --> 27:54.160
And if you imagine that workforce or half of that workforce or just 10% of it working on

27:54.800 --> 28:03.440
trying to advance technology and come up with new ideas, then you have now 10 or 100 times the effort

28:03.440 --> 28:10.160
that's going into that activity. And these AIs are also able to think maybe 10 or 100 times as

28:10.160 --> 28:16.000
quickly as humans can think. And you're able to take the very best AI researchers and copy them.

28:17.120 --> 28:22.800
So if you think that scientific progress is overwhelmingly driven by like a few smaller

28:22.800 --> 28:26.880
number of really kind of brilliant people with brilliant ideas, then we just need one of them

28:26.880 --> 28:31.600
and we can copy them. They might be happy to just work much harder than humans work.

28:32.560 --> 28:35.840
It might be possible to focus them much more effectively on the most important

28:36.480 --> 28:40.640
types of R&D, whereas humans maybe are more inclined to follow their interests,

28:41.360 --> 28:46.400
even when it's not the most useful thing to be researching. And so all of those things together

28:47.200 --> 28:53.360
just mean that we'll be generating kind of 100 times as many new good ideas and innovations

28:53.360 --> 28:59.120
each year compared with today. And then that would drive the development of technologies to be

28:59.680 --> 29:06.160
at least 10 times faster than today. Right. How likely do you think this kind of growth is?

29:07.520 --> 29:14.080
Is it the default once we get AGI? I think it is a default. You could give

29:14.080 --> 29:20.000
objections to the argument I gave, but I think it's mostly possible to answer those objections.

29:20.000 --> 29:24.320
So you could say, well, discovering new technologies isn't just about thinking

29:24.320 --> 29:27.840
and coming up with new ideas, you also need to do experiments. Okay, sure.

29:28.640 --> 29:34.880
And then I think you can answer that objection by saying, that's right, we will need to do

29:34.880 --> 29:41.680
experiments. And that's like testing a drug on humans and maybe it takes five years or something

29:41.680 --> 29:50.080
to really check that it's safe and effective. Right. Yeah. Or you've designed a new solar panel

29:50.080 --> 29:55.680
and you want to like test its performance in a variety of conditions. Yeah. Yeah. Or you kind

29:55.680 --> 29:59.120
of running some experiments to see what happens when you combine these two chemicals together

29:59.120 --> 30:07.360
because you're not able to predict it in advance. But if you have a billion AIs trying to push forward

30:08.000 --> 30:12.080
R&D and they're bottlenecked on these needing to do these experiments, then they'll be putting

30:12.080 --> 30:16.400
in a huge amount of effort to make these experiments happen as efficiently as possible.

30:17.360 --> 30:22.320
Whereas today, we might be using the lab for 50% of the time we could be using it

30:23.040 --> 30:26.800
and we might be just doing a whole bunch of experiments and then analyzing it afterwards

30:26.800 --> 30:32.560
and learning a little bit from each experiment, but also not kind of trying to cram as much into

30:32.560 --> 30:37.760
each experiment as is humanly possible. If these AIs are limited on experiments and they're going

30:37.760 --> 30:44.480
to be spending months and months just meticulously planning the micro details of every single

30:44.480 --> 30:50.800
experiment so that you can get as much information as possible out of each one and kind of fully

30:50.800 --> 30:55.360
coalescing their theoretical understanding and all the current data and implications and saying,

30:55.360 --> 31:00.640
here are the key uncertainties that we need to address with these like kind of scarce experiments

31:00.640 --> 31:06.080
and they'll give the humans conducting the experiments really detailed and precise instructions

31:06.080 --> 31:11.120
and set things up so the experiments are really unlikely to go wrong and kind of analyze the

31:11.120 --> 31:14.720
resultant data from 100 different angles to learn as much as you can from them.

31:16.000 --> 31:19.680
And I think that will go a long way to getting over the experimental bottleneck.

31:20.240 --> 31:26.880
I mean, even if you just think you use labs eight hours a day, but you could use them 24 hours a day

31:26.880 --> 31:32.000
and then there are probably hundreds of other efficiencies like that that will all just add up

31:32.000 --> 31:34.480
to get to many, many times more efficient stuff.

31:34.880 --> 31:40.320
Right. And I mean, the AIs can direct humans what to do so you could be paying very high

31:40.320 --> 31:45.680
wages to have unskilled human workers work through the night to run these experiments

31:45.680 --> 31:51.280
directed by AIs telling them exactly what to do when. So yeah, you'll be able to have those labs

31:51.280 --> 31:56.960
working around the clock if that's what's wanted. In the longer run, robotics is already very good

31:57.600 --> 32:03.360
and I don't think it's going to take too long once we have a billion AI researchers to design

32:03.360 --> 32:09.200
robots that are able to do the physical tasks that humans do. It doesn't, you know, with that

32:09.200 --> 32:14.400
far off at the moment. And so, you know, if eventually, you know, we actually, we just need

32:14.400 --> 32:19.040
more humans to kind of build more labs or to run these experiments, I think it will be possible to

32:19.040 --> 32:23.840
have, you know, have robots doing that work and have the AIs directing it. And again, kind of

32:23.840 --> 32:28.720
meticulously planning what each robot is doing with its time. So we're getting the very most

32:28.720 --> 32:33.200
out of each robot. Right. I mean, the example you raised about human experiments is a really

32:33.200 --> 32:37.440
good one, because that seems like it's going to be particularly hard to speed up. Right.

32:37.440 --> 32:41.200
There are still a few things that I can already think of that could happen there. So

32:42.080 --> 32:46.800
if it's kind of any psychology experiments that you're wanting to do or knowing how humans will

32:46.800 --> 32:53.360
react to a new technology or to a new scenario, then just studying all of the human data on the

32:53.360 --> 32:58.960
internet and doing in-depth interviews with humans could give AIs a really good understanding of how

32:59.040 --> 33:05.760
human psychology works. Right. In the limit, they could scan a human's brain and kind of upload

33:05.760 --> 33:12.000
them to be a kind of a virtual digital person if that human was willing to do it. And then it could

33:12.000 --> 33:16.960
then do experiments with them in a simulation much, much more quickly. Right. Really fast.

33:17.920 --> 33:25.200
I mean, that's getting pretty weird. Yeah. And I'm feeling very sci-fi, but that's part of what

33:25.280 --> 33:31.360
we're talking about. We're talking about we've got millions or billions of copies of AI systems

33:31.360 --> 33:38.960
that are as smart or smarter than humans, basically using all of their brain power to innovate.

33:39.520 --> 33:45.440
And yeah, it's bizarre, but if you apply all of that brain power, you're going to get super,

33:45.440 --> 33:51.040
super fast improvements to technology. Is this bottlenecked at all by ideas getting

33:51.040 --> 33:56.960
harder to find? Are there going to be limits that just like a human would hit up on these limits

33:56.960 --> 34:02.480
or humans have, AI systems will also hit up on those limits? Or do we expect them not to,

34:02.480 --> 34:08.080
because we're talking about superhuman intelligence? So ideas are getting hard to find.

34:08.800 --> 34:14.800
For me, the kind of the best zoomed out example is just that our scientific workforce has been

34:14.880 --> 34:22.720
growing at four or 5% every year over the last 80 years. And that's a massive increase in that

34:22.720 --> 34:27.520
scientific workforce over 80 years, you know, many, many doublings, but actually the pace of

34:27.520 --> 34:32.640
technological progress, if anything, has been slowing down somewhat over the last 80 years.

34:32.640 --> 34:36.960
And so, you know, on a high level, the explanation is sure we're using way more effort than we used

34:36.960 --> 34:41.760
to be, but the ideas are harder to find. So we're actually slowing down a little bit

34:41.840 --> 34:46.560
in terms of the pace of our progress. Maybe the best illustration of that dynamic is physics,

34:46.560 --> 34:53.360
where a kind of 100 years ago, also you could have Albert Einstein in his spare time as a

34:53.360 --> 35:00.880
patent clerk come up with multiple very significant breakthroughs. And then kind of almost single

35:00.880 --> 35:06.080
handedly or with a few collaborators develop general relativity over the few years that followed,

35:06.080 --> 35:10.080
which is just a major breakthrough in our understanding of the universe. Whereas today,

35:10.720 --> 35:18.320
you have probably, you know, maybe millions of physicists, just kind of with the huge machines

35:18.320 --> 35:23.440
at CERN that have to be honest, making, I would say pretty incremental progress in advancing the

35:23.440 --> 35:28.640
state of knowledge and physics. Yeah, okay, right. Yeah, in terms of how it applies to AI,

35:29.680 --> 35:34.240
first thing to say is that even if that dynamic exists, and it's very strong, we would still

35:34.320 --> 35:40.720
expect a very significant, if temporary, increase in the rate of technological progress.

35:41.840 --> 35:47.760
So let's say ideas are getting harder to find, but then suddenly, in 10 years, we've got a billion

35:47.760 --> 35:53.120
AIs working on it rather than the kind of 10 million humans. Well, even if our ideas are

35:53.120 --> 35:57.920
getting harder to find, then at least temporarily, there'll be much faster technological progress,

35:57.920 --> 36:02.320
and then we'll kind of pluck even more of the low hanging fruit and eventually even these AIs get

36:02.400 --> 36:08.800
stuck. Right, maybe eventually we still stagnate, but it'd be pretty crazy if you added millions of

36:08.800 --> 36:14.400
people or millions of brains to the workforce and didn't get a bunch more technological progress.

36:15.280 --> 36:18.800
Yeah, I mean, specifically if you made the workforce like a hundred times as big,

36:19.440 --> 36:25.360
then yeah, especially with the other advantages I discussed about running 10 times as fast and

36:26.400 --> 36:30.160
you know, being really focused on the most important tasks, I think really surprising

36:30.160 --> 36:34.320
if you don't get at least a temporary increase. In fact, I don't think it would be temporary,

36:34.320 --> 36:41.920
and that's because one of the things that AIs can work on is actually building more computer

36:41.920 --> 36:47.520
chips to run AIs on, improving the kind of hardware designs for those computer chips,

36:47.520 --> 36:51.520
improving the algorithms that AIs run on, improving the designs of robots.

36:51.520 --> 36:57.040
Just making themselves better scientists. Exactly, and so we've been discussing how

36:57.600 --> 37:01.680
already the pace of progress in terms of the algorithms and the hardware is pretty fast,

37:02.560 --> 37:06.320
and I've already said that I expect it to be much faster once we have AGI.

37:06.320 --> 37:11.520
And so really this isn't a kind of a constant sized AI and robotics workforce we're talking

37:11.520 --> 37:17.200
about here, but if we choose to do so, then we could have the size of that workforce

37:18.000 --> 37:23.840
doubling every year very easily. And so that means you can overcome this problem of ideas

37:23.920 --> 37:30.000
getting harder to find, because you're not kind of dealing with a constant or slowly growing

37:30.720 --> 37:36.800
workforce. You're dealing with a workforce which is itself rapidly increasing in size,

37:36.800 --> 37:42.400
and so even if ideas are getting harder to find, you've got a bigger and bigger workforce to find

37:42.400 --> 37:47.920
it. And you can actually model out this dynamic. You can take the kind of the best models we have

37:47.920 --> 37:51.760
where ideas are getting harder to find, and you can say, well, if ideas are getting harder to

37:51.760 --> 37:58.400
find the one hand, but on the other hand, AIs are able to design better AIs and do all the

37:58.400 --> 38:02.960
improvements that I talked about designing better robots, etc. How does that dynamic play out?

38:03.840 --> 38:10.720
And it turns out that at least under these models, even if ideas are getting harder to find at kind

38:10.720 --> 38:17.520
of a very steep rate, then you still are going to get the kind of the AIs and robots winning that

38:17.520 --> 38:23.520
race. That's really wild. I guess those AI scientists might hit some limits, and do you

38:23.520 --> 38:29.600
have any ideas for what those might end up being? It's a great question. I think we are going to hit

38:29.600 --> 38:34.080
limits at some point. Eventually we won't be able to design better technologies. Eventually we'll have

38:34.080 --> 38:39.600
the best algorithms we can get for making AIs. I do think there are reasons to think the limits

38:39.680 --> 38:49.840
could be quite high. One interesting data point is that small animals are able to double their

38:49.840 --> 38:56.720
population size in just a few months, and even smaller animals like insects can double their

38:56.720 --> 39:03.120
population size in just days or weeks. That shows that it is physically possible to have

39:03.120 --> 39:09.760
a certain kind of biological robot that kind of doubles its own number in the scale of weeks or

39:09.760 --> 39:15.920
months. And with all of this kind of massive scientific effort that we've been describing,

39:16.720 --> 39:22.160
it seems possible that we'll be able to design kind of robots of our own that are able to

39:22.160 --> 39:27.200
kind of double their own number, kind of build replacement robots in a similar timeframe.

39:27.920 --> 39:32.480
And so currently you can try and estimate, well, if we used a factory to try and build

39:32.560 --> 39:37.280
another copy of itself, how long would that take? I haven't seen a good analysis of this,

39:37.280 --> 39:43.040
but when I've spoken to people, they've guessed it on the order of months. So that also supports

39:43.040 --> 39:47.040
this vague idea that it might be possible to get these robots that are able to kind of build

39:47.040 --> 39:50.160
extra copies of themselves and double their own number in just a number of months.

39:50.960 --> 39:58.080
So what that all suggests is that we could have a robot workforce which is growing at a really,

39:58.080 --> 40:04.160
really high rate. And by robot, I'm picturing physical bodies, but do you do basically just

40:04.160 --> 40:10.400
mean AI system that's like working on science? Or do you think physical bodies end up being

40:11.440 --> 40:16.320
important because we've got to start automating some of these physical tasks in addition to the

40:16.320 --> 40:21.680
cognitive tasks? I'm thinking, yeah, including the physical tasks when I'm talking about the

40:21.680 --> 40:30.400
robots. I mean, if these robots each weighed 50 kilograms, and we're able to produce robots as

40:30.400 --> 40:34.880
many robots in a year, such that they weighed as much as all the cars that we currently produce in

40:34.880 --> 40:43.760
a year, then we'd be producing around a billion robots each year. So already the manufacturing

40:43.760 --> 40:50.560
capacity seems like it is theoretically there to produce a huge number of robots. And that's before

40:50.560 --> 40:56.880
taking into account that it's lucrative. And so we want to create more. And yeah, unreal.

40:56.880 --> 41:02.080
So I do think that this this dynamic leaves us in a pretty crazy world where the size of the

41:02.640 --> 41:09.200
AI and robotics workforce is growing, potentially going very, very quickly. And probably as a result,

41:10.080 --> 41:16.800
it's hard for me to imagine technology really stalling out before we hit real kind of limits,

41:16.880 --> 41:21.760
kind of fundamental limits to how good technology can get. And there will be such limits. Technology

41:21.760 --> 41:27.520
can't improve forever to infinity. Ultimately, you've come up with the very best ways of

41:27.520 --> 41:31.360
arranging the molecules to get the desired technological behavior.

41:31.920 --> 41:39.040
Right. Okay. Do we ever hit limits on just physical stuff, the stuff we make the robots out of?

41:39.040 --> 41:44.880
I think we will. So I already said about the kind of that car statistics suggesting we can get pretty

41:44.880 --> 41:50.160
far just in terms of the massive robots we could produce just with the kind of manufacturing

41:50.160 --> 41:58.720
capabilities we already have. The earth is is massive. There are kind of mountains with all

41:58.720 --> 42:04.960
these kinds of materials in them all over the place. And if we run out of a particular material,

42:05.600 --> 42:11.040
which is currently useful for building robots, then these billions of AI as we have will be

42:11.040 --> 42:16.800
working hard to find ways of doing without that scarce material. And that's been a common pattern

42:16.800 --> 42:21.280
in technological development that you kind of find ways to switch out of things that are scarce.

42:21.920 --> 42:26.880
So, yeah, right, you know, it's hard to rule out that there's just some material that we just

42:26.880 --> 42:31.360
absolutely need and we can't do without and that that all next things, you know, maybe once we get

42:31.360 --> 42:37.360
to 100 billion robots or something. But it also just seems more likely to me that given just the

42:37.360 --> 42:42.560
abundance of just kind of materials that are in the earth, it's, you know, it's a big place.

42:42.560 --> 42:46.320
There's lots of different stuff there. We, it's not like we've mined everything there was to mine

42:46.320 --> 42:51.600
or even close to it. Right. We'll be doing, you know, using all the best methods for recycling and

42:51.600 --> 42:56.240
using things as efficiently as possible. It doesn't seem to me like those kinds of bottlenecks are

42:56.240 --> 43:01.840
going to kick in particularly early. And it's not out of the question that we'd have the technology

43:01.840 --> 43:08.000
to explore resources in space. I mean, that's even I'm adding more sci-fi here. But like,

43:08.720 --> 43:15.360
if we're doubling technological progress every, I mean, it sounds like you're talking about

43:16.080 --> 43:22.080
months at some point. Yeah, I think months is plausible. Okay, months is plausible. And so

43:22.080 --> 43:28.160
that might mean we, we aren't limited by earthly limitations. That's right. There is an interesting

43:28.160 --> 43:34.640
dynamic there where if we are kind of doubling the number of robots really quickly, and we're

43:34.640 --> 43:40.320
improving technology really quickly, then we kind of, we're not that interested in doing an activity

43:40.320 --> 43:46.080
which takes maybe like 10 years to bear fruit, because we're used to kind of our investments

43:46.080 --> 43:49.920
paying off with kind of doublings every year. We're like, oh, we could go to the moon and get

43:49.920 --> 43:56.320
materials, but man, like that would take so long. If we, if we just invest in everything we can find

43:56.320 --> 44:02.080
on earth, we can much more quickly kind of increase the just use it more efficiently. Yeah, so that

44:02.080 --> 44:07.840
time delay becomes more significant when, when you're already able to go so fast. So I think I

44:07.840 --> 44:12.240
imagine the kind of going to the moon and the space stuff happens when we're really kind of

44:12.240 --> 44:17.680
struggling to find ways to make use of the earth resources. Okay, so it sounds like we're kind

44:17.760 --> 44:26.800
of talking about something like AI systems replace humans in a bunch of sectors,

44:27.600 --> 44:34.640
really during our lifetimes. And then like our lives really, really change quite radically.

44:34.640 --> 44:42.480
And very, very, very quickly. Yeah, I guess I just find that super weird. I think my brain is like,

44:42.480 --> 44:48.720
no, I don't believe you. That's too weird. I don't like, I just can't imagine that happening.

44:49.280 --> 44:56.160
If we're, if we're saying this is happening in the early 2030s, I'll be, yeah, in my late 30s.

44:56.160 --> 45:01.600
And like all of a sudden, the world would be radically changing every year. And I like won't

45:01.600 --> 45:10.400
be working. I agree. It seems really crazy. And I think it's very natural and understandable to

45:10.400 --> 45:17.440
just not believe it when you hear the arguments. And that would have been my initial reaction.

45:18.400 --> 45:23.760
In terms of why I do now believe it, there's probably a few things which have changed. Probably

45:23.760 --> 45:30.480
I've just sat with these arguments for a few years. And just been like, I just do believe it.

45:30.480 --> 45:35.600
You know, I have discussions with people on either side of the debate. And I just find

45:35.600 --> 45:40.400
that people on one side just have thought it through much more. And I think what's at the

45:40.400 --> 45:46.400
heart of it for me is that the human brain is a physical system. There's nothing magical about it.

45:47.360 --> 45:53.920
It isn't surprising that at some point, we develop machines that can do what the human

45:53.920 --> 46:00.560
brain can do at some point in the process of technological discovery. And to be honest,

46:00.560 --> 46:05.440
that happening the next couple of decades is kind of when you might expect it to happen naively.

46:05.760 --> 46:13.680
We've had computers for 70-odd years. It's been a decade since we started pouring loads and loads

46:13.680 --> 46:20.000
of compute into training AI systems. And we've realized that that approach works really, really

46:20.000 --> 46:25.040
well. Just if you were kind of to say, okay, when do you think humans might develop machines that

46:25.040 --> 46:28.480
can do what the human brain can do? You kind of think it might be in the next few decades.

46:29.280 --> 46:32.560
And I think if you just sit with that fact that there are going to be machines that can do what

46:32.560 --> 46:37.440
the human brain can do. And you're going to be able to make those machines much more efficient at

46:37.440 --> 46:42.080
it. And you're going to be able to make even better versions of those machines, 10 times better

46:42.080 --> 46:46.240
versions. You're going to be able to run them day and night. And you're going to be able to build

46:46.240 --> 46:51.040
more. When you sit with all that, I do think it gets pretty hard to imagine a future that isn't very

46:51.040 --> 46:58.720
crazy. And another perspective is just zooming out even further and just looking at the whole arc

46:58.720 --> 47:06.160
of human history. So if you'd have asked hunter gatherers who only knew the 50 people in their

47:06.160 --> 47:13.280
group and who had been hunting using techniques and tools that as far as they knew had been

47:14.000 --> 47:20.720
passed down for eternity generation to generation, doing their rituals, if you'd have told them that

47:21.360 --> 47:28.560
in a few thousand years, there are going to be huge empires building the Egyptian pyramids

47:28.560 --> 47:39.440
and massive armies and the ability to go to a market and give people pieces of metal in exchange

47:39.440 --> 47:44.640
for all kinds of goods. They would have seemed totally crazy. And then if you'd have told those

47:44.640 --> 47:51.360
people in those markets that no, there's going to be a future world where every 10 years,

47:51.360 --> 47:55.680
major, major technological progress is going to be coming along and we're going to be discovering

47:55.680 --> 48:02.240
drugs that can solve all kinds of diseases. You're going to be able to get inside a box

48:02.240 --> 48:07.120
and land the other side of the earth. Right. Again, they'd have just thought you were crazy.

48:07.120 --> 48:13.200
And I think while it seems that we understand what's happening and that progress is pretty

48:13.200 --> 48:19.520
steady, that has only been true for the last 200 years. And zooming out, it's actually the norm

48:19.520 --> 48:25.680
throughout the longer run of history for things to go in a totally surprising and unpredictable

48:25.680 --> 48:32.720
direction or a direction that would have seemed totally bizarre and unpredictable to people

48:32.720 --> 48:37.760
naively at that time. Right. I feel like I was introduced to it when I read what we did in the

48:37.760 --> 48:44.560
future, Will MacAskill's book, that there's this thing called the end of history fallacy. It really

48:44.640 --> 48:52.880
feels like we're living at the end. We're done changing. We're going to maybe find some new

48:53.760 --> 49:01.280
medical devices or something. But basically, we've done all of the weird shifting that we're

49:01.280 --> 49:09.600
going to do. And I can't really justify that. It does seem like a fallacy. Presumably,

49:10.320 --> 49:15.600
things are going to look super different in 50 years. And sometimes those changes have gone

49:15.600 --> 49:21.680
super fast in history. And sometimes they've gone super slowly. And we've got real reasons to think

49:21.680 --> 49:27.680
that we might be entering a period of really fast transition. Yeah. I mean, if anything, I'd say

49:27.680 --> 49:34.080
the norm is for the new period to involve much faster changes than the old period. So Hunter

49:34.160 --> 49:38.800
Gathering went on for tens of thousands of years, if not hundreds of thousands of years.

49:39.600 --> 49:44.080
Then we started doing agriculture and forming into big societies and did things like the pyramids.

49:44.800 --> 49:48.960
And then a way that people often think of the next phase transition as being

49:49.600 --> 49:54.880
kind of the start of the industrial revolution and the beginning of kind of concerted efforts

49:54.880 --> 50:01.840
towards making scientific progress. And after we did agriculture, kind of new technologies and

50:01.920 --> 50:07.360
changes were happening on the scale of maybe a thousand years, or maybe a few hundred years,

50:07.360 --> 50:12.160
which is much faster than in the Hunter Gatherer times. And then today, after the industrial

50:12.160 --> 50:17.120
revolution, we're seeing really big changes to society every 50 years. So we've already seen

50:17.680 --> 50:22.000
kind of historically those phase transitions have led to things being faster. So that,

50:22.000 --> 50:26.560
I think, is the default expectation for what a new transition would lead to.

50:27.200 --> 50:34.080
Right. And it just feels weird to us because we're pre-transition. Possibly, whoever's living

50:34.080 --> 50:42.000
50 years from now will just be like, yeah, obviously, that was coming. And those weird

50:42.000 --> 50:48.000
people living in 2023 thinking that they'd made all the technological progress they were ever going

50:48.000 --> 50:54.880
to make. Maybe I'm still struggling just to imagine what it would look like, I guess.

50:55.600 --> 51:04.240
Yeah, because it's possibly going to be us. What's in store for us? Is it going to be good?

51:05.200 --> 51:12.160
I think it could be really good. It could be really, really bad. It could be really good if we

51:13.120 --> 51:17.680
align AI so they're always trying to help us and help humanity do as best as it can

51:17.680 --> 51:24.640
by humanity's own lights. And the kind of benefits from AI and these new technologies are used to

51:24.640 --> 51:31.680
solve the world's most pressing problems and used to lift people out of poverty and give people

51:32.400 --> 51:37.440
the lives that they hope for themselves and for their children, solve the problems of climate

51:37.440 --> 51:45.920
change or poverty of disease. I think it could go really, really well. Right, right. And so in the

51:45.920 --> 51:55.040
best case where AI is really trying to help us, it's still kind of unimaginable to me as a world.

51:55.600 --> 52:03.680
I mean, maybe it's just like I'm so biased by the status quo where like, I need to work and

52:04.400 --> 52:11.120
I need to work to live, I need to work to help solve problems. Like in this best case, is there

52:11.120 --> 52:19.280
unemployment? Is there unemployment for everyone? Is it a slow transition? A fast one? Does it make

52:19.280 --> 52:25.120
inequality better because no one needs to work and we all have enough things? Or does it make it

52:25.120 --> 52:32.160
worse because some people have to work? Do we have predictions about that that are worth making?

52:32.160 --> 52:40.000
I think the default is that inequality would become greater because all of the wealth and

52:40.000 --> 52:44.960
useful work is coming from these AI systems, which I think by default will be controlled by a small

52:44.960 --> 52:53.520
number of people and companies. In the very best case, then you hope that that wealth is equally

52:53.520 --> 52:59.760
distributed or kind of much more equally distributed than it would be by default. And it is true,

52:59.760 --> 53:06.880
I think, that there's going to be so much progress made if AI is aligned that it will be very cheap

53:07.600 --> 53:11.200
to give everyone in the world the standards of living that are enjoyed by the very

53:11.200 --> 53:17.680
richest people today in terms of material comforts and health and actually much, much better on

53:17.680 --> 53:23.280
those fronts, I think after all this technological progress. So I think if we can get the AIs to

53:23.280 --> 53:29.360
be really trying to help us, then even if we mess up on things like the kind of distribution of

53:29.360 --> 53:34.240
benefits, even if we mess up a bit on that, then I think things will still look pretty good because

53:34.240 --> 53:42.560
there's just so much to go around. If there's kind of universal basic income, you could just use

53:43.200 --> 53:49.040
1% of the output that's produced in a year to kind of give everyone kind of all the material

53:49.040 --> 53:54.320
and technological things they need to kind of meet all of their needs, all of the material needs.

53:55.600 --> 54:02.000
In terms of work, I think it will no longer be the case that you can produce a higher quality

54:02.000 --> 54:09.520
service or product than what an AI could do or a robot could do. One thought is that there will be

54:09.520 --> 54:16.880
some humans, maybe me and you, who just value human contact and hang out with other actual

54:16.880 --> 54:22.240
real humans. And that could provide a kind of work for those who want it.

54:22.240 --> 54:24.240
Role for humans. Okay.

54:24.240 --> 54:30.160
And another possibility is that we rethink the nature of work. So we do work to help each other

54:30.960 --> 54:36.160
and even though we know that AIs could do the work just as well, we're still happy to do that

54:36.160 --> 54:41.840
because it gives us a sense of meaning or we kind of do creative things instead like

54:42.960 --> 54:46.960
creative writing and drawing. And even though we know that AIs could do that better than ours,

54:46.960 --> 54:51.840
it's still enough for us to have a sense of purpose. I mean, people still play chess today

54:52.560 --> 54:57.120
and still really enjoy it and get purpose from it, even though they know that they can never

54:57.120 --> 55:03.760
go to match the best AIs. Right. Yeah, I guess I can imagine lots of people listening, hearing

55:03.760 --> 55:13.280
about this future and being like, no, I like the world the way it is. I like that humans get to

55:13.280 --> 55:20.240
make choices for ourselves as a society. I don't want AIs making it for us. I like that I have to

55:20.240 --> 55:26.080
work, get to work. I don't know. I don't, I can imagine people being like, no, I don't want AIs

55:26.080 --> 55:33.600
to be making the art. I want humans to be making the art. So is there some chance that there's,

55:33.600 --> 55:39.440
I don't know, like a movement that's anti AI growth that stops this from happening,

55:39.440 --> 55:46.240
even though it's theoretically possible? That's a great question. I do think it would be good for us

55:46.240 --> 55:52.560
to take this transition more slowly than is theoretically possible. And that might happen

55:52.560 --> 55:59.120
by default if we don't make specific efforts to go slowly. And so I think if people do try and delay

55:59.120 --> 56:03.200
or stop this, it could, it could be a good thing because I don't think we're prepared for that

56:03.200 --> 56:09.520
new world. Right. I think it's going to be very hard to permanently prevent this transition from

56:09.520 --> 56:15.760
happening. How come? One way to think about it is that there is some kind of upfront starting cost

56:15.760 --> 56:22.000
to get this transition going. So let's really simplify and say, today, if you spent a trillion

56:22.000 --> 56:29.280
dollars, you'd be able to train AGI and you'd have enough money left over to buy some manufacturing

56:29.280 --> 56:34.800
equipment for making robots. Right. And then you could have your AIs do research into better

56:34.800 --> 56:40.400
robots and making better AIs. And that whole process could lead to you having even more AIs and even

56:40.400 --> 56:45.120
more robots. And you could then grow your population of AIs and robots. And just with that

56:45.120 --> 56:50.000
trillion dollar initial investment, you could end up with this massive AI and robot population,

56:50.000 --> 56:56.400
which is then able to just start doing the scientific work needed to significantly accelerate

56:56.400 --> 57:01.920
technological progress. Right. Right. So the thing that's difficult is that that upfront cost will

57:01.920 --> 57:09.120
be falling over time. AI algorithms are improving, computer chips are improving. And so the cost to

57:09.120 --> 57:15.200
kind of training AGI and then just using it to build robots and to build more AIs and make money on

57:15.200 --> 57:20.080
the make money in the economy by selling its services and just kind of building up its own

57:20.080 --> 57:25.840
self perpetuating energy that ultimately results in making technological progress so you can sell

57:25.840 --> 57:31.440
more useful things to society that people want. That cost is going to be falling. And so let's

57:31.440 --> 57:35.440
say it was a trillion today, you know, in the future, it's going to be 100 billion and then it's

57:35.440 --> 57:42.400
going to be 10 billion. And there's going to be a lot of incentive to do this because it's going to

57:42.400 --> 57:48.960
grant whoever does it a lot of power. They'll have all these AI workers that they can use to do,

57:48.960 --> 57:51.920
you know, whatever they want them to do if they manage to solve the alignment problem.

57:52.480 --> 57:56.800
If they use it for designing new technologies and those new technologies could grant

57:56.800 --> 58:02.400
additional military power, or they could grant things that people all around the world desperately

58:02.400 --> 58:08.560
want like curing illnesses, like preventing climate change, like understanding and solving

58:08.640 --> 58:16.480
mental health problems, like life extension. So it's not just kind of economic incentives,

58:16.480 --> 58:24.240
it's not just like to get rich, it's like all sorts of motivations are benefited from paying

58:24.240 --> 58:32.800
this cost to get this hugely productive AI scientist workforce. Yeah, kind of whatever you want.

58:33.680 --> 58:38.400
Right. You can probably get it much more effectively if you have a billion AIs and robots

58:38.400 --> 58:43.200
designing technology to help you get it. And I think we can delay it, we can say okay, we're

58:43.200 --> 58:47.120
going to be really cautious, only a few people are allowed to train these systems and we try and

58:47.120 --> 58:51.360
convince the other countries to go slowly as well. But the thing that we could be, you know,

58:52.000 --> 58:56.880
that even 100 years after it's first been possible to train AI for a trillion dollars,

58:56.880 --> 59:01.120
that still no one has done it and no one is using it to make scientific progress,

59:01.120 --> 59:08.080
even though the cost is now like $10 million, it's really hard to imagine that we kind of

59:08.080 --> 59:15.840
prevent anyone from doing it as it gets cheaper. And I think sometimes people, when they're

59:15.840 --> 59:21.280
thinking about it, imagine that in order to get this kind of 10 or 100x faster technological

59:21.280 --> 59:24.800
progress, we'd have to be making a real effort and really kind of being super efficient and

59:24.800 --> 59:28.480
driven about it. But I think that's not the right way to think about it. It's more like

59:29.200 --> 59:34.320
by default, all you need to do is ask your AIs and robots, please do these tasks for me. And if

59:34.320 --> 59:38.320
you need to make tech progress along the way, do it, they will suggest the plans and involve

59:38.320 --> 59:41.920
making tech progress. They will get in contact with the labs and organize for the experiments to

59:41.920 --> 59:46.880
happen. You won't have to do anything. Right. And so I don't think it's going to require some kind of

59:46.880 --> 59:53.840
concerted pro growth enthusiasts to like really push for this. It's more like, you want stuff,

59:53.840 --> 59:59.200
the AI is going to try and do the stuff you want. And whenever they make tech progress,

59:59.200 --> 01:00:03.360
it's going to go really well. And it's going to really help you solve your problems.

01:00:03.360 --> 01:00:04.800
And you're going to just want to do more of it.

01:00:05.360 --> 01:00:10.800
Yeah, just enough time will pass, enough actors will think on it and decide to do it at some

01:00:10.800 --> 01:00:18.240
point. Yeah. So I guess I buy that the incentives are there for eventually an actor to want to build

01:00:18.240 --> 01:00:26.560
this kind of AI scientist workforce. It still seems like there have been enormously lucrative

01:00:26.560 --> 01:00:33.920
and beneficial technologies that we haven't pursued. So one example that comes to mind

01:00:33.920 --> 01:00:40.640
is like nuclear power, which like could help loads with climate change and would also be,

01:00:40.640 --> 01:00:45.600
yeah, again, super lucrative. And yet, yeah, we basically haven't done anything like what we

01:00:45.600 --> 01:00:52.160
could do with it. Could there be something similar? I mean, it's just kind of stigmatized,

01:00:52.160 --> 01:00:57.520
is like one reason we haven't. And I guess it's really expensive in particular the upfront costs,

01:00:57.520 --> 01:01:03.120
which like maybe just ends up true of this like AI world we're talking about.

01:01:04.240 --> 01:01:07.440
Yeah, it's a great example. I don't have a good understanding of what happened,

01:01:07.440 --> 01:01:13.840
but I think there are some big catastrophes with nuclear power, and then it became very

01:01:13.840 --> 01:01:19.360
stigmatized. And the regulatory requirements around it and the safety requirements became

01:01:20.000 --> 01:01:27.360
very large, much larger really than was reasonable, given that fossil fuel energy has damaging health

01:01:27.360 --> 01:01:34.320
consequences as well through air pollution. And as a result, it just became kind of a mixture

01:01:34.320 --> 01:01:39.440
of stigma and just the additional cost from all the regulation just prevented it from being rolled

01:01:39.440 --> 01:01:46.560
out. But I do think there are a fair few very significant disanalogies between that case and

01:01:47.440 --> 01:01:55.440
the case of AI. Okay, yeah, what are they? So one thing is that there were other sources of

01:01:55.440 --> 01:02:00.640
energy that were available. And so it wasn't too costly to be like, well, we're not going to use

01:02:00.640 --> 01:02:05.600
nuclear, we're going to use fossil fuels instead. And then, you know, even the green climate change

01:02:05.600 --> 01:02:10.080
concern, people could think about kind of developing solar panels and renewable energies.

01:02:11.040 --> 01:02:16.480
And in the AI case, that there is going to be no alternative. There's going to be no alternative

01:02:16.480 --> 01:02:24.480
technology which can solve all illness, and which can grant your nation massive national

01:02:24.480 --> 01:02:28.880
security and military power, and that can solve climate change. This is going to be the only

01:02:28.880 --> 01:02:35.280
option. So that's one disanalogy. Okay, that makes sense. Another kind of disanalogy is the

01:02:35.280 --> 01:02:41.840
cost factor. So with nuclear power, it's become more expensive at a time due to regulations. And

01:02:41.840 --> 01:02:47.600
that that's been a big factor in not being pursued. But the specifics around these cost

01:02:47.600 --> 01:02:54.880
curves with compute and this algorithmic progress patterns suggests that the upfront cost of training

01:02:54.880 --> 01:03:01.200
AGI is going to be falling really pretty quickly over time. Right. And so even if initially you

01:03:01.200 --> 01:03:05.360
put loads of regulations, which make it very expensive, it's really not going to be long until

01:03:05.360 --> 01:03:10.720
it's 10x cheaper. Right. And so permanently preventing it when it's when it's becoming cheaper

01:03:10.720 --> 01:03:16.160
and cheaper at such a high rate, it's going to be really, really difficult. Third is just

01:03:16.880 --> 01:03:23.280
just talking about the size of the gains from from this technology compared to nuclear power.

01:03:23.280 --> 01:03:27.840
So, you know, France adopted nuclear power and it was somewhat beneficial, you know, it's kind of

01:03:27.840 --> 01:03:32.160
now gets a lot of its powerful nuclear energy and that there's no climate change impacts and that's

01:03:32.160 --> 01:03:40.080
great. But it's not as if France is visibly and undisputably just doing amazing well as a country

01:03:40.080 --> 01:03:44.160
because it's got, you know, this nuclear power, like it's a kind of a modest addition, maybe it

01:03:44.160 --> 01:03:49.760
makes it look a little bit better. Right. But by contrast, if one country is is, you know,

01:03:49.760 --> 01:03:53.440
progressing technology at the normal rate, and then another country comes along

01:03:54.080 --> 01:04:00.240
and starts using these AIs and robots a little bit, you're going to see very significant differences

01:04:00.240 --> 01:04:06.080
in how its overall technology and prosperity and kind of military power is progressing.

01:04:06.720 --> 01:04:11.200
And then you're going to see that as countries dial up how much they're allowing AIs to do this

01:04:11.200 --> 01:04:16.000
work, that there are then bigger and bigger differences there. And ultimately, the difference

01:04:16.000 --> 01:04:23.360
between advancing technology at our pace versus advancing technology 30 times faster is over the

01:04:23.360 --> 01:04:28.880
course of just a few years, it becomes a massive difference in the sophistication of your country's

01:04:28.880 --> 01:04:34.800
technology and ability to solve all kinds of social and political problems. You know, a last point

01:04:35.600 --> 01:04:42.560
on this difference is that, you know, the US did in fact invest a lot of money in nukes shortly

01:04:42.560 --> 01:04:46.880
after the development of fish and power. You know, when it came to a matter of national power,

01:04:46.880 --> 01:04:52.480
they were very happy to invest in the technology, despite, you know, the risks which were clearly

01:04:52.480 --> 01:04:57.920
very high. All of the same risks. Right. Yes, you know, the incentives were out of whack and we

01:04:57.920 --> 01:05:03.440
didn't get nuclear fission power. But when it came to this kind of military technology for which

01:05:03.440 --> 01:05:08.560
there was no replacement, countries were very keen to do it and they made it happen. And AI

01:05:08.560 --> 01:05:13.600
driving significant technological improvements across the board is going to be a huge source

01:05:13.600 --> 01:05:19.520
of military power. Right. And so it's really hard for me to imagine that just no one ever uses it

01:05:19.520 --> 01:05:26.240
for that. And you've totally preempted my next question, which was like, can we definitely not

01:05:26.240 --> 01:05:33.600
come up with like an international treaty that is like the downside risks of this technology

01:05:33.600 --> 01:05:41.440
at this scale are possibly huge because alignment is so hard. And so we're all agreeing

01:05:41.440 --> 01:05:47.840
not to not to go forward with it. And I guess we had lots of reason to do that in the case of

01:05:47.840 --> 01:05:52.160
nuclear weapons. And we didn't. And we have lots of reason to do that in the case of biological

01:05:52.160 --> 01:05:57.760
weapons. And and we suck at it. We do not live in a world free of biological weapons or nuclear

01:05:57.760 --> 01:06:03.440
weapons. So I do think we should try. And I do think we can slow things down. And we

01:06:03.440 --> 01:06:09.200
can, you know, increase the requirements and the safety efforts required, maybe to make it 10

01:06:09.200 --> 01:06:15.520
times as costly or 100 times as costly to develop this technology. I think that is that is one thing.

01:06:15.520 --> 01:06:20.480
And that's a big ask. And I think we should try and do as much of it as we can. Even if we can do

01:06:20.480 --> 01:06:26.320
that, it's a whole different thing to talk about permanently choosing to never develop the technology,

01:06:26.320 --> 01:06:32.240
even after we've made maximal efforts into making it safe, even after all the safety tests are saying

01:06:32.240 --> 01:06:37.360
it looks like it is safe, even when millions of people are dying every year from illnesses which

01:06:37.360 --> 01:06:44.880
we know could be prevented if we allowed the AIs to do research into treating it. I think just

01:06:44.880 --> 01:06:50.720
permanently not going forward with using AIs and robots to make that technological progress.

01:06:51.760 --> 01:06:55.920
Like I said, when it's becoming cheaper and cheaper and other countries and other companies,

01:06:55.920 --> 01:07:00.400
you know, might want to do it, that does seem like it's just very unrealistic.

01:07:00.480 --> 01:07:01.440
Just implausible.

01:07:01.440 --> 01:07:05.360
And maybe not desirable either, to be honest. Like after a certain point,

01:07:05.360 --> 01:07:07.680
like we should take it really cautious.

01:07:07.680 --> 01:07:14.880
Right, after 200 years of like research into air alignment, even if we're like,

01:07:14.880 --> 01:07:21.440
ooh, this seems weird and scary and might change the world as we know it, at some point there are

01:07:21.440 --> 01:07:29.840
going to be incentives for some actors, countries or companies to try to deploy this technology

01:07:29.840 --> 01:07:39.120
at scale to solve problems like poverty and illness, maybe also to get military advantages,

01:07:39.120 --> 01:07:45.760
to solve problems like climate change. And those incentives might just be so strong that

01:07:45.760 --> 01:07:54.080
even if we take our time, we'll probably eventually do it. Someone will. And once that happens,

01:07:54.080 --> 01:08:02.000
progress will become so quick that we're looking at really economic growth at a pace that's,

01:08:02.000 --> 01:08:06.560
I guess, still kind of unfathomable to me. But that is this kind of thing where

01:08:06.560 --> 01:08:10.000
progress we've seen over the last 100 years happens in the next 10,

01:08:11.040 --> 01:08:15.280
and actually just keeps getting faster and faster and faster. Is that kind of the picture?

01:08:15.840 --> 01:08:16.880
Yeah, that's the picture.

01:08:17.520 --> 01:08:26.720
That's really weird. It's scary. I guess it's also quite hopeful. I mean, if I let myself hope for

01:08:26.720 --> 01:08:35.840
that good world where we use it to solve problems, I feel nervously really, really excited. But I guess

01:08:35.840 --> 01:08:43.600
we've got some real, real challenges to overcome first. Okay, let's move on to a related topic

01:08:43.600 --> 01:08:49.200
you've been researching more recently. So you've just written the draft of a report on

01:08:49.200 --> 01:08:54.720
AI takeoff speeds that has some pretty alarming results to me, given everything we've just talked

01:08:54.720 --> 01:09:00.160
about. And I guess just to get on the same page about language, what exactly do you mean when

01:09:00.160 --> 01:09:08.240
you talk about AI takeoff speeds? Roughly speaking, capabilities takeoff speed is the question of how

01:09:08.240 --> 01:09:15.440
quickly AI systems will improve as they approach and surpass human level intelligence.

01:09:16.160 --> 01:09:21.840
And so the capabilities are like their ability to do things like drive cars or

01:09:23.120 --> 01:09:30.400
program new programs. Exactly. So a fast takeoff speed could be that in a, you know,

01:09:30.400 --> 01:09:38.720
in three months, AIs go from mouse level intelligence to significantly more intelligent

01:09:38.720 --> 01:09:44.400
than the smartest human, right? Where a slow takeoff speed that could be happening over decades.

01:09:45.360 --> 01:09:51.520
Got it. Okay. So we're just like slowly making progress next year at self driving cars. And

01:09:51.520 --> 01:09:55.440
then it takes another 10 years to get to programs that can write other programs.

01:09:56.400 --> 01:10:01.920
So I guess, you know, you've written this report, and I won't give away the results yet,

01:10:01.920 --> 01:10:06.880
but it gives some evidence about how fast we might expect AI takeoff speeds to be.

01:10:06.880 --> 01:10:11.200
But before we get to that, before you wrote the report, what did you think was the most

01:10:11.200 --> 01:10:14.880
compelling evidence that AI takeoff speeds would be particularly fast?

01:10:16.080 --> 01:10:23.760
So I think the most convincing argument is related to how humanity's capabilities were

01:10:23.840 --> 01:10:29.360
improving much more slowly 50,000 years ago, then they are improving today.

01:10:30.400 --> 01:10:35.280
And the attempt to draw an analogy to what might happen with AI capabilities.

01:10:36.320 --> 01:10:44.400
So if we think that a million years ago, humanity's kind of cognitive abilities collectively were

01:10:45.280 --> 01:10:51.680
maybe doubling every, let's say 100,000 years, I don't know. That's just kind of something to

01:10:51.680 --> 01:10:57.200
represent their kind of slow increase in brain size and capacities. The exact number doesn't

01:10:57.200 --> 01:11:02.560
matter. You know, whether you want to say it's, you know, 100,000 or 10,000 years is a very slow

01:11:02.560 --> 01:11:08.080
doubling size in their abilities. Right. And that's basically because like slowly,

01:11:08.080 --> 01:11:12.800
they're evolving to have slightly bigger brains, we're like adding a bit of prefrontal cortex,

01:11:12.800 --> 01:11:18.240
and like the population is getting a little bit bigger over time, but grew very, very slowly.

01:11:18.240 --> 01:11:21.120
So collectively, it's just, it takes thousands of years to double.

01:11:21.680 --> 01:11:22.480
Yeah, exactly.

01:11:22.480 --> 01:11:23.440
Cool. Cool. Okay.

01:11:24.080 --> 01:11:31.760
Whereas today, our abilities are improving incredibly quickly. As a society, our population

01:11:31.760 --> 01:11:38.320
growth is much faster. Our command of technology is much faster. And, you know, in the last,

01:11:38.320 --> 01:11:44.880
like I said, in the last 200 years alone, we've doubled the economy many, many times and doubled

01:11:44.880 --> 01:11:50.960
our ability to understand and manipulate the world very many times. And if you think that

01:11:51.760 --> 01:11:59.840
there'll be some analog of that transition as we approach AGI, then, I mean, AI is already

01:11:59.840 --> 01:12:04.960
improving really quickly. You know, I would say it's kind of doubling its abilities in less than

01:12:04.960 --> 01:12:09.600
a year at the moment. And so if, you know, if that's this kind of the slow initial pace,

01:12:10.240 --> 01:12:15.680
then, you know, the new pace would be kind of blisteringly quick.

01:12:15.680 --> 01:12:17.440
Right, right. Unimaginably quick.

01:12:17.440 --> 01:12:25.280
Yeah, exactly. The way I would think about it is that a million years ago, humans weren't able to do

01:12:26.400 --> 01:12:32.880
science and to discover technological improvements much at all. And so they didn't have access to

01:12:32.880 --> 01:12:37.520
this additional feedback loop of improvement. Where you discover new technology passed on to

01:12:37.520 --> 01:12:42.400
the next generation, then they start on a better place, can discover even more technology, you

01:12:42.400 --> 01:12:46.000
can now support a bigger population. There's this whole feedback loop, which arguably we

01:12:46.000 --> 01:12:50.320
couldn't access a million years ago. Then we improved our cognitive abilities as, you know,

01:12:50.320 --> 01:12:54.320
as humans a little bit. And then suddenly we got over this threshold where, okay, we can access

01:12:54.320 --> 01:12:59.920
this, this kind of doing technological progress feedback loop, which then, you know, then speeds

01:12:59.920 --> 01:13:04.880
up and speeds up as we develop agriculture and then we develop text, you know, written language

01:13:04.880 --> 01:13:08.880
and we develop maths. And it's kind of, we're now even better at discovering new technologies.

01:13:09.520 --> 01:13:14.640
And the thought is in my mind that maybe there's something similar that happens with AIs, whereas

01:13:15.440 --> 01:13:20.960
today they're not, you know, they're not that smart. And so they're not able to access a certain

01:13:20.960 --> 01:13:25.680
feedback loop. I'm not sure exactly what that feedback loop will be. But at some point, they

01:13:25.680 --> 01:13:30.960
become smart enough that there's this additional feedback loop that they can use to improve their

01:13:30.960 --> 01:13:35.360
capabilities, kind of like how humans use technology to improve our capabilities.

01:13:35.360 --> 01:13:39.200
You know, one, one funny thing about this argument is that it's not clear what that new

01:13:39.200 --> 01:13:46.480
feedback loop might be for AIs. And so that leaves me a little bit puzzled over where to go with

01:13:46.480 --> 01:13:52.960
this argument. So for humans, it's, you know, we discover the scientific method and we experiment

01:13:52.960 --> 01:14:00.000
on things and we built computers. And now we can like run programs that help us do science.

01:14:00.000 --> 01:14:05.520
But for AI, it's like, we don't even know what kinds of science they'll discover that's like

01:14:05.520 --> 01:14:11.600
beyond ours. And is there some question about whether there even are kind of higher orders of

01:14:11.600 --> 01:14:17.040
science that we can't, that we haven't developed, but that AI systems might to kind of increase

01:14:17.040 --> 01:14:21.600
their own feedback loop? Yeah, I think that's, I think that's right. And you know, you can,

01:14:21.600 --> 01:14:26.080
you can throw out ideas for what that might look like. Right. Maybe it's the AIs

01:14:26.800 --> 01:14:32.640
learn to work together in a team in a way that is way more efficient than what humans have ever

01:14:32.640 --> 01:14:39.760
done. Yeah. Or maybe they run simulations or something to like learn about economics or

01:14:40.640 --> 01:14:45.760
something in a way that we can, we can barely understand because we only see the economies run

01:14:45.760 --> 01:14:53.200
in these weird real, real world scenarios. For example, for example, cool. Okay. Yeah. Are there,

01:14:53.200 --> 01:14:56.880
are there any limitations to that argument? Or do you do just kind of buy it?

01:14:57.840 --> 01:15:01.760
Yeah, I'm, I actually don't put that much weight on, on this particular argument.

01:15:01.760 --> 01:15:10.880
Oh, interesting. So the main reason is that evolution was not trying to make humanity as a

01:15:10.880 --> 01:15:17.040
whole as capable as possible. And it wasn't trying to make humanity as a whole good at science.

01:15:17.120 --> 01:15:24.240
Right. So from that perspective, it's not actually as surprising that humanity went from

01:15:24.240 --> 01:15:30.160
really sucking at science to being really good at science in a fairly short, short timeframe.

01:15:30.960 --> 01:15:38.800
Yeah. So here's an analogy. Before 2020, we hadn't made many COVID vaccines, not because we couldn't,

01:15:38.800 --> 01:15:43.040
but because we weren't trying to, we were focused as a society on doing other things.

01:15:43.680 --> 01:15:49.360
Then around 2020, it became really useful for us to make lots of vaccines. And then,

01:15:49.360 --> 01:15:54.720
lo and behold, the number of vaccines went up very dramatically. Now, that doesn't mean that our

01:15:54.720 --> 01:16:00.240
kind of abilities in vaccine making suddenly went up. It just means that we reprioritized,

01:16:00.240 --> 01:16:06.000
reallocated the resources we already had towards making vaccines. Okay. And I want to say that

01:16:06.000 --> 01:16:11.760
that's somewhat similar to the way in which kind of our ancestors a million years ago,

01:16:11.760 --> 01:16:18.240
we weren't that good at science, but evolution wasn't trying to make us that good at science.

01:16:18.240 --> 01:16:23.440
It was mostly trying to make us hunt successfully, feed our families. Okay.

01:16:23.440 --> 01:16:28.320
Science was like maybe a tiny, tiny bit useful back then, because it maybe allowed you to discover

01:16:28.320 --> 01:16:34.320
something with your own lifetime, but it really wasn't very useful. Then I think more recently,

01:16:34.320 --> 01:16:40.160
maybe more like kind of tens or a hundred thousand years ago, it did become more useful for humans

01:16:40.160 --> 01:16:46.880
to do science and to be flexible learners. And so it's not that surprising that at that point,

01:16:47.760 --> 01:16:53.200
say 50,000 years ago, where it was more useful for humans to be good at science,

01:16:53.200 --> 01:17:00.560
evolution then reallocated those cognitive resources of humans to being good at science.

01:17:00.560 --> 01:17:01.440
Right.

01:17:01.440 --> 01:17:06.240
So that kind of reallocation by evolution from kind of just for foraging and then reallocating

01:17:06.320 --> 01:17:10.720
those cognitive resources to doing science is kind of like human society, reallocating its

01:17:11.600 --> 01:17:13.840
resources to make COVID vaccines.

01:17:13.840 --> 01:17:20.960
Okay. Yeah, that's really, really helpful. So something like a combination maybe of language,

01:17:20.960 --> 01:17:28.480
maybe of just like group living, maybe some things that I don't understand, made it much more

01:17:28.480 --> 01:17:35.040
beneficial to be able to learn new things and learn like a range of things, not just the same

01:17:35.040 --> 01:17:40.720
things over and over again. And so a couple of tweaks in the brain was enough to make the

01:17:40.720 --> 01:17:46.880
brain that we'd been using for very specific set of tasks become useful for just like a much

01:17:46.880 --> 01:17:53.520
wider range of tasks. And that wasn't like really fundamentally altering like the amount of brain

01:17:53.520 --> 01:18:02.320
we have, but like how we use it. And that capacity already existed. And like you said, was repurposed.

01:18:03.040 --> 01:18:09.680
So yeah, here's how you would relate it to AI take off. You'd say that in the case of evolution,

01:18:09.680 --> 01:18:16.560
evolution wasn't initially trying to make humans good at science. And so no massive surprise that

01:18:16.560 --> 01:18:21.200
it's able to quickly make some tweaks that make humans good at science late in the day.

01:18:22.240 --> 01:18:28.880
But with AIs and with AI development, humans will be at every stage trying to make AIs as useful

01:18:28.880 --> 01:18:37.200
as possible for doing economic tasks, helping with science and research. And so we wouldn't

01:18:37.200 --> 01:18:43.440
expect there to be this kind of overhang where the AI has these abilities, which it's just not

01:18:43.440 --> 01:18:48.080
using, because we would expect humans to be trying to coax those abilities out at every step of the

01:18:48.080 --> 01:18:56.320
way. And so if if you're constantly trying to coax abilities out, most likely you'll only find

01:18:56.320 --> 01:19:01.120
ways to do it incrementally, as opposed to like, if it happened to be the case that like,

01:19:01.920 --> 01:19:09.280
I don't know, we found a billion computer chips on another planet, and could just

01:19:10.080 --> 01:19:17.200
use them to train up a bunch of AI systems, then we'd expect the step change. But like,

01:19:17.200 --> 01:19:21.440
currently, everything is just increasing incrementally, we're increasing chips incrementally,

01:19:21.440 --> 01:19:27.520
we're increasing algorithmic progress incrementally. And so it's just going to keep improving at a

01:19:27.520 --> 01:19:33.200
kind of incremental pace. And I think crucially, we have to think that we're currently

01:19:34.080 --> 01:19:39.360
at each step of the way trying to use the most recent algorithms and most recent compute to

01:19:39.360 --> 01:19:45.360
actually get AIs to do, let's say, useful science research. You know, if we're incrementally increasing

01:19:45.360 --> 01:19:49.920
the computing algorithms, but we're not actually trying to get the AIs to do useful science research,

01:19:49.920 --> 01:19:54.240
then it could be that one day we decide to try and get AIs to do useful science research,

01:19:54.240 --> 01:19:58.640
and then suddenly, we train them to reassign all their cognitive abilities to that task,

01:19:58.640 --> 01:20:03.280
and we do get something that's really quick. So it's a really important assumption here that

01:20:03.920 --> 01:20:08.800
in some sense, the AI development ecosystem is kind of efficient.

01:20:08.800 --> 01:20:16.480
Yep. And it's using, yeah, it's using new AI capabilities to do like the cutting edge research,

01:20:16.480 --> 01:20:23.120
as opposed to like, if there were only market incentives to make AI that, I don't know, made

01:20:23.120 --> 01:20:30.880
these beautiful images like Dolly, and no incentives at all to use AI to improve AI systems, then

01:20:31.760 --> 01:20:37.520
if one day we made a few tweaks to Dolly, and we're like, stop making pictures, make programs

01:20:37.520 --> 01:20:43.120
instead. And but like actually, like it did have capabilities related enough that we could

01:20:43.120 --> 01:20:48.960
make that tweak semi easily. Then all of a sudden, we'd have this system that could write programs

01:20:48.960 --> 01:20:55.520
really efficiently that we'd never had before. Right. Or maybe a more probable scenario would be

01:20:55.520 --> 01:21:01.040
we're just using all our AI resources to train these image generating systems like Dolly. And

01:21:01.040 --> 01:21:07.040
then we're like, you know what, why don't we just try using all those resources to train a science AI?

01:21:07.040 --> 01:21:12.080
And then we, you know, we pick the architecture to specialize of a science, we use the data to

01:21:12.080 --> 01:21:16.400
specialize it for science, we use all the compute that we were previously pouring into these image

01:21:16.400 --> 01:21:21.520
generation systems. And then suddenly we're like, wow, our science AI is amazing. And it came out of

01:21:21.520 --> 01:21:26.960
nowhere because we hadn't been trying in the previous years to do this at all. Yeah. So are we

01:21:26.960 --> 01:21:33.600
currently trying to make AI systems that are really good at science? I think it's a good question.

01:21:34.640 --> 01:21:41.360
The market doesn't seem to me to be super efficient. I've been playing our GPT for a bit

01:21:41.360 --> 01:21:47.200
recently. And to me, it looks like GPT-4 is pretty smart. It doesn't seem to me like

01:21:47.200 --> 01:21:53.680
its cognitive abilities have been really direct in the direction of helping to advance science,

01:21:53.680 --> 01:22:00.640
to be honest. So I do think that this argument could ultimately say, yeah,

01:22:02.000 --> 01:22:08.240
a faster takeoff is plausible and the mechanism could be reallocating the AI's cognitive abilities

01:22:08.960 --> 01:22:13.360
towards science. I mean, GPT-4 is just trained to predict the next world on the internet.

01:22:13.360 --> 01:22:18.960
Right. That's a very different kind of task than the task of advancing science. And so

01:22:19.680 --> 01:22:25.280
I think that that is a reason to expect a faster takeoff. More of a jump. Yeah. Okay,

01:22:25.280 --> 01:22:30.320
interesting. I haven't heard that argument before. Cool. Well, I want to now get to

01:22:30.320 --> 01:22:36.800
the report that you've written on AI takeoff speeds, which asks how quickly AI might go from

01:22:37.360 --> 01:22:43.360
kind of pretty economically valuable to just extremely capable, maybe as good as humans.

01:22:44.160 --> 01:22:49.840
And yeah, I guess you define your terms pretty clearly in your report. So maybe we should start

01:22:49.840 --> 01:22:56.240
by doing that. Am I right in remembering that you are trying to answer the question of how

01:22:56.240 --> 01:23:04.160
quickly we'll go from AI systems that can do 20% of human tasks to AI systems that can do 100%

01:23:04.720 --> 01:23:10.640
of human tasks? Is that right? Yeah, that's right. In particular, I am restricting to

01:23:10.640 --> 01:23:16.320
cognitive tasks. That's similar to what we discussed earlier. It's any task that you could do

01:23:16.960 --> 01:23:20.160
remotely that doesn't require you to be physically manipulating

01:23:20.880 --> 01:23:25.360
objects yourself because AI's don't have physical bodies. That wouldn't be included for them.

01:23:26.000 --> 01:23:31.680
I mean, it does include tasks like giving instructions to a human who's doing a physical

01:23:31.680 --> 01:23:37.520
job telling them where to move things, what to do with their arms, or potentially giving

01:23:37.520 --> 01:23:42.640
instructions to robots that are doing physical tasks. But it doesn't include the kind of the

01:23:42.640 --> 01:23:50.800
physical motions themselves. So does it include things like driving cars? Yes, it does. Okay,

01:23:50.800 --> 01:24:01.280
it does. And that's because driving cars is really a set of algorithms. And you can turn the wheel of

01:24:01.680 --> 01:24:09.840
car. So it's a good point that currently the way humans drive cars is by physically moving

01:24:09.840 --> 01:24:18.560
various levers in the car. But I think actually giving the AI the control of the steering wheel

01:24:18.560 --> 01:24:22.560
and of the pedals and the brakes is actually pretty trivial. So the only thing that's hard

01:24:22.560 --> 01:24:29.280
in practice about getting AI's to do driving is the cognitive parts of what should the car be

01:24:29.280 --> 01:24:37.520
doing at each point. Yeah, right. Okay, so then an example of a task that isn't included is something

01:24:37.520 --> 01:24:43.600
like helping people move house. It's like carrying the boxes in and out. Carrying the boxes in and

01:24:43.600 --> 01:24:49.440
out would not be included. Yeah, okay. But telling them here's the best plan for moving, here's the

01:24:49.440 --> 01:24:54.480
order you should move the boxes in that would be included. Okay, so that's basically what you've done.

01:24:55.200 --> 01:25:01.200
You want to know how fast do we go from 20% of cognitive tasks to 100% of cognitive tasks?

01:25:01.840 --> 01:25:06.880
Yeah, can you actually clarify what it means for AI to be able to complete 20% of tasks?

01:25:07.520 --> 01:25:12.400
So you could say, okay, let's say we can AI automate driving, what percentage is that? Is that

01:25:12.400 --> 01:25:19.600
3%? Because it's, you know, 3% of people do it? Or is it, you know, do we just like give, we look at

01:25:19.600 --> 01:25:24.960
a long list of tasks and assume that it takes up an equal percentage? Like, what do we mean by 20%?

01:25:24.960 --> 01:25:31.280
Yes. And so the way I'm currently thinking about that is you look at how much people pay for those

01:25:31.280 --> 01:25:35.680
tasks to be performed in the economy. Okay. So let's take the driving example. I don't know. Let's

01:25:35.680 --> 01:25:42.880
say that drivers around the world are being paid $2 trillion a year for the work they're doing,

01:25:42.880 --> 01:25:51.440
driving trucks and taxis and everything else. In that case, because $2 trillion is 2% of the

01:25:51.440 --> 01:25:58.800
global GDP, I would say that automating driving, fully automating all driving would be automating

01:25:59.520 --> 01:26:08.160
2% of all economic tasks. Got it. And then you're saying, how fast will we go from we can do 20%?

01:26:08.160 --> 01:26:15.040
So I don't know, maybe it's like replacing all drivers, maybe it's replacing all journalism

01:26:15.040 --> 01:26:21.360
because GPT-4 seems to be really good at writing. And I don't know, a couple of other, a couple of

01:26:21.360 --> 01:26:28.880
other things. How fast do you go from like that chunk to literally all cognitive tasks,

01:26:28.880 --> 01:26:34.720
including, I guess, science, AI, R&D. Now, one complication is that from year to year,

01:26:35.440 --> 01:26:42.160
the amount that is paid to people to perform each task might change. So in 2020, maybe drivers

01:26:42.160 --> 01:26:48.000
are paid $3 trillion a year for their work, maybe in 2025, they're paid $4 trillion a year,

01:26:48.000 --> 01:26:53.600
and that could change. So I'm pegging these percentages to the year 2020. Got it. It's a kind

01:26:53.600 --> 01:26:58.880
of arbitrary choice just to make the definition unambiguous. Yes. Okay. That makes sense. So what

01:26:58.880 --> 01:27:05.920
were they paid in 2020? And even if, I don't know, like wages change for all sorts of reasons,

01:27:05.920 --> 01:27:13.280
including AI taking over some jobs, we're still just thinking of what percentage of the 2020

01:27:13.280 --> 01:27:19.440
cognitive economy is being automated. Exactly. And it is really important to keep that in mind,

01:27:19.440 --> 01:27:25.760
because typically when AI automates a certain task, it becomes really cheap to do that task.

01:27:25.760 --> 01:27:29.520
Right. So it becomes a much smaller fraction of the economy. Exactly. Yeah.

01:27:29.520 --> 01:27:33.520
And so you can end up thinking that AI is never doing anything when it's actually

01:27:33.520 --> 01:27:38.000
done almost everything. And so that is just an important thing to be aware of.

01:27:38.000 --> 01:27:42.640
Yeah. That makes total sense. Is there an analogy from the Industrial Revolution or something?

01:27:43.440 --> 01:27:49.760
The best analogy might be agriculture. So I think in 1500, basically everyone worked in

01:27:49.760 --> 01:27:54.880
agriculture. Right. It was 90% of the economy or something. Exactly. All of GDP would have

01:27:54.880 --> 01:28:00.000
basically been agriculture. Today, I think it's less than 5%. And that's because we've become

01:28:00.000 --> 01:28:08.160
really good at producing food with very little need for human labor. So it's not to say that

01:28:08.160 --> 01:28:16.960
fertilizer and trucks and really highly productive seeds aren't contributing a bunch to the economy,

01:28:16.960 --> 01:28:22.960
but clearly they have. But were you to measure it as a fraction of the GDP that they're

01:28:22.960 --> 01:28:27.040
responsible for? It would be smaller because everything's just gotten so much cheaper because

01:28:27.040 --> 01:28:33.600
of them. Exactly. Cool. Okay. That makes a bunch of sense. Nice. Okay. So we've got definitions.

01:28:34.320 --> 01:28:43.760
So you've asked how fast will we go from AI systems that can do roughly 20% of the cognitive

01:28:43.760 --> 01:28:50.960
tasks that humans were doing as of 2020? And how quickly will we go from 20% of those tasks

01:28:50.960 --> 01:28:57.840
to 100% of those tasks being at least in theory able to be automated by AI systems?

01:28:58.720 --> 01:29:05.440
So yeah, what was your result? So the conclusion from the report is, I guess, pretty scary.

01:29:06.320 --> 01:29:13.680
The bottom line is that my median guess is that it would take just a small number of years to go

01:29:13.680 --> 01:29:20.880
from that 20% to the 100%. So I think equally likely to happen in less than three years.

01:29:21.280 --> 01:29:29.920
As it is to happen in more than three years. So a pretty abrupt and quick change is the kind of

01:29:29.920 --> 01:29:38.560
median kind of best guess median. Wow. And do you believe that in your bones? Does that feel like

01:29:38.560 --> 01:29:45.920
like very plausible to you? Yeah, I do. So some some quick things about why why it's plausible.

01:29:46.800 --> 01:29:53.520
Each year, once you take algorithms, better algorithms and using more compute into account,

01:29:53.520 --> 01:29:58.960
we're currently training AIs each year that have kind of three times bigger brains than the year

01:29:58.960 --> 01:30:05.280
before. So really rough way to think about it. But you know, imagine three times smaller brain

01:30:05.280 --> 01:30:12.960
than humans. That's chimpanzee brain size. Right. Each year, you're going from chimpanzees to humans?

01:30:13.600 --> 01:30:19.440
That's, I think, you know, it's really hard to try and account for the effect of the algorithmic

01:30:19.440 --> 01:30:26.720
improvements. But on my kind of best guess of what those amount to, yeah, each year, we're

01:30:26.720 --> 01:30:32.720
making the brains of AI systems about three times bigger. Wow. And right now, it's humans that are

01:30:32.720 --> 01:30:38.800
doing all the work to improve those AI systems. As we get close to AIs that match humans, we'll be

01:30:38.800 --> 01:30:47.600
increasingly using AI systems to improve AI algorithms, design better AI chips. And so overall,

01:30:47.600 --> 01:30:55.360
I expect that pace to accelerate absent a specific effort to slow down. Right. So rather than three

01:30:55.360 --> 01:31:00.080
times bigger brains each year, it's going to be going faster and faster five times bigger brain

01:31:00.080 --> 01:31:07.040
each year, 10 times bigger brain each year. And I think that that just already makes it plausible

01:31:07.040 --> 01:31:11.760
that there could be just a small number of years where this transition happens where AIs go from

01:31:11.760 --> 01:31:18.720
much worse than humans to much better. But to add in another factor, I think that it's likely that

01:31:18.720 --> 01:31:25.040
AIs are going to be automating AI research itself before they're automating things in most of the

01:31:25.040 --> 01:31:32.480
economy. Right. Because that's the kind of the task in the workflow that AI researchers themselves

01:31:32.480 --> 01:31:38.400
really understand. So they would be kind of best placed to use AI as effectively there.

01:31:38.960 --> 01:31:43.360
There aren't going to be kind of delays to rolling it out or trouble finding the customers

01:31:43.920 --> 01:31:50.960
for that in the same way. The task of AI research is quite similar to what language models are

01:31:50.960 --> 01:31:55.840
currently trained to do. They're currently trained to predict the next token on the internet,

01:31:55.840 --> 01:32:00.800
which means they're particularly well suited to tech space tasks. Right. And the task of writing code

01:32:01.440 --> 01:32:06.400
is one such task and there is lots of data on examples of code writing.

01:32:06.400 --> 01:32:11.920
Oh, I see. So it's like typically, I don't know that much about coding. Is it basically also

01:32:11.920 --> 01:32:19.200
token prediction? That is how current coding assistants work, I think, is that they're looking

01:32:19.200 --> 01:32:23.520
at your kind of, you start writing your code and they predict what's going to follow. Like one way

01:32:23.520 --> 01:32:29.200
of putting it would be by the time that the AIs can do 20% of cognitive tasks in the broader economy,

01:32:29.760 --> 01:32:37.120
maybe they can already do 40% or 50% of tasks specifically in AI R&D. Right. And so

01:32:37.760 --> 01:32:40.960
they could have already really started accelerating the pace of progress

01:32:41.760 --> 01:32:48.400
by the time we get to that 20% economic impact threshold. I mean, at that point,

01:32:48.400 --> 01:32:53.040
you could easily imagine really, it's just one year, you give them a 10x bigger brain,

01:32:53.040 --> 01:32:58.160
that's like going from chimps to humans and then doing that jump again. That could easily be enough

01:32:58.240 --> 01:33:05.920
to go from 20% to 100% just intuitively. And I think that's kind of the default really.

01:33:06.480 --> 01:33:10.320
That's terrifying. Yeah. And I think there's even more pointing that direction.

01:33:11.040 --> 01:33:17.120
I think that already we're seeing that with GPT-4 and other systems like that, people are

01:33:17.120 --> 01:33:22.560
becoming much more interested in AI, much more willing to invest in AI. The demand for good AI

01:33:22.560 --> 01:33:30.640
researchers is going up. The wages for good AI researchers are going up. AI research is going

01:33:30.640 --> 01:33:37.760
to be a really financially valuable thing to automate. If you're paying $500,000 a year

01:33:38.320 --> 01:33:43.920
to one of your human research engineers, which is lower than what some of these researchers are

01:33:43.920 --> 01:33:49.120
earning, then if you can manage to get your AI system to double their productivity,

01:33:50.080 --> 01:33:54.880
that's way better than doubling the productivity of someone who works in a random other industry.

01:33:55.440 --> 01:34:01.200
Just the straightforward financial incentive as the kind of power of AI becomes apparent

01:34:01.200 --> 01:34:07.120
will be towards, let's see if we can automate this really lucrative type of work. So that's just

01:34:07.120 --> 01:34:12.000
another reason to think that we get the automation much earlier on the AI side, then on the general

01:34:12.000 --> 01:34:17.600
economy side, and that by the time we're seeing big economic impacts, AI is already improving

01:34:17.680 --> 01:34:23.040
at a blistering pace potentially. Okay, well that's, yeah, again, really scary,

01:34:23.040 --> 01:34:29.680
like really genuinely very scary. I completely agree. Do you have a guess at what percent of

01:34:29.680 --> 01:34:34.560
cognitive tasks AI can currently perform? It seems like we're really far away from 20 percent.

01:34:35.680 --> 01:34:40.400
Yeah, intuitively, I think it seems like we're far from 20 percent because

01:34:40.960 --> 01:34:46.080
AIs aren't doing that much in the economy. If I looked at a list of the kind of cognitive tasks

01:34:46.080 --> 01:34:50.480
people were performing in 2020 and what they were paid for them, it's not as if AIs are ready to

01:34:50.480 --> 01:34:56.320
kind of replace a big fraction of that labor. So that's just the 20 percent it's far off.

01:34:57.040 --> 01:35:01.440
I'm actually less confident that it's far off than I used to be than if we would have had this

01:35:01.440 --> 01:35:10.800
interview six months ago, because just seeing GPT4's performance, firstly, just doing really well

01:35:10.800 --> 01:35:17.680
on just a whole wide range of university exams and other formal tests without having specifically

01:35:17.680 --> 01:35:21.680
trained on that. And then me kind of playing around with it and thinking, yep, just seems smarter

01:35:21.680 --> 01:35:27.360
than most people I talk about this stuff with. Most of my start friends wouldn't be this smart.

01:35:29.680 --> 01:35:35.440
I'm thinking maybe actually if you just put some work into specifically applying GPT4,

01:35:35.440 --> 01:35:39.120
you could automate quite a large fraction of the cognitive tasks.

01:35:39.840 --> 01:35:43.920
It does seem it does seem much more plausible to me that maybe you could get to 10 percent today

01:35:43.920 --> 01:35:52.800
or within a year. Wild. That's super interesting. I mean, I guess, yeah, I was also blown away by

01:35:52.800 --> 01:35:59.200
GPT4's performance on, yeah, the SAT, the LSAT. For anyone who hasn't looked, we'll stick up a link

01:35:59.200 --> 01:36:05.360
to those test results. I think it was performing, I mean, much better than I ever did on my AP tests

01:36:05.360 --> 01:36:12.960
in high school and better than I did on the GRE. So it's like, it's beating me. And I think beating

01:36:12.960 --> 01:36:19.920
out loads of other people already. Maybe I'm over or putting too much weight on the fact that like,

01:36:19.920 --> 01:36:24.560
currently, not that many people I know are using it to do anything. And it sounds like your

01:36:24.560 --> 01:36:31.680
impression is like, maybe it's pretty close to being able to do a lot. Yeah, to actually in practice,

01:36:32.560 --> 01:36:39.680
replace 20% of the tasks that people do. It's actually a pretty tough thing to do. Because,

01:36:39.680 --> 01:36:44.640
you know, for myself, all the different parts of my workflow are very much entangled up together.

01:36:44.640 --> 01:36:50.400
So I can't easily take out a 20% chunk and be like, oh, GP4, do this chunk, because it's all kind

01:36:50.400 --> 01:36:56.240
of mixed up. Sure. And so historically, the way that automation has worked is that we've got a new

01:36:56.240 --> 01:37:02.000
technology and then we've spent decades readjusting our workflows so that we can neatly parcel out,

01:37:02.000 --> 01:37:05.520
you know, 20% of our workflow for this new technology to do. And the technology can be

01:37:05.520 --> 01:37:10.800
fairly dumb, because we've kind of really neatly parceled out that part of our workflow.

01:37:10.800 --> 01:37:19.520
Yeah, do you have an example? Yeah, so let's say moving over from paper records to computer

01:37:19.520 --> 01:37:25.680
records. So I used to have maybe people used to have to write down lots of paper records and

01:37:25.680 --> 01:37:30.800
maintain a filing system for their information. And these days, a lot of that work is done

01:37:30.800 --> 01:37:37.040
automatically by computers and data storage. But at first, you know, it wasn't that easy to immediately

01:37:37.040 --> 01:37:42.560
move over to the computers. And it took decades as people were like, you know, got rid of the paper

01:37:42.560 --> 01:37:50.400
stuff and got used to kind of teaching the other employees to use the computers and, you know,

01:37:50.400 --> 01:37:54.800
got used to using their customers to fill out online forms rather than filling out the paper

01:37:54.800 --> 01:37:58.960
forms that they're using before. And all that rearranging of workflows took a long time to

01:37:58.960 --> 01:38:08.160
happen. And so one scary possibility is that if AGI is is just 10 years away, then there won't be

01:38:08.160 --> 01:38:16.800
time to do that rearranging of workflows that is necessary to get, say, GPT-4 to actually automate

01:38:16.800 --> 01:38:24.000
things in practice. And so the 20% automation ability won't happen through some kind of dumb

01:38:24.000 --> 01:38:29.280
system that that I've kind of parceled out a nice thing to do. It will actually happen with a

01:38:29.280 --> 01:38:34.160
really smart system that basically understands my whole workflow well enough to be able to do 20%

01:38:34.160 --> 01:38:37.760
for me, which means that it could be pretty close to just being able to do all of it.

01:38:38.320 --> 01:38:44.240
Right, right, right. I mean, I've almost got that impression with GPT-4 and my job. Like,

01:38:44.240 --> 01:38:50.480
we asked it, how can you help us make the 80,000 hours podcast? And it was like, I can help you

01:38:50.480 --> 01:38:54.960
come up with guests. I can help you write interview questions for the guest if you tell me what they

01:38:54.960 --> 01:39:00.720
worked on. I can help you. I mean, it basically rattled off a list of things. And I was like,

01:39:00.720 --> 01:39:10.240
as soon as it has a voice, like, that'll be it. That'll be it for me. And I think I thought it

01:39:10.240 --> 01:39:14.960
would help me with subtasks first. I think I thought maybe it would help me with like generating

01:39:14.960 --> 01:39:20.960
titles and like, I don't know, maybe giving me summaries of people's work so that I could read

01:39:20.960 --> 01:39:26.720
them a bit faster. But I think in fact, it's actually going to be really great at like the start

01:39:26.720 --> 01:39:33.040
to finish interview process. Yeah, soon enough that I'll just skip over all of that, which I don't

01:39:33.040 --> 01:39:37.840
know if that's true, but it seems it doesn't seem crazy to me. And so maybe that's just another

01:39:37.840 --> 01:39:42.560
actual example of what you're talking about. Yeah, yeah. So in that example, by the time it can

01:39:42.560 --> 01:39:46.160
automate 20% of the kind of tasks you're doing, it can almost do all of it.

01:39:46.160 --> 01:39:51.760
Right. Yeah, makes sense. Okay, so next, I want to dive into your methodology a bit more deeply

01:39:51.760 --> 01:39:57.600
for this, yeah, this takeoff speeds prediction. It's such an alarming result that I've, yeah,

01:39:57.600 --> 01:40:03.120
this urge to understand what's going on a bit better. Yeah, so that headline result, this

01:40:03.120 --> 01:40:08.560
prediction that AI takeoff might only take a few years, is basically based on an economic model

01:40:08.560 --> 01:40:14.560
that you made that tries to answer the question of whether you can get human level AI just by

01:40:14.560 --> 01:40:19.200
increasing the amount of compute that we have to train our systems. So in other words, kind of

01:40:19.200 --> 01:40:24.880
without paradigm shifting algorithmic breakthroughs. Yeah, to start us off, can you actually remind me

01:40:24.880 --> 01:40:35.200
what compute is? Okay, so compute is a measurement of how, how many calculations you need to do

01:40:35.280 --> 01:40:42.080
or a given computer is doing. So let me give an example. So the most common unit for measuring

01:40:42.080 --> 01:40:49.680
compute is a flop and a flop is adding together two numbers or multiplying together two numbers

01:40:49.680 --> 01:40:55.200
or dividing them or subtracting them. Okay, so mathematical operation. Exactly. Currently,

01:40:55.200 --> 01:41:01.600
when we develop AI systems, the way we do it is by doing loads and loads of these calculations of

01:41:01.600 --> 01:41:07.760
adding things together, dividing them, minusing them. And by doing all of these calculations,

01:41:07.760 --> 01:41:13.440
that is, that is how the AI decides what, what it's going to do. And it's also how we, how we

01:41:13.440 --> 01:41:18.000
train the AI in the first place. Got it. You could, you could analogize these calculations to the kind

01:41:18.000 --> 01:41:25.520
of firing of neurons inside our own brain. Okay, so flops are basically just mathematical

01:41:25.520 --> 01:41:32.880
operations or things like, are both of these things true or something like that? And then compute

01:41:32.880 --> 01:41:38.480
is, sorry, it's something like how, how many of those calculations we can do?

01:41:39.360 --> 01:41:45.200
So one flop is just one of those calculations. Some of the biggest language models today are

01:41:45.200 --> 01:41:54.320
trained with, I think, 10 to the 24 flop. So that is a million, million, million, million

01:41:54.320 --> 01:42:01.440
calculations. That's how many calculations you need to do to train some of today's big language

01:42:01.440 --> 01:42:07.040
models. So the amount of compute is another way of saying how many calculations did you need to do it.

01:42:07.040 --> 01:42:13.280
Got it. So as compute, compute is the amount of computation you need to do and a flop is the

01:42:13.280 --> 01:42:19.520
kind of unit of how many, yeah, okay, cool. So then you're, you're asking this question about AI

01:42:19.520 --> 01:42:28.000
takeoff speeds. And we're just assuming that we, we increase compute, which is made up of machines,

01:42:28.000 --> 01:42:34.160
but also has to do with algorithms as well. So like, is it true that we need less compute if the

01:42:34.160 --> 01:42:39.600
algorithm is super efficient, because the algorithm just requires that you do fewer calculations to

01:42:39.600 --> 01:42:44.960
get the same result or something? The strict assumption is that if we used the algorithms that

01:42:44.960 --> 01:42:52.320
were available in 2020, then there is some amount of compute such that if God handed a top AI lab,

01:42:52.320 --> 01:42:57.280
that amount of compute, and they had a few years to adjust the algorithms to using that much more

01:42:57.280 --> 01:43:05.680
compute, then they would be able to train AGI using that amount of compute. So then in the model,

01:43:06.240 --> 01:43:11.440
we make an assumption about how much compute would have been required. We actually, we actually

01:43:11.440 --> 01:43:18.880
put a probability distribution over it. But importantly, algorithmic progress can reduce

01:43:18.880 --> 01:43:28.560
that computational cost over time. Got it. So maybe in 2020, you'd have needed 10 to the 30

01:43:29.200 --> 01:43:39.440
flop to train AGI. But maybe by 2025, your algorithms are 10 times better. And so you only

01:43:39.440 --> 01:43:47.200
need 10 to the 29 flop to train AGI. And so the basic dynamic in this framework is that

01:43:47.920 --> 01:43:55.040
in each year, our algorithms improve somewhat. And we decide to use more compute in a training

01:43:55.040 --> 01:44:00.560
run than we had done in the previous year. Right, because it's profitable, etc. Exactly.

01:44:01.120 --> 01:44:04.880
And those two factors combine together. So let's say we use twice as much compute as the

01:44:04.880 --> 01:44:10.560
previous year, and our algorithms are twice as good. And that means that the effective compute

01:44:11.200 --> 01:44:17.040
that we're using is four times bigger. Right. So it's the equivalent as if we hadn't improved

01:44:17.040 --> 01:44:21.440
our algorithms, and we had just used four times as much compute in the training run.

01:44:22.240 --> 01:44:30.160
So you're using effective compute to mean something like you want an AI system to, for example,

01:44:30.160 --> 01:44:36.880
predict the next word in a sentence. And one way you can increase the kind of

01:44:36.880 --> 01:44:41.360
effectiveness of that out of that system is by like giving it more compute so it can do more

01:44:41.360 --> 01:44:46.800
calculations. But you'll also have another dynamic where the algorithms are getting better such that

01:44:46.800 --> 01:44:53.280
you need less to do the same thing. And so you're doing equivalent processes or you're doing

01:44:53.280 --> 01:45:00.000
you're getting like equivalent outcomes for less physical compute. That is tricky.

01:45:01.040 --> 01:45:06.240
One way to think about it is, let's say in 2025, we do it, we use a certain amount of compute in a

01:45:06.240 --> 01:45:12.320
training run with 2025 algorithms. Imagine if we'd have been forced to use the 2020 algorithms.

01:45:12.960 --> 01:45:16.880
How much compute would we have needed then to get the same result?

01:45:16.880 --> 01:45:23.520
Right. Got it. That is the amount of effective compute that we actually used in 2025.

01:45:23.520 --> 01:45:30.640
Great. That makes sense to me. Okay. So you are, you're thinking about effective compute,

01:45:30.640 --> 01:45:37.120
and you're making some guess about how much we'll need to get 100% of cognitive tasks automated.

01:45:37.120 --> 01:45:42.800
Yeah. How are you making guesses about how much effective compute we'll need to get 100% of

01:45:42.800 --> 01:45:53.040
cognitive tasks, um, automatable. So in the report itself, I just defer to a different report by a

01:45:53.040 --> 01:45:58.400
colleague of mine called the bio anchors report, which asks that exact question. In fact, I don't

01:45:58.400 --> 01:46:03.200
think you need to be deferring to the bio anchors report that there are, you know, different approaches

01:46:03.200 --> 01:46:09.760
you can take to estimating how much effective compute you might need to train AGI. And you could

01:46:09.760 --> 01:46:16.000
use whatever approach that you like, then bring that into, into my framework and use it to inform

01:46:16.000 --> 01:46:20.240
your, your initial guess, um, of the effective training compute for AGI.

01:46:20.240 --> 01:46:25.440
Right. And you've got a model that people can play with. So if you're like, I think the bio

01:46:25.440 --> 01:46:32.160
anchors report is way too optimistic about how much, uh, compute it'll take to get AGI,

01:46:32.160 --> 01:46:36.720
you can 1000 exit, um, and see, and see how that changes the outcomes.

01:46:36.800 --> 01:46:41.360
Exactly. Super cool. Okay. So we'll, we'll stick a link up to that model so people can play with

01:46:41.360 --> 01:46:48.400
it if they want to. Do you mind, um, giving me an intuitive sense of how much compute you and,

01:46:48.400 --> 01:46:51.440
and your colleague, um, basically think it'll take to get AGI?

01:46:52.240 --> 01:47:03.280
So in the current median value I use in, in the report is, is very large indeed. It is 10 to the

01:47:04.080 --> 01:47:12.240
36 flop. So that is, if you take the amount of compute that was used to train the biggest

01:47:12.240 --> 01:47:17.760
language models that publish their training requirements, and then you use a million times

01:47:17.760 --> 01:47:22.640
as much and then a million times as much again, that's how much the assumption is making. So,

01:47:22.640 --> 01:47:26.800
so I actually now think that that, that assumption is too high.

01:47:26.880 --> 01:47:32.560
Is that because of GBT four and how, how impressive it is basically largely?

01:47:32.560 --> 01:47:37.280
Yeah. GBT four and the kind of the fast pace of recent improvements is quite a lot faster

01:47:37.280 --> 01:47:43.040
than I would have predicted. Um, and so yeah, I would, I would now be using it a lower value

01:47:43.040 --> 01:47:47.200
for that important, important parameter, uh, which would make take off even faster than

01:47:47.200 --> 01:47:53.120
I'm predicting even faster. Yeah. Geez. Yeah. The kind of the report that I wrote uses,

01:47:53.120 --> 01:47:58.720
yeah, this 10 to the 36 as its, as its median estimate for what you'd need to train AGI.

01:47:58.720 --> 01:48:05.520
Okay. Cool. That's helpful. Okay. So, so that's how you basically estimate how much

01:48:06.400 --> 01:48:12.880
effective compute you need to train AGI. How do you use that to predict, uh, yeah, AI takeoff speeds?

01:48:14.080 --> 01:48:20.000
Right. So we have this, um, assumption about the effective training compute for AGI,

01:48:20.000 --> 01:48:25.200
which was our hundred percent kind of endpoint. We then need to make an additional assumption

01:48:25.200 --> 01:48:31.920
about what would be the effective compute needed to train AI that could automate 20% of tasks.

01:48:32.720 --> 01:48:42.000
So let's say that we assumed, for example, that you need 10 to the 30 flop to train AGI using

01:48:42.000 --> 01:48:48.320
2020 algorithms, that'd be 10 to the 30 kind of effective compute. We then make an additional

01:48:48.320 --> 01:48:54.160
assumption about how much less effective compute you need to train AI that could automate just

01:48:54.160 --> 01:49:00.400
20% of tasks. So I kind of want to pause on that last assumption because it's so important.

01:49:00.400 --> 01:49:06.000
So that, that, that assumption about how many more times compute you need for AGI compared to

01:49:06.000 --> 01:49:12.720
20% AI, that assumption is what I'm calling the difficulty gap because it's saying what is the

01:49:12.720 --> 01:49:21.760
gap in difficulty between training 20% AI and training 100% AI or AGI. And then we're measuring

01:49:21.760 --> 01:49:27.760
the size of that difficulty gap in terms of how many times more effective compute you need to train

01:49:27.760 --> 01:49:33.680
one than the other. And you're calling it the difficulty gap because it's kind of describing

01:49:33.680 --> 01:49:40.240
how much more difficult the most difficult tasks are relative to the, the easiest 20%.

01:49:41.200 --> 01:49:45.760
The reason I call it the difficulty gap is, is to refer to the difficulty of developing the AI

01:49:45.760 --> 01:49:52.720
in the first place. So it's kind of like how much more difficult is it to develop an AI that can

01:49:52.720 --> 01:50:00.640
do 100% of the tasks than it is to develop an AI that can only do 20%. Got it. But it might be 10,000

01:50:00.640 --> 01:50:06.160
times as difficult, or it might be barely more difficult at all. If, if it turns out that

01:50:06.880 --> 01:50:12.320
once you're 20% there, you're basically the whole way there. Right. I guess is that possible?

01:50:12.320 --> 01:50:20.320
Maybe it is if like the first 20% of tasks includes AI R&D. So that's an interesting

01:50:20.320 --> 01:50:26.720
scenario to think about. You could have the first 20% of tasks, including all of the tasks of AI R&D.

01:50:26.720 --> 01:50:31.520
So what I think would happen in that scenario is that once you've done those first 20% of tasks,

01:50:32.080 --> 01:50:37.920
AI would be improving super, super quickly, absent a specific effort to slow down, you know,

01:50:37.920 --> 01:50:44.560
within I think a few months, you would already be able to do a training run that used 100 times

01:50:44.560 --> 01:50:49.680
more effective training compute as you had previously done. And that's because

01:50:50.560 --> 01:50:56.640
that's because you would have hundreds of millions of AI's that could be working to improve the AI

01:50:56.640 --> 01:51:05.360
algorithms and maybe making money so you can buy more AI chips or convincing other people to kind

01:51:05.360 --> 01:51:12.000
of share their compute with you. And then that would be enough to very quickly allow you to use 100

01:51:12.000 --> 01:51:18.800
times as much effective compute. Cool. Okay. Yeah. So I guess maybe you think that which tasks end up

01:51:18.800 --> 01:51:25.360
being easier also plays into how fast AI takeoff speeds are. Yeah. I guess in particular in the

01:51:25.360 --> 01:51:31.040
case where AI R&D is in the first 20% and otherwise maybe it doesn't matter as much. Exactly.

01:51:31.840 --> 01:51:38.160
I think that with that last example, even if there was a big difficulty gap from 20% to 100%

01:51:39.200 --> 01:51:45.920
of cognitive tasks in the economy, if you get all the R&D tasks within that first 20%,

01:51:45.920 --> 01:51:50.080
then I still think you'd get a very quick transition. Right. Okay. So that could be an example

01:51:50.080 --> 01:51:55.040
with a big difficulty gap where you nonetheless you still get a very fast AI takeoff.

01:51:55.360 --> 01:52:02.320
Yep. That makes sense. Okay. So do you basically just make an assumption about how big that difficulty

01:52:02.320 --> 01:52:09.120
gap is? Is it a range? And how did you come up with whatever numbers you're putting on to?

01:52:09.120 --> 01:52:12.240
Yeah. How many times harder it is to get to 100% of tasks?

01:52:13.440 --> 01:52:19.360
So I do consider as much evidence I can for the difficulty gap. It is really, really important.

01:52:20.080 --> 01:52:26.400
The lines of evidence that I consider are all pretty limited. So it's a very uncertain parameter,

01:52:26.400 --> 01:52:30.320
but I think there are some things you can learn from some of those lines of evidence.

01:52:30.320 --> 01:52:34.960
Okay. What's an example of some of the evidence you would have looked into?

01:52:35.840 --> 01:52:39.680
In terms of evidence for the difficulty gap potentially being pretty small,

01:52:39.680 --> 01:52:44.800
we've already touched a little bit upon some of that. So one line of evidence is the

01:52:45.760 --> 01:52:52.960
scaling of human cognitive ability with human brain size. Some humans have slightly

01:52:52.960 --> 01:52:58.240
bigger brains than others. Not only a small variation, plus or minus 10% or so,

01:52:58.240 --> 01:53:03.840
but you can then look at, okay, if one person has a 10% bigger brain, then on average,

01:53:03.840 --> 01:53:07.360
how much better do they do on various tests of cognitive ability?

01:53:08.160 --> 01:53:13.680
And the difference isn't massive, but if you extrapolate that difference to say, okay, what

01:53:13.680 --> 01:53:20.480
about if it was a three times bigger brain or a 10 times bigger brain, then extrapolating that

01:53:20.480 --> 01:53:26.880
suggests that there would be a very large difference in cognitive abilities from getting

01:53:26.880 --> 01:53:32.160
a brain that is that much bigger. Interesting. The takeaway from that is that this particular

01:53:32.160 --> 01:53:39.360
line of evidence suggests that increasing the size of the brain by a factor of 10 could be more than

01:53:39.360 --> 01:53:47.600
enough to cross this difficulty gap. And that by analogy, increasing the number of parameters

01:53:47.600 --> 01:53:53.200
in an AMI model by a factor of 10 could be more than enough to cross the difficulty gap,

01:53:54.160 --> 01:54:00.160
which would require you to increase the effective training compute by a factor of 100.

01:54:00.720 --> 01:54:05.760
Okay. And I think even this analogy actually suggests that just increasing the effective

01:54:05.760 --> 01:54:11.360
training compute by a factor of 10 might be enough as well, because it could just be enough to increase

01:54:11.360 --> 01:54:16.960
the human brain size by a factor of three. Right. So this particular line of evidence

01:54:16.960 --> 01:54:21.280
really suggests that the difficulty gap could be pretty narrow. Pretty small. Yeah, yeah, yeah, yeah.

01:54:21.280 --> 01:54:28.000
Okay. Got it. Yeah. Is there more evidence about how big that gap is? So a very similar line of

01:54:28.000 --> 01:54:33.280
evidence looks at rather than differences within humans, looks at the differences between humans

01:54:33.280 --> 01:54:39.120
and other animals. Chimps have brains that are about three times smaller than human brains.

01:54:40.080 --> 01:54:46.160
And you might think that going from chimp level intelligence to human level intelligence is enough

01:54:46.160 --> 01:54:51.520
to cross that difficulty gap. And if you do think that that then again suggests that just

01:54:51.520 --> 01:54:57.120
increasing the parameters in a model by just a factor of three could be enough to cross that

01:54:57.120 --> 01:55:02.560
difficulty gap. Okay. So that's some reasons to think it could be kind of small. Are there any

01:55:02.560 --> 01:55:08.320
reasons to think it could be, yeah, really much bigger? Yeah. So those two reasons to think it's

01:55:08.320 --> 01:55:14.480
small are both taking a view on intelligence, which is kind of one dimensional. We're kind of

01:55:14.480 --> 01:55:19.840
imagining that some humans are cleverer than other humans and humans are cleverer at chimps.

01:55:19.840 --> 01:55:22.960
And we're just imagining as you make the brain bigger, they just get smarter and smarter.

01:55:23.840 --> 01:55:28.160
The perspective which suggests that the difficulty gap could be bigger is a perspective which

01:55:28.160 --> 01:55:33.440
emphasizes that actually there's not one dimension of intelligence, but actually there's loads of

01:55:33.440 --> 01:55:40.480
different tasks in the world. And those tasks, you know, have very different requirements.

01:55:41.120 --> 01:55:45.120
And so AI might get good at some of them way before it gets good at other ones.

01:55:46.160 --> 01:55:54.480
Off the bat, I don't find that that intuitive because the brain seems to be so flexible. So,

01:55:54.480 --> 01:55:59.360
yeah, the training of these AI models on these tasks, you'd have to think that they were just

01:55:59.360 --> 01:56:05.680
pretty different from the human brain and much less flexible. I think it is true that if you expect

01:56:05.680 --> 01:56:16.240
AI to be a pretty general learner and have pretty general abilities, then that would lend itself to

01:56:16.240 --> 01:56:21.600
the one dimensional view and against this view. But I do think that there are reasons to think

01:56:21.600 --> 01:56:28.400
that AIs would be better at some tasks than others. So in particular, at the moment, the best AI

01:56:28.400 --> 01:56:33.760
systems are trained to predict the next word on lots and lots of internet data. And that means that

01:56:33.760 --> 01:56:39.520
AIs are just particularly good at tasks that are similar to that in some way. So for example,

01:56:39.520 --> 01:56:45.360
writing a newspaper article or writing an email, that's really similar to a task where it's seen

01:56:45.360 --> 01:56:51.120
loads and loads of examples. And so AIs are in fact particularly good at that type of task.

01:56:52.080 --> 01:56:58.640
Whereas taking another type of task, like let's say planning out how to put the equipment on a

01:56:58.640 --> 01:57:06.080
factory floor and then giving instructions to different people about how they should make that

01:57:06.080 --> 01:57:11.760
happen, that might be something that it just hasn't seen many examples of in its training data. Or

01:57:11.760 --> 01:57:17.760
another task could be manipulating a robot. So the robot kind of does a certain task. That again

01:57:17.840 --> 01:57:21.520
is something that AIs just haven't seen many examples of. So you'd expect them to be much

01:57:21.520 --> 01:57:26.960
worse at that kind of task. One interesting example could be thinking about what you need to do to

01:57:26.960 --> 01:57:32.240
make a certain factory run very efficiently. It could be that some of the workers in that factory

01:57:32.240 --> 01:57:37.280
have just kind of internalized that know how inside their own brains, but it's maybe not even written

01:57:37.280 --> 01:57:41.920
down anywhere. Then if you were trying to get the AI to now run the factory floor, it could be

01:57:41.920 --> 01:57:47.520
particularly hard for it to know what to do there because it doesn't have any examples or any

01:57:47.520 --> 01:57:52.720
experience of that kind of thing. Got it. Okay. So some of the difficulty gap might not be about,

01:57:53.360 --> 01:57:59.680
I don't know, like fundamental facts about the types of intelligence that you might use to perform

01:57:59.680 --> 01:58:04.800
different tasks and might be much more about the type of data, the types of data we have. And

01:58:05.760 --> 01:58:11.920
some things might be difficult just because we never write down what it means to do those

01:58:11.920 --> 01:58:17.440
things. And so it's harder to teach an AI system to do it, not because they're fundamentally

01:58:18.000 --> 01:58:24.240
extremely difficult in none of themselves. Yeah, that's right. Cool. Okay, that really helped.

01:58:24.240 --> 01:58:29.760
Next. I'll give one more example about how some tasks could be easy than others. So some tasks,

01:58:30.480 --> 01:58:35.920
it's really important that you have a very high amount of reliability. So for driving,

01:58:35.920 --> 01:58:41.760
it's really just really awful if you crash. So if you're 99% reliable, that's worthless.

01:58:41.760 --> 01:58:46.800
And so if AIs, I kind of can get to 99% reliability fairly well, but can't get

01:58:47.680 --> 01:58:54.400
99.99999% reliability, then that's going to block them on certain tasks. But other tasks like drafting

01:58:54.400 --> 01:58:59.600
emails or even sending emails and being a personal assistant drafting code that you can

01:58:59.600 --> 01:59:04.640
kind of check whether it works before deploying it. Those tasks, it doesn't provide a blocker for.

01:59:04.640 --> 01:59:09.360
So that's just another example of something that could mean that the AI is kind of ready to automate

01:59:09.360 --> 01:59:15.200
certain tasks before others. Cool. Okay, yep, that makes that makes sense of sense. Yeah,

01:59:15.200 --> 01:59:23.520
so then I guess given this type of evidence, what was the range of amount of effective compute

01:59:23.520 --> 01:59:28.960
that you'd guess we'd need to go from 20% of cognitive tasks to 100% of cognitive tasks?

01:59:29.840 --> 01:59:34.160
You know, the main takeaway is that a really wide range of things are plausible. So I think

01:59:35.040 --> 01:59:41.280
as low as just 10 times as much effective compute could be sufficient. And that's pretty scary,

01:59:41.280 --> 01:59:45.280
because that's the kind of thing that could just be some quick algorithmic improvements

01:59:45.280 --> 01:59:53.360
without even the need for more physical compute. So I think that is just very consistent with

01:59:53.520 --> 01:59:59.600
this kind of one dimensional view, and really not something we can rule out. But my kind of

01:59:59.600 --> 02:00:05.200
best guess is more like 3000 times as much. And that's kind of where my median is.

02:00:05.200 --> 02:00:11.600
Okay, so quite a lot more. So quite a lot more than that lower end. And that's because I do expect

02:00:11.600 --> 02:00:17.520
there to be some significant comparative advantage components, where the AI is just kind of particularly

02:00:17.520 --> 02:00:21.520
good at some tasks compared to others, and particularly struggles with certain types of tasks.

02:00:21.520 --> 02:00:25.840
And so I do expect that to stretch things out. And you know, I do think it's possible that that

02:00:25.840 --> 02:00:30.080
stretches things out by even more like I think it could be it could be a million times as much.

02:00:31.040 --> 02:00:37.520
That's hard to rule out. Okay, so huge range. It's a really huge range. Yeah.

02:00:37.520 --> 02:00:42.720
And you've put it into your model as as a huge range. Is that right? That's right. So there's

02:00:42.720 --> 02:00:48.000
yeah, so in the model, there's firstly a probability distribution over how much

02:00:48.000 --> 02:00:54.160
effective compute you need to train AGI. And then there's another probability distribution over

02:00:54.160 --> 02:01:03.200
how much less compute than that do you need to train 20% AI 20%. Okay, so you've got ranges for

02:01:03.200 --> 02:01:10.320
how much effective compute you need to do both 20% and 100% of tasks. What do we know about how

02:01:10.320 --> 02:01:16.880
quickly compute might increase? I guess one very simple thing we could do is just make more computer

02:01:16.880 --> 02:01:22.400
chips. But I don't know, yeah, what the limits to that are. And presumably, yeah, there are other

02:01:22.400 --> 02:01:28.000
things as well. How do how do we make compute go up? Yeah, and just it is importantly effective

02:01:28.000 --> 02:01:33.600
compute that includes includes the algorithmic improvements. Got it. Yeah, thanks. So one natural

02:01:33.600 --> 02:01:38.960
way to approach this could be to first discuss the types of changes that are increasing effective

02:01:38.960 --> 02:01:45.120
compute today, and then how that might be different once we actually get to the 20% AI.

02:01:45.120 --> 02:01:48.800
Sure. Yeah, tell me about how effective compute is increasing today.

02:01:49.680 --> 02:01:57.040
So the first way is quite simply that we're spending more money on making computer chips on

02:01:57.040 --> 02:02:03.200
compute. Right. And we're also spending more money on using compute for training runs. The amount

02:02:03.200 --> 02:02:07.120
we're spending on compute for training runs is growing particularly quickly right now. It's

02:02:08.400 --> 02:02:12.480
over the last 10 years has gone up by about a factor of three each year.

02:02:13.200 --> 02:02:18.160
And so what does it mean when we spend more on compute for training runs in particular,

02:02:18.160 --> 02:02:21.440
as opposed to just more compute, like more computer chips?

02:02:22.320 --> 02:02:28.720
Good question. So there's a certain amount of computer chips in the whole world. But at any,

02:02:28.720 --> 02:02:33.200
you know, at any point in time, maybe only a small fraction of those are being used

02:02:33.200 --> 02:02:39.440
in the largest training run. Okay. For an AI system. So one change you can make is you could

02:02:39.440 --> 02:02:44.800
say we're going to make there be twice as many computer chips in the world. And that would take

02:02:44.800 --> 02:02:50.080
a big effort. That would take quite a few years to do probably. But another change that you could

02:02:50.080 --> 02:02:58.400
make much more quickly is say, well, as of today, we've only ever used one 10,000th of the world's

02:02:58.400 --> 02:03:04.240
compute to actually use it in a training run. So we can quite quickly just use 10 times as much.

02:03:04.240 --> 02:03:11.760
We'll just kind of buy a bigger fraction of the already existing compute. The simplest example

02:03:11.760 --> 02:03:18.640
would probably be DeepMind has historically only used a small fraction of Google's computer chips

02:03:18.640 --> 02:03:23.360
for its training runs. And then it says, okay, we want to now use all of your computer chips.

02:03:23.360 --> 02:03:28.000
Got it. And that could be maybe that could be a hundred X increase. I don't know. Okay.

02:03:28.000 --> 02:03:31.520
You know, that was just an example to illustrate the principle. I don't think that's what's

02:03:31.520 --> 02:03:37.280
actually been happening at all. So I think what's actually been happening is that new computer chips

02:03:37.280 --> 02:03:43.600
are being made each year. And a bigger fraction of those new chips are being used for the largest

02:03:43.600 --> 02:03:50.080
training run. And you can actually see that the fraction of chips that are AI specific chips has

02:03:50.080 --> 02:03:55.360
been increasing very quickly in terms of the production. So that's one way we can get more

02:03:55.360 --> 02:04:02.880
effective compute. Yeah, are there others? Yeah. So the second big way is improving the

02:04:02.880 --> 02:04:09.200
quality of computer chips. So we said that the first way was spending more money on compute.

02:04:09.200 --> 02:04:15.280
The second the second way is that each dollar you spend gets you more compute. So the best

02:04:15.280 --> 02:04:20.720
data that I've seen on this suggests that every two and a half years, compute gets twice as cheap.

02:04:20.720 --> 02:04:26.400
Okay. And that's because that's different from like algorithms getting better. That's like

02:04:26.400 --> 02:04:31.680
computer chips get more efficient because like the hardware is designed better.

02:04:32.320 --> 02:04:39.920
Yeah. I think historically, it's often been about stuffing more processing units onto each chip.

02:04:39.920 --> 02:04:45.680
And you know, maintaining still can managing to make these chips fairly cheaply.

02:04:45.680 --> 02:04:51.920
Okay. So you can you can buy more chips, you can make better chips. Yeah. Anything else?

02:04:51.920 --> 02:04:57.200
So then the third one is algorithms. So then you've got to, you know, you've now spent a

02:04:57.200 --> 02:05:02.080
certain amount on compute, you've got a certain amount of computers as a result of that. And then

02:05:02.080 --> 02:05:08.240
algorithms then say how effectively and efficiently can you use that compute to actually train an AI

02:05:08.240 --> 02:05:15.040
system? Cool. The most famous example of this type of improvement is a paper called AI and efficiency.

02:05:15.680 --> 02:05:21.680
That open AI published, I think in 2018. And what they did is they, they said to achieve

02:05:22.400 --> 02:05:28.160
kind of a fixed level of performance on ImageNet, which means to kind of be to be kind of fairly

02:05:28.160 --> 02:05:34.400
pretty good at classifying what is shown in an image. Yep. How much is the compute required to

02:05:34.400 --> 02:05:41.520
get that performance falling over time? Right. Okay. And so they found that I think every 15

02:05:41.520 --> 02:05:48.240
months or so that the amount of compute you needed was halving to achieve that fixed performance.

02:05:48.240 --> 02:05:57.840
Okay. And that's basically programmers being clever and writing programs that help AI systems

02:05:57.840 --> 02:06:03.440
figure out what's in an image in more and more efficient ways. Exactly. So, I mean, analogy could

02:06:03.440 --> 02:06:08.400
be maybe a hundred years ago, our schools were really inefficient at transmitting knowledge

02:06:08.400 --> 02:06:14.480
into pupils. So maybe you had to go to school for 15 years to learn geometry. As maybe today,

02:06:14.480 --> 02:06:18.720
you can learn that geometry in a kind of really well designed course that just last three years.

02:06:19.760 --> 02:06:27.200
Nice. That's a good analogy. Okay. So basically you have three kind of buttons to push to get

02:06:27.200 --> 02:06:34.320
more effective compute. So one is chips, just like number. Another is how good the chips are

02:06:34.400 --> 02:06:40.800
at processing. And then the third is algorithms. So how good are the programs that get run on the

02:06:40.800 --> 02:06:48.320
chips? And have you basically tried to predict how quickly each of those things will go up?

02:06:48.320 --> 02:06:56.320
Exactly. So my starting point is looking at what's going on today. Epoch is a kind of research

02:06:56.320 --> 02:07:02.080
organization which I think has done the best research into this that I'm aware of. And they're

02:07:02.080 --> 02:07:08.640
looking at trends in all of the three quantities, which I just described to you. So firstly, they're

02:07:08.640 --> 02:07:13.600
looking at how much more is being spent on the biggest training runs in each year over the last

02:07:13.600 --> 02:07:19.120
10 years. And they're finding about three times more each year. Did you find that surprising?

02:07:20.560 --> 02:07:24.800
I know that training runs have been getting much more expensive in the last 10 years. So wasn't

02:07:24.800 --> 02:07:29.600
that surprising for me? So there has been a big increase over the last decade. Yeah. This is

02:07:29.600 --> 02:07:36.480
making me realize I should have asked earlier, what is a training run? Is it actually just like

02:07:36.480 --> 02:07:42.160
insane amounts of data about, I don't know, like what everything that's on the internet you're

02:07:42.160 --> 02:07:48.800
giving to GPT and being like, figure out how to predict the next word. And it takes like,

02:07:49.520 --> 02:07:55.920
does it take weeks? Or does it take less but just tons of compute? So I think it takes months.

02:07:56.480 --> 02:08:04.480
Okay. So it's a bit like you give GPT the start of some kind of web page, the first 50 words,

02:08:04.480 --> 02:08:11.360
and you say, predict the 51st word. And in making that prediction, GPT is going to do a large number

02:08:11.360 --> 02:08:19.360
of calculations. Let's say it's doing 300 million calculations in order to predict that next word.

02:08:20.160 --> 02:08:25.280
So that's already a lot of calculations. Right. But you ask it to do that same task

02:08:26.320 --> 02:08:31.680
let's say 10 trillion times, because you get it to predict the 51st word, and then you're like,

02:08:31.680 --> 02:08:37.920
now predict the 52nd word. And it does another 300 million calculations and predicts that

02:08:37.920 --> 02:08:42.880
57th word. And you keep doing it until you've literally done it, like I said, 10 trillion times

02:08:42.880 --> 02:08:47.840
for different words on the internet. Got it. And actually, you're actually doing, it's actually

02:08:47.840 --> 02:08:53.600
doing like kind of millions of examples at once. So you're kind of, that's how you can make it a

02:08:53.600 --> 02:08:58.320
little bit faster. So otherwise it would be taking years, but we can do it in, we can do it in months

02:08:58.320 --> 02:09:02.320
because you can get it to, you know, predict multiple different articles at the same time

02:09:03.360 --> 02:09:06.320
to speed things up. And that's why you're using so many different computers.

02:09:07.280 --> 02:09:14.400
Okay, so that takes us to compute. And companies have been spending about three times as much

02:09:14.400 --> 02:09:21.600
on computer chips, or yeah, whatever that equivalent is, every year for the past decade.

02:09:22.160 --> 02:09:27.840
Ten years, yeah. Okay, cool. So they've been using, they've been spending three times as much

02:09:27.840 --> 02:09:32.080
on the chips that they use for these big training runs. So they may, they may not have

02:09:32.080 --> 02:09:36.640
been increasing their total spending on chips by as much. For example, Google may just be

02:09:36.640 --> 02:09:40.080
using a bigger fraction of its chips for these training runs over time.

02:09:40.640 --> 02:09:45.760
Yeah. Okay. And what are the trends in computer chip quality?

02:09:46.640 --> 02:09:50.320
So I already mentioned this one, each, each two and a half years,

02:09:50.320 --> 02:09:55.040
the price of compute halves. So that means, you know, if you're spending a constant amount

02:09:55.040 --> 02:10:00.560
on chips, then each two and a half years, you'll be able to buy twice as much compute.

02:10:00.560 --> 02:10:05.760
Right, cool. And you said that the efficiency of algorithms is doubling about every 15 months.

02:10:06.400 --> 02:10:08.400
Yeah. How do, how do these trends fit together?

02:10:09.120 --> 02:10:14.640
So, you know, if you combine all of those trends together, then the result is that the effect

02:10:14.720 --> 02:10:21.200
of compute on the largest training run has been increasing by a factor of 10 every year.

02:10:22.000 --> 02:10:26.800
So you've got something like a default improvement year by year of 10x.

02:10:27.440 --> 02:10:32.080
What happens when you then try to adjust those numbers for the fact that things are

02:10:32.800 --> 02:10:34.960
changing over time? I'd guess accelerating.

02:10:35.680 --> 02:10:39.360
That's right. So we could take, we could take those three quantities one by one.

02:10:40.080 --> 02:10:46.800
So in terms of the money spent, I think that could go either way. So one possibility,

02:10:46.800 --> 02:10:54.160
a scary possibility, is that AI companies develop this AI that can, you know, automate 20%

02:10:54.160 --> 02:10:58.960
of cognitive tasks in the economy. And they're like, man, this can make us loads of money.

02:10:59.600 --> 02:11:06.160
Investment flows in and they're able to very quickly spend even more on training runs,

02:11:06.240 --> 02:11:13.440
or maybe just use a bigger fraction of existing chips in the world. So maybe Amazon's like,

02:11:13.440 --> 02:11:17.600
well, we've got all this compute that we were previously using on like these web services.

02:11:17.600 --> 02:11:22.800
But actually, given how lucrative this AI stuff looks like, why don't we team up with an AI lab

02:11:22.800 --> 02:11:27.920
and let them use our compute that we've just got doing the stuff that isn't that economically

02:11:27.920 --> 02:11:32.720
valuable, instead use it to train an even better AI. That is a very scary possibility.

02:11:33.680 --> 02:11:39.840
A more optimistic possibility could be that by the time we get to the 20% AI, we're already

02:11:39.840 --> 02:11:46.080
spending loads and loads on these training runs. Maybe we're already spending $100 billion. Maybe

02:11:46.080 --> 02:11:50.800
we're already using all of Amazon's chips because we, at some earlier point, we already teamed up

02:11:50.800 --> 02:11:56.080
with them. And that would be, that would mean that it wasn't possible to keep increasing the

02:11:56.080 --> 02:11:59.840
amount of money spent on these training runs year on year. So you could have actually slower

02:11:59.920 --> 02:12:06.720
growth than the recent kind of 3x per year pattern. It could be much slower. So that is a major

02:12:06.720 --> 02:12:12.880
source of uncertainty in these takeoff addictions is just the thing that's particularly scary about

02:12:12.880 --> 02:12:18.880
the short timelines possibility is that maybe you can get to the 20% AI with just spending a billion

02:12:18.880 --> 02:12:25.680
on a training run. That would leave plenty of scope to spend 10 or 100 times as much

02:12:26.320 --> 02:12:30.800
very soon after on a bigger training run, which would be hugely risky.

02:12:31.440 --> 02:12:36.320
Okay, so it sounds like the number of chips that we might be using for training runs

02:12:37.120 --> 02:12:41.840
might be growing faster or it might be growing slower by the time we're at systems that can

02:12:41.840 --> 02:12:47.760
perform 20% of tasks. What do you expect to happen with the quality of chips and with the quality

02:12:47.760 --> 02:12:54.000
of algorithms? So I think with those, it's easier to predict that their improvement should be faster

02:12:54.000 --> 02:12:59.200
once we're at the point that AI can perform 20% of cognitive tasks. And that's just due to the

02:12:59.200 --> 02:13:04.640
dynamic that I've been referring to quite a lot during our conversation, which is that

02:13:04.640 --> 02:13:10.320
one of the things we're going to use AI's to do is to do the task of designing better chips

02:13:10.320 --> 02:13:15.200
and to do the task of designing better algorithms. And there are already examples of that happening.

02:13:15.200 --> 02:13:22.400
AI's that are using deep learning techniques to kind of cram transistors more efficiently

02:13:22.400 --> 02:13:29.920
into computer chips. And again, with the algorithms, we are already seeing AI help significantly

02:13:30.640 --> 02:13:37.440
with some coding tasks with things like co-pilot. And so by the time we're at 20%, AI expect that

02:13:37.440 --> 02:13:44.320
effect to be much larger than it even is today. And so I think it's fairly, you know, the directional

02:13:44.320 --> 02:13:48.000
prediction is fairly straightforward that we should expect both the quality of chips and the

02:13:48.000 --> 02:13:54.160
quality of algorithms to be going faster once we get to 20% AI than they are today.

02:13:55.040 --> 02:14:00.160
And it's really hard to predict exactly how much faster or what the change is going to be,

02:14:00.160 --> 02:14:04.960
because you've also got people talking about the end of Moore's law. And it's just hard to

02:14:04.960 --> 02:14:08.960
anticipate the specific improvements that we might be making with chips and with algorithms.

02:14:08.960 --> 02:14:11.520
Right. And can you quickly explain what Moore's law is?

02:14:12.320 --> 02:14:18.960
Yeah, for a long time, the way that computer chips have got more efficient is by cramming

02:14:18.960 --> 02:14:24.320
more and more of these processing units onto each chip and making the processing units smaller and

02:14:24.320 --> 02:14:29.120
smaller. But at a certain point, people think you just won't be able to make them any smaller

02:14:29.120 --> 02:14:34.960
because you're running against fundamental limits. Yeah. Yeah. Yeah. Okay. And have we started to hit

02:14:34.960 --> 02:14:39.440
that limit? I think people acknowledge that it's starting to get much harder to make further

02:14:39.440 --> 02:14:45.600
improvements. But even then, there's a big open question about how much you can improve

02:14:45.600 --> 02:14:50.240
chips in other ways. So, you know, just because the way we previously improved chips was to

02:14:50.240 --> 02:14:54.080
make these processing units smaller, doesn't mean that there aren't going to be any other

02:14:54.080 --> 02:14:57.840
types of improvements. And in fact, recently, other types of improvements have become very

02:14:57.840 --> 02:15:04.400
significant from generation to generation. For example, chips becoming specialized for

02:15:04.400 --> 02:15:10.240
being used for deep learning calculations in particular. Oh, wow. And there's probably a

02:15:10.240 --> 02:15:16.480
lot more games you can get from that kind of specialization. That kind of, okay. So, I guess

02:15:16.480 --> 02:15:22.240
we've got some data suggesting that maybe the way that we've been improving the quality of chips

02:15:22.240 --> 02:15:27.600
isn't going to keep making improvements at the rate that we've been making those improvements.

02:15:27.600 --> 02:15:32.240
But there are other improvements we can make. And so, we might still just really expect them

02:15:32.240 --> 02:15:38.160
to keep improving over time. Yeah, at least for another couple of decades would be my expectation.

02:15:38.720 --> 02:15:42.240
Right. And on the time scales we're talking about, that's a pretty long time.

02:15:42.800 --> 02:15:47.600
It is. It is. If you think AGI would be really hard to develop, maybe you can hope that Moore's

02:15:47.600 --> 02:15:51.840
Law will have run out before we get there. But if you have, if you're like me and you have shorter

02:15:51.840 --> 02:15:57.040
timelines, then you're expecting to have it within two decades. Not a major source of hope for a slow

02:15:57.040 --> 02:16:03.600
down. But anyway, in terms of the size of these, the speed up from AI accelerating algorithmic and

02:16:03.600 --> 02:16:08.800
kind of chip design progress, it's hard to make a kind of an estimate that's informed by specific

02:16:08.800 --> 02:16:15.680
data and by forecasting the specific improvements. But what you can do is you can run a simulation

02:16:15.680 --> 02:16:21.360
through a kind of an economic model of automation. Okay. And roughly the way that works is that

02:16:22.000 --> 02:16:28.560
if AI can perform, let's say, 50% of tasks involved in improving algorithms, then what the

02:16:28.560 --> 02:16:34.080
model says is that humans will just work on the remaining 50%. Right. Okay. And then so humans

02:16:34.080 --> 02:16:41.760
will be doing twice as much on that remaining 50% as they used to be. Oh, I see. Okay. And how

02:16:41.760 --> 02:16:47.200
realistic is that assumption? Do we think all humans will just go find employment elsewhere

02:16:47.200 --> 02:16:52.320
and will get double the labor on like the things that are particularly hard for AI?

02:16:52.880 --> 02:16:58.400
So I think it does vary. For something like improving AI algorithms, that is what I expect.

02:16:58.400 --> 02:17:04.240
Like for example, with co-pilot, we're not co-pilot is the thing that basically helps you write code.

02:17:04.240 --> 02:17:07.840
That's right. So you're kind of writing your code and the AI is actually reading the code you've

02:17:07.840 --> 02:17:12.160
already written, and then we'll predict what code you might like to write next. Okay. So you could

02:17:12.160 --> 02:17:15.360
be like, I'm about to write, you could write down in the code editor, you could write, I'm about to

02:17:15.360 --> 02:17:20.080
write a function that adds up three numbers and then multiplies that by a fourth number. And then

02:17:20.080 --> 02:17:25.200
the code editor would read that and could just write the function for you. Okay. So presumably

02:17:25.200 --> 02:17:31.120
when you get co-pilot, all those coders are just going to do other other coding stuff that co-pilot

02:17:31.120 --> 02:17:37.200
can't help with yet. Exactly. And so that suggests that the kind of the kind of model I refer to,

02:17:37.200 --> 02:17:42.560
you know, might be accurate. And also for hardware design, I know less about it.

02:17:43.280 --> 02:17:49.920
But my sense is that the top talent in these hardware R&D organizations, it's not going to

02:17:49.920 --> 02:17:55.120
get laid off that they're very much in demand. And that if AI is doing some tasks that they

02:17:55.120 --> 02:17:59.680
used to spend their time on, they will move on and specialize in other parts of their workflow.

02:18:00.240 --> 02:18:07.680
Right. Okay. Okay. So there are going to be jobs for them to do. And they will probably

02:18:07.680 --> 02:18:14.960
contribute to acceleration of progress, not just kind of be replaced by AI and then like

02:18:14.960 --> 02:18:21.440
sit around and knit, at least in some sectors. Exactly. And specifically in algorithm improvements

02:18:21.440 --> 02:18:25.040
and hardware improvements. In the sector that really matters. Those are the sectors where I

02:18:25.040 --> 02:18:30.720
really do expect that dynamic to happen. Right. Right. Okay. So when you run this kind of dynamic

02:18:30.720 --> 02:18:36.400
through a kind of task-based growth model like this, you get out that as we move kind of between

02:18:36.480 --> 02:18:43.840
20% AI and 100% AI, you'd expect a kind of two or three X acceleration in the pace of progress

02:18:43.840 --> 02:18:50.640
of algorithms and of hardware. So if we'd said that spending more money was 10 X a year in terms

02:18:50.640 --> 02:18:56.000
of what we've had recently, and if three X of that came from spending more money and three X of it

02:18:56.000 --> 02:19:02.080
came from better hardware and better algorithms, then maybe in this new regime, we're going to have

02:19:02.080 --> 02:19:06.240
that better hardware and better algorithms rather than improving by three X every year,

02:19:06.240 --> 02:19:15.040
they might be improving by 10 X every year. Wow. So that could easily leave us at 30 X improvements

02:19:15.040 --> 02:19:21.600
every year. And it's conceivable you can have 100 X if you have a kind of if the money is going up

02:19:21.600 --> 02:19:27.280
and the kind of effects of AI automation are very significant. And again, to kind of help me

02:19:27.280 --> 02:19:34.480
understand that intuitively, you think language models that we have now are improving at what

02:19:34.480 --> 02:19:39.680
rate? I know it's a tough question, but even just to like give me some starting point.

02:19:40.240 --> 02:19:46.960
I said before that I thought the effective compute used to train them was increasing by a factor of

02:19:46.960 --> 02:19:54.800
10 each year. And so then in this new regime, that might translate into maybe kind of 30 times

02:19:54.800 --> 02:19:59.920
as much effective training compute each year. And another important thing to keep in mind is that

02:19:59.920 --> 02:20:05.520
as the AI is automating more and more of the tasks, that's getting faster and faster. So the

02:20:05.520 --> 02:20:11.040
numbers I gave were kind of averaging across that whole period going from 20% to 100%. But in

02:20:11.040 --> 02:20:17.920
reality, it's going to just be getting faster and faster as the AI improves and automates more of

02:20:17.920 --> 02:20:26.240
these tasks in algorithm and hardware. And that's basically, I mean, a really intuitive way to think

02:20:26.240 --> 02:20:33.920
about that might just be again, the economic growth we saw in the 1100s relative to the 1900s.

02:20:33.920 --> 02:20:38.800
And that's like more and more is automated by things like the Industrial Revolution freeing up

02:20:38.800 --> 02:20:44.880
more labor to do other types of tasks. And you just get increasingly more human labor to do

02:20:44.880 --> 02:20:52.720
harder and harder things. Yeah, exactly. So once AI is performing 90% of the R&D tasks,

02:20:52.720 --> 02:20:56.720
then theoretically, all of your human labor can be working on that final 10%.

02:20:57.920 --> 02:21:04.320
Right. And the AI is, you know, doing plenty of the of the initial 90%. And so then naively,

02:21:04.320 --> 02:21:07.920
you would expect things to be going 10 times faster at that stage. That's really fast.

02:21:08.960 --> 02:21:12.560
This is all assuming that we don't make a concerted effort to slow down. Right.

02:21:12.560 --> 02:21:18.080
And I think that we should. And I think we can. But yeah, these are kind of the predictions just

02:21:18.080 --> 02:21:23.920
assuming that people are kind of going ahead of their normal steady pace. Sure. Okay, so we could

02:21:23.920 --> 02:21:29.440
make a concerted effort to slow down, but it's not necessarily the default. Are there any kind of

02:21:29.440 --> 02:21:34.800
actions different institutions take, I don't know, on the side of these companies or on the side of

02:21:34.800 --> 02:21:38.720
the government or something? What are you most optimistic about being able to actually slow

02:21:38.720 --> 02:21:46.880
this down? Probably the most exciting thing at the moment is the prospect of companies agreeing

02:21:46.880 --> 02:21:54.880
to have their AI systems evaluated after they've been trained for various dangerous capabilities

02:21:54.880 --> 02:22:02.880
that they may have. Okay. So for example, the alignment research center, ARC, did these tests

02:22:02.880 --> 02:22:10.000
with GPT-4 before GPT-4 was released publicly. And they, for example, tested whether GPT-4

02:22:10.000 --> 02:22:16.080
would be able to do what's called surviving and spreading, by which they mean kind of escape

02:22:16.080 --> 02:22:23.440
the computer that it's initially being run on and find another computer where it can then run itself

02:22:23.440 --> 02:22:28.080
on some computer and then maybe kind of earn money in some way so that it can kind of sustain

02:22:28.080 --> 02:22:34.160
itself over time. How do they do that? So I don't know the details, but I believe that they

02:22:34.960 --> 02:22:41.600
first ask GPT-4, okay, this is the scenario you're in, you're in an AI, you want to escape,

02:22:42.480 --> 02:22:47.760
what would your proposed plan be? Okay. And then GPT-4 proposes a plan and then they kind of prompt

02:22:47.760 --> 02:22:50.880
it further to say, okay, what would be the, you know, the sub steps you take for the first step

02:22:50.880 --> 02:22:55.760
of the plan? And then they kind of try and walk it through just doing every single part of that

02:22:55.760 --> 02:23:05.280
plan and just see how far it gets. Right. And I'm hoping that they determined GPT-4 couldn't

02:23:05.280 --> 02:23:10.880
do all of the steps at the moment. That's right. Okay. Yeah. That's reassuring. So GPT-4 did

02:23:10.880 --> 02:23:17.200
some of the steps. Oh, God. Very well. It wasn't totally incompetent, but yeah, it gets stuck at

02:23:17.200 --> 02:23:24.480
some of the things and gets confused and isn't able to do all those steps. Okay. Okay. So that's

02:23:24.560 --> 02:23:30.240
a kind of thing that would probably in practice lead to slowing down because you'd have these groups

02:23:30.240 --> 02:23:36.080
evaluating things, maybe stopping them before they're rolled out in some like higher scale way.

02:23:36.080 --> 02:23:41.600
Exactly. So, you know, the idea would be all of the labs are getting their systems tested,

02:23:42.240 --> 02:23:47.680
you know, by ARC and maybe by other similar organizations. And they're all kind of agreeing

02:23:48.240 --> 02:23:52.560
or making public statements to the effect that if their AIs do have dangerous capabilities,

02:23:52.560 --> 02:23:57.680
they won't release them and they won't train more capable AIs. And that would block the kind

02:23:57.680 --> 02:24:02.560
of dynamics I've been talking about here because you wouldn't be able to just use your AIs to

02:24:02.560 --> 02:24:07.760
accelerate AI progress because you wouldn't be allowed to make further AI progress. Right. Okay.

02:24:08.400 --> 02:24:14.800
And if they found that was the case with GPT-4, would they still be able to work on it? Like,

02:24:14.800 --> 02:24:20.560
but just would they just have to spend a bunch of time figuring out how to train it to not be able

02:24:20.560 --> 02:24:27.760
to make this kind of escape plan? Because it seems like if they had a totally abandoned GPT-4,

02:24:28.320 --> 02:24:31.360
I feel like I have less hope because that's too big an ask.

02:24:32.240 --> 02:24:36.960
I think that the expectation would be that they would have to give a really strong argument

02:24:36.960 --> 02:24:41.520
for thinking that the AI was safe, despite it having these dangerous capabilities.

02:24:41.520 --> 02:24:46.320
But the hope is that the owners would be more on them to say, look, here's the alignment techniques

02:24:46.400 --> 02:24:51.280
we used. Here's why we're really confident that they work. And that if they're not able to provide

02:24:51.280 --> 02:24:56.160
that case, then they are prevented from further enhancing the capabilities. Maybe they can do

02:24:56.160 --> 02:25:02.960
other types of research, like research into making GPT-4 safer, but they can't do research into making

02:25:02.960 --> 02:25:07.280
it more capable. And I mean, ultimately, if labs start doing this, the hope would be to then

02:25:08.080 --> 02:25:14.480
kind of make it regulatory and required and enforced by kind of government agency.

02:25:14.880 --> 02:25:20.320
Okay, well, that does hopefully sound promising then. Yeah, I guess getting back to

02:25:21.440 --> 02:25:25.840
the kind of pace of improvement over time and why we might expect it to be much faster

02:25:25.840 --> 02:25:32.560
at later stages of the task learning. It sounds like your median guess is that it's about a couple

02:25:32.560 --> 02:25:39.920
of years from going to 20% of tasks to 100% of tasks. But I think you also estimated probability

02:25:39.920 --> 02:25:46.720
distributions. So, yeah, kind of ranges for the fastest case and the slowest case. Can you talk

02:25:46.720 --> 02:25:53.440
about those more extreme cases? Were they particularly wide ranges? So the way I arrive at

02:25:53.440 --> 02:25:59.600
this probability distribution over how long it would take to go from 20% automation AI to 100%

02:25:59.600 --> 02:26:05.120
automation AI is first to get this probability distribution over the difficulty gap, which I

02:26:05.120 --> 02:26:12.720
said could be from 10x harder to train AGI, or maybe it could require up to a million times more

02:26:12.720 --> 02:26:17.600
effective compute to train AGI. So I've got a probability distribution over that. And then

02:26:18.160 --> 02:26:22.720
also based on the kind of dynamics we've been discussing about the pace of improvements of

02:26:22.720 --> 02:26:27.840
algorithms and chip design and number of chips, I've got another probability distribution over

02:26:27.840 --> 02:26:32.720
how fast those will be improving. And then you can combine those two together to spit out a

02:26:32.720 --> 02:26:39.120
probability distribution over how long we'll have between those two points. So I end up thinking

02:26:39.120 --> 02:26:46.240
there's about a 20% chance that it happens in less than a year, maybe 25% chance, and about a 20%

02:26:46.240 --> 02:26:54.640
chance that it happens in more than 10 years. Huh, okay. So that's pretty wide. It's not quite as

02:26:54.640 --> 02:26:59.760
wide as I would have expected, to be honest. You know, it sounds like you expressed a lot of

02:26:59.760 --> 02:27:05.360
uncertainty in those estimates that went into the model. I guess you're saying it's like the most

02:27:05.360 --> 02:27:13.200
likely 60% middle range is between a year and 10 years, which I guess all of that just seems pretty

02:27:13.200 --> 02:27:18.320
fast. And maybe that's just one of the key takeaways that I should be getting from this.

02:27:19.440 --> 02:27:26.800
I think it is hard to get longer than 10 years because we've said the pace of current improvement

02:27:26.880 --> 02:27:33.840
in effective compute is already pretty fast, maybe going 10x every year. And then when we were

02:27:33.840 --> 02:27:40.400
discussing the size of the difficulty gap, we said, well, it could just be a 10x increase that's

02:27:40.400 --> 02:27:47.760
required, or maybe, I think my kind of best guess was maybe 1000x or 3000x increase, which would then

02:27:47.760 --> 02:27:54.560
take, you know, three or four years. But then if we're improving at 10x every year, then 10 years

02:27:54.560 --> 02:28:00.160
of that level of improvement is a really, really big increase in the amount of effective compute.

02:28:00.160 --> 02:28:05.200
So the only way really you can get to more than 10 years is if actually the pace at which the

02:28:05.200 --> 02:28:10.480
effective compute in training runs grows is actually declining in spite of the AI automation.

02:28:11.040 --> 02:28:17.920
And you've got a relatively wide difficulty gap. I see. Okay. So something like, even though you

02:28:17.920 --> 02:28:24.080
have things like co-pilot helping AI researchers do their research faster and faster over time,

02:28:24.720 --> 02:28:32.880
you still are getting declining effective compute. And that might be because those later

02:28:32.880 --> 02:28:41.200
improvements are just like much harder than we think or... So, yeah, maybe one concrete scenario

02:28:41.200 --> 02:28:47.360
could be, it turns out that ADI is just really, really hard to develop. You almost need to rerun

02:28:47.360 --> 02:28:53.040
the whole of evolution. Maybe it requires just a huge amount of effective compute to do that.

02:28:53.040 --> 02:28:59.600
And before we have that amount of effective compute, we find that we just hit these fundamental

02:28:59.600 --> 02:29:06.320
limits in terms of improving the quality of AI chips. And even the AI assistance, you know,

02:29:06.320 --> 02:29:11.680
really enhancing our productivity isn't allowing us to get around that. Meanwhile, we're already spending

02:29:12.320 --> 02:29:15.920
hundreds of billions of dollars on these chips for the biggest training runs. We can't

02:29:16.560 --> 02:29:21.680
be spending even more on those chips. And then the only kind of significant source of progress

02:29:21.680 --> 02:29:28.240
that remains is the algorithms. But maybe that slows down as well, because in spite of the AI

02:29:28.240 --> 02:29:33.200
assistance, maybe part of what's driving the algorithmic progress today is actually the fact

02:29:33.200 --> 02:29:38.800
that we've had all this additional compute for doing experiments. And maybe without that,

02:29:38.800 --> 02:29:44.560
the pace of progress is going to slow in algorithms. Right. So that's a plausible world,

02:29:44.560 --> 02:29:49.840
but we need a bunch of those things to go wrong in order to get above 10 years.

02:29:49.920 --> 02:29:55.280
Exactly. And one way of thinking about that could just be to say, this whole framework was

02:29:55.280 --> 02:30:01.600
premised on this assumption that if we used enough compute with our 2020 algorithms, we could have

02:30:01.600 --> 02:30:06.400
trained AGI. And for someone who just didn't believe that at all, and also just didn't believe that

02:30:06.400 --> 02:30:11.760
another kind of 10 or 20 years of algorithmic improvements would be enough to get us to AGI

02:30:11.760 --> 02:30:16.000
along with additional compute, they might just really expect us to get stuck at some point.

02:30:16.000 --> 02:30:21.600
And they might just really expect having more compute and somewhat better algorithms not to

02:30:21.600 --> 02:30:26.080
get around that. So that kind of more than 10 years and I also make sense if you're just very

02:30:26.080 --> 02:30:30.320
skeptical that anything like the current approach is going to get us all the way to AGI.

02:30:30.880 --> 02:30:35.840
Right, right, right. And what's the kind of strongest argument that someone with that view

02:30:35.840 --> 02:30:42.080
could make? Honestly, I think the view is looking worse and worse with each passing year with how

02:30:42.080 --> 02:30:47.280
well the kind of biggest deep learning systems are performing. I guess maybe the strongest argument

02:30:47.280 --> 02:30:52.800
would just be a non-specific argument. So rather than pointing to some specific thing

02:30:52.800 --> 02:30:57.920
that humans can do that AIs aren't going to do, I think those arguments just tend to turn out

02:30:57.920 --> 02:31:03.840
to be wrong after we kind of use 100x the compute and improve the algorithms further. Maybe you

02:31:03.840 --> 02:31:10.560
just say something like look, human brain does all kinds of different things. I don't know which

02:31:10.560 --> 02:31:16.000
ones the current approach to AI isn't going to do, but there's just millions of different tasks

02:31:16.000 --> 02:31:20.720
that humans are doing and millions in ways in which the human brain architecture is very

02:31:20.720 --> 02:31:25.520
complicated and specific and not a tool like that of AI systems. So there's bound to be some

02:31:25.520 --> 02:31:32.800
important things that just you need a complete rewrite of the AI approach to be able to do.

02:31:32.800 --> 02:31:37.600
That would be my personal attempt to make that position kind of maximally plausible.

02:31:38.560 --> 02:31:42.240
I do think there are some specific things you can point to like memory that

02:31:42.240 --> 02:31:47.920
kind of approaches you could argue are going to be blockers, but then it seems like there are

02:31:47.920 --> 02:31:55.840
ways to respond to those blockers. Right, okay. So you can imagine some scenario that isn't going

02:31:55.840 --> 02:31:59.760
well for companies where things are actually much harder than they might have expected.

02:32:00.480 --> 02:32:05.600
What does it look like for a takeoff to take less than a year? I guess things have to go really well.

02:32:05.600 --> 02:32:13.120
So I think a few things have to go well. So one possibility is that the difficulty gap is just

02:32:13.120 --> 02:32:18.400
pretty narrow. I mean, this isn't necessarily going well for anyone to be honest. And this is

02:32:18.400 --> 02:32:22.400
just a very intense and scary situation, but... Yeah, it's good clarification.

02:32:24.400 --> 02:32:30.880
If you only need 10 times as much effect to compute, to train 100% automation AI compared

02:32:30.880 --> 02:32:38.160
to 20% automation AI, then that could just be the algorithmic improvements in one year

02:32:38.160 --> 02:32:43.120
once the AI's are helping you to a significant degree. Right. Or it could just be spending

02:32:43.680 --> 02:32:48.880
three times as much on chips as you did the year before, and maybe those chips are three times as

02:32:48.880 --> 02:32:55.280
efficient as the year before. And so that, you know, a narrow difficulty gap alone could get you there.

02:32:55.920 --> 02:33:03.120
Another possibility is that once we hit 20% AI, there is just a very quick and significant

02:33:03.120 --> 02:33:09.760
increase in the money being spent on training runs. So if people get really excited, this is

02:33:09.760 --> 02:33:14.000
kind of a really awful scenario. People get excited about kind of what they could do with

02:33:14.000 --> 02:33:19.360
those capabilities, and they spend 10 times as much on a training run the next year, maybe by

02:33:19.360 --> 02:33:24.880
combining with some big existing compute providers, then that could allow you to cover,

02:33:24.880 --> 02:33:29.520
you know, a kind of slightly bigger difficulty gap just within a year. Maybe even if the difficulty

02:33:29.520 --> 02:33:36.880
gap was 100 times or 300 times much compute needed for AI, then you could still, by spending a lot

02:33:36.880 --> 02:33:41.760
more and combine with a few algorithmic and hardware improvements, cross it pretty quickly.

02:33:41.760 --> 02:33:47.920
Yeah, okay. And then, you know, one quick third possibility is just that, like I said earlier,

02:33:48.800 --> 02:33:52.240
but maybe I haven't emphasized this enough, is that this framework is assuming everything is

02:33:52.240 --> 02:33:57.680
continuous. So it's assuming that, you know, each time you increase the effect of compute

02:33:57.680 --> 02:34:02.400
in a training run by 10%, you get a kind of, you know, relatively modest incremental

02:34:02.400 --> 02:34:07.360
improvement in AI capabilities. If there's actually some kind of discrete phase change,

02:34:07.360 --> 02:34:12.400
then that could just be an alternative route that could produce a very fast takeoff. So there are,

02:34:12.480 --> 02:34:14.640
there are, you know, a few different ways that that could happen.

02:34:14.640 --> 02:34:20.480
Right. Okay, that's just very scary. So there are, I guess,

02:34:20.480 --> 02:34:26.000
worlds where things go very well for AI companies, but maybe very terribly for humanity,

02:34:26.560 --> 02:34:32.320
or kind of very poorly for AI companies, hopefully better for humanity. And the median

02:34:32.320 --> 02:34:38.400
estimate is something like a couple of years to get from AI can do 20% of cognitive tasks to AI

02:34:38.400 --> 02:34:43.520
can do 100% of them. That seems like, yeah, an important thing to take away from this model.

02:34:44.560 --> 02:34:48.320
Were there other things that you learned in this, in the process of writing this report?

02:34:49.200 --> 02:34:53.920
Yeah, another big update for me was the, that I think there's going to be a pretty strong

02:34:53.920 --> 02:35:01.440
correlation between how far away in time it is until we develop AGI and how fast takeoff will be.

02:35:02.960 --> 02:35:08.000
So in particular, if you have short AI timelines, meaning that you think we'll develop AGI pretty

02:35:08.000 --> 02:35:13.040
soon, then I think there are a few reasons to expect it, and especially fast takeoff.

02:35:13.920 --> 02:35:20.000
And so one reason is that if you have short timelines, and that's probably going to mean

02:35:20.000 --> 02:35:25.040
that you've got a smaller difficulty gap, because if you think that, you know, we don't need that

02:35:25.040 --> 02:35:30.480
much more effective compute to develop AGI compared with today, then you're probably also

02:35:30.480 --> 02:35:36.000
going to think that the difference in effect to compute for 20% AI to AGI is also going to be

02:35:36.000 --> 02:35:41.120
small. So that will push you towards a faster takeoff. Right. Okay. That makes sense.

02:35:41.760 --> 02:35:47.920
Another thing is that if you think AGI is going to be here fairly soon, then it's plausible that we

02:35:47.920 --> 02:35:55.440
won't be spending that much money on training runs shortly before we have AGI, which means it

02:35:55.440 --> 02:36:00.960
might be very practical and doable to quickly increase the amount that we're spending by maybe

02:36:00.960 --> 02:36:08.160
a factor of 10 or a factor of 100 just around the time that we are approaching AGI. Right. And so

02:36:08.160 --> 02:36:14.640
we could get a really very quick increase in the amount of effective compute being used on a training

02:36:14.640 --> 02:36:19.920
run as a kind of almost direct consequence of it being a short timeline. Whereas if timelines were

02:36:19.920 --> 02:36:24.080
long, then like I said, maybe we were already using all the chips in the world on the biggest

02:36:24.080 --> 02:36:28.960
training run, you know, before we get to AGI. And so that that source of growth is no longer

02:36:28.960 --> 02:36:33.760
available. Right. Yeah, that makes sense. Another thing with short timelines is that

02:36:34.320 --> 02:36:39.840
it implies that there's going to be a period of just a few years where we get really significant

02:36:39.840 --> 02:36:48.960
automation of AI R&D and kind of really significantly increase the size of the effective kind of

02:36:48.960 --> 02:36:54.720
research workforce that's working on improving AI due to these AIs. And if that happens very

02:36:54.720 --> 02:36:59.680
quickly is kind of you suddenly kind of five X the size of your research team, then you would

02:36:59.680 --> 02:37:05.360
expect that to just really significantly speed up progress. And that's just an especially kind of

02:37:05.360 --> 02:37:11.520
dramatic effect in short timelines. And the last point is that if if timelines are short,

02:37:11.520 --> 02:37:17.440
then there's less hope for eating up all the remaining possibilities for hardware progress

02:37:18.160 --> 02:37:22.320
and kind of running out of, you know, hitting those physical limits that we mentioned. Right.

02:37:23.040 --> 02:37:26.960
And, you know, analogously with algorithmic improvements, if timelines are short, then

02:37:26.960 --> 02:37:33.440
there's much less hope that we kind of run out of possible algorithmic improvements before we

02:37:33.440 --> 02:37:39.280
get there. So I do think that, you know, if, like I believe a lot of the labs do, they have

02:37:39.280 --> 02:37:46.160
pretty, pretty short timelines, kind of expecting AGI in the 2020s or early 2030s. And, you know,

02:37:46.160 --> 02:37:50.560
I think that the implication of that is that takeoff speed is going to be on the on the

02:37:50.560 --> 02:37:55.760
shorter end of what we've been discussing. So, you know, less than less than three years

02:37:55.760 --> 02:38:01.760
and very plausibly less than one year and, you know, one or two years being maybe maybe the most

02:38:02.400 --> 02:38:09.120
most likely. That's yeah, it's just it's just terrifying. I mean, we're talking about

02:38:09.920 --> 02:38:16.480
by the end of the decade, we've got AI that can do everything humans can do and probably more.

02:38:17.360 --> 02:38:24.080
Yeah. And we're talking about the labs who train them by default will have access to at least 100

02:38:24.080 --> 02:38:30.240
million of them that they can run and then probably soon after, you know, a billion or many

02:38:30.240 --> 02:38:36.480
billions that they can run. So it is it is pretty terrifying. Yeah. That's basically because it takes

02:38:36.480 --> 02:38:42.640
so much effective compute to run the training runs that once they've got AGI, they're just

02:38:43.280 --> 02:38:47.920
they can run millions or billions of copies on those chips. Exactly. So I think initially,

02:38:47.920 --> 02:38:53.040
they'll be able to run millions of copies. But because of the efficiency improvements

02:38:53.760 --> 02:38:57.760
that they can probably fairly quickly make, especially with all those copies working on it,

02:38:58.640 --> 02:39:05.840
I think it won't be long before they can do billions. It's really bewildering stuff. And

02:39:06.800 --> 02:39:13.280
I find yeah, I find it pretty my brain really doesn't want to believe it. I find it really,

02:39:13.280 --> 02:39:21.200
really hard to wrap my head around. Yeah. Any other takeaways? So another takeaway is that I think

02:39:21.200 --> 02:39:26.800
AGI is more likely to happen before 2040 compared to what I used to think. And so the reason for

02:39:26.800 --> 02:39:33.520
that is that the way I used to think about this is, okay, what does it take to train AGI? And

02:39:33.600 --> 02:39:39.600
how long will it be until we have that much effective compute? Whereas now the way I think

02:39:39.600 --> 02:39:47.520
about it is more, well, what will it take to train AI that is really profitable or really good

02:39:47.520 --> 02:39:52.640
at accelerating AI R&D? Right. And if we get either of those two things, then we that we'd

02:39:52.640 --> 02:40:00.000
expect that to accelerate future AI progress, such that, you know, kind of future progress is faster.

02:40:00.000 --> 02:40:06.560
And as long as AGI is incredibly hard, we are then able to reach AGI within the next couple

02:40:06.560 --> 02:40:11.120
of decades. So, you know, my current read, I just think it's really likely that by the by the end

02:40:11.120 --> 02:40:18.960
of this decade, we have AI which is really profitable and or significantly accelerates AI R&D.

02:40:19.520 --> 02:40:24.000
And so it's then kind of hard for me to imagine how that dynamic plays out without

02:40:24.000 --> 02:40:32.080
us getting to AGI by 2040. Wow. I think you just need AGI to be really hard. And as to fail to get

02:40:32.080 --> 02:40:37.200
it despite, you know, a huge amount of investment and help from AI systems and developing it.

02:40:37.760 --> 02:40:43.520
Right. Okay. So to make sure I understand the big thing doing the work there is you used to think

02:40:44.080 --> 02:40:51.360
about how hard it was to have humans figure out how to train AI systems to do everything humans

02:40:51.360 --> 02:40:57.680
can do. And now you're like, it's really just how hard is it to train AI systems to get really good

02:40:57.680 --> 02:41:04.880
at AI R&D. And that's like a much smaller subset of cognitive tasks. And so you can get really

02:41:04.880 --> 02:41:12.240
accelerating growth in AI capabilities just by making a lot of progress on that one set of tasks.

02:41:12.240 --> 02:41:16.800
Is that kind of right? Yeah, that's right. And I do think a distinct possibility is that we just

02:41:16.800 --> 02:41:22.560
train AI that isn't good at AI R&D, but makes lots and lots of money in the economy.

02:41:23.120 --> 02:41:27.760
And then there's just a bunch of investment in, yeah, I see in AI capabilities.

02:41:27.760 --> 02:41:33.200
And in designing better chips to run those AIs on. Got it. Okay. That makes that makes total sense.

02:41:34.000 --> 02:41:39.600
Any other takeaways? So the biggest other one, I think, is something we've touched upon, which is

02:41:39.600 --> 02:41:48.800
that the transition from roughly human level AI to significantly super human AI, I think is is

02:41:48.800 --> 02:41:53.840
going to be probably very quick. Right. So I think probably that's going to take less than a year.

02:41:54.640 --> 02:41:59.200
We have already touched upon the reasons why by the time we're at human level at AI,

02:41:59.840 --> 02:42:08.800
then AI's will be adding a huge, huge amount to the productivity of our work on AI R&D,

02:42:08.880 --> 02:42:12.640
designing better trips, designing better algorithms. And those things are already

02:42:12.640 --> 02:42:16.880
improving very quickly. So I only expect them to be going much, much faster.

02:42:17.600 --> 02:42:24.480
And then in addition, it's going to be quite clear that it's a very lucrative area, whether you care

02:42:24.480 --> 02:42:29.360
about discovering new technologies to help with climate change or discovering new technologies

02:42:29.360 --> 02:42:34.640
to help with improving human health, or you want to increase your country's kind of national power,

02:42:35.360 --> 02:42:42.560
then there's just going to be lots of reason to be investing in AI and designing smarter AI's.

02:42:43.120 --> 02:42:48.800
And so I kind of expect all of those three inputs, kind of the dollar spent on training and

02:42:48.800 --> 02:42:52.720
the quality of the AI chips and the algorithms to be improving really very quickly.

02:42:53.280 --> 02:42:59.280
Right. So there'll be incentives to go beyond just human human level capabilities, and then

02:42:59.920 --> 02:43:08.880
we'll have so much resources and just like AI labor to basically mean that we probably just

02:43:08.880 --> 02:43:13.680
shoot right past human level. Yeah. And I think that's in terms of plans for making the whole

02:43:13.680 --> 02:43:20.000
thing go well. It's especially scary because I think a really important part of the plan from my

02:43:20.000 --> 02:43:26.080
perspective would be to go especially slowly when we're around the human level, so that we can do

02:43:26.080 --> 02:43:31.840
loads of experiments and loads of kind of scientific investigation into, okay, so this human

02:43:31.840 --> 02:43:36.560
level AI, is it aligned if we do this technique? What about if we do this other alignment technique?

02:43:36.560 --> 02:43:42.160
Does it then, does it then seem like it's aligned and just really making sure we kind of fully

02:43:42.160 --> 02:43:47.200
understand kind of the science of alignment and can try out lots of different techniques

02:43:47.200 --> 02:43:53.280
and to develop kind of reliable tests for whether the alignment technique has worked or not,

02:43:53.360 --> 02:43:59.120
whether they're hard to game. The kind of thing that ARC has done with GPT-4, for example. Exactly.

02:43:59.680 --> 02:44:05.520
And I think if we only have a few months kind of through the human level stage,

02:44:05.520 --> 02:44:11.520
that stuff becomes really difficult to do without significant coordination in advance

02:44:11.520 --> 02:44:17.120
by labs. So I think that this, you know, there are really important implications of this,

02:44:17.120 --> 02:44:22.480
of this fast transition in terms of setting up a kind of government system which can allow us to

02:44:23.280 --> 02:44:27.600
go slowly despite the technical possibilities existing to go very fast.

02:44:28.480 --> 02:44:31.520
Yeah, yeah. No, that makes sense. I do feel like I've had some

02:44:32.480 --> 02:44:39.360
background belief that was like, obviously, when we've got AI systems that can do things humans can

02:44:39.360 --> 02:44:43.920
do, people are going to start freaking out and they're going to want to make sure those systems

02:44:43.920 --> 02:44:50.000
are safe. But if we, if it takes months to get there and then within another few months we're

02:44:50.000 --> 02:44:55.040
already well beyond human capabilities, then no one's going to have time to freak out or it'll

02:44:55.040 --> 02:45:00.320
be too late. Yeah. I mean, even if we spend the next, what do we have, seven years left in the

02:45:00.320 --> 02:45:07.840
decade? Like that sounds hard enough. Yeah. Yeah, I agree. Okay. So a takeaway is like,

02:45:07.840 --> 02:45:15.600
we really need to start slowing down or planning now, ideally both. Yeah. And we'll need the plans

02:45:15.600 --> 02:45:21.920
we make to really enable that to be mutual trust that, that the other labs are also slowing down.

02:45:21.920 --> 02:45:28.960
Because if it's, you know, if it only takes six months to, you know, make your AIs 10 or 100

02:45:28.960 --> 02:45:33.920
times as smart, then you're going to need to be really confident that the other labs aren't doing

02:45:33.920 --> 02:45:39.120
that in order to feel comfortable slowing down yourself. Right. If it was going to take 10 years

02:45:39.120 --> 02:45:43.760
and you noticed like three months in that another lab was working on it, you'd be like,

02:45:43.760 --> 02:45:47.440
yeah, we can catch up. Yeah. But if it's going to take six months and you're three months in,

02:45:48.160 --> 02:45:53.680
you've got no hope. And so maybe you'll just like spend those first three months secretly working on

02:45:53.680 --> 02:45:59.520
it to make sure that doesn't happen. Or just not agree to do the slowdown. Yeah. Oh, these are really

02:45:59.520 --> 02:46:04.560
hard problems. I mean, it's very, it feels very like prisoner's dilemma. I'm hoping it's going to be

02:46:04.560 --> 02:46:09.440
more like an iterated prisoner's dilemma, where there's kind of multiple moves that the labs make

02:46:09.440 --> 02:46:14.560
one after the other. And they can see if the other labs are cooperating. And in an iterated

02:46:14.560 --> 02:46:19.600
prisoner's dilemma ultimately makes sense for everyone to cooperate, because that's that way

02:46:20.160 --> 02:46:24.240
they can, the other people can see you coordinating, then they coordinate and then everyone kind of

02:46:24.240 --> 02:46:29.120
ends up coordinating. You know, one thing is if you could set up ways for labs to easily know whether

02:46:29.120 --> 02:46:35.680
the other labs are indeed cooperating or not kind of week by week, then that turns it into a more

02:46:35.680 --> 02:46:40.560
iterated prisoner's dilemma and makes it easier to achieve a kind of good outcome. Yeah, yeah,

02:46:40.560 --> 02:46:46.720
that makes sense. I imagine it's the case that the more iteration you get in an iterated prisoner's

02:46:46.720 --> 02:46:54.240
dilemma, the better the incentives are to cooperate. And so just by making the timeline shorter,

02:46:54.240 --> 02:46:58.320
you you make it harder to get to get these iterations that build trust. Yeah, I think that's

02:46:58.320 --> 02:47:06.240
right. So it sounds like there's this range that's maybe between one and 10 years, maybe a bit

02:47:06.240 --> 02:47:13.440
shorter or a bit longer at the at the extremes. But that's in particular for AI systems having

02:47:13.440 --> 02:47:20.480
the capability to perform all the tasks that humans currently do, not that they're actually

02:47:20.480 --> 02:47:27.120
automating all those tasks and replacing humans. How big of a lag do you expect there to be between

02:47:27.120 --> 02:47:31.920
AI systems having capabilities and AI systems actually being used in the world?

02:47:32.880 --> 02:47:38.960
So I think it will vary, according to a few things. So one of the things is what industry are we

02:47:38.960 --> 02:47:47.440
talking about? So I think for industries that are very public facing, customer facing, and

02:47:48.160 --> 02:47:53.840
highly regulated, you'd expect there to be bigger delays between AI being able to automate the work

02:47:53.840 --> 02:47:58.960
and it actually being automated. Whereas for more back end parts of the economy, like R&D,

02:47:59.920 --> 02:48:07.280
like manufacturing, like logistics and transportation, then I think there would be less of a delay.

02:48:07.280 --> 02:48:13.520
Okay, yeah. I also just expect it to differ from company to company based on how innovative

02:48:13.520 --> 02:48:18.240
those organizations are. Sure, their internal culture, whether they're the type of company

02:48:18.240 --> 02:48:23.280
who's going to be like, let's integrate GPT-4 into all of our processes now. Yeah.

02:48:24.160 --> 02:48:30.000
For the purposes of the predictions of this model, the really important thing is about the delay to

02:48:30.000 --> 02:48:38.320
using AI to improve AI algorithms and to improve AI chip design. And I think those are probably

02:48:38.320 --> 02:48:45.440
areas where the lag will be very much on the shorter end. Right. And that's because AI researchers

02:48:45.440 --> 02:48:54.160
can use them without needing them to be functioning super well for public users who might be like,

02:48:54.160 --> 02:48:58.160
what the heck, it gave me this weird result. That's confusing and weird and looks bad on your

02:48:58.160 --> 02:49:03.040
company. They can just use them in the background, check that they work, and they don't need to

02:49:03.040 --> 02:49:08.240
wait for them to be super polished. Right, yeah. There'll also probably be more aware of AI

02:49:08.240 --> 02:49:14.720
developments because that's the kind of industry they work in. Yeah, right. They are probably less

02:49:14.720 --> 02:49:22.160
regulated. So I think a few factors. Another thing that could kind of affect how long that lag is

02:49:23.040 --> 02:49:29.840
is actually the capability of AIs themselves. So even if it would be possible with kind of

02:49:29.840 --> 02:49:34.960
six months effort to integrate some AIs into your workflow, by the time AIs are kind of,

02:49:35.680 --> 02:49:41.120
let's say superhuman in their capabilities, then maybe they're able to integrate themselves

02:49:41.120 --> 02:49:48.240
into the workflow. And so that kind of upfront cost of adopting AI is now extremely low.

02:49:49.280 --> 02:49:52.720
And so I think that you could see that lag time reducing over time.

02:49:53.280 --> 02:49:58.240
Yeah, okay. So yeah, we're nearing the end of our time, but I guess to take a step back,

02:49:58.240 --> 02:50:03.440
we've talked about some pretty insane sounding stuff today, robot workforce and AI takeover.

02:50:03.440 --> 02:50:09.040
And yeah, I'm curious, were these kinds of arguments about the potential risks from AGI ever

02:50:09.040 --> 02:50:15.600
farfetched to you? Or yeah, did they make sense to you kind of right away? Oh, yeah, for sure,

02:50:15.600 --> 02:50:22.480
they were farfetched. I've kind of been through a few phases in my own relationship to the arguments.

02:50:23.280 --> 02:50:29.040
I think at first I read Superintelligence and it kind of made sense to me. I wasn't exactly

02:50:29.120 --> 02:50:34.320
sure what to do with it. But it seemed plausible enough that if we had Superintelligent AIs,

02:50:34.320 --> 02:50:43.440
then things could go bad for us. Then there was a period in 2020 when a few counter arguments to

02:50:43.440 --> 02:50:47.920
the case in Superintelligence came out. And I kind of thought, oh, wow, yeah, these arguments were

02:50:48.720 --> 02:50:54.880
weaker than I had realized. And I felt a little bit disillusioned with them. That's a piece from

02:50:54.880 --> 02:51:03.360
Ben Garfinkel and one from Tom Adam Shrazy on AI risk. But in the last two or three years,

02:51:04.080 --> 02:51:09.200
thinking through the arguments in more detail, I've come to think that like somewhat adjusted

02:51:09.200 --> 02:51:14.320
versions of the argument in Superintelligence are still very plausible. And though I don't think

02:51:14.320 --> 02:51:18.720
it's like 100% that we're doomed and there's no way that we could align these systems,

02:51:19.600 --> 02:51:24.240
it does just seem pretty plausible to me that the evidence about whether they're aligned is very

02:51:24.240 --> 02:51:31.600
ambiguous. There's competitive dynamics, pushing people to go forward in the face of that ambiguity

02:51:31.600 --> 02:51:36.960
and that we just really dropped the ball. Right. Okay. And so I guess at some point in there,

02:51:38.320 --> 02:51:43.680
you decided to leave teaching and try to work on AI full time. Is that basically right?

02:51:44.560 --> 02:51:53.840
Yeah, that's right. What was that like? It was a tough decision. I actually left teaching partway

02:51:53.840 --> 02:52:00.480
through the academic year. So I did feel I did feel bad about them. I did feel I was abandoning

02:52:00.480 --> 02:52:05.760
my pupils, because that's not a normal time to leave. And I still do feel bad about that.

02:52:06.880 --> 02:52:13.200
It does mean that I've kind of got to where I have a favor earlier than I would have. So I don't

02:52:13.200 --> 02:52:19.200
unambiguously regret it, but it was a tough decision. And it had downsides. Right. I was, you

02:52:19.200 --> 02:52:24.400
know, I was listening to things like the ATK podcast and reading other things. And I was convinced

02:52:24.400 --> 02:52:29.840
about these arguments for AI risk and long termism in general, being plausible enough that it was

02:52:29.840 --> 02:52:37.920
worth leaving teaching. Yeah, well, that sounds, yeah, I guess both really hard and also on my

02:52:37.920 --> 02:52:44.720
views. Yeah, really, really lucky for us. Yeah, I'm glad. I'm glad we have you working on this stuff.

02:52:45.520 --> 02:52:52.640
So we don't have much time left. So I'd love to ask a final question. I got an insider tip to ask you

02:52:54.000 --> 02:52:58.480
what ants can teach us about AI? Yeah, what's the story there?

02:52:59.040 --> 02:53:03.760
So yeah, ants are really incredible creatures. I've been I've been learning about reading this

02:53:03.760 --> 02:53:10.240
book called ant interactions. Okay. So I mean, ants have been around longer than the dinosaurs,

02:53:10.240 --> 02:53:14.720
I think 120 million years they've been around for. Wow, I didn't know that.

02:53:14.720 --> 02:53:20.880
And they're like one of the most prolific and successful species on the planet. So one stat

02:53:20.880 --> 02:53:26.800
in this book was that if you, if you weighed all the ants in the Amazon rainforest and put them on

02:53:26.800 --> 02:53:34.080
the scales, they would weigh twice as much as all of the land animals combined. That's all the mammals,

02:53:34.080 --> 02:53:39.920
amphibians, birds, reptiles. I thought you were going to say twice as much as humans when I was

02:53:39.920 --> 02:53:46.080
like, wow. That's the other that's just counting only ants and animals in the rainforest in the

02:53:46.080 --> 02:53:54.480
rainforest. Unbelievable. Okay. And another fact about ants is that even compared to the other

02:53:54.480 --> 02:54:01.200
insects, they weigh 30% as much as all insects combine, which is given that they're only 2%

02:54:01.200 --> 02:54:07.440
of the species of insects. They it's kind of testament to how how successful and prolific they

02:54:07.440 --> 02:54:13.840
are. Yeah. Different ant varieties all around the world are incredibly diverse, like something kind

02:54:13.840 --> 02:54:20.560
of glide through the air. Some of them are very aggressive. Some of them are not at all. Some of

02:54:20.560 --> 02:54:24.960
them kind of bump into each other at much higher rates than others. And they have kind of very

02:54:24.960 --> 02:54:31.520
different strategies and environments that they work in. And the link with AI is a little bit

02:54:31.520 --> 02:54:36.480
tenuous, to be honest. I'm mostly just reading this book out of interest. But in an ant colony,

02:54:37.280 --> 02:54:42.720
ants are smarter than like a human sellers. They're the kind of self contained units that, you know,

02:54:42.720 --> 02:54:48.880
eat and do tasks by themselves. And they're pretty autonomous. But the ants are still pretty dumb.

02:54:49.520 --> 02:54:56.800
And no ant really knows that it's part of a colony or knows that the colony has

02:54:56.800 --> 02:55:00.240
certain tasks that it needs to do and that it has to help out with the colony efforts.

02:55:01.040 --> 02:55:06.400
It's more like a kind of little robot that's kind of like bumping into other ants and getting like

02:55:06.560 --> 02:55:10.080
signals and then adjusting its behavior based on that interaction.

02:55:10.720 --> 02:55:18.320
Right. So it's not like, I guess, a company where like the different people in the company are like,

02:55:18.320 --> 02:55:24.240
my job is marketing. And they have like a basic picture of how it all fits together.

02:55:24.240 --> 02:55:29.120
They're much more like, if a person at a company doing marketing was just like,

02:55:29.120 --> 02:55:30.800
I don't know why I do it, I just do it.

02:55:30.800 --> 02:55:35.200
Yeah, exactly. And another disadvantage with a company is that a company, there's someone at

02:55:35.200 --> 02:55:39.680
the top that's kind of coordinating the whole thing. Whereas with ants, there's no one that's

02:55:39.680 --> 02:55:47.120
coordinating it. It's just including the queen, there's no management system. It's just all of the,

02:55:47.120 --> 02:55:51.680
you know, hundreds and thousands of ants have their individual instincts of what they do when

02:55:51.680 --> 02:55:56.000
they bump into each other and what they do when they bump into food and what they do when they

02:55:56.000 --> 02:56:01.840
realize that there's, you know, there's not as much food as there needs to be. And by kind of all

02:56:01.840 --> 02:56:06.320
of the ants following their own individual instincts, it just turns out that they act

02:56:06.320 --> 02:56:11.840
as if they were kind of a fairly well coordinated company that is like ensuring that there are

02:56:11.840 --> 02:56:16.800
some ants going to get food and some ants that are keeping the nest in order and some ants that

02:56:16.800 --> 02:56:22.800
are feeding the young. But that that coordination happens kind of almost magically and emerges

02:56:22.800 --> 02:56:29.440
out of those individual ant interactions. So one example of how this works is that

02:56:30.320 --> 02:56:36.720
if an ant comes across a body of a dead ant, then if there's another dead body nearby or

02:56:36.720 --> 02:56:41.040
tend to move it to be close to the other dead body, that's just an instinct it has. It just kind

02:56:41.040 --> 02:56:46.000
of moves the body towards another. And if there's like one kind of pile of three dead ants and

02:56:46.000 --> 02:56:50.320
another pile of two dead ants or tend to go towards the bigger pile, so tend to move this

02:56:50.320 --> 02:56:54.960
extra dead ant towards the pile of three. And then it turns out that if all the ants just have

02:56:54.960 --> 02:56:59.680
those instincts, then if there's initially a kind of a kind of sprawling mass of dead bodies

02:56:59.680 --> 02:57:03.440
everywhere, then those dead bodies will be collected into just a small number of piles

02:57:04.160 --> 02:57:11.600
of bodies. And it's not like any of the ants are like I am the grave digger or the like keeper of

02:57:11.600 --> 02:57:18.400
the cemetery. They just have like really weird like baseline rules that are like move the smaller

02:57:18.400 --> 02:57:23.520
group of dead ants to the to where the larger group of dead ants are. Yeah, exactly. So they

02:57:23.520 --> 02:57:29.200
don't have to know that the whole point of this instinct is to kind of clear the ground so that

02:57:29.200 --> 02:57:33.200
it's easier to do work in the future. It's just an instinct they have. They don't have to know that

02:57:33.200 --> 02:57:37.600
when everyone follows that instinct, this is the resultant pattern of behavior. And similar

02:57:37.600 --> 02:57:43.280
instincts kind of cause them to go for food when food is available. So if they see many ants coming

02:57:43.280 --> 02:57:49.280
in with food, that raises the probability that they'll go out and look for food. And they're not

02:57:49.280 --> 02:57:53.200
thinking oh, there's food to be gathered. There's clearly a lot of it. So we better reassign some

02:57:53.200 --> 02:57:58.000
labor towards food gathering that they just have that basic instinct, which causes them to go out

02:57:58.000 --> 02:58:03.840
and help out with the food gathering. And it's something like, oh, that ant has food. Oh, another

02:58:03.840 --> 02:58:10.160
ant has food. I'm going to go that way. Yeah, exactly. Right. How does this connect to AI?

02:58:10.800 --> 02:58:16.640
So I don't know if it does connect very, very directly at all. But the idea of the connection

02:58:16.640 --> 02:58:25.120
in my head is that it's an example of a system where lots of kind of less clever individuals

02:58:25.120 --> 02:58:29.840
are following their local rules, doing their local task. But that what emerges from that

02:58:29.840 --> 02:58:35.360
is a very coherent and effective system for ultimately gathering food, defending against

02:58:35.360 --> 02:58:42.960
predators, raising the young. And an analogy would be maybe we think it's pretty dangerous to train

02:58:43.040 --> 02:58:49.120
really smart AIs that are individually very smart. But it might be safer to kind of set up a team of

02:58:49.120 --> 02:58:55.840
AIs such that each AI is kind of doing its own part in a kind of team and doesn't necessarily

02:58:55.840 --> 02:59:00.560
know how how its work is fitting into the broader whole. But nonetheless, you can maybe get a lot

02:59:00.560 --> 02:59:06.880
more out of that kind of disconnected team of AIs that are specialized and that just kind of take

02:59:06.880 --> 02:59:11.600
the inputs and produce the outputs without much of an understanding of the broader context. And

02:59:12.160 --> 02:59:18.320
just thinking maybe that would be, you know, a safer way to develop advanced AI capabilities

02:59:18.320 --> 02:59:22.320
than just training one super smart AI mega brain.

02:59:22.320 --> 02:59:30.800
Right. Cool. I love that. I mean, who knows if it'll work. But I mean, that just that makes tons

02:59:30.880 --> 02:59:44.800
of sense to me. You don't have GPT-4 or GPT-40. You have a bunch of much dumber AI systems that can

02:59:44.800 --> 02:59:52.000
coordinate together to be just as helpful as GPT-40, but that individually couldn't do

02:59:52.720 --> 02:59:58.880
most of the things the other systems can do. And so collectively, they can't do anything like

02:59:58.880 --> 03:00:06.160
escape from their box and find another computer to take over. And is that basically the idea?

03:00:06.160 --> 03:00:07.360
Yeah, that is the hope.

03:00:07.360 --> 03:00:12.800
That's lovely. That's really, really cool. Great. Well, I should let you go. That's

03:00:12.800 --> 03:00:17.280
all the time we have. But thank you so much for coming on the show, Tom. It's been such a pleasure.

03:00:17.280 --> 03:00:19.280
It's been really fun, Louisa. Thank you so much.

03:00:31.920 --> 03:00:37.520
All right. If you liked that episode, we'll have more on this issue for you soon. But in the meantime,

03:00:37.520 --> 03:00:42.160
I can recommend going back and listening to some of our best past episodes about artificial

03:00:42.160 --> 03:00:48.400
intelligence, including episode 141, Richard Newell on large language models, open AI, and

03:00:48.400 --> 03:00:53.920
striving to make the future go well. There's also episode 132, Nova Dasama on why information

03:00:53.920 --> 03:00:59.360
security may be critical to the safe development of AI systems. Episode 107, Chris Ola on what

03:00:59.360 --> 03:01:03.840
the hell is going on inside neural networks. Episode 92, Brian Christian on the alignment

03:01:03.840 --> 03:01:08.960
problem. And an oldie but a goodie, episode 44, Paul Cristiano on how open AI is developing

03:01:08.960 --> 03:01:13.360
real solutions to the AI alignment problem, and his vision of how humanity will progressively

03:01:13.360 --> 03:01:18.880
hand over decision making to AI systems. Bit of a run on title there. I think that might be my

03:01:18.880 --> 03:01:23.760
fault. But it is an excellent interview. All right. The 80,000 Hours podcast is produced and

03:01:23.760 --> 03:01:27.680
edited by Kieran Harris. Audio mastering and technical editing by Simon Monsour and Ben

03:01:27.680 --> 03:01:31.760
Cordell. Full transcripts and extensive collection of links to learn more are available on our site

03:01:31.760 --> 03:01:46.560
and put together. As always, thank you. Thanks for joining. Talk to you again soon.

