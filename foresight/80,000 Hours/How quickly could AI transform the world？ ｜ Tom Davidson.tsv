start	end	text
0	4160	Hi listeners, this is the 80,000 hours podcast where we have unusually in-depth conversations
4160	7680	about the world's most pressing problems, what you can do to solve them, and what did happen to
7680	12400	all those horses after we invented cars. I'm Rob Wiblin, head of research at 80,000 hours.
13200	17920	The last few months have been a crazy time for advances in artificial intelligence.
18640	22960	Over the last couple of years, I've become, for better or worse, increasingly confident that
22960	28560	the future is going to be shaped in a major way by what sorts of AI systems we develop and choose
28560	33920	to deploy as they approach and then exceed human capabilities in the most important areas.
34640	40400	That's always seemed like a really common sense idea to me, but it now is becoming pretty apparent
40400	44720	to people across society who until recently had hardly been paying attention to this issue.
45520	50080	Unfortunately, we've only had two episodes about AI over the last six months due to the
50080	54800	substantial lags that come in between conceiving of an episode, finding the right guest,
54800	59600	scheduling and recording the conversation, and then editing it, and then finally releasing it.
59600	62880	But that scarcity of AI content is fortunately about to change.
63760	68800	In today's episode, Louisa Rodriguez interviews Tom Davidson, a senior research analyst at Open
68800	74800	Philanthropy, whose job is figuring out when and how society is going to be upended by advances in
74800	80720	AI. Not a simple forecasting exercise. Tom has his work cut out for him. But if you've been wondering
80720	85840	when you might be replaced in your job by an AI model and when AIs might be able to do everything
85840	90240	that humans can do for less than what it costs to feed a person and keep them alive, then this
90240	94640	episode will help you think about that a little bit more clearly and perhaps have a little bit
94640	100560	more idea of what to expect. In particular, Louisa and Tom discuss how long it will take for AIs to
100560	105840	go from being able to do 20% of the work humans are doing to being able to do all of it, what
105840	110560	underlying factors are driving progress in AI and how much each of those factors is contributing.
111120	115200	Whether we should expect that progress to speed up or slow down in incoming years,
115760	120640	how much computer hardware is used to train these models and whether it can continue increasing at
120640	125600	the absolutely blistering rate that it has been doing the last 10 years, when AI systems might
125600	130720	be able to do scientific research and what implications that would then have, and also when
130720	136240	we might expect all of this to noticeably increase GDP in a visible way and what that could look like
136240	141440	and then what new bottlenecks might exist in an economy where AI systems were doing most of the
141440	147680	work. And on top of that, plenty of other issues besides. Tom's expectations for the future are
147680	152400	exciting or alarming, I guess, depending on how you want to look at them. Regular listeners will
152400	158640	have heard me do quite a few interviews on AI over the years, but those interviews tend not to
158640	163680	focus on my opinions, at least they're not meant to. So it's possible that people don't have that
163680	168960	much of a sense of where I stand on this personally. In case you're interested, I think
168960	175280	the chances that you and I are either killed due to actions taken by AI systems, or that we live
175280	181280	to see humanity unintentionally lose control of its future because of AI systems are greater than
181280	187280	10%, greater than a one in 10 chance. Looking at surveys and polling now, it seems like both AI
187280	192320	experts and the general public are converging on a view that's not too far from that. It's a view
192320	198320	that previously seemed idiosyncratic, but now is really quite mainstream. Naturally, if that if
198320	204240	the odds are like that, then it makes figuring out how to safely develop and deploy AI, you know,
204240	208880	probably the issue of our time. And indeed, one of the things that we should most care about from
208880	213840	even a selfish point of view, or if we have children care about from a parental point of view.
214480	219440	I'm an economist by training and I entirely understand how the industrial revolution
219440	225200	ultimately raised incomes across generations. And that while factory automation was indeed
225200	230480	financially ruinous for plenty of individuals, it didn't then result in persistent unemployment.
230480	234880	But nonetheless, despite understanding and appreciating all of that, I am skeptical that
234880	240400	there are going to be paying jobs for children being born today, or if not them, that they'll be
240400	245440	paying jobs at least for their children. And we are just we're flying by the seat of our pants
245440	250080	here and have not really figured out a plan ahead of time about what we're going to do
250080	255280	as this technology just completely upends I think existing social relations and economic systems.
256400	262240	Well, I'll describe the overall situation that humanity finds itself in to be pretty terrifying.
262240	267520	The fact that all kinds of different people are waking up now to the risks here does give me hope
267520	272400	that we can coordinate to prevent the worst. As you imagine, we'll have more to say to
272400	278240	expand on all of that in in future episodes. But for now, I bring you Louise Rodriguez and Tom Davidson.
293120	297600	Today, I'm speaking with Tom Davidson. Tom's a senior research analyst at Open Philanthropy,
297600	303040	where his main focus is on when we might get transformative AI. Before joining Open Philanthropy,
303040	306880	Tom taught science through Teach First at a comprehensive school in East London,
306880	311280	and then was a data scientist for an education technology startup. And before all of that,
311280	315040	Tom studied physics and philosophy at Oxford. Thanks for coming on the podcast, Tom.
315600	317200	Thanks, Louise. It's a pleasure to be here.
317840	326640	So I hope to talk about how fast we might go from kind of OK AI to AI that can do everything humans
326640	333760	can do, plus how that'll affect the economy and the world. But first, yeah, how worried are you
333760	340160	personally about the risks from AI? So about a year ago, I sat down and spent spent a morning
340160	345680	trying to figure out, you know, what are my percentages of AI by certain time and what's
345680	352400	my percentage that there's existential catastrophe from AI. And so I was focusing on the possibility
352400	358880	that AI disempowers humanity and just takes over control of of society and the economy and
358880	363520	and then what happens in the future. And a year ago, I landed at a number that was a bit above
363520	372960	10% for the probability that AI takes over by 2070. Right. OK. I mean, that's already pretty high.
374000	377360	Yeah, that is that is already very high and much too high. Yeah.
378320	383440	I think since since then, if I if I read the exercise today, I think I'd be close to 20%
385440	390080	I think I compare to then I think it's just more likely that we develop AI that's capable
390080	395840	of doing that by 2070. I also think that it's it's just pretty likely to happen in the next 20 years.
397520	402240	Which then makes the chance that it goes badly, I think higher because we have less time to repair.
402240	407520	So both those things, I think mean I'd probably be at about 20% if I if I read the exercise today.
408640	411360	Yeah, we'll talk more about a couple of those things. But first,
412560	418160	yeah, I'm curious if there's a type of scenario you have in mind when you're thinking about that
418160	426320	10 to 20% that makes you, yeah, especially worried. Yeah, I think the main line scenario I have
426880	437040	is something like in the next 10 to 15 years, possibly sooner, we train an AI that is able to
438320	445120	massively enhance the productivity of AI R&D workers. So people who are currently working to
445120	450960	make AI better, maybe it makes them let's say five times as productive. So something like a large
450960	457360	language model helps people working on AI R&D in particular, like code much faster or develop
457360	463440	better algorithms or something. Exactly. And it means they can work five times faster ish.
463440	469440	Exactly. Right. Okay. And then I think it won't be long after that, because AI is then going to be
469440	475760	improving more quickly before AI is able to do everything that the current employees of DeepMind
475760	480720	and OpenAI are currently doing for their jobs, at least the ones that can work remotely
480720	485360	and do their work from a computer. Are you kind of distinguishing between the ones that work
485360	491760	with like physical robots or the ones that work with like, I don't know, the mail room at DeepMind?
492640	497440	Yeah. So if there's someone, I don't know if this is true, but if there's someone at DeepMind
497440	502720	who is physically stacking the computer chips into the data center, then that's a kind of
502720	507200	type of physical work, which I don't think would necessarily, you know, follow immediately on.
507920	512480	But I think most of the work is not like that. Right. So I think AI, you know,
512480	517840	especially most of the work for, you know, advancing AI is stuff that you can do on your laptop.
517840	524160	So first we get the kind of 5x productivity gain. A little time later, we get AI that can do all of
524160	529360	the work that people at OpenAI and DeepMind can do. And I really think the gap between those is
529360	534000	not going to be very big. And we'll probably discuss a bit more about that later. At that point,
534000	540880	I think AI is going to be improving at a blistering pace, absent a very specific effort to slow down,
540880	546080	which I hope we really make, but absent that effort and absent coordinating to make sure that
546080	551360	everyone is slowing down. You know, I think like a thousand next improvement in the AI's capabilities
551360	561120	in a year is like a natural kind of conservative default. Wow. And so in this scenario, it wouldn't
561120	568640	be long before the AI is just far outstripping the cognitive intelligence abilities of the smartest
568640	574080	humans and indeed even the smartest massive teams of humans working together. Wow. And when
574080	578400	you kind of crunch the numbers on how many AI's there's likely to be around this time,
578400	584080	there's going to be hundreds of millions and probably many billions of human worker equivalents
584080	590560	just in kind of AI cognitive ability. Is that because there are different AI systems or because
590560	597600	you've got that much brain power being deployed like through AI, even if it's only like five AI
597600	603520	systems or something? It's like running lots of copies of maybe a few AI systems. Got it. So
604080	607600	for example, I haven't crunched the numbers on this, but I would guess that if you took the
608320	613280	kind of computer chips that they use to train GPT-4 and then you just said how many copies of
613280	618880	GPT-4 could we run using these computer chips, I guess that the answer is like maybe a million.
619840	625280	Wow. It might be a bit lower actually. I'm not I'm not sure for GPT-4, but I think by the time that
625280	632960	we train kind of AI that can fully replace all the human workers at an AI lab, I think it's
632960	637440	that number is going to be more extreme. Right. So I think that by the time we can train that kind
637440	643120	of AI, you'll be able to just immediately use the compute that you use for training to run 100
643120	648960	million. Right. That's okay. That's insane. And the reason for that is because it just takes,
649920	657040	I don't know, like many times more compute to train an AI system than it does to then run them.
657760	663760	Yeah, that's exactly right. Okay. One way to think about it is that the AI is trained on kind of
663760	668960	millions of days of experience so that they get to be as good as they are. And that one,
668960	672160	if you've managed to train them for that much, then it's kind of obviously you can run millions.
672160	680560	Yeah. Okay. Got it. Okay. Okay. So then we're there. We've got potentially millions of human
680560	687200	equivalents of copies of AI systems running and doing the kind of work that humans could do
688240	694000	at the human level or better. Exactly. And then what? So then I think it's likely that it won't
694000	699120	be long until we're talking about billions of AI systems. It's run of the mill to see 3x efficiency
699120	705840	improvements in AI at various different levels of the software stacks. You could get a 3x efficiency
705840	713440	in kind of how effective the algorithm is or a 3x efficiency improvement in how well the software
713440	718560	runs on the hardware. And there are these various layers of the software stack that you can make
718560	723920	improvements on. So I think it's probably at that point, once you have hundreds of millions of AI
723920	728480	is looking for these types of improvements, you're probably going to get very, very quick,
728560	734880	further improvements in AI cognitive ability. Again, maybe we coordinate to go very slowly,
734880	740320	but this is absent that targeted coordination. Maybe the default. Yeah. Maybe the default
740320	748000	scarily. And at that point, I think that if the AIs are misaligned, if they have goals that are
748000	754320	different to what humans want them to do, and if those goals imply that it would be useful for them
754320	758160	to get power so they could achieve those goals better, then I don't think it's going to be very
758160	765520	hard for them to do that. Because it's like we've got a billion really smart humans who want to take
765520	770880	power. Probably they'll find out some kind of way to do it. Maybe they invent new technology,
770880	776000	maybe they convince some higher officials to give them control of the military. I'm not sure
776000	780720	exactly how they'll do it. But at that point, I think it's kind of too late for us to be
781360	788320	preventing AI takeover. Okay. And then by AI takeover, do you mind spelling that out?
789280	795440	Yeah. So I think the thing that matters for AI takeover is that AI systems collectively end up
795440	800960	in control of what happens in the future, and that it's kind of their goals and decisions that
800960	808160	dictate the future path, and that it's no longer sensitive to what humans want or trying to achieve
808160	813280	with the future. So I mean, ultimately, I think it does have to come down to physical force,
813280	817600	most likely. I mean, you could imagine the scenario where the AI just convince humans and
817600	821920	hypnotize them or something, and that's how they take over. But more likely they end up
821920	827680	having control of the hard military equipment, and that's what allows them to establish
828640	836240	their power and disempower humanity. So a thing that I have to admit still confuses me
836240	845600	is just how we go from, I don't know, things like GPT-4, which even if it sometimes gets
845600	852320	super confused and says silly things in a way that's like, oh, you clearly misunderstood
852320	860640	what I was asking for, it's really hard for me to understand what the path to that kind of confusion
861200	871120	to a misalignment that's so, I don't know, I guess just incredibly diverging from human values
871120	879040	that the AI systems want to disempower humans. And I mean, one article I read,
879040	885040	I think that just came out recently on Vox, is this article that was actually making the case
885040	889760	that companies creating AI should slow down, should kind of coordinate to slow down,
891040	896240	was walking through the case for why we might expect AI to be misaligned. And the example they
896240	903680	gave just still confuses me. So the example is something like, let's say you've got a super
903680	911200	smart AI system, we've programmed it to solve impossibly difficult problems, like calculating
911280	916240	the number of atoms in a universe, for example, the AI system might realize that it could do a
916240	920560	better job if it gained access to all of the computers on earth. So it releases a weapon of
920560	926640	mass destruction to wipe out all humans, for example, an engineered virus that kills everyone
926640	932240	but leaves infrastructure intact. And now it's free to use all the computer power. And that's
932240	938240	the best way it's able to achieve its goal. And I think I just, I just really, really, I like,
938240	944240	I feel silly. I feel, I feel dumb. I feel like I'm missing something. Like, how will it go from
944240	951440	like, I want to solve this problem for humans to like, I'm going to kill them all to take their
951440	958800	resources so that I can solve the problem. Like, why have we not ruled that kind of extreme behavior
958800	965120	out? Great question. So let's maybe we can try and think about this, this system, which is trying
965120	972800	to solve these these math problems. So maybe the first version of the AI, you just say, look,
973600	980000	we want you to solve the problem using one of these four techniques. And that kind of system is okay.
980560	985520	But then someone comes along and realizes that if you let the AI system do an internet search
986080	993600	and plan its own line of attack on the problem, then it's able to do a better job in solving
993680	997360	even harder and harder problems. And so you say, okay, we'll allow the AI to do that.
998560	1003440	And then over time, in order to improve performance, you give it more and more scope
1003440	1010000	to kind of be creative in planning how it's going to attack each different kind of problem.
1011360	1018400	One thing that might happen internally inside the AI's own head is that the AI may end up
1019200	1025200	developing just an inherent desire to just get the answer to this math question as accurate as
1025200	1030880	possible. That's something which it always gets rewarded for when it's being trained.
1031600	1035280	And you know, maybe it could be thinking, I actually just want the humans to be happy with
1035280	1038400	my answer. But another thing it might end up thinking is, you know what, what I really want
1038400	1043760	is just to get the answer correct. And the kind of the feedback that as humans are giving that
1043760	1049040	system doesn't distinguish between those two possibilities. So maybe we get unlucky and maybe
1049040	1054240	the thing that it wants is to just really get the answer correct. And maybe the way that the
1054240	1058800	AI system is working internally is it's saying, okay, that's my goal. What plan can I use to achieve
1058800	1063360	that goal? And it's kind of creatively going and looking for new new approaches by googling
1063360	1069440	information. Maybe one time it's like, it realizes that if I hacked into a kind of another computing
1069440	1075040	cluster, it could use those computations to help itself the problem. And it does that no one realizes
1075040	1080640	and then that kind of that reinforces the fact that it is now planning on such a broad scale to
1080640	1086880	try and achieve this goal. And then maybe it's much more powerful at a later time. And it realizes
1086880	1092400	that yeah, if it kills all humans, it could have access to all the supercomputers. And then that
1092400	1098000	would help it get an even more accurate answer. And because the thing it cares about is not pleasing
1098080	1102800	the humans, the thing it happened to care about internally was actually just getting an accurate
1102800	1108720	answer. That plan looks great by its own lights. And so it goes and executes the plan.
1108720	1118880	Right. So one that was really helpful. But I still feel confused about one why it's so hard to
1120080	1126400	not give it some instructions that are just like, use whatever you need, but like don't hurt living
1126480	1132880	things. So I think we could definitely give it those instructions. The question is,
1133520	1139120	inside its own mind, what is its goal in the end of the day? So you could give it instructions,
1139120	1142480	don't hurt humans, and it would read that and understand that that's what you wanted.
1143600	1147840	But if throughout its life, it's always been rewarded for getting an accurate answer to
1147840	1153280	these math problems, it might just itself only care about getting our accurate answers to the
1153360	1159920	math problems. So it knows that the humans don't want it to hurt other humans. But it also doesn't
1159920	1163920	care about that itself, because all it cares about is getting accurate answers to this problem.
1163920	1169040	And so sure, it knows that humans don't want it to hurt other humans. And so it makes sure to not
1169040	1174480	do that in an obvious way, because it anticipates and it might get shut down. But its knowledge
1174480	1179920	of what humans want it to do doesn't change what its own desire is internally.
1180640	1187040	So I suppose I understand why you couldn't just give the system an instruction that didn't also
1187040	1196480	come with rewards. Is it impossible to give an AI system a reward for every every problem it solves
1196480	1205360	by not hurting anyone? I think that would help somewhat. So the problem here is that there are
1205360	1210960	kind of two possibilities. And it's going to be hard for us to give rewards that ensure that one
1210960	1215200	of the possibilities happen and not the second possibility. So here are the two possibilities.
1215200	1221040	One possibility is the AI really doesn't want to hurt humans. And it's just going to keep that
1221040	1224720	and take that into account when solving the math problem. That's what we want to happen.
1225440	1231760	The other possibility is that the AI only cares about solving the math problem. And it doesn't
1231760	1238080	care about humans at all. But it understands that humans don't like it when it hurts them.
1238080	1241280	And so it kind of doesn't hurt humans in any obvious way.
1241280	1251120	Oh, right. Okay. And so this is a route to AI not caring about humans, but being kind of deceptive.
1251120	1257840	I guess maybe an analogy that really speaks to me is something like if you were to punish a child
1257920	1265040	for, I don't know, having ice cream before dinner, like you might get them not to have ice
1265040	1271200	cream before dinner, or you might create a thing where they have ice cream before dinner while
1271200	1280720	hiding in the closet. And it's like pretty complicated to teach a nuanced enough lesson
1280720	1287040	to a child about ice cream and why they shouldn't have it for dinner. That doesn't have any risk
1287040	1292400	of the lying version. Is that kind of right? Yeah, I think that's a good analogy. Okay, nice.
1292400	1300640	I think with a child, it might be somewhat easier because you're much more capable than them. So
1300640	1304480	even if they ever did try and eat ice cream in secret, you'd have a good chance of catching them.
1304480	1308160	I think the problem gets really hard when the AIs are much smarter than us,
1308160	1312800	such they could quite easily eat ice cream without us noticing. And then it's really hard
1312800	1317360	for us to give them rewards, which stop them from doing that. Right, right. And something like
1318000	1323120	we have a pretty good idea how children's brains work. They work kind of like ours,
1323120	1326240	but like a bit simpler and like we have some idea of the ways they're different.
1326800	1331760	And so we can like make guesses about the types of motivations that will speak to them.
1333040	1340320	I don't know, maybe it's like we know that our kids work similar to us and that like they feel
1340320	1345040	shame and they'd feel shame if they were punished and want to like please us because that's just
1345040	1350320	like pretty human. And so maybe we have a better sense of how they'd respond to punishment. But
1350320	1357280	maybe AI systems are just like so different to humans that we really have no idea what their
1358720	1365760	or at least we'll have a less clear idea of what other processes that they're using or like things
1365840	1370960	that they experienced or whatever are like and what kinds of behaviors those will push them toward.
1371760	1373440	Yeah, I think that does make it a lot harder.
1374000	1380000	Cool. That's super helpful. Yeah, is that is that basically the key scenario you're worried about?
1380560	1388880	This kind of we train AI systems to achieve certain goals, but it's hard to know what strategies
1388880	1395840	they see as fair game. And it's hard to train them not to pursue harmful strategies. And then
1397200	1402640	they eventually get super smart. Maybe they are deceptive. Maybe they're just very convincing.
1402640	1409360	And so they they're able to get a bunch of power and really disempower humans. Is that kind of what
1409360	1416800	you see as the core risk? Yeah, that's right. And I would emphasize that in the scenarios I described
1416800	1423040	it, AI is improving really, really rapidly as it approaches and then go through the human range.
1423760	1428560	So, you know, when we're talking about this, this example with the AI that's trying to solve math
1428560	1432560	problems, maybe we're thinking, oh, we'll have, you know, a few years with it trying out this kind
1432560	1436960	of strategy. And then we notice it's kind of doing a little bit of hacking into computer resources.
1436960	1443520	So, you know, we tamp down on that. But but if this whole thing plays out over just one year,
1443520	1449920	for example, we go from like, you know, notably below human to superhuman systems. Yeah.
1450720	1455200	I think it makes the risks a lot more intense. Yeah, that makes sense to me.
1455920	1461040	Yeah, to move us on to your research, then. Some of the work you've done that kind of most blew my
1461040	1466880	mind was actually on what happens when we're able to basically build AI that does roughly what we
1466880	1472320	intended to do. I think I naively would have guessed something like the world carries on is normal,
1472320	1479520	but we use GPT-8 a lot in our jobs. But you've looked into the hypothesis that not only will
1479520	1485840	things not stay the same, they actually might change very, very, very quickly if AGI is so good
1485840	1492480	that it kind of causes explosive economic growth, which yeah, in this case, you're defining as the
1492480	1497840	world economy growing something like 10 times faster than it has for the last century. Yeah,
1497840	1503520	so to start, can you help me understand intuitively what it would mean for the economy to grow 10
1503520	1509280	times faster? Sure. So one way to think about this is to think about all the technological
1509280	1515520	changes that have happened over the last 50 years. Okay. Yeah, that feels like a lot. So 50 years ago,
1515520	1524400	it was 1970, we'd had very basic digital computers around, but they weren't being widely used,
1524400	1531280	they weren't very good. I don't think the internet was around. And there's loads of other improvements
1531280	1540640	in manufacturing and in agricultural techniques. Medical care. Exactly. Yeah. And massive improvements
1540640	1546720	across the board in the last 50 years, but probably the most striking is IT. Yeah, sounds right.
1546720	1551200	And so what explosive growth would look like is that all those changes, rather than happening
1551200	1556720	over the course of 50 years, they happened over the course of five years. So we're going to get
1556720	1563680	the internet in five years plus a bunch of other improvements. Exactly. So rather than kind of
1564480	1570720	it taking 50 years to go from these really rubbish slow computers that you could buy in 1970 to the
1570720	1576880	awesome MacBooks of today, that just happens over five years. And similarly, rather than taking 50
1576880	1583440	years for you to go from those kind of rubbish phones to smartphones of today that also have the
1583440	1588480	internet and all these specialized apps, that again, this happens over five years. So the kind
1588480	1593280	of you see the introduction of a new technology and then very, very quickly you see it being refined
1593280	1601040	into a super useful human friendly product. Wow. I mean, on the one hand that sounds kind of incredible
1601040	1609040	and exciting. On the other hand, it just feels like a super strange world to be getting so many
1609040	1617680	new technologies every few years. Can you explain the idea behind why AGI might even make that possible?
1618480	1625760	Yeah. So here's the most basic version of the argument, which you can kind of make it more
1625760	1630080	complicated to adjust various objections. But I think kind of this version captures the core idea.
1631040	1636960	So today, there are maybe tens of millions of people whose job it is to discover new and better
1636960	1642480	technologies, working in science and research and development. They're able to make a certain amount
1642480	1647840	of progress each year. And it's their work that helps us get better computers and phones and
1647840	1654000	discover better types of solar panels and drives all these improvements that we're seeing. But
1654000	1659840	like we've been talking about shortly after AGI, I think there's going to be billions of
1660560	1666640	top human researchers equivalents in terms of a scientific workforce from AI.
1667520	1674160	And if you imagine that workforce or half of that workforce or just 10% of it working on
1674800	1683440	trying to advance technology and come up with new ideas, then you have now 10 or 100 times the effort
1683440	1690160	that's going into that activity. And these AIs are also able to think maybe 10 or 100 times as
1690160	1696000	quickly as humans can think. And you're able to take the very best AI researchers and copy them.
1697120	1702800	So if you think that scientific progress is overwhelmingly driven by like a few smaller
1702800	1706880	number of really kind of brilliant people with brilliant ideas, then we just need one of them
1706880	1711600	and we can copy them. They might be happy to just work much harder than humans work.
1712560	1715840	It might be possible to focus them much more effectively on the most important
1716480	1720640	types of R&D, whereas humans maybe are more inclined to follow their interests,
1721360	1726400	even when it's not the most useful thing to be researching. And so all of those things together
1727200	1733360	just mean that we'll be generating kind of 100 times as many new good ideas and innovations
1733360	1739120	each year compared with today. And then that would drive the development of technologies to be
1739680	1746160	at least 10 times faster than today. Right. How likely do you think this kind of growth is?
1747520	1754080	Is it the default once we get AGI? I think it is a default. You could give
1754080	1760000	objections to the argument I gave, but I think it's mostly possible to answer those objections.
1760000	1764320	So you could say, well, discovering new technologies isn't just about thinking
1764320	1767840	and coming up with new ideas, you also need to do experiments. Okay, sure.
1768640	1774880	And then I think you can answer that objection by saying, that's right, we will need to do
1774880	1781680	experiments. And that's like testing a drug on humans and maybe it takes five years or something
1781680	1790080	to really check that it's safe and effective. Right. Yeah. Or you've designed a new solar panel
1790080	1795680	and you want to like test its performance in a variety of conditions. Yeah. Yeah. Or you kind
1795680	1799120	of running some experiments to see what happens when you combine these two chemicals together
1799120	1807360	because you're not able to predict it in advance. But if you have a billion AIs trying to push forward
1808000	1812080	R&D and they're bottlenecked on these needing to do these experiments, then they'll be putting
1812080	1816400	in a huge amount of effort to make these experiments happen as efficiently as possible.
1817360	1822320	Whereas today, we might be using the lab for 50% of the time we could be using it
1823040	1826800	and we might be just doing a whole bunch of experiments and then analyzing it afterwards
1826800	1832560	and learning a little bit from each experiment, but also not kind of trying to cram as much into
1832560	1837760	each experiment as is humanly possible. If these AIs are limited on experiments and they're going
1837760	1844480	to be spending months and months just meticulously planning the micro details of every single
1844480	1850800	experiment so that you can get as much information as possible out of each one and kind of fully
1850800	1855360	coalescing their theoretical understanding and all the current data and implications and saying,
1855360	1860640	here are the key uncertainties that we need to address with these like kind of scarce experiments
1860640	1866080	and they'll give the humans conducting the experiments really detailed and precise instructions
1866080	1871120	and set things up so the experiments are really unlikely to go wrong and kind of analyze the
1871120	1874720	resultant data from 100 different angles to learn as much as you can from them.
1876000	1879680	And I think that will go a long way to getting over the experimental bottleneck.
1880240	1886880	I mean, even if you just think you use labs eight hours a day, but you could use them 24 hours a day
1886880	1892000	and then there are probably hundreds of other efficiencies like that that will all just add up
1892000	1894480	to get to many, many times more efficient stuff.
1894880	1900320	Right. And I mean, the AIs can direct humans what to do so you could be paying very high
1900320	1905680	wages to have unskilled human workers work through the night to run these experiments
1905680	1911280	directed by AIs telling them exactly what to do when. So yeah, you'll be able to have those labs
1911280	1916960	working around the clock if that's what's wanted. In the longer run, robotics is already very good
1917600	1923360	and I don't think it's going to take too long once we have a billion AI researchers to design
1923360	1929200	robots that are able to do the physical tasks that humans do. It doesn't, you know, with that
1929200	1934400	far off at the moment. And so, you know, if eventually, you know, we actually, we just need
1934400	1939040	more humans to kind of build more labs or to run these experiments, I think it will be possible to
1939040	1943840	have, you know, have robots doing that work and have the AIs directing it. And again, kind of
1943840	1948720	meticulously planning what each robot is doing with its time. So we're getting the very most
1948720	1953200	out of each robot. Right. I mean, the example you raised about human experiments is a really
1953200	1957440	good one, because that seems like it's going to be particularly hard to speed up. Right.
1957440	1961200	There are still a few things that I can already think of that could happen there. So
1962080	1966800	if it's kind of any psychology experiments that you're wanting to do or knowing how humans will
1966800	1973360	react to a new technology or to a new scenario, then just studying all of the human data on the
1973360	1978960	internet and doing in-depth interviews with humans could give AIs a really good understanding of how
1979040	1985760	human psychology works. Right. In the limit, they could scan a human's brain and kind of upload
1985760	1992000	them to be a kind of a virtual digital person if that human was willing to do it. And then it could
1992000	1996960	then do experiments with them in a simulation much, much more quickly. Right. Really fast.
1997920	2005200	I mean, that's getting pretty weird. Yeah. And I'm feeling very sci-fi, but that's part of what
2005280	2011360	we're talking about. We're talking about we've got millions or billions of copies of AI systems
2011360	2018960	that are as smart or smarter than humans, basically using all of their brain power to innovate.
2019520	2025440	And yeah, it's bizarre, but if you apply all of that brain power, you're going to get super,
2025440	2031040	super fast improvements to technology. Is this bottlenecked at all by ideas getting
2031040	2036960	harder to find? Are there going to be limits that just like a human would hit up on these limits
2036960	2042480	or humans have, AI systems will also hit up on those limits? Or do we expect them not to,
2042480	2048080	because we're talking about superhuman intelligence? So ideas are getting hard to find.
2048800	2054800	For me, the kind of the best zoomed out example is just that our scientific workforce has been
2054880	2062720	growing at four or 5% every year over the last 80 years. And that's a massive increase in that
2062720	2067520	scientific workforce over 80 years, you know, many, many doublings, but actually the pace of
2067520	2072640	technological progress, if anything, has been slowing down somewhat over the last 80 years.
2072640	2076960	And so, you know, on a high level, the explanation is sure we're using way more effort than we used
2076960	2081760	to be, but the ideas are harder to find. So we're actually slowing down a little bit
2081840	2086560	in terms of the pace of our progress. Maybe the best illustration of that dynamic is physics,
2086560	2093360	where a kind of 100 years ago, also you could have Albert Einstein in his spare time as a
2093360	2100880	patent clerk come up with multiple very significant breakthroughs. And then kind of almost single
2100880	2106080	handedly or with a few collaborators develop general relativity over the few years that followed,
2106080	2110080	which is just a major breakthrough in our understanding of the universe. Whereas today,
2110720	2118320	you have probably, you know, maybe millions of physicists, just kind of with the huge machines
2118320	2123440	at CERN that have to be honest, making, I would say pretty incremental progress in advancing the
2123440	2128640	state of knowledge and physics. Yeah, okay, right. Yeah, in terms of how it applies to AI,
2129680	2134240	first thing to say is that even if that dynamic exists, and it's very strong, we would still
2134320	2140720	expect a very significant, if temporary, increase in the rate of technological progress.
2141840	2147760	So let's say ideas are getting harder to find, but then suddenly, in 10 years, we've got a billion
2147760	2153120	AIs working on it rather than the kind of 10 million humans. Well, even if our ideas are
2153120	2157920	getting harder to find, then at least temporarily, there'll be much faster technological progress,
2157920	2162320	and then we'll kind of pluck even more of the low hanging fruit and eventually even these AIs get
2162400	2168800	stuck. Right, maybe eventually we still stagnate, but it'd be pretty crazy if you added millions of
2168800	2174400	people or millions of brains to the workforce and didn't get a bunch more technological progress.
2175280	2178800	Yeah, I mean, specifically if you made the workforce like a hundred times as big,
2179440	2185360	then yeah, especially with the other advantages I discussed about running 10 times as fast and
2186400	2190160	you know, being really focused on the most important tasks, I think really surprising
2190160	2194320	if you don't get at least a temporary increase. In fact, I don't think it would be temporary,
2194320	2201920	and that's because one of the things that AIs can work on is actually building more computer
2201920	2207520	chips to run AIs on, improving the kind of hardware designs for those computer chips,
2207520	2211520	improving the algorithms that AIs run on, improving the designs of robots.
2211520	2217040	Just making themselves better scientists. Exactly, and so we've been discussing how
2217600	2221680	already the pace of progress in terms of the algorithms and the hardware is pretty fast,
2222560	2226320	and I've already said that I expect it to be much faster once we have AGI.
2226320	2231520	And so really this isn't a kind of a constant sized AI and robotics workforce we're talking
2231520	2237200	about here, but if we choose to do so, then we could have the size of that workforce
2238000	2243840	doubling every year very easily. And so that means you can overcome this problem of ideas
2243920	2250000	getting harder to find, because you're not kind of dealing with a constant or slowly growing
2250720	2256800	workforce. You're dealing with a workforce which is itself rapidly increasing in size,
2256800	2262400	and so even if ideas are getting harder to find, you've got a bigger and bigger workforce to find
2262400	2267920	it. And you can actually model out this dynamic. You can take the kind of the best models we have
2267920	2271760	where ideas are getting harder to find, and you can say, well, if ideas are getting harder to
2271760	2278400	find the one hand, but on the other hand, AIs are able to design better AIs and do all the
2278400	2282960	improvements that I talked about designing better robots, etc. How does that dynamic play out?
2283840	2290720	And it turns out that at least under these models, even if ideas are getting harder to find at kind
2290720	2297520	of a very steep rate, then you still are going to get the kind of the AIs and robots winning that
2297520	2303520	race. That's really wild. I guess those AI scientists might hit some limits, and do you
2303520	2309600	have any ideas for what those might end up being? It's a great question. I think we are going to hit
2309600	2314080	limits at some point. Eventually we won't be able to design better technologies. Eventually we'll have
2314080	2319600	the best algorithms we can get for making AIs. I do think there are reasons to think the limits
2319680	2329840	could be quite high. One interesting data point is that small animals are able to double their
2329840	2336720	population size in just a few months, and even smaller animals like insects can double their
2336720	2343120	population size in just days or weeks. That shows that it is physically possible to have
2343120	2349760	a certain kind of biological robot that kind of doubles its own number in the scale of weeks or
2349760	2355920	months. And with all of this kind of massive scientific effort that we've been describing,
2356720	2362160	it seems possible that we'll be able to design kind of robots of our own that are able to
2362160	2367200	kind of double their own number, kind of build replacement robots in a similar timeframe.
2367920	2372480	And so currently you can try and estimate, well, if we used a factory to try and build
2372560	2377280	another copy of itself, how long would that take? I haven't seen a good analysis of this,
2377280	2383040	but when I've spoken to people, they've guessed it on the order of months. So that also supports
2383040	2387040	this vague idea that it might be possible to get these robots that are able to kind of build
2387040	2390160	extra copies of themselves and double their own number in just a number of months.
2390960	2398080	So what that all suggests is that we could have a robot workforce which is growing at a really,
2398080	2404160	really high rate. And by robot, I'm picturing physical bodies, but do you do basically just
2404160	2410400	mean AI system that's like working on science? Or do you think physical bodies end up being
2411440	2416320	important because we've got to start automating some of these physical tasks in addition to the
2416320	2421680	cognitive tasks? I'm thinking, yeah, including the physical tasks when I'm talking about the
2421680	2430400	robots. I mean, if these robots each weighed 50 kilograms, and we're able to produce robots as
2430400	2434880	many robots in a year, such that they weighed as much as all the cars that we currently produce in
2434880	2443760	a year, then we'd be producing around a billion robots each year. So already the manufacturing
2443760	2450560	capacity seems like it is theoretically there to produce a huge number of robots. And that's before
2450560	2456880	taking into account that it's lucrative. And so we want to create more. And yeah, unreal.
2456880	2462080	So I do think that this this dynamic leaves us in a pretty crazy world where the size of the
2462640	2469200	AI and robotics workforce is growing, potentially going very, very quickly. And probably as a result,
2470080	2476800	it's hard for me to imagine technology really stalling out before we hit real kind of limits,
2476880	2481760	kind of fundamental limits to how good technology can get. And there will be such limits. Technology
2481760	2487520	can't improve forever to infinity. Ultimately, you've come up with the very best ways of
2487520	2491360	arranging the molecules to get the desired technological behavior.
2491920	2499040	Right. Okay. Do we ever hit limits on just physical stuff, the stuff we make the robots out of?
2499040	2504880	I think we will. So I already said about the kind of that car statistics suggesting we can get pretty
2504880	2510160	far just in terms of the massive robots we could produce just with the kind of manufacturing
2510160	2518720	capabilities we already have. The earth is is massive. There are kind of mountains with all
2518720	2524960	these kinds of materials in them all over the place. And if we run out of a particular material,
2525600	2531040	which is currently useful for building robots, then these billions of AI as we have will be
2531040	2536800	working hard to find ways of doing without that scarce material. And that's been a common pattern
2536800	2541280	in technological development that you kind of find ways to switch out of things that are scarce.
2541920	2546880	So, yeah, right, you know, it's hard to rule out that there's just some material that we just
2546880	2551360	absolutely need and we can't do without and that that all next things, you know, maybe once we get
2551360	2557360	to 100 billion robots or something. But it also just seems more likely to me that given just the
2557360	2562560	abundance of just kind of materials that are in the earth, it's, you know, it's a big place.
2562560	2566320	There's lots of different stuff there. We, it's not like we've mined everything there was to mine
2566320	2571600	or even close to it. Right. We'll be doing, you know, using all the best methods for recycling and
2571600	2576240	using things as efficiently as possible. It doesn't seem to me like those kinds of bottlenecks are
2576240	2581840	going to kick in particularly early. And it's not out of the question that we'd have the technology
2581840	2588000	to explore resources in space. I mean, that's even I'm adding more sci-fi here. But like,
2588720	2595360	if we're doubling technological progress every, I mean, it sounds like you're talking about
2596080	2602080	months at some point. Yeah, I think months is plausible. Okay, months is plausible. And so
2602080	2608160	that might mean we, we aren't limited by earthly limitations. That's right. There is an interesting
2608160	2614640	dynamic there where if we are kind of doubling the number of robots really quickly, and we're
2614640	2620320	improving technology really quickly, then we kind of, we're not that interested in doing an activity
2620320	2626080	which takes maybe like 10 years to bear fruit, because we're used to kind of our investments
2626080	2629920	paying off with kind of doublings every year. We're like, oh, we could go to the moon and get
2629920	2636320	materials, but man, like that would take so long. If we, if we just invest in everything we can find
2636320	2642080	on earth, we can much more quickly kind of increase the just use it more efficiently. Yeah, so that
2642080	2647840	time delay becomes more significant when, when you're already able to go so fast. So I think I
2647840	2652240	imagine the kind of going to the moon and the space stuff happens when we're really kind of
2652240	2657680	struggling to find ways to make use of the earth resources. Okay, so it sounds like we're kind
2657760	2666800	of talking about something like AI systems replace humans in a bunch of sectors,
2667600	2674640	really during our lifetimes. And then like our lives really, really change quite radically.
2674640	2682480	And very, very, very quickly. Yeah, I guess I just find that super weird. I think my brain is like,
2682480	2688720	no, I don't believe you. That's too weird. I don't like, I just can't imagine that happening.
2689280	2696160	If we're, if we're saying this is happening in the early 2030s, I'll be, yeah, in my late 30s.
2696160	2701600	And like all of a sudden, the world would be radically changing every year. And I like won't
2701600	2710400	be working. I agree. It seems really crazy. And I think it's very natural and understandable to
2710400	2717440	just not believe it when you hear the arguments. And that would have been my initial reaction.
2718400	2723760	In terms of why I do now believe it, there's probably a few things which have changed. Probably
2723760	2730480	I've just sat with these arguments for a few years. And just been like, I just do believe it.
2730480	2735600	You know, I have discussions with people on either side of the debate. And I just find
2735600	2740400	that people on one side just have thought it through much more. And I think what's at the
2740400	2746400	heart of it for me is that the human brain is a physical system. There's nothing magical about it.
2747360	2753920	It isn't surprising that at some point, we develop machines that can do what the human
2753920	2760560	brain can do at some point in the process of technological discovery. And to be honest,
2760560	2765440	that happening the next couple of decades is kind of when you might expect it to happen naively.
2765760	2773680	We've had computers for 70-odd years. It's been a decade since we started pouring loads and loads
2773680	2780000	of compute into training AI systems. And we've realized that that approach works really, really
2780000	2785040	well. Just if you were kind of to say, okay, when do you think humans might develop machines that
2785040	2788480	can do what the human brain can do? You kind of think it might be in the next few decades.
2789280	2792560	And I think if you just sit with that fact that there are going to be machines that can do what
2792560	2797440	the human brain can do. And you're going to be able to make those machines much more efficient at
2797440	2802080	it. And you're going to be able to make even better versions of those machines, 10 times better
2802080	2806240	versions. You're going to be able to run them day and night. And you're going to be able to build
2806240	2811040	more. When you sit with all that, I do think it gets pretty hard to imagine a future that isn't very
2811040	2818720	crazy. And another perspective is just zooming out even further and just looking at the whole arc
2818720	2826160	of human history. So if you'd have asked hunter gatherers who only knew the 50 people in their
2826160	2833280	group and who had been hunting using techniques and tools that as far as they knew had been
2834000	2840720	passed down for eternity generation to generation, doing their rituals, if you'd have told them that
2841360	2848560	in a few thousand years, there are going to be huge empires building the Egyptian pyramids
2848560	2859440	and massive armies and the ability to go to a market and give people pieces of metal in exchange
2859440	2864640	for all kinds of goods. They would have seemed totally crazy. And then if you'd have told those
2864640	2871360	people in those markets that no, there's going to be a future world where every 10 years,
2871360	2875680	major, major technological progress is going to be coming along and we're going to be discovering
2875680	2882240	drugs that can solve all kinds of diseases. You're going to be able to get inside a box
2882240	2887120	and land the other side of the earth. Right. Again, they'd have just thought you were crazy.
2887120	2893200	And I think while it seems that we understand what's happening and that progress is pretty
2893200	2899520	steady, that has only been true for the last 200 years. And zooming out, it's actually the norm
2899520	2905680	throughout the longer run of history for things to go in a totally surprising and unpredictable
2905680	2912720	direction or a direction that would have seemed totally bizarre and unpredictable to people
2912720	2917760	naively at that time. Right. I feel like I was introduced to it when I read what we did in the
2917760	2924560	future, Will MacAskill's book, that there's this thing called the end of history fallacy. It really
2924640	2932880	feels like we're living at the end. We're done changing. We're going to maybe find some new
2933760	2941280	medical devices or something. But basically, we've done all of the weird shifting that we're
2941280	2949600	going to do. And I can't really justify that. It does seem like a fallacy. Presumably,
2950320	2955600	things are going to look super different in 50 years. And sometimes those changes have gone
2955600	2961680	super fast in history. And sometimes they've gone super slowly. And we've got real reasons to think
2961680	2967680	that we might be entering a period of really fast transition. Yeah. I mean, if anything, I'd say
2967680	2974080	the norm is for the new period to involve much faster changes than the old period. So Hunter
2974160	2978800	Gathering went on for tens of thousands of years, if not hundreds of thousands of years.
2979600	2984080	Then we started doing agriculture and forming into big societies and did things like the pyramids.
2984800	2988960	And then a way that people often think of the next phase transition as being
2989600	2994880	kind of the start of the industrial revolution and the beginning of kind of concerted efforts
2994880	3001840	towards making scientific progress. And after we did agriculture, kind of new technologies and
3001920	3007360	changes were happening on the scale of maybe a thousand years, or maybe a few hundred years,
3007360	3012160	which is much faster than in the Hunter Gatherer times. And then today, after the industrial
3012160	3017120	revolution, we're seeing really big changes to society every 50 years. So we've already seen
3017680	3022000	kind of historically those phase transitions have led to things being faster. So that,
3022000	3026560	I think, is the default expectation for what a new transition would lead to.
3027200	3034080	Right. And it just feels weird to us because we're pre-transition. Possibly, whoever's living
3034080	3042000	50 years from now will just be like, yeah, obviously, that was coming. And those weird
3042000	3048000	people living in 2023 thinking that they'd made all the technological progress they were ever going
3048000	3054880	to make. Maybe I'm still struggling just to imagine what it would look like, I guess.
3055600	3064240	Yeah, because it's possibly going to be us. What's in store for us? Is it going to be good?
3065200	3072160	I think it could be really good. It could be really, really bad. It could be really good if we
3073120	3077680	align AI so they're always trying to help us and help humanity do as best as it can
3077680	3084640	by humanity's own lights. And the kind of benefits from AI and these new technologies are used to
3084640	3091680	solve the world's most pressing problems and used to lift people out of poverty and give people
3092400	3097440	the lives that they hope for themselves and for their children, solve the problems of climate
3097440	3105920	change or poverty of disease. I think it could go really, really well. Right, right. And so in the
3105920	3115040	best case where AI is really trying to help us, it's still kind of unimaginable to me as a world.
3115600	3123680	I mean, maybe it's just like I'm so biased by the status quo where like, I need to work and
3124400	3131120	I need to work to live, I need to work to help solve problems. Like in this best case, is there
3131120	3139280	unemployment? Is there unemployment for everyone? Is it a slow transition? A fast one? Does it make
3139280	3145120	inequality better because no one needs to work and we all have enough things? Or does it make it
3145120	3152160	worse because some people have to work? Do we have predictions about that that are worth making?
3152160	3160000	I think the default is that inequality would become greater because all of the wealth and
3160000	3164960	useful work is coming from these AI systems, which I think by default will be controlled by a small
3164960	3173520	number of people and companies. In the very best case, then you hope that that wealth is equally
3173520	3179760	distributed or kind of much more equally distributed than it would be by default. And it is true,
3179760	3186880	I think, that there's going to be so much progress made if AI is aligned that it will be very cheap
3187600	3191200	to give everyone in the world the standards of living that are enjoyed by the very
3191200	3197680	richest people today in terms of material comforts and health and actually much, much better on
3197680	3203280	those fronts, I think after all this technological progress. So I think if we can get the AIs to
3203280	3209360	be really trying to help us, then even if we mess up on things like the kind of distribution of
3209360	3214240	benefits, even if we mess up a bit on that, then I think things will still look pretty good because
3214240	3222560	there's just so much to go around. If there's kind of universal basic income, you could just use
3223200	3229040	1% of the output that's produced in a year to kind of give everyone kind of all the material
3229040	3234320	and technological things they need to kind of meet all of their needs, all of the material needs.
3235600	3242000	In terms of work, I think it will no longer be the case that you can produce a higher quality
3242000	3249520	service or product than what an AI could do or a robot could do. One thought is that there will be
3249520	3256880	some humans, maybe me and you, who just value human contact and hang out with other actual
3256880	3262240	real humans. And that could provide a kind of work for those who want it.
3262240	3264240	Role for humans. Okay.
3264240	3270160	And another possibility is that we rethink the nature of work. So we do work to help each other
3270960	3276160	and even though we know that AIs could do the work just as well, we're still happy to do that
3276160	3281840	because it gives us a sense of meaning or we kind of do creative things instead like
3282960	3286960	creative writing and drawing. And even though we know that AIs could do that better than ours,
3286960	3291840	it's still enough for us to have a sense of purpose. I mean, people still play chess today
3292560	3297120	and still really enjoy it and get purpose from it, even though they know that they can never
3297120	3303760	go to match the best AIs. Right. Yeah, I guess I can imagine lots of people listening, hearing
3303760	3313280	about this future and being like, no, I like the world the way it is. I like that humans get to
3313280	3320240	make choices for ourselves as a society. I don't want AIs making it for us. I like that I have to
3320240	3326080	work, get to work. I don't know. I don't, I can imagine people being like, no, I don't want AIs
3326080	3333600	to be making the art. I want humans to be making the art. So is there some chance that there's,
3333600	3339440	I don't know, like a movement that's anti AI growth that stops this from happening,
3339440	3346240	even though it's theoretically possible? That's a great question. I do think it would be good for us
3346240	3352560	to take this transition more slowly than is theoretically possible. And that might happen
3352560	3359120	by default if we don't make specific efforts to go slowly. And so I think if people do try and delay
3359120	3363200	or stop this, it could, it could be a good thing because I don't think we're prepared for that
3363200	3369520	new world. Right. I think it's going to be very hard to permanently prevent this transition from
3369520	3375760	happening. How come? One way to think about it is that there is some kind of upfront starting cost
3375760	3382000	to get this transition going. So let's really simplify and say, today, if you spent a trillion
3382000	3389280	dollars, you'd be able to train AGI and you'd have enough money left over to buy some manufacturing
3389280	3394800	equipment for making robots. Right. And then you could have your AIs do research into better
3394800	3400400	robots and making better AIs. And that whole process could lead to you having even more AIs and even
3400400	3405120	more robots. And you could then grow your population of AIs and robots. And just with that
3405120	3410000	trillion dollar initial investment, you could end up with this massive AI and robot population,
3410000	3416400	which is then able to just start doing the scientific work needed to significantly accelerate
3416400	3421920	technological progress. Right. Right. So the thing that's difficult is that that upfront cost will
3421920	3429120	be falling over time. AI algorithms are improving, computer chips are improving. And so the cost to
3429120	3435200	kind of training AGI and then just using it to build robots and to build more AIs and make money on
3435200	3440080	the make money in the economy by selling its services and just kind of building up its own
3440080	3445840	self perpetuating energy that ultimately results in making technological progress so you can sell
3445840	3451440	more useful things to society that people want. That cost is going to be falling. And so let's
3451440	3455440	say it was a trillion today, you know, in the future, it's going to be 100 billion and then it's
3455440	3462400	going to be 10 billion. And there's going to be a lot of incentive to do this because it's going to
3462400	3468960	grant whoever does it a lot of power. They'll have all these AI workers that they can use to do,
3468960	3471920	you know, whatever they want them to do if they manage to solve the alignment problem.
3472480	3476800	If they use it for designing new technologies and those new technologies could grant
3476800	3482400	additional military power, or they could grant things that people all around the world desperately
3482400	3488560	want like curing illnesses, like preventing climate change, like understanding and solving
3488640	3496480	mental health problems, like life extension. So it's not just kind of economic incentives,
3496480	3504240	it's not just like to get rich, it's like all sorts of motivations are benefited from paying
3504240	3512800	this cost to get this hugely productive AI scientist workforce. Yeah, kind of whatever you want.
3513680	3518400	Right. You can probably get it much more effectively if you have a billion AIs and robots
3518400	3523200	designing technology to help you get it. And I think we can delay it, we can say okay, we're
3523200	3527120	going to be really cautious, only a few people are allowed to train these systems and we try and
3527120	3531360	convince the other countries to go slowly as well. But the thing that we could be, you know,
3532000	3536880	that even 100 years after it's first been possible to train AI for a trillion dollars,
3536880	3541120	that still no one has done it and no one is using it to make scientific progress,
3541120	3548080	even though the cost is now like $10 million, it's really hard to imagine that we kind of
3548080	3555840	prevent anyone from doing it as it gets cheaper. And I think sometimes people, when they're
3555840	3561280	thinking about it, imagine that in order to get this kind of 10 or 100x faster technological
3561280	3564800	progress, we'd have to be making a real effort and really kind of being super efficient and
3564800	3568480	driven about it. But I think that's not the right way to think about it. It's more like
3569200	3574320	by default, all you need to do is ask your AIs and robots, please do these tasks for me. And if
3574320	3578320	you need to make tech progress along the way, do it, they will suggest the plans and involve
3578320	3581920	making tech progress. They will get in contact with the labs and organize for the experiments to
3581920	3586880	happen. You won't have to do anything. Right. And so I don't think it's going to require some kind of
3586880	3593840	concerted pro growth enthusiasts to like really push for this. It's more like, you want stuff,
3593840	3599200	the AI is going to try and do the stuff you want. And whenever they make tech progress,
3599200	3603360	it's going to go really well. And it's going to really help you solve your problems.
3603360	3604800	And you're going to just want to do more of it.
3605360	3610800	Yeah, just enough time will pass, enough actors will think on it and decide to do it at some
3610800	3618240	point. Yeah. So I guess I buy that the incentives are there for eventually an actor to want to build
3618240	3626560	this kind of AI scientist workforce. It still seems like there have been enormously lucrative
3626560	3633920	and beneficial technologies that we haven't pursued. So one example that comes to mind
3633920	3640640	is like nuclear power, which like could help loads with climate change and would also be,
3640640	3645600	yeah, again, super lucrative. And yet, yeah, we basically haven't done anything like what we
3645600	3652160	could do with it. Could there be something similar? I mean, it's just kind of stigmatized,
3652160	3657520	is like one reason we haven't. And I guess it's really expensive in particular the upfront costs,
3657520	3663120	which like maybe just ends up true of this like AI world we're talking about.
3664240	3667440	Yeah, it's a great example. I don't have a good understanding of what happened,
3667440	3673840	but I think there are some big catastrophes with nuclear power, and then it became very
3673840	3679360	stigmatized. And the regulatory requirements around it and the safety requirements became
3680000	3687360	very large, much larger really than was reasonable, given that fossil fuel energy has damaging health
3687360	3694320	consequences as well through air pollution. And as a result, it just became kind of a mixture
3694320	3699440	of stigma and just the additional cost from all the regulation just prevented it from being rolled
3699440	3706560	out. But I do think there are a fair few very significant disanalogies between that case and
3707440	3715440	the case of AI. Okay, yeah, what are they? So one thing is that there were other sources of
3715440	3720640	energy that were available. And so it wasn't too costly to be like, well, we're not going to use
3720640	3725600	nuclear, we're going to use fossil fuels instead. And then, you know, even the green climate change
3725600	3730080	concern, people could think about kind of developing solar panels and renewable energies.
3731040	3736480	And in the AI case, that there is going to be no alternative. There's going to be no alternative
3736480	3744480	technology which can solve all illness, and which can grant your nation massive national
3744480	3748880	security and military power, and that can solve climate change. This is going to be the only
3748880	3755280	option. So that's one disanalogy. Okay, that makes sense. Another kind of disanalogy is the
3755280	3761840	cost factor. So with nuclear power, it's become more expensive at a time due to regulations. And
3761840	3767600	that that's been a big factor in not being pursued. But the specifics around these cost
3767600	3774880	curves with compute and this algorithmic progress patterns suggests that the upfront cost of training
3774880	3781200	AGI is going to be falling really pretty quickly over time. Right. And so even if initially you
3781200	3785360	put loads of regulations, which make it very expensive, it's really not going to be long until
3785360	3790720	it's 10x cheaper. Right. And so permanently preventing it when it's when it's becoming cheaper
3790720	3796160	and cheaper at such a high rate, it's going to be really, really difficult. Third is just
3796880	3803280	just talking about the size of the gains from from this technology compared to nuclear power.
3803280	3807840	So, you know, France adopted nuclear power and it was somewhat beneficial, you know, it's kind of
3807840	3812160	now gets a lot of its powerful nuclear energy and that there's no climate change impacts and that's
3812160	3820080	great. But it's not as if France is visibly and undisputably just doing amazing well as a country
3820080	3824160	because it's got, you know, this nuclear power, like it's a kind of a modest addition, maybe it
3824160	3829760	makes it look a little bit better. Right. But by contrast, if one country is is, you know,
3829760	3833440	progressing technology at the normal rate, and then another country comes along
3834080	3840240	and starts using these AIs and robots a little bit, you're going to see very significant differences
3840240	3846080	in how its overall technology and prosperity and kind of military power is progressing.
3846720	3851200	And then you're going to see that as countries dial up how much they're allowing AIs to do this
3851200	3856000	work, that there are then bigger and bigger differences there. And ultimately, the difference
3856000	3863360	between advancing technology at our pace versus advancing technology 30 times faster is over the
3863360	3868880	course of just a few years, it becomes a massive difference in the sophistication of your country's
3868880	3874800	technology and ability to solve all kinds of social and political problems. You know, a last point
3875600	3882560	on this difference is that, you know, the US did in fact invest a lot of money in nukes shortly
3882560	3886880	after the development of fish and power. You know, when it came to a matter of national power,
3886880	3892480	they were very happy to invest in the technology, despite, you know, the risks which were clearly
3892480	3897920	very high. All of the same risks. Right. Yes, you know, the incentives were out of whack and we
3897920	3903440	didn't get nuclear fission power. But when it came to this kind of military technology for which
3903440	3908560	there was no replacement, countries were very keen to do it and they made it happen. And AI
3908560	3913600	driving significant technological improvements across the board is going to be a huge source
3913600	3919520	of military power. Right. And so it's really hard for me to imagine that just no one ever uses it
3919520	3926240	for that. And you've totally preempted my next question, which was like, can we definitely not
3926240	3933600	come up with like an international treaty that is like the downside risks of this technology
3933600	3941440	at this scale are possibly huge because alignment is so hard. And so we're all agreeing
3941440	3947840	not to not to go forward with it. And I guess we had lots of reason to do that in the case of
3947840	3952160	nuclear weapons. And we didn't. And we have lots of reason to do that in the case of biological
3952160	3957760	weapons. And and we suck at it. We do not live in a world free of biological weapons or nuclear
3957760	3963440	weapons. So I do think we should try. And I do think we can slow things down. And we
3963440	3969200	can, you know, increase the requirements and the safety efforts required, maybe to make it 10
3969200	3975520	times as costly or 100 times as costly to develop this technology. I think that is that is one thing.
3975520	3980480	And that's a big ask. And I think we should try and do as much of it as we can. Even if we can do
3980480	3986320	that, it's a whole different thing to talk about permanently choosing to never develop the technology,
3986320	3992240	even after we've made maximal efforts into making it safe, even after all the safety tests are saying
3992240	3997360	it looks like it is safe, even when millions of people are dying every year from illnesses which
3997360	4004880	we know could be prevented if we allowed the AIs to do research into treating it. I think just
4004880	4010720	permanently not going forward with using AIs and robots to make that technological progress.
4011760	4015920	Like I said, when it's becoming cheaper and cheaper and other countries and other companies,
4015920	4020400	you know, might want to do it, that does seem like it's just very unrealistic.
4020480	4021440	Just implausible.
4021440	4025360	And maybe not desirable either, to be honest. Like after a certain point,
4025360	4027680	like we should take it really cautious.
4027680	4034880	Right, after 200 years of like research into air alignment, even if we're like,
4034880	4041440	ooh, this seems weird and scary and might change the world as we know it, at some point there are
4041440	4049840	going to be incentives for some actors, countries or companies to try to deploy this technology
4049840	4059120	at scale to solve problems like poverty and illness, maybe also to get military advantages,
4059120	4065760	to solve problems like climate change. And those incentives might just be so strong that
4065760	4074080	even if we take our time, we'll probably eventually do it. Someone will. And once that happens,
4074080	4082000	progress will become so quick that we're looking at really economic growth at a pace that's,
4082000	4086560	I guess, still kind of unfathomable to me. But that is this kind of thing where
4086560	4090000	progress we've seen over the last 100 years happens in the next 10,
4091040	4095280	and actually just keeps getting faster and faster and faster. Is that kind of the picture?
4095840	4096880	Yeah, that's the picture.
4097520	4106720	That's really weird. It's scary. I guess it's also quite hopeful. I mean, if I let myself hope for
4106720	4115840	that good world where we use it to solve problems, I feel nervously really, really excited. But I guess
4115840	4123600	we've got some real, real challenges to overcome first. Okay, let's move on to a related topic
4123600	4129200	you've been researching more recently. So you've just written the draft of a report on
4129200	4134720	AI takeoff speeds that has some pretty alarming results to me, given everything we've just talked
4134720	4140160	about. And I guess just to get on the same page about language, what exactly do you mean when
4140160	4148240	you talk about AI takeoff speeds? Roughly speaking, capabilities takeoff speed is the question of how
4148240	4155440	quickly AI systems will improve as they approach and surpass human level intelligence.
4156160	4161840	And so the capabilities are like their ability to do things like drive cars or
4163120	4170400	program new programs. Exactly. So a fast takeoff speed could be that in a, you know,
4170400	4178720	in three months, AIs go from mouse level intelligence to significantly more intelligent
4178720	4184400	than the smartest human, right? Where a slow takeoff speed that could be happening over decades.
4185360	4191520	Got it. Okay. So we're just like slowly making progress next year at self driving cars. And
4191520	4195440	then it takes another 10 years to get to programs that can write other programs.
4196400	4201920	So I guess, you know, you've written this report, and I won't give away the results yet,
4201920	4206880	but it gives some evidence about how fast we might expect AI takeoff speeds to be.
4206880	4211200	But before we get to that, before you wrote the report, what did you think was the most
4211200	4214880	compelling evidence that AI takeoff speeds would be particularly fast?
4216080	4223760	So I think the most convincing argument is related to how humanity's capabilities were
4223840	4229360	improving much more slowly 50,000 years ago, then they are improving today.
4230400	4235280	And the attempt to draw an analogy to what might happen with AI capabilities.
4236320	4244400	So if we think that a million years ago, humanity's kind of cognitive abilities collectively were
4245280	4251680	maybe doubling every, let's say 100,000 years, I don't know. That's just kind of something to
4251680	4257200	represent their kind of slow increase in brain size and capacities. The exact number doesn't
4257200	4262560	matter. You know, whether you want to say it's, you know, 100,000 or 10,000 years is a very slow
4262560	4268080	doubling size in their abilities. Right. And that's basically because like slowly,
4268080	4272800	they're evolving to have slightly bigger brains, we're like adding a bit of prefrontal cortex,
4272800	4278240	and like the population is getting a little bit bigger over time, but grew very, very slowly.
4278240	4281120	So collectively, it's just, it takes thousands of years to double.
4281680	4282480	Yeah, exactly.
4282480	4283440	Cool. Cool. Okay.
4284080	4291760	Whereas today, our abilities are improving incredibly quickly. As a society, our population
4291760	4298320	growth is much faster. Our command of technology is much faster. And, you know, in the last,
4298320	4304880	like I said, in the last 200 years alone, we've doubled the economy many, many times and doubled
4304880	4310960	our ability to understand and manipulate the world very many times. And if you think that
4311760	4319840	there'll be some analog of that transition as we approach AGI, then, I mean, AI is already
4319840	4324960	improving really quickly. You know, I would say it's kind of doubling its abilities in less than
4324960	4329600	a year at the moment. And so if, you know, if that's this kind of the slow initial pace,
4330240	4335680	then, you know, the new pace would be kind of blisteringly quick.
4335680	4337440	Right, right. Unimaginably quick.
4337440	4345280	Yeah, exactly. The way I would think about it is that a million years ago, humans weren't able to do
4346400	4352880	science and to discover technological improvements much at all. And so they didn't have access to
4352880	4357520	this additional feedback loop of improvement. Where you discover new technology passed on to
4357520	4362400	the next generation, then they start on a better place, can discover even more technology, you
4362400	4366000	can now support a bigger population. There's this whole feedback loop, which arguably we
4366000	4370320	couldn't access a million years ago. Then we improved our cognitive abilities as, you know,
4370320	4374320	as humans a little bit. And then suddenly we got over this threshold where, okay, we can access
4374320	4379920	this, this kind of doing technological progress feedback loop, which then, you know, then speeds
4379920	4384880	up and speeds up as we develop agriculture and then we develop text, you know, written language
4384880	4388880	and we develop maths. And it's kind of, we're now even better at discovering new technologies.
4389520	4394640	And the thought is in my mind that maybe there's something similar that happens with AIs, whereas
4395440	4400960	today they're not, you know, they're not that smart. And so they're not able to access a certain
4400960	4405680	feedback loop. I'm not sure exactly what that feedback loop will be. But at some point, they
4405680	4410960	become smart enough that there's this additional feedback loop that they can use to improve their
4410960	4415360	capabilities, kind of like how humans use technology to improve our capabilities.
4415360	4419200	You know, one, one funny thing about this argument is that it's not clear what that new
4419200	4426480	feedback loop might be for AIs. And so that leaves me a little bit puzzled over where to go with
4426480	4432960	this argument. So for humans, it's, you know, we discover the scientific method and we experiment
4432960	4440000	on things and we built computers. And now we can like run programs that help us do science.
4440000	4445520	But for AI, it's like, we don't even know what kinds of science they'll discover that's like
4445520	4451600	beyond ours. And is there some question about whether there even are kind of higher orders of
4451600	4457040	science that we can't, that we haven't developed, but that AI systems might to kind of increase
4457040	4461600	their own feedback loop? Yeah, I think that's, I think that's right. And you know, you can,
4461600	4466080	you can throw out ideas for what that might look like. Right. Maybe it's the AIs
4466800	4472640	learn to work together in a team in a way that is way more efficient than what humans have ever
4472640	4479760	done. Yeah. Or maybe they run simulations or something to like learn about economics or
4480640	4485760	something in a way that we can, we can barely understand because we only see the economies run
4485760	4493200	in these weird real, real world scenarios. For example, for example, cool. Okay. Yeah. Are there,
4493200	4496880	are there any limitations to that argument? Or do you do just kind of buy it?
4497840	4501760	Yeah, I'm, I actually don't put that much weight on, on this particular argument.
4501760	4510880	Oh, interesting. So the main reason is that evolution was not trying to make humanity as a
4510880	4517040	whole as capable as possible. And it wasn't trying to make humanity as a whole good at science.
4517120	4524240	Right. So from that perspective, it's not actually as surprising that humanity went from
4524240	4530160	really sucking at science to being really good at science in a fairly short, short timeframe.
4530960	4538800	Yeah. So here's an analogy. Before 2020, we hadn't made many COVID vaccines, not because we couldn't,
4538800	4543040	but because we weren't trying to, we were focused as a society on doing other things.
4543680	4549360	Then around 2020, it became really useful for us to make lots of vaccines. And then,
4549360	4554720	lo and behold, the number of vaccines went up very dramatically. Now, that doesn't mean that our
4554720	4560240	kind of abilities in vaccine making suddenly went up. It just means that we reprioritized,
4560240	4566000	reallocated the resources we already had towards making vaccines. Okay. And I want to say that
4566000	4571760	that's somewhat similar to the way in which kind of our ancestors a million years ago,
4571760	4578240	we weren't that good at science, but evolution wasn't trying to make us that good at science.
4578240	4583440	It was mostly trying to make us hunt successfully, feed our families. Okay.
4583440	4588320	Science was like maybe a tiny, tiny bit useful back then, because it maybe allowed you to discover
4588320	4594320	something with your own lifetime, but it really wasn't very useful. Then I think more recently,
4594320	4600160	maybe more like kind of tens or a hundred thousand years ago, it did become more useful for humans
4600160	4606880	to do science and to be flexible learners. And so it's not that surprising that at that point,
4607760	4613200	say 50,000 years ago, where it was more useful for humans to be good at science,
4613200	4620560	evolution then reallocated those cognitive resources of humans to being good at science.
4620560	4621440	Right.
4621440	4626240	So that kind of reallocation by evolution from kind of just for foraging and then reallocating
4626320	4630720	those cognitive resources to doing science is kind of like human society, reallocating its
4631600	4633840	resources to make COVID vaccines.
4633840	4640960	Okay. Yeah, that's really, really helpful. So something like a combination maybe of language,
4640960	4648480	maybe of just like group living, maybe some things that I don't understand, made it much more
4648480	4655040	beneficial to be able to learn new things and learn like a range of things, not just the same
4655040	4660720	things over and over again. And so a couple of tweaks in the brain was enough to make the
4660720	4666880	brain that we'd been using for very specific set of tasks become useful for just like a much
4666880	4673520	wider range of tasks. And that wasn't like really fundamentally altering like the amount of brain
4673520	4682320	we have, but like how we use it. And that capacity already existed. And like you said, was repurposed.
4683040	4689680	So yeah, here's how you would relate it to AI take off. You'd say that in the case of evolution,
4689680	4696560	evolution wasn't initially trying to make humans good at science. And so no massive surprise that
4696560	4701200	it's able to quickly make some tweaks that make humans good at science late in the day.
4702240	4708880	But with AIs and with AI development, humans will be at every stage trying to make AIs as useful
4708880	4717200	as possible for doing economic tasks, helping with science and research. And so we wouldn't
4717200	4723440	expect there to be this kind of overhang where the AI has these abilities, which it's just not
4723440	4728080	using, because we would expect humans to be trying to coax those abilities out at every step of the
4728080	4736320	way. And so if if you're constantly trying to coax abilities out, most likely you'll only find
4736320	4741120	ways to do it incrementally, as opposed to like, if it happened to be the case that like,
4741920	4749280	I don't know, we found a billion computer chips on another planet, and could just
4750080	4757200	use them to train up a bunch of AI systems, then we'd expect the step change. But like,
4757200	4761440	currently, everything is just increasing incrementally, we're increasing chips incrementally,
4761440	4767520	we're increasing algorithmic progress incrementally. And so it's just going to keep improving at a
4767520	4773200	kind of incremental pace. And I think crucially, we have to think that we're currently
4774080	4779360	at each step of the way trying to use the most recent algorithms and most recent compute to
4779360	4785360	actually get AIs to do, let's say, useful science research. You know, if we're incrementally increasing
4785360	4789920	the computing algorithms, but we're not actually trying to get the AIs to do useful science research,
4789920	4794240	then it could be that one day we decide to try and get AIs to do useful science research,
4794240	4798640	and then suddenly, we train them to reassign all their cognitive abilities to that task,
4798640	4803280	and we do get something that's really quick. So it's a really important assumption here that
4803920	4808800	in some sense, the AI development ecosystem is kind of efficient.
4808800	4816480	Yep. And it's using, yeah, it's using new AI capabilities to do like the cutting edge research,
4816480	4823120	as opposed to like, if there were only market incentives to make AI that, I don't know, made
4823120	4830880	these beautiful images like Dolly, and no incentives at all to use AI to improve AI systems, then
4831760	4837520	if one day we made a few tweaks to Dolly, and we're like, stop making pictures, make programs
4837520	4843120	instead. And but like actually, like it did have capabilities related enough that we could
4843120	4848960	make that tweak semi easily. Then all of a sudden, we'd have this system that could write programs
4848960	4855520	really efficiently that we'd never had before. Right. Or maybe a more probable scenario would be
4855520	4861040	we're just using all our AI resources to train these image generating systems like Dolly. And
4861040	4867040	then we're like, you know what, why don't we just try using all those resources to train a science AI?
4867040	4872080	And then we, you know, we pick the architecture to specialize of a science, we use the data to
4872080	4876400	specialize it for science, we use all the compute that we were previously pouring into these image
4876400	4881520	generation systems. And then suddenly we're like, wow, our science AI is amazing. And it came out of
4881520	4886960	nowhere because we hadn't been trying in the previous years to do this at all. Yeah. So are we
4886960	4893600	currently trying to make AI systems that are really good at science? I think it's a good question.
4894640	4901360	The market doesn't seem to me to be super efficient. I've been playing our GPT for a bit
4901360	4907200	recently. And to me, it looks like GPT-4 is pretty smart. It doesn't seem to me like
4907200	4913680	its cognitive abilities have been really direct in the direction of helping to advance science,
4913680	4920640	to be honest. So I do think that this argument could ultimately say, yeah,
4922000	4928240	a faster takeoff is plausible and the mechanism could be reallocating the AI's cognitive abilities
4928960	4933360	towards science. I mean, GPT-4 is just trained to predict the next world on the internet.
4933360	4938960	Right. That's a very different kind of task than the task of advancing science. And so
4939680	4945280	I think that that is a reason to expect a faster takeoff. More of a jump. Yeah. Okay,
4945280	4950320	interesting. I haven't heard that argument before. Cool. Well, I want to now get to
4950320	4956800	the report that you've written on AI takeoff speeds, which asks how quickly AI might go from
4957360	4963360	kind of pretty economically valuable to just extremely capable, maybe as good as humans.
4964160	4969840	And yeah, I guess you define your terms pretty clearly in your report. So maybe we should start
4969840	4976240	by doing that. Am I right in remembering that you are trying to answer the question of how
4976240	4984160	quickly we'll go from AI systems that can do 20% of human tasks to AI systems that can do 100%
4984720	4990640	of human tasks? Is that right? Yeah, that's right. In particular, I am restricting to
4990640	4996320	cognitive tasks. That's similar to what we discussed earlier. It's any task that you could do
4996960	5000160	remotely that doesn't require you to be physically manipulating
5000880	5005360	objects yourself because AI's don't have physical bodies. That wouldn't be included for them.
5006000	5011680	I mean, it does include tasks like giving instructions to a human who's doing a physical
5011680	5017520	job telling them where to move things, what to do with their arms, or potentially giving
5017520	5022640	instructions to robots that are doing physical tasks. But it doesn't include the kind of the
5022640	5030800	physical motions themselves. So does it include things like driving cars? Yes, it does. Okay,
5030800	5041280	it does. And that's because driving cars is really a set of algorithms. And you can turn the wheel of
5041680	5049840	car. So it's a good point that currently the way humans drive cars is by physically moving
5049840	5058560	various levers in the car. But I think actually giving the AI the control of the steering wheel
5058560	5062560	and of the pedals and the brakes is actually pretty trivial. So the only thing that's hard
5062560	5069280	in practice about getting AI's to do driving is the cognitive parts of what should the car be
5069280	5077520	doing at each point. Yeah, right. Okay, so then an example of a task that isn't included is something
5077520	5083600	like helping people move house. It's like carrying the boxes in and out. Carrying the boxes in and
5083600	5089440	out would not be included. Yeah, okay. But telling them here's the best plan for moving, here's the
5089440	5094480	order you should move the boxes in that would be included. Okay, so that's basically what you've done.
5095200	5101200	You want to know how fast do we go from 20% of cognitive tasks to 100% of cognitive tasks?
5101840	5106880	Yeah, can you actually clarify what it means for AI to be able to complete 20% of tasks?
5107520	5112400	So you could say, okay, let's say we can AI automate driving, what percentage is that? Is that
5112400	5119600	3%? Because it's, you know, 3% of people do it? Or is it, you know, do we just like give, we look at
5119600	5124960	a long list of tasks and assume that it takes up an equal percentage? Like, what do we mean by 20%?
5124960	5131280	Yes. And so the way I'm currently thinking about that is you look at how much people pay for those
5131280	5135680	tasks to be performed in the economy. Okay. So let's take the driving example. I don't know. Let's
5135680	5142880	say that drivers around the world are being paid $2 trillion a year for the work they're doing,
5142880	5151440	driving trucks and taxis and everything else. In that case, because $2 trillion is 2% of the
5151440	5158800	global GDP, I would say that automating driving, fully automating all driving would be automating
5159520	5168160	2% of all economic tasks. Got it. And then you're saying, how fast will we go from we can do 20%?
5168160	5175040	So I don't know, maybe it's like replacing all drivers, maybe it's replacing all journalism
5175040	5181360	because GPT-4 seems to be really good at writing. And I don't know, a couple of other, a couple of
5181360	5188880	other things. How fast do you go from like that chunk to literally all cognitive tasks,
5188880	5194720	including, I guess, science, AI, R&D. Now, one complication is that from year to year,
5195440	5202160	the amount that is paid to people to perform each task might change. So in 2020, maybe drivers
5202160	5208000	are paid $3 trillion a year for their work, maybe in 2025, they're paid $4 trillion a year,
5208000	5213600	and that could change. So I'm pegging these percentages to the year 2020. Got it. It's a kind
5213600	5218880	of arbitrary choice just to make the definition unambiguous. Yes. Okay. That makes sense. So what
5218880	5225920	were they paid in 2020? And even if, I don't know, like wages change for all sorts of reasons,
5225920	5233280	including AI taking over some jobs, we're still just thinking of what percentage of the 2020
5233280	5239440	cognitive economy is being automated. Exactly. And it is really important to keep that in mind,
5239440	5245760	because typically when AI automates a certain task, it becomes really cheap to do that task.
5245760	5249520	Right. So it becomes a much smaller fraction of the economy. Exactly. Yeah.
5249520	5253520	And so you can end up thinking that AI is never doing anything when it's actually
5253520	5258000	done almost everything. And so that is just an important thing to be aware of.
5258000	5262640	Yeah. That makes total sense. Is there an analogy from the Industrial Revolution or something?
5263440	5269760	The best analogy might be agriculture. So I think in 1500, basically everyone worked in
5269760	5274880	agriculture. Right. It was 90% of the economy or something. Exactly. All of GDP would have
5274880	5280000	basically been agriculture. Today, I think it's less than 5%. And that's because we've become
5280000	5288160	really good at producing food with very little need for human labor. So it's not to say that
5288160	5296960	fertilizer and trucks and really highly productive seeds aren't contributing a bunch to the economy,
5296960	5302960	but clearly they have. But were you to measure it as a fraction of the GDP that they're
5302960	5307040	responsible for? It would be smaller because everything's just gotten so much cheaper because
5307040	5313600	of them. Exactly. Cool. Okay. That makes a bunch of sense. Nice. Okay. So we've got definitions.
5314320	5323760	So you've asked how fast will we go from AI systems that can do roughly 20% of the cognitive
5323760	5330960	tasks that humans were doing as of 2020? And how quickly will we go from 20% of those tasks
5330960	5337840	to 100% of those tasks being at least in theory able to be automated by AI systems?
5338720	5345440	So yeah, what was your result? So the conclusion from the report is, I guess, pretty scary.
5346320	5353680	The bottom line is that my median guess is that it would take just a small number of years to go
5353680	5360880	from that 20% to the 100%. So I think equally likely to happen in less than three years.
5361280	5369920	As it is to happen in more than three years. So a pretty abrupt and quick change is the kind of
5369920	5378560	median kind of best guess median. Wow. And do you believe that in your bones? Does that feel like
5378560	5385920	like very plausible to you? Yeah, I do. So some some quick things about why why it's plausible.
5386800	5393520	Each year, once you take algorithms, better algorithms and using more compute into account,
5393520	5398960	we're currently training AIs each year that have kind of three times bigger brains than the year
5398960	5405280	before. So really rough way to think about it. But you know, imagine three times smaller brain
5405280	5412960	than humans. That's chimpanzee brain size. Right. Each year, you're going from chimpanzees to humans?
5413600	5419440	That's, I think, you know, it's really hard to try and account for the effect of the algorithmic
5419440	5426720	improvements. But on my kind of best guess of what those amount to, yeah, each year, we're
5426720	5432720	making the brains of AI systems about three times bigger. Wow. And right now, it's humans that are
5432720	5438800	doing all the work to improve those AI systems. As we get close to AIs that match humans, we'll be
5438800	5447600	increasingly using AI systems to improve AI algorithms, design better AI chips. And so overall,
5447600	5455360	I expect that pace to accelerate absent a specific effort to slow down. Right. So rather than three
5455360	5460080	times bigger brains each year, it's going to be going faster and faster five times bigger brain
5460080	5467040	each year, 10 times bigger brain each year. And I think that that just already makes it plausible
5467040	5471760	that there could be just a small number of years where this transition happens where AIs go from
5471760	5478720	much worse than humans to much better. But to add in another factor, I think that it's likely that
5478720	5485040	AIs are going to be automating AI research itself before they're automating things in most of the
5485040	5492480	economy. Right. Because that's the kind of the task in the workflow that AI researchers themselves
5492480	5498400	really understand. So they would be kind of best placed to use AI as effectively there.
5498960	5503360	There aren't going to be kind of delays to rolling it out or trouble finding the customers
5503920	5510960	for that in the same way. The task of AI research is quite similar to what language models are
5510960	5515840	currently trained to do. They're currently trained to predict the next token on the internet,
5515840	5520800	which means they're particularly well suited to tech space tasks. Right. And the task of writing code
5521440	5526400	is one such task and there is lots of data on examples of code writing.
5526400	5531920	Oh, I see. So it's like typically, I don't know that much about coding. Is it basically also
5531920	5539200	token prediction? That is how current coding assistants work, I think, is that they're looking
5539200	5543520	at your kind of, you start writing your code and they predict what's going to follow. Like one way
5543520	5549200	of putting it would be by the time that the AIs can do 20% of cognitive tasks in the broader economy,
5549760	5557120	maybe they can already do 40% or 50% of tasks specifically in AI R&D. Right. And so
5557760	5560960	they could have already really started accelerating the pace of progress
5561760	5568400	by the time we get to that 20% economic impact threshold. I mean, at that point,
5568400	5573040	you could easily imagine really, it's just one year, you give them a 10x bigger brain,
5573040	5578160	that's like going from chimps to humans and then doing that jump again. That could easily be enough
5578240	5585920	to go from 20% to 100% just intuitively. And I think that's kind of the default really.
5586480	5590320	That's terrifying. Yeah. And I think there's even more pointing that direction.
5591040	5597120	I think that already we're seeing that with GPT-4 and other systems like that, people are
5597120	5602560	becoming much more interested in AI, much more willing to invest in AI. The demand for good AI
5602560	5610640	researchers is going up. The wages for good AI researchers are going up. AI research is going
5610640	5617760	to be a really financially valuable thing to automate. If you're paying $500,000 a year
5618320	5623920	to one of your human research engineers, which is lower than what some of these researchers are
5623920	5629120	earning, then if you can manage to get your AI system to double their productivity,
5630080	5634880	that's way better than doubling the productivity of someone who works in a random other industry.
5635440	5641200	Just the straightforward financial incentive as the kind of power of AI becomes apparent
5641200	5647120	will be towards, let's see if we can automate this really lucrative type of work. So that's just
5647120	5652000	another reason to think that we get the automation much earlier on the AI side, then on the general
5652000	5657600	economy side, and that by the time we're seeing big economic impacts, AI is already improving
5657680	5663040	at a blistering pace potentially. Okay, well that's, yeah, again, really scary,
5663040	5669680	like really genuinely very scary. I completely agree. Do you have a guess at what percent of
5669680	5674560	cognitive tasks AI can currently perform? It seems like we're really far away from 20 percent.
5675680	5680400	Yeah, intuitively, I think it seems like we're far from 20 percent because
5680960	5686080	AIs aren't doing that much in the economy. If I looked at a list of the kind of cognitive tasks
5686080	5690480	people were performing in 2020 and what they were paid for them, it's not as if AIs are ready to
5690480	5696320	kind of replace a big fraction of that labor. So that's just the 20 percent it's far off.
5697040	5701440	I'm actually less confident that it's far off than I used to be than if we would have had this
5701440	5710800	interview six months ago, because just seeing GPT4's performance, firstly, just doing really well
5710800	5717680	on just a whole wide range of university exams and other formal tests without having specifically
5717680	5721680	trained on that. And then me kind of playing around with it and thinking, yep, just seems smarter
5721680	5727360	than most people I talk about this stuff with. Most of my start friends wouldn't be this smart.
5729680	5735440	I'm thinking maybe actually if you just put some work into specifically applying GPT4,
5735440	5739120	you could automate quite a large fraction of the cognitive tasks.
5739840	5743920	It does seem it does seem much more plausible to me that maybe you could get to 10 percent today
5743920	5752800	or within a year. Wild. That's super interesting. I mean, I guess, yeah, I was also blown away by
5752800	5759200	GPT4's performance on, yeah, the SAT, the LSAT. For anyone who hasn't looked, we'll stick up a link
5759200	5765360	to those test results. I think it was performing, I mean, much better than I ever did on my AP tests
5765360	5772960	in high school and better than I did on the GRE. So it's like, it's beating me. And I think beating
5772960	5779920	out loads of other people already. Maybe I'm over or putting too much weight on the fact that like,
5779920	5784560	currently, not that many people I know are using it to do anything. And it sounds like your
5784560	5791680	impression is like, maybe it's pretty close to being able to do a lot. Yeah, to actually in practice,
5792560	5799680	replace 20% of the tasks that people do. It's actually a pretty tough thing to do. Because,
5799680	5804640	you know, for myself, all the different parts of my workflow are very much entangled up together.
5804640	5810400	So I can't easily take out a 20% chunk and be like, oh, GP4, do this chunk, because it's all kind
5810400	5816240	of mixed up. Sure. And so historically, the way that automation has worked is that we've got a new
5816240	5822000	technology and then we've spent decades readjusting our workflows so that we can neatly parcel out,
5822000	5825520	you know, 20% of our workflow for this new technology to do. And the technology can be
5825520	5830800	fairly dumb, because we've kind of really neatly parceled out that part of our workflow.
5830800	5839520	Yeah, do you have an example? Yeah, so let's say moving over from paper records to computer
5839520	5845680	records. So I used to have maybe people used to have to write down lots of paper records and
5845680	5850800	maintain a filing system for their information. And these days, a lot of that work is done
5850800	5857040	automatically by computers and data storage. But at first, you know, it wasn't that easy to immediately
5857040	5862560	move over to the computers. And it took decades as people were like, you know, got rid of the paper
5862560	5870400	stuff and got used to kind of teaching the other employees to use the computers and, you know,
5870400	5874800	got used to using their customers to fill out online forms rather than filling out the paper
5874800	5878960	forms that they're using before. And all that rearranging of workflows took a long time to
5878960	5888160	happen. And so one scary possibility is that if AGI is is just 10 years away, then there won't be
5888160	5896800	time to do that rearranging of workflows that is necessary to get, say, GPT-4 to actually automate
5896800	5904000	things in practice. And so the 20% automation ability won't happen through some kind of dumb
5904000	5909280	system that that I've kind of parceled out a nice thing to do. It will actually happen with a
5909280	5914160	really smart system that basically understands my whole workflow well enough to be able to do 20%
5914160	5917760	for me, which means that it could be pretty close to just being able to do all of it.
5918320	5924240	Right, right, right. I mean, I've almost got that impression with GPT-4 and my job. Like,
5924240	5930480	we asked it, how can you help us make the 80,000 hours podcast? And it was like, I can help you
5930480	5934960	come up with guests. I can help you write interview questions for the guest if you tell me what they
5934960	5940720	worked on. I can help you. I mean, it basically rattled off a list of things. And I was like,
5940720	5950240	as soon as it has a voice, like, that'll be it. That'll be it for me. And I think I thought it
5950240	5954960	would help me with subtasks first. I think I thought maybe it would help me with like generating
5954960	5960960	titles and like, I don't know, maybe giving me summaries of people's work so that I could read
5960960	5966720	them a bit faster. But I think in fact, it's actually going to be really great at like the start
5966720	5973040	to finish interview process. Yeah, soon enough that I'll just skip over all of that, which I don't
5973040	5977840	know if that's true, but it seems it doesn't seem crazy to me. And so maybe that's just another
5977840	5982560	actual example of what you're talking about. Yeah, yeah. So in that example, by the time it can
5982560	5986160	automate 20% of the kind of tasks you're doing, it can almost do all of it.
5986160	5991760	Right. Yeah, makes sense. Okay, so next, I want to dive into your methodology a bit more deeply
5991760	5997600	for this, yeah, this takeoff speeds prediction. It's such an alarming result that I've, yeah,
5997600	6003120	this urge to understand what's going on a bit better. Yeah, so that headline result, this
6003120	6008560	prediction that AI takeoff might only take a few years, is basically based on an economic model
6008560	6014560	that you made that tries to answer the question of whether you can get human level AI just by
6014560	6019200	increasing the amount of compute that we have to train our systems. So in other words, kind of
6019200	6024880	without paradigm shifting algorithmic breakthroughs. Yeah, to start us off, can you actually remind me
6024880	6035200	what compute is? Okay, so compute is a measurement of how, how many calculations you need to do
6035280	6042080	or a given computer is doing. So let me give an example. So the most common unit for measuring
6042080	6049680	compute is a flop and a flop is adding together two numbers or multiplying together two numbers
6049680	6055200	or dividing them or subtracting them. Okay, so mathematical operation. Exactly. Currently,
6055200	6061600	when we develop AI systems, the way we do it is by doing loads and loads of these calculations of
6061600	6067760	adding things together, dividing them, minusing them. And by doing all of these calculations,
6067760	6073440	that is, that is how the AI decides what, what it's going to do. And it's also how we, how we
6073440	6078000	train the AI in the first place. Got it. You could, you could analogize these calculations to the kind
6078000	6085520	of firing of neurons inside our own brain. Okay, so flops are basically just mathematical
6085520	6092880	operations or things like, are both of these things true or something like that? And then compute
6092880	6098480	is, sorry, it's something like how, how many of those calculations we can do?
6099360	6105200	So one flop is just one of those calculations. Some of the biggest language models today are
6105200	6114320	trained with, I think, 10 to the 24 flop. So that is a million, million, million, million
6114320	6121440	calculations. That's how many calculations you need to do to train some of today's big language
6121440	6127040	models. So the amount of compute is another way of saying how many calculations did you need to do it.
6127040	6133280	Got it. So as compute, compute is the amount of computation you need to do and a flop is the
6133280	6139520	kind of unit of how many, yeah, okay, cool. So then you're, you're asking this question about AI
6139520	6148000	takeoff speeds. And we're just assuming that we, we increase compute, which is made up of machines,
6148000	6154160	but also has to do with algorithms as well. So like, is it true that we need less compute if the
6154160	6159600	algorithm is super efficient, because the algorithm just requires that you do fewer calculations to
6159600	6164960	get the same result or something? The strict assumption is that if we used the algorithms that
6164960	6172320	were available in 2020, then there is some amount of compute such that if God handed a top AI lab,
6172320	6177280	that amount of compute, and they had a few years to adjust the algorithms to using that much more
6177280	6185680	compute, then they would be able to train AGI using that amount of compute. So then in the model,
6186240	6191440	we make an assumption about how much compute would have been required. We actually, we actually
6191440	6198880	put a probability distribution over it. But importantly, algorithmic progress can reduce
6198880	6208560	that computational cost over time. Got it. So maybe in 2020, you'd have needed 10 to the 30
6209200	6219440	flop to train AGI. But maybe by 2025, your algorithms are 10 times better. And so you only
6219440	6227200	need 10 to the 29 flop to train AGI. And so the basic dynamic in this framework is that
6227920	6235040	in each year, our algorithms improve somewhat. And we decide to use more compute in a training
6235040	6240560	run than we had done in the previous year. Right, because it's profitable, etc. Exactly.
6241120	6244880	And those two factors combine together. So let's say we use twice as much compute as the
6244880	6250560	previous year, and our algorithms are twice as good. And that means that the effective compute
6251200	6257040	that we're using is four times bigger. Right. So it's the equivalent as if we hadn't improved
6257040	6261440	our algorithms, and we had just used four times as much compute in the training run.
6262240	6270160	So you're using effective compute to mean something like you want an AI system to, for example,
6270160	6276880	predict the next word in a sentence. And one way you can increase the kind of
6276880	6281360	effectiveness of that out of that system is by like giving it more compute so it can do more
6281360	6286800	calculations. But you'll also have another dynamic where the algorithms are getting better such that
6286800	6293280	you need less to do the same thing. And so you're doing equivalent processes or you're doing
6293280	6300000	you're getting like equivalent outcomes for less physical compute. That is tricky.
6301040	6306240	One way to think about it is, let's say in 2025, we do it, we use a certain amount of compute in a
6306240	6312320	training run with 2025 algorithms. Imagine if we'd have been forced to use the 2020 algorithms.
6312960	6316880	How much compute would we have needed then to get the same result?
6316880	6323520	Right. Got it. That is the amount of effective compute that we actually used in 2025.
6323520	6330640	Great. That makes sense to me. Okay. So you are, you're thinking about effective compute,
6330640	6337120	and you're making some guess about how much we'll need to get 100% of cognitive tasks automated.
6337120	6342800	Yeah. How are you making guesses about how much effective compute we'll need to get 100% of
6342800	6353040	cognitive tasks, um, automatable. So in the report itself, I just defer to a different report by a
6353040	6358400	colleague of mine called the bio anchors report, which asks that exact question. In fact, I don't
6358400	6363200	think you need to be deferring to the bio anchors report that there are, you know, different approaches
6363200	6369760	you can take to estimating how much effective compute you might need to train AGI. And you could
6369760	6376000	use whatever approach that you like, then bring that into, into my framework and use it to inform
6376000	6380240	your, your initial guess, um, of the effective training compute for AGI.
6380240	6385440	Right. And you've got a model that people can play with. So if you're like, I think the bio
6385440	6392160	anchors report is way too optimistic about how much, uh, compute it'll take to get AGI,
6392160	6396720	you can 1000 exit, um, and see, and see how that changes the outcomes.
6396800	6401360	Exactly. Super cool. Okay. So we'll, we'll stick a link up to that model so people can play with
6401360	6408400	it if they want to. Do you mind, um, giving me an intuitive sense of how much compute you and,
6408400	6411440	and your colleague, um, basically think it'll take to get AGI?
6412240	6423280	So in the current median value I use in, in the report is, is very large indeed. It is 10 to the
6424080	6432240	36 flop. So that is, if you take the amount of compute that was used to train the biggest
6432240	6437760	language models that publish their training requirements, and then you use a million times
6437760	6442640	as much and then a million times as much again, that's how much the assumption is making. So,
6442640	6446800	so I actually now think that that, that assumption is too high.
6446880	6452560	Is that because of GBT four and how, how impressive it is basically largely?
6452560	6457280	Yeah. GBT four and the kind of the fast pace of recent improvements is quite a lot faster
6457280	6463040	than I would have predicted. Um, and so yeah, I would, I would now be using it a lower value
6463040	6467200	for that important, important parameter, uh, which would make take off even faster than
6467200	6473120	I'm predicting even faster. Yeah. Geez. Yeah. The kind of the report that I wrote uses,
6473120	6478720	yeah, this 10 to the 36 as its, as its median estimate for what you'd need to train AGI.
6478720	6485520	Okay. Cool. That's helpful. Okay. So, so that's how you basically estimate how much
6486400	6492880	effective compute you need to train AGI. How do you use that to predict, uh, yeah, AI takeoff speeds?
6494080	6500000	Right. So we have this, um, assumption about the effective training compute for AGI,
6500000	6505200	which was our hundred percent kind of endpoint. We then need to make an additional assumption
6505200	6511920	about what would be the effective compute needed to train AI that could automate 20% of tasks.
6512720	6522000	So let's say that we assumed, for example, that you need 10 to the 30 flop to train AGI using
6522000	6528320	2020 algorithms, that'd be 10 to the 30 kind of effective compute. We then make an additional
6528320	6534160	assumption about how much less effective compute you need to train AI that could automate just
6534160	6540400	20% of tasks. So I kind of want to pause on that last assumption because it's so important.
6540400	6546000	So that, that, that assumption about how many more times compute you need for AGI compared to
6546000	6552720	20% AI, that assumption is what I'm calling the difficulty gap because it's saying what is the
6552720	6561760	gap in difficulty between training 20% AI and training 100% AI or AGI. And then we're measuring
6561760	6567760	the size of that difficulty gap in terms of how many times more effective compute you need to train
6567760	6573680	one than the other. And you're calling it the difficulty gap because it's kind of describing
6573680	6580240	how much more difficult the most difficult tasks are relative to the, the easiest 20%.
6581200	6585760	The reason I call it the difficulty gap is, is to refer to the difficulty of developing the AI
6585760	6592720	in the first place. So it's kind of like how much more difficult is it to develop an AI that can
6592720	6600640	do 100% of the tasks than it is to develop an AI that can only do 20%. Got it. But it might be 10,000
6600640	6606160	times as difficult, or it might be barely more difficult at all. If, if it turns out that
6606880	6612320	once you're 20% there, you're basically the whole way there. Right. I guess is that possible?
6612320	6620320	Maybe it is if like the first 20% of tasks includes AI R&D. So that's an interesting
6620320	6626720	scenario to think about. You could have the first 20% of tasks, including all of the tasks of AI R&D.
6626720	6631520	So what I think would happen in that scenario is that once you've done those first 20% of tasks,
6632080	6637920	AI would be improving super, super quickly, absent a specific effort to slow down, you know,
6637920	6644560	within I think a few months, you would already be able to do a training run that used 100 times
6644560	6649680	more effective training compute as you had previously done. And that's because
6650560	6656640	that's because you would have hundreds of millions of AI's that could be working to improve the AI
6656640	6665360	algorithms and maybe making money so you can buy more AI chips or convincing other people to kind
6665360	6672000	of share their compute with you. And then that would be enough to very quickly allow you to use 100
6672000	6678800	times as much effective compute. Cool. Okay. Yeah. So I guess maybe you think that which tasks end up
6678800	6685360	being easier also plays into how fast AI takeoff speeds are. Yeah. I guess in particular in the
6685360	6691040	case where AI R&D is in the first 20% and otherwise maybe it doesn't matter as much. Exactly.
6691840	6698160	I think that with that last example, even if there was a big difficulty gap from 20% to 100%
6699200	6705920	of cognitive tasks in the economy, if you get all the R&D tasks within that first 20%,
6705920	6710080	then I still think you'd get a very quick transition. Right. Okay. So that could be an example
6710080	6715040	with a big difficulty gap where you nonetheless you still get a very fast AI takeoff.
6715360	6722320	Yep. That makes sense. Okay. So do you basically just make an assumption about how big that difficulty
6722320	6729120	gap is? Is it a range? And how did you come up with whatever numbers you're putting on to?
6729120	6732240	Yeah. How many times harder it is to get to 100% of tasks?
6733440	6739360	So I do consider as much evidence I can for the difficulty gap. It is really, really important.
6740080	6746400	The lines of evidence that I consider are all pretty limited. So it's a very uncertain parameter,
6746400	6750320	but I think there are some things you can learn from some of those lines of evidence.
6750320	6754960	Okay. What's an example of some of the evidence you would have looked into?
6755840	6759680	In terms of evidence for the difficulty gap potentially being pretty small,
6759680	6764800	we've already touched a little bit upon some of that. So one line of evidence is the
6765760	6772960	scaling of human cognitive ability with human brain size. Some humans have slightly
6772960	6778240	bigger brains than others. Not only a small variation, plus or minus 10% or so,
6778240	6783840	but you can then look at, okay, if one person has a 10% bigger brain, then on average,
6783840	6787360	how much better do they do on various tests of cognitive ability?
6788160	6793680	And the difference isn't massive, but if you extrapolate that difference to say, okay, what
6793680	6800480	about if it was a three times bigger brain or a 10 times bigger brain, then extrapolating that
6800480	6806880	suggests that there would be a very large difference in cognitive abilities from getting
6806880	6812160	a brain that is that much bigger. Interesting. The takeaway from that is that this particular
6812160	6819360	line of evidence suggests that increasing the size of the brain by a factor of 10 could be more than
6819360	6827600	enough to cross this difficulty gap. And that by analogy, increasing the number of parameters
6827600	6833200	in an AMI model by a factor of 10 could be more than enough to cross the difficulty gap,
6834160	6840160	which would require you to increase the effective training compute by a factor of 100.
6840720	6845760	Okay. And I think even this analogy actually suggests that just increasing the effective
6845760	6851360	training compute by a factor of 10 might be enough as well, because it could just be enough to increase
6851360	6856960	the human brain size by a factor of three. Right. So this particular line of evidence
6856960	6861280	really suggests that the difficulty gap could be pretty narrow. Pretty small. Yeah, yeah, yeah, yeah.
6861280	6868000	Okay. Got it. Yeah. Is there more evidence about how big that gap is? So a very similar line of
6868000	6873280	evidence looks at rather than differences within humans, looks at the differences between humans
6873280	6879120	and other animals. Chimps have brains that are about three times smaller than human brains.
6880080	6886160	And you might think that going from chimp level intelligence to human level intelligence is enough
6886160	6891520	to cross that difficulty gap. And if you do think that that then again suggests that just
6891520	6897120	increasing the parameters in a model by just a factor of three could be enough to cross that
6897120	6902560	difficulty gap. Okay. So that's some reasons to think it could be kind of small. Are there any
6902560	6908320	reasons to think it could be, yeah, really much bigger? Yeah. So those two reasons to think it's
6908320	6914480	small are both taking a view on intelligence, which is kind of one dimensional. We're kind of
6914480	6919840	imagining that some humans are cleverer than other humans and humans are cleverer at chimps.
6919840	6922960	And we're just imagining as you make the brain bigger, they just get smarter and smarter.
6923840	6928160	The perspective which suggests that the difficulty gap could be bigger is a perspective which
6928160	6933440	emphasizes that actually there's not one dimension of intelligence, but actually there's loads of
6933440	6940480	different tasks in the world. And those tasks, you know, have very different requirements.
6941120	6945120	And so AI might get good at some of them way before it gets good at other ones.
6946160	6954480	Off the bat, I don't find that that intuitive because the brain seems to be so flexible. So,
6954480	6959360	yeah, the training of these AI models on these tasks, you'd have to think that they were just
6959360	6965680	pretty different from the human brain and much less flexible. I think it is true that if you expect
6965680	6976240	AI to be a pretty general learner and have pretty general abilities, then that would lend itself to
6976240	6981600	the one dimensional view and against this view. But I do think that there are reasons to think
6981600	6988400	that AIs would be better at some tasks than others. So in particular, at the moment, the best AI
6988400	6993760	systems are trained to predict the next word on lots and lots of internet data. And that means that
6993760	6999520	AIs are just particularly good at tasks that are similar to that in some way. So for example,
6999520	7005360	writing a newspaper article or writing an email, that's really similar to a task where it's seen
7005360	7011120	loads and loads of examples. And so AIs are in fact particularly good at that type of task.
7012080	7018640	Whereas taking another type of task, like let's say planning out how to put the equipment on a
7018640	7026080	factory floor and then giving instructions to different people about how they should make that
7026080	7031760	happen, that might be something that it just hasn't seen many examples of in its training data. Or
7031760	7037760	another task could be manipulating a robot. So the robot kind of does a certain task. That again
7037840	7041520	is something that AIs just haven't seen many examples of. So you'd expect them to be much
7041520	7046960	worse at that kind of task. One interesting example could be thinking about what you need to do to
7046960	7052240	make a certain factory run very efficiently. It could be that some of the workers in that factory
7052240	7057280	have just kind of internalized that know how inside their own brains, but it's maybe not even written
7057280	7061920	down anywhere. Then if you were trying to get the AI to now run the factory floor, it could be
7061920	7067520	particularly hard for it to know what to do there because it doesn't have any examples or any
7067520	7072720	experience of that kind of thing. Got it. Okay. So some of the difficulty gap might not be about,
7073360	7079680	I don't know, like fundamental facts about the types of intelligence that you might use to perform
7079680	7084800	different tasks and might be much more about the type of data, the types of data we have. And
7085760	7091920	some things might be difficult just because we never write down what it means to do those
7091920	7097440	things. And so it's harder to teach an AI system to do it, not because they're fundamentally
7098000	7104240	extremely difficult in none of themselves. Yeah, that's right. Cool. Okay, that really helped.
7104240	7109760	Next. I'll give one more example about how some tasks could be easy than others. So some tasks,
7110480	7115920	it's really important that you have a very high amount of reliability. So for driving,
7115920	7121760	it's really just really awful if you crash. So if you're 99% reliable, that's worthless.
7121760	7126800	And so if AIs, I kind of can get to 99% reliability fairly well, but can't get
7127680	7134400	99.99999% reliability, then that's going to block them on certain tasks. But other tasks like drafting
7134400	7139600	emails or even sending emails and being a personal assistant drafting code that you can
7139600	7144640	kind of check whether it works before deploying it. Those tasks, it doesn't provide a blocker for.
7144640	7149360	So that's just another example of something that could mean that the AI is kind of ready to automate
7149360	7155200	certain tasks before others. Cool. Okay, yep, that makes that makes sense of sense. Yeah,
7155200	7163520	so then I guess given this type of evidence, what was the range of amount of effective compute
7163520	7168960	that you'd guess we'd need to go from 20% of cognitive tasks to 100% of cognitive tasks?
7169840	7174160	You know, the main takeaway is that a really wide range of things are plausible. So I think
7175040	7181280	as low as just 10 times as much effective compute could be sufficient. And that's pretty scary,
7181280	7185280	because that's the kind of thing that could just be some quick algorithmic improvements
7185280	7193360	without even the need for more physical compute. So I think that is just very consistent with
7193520	7199600	this kind of one dimensional view, and really not something we can rule out. But my kind of
7199600	7205200	best guess is more like 3000 times as much. And that's kind of where my median is.
7205200	7211600	Okay, so quite a lot more. So quite a lot more than that lower end. And that's because I do expect
7211600	7217520	there to be some significant comparative advantage components, where the AI is just kind of particularly
7217520	7221520	good at some tasks compared to others, and particularly struggles with certain types of tasks.
7221520	7225840	And so I do expect that to stretch things out. And you know, I do think it's possible that that
7225840	7230080	stretches things out by even more like I think it could be it could be a million times as much.
7231040	7237520	That's hard to rule out. Okay, so huge range. It's a really huge range. Yeah.
7237520	7242720	And you've put it into your model as as a huge range. Is that right? That's right. So there's
7242720	7248000	yeah, so in the model, there's firstly a probability distribution over how much
7248000	7254160	effective compute you need to train AGI. And then there's another probability distribution over
7254160	7263200	how much less compute than that do you need to train 20% AI 20%. Okay, so you've got ranges for
7263200	7270320	how much effective compute you need to do both 20% and 100% of tasks. What do we know about how
7270320	7276880	quickly compute might increase? I guess one very simple thing we could do is just make more computer
7276880	7282400	chips. But I don't know, yeah, what the limits to that are. And presumably, yeah, there are other
7282400	7288000	things as well. How do how do we make compute go up? Yeah, and just it is importantly effective
7288000	7293600	compute that includes includes the algorithmic improvements. Got it. Yeah, thanks. So one natural
7293600	7298960	way to approach this could be to first discuss the types of changes that are increasing effective
7298960	7305120	compute today, and then how that might be different once we actually get to the 20% AI.
7305120	7308800	Sure. Yeah, tell me about how effective compute is increasing today.
7309680	7317040	So the first way is quite simply that we're spending more money on making computer chips on
7317040	7323200	compute. Right. And we're also spending more money on using compute for training runs. The amount
7323200	7327120	we're spending on compute for training runs is growing particularly quickly right now. It's
7328400	7332480	over the last 10 years has gone up by about a factor of three each year.
7333200	7338160	And so what does it mean when we spend more on compute for training runs in particular,
7338160	7341440	as opposed to just more compute, like more computer chips?
7342320	7348720	Good question. So there's a certain amount of computer chips in the whole world. But at any,
7348720	7353200	you know, at any point in time, maybe only a small fraction of those are being used
7353200	7359440	in the largest training run. Okay. For an AI system. So one change you can make is you could
7359440	7364800	say we're going to make there be twice as many computer chips in the world. And that would take
7364800	7370080	a big effort. That would take quite a few years to do probably. But another change that you could
7370080	7378400	make much more quickly is say, well, as of today, we've only ever used one 10,000th of the world's
7378400	7384240	compute to actually use it in a training run. So we can quite quickly just use 10 times as much.
7384240	7391760	We'll just kind of buy a bigger fraction of the already existing compute. The simplest example
7391760	7398640	would probably be DeepMind has historically only used a small fraction of Google's computer chips
7398640	7403360	for its training runs. And then it says, okay, we want to now use all of your computer chips.
7403360	7408000	Got it. And that could be maybe that could be a hundred X increase. I don't know. Okay.
7408000	7411520	You know, that was just an example to illustrate the principle. I don't think that's what's
7411520	7417280	actually been happening at all. So I think what's actually been happening is that new computer chips
7417280	7423600	are being made each year. And a bigger fraction of those new chips are being used for the largest
7423600	7430080	training run. And you can actually see that the fraction of chips that are AI specific chips has
7430080	7435360	been increasing very quickly in terms of the production. So that's one way we can get more
7435360	7442880	effective compute. Yeah, are there others? Yeah. So the second big way is improving the
7442880	7449200	quality of computer chips. So we said that the first way was spending more money on compute.
7449200	7455280	The second the second way is that each dollar you spend gets you more compute. So the best
7455280	7460720	data that I've seen on this suggests that every two and a half years, compute gets twice as cheap.
7460720	7466400	Okay. And that's because that's different from like algorithms getting better. That's like
7466400	7471680	computer chips get more efficient because like the hardware is designed better.
7472320	7479920	Yeah. I think historically, it's often been about stuffing more processing units onto each chip.
7479920	7485680	And you know, maintaining still can managing to make these chips fairly cheaply.
7485680	7491920	Okay. So you can you can buy more chips, you can make better chips. Yeah. Anything else?
7491920	7497200	So then the third one is algorithms. So then you've got to, you know, you've now spent a
7497200	7502080	certain amount on compute, you've got a certain amount of computers as a result of that. And then
7502080	7508240	algorithms then say how effectively and efficiently can you use that compute to actually train an AI
7508240	7515040	system? Cool. The most famous example of this type of improvement is a paper called AI and efficiency.
7515680	7521680	That open AI published, I think in 2018. And what they did is they, they said to achieve
7522400	7528160	kind of a fixed level of performance on ImageNet, which means to kind of be to be kind of fairly
7528160	7534400	pretty good at classifying what is shown in an image. Yep. How much is the compute required to
7534400	7541520	get that performance falling over time? Right. Okay. And so they found that I think every 15
7541520	7548240	months or so that the amount of compute you needed was halving to achieve that fixed performance.
7548240	7557840	Okay. And that's basically programmers being clever and writing programs that help AI systems
7557840	7563440	figure out what's in an image in more and more efficient ways. Exactly. So, I mean, analogy could
7563440	7568400	be maybe a hundred years ago, our schools were really inefficient at transmitting knowledge
7568400	7574480	into pupils. So maybe you had to go to school for 15 years to learn geometry. As maybe today,
7574480	7578720	you can learn that geometry in a kind of really well designed course that just last three years.
7579760	7587200	Nice. That's a good analogy. Okay. So basically you have three kind of buttons to push to get
7587200	7594320	more effective compute. So one is chips, just like number. Another is how good the chips are
7594400	7600800	at processing. And then the third is algorithms. So how good are the programs that get run on the
7600800	7608320	chips? And have you basically tried to predict how quickly each of those things will go up?
7608320	7616320	Exactly. So my starting point is looking at what's going on today. Epoch is a kind of research
7616320	7622080	organization which I think has done the best research into this that I'm aware of. And they're
7622080	7628640	looking at trends in all of the three quantities, which I just described to you. So firstly, they're
7628640	7633600	looking at how much more is being spent on the biggest training runs in each year over the last
7633600	7639120	10 years. And they're finding about three times more each year. Did you find that surprising?
7640560	7644800	I know that training runs have been getting much more expensive in the last 10 years. So wasn't
7644800	7649600	that surprising for me? So there has been a big increase over the last decade. Yeah. This is
7649600	7656480	making me realize I should have asked earlier, what is a training run? Is it actually just like
7656480	7662160	insane amounts of data about, I don't know, like what everything that's on the internet you're
7662160	7668800	giving to GPT and being like, figure out how to predict the next word. And it takes like,
7669520	7675920	does it take weeks? Or does it take less but just tons of compute? So I think it takes months.
7676480	7684480	Okay. So it's a bit like you give GPT the start of some kind of web page, the first 50 words,
7684480	7691360	and you say, predict the 51st word. And in making that prediction, GPT is going to do a large number
7691360	7699360	of calculations. Let's say it's doing 300 million calculations in order to predict that next word.
7700160	7705280	So that's already a lot of calculations. Right. But you ask it to do that same task
7706320	7711680	let's say 10 trillion times, because you get it to predict the 51st word, and then you're like,
7711680	7717920	now predict the 52nd word. And it does another 300 million calculations and predicts that
7717920	7722880	57th word. And you keep doing it until you've literally done it, like I said, 10 trillion times
7722880	7727840	for different words on the internet. Got it. And actually, you're actually doing, it's actually
7727840	7733600	doing like kind of millions of examples at once. So you're kind of, that's how you can make it a
7733600	7738320	little bit faster. So otherwise it would be taking years, but we can do it in, we can do it in months
7738320	7742320	because you can get it to, you know, predict multiple different articles at the same time
7743360	7746320	to speed things up. And that's why you're using so many different computers.
7747280	7754400	Okay, so that takes us to compute. And companies have been spending about three times as much
7754400	7761600	on computer chips, or yeah, whatever that equivalent is, every year for the past decade.
7762160	7767840	Ten years, yeah. Okay, cool. So they've been using, they've been spending three times as much
7767840	7772080	on the chips that they use for these big training runs. So they may, they may not have
7772080	7776640	been increasing their total spending on chips by as much. For example, Google may just be
7776640	7780080	using a bigger fraction of its chips for these training runs over time.
7780640	7785760	Yeah. Okay. And what are the trends in computer chip quality?
7786640	7790320	So I already mentioned this one, each, each two and a half years,
7790320	7795040	the price of compute halves. So that means, you know, if you're spending a constant amount
7795040	7800560	on chips, then each two and a half years, you'll be able to buy twice as much compute.
7800560	7805760	Right, cool. And you said that the efficiency of algorithms is doubling about every 15 months.
7806400	7808400	Yeah. How do, how do these trends fit together?
7809120	7814640	So, you know, if you combine all of those trends together, then the result is that the effect
7814720	7821200	of compute on the largest training run has been increasing by a factor of 10 every year.
7822000	7826800	So you've got something like a default improvement year by year of 10x.
7827440	7832080	What happens when you then try to adjust those numbers for the fact that things are
7832800	7834960	changing over time? I'd guess accelerating.
7835680	7839360	That's right. So we could take, we could take those three quantities one by one.
7840080	7846800	So in terms of the money spent, I think that could go either way. So one possibility,
7846800	7854160	a scary possibility, is that AI companies develop this AI that can, you know, automate 20%
7854160	7858960	of cognitive tasks in the economy. And they're like, man, this can make us loads of money.
7859600	7866160	Investment flows in and they're able to very quickly spend even more on training runs,
7866240	7873440	or maybe just use a bigger fraction of existing chips in the world. So maybe Amazon's like,
7873440	7877600	well, we've got all this compute that we were previously using on like these web services.
7877600	7882800	But actually, given how lucrative this AI stuff looks like, why don't we team up with an AI lab
7882800	7887920	and let them use our compute that we've just got doing the stuff that isn't that economically
7887920	7892720	valuable, instead use it to train an even better AI. That is a very scary possibility.
7893680	7899840	A more optimistic possibility could be that by the time we get to the 20% AI, we're already
7899840	7906080	spending loads and loads on these training runs. Maybe we're already spending $100 billion. Maybe
7906080	7910800	we're already using all of Amazon's chips because we, at some earlier point, we already teamed up
7910800	7916080	with them. And that would be, that would mean that it wasn't possible to keep increasing the
7916080	7919840	amount of money spent on these training runs year on year. So you could have actually slower
7919920	7926720	growth than the recent kind of 3x per year pattern. It could be much slower. So that is a major
7926720	7932880	source of uncertainty in these takeoff addictions is just the thing that's particularly scary about
7932880	7938880	the short timelines possibility is that maybe you can get to the 20% AI with just spending a billion
7938880	7945680	on a training run. That would leave plenty of scope to spend 10 or 100 times as much
7946320	7950800	very soon after on a bigger training run, which would be hugely risky.
7951440	7956320	Okay, so it sounds like the number of chips that we might be using for training runs
7957120	7961840	might be growing faster or it might be growing slower by the time we're at systems that can
7961840	7967760	perform 20% of tasks. What do you expect to happen with the quality of chips and with the quality
7967760	7974000	of algorithms? So I think with those, it's easier to predict that their improvement should be faster
7974000	7979200	once we're at the point that AI can perform 20% of cognitive tasks. And that's just due to the
7979200	7984640	dynamic that I've been referring to quite a lot during our conversation, which is that
7984640	7990320	one of the things we're going to use AI's to do is to do the task of designing better chips
7990320	7995200	and to do the task of designing better algorithms. And there are already examples of that happening.
7995200	8002400	AI's that are using deep learning techniques to kind of cram transistors more efficiently
8002400	8009920	into computer chips. And again, with the algorithms, we are already seeing AI help significantly
8010640	8017440	with some coding tasks with things like co-pilot. And so by the time we're at 20%, AI expect that
8017440	8024320	effect to be much larger than it even is today. And so I think it's fairly, you know, the directional
8024320	8028000	prediction is fairly straightforward that we should expect both the quality of chips and the
8028000	8034160	quality of algorithms to be going faster once we get to 20% AI than they are today.
8035040	8040160	And it's really hard to predict exactly how much faster or what the change is going to be,
8040160	8044960	because you've also got people talking about the end of Moore's law. And it's just hard to
8044960	8048960	anticipate the specific improvements that we might be making with chips and with algorithms.
8048960	8051520	Right. And can you quickly explain what Moore's law is?
8052320	8058960	Yeah, for a long time, the way that computer chips have got more efficient is by cramming
8058960	8064320	more and more of these processing units onto each chip and making the processing units smaller and
8064320	8069120	smaller. But at a certain point, people think you just won't be able to make them any smaller
8069120	8074960	because you're running against fundamental limits. Yeah. Yeah. Yeah. Okay. And have we started to hit
8074960	8079440	that limit? I think people acknowledge that it's starting to get much harder to make further
8079440	8085600	improvements. But even then, there's a big open question about how much you can improve
8085600	8090240	chips in other ways. So, you know, just because the way we previously improved chips was to
8090240	8094080	make these processing units smaller, doesn't mean that there aren't going to be any other
8094080	8097840	types of improvements. And in fact, recently, other types of improvements have become very
8097840	8104400	significant from generation to generation. For example, chips becoming specialized for
8104400	8110240	being used for deep learning calculations in particular. Oh, wow. And there's probably a
8110240	8116480	lot more games you can get from that kind of specialization. That kind of, okay. So, I guess
8116480	8122240	we've got some data suggesting that maybe the way that we've been improving the quality of chips
8122240	8127600	isn't going to keep making improvements at the rate that we've been making those improvements.
8127600	8132240	But there are other improvements we can make. And so, we might still just really expect them
8132240	8138160	to keep improving over time. Yeah, at least for another couple of decades would be my expectation.
8138720	8142240	Right. And on the time scales we're talking about, that's a pretty long time.
8142800	8147600	It is. It is. If you think AGI would be really hard to develop, maybe you can hope that Moore's
8147600	8151840	Law will have run out before we get there. But if you have, if you're like me and you have shorter
8151840	8157040	timelines, then you're expecting to have it within two decades. Not a major source of hope for a slow
8157040	8163600	down. But anyway, in terms of the size of these, the speed up from AI accelerating algorithmic and
8163600	8168800	kind of chip design progress, it's hard to make a kind of an estimate that's informed by specific
8168800	8175680	data and by forecasting the specific improvements. But what you can do is you can run a simulation
8175680	8181360	through a kind of an economic model of automation. Okay. And roughly the way that works is that
8182000	8188560	if AI can perform, let's say, 50% of tasks involved in improving algorithms, then what the
8188560	8194080	model says is that humans will just work on the remaining 50%. Right. Okay. And then so humans
8194080	8201760	will be doing twice as much on that remaining 50% as they used to be. Oh, I see. Okay. And how
8201760	8207200	realistic is that assumption? Do we think all humans will just go find employment elsewhere
8207200	8212320	and will get double the labor on like the things that are particularly hard for AI?
8212880	8218400	So I think it does vary. For something like improving AI algorithms, that is what I expect.
8218400	8224240	Like for example, with co-pilot, we're not co-pilot is the thing that basically helps you write code.
8224240	8227840	That's right. So you're kind of writing your code and the AI is actually reading the code you've
8227840	8232160	already written, and then we'll predict what code you might like to write next. Okay. So you could
8232160	8235360	be like, I'm about to write, you could write down in the code editor, you could write, I'm about to
8235360	8240080	write a function that adds up three numbers and then multiplies that by a fourth number. And then
8240080	8245200	the code editor would read that and could just write the function for you. Okay. So presumably
8245200	8251120	when you get co-pilot, all those coders are just going to do other other coding stuff that co-pilot
8251120	8257200	can't help with yet. Exactly. And so that suggests that the kind of the kind of model I refer to,
8257200	8262560	you know, might be accurate. And also for hardware design, I know less about it.
8263280	8269920	But my sense is that the top talent in these hardware R&D organizations, it's not going to
8269920	8275120	get laid off that they're very much in demand. And that if AI is doing some tasks that they
8275120	8279680	used to spend their time on, they will move on and specialize in other parts of their workflow.
8280240	8287680	Right. Okay. Okay. So there are going to be jobs for them to do. And they will probably
8287680	8294960	contribute to acceleration of progress, not just kind of be replaced by AI and then like
8294960	8301440	sit around and knit, at least in some sectors. Exactly. And specifically in algorithm improvements
8301440	8305040	and hardware improvements. In the sector that really matters. Those are the sectors where I
8305040	8310720	really do expect that dynamic to happen. Right. Right. Okay. So when you run this kind of dynamic
8310720	8316400	through a kind of task-based growth model like this, you get out that as we move kind of between
8316480	8323840	20% AI and 100% AI, you'd expect a kind of two or three X acceleration in the pace of progress
8323840	8330640	of algorithms and of hardware. So if we'd said that spending more money was 10 X a year in terms
8330640	8336000	of what we've had recently, and if three X of that came from spending more money and three X of it
8336000	8342080	came from better hardware and better algorithms, then maybe in this new regime, we're going to have
8342080	8346240	that better hardware and better algorithms rather than improving by three X every year,
8346240	8355040	they might be improving by 10 X every year. Wow. So that could easily leave us at 30 X improvements
8355040	8361600	every year. And it's conceivable you can have 100 X if you have a kind of if the money is going up
8361600	8367280	and the kind of effects of AI automation are very significant. And again, to kind of help me
8367280	8374480	understand that intuitively, you think language models that we have now are improving at what
8374480	8379680	rate? I know it's a tough question, but even just to like give me some starting point.
8380240	8386960	I said before that I thought the effective compute used to train them was increasing by a factor of
8386960	8394800	10 each year. And so then in this new regime, that might translate into maybe kind of 30 times
8394800	8399920	as much effective training compute each year. And another important thing to keep in mind is that
8399920	8405520	as the AI is automating more and more of the tasks, that's getting faster and faster. So the
8405520	8411040	numbers I gave were kind of averaging across that whole period going from 20% to 100%. But in
8411040	8417920	reality, it's going to just be getting faster and faster as the AI improves and automates more of
8417920	8426240	these tasks in algorithm and hardware. And that's basically, I mean, a really intuitive way to think
8426240	8433920	about that might just be again, the economic growth we saw in the 1100s relative to the 1900s.
8433920	8438800	And that's like more and more is automated by things like the Industrial Revolution freeing up
8438800	8444880	more labor to do other types of tasks. And you just get increasingly more human labor to do
8444880	8452720	harder and harder things. Yeah, exactly. So once AI is performing 90% of the R&D tasks,
8452720	8456720	then theoretically, all of your human labor can be working on that final 10%.
8457920	8464320	Right. And the AI is, you know, doing plenty of the of the initial 90%. And so then naively,
8464320	8467920	you would expect things to be going 10 times faster at that stage. That's really fast.
8468960	8472560	This is all assuming that we don't make a concerted effort to slow down. Right.
8472560	8478080	And I think that we should. And I think we can. But yeah, these are kind of the predictions just
8478080	8483920	assuming that people are kind of going ahead of their normal steady pace. Sure. Okay, so we could
8483920	8489440	make a concerted effort to slow down, but it's not necessarily the default. Are there any kind of
8489440	8494800	actions different institutions take, I don't know, on the side of these companies or on the side of
8494800	8498720	the government or something? What are you most optimistic about being able to actually slow
8498720	8506880	this down? Probably the most exciting thing at the moment is the prospect of companies agreeing
8506880	8514880	to have their AI systems evaluated after they've been trained for various dangerous capabilities
8514880	8522880	that they may have. Okay. So for example, the alignment research center, ARC, did these tests
8522880	8530000	with GPT-4 before GPT-4 was released publicly. And they, for example, tested whether GPT-4
8530000	8536080	would be able to do what's called surviving and spreading, by which they mean kind of escape
8536080	8543440	the computer that it's initially being run on and find another computer where it can then run itself
8543440	8548080	on some computer and then maybe kind of earn money in some way so that it can kind of sustain
8548080	8554160	itself over time. How do they do that? So I don't know the details, but I believe that they
8554960	8561600	first ask GPT-4, okay, this is the scenario you're in, you're in an AI, you want to escape,
8562480	8567760	what would your proposed plan be? Okay. And then GPT-4 proposes a plan and then they kind of prompt
8567760	8570880	it further to say, okay, what would be the, you know, the sub steps you take for the first step
8570880	8575760	of the plan? And then they kind of try and walk it through just doing every single part of that
8575760	8585280	plan and just see how far it gets. Right. And I'm hoping that they determined GPT-4 couldn't
8585280	8590880	do all of the steps at the moment. That's right. Okay. Yeah. That's reassuring. So GPT-4 did
8590880	8597200	some of the steps. Oh, God. Very well. It wasn't totally incompetent, but yeah, it gets stuck at
8597200	8604480	some of the things and gets confused and isn't able to do all those steps. Okay. Okay. So that's
8604560	8610240	a kind of thing that would probably in practice lead to slowing down because you'd have these groups
8610240	8616080	evaluating things, maybe stopping them before they're rolled out in some like higher scale way.
8616080	8621600	Exactly. So, you know, the idea would be all of the labs are getting their systems tested,
8622240	8627680	you know, by ARC and maybe by other similar organizations. And they're all kind of agreeing
8628240	8632560	or making public statements to the effect that if their AIs do have dangerous capabilities,
8632560	8637680	they won't release them and they won't train more capable AIs. And that would block the kind
8637680	8642560	of dynamics I've been talking about here because you wouldn't be able to just use your AIs to
8642560	8647760	accelerate AI progress because you wouldn't be allowed to make further AI progress. Right. Okay.
8648400	8654800	And if they found that was the case with GPT-4, would they still be able to work on it? Like,
8654800	8660560	but just would they just have to spend a bunch of time figuring out how to train it to not be able
8660560	8667760	to make this kind of escape plan? Because it seems like if they had a totally abandoned GPT-4,
8668320	8671360	I feel like I have less hope because that's too big an ask.
8672240	8676960	I think that the expectation would be that they would have to give a really strong argument
8676960	8681520	for thinking that the AI was safe, despite it having these dangerous capabilities.
8681520	8686320	But the hope is that the owners would be more on them to say, look, here's the alignment techniques
8686400	8691280	we used. Here's why we're really confident that they work. And that if they're not able to provide
8691280	8696160	that case, then they are prevented from further enhancing the capabilities. Maybe they can do
8696160	8702960	other types of research, like research into making GPT-4 safer, but they can't do research into making
8702960	8707280	it more capable. And I mean, ultimately, if labs start doing this, the hope would be to then
8708080	8714480	kind of make it regulatory and required and enforced by kind of government agency.
8714880	8720320	Okay, well, that does hopefully sound promising then. Yeah, I guess getting back to
8721440	8725840	the kind of pace of improvement over time and why we might expect it to be much faster
8725840	8732560	at later stages of the task learning. It sounds like your median guess is that it's about a couple
8732560	8739920	of years from going to 20% of tasks to 100% of tasks. But I think you also estimated probability
8739920	8746720	distributions. So, yeah, kind of ranges for the fastest case and the slowest case. Can you talk
8746720	8753440	about those more extreme cases? Were they particularly wide ranges? So the way I arrive at
8753440	8759600	this probability distribution over how long it would take to go from 20% automation AI to 100%
8759600	8765120	automation AI is first to get this probability distribution over the difficulty gap, which I
8765120	8772720	said could be from 10x harder to train AGI, or maybe it could require up to a million times more
8772720	8777600	effective compute to train AGI. So I've got a probability distribution over that. And then
8778160	8782720	also based on the kind of dynamics we've been discussing about the pace of improvements of
8782720	8787840	algorithms and chip design and number of chips, I've got another probability distribution over
8787840	8792720	how fast those will be improving. And then you can combine those two together to spit out a
8792720	8799120	probability distribution over how long we'll have between those two points. So I end up thinking
8799120	8806240	there's about a 20% chance that it happens in less than a year, maybe 25% chance, and about a 20%
8806240	8814640	chance that it happens in more than 10 years. Huh, okay. So that's pretty wide. It's not quite as
8814640	8819760	wide as I would have expected, to be honest. You know, it sounds like you expressed a lot of
8819760	8825360	uncertainty in those estimates that went into the model. I guess you're saying it's like the most
8825360	8833200	likely 60% middle range is between a year and 10 years, which I guess all of that just seems pretty
8833200	8838320	fast. And maybe that's just one of the key takeaways that I should be getting from this.
8839440	8846800	I think it is hard to get longer than 10 years because we've said the pace of current improvement
8846880	8853840	in effective compute is already pretty fast, maybe going 10x every year. And then when we were
8853840	8860400	discussing the size of the difficulty gap, we said, well, it could just be a 10x increase that's
8860400	8867760	required, or maybe, I think my kind of best guess was maybe 1000x or 3000x increase, which would then
8867760	8874560	take, you know, three or four years. But then if we're improving at 10x every year, then 10 years
8874560	8880160	of that level of improvement is a really, really big increase in the amount of effective compute.
8880160	8885200	So the only way really you can get to more than 10 years is if actually the pace at which the
8885200	8890480	effective compute in training runs grows is actually declining in spite of the AI automation.
8891040	8897920	And you've got a relatively wide difficulty gap. I see. Okay. So something like, even though you
8897920	8904080	have things like co-pilot helping AI researchers do their research faster and faster over time,
8904720	8912880	you still are getting declining effective compute. And that might be because those later
8912880	8921200	improvements are just like much harder than we think or... So, yeah, maybe one concrete scenario
8921200	8927360	could be, it turns out that ADI is just really, really hard to develop. You almost need to rerun
8927360	8933040	the whole of evolution. Maybe it requires just a huge amount of effective compute to do that.
8933040	8939600	And before we have that amount of effective compute, we find that we just hit these fundamental
8939600	8946320	limits in terms of improving the quality of AI chips. And even the AI assistance, you know,
8946320	8951680	really enhancing our productivity isn't allowing us to get around that. Meanwhile, we're already spending
8952320	8955920	hundreds of billions of dollars on these chips for the biggest training runs. We can't
8956560	8961680	be spending even more on those chips. And then the only kind of significant source of progress
8961680	8968240	that remains is the algorithms. But maybe that slows down as well, because in spite of the AI
8968240	8973200	assistance, maybe part of what's driving the algorithmic progress today is actually the fact
8973200	8978800	that we've had all this additional compute for doing experiments. And maybe without that,
8978800	8984560	the pace of progress is going to slow in algorithms. Right. So that's a plausible world,
8984560	8989840	but we need a bunch of those things to go wrong in order to get above 10 years.
8989920	8995280	Exactly. And one way of thinking about that could just be to say, this whole framework was
8995280	9001600	premised on this assumption that if we used enough compute with our 2020 algorithms, we could have
9001600	9006400	trained AGI. And for someone who just didn't believe that at all, and also just didn't believe that
9006400	9011760	another kind of 10 or 20 years of algorithmic improvements would be enough to get us to AGI
9011760	9016000	along with additional compute, they might just really expect us to get stuck at some point.
9016000	9021600	And they might just really expect having more compute and somewhat better algorithms not to
9021600	9026080	get around that. So that kind of more than 10 years and I also make sense if you're just very
9026080	9030320	skeptical that anything like the current approach is going to get us all the way to AGI.
9030880	9035840	Right, right, right. And what's the kind of strongest argument that someone with that view
9035840	9042080	could make? Honestly, I think the view is looking worse and worse with each passing year with how
9042080	9047280	well the kind of biggest deep learning systems are performing. I guess maybe the strongest argument
9047280	9052800	would just be a non-specific argument. So rather than pointing to some specific thing
9052800	9057920	that humans can do that AIs aren't going to do, I think those arguments just tend to turn out
9057920	9063840	to be wrong after we kind of use 100x the compute and improve the algorithms further. Maybe you
9063840	9070560	just say something like look, human brain does all kinds of different things. I don't know which
9070560	9076000	ones the current approach to AI isn't going to do, but there's just millions of different tasks
9076000	9080720	that humans are doing and millions in ways in which the human brain architecture is very
9080720	9085520	complicated and specific and not a tool like that of AI systems. So there's bound to be some
9085520	9092800	important things that just you need a complete rewrite of the AI approach to be able to do.
9092800	9097600	That would be my personal attempt to make that position kind of maximally plausible.
9098560	9102240	I do think there are some specific things you can point to like memory that
9102240	9107920	kind of approaches you could argue are going to be blockers, but then it seems like there are
9107920	9115840	ways to respond to those blockers. Right, okay. So you can imagine some scenario that isn't going
9115840	9119760	well for companies where things are actually much harder than they might have expected.
9120480	9125600	What does it look like for a takeoff to take less than a year? I guess things have to go really well.
9125600	9133120	So I think a few things have to go well. So one possibility is that the difficulty gap is just
9133120	9138400	pretty narrow. I mean, this isn't necessarily going well for anyone to be honest. And this is
9138400	9142400	just a very intense and scary situation, but... Yeah, it's good clarification.
9144400	9150880	If you only need 10 times as much effect to compute, to train 100% automation AI compared
9150880	9158160	to 20% automation AI, then that could just be the algorithmic improvements in one year
9158160	9163120	once the AI's are helping you to a significant degree. Right. Or it could just be spending
9163680	9168880	three times as much on chips as you did the year before, and maybe those chips are three times as
9168880	9175280	efficient as the year before. And so that, you know, a narrow difficulty gap alone could get you there.
9175920	9183120	Another possibility is that once we hit 20% AI, there is just a very quick and significant
9183120	9189760	increase in the money being spent on training runs. So if people get really excited, this is
9189760	9194000	kind of a really awful scenario. People get excited about kind of what they could do with
9194000	9199360	those capabilities, and they spend 10 times as much on a training run the next year, maybe by
9199360	9204880	combining with some big existing compute providers, then that could allow you to cover,
9204880	9209520	you know, a kind of slightly bigger difficulty gap just within a year. Maybe even if the difficulty
9209520	9216880	gap was 100 times or 300 times much compute needed for AI, then you could still, by spending a lot
9216880	9221760	more and combine with a few algorithmic and hardware improvements, cross it pretty quickly.
9221760	9227920	Yeah, okay. And then, you know, one quick third possibility is just that, like I said earlier,
9228800	9232240	but maybe I haven't emphasized this enough, is that this framework is assuming everything is
9232240	9237680	continuous. So it's assuming that, you know, each time you increase the effect of compute
9237680	9242400	in a training run by 10%, you get a kind of, you know, relatively modest incremental
9242400	9247360	improvement in AI capabilities. If there's actually some kind of discrete phase change,
9247360	9252400	then that could just be an alternative route that could produce a very fast takeoff. So there are,
9252480	9254640	there are, you know, a few different ways that that could happen.
9254640	9260480	Right. Okay, that's just very scary. So there are, I guess,
9260480	9266000	worlds where things go very well for AI companies, but maybe very terribly for humanity,
9266560	9272320	or kind of very poorly for AI companies, hopefully better for humanity. And the median
9272320	9278400	estimate is something like a couple of years to get from AI can do 20% of cognitive tasks to AI
9278400	9283520	can do 100% of them. That seems like, yeah, an important thing to take away from this model.
9284560	9288320	Were there other things that you learned in this, in the process of writing this report?
9289200	9293920	Yeah, another big update for me was the, that I think there's going to be a pretty strong
9293920	9301440	correlation between how far away in time it is until we develop AGI and how fast takeoff will be.
9302960	9308000	So in particular, if you have short AI timelines, meaning that you think we'll develop AGI pretty
9308000	9313040	soon, then I think there are a few reasons to expect it, and especially fast takeoff.
9313920	9320000	And so one reason is that if you have short timelines, and that's probably going to mean
9320000	9325040	that you've got a smaller difficulty gap, because if you think that, you know, we don't need that
9325040	9330480	much more effective compute to develop AGI compared with today, then you're probably also
9330480	9336000	going to think that the difference in effect to compute for 20% AI to AGI is also going to be
9336000	9341120	small. So that will push you towards a faster takeoff. Right. Okay. That makes sense.
9341760	9347920	Another thing is that if you think AGI is going to be here fairly soon, then it's plausible that we
9347920	9355440	won't be spending that much money on training runs shortly before we have AGI, which means it
9355440	9360960	might be very practical and doable to quickly increase the amount that we're spending by maybe
9360960	9368160	a factor of 10 or a factor of 100 just around the time that we are approaching AGI. Right. And so
9368160	9374640	we could get a really very quick increase in the amount of effective compute being used on a training
9374640	9379920	run as a kind of almost direct consequence of it being a short timeline. Whereas if timelines were
9379920	9384080	long, then like I said, maybe we were already using all the chips in the world on the biggest
9384080	9388960	training run, you know, before we get to AGI. And so that that source of growth is no longer
9388960	9393760	available. Right. Yeah, that makes sense. Another thing with short timelines is that
9394320	9399840	it implies that there's going to be a period of just a few years where we get really significant
9399840	9408960	automation of AI R&D and kind of really significantly increase the size of the effective kind of
9408960	9414720	research workforce that's working on improving AI due to these AIs. And if that happens very
9414720	9419680	quickly is kind of you suddenly kind of five X the size of your research team, then you would
9419680	9425360	expect that to just really significantly speed up progress. And that's just an especially kind of
9425360	9431520	dramatic effect in short timelines. And the last point is that if if timelines are short,
9431520	9437440	then there's less hope for eating up all the remaining possibilities for hardware progress
9438160	9442320	and kind of running out of, you know, hitting those physical limits that we mentioned. Right.
9443040	9446960	And, you know, analogously with algorithmic improvements, if timelines are short, then
9446960	9453440	there's much less hope that we kind of run out of possible algorithmic improvements before we
9453440	9459280	get there. So I do think that, you know, if, like I believe a lot of the labs do, they have
9459280	9466160	pretty, pretty short timelines, kind of expecting AGI in the 2020s or early 2030s. And, you know,
9466160	9470560	I think that the implication of that is that takeoff speed is going to be on the on the
9470560	9475760	shorter end of what we've been discussing. So, you know, less than less than three years
9475760	9481760	and very plausibly less than one year and, you know, one or two years being maybe maybe the most
9482400	9489120	most likely. That's yeah, it's just it's just terrifying. I mean, we're talking about
9489920	9496480	by the end of the decade, we've got AI that can do everything humans can do and probably more.
9497360	9504080	Yeah. And we're talking about the labs who train them by default will have access to at least 100
9504080	9510240	million of them that they can run and then probably soon after, you know, a billion or many
9510240	9516480	billions that they can run. So it is it is pretty terrifying. Yeah. That's basically because it takes
9516480	9522640	so much effective compute to run the training runs that once they've got AGI, they're just
9523280	9527920	they can run millions or billions of copies on those chips. Exactly. So I think initially,
9527920	9533040	they'll be able to run millions of copies. But because of the efficiency improvements
9533760	9537760	that they can probably fairly quickly make, especially with all those copies working on it,
9538640	9545840	I think it won't be long before they can do billions. It's really bewildering stuff. And
9546800	9553280	I find yeah, I find it pretty my brain really doesn't want to believe it. I find it really,
9553280	9561200	really hard to wrap my head around. Yeah. Any other takeaways? So another takeaway is that I think
9561200	9566800	AGI is more likely to happen before 2040 compared to what I used to think. And so the reason for
9566800	9573520	that is that the way I used to think about this is, okay, what does it take to train AGI? And
9573600	9579600	how long will it be until we have that much effective compute? Whereas now the way I think
9579600	9587520	about it is more, well, what will it take to train AI that is really profitable or really good
9587520	9592640	at accelerating AI R&D? Right. And if we get either of those two things, then we that we'd
9592640	9600000	expect that to accelerate future AI progress, such that, you know, kind of future progress is faster.
9600000	9606560	And as long as AGI is incredibly hard, we are then able to reach AGI within the next couple
9606560	9611120	of decades. So, you know, my current read, I just think it's really likely that by the by the end
9611120	9618960	of this decade, we have AI which is really profitable and or significantly accelerates AI R&D.
9619520	9624000	And so it's then kind of hard for me to imagine how that dynamic plays out without
9624000	9632080	us getting to AGI by 2040. Wow. I think you just need AGI to be really hard. And as to fail to get
9632080	9637200	it despite, you know, a huge amount of investment and help from AI systems and developing it.
9637760	9643520	Right. Okay. So to make sure I understand the big thing doing the work there is you used to think
9644080	9651360	about how hard it was to have humans figure out how to train AI systems to do everything humans
9651360	9657680	can do. And now you're like, it's really just how hard is it to train AI systems to get really good
9657680	9664880	at AI R&D. And that's like a much smaller subset of cognitive tasks. And so you can get really
9664880	9672240	accelerating growth in AI capabilities just by making a lot of progress on that one set of tasks.
9672240	9676800	Is that kind of right? Yeah, that's right. And I do think a distinct possibility is that we just
9676800	9682560	train AI that isn't good at AI R&D, but makes lots and lots of money in the economy.
9683120	9687760	And then there's just a bunch of investment in, yeah, I see in AI capabilities.
9687760	9693200	And in designing better chips to run those AIs on. Got it. Okay. That makes that makes total sense.
9694000	9699600	Any other takeaways? So the biggest other one, I think, is something we've touched upon, which is
9699600	9708800	that the transition from roughly human level AI to significantly super human AI, I think is is
9708800	9713840	going to be probably very quick. Right. So I think probably that's going to take less than a year.
9714640	9719200	We have already touched upon the reasons why by the time we're at human level at AI,
9719840	9728800	then AI's will be adding a huge, huge amount to the productivity of our work on AI R&D,
9728880	9732640	designing better trips, designing better algorithms. And those things are already
9732640	9736880	improving very quickly. So I only expect them to be going much, much faster.
9737600	9744480	And then in addition, it's going to be quite clear that it's a very lucrative area, whether you care
9744480	9749360	about discovering new technologies to help with climate change or discovering new technologies
9749360	9754640	to help with improving human health, or you want to increase your country's kind of national power,
9755360	9762560	then there's just going to be lots of reason to be investing in AI and designing smarter AI's.
9763120	9768800	And so I kind of expect all of those three inputs, kind of the dollar spent on training and
9768800	9772720	the quality of the AI chips and the algorithms to be improving really very quickly.
9773280	9779280	Right. So there'll be incentives to go beyond just human human level capabilities, and then
9779920	9788880	we'll have so much resources and just like AI labor to basically mean that we probably just
9788880	9793680	shoot right past human level. Yeah. And I think that's in terms of plans for making the whole
9793680	9800000	thing go well. It's especially scary because I think a really important part of the plan from my
9800000	9806080	perspective would be to go especially slowly when we're around the human level, so that we can do
9806080	9811840	loads of experiments and loads of kind of scientific investigation into, okay, so this human
9811840	9816560	level AI, is it aligned if we do this technique? What about if we do this other alignment technique?
9816560	9822160	Does it then, does it then seem like it's aligned and just really making sure we kind of fully
9822160	9827200	understand kind of the science of alignment and can try out lots of different techniques
9827200	9833280	and to develop kind of reliable tests for whether the alignment technique has worked or not,
9833360	9839120	whether they're hard to game. The kind of thing that ARC has done with GPT-4, for example. Exactly.
9839680	9845520	And I think if we only have a few months kind of through the human level stage,
9845520	9851520	that stuff becomes really difficult to do without significant coordination in advance
9851520	9857120	by labs. So I think that this, you know, there are really important implications of this,
9857120	9862480	of this fast transition in terms of setting up a kind of government system which can allow us to
9863280	9867600	go slowly despite the technical possibilities existing to go very fast.
9868480	9871520	Yeah, yeah. No, that makes sense. I do feel like I've had some
9872480	9879360	background belief that was like, obviously, when we've got AI systems that can do things humans can
9879360	9883920	do, people are going to start freaking out and they're going to want to make sure those systems
9883920	9890000	are safe. But if we, if it takes months to get there and then within another few months we're
9890000	9895040	already well beyond human capabilities, then no one's going to have time to freak out or it'll
9895040	9900320	be too late. Yeah. I mean, even if we spend the next, what do we have, seven years left in the
9900320	9907840	decade? Like that sounds hard enough. Yeah. Yeah, I agree. Okay. So a takeaway is like,
9907840	9915600	we really need to start slowing down or planning now, ideally both. Yeah. And we'll need the plans
9915600	9921920	we make to really enable that to be mutual trust that, that the other labs are also slowing down.
9921920	9928960	Because if it's, you know, if it only takes six months to, you know, make your AIs 10 or 100
9928960	9933920	times as smart, then you're going to need to be really confident that the other labs aren't doing
9933920	9939120	that in order to feel comfortable slowing down yourself. Right. If it was going to take 10 years
9939120	9943760	and you noticed like three months in that another lab was working on it, you'd be like,
9943760	9947440	yeah, we can catch up. Yeah. But if it's going to take six months and you're three months in,
9948160	9953680	you've got no hope. And so maybe you'll just like spend those first three months secretly working on
9953680	9959520	it to make sure that doesn't happen. Or just not agree to do the slowdown. Yeah. Oh, these are really
9959520	9964560	hard problems. I mean, it's very, it feels very like prisoner's dilemma. I'm hoping it's going to be
9964560	9969440	more like an iterated prisoner's dilemma, where there's kind of multiple moves that the labs make
9969440	9974560	one after the other. And they can see if the other labs are cooperating. And in an iterated
9974560	9979600	prisoner's dilemma ultimately makes sense for everyone to cooperate, because that's that way
9980160	9984240	they can, the other people can see you coordinating, then they coordinate and then everyone kind of
9984240	9989120	ends up coordinating. You know, one thing is if you could set up ways for labs to easily know whether
9989120	9995680	the other labs are indeed cooperating or not kind of week by week, then that turns it into a more
9995680	10000560	iterated prisoner's dilemma and makes it easier to achieve a kind of good outcome. Yeah, yeah,
10000560	10006720	that makes sense. I imagine it's the case that the more iteration you get in an iterated prisoner's
10006720	10014240	dilemma, the better the incentives are to cooperate. And so just by making the timeline shorter,
10014240	10018320	you you make it harder to get to get these iterations that build trust. Yeah, I think that's
10018320	10026240	right. So it sounds like there's this range that's maybe between one and 10 years, maybe a bit
10026240	10033440	shorter or a bit longer at the at the extremes. But that's in particular for AI systems having
10033440	10040480	the capability to perform all the tasks that humans currently do, not that they're actually
10040480	10047120	automating all those tasks and replacing humans. How big of a lag do you expect there to be between
10047120	10051920	AI systems having capabilities and AI systems actually being used in the world?
10052880	10058960	So I think it will vary, according to a few things. So one of the things is what industry are we
10058960	10067440	talking about? So I think for industries that are very public facing, customer facing, and
10068160	10073840	highly regulated, you'd expect there to be bigger delays between AI being able to automate the work
10073840	10078960	and it actually being automated. Whereas for more back end parts of the economy, like R&D,
10079920	10087280	like manufacturing, like logistics and transportation, then I think there would be less of a delay.
10087280	10093520	Okay, yeah. I also just expect it to differ from company to company based on how innovative
10093520	10098240	those organizations are. Sure, their internal culture, whether they're the type of company
10098240	10103280	who's going to be like, let's integrate GPT-4 into all of our processes now. Yeah.
10104160	10110000	For the purposes of the predictions of this model, the really important thing is about the delay to
10110000	10118320	using AI to improve AI algorithms and to improve AI chip design. And I think those are probably
10118320	10125440	areas where the lag will be very much on the shorter end. Right. And that's because AI researchers
10125440	10134160	can use them without needing them to be functioning super well for public users who might be like,
10134160	10138160	what the heck, it gave me this weird result. That's confusing and weird and looks bad on your
10138160	10143040	company. They can just use them in the background, check that they work, and they don't need to
10143040	10148240	wait for them to be super polished. Right, yeah. There'll also probably be more aware of AI
10148240	10154720	developments because that's the kind of industry they work in. Yeah, right. They are probably less
10154720	10162160	regulated. So I think a few factors. Another thing that could kind of affect how long that lag is
10163040	10169840	is actually the capability of AIs themselves. So even if it would be possible with kind of
10169840	10174960	six months effort to integrate some AIs into your workflow, by the time AIs are kind of,
10175680	10181120	let's say superhuman in their capabilities, then maybe they're able to integrate themselves
10181120	10188240	into the workflow. And so that kind of upfront cost of adopting AI is now extremely low.
10189280	10192720	And so I think that you could see that lag time reducing over time.
10193280	10198240	Yeah, okay. So yeah, we're nearing the end of our time, but I guess to take a step back,
10198240	10203440	we've talked about some pretty insane sounding stuff today, robot workforce and AI takeover.
10203440	10209040	And yeah, I'm curious, were these kinds of arguments about the potential risks from AGI ever
10209040	10215600	farfetched to you? Or yeah, did they make sense to you kind of right away? Oh, yeah, for sure,
10215600	10222480	they were farfetched. I've kind of been through a few phases in my own relationship to the arguments.
10223280	10229040	I think at first I read Superintelligence and it kind of made sense to me. I wasn't exactly
10229120	10234320	sure what to do with it. But it seemed plausible enough that if we had Superintelligent AIs,
10234320	10243440	then things could go bad for us. Then there was a period in 2020 when a few counter arguments to
10243440	10247920	the case in Superintelligence came out. And I kind of thought, oh, wow, yeah, these arguments were
10248720	10254880	weaker than I had realized. And I felt a little bit disillusioned with them. That's a piece from
10254880	10263360	Ben Garfinkel and one from Tom Adam Shrazy on AI risk. But in the last two or three years,
10264080	10269200	thinking through the arguments in more detail, I've come to think that like somewhat adjusted
10269200	10274320	versions of the argument in Superintelligence are still very plausible. And though I don't think
10274320	10278720	it's like 100% that we're doomed and there's no way that we could align these systems,
10279600	10284240	it does just seem pretty plausible to me that the evidence about whether they're aligned is very
10284240	10291600	ambiguous. There's competitive dynamics, pushing people to go forward in the face of that ambiguity
10291600	10296960	and that we just really dropped the ball. Right. Okay. And so I guess at some point in there,
10298320	10303680	you decided to leave teaching and try to work on AI full time. Is that basically right?
10304560	10313840	Yeah, that's right. What was that like? It was a tough decision. I actually left teaching partway
10313840	10320480	through the academic year. So I did feel I did feel bad about them. I did feel I was abandoning
10320480	10325760	my pupils, because that's not a normal time to leave. And I still do feel bad about that.
10326880	10333200	It does mean that I've kind of got to where I have a favor earlier than I would have. So I don't
10333200	10339200	unambiguously regret it, but it was a tough decision. And it had downsides. Right. I was, you
10339200	10344400	know, I was listening to things like the ATK podcast and reading other things. And I was convinced
10344400	10349840	about these arguments for AI risk and long termism in general, being plausible enough that it was
10349840	10357920	worth leaving teaching. Yeah, well, that sounds, yeah, I guess both really hard and also on my
10357920	10364720	views. Yeah, really, really lucky for us. Yeah, I'm glad. I'm glad we have you working on this stuff.
10365520	10372640	So we don't have much time left. So I'd love to ask a final question. I got an insider tip to ask you
10374000	10378480	what ants can teach us about AI? Yeah, what's the story there?
10379040	10383760	So yeah, ants are really incredible creatures. I've been I've been learning about reading this
10383760	10390240	book called ant interactions. Okay. So I mean, ants have been around longer than the dinosaurs,
10390240	10394720	I think 120 million years they've been around for. Wow, I didn't know that.
10394720	10400880	And they're like one of the most prolific and successful species on the planet. So one stat
10400880	10406800	in this book was that if you, if you weighed all the ants in the Amazon rainforest and put them on
10406800	10414080	the scales, they would weigh twice as much as all of the land animals combined. That's all the mammals,
10414080	10419920	amphibians, birds, reptiles. I thought you were going to say twice as much as humans when I was
10419920	10426080	like, wow. That's the other that's just counting only ants and animals in the rainforest in the
10426080	10434480	rainforest. Unbelievable. Okay. And another fact about ants is that even compared to the other
10434480	10441200	insects, they weigh 30% as much as all insects combine, which is given that they're only 2%
10441200	10447440	of the species of insects. They it's kind of testament to how how successful and prolific they
10447440	10453840	are. Yeah. Different ant varieties all around the world are incredibly diverse, like something kind
10453840	10460560	of glide through the air. Some of them are very aggressive. Some of them are not at all. Some of
10460560	10464960	them kind of bump into each other at much higher rates than others. And they have kind of very
10464960	10471520	different strategies and environments that they work in. And the link with AI is a little bit
10471520	10476480	tenuous, to be honest. I'm mostly just reading this book out of interest. But in an ant colony,
10477280	10482720	ants are smarter than like a human sellers. They're the kind of self contained units that, you know,
10482720	10488880	eat and do tasks by themselves. And they're pretty autonomous. But the ants are still pretty dumb.
10489520	10496800	And no ant really knows that it's part of a colony or knows that the colony has
10496800	10500240	certain tasks that it needs to do and that it has to help out with the colony efforts.
10501040	10506400	It's more like a kind of little robot that's kind of like bumping into other ants and getting like
10506560	10510080	signals and then adjusting its behavior based on that interaction.
10510720	10518320	Right. So it's not like, I guess, a company where like the different people in the company are like,
10518320	10524240	my job is marketing. And they have like a basic picture of how it all fits together.
10524240	10529120	They're much more like, if a person at a company doing marketing was just like,
10529120	10530800	I don't know why I do it, I just do it.
10530800	10535200	Yeah, exactly. And another disadvantage with a company is that a company, there's someone at
10535200	10539680	the top that's kind of coordinating the whole thing. Whereas with ants, there's no one that's
10539680	10547120	coordinating it. It's just including the queen, there's no management system. It's just all of the,
10547120	10551680	you know, hundreds and thousands of ants have their individual instincts of what they do when
10551680	10556000	they bump into each other and what they do when they bump into food and what they do when they
10556000	10561840	realize that there's, you know, there's not as much food as there needs to be. And by kind of all
10561840	10566320	of the ants following their own individual instincts, it just turns out that they act
10566320	10571840	as if they were kind of a fairly well coordinated company that is like ensuring that there are
10571840	10576800	some ants going to get food and some ants that are keeping the nest in order and some ants that
10576800	10582800	are feeding the young. But that that coordination happens kind of almost magically and emerges
10582800	10589440	out of those individual ant interactions. So one example of how this works is that
10590320	10596720	if an ant comes across a body of a dead ant, then if there's another dead body nearby or
10596720	10601040	tend to move it to be close to the other dead body, that's just an instinct it has. It just kind
10601040	10606000	of moves the body towards another. And if there's like one kind of pile of three dead ants and
10606000	10610320	another pile of two dead ants or tend to go towards the bigger pile, so tend to move this
10610320	10614960	extra dead ant towards the pile of three. And then it turns out that if all the ants just have
10614960	10619680	those instincts, then if there's initially a kind of a kind of sprawling mass of dead bodies
10619680	10623440	everywhere, then those dead bodies will be collected into just a small number of piles
10624160	10631600	of bodies. And it's not like any of the ants are like I am the grave digger or the like keeper of
10631600	10638400	the cemetery. They just have like really weird like baseline rules that are like move the smaller
10638400	10643520	group of dead ants to the to where the larger group of dead ants are. Yeah, exactly. So they
10643520	10649200	don't have to know that the whole point of this instinct is to kind of clear the ground so that
10649200	10653200	it's easier to do work in the future. It's just an instinct they have. They don't have to know that
10653200	10657600	when everyone follows that instinct, this is the resultant pattern of behavior. And similar
10657600	10663280	instincts kind of cause them to go for food when food is available. So if they see many ants coming
10663280	10669280	in with food, that raises the probability that they'll go out and look for food. And they're not
10669280	10673200	thinking oh, there's food to be gathered. There's clearly a lot of it. So we better reassign some
10673200	10678000	labor towards food gathering that they just have that basic instinct, which causes them to go out
10678000	10683840	and help out with the food gathering. And it's something like, oh, that ant has food. Oh, another
10683840	10690160	ant has food. I'm going to go that way. Yeah, exactly. Right. How does this connect to AI?
10690800	10696640	So I don't know if it does connect very, very directly at all. But the idea of the connection
10696640	10705120	in my head is that it's an example of a system where lots of kind of less clever individuals
10705120	10709840	are following their local rules, doing their local task. But that what emerges from that
10709840	10715360	is a very coherent and effective system for ultimately gathering food, defending against
10715360	10722960	predators, raising the young. And an analogy would be maybe we think it's pretty dangerous to train
10723040	10729120	really smart AIs that are individually very smart. But it might be safer to kind of set up a team of
10729120	10735840	AIs such that each AI is kind of doing its own part in a kind of team and doesn't necessarily
10735840	10740560	know how how its work is fitting into the broader whole. But nonetheless, you can maybe get a lot
10740560	10746880	more out of that kind of disconnected team of AIs that are specialized and that just kind of take
10746880	10751600	the inputs and produce the outputs without much of an understanding of the broader context. And
10752160	10758320	just thinking maybe that would be, you know, a safer way to develop advanced AI capabilities
10758320	10762320	than just training one super smart AI mega brain.
10762320	10770800	Right. Cool. I love that. I mean, who knows if it'll work. But I mean, that just that makes tons
10770880	10784800	of sense to me. You don't have GPT-4 or GPT-40. You have a bunch of much dumber AI systems that can
10784800	10792000	coordinate together to be just as helpful as GPT-40, but that individually couldn't do
10792720	10798880	most of the things the other systems can do. And so collectively, they can't do anything like
10798880	10806160	escape from their box and find another computer to take over. And is that basically the idea?
10806160	10807360	Yeah, that is the hope.
10807360	10812800	That's lovely. That's really, really cool. Great. Well, I should let you go. That's
10812800	10817280	all the time we have. But thank you so much for coming on the show, Tom. It's been such a pleasure.
10817280	10819280	It's been really fun, Louisa. Thank you so much.
10831920	10837520	All right. If you liked that episode, we'll have more on this issue for you soon. But in the meantime,
10837520	10842160	I can recommend going back and listening to some of our best past episodes about artificial
10842160	10848400	intelligence, including episode 141, Richard Newell on large language models, open AI, and
10848400	10853920	striving to make the future go well. There's also episode 132, Nova Dasama on why information
10853920	10859360	security may be critical to the safe development of AI systems. Episode 107, Chris Ola on what
10859360	10863840	the hell is going on inside neural networks. Episode 92, Brian Christian on the alignment
10863840	10868960	problem. And an oldie but a goodie, episode 44, Paul Cristiano on how open AI is developing
10868960	10873360	real solutions to the AI alignment problem, and his vision of how humanity will progressively
10873360	10878880	hand over decision making to AI systems. Bit of a run on title there. I think that might be my
10878880	10883760	fault. But it is an excellent interview. All right. The 80,000 Hours podcast is produced and
10883760	10887680	edited by Kieran Harris. Audio mastering and technical editing by Simon Monsour and Ben
10887680	10891760	Cordell. Full transcripts and extensive collection of links to learn more are available on our site
10891760	10906560	and put together. As always, thank you. Thanks for joining. Talk to you again soon.
