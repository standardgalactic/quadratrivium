start	end	text
0	8160	Let's talk about high-performance computing.
8160	11640	It is clear now that acceleration is going to be the path forward
11640	14520	for scientific and for high-performance computing.
14520	16060	As I mentioned before,
16060	18880	accelerated computing has four pillars.
18880	21960	The first, of course, is the accelerator, the advanced GPUs.
21960	24760	The second is the acceleration stack
24760	26880	for each one of the computational domains.
27000	30480	The third is systems, and last is developers,
30480	33040	ultimately the applications that we accelerate.
33040	35200	This year, we did really great work.
35200	38440	We've accelerated now over 700 applications.
38440	41440	And each and every single year, at every conference I show you,
41440	44600	our golden suite, the suite that we track on a regular basis
44600	47480	to make sure that we continue to engineer advances
47480	49160	into libraries, into the stack,
49160	52360	so that applications continue to improve in performance,
52360	55120	even if we don't introduce new GPUs.
55120	57880	And as you see, over the course of the last four years,
57880	60880	we've increased application performance by 4x.
60880	63400	And the green bar is something that I'm going to talk to you about.
63400	65400	We're going to offer you a new platform
65400	68480	and it's going to give high-performance computing a huge boost.
68480	72120	We also brought CUDA to ARM computing systems.
72120	75800	ARM server CPUs are seeing adoption all over the world.
75800	79640	In hyperscale, there's Amazon, Fujitsu Super Computing,
79640	81560	Cavium, part of Marvell now,
81560	84320	a new exciting company called Ampere Computing,
84320	85760	or in China, Huawei.
85760	89200	All of the suite of NVIDIA tools and libraries
89200	91320	are now available for ARM.
91320	95120	We also introduced a brand new SDK for IO processing this year.
95120	96480	We call Magnum IO.
96480	100000	Magnum IO includes all kinds of great things from our DMA,
100000	104160	of course, to the ability to communicate across multiple nodes
104160	107240	and move data directly from storage to our GPUs.
107240	110400	The suite of libraries is going to continue to advance.
110400	113240	Magnum IO is going to be one of our most important libraries.
113280	115640	Data processing and networking and storage
115640	117320	is going to become more and more important
117320	119600	to data center-scale computing over time.
119600	122720	We introduced two new stacks this year.
122720	125000	NVIDIA parabricks for genomics processing,
125000	128920	the ability to do variant calling at very high performances,
128920	131640	and a large body of work that we've been working on
131640	134600	for several years called NVIDIA Rappers for Data Analytics.
135840	139720	Machine learning has become one of HPC's grand challenges.
139720	141080	The advances of machine learning
141080	143000	and the popularity of this approach
143000	145800	has caused companies, institutes, and data centers
145800	147840	to collect a vast amount of data.
147840	149760	The machine learning pipeline consists of three things.
149760	152000	ETL, which creates the data frame,
152000	153760	does all the feature engineering necessary
153760	156480	for the machine learning algorithms to train on,
156480	159440	which creates the model which is then put into operations
159440	160600	we call inference.
160600	162920	These three stages of the pipeline
162920	166320	have unique and different computational challenges.
166320	169800	The first stage of the machine learning pipeline data processing
169840	172240	is becoming more complex than ever.
172240	174600	In fact, most data scientists will tell you
174600	176760	they spend the vast majority of their time
176760	179240	doing feature engineering and data processing
179240	181800	in the front stage of the machine learning pipeline.
181800	184520	What used to be processing hundreds of megabytes
184520	186640	to gigabytes to terabytes of data,
186640	189680	companies are now routinely processing tens,
189680	191960	if not hundreds of terabytes of data
191960	194280	and moving to petabytes of data.
194280	197760	It is the reason why Spark is so popular.
197760	201120	Spark is an incredible computational platform.
201120	204560	It turns an entire data center into a compute engine.
204560	208600	It partitions a very large data set to be processed
208600	211120	across a bunch of servers in the data center.
211120	213920	It was the brainchild of Matt Zaharia
213920	215280	at the Berkeley AMP Lab
215280	217600	and spun out and became Apache Spark.
217600	220680	It now has over a thousand companies contributing to it,
220680	222400	nearly a million lines of code,
222400	224760	16,000 plus companies around the world
224760	226920	uses it for data processing today.
226960	228560	Well, the amount of data that they're processing
228560	230000	is growing exponentially.
230000	233880	It is now reaching the limits of what Spark can do.
233880	235000	Here's the reason why.
235000	237920	The CPU set is being distributed across
237920	242040	has a fundamental working set in the order of megabytes.
242040	244440	A CPU naturally likes to work in its cache
244440	248080	and its caches typically on the tens of megabytes.
248080	250840	When the data set is now the hundreds of terabytes
250840	252160	and into petabytes,
252160	255160	the overhead of coordinating the CPU servers
255200	257280	is becoming the greatest bottleneck.
257280	260240	And we're starting to see the limits now.
260240	263520	What if instead of working on processors
263520	266120	that has tens of megabytes of working set,
266120	267880	let's move towards a processor
267880	271120	that has tens of gigabytes of working set.
271120	275160	And if we could use multiple GPUs to create large memories,
275160	279800	then it is now possible for us to imagine scaling beyond that.
279800	282240	We started working on GPU acceleration
282240	284880	of the data processing stack several years ago.
284920	287440	And it's a giant body of work.
287440	289800	Ladies and gentlemen, today we're announcing
289800	294120	that Spark 3.0, the next generation of Spark,
294120	296320	will be NVIDIA accelerated.
296320	298720	This is a collaboration between ourselves
298720	301600	and a large community of researchers and developers
301600	303960	in open source all around the world.
303960	306840	And the results are really fantastic.
306840	310960	It's possible because of several groundbreaking achievements.
310960	313120	The first is the work that we did
313160	316800	with Melanox NVIDIA called GPU Direct Storage.
316800	320760	In the acceleration of GPU Direct Storage and UCX,
320760	322720	this framework that makes possible,
322720	325760	the management of IO and storage
325760	329000	and multi-node computing lightning fast.
329000	332440	Second is the scheduler of Spark.
332440	336120	Scheduler of Spark now is aware of GPU and the GPU memory
336120	338480	so that it could partition work to the GPUs
338480	340680	and schedule it in a distributed way
340680	342640	and manage the computation
342640	345240	of this giant network of computers.
345240	348080	Third, a library called Rapids
348080	350320	that has the ability to ingest data,
350320	353000	create data frames, do feature engineering,
353000	357080	do SQL queries and intercept the calls of Spark
357080	359080	to be accelerated by our GPU.
359080	363600	And then lastly, Spark has a Spark SQL accelerator
363600	364960	they call Catalyst,
364960	367520	and that has been optimized for NVIDIA GPUs.
367520	372320	These elements make possible Spark 3.0.
372320	375400	Let me show you the potential acceleration
375400	377440	that data scientists will be able to enjoy.
377440	381200	What you're looking at here is the benchmark of Rapids,
381200	384000	the foundation of Spark 3.0.
384000	387040	This particular benchmark is TPCX BB,
387040	388440	big data benchmark.
388440	391400	This particular data set is a scale factor of 10,000,
391400	393440	which basically is 10 terabytes.
393440	396320	The state of the art is a Dell server
396320	397880	costs about a million dollars
397880	401680	and has the ability to deliver 17 gigabytes per second
401680	404640	of data movement through this benchmark.
404640	406440	This particular benchmark is hard to beat.
406440	407640	And the reason for that
407640	410400	is because not only does it have to be fast,
410400	411800	it also has to be cost effective.
411800	412800	And the reason for that
412800	414480	is because price performance matters.
414480	417160	The fastest in the world today is the Dell server
417160	421040	at a million dollars and 17 gigabytes per second.
421040	424160	With Spark 3.0 sitting on top of Rapids,
424160	429000	Rapids benchmarked on TPCX BB delivers
429000	434000	163 gigabytes per second for two million dollars.
434840	438160	10 times the performance and only twice the cost.
438160	440640	If you were to look at this in another way,
440640	443760	suppose you were to create a data center
443760	446120	that is able to achieve the same performance
446120	448800	as two million dollars of DGX's
448800	453240	accelerating Rapids of this benchmark, TPCX BB,
453240	458320	it would cost you 10 million dollars and 150 kilowatts.
458320	461880	Now, of course, data centers routinely process
461880	463560	a lot more than terabytes.
463560	464800	You're gonna need data centers
464800	467160	way larger than this in the future,
467160	469760	as data continues to grow exponentially.
469760	473880	And so the ability to accelerate Spark 3.0
473880	477520	with a library we call Rapids is utterly groundbreaking.
477520	481240	The result is really spectacular.
481240	484960	One fifth the cost, one third the power,
484960	487600	one fifth the cost and one third the power.
488680	492920	The more you buy, the more you save.
492920	494440	In fact, Databricks,
494440	497280	which offers industrial strength Spark
497280	500440	at a large scale as a service is doing fantastic.
500440	503480	Every single day, a million virtual machines
503480	506560	are spun up to do data processing on Spark.
506560	509040	And they're so delighted by the work and acceleration
509040	511040	that they're gonna go accelerate Databricks
511040	513280	with NVIDIA GPUs.
513280	514560	They're a fantastic partner.
514560	517440	I'm so happy with all the work that we've done together.
517440	519360	Leading cloud service providers
519360	522040	are offering Spark accelerated in their cloud
522040	525440	or they're accelerating their proprietary machine learning
525440	529280	pipeline and data processing pipeline with NVIDIA Rapids.
529280	533400	Amazon SageMaker, Azure Machine Learning, Databricks,
533400	536440	Google Cloud AI, Google Cloud Dataproc
536440	539720	are now going to be accelerated with NVIDIA GPUs
539720	542240	for data processing and data analytics.
542240	545040	Spark acceleration is a great achievement.
545040	546680	I'm so proud of the team.
546680	549600	It's such a large body of work and it's taken us years.
549600	551280	And it requires the collaboration
551280	554680	with hundreds of collaborators in open source
554680	557720	built on several layers of foundational
557720	559680	and fundamental new technology.
559680	562800	And now the part that is growing exponentially difficult,
562800	566440	the first stage of machine learning is now accelerated.
566440	569360	Data scientists all over the world are gonna be thrilled.
569360	574200	Entire end-to-end from data processing to inference.
574200	576040	We have three libraries.
576040	579280	Rapids for data processing, QDNN,
579280	581840	our core library for deep learning AI,
581840	585320	and then third, TensorRT, our optimizing compiler
585320	587120	for these computational graphs
587120	591000	that are created by the training frameworks.
591040	594040	The end-to-end acceleration is now complete
594040	597360	and we will continue to advance it over time.
597360	601720	But this represents the foundation of NVIDIA AI.
601720	603000	I can't be more proud.
603000	604200	The team have done a great job.
604200	605040	Thank you.
