WEBVTT

00:00.000 --> 00:08.160
Let's talk about high-performance computing.

00:08.160 --> 00:11.640
It is clear now that acceleration is going to be the path forward

00:11.640 --> 00:14.520
for scientific and for high-performance computing.

00:14.520 --> 00:16.060
As I mentioned before,

00:16.060 --> 00:18.880
accelerated computing has four pillars.

00:18.880 --> 00:21.960
The first, of course, is the accelerator, the advanced GPUs.

00:21.960 --> 00:24.760
The second is the acceleration stack

00:24.760 --> 00:26.880
for each one of the computational domains.

00:27.000 --> 00:30.480
The third is systems, and last is developers,

00:30.480 --> 00:33.040
ultimately the applications that we accelerate.

00:33.040 --> 00:35.200
This year, we did really great work.

00:35.200 --> 00:38.440
We've accelerated now over 700 applications.

00:38.440 --> 00:41.440
And each and every single year, at every conference I show you,

00:41.440 --> 00:44.600
our golden suite, the suite that we track on a regular basis

00:44.600 --> 00:47.480
to make sure that we continue to engineer advances

00:47.480 --> 00:49.160
into libraries, into the stack,

00:49.160 --> 00:52.360
so that applications continue to improve in performance,

00:52.360 --> 00:55.120
even if we don't introduce new GPUs.

00:55.120 --> 00:57.880
And as you see, over the course of the last four years,

00:57.880 --> 01:00.880
we've increased application performance by 4x.

01:00.880 --> 01:03.400
And the green bar is something that I'm going to talk to you about.

01:03.400 --> 01:05.400
We're going to offer you a new platform

01:05.400 --> 01:08.480
and it's going to give high-performance computing a huge boost.

01:08.480 --> 01:12.120
We also brought CUDA to ARM computing systems.

01:12.120 --> 01:15.800
ARM server CPUs are seeing adoption all over the world.

01:15.800 --> 01:19.640
In hyperscale, there's Amazon, Fujitsu Super Computing,

01:19.640 --> 01:21.560
Cavium, part of Marvell now,

01:21.560 --> 01:24.320
a new exciting company called Ampere Computing,

01:24.320 --> 01:25.760
or in China, Huawei.

01:25.760 --> 01:29.200
All of the suite of NVIDIA tools and libraries

01:29.200 --> 01:31.320
are now available for ARM.

01:31.320 --> 01:35.120
We also introduced a brand new SDK for IO processing this year.

01:35.120 --> 01:36.480
We call Magnum IO.

01:36.480 --> 01:40.000
Magnum IO includes all kinds of great things from our DMA,

01:40.000 --> 01:44.160
of course, to the ability to communicate across multiple nodes

01:44.160 --> 01:47.240
and move data directly from storage to our GPUs.

01:47.240 --> 01:50.400
The suite of libraries is going to continue to advance.

01:50.400 --> 01:53.240
Magnum IO is going to be one of our most important libraries.

01:53.280 --> 01:55.640
Data processing and networking and storage

01:55.640 --> 01:57.320
is going to become more and more important

01:57.320 --> 01:59.600
to data center-scale computing over time.

01:59.600 --> 02:02.720
We introduced two new stacks this year.

02:02.720 --> 02:05.000
NVIDIA parabricks for genomics processing,

02:05.000 --> 02:08.920
the ability to do variant calling at very high performances,

02:08.920 --> 02:11.640
and a large body of work that we've been working on

02:11.640 --> 02:14.600
for several years called NVIDIA Rappers for Data Analytics.

02:15.840 --> 02:19.720
Machine learning has become one of HPC's grand challenges.

02:19.720 --> 02:21.080
The advances of machine learning

02:21.080 --> 02:23.000
and the popularity of this approach

02:23.000 --> 02:25.800
has caused companies, institutes, and data centers

02:25.800 --> 02:27.840
to collect a vast amount of data.

02:27.840 --> 02:29.760
The machine learning pipeline consists of three things.

02:29.760 --> 02:32.000
ETL, which creates the data frame,

02:32.000 --> 02:33.760
does all the feature engineering necessary

02:33.760 --> 02:36.480
for the machine learning algorithms to train on,

02:36.480 --> 02:39.440
which creates the model which is then put into operations

02:39.440 --> 02:40.600
we call inference.

02:40.600 --> 02:42.920
These three stages of the pipeline

02:42.920 --> 02:46.320
have unique and different computational challenges.

02:46.320 --> 02:49.800
The first stage of the machine learning pipeline data processing

02:49.840 --> 02:52.240
is becoming more complex than ever.

02:52.240 --> 02:54.600
In fact, most data scientists will tell you

02:54.600 --> 02:56.760
they spend the vast majority of their time

02:56.760 --> 02:59.240
doing feature engineering and data processing

02:59.240 --> 03:01.800
in the front stage of the machine learning pipeline.

03:01.800 --> 03:04.520
What used to be processing hundreds of megabytes

03:04.520 --> 03:06.640
to gigabytes to terabytes of data,

03:06.640 --> 03:09.680
companies are now routinely processing tens,

03:09.680 --> 03:11.960
if not hundreds of terabytes of data

03:11.960 --> 03:14.280
and moving to petabytes of data.

03:14.280 --> 03:17.760
It is the reason why Spark is so popular.

03:17.760 --> 03:21.120
Spark is an incredible computational platform.

03:21.120 --> 03:24.560
It turns an entire data center into a compute engine.

03:24.560 --> 03:28.600
It partitions a very large data set to be processed

03:28.600 --> 03:31.120
across a bunch of servers in the data center.

03:31.120 --> 03:33.920
It was the brainchild of Matt Zaharia

03:33.920 --> 03:35.280
at the Berkeley AMP Lab

03:35.280 --> 03:37.600
and spun out and became Apache Spark.

03:37.600 --> 03:40.680
It now has over a thousand companies contributing to it,

03:40.680 --> 03:42.400
nearly a million lines of code,

03:42.400 --> 03:44.760
16,000 plus companies around the world

03:44.760 --> 03:46.920
uses it for data processing today.

03:46.960 --> 03:48.560
Well, the amount of data that they're processing

03:48.560 --> 03:50.000
is growing exponentially.

03:50.000 --> 03:53.880
It is now reaching the limits of what Spark can do.

03:53.880 --> 03:55.000
Here's the reason why.

03:55.000 --> 03:57.920
The CPU set is being distributed across

03:57.920 --> 04:02.040
has a fundamental working set in the order of megabytes.

04:02.040 --> 04:04.440
A CPU naturally likes to work in its cache

04:04.440 --> 04:08.080
and its caches typically on the tens of megabytes.

04:08.080 --> 04:10.840
When the data set is now the hundreds of terabytes

04:10.840 --> 04:12.160
and into petabytes,

04:12.160 --> 04:15.160
the overhead of coordinating the CPU servers

04:15.200 --> 04:17.280
is becoming the greatest bottleneck.

04:17.280 --> 04:20.240
And we're starting to see the limits now.

04:20.240 --> 04:23.520
What if instead of working on processors

04:23.520 --> 04:26.120
that has tens of megabytes of working set,

04:26.120 --> 04:27.880
let's move towards a processor

04:27.880 --> 04:31.120
that has tens of gigabytes of working set.

04:31.120 --> 04:35.160
And if we could use multiple GPUs to create large memories,

04:35.160 --> 04:39.800
then it is now possible for us to imagine scaling beyond that.

04:39.800 --> 04:42.240
We started working on GPU acceleration

04:42.240 --> 04:44.880
of the data processing stack several years ago.

04:44.920 --> 04:47.440
And it's a giant body of work.

04:47.440 --> 04:49.800
Ladies and gentlemen, today we're announcing

04:49.800 --> 04:54.120
that Spark 3.0, the next generation of Spark,

04:54.120 --> 04:56.320
will be NVIDIA accelerated.

04:56.320 --> 04:58.720
This is a collaboration between ourselves

04:58.720 --> 05:01.600
and a large community of researchers and developers

05:01.600 --> 05:03.960
in open source all around the world.

05:03.960 --> 05:06.840
And the results are really fantastic.

05:06.840 --> 05:10.960
It's possible because of several groundbreaking achievements.

05:10.960 --> 05:13.120
The first is the work that we did

05:13.160 --> 05:16.800
with Melanox NVIDIA called GPU Direct Storage.

05:16.800 --> 05:20.760
In the acceleration of GPU Direct Storage and UCX,

05:20.760 --> 05:22.720
this framework that makes possible,

05:22.720 --> 05:25.760
the management of IO and storage

05:25.760 --> 05:29.000
and multi-node computing lightning fast.

05:29.000 --> 05:32.440
Second is the scheduler of Spark.

05:32.440 --> 05:36.120
Scheduler of Spark now is aware of GPU and the GPU memory

05:36.120 --> 05:38.480
so that it could partition work to the GPUs

05:38.480 --> 05:40.680
and schedule it in a distributed way

05:40.680 --> 05:42.640
and manage the computation

05:42.640 --> 05:45.240
of this giant network of computers.

05:45.240 --> 05:48.080
Third, a library called Rapids

05:48.080 --> 05:50.320
that has the ability to ingest data,

05:50.320 --> 05:53.000
create data frames, do feature engineering,

05:53.000 --> 05:57.080
do SQL queries and intercept the calls of Spark

05:57.080 --> 05:59.080
to be accelerated by our GPU.

05:59.080 --> 06:03.600
And then lastly, Spark has a Spark SQL accelerator

06:03.600 --> 06:04.960
they call Catalyst,

06:04.960 --> 06:07.520
and that has been optimized for NVIDIA GPUs.

06:07.520 --> 06:12.320
These elements make possible Spark 3.0.

06:12.320 --> 06:15.400
Let me show you the potential acceleration

06:15.400 --> 06:17.440
that data scientists will be able to enjoy.

06:17.440 --> 06:21.200
What you're looking at here is the benchmark of Rapids,

06:21.200 --> 06:24.000
the foundation of Spark 3.0.

06:24.000 --> 06:27.040
This particular benchmark is TPCX BB,

06:27.040 --> 06:28.440
big data benchmark.

06:28.440 --> 06:31.400
This particular data set is a scale factor of 10,000,

06:31.400 --> 06:33.440
which basically is 10 terabytes.

06:33.440 --> 06:36.320
The state of the art is a Dell server

06:36.320 --> 06:37.880
costs about a million dollars

06:37.880 --> 06:41.680
and has the ability to deliver 17 gigabytes per second

06:41.680 --> 06:44.640
of data movement through this benchmark.

06:44.640 --> 06:46.440
This particular benchmark is hard to beat.

06:46.440 --> 06:47.640
And the reason for that

06:47.640 --> 06:50.400
is because not only does it have to be fast,

06:50.400 --> 06:51.800
it also has to be cost effective.

06:51.800 --> 06:52.800
And the reason for that

06:52.800 --> 06:54.480
is because price performance matters.

06:54.480 --> 06:57.160
The fastest in the world today is the Dell server

06:57.160 --> 07:01.040
at a million dollars and 17 gigabytes per second.

07:01.040 --> 07:04.160
With Spark 3.0 sitting on top of Rapids,

07:04.160 --> 07:09.000
Rapids benchmarked on TPCX BB delivers

07:09.000 --> 07:14.000
163 gigabytes per second for two million dollars.

07:14.840 --> 07:18.160
10 times the performance and only twice the cost.

07:18.160 --> 07:20.640
If you were to look at this in another way,

07:20.640 --> 07:23.760
suppose you were to create a data center

07:23.760 --> 07:26.120
that is able to achieve the same performance

07:26.120 --> 07:28.800
as two million dollars of DGX's

07:28.800 --> 07:33.240
accelerating Rapids of this benchmark, TPCX BB,

07:33.240 --> 07:38.320
it would cost you 10 million dollars and 150 kilowatts.

07:38.320 --> 07:41.880
Now, of course, data centers routinely process

07:41.880 --> 07:43.560
a lot more than terabytes.

07:43.560 --> 07:44.800
You're gonna need data centers

07:44.800 --> 07:47.160
way larger than this in the future,

07:47.160 --> 07:49.760
as data continues to grow exponentially.

07:49.760 --> 07:53.880
And so the ability to accelerate Spark 3.0

07:53.880 --> 07:57.520
with a library we call Rapids is utterly groundbreaking.

07:57.520 --> 08:01.240
The result is really spectacular.

08:01.240 --> 08:04.960
One fifth the cost, one third the power,

08:04.960 --> 08:07.600
one fifth the cost and one third the power.

08:08.680 --> 08:12.920
The more you buy, the more you save.

08:12.920 --> 08:14.440
In fact, Databricks,

08:14.440 --> 08:17.280
which offers industrial strength Spark

08:17.280 --> 08:20.440
at a large scale as a service is doing fantastic.

08:20.440 --> 08:23.480
Every single day, a million virtual machines

08:23.480 --> 08:26.560
are spun up to do data processing on Spark.

08:26.560 --> 08:29.040
And they're so delighted by the work and acceleration

08:29.040 --> 08:31.040
that they're gonna go accelerate Databricks

08:31.040 --> 08:33.280
with NVIDIA GPUs.

08:33.280 --> 08:34.560
They're a fantastic partner.

08:34.560 --> 08:37.440
I'm so happy with all the work that we've done together.

08:37.440 --> 08:39.360
Leading cloud service providers

08:39.360 --> 08:42.040
are offering Spark accelerated in their cloud

08:42.040 --> 08:45.440
or they're accelerating their proprietary machine learning

08:45.440 --> 08:49.280
pipeline and data processing pipeline with NVIDIA Rapids.

08:49.280 --> 08:53.400
Amazon SageMaker, Azure Machine Learning, Databricks,

08:53.400 --> 08:56.440
Google Cloud AI, Google Cloud Dataproc

08:56.440 --> 08:59.720
are now going to be accelerated with NVIDIA GPUs

08:59.720 --> 09:02.240
for data processing and data analytics.

09:02.240 --> 09:05.040
Spark acceleration is a great achievement.

09:05.040 --> 09:06.680
I'm so proud of the team.

09:06.680 --> 09:09.600
It's such a large body of work and it's taken us years.

09:09.600 --> 09:11.280
And it requires the collaboration

09:11.280 --> 09:14.680
with hundreds of collaborators in open source

09:14.680 --> 09:17.720
built on several layers of foundational

09:17.720 --> 09:19.680
and fundamental new technology.

09:19.680 --> 09:22.800
And now the part that is growing exponentially difficult,

09:22.800 --> 09:26.440
the first stage of machine learning is now accelerated.

09:26.440 --> 09:29.360
Data scientists all over the world are gonna be thrilled.

09:29.360 --> 09:34.200
Entire end-to-end from data processing to inference.

09:34.200 --> 09:36.040
We have three libraries.

09:36.040 --> 09:39.280
Rapids for data processing, QDNN,

09:39.280 --> 09:41.840
our core library for deep learning AI,

09:41.840 --> 09:45.320
and then third, TensorRT, our optimizing compiler

09:45.320 --> 09:47.120
for these computational graphs

09:47.120 --> 09:51.000
that are created by the training frameworks.

09:51.040 --> 09:54.040
The end-to-end acceleration is now complete

09:54.040 --> 09:57.360
and we will continue to advance it over time.

09:57.360 --> 10:01.720
But this represents the foundation of NVIDIA AI.

10:01.720 --> 10:03.000
I can't be more proud.

10:03.000 --> 10:04.200
The team have done a great job.

10:04.200 --> 10:05.040
Thank you.

