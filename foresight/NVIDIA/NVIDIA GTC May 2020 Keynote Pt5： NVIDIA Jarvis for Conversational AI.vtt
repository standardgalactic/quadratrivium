WEBVTT

00:00.000 --> 00:08.000
Inference is the last stage of the machine learning pipeline.

00:08.000 --> 00:14.000
This is where you take the model that you trained and deploy it into services to make predictions.

00:14.000 --> 00:20.000
What comes out of the machine learning pipeline and frameworks are computational graphs that are incredibly complex.

00:20.000 --> 00:26.000
These are gigantic computational graphs and there are so many different types of neural network architectures

00:26.000 --> 00:31.000
that the computer science of compiling these computational graphs into a target machine

00:31.000 --> 00:38.000
to run as fast as possible with all the different types of numeric precision is an incredibly complex problem.

00:38.000 --> 00:41.000
We created an optimizing compiler, TensorFlow RT.

00:41.000 --> 00:48.000
We're now in our seventh generation. This generation can handle CNNs and transformers and now we can handle RNNs as well.

00:48.000 --> 00:52.000
TensorFlow RT 7 includes over a thousand optimized kernels.

00:52.000 --> 00:59.000
And in fact, our developers could do the same for themselves and create all kinds of custom kernels and custom neural networks.

00:59.000 --> 01:05.000
We support precisions from FP32, FP16 all the way to 8-bit and 4-bit integer.

01:05.000 --> 01:10.000
TensorFlow RT has been a huge success. The number of developers that use TensorFlow RT increased 10x.

01:10.000 --> 01:16.000
The world's top 300 internet services now have NVIDIA GPUs in their data centers to do inference.

01:16.000 --> 01:21.000
It just kind of shows you how many industries deep learning and AI is going to impact.

01:21.000 --> 01:26.000
One of the most important applications that we can now enable is conversational AI.

01:26.000 --> 01:33.000
It is one of the most challenging inference tasks because conversational AI requires interactive performance.

01:33.000 --> 01:39.000
The elements that make up a conversational AI pipeline has achieved tremendous breakthroughs recently.

01:39.000 --> 01:46.000
Starting from ASR, automatic speech recognition, to natural language understanding, to text-to-speech speech synthesis.

01:46.000 --> 01:54.000
And now it's possible for us to imagine for the very first time having an interactive, low-latency conversational AI pipeline.

01:54.000 --> 01:57.000
However, the application is very complex still.

01:57.000 --> 02:02.000
The models are state-of-the-art. Training those models take a tremendous amount of computation.

02:02.000 --> 02:10.000
And the ability to put together all of the models end-to-end on top of a computing platform fully accelerated is something the world's never done before.

02:10.000 --> 02:11.000
Until now.

02:11.000 --> 02:18.000
Ladies and gentlemen, we're announcing today a new application framework we call Jarvis.

02:18.000 --> 02:24.000
And because it has the ability to connect into Omniverse, we have the ability to create an interactive 3D chatbot.

02:24.000 --> 02:29.000
One of the AI models that makes this entire experience possible is what we call audio-to-face.

02:29.000 --> 02:36.000
It takes an audio input and it has learned how to animate a geometric mesh to animate facial expressions while talking.

02:36.000 --> 02:41.000
That then drives our computer graphics system in Omniverse and creates an interactive chatbot.

02:41.000 --> 02:44.000
Let me now show you our AI model audio-to-face.

02:44.000 --> 02:49.000
The speech here is coming from a rapper, one of our employees, John Delabona.

02:49.000 --> 02:52.000
And it is really fantastic. Take a look.

03:06.000 --> 03:11.000
Once we have audio-to-face, we have the ability to create an interactive 3D chatbot.

03:11.000 --> 03:16.000
And we have the ability to create an interactive 3D chatbot.

03:16.000 --> 03:21.000
And we have the ability to create an interactive 3D chatbot.

03:21.000 --> 03:26.000
And we have the ability to create an interactive 3D chatbot.

03:26.000 --> 03:31.000
And we have the ability to create an interactive 3D chatbot.

03:31.000 --> 03:35.000
And we have the ability to create an interactive 3D chatbot.

03:35.000 --> 03:39.000
Once we have audio-to-face, then we can connect it to our conversational AI model,

03:39.000 --> 03:44.000
where you can ask it a question, it would understand the question, answer the question,

03:44.000 --> 03:50.000
and then synthesize speech from that, which goes into our audio-to-face model, which animates and drives a 3D model.

03:50.000 --> 03:55.000
We've created a demonstration we call MISTI, our first interactive 3D chatbot.

03:55.000 --> 03:57.000
Hello.

03:57.000 --> 03:59.000
Hi, I'm MISTI.

03:59.000 --> 04:00.000
How are you?

04:00.000 --> 04:03.000
I'm doing well. Thank you for asking.

04:03.000 --> 04:04.000
What are you here for?

04:04.000 --> 04:08.000
I'm here to give you the latest weather information around the world.

04:08.000 --> 04:12.000
Tell you interesting facts about meteorology and keep you company.

04:12.000 --> 04:14.000
Should I bring an umbrella tomorrow?

04:14.000 --> 04:19.000
It is not expected to rain in Santa Clara, California tomorrow afternoon.

04:19.000 --> 04:22.000
What is the hottest city in the world?

04:22.000 --> 04:29.000
The hottest city in the world is Mecca, in Saudi Arabia where temperatures can reach 122 degrees Fahrenheit.

04:29.000 --> 04:31.000
What is the coldest city?

04:31.000 --> 04:38.000
The coldest city in the world is Yakutsk in Russia with temperatures around minus 40 degrees Fahrenheit in the winter months.

04:42.000 --> 04:44.000
Tell me something about lightning.

04:44.000 --> 04:48.000
We see lightning first, because light travels faster than sound.

04:52.000 --> 04:54.000
How come you know so much about weather?

04:54.000 --> 04:57.000
I know the weather information from the internet.

04:57.000 --> 05:02.000
I have lived for a long time and have been talking about the weather for as long as I know.

05:02.000 --> 05:04.000
Where are you from, MISTI?

05:04.000 --> 05:06.000
I'm from a place called Omniverse.

05:06.000 --> 05:08.000
Who made you?

05:08.000 --> 05:11.000
I'm created by the wonderful scientists of NVIDIA.

05:11.000 --> 05:13.000
What powers you?

05:13.000 --> 05:18.000
I'm powered by NVIDIA GPU and the CUDA neurons inside of them.

05:18.000 --> 05:19.000
Thank you, MISTI.

05:19.000 --> 05:23.000
Thank you. It was a pleasure talking to you.

05:23.000 --> 05:37.000
As you could see, the ability to have an interactive conversation requires the conversation AI to process the speech, the natural language understanding, synthesize and also render the graphics as fast as possible.

05:37.000 --> 05:52.000
And you have to process the entire pipeline end-to-end from speech recognition, language understanding, text-to-speech as well as driving and generating the computer graphics in just a few hundred milliseconds in order to feel like you're having an interactive conversation.

05:52.000 --> 06:02.000
And that's what NVIDIA Jarvis is about, a multimodal conversational AI service framework that simplifies the creation and the development of conversational AI services.

06:02.000 --> 06:17.000
It includes state-of-the-art models that has been pre-pipelined into these helms charts, optimized to run on NVIDIA's Triton inference server, which runs on top of our GPUs, and the performance is interactive, and the entire pipeline end-to-end is only a few hundred milliseconds.

06:17.000 --> 06:18.000
But there's more.

06:18.000 --> 06:21.000
Jarvis comes with pre-trained state-of-the-art models.

06:21.000 --> 06:25.000
These state-of-the-art models have been trained with a great deal of data and a lot of computation.

06:25.000 --> 06:33.000
In fact, what's available in NGC, the NVIDIA GPU cloud, represents several hundred thousand DGX training hours.

06:33.000 --> 06:40.000
If you had one DGX, it would take you something like 10 to 20 years to be able to process all of the data necessary to train these models.

06:40.000 --> 06:41.000
We've done that for you.

06:41.000 --> 06:47.000
And then it comes with a new tool called NEMO, which takes the pre-trained model and augments it with your data.

06:47.000 --> 06:52.000
Your data could, because of a particular domain, it could be in healthcare or insurance or financial services,

06:52.000 --> 07:00.000
or maybe it's a call center for a particular business, and there's special language or special vocabulary that Jarvis needs to learn.

07:00.000 --> 07:07.000
And so you would take that data and you would train, retrain the Jarvis models with this tool we call NEMO.

07:07.000 --> 07:11.000
This end-to-end pipeline from pre-trained models, retraining with NEMO,

07:11.000 --> 07:21.000
and then the application of all of these state-of-the-art models all put together in a Helms chart running on a service on top of trying this entire end-to-end system we call Jarvis.

07:21.000 --> 07:30.000
Conversational AI is going to automate conversations and it's going to make it possible for us to deploy automated services in all kinds of new applications,

07:30.000 --> 07:33.000
whether it's video conferencing, call centers, smart speakers.

07:33.000 --> 07:36.000
One of my favorite applications is video conferencing.

07:36.000 --> 07:40.000
When you have a group conference, one person can talk at a time.

07:40.000 --> 07:44.000
It would be nice sometimes if we can kind of hear multiple people talking.

07:44.000 --> 07:45.000
So it wouldn't be amazing.

07:45.000 --> 07:50.000
Whenever anybody talks, it's picked up in closed caption or translated in real time,

07:50.000 --> 07:54.000
or the end of a conference, a simple summary and transcription is done.

07:54.000 --> 07:59.000
Video conferencing with a conversational AI agent is going to be really transformed.

07:59.000 --> 08:03.000
We know that the number of people who are calling into call centers these days is growing,

08:03.000 --> 08:07.000
and many of the call centers have people wait half an hour to much longer.

08:07.000 --> 08:13.000
Jarvis conversational AI is going to be one of the most important real-time inference applications.

08:13.000 --> 08:15.000
So that's NVIDIA AI.

08:15.000 --> 08:21.000
It is now end-to-end fully accelerated from data processing with the amount of data growing exponentially.

08:21.000 --> 08:25.000
Spark is more important than ever and now it's accelerated by NVIDIA.

08:25.000 --> 08:31.000
Second, the deep learning and the model training pipeline, we have accelerated for a long time.

08:31.000 --> 08:36.000
One of the particular applications recommender systems is so complicated,

08:36.000 --> 08:42.000
we decided to create an application framework just for it so that we can democratize recommender systems.

08:42.000 --> 08:44.000
We call NVIDIA Merlin.

08:44.000 --> 08:46.000
And then lastly, inference.

08:46.000 --> 08:48.000
TensorRT 7.0 is here.

08:48.000 --> 08:53.000
The number of people who are downloading it and using it is really growing and we're super excited about that.

08:53.000 --> 09:00.000
We decided to create an application framework for every company to be able to create state-of-the-art conversational AI models.

09:00.000 --> 09:02.000
We call that NVIDIA Jarvis.

09:02.000 --> 09:10.000
This end-to-end acceleration platform and some of the reference applications is what we call NVIDIA AI.

