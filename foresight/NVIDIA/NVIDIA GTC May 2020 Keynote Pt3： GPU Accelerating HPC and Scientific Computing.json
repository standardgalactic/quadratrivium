{"text": " Let's talk about high-performance computing. It is clear now that acceleration is going to be the path forward for scientific and for high-performance computing. As I mentioned before, accelerated computing has four pillars. The first, of course, is the accelerator, the advanced GPUs. The second is the acceleration stack for each one of the computational domains. The third is systems, and last is developers, ultimately the applications that we accelerate. This year, we did really great work. We've accelerated now over 700 applications. And each and every single year, at every conference I show you, our golden suite, the suite that we track on a regular basis to make sure that we continue to engineer advances into libraries, into the stack, so that applications continue to improve in performance, even if we don't introduce new GPUs. And as you see, over the course of the last four years, we've increased application performance by 4x. And the green bar is something that I'm going to talk to you about. We're going to offer you a new platform and it's going to give high-performance computing a huge boost. We also brought CUDA to ARM computing systems. ARM server CPUs are seeing adoption all over the world. In hyperscale, there's Amazon, Fujitsu Super Computing, Cavium, part of Marvell now, a new exciting company called Ampere Computing, or in China, Huawei. All of the suite of NVIDIA tools and libraries are now available for ARM. We also introduced a brand new SDK for IO processing this year. We call Magnum IO. Magnum IO includes all kinds of great things from our DMA, of course, to the ability to communicate across multiple nodes and move data directly from storage to our GPUs. The suite of libraries is going to continue to advance. Magnum IO is going to be one of our most important libraries. Data processing and networking and storage is going to become more and more important to data center-scale computing over time. We introduced two new stacks this year. NVIDIA parabricks for genomics processing, the ability to do variant calling at very high performances, and a large body of work that we've been working on for several years called NVIDIA Rappers for Data Analytics. Machine learning has become one of HPC's grand challenges. The advances of machine learning and the popularity of this approach has caused companies, institutes, and data centers to collect a vast amount of data. The machine learning pipeline consists of three things. ETL, which creates the data frame, does all the feature engineering necessary for the machine learning algorithms to train on, which creates the model which is then put into operations we call inference. These three stages of the pipeline have unique and different computational challenges. The first stage of the machine learning pipeline data processing is becoming more complex than ever. In fact, most data scientists will tell you they spend the vast majority of their time doing feature engineering and data processing in the front stage of the machine learning pipeline. What used to be processing hundreds of megabytes to gigabytes to terabytes of data, companies are now routinely processing tens, if not hundreds of terabytes of data and moving to petabytes of data. It is the reason why Spark is so popular. Spark is an incredible computational platform. It turns an entire data center into a compute engine. It partitions a very large data set to be processed across a bunch of servers in the data center. It was the brainchild of Matt Zaharia at the Berkeley AMP Lab and spun out and became Apache Spark. It now has over a thousand companies contributing to it, nearly a million lines of code, 16,000 plus companies around the world uses it for data processing today. Well, the amount of data that they're processing is growing exponentially. It is now reaching the limits of what Spark can do. Here's the reason why. The CPU set is being distributed across has a fundamental working set in the order of megabytes. A CPU naturally likes to work in its cache and its caches typically on the tens of megabytes. When the data set is now the hundreds of terabytes and into petabytes, the overhead of coordinating the CPU servers is becoming the greatest bottleneck. And we're starting to see the limits now. What if instead of working on processors that has tens of megabytes of working set, let's move towards a processor that has tens of gigabytes of working set. And if we could use multiple GPUs to create large memories, then it is now possible for us to imagine scaling beyond that. We started working on GPU acceleration of the data processing stack several years ago. And it's a giant body of work. Ladies and gentlemen, today we're announcing that Spark 3.0, the next generation of Spark, will be NVIDIA accelerated. This is a collaboration between ourselves and a large community of researchers and developers in open source all around the world. And the results are really fantastic. It's possible because of several groundbreaking achievements. The first is the work that we did with Melanox NVIDIA called GPU Direct Storage. In the acceleration of GPU Direct Storage and UCX, this framework that makes possible, the management of IO and storage and multi-node computing lightning fast. Second is the scheduler of Spark. Scheduler of Spark now is aware of GPU and the GPU memory so that it could partition work to the GPUs and schedule it in a distributed way and manage the computation of this giant network of computers. Third, a library called Rapids that has the ability to ingest data, create data frames, do feature engineering, do SQL queries and intercept the calls of Spark to be accelerated by our GPU. And then lastly, Spark has a Spark SQL accelerator they call Catalyst, and that has been optimized for NVIDIA GPUs. These elements make possible Spark 3.0. Let me show you the potential acceleration that data scientists will be able to enjoy. What you're looking at here is the benchmark of Rapids, the foundation of Spark 3.0. This particular benchmark is TPCX BB, big data benchmark. This particular data set is a scale factor of 10,000, which basically is 10 terabytes. The state of the art is a Dell server costs about a million dollars and has the ability to deliver 17 gigabytes per second of data movement through this benchmark. This particular benchmark is hard to beat. And the reason for that is because not only does it have to be fast, it also has to be cost effective. And the reason for that is because price performance matters. The fastest in the world today is the Dell server at a million dollars and 17 gigabytes per second. With Spark 3.0 sitting on top of Rapids, Rapids benchmarked on TPCX BB delivers 163 gigabytes per second for two million dollars. 10 times the performance and only twice the cost. If you were to look at this in another way, suppose you were to create a data center that is able to achieve the same performance as two million dollars of DGX's accelerating Rapids of this benchmark, TPCX BB, it would cost you 10 million dollars and 150 kilowatts. Now, of course, data centers routinely process a lot more than terabytes. You're gonna need data centers way larger than this in the future, as data continues to grow exponentially. And so the ability to accelerate Spark 3.0 with a library we call Rapids is utterly groundbreaking. The result is really spectacular. One fifth the cost, one third the power, one fifth the cost and one third the power. The more you buy, the more you save. In fact, Databricks, which offers industrial strength Spark at a large scale as a service is doing fantastic. Every single day, a million virtual machines are spun up to do data processing on Spark. And they're so delighted by the work and acceleration that they're gonna go accelerate Databricks with NVIDIA GPUs. They're a fantastic partner. I'm so happy with all the work that we've done together. Leading cloud service providers are offering Spark accelerated in their cloud or they're accelerating their proprietary machine learning pipeline and data processing pipeline with NVIDIA Rapids. Amazon SageMaker, Azure Machine Learning, Databricks, Google Cloud AI, Google Cloud Dataproc are now going to be accelerated with NVIDIA GPUs for data processing and data analytics. Spark acceleration is a great achievement. I'm so proud of the team. It's such a large body of work and it's taken us years. And it requires the collaboration with hundreds of collaborators in open source built on several layers of foundational and fundamental new technology. And now the part that is growing exponentially difficult, the first stage of machine learning is now accelerated. Data scientists all over the world are gonna be thrilled. Entire end-to-end from data processing to inference. We have three libraries. Rapids for data processing, QDNN, our core library for deep learning AI, and then third, TensorRT, our optimizing compiler for these computational graphs that are created by the training frameworks. The end-to-end acceleration is now complete and we will continue to advance it over time. But this represents the foundation of NVIDIA AI. I can't be more proud. The team have done a great job. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.16, "text": " Let's talk about high-performance computing.", "tokens": [50364, 961, 311, 751, 466, 1090, 12, 50242, 15866, 13, 50772], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 1, "seek": 0, "start": 8.16, "end": 11.64, "text": " It is clear now that acceleration is going to be the path forward", "tokens": [50772, 467, 307, 1850, 586, 300, 17162, 307, 516, 281, 312, 264, 3100, 2128, 50946], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 2, "seek": 0, "start": 11.64, "end": 14.52, "text": " for scientific and for high-performance computing.", "tokens": [50946, 337, 8134, 293, 337, 1090, 12, 50242, 15866, 13, 51090], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 3, "seek": 0, "start": 14.52, "end": 16.06, "text": " As I mentioned before,", "tokens": [51090, 1018, 286, 2835, 949, 11, 51167], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 4, "seek": 0, "start": 16.06, "end": 18.88, "text": " accelerated computing has four pillars.", "tokens": [51167, 29763, 15866, 575, 1451, 26729, 13, 51308], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 5, "seek": 0, "start": 18.88, "end": 21.96, "text": " The first, of course, is the accelerator, the advanced GPUs.", "tokens": [51308, 440, 700, 11, 295, 1164, 11, 307, 264, 39889, 11, 264, 7339, 18407, 82, 13, 51462], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 6, "seek": 0, "start": 21.96, "end": 24.76, "text": " The second is the acceleration stack", "tokens": [51462, 440, 1150, 307, 264, 17162, 8630, 51602], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 7, "seek": 0, "start": 24.76, "end": 26.88, "text": " for each one of the computational domains.", "tokens": [51602, 337, 1184, 472, 295, 264, 28270, 25514, 13, 51708], "temperature": 0.0, "avg_logprob": -0.19796656490711684, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.026682348921895027}, {"id": 8, "seek": 2688, "start": 27.0, "end": 30.48, "text": " The third is systems, and last is developers,", "tokens": [50370, 440, 2636, 307, 3652, 11, 293, 1036, 307, 8849, 11, 50544], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 9, "seek": 2688, "start": 30.48, "end": 33.04, "text": " ultimately the applications that we accelerate.", "tokens": [50544, 6284, 264, 5821, 300, 321, 21341, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 10, "seek": 2688, "start": 33.04, "end": 35.2, "text": " This year, we did really great work.", "tokens": [50672, 639, 1064, 11, 321, 630, 534, 869, 589, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 11, "seek": 2688, "start": 35.2, "end": 38.44, "text": " We've accelerated now over 700 applications.", "tokens": [50780, 492, 600, 29763, 586, 670, 15204, 5821, 13, 50942], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 12, "seek": 2688, "start": 38.44, "end": 41.44, "text": " And each and every single year, at every conference I show you,", "tokens": [50942, 400, 1184, 293, 633, 2167, 1064, 11, 412, 633, 7586, 286, 855, 291, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 13, "seek": 2688, "start": 41.44, "end": 44.599999999999994, "text": " our golden suite, the suite that we track on a regular basis", "tokens": [51092, 527, 9729, 14205, 11, 264, 14205, 300, 321, 2837, 322, 257, 3890, 5143, 51250], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 14, "seek": 2688, "start": 44.599999999999994, "end": 47.480000000000004, "text": " to make sure that we continue to engineer advances", "tokens": [51250, 281, 652, 988, 300, 321, 2354, 281, 11403, 25297, 51394], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 15, "seek": 2688, "start": 47.480000000000004, "end": 49.16, "text": " into libraries, into the stack,", "tokens": [51394, 666, 15148, 11, 666, 264, 8630, 11, 51478], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 16, "seek": 2688, "start": 49.16, "end": 52.36, "text": " so that applications continue to improve in performance,", "tokens": [51478, 370, 300, 5821, 2354, 281, 3470, 294, 3389, 11, 51638], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 17, "seek": 2688, "start": 52.36, "end": 55.120000000000005, "text": " even if we don't introduce new GPUs.", "tokens": [51638, 754, 498, 321, 500, 380, 5366, 777, 18407, 82, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1462474758342161, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0024333533365279436}, {"id": 18, "seek": 5512, "start": 55.12, "end": 57.879999999999995, "text": " And as you see, over the course of the last four years,", "tokens": [50364, 400, 382, 291, 536, 11, 670, 264, 1164, 295, 264, 1036, 1451, 924, 11, 50502], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 19, "seek": 5512, "start": 57.879999999999995, "end": 60.879999999999995, "text": " we've increased application performance by 4x.", "tokens": [50502, 321, 600, 6505, 3861, 3389, 538, 1017, 87, 13, 50652], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 20, "seek": 5512, "start": 60.879999999999995, "end": 63.4, "text": " And the green bar is something that I'm going to talk to you about.", "tokens": [50652, 400, 264, 3092, 2159, 307, 746, 300, 286, 478, 516, 281, 751, 281, 291, 466, 13, 50778], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 21, "seek": 5512, "start": 63.4, "end": 65.39999999999999, "text": " We're going to offer you a new platform", "tokens": [50778, 492, 434, 516, 281, 2626, 291, 257, 777, 3663, 50878], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 22, "seek": 5512, "start": 65.39999999999999, "end": 68.47999999999999, "text": " and it's going to give high-performance computing a huge boost.", "tokens": [50878, 293, 309, 311, 516, 281, 976, 1090, 12, 50242, 15866, 257, 2603, 9194, 13, 51032], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 23, "seek": 5512, "start": 68.47999999999999, "end": 72.12, "text": " We also brought CUDA to ARM computing systems.", "tokens": [51032, 492, 611, 3038, 29777, 7509, 281, 45209, 15866, 3652, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 24, "seek": 5512, "start": 72.12, "end": 75.8, "text": " ARM server CPUs are seeing adoption all over the world.", "tokens": [51214, 45209, 7154, 13199, 82, 366, 2577, 19215, 439, 670, 264, 1002, 13, 51398], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 25, "seek": 5512, "start": 75.8, "end": 79.64, "text": " In hyperscale, there's Amazon, Fujitsu Super Computing,", "tokens": [51398, 682, 7420, 433, 37088, 11, 456, 311, 6795, 11, 43915, 35711, 4548, 37804, 278, 11, 51590], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 26, "seek": 5512, "start": 79.64, "end": 81.56, "text": " Cavium, part of Marvell now,", "tokens": [51590, 28066, 2197, 11, 644, 295, 2039, 48592, 586, 11, 51686], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 27, "seek": 5512, "start": 81.56, "end": 84.32, "text": " a new exciting company called Ampere Computing,", "tokens": [51686, 257, 777, 4670, 2237, 1219, 2012, 79, 323, 37804, 278, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11244186103766692, "compression_ratio": 1.639871382636656, "no_speech_prob": 0.00446316646412015}, {"id": 28, "seek": 8432, "start": 84.32, "end": 85.75999999999999, "text": " or in China, Huawei.", "tokens": [50364, 420, 294, 3533, 11, 28542, 13, 50436], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 29, "seek": 8432, "start": 85.75999999999999, "end": 89.19999999999999, "text": " All of the suite of NVIDIA tools and libraries", "tokens": [50436, 1057, 295, 264, 14205, 295, 426, 3958, 6914, 3873, 293, 15148, 50608], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 30, "seek": 8432, "start": 89.19999999999999, "end": 91.32, "text": " are now available for ARM.", "tokens": [50608, 366, 586, 2435, 337, 45209, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 31, "seek": 8432, "start": 91.32, "end": 95.11999999999999, "text": " We also introduced a brand new SDK for IO processing this year.", "tokens": [50714, 492, 611, 7268, 257, 3360, 777, 37135, 337, 39839, 9007, 341, 1064, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 32, "seek": 8432, "start": 95.11999999999999, "end": 96.47999999999999, "text": " We call Magnum IO.", "tokens": [50904, 492, 818, 19664, 449, 39839, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 33, "seek": 8432, "start": 96.47999999999999, "end": 100.0, "text": " Magnum IO includes all kinds of great things from our DMA,", "tokens": [50972, 19664, 449, 39839, 5974, 439, 3685, 295, 869, 721, 490, 527, 413, 9998, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 34, "seek": 8432, "start": 100.0, "end": 104.16, "text": " of course, to the ability to communicate across multiple nodes", "tokens": [51148, 295, 1164, 11, 281, 264, 3485, 281, 7890, 2108, 3866, 13891, 51356], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 35, "seek": 8432, "start": 104.16, "end": 107.24, "text": " and move data directly from storage to our GPUs.", "tokens": [51356, 293, 1286, 1412, 3838, 490, 6725, 281, 527, 18407, 82, 13, 51510], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 36, "seek": 8432, "start": 107.24, "end": 110.39999999999999, "text": " The suite of libraries is going to continue to advance.", "tokens": [51510, 440, 14205, 295, 15148, 307, 516, 281, 2354, 281, 7295, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 37, "seek": 8432, "start": 110.39999999999999, "end": 113.24, "text": " Magnum IO is going to be one of our most important libraries.", "tokens": [51668, 19664, 449, 39839, 307, 516, 281, 312, 472, 295, 527, 881, 1021, 15148, 13, 51810], "temperature": 0.0, "avg_logprob": -0.1371533203125, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.005463581066578627}, {"id": 38, "seek": 11324, "start": 113.28, "end": 115.64, "text": " Data processing and networking and storage", "tokens": [50366, 11888, 9007, 293, 17985, 293, 6725, 50484], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 39, "seek": 11324, "start": 115.64, "end": 117.32, "text": " is going to become more and more important", "tokens": [50484, 307, 516, 281, 1813, 544, 293, 544, 1021, 50568], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 40, "seek": 11324, "start": 117.32, "end": 119.6, "text": " to data center-scale computing over time.", "tokens": [50568, 281, 1412, 3056, 12, 20033, 15866, 670, 565, 13, 50682], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 41, "seek": 11324, "start": 119.6, "end": 122.72, "text": " We introduced two new stacks this year.", "tokens": [50682, 492, 7268, 732, 777, 30792, 341, 1064, 13, 50838], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 42, "seek": 11324, "start": 122.72, "end": 125.0, "text": " NVIDIA parabricks for genomics processing,", "tokens": [50838, 426, 3958, 6914, 971, 455, 81, 7663, 337, 1049, 29884, 9007, 11, 50952], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 43, "seek": 11324, "start": 125.0, "end": 128.92, "text": " the ability to do variant calling at very high performances,", "tokens": [50952, 264, 3485, 281, 360, 17501, 5141, 412, 588, 1090, 16087, 11, 51148], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 44, "seek": 11324, "start": 128.92, "end": 131.64, "text": " and a large body of work that we've been working on", "tokens": [51148, 293, 257, 2416, 1772, 295, 589, 300, 321, 600, 668, 1364, 322, 51284], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 45, "seek": 11324, "start": 131.64, "end": 134.6, "text": " for several years called NVIDIA Rappers for Data Analytics.", "tokens": [51284, 337, 2940, 924, 1219, 426, 3958, 6914, 497, 1746, 433, 337, 11888, 25944, 13, 51432], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 46, "seek": 11324, "start": 135.84, "end": 139.72, "text": " Machine learning has become one of HPC's grand challenges.", "tokens": [51494, 22155, 2539, 575, 1813, 472, 295, 12557, 34, 311, 2697, 4759, 13, 51688], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 47, "seek": 11324, "start": 139.72, "end": 141.07999999999998, "text": " The advances of machine learning", "tokens": [51688, 440, 25297, 295, 3479, 2539, 51756], "temperature": 0.0, "avg_logprob": -0.14147269625623687, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.00030530328513123095}, {"id": 48, "seek": 14108, "start": 141.08, "end": 143.0, "text": " and the popularity of this approach", "tokens": [50364, 293, 264, 19301, 295, 341, 3109, 50460], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 49, "seek": 14108, "start": 143.0, "end": 145.8, "text": " has caused companies, institutes, and data centers", "tokens": [50460, 575, 7008, 3431, 11, 4348, 1819, 11, 293, 1412, 10898, 50600], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 50, "seek": 14108, "start": 145.8, "end": 147.84, "text": " to collect a vast amount of data.", "tokens": [50600, 281, 2500, 257, 8369, 2372, 295, 1412, 13, 50702], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 51, "seek": 14108, "start": 147.84, "end": 149.76000000000002, "text": " The machine learning pipeline consists of three things.", "tokens": [50702, 440, 3479, 2539, 15517, 14689, 295, 1045, 721, 13, 50798], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 52, "seek": 14108, "start": 149.76000000000002, "end": 152.0, "text": " ETL, which creates the data frame,", "tokens": [50798, 36953, 43, 11, 597, 7829, 264, 1412, 3920, 11, 50910], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 53, "seek": 14108, "start": 152.0, "end": 153.76000000000002, "text": " does all the feature engineering necessary", "tokens": [50910, 775, 439, 264, 4111, 7043, 4818, 50998], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 54, "seek": 14108, "start": 153.76000000000002, "end": 156.48000000000002, "text": " for the machine learning algorithms to train on,", "tokens": [50998, 337, 264, 3479, 2539, 14642, 281, 3847, 322, 11, 51134], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 55, "seek": 14108, "start": 156.48000000000002, "end": 159.44, "text": " which creates the model which is then put into operations", "tokens": [51134, 597, 7829, 264, 2316, 597, 307, 550, 829, 666, 7705, 51282], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 56, "seek": 14108, "start": 159.44, "end": 160.60000000000002, "text": " we call inference.", "tokens": [51282, 321, 818, 38253, 13, 51340], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 57, "seek": 14108, "start": 160.60000000000002, "end": 162.92000000000002, "text": " These three stages of the pipeline", "tokens": [51340, 1981, 1045, 10232, 295, 264, 15517, 51456], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 58, "seek": 14108, "start": 162.92000000000002, "end": 166.32000000000002, "text": " have unique and different computational challenges.", "tokens": [51456, 362, 3845, 293, 819, 28270, 4759, 13, 51626], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 59, "seek": 14108, "start": 166.32000000000002, "end": 169.8, "text": " The first stage of the machine learning pipeline data processing", "tokens": [51626, 440, 700, 3233, 295, 264, 3479, 2539, 15517, 1412, 9007, 51800], "temperature": 0.0, "avg_logprob": -0.08709297974904379, "compression_ratio": 1.8798586572438163, "no_speech_prob": 0.001500986167229712}, {"id": 60, "seek": 16980, "start": 169.84, "end": 172.24, "text": " is becoming more complex than ever.", "tokens": [50366, 307, 5617, 544, 3997, 813, 1562, 13, 50486], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 61, "seek": 16980, "start": 172.24, "end": 174.60000000000002, "text": " In fact, most data scientists will tell you", "tokens": [50486, 682, 1186, 11, 881, 1412, 7708, 486, 980, 291, 50604], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 62, "seek": 16980, "start": 174.60000000000002, "end": 176.76000000000002, "text": " they spend the vast majority of their time", "tokens": [50604, 436, 3496, 264, 8369, 6286, 295, 641, 565, 50712], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 63, "seek": 16980, "start": 176.76000000000002, "end": 179.24, "text": " doing feature engineering and data processing", "tokens": [50712, 884, 4111, 7043, 293, 1412, 9007, 50836], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 64, "seek": 16980, "start": 179.24, "end": 181.8, "text": " in the front stage of the machine learning pipeline.", "tokens": [50836, 294, 264, 1868, 3233, 295, 264, 3479, 2539, 15517, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 65, "seek": 16980, "start": 181.8, "end": 184.52, "text": " What used to be processing hundreds of megabytes", "tokens": [50964, 708, 1143, 281, 312, 9007, 6779, 295, 10816, 24538, 51100], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 66, "seek": 16980, "start": 184.52, "end": 186.64000000000001, "text": " to gigabytes to terabytes of data,", "tokens": [51100, 281, 42741, 281, 1796, 24538, 295, 1412, 11, 51206], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 67, "seek": 16980, "start": 186.64000000000001, "end": 189.68, "text": " companies are now routinely processing tens,", "tokens": [51206, 3431, 366, 586, 40443, 9007, 10688, 11, 51358], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 68, "seek": 16980, "start": 189.68, "end": 191.96, "text": " if not hundreds of terabytes of data", "tokens": [51358, 498, 406, 6779, 295, 1796, 24538, 295, 1412, 51472], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 69, "seek": 16980, "start": 191.96, "end": 194.28, "text": " and moving to petabytes of data.", "tokens": [51472, 293, 2684, 281, 3817, 24538, 295, 1412, 13, 51588], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 70, "seek": 16980, "start": 194.28, "end": 197.76000000000002, "text": " It is the reason why Spark is so popular.", "tokens": [51588, 467, 307, 264, 1778, 983, 23424, 307, 370, 3743, 13, 51762], "temperature": 0.0, "avg_logprob": -0.05547817966394257, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0002694559807423502}, {"id": 71, "seek": 19776, "start": 197.76, "end": 201.12, "text": " Spark is an incredible computational platform.", "tokens": [50364, 23424, 307, 364, 4651, 28270, 3663, 13, 50532], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 72, "seek": 19776, "start": 201.12, "end": 204.56, "text": " It turns an entire data center into a compute engine.", "tokens": [50532, 467, 4523, 364, 2302, 1412, 3056, 666, 257, 14722, 2848, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 73, "seek": 19776, "start": 204.56, "end": 208.6, "text": " It partitions a very large data set to be processed", "tokens": [50704, 467, 644, 2451, 257, 588, 2416, 1412, 992, 281, 312, 18846, 50906], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 74, "seek": 19776, "start": 208.6, "end": 211.12, "text": " across a bunch of servers in the data center.", "tokens": [50906, 2108, 257, 3840, 295, 15909, 294, 264, 1412, 3056, 13, 51032], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 75, "seek": 19776, "start": 211.12, "end": 213.92, "text": " It was the brainchild of Matt Zaharia", "tokens": [51032, 467, 390, 264, 3567, 15129, 295, 7397, 1176, 545, 9831, 51172], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 76, "seek": 19776, "start": 213.92, "end": 215.28, "text": " at the Berkeley AMP Lab", "tokens": [51172, 412, 264, 23684, 31616, 10137, 51240], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 77, "seek": 19776, "start": 215.28, "end": 217.6, "text": " and spun out and became Apache Spark.", "tokens": [51240, 293, 37038, 484, 293, 3062, 46597, 23424, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 78, "seek": 19776, "start": 217.6, "end": 220.68, "text": " It now has over a thousand companies contributing to it,", "tokens": [51356, 467, 586, 575, 670, 257, 4714, 3431, 19270, 281, 309, 11, 51510], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 79, "seek": 19776, "start": 220.68, "end": 222.39999999999998, "text": " nearly a million lines of code,", "tokens": [51510, 6217, 257, 2459, 3876, 295, 3089, 11, 51596], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 80, "seek": 19776, "start": 222.39999999999998, "end": 224.76, "text": " 16,000 plus companies around the world", "tokens": [51596, 3165, 11, 1360, 1804, 3431, 926, 264, 1002, 51714], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 81, "seek": 19776, "start": 224.76, "end": 226.92, "text": " uses it for data processing today.", "tokens": [51714, 4960, 309, 337, 1412, 9007, 965, 13, 51822], "temperature": 0.0, "avg_logprob": -0.1057969582181017, "compression_ratio": 1.6347517730496455, "no_speech_prob": 0.0003799476253334433}, {"id": 82, "seek": 22692, "start": 226.95999999999998, "end": 228.55999999999997, "text": " Well, the amount of data that they're processing", "tokens": [50366, 1042, 11, 264, 2372, 295, 1412, 300, 436, 434, 9007, 50446], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 83, "seek": 22692, "start": 228.55999999999997, "end": 230.0, "text": " is growing exponentially.", "tokens": [50446, 307, 4194, 37330, 13, 50518], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 84, "seek": 22692, "start": 230.0, "end": 233.88, "text": " It is now reaching the limits of what Spark can do.", "tokens": [50518, 467, 307, 586, 9906, 264, 10406, 295, 437, 23424, 393, 360, 13, 50712], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 85, "seek": 22692, "start": 233.88, "end": 235.0, "text": " Here's the reason why.", "tokens": [50712, 1692, 311, 264, 1778, 983, 13, 50768], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 86, "seek": 22692, "start": 235.0, "end": 237.92, "text": " The CPU set is being distributed across", "tokens": [50768, 440, 13199, 992, 307, 885, 12631, 2108, 50914], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 87, "seek": 22692, "start": 237.92, "end": 242.04, "text": " has a fundamental working set in the order of megabytes.", "tokens": [50914, 575, 257, 8088, 1364, 992, 294, 264, 1668, 295, 10816, 24538, 13, 51120], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 88, "seek": 22692, "start": 242.04, "end": 244.44, "text": " A CPU naturally likes to work in its cache", "tokens": [51120, 316, 13199, 8195, 5902, 281, 589, 294, 1080, 19459, 51240], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 89, "seek": 22692, "start": 244.44, "end": 248.07999999999998, "text": " and its caches typically on the tens of megabytes.", "tokens": [51240, 293, 1080, 269, 13272, 5850, 322, 264, 10688, 295, 10816, 24538, 13, 51422], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 90, "seek": 22692, "start": 248.07999999999998, "end": 250.83999999999997, "text": " When the data set is now the hundreds of terabytes", "tokens": [51422, 1133, 264, 1412, 992, 307, 586, 264, 6779, 295, 1796, 24538, 51560], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 91, "seek": 22692, "start": 250.83999999999997, "end": 252.16, "text": " and into petabytes,", "tokens": [51560, 293, 666, 3817, 24538, 11, 51626], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 92, "seek": 22692, "start": 252.16, "end": 255.16, "text": " the overhead of coordinating the CPU servers", "tokens": [51626, 264, 19922, 295, 37824, 264, 13199, 15909, 51776], "temperature": 0.0, "avg_logprob": -0.10684576755812188, "compression_ratio": 1.7207547169811321, "no_speech_prob": 0.00015596294542774558}, {"id": 93, "seek": 25516, "start": 255.2, "end": 257.28, "text": " is becoming the greatest bottleneck.", "tokens": [50366, 307, 5617, 264, 6636, 44641, 547, 13, 50470], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 94, "seek": 25516, "start": 257.28, "end": 260.24, "text": " And we're starting to see the limits now.", "tokens": [50470, 400, 321, 434, 2891, 281, 536, 264, 10406, 586, 13, 50618], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 95, "seek": 25516, "start": 260.24, "end": 263.52, "text": " What if instead of working on processors", "tokens": [50618, 708, 498, 2602, 295, 1364, 322, 27751, 50782], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 96, "seek": 25516, "start": 263.52, "end": 266.12, "text": " that has tens of megabytes of working set,", "tokens": [50782, 300, 575, 10688, 295, 10816, 24538, 295, 1364, 992, 11, 50912], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 97, "seek": 25516, "start": 266.12, "end": 267.88, "text": " let's move towards a processor", "tokens": [50912, 718, 311, 1286, 3030, 257, 15321, 51000], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 98, "seek": 25516, "start": 267.88, "end": 271.12, "text": " that has tens of gigabytes of working set.", "tokens": [51000, 300, 575, 10688, 295, 42741, 295, 1364, 992, 13, 51162], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 99, "seek": 25516, "start": 271.12, "end": 275.15999999999997, "text": " And if we could use multiple GPUs to create large memories,", "tokens": [51162, 400, 498, 321, 727, 764, 3866, 18407, 82, 281, 1884, 2416, 8495, 11, 51364], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 100, "seek": 25516, "start": 275.15999999999997, "end": 279.8, "text": " then it is now possible for us to imagine scaling beyond that.", "tokens": [51364, 550, 309, 307, 586, 1944, 337, 505, 281, 3811, 21589, 4399, 300, 13, 51596], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 101, "seek": 25516, "start": 279.8, "end": 282.24, "text": " We started working on GPU acceleration", "tokens": [51596, 492, 1409, 1364, 322, 18407, 17162, 51718], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 102, "seek": 25516, "start": 282.24, "end": 284.88, "text": " of the data processing stack several years ago.", "tokens": [51718, 295, 264, 1412, 9007, 8630, 2940, 924, 2057, 13, 51850], "temperature": 0.0, "avg_logprob": -0.0789109638759068, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0003101066395174712}, {"id": 103, "seek": 28488, "start": 284.92, "end": 287.44, "text": " And it's a giant body of work.", "tokens": [50366, 400, 309, 311, 257, 7410, 1772, 295, 589, 13, 50492], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 104, "seek": 28488, "start": 287.44, "end": 289.8, "text": " Ladies and gentlemen, today we're announcing", "tokens": [50492, 17084, 293, 11669, 11, 965, 321, 434, 28706, 50610], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 105, "seek": 28488, "start": 289.8, "end": 294.12, "text": " that Spark 3.0, the next generation of Spark,", "tokens": [50610, 300, 23424, 805, 13, 15, 11, 264, 958, 5125, 295, 23424, 11, 50826], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 106, "seek": 28488, "start": 294.12, "end": 296.32, "text": " will be NVIDIA accelerated.", "tokens": [50826, 486, 312, 426, 3958, 6914, 29763, 13, 50936], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 107, "seek": 28488, "start": 296.32, "end": 298.71999999999997, "text": " This is a collaboration between ourselves", "tokens": [50936, 639, 307, 257, 9363, 1296, 4175, 51056], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 108, "seek": 28488, "start": 298.71999999999997, "end": 301.6, "text": " and a large community of researchers and developers", "tokens": [51056, 293, 257, 2416, 1768, 295, 10309, 293, 8849, 51200], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 109, "seek": 28488, "start": 301.6, "end": 303.96, "text": " in open source all around the world.", "tokens": [51200, 294, 1269, 4009, 439, 926, 264, 1002, 13, 51318], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 110, "seek": 28488, "start": 303.96, "end": 306.84, "text": " And the results are really fantastic.", "tokens": [51318, 400, 264, 3542, 366, 534, 5456, 13, 51462], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 111, "seek": 28488, "start": 306.84, "end": 310.96, "text": " It's possible because of several groundbreaking achievements.", "tokens": [51462, 467, 311, 1944, 570, 295, 2940, 42491, 21420, 13, 51668], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 112, "seek": 28488, "start": 310.96, "end": 313.12, "text": " The first is the work that we did", "tokens": [51668, 440, 700, 307, 264, 589, 300, 321, 630, 51776], "temperature": 0.0, "avg_logprob": -0.08782702225905198, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003680412482935935}, {"id": 113, "seek": 31312, "start": 313.16, "end": 316.8, "text": " with Melanox NVIDIA called GPU Direct Storage.", "tokens": [50366, 365, 7375, 3730, 87, 426, 3958, 6914, 1219, 18407, 18308, 36308, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 114, "seek": 31312, "start": 316.8, "end": 320.76, "text": " In the acceleration of GPU Direct Storage and UCX,", "tokens": [50548, 682, 264, 17162, 295, 18407, 18308, 36308, 293, 14079, 55, 11, 50746], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 115, "seek": 31312, "start": 320.76, "end": 322.72, "text": " this framework that makes possible,", "tokens": [50746, 341, 8388, 300, 1669, 1944, 11, 50844], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 116, "seek": 31312, "start": 322.72, "end": 325.76, "text": " the management of IO and storage", "tokens": [50844, 264, 4592, 295, 39839, 293, 6725, 50996], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 117, "seek": 31312, "start": 325.76, "end": 329.0, "text": " and multi-node computing lightning fast.", "tokens": [50996, 293, 4825, 12, 77, 1429, 15866, 16589, 2370, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 118, "seek": 31312, "start": 329.0, "end": 332.44, "text": " Second is the scheduler of Spark.", "tokens": [51158, 5736, 307, 264, 12000, 260, 295, 23424, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 119, "seek": 31312, "start": 332.44, "end": 336.12, "text": " Scheduler of Spark now is aware of GPU and the GPU memory", "tokens": [51330, 44926, 26318, 295, 23424, 586, 307, 3650, 295, 18407, 293, 264, 18407, 4675, 51514], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 120, "seek": 31312, "start": 336.12, "end": 338.48, "text": " so that it could partition work to the GPUs", "tokens": [51514, 370, 300, 309, 727, 24808, 589, 281, 264, 18407, 82, 51632], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 121, "seek": 31312, "start": 338.48, "end": 340.68, "text": " and schedule it in a distributed way", "tokens": [51632, 293, 7567, 309, 294, 257, 12631, 636, 51742], "temperature": 0.0, "avg_logprob": -0.1214911984462364, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00040430272929370403}, {"id": 122, "seek": 34068, "start": 340.68, "end": 342.64, "text": " and manage the computation", "tokens": [50364, 293, 3067, 264, 24903, 50462], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 123, "seek": 34068, "start": 342.64, "end": 345.24, "text": " of this giant network of computers.", "tokens": [50462, 295, 341, 7410, 3209, 295, 10807, 13, 50592], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 124, "seek": 34068, "start": 345.24, "end": 348.08, "text": " Third, a library called Rapids", "tokens": [50592, 12548, 11, 257, 6405, 1219, 16184, 3742, 50734], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 125, "seek": 34068, "start": 348.08, "end": 350.32, "text": " that has the ability to ingest data,", "tokens": [50734, 300, 575, 264, 3485, 281, 3957, 377, 1412, 11, 50846], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 126, "seek": 34068, "start": 350.32, "end": 353.0, "text": " create data frames, do feature engineering,", "tokens": [50846, 1884, 1412, 12083, 11, 360, 4111, 7043, 11, 50980], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 127, "seek": 34068, "start": 353.0, "end": 357.08, "text": " do SQL queries and intercept the calls of Spark", "tokens": [50980, 360, 19200, 24109, 293, 24700, 264, 5498, 295, 23424, 51184], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 128, "seek": 34068, "start": 357.08, "end": 359.08, "text": " to be accelerated by our GPU.", "tokens": [51184, 281, 312, 29763, 538, 527, 18407, 13, 51284], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 129, "seek": 34068, "start": 359.08, "end": 363.6, "text": " And then lastly, Spark has a Spark SQL accelerator", "tokens": [51284, 400, 550, 16386, 11, 23424, 575, 257, 23424, 19200, 39889, 51510], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 130, "seek": 34068, "start": 363.6, "end": 364.96000000000004, "text": " they call Catalyst,", "tokens": [51510, 436, 818, 9565, 19530, 11, 51578], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 131, "seek": 34068, "start": 364.96000000000004, "end": 367.52, "text": " and that has been optimized for NVIDIA GPUs.", "tokens": [51578, 293, 300, 575, 668, 26941, 337, 426, 3958, 6914, 18407, 82, 13, 51706], "temperature": 0.0, "avg_logprob": -0.07421836853027344, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.0008424639818258584}, {"id": 132, "seek": 36752, "start": 367.52, "end": 372.32, "text": " These elements make possible Spark 3.0.", "tokens": [50364, 1981, 4959, 652, 1944, 23424, 805, 13, 15, 13, 50604], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 133, "seek": 36752, "start": 372.32, "end": 375.4, "text": " Let me show you the potential acceleration", "tokens": [50604, 961, 385, 855, 291, 264, 3995, 17162, 50758], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 134, "seek": 36752, "start": 375.4, "end": 377.44, "text": " that data scientists will be able to enjoy.", "tokens": [50758, 300, 1412, 7708, 486, 312, 1075, 281, 2103, 13, 50860], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 135, "seek": 36752, "start": 377.44, "end": 381.2, "text": " What you're looking at here is the benchmark of Rapids,", "tokens": [50860, 708, 291, 434, 1237, 412, 510, 307, 264, 18927, 295, 16184, 3742, 11, 51048], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 136, "seek": 36752, "start": 381.2, "end": 384.0, "text": " the foundation of Spark 3.0.", "tokens": [51048, 264, 7030, 295, 23424, 805, 13, 15, 13, 51188], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 137, "seek": 36752, "start": 384.0, "end": 387.03999999999996, "text": " This particular benchmark is TPCX BB,", "tokens": [51188, 639, 1729, 18927, 307, 314, 12986, 55, 19168, 11, 51340], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 138, "seek": 36752, "start": 387.03999999999996, "end": 388.44, "text": " big data benchmark.", "tokens": [51340, 955, 1412, 18927, 13, 51410], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 139, "seek": 36752, "start": 388.44, "end": 391.4, "text": " This particular data set is a scale factor of 10,000,", "tokens": [51410, 639, 1729, 1412, 992, 307, 257, 4373, 5952, 295, 1266, 11, 1360, 11, 51558], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 140, "seek": 36752, "start": 391.4, "end": 393.44, "text": " which basically is 10 terabytes.", "tokens": [51558, 597, 1936, 307, 1266, 1796, 24538, 13, 51660], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 141, "seek": 36752, "start": 393.44, "end": 396.32, "text": " The state of the art is a Dell server", "tokens": [51660, 440, 1785, 295, 264, 1523, 307, 257, 33319, 7154, 51804], "temperature": 0.0, "avg_logprob": -0.10179865577004173, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000552720797713846}, {"id": 142, "seek": 39632, "start": 396.32, "end": 397.88, "text": " costs about a million dollars", "tokens": [50364, 5497, 466, 257, 2459, 3808, 50442], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 143, "seek": 39632, "start": 397.88, "end": 401.68, "text": " and has the ability to deliver 17 gigabytes per second", "tokens": [50442, 293, 575, 264, 3485, 281, 4239, 3282, 42741, 680, 1150, 50632], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 144, "seek": 39632, "start": 401.68, "end": 404.64, "text": " of data movement through this benchmark.", "tokens": [50632, 295, 1412, 3963, 807, 341, 18927, 13, 50780], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 145, "seek": 39632, "start": 404.64, "end": 406.44, "text": " This particular benchmark is hard to beat.", "tokens": [50780, 639, 1729, 18927, 307, 1152, 281, 4224, 13, 50870], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 146, "seek": 39632, "start": 406.44, "end": 407.64, "text": " And the reason for that", "tokens": [50870, 400, 264, 1778, 337, 300, 50930], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 147, "seek": 39632, "start": 407.64, "end": 410.4, "text": " is because not only does it have to be fast,", "tokens": [50930, 307, 570, 406, 787, 775, 309, 362, 281, 312, 2370, 11, 51068], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 148, "seek": 39632, "start": 410.4, "end": 411.8, "text": " it also has to be cost effective.", "tokens": [51068, 309, 611, 575, 281, 312, 2063, 4942, 13, 51138], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 149, "seek": 39632, "start": 411.8, "end": 412.8, "text": " And the reason for that", "tokens": [51138, 400, 264, 1778, 337, 300, 51188], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 150, "seek": 39632, "start": 412.8, "end": 414.48, "text": " is because price performance matters.", "tokens": [51188, 307, 570, 3218, 3389, 7001, 13, 51272], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 151, "seek": 39632, "start": 414.48, "end": 417.15999999999997, "text": " The fastest in the world today is the Dell server", "tokens": [51272, 440, 14573, 294, 264, 1002, 965, 307, 264, 33319, 7154, 51406], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 152, "seek": 39632, "start": 417.15999999999997, "end": 421.03999999999996, "text": " at a million dollars and 17 gigabytes per second.", "tokens": [51406, 412, 257, 2459, 3808, 293, 3282, 42741, 680, 1150, 13, 51600], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 153, "seek": 39632, "start": 421.03999999999996, "end": 424.15999999999997, "text": " With Spark 3.0 sitting on top of Rapids,", "tokens": [51600, 2022, 23424, 805, 13, 15, 3798, 322, 1192, 295, 16184, 3742, 11, 51756], "temperature": 0.0, "avg_logprob": -0.07818839220496697, "compression_ratio": 1.83011583011583, "no_speech_prob": 0.0012063398025929928}, {"id": 154, "seek": 42416, "start": 424.16, "end": 429.0, "text": " Rapids benchmarked on TPCX BB delivers", "tokens": [50364, 16184, 3742, 18927, 292, 322, 314, 12986, 55, 19168, 24860, 50606], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 155, "seek": 42416, "start": 429.0, "end": 434.0, "text": " 163 gigabytes per second for two million dollars.", "tokens": [50606, 3165, 18, 42741, 680, 1150, 337, 732, 2459, 3808, 13, 50856], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 156, "seek": 42416, "start": 434.84000000000003, "end": 438.16, "text": " 10 times the performance and only twice the cost.", "tokens": [50898, 1266, 1413, 264, 3389, 293, 787, 6091, 264, 2063, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 157, "seek": 42416, "start": 438.16, "end": 440.64000000000004, "text": " If you were to look at this in another way,", "tokens": [51064, 759, 291, 645, 281, 574, 412, 341, 294, 1071, 636, 11, 51188], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 158, "seek": 42416, "start": 440.64000000000004, "end": 443.76000000000005, "text": " suppose you were to create a data center", "tokens": [51188, 7297, 291, 645, 281, 1884, 257, 1412, 3056, 51344], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 159, "seek": 42416, "start": 443.76000000000005, "end": 446.12, "text": " that is able to achieve the same performance", "tokens": [51344, 300, 307, 1075, 281, 4584, 264, 912, 3389, 51462], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 160, "seek": 42416, "start": 446.12, "end": 448.8, "text": " as two million dollars of DGX's", "tokens": [51462, 382, 732, 2459, 3808, 295, 413, 38, 55, 311, 51596], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 161, "seek": 42416, "start": 448.8, "end": 453.24, "text": " accelerating Rapids of this benchmark, TPCX BB,", "tokens": [51596, 34391, 16184, 3742, 295, 341, 18927, 11, 314, 12986, 55, 19168, 11, 51818], "temperature": 0.0, "avg_logprob": -0.12543041507403055, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00021315415506251156}, {"id": 162, "seek": 45324, "start": 453.24, "end": 458.32, "text": " it would cost you 10 million dollars and 150 kilowatts.", "tokens": [50364, 309, 576, 2063, 291, 1266, 2459, 3808, 293, 8451, 41295, 267, 1373, 13, 50618], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 163, "seek": 45324, "start": 458.32, "end": 461.88, "text": " Now, of course, data centers routinely process", "tokens": [50618, 823, 11, 295, 1164, 11, 1412, 10898, 40443, 1399, 50796], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 164, "seek": 45324, "start": 461.88, "end": 463.56, "text": " a lot more than terabytes.", "tokens": [50796, 257, 688, 544, 813, 1796, 24538, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 165, "seek": 45324, "start": 463.56, "end": 464.8, "text": " You're gonna need data centers", "tokens": [50880, 509, 434, 799, 643, 1412, 10898, 50942], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 166, "seek": 45324, "start": 464.8, "end": 467.16, "text": " way larger than this in the future,", "tokens": [50942, 636, 4833, 813, 341, 294, 264, 2027, 11, 51060], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 167, "seek": 45324, "start": 467.16, "end": 469.76, "text": " as data continues to grow exponentially.", "tokens": [51060, 382, 1412, 6515, 281, 1852, 37330, 13, 51190], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 168, "seek": 45324, "start": 469.76, "end": 473.88, "text": " And so the ability to accelerate Spark 3.0", "tokens": [51190, 400, 370, 264, 3485, 281, 21341, 23424, 805, 13, 15, 51396], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 169, "seek": 45324, "start": 473.88, "end": 477.52, "text": " with a library we call Rapids is utterly groundbreaking.", "tokens": [51396, 365, 257, 6405, 321, 818, 16184, 3742, 307, 30251, 42491, 13, 51578], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 170, "seek": 45324, "start": 477.52, "end": 481.24, "text": " The result is really spectacular.", "tokens": [51578, 440, 1874, 307, 534, 18149, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11416529625961461, "compression_ratio": 1.508130081300813, "no_speech_prob": 0.00012147328379796818}, {"id": 171, "seek": 48124, "start": 481.24, "end": 484.96000000000004, "text": " One fifth the cost, one third the power,", "tokens": [50364, 1485, 9266, 264, 2063, 11, 472, 2636, 264, 1347, 11, 50550], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 172, "seek": 48124, "start": 484.96000000000004, "end": 487.6, "text": " one fifth the cost and one third the power.", "tokens": [50550, 472, 9266, 264, 2063, 293, 472, 2636, 264, 1347, 13, 50682], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 173, "seek": 48124, "start": 488.68, "end": 492.92, "text": " The more you buy, the more you save.", "tokens": [50736, 440, 544, 291, 2256, 11, 264, 544, 291, 3155, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 174, "seek": 48124, "start": 492.92, "end": 494.44, "text": " In fact, Databricks,", "tokens": [50948, 682, 1186, 11, 40461, 81, 7663, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 175, "seek": 48124, "start": 494.44, "end": 497.28000000000003, "text": " which offers industrial strength Spark", "tokens": [51024, 597, 7736, 9987, 3800, 23424, 51166], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 176, "seek": 48124, "start": 497.28000000000003, "end": 500.44, "text": " at a large scale as a service is doing fantastic.", "tokens": [51166, 412, 257, 2416, 4373, 382, 257, 2643, 307, 884, 5456, 13, 51324], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 177, "seek": 48124, "start": 500.44, "end": 503.48, "text": " Every single day, a million virtual machines", "tokens": [51324, 2048, 2167, 786, 11, 257, 2459, 6374, 8379, 51476], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 178, "seek": 48124, "start": 503.48, "end": 506.56, "text": " are spun up to do data processing on Spark.", "tokens": [51476, 366, 37038, 493, 281, 360, 1412, 9007, 322, 23424, 13, 51630], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 179, "seek": 48124, "start": 506.56, "end": 509.04, "text": " And they're so delighted by the work and acceleration", "tokens": [51630, 400, 436, 434, 370, 18783, 538, 264, 589, 293, 17162, 51754], "temperature": 0.0, "avg_logprob": -0.1193493380404935, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.0006070118979550898}, {"id": 180, "seek": 50904, "start": 509.04, "end": 511.04, "text": " that they're gonna go accelerate Databricks", "tokens": [50364, 300, 436, 434, 799, 352, 21341, 40461, 81, 7663, 50464], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 181, "seek": 50904, "start": 511.04, "end": 513.28, "text": " with NVIDIA GPUs.", "tokens": [50464, 365, 426, 3958, 6914, 18407, 82, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 182, "seek": 50904, "start": 513.28, "end": 514.5600000000001, "text": " They're a fantastic partner.", "tokens": [50576, 814, 434, 257, 5456, 4975, 13, 50640], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 183, "seek": 50904, "start": 514.5600000000001, "end": 517.44, "text": " I'm so happy with all the work that we've done together.", "tokens": [50640, 286, 478, 370, 2055, 365, 439, 264, 589, 300, 321, 600, 1096, 1214, 13, 50784], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 184, "seek": 50904, "start": 517.44, "end": 519.36, "text": " Leading cloud service providers", "tokens": [50784, 1456, 8166, 4588, 2643, 11330, 50880], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 185, "seek": 50904, "start": 519.36, "end": 522.04, "text": " are offering Spark accelerated in their cloud", "tokens": [50880, 366, 8745, 23424, 29763, 294, 641, 4588, 51014], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 186, "seek": 50904, "start": 522.04, "end": 525.44, "text": " or they're accelerating their proprietary machine learning", "tokens": [51014, 420, 436, 434, 34391, 641, 38992, 3479, 2539, 51184], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 187, "seek": 50904, "start": 525.44, "end": 529.28, "text": " pipeline and data processing pipeline with NVIDIA Rapids.", "tokens": [51184, 15517, 293, 1412, 9007, 15517, 365, 426, 3958, 6914, 16184, 3742, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 188, "seek": 50904, "start": 529.28, "end": 533.4, "text": " Amazon SageMaker, Azure Machine Learning, Databricks,", "tokens": [51376, 6795, 33812, 44, 4003, 11, 11969, 22155, 15205, 11, 40461, 81, 7663, 11, 51582], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 189, "seek": 50904, "start": 533.4, "end": 536.44, "text": " Google Cloud AI, Google Cloud Dataproc", "tokens": [51582, 3329, 8061, 7318, 11, 3329, 8061, 9315, 569, 24174, 51734], "temperature": 0.0, "avg_logprob": -0.11703980820519584, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0005441318498924375}, {"id": 190, "seek": 53644, "start": 536.44, "end": 539.72, "text": " are now going to be accelerated with NVIDIA GPUs", "tokens": [50364, 366, 586, 516, 281, 312, 29763, 365, 426, 3958, 6914, 18407, 82, 50528], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 191, "seek": 53644, "start": 539.72, "end": 542.24, "text": " for data processing and data analytics.", "tokens": [50528, 337, 1412, 9007, 293, 1412, 15370, 13, 50654], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 192, "seek": 53644, "start": 542.24, "end": 545.0400000000001, "text": " Spark acceleration is a great achievement.", "tokens": [50654, 23424, 17162, 307, 257, 869, 15838, 13, 50794], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 193, "seek": 53644, "start": 545.0400000000001, "end": 546.6800000000001, "text": " I'm so proud of the team.", "tokens": [50794, 286, 478, 370, 4570, 295, 264, 1469, 13, 50876], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 194, "seek": 53644, "start": 546.6800000000001, "end": 549.6, "text": " It's such a large body of work and it's taken us years.", "tokens": [50876, 467, 311, 1270, 257, 2416, 1772, 295, 589, 293, 309, 311, 2726, 505, 924, 13, 51022], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 195, "seek": 53644, "start": 549.6, "end": 551.2800000000001, "text": " And it requires the collaboration", "tokens": [51022, 400, 309, 7029, 264, 9363, 51106], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 196, "seek": 53644, "start": 551.2800000000001, "end": 554.6800000000001, "text": " with hundreds of collaborators in open source", "tokens": [51106, 365, 6779, 295, 39789, 294, 1269, 4009, 51276], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 197, "seek": 53644, "start": 554.6800000000001, "end": 557.72, "text": " built on several layers of foundational", "tokens": [51276, 3094, 322, 2940, 7914, 295, 32195, 51428], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 198, "seek": 53644, "start": 557.72, "end": 559.6800000000001, "text": " and fundamental new technology.", "tokens": [51428, 293, 8088, 777, 2899, 13, 51526], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 199, "seek": 53644, "start": 559.6800000000001, "end": 562.8000000000001, "text": " And now the part that is growing exponentially difficult,", "tokens": [51526, 400, 586, 264, 644, 300, 307, 4194, 37330, 2252, 11, 51682], "temperature": 0.0, "avg_logprob": -0.08918148737687331, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.0005110999918542802}, {"id": 200, "seek": 56280, "start": 562.8, "end": 566.4399999999999, "text": " the first stage of machine learning is now accelerated.", "tokens": [50364, 264, 700, 3233, 295, 3479, 2539, 307, 586, 29763, 13, 50546], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 201, "seek": 56280, "start": 566.4399999999999, "end": 569.3599999999999, "text": " Data scientists all over the world are gonna be thrilled.", "tokens": [50546, 11888, 7708, 439, 670, 264, 1002, 366, 799, 312, 18744, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 202, "seek": 56280, "start": 569.3599999999999, "end": 574.1999999999999, "text": " Entire end-to-end from data processing to inference.", "tokens": [50692, 3951, 621, 917, 12, 1353, 12, 521, 490, 1412, 9007, 281, 38253, 13, 50934], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 203, "seek": 56280, "start": 574.1999999999999, "end": 576.04, "text": " We have three libraries.", "tokens": [50934, 492, 362, 1045, 15148, 13, 51026], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 204, "seek": 56280, "start": 576.04, "end": 579.28, "text": " Rapids for data processing, QDNN,", "tokens": [51026, 16184, 3742, 337, 1412, 9007, 11, 1249, 35, 45, 45, 11, 51188], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 205, "seek": 56280, "start": 579.28, "end": 581.8399999999999, "text": " our core library for deep learning AI,", "tokens": [51188, 527, 4965, 6405, 337, 2452, 2539, 7318, 11, 51316], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 206, "seek": 56280, "start": 581.8399999999999, "end": 585.3199999999999, "text": " and then third, TensorRT, our optimizing compiler", "tokens": [51316, 293, 550, 2636, 11, 34306, 49, 51, 11, 527, 40425, 31958, 51490], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 207, "seek": 56280, "start": 585.3199999999999, "end": 587.12, "text": " for these computational graphs", "tokens": [51490, 337, 613, 28270, 24877, 51580], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 208, "seek": 56280, "start": 587.12, "end": 591.0, "text": " that are created by the training frameworks.", "tokens": [51580, 300, 366, 2942, 538, 264, 3097, 29834, 13, 51774], "temperature": 0.0, "avg_logprob": -0.1471146026460251, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00021648309484589845}, {"id": 209, "seek": 59100, "start": 591.04, "end": 594.04, "text": " The end-to-end acceleration is now complete", "tokens": [50366, 440, 917, 12, 1353, 12, 521, 17162, 307, 586, 3566, 50516], "temperature": 0.0, "avg_logprob": -0.07447981834411621, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.0005789411952719092}, {"id": 210, "seek": 59100, "start": 594.04, "end": 597.36, "text": " and we will continue to advance it over time.", "tokens": [50516, 293, 321, 486, 2354, 281, 7295, 309, 670, 565, 13, 50682], "temperature": 0.0, "avg_logprob": -0.07447981834411621, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.0005789411952719092}, {"id": 211, "seek": 59100, "start": 597.36, "end": 601.72, "text": " But this represents the foundation of NVIDIA AI.", "tokens": [50682, 583, 341, 8855, 264, 7030, 295, 426, 3958, 6914, 7318, 13, 50900], "temperature": 0.0, "avg_logprob": -0.07447981834411621, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.0005789411952719092}, {"id": 212, "seek": 59100, "start": 601.72, "end": 603.0, "text": " I can't be more proud.", "tokens": [50900, 286, 393, 380, 312, 544, 4570, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07447981834411621, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.0005789411952719092}, {"id": 213, "seek": 59100, "start": 603.0, "end": 604.2, "text": " The team have done a great job.", "tokens": [50964, 440, 1469, 362, 1096, 257, 869, 1691, 13, 51024], "temperature": 0.0, "avg_logprob": -0.07447981834411621, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.0005789411952719092}, {"id": 214, "seek": 59100, "start": 604.2, "end": 605.04, "text": " Thank you.", "tokens": [51024, 1044, 291, 13, 51066], "temperature": 0.0, "avg_logprob": -0.07447981834411621, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.0005789411952719092}], "language": "en"}