1
00:00:00,000 --> 00:00:08,160
Let's talk about high-performance computing.

2
00:00:08,160 --> 00:00:11,640
It is clear now that acceleration is going to be the path forward

3
00:00:11,640 --> 00:00:14,520
for scientific and for high-performance computing.

4
00:00:14,520 --> 00:00:16,060
As I mentioned before,

5
00:00:16,060 --> 00:00:18,880
accelerated computing has four pillars.

6
00:00:18,880 --> 00:00:21,960
The first, of course, is the accelerator, the advanced GPUs.

7
00:00:21,960 --> 00:00:24,760
The second is the acceleration stack

8
00:00:24,760 --> 00:00:26,880
for each one of the computational domains.

9
00:00:27,000 --> 00:00:30,480
The third is systems, and last is developers,

10
00:00:30,480 --> 00:00:33,040
ultimately the applications that we accelerate.

11
00:00:33,040 --> 00:00:35,200
This year, we did really great work.

12
00:00:35,200 --> 00:00:38,440
We've accelerated now over 700 applications.

13
00:00:38,440 --> 00:00:41,440
And each and every single year, at every conference I show you,

14
00:00:41,440 --> 00:00:44,600
our golden suite, the suite that we track on a regular basis

15
00:00:44,600 --> 00:00:47,480
to make sure that we continue to engineer advances

16
00:00:47,480 --> 00:00:49,160
into libraries, into the stack,

17
00:00:49,160 --> 00:00:52,360
so that applications continue to improve in performance,

18
00:00:52,360 --> 00:00:55,120
even if we don't introduce new GPUs.

19
00:00:55,120 --> 00:00:57,880
And as you see, over the course of the last four years,

20
00:00:57,880 --> 00:01:00,880
we've increased application performance by 4x.

21
00:01:00,880 --> 00:01:03,400
And the green bar is something that I'm going to talk to you about.

22
00:01:03,400 --> 00:01:05,400
We're going to offer you a new platform

23
00:01:05,400 --> 00:01:08,480
and it's going to give high-performance computing a huge boost.

24
00:01:08,480 --> 00:01:12,120
We also brought CUDA to ARM computing systems.

25
00:01:12,120 --> 00:01:15,800
ARM server CPUs are seeing adoption all over the world.

26
00:01:15,800 --> 00:01:19,640
In hyperscale, there's Amazon, Fujitsu Super Computing,

27
00:01:19,640 --> 00:01:21,560
Cavium, part of Marvell now,

28
00:01:21,560 --> 00:01:24,320
a new exciting company called Ampere Computing,

29
00:01:24,320 --> 00:01:25,760
or in China, Huawei.

30
00:01:25,760 --> 00:01:29,200
All of the suite of NVIDIA tools and libraries

31
00:01:29,200 --> 00:01:31,320
are now available for ARM.

32
00:01:31,320 --> 00:01:35,120
We also introduced a brand new SDK for IO processing this year.

33
00:01:35,120 --> 00:01:36,480
We call Magnum IO.

34
00:01:36,480 --> 00:01:40,000
Magnum IO includes all kinds of great things from our DMA,

35
00:01:40,000 --> 00:01:44,160
of course, to the ability to communicate across multiple nodes

36
00:01:44,160 --> 00:01:47,240
and move data directly from storage to our GPUs.

37
00:01:47,240 --> 00:01:50,400
The suite of libraries is going to continue to advance.

38
00:01:50,400 --> 00:01:53,240
Magnum IO is going to be one of our most important libraries.

39
00:01:53,280 --> 00:01:55,640
Data processing and networking and storage

40
00:01:55,640 --> 00:01:57,320
is going to become more and more important

41
00:01:57,320 --> 00:01:59,600
to data center-scale computing over time.

42
00:01:59,600 --> 00:02:02,720
We introduced two new stacks this year.

43
00:02:02,720 --> 00:02:05,000
NVIDIA parabricks for genomics processing,

44
00:02:05,000 --> 00:02:08,920
the ability to do variant calling at very high performances,

45
00:02:08,920 --> 00:02:11,640
and a large body of work that we've been working on

46
00:02:11,640 --> 00:02:14,600
for several years called NVIDIA Rappers for Data Analytics.

47
00:02:15,840 --> 00:02:19,720
Machine learning has become one of HPC's grand challenges.

48
00:02:19,720 --> 00:02:21,080
The advances of machine learning

49
00:02:21,080 --> 00:02:23,000
and the popularity of this approach

50
00:02:23,000 --> 00:02:25,800
has caused companies, institutes, and data centers

51
00:02:25,800 --> 00:02:27,840
to collect a vast amount of data.

52
00:02:27,840 --> 00:02:29,760
The machine learning pipeline consists of three things.

53
00:02:29,760 --> 00:02:32,000
ETL, which creates the data frame,

54
00:02:32,000 --> 00:02:33,760
does all the feature engineering necessary

55
00:02:33,760 --> 00:02:36,480
for the machine learning algorithms to train on,

56
00:02:36,480 --> 00:02:39,440
which creates the model which is then put into operations

57
00:02:39,440 --> 00:02:40,600
we call inference.

58
00:02:40,600 --> 00:02:42,920
These three stages of the pipeline

59
00:02:42,920 --> 00:02:46,320
have unique and different computational challenges.

60
00:02:46,320 --> 00:02:49,800
The first stage of the machine learning pipeline data processing

61
00:02:49,840 --> 00:02:52,240
is becoming more complex than ever.

62
00:02:52,240 --> 00:02:54,600
In fact, most data scientists will tell you

63
00:02:54,600 --> 00:02:56,760
they spend the vast majority of their time

64
00:02:56,760 --> 00:02:59,240
doing feature engineering and data processing

65
00:02:59,240 --> 00:03:01,800
in the front stage of the machine learning pipeline.

66
00:03:01,800 --> 00:03:04,520
What used to be processing hundreds of megabytes

67
00:03:04,520 --> 00:03:06,640
to gigabytes to terabytes of data,

68
00:03:06,640 --> 00:03:09,680
companies are now routinely processing tens,

69
00:03:09,680 --> 00:03:11,960
if not hundreds of terabytes of data

70
00:03:11,960 --> 00:03:14,280
and moving to petabytes of data.

71
00:03:14,280 --> 00:03:17,760
It is the reason why Spark is so popular.

72
00:03:17,760 --> 00:03:21,120
Spark is an incredible computational platform.

73
00:03:21,120 --> 00:03:24,560
It turns an entire data center into a compute engine.

74
00:03:24,560 --> 00:03:28,600
It partitions a very large data set to be processed

75
00:03:28,600 --> 00:03:31,120
across a bunch of servers in the data center.

76
00:03:31,120 --> 00:03:33,920
It was the brainchild of Matt Zaharia

77
00:03:33,920 --> 00:03:35,280
at the Berkeley AMP Lab

78
00:03:35,280 --> 00:03:37,600
and spun out and became Apache Spark.

79
00:03:37,600 --> 00:03:40,680
It now has over a thousand companies contributing to it,

80
00:03:40,680 --> 00:03:42,400
nearly a million lines of code,

81
00:03:42,400 --> 00:03:44,760
16,000 plus companies around the world

82
00:03:44,760 --> 00:03:46,920
uses it for data processing today.

83
00:03:46,960 --> 00:03:48,560
Well, the amount of data that they're processing

84
00:03:48,560 --> 00:03:50,000
is growing exponentially.

85
00:03:50,000 --> 00:03:53,880
It is now reaching the limits of what Spark can do.

86
00:03:53,880 --> 00:03:55,000
Here's the reason why.

87
00:03:55,000 --> 00:03:57,920
The CPU set is being distributed across

88
00:03:57,920 --> 00:04:02,040
has a fundamental working set in the order of megabytes.

89
00:04:02,040 --> 00:04:04,440
A CPU naturally likes to work in its cache

90
00:04:04,440 --> 00:04:08,080
and its caches typically on the tens of megabytes.

91
00:04:08,080 --> 00:04:10,840
When the data set is now the hundreds of terabytes

92
00:04:10,840 --> 00:04:12,160
and into petabytes,

93
00:04:12,160 --> 00:04:15,160
the overhead of coordinating the CPU servers

94
00:04:15,200 --> 00:04:17,280
is becoming the greatest bottleneck.

95
00:04:17,280 --> 00:04:20,240
And we're starting to see the limits now.

96
00:04:20,240 --> 00:04:23,520
What if instead of working on processors

97
00:04:23,520 --> 00:04:26,120
that has tens of megabytes of working set,

98
00:04:26,120 --> 00:04:27,880
let's move towards a processor

99
00:04:27,880 --> 00:04:31,120
that has tens of gigabytes of working set.

100
00:04:31,120 --> 00:04:35,160
And if we could use multiple GPUs to create large memories,

101
00:04:35,160 --> 00:04:39,800
then it is now possible for us to imagine scaling beyond that.

102
00:04:39,800 --> 00:04:42,240
We started working on GPU acceleration

103
00:04:42,240 --> 00:04:44,880
of the data processing stack several years ago.

104
00:04:44,920 --> 00:04:47,440
And it's a giant body of work.

105
00:04:47,440 --> 00:04:49,800
Ladies and gentlemen, today we're announcing

106
00:04:49,800 --> 00:04:54,120
that Spark 3.0, the next generation of Spark,

107
00:04:54,120 --> 00:04:56,320
will be NVIDIA accelerated.

108
00:04:56,320 --> 00:04:58,720
This is a collaboration between ourselves

109
00:04:58,720 --> 00:05:01,600
and a large community of researchers and developers

110
00:05:01,600 --> 00:05:03,960
in open source all around the world.

111
00:05:03,960 --> 00:05:06,840
And the results are really fantastic.

112
00:05:06,840 --> 00:05:10,960
It's possible because of several groundbreaking achievements.

113
00:05:10,960 --> 00:05:13,120
The first is the work that we did

114
00:05:13,160 --> 00:05:16,800
with Melanox NVIDIA called GPU Direct Storage.

115
00:05:16,800 --> 00:05:20,760
In the acceleration of GPU Direct Storage and UCX,

116
00:05:20,760 --> 00:05:22,720
this framework that makes possible,

117
00:05:22,720 --> 00:05:25,760
the management of IO and storage

118
00:05:25,760 --> 00:05:29,000
and multi-node computing lightning fast.

119
00:05:29,000 --> 00:05:32,440
Second is the scheduler of Spark.

120
00:05:32,440 --> 00:05:36,120
Scheduler of Spark now is aware of GPU and the GPU memory

121
00:05:36,120 --> 00:05:38,480
so that it could partition work to the GPUs

122
00:05:38,480 --> 00:05:40,680
and schedule it in a distributed way

123
00:05:40,680 --> 00:05:42,640
and manage the computation

124
00:05:42,640 --> 00:05:45,240
of this giant network of computers.

125
00:05:45,240 --> 00:05:48,080
Third, a library called Rapids

126
00:05:48,080 --> 00:05:50,320
that has the ability to ingest data,

127
00:05:50,320 --> 00:05:53,000
create data frames, do feature engineering,

128
00:05:53,000 --> 00:05:57,080
do SQL queries and intercept the calls of Spark

129
00:05:57,080 --> 00:05:59,080
to be accelerated by our GPU.

130
00:05:59,080 --> 00:06:03,600
And then lastly, Spark has a Spark SQL accelerator

131
00:06:03,600 --> 00:06:04,960
they call Catalyst,

132
00:06:04,960 --> 00:06:07,520
and that has been optimized for NVIDIA GPUs.

133
00:06:07,520 --> 00:06:12,320
These elements make possible Spark 3.0.

134
00:06:12,320 --> 00:06:15,400
Let me show you the potential acceleration

135
00:06:15,400 --> 00:06:17,440
that data scientists will be able to enjoy.

136
00:06:17,440 --> 00:06:21,200
What you're looking at here is the benchmark of Rapids,

137
00:06:21,200 --> 00:06:24,000
the foundation of Spark 3.0.

138
00:06:24,000 --> 00:06:27,040
This particular benchmark is TPCX BB,

139
00:06:27,040 --> 00:06:28,440
big data benchmark.

140
00:06:28,440 --> 00:06:31,400
This particular data set is a scale factor of 10,000,

141
00:06:31,400 --> 00:06:33,440
which basically is 10 terabytes.

142
00:06:33,440 --> 00:06:36,320
The state of the art is a Dell server

143
00:06:36,320 --> 00:06:37,880
costs about a million dollars

144
00:06:37,880 --> 00:06:41,680
and has the ability to deliver 17 gigabytes per second

145
00:06:41,680 --> 00:06:44,640
of data movement through this benchmark.

146
00:06:44,640 --> 00:06:46,440
This particular benchmark is hard to beat.

147
00:06:46,440 --> 00:06:47,640
And the reason for that

148
00:06:47,640 --> 00:06:50,400
is because not only does it have to be fast,

149
00:06:50,400 --> 00:06:51,800
it also has to be cost effective.

150
00:06:51,800 --> 00:06:52,800
And the reason for that

151
00:06:52,800 --> 00:06:54,480
is because price performance matters.

152
00:06:54,480 --> 00:06:57,160
The fastest in the world today is the Dell server

153
00:06:57,160 --> 00:07:01,040
at a million dollars and 17 gigabytes per second.

154
00:07:01,040 --> 00:07:04,160
With Spark 3.0 sitting on top of Rapids,

155
00:07:04,160 --> 00:07:09,000
Rapids benchmarked on TPCX BB delivers

156
00:07:09,000 --> 00:07:14,000
163 gigabytes per second for two million dollars.

157
00:07:14,840 --> 00:07:18,160
10 times the performance and only twice the cost.

158
00:07:18,160 --> 00:07:20,640
If you were to look at this in another way,

159
00:07:20,640 --> 00:07:23,760
suppose you were to create a data center

160
00:07:23,760 --> 00:07:26,120
that is able to achieve the same performance

161
00:07:26,120 --> 00:07:28,800
as two million dollars of DGX's

162
00:07:28,800 --> 00:07:33,240
accelerating Rapids of this benchmark, TPCX BB,

163
00:07:33,240 --> 00:07:38,320
it would cost you 10 million dollars and 150 kilowatts.

164
00:07:38,320 --> 00:07:41,880
Now, of course, data centers routinely process

165
00:07:41,880 --> 00:07:43,560
a lot more than terabytes.

166
00:07:43,560 --> 00:07:44,800
You're gonna need data centers

167
00:07:44,800 --> 00:07:47,160
way larger than this in the future,

168
00:07:47,160 --> 00:07:49,760
as data continues to grow exponentially.

169
00:07:49,760 --> 00:07:53,880
And so the ability to accelerate Spark 3.0

170
00:07:53,880 --> 00:07:57,520
with a library we call Rapids is utterly groundbreaking.

171
00:07:57,520 --> 00:08:01,240
The result is really spectacular.

172
00:08:01,240 --> 00:08:04,960
One fifth the cost, one third the power,

173
00:08:04,960 --> 00:08:07,600
one fifth the cost and one third the power.

174
00:08:08,680 --> 00:08:12,920
The more you buy, the more you save.

175
00:08:12,920 --> 00:08:14,440
In fact, Databricks,

176
00:08:14,440 --> 00:08:17,280
which offers industrial strength Spark

177
00:08:17,280 --> 00:08:20,440
at a large scale as a service is doing fantastic.

178
00:08:20,440 --> 00:08:23,480
Every single day, a million virtual machines

179
00:08:23,480 --> 00:08:26,560
are spun up to do data processing on Spark.

180
00:08:26,560 --> 00:08:29,040
And they're so delighted by the work and acceleration

181
00:08:29,040 --> 00:08:31,040
that they're gonna go accelerate Databricks

182
00:08:31,040 --> 00:08:33,280
with NVIDIA GPUs.

183
00:08:33,280 --> 00:08:34,560
They're a fantastic partner.

184
00:08:34,560 --> 00:08:37,440
I'm so happy with all the work that we've done together.

185
00:08:37,440 --> 00:08:39,360
Leading cloud service providers

186
00:08:39,360 --> 00:08:42,040
are offering Spark accelerated in their cloud

187
00:08:42,040 --> 00:08:45,440
or they're accelerating their proprietary machine learning

188
00:08:45,440 --> 00:08:49,280
pipeline and data processing pipeline with NVIDIA Rapids.

189
00:08:49,280 --> 00:08:53,400
Amazon SageMaker, Azure Machine Learning, Databricks,

190
00:08:53,400 --> 00:08:56,440
Google Cloud AI, Google Cloud Dataproc

191
00:08:56,440 --> 00:08:59,720
are now going to be accelerated with NVIDIA GPUs

192
00:08:59,720 --> 00:09:02,240
for data processing and data analytics.

193
00:09:02,240 --> 00:09:05,040
Spark acceleration is a great achievement.

194
00:09:05,040 --> 00:09:06,680
I'm so proud of the team.

195
00:09:06,680 --> 00:09:09,600
It's such a large body of work and it's taken us years.

196
00:09:09,600 --> 00:09:11,280
And it requires the collaboration

197
00:09:11,280 --> 00:09:14,680
with hundreds of collaborators in open source

198
00:09:14,680 --> 00:09:17,720
built on several layers of foundational

199
00:09:17,720 --> 00:09:19,680
and fundamental new technology.

200
00:09:19,680 --> 00:09:22,800
And now the part that is growing exponentially difficult,

201
00:09:22,800 --> 00:09:26,440
the first stage of machine learning is now accelerated.

202
00:09:26,440 --> 00:09:29,360
Data scientists all over the world are gonna be thrilled.

203
00:09:29,360 --> 00:09:34,200
Entire end-to-end from data processing to inference.

204
00:09:34,200 --> 00:09:36,040
We have three libraries.

205
00:09:36,040 --> 00:09:39,280
Rapids for data processing, QDNN,

206
00:09:39,280 --> 00:09:41,840
our core library for deep learning AI,

207
00:09:41,840 --> 00:09:45,320
and then third, TensorRT, our optimizing compiler

208
00:09:45,320 --> 00:09:47,120
for these computational graphs

209
00:09:47,120 --> 00:09:51,000
that are created by the training frameworks.

210
00:09:51,040 --> 00:09:54,040
The end-to-end acceleration is now complete

211
00:09:54,040 --> 00:09:57,360
and we will continue to advance it over time.

212
00:09:57,360 --> 00:10:01,720
But this represents the foundation of NVIDIA AI.

213
00:10:01,720 --> 00:10:03,000
I can't be more proud.

214
00:10:03,000 --> 00:10:04,200
The team have done a great job.

215
00:10:04,200 --> 00:10:05,040
Thank you.

