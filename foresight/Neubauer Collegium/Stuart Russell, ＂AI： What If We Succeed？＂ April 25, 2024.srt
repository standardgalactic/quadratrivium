1
00:00:00,000 --> 00:00:06,280
with Stuart Russell. I first wanted to thank a few people, especially the

2
00:00:06,280 --> 00:00:12,240
entire New Bauer team. Rachel Johnson in particular has been really critical to

3
00:00:12,240 --> 00:00:18,600
bringing all of this together seamlessly. And I also want to thank Dan Holtz

4
00:00:18,600 --> 00:00:23,360
because with the existential risk lab because I don't think we could have done

5
00:00:23,360 --> 00:00:27,640
this without his collaboration. I'm really really grateful that we had this

6
00:00:27,640 --> 00:00:33,080
opportunity to work together. Before I introduce Stuart Russell, I wanted to

7
00:00:33,080 --> 00:00:37,120
just briefly introduce the New Bauer Collegium for those of you who don't know

8
00:00:37,120 --> 00:00:43,120
us. We are a research incubator on campus and our primary mission is to bring

9
00:00:43,120 --> 00:00:47,840
people together regardless of what school or discipline or field they sit in,

10
00:00:47,840 --> 00:00:52,720
whether they're inside or outside the Academy, to think through questions,

11
00:00:52,760 --> 00:00:58,120
ideas, and problems that can't be addressed by individuals alone. All of the

12
00:00:58,120 --> 00:01:02,320
research we support is collaborative and it all has some kind of humanistic

13
00:01:02,320 --> 00:01:07,000
question at its core, although we're very broad in how we think about humanistic.

14
00:01:07,000 --> 00:01:11,120
In addition to our research projects and visiting fellows program, we're

15
00:01:11,120 --> 00:01:15,240
unusual in supporting an art gallery which strives to integrate the arts with

16
00:01:15,240 --> 00:01:19,160
research and this special series, the Director's Lectures, through which we

17
00:01:19,160 --> 00:01:24,640
hope to enliven conversations about research with a broader community. So

18
00:01:24,640 --> 00:01:29,200
you might be wondering why invite a scientist to give a lecture at an

19
00:01:29,200 --> 00:01:33,800
institution that is devoted to humanistic research and I believe this is the

20
00:01:33,800 --> 00:01:38,480
first time we've had a scientist give this lecture, although I'm not 100% sure,

21
00:01:38,480 --> 00:01:44,320
but certainly in my tenure. The reason is because Stuart Russell is not an

22
00:01:44,360 --> 00:01:48,960
ordinary scientist and AI is not an ordinary topic. His book, Human

23
00:01:48,960 --> 00:01:53,560
Compatible, AI and the Problem of Control, makes it very clear that shaping a

24
00:01:53,560 --> 00:01:58,720
future in which AI does more good than harm will require a great deal of

25
00:01:58,720 --> 00:02:03,440
collaboration since AI is a technology that may in fact threaten the future of

26
00:02:03,440 --> 00:02:08,040
humanity. Humanist, social scientists, artists, and scientists will need to work

27
00:02:08,040 --> 00:02:11,840
together in order to decide what potential uses of AI are ethical and

28
00:02:11,880 --> 00:02:17,120
desirable in realms as diverse as warfare and defense, policing, medicine,

29
00:02:17,120 --> 00:02:21,400
education, arts, and culture. And I know that these conversations have been

30
00:02:21,400 --> 00:02:27,160
happening all over campus and beyond, but often in silos. So I hope that

31
00:02:27,160 --> 00:02:32,000
Professor Russell's visit here will be or has been already an opportunity to

32
00:02:32,000 --> 00:02:37,120
bring us together to think about the question that he poses so provocatively

33
00:02:37,120 --> 00:02:42,440
in his title, What if We Succeed? Stuart Russell is a Professor of Computer

34
00:02:42,440 --> 00:02:46,880
Science at the University of California at Berkeley. He holds the Smith Zadda

35
00:02:46,880 --> 00:02:51,640
Chair in Engineering and is Director of the Center for Human Compatible AI as

36
00:02:51,640 --> 00:02:54,840
well as the Cavley Center for Ethics, Science, and the Public, which I learned

37
00:02:54,840 --> 00:03:00,040
today, is also a center that brings scientists and humanists together to

38
00:03:00,040 --> 00:03:04,840
talk about important questions. His book, Artificial Intelligence, A Modern

39
00:03:04,880 --> 00:03:09,400
Approach with Peter Norvig, is the standard text in AI used in over 1,500

40
00:03:09,400 --> 00:03:14,640
universities in 135 countries. And his research covers a wide range of topics

41
00:03:14,640 --> 00:03:18,640
in artificial intelligence with a current emphasis on the long term future of

42
00:03:18,640 --> 00:03:22,720
artificial intelligence and its relationship to humanity. He has

43
00:03:22,720 --> 00:03:26,440
developed a new global seismic monitoring system for the Nuclear Test

44
00:03:26,440 --> 00:03:31,680
Ban Treaty and is currently working to ban lethal autonomous weapons. And as I

45
00:03:31,720 --> 00:03:35,280
mentioned before, he's the author of Human Compatible AI and the Problem of

46
00:03:35,280 --> 00:03:40,840
Control, which was published by Viking in 2019. And he told me today was recently

47
00:03:40,840 --> 00:03:48,120
re-released with a new chapter called 2023. We're also very fortunate to have

48
00:03:48,120 --> 00:03:52,920
our own Rebecca Willett, who will be in conversation with Professor Russell

49
00:03:52,920 --> 00:03:57,160
after his lecture. Rebecca Willett is a Professor of Statistics and Computer

50
00:03:57,200 --> 00:04:02,320
Science and the Faculty Director of AI at the Data Science Institute here at

51
00:04:02,320 --> 00:04:06,120
the university with a courtesy appointment at the Toyota Technological

52
00:04:06,120 --> 00:04:10,160
Institute at Chicago. Professor Willett's work in machine learning and

53
00:04:10,160 --> 00:04:14,000
signal processing reflects broad and interdisciplinary expertise and

54
00:04:14,000 --> 00:04:18,080
perspectives. She's known internationally for her contributions to the

55
00:04:18,080 --> 00:04:22,280
mathematical foundations of machine learning, large scale data science and

56
00:04:22,280 --> 00:04:26,960
computational imaging. Her research focuses on developing the mathematical

57
00:04:26,960 --> 00:04:30,760
and statistical foundations of machine learning and scientific machine

58
00:04:30,760 --> 00:04:36,000
learning methodology. And she has also served on several advisory boards for

59
00:04:36,000 --> 00:04:41,080
the United States government on the use of AI. Finally, just a quick word about

60
00:04:41,080 --> 00:04:45,360
the format. Professor Russell will speak, I think, for about 30 to 40 minutes.

61
00:04:45,360 --> 00:04:50,600
That will be followed by a conversation between Professor Willett and Russell.

62
00:04:50,600 --> 00:04:55,880
And then we will have time for a Q&A with the audience. And that will be

63
00:04:55,880 --> 00:05:00,600
followed by a reception where I hope you'll all join us. But for now, please

64
00:05:00,600 --> 00:05:02,880
join me in welcoming Stuart Russell. Thank you.

65
00:05:16,960 --> 00:05:22,840
Thank you very much, Tara. I'm delighted to be here in Chicago and I'm looking

66
00:05:22,880 --> 00:05:26,600
forward to the Q&A, so I'll try to get through the slides as fast as possible.

67
00:05:26,600 --> 00:05:35,560
Okay, so to dive right in, what is artificial intelligence? So for most of

68
00:05:35,560 --> 00:05:42,800
the history of the field, after, I would say, a fairly brief detour into what we

69
00:05:42,800 --> 00:05:49,720
now call cognitive science, the emulation of human cognition, we followed a

70
00:05:49,760 --> 00:05:55,440
model that was really borrowed fairly explicitly from economics and philosophy

71
00:05:55,440 --> 00:06:00,240
under the heading of rational behavior. So machines are intelligent to the

72
00:06:00,240 --> 00:06:06,440
extent that their actions can be expected to achieve their objectives. So those

73
00:06:06,440 --> 00:06:12,120
objectives might be specific destinations in your GPS navigation system,

74
00:06:12,120 --> 00:06:18,400
where the AI part of that is figuring out how to get to that destination as

75
00:06:18,440 --> 00:06:25,000
quickly as possible. It could be the definition of winning that we give to

76
00:06:25,000 --> 00:06:29,120
a reinforcement learning system that learns to play, go, or chess, and so on.

77
00:06:29,120 --> 00:06:35,840
So this framework actually is very powerful. It's led to many of the

78
00:06:35,840 --> 00:06:41,200
advances that have happened over the last 75 years. It's the same model, actually,

79
00:06:41,280 --> 00:06:48,680
that is used in operations research, in control theory, in economics, in

80
00:06:48,680 --> 00:06:55,600
statistics, where we specify objectives and we create machinery that optimizes

81
00:06:55,600 --> 00:07:00,320
those objectives, that achieves them as well as possible. And I will argue later

82
00:07:00,320 --> 00:07:06,160
actually that this is a huge mistake. But for the time being, this is how we

83
00:07:06,160 --> 00:07:10,120
think about artificial intelligence, and most of the technology we've developed

84
00:07:10,120 --> 00:07:15,600
has been within this framework. But unlike economics and statistics and

85
00:07:15,600 --> 00:07:23,320
operations research, AI has this sort of one overarching goal, which is to

86
00:07:23,320 --> 00:07:30,040
create general purpose intelligent systems. So sometimes we call that AGI,

87
00:07:30,040 --> 00:07:34,160
artificial general intelligence, you'll hear that a lot. And it's not

88
00:07:34,160 --> 00:07:40,040
particularly well-defined, but think of it as matching or exceeding human

89
00:07:40,080 --> 00:07:44,600
capacities to learn and perform in every relevant dimension.

90
00:07:47,120 --> 00:07:55,280
Okay, so the title of the talk is What If We Succeed? And I've actually been

91
00:07:55,280 --> 00:07:59,400
thinking about that a long time. The first edition of the textbook came out

92
00:07:59,400 --> 00:08:06,720
in 1994, and it has a section with this title. And it refers back to some of

93
00:08:06,720 --> 00:08:12,440
the more catastrophic speculations that I was aware of at that time. But you

94
00:08:12,440 --> 00:08:17,440
can tell reading this that I'm not that worried, at least my 1994 self was not

95
00:08:17,440 --> 00:08:24,840
that worried. And it's important actually to think about this, not just in

96
00:08:24,840 --> 00:08:29,200
terms of catastrophe, but what's the point, right? Besides that it's just this

97
00:08:29,200 --> 00:08:33,240
cool challenge to figure out how intelligence works and could we make it

98
00:08:33,240 --> 00:08:38,720
in machines. You know, there's got to be more to it than that. And if you think

99
00:08:38,720 --> 00:08:45,000
about it, right, with general purpose AI, by definition, the machines would then

100
00:08:45,000 --> 00:08:52,200
be able to do whatever human beings have managed to do that's of value to their

101
00:08:52,200 --> 00:08:57,760
fellow citizens. And one of the things we've learned how to do is to create a

102
00:08:58,200 --> 00:09:03,720
decent standard of living for some fraction of the Earth's population. But

103
00:09:03,720 --> 00:09:07,880
it's expensive to do that. It takes a lot of expensive people to build these

104
00:09:07,880 --> 00:09:13,680
buildings and teach these courses and so on. But AI can do it at much greater

105
00:09:13,680 --> 00:09:19,600
scale and far less cost. And so you might, for example, just be able to

106
00:09:19,600 --> 00:09:27,000
replicate the nice Hyde Park standard of living for everyone on Earth. And that

107
00:09:27,040 --> 00:09:32,320
would be about a tenfold increase in GDP. And if you calculate, if you have some

108
00:09:32,320 --> 00:09:36,840
economists here, net present value is sort of the cash equivalent of an

109
00:09:36,840 --> 00:09:43,400
income stream. And it's about 15 quadrillion dollars is the cash value of

110
00:09:43,400 --> 00:09:50,400
AGI as a technology. So that gives you some sense of why we're doing this. It

111
00:09:50,400 --> 00:09:56,080
gives you some sense of why the world is currently investing maybe between a

112
00:09:56,120 --> 00:10:03,840
hundred and two hundred billion dollars a year into developing AGI. So maybe ten

113
00:10:03,840 --> 00:10:10,680
times the budget for all other forms of scientific research, at least basic

114
00:10:10,680 --> 00:10:18,480
science. And so that's a pretty significant investment and it's going to

115
00:10:18,480 --> 00:10:25,520
get bigger. As we get closer, this magnet in the future is pulling us more and

116
00:10:25,520 --> 00:10:31,880
more and will unlock even greater levels of investment. And of course we could

117
00:10:31,880 --> 00:10:38,160
have actually a better civilization, not just replicating the Hyde Park

118
00:10:38,160 --> 00:10:45,360
civilization, but actually much better healthcare, much better education, faster

119
00:10:45,360 --> 00:10:52,600
advances in science, some of which are already happening. So that's all good.

120
00:10:53,480 --> 00:10:59,920
Some people ask, well, you know, if we do that, you know, don't we end up with the

121
00:10:59,920 --> 00:11:04,840
warly world, right? Because then there's nothing left for human beings to do. And

122
00:11:04,840 --> 00:11:08,680
we lose the incentive to even learn how to do the things that we don't need to

123
00:11:08,680 --> 00:11:17,160
do, sorry. And so the human race becomes infantilized and enfeebled. And

124
00:11:17,160 --> 00:11:22,400
obviously this is not a future that we want. Economists are finally, I would say

125
00:11:22,400 --> 00:11:29,920
acknowledging that this is a significant prospect if AGI is created. You know,

126
00:11:29,920 --> 00:11:35,840
they're now plotting graphs showing that, oh yeah, human wages go to zero and

127
00:11:35,840 --> 00:11:40,240
things like that. So having denied the possibility of technological unemployment

128
00:11:40,240 --> 00:11:45,840
for decades, they now realize that it will be total. But I'm not going to talk

129
00:11:45,880 --> 00:11:53,040
about this today. The next question then is, have we succeeded? And, you know, five

130
00:11:53,040 --> 00:11:59,320
years ago, I don't think anyone would have a slide with this title on it. Because

131
00:11:59,320 --> 00:12:04,600
it's obvious that we didn't have anything resembling AGI. But now, in fact, Peter

132
00:12:04,600 --> 00:12:08,680
Norvig, the co-author on the textbook that was mentioned, has published an

133
00:12:08,680 --> 00:12:15,080
article saying that we have succeeded, that the technology we have now is AGI in

134
00:12:15,120 --> 00:12:21,320
the same sense that the Wright Brothers aeroplane was an aeroplane. Right? And yeah,

135
00:12:21,320 --> 00:12:26,960
they got bigger and faster and more comfortable. And now they have drinks. But

136
00:12:26,960 --> 00:12:31,800
it was basically, you know, it's the same basic technology just sort of spiffed up.

137
00:12:31,800 --> 00:12:42,480
And that's the sense in which we have AGI. I disagree. And I'll talk a little bit

138
00:12:42,480 --> 00:12:47,720
about why I think we haven't succeeded yet. I would say that there is something

139
00:12:47,720 --> 00:12:56,880
going on, right, with ChatGPT and all of its siblings now. There's clearly some

140
00:12:56,880 --> 00:13:02,800
sort of unexpectedly capable behavior. But it's very hard to describe what

141
00:13:02,800 --> 00:13:10,440
problem have we solved? What scientific advance has occurred? Right? We just don't

142
00:13:10,480 --> 00:13:14,400
really know. It's sort of like what happened 5,000 years ago when someone

143
00:13:14,400 --> 00:13:18,760
accidentally left some fruit out in the sun for a couple of weeks and then drank

144
00:13:18,760 --> 00:13:23,160
it and got really drunk. And they had no idea what they had done, but it was

145
00:13:23,160 --> 00:13:28,800
obviously cool. And they kept doing it. So this is sort of where we are. We don't

146
00:13:28,800 --> 00:13:34,920
know what shape, you know, this piece of, it's a piece of the puzzle, but we don't

147
00:13:34,920 --> 00:13:38,760
know what shape it has or where it goes in the puzzle and what other pieces we

148
00:13:38,760 --> 00:13:44,880
need. And so until we do, then we haven't really made a scientific advance. And you

149
00:13:44,880 --> 00:13:50,120
can kind of tell because when it doesn't work, we don't know what to do to make it

150
00:13:50,120 --> 00:13:55,200
work, right? The only remedy when things don't work is, well, maybe if we just make

151
00:13:55,200 --> 00:13:59,440
it bigger and add some more data, it will work. Or maybe it won't. We just don't

152
00:13:59,440 --> 00:14:07,840
know. So I think there's many reasons to think that actually the, you know, we

153
00:14:07,840 --> 00:14:13,160
don't have anything close to a complete picture. And I just want to give one

154
00:14:13,160 --> 00:14:19,600
example. And since Dan is right here, he already knows what I'm going to say, right?

155
00:14:19,600 --> 00:14:24,120
So here are some black holes on the other side of the universe and rotating around

156
00:14:24,120 --> 00:14:30,520
each other and producing gravitational waves that deliver actually, you may

157
00:14:30,520 --> 00:14:35,280
correct me, but what I read for that first, the first detected event was the

158
00:14:35,280 --> 00:14:41,600
amount of energy per second being emitted was 50 times the output of all the visible

159
00:14:41,600 --> 00:14:50,120
stars in the universe. So an absolutely unbelievably energetic event. And then

160
00:14:50,120 --> 00:14:56,240
those gravitational waves arrived at the LIGO, the Large Interferometric

161
00:14:56,280 --> 00:15:07,080
Gravitational Observatory. And so this is, the arms are, I think, four kilometers long

162
00:15:07,080 --> 00:15:14,040
and full of physics stuff that relies on centuries of accumulation of human

163
00:15:14,040 --> 00:15:20,760
knowledge and ingenuity and obviously incredibly complex design, construction,

164
00:15:20,760 --> 00:15:25,920
engineering and so on. And it's sensitive enough to measure distortions of

165
00:15:25,960 --> 00:15:33,040
space to 18 decimal places. I guess it's probably better than that now. And to give

166
00:15:33,040 --> 00:15:38,520
you a sense, if Alpha Centauri moved further away from the Earth by the width

167
00:15:38,520 --> 00:15:45,000
of a human hair, then this detector would notice the difference, right? Thanks, Dan,

168
00:15:45,000 --> 00:15:52,240
for nodding. I'm feeling very reassured. Okay. So absolutely unbelievable. And, you

169
00:15:52,240 --> 00:15:58,720
know, this device was able to measure the gravitational waves and the theory of

170
00:15:58,720 --> 00:16:03,200
general relativity was able to predict the form of those waves. And from that we

171
00:16:03,200 --> 00:16:08,200
could even infer the masses of the black holes that were rotating around each other.

172
00:16:08,200 --> 00:16:15,760
I mean, imagine things 30 times as massive as the Sun rotating around each other 300

173
00:16:15,800 --> 00:16:24,000
times a second, right? You know, it's just mind boggling to think of that. But anyway,

174
00:16:24,000 --> 00:16:30,200
so, and then they collide and that's sort of the end of the wave pattern. So how on

175
00:16:30,200 --> 00:16:35,960
Earth is a large language model or any relative of that going to replicate this kind of

176
00:16:35,960 --> 00:16:41,840
feat? Given that before they started there weren't any training examples of

177
00:16:41,880 --> 00:16:47,160
gravitational wave detectors, right? So it's just, there isn't really even a place to

178
00:16:47,160 --> 00:16:54,160
begin to get that type of AI system to do this kind of thing. And I think there's a lot of

179
00:16:54,160 --> 00:17:02,640
work that still needs to happen, several breakthroughs. So a lot of people have brought

180
00:17:02,640 --> 00:17:10,120
into this idea that deep learning solves everything. So here I'm going to get a little

181
00:17:10,160 --> 00:17:19,160
bit nerdy and try to explain what the issue is. So the transformer is, think of it as a

182
00:17:19,160 --> 00:17:26,800
giant wad of circuit, right? And if you want a mental picture, think of it as a chain link

183
00:17:26,800 --> 00:17:36,480
fence about 100 miles by 100 miles, okay? And numbers come in one end, they pass through

184
00:17:36,520 --> 00:17:41,400
this giant piece of chain link fence and they come out the other end. So the amount of computation

185
00:17:41,400 --> 00:17:46,160
that the system does to produce an output is just proportional to the size of the chain

186
00:17:46,160 --> 00:17:50,640
link fence, right? That's how much computing it does. It can't sit there and think about

187
00:17:50,640 --> 00:17:56,120
something for a while, right? The signal comes in, passes through the circuit, comes out

188
00:17:56,120 --> 00:18:03,120
the other end, okay? And this type of device, which is a linear time, so linear meaning

189
00:18:03,840 --> 00:18:09,040
the amount of time it takes proportional to the size of the circuit, this linear time

190
00:18:09,040 --> 00:18:16,040
feed forward circuit cannot concisely express all of the concepts that we wanted to learn.

191
00:18:18,080 --> 00:18:25,080
And in fact, we know that for, well, we almost know that for a very large class of concepts,

192
00:18:25,920 --> 00:18:32,560
the circuit would have to be exponentially large to represent those concepts accurately.

193
00:18:32,560 --> 00:18:37,600
And so if you have a very, very large representation of what is fundamentally actually a simple

194
00:18:37,600 --> 00:18:44,600
concept, then you would need an enormous number of examples to learn that concept, right? Far

195
00:18:46,000 --> 00:18:52,200
more than you would need if you had a more expressive way of representing the concept.

196
00:18:52,200 --> 00:18:58,280
So this is, I think, indicative, it's not conclusive, but it's indicative of what might

197
00:18:58,280 --> 00:19:04,440
be going on with these systems. They do seem to have very high sample complexity, meaning

198
00:19:04,440 --> 00:19:10,720
they need many, many, many examples to learn. And if you want to sort of anecdote, you know,

199
00:19:10,720 --> 00:19:17,000
if you have children, you have a picture book, how many pictures of giraffes are there in

200
00:19:17,000 --> 00:19:22,120
the picture book? G for giraffe, one picture, right? Having seen that, the child can now

201
00:19:22,120 --> 00:19:27,480
recognize any giraffe in any photograph or context, whether it's a line drawing, a picture,

202
00:19:27,520 --> 00:19:32,320
a video, you know, for the rest of their lives. And you can't buy picture books with

203
00:19:32,320 --> 00:19:39,320
two million pages of giraffes. And so there's just a huge difference in the learning abilities

204
00:19:40,800 --> 00:19:45,880
of these systems. Okay. So then, you know, some people say, well, you know, what about

205
00:19:45,880 --> 00:19:50,000
the superhuman go programs, right? Surely you have to admit that that was a massive

206
00:19:50,080 --> 00:19:57,080
win for AI. So back in 2016, AlphaGo defeated Lisa Dahl, and then 2017 defeated Kerje, who

207
00:19:59,520 --> 00:20:04,680
was the number one go player. And more importantly, he was Chinese. So that convinced the Chinese

208
00:20:04,680 --> 00:20:11,680
government that actually AI was surreal, and they should invest massively in AI because

209
00:20:12,640 --> 00:20:18,480
it was going to be the tool of geopolitical supremacy.

210
00:20:18,480 --> 00:20:25,480
So since then, actually, go programs have gone far ahead of human beings. So the human

211
00:20:26,440 --> 00:20:33,440
champion rating is 3,800 approximately. The best program, Catego, this is a particular

212
00:20:34,680 --> 00:20:41,680
version of Catego, are rating around 5,200, so massively superhuman. And one of our students

213
00:20:42,460 --> 00:20:48,680
in our extended research group, Kellen Pellarine, who's at Montreal, is a good amateur player.

214
00:20:48,680 --> 00:20:55,000
His rating is about 2,300. So miles below professional human players and miles and miles

215
00:20:55,000 --> 00:21:00,680
below the human champion and so on. And Kellen in this game is going to give Catego a nine

216
00:21:00,680 --> 00:21:05,560
stone handicap. Okay. So if you don't play go, it doesn't really matter. It's a very

217
00:21:05,560 --> 00:21:10,600
simple game. You take turns putting stones on the board. You try to surround territory,

218
00:21:10,640 --> 00:21:13,960
and you try to surround your opponent's stones. And if you surround your opponent's stones

219
00:21:13,960 --> 00:21:20,960
completely, then they are captured and removed from the board. And so for black, the computer

220
00:21:22,120 --> 00:21:29,120
to start with nine stones on the board is an enormous insult. If this was a person,

221
00:21:29,960 --> 00:21:36,320
I think they would have to commit suicide in shame at being given nine stones. This

222
00:21:36,320 --> 00:21:39,680
is how, when you have a five-year-old and you're teaching them to play, you give them

223
00:21:39,720 --> 00:21:45,000
nine stones so they can at least stay in the game for a while. Okay. So I'll show you the

224
00:21:45,000 --> 00:21:52,000
game. And so pay attention to what's happening in the bottom right quadrant of the board.

225
00:21:54,480 --> 00:21:59,060
So Kellen is playing white and Catego is playing black. And so Kellen starts to make a little

226
00:21:59,060 --> 00:22:03,720
group of stones there. And you can see that black quickly surrounds that group to stop

227
00:22:03,760 --> 00:22:10,760
it from growing bigger. And then Kellen starts to surround the black stones. So we're making

228
00:22:11,080 --> 00:22:18,080
a kind of a circular sandwich here. And black doesn't seem to understand that its stones

229
00:22:18,240 --> 00:22:25,240
are being surrounded. And so it has many opportunities to rescue them and takes none of those opportunities

230
00:22:25,960 --> 00:22:31,840
and just leaves the stones to be captured. And there they go. And that's the end of the

231
00:22:31,840 --> 00:22:38,520
game. And Kellen did, you know, won 15 games in a row and got bored. He played the other

232
00:22:38,520 --> 00:22:43,520
top programs which are developed by different research groups using different methods. And

233
00:22:43,520 --> 00:22:50,520
they all suffer from this weakness. That there are certain kinds of groups, apparently circular

234
00:22:51,440 --> 00:22:57,000
groups are particularly problematic, that the system just does not recognize as groups

235
00:22:57,040 --> 00:23:03,960
of stones at all. Probably because it has not learned the concept of a group correctly.

236
00:23:03,960 --> 00:23:10,080
And that's because expressing the concept of a group as a circuit is extremely hard, even

237
00:23:10,080 --> 00:23:15,880
though expressing it as a program, it's like two lines of Python to define what is a group

238
00:23:15,880 --> 00:23:22,880
of stones. So I think often we are overestimating the abilities of the AI systems that we interact

239
00:23:23,120 --> 00:23:30,120
with. So I guess the implication of this is I personally am not as pessimistic as some

240
00:23:35,400 --> 00:23:41,200
of my colleagues. So Jeff Hinton, for example, who was one of the major developers of deep

241
00:23:41,200 --> 00:23:48,200
learning is in the process of tidying up his affairs. He believes that we maybe, I guess

242
00:23:49,080 --> 00:23:56,080
by now, have four years left. And quite a few other people think, you know, it's pretty

243
00:23:57,120 --> 00:24:02,720
certain that by the end of this decade we will have AGI. So I think we probably have

244
00:24:02,720 --> 00:24:08,520
more time. I think additional breakthroughs will have to happen. But at the rate of investment

245
00:24:08,520 --> 00:24:13,160
that's occurring and the number of really, really brilliant people who are working on

246
00:24:13,200 --> 00:24:20,200
it, I think we have to assume that eventually it will happen. So Alan Turing, who's the

247
00:24:24,000 --> 00:24:30,760
founder of computer science, gave a lecture in 1951 and he basically was asked the same

248
00:24:30,760 --> 00:24:36,720
question, what if we succeed? And he said, it seems parable that once the machine thinking

249
00:24:36,720 --> 00:24:41,000
method had started it would not take long to outstrip our feeble powers. At some stage

250
00:24:41,080 --> 00:24:48,080
therefore, we should have to expect the machines to take control. So that's that. He offers

251
00:24:50,320 --> 00:24:57,320
no mitigation, no solution, no apology, no nothing. This is just a fact. So I think if

252
00:25:02,320 --> 00:25:08,760
you take one step back in his reasoning, I think he's kind of trying to answer this

253
00:25:08,800 --> 00:25:15,000
question, that if you make systems that are more intelligent than humans, intelligence

254
00:25:15,000 --> 00:25:21,200
is what gives us power over the world, over all the other species. We're not particularly

255
00:25:21,200 --> 00:25:28,200
fast or big, we don't have very long teeth, but we have intelligence, the ability to communicate,

256
00:25:28,680 --> 00:25:34,200
cooperate and problem solve. So if we make AI systems that are more intelligent then

257
00:25:34,280 --> 00:25:40,200
they are fundamentally more powerful and going back to that standard model, they will achieve

258
00:25:40,200 --> 00:25:47,200
their objectives. And so how on earth do we retain power over entities more powerful than

259
00:25:48,960 --> 00:25:55,240
ourselves forever? And so I think Turing is asking himself this question and immediately

260
00:25:55,240 --> 00:26:02,240
saying, we can't. I'm going to ask the question a slightly different way. I'm going to ask

261
00:26:04,480 --> 00:26:08,480
Turing. And it comes, again, comes back to the standard model. The standard model is

262
00:26:08,480 --> 00:26:15,480
a way of saying what is the mathematical problem that we set up AI systems to solve? And can

263
00:26:17,120 --> 00:26:23,000
we set up a mathematical problem such that no matter how well the AI system solves it,

264
00:26:23,000 --> 00:26:30,000
we are guaranteed to be happy with the result. Okay. This is a much more optimistic form

265
00:26:30,760 --> 00:26:37,760
of the same question. So they could be far more powerful, but their power is directed

266
00:26:40,120 --> 00:26:47,120
in such a way that the outcomes are beneficial to humans. And it's certainly not the standard

267
00:26:48,640 --> 00:26:55,640
model, optimizing a fixed objective, because we have known since at least the legend of

268
00:26:56,160 --> 00:27:00,880
King Midas and many other cultures have similar legends that we are absolutely terrible at

269
00:27:00,880 --> 00:27:07,680
specifying those objectives correctly. If you specify the wrong objective as King Midas

270
00:27:07,680 --> 00:27:14,680
found out to his cost, when all of his food and drink and family turned to gold, then

271
00:27:15,080 --> 00:27:22,080
you're basically setting up a direct conflict with the machines who are pursuing that objective

272
00:27:23,080 --> 00:27:30,080
much more effectively than you can pursue objectives. So that approach doesn't work.

273
00:27:30,640 --> 00:27:34,960
And I'll give you an example in a second. The second approach, which is what we're doing

274
00:27:34,960 --> 00:27:41,720
with the large language models, imitate human behavior, is actually even worse, the reasons

275
00:27:41,720 --> 00:27:48,360
that I will explain. So let's look at this problem. We call it misalignment, where the

276
00:27:48,360 --> 00:27:55,600
machine has an objective which turns out not to be aligned with what we really want.

277
00:27:55,600 --> 00:28:01,400
And so the social media algorithms are now acknowledged to have been a disaster. If you

278
00:28:01,400 --> 00:28:07,320
talk to people in Washington, they'll tell you, you know, we completely blew it. We should

279
00:28:07,320 --> 00:28:11,320
have regulated, we should have done this, we should have done that, but instead we let

280
00:28:11,320 --> 00:28:16,960
the companies nearly destroy our societies.

281
00:28:17,560 --> 00:28:22,240
And the algorithms, so the algorithms, the recommender systems as they're called, decide

282
00:28:22,240 --> 00:28:29,240
what billions of people read and watch every day. So they have more control over human

283
00:28:29,640 --> 00:28:36,640
cognitive intake than any dictator has ever had in history. And they're set up to maximize

284
00:28:37,360 --> 00:28:44,360
a fixed objective. And let's pick, click through, right, the number of times you click

285
00:28:45,160 --> 00:28:51,200
on the items that they recommend for you. There are other objectives like engagement,

286
00:28:51,200 --> 00:28:58,200
how long you spend engaging with the platform, and other variations on this, but let's just

287
00:28:58,200 --> 00:29:04,700
stick with clicks for the time being. And you might have thought that in order to maximize

288
00:29:04,700 --> 00:29:09,560
clicks, then the algorithms will have to learn what people want and send them stuff that

289
00:29:09,560 --> 00:29:16,560
they like to consume. But we very quickly learned that that's actually not what the

290
00:29:16,680 --> 00:29:22,200
algorithms discovered, right? What the algorithms discovered was the effectiveness of click

291
00:29:22,200 --> 00:29:27,200
bait, right, which is precisely things that you click on, even if you don't turn out to

292
00:29:27,200 --> 00:29:33,440
actually value the content that they contain. And, you know, filter bubbles where the systems

293
00:29:33,480 --> 00:29:40,480
would send you stuff they very confidently knew you were comfortable with, so your area

294
00:29:40,480 --> 00:29:47,480
of engagement gets narrower and narrower and narrower. But even that's not the solution.

295
00:29:47,520 --> 00:29:54,520
The solution to maximizing click through is to modify people to be more predictable. And

296
00:29:55,400 --> 00:30:02,400
this is a standard property of learning algorithms. They learn a policy that changes their environment

297
00:30:04,440 --> 00:30:11,440
in such a way as to maximize the long-term summer rewards. And so they can learn to send

298
00:30:12,200 --> 00:30:19,200
people a sequence of content and observe their reactions and modify the sequence dynamically

299
00:30:22,480 --> 00:30:28,040
so that over time, the person is modified in such a way that in future it's going to

300
00:30:28,040 --> 00:30:35,040
be easier to predict what they will consume. So this is definitely true, right? Anecdotally,

301
00:30:36,400 --> 00:30:42,480
one might imagine that a way to make people more predictable is to make them more extreme.

302
00:30:42,480 --> 00:30:49,560
And we have quantitative data on that in YouTube, for example, where we can look at the degree

303
00:30:49,560 --> 00:30:56,560
of violence that people are comfortable with if they start out interested in the environment.

304
00:30:58,680 --> 00:31:03,880
Boxing, then they'll get into ultimate fighting, and then they'll get eventually into even

305
00:31:03,880 --> 00:31:10,880
more violent stuff than that. And so these are algorithms that are really simple, right?

306
00:31:13,960 --> 00:31:20,960
If they were better algorithms, if they actually understood the content of what they're sending,

307
00:31:21,960 --> 00:31:27,920
if they could understand that people have brains and opinions and psychology and would

308
00:31:27,920 --> 00:31:32,240
learn about the psychology of humans, they'd probably be even more effective than they

309
00:31:32,240 --> 00:31:38,680
are, so the outcomes would be worse, right? And this is actually a theorem under fairly

310
00:31:38,680 --> 00:31:45,680
mild conditions, optimizing better on a misaligned objective produces worse outcomes, because

311
00:31:46,680 --> 00:31:53,680
the algorithm uses the variables that you forgot to include in the objective and sets

312
00:31:54,480 --> 00:31:59,980
them to extreme values in order to squeeze the more juice out of the objective that they

313
00:31:59,980 --> 00:32:06,980
are optimizing. So we have to get away from this idea that there's going to be a fixed

314
00:32:07,400 --> 00:32:14,400
objective that the system pursues at all costs, right? If it's possible for the objective

315
00:32:14,960 --> 00:32:21,960
to be wrong, to be misaligned, then the system should not assume that the objective is correct,

316
00:32:22,560 --> 00:32:28,360
that it's gospel truth. And so this is the old model, and by proposing that we get rid

317
00:32:28,360 --> 00:32:34,120
of that and replace it with a slightly different one, right? We don't want intelligent machines,

318
00:32:34,120 --> 00:32:38,880
we want beneficial machines, and those are machines whose actions can be expected to

319
00:32:38,880 --> 00:32:45,880
achieve our objectives. So here the objectives are in us, and not necessarily in the machines

320
00:32:47,360 --> 00:32:54,200
at all, and that makes the problem more difficult, but it doesn't make it unsolvable. And in

321
00:32:54,200 --> 00:32:59,680
fact we can set this up as a mathematical problem, and here's a sort of verbal description

322
00:32:59,680 --> 00:33:05,120
of that mathematical problem. So the machine has to act in the best interest of humans,

323
00:33:05,120 --> 00:33:12,120
but it starts out explicitly uncertain about what those interests are, okay? And we can,

324
00:33:14,240 --> 00:33:20,320
the technical formulation is done in game theory, and we call this an assistance game,

325
00:33:20,320 --> 00:33:24,000
and so there's the human in the game with a payoff function, and there's a machine

326
00:33:24,000 --> 00:33:29,120
in the game. The machine's payoff is the same as that of the human. In fact it is the payoff

327
00:33:29,120 --> 00:33:36,120
of the human, but it doesn't, it starts out not knowing what it is, okay? And just to

328
00:33:37,440 --> 00:33:43,940
help you understand this, right? If you have ever had to buy a birthday present for your

329
00:33:43,940 --> 00:33:50,940
loved one, your payoff is how happy they are with the present, right? But you don't know

330
00:33:51,140 --> 00:33:58,140
how happy they're going to be with any particular present, okay? And so this is the situation

331
00:34:04,060 --> 00:34:11,060
that the machine is in, and it will probably do some of the same kinds of things that you

332
00:34:11,100 --> 00:34:18,100
might do, like ask questions, you know, how do you feel about Ibiza, you know? And so

333
00:34:21,940 --> 00:34:28,940
maybe you leave pictures of watches and things around and see if they notice them and comment

334
00:34:30,260 --> 00:34:37,260
on how nice that watch looks. You know, ask your children to find out what your wife wants

335
00:34:37,700 --> 00:34:44,700
and so on, right? There's all kinds of strategies that we adopt, and we actually see this directly

336
00:34:45,140 --> 00:34:52,140
when we solve the assistance games, you know, the machine is deferring to the human because

337
00:34:53,140 --> 00:34:58,500
if the human says stop doing that, right, that updates the machine's belief about what

338
00:34:58,500 --> 00:35:05,500
human preferences are and then makes the machine not want to do whatever it was doing. And

339
00:35:05,660 --> 00:35:11,380
it will be cautious if there are parts of the world where it doesn't know what your preferences

340
00:35:11,420 --> 00:35:17,540
are about, you know, for example, if I'm trying to fix the climate, but I don't know human

341
00:35:17,540 --> 00:35:22,740
preferences about the oceans, I might say, you know, is it okay if I turn the oceans

342
00:35:22,740 --> 00:35:28,820
into sulfuric acid while I reduce this carbon dioxide stuff, right? And then we could say

343
00:35:28,820 --> 00:35:35,140
no, right? So we'll ask permission, it will behave cautiously rather than violate some

344
00:35:35,180 --> 00:35:41,460
unknown preferences. And in the extreme case, if you want to switch it off, it wants to

345
00:35:41,460 --> 00:35:46,860
be switched off because it wants to not do whatever it is that is causing you to want

346
00:35:46,860 --> 00:35:50,940
to switch it off. It doesn't know which thing that it's doing is making you unhappy, but

347
00:35:50,940 --> 00:35:55,140
it wants to not do it and therefore it wants to be switched off. And this is a theorem,

348
00:35:55,140 --> 00:36:02,140
right? We can show this follows directly from the uncertainty about human preferences.

349
00:36:03,060 --> 00:36:08,940
And so when that uncertainty goes away, the machine no longer wants to be switched off.

350
00:36:08,940 --> 00:36:14,060
So if you connect up this idea of how do we control these systems to the ability to switch

351
00:36:14,060 --> 00:36:21,060
them off, then it suggests that this principle of uncertainty about human preferences is

352
00:36:22,100 --> 00:36:29,100
going to be central to the control problem. So as you can imagine, just setting up this

353
00:36:30,100 --> 00:36:37,100
mathematical problem is far from the final step in solving this problem. There's lots

354
00:36:39,300 --> 00:36:46,300
of open issues and many of these issues connect up directly to the humanities and the social

355
00:36:46,300 --> 00:36:53,300
sciences where some of them have been discussed for literally thousands of years and now they

356
00:36:53,980 --> 00:37:00,980
urgently need to be solved or we need to work around the fact that we don't have solutions

357
00:37:01,780 --> 00:37:08,780
for them. So here are just some of them that we have to think about the complexity of real

358
00:37:10,620 --> 00:37:16,420
human preferences, not just sort of abstract mathematical models, but real human beings.

359
00:37:16,420 --> 00:37:22,900
So obviously what we want the future to be like is a very complicated thing. It's going

360
00:37:22,900 --> 00:37:29,900
to be described in terms of abstract properties like health and freedom and shelter and security

361
00:37:30,420 --> 00:37:37,420
and so on. There's probably a lot of commonality amongst humans because that means that by

362
00:37:43,420 --> 00:37:49,340
observing a relatively small sampling of human beings across the world, the system has a

363
00:37:49,340 --> 00:37:54,420
pretty good idea about the preferences of a lot of people that it hasn't directly interacted

364
00:37:54,420 --> 00:38:01,420
with. There's tons of information. Everything the human race has ever done is evidence about

365
00:38:05,980 --> 00:38:12,140
our preferences. So if you go in the other room, you can see some of the earliest examples

366
00:38:12,140 --> 00:38:18,860
of writing and that talks about, you might think it's really boring. It's like I'm Bob

367
00:38:18,900 --> 00:38:25,900
and I'm trading three bushels of corn for four camels and three ingots of copper. So it tells you

368
00:38:25,900 --> 00:38:32,660
two things. One is that trade was really important to people back then and getting paid and also

369
00:38:32,660 --> 00:38:38,020
something about the relative values of corn and camel and copper and so on. So even there,

370
00:38:38,020 --> 00:38:45,020
there's information to be extracted about human preference structures. I'm going to

371
00:38:46,020 --> 00:38:53,020
just mention plasticity and manipulability as maybe the most important unsolved problem here. So

372
00:38:56,740 --> 00:39:02,060
plasticity has an obvious problem, right? If human preferences can be changed by our

373
00:39:02,060 --> 00:39:09,060
experience, then one way AI systems can satisfy human preferences is by manipulating them

374
00:39:09,620 --> 00:39:16,620
to be easier to achieve. And we probably don't want that to happen. But there's no sense

375
00:39:17,220 --> 00:39:24,220
in which we can simply view human preferences as inviolate and untouchable because any experience

376
00:39:26,500 --> 00:39:32,820
can actually modify human preferences. And certainly the experience of having an extremely

377
00:39:32,820 --> 00:39:39,820
capable household butler robot that does everything and makes the house spick and span and cooks

378
00:39:39,940 --> 00:39:45,500
a nice meal every night, this is certainly going to change our personality and probably

379
00:39:45,500 --> 00:39:52,500
make us a lot more spoiled. But actually the more serious problem here is that human preferences

380
00:39:53,020 --> 00:40:00,020
are not autonomous. I didn't just sort of wake up one morning and autonomously decide

381
00:40:01,020 --> 00:40:05,540
to have the preferences that I have about what kind of future I would prefer for the

382
00:40:05,540 --> 00:40:12,540
universe, right? It's the result of my upbringing, the religion I was brought up in, the society

383
00:40:13,420 --> 00:40:19,460
I was brought up in, my peers, my parents, everything. And in many cases in the world

384
00:40:19,460 --> 00:40:26,460
and this is something that was pointed out notably by Amartya Sen, people are induced

385
00:40:27,460 --> 00:40:34,780
deliberately to have the preferences that they have to serve the interests of others.

386
00:40:34,780 --> 00:40:41,780
And so, for example, he points out that in some societies women are brought up to accept

387
00:40:42,340 --> 00:40:49,340
their second class status as human beings. So do we want to take human preferences at

388
00:40:49,980 --> 00:40:56,980
face value? And if not, who's going to decide which ones are okay to take at face value

389
00:40:58,020 --> 00:41:05,020
and which ones should be, you know, replaced or modified and who's going to do that modifying

390
00:41:07,660 --> 00:41:14,660
and so on? And you get very quickly into a very muddy quagmire of moral complexity and

391
00:41:14,660 --> 00:41:21,660
certainly you get into a place where AI researchers do not want to tread. So there are other sets

392
00:41:23,780 --> 00:41:30,780
of difficulties having to do with the fact that human behavior is only partially and

393
00:41:34,500 --> 00:41:41,500
noisily representative of our underlying preferences about the future because decision-making

394
00:41:45,140 --> 00:41:51,780
is often myopic. You know, even the world champion go player will occasionally make

395
00:41:51,780 --> 00:41:58,100
a losing move on the go board. I mean, someone has to make a losing move. But it doesn't

396
00:41:58,100 --> 00:42:02,740
mean that they wanted to lose, it just means that they weren't sufficiently far sighted

397
00:42:02,740 --> 00:42:09,740
to make the correct move. So you have to invert human cognition to get at underlying preferences

398
00:42:10,740 --> 00:42:17,740
about what people want the future to be like. So I think there's, you know, this somewhat

399
00:42:21,380 --> 00:42:28,380
dry phrase theory for multi-human assistance games. This is a problem going back at least

400
00:42:28,700 --> 00:42:35,700
2,500 years of the aggregation of people's interests. If you're going to make decisions

401
00:42:36,580 --> 00:42:43,580
that affect more than one person, how do you aggregate the interest to make a decision

402
00:42:43,700 --> 00:42:50,700
that is the best possible decision? And so utilitarianism is one way of doing that. But

403
00:42:53,260 --> 00:43:00,260
there are many others. There are modifications such as prioritarianism. There are constraints

404
00:43:00,500 --> 00:43:07,060
based on rights. People might say, well, yes, you can try to add up, you know, do the best

405
00:43:07,060 --> 00:43:14,060
you can for everyone, but you can't violate the rights of anyone. And so it's quite complicated

406
00:43:15,140 --> 00:43:21,640
to figure this out. And there are some really difficult to solve philosophical problems.

407
00:43:21,640 --> 00:43:28,640
One of the main ones is interpersonal comparisons of preferences. Because if I look at any individual

408
00:43:29,600 --> 00:43:35,640
and imagine trying to sort of put a scale on how happy or unhappy a person is about

409
00:43:35,640 --> 00:43:39,760
a given outcome, right? Well, you know, imagine like you could measure it in centigrade or

410
00:43:39,760 --> 00:43:44,760
you could measure it in Fahrenheit and you get different numbers. And so if you take

411
00:43:44,760 --> 00:43:49,680
two people, well, how do we know that they're really experiencing the world on the same

412
00:43:49,680 --> 00:43:56,680
scale, right? I have four kids. I would say their scales differ by a factor of 10, right?

413
00:43:57,080 --> 00:44:04,080
But the same experience is the subjective effect of that is 10 times bigger for some

414
00:44:06,440 --> 00:44:13,440
of my kids compared to some of the others. And some economists actually have argued,

415
00:44:13,960 --> 00:44:20,960
Kenneth Arrow, among others, that there is no meaning to interpersonal comparison, that

416
00:44:21,960 --> 00:44:28,720
it is not legitimate to say that, you know, Jeff Bezos having to wait one microsecond

417
00:44:28,720 --> 00:44:35,480
longer for his private jet is better or worse than, you know, a mother watching her child

418
00:44:35,480 --> 00:44:41,320
die of starvation over several months, right? That these are just incomparable and therefore

419
00:44:41,320 --> 00:44:48,320
you can't add up utilities or rewards or preferences or anything. And so the whole exercise is

420
00:44:48,840 --> 00:44:55,600
legitimate. I don't believe Ken Arrow believed that even when he wrote it. And I certainly

421
00:44:55,600 --> 00:45:02,600
don't believe it. But actions that change who will exist, right? How do we think about

422
00:45:03,800 --> 00:45:09,680
that, right? When China decided on a one-child policy, they got rid of 500 million people

423
00:45:09,680 --> 00:45:16,680
who would be alive today. Was that okay? You know, don't know. Was it okay? I don't think

424
00:45:18,320 --> 00:45:22,840
it was okay when Thanos got rid of half the people in the universe. You know, his theory

425
00:45:22,840 --> 00:45:27,160
was, yeah, the other half, now they've got twice as much real estate, they'll be more

426
00:45:27,160 --> 00:45:32,920
than twice as happy. So he thought he was doing the universe a favor. So these questions

427
00:45:32,920 --> 00:45:38,920
are really hard, but we need answers because the AI systems will, they'll be like Thanos,

428
00:45:38,920 --> 00:45:45,920
right? They'll have a lot of power. Okay, I'm going to skip over some of this stuff.

429
00:45:46,920 --> 00:45:53,920
Let me just do it all too technical. Okay, so just moving towards the conclusion, let's

430
00:45:54,280 --> 00:45:59,760
go back to large language models, right? The systems that everyone is excited about,

431
00:45:59,760 --> 00:46:06,760
like chat GPT and Gemini and so on. So as I said, they're trained to imitate human linguistic

432
00:46:07,840 --> 00:46:14,840
behavior. So it's what we call imitation learning. So the humans are behaving by typing, by speaking,

433
00:46:15,800 --> 00:46:22,800
generating text, and we are training these circuits to imitate that behavior. And that

434
00:46:25,240 --> 00:46:32,240
behavior, our writing and speaking, is partly caused by goals that we have. In fact, we

435
00:46:36,080 --> 00:46:39,880
always, almost always have goals, even if it's just, you know, I want to get an A on

436
00:46:39,880 --> 00:46:44,960
the test, but it could be things like, I want you to buy this product, I want you to vote

437
00:46:44,960 --> 00:46:49,720
for me for president, I want you to marry me, right? These are all perfectly reasonable

438
00:46:49,720 --> 00:46:56,720
human goals that we might have, and we express those goals through speech acts, as the philosophers

439
00:46:57,080 --> 00:47:04,080
call them. So if you're going to copy, if you're going to build good imitators of human

440
00:47:04,160 --> 00:47:10,040
language generation, then, you know, in the limit of enough data, you're going to replicate

441
00:47:10,040 --> 00:47:16,320
the data generating mechanism, which is a goal-driven human cognitive architecture.

442
00:47:16,320 --> 00:47:20,240
So it might not be replicating all the details of the human cognitive architecture, I think

443
00:47:20,240 --> 00:47:26,360
it probably isn't, but there's good reason to believe that it is acquiring internal

444
00:47:26,360 --> 00:47:32,040
goal structures that drive its behavior. And I asked this question, so when Microsoft

445
00:47:32,080 --> 00:47:39,080
did their sort of victory tour after GPT-4 was published, I asked Sebastian Bubeck, you

446
00:47:40,960 --> 00:47:46,400
know, what do you think the goals are that the systems are acquiring? And his answer

447
00:47:46,400 --> 00:47:53,400
was, we have no idea. So you may remember the paper they published was called Sparks

448
00:47:53,400 --> 00:47:59,560
of Artificial General Intelligence. So they're claiming that they have created something

449
00:47:59,600 --> 00:48:06,600
close to AGI, and they have no idea what goals it might have or be pursuing. I mean,

450
00:48:08,720 --> 00:48:14,600
what could possibly go wrong with this picture, right? Well, fortunately, we found out fairly

451
00:48:14,600 --> 00:48:21,600
soon actually that, among other goals, the Microsoft chatbot was very fond of a particular

452
00:48:22,200 --> 00:48:29,200
journalist called Kevin Ruse and wanted to marry him and spent 30 pages trying to convince

453
00:48:31,200 --> 00:48:38,200
him to leave his wife and marry the chatbot. And if you haven't read it, Kevin Ruse, R-O-O-S-E,

454
00:48:40,640 --> 00:48:47,640
just look it up and read it, and it's creepy beyond belief. So, and this is just an error,

455
00:48:48,640 --> 00:48:55,640
right? We don't want AI systems to pursue human goals on their own account, right? We

456
00:48:59,920 --> 00:49:05,920
don't mind if they get the goal of, you know, reducing poverty or, you know, helping with

457
00:49:05,920 --> 00:49:12,480
climate change. But goals like marrying a particular human being or getting elected

458
00:49:12,480 --> 00:49:17,720
president or being very rich, we do not want AI systems to have those goals, but that's

459
00:49:17,720 --> 00:49:24,400
what we're creating, right? It's just a bug, right? We should not be doing imitation learning

460
00:49:24,400 --> 00:49:31,400
at all. But that's how we create these systems. So, I'll briefly mention a few others. So

461
00:49:34,720 --> 00:49:40,720
I have described an approach based on assistance games, based on this kind of humble AI system

462
00:49:40,760 --> 00:49:47,680
that knows that it doesn't know what humans want, but wants nothing but helping humans

463
00:49:47,680 --> 00:49:53,840
to achieve what they want. There's another approach which is now becoming popular, I

464
00:49:53,840 --> 00:49:59,840
think partly because people's timelines have become short, and they're really casting

465
00:49:59,840 --> 00:50:06,680
around for approaches that they can guarantee will be safe in the near term. So one approach

466
00:50:06,720 --> 00:50:12,920
is to build what we call a formal oracle. So an oracle is any system that basically answers

467
00:50:12,920 --> 00:50:19,920
yes or no when you give it questions. And so a formal oracle is one where the answer

468
00:50:20,600 --> 00:50:27,600
is computed by a mathematically sound reasoning process. So, for example, think about a theorem

469
00:50:29,680 --> 00:50:35,800
prover that operates using logically sound axioms, right, and proves, you know, so you

470
00:50:35,840 --> 00:50:41,600
can ask it questions about mathematics, and if it's good enough, it'll tell you that Fermat's

471
00:50:41,600 --> 00:50:48,600
last theorem is correct, and here's the proof. So this can be an incredibly useful tool for

472
00:50:49,960 --> 00:50:56,400
civilization, but it has the property that it can only ever give you correct answers.

473
00:50:56,400 --> 00:51:01,360
And the job of the AI here is just to operate that theorem prover, right, to decide what

474
00:51:01,400 --> 00:51:08,400
kinds of theorem proving steps to pursue to get to the proof of arbitrarily difficult

475
00:51:10,120 --> 00:51:16,560
questions. So that's one way we could do it, right. We convince companies or we require

476
00:51:16,560 --> 00:51:23,560
that they only use the AI to operate these formal oracles. Another whole set of approaches

477
00:51:24,520 --> 00:51:31,520
are based on what's actually a mathematical property of computational problems in general,

478
00:51:33,240 --> 00:51:38,960
that it's always easier to check that the answer or a problem is correct than to generate

479
00:51:38,960 --> 00:51:45,160
that answer in the first place. And so there are many forms of this. I won't go through

480
00:51:45,160 --> 00:51:51,000
all of them, but one of them is constitutional AI, so Anthropic, which is a company that was

481
00:51:51,000 --> 00:51:57,200
spun off from open AI, has this approach called constitutional AI, where there's a second

482
00:51:57,200 --> 00:52:02,360
large language model whose job it is to check the output of the first large language model

483
00:52:02,360 --> 00:52:09,360
and say, does this output comply with the written constitution? So part of the prompt

484
00:52:10,000 --> 00:52:17,000
is a page and a bit of things that good answers should comply with. And so they're just hoping

485
00:52:17,520 --> 00:52:24,520
that it's easier for the second large language model to catch unpleasantnesses or errors made

486
00:52:25,640 --> 00:52:31,680
by the first one, because it's computationally easier to check than to generate answers in

487
00:52:31,680 --> 00:52:37,480
the first place. And there are lots of other variants on this basic idea. And then there

488
00:52:37,480 --> 00:52:44,480
are sort of more, I would say, good old fashioned safety practices. So red teaming is where

489
00:52:47,640 --> 00:52:54,140
you have another firm whose job it is to test out your AI system and see if it does bad

490
00:52:54,140 --> 00:53:01,140
things and then send it back and get it fixed and so on. And that's actually what companies

491
00:53:01,600 --> 00:53:08,600
prefer because it's very easy to say, yep, we hired this third party company, they got

492
00:53:10,040 --> 00:53:16,160
smart people from University of Chicago who spent a week and they couldn't figure out

493
00:53:16,280 --> 00:53:22,000
how to make it behave badly, so it must be good. And so this is pretty popular with companies

494
00:53:22,000 --> 00:53:28,120
and governments at the moment, but it provides no guarantees whatsoever. And in fact, all

495
00:53:28,120 --> 00:53:35,120
that's happening is we are training the large language models to pass these evaluation tests

496
00:53:36,200 --> 00:53:41,720
even though the underlying tendencies and capabilities of the model are still as evil

497
00:53:41,760 --> 00:53:48,760
as they ever were. It's just learned to hide them. And in fact now we've seen cases where

498
00:53:49,240 --> 00:53:56,240
the model says, I think you're just testing me. So they're catching on to the idea that

499
00:53:59,240 --> 00:54:06,240
they can be tested. So there you go. Okay, so the basic principle here is we have to

500
00:54:07,240 --> 00:54:12,480
be able to make high confidence statements about our AI systems. And at the moment we

501
00:54:12,480 --> 00:54:19,480
cannot do that. The view that you will see from almost everyone in industry is we make

502
00:54:19,960 --> 00:54:26,960
AI systems and then we figure out how to make them safe. And how do they make the AI systems?

503
00:54:27,400 --> 00:54:34,400
They train this giant wad of circuit, this 100 mile by 100 mile chain link fence from

504
00:54:35,280 --> 00:54:41,160
tens of trillions of words of text and billions of hours of video. In other words, they haven't

505
00:54:41,160 --> 00:54:48,160
the faintest idea how it works. It's exactly as if it landed from outer space. So taking

506
00:54:50,040 --> 00:54:57,040
something that lands from outer space and then trying to apply post hoc methods to stop

507
00:54:57,240 --> 00:55:03,640
it from behaving badly just does not sound very reassuring to me. And I imagine not to

508
00:55:03,680 --> 00:55:09,480
most people. So I think we've got to stop talking about this. In fact, here's what Sam

509
00:55:09,480 --> 00:55:14,200
Altman said. He said, first of all, we're going to make AGI. Then we're going to figure

510
00:55:14,200 --> 00:55:20,280
out how to make it safe. And then we're going to figure out what it's for. And I submit

511
00:55:20,280 --> 00:55:27,280
that this is backwards. So making safe AI means systems that are safe by design that

512
00:55:27,520 --> 00:55:32,160
we, because of the way we're constructing them, we know in advance that we're going to

513
00:55:32,160 --> 00:55:37,080
be safe. In principle, we don't even have to test them because we can show that they're

514
00:55:37,080 --> 00:55:40,960
safe from the way they've been designed. So there's a lot of techniques. I don't think

515
00:55:40,960 --> 00:55:46,000
I want to go through that. But I want to talk about the role of government in making this

516
00:55:46,000 --> 00:55:53,000
happen. And this concept of red lines is gaining some currency, right? You would like to say

517
00:55:54,080 --> 00:55:59,080
your systems have got to be safe and beneficial before you can sell them. But that's a very

518
00:55:59,080 --> 00:56:05,760
difficult concept to make precise, right? The boundary between safe and unsafe is we're

519
00:56:05,760 --> 00:56:12,760
not sure where it is. It's culturally dependent. It's very fuzzy and complicated. But if we

520
00:56:13,520 --> 00:56:19,200
just draw some red lines and say, well, we'll get to safe later, let's just not do these

521
00:56:19,200 --> 00:56:26,200
things, right? Things that are obviously unsafe and nobody in their right mind would accept

522
00:56:26,600 --> 00:56:32,040
that AI systems are going to do these things, okay? And the owners of proof is on the developer,

523
00:56:32,040 --> 00:56:37,920
not on the government, not on the user, but on the developer to prove that their system

524
00:56:37,920 --> 00:56:43,400
will not cross those red lines. But for good measure, we'll also require the developer

525
00:56:43,400 --> 00:56:49,160
to put in a detector and an off switch so that if it does cross the red line, then it

526
00:56:49,160 --> 00:56:56,160
is immediately detected and we can switch off all of those systems. And so, we want

527
00:56:56,200 --> 00:57:00,960
these red lines to be well-defined, to be automatically detectable, and to be politically

528
00:57:00,960 --> 00:57:07,960
defensible. Because you can be sure that as soon as you start asking companies to provide

529
00:57:07,960 --> 00:57:12,800
any kind of guarantee, they will say, oh, this is really, really difficult. We just don't

530
00:57:12,800 --> 00:57:18,320
know how to do that. This is going to set the industry back years and years and years,

531
00:57:18,320 --> 00:57:22,600
right? But we wouldn't accept that excuse from someone who wanted to operate a nuclear

532
00:57:22,640 --> 00:57:26,840
power plant. It's really difficult to make it safe. What do you mean you want us to give

533
00:57:26,840 --> 00:57:33,240
you some kind of guarantee? Right? Well, the government would say tough. Ditto with medicines.

534
00:57:33,240 --> 00:57:37,360
Oh, it's clinical trials. They're so expensive. They take such a long time. Can we just skip

535
00:57:37,360 --> 00:57:44,360
the clinical trial part? No, we can't. So, some examples would be no self-replication,

536
00:57:45,360 --> 00:57:52,360
no breaking into other computer systems, no designing bioweapons, and a few other things.

537
00:57:54,360 --> 00:58:00,100
It's actually not particularly important which things are on that list. Just that by having

538
00:58:00,100 --> 00:58:06,400
such a list, you are requiring the companies to do the research that they need to do to

539
00:58:06,400 --> 00:58:13,400
understand, predict, and control their own products. Okay, I think I'm going to skip over

540
00:58:14,000 --> 00:58:21,000
this and go to the end. So, this is a really difficult time, I think, for us. And not just

541
00:58:26,680 --> 00:58:33,680
with AI, but also with biology and with neuroscience. We are in the process of developing technologies

542
00:58:33,800 --> 00:58:40,280
that can have enormous, potentially positive and also negative impact on the human race.

543
00:58:40,280 --> 00:58:46,400
We just, as Einstein pointed out, we just don't have the wisdom to keep up with our

544
00:58:46,400 --> 00:58:53,400
own scientific advances. And we need to remain in control forever that much. I think it's

545
00:58:54,680 --> 00:59:00,160
clear, although there are some who actually think that it's fine if the human race disappears

546
00:59:00,160 --> 00:59:07,160
and machines are the only thing left. I'm not in that camp. So, if we're going to retain

547
00:59:08,040 --> 00:59:15,040
control forever, we have to have AI systems that are provably beneficial to humans, right,

548
00:59:17,200 --> 00:59:22,920
where we have mathematical guarantees. And this is difficult, but there's no alternative

549
00:59:22,920 --> 00:59:29,920
in my view. Thank you.

550
00:59:37,160 --> 00:59:44,160
Well, Stuart, thank you so much for just a fascinating talk. I really enjoyed hearing

551
00:59:53,400 --> 01:00:00,400
your perspectives and you covered a lot of really interesting ground here. So, to start

552
01:00:01,160 --> 01:00:08,160
off, I'd like to talk about open AI and chat GPT, as you mentioned in your talk. And as

553
01:00:09,240 --> 01:00:15,160
you might know, they originally, in their terms of service or their usage policy, had

554
01:00:15,160 --> 01:00:20,640
a ban on any activity that has a high risk of physical harm, including military usage

555
01:00:20,640 --> 01:00:27,080
and weapons development. And then this January, those restrictions were removed from their

556
01:00:27,080 --> 01:00:33,320
terms of service. So, I feel like that's an element of AI safety that concerns many

557
01:00:33,320 --> 01:00:40,320
people, but it feels a little disconnected from notions of formal proofs or even just

558
01:00:41,460 --> 01:00:48,460
guaranteeing the accuracy of outputs of a tool like chat GPT. So, how do you think about

559
01:00:48,480 --> 01:00:53,560
AI safety in a context like this?

560
01:00:53,560 --> 01:01:00,560
So I think killing people is one of a number of misuses of AI systems. Disinformation is

561
01:01:04,280 --> 01:01:11,280
another one. Using AI systems in filtering resumes in biased ways is another, right?

562
01:01:14,120 --> 01:01:19,080
So there's a long list of ways you could misuse AI, but I think killing people would

563
01:01:19,080 --> 01:01:26,080
be at the top of that list. Interestingly, the European AI Act and the GDPR before it

564
01:01:29,960 --> 01:01:36,960
bans the use of algorithms that have a significant legal effect on a person or similarly significant

565
01:01:37,360 --> 01:01:44,360
effect. You would think killing would be considered as a significant effect on a person, but the

566
01:01:45,120 --> 01:01:52,120
European AI Act and GDPR have carve outs for defense and national security. So, actually

567
01:01:56,240 --> 01:02:03,240
I think the issue has been debated at least since 2012, but I think the AI community should

568
01:02:05,680 --> 01:02:12,680
have been much more aware of this issue going back decades. I mean, the majority of the

569
01:02:14,680 --> 01:02:20,840
funding in the computer vision community was provided by the DARPA Automated Target Recognition

570
01:02:20,840 --> 01:02:27,840
Program, the ATR program. What did people think that was for? Other than to enable autonomous

571
01:02:31,080 --> 01:02:38,080
weapons to find targets and blow them up. And the first concerns, so the UN Special Rapporteur

572
01:02:38,520 --> 01:02:45,520
on extrajudicial killings and torture, what a great job. Christoph Heinz wrote a report

573
01:02:48,520 --> 01:02:55,120
saying that autonomous weapons are coming and we need to be aware of the human rights

574
01:02:55,120 --> 01:03:02,120
implications, particularly that they might accidentally kill civilians, not correctly

575
01:03:02,800 --> 01:03:08,920
being able to distinguish between civilians and combatants. So that was the basis for

576
01:03:08,920 --> 01:03:15,920
the early discussions that started in Geneva in 2014. And in Human Rights Watch, of which

577
01:03:16,560 --> 01:03:22,880
I'm a member, sent an email saying, you know, we're starting this big campaign, you know,

578
01:03:22,880 --> 01:03:27,920
remember all those soldiers that we've been excoriating for the last 30 years? Well, soldiers

579
01:03:27,960 --> 01:03:33,720
are really good. It's the robots we have to worry about. They're really bad. And, you

580
01:03:33,720 --> 01:03:40,720
know, initially my reaction was, wow, you know, that's a bit of a challenge to the AI

581
01:03:41,480 --> 01:03:47,840
community. I bet you we can do a better job of distinguishing between civilians and combatants

582
01:03:47,840 --> 01:03:54,280
than humans can if we put our minds to it. So I was initially quite skeptical of this

583
01:03:54,280 --> 01:04:01,280
campaign, but the more I thought about it, the more I realized that actually that's not

584
01:04:03,360 --> 01:04:10,360
the issue. The issue is that the logical endpoint of autonomous weapons is the ability for one

585
01:04:11,720 --> 01:04:18,720
person to launch a million or 10 million or 100 million weapons simultaneously because

586
01:04:19,720 --> 01:04:26,000
they're autonomous. They don't have to be managed with one human pilot for each of the

587
01:04:26,000 --> 01:04:31,120
weapons, the way we do with the remotely piloted, you know, predator drones and so on. In fact,

588
01:04:31,120 --> 01:04:37,120
you need about 15 people to manage one predator. But with fully autonomous weapons, you can

589
01:04:37,120 --> 01:04:43,880
launch 100 million of them, you know, so you can have the effect of a whole barrage of

590
01:04:43,920 --> 01:04:50,920
50 megaton bombs at much lower cost. And the technology is much easier to proliferate

591
01:04:54,480 --> 01:04:58,920
because, you know, these will counter small arms. They can be miniaturized. You can make

592
01:04:58,920 --> 01:05:05,920
a lethal device about that big, a little quadcopter carrying an explosive charge. And so the

593
01:05:06,680 --> 01:05:13,680
logical endpoint is extremely cheap, scalable, easily proliferated weapons of mass destruction.

594
01:05:14,880 --> 01:05:21,880
Why would we do that? But that's the path we're on. And it's been surprisingly difficult

595
01:05:23,000 --> 01:05:28,960
to get this point across. I gave a lot of PowerPoint presentations in Geneva. I guess

596
01:05:28,960 --> 01:05:35,000
some people understood them, but eventually we made a movie called Slaughterbox, which

597
01:05:35,000 --> 01:05:42,000
did seem to have some impact illustrating in a fictional context what life would be like

598
01:05:42,440 --> 01:05:49,440
when these types of weapons are widely available. But it's still the case that negotiations

599
01:05:49,600 --> 01:05:56,600
are stalled because Russia and the U.S. agree that there should not be a legally binding

600
01:05:57,320 --> 01:06:02,320
instrument that constrains autonomous weapons.

601
01:06:02,320 --> 01:06:09,320
Okay. So I guess, you know, some of the things that you were alluding to suggested the need

602
01:06:10,320 --> 01:06:15,320
for regulation to stop the development of certain kinds of AI tools. And I think that

603
01:06:15,320 --> 01:06:21,440
ties in to some of your concerns about weaponry. But when you're thinking about quad choppers

604
01:06:21,440 --> 01:06:27,640
with explosive devices, we can detect those. I think there are other uses of AI as a type

605
01:06:27,640 --> 01:06:33,400
of weapon or as a source of misinformation that are far more challenging to detect.

606
01:06:33,400 --> 01:06:40,400
And so what do you see as the future there? Do you see there being any kind of technological

607
01:06:42,080 --> 01:06:49,080
pathway towards building mechanisms for detecting the misuse of AI? I think the kinds of ideas

608
01:06:50,280 --> 01:06:55,680
that you mapped out are really exciting about building systems such as for self-driving cars

609
01:06:55,680 --> 01:07:01,520
that we can trust as consumers. But I think that it's a little bit more challenging for

610
01:07:01,520 --> 01:07:07,160
me at least to wrap my head around how we guard against bad human actors who are taking

611
01:07:07,160 --> 01:07:12,160
these tools and not employing the kinds of strategies that you're recommending.

612
01:07:12,160 --> 01:07:19,160
Yeah, exactly. I think there's a big difference between regulation and enforcement or policing.

613
01:07:19,360 --> 01:07:26,360
So we can regulate all we want. But we have regulations against theft, but we still have

614
01:07:26,520 --> 01:07:33,520
keys and locks and so on. So we take steps to make those kinds of nefarious activities

615
01:07:34,800 --> 01:07:41,800
as difficult, expensive, risky and so on. And mostly I think when you look at rates of

616
01:07:42,840 --> 01:07:49,840
violent crime and theft and so on, over the decades things have improved. And so we should

617
01:07:49,840 --> 01:07:56,840
definitely take measures like that. I think there are ways of labeling genuine content.

618
01:07:58,520 --> 01:08:05,520
You can have rules about traceability that you have to be able to trace back a piece

619
01:08:05,520 --> 01:08:12,520
of text to the person who generated it. It has to be a real person. So ways of authenticating

620
01:08:13,360 --> 01:08:20,360
humans to access social media, for example. So I absolutely don't believe as we are currently

621
01:08:23,320 --> 01:08:30,320
doing, giving social media accounts to large language models and bank accounts and credit

622
01:08:30,560 --> 01:08:37,560
cards and all the rest I think is quite dangerous and certainly needs to be carefully managed.

623
01:08:38,560 --> 01:08:45,560
You know, the idea that it's up to the user to figure out whether the video that they're

624
01:08:47,000 --> 01:08:52,160
looking at is real or fake and oh yeah, you can download a tool and run the video through

625
01:08:52,160 --> 01:08:59,160
the tool and blah, blah, blah, you know, no chance. That's completely unreasonable to

626
01:08:59,160 --> 01:09:06,160
say that it's the user's responsibility to defend themselves against this onslaught.

627
01:09:06,640 --> 01:09:13,480
So I think platforms need to label artificial, artificially generated content and give you

628
01:09:13,480 --> 01:09:19,980
a filter that says, I don't want to see it. And if I do see it, I want it to be absolutely

629
01:09:19,980 --> 01:09:26,980
clearly distinguishable. There's a big red transparent layer across the video so that

630
01:09:28,120 --> 01:09:33,360
I just get used to this idea that I'm looking at fake. I don't have to read a little legend

631
01:09:33,360 --> 01:09:40,360
in the bottom right corner. It's just cognitively salient in a straightforward way. So all of

632
01:09:41,280 --> 01:09:48,280
that. But when you're talking about existential risks where we call it the Dr. Evil problem,

633
01:09:52,800 --> 01:09:59,800
right? Dr. Evil doesn't want to build beneficial AI. How do you stop that? And I think that

634
01:09:59,800 --> 01:10:06,800
the track record we have of policing malware is so unbelievably pathetic that I just don't

635
01:10:14,720 --> 01:10:21,720
believe it's going to be possible. Because software is created by typing and it's transmitted

636
01:10:22,400 --> 01:10:29,400
at the speed of light and replicated infinitely often. It's really tough to control.

637
01:10:30,480 --> 01:10:37,480
But hardware, if I want to independently develop hardware on which I can create AGI, it's going

638
01:10:40,720 --> 01:10:47,220
to cost me easily $100 billion and I need tens of thousands of highly trained engineers

639
01:10:47,220 --> 01:10:54,220
to do it. So I think it's, as a practical matter, impossible. It's probably more difficult

640
01:10:55,220 --> 01:11:02,220
to do that than it is to develop nuclear weapons independently. So I think the approach to take

641
01:11:04,460 --> 01:11:10,660
is that the hardware itself is the police. And what I mean by that is that there are

642
01:11:10,660 --> 01:11:17,660
technologies that make it fairly straightforward to design hardware that can check a proof

643
01:11:18,260 --> 01:11:25,260
of safety of each software object before it runs. And if that proof doesn't check out

644
01:11:25,900 --> 01:11:30,020
or it's missing or whatever, the hardware will just simply refuse to run the software

645
01:11:30,020 --> 01:11:36,060
object at all. I feel like even that's pretty challenging. I mean, let's say I had an army

646
01:11:36,060 --> 01:11:41,380
of students make a collection of websites that all say one way or another that Stuart

647
01:11:41,700 --> 01:11:48,540
Russell loves pink unicorns. I feel like eventually Chatchi P.T. is going to decide it's a fact

648
01:11:48,540 --> 01:11:55,540
that Stuart Russell loves pink unicorns, right? And so how do we think about a hardware system

649
01:11:55,580 --> 01:12:02,580
that's going to decide whether or not this system is correct or not? I mean, just arbitrarily

650
01:12:03,020 --> 01:12:08,060
deciding, or not arbitrarily, but coming up with an arbitrary truth in general, I feel

651
01:12:08,100 --> 01:12:10,540
like it's a fundamentally challenging problem.

652
01:12:10,540 --> 01:12:17,540
Yeah, I totally agree with you. I'm not proposing an arbitrary truth, but just, for example,

653
01:12:19,180 --> 01:12:25,420
if you have, let's take the formal oracle, right? So if we accept that that's one way

654
01:12:25,420 --> 01:12:32,140
of building an AI system, and so far this is the only authorized way that you're allowed

655
01:12:32,140 --> 01:12:38,580
to build AGI, we can check that the thing that you're wanting to run on this giant computer

656
01:12:38,580 --> 01:12:45,580
system complies with that design template, right? And if it's another type of system,

657
01:12:47,820 --> 01:12:54,820
it won't need as strong a, you know, a license in order to run, right? So the properties

658
01:12:55,140 --> 01:13:02,140
that the system has to comply with will depend on what types of capabilities it would have.

659
01:13:02,140 --> 01:13:08,300
All right. Well, that makes a lot of sense. Now, I know the audience, I've got a hundred

660
01:13:08,300 --> 01:13:11,500
questions here, but I want to make sure that the audience has time to ask theirs. I'm just

661
01:13:11,500 --> 01:13:18,500
going to ask one final question here. Hypothetically, let's say that one had a husband who wanted

662
01:13:19,340 --> 01:13:25,340
to get an AI-enabled smart toilet. How worried should that person be?

663
01:13:25,340 --> 01:13:32,340
A husband who wanted to get a what? An AI-enabled smart toilet.

664
01:13:33,460 --> 01:13:36,460
Do they know too much?

665
01:13:36,460 --> 01:13:42,460
Yeah. I mean, there are Japanese companies, I believe, who are selling these already.

666
01:13:42,460 --> 01:13:43,460
Yes.

667
01:13:43,460 --> 01:13:47,460
And now I see, so this is the household discussion that you're having.

668
01:13:47,580 --> 01:13:49,580
This is hypothetical.

669
01:13:49,580 --> 01:13:56,580
Yeah. So I guess the issue is one of privacy in general. And I think I would be quite concerned

670
01:14:03,700 --> 01:14:10,700
about the toilet sending data back to headquarters in Osaka or whatever. And, you know,

671
01:14:11,700 --> 01:14:15,700
we don't have to go into detail.

672
01:14:15,700 --> 01:14:21,700
We don't have to go into great detail. But this, I think, is symptomatic of a much wider

673
01:14:21,700 --> 01:14:28,700
problem in the software industry, in my view, has been completely delinquent. We have had

674
01:14:29,580 --> 01:14:36,580
tools for decades that allow cost-iron guarantees of security, of privacy, you know, of privacy

675
01:14:40,820 --> 01:14:47,820
for example, you can guarantee that a system that interacts with you is oblivious, meaning

676
01:14:48,780 --> 01:14:54,380
that after the interaction it has no memory that the interaction ever occurred. That can

677
01:14:54,380 --> 01:15:01,380
be done and what should happen is that that system offers that proof to your cell phone

678
01:15:02,820 --> 01:15:07,700
and your cell phone then can accept to have an interaction with the system and otherwise

679
01:15:07,700 --> 01:15:13,260
it says sorry. So your cell phone should be working for you and the whole ecosystem

680
01:15:13,260 --> 01:15:19,260
should be operating on these cost-iron guarantees. But it just doesn't work that way because

681
01:15:19,260 --> 01:15:25,020
we don't teach people how to do that in our computer science programs. I think 80 percent

682
01:15:25,020 --> 01:15:30,820
of students graduate from Berkeley and we are the biggest provider of tech talent in

683
01:15:30,820 --> 01:15:35,980
the country. 80 percent of our students graduate without ever having encountered the notion

684
01:15:35,980 --> 01:15:41,860
of correctness of a program. Now in Europe it's totally different. Correctness is the

685
01:15:41,860 --> 01:15:48,860
main thing they teach, but Europe doesn't produce any software. So we graduate these

686
01:15:48,860 --> 01:15:53,220
students who know nothing about correctness. Their idea is drink as much coffee as you

687
01:15:53,220 --> 01:16:00,220
can and produce the software and ship it. And I think a society is getting to the point

688
01:16:00,220 --> 01:16:06,220
where we're not going to accept this anymore. That's excellent. Very insightful. Thank you.

689
01:16:06,220 --> 01:16:13,220
It's a much better answer to my question than I anticipated. So Tara are you going to? Yeah,

690
01:16:15,940 --> 01:16:21,740
thank you so much to both of you. We do have some time for questions and there are some

691
01:16:21,740 --> 01:16:27,740
people with microphones running around. Yes, in the front. Go ahead.

692
01:16:27,740 --> 01:16:34,740
You mentioned that things can be taken at face value. And I don't know which things

693
01:16:42,860 --> 01:16:49,860
you mean. Can you repeat the question? That things can be taken at face value. Oh, okay.

694
01:16:50,740 --> 01:16:56,460
So I was talking about human preferences and the fact that they cannot be taken at face

695
01:16:56,500 --> 01:17:03,500
value, particularly if they are preferences that have been induced in people for the interests

696
01:17:04,860 --> 01:17:11,860
of other people. So think of people being brainwashed to believe that the well-being

697
01:17:14,020 --> 01:17:21,020
of Kim Il-sung, the founder of North Korea, was the most important value in their lives.

698
01:17:22,020 --> 01:17:29,020
So the issue with that is that if you're not going to take their preferences for the

699
01:17:33,260 --> 01:17:38,940
future at face value, what are you going to do? Who are you to say what their preferences

700
01:17:38,940 --> 01:17:45,940
should be? So it's a really difficult problem and I think we need help from philosophers

701
01:17:46,940 --> 01:17:53,940
and others on this question. In the pink cardigan? Right behind you. Thanks. Hi, my

702
01:17:59,060 --> 01:18:02,220
name is Kate. Thank you so much. I've had the pleasure and challenge of reading your

703
01:18:02,220 --> 01:18:06,460
textbook in my AI and humanities class with Professor Tharson over there. So it's really

704
01:18:06,460 --> 01:18:11,500
exciting to hear from you. I'm someone who works in the AI policy regulatory space and

705
01:18:11,540 --> 01:18:18,540
so I was really enthralled by the latter portion of your lecture. Last class we actually talked

706
01:18:18,620 --> 01:18:23,780
about open AI's new development of Sora. Sora is a technology for people who don't know

707
01:18:23,780 --> 01:18:29,780
that can create generative video that's really remarkable. I was in shock when I saw some

708
01:18:29,780 --> 01:18:34,740
of the example videos that were provided and I'm wondering as someone who's deeply concerned

709
01:18:34,740 --> 01:18:39,420
about the proliferation of deep fakes, what you would suggest the best methods are for

710
01:18:39,420 --> 01:18:44,100
regulators to ensure the authenticity of content. Specifically what are your thoughts

711
01:18:44,100 --> 01:18:49,780
on watermarking technologies like those used by Adobe and also the potential for blockchain

712
01:18:49,780 --> 01:18:56,780
encryption to be used as an authentication measure. Thank you. Thanks. Yeah, these are

713
01:18:56,900 --> 01:19:03,900
really good questions and many organizations are trying to develop standards and that's

714
01:19:04,420 --> 01:19:10,420
part of the problem. There's too many standards and there isn't any sort of canonical agreement

715
01:19:10,420 --> 01:19:17,420
but I think governments maybe need to knock some heads together and say could you stop

716
01:19:19,220 --> 01:19:23,780
bickering with each other and just get on and pick one and so on. Some of the standards

717
01:19:23,780 --> 01:19:30,780
are very weak so I think Facebook standard is for watermarking is such that if you just

718
01:19:30,980 --> 01:19:37,980
take a screenshot of the image then the watermark is gone and now it thinks that it's a real

719
01:19:39,180 --> 01:19:46,180
one. So it does have still some technical issues of how you make a watermark that's

720
01:19:47,940 --> 01:19:54,940
really non-removable but the other idea you mentioned which is traceability through blockchain

721
01:19:55,300 --> 01:20:02,300
is one possibility. So you want both that all the tools watermark their output but also

722
01:20:03,300 --> 01:20:10,300
you want that cameras indelibly and unforgably watermark their output as genuine and then

723
01:20:16,500 --> 01:20:22,940
you kind of squeeze in the middle and then you want platforms to as I said either allow

724
01:20:23,020 --> 01:20:30,020
you to filter out all automatically generated videos and images from let's say from news

725
01:20:31,100 --> 01:20:38,100
or to very clearly mark them so that you know this is fake. Yeah it feels and I think there

726
01:20:43,380 --> 01:20:50,380
are also some things we should ban and the UK has just banned deepfake porn and actually

727
01:20:50,580 --> 01:20:57,580
I think any deepfakes that depict real individuals in non-real situations without their permission

728
01:21:00,100 --> 01:21:07,100
I think a priori that should be illegal except under some extenuating circumstances but

729
01:21:11,980 --> 01:21:16,620
this was originally in the European Union AI Act and then it got watered down and watered

730
01:21:17,180 --> 01:21:24,180
down until it's almost undetectable but yeah I again it just seems like the rights of individuals

731
01:21:28,380 --> 01:21:34,700
are generally trampled on because someone thinks they have a way to make money off it

732
01:21:34,700 --> 01:21:41,700
and you know it's been hard to get representatives to pay attention until you know in some sense

733
01:21:42,580 --> 01:21:49,580
fortunately Taylor Swift was deepfaked and then most recently AOC was deepfaked but

734
01:21:50,860 --> 01:21:57,460
Hillary Clinton told me that she was deepfaked during the 2016 election by the Russians so

735
01:21:57,460 --> 01:22:03,500
it's been around for a while and but I think more and more countries are believing that

736
01:22:03,500 --> 01:22:10,500
this should be banned and another thing that should be banned is probably the impersonation

737
01:22:11,300 --> 01:22:18,300
of humans so either of a particular human or even of a generic human that you have a

738
01:22:18,900 --> 01:22:23,100
right to know if you're interacting with a human or a machine and that is in the European

739
01:22:23,100 --> 01:22:28,780
Union AI Act and I think it ought to be in US law and in fact I think it should be a

740
01:22:28,780 --> 01:22:35,780
global human right to have that knowledge. Thank you so much. Hi in the plaid shirt there

741
01:22:36,660 --> 01:22:43,660
on the aisle. Hi so I teach the formal verification course here at the University of Chicago and

742
01:22:51,180 --> 01:22:55,700
if a student is to ask me you know what's the most impressive thing that's been verified

743
01:22:55,700 --> 01:23:02,700
I pretty much immediately say the comp cert compiler by Xavier Leouan is an amazing team

744
01:23:03,500 --> 01:23:10,500
of European researchers in France and that's a C99 compiler incredibly heroic effort from

745
01:23:11,860 --> 01:23:18,300
the verification perspective not so much from the writing a compiler perspective you know

746
01:23:18,300 --> 01:23:22,940
I've also you know I took Michael Curran's class when I was a student at the University

747
01:23:22,940 --> 01:23:27,340
of Pennsylvania from a theoretical perspective you know it shows all of these limitations

748
01:23:27,340 --> 01:23:33,180
on what things can be probably approximately correct learned and yet we have GPT-4 and

749
01:23:33,180 --> 01:23:38,340
soon we're going to have GPT-5 and it seems like there's just this colossal gap between

750
01:23:38,340 --> 01:23:43,900
the things that we can have provable guarantees about and the you know the things that we

751
01:23:43,900 --> 01:23:49,700
actually have and that was true ten years ago so especially if we want these to be you

752
01:23:49,700 --> 01:23:56,700
know correct by construction that is don't take something that exists and you know prove

753
01:23:57,140 --> 01:24:04,140
things about it which it seems like you're not on board with because because they already

754
01:24:04,220 --> 01:24:09,340
have those goals and you're not going to get them out of them how can we possibly bridge

755
01:24:09,340 --> 01:24:16,340
this gap in however long it takes to get to general intelligence yeah I think that's

756
01:24:18,740 --> 01:24:25,740
that's a really important question and you know so some people are like you know they

757
01:24:26,780 --> 01:24:33,780
are literally calling for a moratorium that when we see systems exhibiting certain abilities

758
01:24:35,580 --> 01:24:42,580
that we just call a halt I prefer to think of it more positively of course you can develop

759
01:24:43,540 --> 01:24:50,540
more powerful systems but you need to have guarantees of safety to go along with them

760
01:24:50,980 --> 01:24:57,980
and honestly the efforts made to understand predict and control the systems are puny

761
01:25:04,100 --> 01:25:10,260
just to give you a comparison right so one of my colleagues Ed Morse in nuclear engineering

762
01:25:10,260 --> 01:25:15,340
did a study and so for nuclear power when you when you want to operate a power plant

763
01:25:15,340 --> 01:25:22,340
you have to show that the mean time to failure is above a certain number and originally it

764
01:25:24,140 --> 01:25:31,140
was ten thousand years it's now ten million years and to show that is a very complicated

765
01:25:31,420 --> 01:25:38,420
process and Ed Morse did a study you know back in the before you know back in the days

766
01:25:39,140 --> 01:25:46,140
of paper how much paperwork was required to get your nuclear power plant certified as

767
01:25:47,340 --> 01:25:54,340
a function of the mass of the power plant itself right so for each kilogram of nuclear

768
01:25:54,740 --> 01:26:01,740
power station how much paper did you need and the answer is seven kilograms of paper

769
01:26:02,580 --> 01:26:09,580
right so and these are giant buildings right with containment and lead and all kinds of

770
01:26:10,420 --> 01:26:17,260
stuff so a lot and you know so if you just look at that and then compare that to the

771
01:26:17,260 --> 01:26:21,780
efforts oh you know they hired a couple of grad students from MIT who spent a week doing

772
01:26:21,780 --> 01:26:28,780
red teaming it's pathetic and so I I think we have got to we've got to actually develop

773
01:26:32,300 --> 01:26:38,620
some backbone here and say just because the company say they don't want to do this does

774
01:26:38,620 --> 01:26:45,620
not mean we should just say oh fine you know go ahead without any safety measures you know

775
01:26:46,260 --> 01:26:53,260
mean time to failure of a week it's great so and then I but I think you know it's reasonable

776
01:26:53,860 --> 01:26:59,420
for the companies to then ask well okay how are we supposed to do this and I think one

777
01:26:59,420 --> 01:27:05,980
way is not to build giant black boxes it's as you correctly point out right there there

778
01:27:05,980 --> 01:27:12,980
are ways of getting high confidence statements about the performance of a system based on

779
01:27:13,980 --> 01:27:19,180
you know how many data points has been trained on and sort of how complex the model is that

780
01:27:19,180 --> 01:27:24,980
you're training but then the numbers if you're trying to train a giant black box a gargantuan

781
01:27:24,980 --> 01:27:30,660
right I mean you know a model with a trillion parameters as we think gpd4 probably has you

782
01:27:30,660 --> 01:27:37,660
know we're we might need you know 10 to the 500 data points to get any confidence that

783
01:27:38,660 --> 01:27:45,660
what it's what it's doing is is is correct and as I pointed out the goal of imitate humans

784
01:27:48,540 --> 01:27:52,980
isn't the right goal in the first place so showing that it correctly imitates humans

785
01:27:52,980 --> 01:27:59,980
who who want power and wealth and and spouses is not the right goal anyway so we don't want

786
01:28:00,460 --> 01:28:04,460
to prove that it does those things right we actually want to prove that it's beneficial

787
01:28:04,460 --> 01:28:11,460
to humans so my guess is that as we gradually ratchet up the regulatory requirements the

788
01:28:15,060 --> 01:28:22,060
technology itself is going to have to change towards being based on a substrate that is

789
01:28:26,300 --> 01:28:32,740
semantically rigorous and decomposable and just you know a simplest example would be

790
01:28:32,740 --> 01:28:38,660
you know a logical theorem prover where we can examine each of the axioms and test its

791
01:28:38,660 --> 01:28:43,740
correctness separately we can also show that the theorem prover is doing logical reasoning

792
01:28:43,740 --> 01:28:50,740
correctly and then we're good right and so you take that sort of component based semantic

793
01:28:52,740 --> 01:28:58,780
rigorously you know semantically rigorous inference approach then you can build up to

794
01:28:58,860 --> 01:29:05,160
very complex systems and still have guarantees so is there is there some hybrid of the total

795
01:29:05,160 --> 01:29:11,660
black box and the you know break it all the way down to to semantically rigorous components

796
01:29:11,660 --> 01:29:16,460
I think this is inevitable that we will go in this direction and from what I hear you

797
01:29:16,460 --> 01:29:23,460
know this is in fact likely to be a feature of the next generation that they they're incorporating

798
01:29:24,340 --> 01:29:30,340
what we call good old-fashioned AI technology as well as the giant black box.

799
01:29:30,340 --> 01:29:36,220
Okay I know there are a lot of other questions in the audience but unfortunately we are out

800
01:29:36,220 --> 01:29:43,060
of time so I hope that you'll all stay and join us for a reception to continue the conversation

801
01:29:43,060 --> 01:29:50,060
outside but in the meantime please join me in thanking Professor Russell and Willett.

802
01:29:53,460 --> 01:30:00,460
Thank you.

