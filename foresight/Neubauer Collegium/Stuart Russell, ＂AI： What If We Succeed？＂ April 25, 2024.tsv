start	end	text
0	6280	with Stuart Russell. I first wanted to thank a few people, especially the
6280	12240	entire New Bauer team. Rachel Johnson in particular has been really critical to
12240	18600	bringing all of this together seamlessly. And I also want to thank Dan Holtz
18600	23360	because with the existential risk lab because I don't think we could have done
23360	27640	this without his collaboration. I'm really really grateful that we had this
27640	33080	opportunity to work together. Before I introduce Stuart Russell, I wanted to
33080	37120	just briefly introduce the New Bauer Collegium for those of you who don't know
37120	43120	us. We are a research incubator on campus and our primary mission is to bring
43120	47840	people together regardless of what school or discipline or field they sit in,
47840	52720	whether they're inside or outside the Academy, to think through questions,
52760	58120	ideas, and problems that can't be addressed by individuals alone. All of the
58120	62320	research we support is collaborative and it all has some kind of humanistic
62320	67000	question at its core, although we're very broad in how we think about humanistic.
67000	71120	In addition to our research projects and visiting fellows program, we're
71120	75240	unusual in supporting an art gallery which strives to integrate the arts with
75240	79160	research and this special series, the Director's Lectures, through which we
79160	84640	hope to enliven conversations about research with a broader community. So
84640	89200	you might be wondering why invite a scientist to give a lecture at an
89200	93800	institution that is devoted to humanistic research and I believe this is the
93800	98480	first time we've had a scientist give this lecture, although I'm not 100% sure,
98480	104320	but certainly in my tenure. The reason is because Stuart Russell is not an
104360	108960	ordinary scientist and AI is not an ordinary topic. His book, Human
108960	113560	Compatible, AI and the Problem of Control, makes it very clear that shaping a
113560	118720	future in which AI does more good than harm will require a great deal of
118720	123440	collaboration since AI is a technology that may in fact threaten the future of
123440	128040	humanity. Humanist, social scientists, artists, and scientists will need to work
128040	131840	together in order to decide what potential uses of AI are ethical and
131880	137120	desirable in realms as diverse as warfare and defense, policing, medicine,
137120	141400	education, arts, and culture. And I know that these conversations have been
141400	147160	happening all over campus and beyond, but often in silos. So I hope that
147160	152000	Professor Russell's visit here will be or has been already an opportunity to
152000	157120	bring us together to think about the question that he poses so provocatively
157120	162440	in his title, What if We Succeed? Stuart Russell is a Professor of Computer
162440	166880	Science at the University of California at Berkeley. He holds the Smith Zadda
166880	171640	Chair in Engineering and is Director of the Center for Human Compatible AI as
171640	174840	well as the Cavley Center for Ethics, Science, and the Public, which I learned
174840	180040	today, is also a center that brings scientists and humanists together to
180040	184840	talk about important questions. His book, Artificial Intelligence, A Modern
184880	189400	Approach with Peter Norvig, is the standard text in AI used in over 1,500
189400	194640	universities in 135 countries. And his research covers a wide range of topics
194640	198640	in artificial intelligence with a current emphasis on the long term future of
198640	202720	artificial intelligence and its relationship to humanity. He has
202720	206440	developed a new global seismic monitoring system for the Nuclear Test
206440	211680	Ban Treaty and is currently working to ban lethal autonomous weapons. And as I
211720	215280	mentioned before, he's the author of Human Compatible AI and the Problem of
215280	220840	Control, which was published by Viking in 2019. And he told me today was recently
220840	228120	re-released with a new chapter called 2023. We're also very fortunate to have
228120	232920	our own Rebecca Willett, who will be in conversation with Professor Russell
232920	237160	after his lecture. Rebecca Willett is a Professor of Statistics and Computer
237200	242320	Science and the Faculty Director of AI at the Data Science Institute here at
242320	246120	the university with a courtesy appointment at the Toyota Technological
246120	250160	Institute at Chicago. Professor Willett's work in machine learning and
250160	254000	signal processing reflects broad and interdisciplinary expertise and
254000	258080	perspectives. She's known internationally for her contributions to the
258080	262280	mathematical foundations of machine learning, large scale data science and
262280	266960	computational imaging. Her research focuses on developing the mathematical
266960	270760	and statistical foundations of machine learning and scientific machine
270760	276000	learning methodology. And she has also served on several advisory boards for
276000	281080	the United States government on the use of AI. Finally, just a quick word about
281080	285360	the format. Professor Russell will speak, I think, for about 30 to 40 minutes.
285360	290600	That will be followed by a conversation between Professor Willett and Russell.
290600	295880	And then we will have time for a Q&A with the audience. And that will be
295880	300600	followed by a reception where I hope you'll all join us. But for now, please
300600	302880	join me in welcoming Stuart Russell. Thank you.
316960	322840	Thank you very much, Tara. I'm delighted to be here in Chicago and I'm looking
322880	326600	forward to the Q&A, so I'll try to get through the slides as fast as possible.
326600	335560	Okay, so to dive right in, what is artificial intelligence? So for most of
335560	342800	the history of the field, after, I would say, a fairly brief detour into what we
342800	349720	now call cognitive science, the emulation of human cognition, we followed a
349760	355440	model that was really borrowed fairly explicitly from economics and philosophy
355440	360240	under the heading of rational behavior. So machines are intelligent to the
360240	366440	extent that their actions can be expected to achieve their objectives. So those
366440	372120	objectives might be specific destinations in your GPS navigation system,
372120	378400	where the AI part of that is figuring out how to get to that destination as
378440	385000	quickly as possible. It could be the definition of winning that we give to
385000	389120	a reinforcement learning system that learns to play, go, or chess, and so on.
389120	395840	So this framework actually is very powerful. It's led to many of the
395840	401200	advances that have happened over the last 75 years. It's the same model, actually,
401280	408680	that is used in operations research, in control theory, in economics, in
408680	415600	statistics, where we specify objectives and we create machinery that optimizes
415600	420320	those objectives, that achieves them as well as possible. And I will argue later
420320	426160	actually that this is a huge mistake. But for the time being, this is how we
426160	430120	think about artificial intelligence, and most of the technology we've developed
430120	435600	has been within this framework. But unlike economics and statistics and
435600	443320	operations research, AI has this sort of one overarching goal, which is to
443320	450040	create general purpose intelligent systems. So sometimes we call that AGI,
450040	454160	artificial general intelligence, you'll hear that a lot. And it's not
454160	460040	particularly well-defined, but think of it as matching or exceeding human
460080	464600	capacities to learn and perform in every relevant dimension.
467120	475280	Okay, so the title of the talk is What If We Succeed? And I've actually been
475280	479400	thinking about that a long time. The first edition of the textbook came out
479400	486720	in 1994, and it has a section with this title. And it refers back to some of
486720	492440	the more catastrophic speculations that I was aware of at that time. But you
492440	497440	can tell reading this that I'm not that worried, at least my 1994 self was not
497440	504840	that worried. And it's important actually to think about this, not just in
504840	509200	terms of catastrophe, but what's the point, right? Besides that it's just this
509200	513240	cool challenge to figure out how intelligence works and could we make it
513240	518720	in machines. You know, there's got to be more to it than that. And if you think
518720	525000	about it, right, with general purpose AI, by definition, the machines would then
525000	532200	be able to do whatever human beings have managed to do that's of value to their
532200	537760	fellow citizens. And one of the things we've learned how to do is to create a
538200	543720	decent standard of living for some fraction of the Earth's population. But
543720	547880	it's expensive to do that. It takes a lot of expensive people to build these
547880	553680	buildings and teach these courses and so on. But AI can do it at much greater
553680	559600	scale and far less cost. And so you might, for example, just be able to
559600	567000	replicate the nice Hyde Park standard of living for everyone on Earth. And that
567040	572320	would be about a tenfold increase in GDP. And if you calculate, if you have some
572320	576840	economists here, net present value is sort of the cash equivalent of an
576840	583400	income stream. And it's about 15 quadrillion dollars is the cash value of
583400	590400	AGI as a technology. So that gives you some sense of why we're doing this. It
590400	596080	gives you some sense of why the world is currently investing maybe between a
596120	603840	hundred and two hundred billion dollars a year into developing AGI. So maybe ten
603840	610680	times the budget for all other forms of scientific research, at least basic
610680	618480	science. And so that's a pretty significant investment and it's going to
618480	625520	get bigger. As we get closer, this magnet in the future is pulling us more and
625520	631880	more and will unlock even greater levels of investment. And of course we could
631880	638160	have actually a better civilization, not just replicating the Hyde Park
638160	645360	civilization, but actually much better healthcare, much better education, faster
645360	652600	advances in science, some of which are already happening. So that's all good.
653480	659920	Some people ask, well, you know, if we do that, you know, don't we end up with the
659920	664840	warly world, right? Because then there's nothing left for human beings to do. And
664840	668680	we lose the incentive to even learn how to do the things that we don't need to
668680	677160	do, sorry. And so the human race becomes infantilized and enfeebled. And
677160	682400	obviously this is not a future that we want. Economists are finally, I would say
682400	689920	acknowledging that this is a significant prospect if AGI is created. You know,
689920	695840	they're now plotting graphs showing that, oh yeah, human wages go to zero and
695840	700240	things like that. So having denied the possibility of technological unemployment
700240	705840	for decades, they now realize that it will be total. But I'm not going to talk
705880	713040	about this today. The next question then is, have we succeeded? And, you know, five
713040	719320	years ago, I don't think anyone would have a slide with this title on it. Because
719320	724600	it's obvious that we didn't have anything resembling AGI. But now, in fact, Peter
724600	728680	Norvig, the co-author on the textbook that was mentioned, has published an
728680	735080	article saying that we have succeeded, that the technology we have now is AGI in
735120	741320	the same sense that the Wright Brothers aeroplane was an aeroplane. Right? And yeah,
741320	746960	they got bigger and faster and more comfortable. And now they have drinks. But
746960	751800	it was basically, you know, it's the same basic technology just sort of spiffed up.
751800	762480	And that's the sense in which we have AGI. I disagree. And I'll talk a little bit
762480	767720	about why I think we haven't succeeded yet. I would say that there is something
767720	776880	going on, right, with ChatGPT and all of its siblings now. There's clearly some
776880	782800	sort of unexpectedly capable behavior. But it's very hard to describe what
782800	790440	problem have we solved? What scientific advance has occurred? Right? We just don't
790480	794400	really know. It's sort of like what happened 5,000 years ago when someone
794400	798760	accidentally left some fruit out in the sun for a couple of weeks and then drank
798760	803160	it and got really drunk. And they had no idea what they had done, but it was
803160	808800	obviously cool. And they kept doing it. So this is sort of where we are. We don't
808800	814920	know what shape, you know, this piece of, it's a piece of the puzzle, but we don't
814920	818760	know what shape it has or where it goes in the puzzle and what other pieces we
818760	824880	need. And so until we do, then we haven't really made a scientific advance. And you
824880	830120	can kind of tell because when it doesn't work, we don't know what to do to make it
830120	835200	work, right? The only remedy when things don't work is, well, maybe if we just make
835200	839440	it bigger and add some more data, it will work. Or maybe it won't. We just don't
839440	847840	know. So I think there's many reasons to think that actually the, you know, we
847840	853160	don't have anything close to a complete picture. And I just want to give one
853160	859600	example. And since Dan is right here, he already knows what I'm going to say, right?
859600	864120	So here are some black holes on the other side of the universe and rotating around
864120	870520	each other and producing gravitational waves that deliver actually, you may
870520	875280	correct me, but what I read for that first, the first detected event was the
875280	881600	amount of energy per second being emitted was 50 times the output of all the visible
881600	890120	stars in the universe. So an absolutely unbelievably energetic event. And then
890120	896240	those gravitational waves arrived at the LIGO, the Large Interferometric
896280	907080	Gravitational Observatory. And so this is, the arms are, I think, four kilometers long
907080	914040	and full of physics stuff that relies on centuries of accumulation of human
914040	920760	knowledge and ingenuity and obviously incredibly complex design, construction,
920760	925920	engineering and so on. And it's sensitive enough to measure distortions of
925960	933040	space to 18 decimal places. I guess it's probably better than that now. And to give
933040	938520	you a sense, if Alpha Centauri moved further away from the Earth by the width
938520	945000	of a human hair, then this detector would notice the difference, right? Thanks, Dan,
945000	952240	for nodding. I'm feeling very reassured. Okay. So absolutely unbelievable. And, you
952240	958720	know, this device was able to measure the gravitational waves and the theory of
958720	963200	general relativity was able to predict the form of those waves. And from that we
963200	968200	could even infer the masses of the black holes that were rotating around each other.
968200	975760	I mean, imagine things 30 times as massive as the Sun rotating around each other 300
975800	984000	times a second, right? You know, it's just mind boggling to think of that. But anyway,
984000	990200	so, and then they collide and that's sort of the end of the wave pattern. So how on
990200	995960	Earth is a large language model or any relative of that going to replicate this kind of
995960	1001840	feat? Given that before they started there weren't any training examples of
1001880	1007160	gravitational wave detectors, right? So it's just, there isn't really even a place to
1007160	1014160	begin to get that type of AI system to do this kind of thing. And I think there's a lot of
1014160	1022640	work that still needs to happen, several breakthroughs. So a lot of people have brought
1022640	1030120	into this idea that deep learning solves everything. So here I'm going to get a little
1030160	1039160	bit nerdy and try to explain what the issue is. So the transformer is, think of it as a
1039160	1046800	giant wad of circuit, right? And if you want a mental picture, think of it as a chain link
1046800	1056480	fence about 100 miles by 100 miles, okay? And numbers come in one end, they pass through
1056520	1061400	this giant piece of chain link fence and they come out the other end. So the amount of computation
1061400	1066160	that the system does to produce an output is just proportional to the size of the chain
1066160	1070640	link fence, right? That's how much computing it does. It can't sit there and think about
1070640	1076120	something for a while, right? The signal comes in, passes through the circuit, comes out
1076120	1083120	the other end, okay? And this type of device, which is a linear time, so linear meaning
1083840	1089040	the amount of time it takes proportional to the size of the circuit, this linear time
1089040	1096040	feed forward circuit cannot concisely express all of the concepts that we wanted to learn.
1098080	1105080	And in fact, we know that for, well, we almost know that for a very large class of concepts,
1105920	1112560	the circuit would have to be exponentially large to represent those concepts accurately.
1112560	1117600	And so if you have a very, very large representation of what is fundamentally actually a simple
1117600	1124600	concept, then you would need an enormous number of examples to learn that concept, right? Far
1126000	1132200	more than you would need if you had a more expressive way of representing the concept.
1132200	1138280	So this is, I think, indicative, it's not conclusive, but it's indicative of what might
1138280	1144440	be going on with these systems. They do seem to have very high sample complexity, meaning
1144440	1150720	they need many, many, many examples to learn. And if you want to sort of anecdote, you know,
1150720	1157000	if you have children, you have a picture book, how many pictures of giraffes are there in
1157000	1162120	the picture book? G for giraffe, one picture, right? Having seen that, the child can now
1162120	1167480	recognize any giraffe in any photograph or context, whether it's a line drawing, a picture,
1167520	1172320	a video, you know, for the rest of their lives. And you can't buy picture books with
1172320	1179320	two million pages of giraffes. And so there's just a huge difference in the learning abilities
1180800	1185880	of these systems. Okay. So then, you know, some people say, well, you know, what about
1185880	1190000	the superhuman go programs, right? Surely you have to admit that that was a massive
1190080	1197080	win for AI. So back in 2016, AlphaGo defeated Lisa Dahl, and then 2017 defeated Kerje, who
1199520	1204680	was the number one go player. And more importantly, he was Chinese. So that convinced the Chinese
1204680	1211680	government that actually AI was surreal, and they should invest massively in AI because
1212640	1218480	it was going to be the tool of geopolitical supremacy.
1218480	1225480	So since then, actually, go programs have gone far ahead of human beings. So the human
1226440	1233440	champion rating is 3,800 approximately. The best program, Catego, this is a particular
1234680	1241680	version of Catego, are rating around 5,200, so massively superhuman. And one of our students
1242460	1248680	in our extended research group, Kellen Pellarine, who's at Montreal, is a good amateur player.
1248680	1255000	His rating is about 2,300. So miles below professional human players and miles and miles
1255000	1260680	below the human champion and so on. And Kellen in this game is going to give Catego a nine
1260680	1265560	stone handicap. Okay. So if you don't play go, it doesn't really matter. It's a very
1265560	1270600	simple game. You take turns putting stones on the board. You try to surround territory,
1270640	1273960	and you try to surround your opponent's stones. And if you surround your opponent's stones
1273960	1280960	completely, then they are captured and removed from the board. And so for black, the computer
1282120	1289120	to start with nine stones on the board is an enormous insult. If this was a person,
1289960	1296320	I think they would have to commit suicide in shame at being given nine stones. This
1296320	1299680	is how, when you have a five-year-old and you're teaching them to play, you give them
1299720	1305000	nine stones so they can at least stay in the game for a while. Okay. So I'll show you the
1305000	1312000	game. And so pay attention to what's happening in the bottom right quadrant of the board.
1314480	1319060	So Kellen is playing white and Catego is playing black. And so Kellen starts to make a little
1319060	1323720	group of stones there. And you can see that black quickly surrounds that group to stop
1323760	1330760	it from growing bigger. And then Kellen starts to surround the black stones. So we're making
1331080	1338080	a kind of a circular sandwich here. And black doesn't seem to understand that its stones
1338240	1345240	are being surrounded. And so it has many opportunities to rescue them and takes none of those opportunities
1345960	1351840	and just leaves the stones to be captured. And there they go. And that's the end of the
1351840	1358520	game. And Kellen did, you know, won 15 games in a row and got bored. He played the other
1358520	1363520	top programs which are developed by different research groups using different methods. And
1363520	1370520	they all suffer from this weakness. That there are certain kinds of groups, apparently circular
1371440	1377000	groups are particularly problematic, that the system just does not recognize as groups
1377040	1383960	of stones at all. Probably because it has not learned the concept of a group correctly.
1383960	1390080	And that's because expressing the concept of a group as a circuit is extremely hard, even
1390080	1395880	though expressing it as a program, it's like two lines of Python to define what is a group
1395880	1402880	of stones. So I think often we are overestimating the abilities of the AI systems that we interact
1403120	1410120	with. So I guess the implication of this is I personally am not as pessimistic as some
1415400	1421200	of my colleagues. So Jeff Hinton, for example, who was one of the major developers of deep
1421200	1428200	learning is in the process of tidying up his affairs. He believes that we maybe, I guess
1429080	1436080	by now, have four years left. And quite a few other people think, you know, it's pretty
1437120	1442720	certain that by the end of this decade we will have AGI. So I think we probably have
1442720	1448520	more time. I think additional breakthroughs will have to happen. But at the rate of investment
1448520	1453160	that's occurring and the number of really, really brilliant people who are working on
1453200	1460200	it, I think we have to assume that eventually it will happen. So Alan Turing, who's the
1464000	1470760	founder of computer science, gave a lecture in 1951 and he basically was asked the same
1470760	1476720	question, what if we succeed? And he said, it seems parable that once the machine thinking
1476720	1481000	method had started it would not take long to outstrip our feeble powers. At some stage
1481080	1488080	therefore, we should have to expect the machines to take control. So that's that. He offers
1490320	1497320	no mitigation, no solution, no apology, no nothing. This is just a fact. So I think if
1502320	1508760	you take one step back in his reasoning, I think he's kind of trying to answer this
1508800	1515000	question, that if you make systems that are more intelligent than humans, intelligence
1515000	1521200	is what gives us power over the world, over all the other species. We're not particularly
1521200	1528200	fast or big, we don't have very long teeth, but we have intelligence, the ability to communicate,
1528680	1534200	cooperate and problem solve. So if we make AI systems that are more intelligent then
1534280	1540200	they are fundamentally more powerful and going back to that standard model, they will achieve
1540200	1547200	their objectives. And so how on earth do we retain power over entities more powerful than
1548960	1555240	ourselves forever? And so I think Turing is asking himself this question and immediately
1555240	1562240	saying, we can't. I'm going to ask the question a slightly different way. I'm going to ask
1564480	1568480	Turing. And it comes, again, comes back to the standard model. The standard model is
1568480	1575480	a way of saying what is the mathematical problem that we set up AI systems to solve? And can
1577120	1583000	we set up a mathematical problem such that no matter how well the AI system solves it,
1583000	1590000	we are guaranteed to be happy with the result. Okay. This is a much more optimistic form
1590760	1597760	of the same question. So they could be far more powerful, but their power is directed
1600120	1607120	in such a way that the outcomes are beneficial to humans. And it's certainly not the standard
1608640	1615640	model, optimizing a fixed objective, because we have known since at least the legend of
1616160	1620880	King Midas and many other cultures have similar legends that we are absolutely terrible at
1620880	1627680	specifying those objectives correctly. If you specify the wrong objective as King Midas
1627680	1634680	found out to his cost, when all of his food and drink and family turned to gold, then
1635080	1642080	you're basically setting up a direct conflict with the machines who are pursuing that objective
1643080	1650080	much more effectively than you can pursue objectives. So that approach doesn't work.
1650640	1654960	And I'll give you an example in a second. The second approach, which is what we're doing
1654960	1661720	with the large language models, imitate human behavior, is actually even worse, the reasons
1661720	1668360	that I will explain. So let's look at this problem. We call it misalignment, where the
1668360	1675600	machine has an objective which turns out not to be aligned with what we really want.
1675600	1681400	And so the social media algorithms are now acknowledged to have been a disaster. If you
1681400	1687320	talk to people in Washington, they'll tell you, you know, we completely blew it. We should
1687320	1691320	have regulated, we should have done this, we should have done that, but instead we let
1691320	1696960	the companies nearly destroy our societies.
1697560	1702240	And the algorithms, so the algorithms, the recommender systems as they're called, decide
1702240	1709240	what billions of people read and watch every day. So they have more control over human
1709640	1716640	cognitive intake than any dictator has ever had in history. And they're set up to maximize
1717360	1724360	a fixed objective. And let's pick, click through, right, the number of times you click
1725160	1731200	on the items that they recommend for you. There are other objectives like engagement,
1731200	1738200	how long you spend engaging with the platform, and other variations on this, but let's just
1738200	1744700	stick with clicks for the time being. And you might have thought that in order to maximize
1744700	1749560	clicks, then the algorithms will have to learn what people want and send them stuff that
1749560	1756560	they like to consume. But we very quickly learned that that's actually not what the
1756680	1762200	algorithms discovered, right? What the algorithms discovered was the effectiveness of click
1762200	1767200	bait, right, which is precisely things that you click on, even if you don't turn out to
1767200	1773440	actually value the content that they contain. And, you know, filter bubbles where the systems
1773480	1780480	would send you stuff they very confidently knew you were comfortable with, so your area
1780480	1787480	of engagement gets narrower and narrower and narrower. But even that's not the solution.
1787520	1794520	The solution to maximizing click through is to modify people to be more predictable. And
1795400	1802400	this is a standard property of learning algorithms. They learn a policy that changes their environment
1804440	1811440	in such a way as to maximize the long-term summer rewards. And so they can learn to send
1812200	1819200	people a sequence of content and observe their reactions and modify the sequence dynamically
1822480	1828040	so that over time, the person is modified in such a way that in future it's going to
1828040	1835040	be easier to predict what they will consume. So this is definitely true, right? Anecdotally,
1836400	1842480	one might imagine that a way to make people more predictable is to make them more extreme.
1842480	1849560	And we have quantitative data on that in YouTube, for example, where we can look at the degree
1849560	1856560	of violence that people are comfortable with if they start out interested in the environment.
1858680	1863880	Boxing, then they'll get into ultimate fighting, and then they'll get eventually into even
1863880	1870880	more violent stuff than that. And so these are algorithms that are really simple, right?
1873960	1880960	If they were better algorithms, if they actually understood the content of what they're sending,
1881960	1887920	if they could understand that people have brains and opinions and psychology and would
1887920	1892240	learn about the psychology of humans, they'd probably be even more effective than they
1892240	1898680	are, so the outcomes would be worse, right? And this is actually a theorem under fairly
1898680	1905680	mild conditions, optimizing better on a misaligned objective produces worse outcomes, because
1906680	1913680	the algorithm uses the variables that you forgot to include in the objective and sets
1914480	1919980	them to extreme values in order to squeeze the more juice out of the objective that they
1919980	1926980	are optimizing. So we have to get away from this idea that there's going to be a fixed
1927400	1934400	objective that the system pursues at all costs, right? If it's possible for the objective
1934960	1941960	to be wrong, to be misaligned, then the system should not assume that the objective is correct,
1942560	1948360	that it's gospel truth. And so this is the old model, and by proposing that we get rid
1948360	1954120	of that and replace it with a slightly different one, right? We don't want intelligent machines,
1954120	1958880	we want beneficial machines, and those are machines whose actions can be expected to
1958880	1965880	achieve our objectives. So here the objectives are in us, and not necessarily in the machines
1967360	1974200	at all, and that makes the problem more difficult, but it doesn't make it unsolvable. And in
1974200	1979680	fact we can set this up as a mathematical problem, and here's a sort of verbal description
1979680	1985120	of that mathematical problem. So the machine has to act in the best interest of humans,
1985120	1992120	but it starts out explicitly uncertain about what those interests are, okay? And we can,
1994240	2000320	the technical formulation is done in game theory, and we call this an assistance game,
2000320	2004000	and so there's the human in the game with a payoff function, and there's a machine
2004000	2009120	in the game. The machine's payoff is the same as that of the human. In fact it is the payoff
2009120	2016120	of the human, but it doesn't, it starts out not knowing what it is, okay? And just to
2017440	2023940	help you understand this, right? If you have ever had to buy a birthday present for your
2023940	2030940	loved one, your payoff is how happy they are with the present, right? But you don't know
2031140	2038140	how happy they're going to be with any particular present, okay? And so this is the situation
2044060	2051060	that the machine is in, and it will probably do some of the same kinds of things that you
2051100	2058100	might do, like ask questions, you know, how do you feel about Ibiza, you know? And so
2061940	2068940	maybe you leave pictures of watches and things around and see if they notice them and comment
2070260	2077260	on how nice that watch looks. You know, ask your children to find out what your wife wants
2077700	2084700	and so on, right? There's all kinds of strategies that we adopt, and we actually see this directly
2085140	2092140	when we solve the assistance games, you know, the machine is deferring to the human because
2093140	2098500	if the human says stop doing that, right, that updates the machine's belief about what
2098500	2105500	human preferences are and then makes the machine not want to do whatever it was doing. And
2105660	2111380	it will be cautious if there are parts of the world where it doesn't know what your preferences
2111420	2117540	are about, you know, for example, if I'm trying to fix the climate, but I don't know human
2117540	2122740	preferences about the oceans, I might say, you know, is it okay if I turn the oceans
2122740	2128820	into sulfuric acid while I reduce this carbon dioxide stuff, right? And then we could say
2128820	2135140	no, right? So we'll ask permission, it will behave cautiously rather than violate some
2135180	2141460	unknown preferences. And in the extreme case, if you want to switch it off, it wants to
2141460	2146860	be switched off because it wants to not do whatever it is that is causing you to want
2146860	2150940	to switch it off. It doesn't know which thing that it's doing is making you unhappy, but
2150940	2155140	it wants to not do it and therefore it wants to be switched off. And this is a theorem,
2155140	2162140	right? We can show this follows directly from the uncertainty about human preferences.
2163060	2168940	And so when that uncertainty goes away, the machine no longer wants to be switched off.
2168940	2174060	So if you connect up this idea of how do we control these systems to the ability to switch
2174060	2181060	them off, then it suggests that this principle of uncertainty about human preferences is
2182100	2189100	going to be central to the control problem. So as you can imagine, just setting up this
2190100	2197100	mathematical problem is far from the final step in solving this problem. There's lots
2199300	2206300	of open issues and many of these issues connect up directly to the humanities and the social
2206300	2213300	sciences where some of them have been discussed for literally thousands of years and now they
2213980	2220980	urgently need to be solved or we need to work around the fact that we don't have solutions
2221780	2228780	for them. So here are just some of them that we have to think about the complexity of real
2230620	2236420	human preferences, not just sort of abstract mathematical models, but real human beings.
2236420	2242900	So obviously what we want the future to be like is a very complicated thing. It's going
2242900	2249900	to be described in terms of abstract properties like health and freedom and shelter and security
2250420	2257420	and so on. There's probably a lot of commonality amongst humans because that means that by
2263420	2269340	observing a relatively small sampling of human beings across the world, the system has a
2269340	2274420	pretty good idea about the preferences of a lot of people that it hasn't directly interacted
2274420	2281420	with. There's tons of information. Everything the human race has ever done is evidence about
2285980	2292140	our preferences. So if you go in the other room, you can see some of the earliest examples
2292140	2298860	of writing and that talks about, you might think it's really boring. It's like I'm Bob
2298900	2305900	and I'm trading three bushels of corn for four camels and three ingots of copper. So it tells you
2305900	2312660	two things. One is that trade was really important to people back then and getting paid and also
2312660	2318020	something about the relative values of corn and camel and copper and so on. So even there,
2318020	2325020	there's information to be extracted about human preference structures. I'm going to
2326020	2333020	just mention plasticity and manipulability as maybe the most important unsolved problem here. So
2336740	2342060	plasticity has an obvious problem, right? If human preferences can be changed by our
2342060	2349060	experience, then one way AI systems can satisfy human preferences is by manipulating them
2349620	2356620	to be easier to achieve. And we probably don't want that to happen. But there's no sense
2357220	2364220	in which we can simply view human preferences as inviolate and untouchable because any experience
2366500	2372820	can actually modify human preferences. And certainly the experience of having an extremely
2372820	2379820	capable household butler robot that does everything and makes the house spick and span and cooks
2379940	2385500	a nice meal every night, this is certainly going to change our personality and probably
2385500	2392500	make us a lot more spoiled. But actually the more serious problem here is that human preferences
2393020	2400020	are not autonomous. I didn't just sort of wake up one morning and autonomously decide
2401020	2405540	to have the preferences that I have about what kind of future I would prefer for the
2405540	2412540	universe, right? It's the result of my upbringing, the religion I was brought up in, the society
2413420	2419460	I was brought up in, my peers, my parents, everything. And in many cases in the world
2419460	2426460	and this is something that was pointed out notably by Amartya Sen, people are induced
2427460	2434780	deliberately to have the preferences that they have to serve the interests of others.
2434780	2441780	And so, for example, he points out that in some societies women are brought up to accept
2442340	2449340	their second class status as human beings. So do we want to take human preferences at
2449980	2456980	face value? And if not, who's going to decide which ones are okay to take at face value
2458020	2465020	and which ones should be, you know, replaced or modified and who's going to do that modifying
2467660	2474660	and so on? And you get very quickly into a very muddy quagmire of moral complexity and
2474660	2481660	certainly you get into a place where AI researchers do not want to tread. So there are other sets
2483780	2490780	of difficulties having to do with the fact that human behavior is only partially and
2494500	2501500	noisily representative of our underlying preferences about the future because decision-making
2505140	2511780	is often myopic. You know, even the world champion go player will occasionally make
2511780	2518100	a losing move on the go board. I mean, someone has to make a losing move. But it doesn't
2518100	2522740	mean that they wanted to lose, it just means that they weren't sufficiently far sighted
2522740	2529740	to make the correct move. So you have to invert human cognition to get at underlying preferences
2530740	2537740	about what people want the future to be like. So I think there's, you know, this somewhat
2541380	2548380	dry phrase theory for multi-human assistance games. This is a problem going back at least
2548700	2555700	2,500 years of the aggregation of people's interests. If you're going to make decisions
2556580	2563580	that affect more than one person, how do you aggregate the interest to make a decision
2563700	2570700	that is the best possible decision? And so utilitarianism is one way of doing that. But
2573260	2580260	there are many others. There are modifications such as prioritarianism. There are constraints
2580500	2587060	based on rights. People might say, well, yes, you can try to add up, you know, do the best
2587060	2594060	you can for everyone, but you can't violate the rights of anyone. And so it's quite complicated
2595140	2601640	to figure this out. And there are some really difficult to solve philosophical problems.
2601640	2608640	One of the main ones is interpersonal comparisons of preferences. Because if I look at any individual
2609600	2615640	and imagine trying to sort of put a scale on how happy or unhappy a person is about
2615640	2619760	a given outcome, right? Well, you know, imagine like you could measure it in centigrade or
2619760	2624760	you could measure it in Fahrenheit and you get different numbers. And so if you take
2624760	2629680	two people, well, how do we know that they're really experiencing the world on the same
2629680	2636680	scale, right? I have four kids. I would say their scales differ by a factor of 10, right?
2637080	2644080	But the same experience is the subjective effect of that is 10 times bigger for some
2646440	2653440	of my kids compared to some of the others. And some economists actually have argued,
2653960	2660960	Kenneth Arrow, among others, that there is no meaning to interpersonal comparison, that
2661960	2668720	it is not legitimate to say that, you know, Jeff Bezos having to wait one microsecond
2668720	2675480	longer for his private jet is better or worse than, you know, a mother watching her child
2675480	2681320	die of starvation over several months, right? That these are just incomparable and therefore
2681320	2688320	you can't add up utilities or rewards or preferences or anything. And so the whole exercise is
2688840	2695600	legitimate. I don't believe Ken Arrow believed that even when he wrote it. And I certainly
2695600	2702600	don't believe it. But actions that change who will exist, right? How do we think about
2703800	2709680	that, right? When China decided on a one-child policy, they got rid of 500 million people
2709680	2716680	who would be alive today. Was that okay? You know, don't know. Was it okay? I don't think
2718320	2722840	it was okay when Thanos got rid of half the people in the universe. You know, his theory
2722840	2727160	was, yeah, the other half, now they've got twice as much real estate, they'll be more
2727160	2732920	than twice as happy. So he thought he was doing the universe a favor. So these questions
2732920	2738920	are really hard, but we need answers because the AI systems will, they'll be like Thanos,
2738920	2745920	right? They'll have a lot of power. Okay, I'm going to skip over some of this stuff.
2746920	2753920	Let me just do it all too technical. Okay, so just moving towards the conclusion, let's
2754280	2759760	go back to large language models, right? The systems that everyone is excited about,
2759760	2766760	like chat GPT and Gemini and so on. So as I said, they're trained to imitate human linguistic
2767840	2774840	behavior. So it's what we call imitation learning. So the humans are behaving by typing, by speaking,
2775800	2782800	generating text, and we are training these circuits to imitate that behavior. And that
2785240	2792240	behavior, our writing and speaking, is partly caused by goals that we have. In fact, we
2796080	2799880	always, almost always have goals, even if it's just, you know, I want to get an A on
2799880	2804960	the test, but it could be things like, I want you to buy this product, I want you to vote
2804960	2809720	for me for president, I want you to marry me, right? These are all perfectly reasonable
2809720	2816720	human goals that we might have, and we express those goals through speech acts, as the philosophers
2817080	2824080	call them. So if you're going to copy, if you're going to build good imitators of human
2824160	2830040	language generation, then, you know, in the limit of enough data, you're going to replicate
2830040	2836320	the data generating mechanism, which is a goal-driven human cognitive architecture.
2836320	2840240	So it might not be replicating all the details of the human cognitive architecture, I think
2840240	2846360	it probably isn't, but there's good reason to believe that it is acquiring internal
2846360	2852040	goal structures that drive its behavior. And I asked this question, so when Microsoft
2852080	2859080	did their sort of victory tour after GPT-4 was published, I asked Sebastian Bubeck, you
2860960	2866400	know, what do you think the goals are that the systems are acquiring? And his answer
2866400	2873400	was, we have no idea. So you may remember the paper they published was called Sparks
2873400	2879560	of Artificial General Intelligence. So they're claiming that they have created something
2879600	2886600	close to AGI, and they have no idea what goals it might have or be pursuing. I mean,
2888720	2894600	what could possibly go wrong with this picture, right? Well, fortunately, we found out fairly
2894600	2901600	soon actually that, among other goals, the Microsoft chatbot was very fond of a particular
2902200	2909200	journalist called Kevin Ruse and wanted to marry him and spent 30 pages trying to convince
2911200	2918200	him to leave his wife and marry the chatbot. And if you haven't read it, Kevin Ruse, R-O-O-S-E,
2920640	2927640	just look it up and read it, and it's creepy beyond belief. So, and this is just an error,
2928640	2935640	right? We don't want AI systems to pursue human goals on their own account, right? We
2939920	2945920	don't mind if they get the goal of, you know, reducing poverty or, you know, helping with
2945920	2952480	climate change. But goals like marrying a particular human being or getting elected
2952480	2957720	president or being very rich, we do not want AI systems to have those goals, but that's
2957720	2964400	what we're creating, right? It's just a bug, right? We should not be doing imitation learning
2964400	2971400	at all. But that's how we create these systems. So, I'll briefly mention a few others. So
2974720	2980720	I have described an approach based on assistance games, based on this kind of humble AI system
2980760	2987680	that knows that it doesn't know what humans want, but wants nothing but helping humans
2987680	2993840	to achieve what they want. There's another approach which is now becoming popular, I
2993840	2999840	think partly because people's timelines have become short, and they're really casting
2999840	3006680	around for approaches that they can guarantee will be safe in the near term. So one approach
3006720	3012920	is to build what we call a formal oracle. So an oracle is any system that basically answers
3012920	3019920	yes or no when you give it questions. And so a formal oracle is one where the answer
3020600	3027600	is computed by a mathematically sound reasoning process. So, for example, think about a theorem
3029680	3035800	prover that operates using logically sound axioms, right, and proves, you know, so you
3035840	3041600	can ask it questions about mathematics, and if it's good enough, it'll tell you that Fermat's
3041600	3048600	last theorem is correct, and here's the proof. So this can be an incredibly useful tool for
3049960	3056400	civilization, but it has the property that it can only ever give you correct answers.
3056400	3061360	And the job of the AI here is just to operate that theorem prover, right, to decide what
3061400	3068400	kinds of theorem proving steps to pursue to get to the proof of arbitrarily difficult
3070120	3076560	questions. So that's one way we could do it, right. We convince companies or we require
3076560	3083560	that they only use the AI to operate these formal oracles. Another whole set of approaches
3084520	3091520	are based on what's actually a mathematical property of computational problems in general,
3093240	3098960	that it's always easier to check that the answer or a problem is correct than to generate
3098960	3105160	that answer in the first place. And so there are many forms of this. I won't go through
3105160	3111000	all of them, but one of them is constitutional AI, so Anthropic, which is a company that was
3111000	3117200	spun off from open AI, has this approach called constitutional AI, where there's a second
3117200	3122360	large language model whose job it is to check the output of the first large language model
3122360	3129360	and say, does this output comply with the written constitution? So part of the prompt
3130000	3137000	is a page and a bit of things that good answers should comply with. And so they're just hoping
3137520	3144520	that it's easier for the second large language model to catch unpleasantnesses or errors made
3145640	3151680	by the first one, because it's computationally easier to check than to generate answers in
3151680	3157480	the first place. And there are lots of other variants on this basic idea. And then there
3157480	3164480	are sort of more, I would say, good old fashioned safety practices. So red teaming is where
3167640	3174140	you have another firm whose job it is to test out your AI system and see if it does bad
3174140	3181140	things and then send it back and get it fixed and so on. And that's actually what companies
3181600	3188600	prefer because it's very easy to say, yep, we hired this third party company, they got
3190040	3196160	smart people from University of Chicago who spent a week and they couldn't figure out
3196280	3202000	how to make it behave badly, so it must be good. And so this is pretty popular with companies
3202000	3208120	and governments at the moment, but it provides no guarantees whatsoever. And in fact, all
3208120	3215120	that's happening is we are training the large language models to pass these evaluation tests
3216200	3221720	even though the underlying tendencies and capabilities of the model are still as evil
3221760	3228760	as they ever were. It's just learned to hide them. And in fact now we've seen cases where
3229240	3236240	the model says, I think you're just testing me. So they're catching on to the idea that
3239240	3246240	they can be tested. So there you go. Okay, so the basic principle here is we have to
3247240	3252480	be able to make high confidence statements about our AI systems. And at the moment we
3252480	3259480	cannot do that. The view that you will see from almost everyone in industry is we make
3259960	3266960	AI systems and then we figure out how to make them safe. And how do they make the AI systems?
3267400	3274400	They train this giant wad of circuit, this 100 mile by 100 mile chain link fence from
3275280	3281160	tens of trillions of words of text and billions of hours of video. In other words, they haven't
3281160	3288160	the faintest idea how it works. It's exactly as if it landed from outer space. So taking
3290040	3297040	something that lands from outer space and then trying to apply post hoc methods to stop
3297240	3303640	it from behaving badly just does not sound very reassuring to me. And I imagine not to
3303680	3309480	most people. So I think we've got to stop talking about this. In fact, here's what Sam
3309480	3314200	Altman said. He said, first of all, we're going to make AGI. Then we're going to figure
3314200	3320280	out how to make it safe. And then we're going to figure out what it's for. And I submit
3320280	3327280	that this is backwards. So making safe AI means systems that are safe by design that
3327520	3332160	we, because of the way we're constructing them, we know in advance that we're going to
3332160	3337080	be safe. In principle, we don't even have to test them because we can show that they're
3337080	3340960	safe from the way they've been designed. So there's a lot of techniques. I don't think
3340960	3346000	I want to go through that. But I want to talk about the role of government in making this
3346000	3353000	happen. And this concept of red lines is gaining some currency, right? You would like to say
3354080	3359080	your systems have got to be safe and beneficial before you can sell them. But that's a very
3359080	3365760	difficult concept to make precise, right? The boundary between safe and unsafe is we're
3365760	3372760	not sure where it is. It's culturally dependent. It's very fuzzy and complicated. But if we
3373520	3379200	just draw some red lines and say, well, we'll get to safe later, let's just not do these
3379200	3386200	things, right? Things that are obviously unsafe and nobody in their right mind would accept
3386600	3392040	that AI systems are going to do these things, okay? And the owners of proof is on the developer,
3392040	3397920	not on the government, not on the user, but on the developer to prove that their system
3397920	3403400	will not cross those red lines. But for good measure, we'll also require the developer
3403400	3409160	to put in a detector and an off switch so that if it does cross the red line, then it
3409160	3416160	is immediately detected and we can switch off all of those systems. And so, we want
3416200	3420960	these red lines to be well-defined, to be automatically detectable, and to be politically
3420960	3427960	defensible. Because you can be sure that as soon as you start asking companies to provide
3427960	3432800	any kind of guarantee, they will say, oh, this is really, really difficult. We just don't
3432800	3438320	know how to do that. This is going to set the industry back years and years and years,
3438320	3442600	right? But we wouldn't accept that excuse from someone who wanted to operate a nuclear
3442640	3446840	power plant. It's really difficult to make it safe. What do you mean you want us to give
3446840	3453240	you some kind of guarantee? Right? Well, the government would say tough. Ditto with medicines.
3453240	3457360	Oh, it's clinical trials. They're so expensive. They take such a long time. Can we just skip
3457360	3464360	the clinical trial part? No, we can't. So, some examples would be no self-replication,
3465360	3472360	no breaking into other computer systems, no designing bioweapons, and a few other things.
3474360	3480100	It's actually not particularly important which things are on that list. Just that by having
3480100	3486400	such a list, you are requiring the companies to do the research that they need to do to
3486400	3493400	understand, predict, and control their own products. Okay, I think I'm going to skip over
3494000	3501000	this and go to the end. So, this is a really difficult time, I think, for us. And not just
3506680	3513680	with AI, but also with biology and with neuroscience. We are in the process of developing technologies
3513800	3520280	that can have enormous, potentially positive and also negative impact on the human race.
3520280	3526400	We just, as Einstein pointed out, we just don't have the wisdom to keep up with our
3526400	3533400	own scientific advances. And we need to remain in control forever that much. I think it's
3534680	3540160	clear, although there are some who actually think that it's fine if the human race disappears
3540160	3547160	and machines are the only thing left. I'm not in that camp. So, if we're going to retain
3548040	3555040	control forever, we have to have AI systems that are provably beneficial to humans, right,
3557200	3562920	where we have mathematical guarantees. And this is difficult, but there's no alternative
3562920	3569920	in my view. Thank you.
3577160	3584160	Well, Stuart, thank you so much for just a fascinating talk. I really enjoyed hearing
3593400	3600400	your perspectives and you covered a lot of really interesting ground here. So, to start
3601160	3608160	off, I'd like to talk about open AI and chat GPT, as you mentioned in your talk. And as
3609240	3615160	you might know, they originally, in their terms of service or their usage policy, had
3615160	3620640	a ban on any activity that has a high risk of physical harm, including military usage
3620640	3627080	and weapons development. And then this January, those restrictions were removed from their
3627080	3633320	terms of service. So, I feel like that's an element of AI safety that concerns many
3633320	3640320	people, but it feels a little disconnected from notions of formal proofs or even just
3641460	3648460	guaranteeing the accuracy of outputs of a tool like chat GPT. So, how do you think about
3648480	3653560	AI safety in a context like this?
3653560	3660560	So I think killing people is one of a number of misuses of AI systems. Disinformation is
3664280	3671280	another one. Using AI systems in filtering resumes in biased ways is another, right?
3674120	3679080	So there's a long list of ways you could misuse AI, but I think killing people would
3679080	3686080	be at the top of that list. Interestingly, the European AI Act and the GDPR before it
3689960	3696960	bans the use of algorithms that have a significant legal effect on a person or similarly significant
3697360	3704360	effect. You would think killing would be considered as a significant effect on a person, but the
3705120	3712120	European AI Act and GDPR have carve outs for defense and national security. So, actually
3716240	3723240	I think the issue has been debated at least since 2012, but I think the AI community should
3725680	3732680	have been much more aware of this issue going back decades. I mean, the majority of the
3734680	3740840	funding in the computer vision community was provided by the DARPA Automated Target Recognition
3740840	3747840	Program, the ATR program. What did people think that was for? Other than to enable autonomous
3751080	3758080	weapons to find targets and blow them up. And the first concerns, so the UN Special Rapporteur
3758520	3765520	on extrajudicial killings and torture, what a great job. Christoph Heinz wrote a report
3768520	3775120	saying that autonomous weapons are coming and we need to be aware of the human rights
3775120	3782120	implications, particularly that they might accidentally kill civilians, not correctly
3782800	3788920	being able to distinguish between civilians and combatants. So that was the basis for
3788920	3795920	the early discussions that started in Geneva in 2014. And in Human Rights Watch, of which
3796560	3802880	I'm a member, sent an email saying, you know, we're starting this big campaign, you know,
3802880	3807920	remember all those soldiers that we've been excoriating for the last 30 years? Well, soldiers
3807960	3813720	are really good. It's the robots we have to worry about. They're really bad. And, you
3813720	3820720	know, initially my reaction was, wow, you know, that's a bit of a challenge to the AI
3821480	3827840	community. I bet you we can do a better job of distinguishing between civilians and combatants
3827840	3834280	than humans can if we put our minds to it. So I was initially quite skeptical of this
3834280	3841280	campaign, but the more I thought about it, the more I realized that actually that's not
3843360	3850360	the issue. The issue is that the logical endpoint of autonomous weapons is the ability for one
3851720	3858720	person to launch a million or 10 million or 100 million weapons simultaneously because
3859720	3866000	they're autonomous. They don't have to be managed with one human pilot for each of the
3866000	3871120	weapons, the way we do with the remotely piloted, you know, predator drones and so on. In fact,
3871120	3877120	you need about 15 people to manage one predator. But with fully autonomous weapons, you can
3877120	3883880	launch 100 million of them, you know, so you can have the effect of a whole barrage of
3883920	3890920	50 megaton bombs at much lower cost. And the technology is much easier to proliferate
3894480	3898920	because, you know, these will counter small arms. They can be miniaturized. You can make
3898920	3905920	a lethal device about that big, a little quadcopter carrying an explosive charge. And so the
3906680	3913680	logical endpoint is extremely cheap, scalable, easily proliferated weapons of mass destruction.
3914880	3921880	Why would we do that? But that's the path we're on. And it's been surprisingly difficult
3923000	3928960	to get this point across. I gave a lot of PowerPoint presentations in Geneva. I guess
3928960	3935000	some people understood them, but eventually we made a movie called Slaughterbox, which
3935000	3942000	did seem to have some impact illustrating in a fictional context what life would be like
3942440	3949440	when these types of weapons are widely available. But it's still the case that negotiations
3949600	3956600	are stalled because Russia and the U.S. agree that there should not be a legally binding
3957320	3962320	instrument that constrains autonomous weapons.
3962320	3969320	Okay. So I guess, you know, some of the things that you were alluding to suggested the need
3970320	3975320	for regulation to stop the development of certain kinds of AI tools. And I think that
3975320	3981440	ties in to some of your concerns about weaponry. But when you're thinking about quad choppers
3981440	3987640	with explosive devices, we can detect those. I think there are other uses of AI as a type
3987640	3993400	of weapon or as a source of misinformation that are far more challenging to detect.
3993400	4000400	And so what do you see as the future there? Do you see there being any kind of technological
4002080	4009080	pathway towards building mechanisms for detecting the misuse of AI? I think the kinds of ideas
4010280	4015680	that you mapped out are really exciting about building systems such as for self-driving cars
4015680	4021520	that we can trust as consumers. But I think that it's a little bit more challenging for
4021520	4027160	me at least to wrap my head around how we guard against bad human actors who are taking
4027160	4032160	these tools and not employing the kinds of strategies that you're recommending.
4032160	4039160	Yeah, exactly. I think there's a big difference between regulation and enforcement or policing.
4039360	4046360	So we can regulate all we want. But we have regulations against theft, but we still have
4046520	4053520	keys and locks and so on. So we take steps to make those kinds of nefarious activities
4054800	4061800	as difficult, expensive, risky and so on. And mostly I think when you look at rates of
4062840	4069840	violent crime and theft and so on, over the decades things have improved. And so we should
4069840	4076840	definitely take measures like that. I think there are ways of labeling genuine content.
4078520	4085520	You can have rules about traceability that you have to be able to trace back a piece
4085520	4092520	of text to the person who generated it. It has to be a real person. So ways of authenticating
4093360	4100360	humans to access social media, for example. So I absolutely don't believe as we are currently
4103320	4110320	doing, giving social media accounts to large language models and bank accounts and credit
4110560	4117560	cards and all the rest I think is quite dangerous and certainly needs to be carefully managed.
4118560	4125560	You know, the idea that it's up to the user to figure out whether the video that they're
4127000	4132160	looking at is real or fake and oh yeah, you can download a tool and run the video through
4132160	4139160	the tool and blah, blah, blah, you know, no chance. That's completely unreasonable to
4139160	4146160	say that it's the user's responsibility to defend themselves against this onslaught.
4146640	4153480	So I think platforms need to label artificial, artificially generated content and give you
4153480	4159980	a filter that says, I don't want to see it. And if I do see it, I want it to be absolutely
4159980	4166980	clearly distinguishable. There's a big red transparent layer across the video so that
4168120	4173360	I just get used to this idea that I'm looking at fake. I don't have to read a little legend
4173360	4180360	in the bottom right corner. It's just cognitively salient in a straightforward way. So all of
4181280	4188280	that. But when you're talking about existential risks where we call it the Dr. Evil problem,
4192800	4199800	right? Dr. Evil doesn't want to build beneficial AI. How do you stop that? And I think that
4199800	4206800	the track record we have of policing malware is so unbelievably pathetic that I just don't
4214720	4221720	believe it's going to be possible. Because software is created by typing and it's transmitted
4222400	4229400	at the speed of light and replicated infinitely often. It's really tough to control.
4230480	4237480	But hardware, if I want to independently develop hardware on which I can create AGI, it's going
4240720	4247220	to cost me easily $100 billion and I need tens of thousands of highly trained engineers
4247220	4254220	to do it. So I think it's, as a practical matter, impossible. It's probably more difficult
4255220	4262220	to do that than it is to develop nuclear weapons independently. So I think the approach to take
4264460	4270660	is that the hardware itself is the police. And what I mean by that is that there are
4270660	4277660	technologies that make it fairly straightforward to design hardware that can check a proof
4278260	4285260	of safety of each software object before it runs. And if that proof doesn't check out
4285900	4290020	or it's missing or whatever, the hardware will just simply refuse to run the software
4290020	4296060	object at all. I feel like even that's pretty challenging. I mean, let's say I had an army
4296060	4301380	of students make a collection of websites that all say one way or another that Stuart
4301700	4308540	Russell loves pink unicorns. I feel like eventually Chatchi P.T. is going to decide it's a fact
4308540	4315540	that Stuart Russell loves pink unicorns, right? And so how do we think about a hardware system
4315580	4322580	that's going to decide whether or not this system is correct or not? I mean, just arbitrarily
4323020	4328060	deciding, or not arbitrarily, but coming up with an arbitrary truth in general, I feel
4328100	4330540	like it's a fundamentally challenging problem.
4330540	4337540	Yeah, I totally agree with you. I'm not proposing an arbitrary truth, but just, for example,
4339180	4345420	if you have, let's take the formal oracle, right? So if we accept that that's one way
4345420	4352140	of building an AI system, and so far this is the only authorized way that you're allowed
4352140	4358580	to build AGI, we can check that the thing that you're wanting to run on this giant computer
4358580	4365580	system complies with that design template, right? And if it's another type of system,
4367820	4374820	it won't need as strong a, you know, a license in order to run, right? So the properties
4375140	4382140	that the system has to comply with will depend on what types of capabilities it would have.
4382140	4388300	All right. Well, that makes a lot of sense. Now, I know the audience, I've got a hundred
4388300	4391500	questions here, but I want to make sure that the audience has time to ask theirs. I'm just
4391500	4398500	going to ask one final question here. Hypothetically, let's say that one had a husband who wanted
4399340	4405340	to get an AI-enabled smart toilet. How worried should that person be?
4405340	4412340	A husband who wanted to get a what? An AI-enabled smart toilet.
4413460	4416460	Do they know too much?
4416460	4422460	Yeah. I mean, there are Japanese companies, I believe, who are selling these already.
4422460	4423460	Yes.
4423460	4427460	And now I see, so this is the household discussion that you're having.
4427580	4429580	This is hypothetical.
4429580	4436580	Yeah. So I guess the issue is one of privacy in general. And I think I would be quite concerned
4443700	4450700	about the toilet sending data back to headquarters in Osaka or whatever. And, you know,
4451700	4455700	we don't have to go into detail.
4455700	4461700	We don't have to go into great detail. But this, I think, is symptomatic of a much wider
4461700	4468700	problem in the software industry, in my view, has been completely delinquent. We have had
4469580	4476580	tools for decades that allow cost-iron guarantees of security, of privacy, you know, of privacy
4480820	4487820	for example, you can guarantee that a system that interacts with you is oblivious, meaning
4488780	4494380	that after the interaction it has no memory that the interaction ever occurred. That can
4494380	4501380	be done and what should happen is that that system offers that proof to your cell phone
4502820	4507700	and your cell phone then can accept to have an interaction with the system and otherwise
4507700	4513260	it says sorry. So your cell phone should be working for you and the whole ecosystem
4513260	4519260	should be operating on these cost-iron guarantees. But it just doesn't work that way because
4519260	4525020	we don't teach people how to do that in our computer science programs. I think 80 percent
4525020	4530820	of students graduate from Berkeley and we are the biggest provider of tech talent in
4530820	4535980	the country. 80 percent of our students graduate without ever having encountered the notion
4535980	4541860	of correctness of a program. Now in Europe it's totally different. Correctness is the
4541860	4548860	main thing they teach, but Europe doesn't produce any software. So we graduate these
4548860	4553220	students who know nothing about correctness. Their idea is drink as much coffee as you
4553220	4560220	can and produce the software and ship it. And I think a society is getting to the point
4560220	4566220	where we're not going to accept this anymore. That's excellent. Very insightful. Thank you.
4566220	4573220	It's a much better answer to my question than I anticipated. So Tara are you going to? Yeah,
4575940	4581740	thank you so much to both of you. We do have some time for questions and there are some
4581740	4587740	people with microphones running around. Yes, in the front. Go ahead.
4587740	4594740	You mentioned that things can be taken at face value. And I don't know which things
4602860	4609860	you mean. Can you repeat the question? That things can be taken at face value. Oh, okay.
4610740	4616460	So I was talking about human preferences and the fact that they cannot be taken at face
4616500	4623500	value, particularly if they are preferences that have been induced in people for the interests
4624860	4631860	of other people. So think of people being brainwashed to believe that the well-being
4634020	4641020	of Kim Il-sung, the founder of North Korea, was the most important value in their lives.
4642020	4649020	So the issue with that is that if you're not going to take their preferences for the
4653260	4658940	future at face value, what are you going to do? Who are you to say what their preferences
4658940	4665940	should be? So it's a really difficult problem and I think we need help from philosophers
4666940	4673940	and others on this question. In the pink cardigan? Right behind you. Thanks. Hi, my
4679060	4682220	name is Kate. Thank you so much. I've had the pleasure and challenge of reading your
4682220	4686460	textbook in my AI and humanities class with Professor Tharson over there. So it's really
4686460	4691500	exciting to hear from you. I'm someone who works in the AI policy regulatory space and
4691540	4698540	so I was really enthralled by the latter portion of your lecture. Last class we actually talked
4698620	4703780	about open AI's new development of Sora. Sora is a technology for people who don't know
4703780	4709780	that can create generative video that's really remarkable. I was in shock when I saw some
4709780	4714740	of the example videos that were provided and I'm wondering as someone who's deeply concerned
4714740	4719420	about the proliferation of deep fakes, what you would suggest the best methods are for
4719420	4724100	regulators to ensure the authenticity of content. Specifically what are your thoughts
4724100	4729780	on watermarking technologies like those used by Adobe and also the potential for blockchain
4729780	4736780	encryption to be used as an authentication measure. Thank you. Thanks. Yeah, these are
4736900	4743900	really good questions and many organizations are trying to develop standards and that's
4744420	4750420	part of the problem. There's too many standards and there isn't any sort of canonical agreement
4750420	4757420	but I think governments maybe need to knock some heads together and say could you stop
4759220	4763780	bickering with each other and just get on and pick one and so on. Some of the standards
4763780	4770780	are very weak so I think Facebook standard is for watermarking is such that if you just
4770980	4777980	take a screenshot of the image then the watermark is gone and now it thinks that it's a real
4779180	4786180	one. So it does have still some technical issues of how you make a watermark that's
4787940	4794940	really non-removable but the other idea you mentioned which is traceability through blockchain
4795300	4802300	is one possibility. So you want both that all the tools watermark their output but also
4803300	4810300	you want that cameras indelibly and unforgably watermark their output as genuine and then
4816500	4822940	you kind of squeeze in the middle and then you want platforms to as I said either allow
4823020	4830020	you to filter out all automatically generated videos and images from let's say from news
4831100	4838100	or to very clearly mark them so that you know this is fake. Yeah it feels and I think there
4843380	4850380	are also some things we should ban and the UK has just banned deepfake porn and actually
4850580	4857580	I think any deepfakes that depict real individuals in non-real situations without their permission
4860100	4867100	I think a priori that should be illegal except under some extenuating circumstances but
4871980	4876620	this was originally in the European Union AI Act and then it got watered down and watered
4877180	4884180	down until it's almost undetectable but yeah I again it just seems like the rights of individuals
4888380	4894700	are generally trampled on because someone thinks they have a way to make money off it
4894700	4901700	and you know it's been hard to get representatives to pay attention until you know in some sense
4902580	4909580	fortunately Taylor Swift was deepfaked and then most recently AOC was deepfaked but
4910860	4917460	Hillary Clinton told me that she was deepfaked during the 2016 election by the Russians so
4917460	4923500	it's been around for a while and but I think more and more countries are believing that
4923500	4930500	this should be banned and another thing that should be banned is probably the impersonation
4931300	4938300	of humans so either of a particular human or even of a generic human that you have a
4938900	4943100	right to know if you're interacting with a human or a machine and that is in the European
4943100	4948780	Union AI Act and I think it ought to be in US law and in fact I think it should be a
4948780	4955780	global human right to have that knowledge. Thank you so much. Hi in the plaid shirt there
4956660	4963660	on the aisle. Hi so I teach the formal verification course here at the University of Chicago and
4971180	4975700	if a student is to ask me you know what's the most impressive thing that's been verified
4975700	4982700	I pretty much immediately say the comp cert compiler by Xavier Leouan is an amazing team
4983500	4990500	of European researchers in France and that's a C99 compiler incredibly heroic effort from
4991860	4998300	the verification perspective not so much from the writing a compiler perspective you know
4998300	5002940	I've also you know I took Michael Curran's class when I was a student at the University
5002940	5007340	of Pennsylvania from a theoretical perspective you know it shows all of these limitations
5007340	5013180	on what things can be probably approximately correct learned and yet we have GPT-4 and
5013180	5018340	soon we're going to have GPT-5 and it seems like there's just this colossal gap between
5018340	5023900	the things that we can have provable guarantees about and the you know the things that we
5023900	5029700	actually have and that was true ten years ago so especially if we want these to be you
5029700	5036700	know correct by construction that is don't take something that exists and you know prove
5037140	5044140	things about it which it seems like you're not on board with because because they already
5044220	5049340	have those goals and you're not going to get them out of them how can we possibly bridge
5049340	5056340	this gap in however long it takes to get to general intelligence yeah I think that's
5058740	5065740	that's a really important question and you know so some people are like you know they
5066780	5073780	are literally calling for a moratorium that when we see systems exhibiting certain abilities
5075580	5082580	that we just call a halt I prefer to think of it more positively of course you can develop
5083540	5090540	more powerful systems but you need to have guarantees of safety to go along with them
5090980	5097980	and honestly the efforts made to understand predict and control the systems are puny
5104100	5110260	just to give you a comparison right so one of my colleagues Ed Morse in nuclear engineering
5110260	5115340	did a study and so for nuclear power when you when you want to operate a power plant
5115340	5122340	you have to show that the mean time to failure is above a certain number and originally it
5124140	5131140	was ten thousand years it's now ten million years and to show that is a very complicated
5131420	5138420	process and Ed Morse did a study you know back in the before you know back in the days
5139140	5146140	of paper how much paperwork was required to get your nuclear power plant certified as
5147340	5154340	a function of the mass of the power plant itself right so for each kilogram of nuclear
5154740	5161740	power station how much paper did you need and the answer is seven kilograms of paper
5162580	5169580	right so and these are giant buildings right with containment and lead and all kinds of
5170420	5177260	stuff so a lot and you know so if you just look at that and then compare that to the
5177260	5181780	efforts oh you know they hired a couple of grad students from MIT who spent a week doing
5181780	5188780	red teaming it's pathetic and so I I think we have got to we've got to actually develop
5192300	5198620	some backbone here and say just because the company say they don't want to do this does
5198620	5205620	not mean we should just say oh fine you know go ahead without any safety measures you know
5206260	5213260	mean time to failure of a week it's great so and then I but I think you know it's reasonable
5213860	5219420	for the companies to then ask well okay how are we supposed to do this and I think one
5219420	5225980	way is not to build giant black boxes it's as you correctly point out right there there
5225980	5232980	are ways of getting high confidence statements about the performance of a system based on
5233980	5239180	you know how many data points has been trained on and sort of how complex the model is that
5239180	5244980	you're training but then the numbers if you're trying to train a giant black box a gargantuan
5244980	5250660	right I mean you know a model with a trillion parameters as we think gpd4 probably has you
5250660	5257660	know we're we might need you know 10 to the 500 data points to get any confidence that
5258660	5265660	what it's what it's doing is is is correct and as I pointed out the goal of imitate humans
5268540	5272980	isn't the right goal in the first place so showing that it correctly imitates humans
5272980	5279980	who who want power and wealth and and spouses is not the right goal anyway so we don't want
5280460	5284460	to prove that it does those things right we actually want to prove that it's beneficial
5284460	5291460	to humans so my guess is that as we gradually ratchet up the regulatory requirements the
5295060	5302060	technology itself is going to have to change towards being based on a substrate that is
5306300	5312740	semantically rigorous and decomposable and just you know a simplest example would be
5312740	5318660	you know a logical theorem prover where we can examine each of the axioms and test its
5318660	5323740	correctness separately we can also show that the theorem prover is doing logical reasoning
5323740	5330740	correctly and then we're good right and so you take that sort of component based semantic
5332740	5338780	rigorously you know semantically rigorous inference approach then you can build up to
5338860	5345160	very complex systems and still have guarantees so is there is there some hybrid of the total
5345160	5351660	black box and the you know break it all the way down to to semantically rigorous components
5351660	5356460	I think this is inevitable that we will go in this direction and from what I hear you
5356460	5363460	know this is in fact likely to be a feature of the next generation that they they're incorporating
5364340	5370340	what we call good old-fashioned AI technology as well as the giant black box.
5370340	5376220	Okay I know there are a lot of other questions in the audience but unfortunately we are out
5376220	5383060	of time so I hope that you'll all stay and join us for a reception to continue the conversation
5383060	5390060	outside but in the meantime please join me in thanking Professor Russell and Willett.
5393460	5400460	Thank you.
