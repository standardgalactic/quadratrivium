start	end	text
0	16000	Alright. Hello, everybody. I have quite an honor today. I am lucky enough to introduce
16000	22400	Brandon Rhodes. He's a longtime Python developer, maintainer of several amateur astronomy libraries
22400	28240	and a developer for Dropbox. He's also a Quizmaster extraordinaire, as some people got to find
28240	34000	out last night. Today, he will be presenting on, oh, come on, who needs byte arrays? Without
34000	45120	further ado, Brandon Rhodes.
45120	50600	Welcome, everybody. I hope you're having a good time at PyCon and in Montreal. My topic
50600	59080	today is indeed byte arrays. A very interesting recent addition to the Python ecosystem. Because
59080	64280	in Python, normal string objects, the ones that we're accustomed to dealing with are
64280	74200	immutable. They can't be changed or modified. This is true of the types available under
74200	84120	both two and three. The original string type, STR, was renamed to bytes because of its low-level
84120	91800	nature and then that synonym was also backported to Python 2. We will not talk much today about
91800	102920	the newer Unicode byte strings that were renamed to the official string type of Python 3. We're
102920	109400	going to be talking about those lower-level ones. The strings that don't pretend they have symbols
109400	121640	inside of them so much as they know that their innards are really bytes. 8-bit codes, 0-255.
121640	129960	For the most part, I will put that little B in front of the bytes strings that I type. It is
130040	137560	optional under Python 2, but it becomes mandatory under Python 3. So if I write my strings like
137560	144760	this, then it will work wherever you try this later if you want to see the examples run. Strings
144760	150360	are immutable. What does that mean? It means that when you call methods, when you do things to them,
150360	156600	the original object itself doesn't change. You get given a new object as the return value of the
156600	162760	method you call. So if .lower returns to you a new string, you can still peek back and see that
162760	168360	the original is untouched and unchanged. If you run split, you'll see that both of the objects
168360	176040	you've been given are new string objects. The original is still unchanged. They do not allow
176040	181240	assignment because that would make them change. It would make them mutate as the technical term
181320	188280	in computer science. Now, immutability has led a long and very successful career in Python
188280	195160	because it makes things simple. You don't pass a name to a function only to finally
195160	200760	suddenly discover it's another name when it comes back. You don't pass a string or a block to
200760	206200	someone, and suddenly it's a different string or block. It's a very, very simple model that if
206200	212040	you have the word Python, you know it will always remain so. And it actually is one of the most
212040	216760	parts of Python. That's the most functional programming languages where new results are
216760	224120	returned instead of being written onto data structures you already hold. The string types
224120	231480	are a primary example of that. But it sometimes is a little expensive in terms of allocation.
231480	236920	Every time you want to make any little tweak to a string, it has to allocate a new one for you.
236920	244200	That means a lot of data gets copied back and forth into new areas of memory. Not everybody
244200	251400	is happy about that. And so Python 3 introduced and then it was back ported to Python 2, 6,
251400	258520	and 7, the new byte array. It's a built-in. Python 2, 7, Python 3, you can just type byte array
258520	265640	and you'll get access to that type, just like you can with stir and int and list. A byte array
266440	276120	is a mutable string that is based, this is interesting, on Python 3's byte string, not the
276120	283800	old stir string from Python 2. And that's an interesting problem, that it's based on Python
283800	294600	3's string, byte string type, because honestly, the Python 3 bytes type is designed to be awkward
294600	304280	for string operations. Why? So you will want to be a good person and run the code before
304280	312200	treating your data as characters. And this has led to Python 3 programmers tending to write code
312280	317800	that is from the ground up prepared for internationalization and different alphabets because they
317800	323960	think about the issue of decoding on the way in and encoding on the way back out because they
323960	330600	have to. But we'll see it leads to some interesting behaviors. Just professionally, always be aware
330600	339400	of using string types that wish you weren't using them. In Python 2, let's compare. We can build a
339480	345880	string, we can ask its length, we can split it into pieces. Python 3's byte type, there's little
345880	351000	B characters hanging out in front of our byte strings, but we can take the length, we can call
351000	357960	a method like split. In Python 2, we can use those square brackets with a colon in order to do
357960	364680	slicing and get back a copy of the inside of the string. The exact same thing works exactly the
364680	372600	same way in Python 3. In Python 2, it's always a custom in Python that if something has a length,
372600	378200	it should allow itself to be iterated or looped over. In Python 2, if you try looping over a
378200	386280	string, you get out smaller strings, one character strings that are inside. What happens if you
387080	399720	press enter for this line of code in Python 3? You get a syntax error. Syntax error, you failed to
399720	410280	pay the Python 3 per intax. Python 3, it's kind of like an old text-based adventure game where
410280	415560	you can tell the writer just threw an extra obstacle in your way because a room needed to
415560	433320	be a little more complicated. You know, in seriousness, I've known Ruby, some Ruby programmers,
433320	437880	this is actually a big debate in the Ruby community, Ruby makes parenthesis optional
437880	443720	when calling a function. The Rubyists who never use the parenthesis always tell me they could
443720	450040	never stand Python again. I always thought that was the oddest thing. Doesn't everyone want to
450040	456120	type parenthesis whenever they ask their computer to do something? I must admit that once it was
456120	465000	me who was suddenly having to type parenthesis, but they surely must still be wrong. Anyway.
465320	474520	I did a conservative estimate of how much typing is cost me by these print statements in Python 3,
474520	482760	and these are conservative estimates, and it's coming out to quite a bit of typing per year.
482760	489880	I'm having to face going into Perendet just to write some Emax Lisp next week. Anyway, Python 3
489880	494600	wants them for this print statement, and what's another two perends when I've already typed so
494600	506440	many? So once you get the print statement working, you're in for another surprise. Python 3 bytes type
507240	516120	is not made of characters, it is made of numbers. This breaks a very important contract that for
516120	521320	me existed with strings, which is that I can pull them into pieces with either indexing or
521320	527720	slicing and know that they would go back together again. Now, there is a way around by asking for
527720	536840	one element slices instead of looking up integer indexes, but clearly it doesn't want me to treat
536840	543560	it like a string. So bytes objects, even if you learn some workarounds, are kind of an awkward fit
543560	550200	for many of the tasks they're called upon to do. They're kind of this hybrid type between a list of
550200	560440	numbers and a string. They're kind of in between, and they don't necessarily do either one perfectly
560440	567720	well. And so why do I bring all of that up? Why do I rehearse these well-known issues with Python
567800	573880	3 bytes? Which, by the way, are being taken care of. Python 2.5 will, for example, reintroduce
573880	581320	percent formatting for byte strings, because now that the experiment is in its fifth version,
581880	586840	I think it is beginning to become clear that the real problem in Python 2 wasn't that strings were
586840	592840	convenient, and so we ignore Unicode, it's that conversion could happen automatically without
592840	601400	warning. And so they are beginning to add power back into Python 3 byte strings, but they probably
601400	607400	will always be made of numbers now for backwards compatibility, and we bring all that up because
607400	614280	the byte array that I will now talk about is a mutable version of Python 3's byte string,
614840	622360	a mutable version of Python's most underpowered string type. So we'll just quickly look at
622360	629400	a few possible applications and whether a mutable vector of bytes is able to accomplish things
629400	637800	better or worse than traditional Python. So first, let's be fair to it. What if you actually want
637800	648680	a list of numbers between 0 and 255? That never happens to me. So I invented, in those rare cases
648680	654120	where you actually want to store bytes, if you had one, is the byte array a good choice? So I invented
654120	661800	one. I wrote my first Bloom filter as preparation for this talk. A Bloom filter is a way to, let's
661800	667640	say, you have a dictionary of words, and before you go look on disk for whether a word is in your
667640	674280	dictionary, you want a quick way to knock out a lot of words, it's just not being candidates.
675000	681000	What you can do is set up a big bit field and have a couple of hash functions that you throw the
681000	687800	word elephant at them, and they identify some bits for you that belong to elephant. You give them
687800	693880	the word Python. It's a different set of bits. The idea is that if you load up your dictionary
693880	699320	by setting all of the bits for elephant and all of the bits for Python, then when you see those
699320	704280	words later in a document, you can just check whether their bits are set to know if there's
704280	711160	any possibility that elephant is in your dictionary, because many words will have sets of bits that
711160	716120	aren't set at all, or several of which aren't set, and that you know could not have been in the
716120	721960	dictionary you loaded. This is a nice example because it lets us do a pure math operation,
722040	735400	in this case the in place oring of a byte in this array A with itself and with a bit that we create
735400	742280	over here and set, and we can run through, set this up, and then when we want to test a word,
742280	748680	go back in and use the reading version of square brackets, not in an assignment statement, but
748680	755000	in an expression to read back the value of a bit. A nice exercise to see how does this thing
755000	762360	perform storing and receiving a few tens of thousands of bytes. And by the way, the name A
762360	769160	in the previous code can be either an old fashioned array dot array that's been around in Python
769160	775080	forever, or a new fangled byte array. To this extent, they both provide the same interface. Each
775080	783800	slot you can address gets you or lets you store a byte. And so with this application, I ran it
783800	789960	both ways, and byte array scored its first victory, because it is so more specific than
789960	794840	array dot array, which can also hold, I believe, floats and integers and other things like that,
794840	799800	because the byte array's code path has almost no decisions, it's always going to store bytes,
800360	806760	it is more than 7% faster for running that bloom array code, bloom filter code that I just showed
806760	813880	you, than the old general purpose array, array object. So you might think that this is immediately
813880	821240	an obviously a go to data structure for lists of eight bit numbers. I tried it another way.
822200	827080	I want to know what's even faster than a byte array? A list of integers.
830680	838120	1% faster. If you just say, hey, Python, here's a one element list with a zero in it,
838120	844760	make me a lot of these. A plain, I'm sorry, 2%, a plain list of int objects that happen
844760	853000	to be in the range 0 to 255 will run even faster than the byte array. Why? Well, it's because,
853000	858600	think of what the byte array is doing. It's storing real bytes, low level in your computer,
858680	867720	that must each be translated into an int object address when the value is being handed out into
867720	876920	your Python code, and then when an int object is handed back, it has to be changed back into
876920	884280	a simple byte in order to be stored. The list skips all that, it just stores the addresses you
884280	893640	give it. The byte array, by the way, doesn't have to pay any penalty to allocate or create any of
893640	899320	those int objects, because it just so happens that the Python, the CPython interpreter, when it
899320	906120	starts up, preallocates all of the integer objects negative 5 through 256 so that they never have
906120	911320	to be created or destroyed over the lifetime of the interpreter. So when you ask the byte array,
911320	917160	what's it positioned 100, and it wants to say 70, it can just grab the existing 70 integer object
917160	922120	that always exists in memory and hand it back, so it's not having to go do a malloc or anything,
922840	927960	it's not having to allocate new memory, but still it's having to do that step of translation,
927960	934200	and it is honestly just simpler to store the pointer, to store the address of the 70 object.
934200	940360	That's why the list object runs faster, and so this is interesting. We have this new special case
940360	948680	container that's slightly slower than just using Python's well-honed default data types. A plain
948680	955160	old list is a faster bit vector than the fancy new byte array, except if you're using PyPy,
955800	960520	where they're all the same because it becomes the same C code under the hood of a machine code,
960520	966840	I should say, and all three run much faster as well as being exactly equivalent. I tried it out,
966840	970600	and PyPy in each case figured out I was trying to do exactly the same thing.
972120	978440	Well, I guess they're done already, I'll just keep going. So for this first experiment,
978440	985400	what if I need a list of integers between 0 and 255? My verdict is that the bit vector
985400	991560	is space efficient. You don't choose it because it's going to be obviously faster,
991560	995160	it's not, or obviously simpler, it's doing a little more under the hood,
995400	1005000	but the good old-fashioned list of integers has to store in each slot the address of the real
1005000	1013320	integer object 70. The bit vector just stores the seven bits, the eight bits that represent 70,
1013320	1018600	and therefore uses on a 64-bit machine, which is what I'll presume for all of these calculations,
1018680	1025160	eight times less space. And the point of a bloom filter is to save space in RAM.
1025720	1032920	That for bit operations is why you go to the byte array, because it stores bytes as honest to
1032920	1041800	goodness bytes with no extra overhead per byte. It's a great way to get numbers between 0 and 255
1041880	1048680	packed in the minimum space possible. So it is a win, but not in the way you might initially
1048680	1056200	expect. All right. Second, it is a reusable buffer. When you read a string in, you can't do
1056200	1062200	anything to it because it's immutable, but a byte array can be reused. For a quick benchmark,
1062200	1069640	I got a made a random file of a gigabyte of random data, read it with cat, so I was able to
1069720	1076120	estimate that probably Python won't be able to do better than 0.11 seconds on my machine
1076120	1081480	reading in the same data. I tried it with DD. Anyone here ever used DD to rewrite data?
1083000	1092360	It took six times longer. Anyone know why? I S traced them, and it's because of the block size.
1092360	1101080	DD alas is an old and crafty and low-level tool. While cat will zoom along with 128k blocks,
1101080	1107400	so it asks the OS for some data and gets 128,000 bytes in a single shot,
1108440	1114920	DD, because it's an old level for writing to ancient 70s block devices, reads and writes
1114920	1121400	512 bytes by default. Giving DD the same block size does make it perform the same. You can give
1121480	1128120	it a block size of 128k and get 0.11 seconds just right there with cat, but interestingly enough,
1128120	1132200	it's not the default, despite the fact that I seem to know all these people that think DD
1132200	1138840	would be faster somehow by default. It's not. It's the same reads and writes. And cat is the
1138840	1149320	Unix default. DD came from IBM. But this teaches us a first lesson that we will now apply. As we
1149320	1155240	look at Python IO, we need to keep block size in mind. The size of the chunks you read determines
1155240	1160200	how many chunks you need to read, determines how often you need to converse with the operating
1160200	1166520	system, which is often the expense that can come to dominate your runtime. Here's how we do it in
1166520	1173400	normal Python. Read a block and write the data back out. Note this is perfectly safe if an
1173400	1180120	undersized block comes in because the string that I'm here calling data that comes back is labeled
1180120	1188440	with its length. It could be 5 bytes, it could be 128k. Python strings know their length and so
1188440	1195960	right can just ask the length and send that many bytes of data back out. My first attempt at doing
1196040	1204600	a read into seemed to work at first until I noticed that every file I wrote, however big it was
1205320	1211320	originally, the copy that I made with this routine would always be a multiple of my block size.
1212280	1219640	Why is that? Because when I create one of these new byte arrays of, let's say 128k, what this
1219640	1226280	loop was doing is reading some number of bytes, who knows how many come in in the next block of
1226280	1234200	the file if I'm near the end, reads some number of bytes into my byte array, and then writes out
1234200	1239560	the whole byte array, including all the junk at the end that's maybe not part of the current block
1239560	1248520	of the file. I was doing my right of the whole 128k block without consulting the length to see
1248520	1259240	if I should have been writing the entire 128k block. One fix is to simply use slicing, is to get
1259240	1269080	that byte array called data and take from it to write each time the slice that is length long.
1269080	1275880	So if I get a full-sized block, I'm writing out all of the data, but if I get only half a block
1275880	1282760	at the end of the file, I only write that last half block out from the initial part of my byte
1282760	1290680	array. What if we didn't want the expense, though, of having to do that, back one slide, expensive
1290680	1298440	slicing operation, because asking a Python string, unicode string, or byte array for a slice creates
1298440	1305560	a whole new one and copies as much data into the new one as you ask for with the limits you set in
1305560	1312120	the slice. If we wanted to achieve zero copy, the people who added byte array to the language,
1312680	1319560	they thought of that as well. They added a second feature that works with byte array called a memory
1319560	1329400	view. A memory view is a sliceable object. Here I take the slice 3 colon 6 of that byte array that
1329480	1338680	I create up there. It's a slice which has no memory of its own but is letting you reach into the
1338680	1345480	memory it was sliced from to make the change. Essentially, this memory view, the V that I
1345480	1353480	create here, is just a, essentially, it's like a string object, but the addresses that it wants
1353480	1358920	to write to in memory are the addresses right there in the middle of the byte array. So when I try
1358920	1370920	to set its index zero value, it really goes to index 3 in the real byte array. When I set item 1,
1370920	1378200	it really goes to slot 4. When I set item 2, it goes to slot 5. It really is just creating an
1378200	1386600	object that acts like it's a little byte array but is, in fact, just an offset. It's adding
1386600	1394040	something to each index you use as you read and write from it. And this is what can help us in the
1394040	1402920	situation we're in. Here is a zero copy version of my fixed code to try to read in lots of blocks.
1402920	1409560	Before I was asking data, the byte array itself to do the slicing, and like all Python strings,
1409640	1417160	it gives me a whole new one when I ask for a slice. Now, I'm looking at it through a memory view.
1417160	1424440	So if I ask, let's say, for a view of, you know, if length is 128k and asking for all the data,
1424440	1430040	it just gives me a little memory view object whose addresses are pointing at the whole block of data.
1430040	1435880	A very cheap operation. And so memory views are often necessary to get any kind of performance
1435880	1441800	out of the byte array when doing IO, especially when you can't predict how big the next delivery
1441800	1450280	of information will be. Here are the runtimes of DD and cat that we discussed earlier. Compared to
1450280	1460040	just plain old read with normal Python strings, read into my first version that was broken because
1460120	1466200	it wasn't careful about how much it wrote does run slightly faster than the traditional Python
1466200	1474840	way of doing things. But when you then pivot to using a slice byte array and slicing it,
1474840	1479560	because you're copying every piece of data into memory twice, it's much more expensive,
1479560	1487640	it is the memory view. It is that zero expense, very little expense, constant time expense,
1487640	1494760	I should say, ability to slice without copying data that lets us create a correct version of a
1494760	1506360	file copy while still slightly beating read and write and traditional strings. So that was a lot
1506360	1515640	of work and we got a 4% speed up with byte array. Now, small blocks, things get worse for byte array
1515720	1521720	because what will slicing here so often do, it creates a new object every time and creating
1521720	1526920	one of these little view objects with its pointers into the part of the byte array I want to look at
1526920	1532360	was fine when I was only doing it every few hundred K of data, but what if I'm like the
1532360	1538200	defaults of DD and I'm going to be reading and writing 512 bytes at a time, what if I have to
1538280	1546520	spin up a new memory view for every half K of data? Then things start to look very bad and,
1546520	1552120	in fact, the memory view used correctly where you're careful of your lengths is simply a loss.
1553000	1561960	It's much, reading where it just returns a Python string is really efficient. A write of a Python
1561960	1568120	string is really efficient. You can easily get into situations with the fancy attempts one
1568120	1575080	makes with a byte array to create more expensive IO than you had when you just used traditional
1576200	1581960	immutable strings that, yes, required Python to build a new string for every call to read,
1581960	1590680	but cut out all of the rest of that expense. I was sad for the byte array at this point in my talk.
1592040	1604040	So I stared at the example. 20% slow down for a small block size, but then I thought of something.
1606120	1610920	What if we don't always slice? Because when reading from a file, different from a network,
1610920	1615240	when reading from a file, the normal case is that unless you're at the very end of the file,
1615240	1621320	you're going to get as much as you ask for. Ask for 128 K, you're going to get it. The normal case
1621320	1627800	is that the length equals the block size, and in that case, there's not only no reason to ask the
1627800	1633800	byte array to take a slice of itself and copy all that data, there's no reason to use the view to
1633800	1638520	limit the amount of data you're using from the block. You're going to use all of it.
1638520	1646360	And so if you handle that special case, you don't incur an object creation step in order to get that
1646360	1656440	right call started, and you slightly beat the performance of the traditional read write loop
1657160	1671560	by 4%. Just like for the big block case. So even if your I.O. is in a situation where the
1671640	1678600	block size will be varying or might be small, you can, if you're careful and cut and paste from
1678600	1686120	my talk slides, you can run slightly faster than the traditional read and write with immutable
1686120	1692360	strings. Python 2.7, by the way, has the same relative behaviors between those different choices
1692360	1698840	on my machine slightly slower. And I think the lesson here is that it is just hard to beat
1698840	1703400	old fashioned strings when you're pulling in data and then just immediately sending it back to
1703400	1712040	the operating system over some other channel. It's really something how the good old fashioned
1712040	1719640	immutable string that makes functional programmers' hearts sing is pretty much as good in this case
1719640	1727960	as our weird side effect idea of constantly modifying this single byte array that we have created.
1728680	1733960	So my verdict is that it is dangerous because it's so easy to write what looks like pretty code,
1733960	1738760	it looks like almost the same little read write loop, but is going to operate substantially
1738760	1744520	worse in situations that you might not think to test for unless you think of the small blocks case.
1745160	1753720	The one advantage it does offer is a great memory profile because there's a link later to a great
1753720	1758280	blog post online about someone writing an audio server that needed to keep lots of strings in
1758280	1763080	a buffer and his memory usage was going through the roof because if you're constantly allocating
1763080	1767480	and deallocating differently sized strings, because every call to read needs to make a new
1767480	1773720	string to hand it back to you, then you can get a lot of memory fragmentation. If instead you have
1773720	1779880	one byte array and you use it over and over and over and over, there's nothing happening to get
1779960	1785160	fragmented. So don't do the byte array, I wouldn't do the byte array for the 4% speedup.
1786760	1791720	I would do it because I wanted to control my memory profile, but only if I knew that was a
1791720	1798920	problem in my application domain. All right, now we go on to another and more interesting situation
1798920	1805000	using the byte array as the accumulator. Fun question for people doing new network programming,
1805000	1815960	how many bytes will receive 1024 return? The answer is one. Or more if the network stack
1815960	1822360	is in the mood, but you're only guaranteed one. And this is the opposite, a file IO. File IO,
1822360	1829000	you ask for 128K, if there's 128K left in the file, it will wait for the disk to spin, it will
1829000	1834600	wait for the head to be in the right place, it will leave you paused until a full 128K is ready
1834600	1840040	for delivery, and then wake you back up. The network is the opposite. Receive will block
1840040	1846760	only until at least a single byte is available and then set you off running to process it.
1846760	1853320	And that can happen if your buffer size happens to be just a little less than the size of the last
1853320	1859000	few packets that arrive. You can have a call to receive that finds only one or two bytes left,
1859640	1867720	meaning unlike in the case where we were choosing our read size for files, usually it's the network,
1867720	1874040	it's the clients you're communicating with that kind of decide how big your chunks of IOR when
1874040	1878120	you're talking on the network. So you're always potentially in the case where you're dealing
1878120	1884760	with little pieces of data. This fact, by the way, that you always are given an answer when
1884760	1891000	even just a few bytes can be sent or received is why new network programmers tend to get into the,
1891000	1896680	but it worked when I ran against local host problem. They get into that situation because
1896680	1900840	when you run your server that you've just written and your little client that you've just written
1900840	1906600	on local host, the OS will send enormous blocks of data back and forth between the two processes.
1908120	1912680	Then they'll take it to their team and say, look what I wrote, try it between two different
1912680	1917960	machines and it'll hang and never get all of the data because they didn't learn on local host
1917960	1923400	that you receive will often just give you a few thousand bytes and you need to keep at it
1923400	1930920	and watch until everything you need has arrived. So what is it like to use a traditional receive
1930920	1938360	solution getting a new string each time holding the new data that's come in? This is what it looks
1939080	1945560	like. Again, here we're getting lots of maybe little pieces of data which I'm simulating by
1945560	1952920	only asking for a single Ethernet packet length. So even when I run this on local host, it'll pretend
1952920	1961640	like packets are coming in. This is what many Python programmers start with. They just create
1961720	1969560	an empty string and they plus equal more data to it each time. In Python tutorials, many of
1969560	1974920	you will have seen this, the creating of the string and data plus equals more as an anti-pattern
1974920	1979720	that you avoid because I tried running this. How long does the plus equals approach take?
1980680	1986360	Infinity time. Meaning that I finally needed my laptop back so I killed it and I'll never
1986360	1990920	know how long the loop would have taken to read my gigabyte of data. But when you do plus equals,
1990920	1996200	you're asking Python to create a little string and then your first plus equals makes a slightly
1996200	2001400	longer one. Your next plus equals through the loop creates a slightly longer one into which
2001400	2006440	all the data from the second string has to be copied to make the third one. Then you go through
2006440	2012040	the loop again and now you have to copy all that data again to make your fourth string. And if you
2012040	2017400	have a million bytes to read, you wind up doing half a trillion operations. It's called an order
2017400	2022120	in squared algorithm, generally to be avoided if you wanted to finish by lunchtime.
2023960	2029880	So this is what we tell everyone to do. Pivot to keeping a list of blocks that you've received
2029880	2036120	and join them together at the end in a single step. Python's much better at that. This actually
2036120	2041000	finished on my laptop. It's the traditional way of accumulating data from the internet
2041000	2046920	in Python or from a network. Took about a little more than a second to read in a gig of data
2046920	2056680	in those small 1.5K chunks. Now, there is a version like read into, but that receives
2056680	2062120	into a byte array you've already built instead of building a new string, but it now runs into a
2062120	2068600	problem. When we do read into or receive into, where does it put the data? At the beginning of the
2068600	2073480	array. And all of our incoming blocks will overwrite each other. What we want as more and more
2073480	2079480	of blocks come in from the network is to arrange them along our byte array. So that memory view
2079480	2085400	slicing expense that I added in its statement to avoid whenever possible in the previous code,
2085400	2093240	it now becomes mandatory. Again, this ability with a view to write into byte locations that
2093240	2098120	aren't at the beginning, but are somewhere in the middle of the byte array that you've built.
2098120	2102520	The first block can go at the beginning, but you need to build a memory view to target the
2102600	2110840	second block after the first block, the third block after that, and so forth. And so you need
2110840	2118280	to build a memory view and you're going to need to use it to target that receive V into at subsequent
2119320	2125080	positions inside of your big byte array. I'm presuming that you know the content
2125080	2129320	like the head of time and have preallocated it and you're waiting to fill it with data.
2130280	2136280	This takes about eight tenths of a second because we a bit of a win here because you
2136280	2140760	haven't had to build a list, you haven't had to call join, you haven't built a bunch of intermediate
2140760	2146840	data structures. It actually is a significant win when you need to keep the data that you're
2146840	2153240	reading rather than just immediately passing it back to the OS. Another possibility I saw on
2153240	2158440	someone's blog is to do an old fashioned receive of a new string and then try to do a byte array
2158440	2165080	extend to grow your byte array with these new strings. It copies the data twice but does get
2165080	2172280	rid of that join concatenation. It looks something like this, data.extend down there near the bottom.
2173240	2181000	It is not a win over the normal way of using byte arrays because it turns out byte array extend
2181000	2189480	is pretty inefficient. It asks for an iterator over its argument and then calls the iterator's
2189480	2195320	next function over and over for every byte and then asks the int object what its value is
2196200	2202440	so that it can then put it in an intermediate array and that involves having to increment and
2202440	2207160	decrement the reference pointer of the integer it's given and by the time you're done you can
2207240	2212600	compute that you've done at least 40 bytes of bandwidth to RAM even ignoring the instructions
2212600	2219640	and stacks and arguments that are passing in order to get that single byte value extended onto the
2219640	2227800	end of your byte array plus it doesn't write to your byte array. It writes to an intermediate
2227800	2232280	buffer that it grows dynamically and then does the append when it's done so that should that
2232360	2238280	iteration die part way through you don't wind up having modified the byte array some it wants to
2238280	2244680	either succeed or fail as an atomic operation so that's why it's kind of slow kind of klugey
2245640	2251640	but seeing that blog post made me ask a question does the byte array have an append operation
2251640	2257240	that's any good? I mean surely the people writing it knew that we'd want to do that without spinning
2257240	2265000	up an iterator and calling it 1500 times does it have an operation that's really good and yes it
2265000	2273080	does I read the c code to find it now think about it where would you put the real extend operator
2273080	2279880	the real ability to make your byte array longer obviously you'd hide it behind the operator
2279880	2284920	that we've spent 20 years telling people to never use with string values
2288120	2296600	this might be so difficult that some of you will never do it but if you can convince yourself to
2296600	2304040	type this after all of this time this is actually something that byte arrays do magnificently just
2304040	2312360	plus equal the additional data to your byte array and you will be even receive into's ability
2312360	2319720	to grow your array with data so this case where we need to accumulate and keep the whole pay road
2319720	2325240	is a real win for the byte array in all of the approaches and there don't seem to be sharp edges
2325240	2332760	that can suddenly make it behave much worse than the the list and joint 33 speed up in that last
2332760	2339800	version of the algorithm and cleaner code I mean admit it you've always wanted to just plus equal
2339800	2344840	haven't you it's the natural way to write it in python and this is one of those neat intersections
2344840	2353240	of the fast way with the way that looks good on the page as well so I'm not going to talk about
2353240	2357720	send you might think I'll now get into the fact that send doesn't always send the whole payload
2357720	2363720	but python has long had you covered here python sockets have for a very long time had a send
2363720	2371080	all that sits in a loop in c sending shorter and shorter tails of your data until finally the os
2372680	2377560	buffers have been able to receive it all so I don't see that we need byte arrays for that
2377560	2384600	and I can declare the byte array the winner if you need an accumulator if you need to very quickly
2384600	2391400	in a performance sensitive environment accumulate a lot of incoming data it is a noticeably good
2391400	2397160	win with two different techniques that work pretty well I'll briefly mention that some people want
2397160	2402600	a freestyle mutable string when they see the byte array they don't think of io they don't think of
2402600	2409480	bloom filters and bit vectors they want to mess with a string they want a string that they can just
2409480	2417400	change and all ish I have not found yet a good use for this and I'll sort of show you why it
2417400	2424120	winds up being awkward you want to change part of a payload before using storing or retransmitting
2424120	2427800	that would be the use case here because if you want to lowercase the whole thing you have to
2427800	2432600	touch all the bytes anyway so you might as well build a new one good thing is that the byte array
2432600	2438600	is mutable you can get it and change it but none of the methods that it shares with strings
2439640	2446920	do mutation to it if you call dot upper on your byte array you get a new byte array so you have
2446920	2454760	a mutable string type that does nothing string like mutably a byte array only changes when subjected
2454760	2461880	to list like operations like assignment to an index or assignment to a slice or dot clear
2461880	2466920	and so the result if you try writing a network algorithm or something with this is curious
2466920	2473640	you have a mutable string that alas does no mutation precisely when you start calling
2473640	2480600	its string methods want to lowercase a word well you're going to have to make a copy to call lower
2480600	2486440	on you're going to have to do slicing giving you a copy call lower making another copy and then
2486440	2493240	assignment to copy it a third time back into the data structure in order to get that accomplished
2493240	2499800	there isn't I looked there isn't a lower into and an upper into that would let you do smaller
2499800	2505720	grained uh manipulations that wrote directly to your new byte array can the memory view save us
2505720	2515560	though it did in all of the previous occasions no because memory views don't do anything string
2515560	2521560	like the moment you move to a memory view which lets you look at a piece of string efficiently
2522520	2527240	you're not going to be able to do anything string like to it so you have to do a round
2527240	2536360	trip out to a smaller string to do a manipulation and store data back to mutate a byte array without
2536360	2542040	rewriting its whole content you're going to need indexes do you remember indexes back the first
2542040	2548600	week before you'd found split strip and join and we're like doing everything like this you get to
2548600	2554280	do it again if you decide to try mutating a byte array the byte array will let you enjoy those
2554280	2560920	days all over again because all the mutation operations are powered only by indexes one hint
2560920	2567400	regular expressions while they turned off a lot of other string like things were left turned on
2567400	2574120	and do work against byte arrays and can help give you some useful indexes to use freestyle
2574120	2581720	mutable string it's awkward I have here at the end of the slides some uh links and pointers to other
2581720	2586920	documentation including the blog posts that inspired this talk and made me want to bring
2586920	2593720	everything together in one place in conclusion the byte array it is a very memory efficient
2593720	2601000	not faster but memory efficient store of byte integers should you ever need them it can help
2601000	2606680	control memory fragmentation when doing uh high-performance i o because you don't have to create
2606680	2613240	a new string but it's hard to make it faster in a reliable way be careful it's a great way to
2613240	2618840	accumulate data that's coming in a piece at a time that's its real superpower and though it's
2618840	2626040	very tempting for string operations it's also a bit underpowered and a bit awkward that's what
2626040	2644840	I've learned so far thank you very much we are out of time so I will meet interested byte array
2644840	2657640	fans outside the door in a few minutes
