WEBVTT

00:00.000 --> 00:16.000
Alright. Hello, everybody. I have quite an honor today. I am lucky enough to introduce

00:16.000 --> 00:22.400
Brandon Rhodes. He's a longtime Python developer, maintainer of several amateur astronomy libraries

00:22.400 --> 00:28.240
and a developer for Dropbox. He's also a Quizmaster extraordinaire, as some people got to find

00:28.240 --> 00:34.000
out last night. Today, he will be presenting on, oh, come on, who needs byte arrays? Without

00:34.000 --> 00:45.120
further ado, Brandon Rhodes.

00:45.120 --> 00:50.600
Welcome, everybody. I hope you're having a good time at PyCon and in Montreal. My topic

00:50.600 --> 00:59.080
today is indeed byte arrays. A very interesting recent addition to the Python ecosystem. Because

00:59.080 --> 01:04.280
in Python, normal string objects, the ones that we're accustomed to dealing with are

01:04.280 --> 01:14.200
immutable. They can't be changed or modified. This is true of the types available under

01:14.200 --> 01:24.120
both two and three. The original string type, STR, was renamed to bytes because of its low-level

01:24.120 --> 01:31.800
nature and then that synonym was also backported to Python 2. We will not talk much today about

01:31.800 --> 01:42.920
the newer Unicode byte strings that were renamed to the official string type of Python 3. We're

01:42.920 --> 01:49.400
going to be talking about those lower-level ones. The strings that don't pretend they have symbols

01:49.400 --> 02:01.640
inside of them so much as they know that their innards are really bytes. 8-bit codes, 0-255.

02:01.640 --> 02:09.960
For the most part, I will put that little B in front of the bytes strings that I type. It is

02:10.040 --> 02:17.560
optional under Python 2, but it becomes mandatory under Python 3. So if I write my strings like

02:17.560 --> 02:24.760
this, then it will work wherever you try this later if you want to see the examples run. Strings

02:24.760 --> 02:30.360
are immutable. What does that mean? It means that when you call methods, when you do things to them,

02:30.360 --> 02:36.600
the original object itself doesn't change. You get given a new object as the return value of the

02:36.600 --> 02:42.760
method you call. So if .lower returns to you a new string, you can still peek back and see that

02:42.760 --> 02:48.360
the original is untouched and unchanged. If you run split, you'll see that both of the objects

02:48.360 --> 02:56.040
you've been given are new string objects. The original is still unchanged. They do not allow

02:56.040 --> 03:01.240
assignment because that would make them change. It would make them mutate as the technical term

03:01.320 --> 03:08.280
in computer science. Now, immutability has led a long and very successful career in Python

03:08.280 --> 03:15.160
because it makes things simple. You don't pass a name to a function only to finally

03:15.160 --> 03:20.760
suddenly discover it's another name when it comes back. You don't pass a string or a block to

03:20.760 --> 03:26.200
someone, and suddenly it's a different string or block. It's a very, very simple model that if

03:26.200 --> 03:32.040
you have the word Python, you know it will always remain so. And it actually is one of the most

03:32.040 --> 03:36.760
parts of Python. That's the most functional programming languages where new results are

03:36.760 --> 03:44.120
returned instead of being written onto data structures you already hold. The string types

03:44.120 --> 03:51.480
are a primary example of that. But it sometimes is a little expensive in terms of allocation.

03:51.480 --> 03:56.920
Every time you want to make any little tweak to a string, it has to allocate a new one for you.

03:56.920 --> 04:04.200
That means a lot of data gets copied back and forth into new areas of memory. Not everybody

04:04.200 --> 04:11.400
is happy about that. And so Python 3 introduced and then it was back ported to Python 2, 6,

04:11.400 --> 04:18.520
and 7, the new byte array. It's a built-in. Python 2, 7, Python 3, you can just type byte array

04:18.520 --> 04:25.640
and you'll get access to that type, just like you can with stir and int and list. A byte array

04:26.440 --> 04:36.120
is a mutable string that is based, this is interesting, on Python 3's byte string, not the

04:36.120 --> 04:43.800
old stir string from Python 2. And that's an interesting problem, that it's based on Python

04:43.800 --> 04:54.600
3's string, byte string type, because honestly, the Python 3 bytes type is designed to be awkward

04:54.600 --> 05:04.280
for string operations. Why? So you will want to be a good person and run the code before

05:04.280 --> 05:12.200
treating your data as characters. And this has led to Python 3 programmers tending to write code

05:12.280 --> 05:17.800
that is from the ground up prepared for internationalization and different alphabets because they

05:17.800 --> 05:23.960
think about the issue of decoding on the way in and encoding on the way back out because they

05:23.960 --> 05:30.600
have to. But we'll see it leads to some interesting behaviors. Just professionally, always be aware

05:30.600 --> 05:39.400
of using string types that wish you weren't using them. In Python 2, let's compare. We can build a

05:39.480 --> 05:45.880
string, we can ask its length, we can split it into pieces. Python 3's byte type, there's little

05:45.880 --> 05:51.000
B characters hanging out in front of our byte strings, but we can take the length, we can call

05:51.000 --> 05:57.960
a method like split. In Python 2, we can use those square brackets with a colon in order to do

05:57.960 --> 06:04.680
slicing and get back a copy of the inside of the string. The exact same thing works exactly the

06:04.680 --> 06:12.600
same way in Python 3. In Python 2, it's always a custom in Python that if something has a length,

06:12.600 --> 06:18.200
it should allow itself to be iterated or looped over. In Python 2, if you try looping over a

06:18.200 --> 06:26.280
string, you get out smaller strings, one character strings that are inside. What happens if you

06:27.080 --> 06:39.720
press enter for this line of code in Python 3? You get a syntax error. Syntax error, you failed to

06:39.720 --> 06:50.280
pay the Python 3 per intax. Python 3, it's kind of like an old text-based adventure game where

06:50.280 --> 06:55.560
you can tell the writer just threw an extra obstacle in your way because a room needed to

06:55.560 --> 07:13.320
be a little more complicated. You know, in seriousness, I've known Ruby, some Ruby programmers,

07:13.320 --> 07:17.880
this is actually a big debate in the Ruby community, Ruby makes parenthesis optional

07:17.880 --> 07:23.720
when calling a function. The Rubyists who never use the parenthesis always tell me they could

07:23.720 --> 07:30.040
never stand Python again. I always thought that was the oddest thing. Doesn't everyone want to

07:30.040 --> 07:36.120
type parenthesis whenever they ask their computer to do something? I must admit that once it was

07:36.120 --> 07:45.000
me who was suddenly having to type parenthesis, but they surely must still be wrong. Anyway.

07:45.320 --> 07:54.520
I did a conservative estimate of how much typing is cost me by these print statements in Python 3,

07:54.520 --> 08:02.760
and these are conservative estimates, and it's coming out to quite a bit of typing per year.

08:02.760 --> 08:09.880
I'm having to face going into Perendet just to write some Emax Lisp next week. Anyway, Python 3

08:09.880 --> 08:14.600
wants them for this print statement, and what's another two perends when I've already typed so

08:14.600 --> 08:26.440
many? So once you get the print statement working, you're in for another surprise. Python 3 bytes type

08:27.240 --> 08:36.120
is not made of characters, it is made of numbers. This breaks a very important contract that for

08:36.120 --> 08:41.320
me existed with strings, which is that I can pull them into pieces with either indexing or

08:41.320 --> 08:47.720
slicing and know that they would go back together again. Now, there is a way around by asking for

08:47.720 --> 08:56.840
one element slices instead of looking up integer indexes, but clearly it doesn't want me to treat

08:56.840 --> 09:03.560
it like a string. So bytes objects, even if you learn some workarounds, are kind of an awkward fit

09:03.560 --> 09:10.200
for many of the tasks they're called upon to do. They're kind of this hybrid type between a list of

09:10.200 --> 09:20.440
numbers and a string. They're kind of in between, and they don't necessarily do either one perfectly

09:20.440 --> 09:27.720
well. And so why do I bring all of that up? Why do I rehearse these well-known issues with Python

09:27.800 --> 09:33.880
3 bytes? Which, by the way, are being taken care of. Python 2.5 will, for example, reintroduce

09:33.880 --> 09:41.320
percent formatting for byte strings, because now that the experiment is in its fifth version,

09:41.880 --> 09:46.840
I think it is beginning to become clear that the real problem in Python 2 wasn't that strings were

09:46.840 --> 09:52.840
convenient, and so we ignore Unicode, it's that conversion could happen automatically without

09:52.840 --> 10:01.400
warning. And so they are beginning to add power back into Python 3 byte strings, but they probably

10:01.400 --> 10:07.400
will always be made of numbers now for backwards compatibility, and we bring all that up because

10:07.400 --> 10:14.280
the byte array that I will now talk about is a mutable version of Python 3's byte string,

10:14.840 --> 10:22.360
a mutable version of Python's most underpowered string type. So we'll just quickly look at

10:22.360 --> 10:29.400
a few possible applications and whether a mutable vector of bytes is able to accomplish things

10:29.400 --> 10:37.800
better or worse than traditional Python. So first, let's be fair to it. What if you actually want

10:37.800 --> 10:48.680
a list of numbers between 0 and 255? That never happens to me. So I invented, in those rare cases

10:48.680 --> 10:54.120
where you actually want to store bytes, if you had one, is the byte array a good choice? So I invented

10:54.120 --> 11:01.800
one. I wrote my first Bloom filter as preparation for this talk. A Bloom filter is a way to, let's

11:01.800 --> 11:07.640
say, you have a dictionary of words, and before you go look on disk for whether a word is in your

11:07.640 --> 11:14.280
dictionary, you want a quick way to knock out a lot of words, it's just not being candidates.

11:15.000 --> 11:21.000
What you can do is set up a big bit field and have a couple of hash functions that you throw the

11:21.000 --> 11:27.800
word elephant at them, and they identify some bits for you that belong to elephant. You give them

11:27.800 --> 11:33.880
the word Python. It's a different set of bits. The idea is that if you load up your dictionary

11:33.880 --> 11:39.320
by setting all of the bits for elephant and all of the bits for Python, then when you see those

11:39.320 --> 11:44.280
words later in a document, you can just check whether their bits are set to know if there's

11:44.280 --> 11:51.160
any possibility that elephant is in your dictionary, because many words will have sets of bits that

11:51.160 --> 11:56.120
aren't set at all, or several of which aren't set, and that you know could not have been in the

11:56.120 --> 12:01.960
dictionary you loaded. This is a nice example because it lets us do a pure math operation,

12:02.040 --> 12:15.400
in this case the in place oring of a byte in this array A with itself and with a bit that we create

12:15.400 --> 12:22.280
over here and set, and we can run through, set this up, and then when we want to test a word,

12:22.280 --> 12:28.680
go back in and use the reading version of square brackets, not in an assignment statement, but

12:28.680 --> 12:35.000
in an expression to read back the value of a bit. A nice exercise to see how does this thing

12:35.000 --> 12:42.360
perform storing and receiving a few tens of thousands of bytes. And by the way, the name A

12:42.360 --> 12:49.160
in the previous code can be either an old fashioned array dot array that's been around in Python

12:49.160 --> 12:55.080
forever, or a new fangled byte array. To this extent, they both provide the same interface. Each

12:55.080 --> 13:03.800
slot you can address gets you or lets you store a byte. And so with this application, I ran it

13:03.800 --> 13:09.960
both ways, and byte array scored its first victory, because it is so more specific than

13:09.960 --> 13:14.840
array dot array, which can also hold, I believe, floats and integers and other things like that,

13:14.840 --> 13:19.800
because the byte array's code path has almost no decisions, it's always going to store bytes,

13:20.360 --> 13:26.760
it is more than 7% faster for running that bloom array code, bloom filter code that I just showed

13:26.760 --> 13:33.880
you, than the old general purpose array, array object. So you might think that this is immediately

13:33.880 --> 13:41.240
an obviously a go to data structure for lists of eight bit numbers. I tried it another way.

13:42.200 --> 13:47.080
I want to know what's even faster than a byte array? A list of integers.

13:50.680 --> 13:58.120
1% faster. If you just say, hey, Python, here's a one element list with a zero in it,

13:58.120 --> 14:04.760
make me a lot of these. A plain, I'm sorry, 2%, a plain list of int objects that happen

14:04.760 --> 14:13.000
to be in the range 0 to 255 will run even faster than the byte array. Why? Well, it's because,

14:13.000 --> 14:18.600
think of what the byte array is doing. It's storing real bytes, low level in your computer,

14:18.680 --> 14:27.720
that must each be translated into an int object address when the value is being handed out into

14:27.720 --> 14:36.920
your Python code, and then when an int object is handed back, it has to be changed back into

14:36.920 --> 14:44.280
a simple byte in order to be stored. The list skips all that, it just stores the addresses you

14:44.280 --> 14:53.640
give it. The byte array, by the way, doesn't have to pay any penalty to allocate or create any of

14:53.640 --> 14:59.320
those int objects, because it just so happens that the Python, the CPython interpreter, when it

14:59.320 --> 15:06.120
starts up, preallocates all of the integer objects negative 5 through 256 so that they never have

15:06.120 --> 15:11.320
to be created or destroyed over the lifetime of the interpreter. So when you ask the byte array,

15:11.320 --> 15:17.160
what's it positioned 100, and it wants to say 70, it can just grab the existing 70 integer object

15:17.160 --> 15:22.120
that always exists in memory and hand it back, so it's not having to go do a malloc or anything,

15:22.840 --> 15:27.960
it's not having to allocate new memory, but still it's having to do that step of translation,

15:27.960 --> 15:34.200
and it is honestly just simpler to store the pointer, to store the address of the 70 object.

15:34.200 --> 15:40.360
That's why the list object runs faster, and so this is interesting. We have this new special case

15:40.360 --> 15:48.680
container that's slightly slower than just using Python's well-honed default data types. A plain

15:48.680 --> 15:55.160
old list is a faster bit vector than the fancy new byte array, except if you're using PyPy,

15:55.800 --> 16:00.520
where they're all the same because it becomes the same C code under the hood of a machine code,

16:00.520 --> 16:06.840
I should say, and all three run much faster as well as being exactly equivalent. I tried it out,

16:06.840 --> 16:10.600
and PyPy in each case figured out I was trying to do exactly the same thing.

16:12.120 --> 16:18.440
Well, I guess they're done already, I'll just keep going. So for this first experiment,

16:18.440 --> 16:25.400
what if I need a list of integers between 0 and 255? My verdict is that the bit vector

16:25.400 --> 16:31.560
is space efficient. You don't choose it because it's going to be obviously faster,

16:31.560 --> 16:35.160
it's not, or obviously simpler, it's doing a little more under the hood,

16:35.400 --> 16:45.000
but the good old-fashioned list of integers has to store in each slot the address of the real

16:45.000 --> 16:53.320
integer object 70. The bit vector just stores the seven bits, the eight bits that represent 70,

16:53.320 --> 16:58.600
and therefore uses on a 64-bit machine, which is what I'll presume for all of these calculations,

16:58.680 --> 17:05.160
eight times less space. And the point of a bloom filter is to save space in RAM.

17:05.720 --> 17:12.920
That for bit operations is why you go to the byte array, because it stores bytes as honest to

17:12.920 --> 17:21.800
goodness bytes with no extra overhead per byte. It's a great way to get numbers between 0 and 255

17:21.880 --> 17:28.680
packed in the minimum space possible. So it is a win, but not in the way you might initially

17:28.680 --> 17:36.200
expect. All right. Second, it is a reusable buffer. When you read a string in, you can't do

17:36.200 --> 17:42.200
anything to it because it's immutable, but a byte array can be reused. For a quick benchmark,

17:42.200 --> 17:49.640
I got a made a random file of a gigabyte of random data, read it with cat, so I was able to

17:49.720 --> 17:56.120
estimate that probably Python won't be able to do better than 0.11 seconds on my machine

17:56.120 --> 18:01.480
reading in the same data. I tried it with DD. Anyone here ever used DD to rewrite data?

18:03.000 --> 18:12.360
It took six times longer. Anyone know why? I S traced them, and it's because of the block size.

18:12.360 --> 18:21.080
DD alas is an old and crafty and low-level tool. While cat will zoom along with 128k blocks,

18:21.080 --> 18:27.400
so it asks the OS for some data and gets 128,000 bytes in a single shot,

18:28.440 --> 18:34.920
DD, because it's an old level for writing to ancient 70s block devices, reads and writes

18:34.920 --> 18:41.400
512 bytes by default. Giving DD the same block size does make it perform the same. You can give

18:41.480 --> 18:48.120
it a block size of 128k and get 0.11 seconds just right there with cat, but interestingly enough,

18:48.120 --> 18:52.200
it's not the default, despite the fact that I seem to know all these people that think DD

18:52.200 --> 18:58.840
would be faster somehow by default. It's not. It's the same reads and writes. And cat is the

18:58.840 --> 19:09.320
Unix default. DD came from IBM. But this teaches us a first lesson that we will now apply. As we

19:09.320 --> 19:15.240
look at Python IO, we need to keep block size in mind. The size of the chunks you read determines

19:15.240 --> 19:20.200
how many chunks you need to read, determines how often you need to converse with the operating

19:20.200 --> 19:26.520
system, which is often the expense that can come to dominate your runtime. Here's how we do it in

19:26.520 --> 19:33.400
normal Python. Read a block and write the data back out. Note this is perfectly safe if an

19:33.400 --> 19:40.120
undersized block comes in because the string that I'm here calling data that comes back is labeled

19:40.120 --> 19:48.440
with its length. It could be 5 bytes, it could be 128k. Python strings know their length and so

19:48.440 --> 19:55.960
right can just ask the length and send that many bytes of data back out. My first attempt at doing

19:56.040 --> 20:04.600
a read into seemed to work at first until I noticed that every file I wrote, however big it was

20:05.320 --> 20:11.320
originally, the copy that I made with this routine would always be a multiple of my block size.

20:12.280 --> 20:19.640
Why is that? Because when I create one of these new byte arrays of, let's say 128k, what this

20:19.640 --> 20:26.280
loop was doing is reading some number of bytes, who knows how many come in in the next block of

20:26.280 --> 20:34.200
the file if I'm near the end, reads some number of bytes into my byte array, and then writes out

20:34.200 --> 20:39.560
the whole byte array, including all the junk at the end that's maybe not part of the current block

20:39.560 --> 20:48.520
of the file. I was doing my right of the whole 128k block without consulting the length to see

20:48.520 --> 20:59.240
if I should have been writing the entire 128k block. One fix is to simply use slicing, is to get

20:59.240 --> 21:09.080
that byte array called data and take from it to write each time the slice that is length long.

21:09.080 --> 21:15.880
So if I get a full-sized block, I'm writing out all of the data, but if I get only half a block

21:15.880 --> 21:22.760
at the end of the file, I only write that last half block out from the initial part of my byte

21:22.760 --> 21:30.680
array. What if we didn't want the expense, though, of having to do that, back one slide, expensive

21:30.680 --> 21:38.440
slicing operation, because asking a Python string, unicode string, or byte array for a slice creates

21:38.440 --> 21:45.560
a whole new one and copies as much data into the new one as you ask for with the limits you set in

21:45.560 --> 21:52.120
the slice. If we wanted to achieve zero copy, the people who added byte array to the language,

21:52.680 --> 21:59.560
they thought of that as well. They added a second feature that works with byte array called a memory

21:59.560 --> 22:09.400
view. A memory view is a sliceable object. Here I take the slice 3 colon 6 of that byte array that

22:09.480 --> 22:18.680
I create up there. It's a slice which has no memory of its own but is letting you reach into the

22:18.680 --> 22:25.480
memory it was sliced from to make the change. Essentially, this memory view, the V that I

22:25.480 --> 22:33.480
create here, is just a, essentially, it's like a string object, but the addresses that it wants

22:33.480 --> 22:38.920
to write to in memory are the addresses right there in the middle of the byte array. So when I try

22:38.920 --> 22:50.920
to set its index zero value, it really goes to index 3 in the real byte array. When I set item 1,

22:50.920 --> 22:58.200
it really goes to slot 4. When I set item 2, it goes to slot 5. It really is just creating an

22:58.200 --> 23:06.600
object that acts like it's a little byte array but is, in fact, just an offset. It's adding

23:06.600 --> 23:14.040
something to each index you use as you read and write from it. And this is what can help us in the

23:14.040 --> 23:22.920
situation we're in. Here is a zero copy version of my fixed code to try to read in lots of blocks.

23:22.920 --> 23:29.560
Before I was asking data, the byte array itself to do the slicing, and like all Python strings,

23:29.640 --> 23:37.160
it gives me a whole new one when I ask for a slice. Now, I'm looking at it through a memory view.

23:37.160 --> 23:44.440
So if I ask, let's say, for a view of, you know, if length is 128k and asking for all the data,

23:44.440 --> 23:50.040
it just gives me a little memory view object whose addresses are pointing at the whole block of data.

23:50.040 --> 23:55.880
A very cheap operation. And so memory views are often necessary to get any kind of performance

23:55.880 --> 24:01.800
out of the byte array when doing IO, especially when you can't predict how big the next delivery

24:01.800 --> 24:10.280
of information will be. Here are the runtimes of DD and cat that we discussed earlier. Compared to

24:10.280 --> 24:20.040
just plain old read with normal Python strings, read into my first version that was broken because

24:20.120 --> 24:26.200
it wasn't careful about how much it wrote does run slightly faster than the traditional Python

24:26.200 --> 24:34.840
way of doing things. But when you then pivot to using a slice byte array and slicing it,

24:34.840 --> 24:39.560
because you're copying every piece of data into memory twice, it's much more expensive,

24:39.560 --> 24:47.640
it is the memory view. It is that zero expense, very little expense, constant time expense,

24:47.640 --> 24:54.760
I should say, ability to slice without copying data that lets us create a correct version of a

24:54.760 --> 25:06.360
file copy while still slightly beating read and write and traditional strings. So that was a lot

25:06.360 --> 25:15.640
of work and we got a 4% speed up with byte array. Now, small blocks, things get worse for byte array

25:15.720 --> 25:21.720
because what will slicing here so often do, it creates a new object every time and creating

25:21.720 --> 25:26.920
one of these little view objects with its pointers into the part of the byte array I want to look at

25:26.920 --> 25:32.360
was fine when I was only doing it every few hundred K of data, but what if I'm like the

25:32.360 --> 25:38.200
defaults of DD and I'm going to be reading and writing 512 bytes at a time, what if I have to

25:38.280 --> 25:46.520
spin up a new memory view for every half K of data? Then things start to look very bad and,

25:46.520 --> 25:52.120
in fact, the memory view used correctly where you're careful of your lengths is simply a loss.

25:53.000 --> 26:01.960
It's much, reading where it just returns a Python string is really efficient. A write of a Python

26:01.960 --> 26:08.120
string is really efficient. You can easily get into situations with the fancy attempts one

26:08.120 --> 26:15.080
makes with a byte array to create more expensive IO than you had when you just used traditional

26:16.200 --> 26:21.960
immutable strings that, yes, required Python to build a new string for every call to read,

26:21.960 --> 26:30.680
but cut out all of the rest of that expense. I was sad for the byte array at this point in my talk.

26:32.040 --> 26:44.040
So I stared at the example. 20% slow down for a small block size, but then I thought of something.

26:46.120 --> 26:50.920
What if we don't always slice? Because when reading from a file, different from a network,

26:50.920 --> 26:55.240
when reading from a file, the normal case is that unless you're at the very end of the file,

26:55.240 --> 27:01.320
you're going to get as much as you ask for. Ask for 128 K, you're going to get it. The normal case

27:01.320 --> 27:07.800
is that the length equals the block size, and in that case, there's not only no reason to ask the

27:07.800 --> 27:13.800
byte array to take a slice of itself and copy all that data, there's no reason to use the view to

27:13.800 --> 27:18.520
limit the amount of data you're using from the block. You're going to use all of it.

27:18.520 --> 27:26.360
And so if you handle that special case, you don't incur an object creation step in order to get that

27:26.360 --> 27:36.440
right call started, and you slightly beat the performance of the traditional read write loop

27:37.160 --> 27:51.560
by 4%. Just like for the big block case. So even if your I.O. is in a situation where the

27:51.640 --> 27:58.600
block size will be varying or might be small, you can, if you're careful and cut and paste from

27:58.600 --> 28:06.120
my talk slides, you can run slightly faster than the traditional read and write with immutable

28:06.120 --> 28:12.360
strings. Python 2.7, by the way, has the same relative behaviors between those different choices

28:12.360 --> 28:18.840
on my machine slightly slower. And I think the lesson here is that it is just hard to beat

28:18.840 --> 28:23.400
old fashioned strings when you're pulling in data and then just immediately sending it back to

28:23.400 --> 28:32.040
the operating system over some other channel. It's really something how the good old fashioned

28:32.040 --> 28:39.640
immutable string that makes functional programmers' hearts sing is pretty much as good in this case

28:39.640 --> 28:47.960
as our weird side effect idea of constantly modifying this single byte array that we have created.

28:48.680 --> 28:53.960
So my verdict is that it is dangerous because it's so easy to write what looks like pretty code,

28:53.960 --> 28:58.760
it looks like almost the same little read write loop, but is going to operate substantially

28:58.760 --> 29:04.520
worse in situations that you might not think to test for unless you think of the small blocks case.

29:05.160 --> 29:13.720
The one advantage it does offer is a great memory profile because there's a link later to a great

29:13.720 --> 29:18.280
blog post online about someone writing an audio server that needed to keep lots of strings in

29:18.280 --> 29:23.080
a buffer and his memory usage was going through the roof because if you're constantly allocating

29:23.080 --> 29:27.480
and deallocating differently sized strings, because every call to read needs to make a new

29:27.480 --> 29:33.720
string to hand it back to you, then you can get a lot of memory fragmentation. If instead you have

29:33.720 --> 29:39.880
one byte array and you use it over and over and over and over, there's nothing happening to get

29:39.960 --> 29:45.160
fragmented. So don't do the byte array, I wouldn't do the byte array for the 4% speedup.

29:46.760 --> 29:51.720
I would do it because I wanted to control my memory profile, but only if I knew that was a

29:51.720 --> 29:58.920
problem in my application domain. All right, now we go on to another and more interesting situation

29:58.920 --> 30:05.000
using the byte array as the accumulator. Fun question for people doing new network programming,

30:05.000 --> 30:15.960
how many bytes will receive 1024 return? The answer is one. Or more if the network stack

30:15.960 --> 30:22.360
is in the mood, but you're only guaranteed one. And this is the opposite, a file IO. File IO,

30:22.360 --> 30:29.000
you ask for 128K, if there's 128K left in the file, it will wait for the disk to spin, it will

30:29.000 --> 30:34.600
wait for the head to be in the right place, it will leave you paused until a full 128K is ready

30:34.600 --> 30:40.040
for delivery, and then wake you back up. The network is the opposite. Receive will block

30:40.040 --> 30:46.760
only until at least a single byte is available and then set you off running to process it.

30:46.760 --> 30:53.320
And that can happen if your buffer size happens to be just a little less than the size of the last

30:53.320 --> 30:59.000
few packets that arrive. You can have a call to receive that finds only one or two bytes left,

30:59.640 --> 31:07.720
meaning unlike in the case where we were choosing our read size for files, usually it's the network,

31:07.720 --> 31:14.040
it's the clients you're communicating with that kind of decide how big your chunks of IOR when

31:14.040 --> 31:18.120
you're talking on the network. So you're always potentially in the case where you're dealing

31:18.120 --> 31:24.760
with little pieces of data. This fact, by the way, that you always are given an answer when

31:24.760 --> 31:31.000
even just a few bytes can be sent or received is why new network programmers tend to get into the,

31:31.000 --> 31:36.680
but it worked when I ran against local host problem. They get into that situation because

31:36.680 --> 31:40.840
when you run your server that you've just written and your little client that you've just written

31:40.840 --> 31:46.600
on local host, the OS will send enormous blocks of data back and forth between the two processes.

31:48.120 --> 31:52.680
Then they'll take it to their team and say, look what I wrote, try it between two different

31:52.680 --> 31:57.960
machines and it'll hang and never get all of the data because they didn't learn on local host

31:57.960 --> 32:03.400
that you receive will often just give you a few thousand bytes and you need to keep at it

32:03.400 --> 32:10.920
and watch until everything you need has arrived. So what is it like to use a traditional receive

32:10.920 --> 32:18.360
solution getting a new string each time holding the new data that's come in? This is what it looks

32:19.080 --> 32:25.560
like. Again, here we're getting lots of maybe little pieces of data which I'm simulating by

32:25.560 --> 32:32.920
only asking for a single Ethernet packet length. So even when I run this on local host, it'll pretend

32:32.920 --> 32:41.640
like packets are coming in. This is what many Python programmers start with. They just create

32:41.720 --> 32:49.560
an empty string and they plus equal more data to it each time. In Python tutorials, many of

32:49.560 --> 32:54.920
you will have seen this, the creating of the string and data plus equals more as an anti-pattern

32:54.920 --> 32:59.720
that you avoid because I tried running this. How long does the plus equals approach take?

33:00.680 --> 33:06.360
Infinity time. Meaning that I finally needed my laptop back so I killed it and I'll never

33:06.360 --> 33:10.920
know how long the loop would have taken to read my gigabyte of data. But when you do plus equals,

33:10.920 --> 33:16.200
you're asking Python to create a little string and then your first plus equals makes a slightly

33:16.200 --> 33:21.400
longer one. Your next plus equals through the loop creates a slightly longer one into which

33:21.400 --> 33:26.440
all the data from the second string has to be copied to make the third one. Then you go through

33:26.440 --> 33:32.040
the loop again and now you have to copy all that data again to make your fourth string. And if you

33:32.040 --> 33:37.400
have a million bytes to read, you wind up doing half a trillion operations. It's called an order

33:37.400 --> 33:42.120
in squared algorithm, generally to be avoided if you wanted to finish by lunchtime.

33:43.960 --> 33:49.880
So this is what we tell everyone to do. Pivot to keeping a list of blocks that you've received

33:49.880 --> 33:56.120
and join them together at the end in a single step. Python's much better at that. This actually

33:56.120 --> 34:01.000
finished on my laptop. It's the traditional way of accumulating data from the internet

34:01.000 --> 34:06.920
in Python or from a network. Took about a little more than a second to read in a gig of data

34:06.920 --> 34:16.680
in those small 1.5K chunks. Now, there is a version like read into, but that receives

34:16.680 --> 34:22.120
into a byte array you've already built instead of building a new string, but it now runs into a

34:22.120 --> 34:28.600
problem. When we do read into or receive into, where does it put the data? At the beginning of the

34:28.600 --> 34:33.480
array. And all of our incoming blocks will overwrite each other. What we want as more and more

34:33.480 --> 34:39.480
of blocks come in from the network is to arrange them along our byte array. So that memory view

34:39.480 --> 34:45.400
slicing expense that I added in its statement to avoid whenever possible in the previous code,

34:45.400 --> 34:53.240
it now becomes mandatory. Again, this ability with a view to write into byte locations that

34:53.240 --> 34:58.120
aren't at the beginning, but are somewhere in the middle of the byte array that you've built.

34:58.120 --> 35:02.520
The first block can go at the beginning, but you need to build a memory view to target the

35:02.600 --> 35:10.840
second block after the first block, the third block after that, and so forth. And so you need

35:10.840 --> 35:18.280
to build a memory view and you're going to need to use it to target that receive V into at subsequent

35:19.320 --> 35:25.080
positions inside of your big byte array. I'm presuming that you know the content

35:25.080 --> 35:29.320
like the head of time and have preallocated it and you're waiting to fill it with data.

35:30.280 --> 35:36.280
This takes about eight tenths of a second because we a bit of a win here because you

35:36.280 --> 35:40.760
haven't had to build a list, you haven't had to call join, you haven't built a bunch of intermediate

35:40.760 --> 35:46.840
data structures. It actually is a significant win when you need to keep the data that you're

35:46.840 --> 35:53.240
reading rather than just immediately passing it back to the OS. Another possibility I saw on

35:53.240 --> 35:58.440
someone's blog is to do an old fashioned receive of a new string and then try to do a byte array

35:58.440 --> 36:05.080
extend to grow your byte array with these new strings. It copies the data twice but does get

36:05.080 --> 36:12.280
rid of that join concatenation. It looks something like this, data.extend down there near the bottom.

36:13.240 --> 36:21.000
It is not a win over the normal way of using byte arrays because it turns out byte array extend

36:21.000 --> 36:29.480
is pretty inefficient. It asks for an iterator over its argument and then calls the iterator's

36:29.480 --> 36:35.320
next function over and over for every byte and then asks the int object what its value is

36:36.200 --> 36:42.440
so that it can then put it in an intermediate array and that involves having to increment and

36:42.440 --> 36:47.160
decrement the reference pointer of the integer it's given and by the time you're done you can

36:47.240 --> 36:52.600
compute that you've done at least 40 bytes of bandwidth to RAM even ignoring the instructions

36:52.600 --> 36:59.640
and stacks and arguments that are passing in order to get that single byte value extended onto the

36:59.640 --> 37:07.800
end of your byte array plus it doesn't write to your byte array. It writes to an intermediate

37:07.800 --> 37:12.280
buffer that it grows dynamically and then does the append when it's done so that should that

37:12.360 --> 37:18.280
iteration die part way through you don't wind up having modified the byte array some it wants to

37:18.280 --> 37:24.680
either succeed or fail as an atomic operation so that's why it's kind of slow kind of klugey

37:25.640 --> 37:31.640
but seeing that blog post made me ask a question does the byte array have an append operation

37:31.640 --> 37:37.240
that's any good? I mean surely the people writing it knew that we'd want to do that without spinning

37:37.240 --> 37:45.000
up an iterator and calling it 1500 times does it have an operation that's really good and yes it

37:45.000 --> 37:53.080
does I read the c code to find it now think about it where would you put the real extend operator

37:53.080 --> 37:59.880
the real ability to make your byte array longer obviously you'd hide it behind the operator

37:59.880 --> 38:04.920
that we've spent 20 years telling people to never use with string values

38:08.120 --> 38:16.600
this might be so difficult that some of you will never do it but if you can convince yourself to

38:16.600 --> 38:24.040
type this after all of this time this is actually something that byte arrays do magnificently just

38:24.040 --> 38:32.360
plus equal the additional data to your byte array and you will be even receive into's ability

38:32.360 --> 38:39.720
to grow your array with data so this case where we need to accumulate and keep the whole pay road

38:39.720 --> 38:45.240
is a real win for the byte array in all of the approaches and there don't seem to be sharp edges

38:45.240 --> 38:52.760
that can suddenly make it behave much worse than the the list and joint 33 speed up in that last

38:52.760 --> 38:59.800
version of the algorithm and cleaner code I mean admit it you've always wanted to just plus equal

38:59.800 --> 39:04.840
haven't you it's the natural way to write it in python and this is one of those neat intersections

39:04.840 --> 39:13.240
of the fast way with the way that looks good on the page as well so I'm not going to talk about

39:13.240 --> 39:17.720
send you might think I'll now get into the fact that send doesn't always send the whole payload

39:17.720 --> 39:23.720
but python has long had you covered here python sockets have for a very long time had a send

39:23.720 --> 39:31.080
all that sits in a loop in c sending shorter and shorter tails of your data until finally the os

39:32.680 --> 39:37.560
buffers have been able to receive it all so I don't see that we need byte arrays for that

39:37.560 --> 39:44.600
and I can declare the byte array the winner if you need an accumulator if you need to very quickly

39:44.600 --> 39:51.400
in a performance sensitive environment accumulate a lot of incoming data it is a noticeably good

39:51.400 --> 39:57.160
win with two different techniques that work pretty well I'll briefly mention that some people want

39:57.160 --> 40:02.600
a freestyle mutable string when they see the byte array they don't think of io they don't think of

40:02.600 --> 40:09.480
bloom filters and bit vectors they want to mess with a string they want a string that they can just

40:09.480 --> 40:17.400
change and all ish I have not found yet a good use for this and I'll sort of show you why it

40:17.400 --> 40:24.120
winds up being awkward you want to change part of a payload before using storing or retransmitting

40:24.120 --> 40:27.800
that would be the use case here because if you want to lowercase the whole thing you have to

40:27.800 --> 40:32.600
touch all the bytes anyway so you might as well build a new one good thing is that the byte array

40:32.600 --> 40:38.600
is mutable you can get it and change it but none of the methods that it shares with strings

40:39.640 --> 40:46.920
do mutation to it if you call dot upper on your byte array you get a new byte array so you have

40:46.920 --> 40:54.760
a mutable string type that does nothing string like mutably a byte array only changes when subjected

40:54.760 --> 41:01.880
to list like operations like assignment to an index or assignment to a slice or dot clear

41:01.880 --> 41:06.920
and so the result if you try writing a network algorithm or something with this is curious

41:06.920 --> 41:13.640
you have a mutable string that alas does no mutation precisely when you start calling

41:13.640 --> 41:20.600
its string methods want to lowercase a word well you're going to have to make a copy to call lower

41:20.600 --> 41:26.440
on you're going to have to do slicing giving you a copy call lower making another copy and then

41:26.440 --> 41:33.240
assignment to copy it a third time back into the data structure in order to get that accomplished

41:33.240 --> 41:39.800
there isn't I looked there isn't a lower into and an upper into that would let you do smaller

41:39.800 --> 41:45.720
grained uh manipulations that wrote directly to your new byte array can the memory view save us

41:45.720 --> 41:55.560
though it did in all of the previous occasions no because memory views don't do anything string

41:55.560 --> 42:01.560
like the moment you move to a memory view which lets you look at a piece of string efficiently

42:02.520 --> 42:07.240
you're not going to be able to do anything string like to it so you have to do a round

42:07.240 --> 42:16.360
trip out to a smaller string to do a manipulation and store data back to mutate a byte array without

42:16.360 --> 42:22.040
rewriting its whole content you're going to need indexes do you remember indexes back the first

42:22.040 --> 42:28.600
week before you'd found split strip and join and we're like doing everything like this you get to

42:28.600 --> 42:34.280
do it again if you decide to try mutating a byte array the byte array will let you enjoy those

42:34.280 --> 42:40.920
days all over again because all the mutation operations are powered only by indexes one hint

42:40.920 --> 42:47.400
regular expressions while they turned off a lot of other string like things were left turned on

42:47.400 --> 42:54.120
and do work against byte arrays and can help give you some useful indexes to use freestyle

42:54.120 --> 43:01.720
mutable string it's awkward I have here at the end of the slides some uh links and pointers to other

43:01.720 --> 43:06.920
documentation including the blog posts that inspired this talk and made me want to bring

43:06.920 --> 43:13.720
everything together in one place in conclusion the byte array it is a very memory efficient

43:13.720 --> 43:21.000
not faster but memory efficient store of byte integers should you ever need them it can help

43:21.000 --> 43:26.680
control memory fragmentation when doing uh high-performance i o because you don't have to create

43:26.680 --> 43:33.240
a new string but it's hard to make it faster in a reliable way be careful it's a great way to

43:33.240 --> 43:38.840
accumulate data that's coming in a piece at a time that's its real superpower and though it's

43:38.840 --> 43:46.040
very tempting for string operations it's also a bit underpowered and a bit awkward that's what

43:46.040 --> 44:04.840
I've learned so far thank you very much we are out of time so I will meet interested byte array

44:04.840 --> 44:17.640
fans outside the door in a few minutes

