1
00:00:00,000 --> 00:00:16,000
Alright. Hello, everybody. I have quite an honor today. I am lucky enough to introduce

2
00:00:16,000 --> 00:00:22,400
Brandon Rhodes. He's a longtime Python developer, maintainer of several amateur astronomy libraries

3
00:00:22,400 --> 00:00:28,240
and a developer for Dropbox. He's also a Quizmaster extraordinaire, as some people got to find

4
00:00:28,240 --> 00:00:34,000
out last night. Today, he will be presenting on, oh, come on, who needs byte arrays? Without

5
00:00:34,000 --> 00:00:45,120
further ado, Brandon Rhodes.

6
00:00:45,120 --> 00:00:50,600
Welcome, everybody. I hope you're having a good time at PyCon and in Montreal. My topic

7
00:00:50,600 --> 00:00:59,080
today is indeed byte arrays. A very interesting recent addition to the Python ecosystem. Because

8
00:00:59,080 --> 00:01:04,280
in Python, normal string objects, the ones that we're accustomed to dealing with are

9
00:01:04,280 --> 00:01:14,200
immutable. They can't be changed or modified. This is true of the types available under

10
00:01:14,200 --> 00:01:24,120
both two and three. The original string type, STR, was renamed to bytes because of its low-level

11
00:01:24,120 --> 00:01:31,800
nature and then that synonym was also backported to Python 2. We will not talk much today about

12
00:01:31,800 --> 00:01:42,920
the newer Unicode byte strings that were renamed to the official string type of Python 3. We're

13
00:01:42,920 --> 00:01:49,400
going to be talking about those lower-level ones. The strings that don't pretend they have symbols

14
00:01:49,400 --> 00:02:01,640
inside of them so much as they know that their innards are really bytes. 8-bit codes, 0-255.

15
00:02:01,640 --> 00:02:09,960
For the most part, I will put that little B in front of the bytes strings that I type. It is

16
00:02:10,040 --> 00:02:17,560
optional under Python 2, but it becomes mandatory under Python 3. So if I write my strings like

17
00:02:17,560 --> 00:02:24,760
this, then it will work wherever you try this later if you want to see the examples run. Strings

18
00:02:24,760 --> 00:02:30,360
are immutable. What does that mean? It means that when you call methods, when you do things to them,

19
00:02:30,360 --> 00:02:36,600
the original object itself doesn't change. You get given a new object as the return value of the

20
00:02:36,600 --> 00:02:42,760
method you call. So if .lower returns to you a new string, you can still peek back and see that

21
00:02:42,760 --> 00:02:48,360
the original is untouched and unchanged. If you run split, you'll see that both of the objects

22
00:02:48,360 --> 00:02:56,040
you've been given are new string objects. The original is still unchanged. They do not allow

23
00:02:56,040 --> 00:03:01,240
assignment because that would make them change. It would make them mutate as the technical term

24
00:03:01,320 --> 00:03:08,280
in computer science. Now, immutability has led a long and very successful career in Python

25
00:03:08,280 --> 00:03:15,160
because it makes things simple. You don't pass a name to a function only to finally

26
00:03:15,160 --> 00:03:20,760
suddenly discover it's another name when it comes back. You don't pass a string or a block to

27
00:03:20,760 --> 00:03:26,200
someone, and suddenly it's a different string or block. It's a very, very simple model that if

28
00:03:26,200 --> 00:03:32,040
you have the word Python, you know it will always remain so. And it actually is one of the most

29
00:03:32,040 --> 00:03:36,760
parts of Python. That's the most functional programming languages where new results are

30
00:03:36,760 --> 00:03:44,120
returned instead of being written onto data structures you already hold. The string types

31
00:03:44,120 --> 00:03:51,480
are a primary example of that. But it sometimes is a little expensive in terms of allocation.

32
00:03:51,480 --> 00:03:56,920
Every time you want to make any little tweak to a string, it has to allocate a new one for you.

33
00:03:56,920 --> 00:04:04,200
That means a lot of data gets copied back and forth into new areas of memory. Not everybody

34
00:04:04,200 --> 00:04:11,400
is happy about that. And so Python 3 introduced and then it was back ported to Python 2, 6,

35
00:04:11,400 --> 00:04:18,520
and 7, the new byte array. It's a built-in. Python 2, 7, Python 3, you can just type byte array

36
00:04:18,520 --> 00:04:25,640
and you'll get access to that type, just like you can with stir and int and list. A byte array

37
00:04:26,440 --> 00:04:36,120
is a mutable string that is based, this is interesting, on Python 3's byte string, not the

38
00:04:36,120 --> 00:04:43,800
old stir string from Python 2. And that's an interesting problem, that it's based on Python

39
00:04:43,800 --> 00:04:54,600
3's string, byte string type, because honestly, the Python 3 bytes type is designed to be awkward

40
00:04:54,600 --> 00:05:04,280
for string operations. Why? So you will want to be a good person and run the code before

41
00:05:04,280 --> 00:05:12,200
treating your data as characters. And this has led to Python 3 programmers tending to write code

42
00:05:12,280 --> 00:05:17,800
that is from the ground up prepared for internationalization and different alphabets because they

43
00:05:17,800 --> 00:05:23,960
think about the issue of decoding on the way in and encoding on the way back out because they

44
00:05:23,960 --> 00:05:30,600
have to. But we'll see it leads to some interesting behaviors. Just professionally, always be aware

45
00:05:30,600 --> 00:05:39,400
of using string types that wish you weren't using them. In Python 2, let's compare. We can build a

46
00:05:39,480 --> 00:05:45,880
string, we can ask its length, we can split it into pieces. Python 3's byte type, there's little

47
00:05:45,880 --> 00:05:51,000
B characters hanging out in front of our byte strings, but we can take the length, we can call

48
00:05:51,000 --> 00:05:57,960
a method like split. In Python 2, we can use those square brackets with a colon in order to do

49
00:05:57,960 --> 00:06:04,680
slicing and get back a copy of the inside of the string. The exact same thing works exactly the

50
00:06:04,680 --> 00:06:12,600
same way in Python 3. In Python 2, it's always a custom in Python that if something has a length,

51
00:06:12,600 --> 00:06:18,200
it should allow itself to be iterated or looped over. In Python 2, if you try looping over a

52
00:06:18,200 --> 00:06:26,280
string, you get out smaller strings, one character strings that are inside. What happens if you

53
00:06:27,080 --> 00:06:39,720
press enter for this line of code in Python 3? You get a syntax error. Syntax error, you failed to

54
00:06:39,720 --> 00:06:50,280
pay the Python 3 per intax. Python 3, it's kind of like an old text-based adventure game where

55
00:06:50,280 --> 00:06:55,560
you can tell the writer just threw an extra obstacle in your way because a room needed to

56
00:06:55,560 --> 00:07:13,320
be a little more complicated. You know, in seriousness, I've known Ruby, some Ruby programmers,

57
00:07:13,320 --> 00:07:17,880
this is actually a big debate in the Ruby community, Ruby makes parenthesis optional

58
00:07:17,880 --> 00:07:23,720
when calling a function. The Rubyists who never use the parenthesis always tell me they could

59
00:07:23,720 --> 00:07:30,040
never stand Python again. I always thought that was the oddest thing. Doesn't everyone want to

60
00:07:30,040 --> 00:07:36,120
type parenthesis whenever they ask their computer to do something? I must admit that once it was

61
00:07:36,120 --> 00:07:45,000
me who was suddenly having to type parenthesis, but they surely must still be wrong. Anyway.

62
00:07:45,320 --> 00:07:54,520
I did a conservative estimate of how much typing is cost me by these print statements in Python 3,

63
00:07:54,520 --> 00:08:02,760
and these are conservative estimates, and it's coming out to quite a bit of typing per year.

64
00:08:02,760 --> 00:08:09,880
I'm having to face going into Perendet just to write some Emax Lisp next week. Anyway, Python 3

65
00:08:09,880 --> 00:08:14,600
wants them for this print statement, and what's another two perends when I've already typed so

66
00:08:14,600 --> 00:08:26,440
many? So once you get the print statement working, you're in for another surprise. Python 3 bytes type

67
00:08:27,240 --> 00:08:36,120
is not made of characters, it is made of numbers. This breaks a very important contract that for

68
00:08:36,120 --> 00:08:41,320
me existed with strings, which is that I can pull them into pieces with either indexing or

69
00:08:41,320 --> 00:08:47,720
slicing and know that they would go back together again. Now, there is a way around by asking for

70
00:08:47,720 --> 00:08:56,840
one element slices instead of looking up integer indexes, but clearly it doesn't want me to treat

71
00:08:56,840 --> 00:09:03,560
it like a string. So bytes objects, even if you learn some workarounds, are kind of an awkward fit

72
00:09:03,560 --> 00:09:10,200
for many of the tasks they're called upon to do. They're kind of this hybrid type between a list of

73
00:09:10,200 --> 00:09:20,440
numbers and a string. They're kind of in between, and they don't necessarily do either one perfectly

74
00:09:20,440 --> 00:09:27,720
well. And so why do I bring all of that up? Why do I rehearse these well-known issues with Python

75
00:09:27,800 --> 00:09:33,880
3 bytes? Which, by the way, are being taken care of. Python 2.5 will, for example, reintroduce

76
00:09:33,880 --> 00:09:41,320
percent formatting for byte strings, because now that the experiment is in its fifth version,

77
00:09:41,880 --> 00:09:46,840
I think it is beginning to become clear that the real problem in Python 2 wasn't that strings were

78
00:09:46,840 --> 00:09:52,840
convenient, and so we ignore Unicode, it's that conversion could happen automatically without

79
00:09:52,840 --> 00:10:01,400
warning. And so they are beginning to add power back into Python 3 byte strings, but they probably

80
00:10:01,400 --> 00:10:07,400
will always be made of numbers now for backwards compatibility, and we bring all that up because

81
00:10:07,400 --> 00:10:14,280
the byte array that I will now talk about is a mutable version of Python 3's byte string,

82
00:10:14,840 --> 00:10:22,360
a mutable version of Python's most underpowered string type. So we'll just quickly look at

83
00:10:22,360 --> 00:10:29,400
a few possible applications and whether a mutable vector of bytes is able to accomplish things

84
00:10:29,400 --> 00:10:37,800
better or worse than traditional Python. So first, let's be fair to it. What if you actually want

85
00:10:37,800 --> 00:10:48,680
a list of numbers between 0 and 255? That never happens to me. So I invented, in those rare cases

86
00:10:48,680 --> 00:10:54,120
where you actually want to store bytes, if you had one, is the byte array a good choice? So I invented

87
00:10:54,120 --> 00:11:01,800
one. I wrote my first Bloom filter as preparation for this talk. A Bloom filter is a way to, let's

88
00:11:01,800 --> 00:11:07,640
say, you have a dictionary of words, and before you go look on disk for whether a word is in your

89
00:11:07,640 --> 00:11:14,280
dictionary, you want a quick way to knock out a lot of words, it's just not being candidates.

90
00:11:15,000 --> 00:11:21,000
What you can do is set up a big bit field and have a couple of hash functions that you throw the

91
00:11:21,000 --> 00:11:27,800
word elephant at them, and they identify some bits for you that belong to elephant. You give them

92
00:11:27,800 --> 00:11:33,880
the word Python. It's a different set of bits. The idea is that if you load up your dictionary

93
00:11:33,880 --> 00:11:39,320
by setting all of the bits for elephant and all of the bits for Python, then when you see those

94
00:11:39,320 --> 00:11:44,280
words later in a document, you can just check whether their bits are set to know if there's

95
00:11:44,280 --> 00:11:51,160
any possibility that elephant is in your dictionary, because many words will have sets of bits that

96
00:11:51,160 --> 00:11:56,120
aren't set at all, or several of which aren't set, and that you know could not have been in the

97
00:11:56,120 --> 00:12:01,960
dictionary you loaded. This is a nice example because it lets us do a pure math operation,

98
00:12:02,040 --> 00:12:15,400
in this case the in place oring of a byte in this array A with itself and with a bit that we create

99
00:12:15,400 --> 00:12:22,280
over here and set, and we can run through, set this up, and then when we want to test a word,

100
00:12:22,280 --> 00:12:28,680
go back in and use the reading version of square brackets, not in an assignment statement, but

101
00:12:28,680 --> 00:12:35,000
in an expression to read back the value of a bit. A nice exercise to see how does this thing

102
00:12:35,000 --> 00:12:42,360
perform storing and receiving a few tens of thousands of bytes. And by the way, the name A

103
00:12:42,360 --> 00:12:49,160
in the previous code can be either an old fashioned array dot array that's been around in Python

104
00:12:49,160 --> 00:12:55,080
forever, or a new fangled byte array. To this extent, they both provide the same interface. Each

105
00:12:55,080 --> 00:13:03,800
slot you can address gets you or lets you store a byte. And so with this application, I ran it

106
00:13:03,800 --> 00:13:09,960
both ways, and byte array scored its first victory, because it is so more specific than

107
00:13:09,960 --> 00:13:14,840
array dot array, which can also hold, I believe, floats and integers and other things like that,

108
00:13:14,840 --> 00:13:19,800
because the byte array's code path has almost no decisions, it's always going to store bytes,

109
00:13:20,360 --> 00:13:26,760
it is more than 7% faster for running that bloom array code, bloom filter code that I just showed

110
00:13:26,760 --> 00:13:33,880
you, than the old general purpose array, array object. So you might think that this is immediately

111
00:13:33,880 --> 00:13:41,240
an obviously a go to data structure for lists of eight bit numbers. I tried it another way.

112
00:13:42,200 --> 00:13:47,080
I want to know what's even faster than a byte array? A list of integers.

113
00:13:50,680 --> 00:13:58,120
1% faster. If you just say, hey, Python, here's a one element list with a zero in it,

114
00:13:58,120 --> 00:14:04,760
make me a lot of these. A plain, I'm sorry, 2%, a plain list of int objects that happen

115
00:14:04,760 --> 00:14:13,000
to be in the range 0 to 255 will run even faster than the byte array. Why? Well, it's because,

116
00:14:13,000 --> 00:14:18,600
think of what the byte array is doing. It's storing real bytes, low level in your computer,

117
00:14:18,680 --> 00:14:27,720
that must each be translated into an int object address when the value is being handed out into

118
00:14:27,720 --> 00:14:36,920
your Python code, and then when an int object is handed back, it has to be changed back into

119
00:14:36,920 --> 00:14:44,280
a simple byte in order to be stored. The list skips all that, it just stores the addresses you

120
00:14:44,280 --> 00:14:53,640
give it. The byte array, by the way, doesn't have to pay any penalty to allocate or create any of

121
00:14:53,640 --> 00:14:59,320
those int objects, because it just so happens that the Python, the CPython interpreter, when it

122
00:14:59,320 --> 00:15:06,120
starts up, preallocates all of the integer objects negative 5 through 256 so that they never have

123
00:15:06,120 --> 00:15:11,320
to be created or destroyed over the lifetime of the interpreter. So when you ask the byte array,

124
00:15:11,320 --> 00:15:17,160
what's it positioned 100, and it wants to say 70, it can just grab the existing 70 integer object

125
00:15:17,160 --> 00:15:22,120
that always exists in memory and hand it back, so it's not having to go do a malloc or anything,

126
00:15:22,840 --> 00:15:27,960
it's not having to allocate new memory, but still it's having to do that step of translation,

127
00:15:27,960 --> 00:15:34,200
and it is honestly just simpler to store the pointer, to store the address of the 70 object.

128
00:15:34,200 --> 00:15:40,360
That's why the list object runs faster, and so this is interesting. We have this new special case

129
00:15:40,360 --> 00:15:48,680
container that's slightly slower than just using Python's well-honed default data types. A plain

130
00:15:48,680 --> 00:15:55,160
old list is a faster bit vector than the fancy new byte array, except if you're using PyPy,

131
00:15:55,800 --> 00:16:00,520
where they're all the same because it becomes the same C code under the hood of a machine code,

132
00:16:00,520 --> 00:16:06,840
I should say, and all three run much faster as well as being exactly equivalent. I tried it out,

133
00:16:06,840 --> 00:16:10,600
and PyPy in each case figured out I was trying to do exactly the same thing.

134
00:16:12,120 --> 00:16:18,440
Well, I guess they're done already, I'll just keep going. So for this first experiment,

135
00:16:18,440 --> 00:16:25,400
what if I need a list of integers between 0 and 255? My verdict is that the bit vector

136
00:16:25,400 --> 00:16:31,560
is space efficient. You don't choose it because it's going to be obviously faster,

137
00:16:31,560 --> 00:16:35,160
it's not, or obviously simpler, it's doing a little more under the hood,

138
00:16:35,400 --> 00:16:45,000
but the good old-fashioned list of integers has to store in each slot the address of the real

139
00:16:45,000 --> 00:16:53,320
integer object 70. The bit vector just stores the seven bits, the eight bits that represent 70,

140
00:16:53,320 --> 00:16:58,600
and therefore uses on a 64-bit machine, which is what I'll presume for all of these calculations,

141
00:16:58,680 --> 00:17:05,160
eight times less space. And the point of a bloom filter is to save space in RAM.

142
00:17:05,720 --> 00:17:12,920
That for bit operations is why you go to the byte array, because it stores bytes as honest to

143
00:17:12,920 --> 00:17:21,800
goodness bytes with no extra overhead per byte. It's a great way to get numbers between 0 and 255

144
00:17:21,880 --> 00:17:28,680
packed in the minimum space possible. So it is a win, but not in the way you might initially

145
00:17:28,680 --> 00:17:36,200
expect. All right. Second, it is a reusable buffer. When you read a string in, you can't do

146
00:17:36,200 --> 00:17:42,200
anything to it because it's immutable, but a byte array can be reused. For a quick benchmark,

147
00:17:42,200 --> 00:17:49,640
I got a made a random file of a gigabyte of random data, read it with cat, so I was able to

148
00:17:49,720 --> 00:17:56,120
estimate that probably Python won't be able to do better than 0.11 seconds on my machine

149
00:17:56,120 --> 00:18:01,480
reading in the same data. I tried it with DD. Anyone here ever used DD to rewrite data?

150
00:18:03,000 --> 00:18:12,360
It took six times longer. Anyone know why? I S traced them, and it's because of the block size.

151
00:18:12,360 --> 00:18:21,080
DD alas is an old and crafty and low-level tool. While cat will zoom along with 128k blocks,

152
00:18:21,080 --> 00:18:27,400
so it asks the OS for some data and gets 128,000 bytes in a single shot,

153
00:18:28,440 --> 00:18:34,920
DD, because it's an old level for writing to ancient 70s block devices, reads and writes

154
00:18:34,920 --> 00:18:41,400
512 bytes by default. Giving DD the same block size does make it perform the same. You can give

155
00:18:41,480 --> 00:18:48,120
it a block size of 128k and get 0.11 seconds just right there with cat, but interestingly enough,

156
00:18:48,120 --> 00:18:52,200
it's not the default, despite the fact that I seem to know all these people that think DD

157
00:18:52,200 --> 00:18:58,840
would be faster somehow by default. It's not. It's the same reads and writes. And cat is the

158
00:18:58,840 --> 00:19:09,320
Unix default. DD came from IBM. But this teaches us a first lesson that we will now apply. As we

159
00:19:09,320 --> 00:19:15,240
look at Python IO, we need to keep block size in mind. The size of the chunks you read determines

160
00:19:15,240 --> 00:19:20,200
how many chunks you need to read, determines how often you need to converse with the operating

161
00:19:20,200 --> 00:19:26,520
system, which is often the expense that can come to dominate your runtime. Here's how we do it in

162
00:19:26,520 --> 00:19:33,400
normal Python. Read a block and write the data back out. Note this is perfectly safe if an

163
00:19:33,400 --> 00:19:40,120
undersized block comes in because the string that I'm here calling data that comes back is labeled

164
00:19:40,120 --> 00:19:48,440
with its length. It could be 5 bytes, it could be 128k. Python strings know their length and so

165
00:19:48,440 --> 00:19:55,960
right can just ask the length and send that many bytes of data back out. My first attempt at doing

166
00:19:56,040 --> 00:20:04,600
a read into seemed to work at first until I noticed that every file I wrote, however big it was

167
00:20:05,320 --> 00:20:11,320
originally, the copy that I made with this routine would always be a multiple of my block size.

168
00:20:12,280 --> 00:20:19,640
Why is that? Because when I create one of these new byte arrays of, let's say 128k, what this

169
00:20:19,640 --> 00:20:26,280
loop was doing is reading some number of bytes, who knows how many come in in the next block of

170
00:20:26,280 --> 00:20:34,200
the file if I'm near the end, reads some number of bytes into my byte array, and then writes out

171
00:20:34,200 --> 00:20:39,560
the whole byte array, including all the junk at the end that's maybe not part of the current block

172
00:20:39,560 --> 00:20:48,520
of the file. I was doing my right of the whole 128k block without consulting the length to see

173
00:20:48,520 --> 00:20:59,240
if I should have been writing the entire 128k block. One fix is to simply use slicing, is to get

174
00:20:59,240 --> 00:21:09,080
that byte array called data and take from it to write each time the slice that is length long.

175
00:21:09,080 --> 00:21:15,880
So if I get a full-sized block, I'm writing out all of the data, but if I get only half a block

176
00:21:15,880 --> 00:21:22,760
at the end of the file, I only write that last half block out from the initial part of my byte

177
00:21:22,760 --> 00:21:30,680
array. What if we didn't want the expense, though, of having to do that, back one slide, expensive

178
00:21:30,680 --> 00:21:38,440
slicing operation, because asking a Python string, unicode string, or byte array for a slice creates

179
00:21:38,440 --> 00:21:45,560
a whole new one and copies as much data into the new one as you ask for with the limits you set in

180
00:21:45,560 --> 00:21:52,120
the slice. If we wanted to achieve zero copy, the people who added byte array to the language,

181
00:21:52,680 --> 00:21:59,560
they thought of that as well. They added a second feature that works with byte array called a memory

182
00:21:59,560 --> 00:22:09,400
view. A memory view is a sliceable object. Here I take the slice 3 colon 6 of that byte array that

183
00:22:09,480 --> 00:22:18,680
I create up there. It's a slice which has no memory of its own but is letting you reach into the

184
00:22:18,680 --> 00:22:25,480
memory it was sliced from to make the change. Essentially, this memory view, the V that I

185
00:22:25,480 --> 00:22:33,480
create here, is just a, essentially, it's like a string object, but the addresses that it wants

186
00:22:33,480 --> 00:22:38,920
to write to in memory are the addresses right there in the middle of the byte array. So when I try

187
00:22:38,920 --> 00:22:50,920
to set its index zero value, it really goes to index 3 in the real byte array. When I set item 1,

188
00:22:50,920 --> 00:22:58,200
it really goes to slot 4. When I set item 2, it goes to slot 5. It really is just creating an

189
00:22:58,200 --> 00:23:06,600
object that acts like it's a little byte array but is, in fact, just an offset. It's adding

190
00:23:06,600 --> 00:23:14,040
something to each index you use as you read and write from it. And this is what can help us in the

191
00:23:14,040 --> 00:23:22,920
situation we're in. Here is a zero copy version of my fixed code to try to read in lots of blocks.

192
00:23:22,920 --> 00:23:29,560
Before I was asking data, the byte array itself to do the slicing, and like all Python strings,

193
00:23:29,640 --> 00:23:37,160
it gives me a whole new one when I ask for a slice. Now, I'm looking at it through a memory view.

194
00:23:37,160 --> 00:23:44,440
So if I ask, let's say, for a view of, you know, if length is 128k and asking for all the data,

195
00:23:44,440 --> 00:23:50,040
it just gives me a little memory view object whose addresses are pointing at the whole block of data.

196
00:23:50,040 --> 00:23:55,880
A very cheap operation. And so memory views are often necessary to get any kind of performance

197
00:23:55,880 --> 00:24:01,800
out of the byte array when doing IO, especially when you can't predict how big the next delivery

198
00:24:01,800 --> 00:24:10,280
of information will be. Here are the runtimes of DD and cat that we discussed earlier. Compared to

199
00:24:10,280 --> 00:24:20,040
just plain old read with normal Python strings, read into my first version that was broken because

200
00:24:20,120 --> 00:24:26,200
it wasn't careful about how much it wrote does run slightly faster than the traditional Python

201
00:24:26,200 --> 00:24:34,840
way of doing things. But when you then pivot to using a slice byte array and slicing it,

202
00:24:34,840 --> 00:24:39,560
because you're copying every piece of data into memory twice, it's much more expensive,

203
00:24:39,560 --> 00:24:47,640
it is the memory view. It is that zero expense, very little expense, constant time expense,

204
00:24:47,640 --> 00:24:54,760
I should say, ability to slice without copying data that lets us create a correct version of a

205
00:24:54,760 --> 00:25:06,360
file copy while still slightly beating read and write and traditional strings. So that was a lot

206
00:25:06,360 --> 00:25:15,640
of work and we got a 4% speed up with byte array. Now, small blocks, things get worse for byte array

207
00:25:15,720 --> 00:25:21,720
because what will slicing here so often do, it creates a new object every time and creating

208
00:25:21,720 --> 00:25:26,920
one of these little view objects with its pointers into the part of the byte array I want to look at

209
00:25:26,920 --> 00:25:32,360
was fine when I was only doing it every few hundred K of data, but what if I'm like the

210
00:25:32,360 --> 00:25:38,200
defaults of DD and I'm going to be reading and writing 512 bytes at a time, what if I have to

211
00:25:38,280 --> 00:25:46,520
spin up a new memory view for every half K of data? Then things start to look very bad and,

212
00:25:46,520 --> 00:25:52,120
in fact, the memory view used correctly where you're careful of your lengths is simply a loss.

213
00:25:53,000 --> 00:26:01,960
It's much, reading where it just returns a Python string is really efficient. A write of a Python

214
00:26:01,960 --> 00:26:08,120
string is really efficient. You can easily get into situations with the fancy attempts one

215
00:26:08,120 --> 00:26:15,080
makes with a byte array to create more expensive IO than you had when you just used traditional

216
00:26:16,200 --> 00:26:21,960
immutable strings that, yes, required Python to build a new string for every call to read,

217
00:26:21,960 --> 00:26:30,680
but cut out all of the rest of that expense. I was sad for the byte array at this point in my talk.

218
00:26:32,040 --> 00:26:44,040
So I stared at the example. 20% slow down for a small block size, but then I thought of something.

219
00:26:46,120 --> 00:26:50,920
What if we don't always slice? Because when reading from a file, different from a network,

220
00:26:50,920 --> 00:26:55,240
when reading from a file, the normal case is that unless you're at the very end of the file,

221
00:26:55,240 --> 00:27:01,320
you're going to get as much as you ask for. Ask for 128 K, you're going to get it. The normal case

222
00:27:01,320 --> 00:27:07,800
is that the length equals the block size, and in that case, there's not only no reason to ask the

223
00:27:07,800 --> 00:27:13,800
byte array to take a slice of itself and copy all that data, there's no reason to use the view to

224
00:27:13,800 --> 00:27:18,520
limit the amount of data you're using from the block. You're going to use all of it.

225
00:27:18,520 --> 00:27:26,360
And so if you handle that special case, you don't incur an object creation step in order to get that

226
00:27:26,360 --> 00:27:36,440
right call started, and you slightly beat the performance of the traditional read write loop

227
00:27:37,160 --> 00:27:51,560
by 4%. Just like for the big block case. So even if your I.O. is in a situation where the

228
00:27:51,640 --> 00:27:58,600
block size will be varying or might be small, you can, if you're careful and cut and paste from

229
00:27:58,600 --> 00:28:06,120
my talk slides, you can run slightly faster than the traditional read and write with immutable

230
00:28:06,120 --> 00:28:12,360
strings. Python 2.7, by the way, has the same relative behaviors between those different choices

231
00:28:12,360 --> 00:28:18,840
on my machine slightly slower. And I think the lesson here is that it is just hard to beat

232
00:28:18,840 --> 00:28:23,400
old fashioned strings when you're pulling in data and then just immediately sending it back to

233
00:28:23,400 --> 00:28:32,040
the operating system over some other channel. It's really something how the good old fashioned

234
00:28:32,040 --> 00:28:39,640
immutable string that makes functional programmers' hearts sing is pretty much as good in this case

235
00:28:39,640 --> 00:28:47,960
as our weird side effect idea of constantly modifying this single byte array that we have created.

236
00:28:48,680 --> 00:28:53,960
So my verdict is that it is dangerous because it's so easy to write what looks like pretty code,

237
00:28:53,960 --> 00:28:58,760
it looks like almost the same little read write loop, but is going to operate substantially

238
00:28:58,760 --> 00:29:04,520
worse in situations that you might not think to test for unless you think of the small blocks case.

239
00:29:05,160 --> 00:29:13,720
The one advantage it does offer is a great memory profile because there's a link later to a great

240
00:29:13,720 --> 00:29:18,280
blog post online about someone writing an audio server that needed to keep lots of strings in

241
00:29:18,280 --> 00:29:23,080
a buffer and his memory usage was going through the roof because if you're constantly allocating

242
00:29:23,080 --> 00:29:27,480
and deallocating differently sized strings, because every call to read needs to make a new

243
00:29:27,480 --> 00:29:33,720
string to hand it back to you, then you can get a lot of memory fragmentation. If instead you have

244
00:29:33,720 --> 00:29:39,880
one byte array and you use it over and over and over and over, there's nothing happening to get

245
00:29:39,960 --> 00:29:45,160
fragmented. So don't do the byte array, I wouldn't do the byte array for the 4% speedup.

246
00:29:46,760 --> 00:29:51,720
I would do it because I wanted to control my memory profile, but only if I knew that was a

247
00:29:51,720 --> 00:29:58,920
problem in my application domain. All right, now we go on to another and more interesting situation

248
00:29:58,920 --> 00:30:05,000
using the byte array as the accumulator. Fun question for people doing new network programming,

249
00:30:05,000 --> 00:30:15,960
how many bytes will receive 1024 return? The answer is one. Or more if the network stack

250
00:30:15,960 --> 00:30:22,360
is in the mood, but you're only guaranteed one. And this is the opposite, a file IO. File IO,

251
00:30:22,360 --> 00:30:29,000
you ask for 128K, if there's 128K left in the file, it will wait for the disk to spin, it will

252
00:30:29,000 --> 00:30:34,600
wait for the head to be in the right place, it will leave you paused until a full 128K is ready

253
00:30:34,600 --> 00:30:40,040
for delivery, and then wake you back up. The network is the opposite. Receive will block

254
00:30:40,040 --> 00:30:46,760
only until at least a single byte is available and then set you off running to process it.

255
00:30:46,760 --> 00:30:53,320
And that can happen if your buffer size happens to be just a little less than the size of the last

256
00:30:53,320 --> 00:30:59,000
few packets that arrive. You can have a call to receive that finds only one or two bytes left,

257
00:30:59,640 --> 00:31:07,720
meaning unlike in the case where we were choosing our read size for files, usually it's the network,

258
00:31:07,720 --> 00:31:14,040
it's the clients you're communicating with that kind of decide how big your chunks of IOR when

259
00:31:14,040 --> 00:31:18,120
you're talking on the network. So you're always potentially in the case where you're dealing

260
00:31:18,120 --> 00:31:24,760
with little pieces of data. This fact, by the way, that you always are given an answer when

261
00:31:24,760 --> 00:31:31,000
even just a few bytes can be sent or received is why new network programmers tend to get into the,

262
00:31:31,000 --> 00:31:36,680
but it worked when I ran against local host problem. They get into that situation because

263
00:31:36,680 --> 00:31:40,840
when you run your server that you've just written and your little client that you've just written

264
00:31:40,840 --> 00:31:46,600
on local host, the OS will send enormous blocks of data back and forth between the two processes.

265
00:31:48,120 --> 00:31:52,680
Then they'll take it to their team and say, look what I wrote, try it between two different

266
00:31:52,680 --> 00:31:57,960
machines and it'll hang and never get all of the data because they didn't learn on local host

267
00:31:57,960 --> 00:32:03,400
that you receive will often just give you a few thousand bytes and you need to keep at it

268
00:32:03,400 --> 00:32:10,920
and watch until everything you need has arrived. So what is it like to use a traditional receive

269
00:32:10,920 --> 00:32:18,360
solution getting a new string each time holding the new data that's come in? This is what it looks

270
00:32:19,080 --> 00:32:25,560
like. Again, here we're getting lots of maybe little pieces of data which I'm simulating by

271
00:32:25,560 --> 00:32:32,920
only asking for a single Ethernet packet length. So even when I run this on local host, it'll pretend

272
00:32:32,920 --> 00:32:41,640
like packets are coming in. This is what many Python programmers start with. They just create

273
00:32:41,720 --> 00:32:49,560
an empty string and they plus equal more data to it each time. In Python tutorials, many of

274
00:32:49,560 --> 00:32:54,920
you will have seen this, the creating of the string and data plus equals more as an anti-pattern

275
00:32:54,920 --> 00:32:59,720
that you avoid because I tried running this. How long does the plus equals approach take?

276
00:33:00,680 --> 00:33:06,360
Infinity time. Meaning that I finally needed my laptop back so I killed it and I'll never

277
00:33:06,360 --> 00:33:10,920
know how long the loop would have taken to read my gigabyte of data. But when you do plus equals,

278
00:33:10,920 --> 00:33:16,200
you're asking Python to create a little string and then your first plus equals makes a slightly

279
00:33:16,200 --> 00:33:21,400
longer one. Your next plus equals through the loop creates a slightly longer one into which

280
00:33:21,400 --> 00:33:26,440
all the data from the second string has to be copied to make the third one. Then you go through

281
00:33:26,440 --> 00:33:32,040
the loop again and now you have to copy all that data again to make your fourth string. And if you

282
00:33:32,040 --> 00:33:37,400
have a million bytes to read, you wind up doing half a trillion operations. It's called an order

283
00:33:37,400 --> 00:33:42,120
in squared algorithm, generally to be avoided if you wanted to finish by lunchtime.

284
00:33:43,960 --> 00:33:49,880
So this is what we tell everyone to do. Pivot to keeping a list of blocks that you've received

285
00:33:49,880 --> 00:33:56,120
and join them together at the end in a single step. Python's much better at that. This actually

286
00:33:56,120 --> 00:34:01,000
finished on my laptop. It's the traditional way of accumulating data from the internet

287
00:34:01,000 --> 00:34:06,920
in Python or from a network. Took about a little more than a second to read in a gig of data

288
00:34:06,920 --> 00:34:16,680
in those small 1.5K chunks. Now, there is a version like read into, but that receives

289
00:34:16,680 --> 00:34:22,120
into a byte array you've already built instead of building a new string, but it now runs into a

290
00:34:22,120 --> 00:34:28,600
problem. When we do read into or receive into, where does it put the data? At the beginning of the

291
00:34:28,600 --> 00:34:33,480
array. And all of our incoming blocks will overwrite each other. What we want as more and more

292
00:34:33,480 --> 00:34:39,480
of blocks come in from the network is to arrange them along our byte array. So that memory view

293
00:34:39,480 --> 00:34:45,400
slicing expense that I added in its statement to avoid whenever possible in the previous code,

294
00:34:45,400 --> 00:34:53,240
it now becomes mandatory. Again, this ability with a view to write into byte locations that

295
00:34:53,240 --> 00:34:58,120
aren't at the beginning, but are somewhere in the middle of the byte array that you've built.

296
00:34:58,120 --> 00:35:02,520
The first block can go at the beginning, but you need to build a memory view to target the

297
00:35:02,600 --> 00:35:10,840
second block after the first block, the third block after that, and so forth. And so you need

298
00:35:10,840 --> 00:35:18,280
to build a memory view and you're going to need to use it to target that receive V into at subsequent

299
00:35:19,320 --> 00:35:25,080
positions inside of your big byte array. I'm presuming that you know the content

300
00:35:25,080 --> 00:35:29,320
like the head of time and have preallocated it and you're waiting to fill it with data.

301
00:35:30,280 --> 00:35:36,280
This takes about eight tenths of a second because we a bit of a win here because you

302
00:35:36,280 --> 00:35:40,760
haven't had to build a list, you haven't had to call join, you haven't built a bunch of intermediate

303
00:35:40,760 --> 00:35:46,840
data structures. It actually is a significant win when you need to keep the data that you're

304
00:35:46,840 --> 00:35:53,240
reading rather than just immediately passing it back to the OS. Another possibility I saw on

305
00:35:53,240 --> 00:35:58,440
someone's blog is to do an old fashioned receive of a new string and then try to do a byte array

306
00:35:58,440 --> 00:36:05,080
extend to grow your byte array with these new strings. It copies the data twice but does get

307
00:36:05,080 --> 00:36:12,280
rid of that join concatenation. It looks something like this, data.extend down there near the bottom.

308
00:36:13,240 --> 00:36:21,000
It is not a win over the normal way of using byte arrays because it turns out byte array extend

309
00:36:21,000 --> 00:36:29,480
is pretty inefficient. It asks for an iterator over its argument and then calls the iterator's

310
00:36:29,480 --> 00:36:35,320
next function over and over for every byte and then asks the int object what its value is

311
00:36:36,200 --> 00:36:42,440
so that it can then put it in an intermediate array and that involves having to increment and

312
00:36:42,440 --> 00:36:47,160
decrement the reference pointer of the integer it's given and by the time you're done you can

313
00:36:47,240 --> 00:36:52,600
compute that you've done at least 40 bytes of bandwidth to RAM even ignoring the instructions

314
00:36:52,600 --> 00:36:59,640
and stacks and arguments that are passing in order to get that single byte value extended onto the

315
00:36:59,640 --> 00:37:07,800
end of your byte array plus it doesn't write to your byte array. It writes to an intermediate

316
00:37:07,800 --> 00:37:12,280
buffer that it grows dynamically and then does the append when it's done so that should that

317
00:37:12,360 --> 00:37:18,280
iteration die part way through you don't wind up having modified the byte array some it wants to

318
00:37:18,280 --> 00:37:24,680
either succeed or fail as an atomic operation so that's why it's kind of slow kind of klugey

319
00:37:25,640 --> 00:37:31,640
but seeing that blog post made me ask a question does the byte array have an append operation

320
00:37:31,640 --> 00:37:37,240
that's any good? I mean surely the people writing it knew that we'd want to do that without spinning

321
00:37:37,240 --> 00:37:45,000
up an iterator and calling it 1500 times does it have an operation that's really good and yes it

322
00:37:45,000 --> 00:37:53,080
does I read the c code to find it now think about it where would you put the real extend operator

323
00:37:53,080 --> 00:37:59,880
the real ability to make your byte array longer obviously you'd hide it behind the operator

324
00:37:59,880 --> 00:38:04,920
that we've spent 20 years telling people to never use with string values

325
00:38:08,120 --> 00:38:16,600
this might be so difficult that some of you will never do it but if you can convince yourself to

326
00:38:16,600 --> 00:38:24,040
type this after all of this time this is actually something that byte arrays do magnificently just

327
00:38:24,040 --> 00:38:32,360
plus equal the additional data to your byte array and you will be even receive into's ability

328
00:38:32,360 --> 00:38:39,720
to grow your array with data so this case where we need to accumulate and keep the whole pay road

329
00:38:39,720 --> 00:38:45,240
is a real win for the byte array in all of the approaches and there don't seem to be sharp edges

330
00:38:45,240 --> 00:38:52,760
that can suddenly make it behave much worse than the the list and joint 33 speed up in that last

331
00:38:52,760 --> 00:38:59,800
version of the algorithm and cleaner code I mean admit it you've always wanted to just plus equal

332
00:38:59,800 --> 00:39:04,840
haven't you it's the natural way to write it in python and this is one of those neat intersections

333
00:39:04,840 --> 00:39:13,240
of the fast way with the way that looks good on the page as well so I'm not going to talk about

334
00:39:13,240 --> 00:39:17,720
send you might think I'll now get into the fact that send doesn't always send the whole payload

335
00:39:17,720 --> 00:39:23,720
but python has long had you covered here python sockets have for a very long time had a send

336
00:39:23,720 --> 00:39:31,080
all that sits in a loop in c sending shorter and shorter tails of your data until finally the os

337
00:39:32,680 --> 00:39:37,560
buffers have been able to receive it all so I don't see that we need byte arrays for that

338
00:39:37,560 --> 00:39:44,600
and I can declare the byte array the winner if you need an accumulator if you need to very quickly

339
00:39:44,600 --> 00:39:51,400
in a performance sensitive environment accumulate a lot of incoming data it is a noticeably good

340
00:39:51,400 --> 00:39:57,160
win with two different techniques that work pretty well I'll briefly mention that some people want

341
00:39:57,160 --> 00:40:02,600
a freestyle mutable string when they see the byte array they don't think of io they don't think of

342
00:40:02,600 --> 00:40:09,480
bloom filters and bit vectors they want to mess with a string they want a string that they can just

343
00:40:09,480 --> 00:40:17,400
change and all ish I have not found yet a good use for this and I'll sort of show you why it

344
00:40:17,400 --> 00:40:24,120
winds up being awkward you want to change part of a payload before using storing or retransmitting

345
00:40:24,120 --> 00:40:27,800
that would be the use case here because if you want to lowercase the whole thing you have to

346
00:40:27,800 --> 00:40:32,600
touch all the bytes anyway so you might as well build a new one good thing is that the byte array

347
00:40:32,600 --> 00:40:38,600
is mutable you can get it and change it but none of the methods that it shares with strings

348
00:40:39,640 --> 00:40:46,920
do mutation to it if you call dot upper on your byte array you get a new byte array so you have

349
00:40:46,920 --> 00:40:54,760
a mutable string type that does nothing string like mutably a byte array only changes when subjected

350
00:40:54,760 --> 00:41:01,880
to list like operations like assignment to an index or assignment to a slice or dot clear

351
00:41:01,880 --> 00:41:06,920
and so the result if you try writing a network algorithm or something with this is curious

352
00:41:06,920 --> 00:41:13,640
you have a mutable string that alas does no mutation precisely when you start calling

353
00:41:13,640 --> 00:41:20,600
its string methods want to lowercase a word well you're going to have to make a copy to call lower

354
00:41:20,600 --> 00:41:26,440
on you're going to have to do slicing giving you a copy call lower making another copy and then

355
00:41:26,440 --> 00:41:33,240
assignment to copy it a third time back into the data structure in order to get that accomplished

356
00:41:33,240 --> 00:41:39,800
there isn't I looked there isn't a lower into and an upper into that would let you do smaller

357
00:41:39,800 --> 00:41:45,720
grained uh manipulations that wrote directly to your new byte array can the memory view save us

358
00:41:45,720 --> 00:41:55,560
though it did in all of the previous occasions no because memory views don't do anything string

359
00:41:55,560 --> 00:42:01,560
like the moment you move to a memory view which lets you look at a piece of string efficiently

360
00:42:02,520 --> 00:42:07,240
you're not going to be able to do anything string like to it so you have to do a round

361
00:42:07,240 --> 00:42:16,360
trip out to a smaller string to do a manipulation and store data back to mutate a byte array without

362
00:42:16,360 --> 00:42:22,040
rewriting its whole content you're going to need indexes do you remember indexes back the first

363
00:42:22,040 --> 00:42:28,600
week before you'd found split strip and join and we're like doing everything like this you get to

364
00:42:28,600 --> 00:42:34,280
do it again if you decide to try mutating a byte array the byte array will let you enjoy those

365
00:42:34,280 --> 00:42:40,920
days all over again because all the mutation operations are powered only by indexes one hint

366
00:42:40,920 --> 00:42:47,400
regular expressions while they turned off a lot of other string like things were left turned on

367
00:42:47,400 --> 00:42:54,120
and do work against byte arrays and can help give you some useful indexes to use freestyle

368
00:42:54,120 --> 00:43:01,720
mutable string it's awkward I have here at the end of the slides some uh links and pointers to other

369
00:43:01,720 --> 00:43:06,920
documentation including the blog posts that inspired this talk and made me want to bring

370
00:43:06,920 --> 00:43:13,720
everything together in one place in conclusion the byte array it is a very memory efficient

371
00:43:13,720 --> 00:43:21,000
not faster but memory efficient store of byte integers should you ever need them it can help

372
00:43:21,000 --> 00:43:26,680
control memory fragmentation when doing uh high-performance i o because you don't have to create

373
00:43:26,680 --> 00:43:33,240
a new string but it's hard to make it faster in a reliable way be careful it's a great way to

374
00:43:33,240 --> 00:43:38,840
accumulate data that's coming in a piece at a time that's its real superpower and though it's

375
00:43:38,840 --> 00:43:46,040
very tempting for string operations it's also a bit underpowered and a bit awkward that's what

376
00:43:46,040 --> 00:44:04,840
I've learned so far thank you very much we are out of time so I will meet interested byte array

377
00:44:04,840 --> 00:44:17,640
fans outside the door in a few minutes

