1
00:00:00,000 --> 00:00:05,240
â€¦uchos MARH

2
00:00:08,140 --> 00:00:12,140
welcome to this evening's lecture

3
00:00:12,140 --> 00:00:15,660
in the splendor of the Sheldonian theatre.

4
00:00:15,880 --> 00:00:19,820
Hollol teis endseth at Oxford UK University,

5
00:00:19,960 --> 00:00:23,440
part of the OpenSeed Challenge lectures

6
00:00:23,460 --> 00:00:26,660
on artificial intelligence and human values.

7
00:00:26,660 --> 00:00:30,060
My name is Nigel Shadbolt, principal of Jesus College.

8
00:00:30,060 --> 00:00:35,460
I'm also a professor of computer science here in Oxford and chair the Institute Steering Group.

9
00:00:35,460 --> 00:00:40,260
It was my privilege to help set up the Institute which brings together world-leading philosophers

10
00:00:40,260 --> 00:00:47,260
and other experts in the humanities with the researchers, developers and users of AI.

11
00:00:47,260 --> 00:00:50,660
The director of the Institute is Professor John Tosulis,

12
00:00:50,660 --> 00:00:55,060
and its ultimate home will be the Stephen A. Schwarzman Centre for the Humanities,

13
00:00:55,060 --> 00:00:58,860
whose construction is soon to start.

14
00:00:58,860 --> 00:01:02,860
In recent years, AI has gone from strength to strength.

15
00:01:02,860 --> 00:01:05,060
It's now ubiquitous.

16
00:01:05,060 --> 00:01:09,660
In our phones, the games we play, in our cars, our drug discovery companies,

17
00:01:09,660 --> 00:01:14,260
the search engines we use and the translation tools we depend on.

18
00:01:14,260 --> 00:01:18,260
Much of that is down to a new generation of AI methods and techniques

19
00:01:18,260 --> 00:01:22,260
that are powered by modern machine learning algorithms,

20
00:01:22,260 --> 00:01:28,060
great swathes of data and the prodigious power of modern-day computing hardware.

21
00:01:28,060 --> 00:01:31,060
Some of AI's most dramatic recent accomplishments

22
00:01:31,060 --> 00:01:37,060
owe a great deal to our speaker here with us this evening and the company he co-founded.

23
00:01:37,060 --> 00:01:40,460
Demis Arsabes, CEO and co-founder of DeepMind,

24
00:01:40,460 --> 00:01:44,060
one of the world's leading AI research companies.

25
00:01:44,060 --> 00:01:47,860
Demis' own career and intellectual journey is an extraordinary one.

26
00:01:47,860 --> 00:01:51,660
A chess prodigy, hugely successful computer games developer,

27
00:01:51,660 --> 00:01:55,260
with a double first in computer science from Cambridge.

28
00:01:55,260 --> 00:01:58,860
Demis has always been fascinated by the human brain,

29
00:01:58,860 --> 00:02:02,260
understanding how it gives rise to intelligence.

30
00:02:02,260 --> 00:02:07,660
After the success of his games company, he went on to a PhD in cognitive neuroscience at UCL,

31
00:02:07,660 --> 00:02:10,660
followed by a Henry Welcombe Postdoctoral Research Fellowship

32
00:02:10,660 --> 00:02:15,660
at the Gatsby Computational Neuroscience Unit, also at UCL.

33
00:02:15,660 --> 00:02:19,860
His papers in cognitive neuroscience investigated imagination,

34
00:02:19,860 --> 00:02:25,660
memory and amnesia and appeared in leading journals such as Nature and Science.

35
00:02:25,660 --> 00:02:28,460
He combined his interest in computing and neuroscience

36
00:02:28,460 --> 00:02:31,660
with the formation of DeepMind in 2010.

37
00:02:31,660 --> 00:02:35,260
It's compelling ambition to solve intelligence

38
00:02:35,260 --> 00:02:39,060
and then use intelligence to solve everything else.

39
00:02:39,060 --> 00:02:43,660
He and his team used games as the context in which to test new ideas

40
00:02:43,660 --> 00:02:49,260
about how to build AI systems using machine learning methods inspired by neuroscience.

41
00:02:49,340 --> 00:02:53,060
First arcade games and then famously Go.

42
00:02:53,060 --> 00:02:57,060
A previous talk here in the Sheldonian in February 2016

43
00:02:57,060 --> 00:03:04,260
prefigured AlphaGo winning 4-1 against former world champion Lee Sodol just a month later.

44
00:03:04,260 --> 00:03:09,860
Games have proven to be a great training ground for developing and testing AI algorithms,

45
00:03:09,860 --> 00:03:13,860
but the aim of DeepMind has always been to build general learning systems

46
00:03:13,860 --> 00:03:17,860
ultimately capable of solving important problems in the real world.

47
00:03:17,860 --> 00:03:21,860
DeepMind's AlphaFold system is a solution to the 50-year grand challenge

48
00:03:21,860 --> 00:03:26,260
of protein structure prediction, culminating the release of the most accurate

49
00:03:26,260 --> 00:03:29,460
and complete picture of the human proteome.

50
00:03:29,460 --> 00:03:34,260
A core aim for the Institute for Ethics at AI is to bring together world-leading academics

51
00:03:34,260 --> 00:03:37,260
and the practitioners at the cutting edge of AI development.

52
00:03:37,260 --> 00:03:42,660
Tonight we will hear first hand experience of AI's enormous potential

53
00:03:42,660 --> 00:03:45,260
to accelerate scientific discovery.

54
00:03:45,260 --> 00:03:50,660
Experience which will inform our research and thinking about the critical ethical considerations

55
00:03:50,660 --> 00:03:56,060
that must be considered by policy makers and technical developers of AI.

56
00:03:56,060 --> 00:04:01,860
Demis has predicted that artificial intelligence will be one of the most beneficial technologies ever

57
00:04:01,860 --> 00:04:04,860
but that significant ethical issues remain.

58
00:04:04,860 --> 00:04:09,860
Please join me in welcoming Demis Hasabas to deliver tonight's Tana lecture

59
00:04:09,860 --> 00:04:12,860
using AI to accelerate scientific discovery.

60
00:04:15,260 --> 00:04:29,260
Thank you. Thank you, Senaigel, for such a great introduction.

61
00:04:29,260 --> 00:04:33,260
It's a real pleasure to be back here in Oxford in the Shadonian

62
00:04:33,260 --> 00:04:36,260
and giving the Tana lecture. It's a real honour.

63
00:04:36,260 --> 00:04:41,260
What I'm going to talk about today is using AI to accelerate scientific discovery.

64
00:04:41,260 --> 00:04:45,260
As you'll see throughout my talk, this was my original motivation

65
00:04:45,260 --> 00:04:49,260
and has always been my motivation behind spending my entire career

66
00:04:49,260 --> 00:04:52,260
and trying to make AI a reality.

67
00:04:52,260 --> 00:04:55,260
I'm going to talk a lot about some of our most recent advances

68
00:04:55,260 --> 00:04:59,260
actually now coming to fruition, especially the last year or two,

69
00:04:59,260 --> 00:05:03,260
of using AI to crack difficult scientific problems.

70
00:05:03,260 --> 00:05:06,260
I'm also going to talk about the lead up to there

71
00:05:06,260 --> 00:05:10,260
and how I think about the games work we did originally and the foundation work we did originally.

72
00:05:10,260 --> 00:05:15,260
Last time I talked here was just before the AlphaGo match in Korea.

73
00:05:15,260 --> 00:05:19,260
That was a major moment for us and how in the last five or six years

74
00:05:19,260 --> 00:05:23,260
things have progressed enormously.

75
00:05:23,260 --> 00:05:29,260
Just to talk a little bit about what our vision was behind DeepMind back in 2010.

76
00:05:29,260 --> 00:05:32,260
It's quite hard to remember the state of AI back in 2010

77
00:05:32,260 --> 00:05:36,260
because today, as Nigel was saying, AI is ubiquitous all around us.

78
00:05:36,260 --> 00:05:39,260
It's one of the biggest buzzwords in industry.

79
00:05:39,260 --> 00:05:43,260
It's hard to remember just 12 years ago

80
00:05:43,260 --> 00:05:46,260
almost nobody was talking about AI, I would say,

81
00:05:46,260 --> 00:05:51,260
and it was almost impossible to actually get funding in the private sector for AI at all.

82
00:05:51,260 --> 00:05:54,260
We have many funny stories back in the day

83
00:05:54,260 --> 00:05:57,260
of trying to do some fundraising back in 2009 and 2010

84
00:05:57,260 --> 00:06:00,260
and most people thinking we were completely mad

85
00:06:00,260 --> 00:06:03,260
to be embarking on this journey.

86
00:06:03,260 --> 00:06:07,260
We founded it with this in mind of trying to build one day

87
00:06:07,260 --> 00:06:11,260
a hollow programme-like effort to build AGI,

88
00:06:11,260 --> 00:06:13,260
artificial general intelligence.

89
00:06:13,260 --> 00:06:16,260
We use this term artificial general intelligence

90
00:06:16,260 --> 00:06:19,260
to distinguish it from normal everyday AI

91
00:06:19,260 --> 00:06:22,260
where we're talking about a general system

92
00:06:22,260 --> 00:06:27,260
that can perform well on many tasks to at least human level.

93
00:06:27,260 --> 00:06:31,260
That's the general aspect that we are always striving for

94
00:06:31,260 --> 00:06:33,260
in all the work that we do.

95
00:06:33,260 --> 00:06:36,260
We're still on this mission now and I think we've done

96
00:06:36,260 --> 00:06:41,260
a pretty good job of staying true to this original vision

97
00:06:41,260 --> 00:06:44,260
that we had in 2010 when we were just a few people

98
00:06:44,260 --> 00:06:49,260
in a small little office in an attic in Russell Square.

99
00:06:49,260 --> 00:06:52,260
As Nigel said, our original mission statement

100
00:06:52,260 --> 00:06:54,260
was step one, solve intelligence,

101
00:06:54,260 --> 00:06:57,260
step two, use it to solve everything else.

102
00:06:57,260 --> 00:07:00,260
We have updated that mission statement a little bit,

103
00:07:00,260 --> 00:07:03,260
still means the same thing, but just to be a little bit more descriptive now

104
00:07:03,260 --> 00:07:05,260
in the last few years, just to be a bit clearer

105
00:07:05,260 --> 00:07:07,260
about what we mean by solving everything else,

106
00:07:07,260 --> 00:07:09,260
what exactly are we talking about.

107
00:07:09,260 --> 00:07:13,260
The way we discuss our mission now is solving intelligence

108
00:07:13,260 --> 00:07:17,260
to advance science and of course for the benefit of humanity.

109
00:07:17,260 --> 00:07:21,260
That's always been the cornerstone of what we think about

110
00:07:21,260 --> 00:07:24,260
when we think about what should we apply AI to.

111
00:07:26,260 --> 00:07:30,260
Now, there are two, broadly two ways that I think AI

112
00:07:30,260 --> 00:07:32,260
can be attempted to be built.

113
00:07:32,260 --> 00:07:37,260
One is the more traditional way of building logic systems

114
00:07:37,260 --> 00:07:41,260
or expert systems and these are hard coded systems

115
00:07:41,260 --> 00:07:44,260
that effectively teams of programmers solve the problem.

116
00:07:44,260 --> 00:07:46,260
They then incorporate those solutions

117
00:07:46,260 --> 00:07:49,260
in sometimes very clever expert systems,

118
00:07:49,260 --> 00:07:53,260
but the problem with them is that they are very limited

119
00:07:53,260 --> 00:07:55,260
in terms of what they can generalise to.

120
00:07:55,260 --> 00:07:57,260
They can't deal with the unexpected

121
00:07:57,260 --> 00:08:01,260
and they're basically limited to what the programmers

122
00:08:01,260 --> 00:08:05,260
foresaw, the situations that the system might be in.

123
00:08:05,260 --> 00:08:08,260
Of course, this line of work was inspired by mathematics

124
00:08:08,260 --> 00:08:10,260
and logic systems.

125
00:08:10,260 --> 00:08:13,260
On the other hand, the big renaissance in the last decade plus

126
00:08:13,260 --> 00:08:19,260
is the sort of progress of learning systems.

127
00:08:19,260 --> 00:08:24,260
Of course, in the 80s there was a flurry of work done on neural networks,

128
00:08:24,260 --> 00:08:26,260
then that died down.

129
00:08:26,260 --> 00:08:29,260
We now know that probably we didn't have enough computing power

130
00:08:29,260 --> 00:08:32,260
or data, maybe not the right algorithms as well.

131
00:08:32,260 --> 00:08:35,260
Basically, in essence, the ideas were correct.

132
00:08:35,260 --> 00:08:39,260
An idea of a learning system is that it learns for itself,

133
00:08:39,260 --> 00:08:41,260
solutions for itself from first principles,

134
00:08:41,260 --> 00:08:43,260
directly from experience.

135
00:08:43,260 --> 00:08:46,260
The amazing thing about these systems and their huge promise

136
00:08:46,260 --> 00:08:49,260
is that they can maybe generalise to tasks

137
00:08:49,260 --> 00:08:52,260
and that it's not being programmed for explicitly

138
00:08:52,260 --> 00:08:56,260
and maybe solve problems that we ourselves as the designers

139
00:08:56,260 --> 00:08:59,260
or scientists behind those systems don't know how to solve.

140
00:08:59,260 --> 00:09:01,260
Of course, that's the huge potential

141
00:09:01,260 --> 00:09:04,260
and also the risk of these kinds of systems.

142
00:09:04,260 --> 00:09:08,260
Originally, these learning systems took a lot of inspiration

143
00:09:08,260 --> 00:09:10,260
and also could be validated, some of the ideas

144
00:09:10,260 --> 00:09:13,260
like reinforcement learning and neural networks

145
00:09:13,260 --> 00:09:17,260
by systems neuroscience in comparing what these systems do,

146
00:09:17,260 --> 00:09:20,260
comparing them on a systems and algorithmic level

147
00:09:20,260 --> 00:09:23,260
to what we know about how the brain works.

148
00:09:24,260 --> 00:09:26,260
Everything we do at DeepMind, of course,

149
00:09:26,260 --> 00:09:29,260
is on the learning system side.

150
00:09:29,260 --> 00:09:33,260
We've been lucky enough to be in the vanguard of this almost revolution

151
00:09:33,260 --> 00:09:37,260
or anasence in the last decade of these types of approaches.

152
00:09:37,260 --> 00:09:41,260
How do we think about what's our special take

153
00:09:41,260 --> 00:09:44,260
on learning systems and how powerful they can be?

154
00:09:44,260 --> 00:09:49,260
There are two component algorithms or approaches,

155
00:09:49,260 --> 00:09:51,260
one could say, that we've fused together.

156
00:09:51,260 --> 00:09:54,260
Of course, there's deep learning or deep neural networks

157
00:09:54,260 --> 00:09:57,260
and the way I think about this is that the deep neural network system

158
00:09:57,260 --> 00:10:00,260
is there to build a model of the environment

159
00:10:00,260 --> 00:10:03,260
of the data and the experience.

160
00:10:03,260 --> 00:10:05,260
Then what do you use that model for?

161
00:10:05,260 --> 00:10:08,260
Well, you can use reinforcement learning,

162
00:10:08,260 --> 00:10:13,260
which is a sort of goal seeking and reward maximising system.

163
00:10:13,260 --> 00:10:17,260
You can use that model and use it to plan

164
00:10:17,260 --> 00:10:22,260
and basically plan and take actions towards a goal,

165
00:10:22,260 --> 00:10:25,260
a goal that may be specified by the designers of that system.

166
00:10:25,260 --> 00:10:29,260
You have the model and then you have the action

167
00:10:29,260 --> 00:10:33,260
and goal-solving element of the systems.

168
00:10:33,260 --> 00:10:38,260
One of our early innovations was to fuse those two things together at scale.

169
00:10:38,260 --> 00:10:41,260
We call it deep reinforcement learning now.

170
00:10:41,260 --> 00:10:45,260
The cool thing about these systems is that they can discover new knowledge

171
00:10:45,260 --> 00:10:49,260
from first principles through this process of trial and error

172
00:10:49,260 --> 00:10:51,260
using these models.

173
00:10:51,260 --> 00:10:54,260
The idea here on this diagram of the agent system

174
00:10:54,260 --> 00:10:57,260
is it gets observations from the environment.

175
00:10:57,260 --> 00:11:00,260
Those observations go towards building and updating

176
00:11:00,260 --> 00:11:03,260
an internal model of how the environment works

177
00:11:03,260 --> 00:11:06,260
and the transition matrices of the environment.

178
00:11:06,260 --> 00:11:09,260
There's some goal it's trying to solve in the environment,

179
00:11:09,260 --> 00:11:12,260
and then after its thinking time has run out,

180
00:11:12,260 --> 00:11:16,260
it has to select an action from the action set

181
00:11:16,260 --> 00:11:18,260
available to it at that moment in time

182
00:11:18,260 --> 00:11:21,260
that will best get it incrementally towards its goal.

183
00:11:21,260 --> 00:11:23,260
Then the action gets output.

184
00:11:23,260 --> 00:11:25,260
It may or may not make a change in the environment.

185
00:11:25,260 --> 00:11:30,260
That drives a new observation and then the model updates further.

186
00:11:30,260 --> 00:11:33,260
You can see with this type of system,

187
00:11:33,260 --> 00:11:37,260
the AI system is actually an active learner.

188
00:11:37,260 --> 00:11:39,260
It participates in its own learning.

189
00:11:39,260 --> 00:11:43,260
The decisions it makes in large part governs what experiences

190
00:11:43,260 --> 00:11:47,260
and what data it will get next to learn more from.

191
00:11:47,260 --> 00:11:50,260
Although this is a pretty simple diagram

192
00:11:50,260 --> 00:11:53,260
and basically describes the whole of reinforcement learning,

193
00:11:53,260 --> 00:11:55,260
the reinforcement learning problem,

194
00:11:55,260 --> 00:11:57,260
there's huge complexities of course of theoretical

195
00:11:57,260 --> 00:12:01,260
and practical complexities underlying this diagram

196
00:12:01,260 --> 00:12:03,260
that need to be solved.

197
00:12:03,260 --> 00:12:06,260
We know that in the limit this must work

198
00:12:06,260 --> 00:12:10,260
because this is how mammalian brains work, including humans.

199
00:12:10,260 --> 00:12:14,260
This is one of the learning mechanisms that we have in our own brains.

200
00:12:14,260 --> 00:12:16,260
Reinforcement learning was found to be implemented

201
00:12:16,260 --> 00:12:21,260
by dopamine neurons in the brain in the late 90s.

202
00:12:21,260 --> 00:12:23,260
We know if we push this hard enough,

203
00:12:23,260 --> 00:12:28,260
this should be one path towards general artificial intelligence.

204
00:12:28,260 --> 00:12:30,260
What did we famously use this for?

205
00:12:30,260 --> 00:12:35,260
AlfaGo was the program that I think we did a lot of things before this.

206
00:12:35,260 --> 00:12:39,260
Like Atari games and other proof points.

207
00:12:39,260 --> 00:12:43,260
AlfaGo was really our first attempt at doing this at a huge scale

208
00:12:43,260 --> 00:12:46,260
to crack a big problem that was unsolved in AI,

209
00:12:46,260 --> 00:12:49,260
one of the holy grails of AI research,

210
00:12:49,260 --> 00:12:53,260
which was a program to beat the world champion at the Game of Go.

211
00:12:53,260 --> 00:12:57,260
I want to talk a little bit about this in hindsight now,

212
00:12:57,260 --> 00:13:02,260
knowing what I know now, how I've reinterpreted what we did with AlfaGo.

213
00:13:02,260 --> 00:13:05,260
I think I can explain it in a much more simple in general way

214
00:13:05,260 --> 00:13:09,260
than perhaps how I was explaining it back five, six years ago

215
00:13:09,260 --> 00:13:12,260
when we were in the midst of building this system.

216
00:13:12,260 --> 00:13:15,260
Just for those of you who don't know...

217
00:13:16,260 --> 00:13:18,260
I don't know why that's not updating.

218
00:13:21,260 --> 00:13:22,260
There we go.

219
00:13:22,260 --> 00:13:24,260
This is the Game of Go.

220
00:13:26,260 --> 00:13:29,260
This is the Game of Go, the board game.

221
00:13:29,260 --> 00:13:35,260
It's a phenomenal game and it's a much more esoteric game

222
00:13:35,260 --> 00:13:39,260
and artistic game, one could say, than chess.

223
00:13:39,260 --> 00:13:43,260
It occupies the same intellectual echelon chess stars in the West.

224
00:13:43,260 --> 00:13:48,260
In China, in Japan, in Korea and other Asian countries, they play Go.

225
00:13:48,260 --> 00:13:55,260
Go has resisted old-fashioned logic system and expert system approaches,

226
00:13:55,260 --> 00:13:58,260
whereas chess was solved by those things

227
00:13:58,260 --> 00:14:00,260
because of various factors.

228
00:14:00,260 --> 00:14:03,260
One is the search space is truly enormous in Go.

229
00:14:03,260 --> 00:14:08,260
It's roughly 10 to the power, 170 possible board positions,

230
00:14:08,260 --> 00:14:11,260
which is way more than there are atoms in the universe,

231
00:14:11,260 --> 00:14:14,260
so there's no way one could exhaustively search

232
00:14:14,260 --> 00:14:19,260
all of the possible board positions in order to find the right path through.

233
00:14:19,260 --> 00:14:23,260
Even bigger problem actually is that it's impossible,

234
00:14:23,260 --> 00:14:27,260
or thought was, it was impossible to write down an evaluation function.

235
00:14:27,260 --> 00:14:29,260
To hand code an evaluation function,

236
00:14:29,260 --> 00:14:32,260
which is what most modern-day chess programs use.

237
00:14:32,260 --> 00:14:36,260
The reason is because Go is such an esoteric game.

238
00:14:36,260 --> 00:14:39,260
It doesn't have materiality in chess.

239
00:14:39,260 --> 00:14:43,260
As a first approximation, one can add up the piece values on both sides,

240
00:14:43,260 --> 00:14:46,260
and that will tell you very crudely,

241
00:14:46,260 --> 00:14:48,260
which side is winning in that position.

242
00:14:48,260 --> 00:14:52,260
You need to know that in order to make decisions about what to do next.

243
00:14:52,260 --> 00:14:56,260
Many people are tempted to, over 20 years since Deep Blue,

244
00:14:56,260 --> 00:15:01,260
attempted to write to construct these evaluation functions for Go.

245
00:15:01,260 --> 00:15:05,260
One of the issues is that Go players themselves do not know,

246
00:15:05,260 --> 00:15:08,260
consciously at least, what that information is.

247
00:15:08,260 --> 00:15:12,260
Because it's so complex a game,

248
00:15:12,260 --> 00:15:16,260
they actually use their intuition rather than explicit calculation

249
00:15:16,260 --> 00:15:18,260
in order to deal with the complexity of Go.

250
00:15:18,260 --> 00:15:22,260
Whereas chess players, if you ask them how, why did they make a decision,

251
00:15:22,260 --> 00:15:26,260
a chess grandmaster will be able to tell you explicitly

252
00:15:26,260 --> 00:15:28,260
the various factors involved.

253
00:15:28,260 --> 00:15:30,260
A Go player generally won't do that.

254
00:15:30,260 --> 00:15:32,260
They'll just say things like it felt right.

255
00:15:32,260 --> 00:15:34,260
This felt like the right move,

256
00:15:34,260 --> 00:15:37,260
which is what I think also makes Go an incredible game.

257
00:15:37,260 --> 00:15:41,260
Of course, intuition is not something one would associate

258
00:15:41,260 --> 00:15:45,260
with computer programs, especially logic systems.

259
00:15:45,260 --> 00:15:48,260
Maybe in the Q&A we can discuss a little bit more

260
00:15:48,260 --> 00:15:50,260
about what intuition may be.

261
00:15:50,260 --> 00:15:54,260
But I don't think it's my conclusion now,

262
00:15:54,260 --> 00:15:56,260
after doing all these games,

263
00:15:56,260 --> 00:15:58,260
and indeed some of the science things we've done,

264
00:15:58,260 --> 00:16:01,260
is that it's not some mysterious thing.

265
00:16:01,260 --> 00:16:04,260
It's actually information that our brain knows about

266
00:16:04,260 --> 00:16:06,260
and has learnt through experience, of course.

267
00:16:06,260 --> 00:16:09,260
I mean, there's no other way one can learn information.

268
00:16:09,260 --> 00:16:12,260
But it's just in the association courtesies.

269
00:16:12,260 --> 00:16:16,260
So it's not actually consciously available to a high-level cortex.

270
00:16:16,260 --> 00:16:19,260
So it seems mysterious to us how we ride a bike,

271
00:16:19,260 --> 00:16:23,260
when these sort of motor, sensory motor things we're able to do,

272
00:16:23,260 --> 00:16:25,260
because our conscious part of our brain

273
00:16:25,260 --> 00:16:27,260
cannot access those representations.

274
00:16:27,260 --> 00:16:29,260
And if we can't do that,

275
00:16:29,260 --> 00:16:33,260
then we definitely can't explicitly code it in some logic code,

276
00:16:33,260 --> 00:16:35,260
which is why traditionally those tasks,

277
00:16:35,260 --> 00:16:37,260
including things like computer vision,

278
00:16:37,260 --> 00:16:40,260
have been quite hard for logic systems to solve,

279
00:16:40,260 --> 00:16:43,260
even over the last 50 years.

280
00:16:43,260 --> 00:16:45,260
So a lot about what we were doing

281
00:16:45,260 --> 00:16:47,260
was trying to approximate this kind of intuition

282
00:16:47,260 --> 00:16:49,260
in these learning systems.

283
00:16:49,260 --> 00:16:50,260
So how did we work?

284
00:16:50,260 --> 00:16:52,260
And I'm actually going to describe,

285
00:16:52,260 --> 00:16:53,260
not just AlphaGo here,

286
00:16:53,260 --> 00:16:56,260
but the whole series of AlphaX programmes.

287
00:16:56,260 --> 00:17:01,260
So AlphaGo, the original one that beat Lisa Doll in 2016.

288
00:17:01,260 --> 00:17:04,260
And then AlphaGo Zero,

289
00:17:04,260 --> 00:17:07,260
that then didn't need human data to learn from,

290
00:17:07,260 --> 00:17:08,260
just learn for itself.

291
00:17:08,260 --> 00:17:10,260
And then finally AlphaZero,

292
00:17:10,260 --> 00:17:12,260
which could play any two player game.

293
00:17:12,260 --> 00:17:15,260
So I'm going to sort of describe them all,

294
00:17:15,260 --> 00:17:16,260
roughly speaking,

295
00:17:16,260 --> 00:17:20,260
with this sort of demonstrative diagram.

296
00:17:20,260 --> 00:17:22,260
So the way you can think of all of these systems

297
00:17:22,260 --> 00:17:26,260
is we're initially training a neural network through self-play.

298
00:17:26,260 --> 00:17:28,260
So the system plays against itself,

299
00:17:28,260 --> 00:17:32,260
and it learns to evaluate positions

300
00:17:32,260 --> 00:17:34,260
and to pick the most likely moves

301
00:17:34,260 --> 00:17:38,260
that are most useful for it to look at.

302
00:17:38,260 --> 00:17:40,260
So that's what it's got to do.

303
00:17:40,260 --> 00:17:42,260
Now initially, it starts with no knowledge.

304
00:17:42,260 --> 00:17:44,260
So you have an initialised neural network,

305
00:17:44,260 --> 00:17:46,260
it starts with zero knowledge.

306
00:17:46,260 --> 00:17:48,260
So it literally is moving randomly.

307
00:17:48,260 --> 00:17:51,260
So we can call that version one.

308
00:17:51,260 --> 00:17:52,260
That's the neural network.

309
00:17:52,260 --> 00:17:56,260
And what it does is it plays roughly 100,000 games against itself.

310
00:17:56,260 --> 00:17:59,260
And so that then becomes a data set.

311
00:17:59,260 --> 00:18:02,260
So that 100,000 games, we take that as a data set.

312
00:18:02,260 --> 00:18:03,260
And what we try to do with it

313
00:18:03,260 --> 00:18:06,260
is train a version two of that network,

314
00:18:06,260 --> 00:18:07,260
a new neural network.

315
00:18:07,260 --> 00:18:11,260
But we try and train it on this version one data set

316
00:18:11,260 --> 00:18:13,260
to predict in the middle of a position,

317
00:18:13,260 --> 00:18:14,260
in the middle of a game,

318
00:18:14,260 --> 00:18:16,260
from a position in the middle of a game,

319
00:18:16,260 --> 00:18:18,260
which side is going to win.

320
00:18:18,260 --> 00:18:20,260
So predict ahead of time.

321
00:18:20,260 --> 00:18:24,260
And also what sorts of moves does the V1 system choose

322
00:18:24,260 --> 00:18:26,260
in a particular position.

323
00:18:26,260 --> 00:18:30,260
So it's trying to be better at both those two things.

324
00:18:30,260 --> 00:18:33,260
And then what happens is we train that V2 system,

325
00:18:33,260 --> 00:18:35,260
and then we have a little mini tournament

326
00:18:35,260 --> 00:18:37,260
between V1 and V2.

327
00:18:37,260 --> 00:18:39,260
So it's roughly 100 games,

328
00:18:39,260 --> 00:18:40,260
and they have a little match off.

329
00:18:40,260 --> 00:18:44,260
And basically, if the V2 system

330
00:18:44,260 --> 00:18:46,260
hits a particular threshold win rate,

331
00:18:46,260 --> 00:18:48,260
55% in this case,

332
00:18:48,260 --> 00:18:52,260
then we say it's significantly better than V1.

333
00:18:52,260 --> 00:18:53,260
And if that's true,

334
00:18:53,260 --> 00:18:57,260
then what we do is we replace V1 with version two network,

335
00:18:57,260 --> 00:18:59,260
this new network in Purple.

336
00:18:59,260 --> 00:19:00,260
And that, of course,

337
00:19:00,260 --> 00:19:02,260
plays another 100,000 games against itself.

338
00:19:02,260 --> 00:19:04,260
And now it creates a new data set.

339
00:19:04,260 --> 00:19:07,260
But this data set now in Purple, in the middle,

340
00:19:07,260 --> 00:19:10,260
is slightly better quality than that first data set,

341
00:19:10,260 --> 00:19:12,260
because the player is slightly better.

342
00:19:12,260 --> 00:19:15,260
And to begin with, almost imperceptively better.

343
00:19:15,260 --> 00:19:17,260
So it's just slightly better than random now.

344
00:19:17,260 --> 00:19:20,260
But that's enough signal to then train,

345
00:19:20,260 --> 00:19:23,260
of course we train a version three system,

346
00:19:23,260 --> 00:19:25,260
and that plays off against version two.

347
00:19:25,260 --> 00:19:29,260
Now, if you don't reach this 55% win rate,

348
00:19:29,260 --> 00:19:32,260
what you do instead is you take back the version two,

349
00:19:32,260 --> 00:19:34,260
and you continue to generate more data with that,

350
00:19:34,260 --> 00:19:36,260
another 100,000 games.

351
00:19:36,260 --> 00:19:39,260
And you have 200,000 to train your next version three.

352
00:19:39,260 --> 00:19:41,260
And eventually that version three

353
00:19:41,260 --> 00:19:43,260
will be better than version two.

354
00:19:43,260 --> 00:19:48,260
So after one does this around 17 or 18 times,

355
00:19:48,260 --> 00:19:52,260
you go from random to better than world champion.

356
00:19:52,260 --> 00:19:54,260
That's it.

357
00:19:54,260 --> 00:19:58,260
And you can do this with any two player game,

358
00:19:58,260 --> 00:20:00,260
perfect information game.

359
00:20:00,260 --> 00:20:03,260
So the same network can do that.

360
00:20:03,260 --> 00:20:07,260
Get to better to world champion within 20 to 30 generations

361
00:20:07,260 --> 00:20:08,260
of doing this.

362
00:20:08,260 --> 00:20:10,260
So you literally, and we got to the point where so fast

363
00:20:10,260 --> 00:20:12,260
you literally set it off in the morning,

364
00:20:12,260 --> 00:20:14,260
you could play chess about it at lunchtime

365
00:20:14,260 --> 00:20:17,260
and maybe just beat it, and then by tea time,

366
00:20:17,260 --> 00:20:19,260
you know, you no chance.

367
00:20:19,260 --> 00:20:21,260
Literally in the day, you could actually see the evolution

368
00:20:21,260 --> 00:20:22,260
in one day.

369
00:20:22,260 --> 00:20:25,260
It's kind of incredible to watch as a chess player.

370
00:20:25,260 --> 00:20:27,260
So what is it doing then,

371
00:20:27,260 --> 00:20:31,260
in terms of thinking about this enormous search space?

372
00:20:31,260 --> 00:20:33,260
So what's happening is,

373
00:20:33,260 --> 00:20:36,260
and the sort of, I think, advance of AlphaGo,

374
00:20:36,260 --> 00:20:40,260
one of the advances was combining this neural network system,

375
00:20:40,260 --> 00:20:45,260
or model, with a kind of more classical tree search algorithm.

376
00:20:45,260 --> 00:20:48,260
In this case, we use Monte Carlo tree search.

377
00:20:48,260 --> 00:20:51,260
And you can think of the tree of possibilities

378
00:20:51,260 --> 00:20:53,260
looking a bit like this in Go,

379
00:20:53,260 --> 00:20:55,260
where each node here is a positioning in Go,

380
00:20:55,260 --> 00:20:58,260
obviously shown by these little mini Go boards.

381
00:20:58,260 --> 00:21:01,260
And you can imagine if you're some middle game position,

382
00:21:01,260 --> 00:21:03,260
you know, there's just this countless

383
00:21:03,260 --> 00:21:06,260
10 to the 170 possibilities in the limit.

384
00:21:06,260 --> 00:21:10,260
How is one supposed to find the needle in the haystack, right?

385
00:21:10,260 --> 00:21:12,260
The good moves that could be world champion

386
00:21:12,260 --> 00:21:15,260
or better level decisions.

387
00:21:15,260 --> 00:21:19,260
So what the neural network does is it constrains,

388
00:21:19,260 --> 00:21:24,260
that model constrains the search to things to make it tractable,

389
00:21:24,260 --> 00:21:26,260
to things that are reasonably likely to work,

390
00:21:26,260 --> 00:21:28,260
reasonably effective,

391
00:21:28,260 --> 00:21:30,260
and it can evaluate that at each node level

392
00:21:30,260 --> 00:21:32,260
with its evaluation function.

393
00:21:32,260 --> 00:21:34,260
And so instead of having to do, you know,

394
00:21:34,260 --> 00:21:37,260
10 to the hundreds of possibilities,

395
00:21:37,260 --> 00:21:40,260
one can just zoom into, you know, mere thousands,

396
00:21:40,260 --> 00:21:43,260
10,000 or so searches.

397
00:21:43,260 --> 00:21:44,260
And so therefore,

398
00:21:44,260 --> 00:21:47,260
instead of that searching the entire grey tree of all possibilities,

399
00:21:47,260 --> 00:21:50,260
one just looks at this far more limited, you know,

400
00:21:50,260 --> 00:21:52,260
search tree in blue here.

401
00:21:52,260 --> 00:21:54,260
And then when you run out of thinking time,

402
00:21:54,260 --> 00:21:56,260
of course, you select the best path

403
00:21:56,260 --> 00:22:00,260
that you found so far in pink here.

404
00:22:00,260 --> 00:22:03,260
So, you know, we did this back in 2015,

405
00:22:03,260 --> 00:22:06,260
and then in the subsequent years, we still work on this now.

406
00:22:06,260 --> 00:22:08,260
There's a system called Mu Zero,

407
00:22:08,260 --> 00:22:10,260
which is our latest version of this that can do,

408
00:22:10,260 --> 00:22:13,260
not only do two player perfect information board games,

409
00:22:13,260 --> 00:22:17,260
but can also build models of its environment.

410
00:22:17,260 --> 00:22:20,260
So it can actually also do things like Atari games

411
00:22:20,260 --> 00:22:22,260
and video games where you actually don't have

412
00:22:22,260 --> 00:22:25,260
the rules of the game given to you.

413
00:22:25,260 --> 00:22:27,260
It has to actually figure that out for itself

414
00:22:27,260 --> 00:22:28,260
through observation as well.

415
00:22:28,260 --> 00:22:31,260
So it's one step even more general than Alpha Zero.

416
00:22:31,260 --> 00:22:34,260
And what we did with Alpha Go, of course, now is,

417
00:22:34,260 --> 00:22:38,260
as Sir Nigel mentioned, is we took it to Seoul in 2016

418
00:22:38,260 --> 00:22:40,260
in this million dollar challenge match with Lisa Doll.

419
00:22:40,260 --> 00:22:43,260
And some of you may remember this,

420
00:22:43,260 --> 00:22:44,260
but we won for one.

421
00:22:44,260 --> 00:22:47,260
You know, it was a huge thing, especially in Asia and in Korea.

422
00:22:47,260 --> 00:22:49,260
I mean, the country almost came to stand still.

423
00:22:49,260 --> 00:22:52,260
There's over 200 million people watch the games.

424
00:22:52,260 --> 00:22:54,260
And we won for one,

425
00:22:54,260 --> 00:22:59,260
and experts in both AI and in Go proclaimed this advance

426
00:22:59,260 --> 00:23:03,260
to be a decade before they would have predicted.

427
00:23:03,260 --> 00:23:06,260
But the important thing in the end was actually not just the fact

428
00:23:06,260 --> 00:23:08,260
that Alpha Go won the match,

429
00:23:08,260 --> 00:23:12,260
but how it won was, I think, really instructive.

430
00:23:12,260 --> 00:23:15,260
So I'm just going to give one example of this,

431
00:23:15,260 --> 00:23:17,260
but actually Alpha Go, I think, is in the end changed

432
00:23:17,260 --> 00:23:21,260
the way that we as human beings view the game of Go.

433
00:23:21,260 --> 00:23:24,260
But this is the most famous game of that set of five.

434
00:23:24,260 --> 00:23:26,260
There are actually some amazing different games,

435
00:23:26,260 --> 00:23:30,260
including the one that Lisa Doll won with a genius move in game four.

436
00:23:30,260 --> 00:23:35,260
But move 37 in game two, I think, will go down in Go history.

437
00:23:35,260 --> 00:23:37,260
And this was the ball position at that time.

438
00:23:37,260 --> 00:23:40,260
And I haven't got time to go into why this was so amazing.

439
00:23:40,260 --> 00:23:43,260
But suffice to say, Alpha Go here was black,

440
00:23:43,260 --> 00:23:45,260
and Lisa Doll is the white stones.

441
00:23:45,260 --> 00:23:48,260
And this is very early on in the game, move 37.

442
00:23:48,260 --> 00:23:52,260
You know, Go games last for a few hundred moves generally.

443
00:23:52,260 --> 00:23:55,260
And Alpha Go played this move 37 stone

444
00:23:55,260 --> 00:23:58,260
on the right hand side here, marked in red.

445
00:23:58,260 --> 00:24:01,260
And the amazing thing about this was the position of the stone

446
00:24:01,260 --> 00:24:04,260
was on the fifth line from the edge of the board.

447
00:24:04,260 --> 00:24:08,260
And that, if you're an expert Go player, is unthinkable.

448
00:24:08,260 --> 00:24:11,260
It's like you would be told off by your Go master

449
00:24:11,260 --> 00:24:14,260
that you should never do make a move like that.

450
00:24:14,260 --> 00:24:18,260
Because it gives white too much space on the side of the board.

451
00:24:18,260 --> 00:24:21,260
But Alpha Go decided to do it.

452
00:24:21,260 --> 00:24:24,260
Never seen before in master play would be recommended against.

453
00:24:24,260 --> 00:24:28,260
And then 100 moves or so later, it turned out this stone,

454
00:24:28,260 --> 00:24:31,260
this move 37 stone, was in the perfect position

455
00:24:31,260 --> 00:24:34,260
to decide the battle that spread out from the bottom left

456
00:24:34,260 --> 00:24:36,260
all the way across the board.

457
00:24:36,260 --> 00:24:38,260
And it was just in the right place to decide that battle,

458
00:24:38,260 --> 00:24:40,260
which decided the whole game.

459
00:24:40,260 --> 00:24:47,260
And almost as if it had presciently sort of seen that influence ahead of time.

460
00:24:47,260 --> 00:24:50,260
So now people play on the fifth line all the time, I'm told.

461
00:24:50,260 --> 00:24:53,260
So this has changed everything.

462
00:24:53,260 --> 00:24:58,260
And there's multiple books now written about Alpha Go's strategies.

463
00:24:58,260 --> 00:25:04,260
And this is an original strategy because this is not something

464
00:25:04,260 --> 00:25:07,260
that Alpha Go could have learned from human play.

465
00:25:07,260 --> 00:25:09,260
In fact, it would have learned the opposite.

466
00:25:09,260 --> 00:25:13,260
It would have learned not to do this kind of move.

467
00:25:13,260 --> 00:25:15,260
So if you're interested in more on that Alpha Go,

468
00:25:15,260 --> 00:25:18,260
I recommend you this amazing award-winning documentary

469
00:25:18,260 --> 00:25:21,260
that was done by an independent filmmaker on YouTube now.

470
00:25:21,260 --> 00:25:23,260
If you want to see the sort of ins and outs of it,

471
00:25:23,260 --> 00:25:27,260
it was very emotional as an experience for us from all sides,

472
00:25:27,260 --> 00:25:30,260
especially me being an ex-games player.

473
00:25:30,260 --> 00:25:35,260
I could really understand it from Lisa Doll's point of view too.

474
00:25:35,260 --> 00:25:38,260
So as I said, we then took this to Alpha Zero a couple of years ago,

475
00:25:38,260 --> 00:25:42,260
two, three years ago now, and generalised this to all two-player games.

476
00:25:42,260 --> 00:25:49,260
And these graphs show how Alpha Zero did against the best machines at the time

477
00:25:49,260 --> 00:25:52,260
in the specialised games of chess.

478
00:25:52,260 --> 00:25:54,260
It beat the best version of Stockfish,

479
00:25:54,260 --> 00:26:00,260
which is this incredible handcrafted system, the descendant of Deep Blue.

480
00:26:00,260 --> 00:26:02,260
And it was able to beat Stockfish 8,

481
00:26:02,260 --> 00:26:06,260
which was the best Stockfish at the time, in four hours of training.

482
00:26:06,260 --> 00:26:12,260
It could beat Alpha Go, Alpha Zero beat Alpha Go in eight hours at Go.

483
00:26:12,260 --> 00:26:16,260
And then we just tried it with one other game, Japanese chess shogi,

484
00:26:16,260 --> 00:26:19,260
actually, which is a really interesting variation on chess.

485
00:26:19,260 --> 00:26:25,260
And it could beat the best handcrafted programme called ELMO within two hours of training.

486
00:26:25,260 --> 00:26:28,260
The same system, all three games.

487
00:26:28,260 --> 00:26:30,260
So that was generalised.

488
00:26:30,260 --> 00:26:32,260
And then, of course, because I'm a chess player,

489
00:26:32,260 --> 00:26:35,260
I play a little bit of Go, but I'm not very strong, but so chess is my game.

490
00:26:35,260 --> 00:26:39,260
And so for me, this was the most exciting part of applying Alpha Zero,

491
00:26:39,260 --> 00:26:44,260
because I actually had a discussion with Murray Campbell,

492
00:26:44,260 --> 00:26:47,260
who some of you will know was one of the project leaders

493
00:26:47,260 --> 00:26:51,260
behind Deep Blue, our IBM back in the 90s.

494
00:26:51,260 --> 00:26:56,260
And we just, I think we just were about to play the Lisa Doll match,

495
00:26:56,260 --> 00:26:57,260
or maybe we just finished.

496
00:26:57,260 --> 00:26:59,260
And I was giving a lecturer at a conference,

497
00:26:59,260 --> 00:27:01,260
and Murray Campbell was there as well in the audience.

498
00:27:01,260 --> 00:27:03,260
And he came up to me afterwards, and we were discussing,

499
00:27:03,260 --> 00:27:05,260
I said to him, I'm thinking about,

500
00:27:05,260 --> 00:27:08,260
maybe we should try this with chess and see what happens.

501
00:27:08,260 --> 00:27:11,260
And I wanted to know what his prediction would be.

502
00:27:11,260 --> 00:27:16,260
Do you think these incredibly powerful handcrafted systems,

503
00:27:16,260 --> 00:27:18,260
like stockfish, could be beaten?

504
00:27:18,260 --> 00:27:21,260
Was there any more headroom in chess?

505
00:27:21,260 --> 00:27:24,260
Chess is probably the oldest application of AI, right?

506
00:27:24,260 --> 00:27:27,260
I mean, Turing and Shannon and people like that

507
00:27:27,260 --> 00:27:28,260
have all tried their hand.

508
00:27:28,260 --> 00:27:31,260
Every AI researcher at some point has tried their hand

509
00:27:31,260 --> 00:27:34,260
on a chess program back to the 40s and 50s,

510
00:27:34,260 --> 00:27:37,260
even if Turing had to run the program by hand

511
00:27:37,260 --> 00:27:40,260
on a piece of paper and a pen.

512
00:27:40,260 --> 00:27:44,260
And then, of course, in the last 25 years or so,

513
00:27:44,260 --> 00:27:47,260
world champions have been studying with their chess programs

514
00:27:47,260 --> 00:27:50,260
and mapping out all of chess, opening theory, all of these things.

515
00:27:50,260 --> 00:27:53,260
So it was a legitimate question actually to ask is,

516
00:27:53,260 --> 00:27:55,260
was there any more headroom left?

517
00:27:55,260 --> 00:27:57,260
And what sort of chess would AlphaZero play

518
00:27:57,260 --> 00:28:01,260
if we were to train it from first principles

519
00:28:01,260 --> 00:28:08,260
and play it against these amazing hand-engineered monsters

520
00:28:08,260 --> 00:28:11,260
in some sense of a machine, incredible calculating machines?

521
00:28:11,260 --> 00:28:14,260
And so, of course, we couldn't actually come to an agreement on that.

522
00:28:14,260 --> 00:28:16,260
And that, as the scientists in the orders will know,

523
00:28:16,260 --> 00:28:18,260
that's the sign of a good question, I think,

524
00:28:18,260 --> 00:28:20,260
where either answer would be interesting.

525
00:28:20,260 --> 00:28:22,260
If we were to win and there was some new style out there,

526
00:28:22,260 --> 00:28:23,260
there would be incredibly interesting.

527
00:28:23,260 --> 00:28:26,260
And also be interesting if these hand-crofter systems,

528
00:28:26,260 --> 00:28:30,260
at least in one domain, chess, had reached the limit.

529
00:28:30,260 --> 00:28:33,260
So we got off and started doing that.

530
00:28:33,260 --> 00:28:37,260
And I'm pleased to say that AlphaZero not only played stronger,

531
00:28:37,260 --> 00:28:41,260
but it did come up with a completely new style of chess,

532
00:28:41,260 --> 00:28:44,260
which I think, and my chess friends tell me,

533
00:28:44,260 --> 00:28:49,260
is more aesthetically pleasing as well as a chess program.

534
00:28:49,260 --> 00:28:52,260
Obviously, subjectively from a human expert's point of view.

535
00:28:52,260 --> 00:28:54,260
And the reason it is is because what it does,

536
00:28:54,260 --> 00:28:56,260
and it does many innovations,

537
00:28:56,260 --> 00:29:01,260
but the main one is that it favours mobility over materiality.

538
00:29:01,260 --> 00:29:04,260
So traditionally, hand-crofter chess programs

539
00:29:04,260 --> 00:29:06,260
have always favoured materiality.

540
00:29:06,260 --> 00:29:10,260
The joke within the chess circles is that chess computer sees a pawn

541
00:29:10,260 --> 00:29:13,260
and then grabs the pawn because it loves material

542
00:29:13,260 --> 00:29:15,260
because it gets plus one in its evaluation function.

543
00:29:15,260 --> 00:29:17,260
And then it tries to hang on for dear life

544
00:29:17,260 --> 00:29:19,260
in a really ugly position,

545
00:29:19,260 --> 00:29:21,260
but it wins because it never makes any tactical mistakes.

546
00:29:21,260 --> 00:29:23,260
So it's sort of very effective,

547
00:29:23,260 --> 00:29:27,260
but it's a little bit sort of aesthetically unsatisfying,

548
00:29:27,260 --> 00:29:29,260
one would say, as a style.

549
00:29:29,260 --> 00:29:32,260
But instead of that, actually AlphaZero does the opposite.

550
00:29:32,260 --> 00:29:36,260
It loves sacrificing pieces, material, to get mobility,

551
00:29:36,260 --> 00:29:40,260
to get more mobility for its remaining pieces.

552
00:29:40,260 --> 00:29:44,260
So this is a game from, we did a 100 match

553
00:29:44,260 --> 00:29:46,260
between AlphaZero and Stockfish,

554
00:29:46,260 --> 00:29:50,260
and then we gave it to the British chess champion to analyse,

555
00:29:50,260 --> 00:29:53,260
and he picked out the coolest positions.

556
00:29:53,260 --> 00:29:54,260
This is my favourite.

557
00:29:54,260 --> 00:29:56,260
It's sometimes called the immortal Zugswang game.

558
00:29:56,260 --> 00:29:59,260
Zugswang is a phrase in chess,

559
00:29:59,260 --> 00:30:02,260
a German phrase that means any move that one makes in that position

560
00:30:02,260 --> 00:30:04,260
makes your position worse.

561
00:30:04,260 --> 00:30:07,260
So it's a special type of position where you're in Zugswang,

562
00:30:07,260 --> 00:30:10,260
which means anything you do, it's going to make it worse,

563
00:30:10,260 --> 00:30:11,260
which is very unusual.

564
00:30:11,260 --> 00:30:13,260
And it's super unusual in this kind of position,

565
00:30:13,260 --> 00:30:15,260
for those of you who know chess, where black,

566
00:30:15,260 --> 00:30:17,260
which has got more pieces, the two rooks and a queen,

567
00:30:17,260 --> 00:30:20,260
so it's got big material advantage, very powerful pieces,

568
00:30:20,260 --> 00:30:22,260
the most powerful pieces remaining in chess,

569
00:30:22,260 --> 00:30:24,260
but they will stuck in the corner,

570
00:30:24,260 --> 00:30:28,260
and AlphaZero has sort of sealed them up with cement with its pieces,

571
00:30:28,260 --> 00:30:30,260
and basically none of those pieces can move.

572
00:30:30,260 --> 00:30:32,260
So this is kind of an incredible position.

573
00:30:32,260 --> 00:30:36,260
So almost anything black does in this position,

574
00:30:36,260 --> 00:30:38,260
its black to move, will make its position worse,

575
00:30:38,260 --> 00:30:42,260
even though it's got all of these very powerful pieces.

576
00:30:42,260 --> 00:30:45,260
So that was one innovation.

577
00:30:45,260 --> 00:30:47,260
There were lots of interesting poppies about AlphaZero

578
00:30:47,260 --> 00:30:50,260
that I won't go into, but one can think about,

579
00:30:50,260 --> 00:30:53,260
well, why is it that AlphaZero plays like this,

580
00:30:53,260 --> 00:30:57,260
and traditional chess engines didn't?

581
00:30:57,260 --> 00:30:59,260
Nowadays, actually, interestingly, they've updated Stockfish

582
00:30:59,260 --> 00:31:03,260
to include some of these ideas by hand in Stockfish,

583
00:31:03,260 --> 00:31:06,260
and actually now it's even more powerful.

584
00:31:06,260 --> 00:31:08,260
So it's kind of interesting hybrid system.

585
00:31:08,260 --> 00:31:12,260
But my feeling is that it's better at evaluating positions

586
00:31:12,260 --> 00:31:15,260
than chess engines, so that's one thing,

587
00:31:15,260 --> 00:31:17,260
so it's got a better evaluation function.

588
00:31:17,260 --> 00:31:19,260
And the main thing is it doesn't have to overcome

589
00:31:19,260 --> 00:31:21,260
these inbuilt rules.

590
00:31:21,260 --> 00:31:23,260
That's why it's sacrificing pieces,

591
00:31:23,260 --> 00:31:25,260
because if you think about it, a hard-coded chess engine

592
00:31:25,260 --> 00:31:28,260
would have to calculate in its search tree

593
00:31:28,260 --> 00:31:30,260
that if it was going to sacrifice a rook for a bishop,

594
00:31:30,260 --> 00:31:32,260
that's minus two points,

595
00:31:32,260 --> 00:31:35,260
is it going to get back those two points of value

596
00:31:35,260 --> 00:31:37,260
within its search tree horizon?

597
00:31:37,260 --> 00:31:39,260
AlphaZero doesn't have to worry about that,

598
00:31:39,260 --> 00:31:41,260
because there's no rules like that in there.

599
00:31:41,260 --> 00:31:43,260
It can evaluate things contextually,

600
00:31:43,260 --> 00:31:46,260
based on the particular situation at hand,

601
00:31:46,260 --> 00:31:48,260
and the patterns involved there.

602
00:31:48,260 --> 00:31:50,260
And also, the other big thing is,

603
00:31:50,260 --> 00:31:52,260
Stockfish and programmes like that,

604
00:31:52,260 --> 00:31:54,260
they have thousands of handcrafted rules,

605
00:31:54,260 --> 00:31:57,260
so one problem is generating those rules,

606
00:31:57,260 --> 00:31:59,260
but an even bigger problem, in my opinion,

607
00:31:59,260 --> 00:32:01,260
is balancing those factors together.

608
00:32:01,260 --> 00:32:04,260
That's a huge handcrafted juggling act.

609
00:32:04,260 --> 00:32:07,260
And instead of that, obviously AlphaZero learns itself

610
00:32:07,260 --> 00:32:10,260
how to balance out the factors that it's learned,

611
00:32:10,260 --> 00:32:14,260
and to do that automatically.

612
00:32:14,260 --> 00:32:17,260
So one can actually see how efficient this system is,

613
00:32:17,260 --> 00:32:21,260
based on the amount of search that traditional search engines

614
00:32:21,260 --> 00:32:24,260
have to do per each move they make.

615
00:32:24,260 --> 00:32:27,260
And a human grandmaster makes only the order of,

616
00:32:27,260 --> 00:32:29,260
looks at about 100 moves per decision,

617
00:32:29,260 --> 00:32:31,260
so incredibly efficient with our models.

618
00:32:31,260 --> 00:32:34,260
And the state-of-the-art chess engine, like Stockfish,

619
00:32:34,260 --> 00:32:39,260
would make tens of millions of evaluations per move.

620
00:32:39,260 --> 00:32:42,260
And AlphaZero is sort of in the middle here,

621
00:32:42,260 --> 00:32:46,260
in terms of orders of magnitude, tens of thousands of moves.

622
00:32:46,260 --> 00:32:50,260
So not as efficient as human players,

623
00:32:50,260 --> 00:32:53,260
but far more efficient than the search one would get

624
00:32:53,260 --> 00:32:56,260
in these search engines.

625
00:32:56,260 --> 00:32:59,260
So again, if you're interested in the details about,

626
00:32:59,260 --> 00:33:02,260
or your chess playing, and the details about what this changed,

627
00:33:02,260 --> 00:33:06,260
the British champion and Natasha Reagan

628
00:33:06,260 --> 00:33:08,260
wrote an amazing book called Game Changer,

629
00:33:08,260 --> 00:33:12,260
when we gave them behind the scenes access to AlphaZero,

630
00:33:12,260 --> 00:33:14,260
and what new motifs they found,

631
00:33:14,260 --> 00:33:17,260
at least a dozen new motifs they found in chess.

632
00:33:17,260 --> 00:33:20,260
And the cool thing is that it's very gratifying for me,

633
00:33:20,260 --> 00:33:22,260
is that people like Magnus Carlson,

634
00:33:22,260 --> 00:33:25,260
who's the current world champion, incredible player,

635
00:33:25,260 --> 00:33:27,260
he said a few years back he was one of the first people

636
00:33:27,260 --> 00:33:29,260
to read the book and who we sent it to,

637
00:33:29,260 --> 00:33:31,260
and I've been influenced by my heroes recently,

638
00:33:31,260 --> 00:33:34,260
one of which is AlphaZero, which is really cool to say.

639
00:33:34,260 --> 00:33:36,260
And he actually incorporated, because he's so talented,

640
00:33:36,260 --> 00:33:38,260
he was able to quite quickly,

641
00:33:38,260 --> 00:33:40,260
quicker than all the other chess players,

642
00:33:40,260 --> 00:33:42,260
incorporate some of these ideas into his play.

643
00:33:42,260 --> 00:33:45,260
And then Garry Casparov, he used to be a hero of mine

644
00:33:45,260 --> 00:33:48,260
when he was world champion when I was growing up and playing chess.

645
00:33:48,260 --> 00:33:50,260
He worked the forward for the book,

646
00:33:50,260 --> 00:33:52,260
and he said programs usually reflect priorities and prejudices

647
00:33:52,260 --> 00:33:55,260
of programmers, but AlphaZero, it learns for itself,

648
00:33:55,260 --> 00:33:57,260
and I would say it's star reflects the truth,

649
00:33:57,260 --> 00:34:00,260
which is, you know, I think, a beautiful quote.

650
00:34:01,260 --> 00:34:05,260
So we've been lucky enough to have several of these sort of fundamental

651
00:34:05,260 --> 00:34:07,260
breakthroughs in games.

652
00:34:07,260 --> 00:34:10,260
We started with Atari, and our program called DQN,

653
00:34:10,260 --> 00:34:13,260
being able to play Atari games directly from pixels

654
00:34:13,260 --> 00:34:15,260
and maximise the score just from pixels,

655
00:34:15,260 --> 00:34:18,260
not being told the rules of the game, AlphaGo and AlphaZero,

656
00:34:18,260 --> 00:34:19,260
I just mentioned.

657
00:34:19,260 --> 00:34:22,260
And then we went further with programs like AlphaStar,

658
00:34:22,260 --> 00:34:26,260
which played the most complex video game called StarCraft 2,

659
00:34:26,260 --> 00:34:29,260
which is a very complicated real-time strategy game

660
00:34:29,260 --> 00:34:31,260
with huge other challenges.

661
00:34:31,260 --> 00:34:34,260
It's only partially observable, it's not perfect information,

662
00:34:34,260 --> 00:34:36,260
there's an economy system to it,

663
00:34:36,260 --> 00:34:39,260
and you have generally thousands of possible actions

664
00:34:39,260 --> 00:34:42,260
you can take for any choice, not a few dozen.

665
00:34:43,260 --> 00:34:47,260
And we managed to also get to grandmaster level at that.

666
00:34:48,260 --> 00:34:50,260
So that was all of our games work,

667
00:34:50,260 --> 00:34:53,260
but really it was leading up to this moment,

668
00:34:53,260 --> 00:34:57,260
which in the last couple of years has been just so exciting

669
00:34:57,260 --> 00:35:01,260
and so gratifying for us to make progress with,

670
00:35:01,260 --> 00:35:05,260
which is that the games, and I love games, always will love games,

671
00:35:05,260 --> 00:35:09,260
playing them, designing them and using them as testing grounds,

672
00:35:09,260 --> 00:35:12,260
they were the perfect testing ground for developing AI,

673
00:35:12,260 --> 00:35:16,260
but ultimately the aim was not to play games to world championship level,

674
00:35:16,260 --> 00:35:19,260
it was to build general systems that could generalise

675
00:35:19,260 --> 00:35:21,260
and solve real world problems.

676
00:35:21,260 --> 00:35:23,260
And the one that's particularly passionate for me

677
00:35:23,260 --> 00:35:26,260
is using AI for scientific discovery.

678
00:35:27,260 --> 00:35:29,260
And there are three things that I look for when currently,

679
00:35:29,260 --> 00:35:32,260
when we want to select a scientific problem

680
00:35:32,260 --> 00:35:36,260
that we believe our systems could be good at.

681
00:35:36,260 --> 00:35:40,260
So number one is we actually search out massive combinatorial

682
00:35:40,260 --> 00:35:42,260
search spaces or state spaces.

683
00:35:42,260 --> 00:35:44,260
So the bigger, the better actually.

684
00:35:44,260 --> 00:35:45,260
Why is that?

685
00:35:45,260 --> 00:35:48,260
Well, because we know then traditional methods

686
00:35:48,260 --> 00:35:50,260
and exhaustive brute force methods won't work.

687
00:35:50,260 --> 00:35:53,260
So we're in a razy where something else is needed

688
00:35:53,260 --> 00:35:56,260
and we think that we're good at that something else.

689
00:35:56,260 --> 00:35:59,260
Number two is that we want to have,

690
00:35:59,260 --> 00:36:02,260
we like problems that have a clear objective function

691
00:36:02,260 --> 00:36:04,260
or metric that one can specify

692
00:36:04,260 --> 00:36:06,260
so that you can optimise and hill climb against it

693
00:36:06,260 --> 00:36:08,260
with your learning system.

694
00:36:08,260 --> 00:36:10,260
And then number three is we look for problems

695
00:36:10,260 --> 00:36:13,260
that either have a lot of data available

696
00:36:13,260 --> 00:36:15,260
to learn and train from,

697
00:36:15,260 --> 00:36:17,260
or, and ideally it's and or,

698
00:36:17,260 --> 00:36:19,260
an accurate and efficient simulator

699
00:36:19,260 --> 00:36:21,260
that one can use to generate more data.

700
00:36:21,260 --> 00:36:23,260
And that simulator doesn't have to be perfect.

701
00:36:23,260 --> 00:36:25,260
It just has to be good enough

702
00:36:25,260 --> 00:36:28,260
that you can extract some signal from the data that it generates.

703
00:36:29,260 --> 00:36:33,260
Now it turns out that when you look at a lot of problems

704
00:36:33,260 --> 00:36:37,260
with this prism, then actually a lot of surprising

705
00:36:37,260 --> 00:36:41,260
number of problems can be made to fit these criteria.

706
00:36:41,260 --> 00:36:45,260
And of course, the number one thing we were looking at

707
00:36:45,260 --> 00:36:48,260
was protein folding, which I want to talk a bit about now.

708
00:36:48,260 --> 00:36:50,260
And we look for problems,

709
00:36:50,260 --> 00:36:52,260
not only that just fit those three criteria,

710
00:36:52,260 --> 00:36:54,260
but of course there's always an opportunity cost

711
00:36:54,260 --> 00:36:57,260
when you embark on applying AI to something major.

712
00:36:57,260 --> 00:36:59,260
It's going to take you many years,

713
00:36:59,260 --> 00:37:01,260
depending on how hard that problem is.

714
00:37:01,260 --> 00:37:05,260
And we look for something that will have really huge impact.

715
00:37:05,260 --> 00:37:07,260
Perhaps we sometimes talk about root nodes

716
00:37:07,260 --> 00:37:10,260
that can open up whole new branches

717
00:37:10,260 --> 00:37:14,260
of scientific discovery if they were to be solved.

718
00:37:14,260 --> 00:37:17,260
And protein folding ticked all of those boxes.

719
00:37:17,260 --> 00:37:20,260
So if you don't know what protein folding is,

720
00:37:20,260 --> 00:37:23,260
it's this classic problem of can one go

721
00:37:23,260 --> 00:37:26,260
from a one-dimensional amino acid sequence,

722
00:37:26,260 --> 00:37:28,260
you can think of it as the genetic sequence

723
00:37:28,260 --> 00:37:32,260
for a protein that describes a protein coded by the genome.

724
00:37:32,260 --> 00:37:34,260
And can you predict from that directly

725
00:37:34,260 --> 00:37:37,260
the 3D structure of the protein in your body,

726
00:37:37,260 --> 00:37:39,260
the 3D form that it takes.

727
00:37:39,260 --> 00:37:42,260
And the reason this is important is that proteins

728
00:37:42,260 --> 00:37:45,260
are basically essential for everything in life,

729
00:37:45,260 --> 00:37:47,260
every function in your body.

730
00:37:47,260 --> 00:37:51,260
And it's thought that the 3D structure of the protein,

731
00:37:51,260 --> 00:37:54,260
at least in the large part, governs its function.

732
00:37:54,260 --> 00:37:56,260
So if one can understand the structure,

733
00:37:56,260 --> 00:38:01,260
then one can get closer to the function of the protein.

734
00:38:01,260 --> 00:38:04,260
Now, until AlphaFol came along,

735
00:38:04,260 --> 00:38:06,260
the way you would do this is experimentally,

736
00:38:06,260 --> 00:38:09,260
and it's extremely painstaking expert work

737
00:38:09,260 --> 00:38:10,260
that needs to be done.

738
00:38:10,260 --> 00:38:14,260
And using x-ray crystallography and electron microscopy.

739
00:38:14,260 --> 00:38:18,260
And the rule of thumb is generally that it takes one PhD student,

740
00:38:18,260 --> 00:38:21,260
their whole PhD, to do one protein.

741
00:38:21,260 --> 00:38:23,260
And that's if you get lucky, you can be unlucky.

742
00:38:23,260 --> 00:38:28,260
So it's hard and really painstaking and difficult.

743
00:38:28,260 --> 00:38:31,260
And what happened is that the Nobel Prize winner

744
00:38:31,260 --> 00:38:35,260
Christian Anfinsen, in part of his Nobel lecture in 1972,

745
00:38:35,260 --> 00:38:38,260
so 50 years ago, exactly now,

746
00:38:38,260 --> 00:38:41,260
he conjectured that the 3D structure of proteins

747
00:38:41,260 --> 00:38:44,260
should be fully determined by the amino acid sequence,

748
00:38:44,260 --> 00:38:47,260
i.e. this should be possible this mapping.

749
00:38:47,260 --> 00:38:51,260
And it's a bit like, sometimes this problem is called

750
00:38:51,260 --> 00:38:54,260
like Fermat's Last Theorem equivalent in biology,

751
00:38:54,260 --> 00:38:56,260
because it's a bit like saying this is possible,

752
00:38:56,260 --> 00:38:59,260
but the margin is too small, can't give you the answer.

753
00:38:59,260 --> 00:39:03,260
And so what happened instead is obviously it set off a 50 year quest

754
00:39:03,260 --> 00:39:05,260
in biology, in computational biology,

755
00:39:05,260 --> 00:39:08,260
to try and solve this problem.

756
00:39:08,260 --> 00:39:14,260
And it's been ongoing ever since the 1970s.

757
00:39:14,260 --> 00:39:16,260
So the big question is,

758
00:39:16,260 --> 00:39:19,260
is can protein structure prediction,

759
00:39:19,260 --> 00:39:21,260
the protein structure prediction problem,

760
00:39:21,260 --> 00:39:23,260
which is the specific part of protein folding

761
00:39:23,260 --> 00:39:26,260
that we're interested in, be solved computationally?

762
00:39:26,260 --> 00:39:28,260
Just computationally.

763
00:39:28,260 --> 00:39:32,260
And Leventhal, who is another famous contemporary of Anfinsen,

764
00:39:32,260 --> 00:39:34,260
in the 60s and 70s as well,

765
00:39:34,260 --> 00:39:36,260
he calculated, back of envelope,

766
00:39:36,260 --> 00:39:40,260
that there would be roughly 10 to the 300 possible confirmations,

767
00:39:40,260 --> 00:39:43,260
shapes of an average size protein that it could take.

768
00:39:43,260 --> 00:39:46,260
So 10 to the 300, so that's a good number,

769
00:39:46,260 --> 00:39:48,260
that's ones we like, because it's bigger than go.

770
00:39:48,260 --> 00:39:51,260
And obviously that means exhaustively sampling this

771
00:39:51,260 --> 00:39:53,260
is totally intractable,

772
00:39:53,260 --> 00:39:58,260
but of course the chink of light is that in nature,

773
00:39:58,260 --> 00:40:01,260
in our bodies, physics solves this.

774
00:40:01,260 --> 00:40:06,260
So it can, if proteins spontaneously fold in a matter of seconds,

775
00:40:06,260 --> 00:40:08,260
sometimes milliseconds in the body.

776
00:40:08,260 --> 00:40:12,260
So there's obviously some energy path through this.

777
00:40:12,260 --> 00:40:14,260
So how do we get to this problem?

778
00:40:14,260 --> 00:40:18,260
Well actually it's quite a long winding road for me personally,

779
00:40:18,260 --> 00:40:20,260
for others in the team less so.

780
00:40:20,260 --> 00:40:23,260
But for me, I actually came across the protein folding problem

781
00:40:23,260 --> 00:40:26,260
in the 90s as an undergrad in Cambridge,

782
00:40:26,260 --> 00:40:30,260
because one of my friends in our sort of group

783
00:40:30,260 --> 00:40:33,260
of colleagues was obsessed with this problem.

784
00:40:33,260 --> 00:40:36,260
And he would talk about it, and I remember this very clearly,

785
00:40:36,260 --> 00:40:39,260
every opportunity in the bar playing pool, whatever it was.

786
00:40:39,260 --> 00:40:44,260
If we can crack this, that will open up all sorts of things in biology.

787
00:40:44,260 --> 00:40:46,260
And I sort of listened to him and I was thinking about this,

788
00:40:46,260 --> 00:40:48,260
I was fascinated by the problem as a problem,

789
00:40:48,260 --> 00:40:52,260
and I felt it was actually very well suited to potentially to AI.

790
00:40:52,260 --> 00:40:56,260
Although obviously at the time I didn't know how it could be tackled.

791
00:40:56,260 --> 00:40:58,260
But I filed that away as an interesting thing.

792
00:40:58,260 --> 00:41:01,260
And then it came up again in the late 2000s

793
00:41:01,260 --> 00:41:03,260
when I was doing my postdoc over at MIT.

794
00:41:03,260 --> 00:41:08,260
And this game called Fold It came out from David Baker's lab,

795
00:41:08,260 --> 00:41:10,260
who works on proteins.

796
00:41:10,260 --> 00:41:13,260
And it was a citizen science game, you can see it on the left here.

797
00:41:13,260 --> 00:41:15,260
And what they've done really interestingly

798
00:41:15,260 --> 00:41:18,260
is turn protein folding into a puzzle game.

799
00:41:18,260 --> 00:41:23,260
And they actually got a couple hundred gamers to fold proteins,

800
00:41:23,260 --> 00:41:25,260
bit like playing Tetris or something.

801
00:41:25,260 --> 00:41:29,260
And some of them actually became really good.

802
00:41:29,260 --> 00:41:32,260
And I remember, so of course I was fascinated this

803
00:41:32,260 --> 00:41:34,260
just from games design perspective.

804
00:41:34,260 --> 00:41:36,260
Wouldn't it be amazing if we could design more games

805
00:41:36,260 --> 00:41:39,260
where people played them, they were actually doing useful science

806
00:41:39,260 --> 00:41:41,260
while they were having fun, that would be amazing.

807
00:41:41,260 --> 00:41:43,260
And I think this is still the best example of that.

808
00:41:43,260 --> 00:41:47,260
But also again protein folding was coming up.

809
00:41:47,260 --> 00:41:50,260
And in fact, it turned out that a couple of,

810
00:41:50,260 --> 00:41:53,260
a few really important proteins structures

811
00:41:53,260 --> 00:41:55,260
were found this way by gamers

812
00:41:55,260 --> 00:41:58,260
and published in Nature and Nature Structural Biology.

813
00:41:58,260 --> 00:42:01,260
And so this actually really worked.

814
00:42:01,260 --> 00:42:03,260
And that, when we then got to, you know,

815
00:42:03,260 --> 00:42:05,260
the third piece of the puzzle was doing Go

816
00:42:05,260 --> 00:42:08,260
and trying to sort of think about what we'd done

817
00:42:08,260 --> 00:42:10,260
with intuition and other things, as I mentioned earlier.

818
00:42:10,260 --> 00:42:12,260
And I felt that actually, you know,

819
00:42:12,260 --> 00:42:14,260
if we'd managed to mimic in some sense

820
00:42:14,260 --> 00:42:17,260
the intuition of Go players, master Go players

821
00:42:17,260 --> 00:42:19,260
who spent their entire life studying Go,

822
00:42:19,260 --> 00:42:22,260
you know, maybe one could mimic the intuition of these gamers

823
00:42:22,260 --> 00:42:25,260
who were only, by the way, of course, amateur biologists.

824
00:42:25,260 --> 00:42:28,260
Right? But somehow some of them were able to make

825
00:42:28,260 --> 00:42:30,260
counter-intuitive folds of the backbone

826
00:42:30,260 --> 00:42:33,260
that were, if you just followed an energy landscape

827
00:42:33,260 --> 00:42:35,260
in a greedy fashion, one would not, you know,

828
00:42:35,260 --> 00:42:38,260
reach a local minima or local maxima

829
00:42:38,260 --> 00:42:42,260
and you would not be able to find the right structure.

830
00:42:42,260 --> 00:42:46,260
So it's almost the day after we got back from Korea

831
00:42:46,260 --> 00:42:50,260
we then, you know, I instigated the Alpha Fold project

832
00:42:50,260 --> 00:42:53,260
and I thought it was the right time

833
00:42:53,260 --> 00:42:57,260
to basically start working on this problem.

834
00:42:57,260 --> 00:42:59,260
The other important piece of the puzzle

835
00:42:59,260 --> 00:43:01,260
was this competition called CASP,

836
00:43:01,260 --> 00:43:04,260
which is sometimes thought of as, like,

837
00:43:04,260 --> 00:43:06,260
the Olympics for protein folding,

838
00:43:06,260 --> 00:43:09,260
and it's sort of run every two years in external benchmarks.

839
00:43:09,260 --> 00:43:10,260
It's an amazing thing, actually,

840
00:43:10,260 --> 00:43:12,260
that I think more areas of science should do.

841
00:43:12,260 --> 00:43:15,260
And it's been run sort of religiously

842
00:43:15,260 --> 00:43:17,260
for every two years, for nearly 30 years.

843
00:43:17,260 --> 00:43:20,260
So, you know, huge culos to the organisers,

844
00:43:20,260 --> 00:43:23,260
John Mull and his team for doing this

845
00:43:23,260 --> 00:43:26,260
and organising it so professionally for every two years

846
00:43:26,260 --> 00:43:29,260
without fail for 30 years.

847
00:43:29,260 --> 00:43:32,260
And the cool thing about it is it's a blind prediction assessment.

848
00:43:32,260 --> 00:43:36,260
So there's no way you can accidentally sort of train on test data

849
00:43:36,260 --> 00:43:38,260
or any of these kinds of pitfalls

850
00:43:38,260 --> 00:43:40,260
because at the time when the competition runs

851
00:43:40,260 --> 00:43:42,260
over summer usually every two years,

852
00:43:42,260 --> 00:43:46,260
the experimentalists globally agree to hold back

853
00:43:46,260 --> 00:43:48,260
a few of their structures that they've just found,

854
00:43:48,260 --> 00:43:50,260
but at that point in time they're the only ones

855
00:43:50,260 --> 00:43:52,260
who know what that structure looks like.

856
00:43:52,260 --> 00:43:54,260
They hold back the publication for a couple of months

857
00:43:54,260 --> 00:43:56,260
and they give it to John Mull and his colleagues

858
00:43:56,260 --> 00:43:58,260
to put it into the competition.

859
00:43:58,260 --> 00:43:59,260
And then you get those.

860
00:43:59,260 --> 00:44:02,260
It's quite fun tournament because then, you know,

861
00:44:02,260 --> 00:44:03,260
it's quite exciting.

862
00:44:03,260 --> 00:44:05,260
You get the email and then there's a new structure

863
00:44:05,260 --> 00:44:08,260
that amino acid sequence nobody has ever, you know,

864
00:44:08,260 --> 00:44:09,260
knows the structure of.

865
00:44:09,260 --> 00:44:11,260
And then you have a week to sort of get it back

866
00:44:11,260 --> 00:44:14,260
to the competition organisers before it's published.

867
00:44:14,260 --> 00:44:16,260
And then at the end of that three, four month period,

868
00:44:16,260 --> 00:44:21,260
they obviously score your predictions against the ground truth,

869
00:44:21,260 --> 00:44:24,260
which at that point is published, obviously in peer review journals,

870
00:44:24,260 --> 00:44:26,260
that the experimental ground truth.

871
00:44:26,260 --> 00:44:30,260
And then you get a kind of distance measure between your predictions

872
00:44:30,260 --> 00:44:32,260
and the molecules in that prediction

873
00:44:32,260 --> 00:44:36,260
and where they really are in 3D coordinate space.

874
00:44:36,260 --> 00:44:40,260
So when we started getting involved in this area post 2016,

875
00:44:40,260 --> 00:44:43,260
you know, we looked at CASP and the history of it

876
00:44:43,260 --> 00:44:45,260
and actually they'd been very little progress

877
00:44:45,260 --> 00:44:46,260
for over a decade.

878
00:44:46,260 --> 00:44:48,260
It's sort of the field had stalled.

879
00:44:48,260 --> 00:44:54,260
And this graph here shows you the scores of the winning team

880
00:44:54,260 --> 00:44:56,260
on the hardest category of protein,

881
00:44:56,260 --> 00:44:59,260
where you don't have any evolutionary similar template proteins

882
00:44:59,260 --> 00:45:01,260
to sort of rely on.

883
00:45:01,260 --> 00:45:02,260
So it's called free modelling.

884
00:45:02,260 --> 00:45:04,260
And this is a percentage accuracy.

885
00:45:04,260 --> 00:45:05,260
It's called GDT.

886
00:45:05,260 --> 00:45:07,260
It's a slight nuance of the measure,

887
00:45:07,260 --> 00:45:10,260
but you can think of it as the number of molecules,

888
00:45:10,260 --> 00:45:12,260
the percentage number of molecules you've got roughly

889
00:45:12,260 --> 00:45:15,260
in that place to a certain tolerance, distance tolerance.

890
00:45:15,260 --> 00:45:18,260
And you can see they were hovering around 40% or less,

891
00:45:18,260 --> 00:45:21,260
which is useless for experimentation, right?

892
00:45:21,260 --> 00:45:23,260
Basically, it's pretty much random.

893
00:45:23,260 --> 00:45:26,260
And so that was the average and it hadn't really moved.

894
00:45:26,260 --> 00:45:33,260
And so what we did in 2018 is that we came along with Alpha Fold 1

895
00:45:33,260 --> 00:45:36,260
as our first entry after a couple of years of working on this.

896
00:45:36,260 --> 00:45:39,260
And we sort of, you know, I think we revolutionised the field in a way,

897
00:45:39,260 --> 00:45:42,260
is that for the first time we brought cutting edge machine learning techniques,

898
00:45:42,260 --> 00:45:45,260
the sort of techniques we developed in AlphaGo

899
00:45:45,260 --> 00:45:47,260
and other new ones for this domain.

900
00:45:47,260 --> 00:45:50,260
And we, as the core part of the system,

901
00:45:50,260 --> 00:45:53,260
and we improved the winning scores by 50%.

902
00:45:53,260 --> 00:45:56,260
You know, we got close to 60 GDT here.

903
00:45:56,260 --> 00:45:58,260
And then, of course, we didn't stop there.

904
00:45:58,260 --> 00:46:01,260
We then re-architected based on that knowledge.

905
00:46:01,260 --> 00:46:03,260
We actually tried to push that system further

906
00:46:03,260 --> 00:46:04,260
and it turned out it hit a brick wall,

907
00:46:04,260 --> 00:46:06,260
so we had to go back to the drawing board

908
00:46:06,260 --> 00:46:07,260
with the knowledge that we had,

909
00:46:07,260 --> 00:46:09,260
re-architected with a brand new system.

910
00:46:09,260 --> 00:46:12,260
And then that finally reached in CAS14 in 2020,

911
00:46:12,260 --> 00:46:14,260
atomic accuracy.

912
00:46:14,260 --> 00:46:18,260
So accuracy within the width of an atom, right,

913
00:46:18,260 --> 00:46:20,260
for all the molecules.

914
00:46:20,260 --> 00:46:26,260
So when we look at the scores and the results of CAS14,

915
00:46:26,260 --> 00:46:29,260
what you see here is that Alpha Fold 2,

916
00:46:29,260 --> 00:46:31,260
this is the root mean squared error,

917
00:46:31,260 --> 00:46:36,260
is less than one angstrom error on average.

918
00:46:36,260 --> 00:46:38,260
And, you know, from the 100 or so proteins

919
00:46:38,260 --> 00:46:40,260
that we're supposed to predict.

920
00:46:40,260 --> 00:46:42,260
So, and one angstrom is the, you know,

921
00:46:42,260 --> 00:46:44,260
the width of basically a carbon atom.

922
00:46:44,260 --> 00:46:47,260
So that's finally, that was the magic threshold

923
00:46:47,260 --> 00:46:51,260
that John Moll and others of the organisers said

924
00:46:51,260 --> 00:46:53,260
that they always set out CASP to do,

925
00:46:53,260 --> 00:46:55,260
because that would make you competitive

926
00:46:55,260 --> 00:46:57,260
with experimental techniques,

927
00:46:57,260 --> 00:46:59,260
which are roughly, you know, the best ones

928
00:46:59,260 --> 00:47:01,260
are at that kind of error rate.

929
00:47:01,260 --> 00:47:03,260
So if one could do that computationally,

930
00:47:03,260 --> 00:47:05,260
then suddenly you have a technique that could be,

931
00:47:05,260 --> 00:47:09,260
you could rely on in tandem with experimental instead of.

932
00:47:09,260 --> 00:47:14,260
And so Alpha Fold 2 got an error of 0.96 angstroms,

933
00:47:14,260 --> 00:47:16,260
which was three times more accurate

934
00:47:16,260 --> 00:47:18,260
than the next best system in CAS14,

935
00:47:18,260 --> 00:47:21,260
even though those systems obviously incorporated

936
00:47:21,260 --> 00:47:23,260
the Alpha Fold 1 techniques

937
00:47:23,260 --> 00:47:26,260
that we'd already published by then.

938
00:47:26,260 --> 00:47:28,260
So this led to the CASP organisers and John Moll

939
00:47:28,260 --> 00:47:30,260
declaring that the structure prediction problem

940
00:47:30,260 --> 00:47:34,260
had essentially been solved after all of these years.

941
00:47:34,260 --> 00:47:36,260
And this is what the predictions look like.

942
00:47:36,260 --> 00:47:39,260
So the ground truth is in green,

943
00:47:39,260 --> 00:47:43,260
and you can see the prediction from Alpha Fold 2 in blue.

944
00:47:43,260 --> 00:47:45,260
And you can see firstly proteins are exquisitely beautiful.

945
00:47:45,260 --> 00:47:48,260
It's one thing to note that I've learned over the many years

946
00:47:48,260 --> 00:47:50,260
I've been working on this now.

947
00:47:50,260 --> 00:47:53,260
They're like exquisite little nano machines.

948
00:47:53,260 --> 00:47:56,260
And you can see how accurate the overlays are.

949
00:47:56,260 --> 00:47:58,260
And we were astounded, of course,

950
00:47:58,260 --> 00:48:01,260
when we first got these results back.

951
00:48:01,260 --> 00:48:03,260
And then, you know, there are many,

952
00:48:03,260 --> 00:48:06,260
this is the architecture for Alpha Fold 2,

953
00:48:06,260 --> 00:48:08,260
so you don't have time to go into the details of today,

954
00:48:08,260 --> 00:48:10,260
but there were a huge number of innovations

955
00:48:10,260 --> 00:48:13,260
that were required to make this work.

956
00:48:13,260 --> 00:48:15,260
And the key technical advances were basically,

957
00:48:15,260 --> 00:48:19,260
first of all, I should say there was no silver bullet.

958
00:48:19,260 --> 00:48:23,260
It needed actually 32 component algorithms

959
00:48:23,260 --> 00:48:25,260
described in 60 pages of supplemental information

960
00:48:25,260 --> 00:48:27,260
actually in the paper.

961
00:48:27,260 --> 00:48:29,260
And that was required.

962
00:48:29,260 --> 00:48:31,260
And every single part of that was required.

963
00:48:31,260 --> 00:48:33,260
So we did these ablation analyses,

964
00:48:33,260 --> 00:48:35,260
which sort of took out components to see

965
00:48:35,260 --> 00:48:37,260
if we could get away without having them.

966
00:48:37,260 --> 00:48:40,260
And the result of that was everything was required.

967
00:48:40,260 --> 00:48:43,260
And the three key sort of takeaways of why Alpha Fold 2

968
00:48:43,260 --> 00:48:45,260
was an improvement over Alpha Fold 1

969
00:48:45,260 --> 00:48:48,260
is we made the system fully end-to-end.

970
00:48:48,260 --> 00:48:51,260
So you can think of it as sort of going end-to-end

971
00:48:51,260 --> 00:48:54,260
with a recycling iterative stage.

972
00:48:54,260 --> 00:48:57,260
At the time, it sort of jigs the protein structure

973
00:48:57,260 --> 00:49:00,260
nearer and closer and closer to the final structure

974
00:49:00,260 --> 00:49:02,260
that it's going to predict.

975
00:49:02,260 --> 00:49:04,260
And Alpha Fold 1 system didn't do that.

976
00:49:04,260 --> 00:49:06,260
It went from the amino acid sequence

977
00:49:06,260 --> 00:49:09,260
to this intermediate representation called a dystagram,

978
00:49:09,260 --> 00:49:13,260
which is a pair-wise dystagram of all the protein molecules

979
00:49:13,260 --> 00:49:16,260
and their distance to each of the other molecules,

980
00:49:16,260 --> 00:49:18,260
the other end molecules.

981
00:49:18,260 --> 00:49:20,260
And then from that, we used a different method

982
00:49:20,260 --> 00:49:22,260
to create the 3D structure.

983
00:49:22,260 --> 00:49:25,260
So we went straight for predicting the 3D structure.

984
00:49:25,260 --> 00:49:27,260
And those of you who work in machine learning

985
00:49:27,260 --> 00:49:29,260
will know that generally speaking,

986
00:49:29,260 --> 00:49:31,260
if you can make something end-to-end

987
00:49:31,260 --> 00:49:33,260
and optimize directly for the thing that you're after,

988
00:49:33,260 --> 00:49:36,260
usually your system will have better performance.

989
00:49:36,260 --> 00:49:38,260
We used an attention-based neural network

990
00:49:38,260 --> 00:49:41,260
to infer this implicit graph structure

991
00:49:41,260 --> 00:49:46,260
of the residues, of the amino acid sequences.

992
00:49:46,260 --> 00:49:49,260
In Alpha Fold 1, we used a convolutional neural net,

993
00:49:49,260 --> 00:49:51,260
which was sort of borrowed from computer vision.

994
00:49:51,260 --> 00:49:53,260
And if you think about it,

995
00:49:53,260 --> 00:49:56,260
that was introducing the wrong bias into protein folding

996
00:49:56,260 --> 00:49:59,260
because with computer vision, pixels next to each other

997
00:49:59,260 --> 00:50:02,260
are obviously going to be correlated in an image, in some sense.

998
00:50:02,260 --> 00:50:04,260
So convolutions make sense.

999
00:50:04,260 --> 00:50:08,260
But actually, for a protein, the amino acid sequence,

1000
00:50:08,260 --> 00:50:10,260
residues that are next to each other

1001
00:50:10,260 --> 00:50:12,260
or close to each other on the string of letters

1002
00:50:12,260 --> 00:50:14,260
may not end up being near each other

1003
00:50:14,260 --> 00:50:16,260
once you get the full 3D fold,

1004
00:50:16,260 --> 00:50:19,260
or things very far away could end up folding over near each other.

1005
00:50:19,260 --> 00:50:22,260
So, in a way, we were giving it the wrong biases,

1006
00:50:22,260 --> 00:50:24,260
so we actually had to remove that.

1007
00:50:24,260 --> 00:50:28,260
And then finally, we built in some biological and evolutionary

1008
00:50:28,260 --> 00:50:30,260
and physics constraints into the system

1009
00:50:30,260 --> 00:50:32,260
without impacting the learning.

1010
00:50:32,260 --> 00:50:34,260
And again, usually, so you can think of it

1011
00:50:34,260 --> 00:50:36,260
as a little bit of a hybrid system,

1012
00:50:36,260 --> 00:50:38,260
that usually, if you put in constraints,

1013
00:50:38,260 --> 00:50:40,260
that impacts the learning.

1014
00:50:40,260 --> 00:50:43,260
We managed to do that without that.

1015
00:50:43,260 --> 00:50:45,260
So this was a huge research effort over sort of five years,

1016
00:50:45,260 --> 00:50:48,260
took about 20 people at its maximum,

1017
00:50:48,260 --> 00:50:50,260
and it was a truly multidisciplinary effort.

1018
00:50:50,260 --> 00:50:53,260
So we needed biologists and physicists and chemists

1019
00:50:53,260 --> 00:50:55,260
as well as machine learners.

1020
00:50:55,260 --> 00:50:57,260
And I think that's an interesting lesson, maybe,

1021
00:50:57,260 --> 00:51:00,260
to learn about cross-disciplinary work in AI for Sciences,

1022
00:51:00,260 --> 00:51:04,260
is you need the experts also from the domain.

1023
00:51:04,260 --> 00:51:06,260
And then the final, maybe interesting point to note on this,

1024
00:51:06,260 --> 00:51:09,260
is that normally, we're always after generality,

1025
00:51:09,260 --> 00:51:13,260
so you can see that from the journey from AlphaGo to AlphaZero,

1026
00:51:13,260 --> 00:51:15,260
was we increasingly made things general.

1027
00:51:15,260 --> 00:51:17,260
You start with performance,

1028
00:51:17,260 --> 00:51:19,260
then you start throwing things out of that system

1029
00:51:19,260 --> 00:51:21,260
to try and make it simpler and more elegant,

1030
00:51:21,260 --> 00:51:23,260
and that usually makes it more general,

1031
00:51:23,260 --> 00:51:25,260
as you understand what it is that you're doing.

1032
00:51:25,260 --> 00:51:29,260
But that's because Go and Chess and those things

1033
00:51:29,260 --> 00:51:32,260
were testbeds for what we wanted to do.

1034
00:51:32,260 --> 00:51:34,260
If you are trying to solve a real-world problem

1035
00:51:34,260 --> 00:51:38,260
that really matters to other scientists or health,

1036
00:51:38,260 --> 00:51:40,260
or in this case, you know, biology,

1037
00:51:40,260 --> 00:51:44,260
then actually, you might as well throw the kitchen sink at it,

1038
00:51:44,260 --> 00:51:47,260
because you actually are really after the output itself,

1039
00:51:47,260 --> 00:51:49,260
in this case, protein structures.

1040
00:51:49,260 --> 00:51:51,260
And that's what we did here.

1041
00:51:51,260 --> 00:51:53,260
We really threw everything we had at it,

1042
00:51:53,260 --> 00:51:57,260
and it's, I think, the most complex system that we've ever built.

1043
00:51:57,260 --> 00:52:00,260
Other things to note about this system is that it's also,

1044
00:52:00,260 --> 00:52:02,260
AlphaFold 1 was relatively slow,

1045
00:52:02,260 --> 00:52:06,260
took a few weeks of compute time to do a protein.

1046
00:52:06,260 --> 00:52:10,260
AlphaFold 2 took two weeks to train the whole system

1047
00:52:10,260 --> 00:52:14,260
on a relatively modest setup of eight TPUs or 150 GPUs,

1048
00:52:14,260 --> 00:52:17,260
which, by modern-day machine learning standards, is quite small.

1049
00:52:17,260 --> 00:52:19,260
And then the inference, the predictions,

1050
00:52:19,260 --> 00:52:22,260
can be done lightening fast in an order of minutes,

1051
00:52:22,260 --> 00:52:27,260
sometimes seconds for an average protein on a single GPU.

1052
00:52:27,260 --> 00:52:30,260
So when we did this, AlphaFold 2, we announced the results,

1053
00:52:30,260 --> 00:52:32,260
published the methods.

1054
00:52:32,260 --> 00:52:36,260
Over Christmas, that Christmas, this is back in 2020,

1055
00:52:36,260 --> 00:52:39,260
we were thinking, okay, how should we give access to the system

1056
00:52:39,260 --> 00:52:41,260
to biologists around the world?

1057
00:52:41,260 --> 00:52:44,260
And normally what you do is that you set up a server,

1058
00:52:44,260 --> 00:52:48,260
people, biologists, send you their amino acid sequences,

1059
00:52:48,260 --> 00:52:50,260
and then you give back, a few days later,

1060
00:52:50,260 --> 00:52:52,260
you might give them back the prediction.

1061
00:52:52,260 --> 00:52:55,260
But actually what we realised, because AlphaFold 2 was so fast,

1062
00:52:55,260 --> 00:52:59,260
we could actually just fold everything ourselves in one go.

1063
00:52:59,260 --> 00:53:01,260
So we just fold all proteins.

1064
00:53:01,260 --> 00:53:04,260
And we'll start with the human proteome,

1065
00:53:04,260 --> 00:53:08,260
just like the human genome equivalent, but in protein space.

1066
00:53:08,260 --> 00:53:11,260
And so that's what we did over the Christmas.

1067
00:53:11,260 --> 00:53:14,260
We folded the whole human proteome.

1068
00:53:14,260 --> 00:53:17,260
And so, which is another thing I love about AI and computing,

1069
00:53:17,260 --> 00:53:19,260
is you can have your Christmas lunch,

1070
00:53:19,260 --> 00:53:22,260
and while you're doing that, offer our AIs doing something useful

1071
00:53:22,260 --> 00:53:24,260
for the world.

1072
00:53:24,260 --> 00:53:26,260
So the human proteome, so we published that as well

1073
00:53:26,260 --> 00:53:29,260
in the summer of 21 last summer.

1074
00:53:29,260 --> 00:53:32,260
So AlphaFold 2, we predicted that every protein

1075
00:53:32,260 --> 00:53:35,260
in the human body is around 20,000 proteins,

1076
00:53:35,260 --> 00:53:38,260
represented, obviously, expressed by the human genome.

1077
00:53:38,260 --> 00:53:41,260
And at the point where we did this, experiments,

1078
00:53:41,260 --> 00:53:44,260
30 years of experiments, 30, 40 years of experiments,

1079
00:53:44,260 --> 00:53:48,260
had covered about 17% of the human proteome.

1080
00:53:48,260 --> 00:53:51,260
And we more than doubled that overnight

1081
00:53:51,260 --> 00:53:54,260
in terms of very high accuracy structures.

1082
00:53:54,260 --> 00:53:57,260
Obviously we folded all of them, but very high accuracy,

1083
00:53:57,260 --> 00:53:59,260
so that's less than one angstrom error.

1084
00:53:59,260 --> 00:54:02,260
They're sort of up to experimental quality.

1085
00:54:02,260 --> 00:54:04,260
We went to 36%.

1086
00:54:04,260 --> 00:54:07,260
And 58% at high accuracy, where we call high accuracy

1087
00:54:07,260 --> 00:54:11,260
when the backbone is mostly where you can be confident in.

1088
00:54:11,260 --> 00:54:14,260
But the side chains may be slightly out.

1089
00:54:14,260 --> 00:54:17,260
And then, of course, the question is what about the rest,

1090
00:54:17,260 --> 00:54:19,260
the other 42%.

1091
00:54:19,260 --> 00:54:22,260
And it may be that some of those AlphaFold 2

1092
00:54:22,260 --> 00:54:24,260
is just bad at, but increasingly,

1093
00:54:24,260 --> 00:54:26,260
and this is an open research question,

1094
00:54:26,260 --> 00:54:29,260
when we look at it with biologists, and biologists often send us in results,

1095
00:54:29,260 --> 00:54:31,260
like, I look at this one folded really well,

1096
00:54:31,260 --> 00:54:33,260
or this one didn't fold well,

1097
00:54:33,260 --> 00:54:36,260
we often find that the ones that didn't fold well

1098
00:54:36,260 --> 00:54:39,260
were actually what's called unstructured in isolation.

1099
00:54:39,260 --> 00:54:42,260
So they're disorder, intrinsically disorder proteins,

1100
00:54:42,260 --> 00:54:45,260
which means that until you know what they interact with,

1101
00:54:45,260 --> 00:54:47,260
they're basically squiggly bits of string.

1102
00:54:47,260 --> 00:54:50,260
And then presumably, when they interact with something in the body,

1103
00:54:50,260 --> 00:54:53,260
they then, another protein usually, they'll then form a shape.

1104
00:54:53,260 --> 00:54:56,260
But we don't know what that shape is in isolation, right?

1105
00:54:56,260 --> 00:54:59,260
We may not even know what it interacts with at this stage.

1106
00:54:59,260 --> 00:55:02,260
So, actually, people have turned this around now

1107
00:55:02,260 --> 00:55:05,260
to use it as a disordered protein predictor.

1108
00:55:05,260 --> 00:55:07,260
So, where AlphaFold doesn't do well,

1109
00:55:07,260 --> 00:55:10,260
perhaps that's pretty good evidence that it's a disordered protein,

1110
00:55:10,260 --> 00:55:13,260
which, of course, is very important in things like disease,

1111
00:55:13,260 --> 00:55:18,260
Alzheimer's, other things are thought to be to do with badly folded or disordered proteins.

1112
00:55:20,260 --> 00:55:23,260
One of the other things we did, which was a nice innovation for AlphaFold,

1113
00:55:23,260 --> 00:55:27,260
was have the system predicted its own confidence in its own predictions.

1114
00:55:27,260 --> 00:55:30,260
And the reason we did this is we wanted biologists to use this,

1115
00:55:30,260 --> 00:55:32,260
who maybe would not care about the machine learning techniques

1116
00:55:32,260 --> 00:55:35,260
or not understand them, or frankly, it would be irrelevant to them.

1117
00:55:35,260 --> 00:55:37,260
They would just be interested in the structure.

1118
00:55:37,260 --> 00:55:40,260
And we wanted to make sure that they were easily able to evaluate

1119
00:55:40,260 --> 00:55:44,260
the quality of that prediction and what parts of it they could rely on.

1120
00:55:44,260 --> 00:55:48,260
And which other parts they maybe need to check experimentally.

1121
00:55:48,260 --> 00:55:52,260
So what we did is AlphaFold, basically, we produced predictions,

1122
00:55:52,260 --> 00:55:57,260
there were split into three thresholds,

1123
00:55:57,260 --> 00:56:00,260
over 90 was what we call very high accuracy,

1124
00:56:00,260 --> 00:56:03,260
so less than one angstrom error, experimental quality,

1125
00:56:03,260 --> 00:56:05,260
greater than 70 was the backbone's correct,

1126
00:56:05,260 --> 00:56:08,260
and then less than 50 may be these red regions.

1127
00:56:08,260 --> 00:56:11,260
So you can see in the database that's what they look like.

1128
00:56:11,260 --> 00:56:14,260
It's something that should not be trusted.

1129
00:56:14,260 --> 00:56:19,260
We did a further 20 model organisms covering all of the critical model organisms

1130
00:56:19,260 --> 00:56:23,260
used in research, and also some important other ones in disease,

1131
00:56:23,260 --> 00:56:28,260
like tuberculosis, and also agriculture, like wheat and rice.

1132
00:56:28,260 --> 00:56:34,260
And a lot of these proteomes are much less covered than the human proteome.

1133
00:56:34,260 --> 00:56:36,260
Of course, the human one is where the most effort is being,

1134
00:56:36,260 --> 00:56:37,260
that's at 17%.

1135
00:56:37,260 --> 00:56:40,260
For some of these organisms, it's like less than 1%.

1136
00:56:40,260 --> 00:56:43,260
So for the researchers in those plant scientists and other things,

1137
00:56:43,260 --> 00:56:46,260
this is a huge boon for them because they would never have the resources

1138
00:56:46,260 --> 00:56:51,260
to spend that time to crystallise the proteins they're interested in.

1139
00:56:51,260 --> 00:56:54,260
We then teamed up with Emble EBI,

1140
00:56:54,260 --> 00:56:57,260
the European Bioinformatics Institute at Cambridge,

1141
00:56:57,260 --> 00:56:59,260
and they're amazing as a partnership team.

1142
00:56:59,260 --> 00:57:02,260
They host a lot of the biggest databases around the world already,

1143
00:57:02,260 --> 00:57:05,260
and we thought the best way to host all this data

1144
00:57:05,260 --> 00:57:08,260
is to just give it to them and allow them to host it

1145
00:57:08,260 --> 00:57:11,260
and plug it into the mainstream of biology tools.

1146
00:57:11,260 --> 00:57:13,260
And so we had a great collaboration with them,

1147
00:57:13,260 --> 00:57:17,260
and then we basically released all this data for free and unrestricted access

1148
00:57:17,260 --> 00:57:22,260
for any use, industrial or academic, because it's so completely free.

1149
00:57:22,260 --> 00:57:26,260
And it's amazing to see the impact of that,

1150
00:57:26,260 --> 00:57:30,260
and we tried to sort of maximise the scientific impact of this

1151
00:57:30,260 --> 00:57:32,260
by releasing it in that way.

1152
00:57:32,260 --> 00:57:35,260
The other thing we did do, and I want to touch this on this at the end,

1153
00:57:35,260 --> 00:57:37,260
is think about the safety and ethics of this,

1154
00:57:37,260 --> 00:57:42,260
and we consulted with over 30 experts in various areas of biology

1155
00:57:42,260 --> 00:57:45,260
bioinformatics, biosecurity and pharma

1156
00:57:45,260 --> 00:57:49,260
to check that this was going to be okay to release this type of information.

1157
00:57:49,260 --> 00:57:52,260
And they all came back with that they were not worried about this,

1158
00:57:52,260 --> 00:57:55,260
but they were potentially worried about future things.

1159
00:57:55,260 --> 00:57:57,260
So that's something that we bear in mind.

1160
00:57:57,260 --> 00:58:00,260
There are now a million predictions in the database today.

1161
00:58:00,260 --> 00:58:02,260
I just want to call out one thing.

1162
00:58:02,260 --> 00:58:07,260
We specially, ourselves, we specially prioritise neglected tropical diseases,

1163
00:58:07,260 --> 00:58:10,260
because those are the ones that affect the developing world,

1164
00:58:10,260 --> 00:58:12,260
the poorest people in the world the most,

1165
00:58:12,260 --> 00:58:15,260
and they're the least researched, because of course there's no money in it for pharma companies,

1166
00:58:15,260 --> 00:58:19,260
so that often it's NGOs and non-profits that have to do the work there.

1167
00:58:19,260 --> 00:58:21,260
So for them, it's amazing to get all the structures,

1168
00:58:21,260 --> 00:58:23,260
because they can go straight to drug discovery

1169
00:58:23,260 --> 00:58:27,260
without having to go to the intermediate step of finding these structures.

1170
00:58:27,260 --> 00:58:29,260
So we prioritise all these diseases,

1171
00:58:29,260 --> 00:58:33,260
and including ones that we've got being given from the WHO

1172
00:58:33,260 --> 00:58:36,260
about potential future pathogens.

1173
00:58:36,260 --> 00:58:38,260
And what's the community done with AlphaFold already?

1174
00:58:38,260 --> 00:58:43,260
We've seen just in nine months or 10 months incredible amount of work has been done.

1175
00:58:43,260 --> 00:58:46,260
This is really cool on the left here with some colleagues at Emble.

1176
00:58:46,260 --> 00:58:51,260
They used AlphaFold and Experiment to combine with their experimental data

1177
00:58:51,260 --> 00:58:54,260
to put together what's called the nuclear pore complex,

1178
00:58:54,260 --> 00:58:56,260
which is one of the biggest proteins in the body.

1179
00:58:56,260 --> 00:58:58,260
It's massive for a protein,

1180
00:58:58,260 --> 00:59:01,260
and what it is is it's a little gateway into the nucleus of your cell,

1181
00:59:01,260 --> 00:59:04,260
and it opens and closes to let things in.

1182
00:59:04,260 --> 00:59:07,260
And they were able to, you know, it's beautiful if you look at it,

1183
00:59:07,260 --> 00:59:10,260
able to put it all together and then visualise it.

1184
00:59:10,260 --> 00:59:14,260
I talked about this disorder predictor, WHO top 30 pathogens,

1185
00:59:14,260 --> 00:59:17,260
and actually interestingly, it's helped experimentalists,

1186
00:59:17,260 --> 00:59:19,260
the ones that benefited first from this,

1187
00:59:19,260 --> 00:59:25,260
because they can combine this with their maybe some low resolution images they have,

1188
00:59:25,260 --> 00:59:28,260
and if they have two sources of information, they can then make a sharp prediction

1189
00:59:28,260 --> 00:59:31,260
from their maybe their slightly lower resolution experimental data,

1190
00:59:31,260 --> 00:59:35,260
and then a computational prediction.

1191
00:59:35,260 --> 00:59:39,260
So it's been really gratifying to see hundreds of papers now

1192
00:59:39,260 --> 00:59:43,260
and applications already with being used for AlphaFold,

1193
00:59:43,260 --> 00:59:46,260
also in industry too for drug discovery.

1194
00:59:46,260 --> 00:59:48,260
So what has the impact been?

1195
00:59:48,260 --> 00:59:53,260
So we already have 500,000 researchers have used the database.

1196
00:59:53,260 --> 00:59:55,260
We think that's almost every biologist in the world

1197
00:59:55,260 --> 00:59:58,260
has probably looked up their proteins they're interested in.

1198
00:59:58,260 --> 01:00:01,260
190 countries, 1.5 million structures viewed,

1199
01:00:01,260 --> 01:00:04,260
and already over 3,000 citations,

1200
01:00:04,260 --> 01:00:09,260
and we've had some nice accolades along the way from science and nature on the method.

1201
01:00:09,260 --> 01:00:14,260
And then over the next year, we plan to fold every protein, you know,

1202
01:00:14,260 --> 01:00:16,260
in known to science, which is in Uniprot,

1203
01:00:16,260 --> 01:00:19,260
which is the massive database that has all the genetic sequences,

1204
01:00:19,260 --> 01:00:21,260
and there's over 100 million proteins known to science,

1205
01:00:21,260 --> 01:00:24,260
and we're steadily sort of progressing through that right now,

1206
01:00:24,260 --> 01:00:27,260
and we'll be releasing that over time.

1207
01:00:27,260 --> 01:00:29,260
So stepping back then, what does this mean?

1208
01:00:29,260 --> 01:00:33,260
I think that maybe, you know, we're entering a new era

1209
01:00:33,260 --> 01:00:35,260
of what I would like to call digital biology.

1210
01:00:35,260 --> 01:00:39,260
So I think the way I think about biology is that at the most fundamental level,

1211
01:00:39,260 --> 01:00:41,260
it's an information processing system,

1212
01:00:41,260 --> 01:00:46,260
albeit an exquisitely complex and emergent one.

1213
01:00:46,260 --> 01:00:49,260
And I think of it as maybe the potential,

1214
01:00:49,260 --> 01:00:52,260
the perfect sort of regime for AI to be useful in,

1215
01:00:52,260 --> 01:00:56,260
because, you know, one thing I think of it analogous to is in physics,

1216
01:00:56,260 --> 01:00:59,260
you know, we use mathematics to describe physical phenomena,

1217
01:00:59,260 --> 01:01:02,260
and it's been extraordinarily successful in doing that.

1218
01:01:02,260 --> 01:01:05,260
Of course, mathematics can also be applied to biology

1219
01:01:05,260 --> 01:01:07,260
and has been applied successfully in many domains,

1220
01:01:07,260 --> 01:01:10,260
but I think a lot of these emergent and complex phenomena

1221
01:01:10,260 --> 01:01:13,260
are just too complicated to be described with a few equations, right?

1222
01:01:13,260 --> 01:01:16,260
I just don't really see how you can say,

1223
01:01:16,260 --> 01:01:20,260
come up with, you know, Kepler's laws of motion just from other cell, right?

1224
01:01:20,260 --> 01:01:23,260
How would one do that? You know, just a few differential equations.

1225
01:01:23,260 --> 01:01:25,260
It doesn't seem to me likely.

1226
01:01:25,260 --> 01:01:29,260
And I think maybe a learn model is a better way to approach that.

1227
01:01:29,260 --> 01:01:32,260
And I think, and I hope that AlphaFold is a proof of concept

1228
01:01:32,260 --> 01:01:34,260
that this may be possible,

1229
01:01:34,260 --> 01:01:38,260
and maybe usher, can help usher in this new dawn of digital biology.

1230
01:01:38,260 --> 01:01:40,260
And our attempts to go further in that space

1231
01:01:40,260 --> 01:01:42,260
is obviously we're researching further at DeepMind,

1232
01:01:42,260 --> 01:01:45,260
and the science team, we sort of doubled down on all these things

1233
01:01:45,260 --> 01:01:47,260
within the biology team at DeepMind.

1234
01:01:47,260 --> 01:01:50,260
And we've also spun out a new company, Isomorphic Labs,

1235
01:01:50,260 --> 01:01:54,260
to specifically build on this work and other related work,

1236
01:01:54,260 --> 01:01:58,260
specifically for drug discovery to accelerate drug discovery,

1237
01:01:58,260 --> 01:02:00,260
which we hope, using computational and AI methods,

1238
01:02:00,260 --> 01:02:03,260
can maybe be an order of magnitude quicker.

1239
01:02:03,260 --> 01:02:05,260
Currently, you know, it takes an average of 10 years

1240
01:02:05,260 --> 01:02:09,260
to go from identifying a target to a candidate drug.

1241
01:02:10,260 --> 01:02:13,260
So just to start closing then, I just, you know,

1242
01:02:13,260 --> 01:02:15,260
there isn't time to go into this, but it's for us,

1243
01:02:15,260 --> 01:02:17,260
it's been like a renaissance year in some sense.

1244
01:02:17,260 --> 01:02:21,260
I've been having so much fun ticking off all of my sort of

1245
01:02:21,260 --> 01:02:23,260
childhood dream projects,

1246
01:02:23,260 --> 01:02:26,260
infusion and quantum chemistry and conjectures in maths,

1247
01:02:26,260 --> 01:02:28,260
material science, weather prediction.

1248
01:02:28,260 --> 01:02:31,260
This has all become reality now in the last year

1249
01:02:31,260 --> 01:02:34,260
of applying it to important problems in each of these domains

1250
01:02:34,260 --> 01:02:36,260
and, you know, publishing nice and important work

1251
01:02:36,260 --> 01:02:38,260
in each of these areas.

1252
01:02:38,260 --> 01:02:41,260
In applications, of course, there are lots of amazing

1253
01:02:41,260 --> 01:02:43,260
industrial applications that we've been doing,

1254
01:02:43,260 --> 01:02:45,260
and we have an applied team at DeepMind

1255
01:02:45,260 --> 01:02:48,260
that works with Google product teams to incorporate

1256
01:02:48,260 --> 01:02:51,260
all of our research into hundreds of products now at Google.

1257
01:02:51,260 --> 01:02:54,260
Pretty much every product you use of Google's

1258
01:02:54,260 --> 01:02:56,260
will have some DeepMind technology in it.

1259
01:02:56,260 --> 01:02:59,260
Some of the ones I just want to call out are our data centre work

1260
01:02:59,260 --> 01:03:02,260
and energy optimisation of data centres and the energy they use

1261
01:03:02,260 --> 01:03:04,260
and the cooling systems they use,

1262
01:03:04,260 --> 01:03:06,260
and we're looking at applying that to grid scale now,

1263
01:03:06,260 --> 01:03:10,260
WaveNet, which is the best text-to-speech system in the world.

1264
01:03:10,260 --> 01:03:13,260
So any device that you talk to that talks back to you

1265
01:03:13,260 --> 01:03:17,260
will be using WaveNet to have really realistic voices.

1266
01:03:17,260 --> 01:03:21,260
Even interesting things like better video compression for YouTube.

1267
01:03:21,260 --> 01:03:24,260
We can save 4% of the bit rate that is used

1268
01:03:24,260 --> 01:03:27,260
whilst maintaining video quality

1269
01:03:27,260 --> 01:03:29,260
and also things like recommendation systems,

1270
01:03:29,260 --> 01:03:32,260
but there's just too many to mention, actually.

1271
01:03:32,260 --> 01:03:34,260
And then, of course, very in vogue now,

1272
01:03:34,260 --> 01:03:36,260
and we have a ton of work on this area,

1273
01:03:36,260 --> 01:03:38,260
but it will be a whole talk in itself,

1274
01:03:38,260 --> 01:03:41,260
is large models, and we have our own really cool large models

1275
01:03:41,260 --> 01:03:45,260
that alpha code that can programme from a text description

1276
01:03:45,260 --> 01:03:46,260
and write code.

1277
01:03:46,260 --> 01:03:49,260
It's still amazing to me in competitive programming level.

1278
01:03:49,260 --> 01:03:52,260
Chinchilla, which is our large language model

1279
01:03:52,260 --> 01:03:54,260
that is computer-efficient.

1280
01:03:54,260 --> 01:03:57,260
Flamingo, that's our vision language combined model

1281
01:03:57,260 --> 01:03:58,260
that can describe images,

1282
01:03:58,260 --> 01:04:01,260
and then Gata, our latest model that is super general,

1283
01:04:01,260 --> 01:04:03,260
can do robotics, video games,

1284
01:04:03,260 --> 01:04:07,260
all sorts of things, language just with one model.

1285
01:04:07,260 --> 01:04:09,260
So this is all very exciting,

1286
01:04:09,260 --> 01:04:12,260
but I just want to end my last couple of slides

1287
01:04:12,260 --> 01:04:14,260
with a bit about ethics,

1288
01:04:14,260 --> 01:04:17,260
because obviously this is hosted by the Institute of Ethics,

1289
01:04:17,260 --> 01:04:20,260
and it's a very important topic,

1290
01:04:20,260 --> 01:04:22,260
and not just because of that,

1291
01:04:22,260 --> 01:04:25,260
but it's also what the Tana lectures are about, too.

1292
01:04:25,260 --> 01:04:28,260
So we think a lot about pioneering responsibly.

1293
01:04:28,260 --> 01:04:32,260
This is actually two of our values at DeepMind combined,

1294
01:04:32,260 --> 01:04:35,260
pioneering and being responsible.

1295
01:04:35,260 --> 01:04:38,260
I hope I've convinced you and you hope you will realise

1296
01:04:38,260 --> 01:04:41,260
that AI is this incredible potential to help

1297
01:04:41,260 --> 01:04:44,260
with some of humanity's greatest challenges.

1298
01:04:44,260 --> 01:04:46,260
I think disease, climate,

1299
01:04:46,260 --> 01:04:48,260
all of these things could be in scope,

1300
01:04:48,260 --> 01:04:52,260
but obviously AI has to be built responsibly and safely,

1301
01:04:52,260 --> 01:04:55,260
and we have to make sure the people who are building these things,

1302
01:04:55,260 --> 01:04:57,260
it's used for the benefit of everyone.

1303
01:04:57,260 --> 01:05:00,260
So we've had this sort of front of mind

1304
01:05:00,260 --> 01:05:02,260
from the beginning of DeepMind,

1305
01:05:02,260 --> 01:05:05,260
and as with any powerful technology,

1306
01:05:05,260 --> 01:05:07,260
and I think AI is no different,

1307
01:05:07,260 --> 01:05:09,260
although it may be more general and more powerful

1308
01:05:09,260 --> 01:05:11,260
than any that has gone before,

1309
01:05:11,260 --> 01:05:14,260
whether or not it's beneficial or harmful to us in society,

1310
01:05:14,260 --> 01:05:17,260
depends on how we deploy it and how we use it,

1311
01:05:17,260 --> 01:05:20,260
and what sorts of things we decide to use it for.

1312
01:05:20,260 --> 01:05:23,260
And I think it's important that we have a really wide debate

1313
01:05:23,260 --> 01:05:26,260
about that at places like this and the Institute of Ethics.

1314
01:05:26,260 --> 01:05:29,260
I'm very excited to see that being set up

1315
01:05:29,260 --> 01:05:33,260
and for us to interact with the new Institute.

1316
01:05:33,260 --> 01:05:36,260
Here, just one mention is that DNA has been really critical,

1317
01:05:36,260 --> 01:05:39,260
and we've been pushing very hard on this the last few years,

1318
01:05:39,260 --> 01:05:41,260
and I think it's critical to this,

1319
01:05:41,260 --> 01:05:44,260
to make sure we get the broadest possible input

1320
01:05:44,260 --> 01:05:47,260
into the design and deployment decisions of these systems,

1321
01:05:47,260 --> 01:05:51,260
especially for the people that this affects the most,

1322
01:05:51,260 --> 01:05:53,260
that these systems affect the most.

1323
01:05:53,260 --> 01:05:55,260
That's something we've been pushing very hard on.

1324
01:05:55,260 --> 01:05:57,260
There's still a lot more work to do,

1325
01:05:57,260 --> 01:06:00,260
and there's still a lot more progress at DeepMind,

1326
01:06:00,260 --> 01:06:03,260
and we've been also doing that with all of our sponsorship that we do.

1327
01:06:03,260 --> 01:06:06,260
We've now done nearly $50 million worth of sponsorship

1328
01:06:06,260 --> 01:06:09,260
of scholarships, diversity scholarships, chairs,

1329
01:06:09,260 --> 01:06:11,260
and academic institutions and projects,

1330
01:06:11,260 --> 01:06:14,260
and also funding things like the Deep Learning in Darba,

1331
01:06:14,260 --> 01:06:17,260
which is Africa's biggest conference on machine learning.

1332
01:06:17,260 --> 01:06:21,260
I'm really proud to say that a lot of DeepMinders helped set that up.

1333
01:06:21,260 --> 01:06:24,260
And so there's many, many things that we're doing across the industry

1334
01:06:24,260 --> 01:06:27,260
and act as a role model for the rest of industry.

1335
01:06:28,260 --> 01:06:30,260
So then on ethics and safety,

1336
01:06:30,260 --> 01:06:32,260
this has always been central to our mission,

1337
01:06:32,260 --> 01:06:35,260
because you saw our audacious mission at the start,

1338
01:06:35,260 --> 01:06:39,260
and we, even back in 2010 in our little attic room,

1339
01:06:39,260 --> 01:06:41,260
we were planning for success,

1340
01:06:41,260 --> 01:06:44,260
and of course we had to think through as scientists

1341
01:06:44,260 --> 01:06:46,260
what does success mean, what will the world look like,

1342
01:06:46,260 --> 01:06:48,260
and obviously if one thinks that through

1343
01:06:48,260 --> 01:06:51,260
and it's becoming obvious now in 2022,

1344
01:06:51,260 --> 01:06:54,260
but it was obvious to us then in 2010

1345
01:06:54,260 --> 01:06:56,260
that this would have to be critical,

1346
01:06:56,260 --> 01:06:59,260
that it would be really important questions

1347
01:06:59,260 --> 01:07:01,260
that would have to be addressed.

1348
01:07:01,260 --> 01:07:04,260
And part of that, so we've been doing this in the background all along,

1349
01:07:04,260 --> 01:07:06,260
and we'll be talking more about this work probably in future.

1350
01:07:06,260 --> 01:07:09,260
We were instrumental in drafting Google's AI principles,

1351
01:07:09,260 --> 01:07:11,260
which are now publicly available,

1352
01:07:11,260 --> 01:07:14,260
and they were partly based on our original ethics charter

1353
01:07:14,260 --> 01:07:16,260
that we've had from the very beginning of DeepMind.

1354
01:07:16,260 --> 01:07:19,260
And the aim of these principles, and you can look them up later

1355
01:07:19,260 --> 01:07:21,260
if you want to look at what they say,

1356
01:07:21,260 --> 01:07:24,260
is obviously to help realise the far-ranging benefits

1357
01:07:24,260 --> 01:07:26,260
that clearly AI could have for everyone

1358
01:07:26,260 --> 01:07:31,260
whilst identifying and mitigating potential risks and harms ahead of time.

1359
01:07:31,260 --> 01:07:34,260
And we continue to try and act as thought leadership

1360
01:07:34,260 --> 01:07:37,260
for the AI community on many of these topics,

1361
01:07:37,260 --> 01:07:40,260
strategy risks, ethics, and safety.

1362
01:07:40,260 --> 01:07:42,260
So what should we do then?

1363
01:07:42,260 --> 01:07:44,260
And I just want to end with this last slide here,

1364
01:07:44,260 --> 01:07:48,260
is what I think we should not do is move fast and break things,

1365
01:07:48,260 --> 01:07:50,260
sort of the Silicon Valley trope.

1366
01:07:50,260 --> 01:07:53,260
And I think we've seen the consequence of that playing out.

1367
01:07:53,260 --> 01:07:57,260
It can be very extraordinarily effective to get powerful systems

1368
01:07:57,260 --> 01:07:59,260
and growth and other things,

1369
01:07:59,260 --> 01:08:03,260
but I do not think it's the right way to address really powerful

1370
01:08:03,260 --> 01:08:07,260
potential dual-use technologies like AI.

1371
01:08:07,260 --> 01:08:12,260
And the problem with it is that one of the things that falls out

1372
01:08:12,260 --> 01:08:16,260
of moving fast and break things is actually doing live A-B testing in the world,

1373
01:08:16,260 --> 01:08:18,260
with your minimum viable products and other things.

1374
01:08:18,260 --> 01:08:21,260
And of course, the question is, if one does that,

1375
01:08:21,260 --> 01:08:26,260
where does the option B turns out to be a terrible option?

1376
01:08:26,260 --> 01:08:28,260
Well, where does the harm of that happen?

1377
01:08:28,260 --> 01:08:30,260
Well, it resides in society, doesn't it?

1378
01:08:30,260 --> 01:08:32,260
That pays the cost of your learning,

1379
01:08:32,260 --> 01:08:34,260
because you've done it in the world.

1380
01:08:34,260 --> 01:08:39,260
And it's probably fine if you're just doing a little gaming app

1381
01:08:39,260 --> 01:08:41,260
or photo app or something,

1382
01:08:41,260 --> 01:08:43,260
but we already see with social networks,

1383
01:08:43,260 --> 01:08:46,260
it's not fine when you're at billion-user scale

1384
01:08:46,260 --> 01:08:50,260
and things really matter in terms of your A-B testing.

1385
01:08:50,260 --> 01:08:54,260
I don't think it's responsible to do that.

1386
01:08:54,260 --> 01:08:56,260
So what should we do instead?

1387
01:08:56,260 --> 01:09:01,260
Well, fortunately, we already have another method,

1388
01:09:01,260 --> 01:09:03,260
which I think would be better, the scientific method,

1389
01:09:03,260 --> 01:09:07,260
which I do think is probably maybe humanity's greatest idea ever.

1390
01:09:07,260 --> 01:09:09,260
And I think it can apply here.

1391
01:09:09,260 --> 01:09:12,260
And I think we should use the scientific method

1392
01:09:12,260 --> 01:09:15,260
when we're approaching how to deal with these

1393
01:09:15,260 --> 01:09:19,260
very powerful, incredible potential technologies.

1394
01:09:19,260 --> 01:09:22,260
And what does the scientific method involve here in this domain?

1395
01:09:22,260 --> 01:09:25,260
Well, it's sort of thoughtful deliberation and thought

1396
01:09:25,260 --> 01:09:28,260
ahead of time and foresight ahead of time,

1397
01:09:28,260 --> 01:09:30,260
where you have hypothesis generation

1398
01:09:30,260 --> 01:09:32,260
on what might happen if one were to be successful

1399
01:09:32,260 --> 01:09:34,260
with what you're trying to do, right?

1400
01:09:34,260 --> 01:09:38,260
So how about we think about that ahead of time, not afterwards?

1401
01:09:38,260 --> 01:09:42,260
Then there's rigorous and careful and control testing.

1402
01:09:42,260 --> 01:09:44,260
I think that's one of the main things I learned from my PhD,

1403
01:09:44,260 --> 01:09:48,260
apart from all the neuroscience, was also the value of control tests.

1404
01:09:48,260 --> 01:09:50,260
I don't think you can really understand.

1405
01:09:50,260 --> 01:09:52,260
In a way, I think when I started my PhD, at least,

1406
01:09:52,260 --> 01:09:55,260
I was all about what's the condition of interest,

1407
01:09:55,260 --> 01:09:58,260
and that's the thing that you're going to make your new advance with.

1408
01:09:58,260 --> 01:10:00,260
But actually, you can't conclude anything, of course,

1409
01:10:00,260 --> 01:10:02,260
unless you have good controls.

1410
01:10:02,260 --> 01:10:05,260
And I think that's something I don't think engineers get

1411
01:10:05,260 --> 01:10:07,260
first time around, actually.

1412
01:10:07,260 --> 01:10:10,260
But scientists and researchers, of course, do get that,

1413
01:10:10,260 --> 01:10:12,260
because that's one of the things that you learn

1414
01:10:12,260 --> 01:10:14,260
from doing a research PhD.

1415
01:10:14,260 --> 01:10:16,260
So control testing in controlled environments,

1416
01:10:16,260 --> 01:10:18,260
not out in the world,

1417
01:10:18,260 --> 01:10:21,260
until you better understand what it is that you're doing.

1418
01:10:21,260 --> 01:10:24,260
So, of course, one updates on empirical data,

1419
01:10:24,260 --> 01:10:26,260
obviously ideally with peer review,

1420
01:10:26,260 --> 01:10:28,260
so you get critique from the outside

1421
01:10:28,260 --> 01:10:30,260
and people who are independent from your work,

1422
01:10:30,260 --> 01:10:33,260
all of these things that are standard in the scientific method, right,

1423
01:10:33,260 --> 01:10:35,260
but are not standard in engineering.

1424
01:10:35,260 --> 01:10:37,260
And all of this is in service

1425
01:10:37,260 --> 01:10:39,260
of getting a better understanding of the system

1426
01:10:39,260 --> 01:10:42,260
before one deploys it at scale, right,

1427
01:10:42,260 --> 01:10:45,260
and then maybe you find out something.

1428
01:10:45,260 --> 01:10:49,260
So my view is that as we approach artificial general intelligence,

1429
01:10:49,260 --> 01:10:52,260
and it's a super exciting moment in time,

1430
01:10:52,260 --> 01:10:55,260
as you can hopefully get from my talk

1431
01:10:55,260 --> 01:10:57,260
and my excitement over that,

1432
01:10:57,260 --> 01:11:00,260
but we need to treat it with the respect and precaution

1433
01:11:00,260 --> 01:11:02,260
and sort of humblness, I would say,

1434
01:11:02,260 --> 01:11:05,260
that the technology of this magnitude demands.

1435
01:11:05,260 --> 01:11:08,260
And I think that's what we are trying to be at the forefront on,

1436
01:11:08,260 --> 01:11:12,260
and I think I'll be talking a lot more about this in the future.

1437
01:11:12,260 --> 01:11:17,260
So I'll just end by on the sort of going back to the science question.

1438
01:11:17,260 --> 01:11:20,260
I think if we get AI right,

1439
01:11:20,260 --> 01:11:22,260
it could potentially be the greatest and most beneficial

1440
01:11:22,260 --> 01:11:24,260
technology humanity has ever invented.

1441
01:11:24,260 --> 01:11:27,260
And I think of AI as this ultimate general purpose tool

1442
01:11:27,260 --> 01:11:31,260
to help us as scientists understand the universe better

1443
01:11:31,260 --> 01:11:33,260
and perhaps our place in it.

1444
01:11:33,260 --> 01:11:35,260
Thank you.

1445
01:11:35,260 --> 01:11:38,260
APPLAUSE

1446
01:11:58,260 --> 01:12:01,260
Well, thank you, Dennis, for that extraordinary tour de force.

1447
01:12:01,260 --> 01:12:04,260
We do have a little time for questions.

1448
01:12:04,260 --> 01:12:07,260
But we wanted to give you the chance to kind of give us

1449
01:12:07,260 --> 01:12:09,260
that sense of your vision.

1450
01:12:09,260 --> 01:12:13,260
Now, we've got an opportunity to have questions from the audience.

1451
01:12:13,260 --> 01:12:16,260
Got to wait for the microphone to be handed to them

1452
01:12:16,260 --> 01:12:18,260
and to stand up if possible when asking questions,

1453
01:12:18,260 --> 01:12:21,260
but I'm afraid there is a kind of discrimination.

1454
01:12:21,260 --> 01:12:25,260
It's only those on the ground floor

1455
01:12:25,260 --> 01:12:30,260
that can ask a question due to health and safety policies in the theatre.

1456
01:12:30,260 --> 01:12:33,260
So please, if you have a question,

1457
01:12:33,260 --> 01:12:35,260
please raise your hand,

1458
01:12:35,260 --> 01:12:37,260
and I'm happy to take questions at this point.

1459
01:12:37,260 --> 01:12:40,260
So, John, perhaps I'll start with John.

1460
01:12:40,260 --> 01:12:42,260
I'll give you the provision.

1461
01:12:42,260 --> 01:12:44,260
There is a roving microphone.

1462
01:12:46,260 --> 01:12:49,260
And just declare who you are, John,

1463
01:12:49,260 --> 01:12:51,260
and perhaps stand up and just ask a question.

1464
01:12:51,260 --> 01:12:54,260
Interesting to begin an ethics talk with some discrimination, Nigel,

1465
01:12:54,260 --> 01:12:56,260
but I'm John Tysulis.

1466
01:12:56,260 --> 01:12:59,260
I'm the director of the Institute for Ethics in AI.

1467
01:12:59,260 --> 01:13:02,260
Thanks so much for a really fascinating and inspirational talk.

1468
01:13:03,260 --> 01:13:05,260
I guess I want to ask two questions.

1469
01:13:05,260 --> 01:13:09,260
One is a very general question about the nature of the project you're embarked on.

1470
01:13:09,260 --> 01:13:13,260
So the objective is to generate a powerful all-purpose tool

1471
01:13:13,260 --> 01:13:18,260
that will help create new scientific understanding.

1472
01:13:18,260 --> 01:13:23,260
And the nature of this tool is artificial general intelligence.

1473
01:13:23,260 --> 01:13:27,260
So that is a tool that can replicate or outperform human beings

1474
01:13:27,260 --> 01:13:30,260
across a wide range of cognitive tasks.

1475
01:13:31,260 --> 01:13:34,260
The worry is there attention there.

1476
01:13:34,260 --> 01:13:37,260
If you had something that could outperform human beings

1477
01:13:37,260 --> 01:13:40,260
across a wide range of cognitive tasks,

1478
01:13:40,260 --> 01:13:43,260
could we still regard that as a tool?

1479
01:13:43,260 --> 01:13:45,260
Or would it become a colleague?

1480
01:13:45,260 --> 01:13:47,260
So you talked about respecting AI at the end,

1481
01:13:47,260 --> 01:13:50,260
but it looks like something with that level of capacity

1482
01:13:50,260 --> 01:13:53,260
would demand a different form of respect

1483
01:13:53,260 --> 01:13:57,260
that would preclude the original objective of now treating it as a tool.

1484
01:13:57,260 --> 01:13:59,260
So that's one question.

1485
01:13:59,260 --> 01:14:03,260
The second question is, you've talked about what will benefit humanity.

1486
01:14:03,260 --> 01:14:06,260
And so I guess one question I have along these lines,

1487
01:14:06,260 --> 01:14:08,260
how do you make that determination?

1488
01:14:08,260 --> 01:14:13,260
So you might say, look, some people have the view that AI applied to military applications

1489
01:14:13,260 --> 01:14:16,260
will benefit humanity. Others don't.

1490
01:14:16,260 --> 01:14:18,260
How do you make that determination?

1491
01:14:18,260 --> 01:14:21,260
And I guess there's also this further dimension.

1492
01:14:21,260 --> 01:14:24,260
There's a division of labour in making that assessment.

1493
01:14:24,260 --> 01:14:30,260
Do you think too much has been placed on the shoulders of developers,

1494
01:14:30,260 --> 01:14:32,260
researchers, corporations,

1495
01:14:32,260 --> 01:14:37,260
and that really government should step in and resolve some of these issues?

1496
01:14:37,260 --> 01:14:39,260
Thanks, John. Great question.

1497
01:14:39,260 --> 01:14:44,260
So I think with your first question,

1498
01:14:44,260 --> 01:14:49,260
the reason human capabilities are an interesting mapping is because

1499
01:14:49,260 --> 01:14:52,260
the human brain is the only evidence of general intelligence we have

1500
01:14:52,260 --> 01:14:54,260
in the universe as far as we know.

1501
01:14:54,260 --> 01:14:58,260
So I think there's always the question is how do you know you've got there?

1502
01:14:58,260 --> 01:15:03,260
And you can approximate it with millions of tasks, potentially.

1503
01:15:03,260 --> 01:15:04,260
So that's one approach.

1504
01:15:04,260 --> 01:15:07,260
The more tasks you have in your grab bag and it can do all of them

1505
01:15:07,260 --> 01:15:10,260
and pair against human performance, you might have done it.

1506
01:15:10,260 --> 01:15:13,260
But there's always the possibility that one might have missed out

1507
01:15:13,260 --> 01:15:17,260
a particular type of cognitive ability, like creativity or something.

1508
01:15:17,260 --> 01:15:19,260
So that's why I think...

1509
01:15:19,260 --> 01:15:23,260
And also I think AI can be applied back to neuroscience as well, by the way.

1510
01:15:23,260 --> 01:15:26,260
That's one of our scientific areas that we apply AI to,

1511
01:15:26,260 --> 01:15:29,260
is neuroscience itself and better understanding our own minds.

1512
01:15:29,260 --> 01:15:33,260
So I have this view that as a neuroscientist

1513
01:15:33,260 --> 01:15:35,260
that this journey we're embarked on with AI

1514
01:15:35,260 --> 01:15:39,260
is the most fascinating journey one can ever take scientifically

1515
01:15:39,260 --> 01:15:41,260
because there's not only the artificial building,

1516
01:15:41,260 --> 01:15:44,260
it's then comparing that to the human mind

1517
01:15:44,260 --> 01:15:47,260
and then seeing, I think, uncovering the mysteries of our own minds,

1518
01:15:47,260 --> 01:15:49,260
what's dreaming, what is creativity, what are emotions,

1519
01:15:49,260 --> 01:15:52,260
all of these questions that we have, free will,

1520
01:15:52,260 --> 01:15:55,260
potentially even consciousness, the big questions.

1521
01:15:55,260 --> 01:15:59,260
I think building AI and intelligent artefacts

1522
01:15:59,260 --> 01:16:02,260
and then seeing what is missing in them

1523
01:16:02,260 --> 01:16:05,260
is a good way to explore that scientifically.

1524
01:16:05,260 --> 01:16:07,260
And so then, I don't know the answer to your question,

1525
01:16:07,260 --> 01:16:09,260
I think that's part of this journey,

1526
01:16:09,260 --> 01:16:12,260
is at what point would these things not become just tools.

1527
01:16:12,260 --> 01:16:15,260
And it may even be that it's a design question

1528
01:16:15,260 --> 01:16:19,260
because whether we should build what is consciousness we don't know

1529
01:16:19,260 --> 01:16:22,260
and that would be a whole, obviously, debate in itself,

1530
01:16:22,260 --> 01:16:24,260
but should we build it to the extent of what it is,

1531
01:16:24,260 --> 01:16:26,260
should we build them in our systems?

1532
01:16:26,260 --> 01:16:29,260
I would say no to begin with if we have that choice

1533
01:16:29,260 --> 01:16:31,260
until we better understand them as tools

1534
01:16:31,260 --> 01:16:33,260
and then we can bring in that extra complexity of free will

1535
01:16:33,260 --> 01:16:36,260
and where do they get their goals from?

1536
01:16:36,260 --> 01:16:38,260
Initially it will be designers,

1537
01:16:38,260 --> 01:16:40,260
but if they could be self-generated.

1538
01:16:40,260 --> 01:16:42,260
So I think we're still a long way away from those things,

1539
01:16:42,260 --> 01:16:45,260
but that's one of the things I think we should inch towards

1540
01:16:45,260 --> 01:16:48,260
very cautiously and with precautions

1541
01:16:48,260 --> 01:16:52,260
because also it will get to the heart of what it means to be human.

1542
01:16:52,260 --> 01:16:55,260
And I think that should exactly be done multidisciplinary

1543
01:16:55,260 --> 01:17:00,260
with philosophers and ethicists and theologians

1544
01:17:00,260 --> 01:17:02,260
and the wider humanities.

1545
01:17:02,260 --> 01:17:05,260
I think this is where the humanities comes in,

1546
01:17:05,260 --> 01:17:07,260
as well as the science.

1547
01:17:07,260 --> 01:17:12,260
So I think that's all to come.

1548
01:17:12,260 --> 01:17:14,260
OK, a question in the front row?

1549
01:17:22,260 --> 01:17:24,260
Thank you so much for a great presentation.

1550
01:17:24,260 --> 01:17:26,260
Carina Prunkel, I'm a research fellow at the Institute.

1551
01:17:26,260 --> 01:17:31,260
So you mentioned at various points the potential for dual use

1552
01:17:31,260 --> 01:17:33,260
and in particular malicious dual use.

1553
01:17:33,260 --> 01:17:38,260
So I'm curious to hear how you approach this topic at DeepMind.

1554
01:17:38,260 --> 01:17:42,260
So what precautions or how do you address the potential for dual use?

1555
01:17:42,260 --> 01:17:44,260
Great.

1556
01:17:44,260 --> 01:17:47,260
So we have a lot of different mechanisms now at DeepMind

1557
01:17:47,260 --> 01:17:49,260
that have been built up over time.

1558
01:17:49,260 --> 01:17:52,260
So one is the Institutional Review Committee we have,

1559
01:17:52,260 --> 01:17:56,260
which is formed, so it's chaired by Laila Ibrahim, our COO,

1560
01:17:56,260 --> 01:18:01,260
and it's formed with different people from across the company.

1561
01:18:01,260 --> 01:18:04,260
We have legal, we have ethicists and philosophers as well.

1562
01:18:04,260 --> 01:18:07,260
It's also rotating boards, some senior researchers,

1563
01:18:07,260 --> 01:18:09,260
and they get involved early with research projects

1564
01:18:09,260 --> 01:18:12,260
and try to assess them from all aspects,

1565
01:18:12,260 --> 01:18:15,260
and they will draw on outside experts.

1566
01:18:15,260 --> 01:18:17,260
So they bring in biologists, for example,

1567
01:18:17,260 --> 01:18:19,260
for alpha-fold biothesists,

1568
01:18:19,260 --> 01:18:21,260
so things we might not have in-house.

1569
01:18:21,260 --> 01:18:26,260
And then they work with the research teams to either say no,

1570
01:18:26,260 --> 01:18:29,260
that project should not proceed, OK, it can with caveats,

1571
01:18:29,260 --> 01:18:31,260
or why don't you build or do it in a different way

1572
01:18:31,260 --> 01:18:33,260
with these safeguards?

1573
01:18:33,260 --> 01:18:36,260
So that's our prototype, I would say, committee

1574
01:18:36,260 --> 01:18:39,260
that does these things, and we're kind of exercising our muscle

1575
01:18:39,260 --> 01:18:42,260
when the stakes are relatively low currently

1576
01:18:42,260 --> 01:18:45,260
so that we can learn from what works and is effective

1577
01:18:45,260 --> 01:18:49,260
as we get more powerful systems.

1578
01:18:49,260 --> 01:18:51,260
And obviously over time, I think at some point

1579
01:18:51,260 --> 01:18:55,260
there have got to be outside bodies that get involved.

1580
01:18:55,260 --> 01:18:59,260
But the problem is that, and we've experimented with that too,

1581
01:18:59,260 --> 01:19:01,260
is that a lot of these things are very specific

1582
01:19:01,260 --> 01:19:03,260
to the technology itself.

1583
01:19:03,260 --> 01:19:07,260
So one has to sort of understand the technology to a deep level,

1584
01:19:07,260 --> 01:19:09,260
maybe even have access to it somehow,

1585
01:19:09,260 --> 01:19:12,260
but in a controlled way, because one can't just...

1586
01:19:12,260 --> 01:19:14,260
Open sourcing is not just a panacea either,

1587
01:19:14,260 --> 01:19:16,260
because if it's a dangerous system, open sourcing

1588
01:19:16,260 --> 01:19:19,260
it means any bad actor can use it too, for anything.

1589
01:19:19,260 --> 01:19:21,260
So there's a lot of complicated, I think,

1590
01:19:21,260 --> 01:19:23,260
ethical questions around this.

1591
01:19:23,260 --> 01:19:25,260
But I don't think there's an easy answer.

1592
01:19:25,260 --> 01:19:28,260
So anyone who thinks there is one, I think, is kidding themselves.

1593
01:19:28,260 --> 01:19:31,260
I hope everyone realises the complexity involved.

1594
01:19:31,260 --> 01:19:33,260
But I think it's pretty...

1595
01:19:33,260 --> 01:19:35,260
I'm very happy with our internal system,

1596
01:19:35,260 --> 01:19:38,260
but I appreciate more is going to be needed than that

1597
01:19:38,260 --> 01:19:41,260
as the systems get more powerful and impact more of the world.

1598
01:19:41,260 --> 01:19:43,260
A question just behind you, I think,

1599
01:19:43,260 --> 01:19:46,260
if you just pass the microphone literally behind you.

1600
01:19:46,260 --> 01:19:48,260
Hi, my name is Ulrich.

1601
01:19:48,260 --> 01:19:50,260
I'm a postdoc at the Computer Science Department

1602
01:19:50,260 --> 01:19:52,260
in the Human Central Computing Group.

1603
01:19:52,260 --> 01:19:54,260
So DeepMind looks like it's this great example

1604
01:19:54,260 --> 01:19:57,260
of how you can take the best from science

1605
01:19:57,260 --> 01:20:00,260
and then sort of bring it together with a commercial company

1606
01:20:00,260 --> 01:20:02,260
and then make very rapid progress.

1607
01:20:02,260 --> 01:20:04,260
And you mentioned in the end here how you thought

1608
01:20:04,260 --> 01:20:06,260
that the scientific process should sort of inspire

1609
01:20:06,260 --> 01:20:09,260
the commercial world, as it were.

1610
01:20:09,260 --> 01:20:12,260
I'm curious about what you think about the other way around.

1611
01:20:12,260 --> 01:20:16,260
So what have you learnt by being embedded in Google

1612
01:20:16,260 --> 01:20:20,260
that you think we as researchers should learn from

1613
01:20:20,260 --> 01:20:22,260
in order to make more rapid progress?

1614
01:20:22,260 --> 01:20:24,260
Yeah, you're absolutely right.

1615
01:20:24,260 --> 01:20:27,260
That was the thinking, the original vision behind the...

1616
01:20:27,260 --> 01:20:30,260
So I spoke about the original vision of the company,

1617
01:20:30,260 --> 01:20:32,260
this Apollo programme,

1618
01:20:32,260 --> 01:20:36,260
but the original vision behind the organisational setup

1619
01:20:36,260 --> 01:20:38,260
and processes was to be a hybrid

1620
01:20:38,260 --> 01:20:40,260
like the best of both worlds.

1621
01:20:40,260 --> 01:20:43,260
Startups and the energy and creativity

1622
01:20:43,260 --> 01:20:46,260
and pace that they have and nimblness

1623
01:20:46,260 --> 01:20:48,260
and the best from academic research,

1624
01:20:48,260 --> 01:20:50,260
the blue sky thinking, ambitious thinking

1625
01:20:50,260 --> 01:20:53,260
that happens there, but sometimes with a lot of bureaucracy.

1626
01:20:53,260 --> 01:20:56,260
So I think that we did actually successfully

1627
01:20:56,260 --> 01:20:58,260
combine those two things.

1628
01:20:58,260 --> 01:21:01,260
And then when we agreed to get acquired,

1629
01:21:01,260 --> 01:21:03,260
we combined it with the third thing,

1630
01:21:03,260 --> 01:21:06,260
which is scale and resources of a large,

1631
01:21:06,260 --> 01:21:08,260
very successful company like Google.

1632
01:21:08,260 --> 01:21:10,260
And I think that's the main lesson,

1633
01:21:10,260 --> 01:21:13,260
is to make sure you do things at huge impact

1634
01:21:13,260 --> 01:21:17,260
and have ambition and realise that you can scale things

1635
01:21:17,260 --> 01:21:19,260
to that and the consequence that come with that,

1636
01:21:19,260 --> 01:21:21,260
but also the potential of that.

1637
01:21:21,260 --> 01:21:24,260
So I think we've done that now and very well,

1638
01:21:24,260 --> 01:21:27,260
like Mary or three of those aspects together.

1639
01:21:27,260 --> 01:21:29,260
It's a daily challenge because as we get bigger,

1640
01:21:29,260 --> 01:21:31,260
one tends to get slower as an organisation.

1641
01:21:31,260 --> 01:21:35,260
So we have to fight against that all the time.

1642
01:21:35,260 --> 01:21:37,260
But it's pretty unique, I would say,

1643
01:21:37,260 --> 01:21:40,260
the organisational and cultural feel of the mind.

1644
01:21:40,260 --> 01:21:42,260
But it could be a blueprint for other,

1645
01:21:42,260 --> 01:21:46,260
I would say, grand projects could be organised in a similar way.

1646
01:21:46,260 --> 01:21:48,260
OK, I'm going to just switch to this side

1647
01:21:48,260 --> 01:21:50,260
and then I'm going to go to the question there

1648
01:21:50,260 --> 01:21:52,260
and the question about that.

1649
01:21:59,260 --> 01:22:01,260
So move fast and break things with a quote

1650
01:22:01,260 --> 01:22:04,260
from people who built a social network.

1651
01:22:04,260 --> 01:22:07,260
If the mind was to build a social network

1652
01:22:07,260 --> 01:22:11,260
using the deep-wined way of doing things,

1653
01:22:11,260 --> 01:22:15,260
then what metrics would you use,

1654
01:22:15,260 --> 01:22:19,260
would you optimise to judge the quality of your social network?

1655
01:22:19,260 --> 01:22:22,260
And the second question that comes with it is,

1656
01:22:22,260 --> 01:22:25,260
do you in fact have a moral obligation

1657
01:22:25,260 --> 01:22:27,260
to build that social network?

1658
01:22:29,260 --> 01:22:31,260
OK, so thanks Tim,

1659
01:22:31,260 --> 01:22:33,260
two complicated questions there.

1660
01:22:33,260 --> 01:22:36,260
It's actually just generally,

1661
01:22:36,260 --> 01:22:39,260
so let's see, I have to be careful what I say,

1662
01:22:39,260 --> 01:22:43,260
but I think social networks have never really been my thing,

1663
01:22:43,260 --> 01:22:46,260
but also I haven't really thought a lot about it

1664
01:22:46,260 --> 01:22:48,260
relative to scientific advances

1665
01:22:48,260 --> 01:22:51,260
and the sorts of things that are my personal passion.

1666
01:22:51,260 --> 01:22:54,260
I would question actually the premise of your question,

1667
01:22:54,260 --> 01:22:58,260
which is that how much value does weak ties like that give,

1668
01:22:58,260 --> 01:23:01,260
like superficial connections like that,

1669
01:23:01,260 --> 01:23:03,260
versus deeper ties that you get in real life

1670
01:23:03,260 --> 01:23:05,260
with your real family and friends?

1671
01:23:05,260 --> 01:23:08,260
I think it's an interesting thing to understand.

1672
01:23:08,260 --> 01:23:11,260
Are we sacrificing deeper, more meaningful moments

1673
01:23:11,260 --> 01:23:14,260
for hundreds of more superficial moments?

1674
01:23:14,260 --> 01:23:17,260
It's not entirely clear to me that the metric of,

1675
01:23:17,260 --> 01:23:19,260
and it sounds seductive, connect the world,

1676
01:23:19,260 --> 01:23:21,260
like why would that be bad?

1677
01:23:21,260 --> 01:23:23,260
But this is the thing I'm talking about with the scientific method,

1678
01:23:23,260 --> 01:23:26,260
is to try and think through the full consequences of what that would mean.

1679
01:23:26,260 --> 01:23:29,260
Echo chambers, manipulation, all the rest of it

1680
01:23:29,260 --> 01:23:32,260
that we all know very well don't need to go into.

1681
01:23:32,260 --> 01:23:34,260
So I think if I was to do something like that,

1682
01:23:34,260 --> 01:23:37,260
I would use the scientific method again

1683
01:23:37,260 --> 01:23:40,260
to try and really think through ahead of time

1684
01:23:40,260 --> 01:23:44,260
what do you want as the outcomes and metrics?

1685
01:23:44,260 --> 01:23:47,260
In fact, often trying to find the right metrics

1686
01:23:47,260 --> 01:23:50,260
that actually drive the right behaviour that you think is good in the world

1687
01:23:50,260 --> 01:23:51,260
is half the challenge.

1688
01:23:51,260 --> 01:23:53,260
It's like asking the right question in science.

1689
01:23:53,260 --> 01:23:56,260
Everybody who does science knows that asking the question is the hardest thing.

1690
01:23:56,260 --> 01:23:58,260
What is the right question?

1691
01:23:58,260 --> 01:24:00,260
And it's especially hard.

1692
01:24:00,260 --> 01:24:01,260
Oh, you want an answer?

1693
01:24:01,260 --> 01:24:04,260
Well, I wouldn't want to give you an answer on the spot,

1694
01:24:04,260 --> 01:24:06,260
but we can talk about it over dinner.

1695
01:24:06,260 --> 01:24:10,260
At least one should attempt to start with serious thinking

1696
01:24:10,260 --> 01:24:12,260
about the question first, right?

1697
01:24:12,260 --> 01:24:13,260
That's the first part.

1698
01:24:13,260 --> 01:24:16,260
I don't know what the answer is because I've not given it enough thought.

1699
01:24:16,260 --> 01:24:19,260
But one should at least understand the meta level of like

1700
01:24:19,260 --> 01:24:21,260
that's how one should start,

1701
01:24:21,260 --> 01:24:24,260
including whether one should do that thing at all, potentially.

1702
01:24:24,260 --> 01:24:27,260
It could be the answer of that hypothesis generation.

1703
01:24:27,260 --> 01:24:28,260
Okay.

1704
01:24:28,260 --> 01:24:31,260
I'm going to try and get three more questions in.

1705
01:24:31,260 --> 01:24:32,260
We're right up against the clock.

1706
01:24:32,260 --> 01:24:33,260
We've got about seven and a half minutes.

1707
01:24:33,260 --> 01:24:35,260
There's a question here from Helen.

1708
01:24:39,260 --> 01:24:40,260
Thank you.

1709
01:24:40,260 --> 01:24:42,260
I'm Ellen Landomer from Yale University

1710
01:24:42,260 --> 01:24:44,260
and visiting fellow at the Research Centre for Ethics in AI.

1711
01:24:44,260 --> 01:24:46,260
Thank you for a brilliant talk.

1712
01:24:46,260 --> 01:24:51,260
So you showed us how AI can help us figure out the truth of the universe.

1713
01:24:51,260 --> 01:24:52,260
Pretty much.

1714
01:24:52,260 --> 01:24:54,260
How about the moral world?

1715
01:24:54,260 --> 01:24:56,260
How about the political universe?

1716
01:24:56,260 --> 01:24:58,260
Philosophy starts with Plato's Republic,

1717
01:24:58,260 --> 01:25:00,260
which is an attempt to figure out the best constitution.

1718
01:25:00,260 --> 01:25:03,260
Surely, unless one is a complete moral relativist,

1719
01:25:03,260 --> 01:25:07,260
there are some invariants we're trying to figure out about the moral world.

1720
01:25:07,260 --> 01:25:09,260
Could AI help us map that out?

1721
01:25:09,260 --> 01:25:12,260
Could it figure out like the best social organisation,

1722
01:25:12,260 --> 01:25:14,260
you know, boring from, I don't know,

1723
01:25:14,260 --> 01:25:20,260
all the things we've tried, capitalism, socialism, libertarianism, egalitarianism?

1724
01:25:20,260 --> 01:25:22,260
Would it help expand our imagination

1725
01:25:22,260 --> 01:25:24,260
and perhaps assuming you have an objective function

1726
01:25:24,260 --> 01:25:28,260
like satisfying major Italian preferences

1727
01:25:28,260 --> 01:25:32,260
subject to constraints to protect minority rights or something like that?

1728
01:25:32,260 --> 01:25:33,260
What do you see in the future?

1729
01:25:33,260 --> 01:25:36,260
We took through 2,000 years and we haven't made much progress.

1730
01:25:38,260 --> 01:25:39,260
Good question.

1731
01:25:39,260 --> 01:25:43,260
I mean, look, I think the morality and political science

1732
01:25:43,260 --> 01:25:47,260
I think is one of the hardest things that AI,

1733
01:25:47,260 --> 01:25:49,260
you know, I think it can contribute in some way,

1734
01:25:49,260 --> 01:25:52,260
but I would say it's far harder than the physical sciences,

1735
01:25:52,260 --> 01:25:53,260
right, or the life sciences,

1736
01:25:53,260 --> 01:25:56,260
because the most complex things in the world are humans,

1737
01:25:56,260 --> 01:26:00,260
for human beings to understand and to model

1738
01:26:00,260 --> 01:26:04,260
and to understand people's motivations, especially in aggregate.

1739
01:26:04,260 --> 01:26:08,260
I think one way it could help is there's also the question of

1740
01:26:08,260 --> 01:26:13,260
even if an theoretical AI could come up with a better political construct,

1741
01:26:13,260 --> 01:26:18,260
would humans, beings and society accept that or even care or understand it?

1742
01:26:18,260 --> 01:26:22,260
So there's all those questions to try and and would it be implemented correctly?

1743
01:26:22,260 --> 01:26:24,260
Obviously there's obviously implementation problems.

1744
01:26:24,260 --> 01:26:27,260
I think more interesting maybe would be,

1745
01:26:27,260 --> 01:26:29,260
and I've talked to economists about this is,

1746
01:26:29,260 --> 01:26:32,260
and we did quite a lot of research on multi-agent systems.

1747
01:26:32,260 --> 01:26:37,260
So again, having a little sandbox or simulation of millions of agents

1748
01:26:37,260 --> 01:26:41,260
with interacting with each other, with motivations and some goals seeking things.

1749
01:26:41,260 --> 01:26:45,260
And I think we're missing that experimental testbed actually

1750
01:26:45,260 --> 01:26:47,260
from political science and economics quite a lot,

1751
01:26:47,260 --> 01:26:50,260
because again, economics is one of those things where

1752
01:26:50,260 --> 01:26:52,260
and political science where you sort of have to test it live,

1753
01:26:52,260 --> 01:26:53,260
a, b, test it in the world.

1754
01:26:53,260 --> 01:26:56,260
It's like, are we going to go for this political system or not?

1755
01:26:56,260 --> 01:26:58,260
Should we raise inflation or not?

1756
01:26:58,260 --> 01:27:00,260
Well, you've got models, but then you actually just have to do it

1757
01:27:00,260 --> 01:27:03,260
and then see, oh, it's caused a recession or something,

1758
01:27:03,260 --> 01:27:05,260
where maybe we shouldn't do that next time.

1759
01:27:05,260 --> 01:27:08,260
And so it would be better if I think if we had a simulation

1760
01:27:08,260 --> 01:27:11,260
or a sandbox perhaps populated with AI systems

1761
01:27:11,260 --> 01:27:15,260
that are approximates to idealise forms of humans

1762
01:27:15,260 --> 01:27:19,260
and then we can maybe make some interesting,

1763
01:27:19,260 --> 01:27:23,260
experimental work in that much lower stakes.

1764
01:27:23,260 --> 01:27:26,260
So I think that could be really fascinating exploration area

1765
01:27:26,260 --> 01:27:31,260
for things like market dynamics and setting the environmental settings

1766
01:27:31,260 --> 01:27:34,260
to create more cooperation or something.

1767
01:27:34,260 --> 01:27:38,260
I would be, far as economists, I would be trying to use all those things.

1768
01:27:38,260 --> 01:27:42,260
I used to be fascinated when I was a kid with Santa Fe Institute

1769
01:27:42,260 --> 01:27:46,260
and they used to do lots of really cool models of agent-based systems

1770
01:27:46,260 --> 01:27:48,260
in little grid worlds.

1771
01:27:48,260 --> 01:27:52,260
And I loved going artificial societies, I think, by Axelrod.

1772
01:27:52,260 --> 01:27:55,260
I loved those kind of work, actually what I used to dream about,

1773
01:27:55,260 --> 01:27:58,260
going to Santa Fe to work on something like that.

1774
01:27:58,260 --> 01:28:03,260
And I still think that would be pretty cool to have some sort of system like that.

1775
01:28:03,260 --> 01:28:05,260
Let's see if we can squeeze just a few more.

1776
01:28:05,260 --> 01:28:09,260
There's a question chap in the, who caught my eye there, yes?

1777
01:28:11,260 --> 01:28:13,260
Just very, and try and squeeze them in,

1778
01:28:13,260 --> 01:28:15,260
because there's two more questions over here

1779
01:28:16,260 --> 01:28:18,260
Super, yeah, I just have a quick question to be honest.

1780
01:28:18,260 --> 01:28:24,260
So I think at the end you mentioned creating AI in the image of scientific method.

1781
01:28:24,260 --> 01:28:29,260
And the title of your lecture is advancement of science through AI.

1782
01:28:29,260 --> 01:28:33,260
But in what sense do you think that neural networks

1783
01:28:33,260 --> 01:28:36,260
or the limited understanding I have of AI is,

1784
01:28:36,260 --> 01:28:40,260
in what sense do they follow the notion of scientific method we have?

1785
01:28:40,260 --> 01:28:45,260
Is there any sense of talking about hypothesis and then testing?

1786
01:28:45,260 --> 01:28:48,260
Because it doesn't seem that neural networks work in that way.

1787
01:28:48,260 --> 01:28:51,260
They're opaque for most practical purposes.

1788
01:28:51,260 --> 01:28:56,260
And if they do outperform us, should we just get rid of the scientific method?

1789
01:28:56,260 --> 01:29:01,260
So by the way, it's not in the image of the scientific method just to be clear.

1790
01:29:01,260 --> 01:29:04,260
It's using the approach of the scientific method.

1791
01:29:04,260 --> 01:29:06,260
I'm not sure what image in the scientific method means.

1792
01:29:07,260 --> 01:29:13,260
And yes, today that is true that a lot of the systems we have are kind of black box-like.

1793
01:29:13,260 --> 01:29:16,260
But I think that's exactly what we should be doing more work on,

1794
01:29:16,260 --> 01:29:18,260
is making them less opaque.

1795
01:29:18,260 --> 01:29:20,260
There's no reason why they should be.

1796
01:29:20,260 --> 01:29:22,260
The way I say it to my neuroscience team is,

1797
01:29:22,260 --> 01:29:25,260
look, we understand quite a lot about the brain now, the ultimate black boxes.

1798
01:29:25,260 --> 01:29:29,260
We have MRI machines and amazing tools and single cell recording.

1799
01:29:29,260 --> 01:29:30,260
So it's amazing.

1800
01:29:30,260 --> 01:29:32,260
And that's why I got into neuroscience in the mid 2000s.

1801
01:29:32,260 --> 01:29:36,260
So we can actually look into, we don't have to do philosophy of mind necessarily,

1802
01:29:36,260 --> 01:29:38,260
although we should know about that.

1803
01:29:38,260 --> 01:29:42,260
But we can actually empirically look at this, not just do introspection.

1804
01:29:42,260 --> 01:29:47,260
And so as a minimum, in the field of artificial minds,

1805
01:29:47,260 --> 01:29:52,260
we should know as much about them as we do with the real brain.

1806
01:29:52,260 --> 01:29:54,260
And we don't know everything about the real brain.

1807
01:29:54,260 --> 01:29:56,260
Obviously there's tons still we don't know.

1808
01:29:56,260 --> 01:30:00,260
But there's a lot more that we do know than we do about these artificial systems.

1809
01:30:00,260 --> 01:30:02,260
And it should be the other way around.

1810
01:30:02,260 --> 01:30:06,260
That should be the minimum we understand because we have access to every neuron,

1811
01:30:06,260 --> 01:30:09,260
you know, neuron, artificial neuron in the artificial brain.

1812
01:30:09,260 --> 01:30:12,260
And we can completely control the experimental conditions.

1813
01:30:12,260 --> 01:30:15,260
So as a minimum, so I sometimes say this is a challenge to the team.

1814
01:30:15,260 --> 01:30:19,260
What's the equivalent of fMRI for a neural network?

1815
01:30:19,260 --> 01:30:21,260
What's the equivalent of single cell recording?

1816
01:30:21,260 --> 01:30:22,260
We do ablation studies.

1817
01:30:22,260 --> 01:30:24,260
So we have a whole neuroscience team that's thinking about this

1818
01:30:24,260 --> 01:30:29,260
and bringing neuroscience techniques, analysis techniques over to AI.

1819
01:30:29,260 --> 01:30:34,260
Now, in the defence of the engineers, one of the reasons that this has happened

1820
01:30:34,260 --> 01:30:39,260
is because the brain is obviously a static system we're all fascinated by, of course.

1821
01:30:39,260 --> 01:30:42,260
But artificial systems change over time.

1822
01:30:42,260 --> 01:30:45,260
Like AlphaGo is now in ancient history of AI, right?

1823
01:30:45,260 --> 01:30:47,260
Although it was very meaningful over time.

1824
01:30:47,260 --> 01:30:50,260
And it takes years to study a system, right?

1825
01:30:50,260 --> 01:30:52,260
It takes years to build it, and then it takes years to study it.

1826
01:30:52,260 --> 01:30:55,260
So should you use that research time on studying a system

1827
01:30:55,260 --> 01:30:59,260
that itself will be out of date by the time you come to any conclusions about it?

1828
01:30:59,260 --> 01:31:02,260
So I think only now are we reaching the point where we have systems

1829
01:31:02,260 --> 01:31:05,260
that are interesting enough, do enough interesting things in the world,

1830
01:31:05,260 --> 01:31:09,260
like large models and AlphaFold type things,

1831
01:31:09,260 --> 01:31:13,260
that probably it's worth spending the research time on that.

1832
01:31:13,260 --> 01:31:17,260
And so I think over the next decade we're going to see a lot more understanding

1833
01:31:17,260 --> 01:31:18,260
of what these systems do.

1834
01:31:18,260 --> 01:31:21,260
I don't think there's some weird reason why that can't happen.

1835
01:31:21,260 --> 01:31:23,260
Okay, there are so many more questions.

1836
01:31:23,260 --> 01:31:25,260
I am literally in the red now.

1837
01:31:25,260 --> 01:31:28,260
I'm going to have to call this to a close.

1838
01:31:28,260 --> 01:31:29,260
I do apologise.

1839
01:31:29,260 --> 01:31:33,260
There is so much pent up, I think, interest and questions for you, Demis.

1840
01:31:33,260 --> 01:31:37,260
All I can say at this point is absolutely a wonderful lecture

1841
01:31:37,260 --> 01:31:39,260
where five minutes later than we should have been.

1842
01:31:39,260 --> 01:31:42,260
The Sheldanian was to a strict regime when it comes to timekeeping.

1843
01:31:42,260 --> 01:31:45,260
You gave us the most fascinating insights

1844
01:31:45,260 --> 01:31:49,260
and you have given, I think, to the world with your company

1845
01:31:49,260 --> 01:31:53,260
and your own talents, a quite wonderful vision of a future

1846
01:31:53,260 --> 01:31:58,260
in which AI can help us flourish, empower us and not oppress us.

1847
01:31:58,260 --> 01:32:00,260
So thank you very much.

1848
01:32:19,260 --> 01:32:21,260
Thank you.

