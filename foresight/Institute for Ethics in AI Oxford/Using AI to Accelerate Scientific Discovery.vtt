WEBVTT

00:00.000 --> 00:05.240
â€¦uchos MARH

00:08.140 --> 00:12.140
welcome to this evening's lecture

00:12.140 --> 00:15.660
in the splendor of the Sheldonian theatre.

00:15.880 --> 00:19.820
Hollol teis endseth at Oxford UK University,

00:19.960 --> 00:23.440
part of the OpenSeed Challenge lectures

00:23.460 --> 00:26.660
on artificial intelligence and human values.

00:26.660 --> 00:30.060
My name is Nigel Shadbolt, principal of Jesus College.

00:30.060 --> 00:35.460
I'm also a professor of computer science here in Oxford and chair the Institute Steering Group.

00:35.460 --> 00:40.260
It was my privilege to help set up the Institute which brings together world-leading philosophers

00:40.260 --> 00:47.260
and other experts in the humanities with the researchers, developers and users of AI.

00:47.260 --> 00:50.660
The director of the Institute is Professor John Tosulis,

00:50.660 --> 00:55.060
and its ultimate home will be the Stephen A. Schwarzman Centre for the Humanities,

00:55.060 --> 00:58.860
whose construction is soon to start.

00:58.860 --> 01:02.860
In recent years, AI has gone from strength to strength.

01:02.860 --> 01:05.060
It's now ubiquitous.

01:05.060 --> 01:09.660
In our phones, the games we play, in our cars, our drug discovery companies,

01:09.660 --> 01:14.260
the search engines we use and the translation tools we depend on.

01:14.260 --> 01:18.260
Much of that is down to a new generation of AI methods and techniques

01:18.260 --> 01:22.260
that are powered by modern machine learning algorithms,

01:22.260 --> 01:28.060
great swathes of data and the prodigious power of modern-day computing hardware.

01:28.060 --> 01:31.060
Some of AI's most dramatic recent accomplishments

01:31.060 --> 01:37.060
owe a great deal to our speaker here with us this evening and the company he co-founded.

01:37.060 --> 01:40.460
Demis Arsabes, CEO and co-founder of DeepMind,

01:40.460 --> 01:44.060
one of the world's leading AI research companies.

01:44.060 --> 01:47.860
Demis' own career and intellectual journey is an extraordinary one.

01:47.860 --> 01:51.660
A chess prodigy, hugely successful computer games developer,

01:51.660 --> 01:55.260
with a double first in computer science from Cambridge.

01:55.260 --> 01:58.860
Demis has always been fascinated by the human brain,

01:58.860 --> 02:02.260
understanding how it gives rise to intelligence.

02:02.260 --> 02:07.660
After the success of his games company, he went on to a PhD in cognitive neuroscience at UCL,

02:07.660 --> 02:10.660
followed by a Henry Welcombe Postdoctoral Research Fellowship

02:10.660 --> 02:15.660
at the Gatsby Computational Neuroscience Unit, also at UCL.

02:15.660 --> 02:19.860
His papers in cognitive neuroscience investigated imagination,

02:19.860 --> 02:25.660
memory and amnesia and appeared in leading journals such as Nature and Science.

02:25.660 --> 02:28.460
He combined his interest in computing and neuroscience

02:28.460 --> 02:31.660
with the formation of DeepMind in 2010.

02:31.660 --> 02:35.260
It's compelling ambition to solve intelligence

02:35.260 --> 02:39.060
and then use intelligence to solve everything else.

02:39.060 --> 02:43.660
He and his team used games as the context in which to test new ideas

02:43.660 --> 02:49.260
about how to build AI systems using machine learning methods inspired by neuroscience.

02:49.340 --> 02:53.060
First arcade games and then famously Go.

02:53.060 --> 02:57.060
A previous talk here in the Sheldonian in February 2016

02:57.060 --> 03:04.260
prefigured AlphaGo winning 4-1 against former world champion Lee Sodol just a month later.

03:04.260 --> 03:09.860
Games have proven to be a great training ground for developing and testing AI algorithms,

03:09.860 --> 03:13.860
but the aim of DeepMind has always been to build general learning systems

03:13.860 --> 03:17.860
ultimately capable of solving important problems in the real world.

03:17.860 --> 03:21.860
DeepMind's AlphaFold system is a solution to the 50-year grand challenge

03:21.860 --> 03:26.260
of protein structure prediction, culminating the release of the most accurate

03:26.260 --> 03:29.460
and complete picture of the human proteome.

03:29.460 --> 03:34.260
A core aim for the Institute for Ethics at AI is to bring together world-leading academics

03:34.260 --> 03:37.260
and the practitioners at the cutting edge of AI development.

03:37.260 --> 03:42.660
Tonight we will hear first hand experience of AI's enormous potential

03:42.660 --> 03:45.260
to accelerate scientific discovery.

03:45.260 --> 03:50.660
Experience which will inform our research and thinking about the critical ethical considerations

03:50.660 --> 03:56.060
that must be considered by policy makers and technical developers of AI.

03:56.060 --> 04:01.860
Demis has predicted that artificial intelligence will be one of the most beneficial technologies ever

04:01.860 --> 04:04.860
but that significant ethical issues remain.

04:04.860 --> 04:09.860
Please join me in welcoming Demis Hasabas to deliver tonight's Tana lecture

04:09.860 --> 04:12.860
using AI to accelerate scientific discovery.

04:15.260 --> 04:29.260
Thank you. Thank you, Senaigel, for such a great introduction.

04:29.260 --> 04:33.260
It's a real pleasure to be back here in Oxford in the Shadonian

04:33.260 --> 04:36.260
and giving the Tana lecture. It's a real honour.

04:36.260 --> 04:41.260
What I'm going to talk about today is using AI to accelerate scientific discovery.

04:41.260 --> 04:45.260
As you'll see throughout my talk, this was my original motivation

04:45.260 --> 04:49.260
and has always been my motivation behind spending my entire career

04:49.260 --> 04:52.260
and trying to make AI a reality.

04:52.260 --> 04:55.260
I'm going to talk a lot about some of our most recent advances

04:55.260 --> 04:59.260
actually now coming to fruition, especially the last year or two,

04:59.260 --> 05:03.260
of using AI to crack difficult scientific problems.

05:03.260 --> 05:06.260
I'm also going to talk about the lead up to there

05:06.260 --> 05:10.260
and how I think about the games work we did originally and the foundation work we did originally.

05:10.260 --> 05:15.260
Last time I talked here was just before the AlphaGo match in Korea.

05:15.260 --> 05:19.260
That was a major moment for us and how in the last five or six years

05:19.260 --> 05:23.260
things have progressed enormously.

05:23.260 --> 05:29.260
Just to talk a little bit about what our vision was behind DeepMind back in 2010.

05:29.260 --> 05:32.260
It's quite hard to remember the state of AI back in 2010

05:32.260 --> 05:36.260
because today, as Nigel was saying, AI is ubiquitous all around us.

05:36.260 --> 05:39.260
It's one of the biggest buzzwords in industry.

05:39.260 --> 05:43.260
It's hard to remember just 12 years ago

05:43.260 --> 05:46.260
almost nobody was talking about AI, I would say,

05:46.260 --> 05:51.260
and it was almost impossible to actually get funding in the private sector for AI at all.

05:51.260 --> 05:54.260
We have many funny stories back in the day

05:54.260 --> 05:57.260
of trying to do some fundraising back in 2009 and 2010

05:57.260 --> 06:00.260
and most people thinking we were completely mad

06:00.260 --> 06:03.260
to be embarking on this journey.

06:03.260 --> 06:07.260
We founded it with this in mind of trying to build one day

06:07.260 --> 06:11.260
a hollow programme-like effort to build AGI,

06:11.260 --> 06:13.260
artificial general intelligence.

06:13.260 --> 06:16.260
We use this term artificial general intelligence

06:16.260 --> 06:19.260
to distinguish it from normal everyday AI

06:19.260 --> 06:22.260
where we're talking about a general system

06:22.260 --> 06:27.260
that can perform well on many tasks to at least human level.

06:27.260 --> 06:31.260
That's the general aspect that we are always striving for

06:31.260 --> 06:33.260
in all the work that we do.

06:33.260 --> 06:36.260
We're still on this mission now and I think we've done

06:36.260 --> 06:41.260
a pretty good job of staying true to this original vision

06:41.260 --> 06:44.260
that we had in 2010 when we were just a few people

06:44.260 --> 06:49.260
in a small little office in an attic in Russell Square.

06:49.260 --> 06:52.260
As Nigel said, our original mission statement

06:52.260 --> 06:54.260
was step one, solve intelligence,

06:54.260 --> 06:57.260
step two, use it to solve everything else.

06:57.260 --> 07:00.260
We have updated that mission statement a little bit,

07:00.260 --> 07:03.260
still means the same thing, but just to be a little bit more descriptive now

07:03.260 --> 07:05.260
in the last few years, just to be a bit clearer

07:05.260 --> 07:07.260
about what we mean by solving everything else,

07:07.260 --> 07:09.260
what exactly are we talking about.

07:09.260 --> 07:13.260
The way we discuss our mission now is solving intelligence

07:13.260 --> 07:17.260
to advance science and of course for the benefit of humanity.

07:17.260 --> 07:21.260
That's always been the cornerstone of what we think about

07:21.260 --> 07:24.260
when we think about what should we apply AI to.

07:26.260 --> 07:30.260
Now, there are two, broadly two ways that I think AI

07:30.260 --> 07:32.260
can be attempted to be built.

07:32.260 --> 07:37.260
One is the more traditional way of building logic systems

07:37.260 --> 07:41.260
or expert systems and these are hard coded systems

07:41.260 --> 07:44.260
that effectively teams of programmers solve the problem.

07:44.260 --> 07:46.260
They then incorporate those solutions

07:46.260 --> 07:49.260
in sometimes very clever expert systems,

07:49.260 --> 07:53.260
but the problem with them is that they are very limited

07:53.260 --> 07:55.260
in terms of what they can generalise to.

07:55.260 --> 07:57.260
They can't deal with the unexpected

07:57.260 --> 08:01.260
and they're basically limited to what the programmers

08:01.260 --> 08:05.260
foresaw, the situations that the system might be in.

08:05.260 --> 08:08.260
Of course, this line of work was inspired by mathematics

08:08.260 --> 08:10.260
and logic systems.

08:10.260 --> 08:13.260
On the other hand, the big renaissance in the last decade plus

08:13.260 --> 08:19.260
is the sort of progress of learning systems.

08:19.260 --> 08:24.260
Of course, in the 80s there was a flurry of work done on neural networks,

08:24.260 --> 08:26.260
then that died down.

08:26.260 --> 08:29.260
We now know that probably we didn't have enough computing power

08:29.260 --> 08:32.260
or data, maybe not the right algorithms as well.

08:32.260 --> 08:35.260
Basically, in essence, the ideas were correct.

08:35.260 --> 08:39.260
An idea of a learning system is that it learns for itself,

08:39.260 --> 08:41.260
solutions for itself from first principles,

08:41.260 --> 08:43.260
directly from experience.

08:43.260 --> 08:46.260
The amazing thing about these systems and their huge promise

08:46.260 --> 08:49.260
is that they can maybe generalise to tasks

08:49.260 --> 08:52.260
and that it's not being programmed for explicitly

08:52.260 --> 08:56.260
and maybe solve problems that we ourselves as the designers

08:56.260 --> 08:59.260
or scientists behind those systems don't know how to solve.

08:59.260 --> 09:01.260
Of course, that's the huge potential

09:01.260 --> 09:04.260
and also the risk of these kinds of systems.

09:04.260 --> 09:08.260
Originally, these learning systems took a lot of inspiration

09:08.260 --> 09:10.260
and also could be validated, some of the ideas

09:10.260 --> 09:13.260
like reinforcement learning and neural networks

09:13.260 --> 09:17.260
by systems neuroscience in comparing what these systems do,

09:17.260 --> 09:20.260
comparing them on a systems and algorithmic level

09:20.260 --> 09:23.260
to what we know about how the brain works.

09:24.260 --> 09:26.260
Everything we do at DeepMind, of course,

09:26.260 --> 09:29.260
is on the learning system side.

09:29.260 --> 09:33.260
We've been lucky enough to be in the vanguard of this almost revolution

09:33.260 --> 09:37.260
or anasence in the last decade of these types of approaches.

09:37.260 --> 09:41.260
How do we think about what's our special take

09:41.260 --> 09:44.260
on learning systems and how powerful they can be?

09:44.260 --> 09:49.260
There are two component algorithms or approaches,

09:49.260 --> 09:51.260
one could say, that we've fused together.

09:51.260 --> 09:54.260
Of course, there's deep learning or deep neural networks

09:54.260 --> 09:57.260
and the way I think about this is that the deep neural network system

09:57.260 --> 10:00.260
is there to build a model of the environment

10:00.260 --> 10:03.260
of the data and the experience.

10:03.260 --> 10:05.260
Then what do you use that model for?

10:05.260 --> 10:08.260
Well, you can use reinforcement learning,

10:08.260 --> 10:13.260
which is a sort of goal seeking and reward maximising system.

10:13.260 --> 10:17.260
You can use that model and use it to plan

10:17.260 --> 10:22.260
and basically plan and take actions towards a goal,

10:22.260 --> 10:25.260
a goal that may be specified by the designers of that system.

10:25.260 --> 10:29.260
You have the model and then you have the action

10:29.260 --> 10:33.260
and goal-solving element of the systems.

10:33.260 --> 10:38.260
One of our early innovations was to fuse those two things together at scale.

10:38.260 --> 10:41.260
We call it deep reinforcement learning now.

10:41.260 --> 10:45.260
The cool thing about these systems is that they can discover new knowledge

10:45.260 --> 10:49.260
from first principles through this process of trial and error

10:49.260 --> 10:51.260
using these models.

10:51.260 --> 10:54.260
The idea here on this diagram of the agent system

10:54.260 --> 10:57.260
is it gets observations from the environment.

10:57.260 --> 11:00.260
Those observations go towards building and updating

11:00.260 --> 11:03.260
an internal model of how the environment works

11:03.260 --> 11:06.260
and the transition matrices of the environment.

11:06.260 --> 11:09.260
There's some goal it's trying to solve in the environment,

11:09.260 --> 11:12.260
and then after its thinking time has run out,

11:12.260 --> 11:16.260
it has to select an action from the action set

11:16.260 --> 11:18.260
available to it at that moment in time

11:18.260 --> 11:21.260
that will best get it incrementally towards its goal.

11:21.260 --> 11:23.260
Then the action gets output.

11:23.260 --> 11:25.260
It may or may not make a change in the environment.

11:25.260 --> 11:30.260
That drives a new observation and then the model updates further.

11:30.260 --> 11:33.260
You can see with this type of system,

11:33.260 --> 11:37.260
the AI system is actually an active learner.

11:37.260 --> 11:39.260
It participates in its own learning.

11:39.260 --> 11:43.260
The decisions it makes in large part governs what experiences

11:43.260 --> 11:47.260
and what data it will get next to learn more from.

11:47.260 --> 11:50.260
Although this is a pretty simple diagram

11:50.260 --> 11:53.260
and basically describes the whole of reinforcement learning,

11:53.260 --> 11:55.260
the reinforcement learning problem,

11:55.260 --> 11:57.260
there's huge complexities of course of theoretical

11:57.260 --> 12:01.260
and practical complexities underlying this diagram

12:01.260 --> 12:03.260
that need to be solved.

12:03.260 --> 12:06.260
We know that in the limit this must work

12:06.260 --> 12:10.260
because this is how mammalian brains work, including humans.

12:10.260 --> 12:14.260
This is one of the learning mechanisms that we have in our own brains.

12:14.260 --> 12:16.260
Reinforcement learning was found to be implemented

12:16.260 --> 12:21.260
by dopamine neurons in the brain in the late 90s.

12:21.260 --> 12:23.260
We know if we push this hard enough,

12:23.260 --> 12:28.260
this should be one path towards general artificial intelligence.

12:28.260 --> 12:30.260
What did we famously use this for?

12:30.260 --> 12:35.260
AlfaGo was the program that I think we did a lot of things before this.

12:35.260 --> 12:39.260
Like Atari games and other proof points.

12:39.260 --> 12:43.260
AlfaGo was really our first attempt at doing this at a huge scale

12:43.260 --> 12:46.260
to crack a big problem that was unsolved in AI,

12:46.260 --> 12:49.260
one of the holy grails of AI research,

12:49.260 --> 12:53.260
which was a program to beat the world champion at the Game of Go.

12:53.260 --> 12:57.260
I want to talk a little bit about this in hindsight now,

12:57.260 --> 13:02.260
knowing what I know now, how I've reinterpreted what we did with AlfaGo.

13:02.260 --> 13:05.260
I think I can explain it in a much more simple in general way

13:05.260 --> 13:09.260
than perhaps how I was explaining it back five, six years ago

13:09.260 --> 13:12.260
when we were in the midst of building this system.

13:12.260 --> 13:15.260
Just for those of you who don't know...

13:16.260 --> 13:18.260
I don't know why that's not updating.

13:21.260 --> 13:22.260
There we go.

13:22.260 --> 13:24.260
This is the Game of Go.

13:26.260 --> 13:29.260
This is the Game of Go, the board game.

13:29.260 --> 13:35.260
It's a phenomenal game and it's a much more esoteric game

13:35.260 --> 13:39.260
and artistic game, one could say, than chess.

13:39.260 --> 13:43.260
It occupies the same intellectual echelon chess stars in the West.

13:43.260 --> 13:48.260
In China, in Japan, in Korea and other Asian countries, they play Go.

13:48.260 --> 13:55.260
Go has resisted old-fashioned logic system and expert system approaches,

13:55.260 --> 13:58.260
whereas chess was solved by those things

13:58.260 --> 14:00.260
because of various factors.

14:00.260 --> 14:03.260
One is the search space is truly enormous in Go.

14:03.260 --> 14:08.260
It's roughly 10 to the power, 170 possible board positions,

14:08.260 --> 14:11.260
which is way more than there are atoms in the universe,

14:11.260 --> 14:14.260
so there's no way one could exhaustively search

14:14.260 --> 14:19.260
all of the possible board positions in order to find the right path through.

14:19.260 --> 14:23.260
Even bigger problem actually is that it's impossible,

14:23.260 --> 14:27.260
or thought was, it was impossible to write down an evaluation function.

14:27.260 --> 14:29.260
To hand code an evaluation function,

14:29.260 --> 14:32.260
which is what most modern-day chess programs use.

14:32.260 --> 14:36.260
The reason is because Go is such an esoteric game.

14:36.260 --> 14:39.260
It doesn't have materiality in chess.

14:39.260 --> 14:43.260
As a first approximation, one can add up the piece values on both sides,

14:43.260 --> 14:46.260
and that will tell you very crudely,

14:46.260 --> 14:48.260
which side is winning in that position.

14:48.260 --> 14:52.260
You need to know that in order to make decisions about what to do next.

14:52.260 --> 14:56.260
Many people are tempted to, over 20 years since Deep Blue,

14:56.260 --> 15:01.260
attempted to write to construct these evaluation functions for Go.

15:01.260 --> 15:05.260
One of the issues is that Go players themselves do not know,

15:05.260 --> 15:08.260
consciously at least, what that information is.

15:08.260 --> 15:12.260
Because it's so complex a game,

15:12.260 --> 15:16.260
they actually use their intuition rather than explicit calculation

15:16.260 --> 15:18.260
in order to deal with the complexity of Go.

15:18.260 --> 15:22.260
Whereas chess players, if you ask them how, why did they make a decision,

15:22.260 --> 15:26.260
a chess grandmaster will be able to tell you explicitly

15:26.260 --> 15:28.260
the various factors involved.

15:28.260 --> 15:30.260
A Go player generally won't do that.

15:30.260 --> 15:32.260
They'll just say things like it felt right.

15:32.260 --> 15:34.260
This felt like the right move,

15:34.260 --> 15:37.260
which is what I think also makes Go an incredible game.

15:37.260 --> 15:41.260
Of course, intuition is not something one would associate

15:41.260 --> 15:45.260
with computer programs, especially logic systems.

15:45.260 --> 15:48.260
Maybe in the Q&A we can discuss a little bit more

15:48.260 --> 15:50.260
about what intuition may be.

15:50.260 --> 15:54.260
But I don't think it's my conclusion now,

15:54.260 --> 15:56.260
after doing all these games,

15:56.260 --> 15:58.260
and indeed some of the science things we've done,

15:58.260 --> 16:01.260
is that it's not some mysterious thing.

16:01.260 --> 16:04.260
It's actually information that our brain knows about

16:04.260 --> 16:06.260
and has learnt through experience, of course.

16:06.260 --> 16:09.260
I mean, there's no other way one can learn information.

16:09.260 --> 16:12.260
But it's just in the association courtesies.

16:12.260 --> 16:16.260
So it's not actually consciously available to a high-level cortex.

16:16.260 --> 16:19.260
So it seems mysterious to us how we ride a bike,

16:19.260 --> 16:23.260
when these sort of motor, sensory motor things we're able to do,

16:23.260 --> 16:25.260
because our conscious part of our brain

16:25.260 --> 16:27.260
cannot access those representations.

16:27.260 --> 16:29.260
And if we can't do that,

16:29.260 --> 16:33.260
then we definitely can't explicitly code it in some logic code,

16:33.260 --> 16:35.260
which is why traditionally those tasks,

16:35.260 --> 16:37.260
including things like computer vision,

16:37.260 --> 16:40.260
have been quite hard for logic systems to solve,

16:40.260 --> 16:43.260
even over the last 50 years.

16:43.260 --> 16:45.260
So a lot about what we were doing

16:45.260 --> 16:47.260
was trying to approximate this kind of intuition

16:47.260 --> 16:49.260
in these learning systems.

16:49.260 --> 16:50.260
So how did we work?

16:50.260 --> 16:52.260
And I'm actually going to describe,

16:52.260 --> 16:53.260
not just AlphaGo here,

16:53.260 --> 16:56.260
but the whole series of AlphaX programmes.

16:56.260 --> 17:01.260
So AlphaGo, the original one that beat Lisa Doll in 2016.

17:01.260 --> 17:04.260
And then AlphaGo Zero,

17:04.260 --> 17:07.260
that then didn't need human data to learn from,

17:07.260 --> 17:08.260
just learn for itself.

17:08.260 --> 17:10.260
And then finally AlphaZero,

17:10.260 --> 17:12.260
which could play any two player game.

17:12.260 --> 17:15.260
So I'm going to sort of describe them all,

17:15.260 --> 17:16.260
roughly speaking,

17:16.260 --> 17:20.260
with this sort of demonstrative diagram.

17:20.260 --> 17:22.260
So the way you can think of all of these systems

17:22.260 --> 17:26.260
is we're initially training a neural network through self-play.

17:26.260 --> 17:28.260
So the system plays against itself,

17:28.260 --> 17:32.260
and it learns to evaluate positions

17:32.260 --> 17:34.260
and to pick the most likely moves

17:34.260 --> 17:38.260
that are most useful for it to look at.

17:38.260 --> 17:40.260
So that's what it's got to do.

17:40.260 --> 17:42.260
Now initially, it starts with no knowledge.

17:42.260 --> 17:44.260
So you have an initialised neural network,

17:44.260 --> 17:46.260
it starts with zero knowledge.

17:46.260 --> 17:48.260
So it literally is moving randomly.

17:48.260 --> 17:51.260
So we can call that version one.

17:51.260 --> 17:52.260
That's the neural network.

17:52.260 --> 17:56.260
And what it does is it plays roughly 100,000 games against itself.

17:56.260 --> 17:59.260
And so that then becomes a data set.

17:59.260 --> 18:02.260
So that 100,000 games, we take that as a data set.

18:02.260 --> 18:03.260
And what we try to do with it

18:03.260 --> 18:06.260
is train a version two of that network,

18:06.260 --> 18:07.260
a new neural network.

18:07.260 --> 18:11.260
But we try and train it on this version one data set

18:11.260 --> 18:13.260
to predict in the middle of a position,

18:13.260 --> 18:14.260
in the middle of a game,

18:14.260 --> 18:16.260
from a position in the middle of a game,

18:16.260 --> 18:18.260
which side is going to win.

18:18.260 --> 18:20.260
So predict ahead of time.

18:20.260 --> 18:24.260
And also what sorts of moves does the V1 system choose

18:24.260 --> 18:26.260
in a particular position.

18:26.260 --> 18:30.260
So it's trying to be better at both those two things.

18:30.260 --> 18:33.260
And then what happens is we train that V2 system,

18:33.260 --> 18:35.260
and then we have a little mini tournament

18:35.260 --> 18:37.260
between V1 and V2.

18:37.260 --> 18:39.260
So it's roughly 100 games,

18:39.260 --> 18:40.260
and they have a little match off.

18:40.260 --> 18:44.260
And basically, if the V2 system

18:44.260 --> 18:46.260
hits a particular threshold win rate,

18:46.260 --> 18:48.260
55% in this case,

18:48.260 --> 18:52.260
then we say it's significantly better than V1.

18:52.260 --> 18:53.260
And if that's true,

18:53.260 --> 18:57.260
then what we do is we replace V1 with version two network,

18:57.260 --> 18:59.260
this new network in Purple.

18:59.260 --> 19:00.260
And that, of course,

19:00.260 --> 19:02.260
plays another 100,000 games against itself.

19:02.260 --> 19:04.260
And now it creates a new data set.

19:04.260 --> 19:07.260
But this data set now in Purple, in the middle,

19:07.260 --> 19:10.260
is slightly better quality than that first data set,

19:10.260 --> 19:12.260
because the player is slightly better.

19:12.260 --> 19:15.260
And to begin with, almost imperceptively better.

19:15.260 --> 19:17.260
So it's just slightly better than random now.

19:17.260 --> 19:20.260
But that's enough signal to then train,

19:20.260 --> 19:23.260
of course we train a version three system,

19:23.260 --> 19:25.260
and that plays off against version two.

19:25.260 --> 19:29.260
Now, if you don't reach this 55% win rate,

19:29.260 --> 19:32.260
what you do instead is you take back the version two,

19:32.260 --> 19:34.260
and you continue to generate more data with that,

19:34.260 --> 19:36.260
another 100,000 games.

19:36.260 --> 19:39.260
And you have 200,000 to train your next version three.

19:39.260 --> 19:41.260
And eventually that version three

19:41.260 --> 19:43.260
will be better than version two.

19:43.260 --> 19:48.260
So after one does this around 17 or 18 times,

19:48.260 --> 19:52.260
you go from random to better than world champion.

19:52.260 --> 19:54.260
That's it.

19:54.260 --> 19:58.260
And you can do this with any two player game,

19:58.260 --> 20:00.260
perfect information game.

20:00.260 --> 20:03.260
So the same network can do that.

20:03.260 --> 20:07.260
Get to better to world champion within 20 to 30 generations

20:07.260 --> 20:08.260
of doing this.

20:08.260 --> 20:10.260
So you literally, and we got to the point where so fast

20:10.260 --> 20:12.260
you literally set it off in the morning,

20:12.260 --> 20:14.260
you could play chess about it at lunchtime

20:14.260 --> 20:17.260
and maybe just beat it, and then by tea time,

20:17.260 --> 20:19.260
you know, you no chance.

20:19.260 --> 20:21.260
Literally in the day, you could actually see the evolution

20:21.260 --> 20:22.260
in one day.

20:22.260 --> 20:25.260
It's kind of incredible to watch as a chess player.

20:25.260 --> 20:27.260
So what is it doing then,

20:27.260 --> 20:31.260
in terms of thinking about this enormous search space?

20:31.260 --> 20:33.260
So what's happening is,

20:33.260 --> 20:36.260
and the sort of, I think, advance of AlphaGo,

20:36.260 --> 20:40.260
one of the advances was combining this neural network system,

20:40.260 --> 20:45.260
or model, with a kind of more classical tree search algorithm.

20:45.260 --> 20:48.260
In this case, we use Monte Carlo tree search.

20:48.260 --> 20:51.260
And you can think of the tree of possibilities

20:51.260 --> 20:53.260
looking a bit like this in Go,

20:53.260 --> 20:55.260
where each node here is a positioning in Go,

20:55.260 --> 20:58.260
obviously shown by these little mini Go boards.

20:58.260 --> 21:01.260
And you can imagine if you're some middle game position,

21:01.260 --> 21:03.260
you know, there's just this countless

21:03.260 --> 21:06.260
10 to the 170 possibilities in the limit.

21:06.260 --> 21:10.260
How is one supposed to find the needle in the haystack, right?

21:10.260 --> 21:12.260
The good moves that could be world champion

21:12.260 --> 21:15.260
or better level decisions.

21:15.260 --> 21:19.260
So what the neural network does is it constrains,

21:19.260 --> 21:24.260
that model constrains the search to things to make it tractable,

21:24.260 --> 21:26.260
to things that are reasonably likely to work,

21:26.260 --> 21:28.260
reasonably effective,

21:28.260 --> 21:30.260
and it can evaluate that at each node level

21:30.260 --> 21:32.260
with its evaluation function.

21:32.260 --> 21:34.260
And so instead of having to do, you know,

21:34.260 --> 21:37.260
10 to the hundreds of possibilities,

21:37.260 --> 21:40.260
one can just zoom into, you know, mere thousands,

21:40.260 --> 21:43.260
10,000 or so searches.

21:43.260 --> 21:44.260
And so therefore,

21:44.260 --> 21:47.260
instead of that searching the entire grey tree of all possibilities,

21:47.260 --> 21:50.260
one just looks at this far more limited, you know,

21:50.260 --> 21:52.260
search tree in blue here.

21:52.260 --> 21:54.260
And then when you run out of thinking time,

21:54.260 --> 21:56.260
of course, you select the best path

21:56.260 --> 22:00.260
that you found so far in pink here.

22:00.260 --> 22:03.260
So, you know, we did this back in 2015,

22:03.260 --> 22:06.260
and then in the subsequent years, we still work on this now.

22:06.260 --> 22:08.260
There's a system called Mu Zero,

22:08.260 --> 22:10.260
which is our latest version of this that can do,

22:10.260 --> 22:13.260
not only do two player perfect information board games,

22:13.260 --> 22:17.260
but can also build models of its environment.

22:17.260 --> 22:20.260
So it can actually also do things like Atari games

22:20.260 --> 22:22.260
and video games where you actually don't have

22:22.260 --> 22:25.260
the rules of the game given to you.

22:25.260 --> 22:27.260
It has to actually figure that out for itself

22:27.260 --> 22:28.260
through observation as well.

22:28.260 --> 22:31.260
So it's one step even more general than Alpha Zero.

22:31.260 --> 22:34.260
And what we did with Alpha Go, of course, now is,

22:34.260 --> 22:38.260
as Sir Nigel mentioned, is we took it to Seoul in 2016

22:38.260 --> 22:40.260
in this million dollar challenge match with Lisa Doll.

22:40.260 --> 22:43.260
And some of you may remember this,

22:43.260 --> 22:44.260
but we won for one.

22:44.260 --> 22:47.260
You know, it was a huge thing, especially in Asia and in Korea.

22:47.260 --> 22:49.260
I mean, the country almost came to stand still.

22:49.260 --> 22:52.260
There's over 200 million people watch the games.

22:52.260 --> 22:54.260
And we won for one,

22:54.260 --> 22:59.260
and experts in both AI and in Go proclaimed this advance

22:59.260 --> 23:03.260
to be a decade before they would have predicted.

23:03.260 --> 23:06.260
But the important thing in the end was actually not just the fact

23:06.260 --> 23:08.260
that Alpha Go won the match,

23:08.260 --> 23:12.260
but how it won was, I think, really instructive.

23:12.260 --> 23:15.260
So I'm just going to give one example of this,

23:15.260 --> 23:17.260
but actually Alpha Go, I think, is in the end changed

23:17.260 --> 23:21.260
the way that we as human beings view the game of Go.

23:21.260 --> 23:24.260
But this is the most famous game of that set of five.

23:24.260 --> 23:26.260
There are actually some amazing different games,

23:26.260 --> 23:30.260
including the one that Lisa Doll won with a genius move in game four.

23:30.260 --> 23:35.260
But move 37 in game two, I think, will go down in Go history.

23:35.260 --> 23:37.260
And this was the ball position at that time.

23:37.260 --> 23:40.260
And I haven't got time to go into why this was so amazing.

23:40.260 --> 23:43.260
But suffice to say, Alpha Go here was black,

23:43.260 --> 23:45.260
and Lisa Doll is the white stones.

23:45.260 --> 23:48.260
And this is very early on in the game, move 37.

23:48.260 --> 23:52.260
You know, Go games last for a few hundred moves generally.

23:52.260 --> 23:55.260
And Alpha Go played this move 37 stone

23:55.260 --> 23:58.260
on the right hand side here, marked in red.

23:58.260 --> 24:01.260
And the amazing thing about this was the position of the stone

24:01.260 --> 24:04.260
was on the fifth line from the edge of the board.

24:04.260 --> 24:08.260
And that, if you're an expert Go player, is unthinkable.

24:08.260 --> 24:11.260
It's like you would be told off by your Go master

24:11.260 --> 24:14.260
that you should never do make a move like that.

24:14.260 --> 24:18.260
Because it gives white too much space on the side of the board.

24:18.260 --> 24:21.260
But Alpha Go decided to do it.

24:21.260 --> 24:24.260
Never seen before in master play would be recommended against.

24:24.260 --> 24:28.260
And then 100 moves or so later, it turned out this stone,

24:28.260 --> 24:31.260
this move 37 stone, was in the perfect position

24:31.260 --> 24:34.260
to decide the battle that spread out from the bottom left

24:34.260 --> 24:36.260
all the way across the board.

24:36.260 --> 24:38.260
And it was just in the right place to decide that battle,

24:38.260 --> 24:40.260
which decided the whole game.

24:40.260 --> 24:47.260
And almost as if it had presciently sort of seen that influence ahead of time.

24:47.260 --> 24:50.260
So now people play on the fifth line all the time, I'm told.

24:50.260 --> 24:53.260
So this has changed everything.

24:53.260 --> 24:58.260
And there's multiple books now written about Alpha Go's strategies.

24:58.260 --> 25:04.260
And this is an original strategy because this is not something

25:04.260 --> 25:07.260
that Alpha Go could have learned from human play.

25:07.260 --> 25:09.260
In fact, it would have learned the opposite.

25:09.260 --> 25:13.260
It would have learned not to do this kind of move.

25:13.260 --> 25:15.260
So if you're interested in more on that Alpha Go,

25:15.260 --> 25:18.260
I recommend you this amazing award-winning documentary

25:18.260 --> 25:21.260
that was done by an independent filmmaker on YouTube now.

25:21.260 --> 25:23.260
If you want to see the sort of ins and outs of it,

25:23.260 --> 25:27.260
it was very emotional as an experience for us from all sides,

25:27.260 --> 25:30.260
especially me being an ex-games player.

25:30.260 --> 25:35.260
I could really understand it from Lisa Doll's point of view too.

25:35.260 --> 25:38.260
So as I said, we then took this to Alpha Zero a couple of years ago,

25:38.260 --> 25:42.260
two, three years ago now, and generalised this to all two-player games.

25:42.260 --> 25:49.260
And these graphs show how Alpha Zero did against the best machines at the time

25:49.260 --> 25:52.260
in the specialised games of chess.

25:52.260 --> 25:54.260
It beat the best version of Stockfish,

25:54.260 --> 26:00.260
which is this incredible handcrafted system, the descendant of Deep Blue.

26:00.260 --> 26:02.260
And it was able to beat Stockfish 8,

26:02.260 --> 26:06.260
which was the best Stockfish at the time, in four hours of training.

26:06.260 --> 26:12.260
It could beat Alpha Go, Alpha Zero beat Alpha Go in eight hours at Go.

26:12.260 --> 26:16.260
And then we just tried it with one other game, Japanese chess shogi,

26:16.260 --> 26:19.260
actually, which is a really interesting variation on chess.

26:19.260 --> 26:25.260
And it could beat the best handcrafted programme called ELMO within two hours of training.

26:25.260 --> 26:28.260
The same system, all three games.

26:28.260 --> 26:30.260
So that was generalised.

26:30.260 --> 26:32.260
And then, of course, because I'm a chess player,

26:32.260 --> 26:35.260
I play a little bit of Go, but I'm not very strong, but so chess is my game.

26:35.260 --> 26:39.260
And so for me, this was the most exciting part of applying Alpha Zero,

26:39.260 --> 26:44.260
because I actually had a discussion with Murray Campbell,

26:44.260 --> 26:47.260
who some of you will know was one of the project leaders

26:47.260 --> 26:51.260
behind Deep Blue, our IBM back in the 90s.

26:51.260 --> 26:56.260
And we just, I think we just were about to play the Lisa Doll match,

26:56.260 --> 26:57.260
or maybe we just finished.

26:57.260 --> 26:59.260
And I was giving a lecturer at a conference,

26:59.260 --> 27:01.260
and Murray Campbell was there as well in the audience.

27:01.260 --> 27:03.260
And he came up to me afterwards, and we were discussing,

27:03.260 --> 27:05.260
I said to him, I'm thinking about,

27:05.260 --> 27:08.260
maybe we should try this with chess and see what happens.

27:08.260 --> 27:11.260
And I wanted to know what his prediction would be.

27:11.260 --> 27:16.260
Do you think these incredibly powerful handcrafted systems,

27:16.260 --> 27:18.260
like stockfish, could be beaten?

27:18.260 --> 27:21.260
Was there any more headroom in chess?

27:21.260 --> 27:24.260
Chess is probably the oldest application of AI, right?

27:24.260 --> 27:27.260
I mean, Turing and Shannon and people like that

27:27.260 --> 27:28.260
have all tried their hand.

27:28.260 --> 27:31.260
Every AI researcher at some point has tried their hand

27:31.260 --> 27:34.260
on a chess program back to the 40s and 50s,

27:34.260 --> 27:37.260
even if Turing had to run the program by hand

27:37.260 --> 27:40.260
on a piece of paper and a pen.

27:40.260 --> 27:44.260
And then, of course, in the last 25 years or so,

27:44.260 --> 27:47.260
world champions have been studying with their chess programs

27:47.260 --> 27:50.260
and mapping out all of chess, opening theory, all of these things.

27:50.260 --> 27:53.260
So it was a legitimate question actually to ask is,

27:53.260 --> 27:55.260
was there any more headroom left?

27:55.260 --> 27:57.260
And what sort of chess would AlphaZero play

27:57.260 --> 28:01.260
if we were to train it from first principles

28:01.260 --> 28:08.260
and play it against these amazing hand-engineered monsters

28:08.260 --> 28:11.260
in some sense of a machine, incredible calculating machines?

28:11.260 --> 28:14.260
And so, of course, we couldn't actually come to an agreement on that.

28:14.260 --> 28:16.260
And that, as the scientists in the orders will know,

28:16.260 --> 28:18.260
that's the sign of a good question, I think,

28:18.260 --> 28:20.260
where either answer would be interesting.

28:20.260 --> 28:22.260
If we were to win and there was some new style out there,

28:22.260 --> 28:23.260
there would be incredibly interesting.

28:23.260 --> 28:26.260
And also be interesting if these hand-crofter systems,

28:26.260 --> 28:30.260
at least in one domain, chess, had reached the limit.

28:30.260 --> 28:33.260
So we got off and started doing that.

28:33.260 --> 28:37.260
And I'm pleased to say that AlphaZero not only played stronger,

28:37.260 --> 28:41.260
but it did come up with a completely new style of chess,

28:41.260 --> 28:44.260
which I think, and my chess friends tell me,

28:44.260 --> 28:49.260
is more aesthetically pleasing as well as a chess program.

28:49.260 --> 28:52.260
Obviously, subjectively from a human expert's point of view.

28:52.260 --> 28:54.260
And the reason it is is because what it does,

28:54.260 --> 28:56.260
and it does many innovations,

28:56.260 --> 29:01.260
but the main one is that it favours mobility over materiality.

29:01.260 --> 29:04.260
So traditionally, hand-crofter chess programs

29:04.260 --> 29:06.260
have always favoured materiality.

29:06.260 --> 29:10.260
The joke within the chess circles is that chess computer sees a pawn

29:10.260 --> 29:13.260
and then grabs the pawn because it loves material

29:13.260 --> 29:15.260
because it gets plus one in its evaluation function.

29:15.260 --> 29:17.260
And then it tries to hang on for dear life

29:17.260 --> 29:19.260
in a really ugly position,

29:19.260 --> 29:21.260
but it wins because it never makes any tactical mistakes.

29:21.260 --> 29:23.260
So it's sort of very effective,

29:23.260 --> 29:27.260
but it's a little bit sort of aesthetically unsatisfying,

29:27.260 --> 29:29.260
one would say, as a style.

29:29.260 --> 29:32.260
But instead of that, actually AlphaZero does the opposite.

29:32.260 --> 29:36.260
It loves sacrificing pieces, material, to get mobility,

29:36.260 --> 29:40.260
to get more mobility for its remaining pieces.

29:40.260 --> 29:44.260
So this is a game from, we did a 100 match

29:44.260 --> 29:46.260
between AlphaZero and Stockfish,

29:46.260 --> 29:50.260
and then we gave it to the British chess champion to analyse,

29:50.260 --> 29:53.260
and he picked out the coolest positions.

29:53.260 --> 29:54.260
This is my favourite.

29:54.260 --> 29:56.260
It's sometimes called the immortal Zugswang game.

29:56.260 --> 29:59.260
Zugswang is a phrase in chess,

29:59.260 --> 30:02.260
a German phrase that means any move that one makes in that position

30:02.260 --> 30:04.260
makes your position worse.

30:04.260 --> 30:07.260
So it's a special type of position where you're in Zugswang,

30:07.260 --> 30:10.260
which means anything you do, it's going to make it worse,

30:10.260 --> 30:11.260
which is very unusual.

30:11.260 --> 30:13.260
And it's super unusual in this kind of position,

30:13.260 --> 30:15.260
for those of you who know chess, where black,

30:15.260 --> 30:17.260
which has got more pieces, the two rooks and a queen,

30:17.260 --> 30:20.260
so it's got big material advantage, very powerful pieces,

30:20.260 --> 30:22.260
the most powerful pieces remaining in chess,

30:22.260 --> 30:24.260
but they will stuck in the corner,

30:24.260 --> 30:28.260
and AlphaZero has sort of sealed them up with cement with its pieces,

30:28.260 --> 30:30.260
and basically none of those pieces can move.

30:30.260 --> 30:32.260
So this is kind of an incredible position.

30:32.260 --> 30:36.260
So almost anything black does in this position,

30:36.260 --> 30:38.260
its black to move, will make its position worse,

30:38.260 --> 30:42.260
even though it's got all of these very powerful pieces.

30:42.260 --> 30:45.260
So that was one innovation.

30:45.260 --> 30:47.260
There were lots of interesting poppies about AlphaZero

30:47.260 --> 30:50.260
that I won't go into, but one can think about,

30:50.260 --> 30:53.260
well, why is it that AlphaZero plays like this,

30:53.260 --> 30:57.260
and traditional chess engines didn't?

30:57.260 --> 30:59.260
Nowadays, actually, interestingly, they've updated Stockfish

30:59.260 --> 31:03.260
to include some of these ideas by hand in Stockfish,

31:03.260 --> 31:06.260
and actually now it's even more powerful.

31:06.260 --> 31:08.260
So it's kind of interesting hybrid system.

31:08.260 --> 31:12.260
But my feeling is that it's better at evaluating positions

31:12.260 --> 31:15.260
than chess engines, so that's one thing,

31:15.260 --> 31:17.260
so it's got a better evaluation function.

31:17.260 --> 31:19.260
And the main thing is it doesn't have to overcome

31:19.260 --> 31:21.260
these inbuilt rules.

31:21.260 --> 31:23.260
That's why it's sacrificing pieces,

31:23.260 --> 31:25.260
because if you think about it, a hard-coded chess engine

31:25.260 --> 31:28.260
would have to calculate in its search tree

31:28.260 --> 31:30.260
that if it was going to sacrifice a rook for a bishop,

31:30.260 --> 31:32.260
that's minus two points,

31:32.260 --> 31:35.260
is it going to get back those two points of value

31:35.260 --> 31:37.260
within its search tree horizon?

31:37.260 --> 31:39.260
AlphaZero doesn't have to worry about that,

31:39.260 --> 31:41.260
because there's no rules like that in there.

31:41.260 --> 31:43.260
It can evaluate things contextually,

31:43.260 --> 31:46.260
based on the particular situation at hand,

31:46.260 --> 31:48.260
and the patterns involved there.

31:48.260 --> 31:50.260
And also, the other big thing is,

31:50.260 --> 31:52.260
Stockfish and programmes like that,

31:52.260 --> 31:54.260
they have thousands of handcrafted rules,

31:54.260 --> 31:57.260
so one problem is generating those rules,

31:57.260 --> 31:59.260
but an even bigger problem, in my opinion,

31:59.260 --> 32:01.260
is balancing those factors together.

32:01.260 --> 32:04.260
That's a huge handcrafted juggling act.

32:04.260 --> 32:07.260
And instead of that, obviously AlphaZero learns itself

32:07.260 --> 32:10.260
how to balance out the factors that it's learned,

32:10.260 --> 32:14.260
and to do that automatically.

32:14.260 --> 32:17.260
So one can actually see how efficient this system is,

32:17.260 --> 32:21.260
based on the amount of search that traditional search engines

32:21.260 --> 32:24.260
have to do per each move they make.

32:24.260 --> 32:27.260
And a human grandmaster makes only the order of,

32:27.260 --> 32:29.260
looks at about 100 moves per decision,

32:29.260 --> 32:31.260
so incredibly efficient with our models.

32:31.260 --> 32:34.260
And the state-of-the-art chess engine, like Stockfish,

32:34.260 --> 32:39.260
would make tens of millions of evaluations per move.

32:39.260 --> 32:42.260
And AlphaZero is sort of in the middle here,

32:42.260 --> 32:46.260
in terms of orders of magnitude, tens of thousands of moves.

32:46.260 --> 32:50.260
So not as efficient as human players,

32:50.260 --> 32:53.260
but far more efficient than the search one would get

32:53.260 --> 32:56.260
in these search engines.

32:56.260 --> 32:59.260
So again, if you're interested in the details about,

32:59.260 --> 33:02.260
or your chess playing, and the details about what this changed,

33:02.260 --> 33:06.260
the British champion and Natasha Reagan

33:06.260 --> 33:08.260
wrote an amazing book called Game Changer,

33:08.260 --> 33:12.260
when we gave them behind the scenes access to AlphaZero,

33:12.260 --> 33:14.260
and what new motifs they found,

33:14.260 --> 33:17.260
at least a dozen new motifs they found in chess.

33:17.260 --> 33:20.260
And the cool thing is that it's very gratifying for me,

33:20.260 --> 33:22.260
is that people like Magnus Carlson,

33:22.260 --> 33:25.260
who's the current world champion, incredible player,

33:25.260 --> 33:27.260
he said a few years back he was one of the first people

33:27.260 --> 33:29.260
to read the book and who we sent it to,

33:29.260 --> 33:31.260
and I've been influenced by my heroes recently,

33:31.260 --> 33:34.260
one of which is AlphaZero, which is really cool to say.

33:34.260 --> 33:36.260
And he actually incorporated, because he's so talented,

33:36.260 --> 33:38.260
he was able to quite quickly,

33:38.260 --> 33:40.260
quicker than all the other chess players,

33:40.260 --> 33:42.260
incorporate some of these ideas into his play.

33:42.260 --> 33:45.260
And then Garry Casparov, he used to be a hero of mine

33:45.260 --> 33:48.260
when he was world champion when I was growing up and playing chess.

33:48.260 --> 33:50.260
He worked the forward for the book,

33:50.260 --> 33:52.260
and he said programs usually reflect priorities and prejudices

33:52.260 --> 33:55.260
of programmers, but AlphaZero, it learns for itself,

33:55.260 --> 33:57.260
and I would say it's star reflects the truth,

33:57.260 --> 34:00.260
which is, you know, I think, a beautiful quote.

34:01.260 --> 34:05.260
So we've been lucky enough to have several of these sort of fundamental

34:05.260 --> 34:07.260
breakthroughs in games.

34:07.260 --> 34:10.260
We started with Atari, and our program called DQN,

34:10.260 --> 34:13.260
being able to play Atari games directly from pixels

34:13.260 --> 34:15.260
and maximise the score just from pixels,

34:15.260 --> 34:18.260
not being told the rules of the game, AlphaGo and AlphaZero,

34:18.260 --> 34:19.260
I just mentioned.

34:19.260 --> 34:22.260
And then we went further with programs like AlphaStar,

34:22.260 --> 34:26.260
which played the most complex video game called StarCraft 2,

34:26.260 --> 34:29.260
which is a very complicated real-time strategy game

34:29.260 --> 34:31.260
with huge other challenges.

34:31.260 --> 34:34.260
It's only partially observable, it's not perfect information,

34:34.260 --> 34:36.260
there's an economy system to it,

34:36.260 --> 34:39.260
and you have generally thousands of possible actions

34:39.260 --> 34:42.260
you can take for any choice, not a few dozen.

34:43.260 --> 34:47.260
And we managed to also get to grandmaster level at that.

34:48.260 --> 34:50.260
So that was all of our games work,

34:50.260 --> 34:53.260
but really it was leading up to this moment,

34:53.260 --> 34:57.260
which in the last couple of years has been just so exciting

34:57.260 --> 35:01.260
and so gratifying for us to make progress with,

35:01.260 --> 35:05.260
which is that the games, and I love games, always will love games,

35:05.260 --> 35:09.260
playing them, designing them and using them as testing grounds,

35:09.260 --> 35:12.260
they were the perfect testing ground for developing AI,

35:12.260 --> 35:16.260
but ultimately the aim was not to play games to world championship level,

35:16.260 --> 35:19.260
it was to build general systems that could generalise

35:19.260 --> 35:21.260
and solve real world problems.

35:21.260 --> 35:23.260
And the one that's particularly passionate for me

35:23.260 --> 35:26.260
is using AI for scientific discovery.

35:27.260 --> 35:29.260
And there are three things that I look for when currently,

35:29.260 --> 35:32.260
when we want to select a scientific problem

35:32.260 --> 35:36.260
that we believe our systems could be good at.

35:36.260 --> 35:40.260
So number one is we actually search out massive combinatorial

35:40.260 --> 35:42.260
search spaces or state spaces.

35:42.260 --> 35:44.260
So the bigger, the better actually.

35:44.260 --> 35:45.260
Why is that?

35:45.260 --> 35:48.260
Well, because we know then traditional methods

35:48.260 --> 35:50.260
and exhaustive brute force methods won't work.

35:50.260 --> 35:53.260
So we're in a razy where something else is needed

35:53.260 --> 35:56.260
and we think that we're good at that something else.

35:56.260 --> 35:59.260
Number two is that we want to have,

35:59.260 --> 36:02.260
we like problems that have a clear objective function

36:02.260 --> 36:04.260
or metric that one can specify

36:04.260 --> 36:06.260
so that you can optimise and hill climb against it

36:06.260 --> 36:08.260
with your learning system.

36:08.260 --> 36:10.260
And then number three is we look for problems

36:10.260 --> 36:13.260
that either have a lot of data available

36:13.260 --> 36:15.260
to learn and train from,

36:15.260 --> 36:17.260
or, and ideally it's and or,

36:17.260 --> 36:19.260
an accurate and efficient simulator

36:19.260 --> 36:21.260
that one can use to generate more data.

36:21.260 --> 36:23.260
And that simulator doesn't have to be perfect.

36:23.260 --> 36:25.260
It just has to be good enough

36:25.260 --> 36:28.260
that you can extract some signal from the data that it generates.

36:29.260 --> 36:33.260
Now it turns out that when you look at a lot of problems

36:33.260 --> 36:37.260
with this prism, then actually a lot of surprising

36:37.260 --> 36:41.260
number of problems can be made to fit these criteria.

36:41.260 --> 36:45.260
And of course, the number one thing we were looking at

36:45.260 --> 36:48.260
was protein folding, which I want to talk a bit about now.

36:48.260 --> 36:50.260
And we look for problems,

36:50.260 --> 36:52.260
not only that just fit those three criteria,

36:52.260 --> 36:54.260
but of course there's always an opportunity cost

36:54.260 --> 36:57.260
when you embark on applying AI to something major.

36:57.260 --> 36:59.260
It's going to take you many years,

36:59.260 --> 37:01.260
depending on how hard that problem is.

37:01.260 --> 37:05.260
And we look for something that will have really huge impact.

37:05.260 --> 37:07.260
Perhaps we sometimes talk about root nodes

37:07.260 --> 37:10.260
that can open up whole new branches

37:10.260 --> 37:14.260
of scientific discovery if they were to be solved.

37:14.260 --> 37:17.260
And protein folding ticked all of those boxes.

37:17.260 --> 37:20.260
So if you don't know what protein folding is,

37:20.260 --> 37:23.260
it's this classic problem of can one go

37:23.260 --> 37:26.260
from a one-dimensional amino acid sequence,

37:26.260 --> 37:28.260
you can think of it as the genetic sequence

37:28.260 --> 37:32.260
for a protein that describes a protein coded by the genome.

37:32.260 --> 37:34.260
And can you predict from that directly

37:34.260 --> 37:37.260
the 3D structure of the protein in your body,

37:37.260 --> 37:39.260
the 3D form that it takes.

37:39.260 --> 37:42.260
And the reason this is important is that proteins

37:42.260 --> 37:45.260
are basically essential for everything in life,

37:45.260 --> 37:47.260
every function in your body.

37:47.260 --> 37:51.260
And it's thought that the 3D structure of the protein,

37:51.260 --> 37:54.260
at least in the large part, governs its function.

37:54.260 --> 37:56.260
So if one can understand the structure,

37:56.260 --> 38:01.260
then one can get closer to the function of the protein.

38:01.260 --> 38:04.260
Now, until AlphaFol came along,

38:04.260 --> 38:06.260
the way you would do this is experimentally,

38:06.260 --> 38:09.260
and it's extremely painstaking expert work

38:09.260 --> 38:10.260
that needs to be done.

38:10.260 --> 38:14.260
And using x-ray crystallography and electron microscopy.

38:14.260 --> 38:18.260
And the rule of thumb is generally that it takes one PhD student,

38:18.260 --> 38:21.260
their whole PhD, to do one protein.

38:21.260 --> 38:23.260
And that's if you get lucky, you can be unlucky.

38:23.260 --> 38:28.260
So it's hard and really painstaking and difficult.

38:28.260 --> 38:31.260
And what happened is that the Nobel Prize winner

38:31.260 --> 38:35.260
Christian Anfinsen, in part of his Nobel lecture in 1972,

38:35.260 --> 38:38.260
so 50 years ago, exactly now,

38:38.260 --> 38:41.260
he conjectured that the 3D structure of proteins

38:41.260 --> 38:44.260
should be fully determined by the amino acid sequence,

38:44.260 --> 38:47.260
i.e. this should be possible this mapping.

38:47.260 --> 38:51.260
And it's a bit like, sometimes this problem is called

38:51.260 --> 38:54.260
like Fermat's Last Theorem equivalent in biology,

38:54.260 --> 38:56.260
because it's a bit like saying this is possible,

38:56.260 --> 38:59.260
but the margin is too small, can't give you the answer.

38:59.260 --> 39:03.260
And so what happened instead is obviously it set off a 50 year quest

39:03.260 --> 39:05.260
in biology, in computational biology,

39:05.260 --> 39:08.260
to try and solve this problem.

39:08.260 --> 39:14.260
And it's been ongoing ever since the 1970s.

39:14.260 --> 39:16.260
So the big question is,

39:16.260 --> 39:19.260
is can protein structure prediction,

39:19.260 --> 39:21.260
the protein structure prediction problem,

39:21.260 --> 39:23.260
which is the specific part of protein folding

39:23.260 --> 39:26.260
that we're interested in, be solved computationally?

39:26.260 --> 39:28.260
Just computationally.

39:28.260 --> 39:32.260
And Leventhal, who is another famous contemporary of Anfinsen,

39:32.260 --> 39:34.260
in the 60s and 70s as well,

39:34.260 --> 39:36.260
he calculated, back of envelope,

39:36.260 --> 39:40.260
that there would be roughly 10 to the 300 possible confirmations,

39:40.260 --> 39:43.260
shapes of an average size protein that it could take.

39:43.260 --> 39:46.260
So 10 to the 300, so that's a good number,

39:46.260 --> 39:48.260
that's ones we like, because it's bigger than go.

39:48.260 --> 39:51.260
And obviously that means exhaustively sampling this

39:51.260 --> 39:53.260
is totally intractable,

39:53.260 --> 39:58.260
but of course the chink of light is that in nature,

39:58.260 --> 40:01.260
in our bodies, physics solves this.

40:01.260 --> 40:06.260
So it can, if proteins spontaneously fold in a matter of seconds,

40:06.260 --> 40:08.260
sometimes milliseconds in the body.

40:08.260 --> 40:12.260
So there's obviously some energy path through this.

40:12.260 --> 40:14.260
So how do we get to this problem?

40:14.260 --> 40:18.260
Well actually it's quite a long winding road for me personally,

40:18.260 --> 40:20.260
for others in the team less so.

40:20.260 --> 40:23.260
But for me, I actually came across the protein folding problem

40:23.260 --> 40:26.260
in the 90s as an undergrad in Cambridge,

40:26.260 --> 40:30.260
because one of my friends in our sort of group

40:30.260 --> 40:33.260
of colleagues was obsessed with this problem.

40:33.260 --> 40:36.260
And he would talk about it, and I remember this very clearly,

40:36.260 --> 40:39.260
every opportunity in the bar playing pool, whatever it was.

40:39.260 --> 40:44.260
If we can crack this, that will open up all sorts of things in biology.

40:44.260 --> 40:46.260
And I sort of listened to him and I was thinking about this,

40:46.260 --> 40:48.260
I was fascinated by the problem as a problem,

40:48.260 --> 40:52.260
and I felt it was actually very well suited to potentially to AI.

40:52.260 --> 40:56.260
Although obviously at the time I didn't know how it could be tackled.

40:56.260 --> 40:58.260
But I filed that away as an interesting thing.

40:58.260 --> 41:01.260
And then it came up again in the late 2000s

41:01.260 --> 41:03.260
when I was doing my postdoc over at MIT.

41:03.260 --> 41:08.260
And this game called Fold It came out from David Baker's lab,

41:08.260 --> 41:10.260
who works on proteins.

41:10.260 --> 41:13.260
And it was a citizen science game, you can see it on the left here.

41:13.260 --> 41:15.260
And what they've done really interestingly

41:15.260 --> 41:18.260
is turn protein folding into a puzzle game.

41:18.260 --> 41:23.260
And they actually got a couple hundred gamers to fold proteins,

41:23.260 --> 41:25.260
bit like playing Tetris or something.

41:25.260 --> 41:29.260
And some of them actually became really good.

41:29.260 --> 41:32.260
And I remember, so of course I was fascinated this

41:32.260 --> 41:34.260
just from games design perspective.

41:34.260 --> 41:36.260
Wouldn't it be amazing if we could design more games

41:36.260 --> 41:39.260
where people played them, they were actually doing useful science

41:39.260 --> 41:41.260
while they were having fun, that would be amazing.

41:41.260 --> 41:43.260
And I think this is still the best example of that.

41:43.260 --> 41:47.260
But also again protein folding was coming up.

41:47.260 --> 41:50.260
And in fact, it turned out that a couple of,

41:50.260 --> 41:53.260
a few really important proteins structures

41:53.260 --> 41:55.260
were found this way by gamers

41:55.260 --> 41:58.260
and published in Nature and Nature Structural Biology.

41:58.260 --> 42:01.260
And so this actually really worked.

42:01.260 --> 42:03.260
And that, when we then got to, you know,

42:03.260 --> 42:05.260
the third piece of the puzzle was doing Go

42:05.260 --> 42:08.260
and trying to sort of think about what we'd done

42:08.260 --> 42:10.260
with intuition and other things, as I mentioned earlier.

42:10.260 --> 42:12.260
And I felt that actually, you know,

42:12.260 --> 42:14.260
if we'd managed to mimic in some sense

42:14.260 --> 42:17.260
the intuition of Go players, master Go players

42:17.260 --> 42:19.260
who spent their entire life studying Go,

42:19.260 --> 42:22.260
you know, maybe one could mimic the intuition of these gamers

42:22.260 --> 42:25.260
who were only, by the way, of course, amateur biologists.

42:25.260 --> 42:28.260
Right? But somehow some of them were able to make

42:28.260 --> 42:30.260
counter-intuitive folds of the backbone

42:30.260 --> 42:33.260
that were, if you just followed an energy landscape

42:33.260 --> 42:35.260
in a greedy fashion, one would not, you know,

42:35.260 --> 42:38.260
reach a local minima or local maxima

42:38.260 --> 42:42.260
and you would not be able to find the right structure.

42:42.260 --> 42:46.260
So it's almost the day after we got back from Korea

42:46.260 --> 42:50.260
we then, you know, I instigated the Alpha Fold project

42:50.260 --> 42:53.260
and I thought it was the right time

42:53.260 --> 42:57.260
to basically start working on this problem.

42:57.260 --> 42:59.260
The other important piece of the puzzle

42:59.260 --> 43:01.260
was this competition called CASP,

43:01.260 --> 43:04.260
which is sometimes thought of as, like,

43:04.260 --> 43:06.260
the Olympics for protein folding,

43:06.260 --> 43:09.260
and it's sort of run every two years in external benchmarks.

43:09.260 --> 43:10.260
It's an amazing thing, actually,

43:10.260 --> 43:12.260
that I think more areas of science should do.

43:12.260 --> 43:15.260
And it's been run sort of religiously

43:15.260 --> 43:17.260
for every two years, for nearly 30 years.

43:17.260 --> 43:20.260
So, you know, huge culos to the organisers,

43:20.260 --> 43:23.260
John Mull and his team for doing this

43:23.260 --> 43:26.260
and organising it so professionally for every two years

43:26.260 --> 43:29.260
without fail for 30 years.

43:29.260 --> 43:32.260
And the cool thing about it is it's a blind prediction assessment.

43:32.260 --> 43:36.260
So there's no way you can accidentally sort of train on test data

43:36.260 --> 43:38.260
or any of these kinds of pitfalls

43:38.260 --> 43:40.260
because at the time when the competition runs

43:40.260 --> 43:42.260
over summer usually every two years,

43:42.260 --> 43:46.260
the experimentalists globally agree to hold back

43:46.260 --> 43:48.260
a few of their structures that they've just found,

43:48.260 --> 43:50.260
but at that point in time they're the only ones

43:50.260 --> 43:52.260
who know what that structure looks like.

43:52.260 --> 43:54.260
They hold back the publication for a couple of months

43:54.260 --> 43:56.260
and they give it to John Mull and his colleagues

43:56.260 --> 43:58.260
to put it into the competition.

43:58.260 --> 43:59.260
And then you get those.

43:59.260 --> 44:02.260
It's quite fun tournament because then, you know,

44:02.260 --> 44:03.260
it's quite exciting.

44:03.260 --> 44:05.260
You get the email and then there's a new structure

44:05.260 --> 44:08.260
that amino acid sequence nobody has ever, you know,

44:08.260 --> 44:09.260
knows the structure of.

44:09.260 --> 44:11.260
And then you have a week to sort of get it back

44:11.260 --> 44:14.260
to the competition organisers before it's published.

44:14.260 --> 44:16.260
And then at the end of that three, four month period,

44:16.260 --> 44:21.260
they obviously score your predictions against the ground truth,

44:21.260 --> 44:24.260
which at that point is published, obviously in peer review journals,

44:24.260 --> 44:26.260
that the experimental ground truth.

44:26.260 --> 44:30.260
And then you get a kind of distance measure between your predictions

44:30.260 --> 44:32.260
and the molecules in that prediction

44:32.260 --> 44:36.260
and where they really are in 3D coordinate space.

44:36.260 --> 44:40.260
So when we started getting involved in this area post 2016,

44:40.260 --> 44:43.260
you know, we looked at CASP and the history of it

44:43.260 --> 44:45.260
and actually they'd been very little progress

44:45.260 --> 44:46.260
for over a decade.

44:46.260 --> 44:48.260
It's sort of the field had stalled.

44:48.260 --> 44:54.260
And this graph here shows you the scores of the winning team

44:54.260 --> 44:56.260
on the hardest category of protein,

44:56.260 --> 44:59.260
where you don't have any evolutionary similar template proteins

44:59.260 --> 45:01.260
to sort of rely on.

45:01.260 --> 45:02.260
So it's called free modelling.

45:02.260 --> 45:04.260
And this is a percentage accuracy.

45:04.260 --> 45:05.260
It's called GDT.

45:05.260 --> 45:07.260
It's a slight nuance of the measure,

45:07.260 --> 45:10.260
but you can think of it as the number of molecules,

45:10.260 --> 45:12.260
the percentage number of molecules you've got roughly

45:12.260 --> 45:15.260
in that place to a certain tolerance, distance tolerance.

45:15.260 --> 45:18.260
And you can see they were hovering around 40% or less,

45:18.260 --> 45:21.260
which is useless for experimentation, right?

45:21.260 --> 45:23.260
Basically, it's pretty much random.

45:23.260 --> 45:26.260
And so that was the average and it hadn't really moved.

45:26.260 --> 45:33.260
And so what we did in 2018 is that we came along with Alpha Fold 1

45:33.260 --> 45:36.260
as our first entry after a couple of years of working on this.

45:36.260 --> 45:39.260
And we sort of, you know, I think we revolutionised the field in a way,

45:39.260 --> 45:42.260
is that for the first time we brought cutting edge machine learning techniques,

45:42.260 --> 45:45.260
the sort of techniques we developed in AlphaGo

45:45.260 --> 45:47.260
and other new ones for this domain.

45:47.260 --> 45:50.260
And we, as the core part of the system,

45:50.260 --> 45:53.260
and we improved the winning scores by 50%.

45:53.260 --> 45:56.260
You know, we got close to 60 GDT here.

45:56.260 --> 45:58.260
And then, of course, we didn't stop there.

45:58.260 --> 46:01.260
We then re-architected based on that knowledge.

46:01.260 --> 46:03.260
We actually tried to push that system further

46:03.260 --> 46:04.260
and it turned out it hit a brick wall,

46:04.260 --> 46:06.260
so we had to go back to the drawing board

46:06.260 --> 46:07.260
with the knowledge that we had,

46:07.260 --> 46:09.260
re-architected with a brand new system.

46:09.260 --> 46:12.260
And then that finally reached in CAS14 in 2020,

46:12.260 --> 46:14.260
atomic accuracy.

46:14.260 --> 46:18.260
So accuracy within the width of an atom, right,

46:18.260 --> 46:20.260
for all the molecules.

46:20.260 --> 46:26.260
So when we look at the scores and the results of CAS14,

46:26.260 --> 46:29.260
what you see here is that Alpha Fold 2,

46:29.260 --> 46:31.260
this is the root mean squared error,

46:31.260 --> 46:36.260
is less than one angstrom error on average.

46:36.260 --> 46:38.260
And, you know, from the 100 or so proteins

46:38.260 --> 46:40.260
that we're supposed to predict.

46:40.260 --> 46:42.260
So, and one angstrom is the, you know,

46:42.260 --> 46:44.260
the width of basically a carbon atom.

46:44.260 --> 46:47.260
So that's finally, that was the magic threshold

46:47.260 --> 46:51.260
that John Moll and others of the organisers said

46:51.260 --> 46:53.260
that they always set out CASP to do,

46:53.260 --> 46:55.260
because that would make you competitive

46:55.260 --> 46:57.260
with experimental techniques,

46:57.260 --> 46:59.260
which are roughly, you know, the best ones

46:59.260 --> 47:01.260
are at that kind of error rate.

47:01.260 --> 47:03.260
So if one could do that computationally,

47:03.260 --> 47:05.260
then suddenly you have a technique that could be,

47:05.260 --> 47:09.260
you could rely on in tandem with experimental instead of.

47:09.260 --> 47:14.260
And so Alpha Fold 2 got an error of 0.96 angstroms,

47:14.260 --> 47:16.260
which was three times more accurate

47:16.260 --> 47:18.260
than the next best system in CAS14,

47:18.260 --> 47:21.260
even though those systems obviously incorporated

47:21.260 --> 47:23.260
the Alpha Fold 1 techniques

47:23.260 --> 47:26.260
that we'd already published by then.

47:26.260 --> 47:28.260
So this led to the CASP organisers and John Moll

47:28.260 --> 47:30.260
declaring that the structure prediction problem

47:30.260 --> 47:34.260
had essentially been solved after all of these years.

47:34.260 --> 47:36.260
And this is what the predictions look like.

47:36.260 --> 47:39.260
So the ground truth is in green,

47:39.260 --> 47:43.260
and you can see the prediction from Alpha Fold 2 in blue.

47:43.260 --> 47:45.260
And you can see firstly proteins are exquisitely beautiful.

47:45.260 --> 47:48.260
It's one thing to note that I've learned over the many years

47:48.260 --> 47:50.260
I've been working on this now.

47:50.260 --> 47:53.260
They're like exquisite little nano machines.

47:53.260 --> 47:56.260
And you can see how accurate the overlays are.

47:56.260 --> 47:58.260
And we were astounded, of course,

47:58.260 --> 48:01.260
when we first got these results back.

48:01.260 --> 48:03.260
And then, you know, there are many,

48:03.260 --> 48:06.260
this is the architecture for Alpha Fold 2,

48:06.260 --> 48:08.260
so you don't have time to go into the details of today,

48:08.260 --> 48:10.260
but there were a huge number of innovations

48:10.260 --> 48:13.260
that were required to make this work.

48:13.260 --> 48:15.260
And the key technical advances were basically,

48:15.260 --> 48:19.260
first of all, I should say there was no silver bullet.

48:19.260 --> 48:23.260
It needed actually 32 component algorithms

48:23.260 --> 48:25.260
described in 60 pages of supplemental information

48:25.260 --> 48:27.260
actually in the paper.

48:27.260 --> 48:29.260
And that was required.

48:29.260 --> 48:31.260
And every single part of that was required.

48:31.260 --> 48:33.260
So we did these ablation analyses,

48:33.260 --> 48:35.260
which sort of took out components to see

48:35.260 --> 48:37.260
if we could get away without having them.

48:37.260 --> 48:40.260
And the result of that was everything was required.

48:40.260 --> 48:43.260
And the three key sort of takeaways of why Alpha Fold 2

48:43.260 --> 48:45.260
was an improvement over Alpha Fold 1

48:45.260 --> 48:48.260
is we made the system fully end-to-end.

48:48.260 --> 48:51.260
So you can think of it as sort of going end-to-end

48:51.260 --> 48:54.260
with a recycling iterative stage.

48:54.260 --> 48:57.260
At the time, it sort of jigs the protein structure

48:57.260 --> 49:00.260
nearer and closer and closer to the final structure

49:00.260 --> 49:02.260
that it's going to predict.

49:02.260 --> 49:04.260
And Alpha Fold 1 system didn't do that.

49:04.260 --> 49:06.260
It went from the amino acid sequence

49:06.260 --> 49:09.260
to this intermediate representation called a dystagram,

49:09.260 --> 49:13.260
which is a pair-wise dystagram of all the protein molecules

49:13.260 --> 49:16.260
and their distance to each of the other molecules,

49:16.260 --> 49:18.260
the other end molecules.

49:18.260 --> 49:20.260
And then from that, we used a different method

49:20.260 --> 49:22.260
to create the 3D structure.

49:22.260 --> 49:25.260
So we went straight for predicting the 3D structure.

49:25.260 --> 49:27.260
And those of you who work in machine learning

49:27.260 --> 49:29.260
will know that generally speaking,

49:29.260 --> 49:31.260
if you can make something end-to-end

49:31.260 --> 49:33.260
and optimize directly for the thing that you're after,

49:33.260 --> 49:36.260
usually your system will have better performance.

49:36.260 --> 49:38.260
We used an attention-based neural network

49:38.260 --> 49:41.260
to infer this implicit graph structure

49:41.260 --> 49:46.260
of the residues, of the amino acid sequences.

49:46.260 --> 49:49.260
In Alpha Fold 1, we used a convolutional neural net,

49:49.260 --> 49:51.260
which was sort of borrowed from computer vision.

49:51.260 --> 49:53.260
And if you think about it,

49:53.260 --> 49:56.260
that was introducing the wrong bias into protein folding

49:56.260 --> 49:59.260
because with computer vision, pixels next to each other

49:59.260 --> 50:02.260
are obviously going to be correlated in an image, in some sense.

50:02.260 --> 50:04.260
So convolutions make sense.

50:04.260 --> 50:08.260
But actually, for a protein, the amino acid sequence,

50:08.260 --> 50:10.260
residues that are next to each other

50:10.260 --> 50:12.260
or close to each other on the string of letters

50:12.260 --> 50:14.260
may not end up being near each other

50:14.260 --> 50:16.260
once you get the full 3D fold,

50:16.260 --> 50:19.260
or things very far away could end up folding over near each other.

50:19.260 --> 50:22.260
So, in a way, we were giving it the wrong biases,

50:22.260 --> 50:24.260
so we actually had to remove that.

50:24.260 --> 50:28.260
And then finally, we built in some biological and evolutionary

50:28.260 --> 50:30.260
and physics constraints into the system

50:30.260 --> 50:32.260
without impacting the learning.

50:32.260 --> 50:34.260
And again, usually, so you can think of it

50:34.260 --> 50:36.260
as a little bit of a hybrid system,

50:36.260 --> 50:38.260
that usually, if you put in constraints,

50:38.260 --> 50:40.260
that impacts the learning.

50:40.260 --> 50:43.260
We managed to do that without that.

50:43.260 --> 50:45.260
So this was a huge research effort over sort of five years,

50:45.260 --> 50:48.260
took about 20 people at its maximum,

50:48.260 --> 50:50.260
and it was a truly multidisciplinary effort.

50:50.260 --> 50:53.260
So we needed biologists and physicists and chemists

50:53.260 --> 50:55.260
as well as machine learners.

50:55.260 --> 50:57.260
And I think that's an interesting lesson, maybe,

50:57.260 --> 51:00.260
to learn about cross-disciplinary work in AI for Sciences,

51:00.260 --> 51:04.260
is you need the experts also from the domain.

51:04.260 --> 51:06.260
And then the final, maybe interesting point to note on this,

51:06.260 --> 51:09.260
is that normally, we're always after generality,

51:09.260 --> 51:13.260
so you can see that from the journey from AlphaGo to AlphaZero,

51:13.260 --> 51:15.260
was we increasingly made things general.

51:15.260 --> 51:17.260
You start with performance,

51:17.260 --> 51:19.260
then you start throwing things out of that system

51:19.260 --> 51:21.260
to try and make it simpler and more elegant,

51:21.260 --> 51:23.260
and that usually makes it more general,

51:23.260 --> 51:25.260
as you understand what it is that you're doing.

51:25.260 --> 51:29.260
But that's because Go and Chess and those things

51:29.260 --> 51:32.260
were testbeds for what we wanted to do.

51:32.260 --> 51:34.260
If you are trying to solve a real-world problem

51:34.260 --> 51:38.260
that really matters to other scientists or health,

51:38.260 --> 51:40.260
or in this case, you know, biology,

51:40.260 --> 51:44.260
then actually, you might as well throw the kitchen sink at it,

51:44.260 --> 51:47.260
because you actually are really after the output itself,

51:47.260 --> 51:49.260
in this case, protein structures.

51:49.260 --> 51:51.260
And that's what we did here.

51:51.260 --> 51:53.260
We really threw everything we had at it,

51:53.260 --> 51:57.260
and it's, I think, the most complex system that we've ever built.

51:57.260 --> 52:00.260
Other things to note about this system is that it's also,

52:00.260 --> 52:02.260
AlphaFold 1 was relatively slow,

52:02.260 --> 52:06.260
took a few weeks of compute time to do a protein.

52:06.260 --> 52:10.260
AlphaFold 2 took two weeks to train the whole system

52:10.260 --> 52:14.260
on a relatively modest setup of eight TPUs or 150 GPUs,

52:14.260 --> 52:17.260
which, by modern-day machine learning standards, is quite small.

52:17.260 --> 52:19.260
And then the inference, the predictions,

52:19.260 --> 52:22.260
can be done lightening fast in an order of minutes,

52:22.260 --> 52:27.260
sometimes seconds for an average protein on a single GPU.

52:27.260 --> 52:30.260
So when we did this, AlphaFold 2, we announced the results,

52:30.260 --> 52:32.260
published the methods.

52:32.260 --> 52:36.260
Over Christmas, that Christmas, this is back in 2020,

52:36.260 --> 52:39.260
we were thinking, okay, how should we give access to the system

52:39.260 --> 52:41.260
to biologists around the world?

52:41.260 --> 52:44.260
And normally what you do is that you set up a server,

52:44.260 --> 52:48.260
people, biologists, send you their amino acid sequences,

52:48.260 --> 52:50.260
and then you give back, a few days later,

52:50.260 --> 52:52.260
you might give them back the prediction.

52:52.260 --> 52:55.260
But actually what we realised, because AlphaFold 2 was so fast,

52:55.260 --> 52:59.260
we could actually just fold everything ourselves in one go.

52:59.260 --> 53:01.260
So we just fold all proteins.

53:01.260 --> 53:04.260
And we'll start with the human proteome,

53:04.260 --> 53:08.260
just like the human genome equivalent, but in protein space.

53:08.260 --> 53:11.260
And so that's what we did over the Christmas.

53:11.260 --> 53:14.260
We folded the whole human proteome.

53:14.260 --> 53:17.260
And so, which is another thing I love about AI and computing,

53:17.260 --> 53:19.260
is you can have your Christmas lunch,

53:19.260 --> 53:22.260
and while you're doing that, offer our AIs doing something useful

53:22.260 --> 53:24.260
for the world.

53:24.260 --> 53:26.260
So the human proteome, so we published that as well

53:26.260 --> 53:29.260
in the summer of 21 last summer.

53:29.260 --> 53:32.260
So AlphaFold 2, we predicted that every protein

53:32.260 --> 53:35.260
in the human body is around 20,000 proteins,

53:35.260 --> 53:38.260
represented, obviously, expressed by the human genome.

53:38.260 --> 53:41.260
And at the point where we did this, experiments,

53:41.260 --> 53:44.260
30 years of experiments, 30, 40 years of experiments,

53:44.260 --> 53:48.260
had covered about 17% of the human proteome.

53:48.260 --> 53:51.260
And we more than doubled that overnight

53:51.260 --> 53:54.260
in terms of very high accuracy structures.

53:54.260 --> 53:57.260
Obviously we folded all of them, but very high accuracy,

53:57.260 --> 53:59.260
so that's less than one angstrom error.

53:59.260 --> 54:02.260
They're sort of up to experimental quality.

54:02.260 --> 54:04.260
We went to 36%.

54:04.260 --> 54:07.260
And 58% at high accuracy, where we call high accuracy

54:07.260 --> 54:11.260
when the backbone is mostly where you can be confident in.

54:11.260 --> 54:14.260
But the side chains may be slightly out.

54:14.260 --> 54:17.260
And then, of course, the question is what about the rest,

54:17.260 --> 54:19.260
the other 42%.

54:19.260 --> 54:22.260
And it may be that some of those AlphaFold 2

54:22.260 --> 54:24.260
is just bad at, but increasingly,

54:24.260 --> 54:26.260
and this is an open research question,

54:26.260 --> 54:29.260
when we look at it with biologists, and biologists often send us in results,

54:29.260 --> 54:31.260
like, I look at this one folded really well,

54:31.260 --> 54:33.260
or this one didn't fold well,

54:33.260 --> 54:36.260
we often find that the ones that didn't fold well

54:36.260 --> 54:39.260
were actually what's called unstructured in isolation.

54:39.260 --> 54:42.260
So they're disorder, intrinsically disorder proteins,

54:42.260 --> 54:45.260
which means that until you know what they interact with,

54:45.260 --> 54:47.260
they're basically squiggly bits of string.

54:47.260 --> 54:50.260
And then presumably, when they interact with something in the body,

54:50.260 --> 54:53.260
they then, another protein usually, they'll then form a shape.

54:53.260 --> 54:56.260
But we don't know what that shape is in isolation, right?

54:56.260 --> 54:59.260
We may not even know what it interacts with at this stage.

54:59.260 --> 55:02.260
So, actually, people have turned this around now

55:02.260 --> 55:05.260
to use it as a disordered protein predictor.

55:05.260 --> 55:07.260
So, where AlphaFold doesn't do well,

55:07.260 --> 55:10.260
perhaps that's pretty good evidence that it's a disordered protein,

55:10.260 --> 55:13.260
which, of course, is very important in things like disease,

55:13.260 --> 55:18.260
Alzheimer's, other things are thought to be to do with badly folded or disordered proteins.

55:20.260 --> 55:23.260
One of the other things we did, which was a nice innovation for AlphaFold,

55:23.260 --> 55:27.260
was have the system predicted its own confidence in its own predictions.

55:27.260 --> 55:30.260
And the reason we did this is we wanted biologists to use this,

55:30.260 --> 55:32.260
who maybe would not care about the machine learning techniques

55:32.260 --> 55:35.260
or not understand them, or frankly, it would be irrelevant to them.

55:35.260 --> 55:37.260
They would just be interested in the structure.

55:37.260 --> 55:40.260
And we wanted to make sure that they were easily able to evaluate

55:40.260 --> 55:44.260
the quality of that prediction and what parts of it they could rely on.

55:44.260 --> 55:48.260
And which other parts they maybe need to check experimentally.

55:48.260 --> 55:52.260
So what we did is AlphaFold, basically, we produced predictions,

55:52.260 --> 55:57.260
there were split into three thresholds,

55:57.260 --> 56:00.260
over 90 was what we call very high accuracy,

56:00.260 --> 56:03.260
so less than one angstrom error, experimental quality,

56:03.260 --> 56:05.260
greater than 70 was the backbone's correct,

56:05.260 --> 56:08.260
and then less than 50 may be these red regions.

56:08.260 --> 56:11.260
So you can see in the database that's what they look like.

56:11.260 --> 56:14.260
It's something that should not be trusted.

56:14.260 --> 56:19.260
We did a further 20 model organisms covering all of the critical model organisms

56:19.260 --> 56:23.260
used in research, and also some important other ones in disease,

56:23.260 --> 56:28.260
like tuberculosis, and also agriculture, like wheat and rice.

56:28.260 --> 56:34.260
And a lot of these proteomes are much less covered than the human proteome.

56:34.260 --> 56:36.260
Of course, the human one is where the most effort is being,

56:36.260 --> 56:37.260
that's at 17%.

56:37.260 --> 56:40.260
For some of these organisms, it's like less than 1%.

56:40.260 --> 56:43.260
So for the researchers in those plant scientists and other things,

56:43.260 --> 56:46.260
this is a huge boon for them because they would never have the resources

56:46.260 --> 56:51.260
to spend that time to crystallise the proteins they're interested in.

56:51.260 --> 56:54.260
We then teamed up with Emble EBI,

56:54.260 --> 56:57.260
the European Bioinformatics Institute at Cambridge,

56:57.260 --> 56:59.260
and they're amazing as a partnership team.

56:59.260 --> 57:02.260
They host a lot of the biggest databases around the world already,

57:02.260 --> 57:05.260
and we thought the best way to host all this data

57:05.260 --> 57:08.260
is to just give it to them and allow them to host it

57:08.260 --> 57:11.260
and plug it into the mainstream of biology tools.

57:11.260 --> 57:13.260
And so we had a great collaboration with them,

57:13.260 --> 57:17.260
and then we basically released all this data for free and unrestricted access

57:17.260 --> 57:22.260
for any use, industrial or academic, because it's so completely free.

57:22.260 --> 57:26.260
And it's amazing to see the impact of that,

57:26.260 --> 57:30.260
and we tried to sort of maximise the scientific impact of this

57:30.260 --> 57:32.260
by releasing it in that way.

57:32.260 --> 57:35.260
The other thing we did do, and I want to touch this on this at the end,

57:35.260 --> 57:37.260
is think about the safety and ethics of this,

57:37.260 --> 57:42.260
and we consulted with over 30 experts in various areas of biology

57:42.260 --> 57:45.260
bioinformatics, biosecurity and pharma

57:45.260 --> 57:49.260
to check that this was going to be okay to release this type of information.

57:49.260 --> 57:52.260
And they all came back with that they were not worried about this,

57:52.260 --> 57:55.260
but they were potentially worried about future things.

57:55.260 --> 57:57.260
So that's something that we bear in mind.

57:57.260 --> 58:00.260
There are now a million predictions in the database today.

58:00.260 --> 58:02.260
I just want to call out one thing.

58:02.260 --> 58:07.260
We specially, ourselves, we specially prioritise neglected tropical diseases,

58:07.260 --> 58:10.260
because those are the ones that affect the developing world,

58:10.260 --> 58:12.260
the poorest people in the world the most,

58:12.260 --> 58:15.260
and they're the least researched, because of course there's no money in it for pharma companies,

58:15.260 --> 58:19.260
so that often it's NGOs and non-profits that have to do the work there.

58:19.260 --> 58:21.260
So for them, it's amazing to get all the structures,

58:21.260 --> 58:23.260
because they can go straight to drug discovery

58:23.260 --> 58:27.260
without having to go to the intermediate step of finding these structures.

58:27.260 --> 58:29.260
So we prioritise all these diseases,

58:29.260 --> 58:33.260
and including ones that we've got being given from the WHO

58:33.260 --> 58:36.260
about potential future pathogens.

58:36.260 --> 58:38.260
And what's the community done with AlphaFold already?

58:38.260 --> 58:43.260
We've seen just in nine months or 10 months incredible amount of work has been done.

58:43.260 --> 58:46.260
This is really cool on the left here with some colleagues at Emble.

58:46.260 --> 58:51.260
They used AlphaFold and Experiment to combine with their experimental data

58:51.260 --> 58:54.260
to put together what's called the nuclear pore complex,

58:54.260 --> 58:56.260
which is one of the biggest proteins in the body.

58:56.260 --> 58:58.260
It's massive for a protein,

58:58.260 --> 59:01.260
and what it is is it's a little gateway into the nucleus of your cell,

59:01.260 --> 59:04.260
and it opens and closes to let things in.

59:04.260 --> 59:07.260
And they were able to, you know, it's beautiful if you look at it,

59:07.260 --> 59:10.260
able to put it all together and then visualise it.

59:10.260 --> 59:14.260
I talked about this disorder predictor, WHO top 30 pathogens,

59:14.260 --> 59:17.260
and actually interestingly, it's helped experimentalists,

59:17.260 --> 59:19.260
the ones that benefited first from this,

59:19.260 --> 59:25.260
because they can combine this with their maybe some low resolution images they have,

59:25.260 --> 59:28.260
and if they have two sources of information, they can then make a sharp prediction

59:28.260 --> 59:31.260
from their maybe their slightly lower resolution experimental data,

59:31.260 --> 59:35.260
and then a computational prediction.

59:35.260 --> 59:39.260
So it's been really gratifying to see hundreds of papers now

59:39.260 --> 59:43.260
and applications already with being used for AlphaFold,

59:43.260 --> 59:46.260
also in industry too for drug discovery.

59:46.260 --> 59:48.260
So what has the impact been?

59:48.260 --> 59:53.260
So we already have 500,000 researchers have used the database.

59:53.260 --> 59:55.260
We think that's almost every biologist in the world

59:55.260 --> 59:58.260
has probably looked up their proteins they're interested in.

59:58.260 --> 01:00:01.260
190 countries, 1.5 million structures viewed,

01:00:01.260 --> 01:00:04.260
and already over 3,000 citations,

01:00:04.260 --> 01:00:09.260
and we've had some nice accolades along the way from science and nature on the method.

01:00:09.260 --> 01:00:14.260
And then over the next year, we plan to fold every protein, you know,

01:00:14.260 --> 01:00:16.260
in known to science, which is in Uniprot,

01:00:16.260 --> 01:00:19.260
which is the massive database that has all the genetic sequences,

01:00:19.260 --> 01:00:21.260
and there's over 100 million proteins known to science,

01:00:21.260 --> 01:00:24.260
and we're steadily sort of progressing through that right now,

01:00:24.260 --> 01:00:27.260
and we'll be releasing that over time.

01:00:27.260 --> 01:00:29.260
So stepping back then, what does this mean?

01:00:29.260 --> 01:00:33.260
I think that maybe, you know, we're entering a new era

01:00:33.260 --> 01:00:35.260
of what I would like to call digital biology.

01:00:35.260 --> 01:00:39.260
So I think the way I think about biology is that at the most fundamental level,

01:00:39.260 --> 01:00:41.260
it's an information processing system,

01:00:41.260 --> 01:00:46.260
albeit an exquisitely complex and emergent one.

01:00:46.260 --> 01:00:49.260
And I think of it as maybe the potential,

01:00:49.260 --> 01:00:52.260
the perfect sort of regime for AI to be useful in,

01:00:52.260 --> 01:00:56.260
because, you know, one thing I think of it analogous to is in physics,

01:00:56.260 --> 01:00:59.260
you know, we use mathematics to describe physical phenomena,

01:00:59.260 --> 01:01:02.260
and it's been extraordinarily successful in doing that.

01:01:02.260 --> 01:01:05.260
Of course, mathematics can also be applied to biology

01:01:05.260 --> 01:01:07.260
and has been applied successfully in many domains,

01:01:07.260 --> 01:01:10.260
but I think a lot of these emergent and complex phenomena

01:01:10.260 --> 01:01:13.260
are just too complicated to be described with a few equations, right?

01:01:13.260 --> 01:01:16.260
I just don't really see how you can say,

01:01:16.260 --> 01:01:20.260
come up with, you know, Kepler's laws of motion just from other cell, right?

01:01:20.260 --> 01:01:23.260
How would one do that? You know, just a few differential equations.

01:01:23.260 --> 01:01:25.260
It doesn't seem to me likely.

01:01:25.260 --> 01:01:29.260
And I think maybe a learn model is a better way to approach that.

01:01:29.260 --> 01:01:32.260
And I think, and I hope that AlphaFold is a proof of concept

01:01:32.260 --> 01:01:34.260
that this may be possible,

01:01:34.260 --> 01:01:38.260
and maybe usher, can help usher in this new dawn of digital biology.

01:01:38.260 --> 01:01:40.260
And our attempts to go further in that space

01:01:40.260 --> 01:01:42.260
is obviously we're researching further at DeepMind,

01:01:42.260 --> 01:01:45.260
and the science team, we sort of doubled down on all these things

01:01:45.260 --> 01:01:47.260
within the biology team at DeepMind.

01:01:47.260 --> 01:01:50.260
And we've also spun out a new company, Isomorphic Labs,

01:01:50.260 --> 01:01:54.260
to specifically build on this work and other related work,

01:01:54.260 --> 01:01:58.260
specifically for drug discovery to accelerate drug discovery,

01:01:58.260 --> 01:02:00.260
which we hope, using computational and AI methods,

01:02:00.260 --> 01:02:03.260
can maybe be an order of magnitude quicker.

01:02:03.260 --> 01:02:05.260
Currently, you know, it takes an average of 10 years

01:02:05.260 --> 01:02:09.260
to go from identifying a target to a candidate drug.

01:02:10.260 --> 01:02:13.260
So just to start closing then, I just, you know,

01:02:13.260 --> 01:02:15.260
there isn't time to go into this, but it's for us,

01:02:15.260 --> 01:02:17.260
it's been like a renaissance year in some sense.

01:02:17.260 --> 01:02:21.260
I've been having so much fun ticking off all of my sort of

01:02:21.260 --> 01:02:23.260
childhood dream projects,

01:02:23.260 --> 01:02:26.260
infusion and quantum chemistry and conjectures in maths,

01:02:26.260 --> 01:02:28.260
material science, weather prediction.

01:02:28.260 --> 01:02:31.260
This has all become reality now in the last year

01:02:31.260 --> 01:02:34.260
of applying it to important problems in each of these domains

01:02:34.260 --> 01:02:36.260
and, you know, publishing nice and important work

01:02:36.260 --> 01:02:38.260
in each of these areas.

01:02:38.260 --> 01:02:41.260
In applications, of course, there are lots of amazing

01:02:41.260 --> 01:02:43.260
industrial applications that we've been doing,

01:02:43.260 --> 01:02:45.260
and we have an applied team at DeepMind

01:02:45.260 --> 01:02:48.260
that works with Google product teams to incorporate

01:02:48.260 --> 01:02:51.260
all of our research into hundreds of products now at Google.

01:02:51.260 --> 01:02:54.260
Pretty much every product you use of Google's

01:02:54.260 --> 01:02:56.260
will have some DeepMind technology in it.

01:02:56.260 --> 01:02:59.260
Some of the ones I just want to call out are our data centre work

01:02:59.260 --> 01:03:02.260
and energy optimisation of data centres and the energy they use

01:03:02.260 --> 01:03:04.260
and the cooling systems they use,

01:03:04.260 --> 01:03:06.260
and we're looking at applying that to grid scale now,

01:03:06.260 --> 01:03:10.260
WaveNet, which is the best text-to-speech system in the world.

01:03:10.260 --> 01:03:13.260
So any device that you talk to that talks back to you

01:03:13.260 --> 01:03:17.260
will be using WaveNet to have really realistic voices.

01:03:17.260 --> 01:03:21.260
Even interesting things like better video compression for YouTube.

01:03:21.260 --> 01:03:24.260
We can save 4% of the bit rate that is used

01:03:24.260 --> 01:03:27.260
whilst maintaining video quality

01:03:27.260 --> 01:03:29.260
and also things like recommendation systems,

01:03:29.260 --> 01:03:32.260
but there's just too many to mention, actually.

01:03:32.260 --> 01:03:34.260
And then, of course, very in vogue now,

01:03:34.260 --> 01:03:36.260
and we have a ton of work on this area,

01:03:36.260 --> 01:03:38.260
but it will be a whole talk in itself,

01:03:38.260 --> 01:03:41.260
is large models, and we have our own really cool large models

01:03:41.260 --> 01:03:45.260
that alpha code that can programme from a text description

01:03:45.260 --> 01:03:46.260
and write code.

01:03:46.260 --> 01:03:49.260
It's still amazing to me in competitive programming level.

01:03:49.260 --> 01:03:52.260
Chinchilla, which is our large language model

01:03:52.260 --> 01:03:54.260
that is computer-efficient.

01:03:54.260 --> 01:03:57.260
Flamingo, that's our vision language combined model

01:03:57.260 --> 01:03:58.260
that can describe images,

01:03:58.260 --> 01:04:01.260
and then Gata, our latest model that is super general,

01:04:01.260 --> 01:04:03.260
can do robotics, video games,

01:04:03.260 --> 01:04:07.260
all sorts of things, language just with one model.

01:04:07.260 --> 01:04:09.260
So this is all very exciting,

01:04:09.260 --> 01:04:12.260
but I just want to end my last couple of slides

01:04:12.260 --> 01:04:14.260
with a bit about ethics,

01:04:14.260 --> 01:04:17.260
because obviously this is hosted by the Institute of Ethics,

01:04:17.260 --> 01:04:20.260
and it's a very important topic,

01:04:20.260 --> 01:04:22.260
and not just because of that,

01:04:22.260 --> 01:04:25.260
but it's also what the Tana lectures are about, too.

01:04:25.260 --> 01:04:28.260
So we think a lot about pioneering responsibly.

01:04:28.260 --> 01:04:32.260
This is actually two of our values at DeepMind combined,

01:04:32.260 --> 01:04:35.260
pioneering and being responsible.

01:04:35.260 --> 01:04:38.260
I hope I've convinced you and you hope you will realise

01:04:38.260 --> 01:04:41.260
that AI is this incredible potential to help

01:04:41.260 --> 01:04:44.260
with some of humanity's greatest challenges.

01:04:44.260 --> 01:04:46.260
I think disease, climate,

01:04:46.260 --> 01:04:48.260
all of these things could be in scope,

01:04:48.260 --> 01:04:52.260
but obviously AI has to be built responsibly and safely,

01:04:52.260 --> 01:04:55.260
and we have to make sure the people who are building these things,

01:04:55.260 --> 01:04:57.260
it's used for the benefit of everyone.

01:04:57.260 --> 01:05:00.260
So we've had this sort of front of mind

01:05:00.260 --> 01:05:02.260
from the beginning of DeepMind,

01:05:02.260 --> 01:05:05.260
and as with any powerful technology,

01:05:05.260 --> 01:05:07.260
and I think AI is no different,

01:05:07.260 --> 01:05:09.260
although it may be more general and more powerful

01:05:09.260 --> 01:05:11.260
than any that has gone before,

01:05:11.260 --> 01:05:14.260
whether or not it's beneficial or harmful to us in society,

01:05:14.260 --> 01:05:17.260
depends on how we deploy it and how we use it,

01:05:17.260 --> 01:05:20.260
and what sorts of things we decide to use it for.

01:05:20.260 --> 01:05:23.260
And I think it's important that we have a really wide debate

01:05:23.260 --> 01:05:26.260
about that at places like this and the Institute of Ethics.

01:05:26.260 --> 01:05:29.260
I'm very excited to see that being set up

01:05:29.260 --> 01:05:33.260
and for us to interact with the new Institute.

01:05:33.260 --> 01:05:36.260
Here, just one mention is that DNA has been really critical,

01:05:36.260 --> 01:05:39.260
and we've been pushing very hard on this the last few years,

01:05:39.260 --> 01:05:41.260
and I think it's critical to this,

01:05:41.260 --> 01:05:44.260
to make sure we get the broadest possible input

01:05:44.260 --> 01:05:47.260
into the design and deployment decisions of these systems,

01:05:47.260 --> 01:05:51.260
especially for the people that this affects the most,

01:05:51.260 --> 01:05:53.260
that these systems affect the most.

01:05:53.260 --> 01:05:55.260
That's something we've been pushing very hard on.

01:05:55.260 --> 01:05:57.260
There's still a lot more work to do,

01:05:57.260 --> 01:06:00.260
and there's still a lot more progress at DeepMind,

01:06:00.260 --> 01:06:03.260
and we've been also doing that with all of our sponsorship that we do.

01:06:03.260 --> 01:06:06.260
We've now done nearly $50 million worth of sponsorship

01:06:06.260 --> 01:06:09.260
of scholarships, diversity scholarships, chairs,

01:06:09.260 --> 01:06:11.260
and academic institutions and projects,

01:06:11.260 --> 01:06:14.260
and also funding things like the Deep Learning in Darba,

01:06:14.260 --> 01:06:17.260
which is Africa's biggest conference on machine learning.

01:06:17.260 --> 01:06:21.260
I'm really proud to say that a lot of DeepMinders helped set that up.

01:06:21.260 --> 01:06:24.260
And so there's many, many things that we're doing across the industry

01:06:24.260 --> 01:06:27.260
and act as a role model for the rest of industry.

01:06:28.260 --> 01:06:30.260
So then on ethics and safety,

01:06:30.260 --> 01:06:32.260
this has always been central to our mission,

01:06:32.260 --> 01:06:35.260
because you saw our audacious mission at the start,

01:06:35.260 --> 01:06:39.260
and we, even back in 2010 in our little attic room,

01:06:39.260 --> 01:06:41.260
we were planning for success,

01:06:41.260 --> 01:06:44.260
and of course we had to think through as scientists

01:06:44.260 --> 01:06:46.260
what does success mean, what will the world look like,

01:06:46.260 --> 01:06:48.260
and obviously if one thinks that through

01:06:48.260 --> 01:06:51.260
and it's becoming obvious now in 2022,

01:06:51.260 --> 01:06:54.260
but it was obvious to us then in 2010

01:06:54.260 --> 01:06:56.260
that this would have to be critical,

01:06:56.260 --> 01:06:59.260
that it would be really important questions

01:06:59.260 --> 01:07:01.260
that would have to be addressed.

01:07:01.260 --> 01:07:04.260
And part of that, so we've been doing this in the background all along,

01:07:04.260 --> 01:07:06.260
and we'll be talking more about this work probably in future.

01:07:06.260 --> 01:07:09.260
We were instrumental in drafting Google's AI principles,

01:07:09.260 --> 01:07:11.260
which are now publicly available,

01:07:11.260 --> 01:07:14.260
and they were partly based on our original ethics charter

01:07:14.260 --> 01:07:16.260
that we've had from the very beginning of DeepMind.

01:07:16.260 --> 01:07:19.260
And the aim of these principles, and you can look them up later

01:07:19.260 --> 01:07:21.260
if you want to look at what they say,

01:07:21.260 --> 01:07:24.260
is obviously to help realise the far-ranging benefits

01:07:24.260 --> 01:07:26.260
that clearly AI could have for everyone

01:07:26.260 --> 01:07:31.260
whilst identifying and mitigating potential risks and harms ahead of time.

01:07:31.260 --> 01:07:34.260
And we continue to try and act as thought leadership

01:07:34.260 --> 01:07:37.260
for the AI community on many of these topics,

01:07:37.260 --> 01:07:40.260
strategy risks, ethics, and safety.

01:07:40.260 --> 01:07:42.260
So what should we do then?

01:07:42.260 --> 01:07:44.260
And I just want to end with this last slide here,

01:07:44.260 --> 01:07:48.260
is what I think we should not do is move fast and break things,

01:07:48.260 --> 01:07:50.260
sort of the Silicon Valley trope.

01:07:50.260 --> 01:07:53.260
And I think we've seen the consequence of that playing out.

01:07:53.260 --> 01:07:57.260
It can be very extraordinarily effective to get powerful systems

01:07:57.260 --> 01:07:59.260
and growth and other things,

01:07:59.260 --> 01:08:03.260
but I do not think it's the right way to address really powerful

01:08:03.260 --> 01:08:07.260
potential dual-use technologies like AI.

01:08:07.260 --> 01:08:12.260
And the problem with it is that one of the things that falls out

01:08:12.260 --> 01:08:16.260
of moving fast and break things is actually doing live A-B testing in the world,

01:08:16.260 --> 01:08:18.260
with your minimum viable products and other things.

01:08:18.260 --> 01:08:21.260
And of course, the question is, if one does that,

01:08:21.260 --> 01:08:26.260
where does the option B turns out to be a terrible option?

01:08:26.260 --> 01:08:28.260
Well, where does the harm of that happen?

01:08:28.260 --> 01:08:30.260
Well, it resides in society, doesn't it?

01:08:30.260 --> 01:08:32.260
That pays the cost of your learning,

01:08:32.260 --> 01:08:34.260
because you've done it in the world.

01:08:34.260 --> 01:08:39.260
And it's probably fine if you're just doing a little gaming app

01:08:39.260 --> 01:08:41.260
or photo app or something,

01:08:41.260 --> 01:08:43.260
but we already see with social networks,

01:08:43.260 --> 01:08:46.260
it's not fine when you're at billion-user scale

01:08:46.260 --> 01:08:50.260
and things really matter in terms of your A-B testing.

01:08:50.260 --> 01:08:54.260
I don't think it's responsible to do that.

01:08:54.260 --> 01:08:56.260
So what should we do instead?

01:08:56.260 --> 01:09:01.260
Well, fortunately, we already have another method,

01:09:01.260 --> 01:09:03.260
which I think would be better, the scientific method,

01:09:03.260 --> 01:09:07.260
which I do think is probably maybe humanity's greatest idea ever.

01:09:07.260 --> 01:09:09.260
And I think it can apply here.

01:09:09.260 --> 01:09:12.260
And I think we should use the scientific method

01:09:12.260 --> 01:09:15.260
when we're approaching how to deal with these

01:09:15.260 --> 01:09:19.260
very powerful, incredible potential technologies.

01:09:19.260 --> 01:09:22.260
And what does the scientific method involve here in this domain?

01:09:22.260 --> 01:09:25.260
Well, it's sort of thoughtful deliberation and thought

01:09:25.260 --> 01:09:28.260
ahead of time and foresight ahead of time,

01:09:28.260 --> 01:09:30.260
where you have hypothesis generation

01:09:30.260 --> 01:09:32.260
on what might happen if one were to be successful

01:09:32.260 --> 01:09:34.260
with what you're trying to do, right?

01:09:34.260 --> 01:09:38.260
So how about we think about that ahead of time, not afterwards?

01:09:38.260 --> 01:09:42.260
Then there's rigorous and careful and control testing.

01:09:42.260 --> 01:09:44.260
I think that's one of the main things I learned from my PhD,

01:09:44.260 --> 01:09:48.260
apart from all the neuroscience, was also the value of control tests.

01:09:48.260 --> 01:09:50.260
I don't think you can really understand.

01:09:50.260 --> 01:09:52.260
In a way, I think when I started my PhD, at least,

01:09:52.260 --> 01:09:55.260
I was all about what's the condition of interest,

01:09:55.260 --> 01:09:58.260
and that's the thing that you're going to make your new advance with.

01:09:58.260 --> 01:10:00.260
But actually, you can't conclude anything, of course,

01:10:00.260 --> 01:10:02.260
unless you have good controls.

01:10:02.260 --> 01:10:05.260
And I think that's something I don't think engineers get

01:10:05.260 --> 01:10:07.260
first time around, actually.

01:10:07.260 --> 01:10:10.260
But scientists and researchers, of course, do get that,

01:10:10.260 --> 01:10:12.260
because that's one of the things that you learn

01:10:12.260 --> 01:10:14.260
from doing a research PhD.

01:10:14.260 --> 01:10:16.260
So control testing in controlled environments,

01:10:16.260 --> 01:10:18.260
not out in the world,

01:10:18.260 --> 01:10:21.260
until you better understand what it is that you're doing.

01:10:21.260 --> 01:10:24.260
So, of course, one updates on empirical data,

01:10:24.260 --> 01:10:26.260
obviously ideally with peer review,

01:10:26.260 --> 01:10:28.260
so you get critique from the outside

01:10:28.260 --> 01:10:30.260
and people who are independent from your work,

01:10:30.260 --> 01:10:33.260
all of these things that are standard in the scientific method, right,

01:10:33.260 --> 01:10:35.260
but are not standard in engineering.

01:10:35.260 --> 01:10:37.260
And all of this is in service

01:10:37.260 --> 01:10:39.260
of getting a better understanding of the system

01:10:39.260 --> 01:10:42.260
before one deploys it at scale, right,

01:10:42.260 --> 01:10:45.260
and then maybe you find out something.

01:10:45.260 --> 01:10:49.260
So my view is that as we approach artificial general intelligence,

01:10:49.260 --> 01:10:52.260
and it's a super exciting moment in time,

01:10:52.260 --> 01:10:55.260
as you can hopefully get from my talk

01:10:55.260 --> 01:10:57.260
and my excitement over that,

01:10:57.260 --> 01:11:00.260
but we need to treat it with the respect and precaution

01:11:00.260 --> 01:11:02.260
and sort of humblness, I would say,

01:11:02.260 --> 01:11:05.260
that the technology of this magnitude demands.

01:11:05.260 --> 01:11:08.260
And I think that's what we are trying to be at the forefront on,

01:11:08.260 --> 01:11:12.260
and I think I'll be talking a lot more about this in the future.

01:11:12.260 --> 01:11:17.260
So I'll just end by on the sort of going back to the science question.

01:11:17.260 --> 01:11:20.260
I think if we get AI right,

01:11:20.260 --> 01:11:22.260
it could potentially be the greatest and most beneficial

01:11:22.260 --> 01:11:24.260
technology humanity has ever invented.

01:11:24.260 --> 01:11:27.260
And I think of AI as this ultimate general purpose tool

01:11:27.260 --> 01:11:31.260
to help us as scientists understand the universe better

01:11:31.260 --> 01:11:33.260
and perhaps our place in it.

01:11:33.260 --> 01:11:35.260
Thank you.

01:11:35.260 --> 01:11:38.260
APPLAUSE

01:11:58.260 --> 01:12:01.260
Well, thank you, Dennis, for that extraordinary tour de force.

01:12:01.260 --> 01:12:04.260
We do have a little time for questions.

01:12:04.260 --> 01:12:07.260
But we wanted to give you the chance to kind of give us

01:12:07.260 --> 01:12:09.260
that sense of your vision.

01:12:09.260 --> 01:12:13.260
Now, we've got an opportunity to have questions from the audience.

01:12:13.260 --> 01:12:16.260
Got to wait for the microphone to be handed to them

01:12:16.260 --> 01:12:18.260
and to stand up if possible when asking questions,

01:12:18.260 --> 01:12:21.260
but I'm afraid there is a kind of discrimination.

01:12:21.260 --> 01:12:25.260
It's only those on the ground floor

01:12:25.260 --> 01:12:30.260
that can ask a question due to health and safety policies in the theatre.

01:12:30.260 --> 01:12:33.260
So please, if you have a question,

01:12:33.260 --> 01:12:35.260
please raise your hand,

01:12:35.260 --> 01:12:37.260
and I'm happy to take questions at this point.

01:12:37.260 --> 01:12:40.260
So, John, perhaps I'll start with John.

01:12:40.260 --> 01:12:42.260
I'll give you the provision.

01:12:42.260 --> 01:12:44.260
There is a roving microphone.

01:12:46.260 --> 01:12:49.260
And just declare who you are, John,

01:12:49.260 --> 01:12:51.260
and perhaps stand up and just ask a question.

01:12:51.260 --> 01:12:54.260
Interesting to begin an ethics talk with some discrimination, Nigel,

01:12:54.260 --> 01:12:56.260
but I'm John Tysulis.

01:12:56.260 --> 01:12:59.260
I'm the director of the Institute for Ethics in AI.

01:12:59.260 --> 01:13:02.260
Thanks so much for a really fascinating and inspirational talk.

01:13:03.260 --> 01:13:05.260
I guess I want to ask two questions.

01:13:05.260 --> 01:13:09.260
One is a very general question about the nature of the project you're embarked on.

01:13:09.260 --> 01:13:13.260
So the objective is to generate a powerful all-purpose tool

01:13:13.260 --> 01:13:18.260
that will help create new scientific understanding.

01:13:18.260 --> 01:13:23.260
And the nature of this tool is artificial general intelligence.

01:13:23.260 --> 01:13:27.260
So that is a tool that can replicate or outperform human beings

01:13:27.260 --> 01:13:30.260
across a wide range of cognitive tasks.

01:13:31.260 --> 01:13:34.260
The worry is there attention there.

01:13:34.260 --> 01:13:37.260
If you had something that could outperform human beings

01:13:37.260 --> 01:13:40.260
across a wide range of cognitive tasks,

01:13:40.260 --> 01:13:43.260
could we still regard that as a tool?

01:13:43.260 --> 01:13:45.260
Or would it become a colleague?

01:13:45.260 --> 01:13:47.260
So you talked about respecting AI at the end,

01:13:47.260 --> 01:13:50.260
but it looks like something with that level of capacity

01:13:50.260 --> 01:13:53.260
would demand a different form of respect

01:13:53.260 --> 01:13:57.260
that would preclude the original objective of now treating it as a tool.

01:13:57.260 --> 01:13:59.260
So that's one question.

01:13:59.260 --> 01:14:03.260
The second question is, you've talked about what will benefit humanity.

01:14:03.260 --> 01:14:06.260
And so I guess one question I have along these lines,

01:14:06.260 --> 01:14:08.260
how do you make that determination?

01:14:08.260 --> 01:14:13.260
So you might say, look, some people have the view that AI applied to military applications

01:14:13.260 --> 01:14:16.260
will benefit humanity. Others don't.

01:14:16.260 --> 01:14:18.260
How do you make that determination?

01:14:18.260 --> 01:14:21.260
And I guess there's also this further dimension.

01:14:21.260 --> 01:14:24.260
There's a division of labour in making that assessment.

01:14:24.260 --> 01:14:30.260
Do you think too much has been placed on the shoulders of developers,

01:14:30.260 --> 01:14:32.260
researchers, corporations,

01:14:32.260 --> 01:14:37.260
and that really government should step in and resolve some of these issues?

01:14:37.260 --> 01:14:39.260
Thanks, John. Great question.

01:14:39.260 --> 01:14:44.260
So I think with your first question,

01:14:44.260 --> 01:14:49.260
the reason human capabilities are an interesting mapping is because

01:14:49.260 --> 01:14:52.260
the human brain is the only evidence of general intelligence we have

01:14:52.260 --> 01:14:54.260
in the universe as far as we know.

01:14:54.260 --> 01:14:58.260
So I think there's always the question is how do you know you've got there?

01:14:58.260 --> 01:15:03.260
And you can approximate it with millions of tasks, potentially.

01:15:03.260 --> 01:15:04.260
So that's one approach.

01:15:04.260 --> 01:15:07.260
The more tasks you have in your grab bag and it can do all of them

01:15:07.260 --> 01:15:10.260
and pair against human performance, you might have done it.

01:15:10.260 --> 01:15:13.260
But there's always the possibility that one might have missed out

01:15:13.260 --> 01:15:17.260
a particular type of cognitive ability, like creativity or something.

01:15:17.260 --> 01:15:19.260
So that's why I think...

01:15:19.260 --> 01:15:23.260
And also I think AI can be applied back to neuroscience as well, by the way.

01:15:23.260 --> 01:15:26.260
That's one of our scientific areas that we apply AI to,

01:15:26.260 --> 01:15:29.260
is neuroscience itself and better understanding our own minds.

01:15:29.260 --> 01:15:33.260
So I have this view that as a neuroscientist

01:15:33.260 --> 01:15:35.260
that this journey we're embarked on with AI

01:15:35.260 --> 01:15:39.260
is the most fascinating journey one can ever take scientifically

01:15:39.260 --> 01:15:41.260
because there's not only the artificial building,

01:15:41.260 --> 01:15:44.260
it's then comparing that to the human mind

01:15:44.260 --> 01:15:47.260
and then seeing, I think, uncovering the mysteries of our own minds,

01:15:47.260 --> 01:15:49.260
what's dreaming, what is creativity, what are emotions,

01:15:49.260 --> 01:15:52.260
all of these questions that we have, free will,

01:15:52.260 --> 01:15:55.260
potentially even consciousness, the big questions.

01:15:55.260 --> 01:15:59.260
I think building AI and intelligent artefacts

01:15:59.260 --> 01:16:02.260
and then seeing what is missing in them

01:16:02.260 --> 01:16:05.260
is a good way to explore that scientifically.

01:16:05.260 --> 01:16:07.260
And so then, I don't know the answer to your question,

01:16:07.260 --> 01:16:09.260
I think that's part of this journey,

01:16:09.260 --> 01:16:12.260
is at what point would these things not become just tools.

01:16:12.260 --> 01:16:15.260
And it may even be that it's a design question

01:16:15.260 --> 01:16:19.260
because whether we should build what is consciousness we don't know

01:16:19.260 --> 01:16:22.260
and that would be a whole, obviously, debate in itself,

01:16:22.260 --> 01:16:24.260
but should we build it to the extent of what it is,

01:16:24.260 --> 01:16:26.260
should we build them in our systems?

01:16:26.260 --> 01:16:29.260
I would say no to begin with if we have that choice

01:16:29.260 --> 01:16:31.260
until we better understand them as tools

01:16:31.260 --> 01:16:33.260
and then we can bring in that extra complexity of free will

01:16:33.260 --> 01:16:36.260
and where do they get their goals from?

01:16:36.260 --> 01:16:38.260
Initially it will be designers,

01:16:38.260 --> 01:16:40.260
but if they could be self-generated.

01:16:40.260 --> 01:16:42.260
So I think we're still a long way away from those things,

01:16:42.260 --> 01:16:45.260
but that's one of the things I think we should inch towards

01:16:45.260 --> 01:16:48.260
very cautiously and with precautions

01:16:48.260 --> 01:16:52.260
because also it will get to the heart of what it means to be human.

01:16:52.260 --> 01:16:55.260
And I think that should exactly be done multidisciplinary

01:16:55.260 --> 01:17:00.260
with philosophers and ethicists and theologians

01:17:00.260 --> 01:17:02.260
and the wider humanities.

01:17:02.260 --> 01:17:05.260
I think this is where the humanities comes in,

01:17:05.260 --> 01:17:07.260
as well as the science.

01:17:07.260 --> 01:17:12.260
So I think that's all to come.

01:17:12.260 --> 01:17:14.260
OK, a question in the front row?

01:17:22.260 --> 01:17:24.260
Thank you so much for a great presentation.

01:17:24.260 --> 01:17:26.260
Carina Prunkel, I'm a research fellow at the Institute.

01:17:26.260 --> 01:17:31.260
So you mentioned at various points the potential for dual use

01:17:31.260 --> 01:17:33.260
and in particular malicious dual use.

01:17:33.260 --> 01:17:38.260
So I'm curious to hear how you approach this topic at DeepMind.

01:17:38.260 --> 01:17:42.260
So what precautions or how do you address the potential for dual use?

01:17:42.260 --> 01:17:44.260
Great.

01:17:44.260 --> 01:17:47.260
So we have a lot of different mechanisms now at DeepMind

01:17:47.260 --> 01:17:49.260
that have been built up over time.

01:17:49.260 --> 01:17:52.260
So one is the Institutional Review Committee we have,

01:17:52.260 --> 01:17:56.260
which is formed, so it's chaired by Laila Ibrahim, our COO,

01:17:56.260 --> 01:18:01.260
and it's formed with different people from across the company.

01:18:01.260 --> 01:18:04.260
We have legal, we have ethicists and philosophers as well.

01:18:04.260 --> 01:18:07.260
It's also rotating boards, some senior researchers,

01:18:07.260 --> 01:18:09.260
and they get involved early with research projects

01:18:09.260 --> 01:18:12.260
and try to assess them from all aspects,

01:18:12.260 --> 01:18:15.260
and they will draw on outside experts.

01:18:15.260 --> 01:18:17.260
So they bring in biologists, for example,

01:18:17.260 --> 01:18:19.260
for alpha-fold biothesists,

01:18:19.260 --> 01:18:21.260
so things we might not have in-house.

01:18:21.260 --> 01:18:26.260
And then they work with the research teams to either say no,

01:18:26.260 --> 01:18:29.260
that project should not proceed, OK, it can with caveats,

01:18:29.260 --> 01:18:31.260
or why don't you build or do it in a different way

01:18:31.260 --> 01:18:33.260
with these safeguards?

01:18:33.260 --> 01:18:36.260
So that's our prototype, I would say, committee

01:18:36.260 --> 01:18:39.260
that does these things, and we're kind of exercising our muscle

01:18:39.260 --> 01:18:42.260
when the stakes are relatively low currently

01:18:42.260 --> 01:18:45.260
so that we can learn from what works and is effective

01:18:45.260 --> 01:18:49.260
as we get more powerful systems.

01:18:49.260 --> 01:18:51.260
And obviously over time, I think at some point

01:18:51.260 --> 01:18:55.260
there have got to be outside bodies that get involved.

01:18:55.260 --> 01:18:59.260
But the problem is that, and we've experimented with that too,

01:18:59.260 --> 01:19:01.260
is that a lot of these things are very specific

01:19:01.260 --> 01:19:03.260
to the technology itself.

01:19:03.260 --> 01:19:07.260
So one has to sort of understand the technology to a deep level,

01:19:07.260 --> 01:19:09.260
maybe even have access to it somehow,

01:19:09.260 --> 01:19:12.260
but in a controlled way, because one can't just...

01:19:12.260 --> 01:19:14.260
Open sourcing is not just a panacea either,

01:19:14.260 --> 01:19:16.260
because if it's a dangerous system, open sourcing

01:19:16.260 --> 01:19:19.260
it means any bad actor can use it too, for anything.

01:19:19.260 --> 01:19:21.260
So there's a lot of complicated, I think,

01:19:21.260 --> 01:19:23.260
ethical questions around this.

01:19:23.260 --> 01:19:25.260
But I don't think there's an easy answer.

01:19:25.260 --> 01:19:28.260
So anyone who thinks there is one, I think, is kidding themselves.

01:19:28.260 --> 01:19:31.260
I hope everyone realises the complexity involved.

01:19:31.260 --> 01:19:33.260
But I think it's pretty...

01:19:33.260 --> 01:19:35.260
I'm very happy with our internal system,

01:19:35.260 --> 01:19:38.260
but I appreciate more is going to be needed than that

01:19:38.260 --> 01:19:41.260
as the systems get more powerful and impact more of the world.

01:19:41.260 --> 01:19:43.260
A question just behind you, I think,

01:19:43.260 --> 01:19:46.260
if you just pass the microphone literally behind you.

01:19:46.260 --> 01:19:48.260
Hi, my name is Ulrich.

01:19:48.260 --> 01:19:50.260
I'm a postdoc at the Computer Science Department

01:19:50.260 --> 01:19:52.260
in the Human Central Computing Group.

01:19:52.260 --> 01:19:54.260
So DeepMind looks like it's this great example

01:19:54.260 --> 01:19:57.260
of how you can take the best from science

01:19:57.260 --> 01:20:00.260
and then sort of bring it together with a commercial company

01:20:00.260 --> 01:20:02.260
and then make very rapid progress.

01:20:02.260 --> 01:20:04.260
And you mentioned in the end here how you thought

01:20:04.260 --> 01:20:06.260
that the scientific process should sort of inspire

01:20:06.260 --> 01:20:09.260
the commercial world, as it were.

01:20:09.260 --> 01:20:12.260
I'm curious about what you think about the other way around.

01:20:12.260 --> 01:20:16.260
So what have you learnt by being embedded in Google

01:20:16.260 --> 01:20:20.260
that you think we as researchers should learn from

01:20:20.260 --> 01:20:22.260
in order to make more rapid progress?

01:20:22.260 --> 01:20:24.260
Yeah, you're absolutely right.

01:20:24.260 --> 01:20:27.260
That was the thinking, the original vision behind the...

01:20:27.260 --> 01:20:30.260
So I spoke about the original vision of the company,

01:20:30.260 --> 01:20:32.260
this Apollo programme,

01:20:32.260 --> 01:20:36.260
but the original vision behind the organisational setup

01:20:36.260 --> 01:20:38.260
and processes was to be a hybrid

01:20:38.260 --> 01:20:40.260
like the best of both worlds.

01:20:40.260 --> 01:20:43.260
Startups and the energy and creativity

01:20:43.260 --> 01:20:46.260
and pace that they have and nimblness

01:20:46.260 --> 01:20:48.260
and the best from academic research,

01:20:48.260 --> 01:20:50.260
the blue sky thinking, ambitious thinking

01:20:50.260 --> 01:20:53.260
that happens there, but sometimes with a lot of bureaucracy.

01:20:53.260 --> 01:20:56.260
So I think that we did actually successfully

01:20:56.260 --> 01:20:58.260
combine those two things.

01:20:58.260 --> 01:21:01.260
And then when we agreed to get acquired,

01:21:01.260 --> 01:21:03.260
we combined it with the third thing,

01:21:03.260 --> 01:21:06.260
which is scale and resources of a large,

01:21:06.260 --> 01:21:08.260
very successful company like Google.

01:21:08.260 --> 01:21:10.260
And I think that's the main lesson,

01:21:10.260 --> 01:21:13.260
is to make sure you do things at huge impact

01:21:13.260 --> 01:21:17.260
and have ambition and realise that you can scale things

01:21:17.260 --> 01:21:19.260
to that and the consequence that come with that,

01:21:19.260 --> 01:21:21.260
but also the potential of that.

01:21:21.260 --> 01:21:24.260
So I think we've done that now and very well,

01:21:24.260 --> 01:21:27.260
like Mary or three of those aspects together.

01:21:27.260 --> 01:21:29.260
It's a daily challenge because as we get bigger,

01:21:29.260 --> 01:21:31.260
one tends to get slower as an organisation.

01:21:31.260 --> 01:21:35.260
So we have to fight against that all the time.

01:21:35.260 --> 01:21:37.260
But it's pretty unique, I would say,

01:21:37.260 --> 01:21:40.260
the organisational and cultural feel of the mind.

01:21:40.260 --> 01:21:42.260
But it could be a blueprint for other,

01:21:42.260 --> 01:21:46.260
I would say, grand projects could be organised in a similar way.

01:21:46.260 --> 01:21:48.260
OK, I'm going to just switch to this side

01:21:48.260 --> 01:21:50.260
and then I'm going to go to the question there

01:21:50.260 --> 01:21:52.260
and the question about that.

01:21:59.260 --> 01:22:01.260
So move fast and break things with a quote

01:22:01.260 --> 01:22:04.260
from people who built a social network.

01:22:04.260 --> 01:22:07.260
If the mind was to build a social network

01:22:07.260 --> 01:22:11.260
using the deep-wined way of doing things,

01:22:11.260 --> 01:22:15.260
then what metrics would you use,

01:22:15.260 --> 01:22:19.260
would you optimise to judge the quality of your social network?

01:22:19.260 --> 01:22:22.260
And the second question that comes with it is,

01:22:22.260 --> 01:22:25.260
do you in fact have a moral obligation

01:22:25.260 --> 01:22:27.260
to build that social network?

01:22:29.260 --> 01:22:31.260
OK, so thanks Tim,

01:22:31.260 --> 01:22:33.260
two complicated questions there.

01:22:33.260 --> 01:22:36.260
It's actually just generally,

01:22:36.260 --> 01:22:39.260
so let's see, I have to be careful what I say,

01:22:39.260 --> 01:22:43.260
but I think social networks have never really been my thing,

01:22:43.260 --> 01:22:46.260
but also I haven't really thought a lot about it

01:22:46.260 --> 01:22:48.260
relative to scientific advances

01:22:48.260 --> 01:22:51.260
and the sorts of things that are my personal passion.

01:22:51.260 --> 01:22:54.260
I would question actually the premise of your question,

01:22:54.260 --> 01:22:58.260
which is that how much value does weak ties like that give,

01:22:58.260 --> 01:23:01.260
like superficial connections like that,

01:23:01.260 --> 01:23:03.260
versus deeper ties that you get in real life

01:23:03.260 --> 01:23:05.260
with your real family and friends?

01:23:05.260 --> 01:23:08.260
I think it's an interesting thing to understand.

01:23:08.260 --> 01:23:11.260
Are we sacrificing deeper, more meaningful moments

01:23:11.260 --> 01:23:14.260
for hundreds of more superficial moments?

01:23:14.260 --> 01:23:17.260
It's not entirely clear to me that the metric of,

01:23:17.260 --> 01:23:19.260
and it sounds seductive, connect the world,

01:23:19.260 --> 01:23:21.260
like why would that be bad?

01:23:21.260 --> 01:23:23.260
But this is the thing I'm talking about with the scientific method,

01:23:23.260 --> 01:23:26.260
is to try and think through the full consequences of what that would mean.

01:23:26.260 --> 01:23:29.260
Echo chambers, manipulation, all the rest of it

01:23:29.260 --> 01:23:32.260
that we all know very well don't need to go into.

01:23:32.260 --> 01:23:34.260
So I think if I was to do something like that,

01:23:34.260 --> 01:23:37.260
I would use the scientific method again

01:23:37.260 --> 01:23:40.260
to try and really think through ahead of time

01:23:40.260 --> 01:23:44.260
what do you want as the outcomes and metrics?

01:23:44.260 --> 01:23:47.260
In fact, often trying to find the right metrics

01:23:47.260 --> 01:23:50.260
that actually drive the right behaviour that you think is good in the world

01:23:50.260 --> 01:23:51.260
is half the challenge.

01:23:51.260 --> 01:23:53.260
It's like asking the right question in science.

01:23:53.260 --> 01:23:56.260
Everybody who does science knows that asking the question is the hardest thing.

01:23:56.260 --> 01:23:58.260
What is the right question?

01:23:58.260 --> 01:24:00.260
And it's especially hard.

01:24:00.260 --> 01:24:01.260
Oh, you want an answer?

01:24:01.260 --> 01:24:04.260
Well, I wouldn't want to give you an answer on the spot,

01:24:04.260 --> 01:24:06.260
but we can talk about it over dinner.

01:24:06.260 --> 01:24:10.260
At least one should attempt to start with serious thinking

01:24:10.260 --> 01:24:12.260
about the question first, right?

01:24:12.260 --> 01:24:13.260
That's the first part.

01:24:13.260 --> 01:24:16.260
I don't know what the answer is because I've not given it enough thought.

01:24:16.260 --> 01:24:19.260
But one should at least understand the meta level of like

01:24:19.260 --> 01:24:21.260
that's how one should start,

01:24:21.260 --> 01:24:24.260
including whether one should do that thing at all, potentially.

01:24:24.260 --> 01:24:27.260
It could be the answer of that hypothesis generation.

01:24:27.260 --> 01:24:28.260
Okay.

01:24:28.260 --> 01:24:31.260
I'm going to try and get three more questions in.

01:24:31.260 --> 01:24:32.260
We're right up against the clock.

01:24:32.260 --> 01:24:33.260
We've got about seven and a half minutes.

01:24:33.260 --> 01:24:35.260
There's a question here from Helen.

01:24:39.260 --> 01:24:40.260
Thank you.

01:24:40.260 --> 01:24:42.260
I'm Ellen Landomer from Yale University

01:24:42.260 --> 01:24:44.260
and visiting fellow at the Research Centre for Ethics in AI.

01:24:44.260 --> 01:24:46.260
Thank you for a brilliant talk.

01:24:46.260 --> 01:24:51.260
So you showed us how AI can help us figure out the truth of the universe.

01:24:51.260 --> 01:24:52.260
Pretty much.

01:24:52.260 --> 01:24:54.260
How about the moral world?

01:24:54.260 --> 01:24:56.260
How about the political universe?

01:24:56.260 --> 01:24:58.260
Philosophy starts with Plato's Republic,

01:24:58.260 --> 01:25:00.260
which is an attempt to figure out the best constitution.

01:25:00.260 --> 01:25:03.260
Surely, unless one is a complete moral relativist,

01:25:03.260 --> 01:25:07.260
there are some invariants we're trying to figure out about the moral world.

01:25:07.260 --> 01:25:09.260
Could AI help us map that out?

01:25:09.260 --> 01:25:12.260
Could it figure out like the best social organisation,

01:25:12.260 --> 01:25:14.260
you know, boring from, I don't know,

01:25:14.260 --> 01:25:20.260
all the things we've tried, capitalism, socialism, libertarianism, egalitarianism?

01:25:20.260 --> 01:25:22.260
Would it help expand our imagination

01:25:22.260 --> 01:25:24.260
and perhaps assuming you have an objective function

01:25:24.260 --> 01:25:28.260
like satisfying major Italian preferences

01:25:28.260 --> 01:25:32.260
subject to constraints to protect minority rights or something like that?

01:25:32.260 --> 01:25:33.260
What do you see in the future?

01:25:33.260 --> 01:25:36.260
We took through 2,000 years and we haven't made much progress.

01:25:38.260 --> 01:25:39.260
Good question.

01:25:39.260 --> 01:25:43.260
I mean, look, I think the morality and political science

01:25:43.260 --> 01:25:47.260
I think is one of the hardest things that AI,

01:25:47.260 --> 01:25:49.260
you know, I think it can contribute in some way,

01:25:49.260 --> 01:25:52.260
but I would say it's far harder than the physical sciences,

01:25:52.260 --> 01:25:53.260
right, or the life sciences,

01:25:53.260 --> 01:25:56.260
because the most complex things in the world are humans,

01:25:56.260 --> 01:26:00.260
for human beings to understand and to model

01:26:00.260 --> 01:26:04.260
and to understand people's motivations, especially in aggregate.

01:26:04.260 --> 01:26:08.260
I think one way it could help is there's also the question of

01:26:08.260 --> 01:26:13.260
even if an theoretical AI could come up with a better political construct,

01:26:13.260 --> 01:26:18.260
would humans, beings and society accept that or even care or understand it?

01:26:18.260 --> 01:26:22.260
So there's all those questions to try and and would it be implemented correctly?

01:26:22.260 --> 01:26:24.260
Obviously there's obviously implementation problems.

01:26:24.260 --> 01:26:27.260
I think more interesting maybe would be,

01:26:27.260 --> 01:26:29.260
and I've talked to economists about this is,

01:26:29.260 --> 01:26:32.260
and we did quite a lot of research on multi-agent systems.

01:26:32.260 --> 01:26:37.260
So again, having a little sandbox or simulation of millions of agents

01:26:37.260 --> 01:26:41.260
with interacting with each other, with motivations and some goals seeking things.

01:26:41.260 --> 01:26:45.260
And I think we're missing that experimental testbed actually

01:26:45.260 --> 01:26:47.260
from political science and economics quite a lot,

01:26:47.260 --> 01:26:50.260
because again, economics is one of those things where

01:26:50.260 --> 01:26:52.260
and political science where you sort of have to test it live,

01:26:52.260 --> 01:26:53.260
a, b, test it in the world.

01:26:53.260 --> 01:26:56.260
It's like, are we going to go for this political system or not?

01:26:56.260 --> 01:26:58.260
Should we raise inflation or not?

01:26:58.260 --> 01:27:00.260
Well, you've got models, but then you actually just have to do it

01:27:00.260 --> 01:27:03.260
and then see, oh, it's caused a recession or something,

01:27:03.260 --> 01:27:05.260
where maybe we shouldn't do that next time.

01:27:05.260 --> 01:27:08.260
And so it would be better if I think if we had a simulation

01:27:08.260 --> 01:27:11.260
or a sandbox perhaps populated with AI systems

01:27:11.260 --> 01:27:15.260
that are approximates to idealise forms of humans

01:27:15.260 --> 01:27:19.260
and then we can maybe make some interesting,

01:27:19.260 --> 01:27:23.260
experimental work in that much lower stakes.

01:27:23.260 --> 01:27:26.260
So I think that could be really fascinating exploration area

01:27:26.260 --> 01:27:31.260
for things like market dynamics and setting the environmental settings

01:27:31.260 --> 01:27:34.260
to create more cooperation or something.

01:27:34.260 --> 01:27:38.260
I would be, far as economists, I would be trying to use all those things.

01:27:38.260 --> 01:27:42.260
I used to be fascinated when I was a kid with Santa Fe Institute

01:27:42.260 --> 01:27:46.260
and they used to do lots of really cool models of agent-based systems

01:27:46.260 --> 01:27:48.260
in little grid worlds.

01:27:48.260 --> 01:27:52.260
And I loved going artificial societies, I think, by Axelrod.

01:27:52.260 --> 01:27:55.260
I loved those kind of work, actually what I used to dream about,

01:27:55.260 --> 01:27:58.260
going to Santa Fe to work on something like that.

01:27:58.260 --> 01:28:03.260
And I still think that would be pretty cool to have some sort of system like that.

01:28:03.260 --> 01:28:05.260
Let's see if we can squeeze just a few more.

01:28:05.260 --> 01:28:09.260
There's a question chap in the, who caught my eye there, yes?

01:28:11.260 --> 01:28:13.260
Just very, and try and squeeze them in,

01:28:13.260 --> 01:28:15.260
because there's two more questions over here

01:28:16.260 --> 01:28:18.260
Super, yeah, I just have a quick question to be honest.

01:28:18.260 --> 01:28:24.260
So I think at the end you mentioned creating AI in the image of scientific method.

01:28:24.260 --> 01:28:29.260
And the title of your lecture is advancement of science through AI.

01:28:29.260 --> 01:28:33.260
But in what sense do you think that neural networks

01:28:33.260 --> 01:28:36.260
or the limited understanding I have of AI is,

01:28:36.260 --> 01:28:40.260
in what sense do they follow the notion of scientific method we have?

01:28:40.260 --> 01:28:45.260
Is there any sense of talking about hypothesis and then testing?

01:28:45.260 --> 01:28:48.260
Because it doesn't seem that neural networks work in that way.

01:28:48.260 --> 01:28:51.260
They're opaque for most practical purposes.

01:28:51.260 --> 01:28:56.260
And if they do outperform us, should we just get rid of the scientific method?

01:28:56.260 --> 01:29:01.260
So by the way, it's not in the image of the scientific method just to be clear.

01:29:01.260 --> 01:29:04.260
It's using the approach of the scientific method.

01:29:04.260 --> 01:29:06.260
I'm not sure what image in the scientific method means.

01:29:07.260 --> 01:29:13.260
And yes, today that is true that a lot of the systems we have are kind of black box-like.

01:29:13.260 --> 01:29:16.260
But I think that's exactly what we should be doing more work on,

01:29:16.260 --> 01:29:18.260
is making them less opaque.

01:29:18.260 --> 01:29:20.260
There's no reason why they should be.

01:29:20.260 --> 01:29:22.260
The way I say it to my neuroscience team is,

01:29:22.260 --> 01:29:25.260
look, we understand quite a lot about the brain now, the ultimate black boxes.

01:29:25.260 --> 01:29:29.260
We have MRI machines and amazing tools and single cell recording.

01:29:29.260 --> 01:29:30.260
So it's amazing.

01:29:30.260 --> 01:29:32.260
And that's why I got into neuroscience in the mid 2000s.

01:29:32.260 --> 01:29:36.260
So we can actually look into, we don't have to do philosophy of mind necessarily,

01:29:36.260 --> 01:29:38.260
although we should know about that.

01:29:38.260 --> 01:29:42.260
But we can actually empirically look at this, not just do introspection.

01:29:42.260 --> 01:29:47.260
And so as a minimum, in the field of artificial minds,

01:29:47.260 --> 01:29:52.260
we should know as much about them as we do with the real brain.

01:29:52.260 --> 01:29:54.260
And we don't know everything about the real brain.

01:29:54.260 --> 01:29:56.260
Obviously there's tons still we don't know.

01:29:56.260 --> 01:30:00.260
But there's a lot more that we do know than we do about these artificial systems.

01:30:00.260 --> 01:30:02.260
And it should be the other way around.

01:30:02.260 --> 01:30:06.260
That should be the minimum we understand because we have access to every neuron,

01:30:06.260 --> 01:30:09.260
you know, neuron, artificial neuron in the artificial brain.

01:30:09.260 --> 01:30:12.260
And we can completely control the experimental conditions.

01:30:12.260 --> 01:30:15.260
So as a minimum, so I sometimes say this is a challenge to the team.

01:30:15.260 --> 01:30:19.260
What's the equivalent of fMRI for a neural network?

01:30:19.260 --> 01:30:21.260
What's the equivalent of single cell recording?

01:30:21.260 --> 01:30:22.260
We do ablation studies.

01:30:22.260 --> 01:30:24.260
So we have a whole neuroscience team that's thinking about this

01:30:24.260 --> 01:30:29.260
and bringing neuroscience techniques, analysis techniques over to AI.

01:30:29.260 --> 01:30:34.260
Now, in the defence of the engineers, one of the reasons that this has happened

01:30:34.260 --> 01:30:39.260
is because the brain is obviously a static system we're all fascinated by, of course.

01:30:39.260 --> 01:30:42.260
But artificial systems change over time.

01:30:42.260 --> 01:30:45.260
Like AlphaGo is now in ancient history of AI, right?

01:30:45.260 --> 01:30:47.260
Although it was very meaningful over time.

01:30:47.260 --> 01:30:50.260
And it takes years to study a system, right?

01:30:50.260 --> 01:30:52.260
It takes years to build it, and then it takes years to study it.

01:30:52.260 --> 01:30:55.260
So should you use that research time on studying a system

01:30:55.260 --> 01:30:59.260
that itself will be out of date by the time you come to any conclusions about it?

01:30:59.260 --> 01:31:02.260
So I think only now are we reaching the point where we have systems

01:31:02.260 --> 01:31:05.260
that are interesting enough, do enough interesting things in the world,

01:31:05.260 --> 01:31:09.260
like large models and AlphaFold type things,

01:31:09.260 --> 01:31:13.260
that probably it's worth spending the research time on that.

01:31:13.260 --> 01:31:17.260
And so I think over the next decade we're going to see a lot more understanding

01:31:17.260 --> 01:31:18.260
of what these systems do.

01:31:18.260 --> 01:31:21.260
I don't think there's some weird reason why that can't happen.

01:31:21.260 --> 01:31:23.260
Okay, there are so many more questions.

01:31:23.260 --> 01:31:25.260
I am literally in the red now.

01:31:25.260 --> 01:31:28.260
I'm going to have to call this to a close.

01:31:28.260 --> 01:31:29.260
I do apologise.

01:31:29.260 --> 01:31:33.260
There is so much pent up, I think, interest and questions for you, Demis.

01:31:33.260 --> 01:31:37.260
All I can say at this point is absolutely a wonderful lecture

01:31:37.260 --> 01:31:39.260
where five minutes later than we should have been.

01:31:39.260 --> 01:31:42.260
The Sheldanian was to a strict regime when it comes to timekeeping.

01:31:42.260 --> 01:31:45.260
You gave us the most fascinating insights

01:31:45.260 --> 01:31:49.260
and you have given, I think, to the world with your company

01:31:49.260 --> 01:31:53.260
and your own talents, a quite wonderful vision of a future

01:31:53.260 --> 01:31:58.260
in which AI can help us flourish, empower us and not oppress us.

01:31:58.260 --> 01:32:00.260
So thank you very much.

01:32:19.260 --> 01:32:21.260
Thank you.

