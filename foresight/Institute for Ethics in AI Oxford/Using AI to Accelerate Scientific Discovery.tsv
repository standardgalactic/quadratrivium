start	end	text
0	5240	â€¦uchos MARH
8140	12140	welcome to this evening's lecture
12140	15660	in the splendor of the Sheldonian theatre.
15880	19820	Hollol teis endseth at Oxford UK University,
19960	23440	part of the OpenSeed Challenge lectures
23460	26660	on artificial intelligence and human values.
26660	30060	My name is Nigel Shadbolt, principal of Jesus College.
30060	35460	I'm also a professor of computer science here in Oxford and chair the Institute Steering Group.
35460	40260	It was my privilege to help set up the Institute which brings together world-leading philosophers
40260	47260	and other experts in the humanities with the researchers, developers and users of AI.
47260	50660	The director of the Institute is Professor John Tosulis,
50660	55060	and its ultimate home will be the Stephen A. Schwarzman Centre for the Humanities,
55060	58860	whose construction is soon to start.
58860	62860	In recent years, AI has gone from strength to strength.
62860	65060	It's now ubiquitous.
65060	69660	In our phones, the games we play, in our cars, our drug discovery companies,
69660	74260	the search engines we use and the translation tools we depend on.
74260	78260	Much of that is down to a new generation of AI methods and techniques
78260	82260	that are powered by modern machine learning algorithms,
82260	88060	great swathes of data and the prodigious power of modern-day computing hardware.
88060	91060	Some of AI's most dramatic recent accomplishments
91060	97060	owe a great deal to our speaker here with us this evening and the company he co-founded.
97060	100460	Demis Arsabes, CEO and co-founder of DeepMind,
100460	104060	one of the world's leading AI research companies.
104060	107860	Demis' own career and intellectual journey is an extraordinary one.
107860	111660	A chess prodigy, hugely successful computer games developer,
111660	115260	with a double first in computer science from Cambridge.
115260	118860	Demis has always been fascinated by the human brain,
118860	122260	understanding how it gives rise to intelligence.
122260	127660	After the success of his games company, he went on to a PhD in cognitive neuroscience at UCL,
127660	130660	followed by a Henry Welcombe Postdoctoral Research Fellowship
130660	135660	at the Gatsby Computational Neuroscience Unit, also at UCL.
135660	139860	His papers in cognitive neuroscience investigated imagination,
139860	145660	memory and amnesia and appeared in leading journals such as Nature and Science.
145660	148460	He combined his interest in computing and neuroscience
148460	151660	with the formation of DeepMind in 2010.
151660	155260	It's compelling ambition to solve intelligence
155260	159060	and then use intelligence to solve everything else.
159060	163660	He and his team used games as the context in which to test new ideas
163660	169260	about how to build AI systems using machine learning methods inspired by neuroscience.
169340	173060	First arcade games and then famously Go.
173060	177060	A previous talk here in the Sheldonian in February 2016
177060	184260	prefigured AlphaGo winning 4-1 against former world champion Lee Sodol just a month later.
184260	189860	Games have proven to be a great training ground for developing and testing AI algorithms,
189860	193860	but the aim of DeepMind has always been to build general learning systems
193860	197860	ultimately capable of solving important problems in the real world.
197860	201860	DeepMind's AlphaFold system is a solution to the 50-year grand challenge
201860	206260	of protein structure prediction, culminating the release of the most accurate
206260	209460	and complete picture of the human proteome.
209460	214260	A core aim for the Institute for Ethics at AI is to bring together world-leading academics
214260	217260	and the practitioners at the cutting edge of AI development.
217260	222660	Tonight we will hear first hand experience of AI's enormous potential
222660	225260	to accelerate scientific discovery.
225260	230660	Experience which will inform our research and thinking about the critical ethical considerations
230660	236060	that must be considered by policy makers and technical developers of AI.
236060	241860	Demis has predicted that artificial intelligence will be one of the most beneficial technologies ever
241860	244860	but that significant ethical issues remain.
244860	249860	Please join me in welcoming Demis Hasabas to deliver tonight's Tana lecture
249860	252860	using AI to accelerate scientific discovery.
255260	269260	Thank you. Thank you, Senaigel, for such a great introduction.
269260	273260	It's a real pleasure to be back here in Oxford in the Shadonian
273260	276260	and giving the Tana lecture. It's a real honour.
276260	281260	What I'm going to talk about today is using AI to accelerate scientific discovery.
281260	285260	As you'll see throughout my talk, this was my original motivation
285260	289260	and has always been my motivation behind spending my entire career
289260	292260	and trying to make AI a reality.
292260	295260	I'm going to talk a lot about some of our most recent advances
295260	299260	actually now coming to fruition, especially the last year or two,
299260	303260	of using AI to crack difficult scientific problems.
303260	306260	I'm also going to talk about the lead up to there
306260	310260	and how I think about the games work we did originally and the foundation work we did originally.
310260	315260	Last time I talked here was just before the AlphaGo match in Korea.
315260	319260	That was a major moment for us and how in the last five or six years
319260	323260	things have progressed enormously.
323260	329260	Just to talk a little bit about what our vision was behind DeepMind back in 2010.
329260	332260	It's quite hard to remember the state of AI back in 2010
332260	336260	because today, as Nigel was saying, AI is ubiquitous all around us.
336260	339260	It's one of the biggest buzzwords in industry.
339260	343260	It's hard to remember just 12 years ago
343260	346260	almost nobody was talking about AI, I would say,
346260	351260	and it was almost impossible to actually get funding in the private sector for AI at all.
351260	354260	We have many funny stories back in the day
354260	357260	of trying to do some fundraising back in 2009 and 2010
357260	360260	and most people thinking we were completely mad
360260	363260	to be embarking on this journey.
363260	367260	We founded it with this in mind of trying to build one day
367260	371260	a hollow programme-like effort to build AGI,
371260	373260	artificial general intelligence.
373260	376260	We use this term artificial general intelligence
376260	379260	to distinguish it from normal everyday AI
379260	382260	where we're talking about a general system
382260	387260	that can perform well on many tasks to at least human level.
387260	391260	That's the general aspect that we are always striving for
391260	393260	in all the work that we do.
393260	396260	We're still on this mission now and I think we've done
396260	401260	a pretty good job of staying true to this original vision
401260	404260	that we had in 2010 when we were just a few people
404260	409260	in a small little office in an attic in Russell Square.
409260	412260	As Nigel said, our original mission statement
412260	414260	was step one, solve intelligence,
414260	417260	step two, use it to solve everything else.
417260	420260	We have updated that mission statement a little bit,
420260	423260	still means the same thing, but just to be a little bit more descriptive now
423260	425260	in the last few years, just to be a bit clearer
425260	427260	about what we mean by solving everything else,
427260	429260	what exactly are we talking about.
429260	433260	The way we discuss our mission now is solving intelligence
433260	437260	to advance science and of course for the benefit of humanity.
437260	441260	That's always been the cornerstone of what we think about
441260	444260	when we think about what should we apply AI to.
446260	450260	Now, there are two, broadly two ways that I think AI
450260	452260	can be attempted to be built.
452260	457260	One is the more traditional way of building logic systems
457260	461260	or expert systems and these are hard coded systems
461260	464260	that effectively teams of programmers solve the problem.
464260	466260	They then incorporate those solutions
466260	469260	in sometimes very clever expert systems,
469260	473260	but the problem with them is that they are very limited
473260	475260	in terms of what they can generalise to.
475260	477260	They can't deal with the unexpected
477260	481260	and they're basically limited to what the programmers
481260	485260	foresaw, the situations that the system might be in.
485260	488260	Of course, this line of work was inspired by mathematics
488260	490260	and logic systems.
490260	493260	On the other hand, the big renaissance in the last decade plus
493260	499260	is the sort of progress of learning systems.
499260	504260	Of course, in the 80s there was a flurry of work done on neural networks,
504260	506260	then that died down.
506260	509260	We now know that probably we didn't have enough computing power
509260	512260	or data, maybe not the right algorithms as well.
512260	515260	Basically, in essence, the ideas were correct.
515260	519260	An idea of a learning system is that it learns for itself,
519260	521260	solutions for itself from first principles,
521260	523260	directly from experience.
523260	526260	The amazing thing about these systems and their huge promise
526260	529260	is that they can maybe generalise to tasks
529260	532260	and that it's not being programmed for explicitly
532260	536260	and maybe solve problems that we ourselves as the designers
536260	539260	or scientists behind those systems don't know how to solve.
539260	541260	Of course, that's the huge potential
541260	544260	and also the risk of these kinds of systems.
544260	548260	Originally, these learning systems took a lot of inspiration
548260	550260	and also could be validated, some of the ideas
550260	553260	like reinforcement learning and neural networks
553260	557260	by systems neuroscience in comparing what these systems do,
557260	560260	comparing them on a systems and algorithmic level
560260	563260	to what we know about how the brain works.
564260	566260	Everything we do at DeepMind, of course,
566260	569260	is on the learning system side.
569260	573260	We've been lucky enough to be in the vanguard of this almost revolution
573260	577260	or anasence in the last decade of these types of approaches.
577260	581260	How do we think about what's our special take
581260	584260	on learning systems and how powerful they can be?
584260	589260	There are two component algorithms or approaches,
589260	591260	one could say, that we've fused together.
591260	594260	Of course, there's deep learning or deep neural networks
594260	597260	and the way I think about this is that the deep neural network system
597260	600260	is there to build a model of the environment
600260	603260	of the data and the experience.
603260	605260	Then what do you use that model for?
605260	608260	Well, you can use reinforcement learning,
608260	613260	which is a sort of goal seeking and reward maximising system.
613260	617260	You can use that model and use it to plan
617260	622260	and basically plan and take actions towards a goal,
622260	625260	a goal that may be specified by the designers of that system.
625260	629260	You have the model and then you have the action
629260	633260	and goal-solving element of the systems.
633260	638260	One of our early innovations was to fuse those two things together at scale.
638260	641260	We call it deep reinforcement learning now.
641260	645260	The cool thing about these systems is that they can discover new knowledge
645260	649260	from first principles through this process of trial and error
649260	651260	using these models.
651260	654260	The idea here on this diagram of the agent system
654260	657260	is it gets observations from the environment.
657260	660260	Those observations go towards building and updating
660260	663260	an internal model of how the environment works
663260	666260	and the transition matrices of the environment.
666260	669260	There's some goal it's trying to solve in the environment,
669260	672260	and then after its thinking time has run out,
672260	676260	it has to select an action from the action set
676260	678260	available to it at that moment in time
678260	681260	that will best get it incrementally towards its goal.
681260	683260	Then the action gets output.
683260	685260	It may or may not make a change in the environment.
685260	690260	That drives a new observation and then the model updates further.
690260	693260	You can see with this type of system,
693260	697260	the AI system is actually an active learner.
697260	699260	It participates in its own learning.
699260	703260	The decisions it makes in large part governs what experiences
703260	707260	and what data it will get next to learn more from.
707260	710260	Although this is a pretty simple diagram
710260	713260	and basically describes the whole of reinforcement learning,
713260	715260	the reinforcement learning problem,
715260	717260	there's huge complexities of course of theoretical
717260	721260	and practical complexities underlying this diagram
721260	723260	that need to be solved.
723260	726260	We know that in the limit this must work
726260	730260	because this is how mammalian brains work, including humans.
730260	734260	This is one of the learning mechanisms that we have in our own brains.
734260	736260	Reinforcement learning was found to be implemented
736260	741260	by dopamine neurons in the brain in the late 90s.
741260	743260	We know if we push this hard enough,
743260	748260	this should be one path towards general artificial intelligence.
748260	750260	What did we famously use this for?
750260	755260	AlfaGo was the program that I think we did a lot of things before this.
755260	759260	Like Atari games and other proof points.
759260	763260	AlfaGo was really our first attempt at doing this at a huge scale
763260	766260	to crack a big problem that was unsolved in AI,
766260	769260	one of the holy grails of AI research,
769260	773260	which was a program to beat the world champion at the Game of Go.
773260	777260	I want to talk a little bit about this in hindsight now,
777260	782260	knowing what I know now, how I've reinterpreted what we did with AlfaGo.
782260	785260	I think I can explain it in a much more simple in general way
785260	789260	than perhaps how I was explaining it back five, six years ago
789260	792260	when we were in the midst of building this system.
792260	795260	Just for those of you who don't know...
796260	798260	I don't know why that's not updating.
801260	802260	There we go.
802260	804260	This is the Game of Go.
806260	809260	This is the Game of Go, the board game.
809260	815260	It's a phenomenal game and it's a much more esoteric game
815260	819260	and artistic game, one could say, than chess.
819260	823260	It occupies the same intellectual echelon chess stars in the West.
823260	828260	In China, in Japan, in Korea and other Asian countries, they play Go.
828260	835260	Go has resisted old-fashioned logic system and expert system approaches,
835260	838260	whereas chess was solved by those things
838260	840260	because of various factors.
840260	843260	One is the search space is truly enormous in Go.
843260	848260	It's roughly 10 to the power, 170 possible board positions,
848260	851260	which is way more than there are atoms in the universe,
851260	854260	so there's no way one could exhaustively search
854260	859260	all of the possible board positions in order to find the right path through.
859260	863260	Even bigger problem actually is that it's impossible,
863260	867260	or thought was, it was impossible to write down an evaluation function.
867260	869260	To hand code an evaluation function,
869260	872260	which is what most modern-day chess programs use.
872260	876260	The reason is because Go is such an esoteric game.
876260	879260	It doesn't have materiality in chess.
879260	883260	As a first approximation, one can add up the piece values on both sides,
883260	886260	and that will tell you very crudely,
886260	888260	which side is winning in that position.
888260	892260	You need to know that in order to make decisions about what to do next.
892260	896260	Many people are tempted to, over 20 years since Deep Blue,
896260	901260	attempted to write to construct these evaluation functions for Go.
901260	905260	One of the issues is that Go players themselves do not know,
905260	908260	consciously at least, what that information is.
908260	912260	Because it's so complex a game,
912260	916260	they actually use their intuition rather than explicit calculation
916260	918260	in order to deal with the complexity of Go.
918260	922260	Whereas chess players, if you ask them how, why did they make a decision,
922260	926260	a chess grandmaster will be able to tell you explicitly
926260	928260	the various factors involved.
928260	930260	A Go player generally won't do that.
930260	932260	They'll just say things like it felt right.
932260	934260	This felt like the right move,
934260	937260	which is what I think also makes Go an incredible game.
937260	941260	Of course, intuition is not something one would associate
941260	945260	with computer programs, especially logic systems.
945260	948260	Maybe in the Q&A we can discuss a little bit more
948260	950260	about what intuition may be.
950260	954260	But I don't think it's my conclusion now,
954260	956260	after doing all these games,
956260	958260	and indeed some of the science things we've done,
958260	961260	is that it's not some mysterious thing.
961260	964260	It's actually information that our brain knows about
964260	966260	and has learnt through experience, of course.
966260	969260	I mean, there's no other way one can learn information.
969260	972260	But it's just in the association courtesies.
972260	976260	So it's not actually consciously available to a high-level cortex.
976260	979260	So it seems mysterious to us how we ride a bike,
979260	983260	when these sort of motor, sensory motor things we're able to do,
983260	985260	because our conscious part of our brain
985260	987260	cannot access those representations.
987260	989260	And if we can't do that,
989260	993260	then we definitely can't explicitly code it in some logic code,
993260	995260	which is why traditionally those tasks,
995260	997260	including things like computer vision,
997260	1000260	have been quite hard for logic systems to solve,
1000260	1003260	even over the last 50 years.
1003260	1005260	So a lot about what we were doing
1005260	1007260	was trying to approximate this kind of intuition
1007260	1009260	in these learning systems.
1009260	1010260	So how did we work?
1010260	1012260	And I'm actually going to describe,
1012260	1013260	not just AlphaGo here,
1013260	1016260	but the whole series of AlphaX programmes.
1016260	1021260	So AlphaGo, the original one that beat Lisa Doll in 2016.
1021260	1024260	And then AlphaGo Zero,
1024260	1027260	that then didn't need human data to learn from,
1027260	1028260	just learn for itself.
1028260	1030260	And then finally AlphaZero,
1030260	1032260	which could play any two player game.
1032260	1035260	So I'm going to sort of describe them all,
1035260	1036260	roughly speaking,
1036260	1040260	with this sort of demonstrative diagram.
1040260	1042260	So the way you can think of all of these systems
1042260	1046260	is we're initially training a neural network through self-play.
1046260	1048260	So the system plays against itself,
1048260	1052260	and it learns to evaluate positions
1052260	1054260	and to pick the most likely moves
1054260	1058260	that are most useful for it to look at.
1058260	1060260	So that's what it's got to do.
1060260	1062260	Now initially, it starts with no knowledge.
1062260	1064260	So you have an initialised neural network,
1064260	1066260	it starts with zero knowledge.
1066260	1068260	So it literally is moving randomly.
1068260	1071260	So we can call that version one.
1071260	1072260	That's the neural network.
1072260	1076260	And what it does is it plays roughly 100,000 games against itself.
1076260	1079260	And so that then becomes a data set.
1079260	1082260	So that 100,000 games, we take that as a data set.
1082260	1083260	And what we try to do with it
1083260	1086260	is train a version two of that network,
1086260	1087260	a new neural network.
1087260	1091260	But we try and train it on this version one data set
1091260	1093260	to predict in the middle of a position,
1093260	1094260	in the middle of a game,
1094260	1096260	from a position in the middle of a game,
1096260	1098260	which side is going to win.
1098260	1100260	So predict ahead of time.
1100260	1104260	And also what sorts of moves does the V1 system choose
1104260	1106260	in a particular position.
1106260	1110260	So it's trying to be better at both those two things.
1110260	1113260	And then what happens is we train that V2 system,
1113260	1115260	and then we have a little mini tournament
1115260	1117260	between V1 and V2.
1117260	1119260	So it's roughly 100 games,
1119260	1120260	and they have a little match off.
1120260	1124260	And basically, if the V2 system
1124260	1126260	hits a particular threshold win rate,
1126260	1128260	55% in this case,
1128260	1132260	then we say it's significantly better than V1.
1132260	1133260	And if that's true,
1133260	1137260	then what we do is we replace V1 with version two network,
1137260	1139260	this new network in Purple.
1139260	1140260	And that, of course,
1140260	1142260	plays another 100,000 games against itself.
1142260	1144260	And now it creates a new data set.
1144260	1147260	But this data set now in Purple, in the middle,
1147260	1150260	is slightly better quality than that first data set,
1150260	1152260	because the player is slightly better.
1152260	1155260	And to begin with, almost imperceptively better.
1155260	1157260	So it's just slightly better than random now.
1157260	1160260	But that's enough signal to then train,
1160260	1163260	of course we train a version three system,
1163260	1165260	and that plays off against version two.
1165260	1169260	Now, if you don't reach this 55% win rate,
1169260	1172260	what you do instead is you take back the version two,
1172260	1174260	and you continue to generate more data with that,
1174260	1176260	another 100,000 games.
1176260	1179260	And you have 200,000 to train your next version three.
1179260	1181260	And eventually that version three
1181260	1183260	will be better than version two.
1183260	1188260	So after one does this around 17 or 18 times,
1188260	1192260	you go from random to better than world champion.
1192260	1194260	That's it.
1194260	1198260	And you can do this with any two player game,
1198260	1200260	perfect information game.
1200260	1203260	So the same network can do that.
1203260	1207260	Get to better to world champion within 20 to 30 generations
1207260	1208260	of doing this.
1208260	1210260	So you literally, and we got to the point where so fast
1210260	1212260	you literally set it off in the morning,
1212260	1214260	you could play chess about it at lunchtime
1214260	1217260	and maybe just beat it, and then by tea time,
1217260	1219260	you know, you no chance.
1219260	1221260	Literally in the day, you could actually see the evolution
1221260	1222260	in one day.
1222260	1225260	It's kind of incredible to watch as a chess player.
1225260	1227260	So what is it doing then,
1227260	1231260	in terms of thinking about this enormous search space?
1231260	1233260	So what's happening is,
1233260	1236260	and the sort of, I think, advance of AlphaGo,
1236260	1240260	one of the advances was combining this neural network system,
1240260	1245260	or model, with a kind of more classical tree search algorithm.
1245260	1248260	In this case, we use Monte Carlo tree search.
1248260	1251260	And you can think of the tree of possibilities
1251260	1253260	looking a bit like this in Go,
1253260	1255260	where each node here is a positioning in Go,
1255260	1258260	obviously shown by these little mini Go boards.
1258260	1261260	And you can imagine if you're some middle game position,
1261260	1263260	you know, there's just this countless
1263260	1266260	10 to the 170 possibilities in the limit.
1266260	1270260	How is one supposed to find the needle in the haystack, right?
1270260	1272260	The good moves that could be world champion
1272260	1275260	or better level decisions.
1275260	1279260	So what the neural network does is it constrains,
1279260	1284260	that model constrains the search to things to make it tractable,
1284260	1286260	to things that are reasonably likely to work,
1286260	1288260	reasonably effective,
1288260	1290260	and it can evaluate that at each node level
1290260	1292260	with its evaluation function.
1292260	1294260	And so instead of having to do, you know,
1294260	1297260	10 to the hundreds of possibilities,
1297260	1300260	one can just zoom into, you know, mere thousands,
1300260	1303260	10,000 or so searches.
1303260	1304260	And so therefore,
1304260	1307260	instead of that searching the entire grey tree of all possibilities,
1307260	1310260	one just looks at this far more limited, you know,
1310260	1312260	search tree in blue here.
1312260	1314260	And then when you run out of thinking time,
1314260	1316260	of course, you select the best path
1316260	1320260	that you found so far in pink here.
1320260	1323260	So, you know, we did this back in 2015,
1323260	1326260	and then in the subsequent years, we still work on this now.
1326260	1328260	There's a system called Mu Zero,
1328260	1330260	which is our latest version of this that can do,
1330260	1333260	not only do two player perfect information board games,
1333260	1337260	but can also build models of its environment.
1337260	1340260	So it can actually also do things like Atari games
1340260	1342260	and video games where you actually don't have
1342260	1345260	the rules of the game given to you.
1345260	1347260	It has to actually figure that out for itself
1347260	1348260	through observation as well.
1348260	1351260	So it's one step even more general than Alpha Zero.
1351260	1354260	And what we did with Alpha Go, of course, now is,
1354260	1358260	as Sir Nigel mentioned, is we took it to Seoul in 2016
1358260	1360260	in this million dollar challenge match with Lisa Doll.
1360260	1363260	And some of you may remember this,
1363260	1364260	but we won for one.
1364260	1367260	You know, it was a huge thing, especially in Asia and in Korea.
1367260	1369260	I mean, the country almost came to stand still.
1369260	1372260	There's over 200 million people watch the games.
1372260	1374260	And we won for one,
1374260	1379260	and experts in both AI and in Go proclaimed this advance
1379260	1383260	to be a decade before they would have predicted.
1383260	1386260	But the important thing in the end was actually not just the fact
1386260	1388260	that Alpha Go won the match,
1388260	1392260	but how it won was, I think, really instructive.
1392260	1395260	So I'm just going to give one example of this,
1395260	1397260	but actually Alpha Go, I think, is in the end changed
1397260	1401260	the way that we as human beings view the game of Go.
1401260	1404260	But this is the most famous game of that set of five.
1404260	1406260	There are actually some amazing different games,
1406260	1410260	including the one that Lisa Doll won with a genius move in game four.
1410260	1415260	But move 37 in game two, I think, will go down in Go history.
1415260	1417260	And this was the ball position at that time.
1417260	1420260	And I haven't got time to go into why this was so amazing.
1420260	1423260	But suffice to say, Alpha Go here was black,
1423260	1425260	and Lisa Doll is the white stones.
1425260	1428260	And this is very early on in the game, move 37.
1428260	1432260	You know, Go games last for a few hundred moves generally.
1432260	1435260	And Alpha Go played this move 37 stone
1435260	1438260	on the right hand side here, marked in red.
1438260	1441260	And the amazing thing about this was the position of the stone
1441260	1444260	was on the fifth line from the edge of the board.
1444260	1448260	And that, if you're an expert Go player, is unthinkable.
1448260	1451260	It's like you would be told off by your Go master
1451260	1454260	that you should never do make a move like that.
1454260	1458260	Because it gives white too much space on the side of the board.
1458260	1461260	But Alpha Go decided to do it.
1461260	1464260	Never seen before in master play would be recommended against.
1464260	1468260	And then 100 moves or so later, it turned out this stone,
1468260	1471260	this move 37 stone, was in the perfect position
1471260	1474260	to decide the battle that spread out from the bottom left
1474260	1476260	all the way across the board.
1476260	1478260	And it was just in the right place to decide that battle,
1478260	1480260	which decided the whole game.
1480260	1487260	And almost as if it had presciently sort of seen that influence ahead of time.
1487260	1490260	So now people play on the fifth line all the time, I'm told.
1490260	1493260	So this has changed everything.
1493260	1498260	And there's multiple books now written about Alpha Go's strategies.
1498260	1504260	And this is an original strategy because this is not something
1504260	1507260	that Alpha Go could have learned from human play.
1507260	1509260	In fact, it would have learned the opposite.
1509260	1513260	It would have learned not to do this kind of move.
1513260	1515260	So if you're interested in more on that Alpha Go,
1515260	1518260	I recommend you this amazing award-winning documentary
1518260	1521260	that was done by an independent filmmaker on YouTube now.
1521260	1523260	If you want to see the sort of ins and outs of it,
1523260	1527260	it was very emotional as an experience for us from all sides,
1527260	1530260	especially me being an ex-games player.
1530260	1535260	I could really understand it from Lisa Doll's point of view too.
1535260	1538260	So as I said, we then took this to Alpha Zero a couple of years ago,
1538260	1542260	two, three years ago now, and generalised this to all two-player games.
1542260	1549260	And these graphs show how Alpha Zero did against the best machines at the time
1549260	1552260	in the specialised games of chess.
1552260	1554260	It beat the best version of Stockfish,
1554260	1560260	which is this incredible handcrafted system, the descendant of Deep Blue.
1560260	1562260	And it was able to beat Stockfish 8,
1562260	1566260	which was the best Stockfish at the time, in four hours of training.
1566260	1572260	It could beat Alpha Go, Alpha Zero beat Alpha Go in eight hours at Go.
1572260	1576260	And then we just tried it with one other game, Japanese chess shogi,
1576260	1579260	actually, which is a really interesting variation on chess.
1579260	1585260	And it could beat the best handcrafted programme called ELMO within two hours of training.
1585260	1588260	The same system, all three games.
1588260	1590260	So that was generalised.
1590260	1592260	And then, of course, because I'm a chess player,
1592260	1595260	I play a little bit of Go, but I'm not very strong, but so chess is my game.
1595260	1599260	And so for me, this was the most exciting part of applying Alpha Zero,
1599260	1604260	because I actually had a discussion with Murray Campbell,
1604260	1607260	who some of you will know was one of the project leaders
1607260	1611260	behind Deep Blue, our IBM back in the 90s.
1611260	1616260	And we just, I think we just were about to play the Lisa Doll match,
1616260	1617260	or maybe we just finished.
1617260	1619260	And I was giving a lecturer at a conference,
1619260	1621260	and Murray Campbell was there as well in the audience.
1621260	1623260	And he came up to me afterwards, and we were discussing,
1623260	1625260	I said to him, I'm thinking about,
1625260	1628260	maybe we should try this with chess and see what happens.
1628260	1631260	And I wanted to know what his prediction would be.
1631260	1636260	Do you think these incredibly powerful handcrafted systems,
1636260	1638260	like stockfish, could be beaten?
1638260	1641260	Was there any more headroom in chess?
1641260	1644260	Chess is probably the oldest application of AI, right?
1644260	1647260	I mean, Turing and Shannon and people like that
1647260	1648260	have all tried their hand.
1648260	1651260	Every AI researcher at some point has tried their hand
1651260	1654260	on a chess program back to the 40s and 50s,
1654260	1657260	even if Turing had to run the program by hand
1657260	1660260	on a piece of paper and a pen.
1660260	1664260	And then, of course, in the last 25 years or so,
1664260	1667260	world champions have been studying with their chess programs
1667260	1670260	and mapping out all of chess, opening theory, all of these things.
1670260	1673260	So it was a legitimate question actually to ask is,
1673260	1675260	was there any more headroom left?
1675260	1677260	And what sort of chess would AlphaZero play
1677260	1681260	if we were to train it from first principles
1681260	1688260	and play it against these amazing hand-engineered monsters
1688260	1691260	in some sense of a machine, incredible calculating machines?
1691260	1694260	And so, of course, we couldn't actually come to an agreement on that.
1694260	1696260	And that, as the scientists in the orders will know,
1696260	1698260	that's the sign of a good question, I think,
1698260	1700260	where either answer would be interesting.
1700260	1702260	If we were to win and there was some new style out there,
1702260	1703260	there would be incredibly interesting.
1703260	1706260	And also be interesting if these hand-crofter systems,
1706260	1710260	at least in one domain, chess, had reached the limit.
1710260	1713260	So we got off and started doing that.
1713260	1717260	And I'm pleased to say that AlphaZero not only played stronger,
1717260	1721260	but it did come up with a completely new style of chess,
1721260	1724260	which I think, and my chess friends tell me,
1724260	1729260	is more aesthetically pleasing as well as a chess program.
1729260	1732260	Obviously, subjectively from a human expert's point of view.
1732260	1734260	And the reason it is is because what it does,
1734260	1736260	and it does many innovations,
1736260	1741260	but the main one is that it favours mobility over materiality.
1741260	1744260	So traditionally, hand-crofter chess programs
1744260	1746260	have always favoured materiality.
1746260	1750260	The joke within the chess circles is that chess computer sees a pawn
1750260	1753260	and then grabs the pawn because it loves material
1753260	1755260	because it gets plus one in its evaluation function.
1755260	1757260	And then it tries to hang on for dear life
1757260	1759260	in a really ugly position,
1759260	1761260	but it wins because it never makes any tactical mistakes.
1761260	1763260	So it's sort of very effective,
1763260	1767260	but it's a little bit sort of aesthetically unsatisfying,
1767260	1769260	one would say, as a style.
1769260	1772260	But instead of that, actually AlphaZero does the opposite.
1772260	1776260	It loves sacrificing pieces, material, to get mobility,
1776260	1780260	to get more mobility for its remaining pieces.
1780260	1784260	So this is a game from, we did a 100 match
1784260	1786260	between AlphaZero and Stockfish,
1786260	1790260	and then we gave it to the British chess champion to analyse,
1790260	1793260	and he picked out the coolest positions.
1793260	1794260	This is my favourite.
1794260	1796260	It's sometimes called the immortal Zugswang game.
1796260	1799260	Zugswang is a phrase in chess,
1799260	1802260	a German phrase that means any move that one makes in that position
1802260	1804260	makes your position worse.
1804260	1807260	So it's a special type of position where you're in Zugswang,
1807260	1810260	which means anything you do, it's going to make it worse,
1810260	1811260	which is very unusual.
1811260	1813260	And it's super unusual in this kind of position,
1813260	1815260	for those of you who know chess, where black,
1815260	1817260	which has got more pieces, the two rooks and a queen,
1817260	1820260	so it's got big material advantage, very powerful pieces,
1820260	1822260	the most powerful pieces remaining in chess,
1822260	1824260	but they will stuck in the corner,
1824260	1828260	and AlphaZero has sort of sealed them up with cement with its pieces,
1828260	1830260	and basically none of those pieces can move.
1830260	1832260	So this is kind of an incredible position.
1832260	1836260	So almost anything black does in this position,
1836260	1838260	its black to move, will make its position worse,
1838260	1842260	even though it's got all of these very powerful pieces.
1842260	1845260	So that was one innovation.
1845260	1847260	There were lots of interesting poppies about AlphaZero
1847260	1850260	that I won't go into, but one can think about,
1850260	1853260	well, why is it that AlphaZero plays like this,
1853260	1857260	and traditional chess engines didn't?
1857260	1859260	Nowadays, actually, interestingly, they've updated Stockfish
1859260	1863260	to include some of these ideas by hand in Stockfish,
1863260	1866260	and actually now it's even more powerful.
1866260	1868260	So it's kind of interesting hybrid system.
1868260	1872260	But my feeling is that it's better at evaluating positions
1872260	1875260	than chess engines, so that's one thing,
1875260	1877260	so it's got a better evaluation function.
1877260	1879260	And the main thing is it doesn't have to overcome
1879260	1881260	these inbuilt rules.
1881260	1883260	That's why it's sacrificing pieces,
1883260	1885260	because if you think about it, a hard-coded chess engine
1885260	1888260	would have to calculate in its search tree
1888260	1890260	that if it was going to sacrifice a rook for a bishop,
1890260	1892260	that's minus two points,
1892260	1895260	is it going to get back those two points of value
1895260	1897260	within its search tree horizon?
1897260	1899260	AlphaZero doesn't have to worry about that,
1899260	1901260	because there's no rules like that in there.
1901260	1903260	It can evaluate things contextually,
1903260	1906260	based on the particular situation at hand,
1906260	1908260	and the patterns involved there.
1908260	1910260	And also, the other big thing is,
1910260	1912260	Stockfish and programmes like that,
1912260	1914260	they have thousands of handcrafted rules,
1914260	1917260	so one problem is generating those rules,
1917260	1919260	but an even bigger problem, in my opinion,
1919260	1921260	is balancing those factors together.
1921260	1924260	That's a huge handcrafted juggling act.
1924260	1927260	And instead of that, obviously AlphaZero learns itself
1927260	1930260	how to balance out the factors that it's learned,
1930260	1934260	and to do that automatically.
1934260	1937260	So one can actually see how efficient this system is,
1937260	1941260	based on the amount of search that traditional search engines
1941260	1944260	have to do per each move they make.
1944260	1947260	And a human grandmaster makes only the order of,
1947260	1949260	looks at about 100 moves per decision,
1949260	1951260	so incredibly efficient with our models.
1951260	1954260	And the state-of-the-art chess engine, like Stockfish,
1954260	1959260	would make tens of millions of evaluations per move.
1959260	1962260	And AlphaZero is sort of in the middle here,
1962260	1966260	in terms of orders of magnitude, tens of thousands of moves.
1966260	1970260	So not as efficient as human players,
1970260	1973260	but far more efficient than the search one would get
1973260	1976260	in these search engines.
1976260	1979260	So again, if you're interested in the details about,
1979260	1982260	or your chess playing, and the details about what this changed,
1982260	1986260	the British champion and Natasha Reagan
1986260	1988260	wrote an amazing book called Game Changer,
1988260	1992260	when we gave them behind the scenes access to AlphaZero,
1992260	1994260	and what new motifs they found,
1994260	1997260	at least a dozen new motifs they found in chess.
1997260	2000260	And the cool thing is that it's very gratifying for me,
2000260	2002260	is that people like Magnus Carlson,
2002260	2005260	who's the current world champion, incredible player,
2005260	2007260	he said a few years back he was one of the first people
2007260	2009260	to read the book and who we sent it to,
2009260	2011260	and I've been influenced by my heroes recently,
2011260	2014260	one of which is AlphaZero, which is really cool to say.
2014260	2016260	And he actually incorporated, because he's so talented,
2016260	2018260	he was able to quite quickly,
2018260	2020260	quicker than all the other chess players,
2020260	2022260	incorporate some of these ideas into his play.
2022260	2025260	And then Garry Casparov, he used to be a hero of mine
2025260	2028260	when he was world champion when I was growing up and playing chess.
2028260	2030260	He worked the forward for the book,
2030260	2032260	and he said programs usually reflect priorities and prejudices
2032260	2035260	of programmers, but AlphaZero, it learns for itself,
2035260	2037260	and I would say it's star reflects the truth,
2037260	2040260	which is, you know, I think, a beautiful quote.
2041260	2045260	So we've been lucky enough to have several of these sort of fundamental
2045260	2047260	breakthroughs in games.
2047260	2050260	We started with Atari, and our program called DQN,
2050260	2053260	being able to play Atari games directly from pixels
2053260	2055260	and maximise the score just from pixels,
2055260	2058260	not being told the rules of the game, AlphaGo and AlphaZero,
2058260	2059260	I just mentioned.
2059260	2062260	And then we went further with programs like AlphaStar,
2062260	2066260	which played the most complex video game called StarCraft 2,
2066260	2069260	which is a very complicated real-time strategy game
2069260	2071260	with huge other challenges.
2071260	2074260	It's only partially observable, it's not perfect information,
2074260	2076260	there's an economy system to it,
2076260	2079260	and you have generally thousands of possible actions
2079260	2082260	you can take for any choice, not a few dozen.
2083260	2087260	And we managed to also get to grandmaster level at that.
2088260	2090260	So that was all of our games work,
2090260	2093260	but really it was leading up to this moment,
2093260	2097260	which in the last couple of years has been just so exciting
2097260	2101260	and so gratifying for us to make progress with,
2101260	2105260	which is that the games, and I love games, always will love games,
2105260	2109260	playing them, designing them and using them as testing grounds,
2109260	2112260	they were the perfect testing ground for developing AI,
2112260	2116260	but ultimately the aim was not to play games to world championship level,
2116260	2119260	it was to build general systems that could generalise
2119260	2121260	and solve real world problems.
2121260	2123260	And the one that's particularly passionate for me
2123260	2126260	is using AI for scientific discovery.
2127260	2129260	And there are three things that I look for when currently,
2129260	2132260	when we want to select a scientific problem
2132260	2136260	that we believe our systems could be good at.
2136260	2140260	So number one is we actually search out massive combinatorial
2140260	2142260	search spaces or state spaces.
2142260	2144260	So the bigger, the better actually.
2144260	2145260	Why is that?
2145260	2148260	Well, because we know then traditional methods
2148260	2150260	and exhaustive brute force methods won't work.
2150260	2153260	So we're in a razy where something else is needed
2153260	2156260	and we think that we're good at that something else.
2156260	2159260	Number two is that we want to have,
2159260	2162260	we like problems that have a clear objective function
2162260	2164260	or metric that one can specify
2164260	2166260	so that you can optimise and hill climb against it
2166260	2168260	with your learning system.
2168260	2170260	And then number three is we look for problems
2170260	2173260	that either have a lot of data available
2173260	2175260	to learn and train from,
2175260	2177260	or, and ideally it's and or,
2177260	2179260	an accurate and efficient simulator
2179260	2181260	that one can use to generate more data.
2181260	2183260	And that simulator doesn't have to be perfect.
2183260	2185260	It just has to be good enough
2185260	2188260	that you can extract some signal from the data that it generates.
2189260	2193260	Now it turns out that when you look at a lot of problems
2193260	2197260	with this prism, then actually a lot of surprising
2197260	2201260	number of problems can be made to fit these criteria.
2201260	2205260	And of course, the number one thing we were looking at
2205260	2208260	was protein folding, which I want to talk a bit about now.
2208260	2210260	And we look for problems,
2210260	2212260	not only that just fit those three criteria,
2212260	2214260	but of course there's always an opportunity cost
2214260	2217260	when you embark on applying AI to something major.
2217260	2219260	It's going to take you many years,
2219260	2221260	depending on how hard that problem is.
2221260	2225260	And we look for something that will have really huge impact.
2225260	2227260	Perhaps we sometimes talk about root nodes
2227260	2230260	that can open up whole new branches
2230260	2234260	of scientific discovery if they were to be solved.
2234260	2237260	And protein folding ticked all of those boxes.
2237260	2240260	So if you don't know what protein folding is,
2240260	2243260	it's this classic problem of can one go
2243260	2246260	from a one-dimensional amino acid sequence,
2246260	2248260	you can think of it as the genetic sequence
2248260	2252260	for a protein that describes a protein coded by the genome.
2252260	2254260	And can you predict from that directly
2254260	2257260	the 3D structure of the protein in your body,
2257260	2259260	the 3D form that it takes.
2259260	2262260	And the reason this is important is that proteins
2262260	2265260	are basically essential for everything in life,
2265260	2267260	every function in your body.
2267260	2271260	And it's thought that the 3D structure of the protein,
2271260	2274260	at least in the large part, governs its function.
2274260	2276260	So if one can understand the structure,
2276260	2281260	then one can get closer to the function of the protein.
2281260	2284260	Now, until AlphaFol came along,
2284260	2286260	the way you would do this is experimentally,
2286260	2289260	and it's extremely painstaking expert work
2289260	2290260	that needs to be done.
2290260	2294260	And using x-ray crystallography and electron microscopy.
2294260	2298260	And the rule of thumb is generally that it takes one PhD student,
2298260	2301260	their whole PhD, to do one protein.
2301260	2303260	And that's if you get lucky, you can be unlucky.
2303260	2308260	So it's hard and really painstaking and difficult.
2308260	2311260	And what happened is that the Nobel Prize winner
2311260	2315260	Christian Anfinsen, in part of his Nobel lecture in 1972,
2315260	2318260	so 50 years ago, exactly now,
2318260	2321260	he conjectured that the 3D structure of proteins
2321260	2324260	should be fully determined by the amino acid sequence,
2324260	2327260	i.e. this should be possible this mapping.
2327260	2331260	And it's a bit like, sometimes this problem is called
2331260	2334260	like Fermat's Last Theorem equivalent in biology,
2334260	2336260	because it's a bit like saying this is possible,
2336260	2339260	but the margin is too small, can't give you the answer.
2339260	2343260	And so what happened instead is obviously it set off a 50 year quest
2343260	2345260	in biology, in computational biology,
2345260	2348260	to try and solve this problem.
2348260	2354260	And it's been ongoing ever since the 1970s.
2354260	2356260	So the big question is,
2356260	2359260	is can protein structure prediction,
2359260	2361260	the protein structure prediction problem,
2361260	2363260	which is the specific part of protein folding
2363260	2366260	that we're interested in, be solved computationally?
2366260	2368260	Just computationally.
2368260	2372260	And Leventhal, who is another famous contemporary of Anfinsen,
2372260	2374260	in the 60s and 70s as well,
2374260	2376260	he calculated, back of envelope,
2376260	2380260	that there would be roughly 10 to the 300 possible confirmations,
2380260	2383260	shapes of an average size protein that it could take.
2383260	2386260	So 10 to the 300, so that's a good number,
2386260	2388260	that's ones we like, because it's bigger than go.
2388260	2391260	And obviously that means exhaustively sampling this
2391260	2393260	is totally intractable,
2393260	2398260	but of course the chink of light is that in nature,
2398260	2401260	in our bodies, physics solves this.
2401260	2406260	So it can, if proteins spontaneously fold in a matter of seconds,
2406260	2408260	sometimes milliseconds in the body.
2408260	2412260	So there's obviously some energy path through this.
2412260	2414260	So how do we get to this problem?
2414260	2418260	Well actually it's quite a long winding road for me personally,
2418260	2420260	for others in the team less so.
2420260	2423260	But for me, I actually came across the protein folding problem
2423260	2426260	in the 90s as an undergrad in Cambridge,
2426260	2430260	because one of my friends in our sort of group
2430260	2433260	of colleagues was obsessed with this problem.
2433260	2436260	And he would talk about it, and I remember this very clearly,
2436260	2439260	every opportunity in the bar playing pool, whatever it was.
2439260	2444260	If we can crack this, that will open up all sorts of things in biology.
2444260	2446260	And I sort of listened to him and I was thinking about this,
2446260	2448260	I was fascinated by the problem as a problem,
2448260	2452260	and I felt it was actually very well suited to potentially to AI.
2452260	2456260	Although obviously at the time I didn't know how it could be tackled.
2456260	2458260	But I filed that away as an interesting thing.
2458260	2461260	And then it came up again in the late 2000s
2461260	2463260	when I was doing my postdoc over at MIT.
2463260	2468260	And this game called Fold It came out from David Baker's lab,
2468260	2470260	who works on proteins.
2470260	2473260	And it was a citizen science game, you can see it on the left here.
2473260	2475260	And what they've done really interestingly
2475260	2478260	is turn protein folding into a puzzle game.
2478260	2483260	And they actually got a couple hundred gamers to fold proteins,
2483260	2485260	bit like playing Tetris or something.
2485260	2489260	And some of them actually became really good.
2489260	2492260	And I remember, so of course I was fascinated this
2492260	2494260	just from games design perspective.
2494260	2496260	Wouldn't it be amazing if we could design more games
2496260	2499260	where people played them, they were actually doing useful science
2499260	2501260	while they were having fun, that would be amazing.
2501260	2503260	And I think this is still the best example of that.
2503260	2507260	But also again protein folding was coming up.
2507260	2510260	And in fact, it turned out that a couple of,
2510260	2513260	a few really important proteins structures
2513260	2515260	were found this way by gamers
2515260	2518260	and published in Nature and Nature Structural Biology.
2518260	2521260	And so this actually really worked.
2521260	2523260	And that, when we then got to, you know,
2523260	2525260	the third piece of the puzzle was doing Go
2525260	2528260	and trying to sort of think about what we'd done
2528260	2530260	with intuition and other things, as I mentioned earlier.
2530260	2532260	And I felt that actually, you know,
2532260	2534260	if we'd managed to mimic in some sense
2534260	2537260	the intuition of Go players, master Go players
2537260	2539260	who spent their entire life studying Go,
2539260	2542260	you know, maybe one could mimic the intuition of these gamers
2542260	2545260	who were only, by the way, of course, amateur biologists.
2545260	2548260	Right? But somehow some of them were able to make
2548260	2550260	counter-intuitive folds of the backbone
2550260	2553260	that were, if you just followed an energy landscape
2553260	2555260	in a greedy fashion, one would not, you know,
2555260	2558260	reach a local minima or local maxima
2558260	2562260	and you would not be able to find the right structure.
2562260	2566260	So it's almost the day after we got back from Korea
2566260	2570260	we then, you know, I instigated the Alpha Fold project
2570260	2573260	and I thought it was the right time
2573260	2577260	to basically start working on this problem.
2577260	2579260	The other important piece of the puzzle
2579260	2581260	was this competition called CASP,
2581260	2584260	which is sometimes thought of as, like,
2584260	2586260	the Olympics for protein folding,
2586260	2589260	and it's sort of run every two years in external benchmarks.
2589260	2590260	It's an amazing thing, actually,
2590260	2592260	that I think more areas of science should do.
2592260	2595260	And it's been run sort of religiously
2595260	2597260	for every two years, for nearly 30 years.
2597260	2600260	So, you know, huge culos to the organisers,
2600260	2603260	John Mull and his team for doing this
2603260	2606260	and organising it so professionally for every two years
2606260	2609260	without fail for 30 years.
2609260	2612260	And the cool thing about it is it's a blind prediction assessment.
2612260	2616260	So there's no way you can accidentally sort of train on test data
2616260	2618260	or any of these kinds of pitfalls
2618260	2620260	because at the time when the competition runs
2620260	2622260	over summer usually every two years,
2622260	2626260	the experimentalists globally agree to hold back
2626260	2628260	a few of their structures that they've just found,
2628260	2630260	but at that point in time they're the only ones
2630260	2632260	who know what that structure looks like.
2632260	2634260	They hold back the publication for a couple of months
2634260	2636260	and they give it to John Mull and his colleagues
2636260	2638260	to put it into the competition.
2638260	2639260	And then you get those.
2639260	2642260	It's quite fun tournament because then, you know,
2642260	2643260	it's quite exciting.
2643260	2645260	You get the email and then there's a new structure
2645260	2648260	that amino acid sequence nobody has ever, you know,
2648260	2649260	knows the structure of.
2649260	2651260	And then you have a week to sort of get it back
2651260	2654260	to the competition organisers before it's published.
2654260	2656260	And then at the end of that three, four month period,
2656260	2661260	they obviously score your predictions against the ground truth,
2661260	2664260	which at that point is published, obviously in peer review journals,
2664260	2666260	that the experimental ground truth.
2666260	2670260	And then you get a kind of distance measure between your predictions
2670260	2672260	and the molecules in that prediction
2672260	2676260	and where they really are in 3D coordinate space.
2676260	2680260	So when we started getting involved in this area post 2016,
2680260	2683260	you know, we looked at CASP and the history of it
2683260	2685260	and actually they'd been very little progress
2685260	2686260	for over a decade.
2686260	2688260	It's sort of the field had stalled.
2688260	2694260	And this graph here shows you the scores of the winning team
2694260	2696260	on the hardest category of protein,
2696260	2699260	where you don't have any evolutionary similar template proteins
2699260	2701260	to sort of rely on.
2701260	2702260	So it's called free modelling.
2702260	2704260	And this is a percentage accuracy.
2704260	2705260	It's called GDT.
2705260	2707260	It's a slight nuance of the measure,
2707260	2710260	but you can think of it as the number of molecules,
2710260	2712260	the percentage number of molecules you've got roughly
2712260	2715260	in that place to a certain tolerance, distance tolerance.
2715260	2718260	And you can see they were hovering around 40% or less,
2718260	2721260	which is useless for experimentation, right?
2721260	2723260	Basically, it's pretty much random.
2723260	2726260	And so that was the average and it hadn't really moved.
2726260	2733260	And so what we did in 2018 is that we came along with Alpha Fold 1
2733260	2736260	as our first entry after a couple of years of working on this.
2736260	2739260	And we sort of, you know, I think we revolutionised the field in a way,
2739260	2742260	is that for the first time we brought cutting edge machine learning techniques,
2742260	2745260	the sort of techniques we developed in AlphaGo
2745260	2747260	and other new ones for this domain.
2747260	2750260	And we, as the core part of the system,
2750260	2753260	and we improved the winning scores by 50%.
2753260	2756260	You know, we got close to 60 GDT here.
2756260	2758260	And then, of course, we didn't stop there.
2758260	2761260	We then re-architected based on that knowledge.
2761260	2763260	We actually tried to push that system further
2763260	2764260	and it turned out it hit a brick wall,
2764260	2766260	so we had to go back to the drawing board
2766260	2767260	with the knowledge that we had,
2767260	2769260	re-architected with a brand new system.
2769260	2772260	And then that finally reached in CAS14 in 2020,
2772260	2774260	atomic accuracy.
2774260	2778260	So accuracy within the width of an atom, right,
2778260	2780260	for all the molecules.
2780260	2786260	So when we look at the scores and the results of CAS14,
2786260	2789260	what you see here is that Alpha Fold 2,
2789260	2791260	this is the root mean squared error,
2791260	2796260	is less than one angstrom error on average.
2796260	2798260	And, you know, from the 100 or so proteins
2798260	2800260	that we're supposed to predict.
2800260	2802260	So, and one angstrom is the, you know,
2802260	2804260	the width of basically a carbon atom.
2804260	2807260	So that's finally, that was the magic threshold
2807260	2811260	that John Moll and others of the organisers said
2811260	2813260	that they always set out CASP to do,
2813260	2815260	because that would make you competitive
2815260	2817260	with experimental techniques,
2817260	2819260	which are roughly, you know, the best ones
2819260	2821260	are at that kind of error rate.
2821260	2823260	So if one could do that computationally,
2823260	2825260	then suddenly you have a technique that could be,
2825260	2829260	you could rely on in tandem with experimental instead of.
2829260	2834260	And so Alpha Fold 2 got an error of 0.96 angstroms,
2834260	2836260	which was three times more accurate
2836260	2838260	than the next best system in CAS14,
2838260	2841260	even though those systems obviously incorporated
2841260	2843260	the Alpha Fold 1 techniques
2843260	2846260	that we'd already published by then.
2846260	2848260	So this led to the CASP organisers and John Moll
2848260	2850260	declaring that the structure prediction problem
2850260	2854260	had essentially been solved after all of these years.
2854260	2856260	And this is what the predictions look like.
2856260	2859260	So the ground truth is in green,
2859260	2863260	and you can see the prediction from Alpha Fold 2 in blue.
2863260	2865260	And you can see firstly proteins are exquisitely beautiful.
2865260	2868260	It's one thing to note that I've learned over the many years
2868260	2870260	I've been working on this now.
2870260	2873260	They're like exquisite little nano machines.
2873260	2876260	And you can see how accurate the overlays are.
2876260	2878260	And we were astounded, of course,
2878260	2881260	when we first got these results back.
2881260	2883260	And then, you know, there are many,
2883260	2886260	this is the architecture for Alpha Fold 2,
2886260	2888260	so you don't have time to go into the details of today,
2888260	2890260	but there were a huge number of innovations
2890260	2893260	that were required to make this work.
2893260	2895260	And the key technical advances were basically,
2895260	2899260	first of all, I should say there was no silver bullet.
2899260	2903260	It needed actually 32 component algorithms
2903260	2905260	described in 60 pages of supplemental information
2905260	2907260	actually in the paper.
2907260	2909260	And that was required.
2909260	2911260	And every single part of that was required.
2911260	2913260	So we did these ablation analyses,
2913260	2915260	which sort of took out components to see
2915260	2917260	if we could get away without having them.
2917260	2920260	And the result of that was everything was required.
2920260	2923260	And the three key sort of takeaways of why Alpha Fold 2
2923260	2925260	was an improvement over Alpha Fold 1
2925260	2928260	is we made the system fully end-to-end.
2928260	2931260	So you can think of it as sort of going end-to-end
2931260	2934260	with a recycling iterative stage.
2934260	2937260	At the time, it sort of jigs the protein structure
2937260	2940260	nearer and closer and closer to the final structure
2940260	2942260	that it's going to predict.
2942260	2944260	And Alpha Fold 1 system didn't do that.
2944260	2946260	It went from the amino acid sequence
2946260	2949260	to this intermediate representation called a dystagram,
2949260	2953260	which is a pair-wise dystagram of all the protein molecules
2953260	2956260	and their distance to each of the other molecules,
2956260	2958260	the other end molecules.
2958260	2960260	And then from that, we used a different method
2960260	2962260	to create the 3D structure.
2962260	2965260	So we went straight for predicting the 3D structure.
2965260	2967260	And those of you who work in machine learning
2967260	2969260	will know that generally speaking,
2969260	2971260	if you can make something end-to-end
2971260	2973260	and optimize directly for the thing that you're after,
2973260	2976260	usually your system will have better performance.
2976260	2978260	We used an attention-based neural network
2978260	2981260	to infer this implicit graph structure
2981260	2986260	of the residues, of the amino acid sequences.
2986260	2989260	In Alpha Fold 1, we used a convolutional neural net,
2989260	2991260	which was sort of borrowed from computer vision.
2991260	2993260	And if you think about it,
2993260	2996260	that was introducing the wrong bias into protein folding
2996260	2999260	because with computer vision, pixels next to each other
2999260	3002260	are obviously going to be correlated in an image, in some sense.
3002260	3004260	So convolutions make sense.
3004260	3008260	But actually, for a protein, the amino acid sequence,
3008260	3010260	residues that are next to each other
3010260	3012260	or close to each other on the string of letters
3012260	3014260	may not end up being near each other
3014260	3016260	once you get the full 3D fold,
3016260	3019260	or things very far away could end up folding over near each other.
3019260	3022260	So, in a way, we were giving it the wrong biases,
3022260	3024260	so we actually had to remove that.
3024260	3028260	And then finally, we built in some biological and evolutionary
3028260	3030260	and physics constraints into the system
3030260	3032260	without impacting the learning.
3032260	3034260	And again, usually, so you can think of it
3034260	3036260	as a little bit of a hybrid system,
3036260	3038260	that usually, if you put in constraints,
3038260	3040260	that impacts the learning.
3040260	3043260	We managed to do that without that.
3043260	3045260	So this was a huge research effort over sort of five years,
3045260	3048260	took about 20 people at its maximum,
3048260	3050260	and it was a truly multidisciplinary effort.
3050260	3053260	So we needed biologists and physicists and chemists
3053260	3055260	as well as machine learners.
3055260	3057260	And I think that's an interesting lesson, maybe,
3057260	3060260	to learn about cross-disciplinary work in AI for Sciences,
3060260	3064260	is you need the experts also from the domain.
3064260	3066260	And then the final, maybe interesting point to note on this,
3066260	3069260	is that normally, we're always after generality,
3069260	3073260	so you can see that from the journey from AlphaGo to AlphaZero,
3073260	3075260	was we increasingly made things general.
3075260	3077260	You start with performance,
3077260	3079260	then you start throwing things out of that system
3079260	3081260	to try and make it simpler and more elegant,
3081260	3083260	and that usually makes it more general,
3083260	3085260	as you understand what it is that you're doing.
3085260	3089260	But that's because Go and Chess and those things
3089260	3092260	were testbeds for what we wanted to do.
3092260	3094260	If you are trying to solve a real-world problem
3094260	3098260	that really matters to other scientists or health,
3098260	3100260	or in this case, you know, biology,
3100260	3104260	then actually, you might as well throw the kitchen sink at it,
3104260	3107260	because you actually are really after the output itself,
3107260	3109260	in this case, protein structures.
3109260	3111260	And that's what we did here.
3111260	3113260	We really threw everything we had at it,
3113260	3117260	and it's, I think, the most complex system that we've ever built.
3117260	3120260	Other things to note about this system is that it's also,
3120260	3122260	AlphaFold 1 was relatively slow,
3122260	3126260	took a few weeks of compute time to do a protein.
3126260	3130260	AlphaFold 2 took two weeks to train the whole system
3130260	3134260	on a relatively modest setup of eight TPUs or 150 GPUs,
3134260	3137260	which, by modern-day machine learning standards, is quite small.
3137260	3139260	And then the inference, the predictions,
3139260	3142260	can be done lightening fast in an order of minutes,
3142260	3147260	sometimes seconds for an average protein on a single GPU.
3147260	3150260	So when we did this, AlphaFold 2, we announced the results,
3150260	3152260	published the methods.
3152260	3156260	Over Christmas, that Christmas, this is back in 2020,
3156260	3159260	we were thinking, okay, how should we give access to the system
3159260	3161260	to biologists around the world?
3161260	3164260	And normally what you do is that you set up a server,
3164260	3168260	people, biologists, send you their amino acid sequences,
3168260	3170260	and then you give back, a few days later,
3170260	3172260	you might give them back the prediction.
3172260	3175260	But actually what we realised, because AlphaFold 2 was so fast,
3175260	3179260	we could actually just fold everything ourselves in one go.
3179260	3181260	So we just fold all proteins.
3181260	3184260	And we'll start with the human proteome,
3184260	3188260	just like the human genome equivalent, but in protein space.
3188260	3191260	And so that's what we did over the Christmas.
3191260	3194260	We folded the whole human proteome.
3194260	3197260	And so, which is another thing I love about AI and computing,
3197260	3199260	is you can have your Christmas lunch,
3199260	3202260	and while you're doing that, offer our AIs doing something useful
3202260	3204260	for the world.
3204260	3206260	So the human proteome, so we published that as well
3206260	3209260	in the summer of 21 last summer.
3209260	3212260	So AlphaFold 2, we predicted that every protein
3212260	3215260	in the human body is around 20,000 proteins,
3215260	3218260	represented, obviously, expressed by the human genome.
3218260	3221260	And at the point where we did this, experiments,
3221260	3224260	30 years of experiments, 30, 40 years of experiments,
3224260	3228260	had covered about 17% of the human proteome.
3228260	3231260	And we more than doubled that overnight
3231260	3234260	in terms of very high accuracy structures.
3234260	3237260	Obviously we folded all of them, but very high accuracy,
3237260	3239260	so that's less than one angstrom error.
3239260	3242260	They're sort of up to experimental quality.
3242260	3244260	We went to 36%.
3244260	3247260	And 58% at high accuracy, where we call high accuracy
3247260	3251260	when the backbone is mostly where you can be confident in.
3251260	3254260	But the side chains may be slightly out.
3254260	3257260	And then, of course, the question is what about the rest,
3257260	3259260	the other 42%.
3259260	3262260	And it may be that some of those AlphaFold 2
3262260	3264260	is just bad at, but increasingly,
3264260	3266260	and this is an open research question,
3266260	3269260	when we look at it with biologists, and biologists often send us in results,
3269260	3271260	like, I look at this one folded really well,
3271260	3273260	or this one didn't fold well,
3273260	3276260	we often find that the ones that didn't fold well
3276260	3279260	were actually what's called unstructured in isolation.
3279260	3282260	So they're disorder, intrinsically disorder proteins,
3282260	3285260	which means that until you know what they interact with,
3285260	3287260	they're basically squiggly bits of string.
3287260	3290260	And then presumably, when they interact with something in the body,
3290260	3293260	they then, another protein usually, they'll then form a shape.
3293260	3296260	But we don't know what that shape is in isolation, right?
3296260	3299260	We may not even know what it interacts with at this stage.
3299260	3302260	So, actually, people have turned this around now
3302260	3305260	to use it as a disordered protein predictor.
3305260	3307260	So, where AlphaFold doesn't do well,
3307260	3310260	perhaps that's pretty good evidence that it's a disordered protein,
3310260	3313260	which, of course, is very important in things like disease,
3313260	3318260	Alzheimer's, other things are thought to be to do with badly folded or disordered proteins.
3320260	3323260	One of the other things we did, which was a nice innovation for AlphaFold,
3323260	3327260	was have the system predicted its own confidence in its own predictions.
3327260	3330260	And the reason we did this is we wanted biologists to use this,
3330260	3332260	who maybe would not care about the machine learning techniques
3332260	3335260	or not understand them, or frankly, it would be irrelevant to them.
3335260	3337260	They would just be interested in the structure.
3337260	3340260	And we wanted to make sure that they were easily able to evaluate
3340260	3344260	the quality of that prediction and what parts of it they could rely on.
3344260	3348260	And which other parts they maybe need to check experimentally.
3348260	3352260	So what we did is AlphaFold, basically, we produced predictions,
3352260	3357260	there were split into three thresholds,
3357260	3360260	over 90 was what we call very high accuracy,
3360260	3363260	so less than one angstrom error, experimental quality,
3363260	3365260	greater than 70 was the backbone's correct,
3365260	3368260	and then less than 50 may be these red regions.
3368260	3371260	So you can see in the database that's what they look like.
3371260	3374260	It's something that should not be trusted.
3374260	3379260	We did a further 20 model organisms covering all of the critical model organisms
3379260	3383260	used in research, and also some important other ones in disease,
3383260	3388260	like tuberculosis, and also agriculture, like wheat and rice.
3388260	3394260	And a lot of these proteomes are much less covered than the human proteome.
3394260	3396260	Of course, the human one is where the most effort is being,
3396260	3397260	that's at 17%.
3397260	3400260	For some of these organisms, it's like less than 1%.
3400260	3403260	So for the researchers in those plant scientists and other things,
3403260	3406260	this is a huge boon for them because they would never have the resources
3406260	3411260	to spend that time to crystallise the proteins they're interested in.
3411260	3414260	We then teamed up with Emble EBI,
3414260	3417260	the European Bioinformatics Institute at Cambridge,
3417260	3419260	and they're amazing as a partnership team.
3419260	3422260	They host a lot of the biggest databases around the world already,
3422260	3425260	and we thought the best way to host all this data
3425260	3428260	is to just give it to them and allow them to host it
3428260	3431260	and plug it into the mainstream of biology tools.
3431260	3433260	And so we had a great collaboration with them,
3433260	3437260	and then we basically released all this data for free and unrestricted access
3437260	3442260	for any use, industrial or academic, because it's so completely free.
3442260	3446260	And it's amazing to see the impact of that,
3446260	3450260	and we tried to sort of maximise the scientific impact of this
3450260	3452260	by releasing it in that way.
3452260	3455260	The other thing we did do, and I want to touch this on this at the end,
3455260	3457260	is think about the safety and ethics of this,
3457260	3462260	and we consulted with over 30 experts in various areas of biology
3462260	3465260	bioinformatics, biosecurity and pharma
3465260	3469260	to check that this was going to be okay to release this type of information.
3469260	3472260	And they all came back with that they were not worried about this,
3472260	3475260	but they were potentially worried about future things.
3475260	3477260	So that's something that we bear in mind.
3477260	3480260	There are now a million predictions in the database today.
3480260	3482260	I just want to call out one thing.
3482260	3487260	We specially, ourselves, we specially prioritise neglected tropical diseases,
3487260	3490260	because those are the ones that affect the developing world,
3490260	3492260	the poorest people in the world the most,
3492260	3495260	and they're the least researched, because of course there's no money in it for pharma companies,
3495260	3499260	so that often it's NGOs and non-profits that have to do the work there.
3499260	3501260	So for them, it's amazing to get all the structures,
3501260	3503260	because they can go straight to drug discovery
3503260	3507260	without having to go to the intermediate step of finding these structures.
3507260	3509260	So we prioritise all these diseases,
3509260	3513260	and including ones that we've got being given from the WHO
3513260	3516260	about potential future pathogens.
3516260	3518260	And what's the community done with AlphaFold already?
3518260	3523260	We've seen just in nine months or 10 months incredible amount of work has been done.
3523260	3526260	This is really cool on the left here with some colleagues at Emble.
3526260	3531260	They used AlphaFold and Experiment to combine with their experimental data
3531260	3534260	to put together what's called the nuclear pore complex,
3534260	3536260	which is one of the biggest proteins in the body.
3536260	3538260	It's massive for a protein,
3538260	3541260	and what it is is it's a little gateway into the nucleus of your cell,
3541260	3544260	and it opens and closes to let things in.
3544260	3547260	And they were able to, you know, it's beautiful if you look at it,
3547260	3550260	able to put it all together and then visualise it.
3550260	3554260	I talked about this disorder predictor, WHO top 30 pathogens,
3554260	3557260	and actually interestingly, it's helped experimentalists,
3557260	3559260	the ones that benefited first from this,
3559260	3565260	because they can combine this with their maybe some low resolution images they have,
3565260	3568260	and if they have two sources of information, they can then make a sharp prediction
3568260	3571260	from their maybe their slightly lower resolution experimental data,
3571260	3575260	and then a computational prediction.
3575260	3579260	So it's been really gratifying to see hundreds of papers now
3579260	3583260	and applications already with being used for AlphaFold,
3583260	3586260	also in industry too for drug discovery.
3586260	3588260	So what has the impact been?
3588260	3593260	So we already have 500,000 researchers have used the database.
3593260	3595260	We think that's almost every biologist in the world
3595260	3598260	has probably looked up their proteins they're interested in.
3598260	3601260	190 countries, 1.5 million structures viewed,
3601260	3604260	and already over 3,000 citations,
3604260	3609260	and we've had some nice accolades along the way from science and nature on the method.
3609260	3614260	And then over the next year, we plan to fold every protein, you know,
3614260	3616260	in known to science, which is in Uniprot,
3616260	3619260	which is the massive database that has all the genetic sequences,
3619260	3621260	and there's over 100 million proteins known to science,
3621260	3624260	and we're steadily sort of progressing through that right now,
3624260	3627260	and we'll be releasing that over time.
3627260	3629260	So stepping back then, what does this mean?
3629260	3633260	I think that maybe, you know, we're entering a new era
3633260	3635260	of what I would like to call digital biology.
3635260	3639260	So I think the way I think about biology is that at the most fundamental level,
3639260	3641260	it's an information processing system,
3641260	3646260	albeit an exquisitely complex and emergent one.
3646260	3649260	And I think of it as maybe the potential,
3649260	3652260	the perfect sort of regime for AI to be useful in,
3652260	3656260	because, you know, one thing I think of it analogous to is in physics,
3656260	3659260	you know, we use mathematics to describe physical phenomena,
3659260	3662260	and it's been extraordinarily successful in doing that.
3662260	3665260	Of course, mathematics can also be applied to biology
3665260	3667260	and has been applied successfully in many domains,
3667260	3670260	but I think a lot of these emergent and complex phenomena
3670260	3673260	are just too complicated to be described with a few equations, right?
3673260	3676260	I just don't really see how you can say,
3676260	3680260	come up with, you know, Kepler's laws of motion just from other cell, right?
3680260	3683260	How would one do that? You know, just a few differential equations.
3683260	3685260	It doesn't seem to me likely.
3685260	3689260	And I think maybe a learn model is a better way to approach that.
3689260	3692260	And I think, and I hope that AlphaFold is a proof of concept
3692260	3694260	that this may be possible,
3694260	3698260	and maybe usher, can help usher in this new dawn of digital biology.
3698260	3700260	And our attempts to go further in that space
3700260	3702260	is obviously we're researching further at DeepMind,
3702260	3705260	and the science team, we sort of doubled down on all these things
3705260	3707260	within the biology team at DeepMind.
3707260	3710260	And we've also spun out a new company, Isomorphic Labs,
3710260	3714260	to specifically build on this work and other related work,
3714260	3718260	specifically for drug discovery to accelerate drug discovery,
3718260	3720260	which we hope, using computational and AI methods,
3720260	3723260	can maybe be an order of magnitude quicker.
3723260	3725260	Currently, you know, it takes an average of 10 years
3725260	3729260	to go from identifying a target to a candidate drug.
3730260	3733260	So just to start closing then, I just, you know,
3733260	3735260	there isn't time to go into this, but it's for us,
3735260	3737260	it's been like a renaissance year in some sense.
3737260	3741260	I've been having so much fun ticking off all of my sort of
3741260	3743260	childhood dream projects,
3743260	3746260	infusion and quantum chemistry and conjectures in maths,
3746260	3748260	material science, weather prediction.
3748260	3751260	This has all become reality now in the last year
3751260	3754260	of applying it to important problems in each of these domains
3754260	3756260	and, you know, publishing nice and important work
3756260	3758260	in each of these areas.
3758260	3761260	In applications, of course, there are lots of amazing
3761260	3763260	industrial applications that we've been doing,
3763260	3765260	and we have an applied team at DeepMind
3765260	3768260	that works with Google product teams to incorporate
3768260	3771260	all of our research into hundreds of products now at Google.
3771260	3774260	Pretty much every product you use of Google's
3774260	3776260	will have some DeepMind technology in it.
3776260	3779260	Some of the ones I just want to call out are our data centre work
3779260	3782260	and energy optimisation of data centres and the energy they use
3782260	3784260	and the cooling systems they use,
3784260	3786260	and we're looking at applying that to grid scale now,
3786260	3790260	WaveNet, which is the best text-to-speech system in the world.
3790260	3793260	So any device that you talk to that talks back to you
3793260	3797260	will be using WaveNet to have really realistic voices.
3797260	3801260	Even interesting things like better video compression for YouTube.
3801260	3804260	We can save 4% of the bit rate that is used
3804260	3807260	whilst maintaining video quality
3807260	3809260	and also things like recommendation systems,
3809260	3812260	but there's just too many to mention, actually.
3812260	3814260	And then, of course, very in vogue now,
3814260	3816260	and we have a ton of work on this area,
3816260	3818260	but it will be a whole talk in itself,
3818260	3821260	is large models, and we have our own really cool large models
3821260	3825260	that alpha code that can programme from a text description
3825260	3826260	and write code.
3826260	3829260	It's still amazing to me in competitive programming level.
3829260	3832260	Chinchilla, which is our large language model
3832260	3834260	that is computer-efficient.
3834260	3837260	Flamingo, that's our vision language combined model
3837260	3838260	that can describe images,
3838260	3841260	and then Gata, our latest model that is super general,
3841260	3843260	can do robotics, video games,
3843260	3847260	all sorts of things, language just with one model.
3847260	3849260	So this is all very exciting,
3849260	3852260	but I just want to end my last couple of slides
3852260	3854260	with a bit about ethics,
3854260	3857260	because obviously this is hosted by the Institute of Ethics,
3857260	3860260	and it's a very important topic,
3860260	3862260	and not just because of that,
3862260	3865260	but it's also what the Tana lectures are about, too.
3865260	3868260	So we think a lot about pioneering responsibly.
3868260	3872260	This is actually two of our values at DeepMind combined,
3872260	3875260	pioneering and being responsible.
3875260	3878260	I hope I've convinced you and you hope you will realise
3878260	3881260	that AI is this incredible potential to help
3881260	3884260	with some of humanity's greatest challenges.
3884260	3886260	I think disease, climate,
3886260	3888260	all of these things could be in scope,
3888260	3892260	but obviously AI has to be built responsibly and safely,
3892260	3895260	and we have to make sure the people who are building these things,
3895260	3897260	it's used for the benefit of everyone.
3897260	3900260	So we've had this sort of front of mind
3900260	3902260	from the beginning of DeepMind,
3902260	3905260	and as with any powerful technology,
3905260	3907260	and I think AI is no different,
3907260	3909260	although it may be more general and more powerful
3909260	3911260	than any that has gone before,
3911260	3914260	whether or not it's beneficial or harmful to us in society,
3914260	3917260	depends on how we deploy it and how we use it,
3917260	3920260	and what sorts of things we decide to use it for.
3920260	3923260	And I think it's important that we have a really wide debate
3923260	3926260	about that at places like this and the Institute of Ethics.
3926260	3929260	I'm very excited to see that being set up
3929260	3933260	and for us to interact with the new Institute.
3933260	3936260	Here, just one mention is that DNA has been really critical,
3936260	3939260	and we've been pushing very hard on this the last few years,
3939260	3941260	and I think it's critical to this,
3941260	3944260	to make sure we get the broadest possible input
3944260	3947260	into the design and deployment decisions of these systems,
3947260	3951260	especially for the people that this affects the most,
3951260	3953260	that these systems affect the most.
3953260	3955260	That's something we've been pushing very hard on.
3955260	3957260	There's still a lot more work to do,
3957260	3960260	and there's still a lot more progress at DeepMind,
3960260	3963260	and we've been also doing that with all of our sponsorship that we do.
3963260	3966260	We've now done nearly $50 million worth of sponsorship
3966260	3969260	of scholarships, diversity scholarships, chairs,
3969260	3971260	and academic institutions and projects,
3971260	3974260	and also funding things like the Deep Learning in Darba,
3974260	3977260	which is Africa's biggest conference on machine learning.
3977260	3981260	I'm really proud to say that a lot of DeepMinders helped set that up.
3981260	3984260	And so there's many, many things that we're doing across the industry
3984260	3987260	and act as a role model for the rest of industry.
3988260	3990260	So then on ethics and safety,
3990260	3992260	this has always been central to our mission,
3992260	3995260	because you saw our audacious mission at the start,
3995260	3999260	and we, even back in 2010 in our little attic room,
3999260	4001260	we were planning for success,
4001260	4004260	and of course we had to think through as scientists
4004260	4006260	what does success mean, what will the world look like,
4006260	4008260	and obviously if one thinks that through
4008260	4011260	and it's becoming obvious now in 2022,
4011260	4014260	but it was obvious to us then in 2010
4014260	4016260	that this would have to be critical,
4016260	4019260	that it would be really important questions
4019260	4021260	that would have to be addressed.
4021260	4024260	And part of that, so we've been doing this in the background all along,
4024260	4026260	and we'll be talking more about this work probably in future.
4026260	4029260	We were instrumental in drafting Google's AI principles,
4029260	4031260	which are now publicly available,
4031260	4034260	and they were partly based on our original ethics charter
4034260	4036260	that we've had from the very beginning of DeepMind.
4036260	4039260	And the aim of these principles, and you can look them up later
4039260	4041260	if you want to look at what they say,
4041260	4044260	is obviously to help realise the far-ranging benefits
4044260	4046260	that clearly AI could have for everyone
4046260	4051260	whilst identifying and mitigating potential risks and harms ahead of time.
4051260	4054260	And we continue to try and act as thought leadership
4054260	4057260	for the AI community on many of these topics,
4057260	4060260	strategy risks, ethics, and safety.
4060260	4062260	So what should we do then?
4062260	4064260	And I just want to end with this last slide here,
4064260	4068260	is what I think we should not do is move fast and break things,
4068260	4070260	sort of the Silicon Valley trope.
4070260	4073260	And I think we've seen the consequence of that playing out.
4073260	4077260	It can be very extraordinarily effective to get powerful systems
4077260	4079260	and growth and other things,
4079260	4083260	but I do not think it's the right way to address really powerful
4083260	4087260	potential dual-use technologies like AI.
4087260	4092260	And the problem with it is that one of the things that falls out
4092260	4096260	of moving fast and break things is actually doing live A-B testing in the world,
4096260	4098260	with your minimum viable products and other things.
4098260	4101260	And of course, the question is, if one does that,
4101260	4106260	where does the option B turns out to be a terrible option?
4106260	4108260	Well, where does the harm of that happen?
4108260	4110260	Well, it resides in society, doesn't it?
4110260	4112260	That pays the cost of your learning,
4112260	4114260	because you've done it in the world.
4114260	4119260	And it's probably fine if you're just doing a little gaming app
4119260	4121260	or photo app or something,
4121260	4123260	but we already see with social networks,
4123260	4126260	it's not fine when you're at billion-user scale
4126260	4130260	and things really matter in terms of your A-B testing.
4130260	4134260	I don't think it's responsible to do that.
4134260	4136260	So what should we do instead?
4136260	4141260	Well, fortunately, we already have another method,
4141260	4143260	which I think would be better, the scientific method,
4143260	4147260	which I do think is probably maybe humanity's greatest idea ever.
4147260	4149260	And I think it can apply here.
4149260	4152260	And I think we should use the scientific method
4152260	4155260	when we're approaching how to deal with these
4155260	4159260	very powerful, incredible potential technologies.
4159260	4162260	And what does the scientific method involve here in this domain?
4162260	4165260	Well, it's sort of thoughtful deliberation and thought
4165260	4168260	ahead of time and foresight ahead of time,
4168260	4170260	where you have hypothesis generation
4170260	4172260	on what might happen if one were to be successful
4172260	4174260	with what you're trying to do, right?
4174260	4178260	So how about we think about that ahead of time, not afterwards?
4178260	4182260	Then there's rigorous and careful and control testing.
4182260	4184260	I think that's one of the main things I learned from my PhD,
4184260	4188260	apart from all the neuroscience, was also the value of control tests.
4188260	4190260	I don't think you can really understand.
4190260	4192260	In a way, I think when I started my PhD, at least,
4192260	4195260	I was all about what's the condition of interest,
4195260	4198260	and that's the thing that you're going to make your new advance with.
4198260	4200260	But actually, you can't conclude anything, of course,
4200260	4202260	unless you have good controls.
4202260	4205260	And I think that's something I don't think engineers get
4205260	4207260	first time around, actually.
4207260	4210260	But scientists and researchers, of course, do get that,
4210260	4212260	because that's one of the things that you learn
4212260	4214260	from doing a research PhD.
4214260	4216260	So control testing in controlled environments,
4216260	4218260	not out in the world,
4218260	4221260	until you better understand what it is that you're doing.
4221260	4224260	So, of course, one updates on empirical data,
4224260	4226260	obviously ideally with peer review,
4226260	4228260	so you get critique from the outside
4228260	4230260	and people who are independent from your work,
4230260	4233260	all of these things that are standard in the scientific method, right,
4233260	4235260	but are not standard in engineering.
4235260	4237260	And all of this is in service
4237260	4239260	of getting a better understanding of the system
4239260	4242260	before one deploys it at scale, right,
4242260	4245260	and then maybe you find out something.
4245260	4249260	So my view is that as we approach artificial general intelligence,
4249260	4252260	and it's a super exciting moment in time,
4252260	4255260	as you can hopefully get from my talk
4255260	4257260	and my excitement over that,
4257260	4260260	but we need to treat it with the respect and precaution
4260260	4262260	and sort of humblness, I would say,
4262260	4265260	that the technology of this magnitude demands.
4265260	4268260	And I think that's what we are trying to be at the forefront on,
4268260	4272260	and I think I'll be talking a lot more about this in the future.
4272260	4277260	So I'll just end by on the sort of going back to the science question.
4277260	4280260	I think if we get AI right,
4280260	4282260	it could potentially be the greatest and most beneficial
4282260	4284260	technology humanity has ever invented.
4284260	4287260	And I think of AI as this ultimate general purpose tool
4287260	4291260	to help us as scientists understand the universe better
4291260	4293260	and perhaps our place in it.
4293260	4295260	Thank you.
4295260	4298260	APPLAUSE
4318260	4321260	Well, thank you, Dennis, for that extraordinary tour de force.
4321260	4324260	We do have a little time for questions.
4324260	4327260	But we wanted to give you the chance to kind of give us
4327260	4329260	that sense of your vision.
4329260	4333260	Now, we've got an opportunity to have questions from the audience.
4333260	4336260	Got to wait for the microphone to be handed to them
4336260	4338260	and to stand up if possible when asking questions,
4338260	4341260	but I'm afraid there is a kind of discrimination.
4341260	4345260	It's only those on the ground floor
4345260	4350260	that can ask a question due to health and safety policies in the theatre.
4350260	4353260	So please, if you have a question,
4353260	4355260	please raise your hand,
4355260	4357260	and I'm happy to take questions at this point.
4357260	4360260	So, John, perhaps I'll start with John.
4360260	4362260	I'll give you the provision.
4362260	4364260	There is a roving microphone.
4366260	4369260	And just declare who you are, John,
4369260	4371260	and perhaps stand up and just ask a question.
4371260	4374260	Interesting to begin an ethics talk with some discrimination, Nigel,
4374260	4376260	but I'm John Tysulis.
4376260	4379260	I'm the director of the Institute for Ethics in AI.
4379260	4382260	Thanks so much for a really fascinating and inspirational talk.
4383260	4385260	I guess I want to ask two questions.
4385260	4389260	One is a very general question about the nature of the project you're embarked on.
4389260	4393260	So the objective is to generate a powerful all-purpose tool
4393260	4398260	that will help create new scientific understanding.
4398260	4403260	And the nature of this tool is artificial general intelligence.
4403260	4407260	So that is a tool that can replicate or outperform human beings
4407260	4410260	across a wide range of cognitive tasks.
4411260	4414260	The worry is there attention there.
4414260	4417260	If you had something that could outperform human beings
4417260	4420260	across a wide range of cognitive tasks,
4420260	4423260	could we still regard that as a tool?
4423260	4425260	Or would it become a colleague?
4425260	4427260	So you talked about respecting AI at the end,
4427260	4430260	but it looks like something with that level of capacity
4430260	4433260	would demand a different form of respect
4433260	4437260	that would preclude the original objective of now treating it as a tool.
4437260	4439260	So that's one question.
4439260	4443260	The second question is, you've talked about what will benefit humanity.
4443260	4446260	And so I guess one question I have along these lines,
4446260	4448260	how do you make that determination?
4448260	4453260	So you might say, look, some people have the view that AI applied to military applications
4453260	4456260	will benefit humanity. Others don't.
4456260	4458260	How do you make that determination?
4458260	4461260	And I guess there's also this further dimension.
4461260	4464260	There's a division of labour in making that assessment.
4464260	4470260	Do you think too much has been placed on the shoulders of developers,
4470260	4472260	researchers, corporations,
4472260	4477260	and that really government should step in and resolve some of these issues?
4477260	4479260	Thanks, John. Great question.
4479260	4484260	So I think with your first question,
4484260	4489260	the reason human capabilities are an interesting mapping is because
4489260	4492260	the human brain is the only evidence of general intelligence we have
4492260	4494260	in the universe as far as we know.
4494260	4498260	So I think there's always the question is how do you know you've got there?
4498260	4503260	And you can approximate it with millions of tasks, potentially.
4503260	4504260	So that's one approach.
4504260	4507260	The more tasks you have in your grab bag and it can do all of them
4507260	4510260	and pair against human performance, you might have done it.
4510260	4513260	But there's always the possibility that one might have missed out
4513260	4517260	a particular type of cognitive ability, like creativity or something.
4517260	4519260	So that's why I think...
4519260	4523260	And also I think AI can be applied back to neuroscience as well, by the way.
4523260	4526260	That's one of our scientific areas that we apply AI to,
4526260	4529260	is neuroscience itself and better understanding our own minds.
4529260	4533260	So I have this view that as a neuroscientist
4533260	4535260	that this journey we're embarked on with AI
4535260	4539260	is the most fascinating journey one can ever take scientifically
4539260	4541260	because there's not only the artificial building,
4541260	4544260	it's then comparing that to the human mind
4544260	4547260	and then seeing, I think, uncovering the mysteries of our own minds,
4547260	4549260	what's dreaming, what is creativity, what are emotions,
4549260	4552260	all of these questions that we have, free will,
4552260	4555260	potentially even consciousness, the big questions.
4555260	4559260	I think building AI and intelligent artefacts
4559260	4562260	and then seeing what is missing in them
4562260	4565260	is a good way to explore that scientifically.
4565260	4567260	And so then, I don't know the answer to your question,
4567260	4569260	I think that's part of this journey,
4569260	4572260	is at what point would these things not become just tools.
4572260	4575260	And it may even be that it's a design question
4575260	4579260	because whether we should build what is consciousness we don't know
4579260	4582260	and that would be a whole, obviously, debate in itself,
4582260	4584260	but should we build it to the extent of what it is,
4584260	4586260	should we build them in our systems?
4586260	4589260	I would say no to begin with if we have that choice
4589260	4591260	until we better understand them as tools
4591260	4593260	and then we can bring in that extra complexity of free will
4593260	4596260	and where do they get their goals from?
4596260	4598260	Initially it will be designers,
4598260	4600260	but if they could be self-generated.
4600260	4602260	So I think we're still a long way away from those things,
4602260	4605260	but that's one of the things I think we should inch towards
4605260	4608260	very cautiously and with precautions
4608260	4612260	because also it will get to the heart of what it means to be human.
4612260	4615260	And I think that should exactly be done multidisciplinary
4615260	4620260	with philosophers and ethicists and theologians
4620260	4622260	and the wider humanities.
4622260	4625260	I think this is where the humanities comes in,
4625260	4627260	as well as the science.
4627260	4632260	So I think that's all to come.
4632260	4634260	OK, a question in the front row?
4642260	4644260	Thank you so much for a great presentation.
4644260	4646260	Carina Prunkel, I'm a research fellow at the Institute.
4646260	4651260	So you mentioned at various points the potential for dual use
4651260	4653260	and in particular malicious dual use.
4653260	4658260	So I'm curious to hear how you approach this topic at DeepMind.
4658260	4662260	So what precautions or how do you address the potential for dual use?
4662260	4664260	Great.
4664260	4667260	So we have a lot of different mechanisms now at DeepMind
4667260	4669260	that have been built up over time.
4669260	4672260	So one is the Institutional Review Committee we have,
4672260	4676260	which is formed, so it's chaired by Laila Ibrahim, our COO,
4676260	4681260	and it's formed with different people from across the company.
4681260	4684260	We have legal, we have ethicists and philosophers as well.
4684260	4687260	It's also rotating boards, some senior researchers,
4687260	4689260	and they get involved early with research projects
4689260	4692260	and try to assess them from all aspects,
4692260	4695260	and they will draw on outside experts.
4695260	4697260	So they bring in biologists, for example,
4697260	4699260	for alpha-fold biothesists,
4699260	4701260	so things we might not have in-house.
4701260	4706260	And then they work with the research teams to either say no,
4706260	4709260	that project should not proceed, OK, it can with caveats,
4709260	4711260	or why don't you build or do it in a different way
4711260	4713260	with these safeguards?
4713260	4716260	So that's our prototype, I would say, committee
4716260	4719260	that does these things, and we're kind of exercising our muscle
4719260	4722260	when the stakes are relatively low currently
4722260	4725260	so that we can learn from what works and is effective
4725260	4729260	as we get more powerful systems.
4729260	4731260	And obviously over time, I think at some point
4731260	4735260	there have got to be outside bodies that get involved.
4735260	4739260	But the problem is that, and we've experimented with that too,
4739260	4741260	is that a lot of these things are very specific
4741260	4743260	to the technology itself.
4743260	4747260	So one has to sort of understand the technology to a deep level,
4747260	4749260	maybe even have access to it somehow,
4749260	4752260	but in a controlled way, because one can't just...
4752260	4754260	Open sourcing is not just a panacea either,
4754260	4756260	because if it's a dangerous system, open sourcing
4756260	4759260	it means any bad actor can use it too, for anything.
4759260	4761260	So there's a lot of complicated, I think,
4761260	4763260	ethical questions around this.
4763260	4765260	But I don't think there's an easy answer.
4765260	4768260	So anyone who thinks there is one, I think, is kidding themselves.
4768260	4771260	I hope everyone realises the complexity involved.
4771260	4773260	But I think it's pretty...
4773260	4775260	I'm very happy with our internal system,
4775260	4778260	but I appreciate more is going to be needed than that
4778260	4781260	as the systems get more powerful and impact more of the world.
4781260	4783260	A question just behind you, I think,
4783260	4786260	if you just pass the microphone literally behind you.
4786260	4788260	Hi, my name is Ulrich.
4788260	4790260	I'm a postdoc at the Computer Science Department
4790260	4792260	in the Human Central Computing Group.
4792260	4794260	So DeepMind looks like it's this great example
4794260	4797260	of how you can take the best from science
4797260	4800260	and then sort of bring it together with a commercial company
4800260	4802260	and then make very rapid progress.
4802260	4804260	And you mentioned in the end here how you thought
4804260	4806260	that the scientific process should sort of inspire
4806260	4809260	the commercial world, as it were.
4809260	4812260	I'm curious about what you think about the other way around.
4812260	4816260	So what have you learnt by being embedded in Google
4816260	4820260	that you think we as researchers should learn from
4820260	4822260	in order to make more rapid progress?
4822260	4824260	Yeah, you're absolutely right.
4824260	4827260	That was the thinking, the original vision behind the...
4827260	4830260	So I spoke about the original vision of the company,
4830260	4832260	this Apollo programme,
4832260	4836260	but the original vision behind the organisational setup
4836260	4838260	and processes was to be a hybrid
4838260	4840260	like the best of both worlds.
4840260	4843260	Startups and the energy and creativity
4843260	4846260	and pace that they have and nimblness
4846260	4848260	and the best from academic research,
4848260	4850260	the blue sky thinking, ambitious thinking
4850260	4853260	that happens there, but sometimes with a lot of bureaucracy.
4853260	4856260	So I think that we did actually successfully
4856260	4858260	combine those two things.
4858260	4861260	And then when we agreed to get acquired,
4861260	4863260	we combined it with the third thing,
4863260	4866260	which is scale and resources of a large,
4866260	4868260	very successful company like Google.
4868260	4870260	And I think that's the main lesson,
4870260	4873260	is to make sure you do things at huge impact
4873260	4877260	and have ambition and realise that you can scale things
4877260	4879260	to that and the consequence that come with that,
4879260	4881260	but also the potential of that.
4881260	4884260	So I think we've done that now and very well,
4884260	4887260	like Mary or three of those aspects together.
4887260	4889260	It's a daily challenge because as we get bigger,
4889260	4891260	one tends to get slower as an organisation.
4891260	4895260	So we have to fight against that all the time.
4895260	4897260	But it's pretty unique, I would say,
4897260	4900260	the organisational and cultural feel of the mind.
4900260	4902260	But it could be a blueprint for other,
4902260	4906260	I would say, grand projects could be organised in a similar way.
4906260	4908260	OK, I'm going to just switch to this side
4908260	4910260	and then I'm going to go to the question there
4910260	4912260	and the question about that.
4919260	4921260	So move fast and break things with a quote
4921260	4924260	from people who built a social network.
4924260	4927260	If the mind was to build a social network
4927260	4931260	using the deep-wined way of doing things,
4931260	4935260	then what metrics would you use,
4935260	4939260	would you optimise to judge the quality of your social network?
4939260	4942260	And the second question that comes with it is,
4942260	4945260	do you in fact have a moral obligation
4945260	4947260	to build that social network?
4949260	4951260	OK, so thanks Tim,
4951260	4953260	two complicated questions there.
4953260	4956260	It's actually just generally,
4956260	4959260	so let's see, I have to be careful what I say,
4959260	4963260	but I think social networks have never really been my thing,
4963260	4966260	but also I haven't really thought a lot about it
4966260	4968260	relative to scientific advances
4968260	4971260	and the sorts of things that are my personal passion.
4971260	4974260	I would question actually the premise of your question,
4974260	4978260	which is that how much value does weak ties like that give,
4978260	4981260	like superficial connections like that,
4981260	4983260	versus deeper ties that you get in real life
4983260	4985260	with your real family and friends?
4985260	4988260	I think it's an interesting thing to understand.
4988260	4991260	Are we sacrificing deeper, more meaningful moments
4991260	4994260	for hundreds of more superficial moments?
4994260	4997260	It's not entirely clear to me that the metric of,
4997260	4999260	and it sounds seductive, connect the world,
4999260	5001260	like why would that be bad?
5001260	5003260	But this is the thing I'm talking about with the scientific method,
5003260	5006260	is to try and think through the full consequences of what that would mean.
5006260	5009260	Echo chambers, manipulation, all the rest of it
5009260	5012260	that we all know very well don't need to go into.
5012260	5014260	So I think if I was to do something like that,
5014260	5017260	I would use the scientific method again
5017260	5020260	to try and really think through ahead of time
5020260	5024260	what do you want as the outcomes and metrics?
5024260	5027260	In fact, often trying to find the right metrics
5027260	5030260	that actually drive the right behaviour that you think is good in the world
5030260	5031260	is half the challenge.
5031260	5033260	It's like asking the right question in science.
5033260	5036260	Everybody who does science knows that asking the question is the hardest thing.
5036260	5038260	What is the right question?
5038260	5040260	And it's especially hard.
5040260	5041260	Oh, you want an answer?
5041260	5044260	Well, I wouldn't want to give you an answer on the spot,
5044260	5046260	but we can talk about it over dinner.
5046260	5050260	At least one should attempt to start with serious thinking
5050260	5052260	about the question first, right?
5052260	5053260	That's the first part.
5053260	5056260	I don't know what the answer is because I've not given it enough thought.
5056260	5059260	But one should at least understand the meta level of like
5059260	5061260	that's how one should start,
5061260	5064260	including whether one should do that thing at all, potentially.
5064260	5067260	It could be the answer of that hypothesis generation.
5067260	5068260	Okay.
5068260	5071260	I'm going to try and get three more questions in.
5071260	5072260	We're right up against the clock.
5072260	5073260	We've got about seven and a half minutes.
5073260	5075260	There's a question here from Helen.
5079260	5080260	Thank you.
5080260	5082260	I'm Ellen Landomer from Yale University
5082260	5084260	and visiting fellow at the Research Centre for Ethics in AI.
5084260	5086260	Thank you for a brilliant talk.
5086260	5091260	So you showed us how AI can help us figure out the truth of the universe.
5091260	5092260	Pretty much.
5092260	5094260	How about the moral world?
5094260	5096260	How about the political universe?
5096260	5098260	Philosophy starts with Plato's Republic,
5098260	5100260	which is an attempt to figure out the best constitution.
5100260	5103260	Surely, unless one is a complete moral relativist,
5103260	5107260	there are some invariants we're trying to figure out about the moral world.
5107260	5109260	Could AI help us map that out?
5109260	5112260	Could it figure out like the best social organisation,
5112260	5114260	you know, boring from, I don't know,
5114260	5120260	all the things we've tried, capitalism, socialism, libertarianism, egalitarianism?
5120260	5122260	Would it help expand our imagination
5122260	5124260	and perhaps assuming you have an objective function
5124260	5128260	like satisfying major Italian preferences
5128260	5132260	subject to constraints to protect minority rights or something like that?
5132260	5133260	What do you see in the future?
5133260	5136260	We took through 2,000 years and we haven't made much progress.
5138260	5139260	Good question.
5139260	5143260	I mean, look, I think the morality and political science
5143260	5147260	I think is one of the hardest things that AI,
5147260	5149260	you know, I think it can contribute in some way,
5149260	5152260	but I would say it's far harder than the physical sciences,
5152260	5153260	right, or the life sciences,
5153260	5156260	because the most complex things in the world are humans,
5156260	5160260	for human beings to understand and to model
5160260	5164260	and to understand people's motivations, especially in aggregate.
5164260	5168260	I think one way it could help is there's also the question of
5168260	5173260	even if an theoretical AI could come up with a better political construct,
5173260	5178260	would humans, beings and society accept that or even care or understand it?
5178260	5182260	So there's all those questions to try and and would it be implemented correctly?
5182260	5184260	Obviously there's obviously implementation problems.
5184260	5187260	I think more interesting maybe would be,
5187260	5189260	and I've talked to economists about this is,
5189260	5192260	and we did quite a lot of research on multi-agent systems.
5192260	5197260	So again, having a little sandbox or simulation of millions of agents
5197260	5201260	with interacting with each other, with motivations and some goals seeking things.
5201260	5205260	And I think we're missing that experimental testbed actually
5205260	5207260	from political science and economics quite a lot,
5207260	5210260	because again, economics is one of those things where
5210260	5212260	and political science where you sort of have to test it live,
5212260	5213260	a, b, test it in the world.
5213260	5216260	It's like, are we going to go for this political system or not?
5216260	5218260	Should we raise inflation or not?
5218260	5220260	Well, you've got models, but then you actually just have to do it
5220260	5223260	and then see, oh, it's caused a recession or something,
5223260	5225260	where maybe we shouldn't do that next time.
5225260	5228260	And so it would be better if I think if we had a simulation
5228260	5231260	or a sandbox perhaps populated with AI systems
5231260	5235260	that are approximates to idealise forms of humans
5235260	5239260	and then we can maybe make some interesting,
5239260	5243260	experimental work in that much lower stakes.
5243260	5246260	So I think that could be really fascinating exploration area
5246260	5251260	for things like market dynamics and setting the environmental settings
5251260	5254260	to create more cooperation or something.
5254260	5258260	I would be, far as economists, I would be trying to use all those things.
5258260	5262260	I used to be fascinated when I was a kid with Santa Fe Institute
5262260	5266260	and they used to do lots of really cool models of agent-based systems
5266260	5268260	in little grid worlds.
5268260	5272260	And I loved going artificial societies, I think, by Axelrod.
5272260	5275260	I loved those kind of work, actually what I used to dream about,
5275260	5278260	going to Santa Fe to work on something like that.
5278260	5283260	And I still think that would be pretty cool to have some sort of system like that.
5283260	5285260	Let's see if we can squeeze just a few more.
5285260	5289260	There's a question chap in the, who caught my eye there, yes?
5291260	5293260	Just very, and try and squeeze them in,
5293260	5295260	because there's two more questions over here
5296260	5298260	Super, yeah, I just have a quick question to be honest.
5298260	5304260	So I think at the end you mentioned creating AI in the image of scientific method.
5304260	5309260	And the title of your lecture is advancement of science through AI.
5309260	5313260	But in what sense do you think that neural networks
5313260	5316260	or the limited understanding I have of AI is,
5316260	5320260	in what sense do they follow the notion of scientific method we have?
5320260	5325260	Is there any sense of talking about hypothesis and then testing?
5325260	5328260	Because it doesn't seem that neural networks work in that way.
5328260	5331260	They're opaque for most practical purposes.
5331260	5336260	And if they do outperform us, should we just get rid of the scientific method?
5336260	5341260	So by the way, it's not in the image of the scientific method just to be clear.
5341260	5344260	It's using the approach of the scientific method.
5344260	5346260	I'm not sure what image in the scientific method means.
5347260	5353260	And yes, today that is true that a lot of the systems we have are kind of black box-like.
5353260	5356260	But I think that's exactly what we should be doing more work on,
5356260	5358260	is making them less opaque.
5358260	5360260	There's no reason why they should be.
5360260	5362260	The way I say it to my neuroscience team is,
5362260	5365260	look, we understand quite a lot about the brain now, the ultimate black boxes.
5365260	5369260	We have MRI machines and amazing tools and single cell recording.
5369260	5370260	So it's amazing.
5370260	5372260	And that's why I got into neuroscience in the mid 2000s.
5372260	5376260	So we can actually look into, we don't have to do philosophy of mind necessarily,
5376260	5378260	although we should know about that.
5378260	5382260	But we can actually empirically look at this, not just do introspection.
5382260	5387260	And so as a minimum, in the field of artificial minds,
5387260	5392260	we should know as much about them as we do with the real brain.
5392260	5394260	And we don't know everything about the real brain.
5394260	5396260	Obviously there's tons still we don't know.
5396260	5400260	But there's a lot more that we do know than we do about these artificial systems.
5400260	5402260	And it should be the other way around.
5402260	5406260	That should be the minimum we understand because we have access to every neuron,
5406260	5409260	you know, neuron, artificial neuron in the artificial brain.
5409260	5412260	And we can completely control the experimental conditions.
5412260	5415260	So as a minimum, so I sometimes say this is a challenge to the team.
5415260	5419260	What's the equivalent of fMRI for a neural network?
5419260	5421260	What's the equivalent of single cell recording?
5421260	5422260	We do ablation studies.
5422260	5424260	So we have a whole neuroscience team that's thinking about this
5424260	5429260	and bringing neuroscience techniques, analysis techniques over to AI.
5429260	5434260	Now, in the defence of the engineers, one of the reasons that this has happened
5434260	5439260	is because the brain is obviously a static system we're all fascinated by, of course.
5439260	5442260	But artificial systems change over time.
5442260	5445260	Like AlphaGo is now in ancient history of AI, right?
5445260	5447260	Although it was very meaningful over time.
5447260	5450260	And it takes years to study a system, right?
5450260	5452260	It takes years to build it, and then it takes years to study it.
5452260	5455260	So should you use that research time on studying a system
5455260	5459260	that itself will be out of date by the time you come to any conclusions about it?
5459260	5462260	So I think only now are we reaching the point where we have systems
5462260	5465260	that are interesting enough, do enough interesting things in the world,
5465260	5469260	like large models and AlphaFold type things,
5469260	5473260	that probably it's worth spending the research time on that.
5473260	5477260	And so I think over the next decade we're going to see a lot more understanding
5477260	5478260	of what these systems do.
5478260	5481260	I don't think there's some weird reason why that can't happen.
5481260	5483260	Okay, there are so many more questions.
5483260	5485260	I am literally in the red now.
5485260	5488260	I'm going to have to call this to a close.
5488260	5489260	I do apologise.
5489260	5493260	There is so much pent up, I think, interest and questions for you, Demis.
5493260	5497260	All I can say at this point is absolutely a wonderful lecture
5497260	5499260	where five minutes later than we should have been.
5499260	5502260	The Sheldanian was to a strict regime when it comes to timekeeping.
5502260	5505260	You gave us the most fascinating insights
5505260	5509260	and you have given, I think, to the world with your company
5509260	5513260	and your own talents, a quite wonderful vision of a future
5513260	5518260	in which AI can help us flourish, empower us and not oppress us.
5518260	5520260	So thank you very much.
5539260	5541260	Thank you.
