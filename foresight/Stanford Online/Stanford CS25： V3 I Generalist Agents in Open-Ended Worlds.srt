1
00:00:00,000 --> 00:00:09,640
Hi everyone, thanks for coming to our CS35 lecture today.

2
00:00:09,640 --> 00:00:14,240
So today we're honored to have Jim Van from NVIDIA, who we're talking about generalist

3
00:00:14,240 --> 00:00:21,080
agents in open-ended worlds, and he's a senior AI research scientist at NVIDIA, where his

4
00:00:21,080 --> 00:00:26,800
mission is to build generally capable AI agents with applications to gaming, robotics, and

5
00:00:26,800 --> 00:00:29,000
software automation.

6
00:00:29,000 --> 00:00:33,480
He has research spans, foundation models, multi-modal AI, reinforcement learning, and

7
00:00:33,480 --> 00:00:34,800
open-ended learning.

8
00:00:34,800 --> 00:00:41,360
He obtained his PhD degree in computer science from here, Stanford, advised by Professor

9
00:00:41,360 --> 00:00:42,360
Pepe Lee.

10
00:00:42,360 --> 00:00:49,080
And previously, he did research internships at OpenAI, Google AI, as well as Mila Quebec

11
00:00:49,080 --> 00:00:57,760
AI Institute, so yeah, we'll give it up for Jim.

12
00:00:57,760 --> 00:01:02,440
Yeah, thanks for having me.

13
00:01:02,440 --> 00:01:06,880
So I want to start with a story of two kittens.

14
00:01:06,880 --> 00:01:12,320
It's a story that gave me a lot of inspiration over the career, over my career.

15
00:01:12,320 --> 00:01:14,600
So I want to share this one first.

16
00:01:14,600 --> 00:01:19,520
Back in 1963, there were two scientists from MIT held in Hine.

17
00:01:19,520 --> 00:01:25,720
They did this ingenious experiment, where they put two newborn kittens in this device,

18
00:01:25,720 --> 00:01:28,160
and the kittens have not seen the visual world yet.

19
00:01:28,160 --> 00:01:33,120
So it's kind of like a merry-go-round, where the two kittens are linked by a rigid mechanical

20
00:01:33,120 --> 00:01:37,040
bar, so their movements are exactly mirrored.

21
00:01:37,040 --> 00:01:41,040
And there's an active kitten on the right-hand side, and that's the only one able to move

22
00:01:41,040 --> 00:01:46,880
freely, and then transmit the motion over this link to the passive kitten, which is

23
00:01:46,880 --> 00:01:51,760
confined to the basket, and cannot really control its own movements.

24
00:01:51,760 --> 00:01:57,280
And then after a couple of days, held in Hine, kind of take the kittens out of this merry-go-round,

25
00:01:57,280 --> 00:01:59,440
and then did visual testing on them.

26
00:01:59,440 --> 00:02:03,560
And they found that only the active kitten was able to develop a healthy visual motor

27
00:02:03,560 --> 00:02:09,520
loop, like responding correctly to approaching objects or visual cliffs, but the passive

28
00:02:09,520 --> 00:02:13,200
kitten did not have a healthy visual system.

29
00:02:13,200 --> 00:02:19,960
So I find this experiment fascinating, because it shows the importance of having this embodied

30
00:02:19,960 --> 00:02:26,400
active experience to really ground a system of intelligence.

31
00:02:26,400 --> 00:02:30,440
And let's put this experiment in today's AI context right.

32
00:02:30,440 --> 00:02:35,800
We actually have a very powerful passive kitten, and that is ChargerBT.

33
00:02:35,800 --> 00:02:41,880
It passively observes and rehearses the text on the internet, and it doesn't have any embodiment.

34
00:02:41,880 --> 00:02:46,920
And because of this, its knowledge is kind of abstract and ungrounded, and that partially

35
00:02:46,920 --> 00:02:51,760
contributes to the fact that ChargerBT hallucinates things that are just incompatible with our

36
00:02:51,760 --> 00:02:55,480
common sense and our physical experience.

37
00:02:55,480 --> 00:03:01,840
And I believe the future belongs to active kittens, which translates to generalist agents.

38
00:03:01,840 --> 00:03:06,640
They are the decision makers in a constant feedback loop, and they're embodied in this

39
00:03:06,640 --> 00:03:08,120
fully immersive world.

40
00:03:08,120 --> 00:03:12,280
They're also not mutually exclusive with the passive kitten.

41
00:03:12,280 --> 00:03:18,360
And in fact, I see the active embodiment part as a layer on top of the passive pre-training

42
00:03:18,360 --> 00:03:22,560
from lots and lots of internet data.

43
00:03:22,560 --> 00:03:25,000
So are we there yet?

44
00:03:25,000 --> 00:03:27,640
Have we achieved generalist agent?

45
00:03:27,640 --> 00:03:31,760
Back in 2016, I remember it was like spring of 2016.

46
00:03:31,760 --> 00:03:35,440
I was sitting in an undergraduate class at Columbia University, but I wasn't paying attention

47
00:03:35,440 --> 00:03:36,720
to the lecture.

48
00:03:36,720 --> 00:03:43,720
I was watching a board game tournament on my laptop, and this screenshot was the moment

49
00:03:43,720 --> 00:03:49,560
when AlphaGo versus LisaDoll and AlphaGo won three matches out of five and became the

50
00:03:49,560 --> 00:03:52,800
first ever to be the human champion at the game of Go.

51
00:03:52,800 --> 00:03:54,600
I remember the adrenaline that day, right?

52
00:03:54,600 --> 00:03:56,080
I've seen history unfold.

53
00:03:56,520 --> 00:04:00,960
Oh my God, we're finally getting to AGI, and everyone's so excited.

54
00:04:00,960 --> 00:04:05,440
And I think that was the moment when AI agents entered the mainstream.

55
00:04:05,440 --> 00:04:14,920
But when the excitement fades, I felt that even though AlphaGo was so mighty and so great,

56
00:04:14,920 --> 00:04:18,480
it could only do one thing and one thing alone.

57
00:04:18,480 --> 00:04:25,560
And afterwards, in 2019, there were more impressive achievements, like Open AI 5 beating the human

58
00:04:25,560 --> 00:04:30,960
champions at the game of Dota and AlphaStar from DeepMind beat StarCraft.

59
00:04:30,960 --> 00:04:37,200
But all of these, with AlphaGo, they all have a single kind of theme, and that is to beat

60
00:04:37,200 --> 00:04:38,200
the opponent.

61
00:04:38,200 --> 00:04:45,200
There is this one objective that the agent needs to do, and the models trained on Dota

62
00:04:45,200 --> 00:04:48,760
or Go cannot generalize to any other tasks.

63
00:04:48,760 --> 00:04:52,760
It cannot even play other games like Super Mario or Minecraft.

64
00:04:52,760 --> 00:04:59,880
The world is fixed and has very little room for open-ended creativity and exploration.

65
00:04:59,880 --> 00:05:04,800
So I argued that a journalist agent should have the following essential properties.

66
00:05:04,800 --> 00:05:10,360
First, it should be able to pursue very complex, cementally rich and open-world objectives.

67
00:05:10,360 --> 00:05:14,800
Basically, you explain what you want in natural language, and the agent should perform the

68
00:05:14,800 --> 00:05:17,200
actions for you in a dynamic world.

69
00:05:17,200 --> 00:05:22,120
And second, the agent should have a large amount of pre-trained knowledge instead of

70
00:05:22,120 --> 00:05:27,600
knowing only a few concepts that's extremely specific to the task.

71
00:05:27,600 --> 00:05:29,920
And third, massively multitask.

72
00:05:29,920 --> 00:05:35,680
A journalist agent, as the name implies, needs to do more than just a couple of things.

73
00:05:35,680 --> 00:05:42,240
It should be, in the best case, infinitely multitask, as expressive as human language

74
00:05:42,240 --> 00:05:44,440
can dictate.

75
00:05:44,440 --> 00:05:46,200
So what does it take?

76
00:05:46,200 --> 00:05:49,120
Correspondingly, we need three main ingredients.

77
00:05:49,600 --> 00:05:51,960
First is the environment.

78
00:05:51,960 --> 00:05:58,320
The environment used to be open-ended enough because the agent's capability is upper-bounded

79
00:05:58,320 --> 00:06:01,240
by the environment complexity.

80
00:06:01,240 --> 00:06:05,680
And I'd argue that Earth is actually a perfect example because it's so open-ended, this world

81
00:06:05,680 --> 00:06:10,960
we live in, that it allows an algorithm called natural evolution to produce all the diverse

82
00:06:10,960 --> 00:06:14,400
forms and behaviors of life on this planet.

83
00:06:14,400 --> 00:06:19,760
So can we have a simulator that is essentially a low-fi Earth, but we can still run it on

84
00:06:19,760 --> 00:06:23,360
the lab clusters?

85
00:06:23,360 --> 00:06:27,760
And second, we need to provide the agent with massive pre-training data because exploration

86
00:06:27,760 --> 00:06:31,800
in an open-ended world from scratch is just intractable.

87
00:06:31,800 --> 00:06:34,280
And the data will serve at least two purposes.

88
00:06:34,280 --> 00:06:37,520
One as a reference manual on how to do things.

89
00:06:37,520 --> 00:06:42,680
And second, as a guidance on what are the interesting things worth pursuing.

90
00:06:43,080 --> 00:06:48,800
GPT is only, at least up to GPT-4, it only learns from pure text on the web.

91
00:06:48,800 --> 00:06:55,600
But can we provide the agent with much richer data, such as video walkthrough, or like multimedia,

92
00:06:55,600 --> 00:07:00,600
Wiki documents, and other media forms?

93
00:07:00,600 --> 00:07:06,000
And finally, once we have the environment and the database, we are ready to train foundation

94
00:07:06,000 --> 00:07:08,320
models for the agents.

95
00:07:08,320 --> 00:07:13,320
And it should be flexible enough to pursue the open-ended tasks without any task specific

96
00:07:13,320 --> 00:07:20,960
assumptions, and also scalable enough to compress all of the multi-modal data that I just described.

97
00:07:20,960 --> 00:07:24,880
And here language, I argue, will play at least two key roles.

98
00:07:24,880 --> 00:07:30,960
One is as a simple interface to communicate a task, to communicate the human intentions

99
00:07:30,960 --> 00:07:38,040
to the agent, and second as a bridge to ground all of the multi-modal concepts and signals.

100
00:07:38,040 --> 00:07:45,480
And that train of thought landed us in Minecraft, the best-selling video game of all time.

101
00:07:45,480 --> 00:07:51,640
And for those who are unfamiliar, Minecraft is a procedurally generated 3D voxel world,

102
00:07:51,640 --> 00:07:55,480
and in the game you can basically do whatever your heart desires.

103
00:07:55,480 --> 00:08:01,720
And what's so special about the game is that unlike AlphaGo, StarCraft, or Dota, Minecraft

104
00:08:01,720 --> 00:08:07,880
defines no particular objective to maximize, no particular opponent to beat, and doesn't

105
00:08:07,880 --> 00:08:09,960
even have a fixed storyline.

106
00:08:09,960 --> 00:08:13,760
And that makes it very well-suited as a truly open-ended AM playground.

107
00:08:13,760 --> 00:08:18,160
And here we see people doing extremely impressive things in Minecraft.

108
00:08:18,160 --> 00:08:24,840
Like this is a YouTube video where a gamer built the entire Hogwarts castle block-by-block

109
00:08:24,840 --> 00:08:28,200
by hand in the game.

110
00:08:28,200 --> 00:08:32,120
And here's another example of someone just digging a big hole in the ground and then

111
00:08:32,120 --> 00:08:36,680
making this beautiful underground temple with a river nearby.

112
00:08:36,680 --> 00:08:40,120
It's all crafted by hand.

113
00:08:40,120 --> 00:08:41,120
And one more.

114
00:08:41,120 --> 00:08:46,560
This is someone building a functioning CPU circuit inside a game because there is something

115
00:08:46,560 --> 00:08:53,400
called Redstone in Minecraft that you can build circuits out of it, like logical gates.

116
00:08:53,400 --> 00:08:55,360
And actually the game is too incomplete.

117
00:08:55,360 --> 00:08:58,400
You can, you know, simulate a computer inside a game.

118
00:08:58,400 --> 00:09:00,600
Just think about how crazy that is.

119
00:09:00,600 --> 00:09:06,400
And here I want to highlight a number that is 140 million active players.

120
00:09:06,400 --> 00:09:11,360
And just to quote this numbering perspective, this is more than twice the population of

121
00:09:11,360 --> 00:09:13,240
the UK.

122
00:09:13,240 --> 00:09:17,680
And that is the amount of people playing Minecraft on a daily basis.

123
00:09:17,680 --> 00:09:22,800
And you know, it just so happens that gamers are generally happier than PhDs.

124
00:09:22,800 --> 00:09:26,640
So they love to stream and share what they're doing.

125
00:09:26,640 --> 00:09:30,640
And that produces a huge amount of data every day online.

126
00:09:30,640 --> 00:09:35,720
And there's this treasure trove of learning materials that we can tap into for training

127
00:09:35,720 --> 00:09:36,720
generalizations.

128
00:09:36,720 --> 00:09:41,680
You know, remember the data is the key for foundation models.

129
00:09:41,680 --> 00:09:48,600
So we introduce MindDojo, a new open framework to help the community develop generally capable

130
00:09:48,600 --> 00:09:55,440
agents using Minecraft as a kind of primordial soup.

131
00:09:55,440 --> 00:10:01,360
MindDojo features three major parts, an open-ended environment, an international knowledge base,

132
00:10:01,360 --> 00:10:06,560
and then a generalized agent developed with a simulator and a massive data.

133
00:10:06,560 --> 00:10:09,080
So let's zoom in the first one.

134
00:10:09,080 --> 00:10:15,280
Here's a sample gallery of the interesting things that you can do with MindDojo's API.

135
00:10:15,280 --> 00:10:19,440
We feature a massive benchmark suite of more than 3,000 tasks.

136
00:10:19,440 --> 00:10:25,560
And this is by far the largest open source agent benchmark to our knowledge.

137
00:10:25,560 --> 00:10:29,720
And we implement a very versatile API that unlocks the full potential of the game.

138
00:10:29,720 --> 00:10:36,960
Like for example, MindDojo supports multi-modal observation for action space, like moving

139
00:10:36,960 --> 00:10:39,840
or attack or inventory management.

140
00:10:39,840 --> 00:10:46,640
And that can be customized at every detail, like you can tweak the terrain, the weather,

141
00:10:46,640 --> 00:10:52,680
block placement, monster spawning, and just anything you want to customize in the game.

142
00:10:52,680 --> 00:10:58,920
And given this simulator, we introduce around 1,500 programmatic tasks, which are tasks

143
00:10:58,960 --> 00:11:03,240
that have ground true success conditions defined in Python code.

144
00:11:03,240 --> 00:11:08,160
And you can also explicitly write down spars or the best reward functions using this API.

145
00:11:08,160 --> 00:11:12,840
And some examples are harvesting different resources and unlocking the tech tree

146
00:11:12,840 --> 00:11:16,600
or fighting various monsters and getting reward.

147
00:11:16,600 --> 00:11:21,480
And all these tasks come with language prompts that are templated.

148
00:11:21,480 --> 00:11:26,560
Next, we also introduce 1,500 creative tasks that are freeform and open-ended.

149
00:11:26,560 --> 00:11:30,640
And that is in contrast to the programmatic tasks I just mentioned.

150
00:11:30,640 --> 00:11:35,280
So for example, let's say we want the agent to build a house.

151
00:11:35,280 --> 00:11:37,920
But what makes a house a house, right?

152
00:11:37,920 --> 00:11:40,480
It is L defined and just like image generation.

153
00:11:40,480 --> 00:11:45,200
You don't know if it generates a cat correctly or not.

154
00:11:45,200 --> 00:11:50,760
So it's very difficult to use simple Python programs to give these kind of tasks reward functions.

155
00:11:50,760 --> 00:11:56,360
And the best way is to use foundation models trained on Internet skill knowledge.

156
00:11:56,360 --> 00:12:04,360
So that the model itself understands abstract concepts like, you know, the concept of a house.

157
00:12:04,360 --> 00:12:08,280
And finally, there's one task that holds a very special status called play suit,

158
00:12:08,280 --> 00:12:12,520
which is to beat the final boss of Minecraft, the ender dragon.

159
00:12:12,520 --> 00:12:14,560
So Minecraft doesn't force you to do this task.

160
00:12:14,560 --> 00:12:17,120
As we said, it doesn't have a fixed storyline.

161
00:12:17,120 --> 00:12:23,120
But it's still considered a really big milestone for any kind of beginner human players.

162
00:12:23,120 --> 00:12:28,240
I want to highlight it is an extremely difficult task that requires very complex preparation,

163
00:12:28,240 --> 00:12:30,920
exploration, and also martial skills.

164
00:12:30,920 --> 00:12:36,800
And for an average human, it will take many hours or even days to solve.

165
00:12:36,800 --> 00:12:40,440
Easily over like one million action steps in a single episode.

166
00:12:40,440 --> 00:12:45,600
And that would be the longest benchmarking task for policy learning ever created here.

167
00:12:45,600 --> 00:12:49,280
So I admit, I am personally a below average human.

168
00:12:49,280 --> 00:12:51,960
I was never able to beat the ender dragon.

169
00:12:51,960 --> 00:12:59,160
And my friends laugh at me, and I'm like, OK, one day my AI will avenge my poor skills.

170
00:12:59,160 --> 00:13:03,200
That was one of the motivations for this project.

171
00:13:03,200 --> 00:13:09,280
Now, let's move on to the second ingredient, the Internet skill knowledge base part of my module.

172
00:13:09,280 --> 00:13:13,120
We offer three datasets here, the YouTube, Wiki, and Reddit.

173
00:13:13,120 --> 00:13:20,800
And combined, they are the largest open-ended agent behavior database ever compiled to our knowledge.

174
00:13:20,840 --> 00:13:22,280
The first is YouTube.

175
00:13:22,280 --> 00:13:27,640
And we already said Minecraft is one of the most streamed games on YouTube.

176
00:13:27,640 --> 00:13:30,600
And gamers love to narrate what they are doing.

177
00:13:30,600 --> 00:13:37,640
So we collected more than 700,000 videos with two billion words in the corresponding transfers.

178
00:13:37,640 --> 00:13:43,240
And these transfers will help the agent learn about human strategies and creativities

179
00:13:43,240 --> 00:13:48,120
without us manually labeling things.

180
00:13:48,160 --> 00:13:56,680
And second, the Minecraft player base is so crazy that they have compiled a huge Minecraft-specific Wikipedia

181
00:13:56,680 --> 00:14:02,320
that basically explains everything you ever need to know in every version of the game.

182
00:14:02,320 --> 00:14:03,360
It's crazy.

183
00:14:03,360 --> 00:14:10,600
And we scraped 7,000 Wikipedia pages with interleaving, multi-modal data, like images, tables, and diagrams.

184
00:14:10,600 --> 00:14:12,480
And here are some screenshots.

185
00:14:12,480 --> 00:14:17,520
Like, this is a gallery of all of the monsters and their corresponding behaviors,

186
00:14:17,560 --> 00:14:20,200
like spawn and attack patterns.

187
00:14:20,200 --> 00:14:26,400
And also, the thousands of crafting recipes are all present on the Wiki, and we scraped all of them.

188
00:14:26,400 --> 00:14:29,680
And more complex diagrams and tables and embedded figures.

189
00:14:29,680 --> 00:14:32,400
Now we have something like GPT-4V.

190
00:14:32,400 --> 00:14:36,880
It may be able to understand many of these diagrams.

191
00:14:36,880 --> 00:14:43,960
And finally, the Minecraft subreddit is one of the most active forums across the entire Reddit.

192
00:14:43,960 --> 00:14:47,800
And players showcase their creations and also ask questions for help.

193
00:14:47,800 --> 00:14:52,520
So we scraped more than 300,000 posts from Minecraft Reddit.

194
00:14:52,520 --> 00:14:59,480
And here are some examples of how people use the Reddit as a kind of stack overflow for Minecraft.

195
00:14:59,480 --> 00:15:02,920
And we can see that some of the top-golded answers are actually quite good.

196
00:15:02,920 --> 00:15:06,480
Like someone is asking, oh, why doesn't my wheat farm grow?

197
00:15:06,480 --> 00:15:09,280
And the answer says you need to light up the room with more torches.

198
00:15:09,280 --> 00:15:12,560
You don't have enough lighting.

199
00:15:12,560 --> 00:15:16,040
Now, given the massive task suite and internet data,

200
00:15:16,040 --> 00:15:21,480
we have the essential components to build a journalist's agents.

201
00:15:21,480 --> 00:15:26,120
So in the first mind-dozer paper, we introduce a foundation model called Minecraft.

202
00:15:26,120 --> 00:15:27,840
And the idea is very simple.

203
00:15:27,840 --> 00:15:30,280
I can explain in three slides.

204
00:15:30,280 --> 00:15:35,800
Basically, for our YouTube database, we have time-aligned videos and transfers.

205
00:15:35,800 --> 00:15:39,840
And these are actually the real tutorial videos from our dataset.

206
00:15:39,840 --> 00:15:44,320
You see on the third clip, as I raise my axe in front of this pig,

207
00:15:44,320 --> 00:15:47,600
there's only one thing that you know is going to happen.

208
00:15:47,600 --> 00:15:53,080
It's actually someone said this, a big YouTuber of Minecraft.

209
00:15:53,080 --> 00:15:58,960
And then, given this data, we train Minecraft in the same spirit as Open AI Club.

210
00:15:58,960 --> 00:16:03,560
So for those who are unfamiliar, Open AI Club is a contrastive model

211
00:16:03,560 --> 00:16:06,720
that learns the association between an image and its caption.

212
00:16:06,720 --> 00:16:09,000
And here, it's a very simple idea.

213
00:16:09,000 --> 00:16:12,440
By this time, it is a video text contrastive model.

214
00:16:12,440 --> 00:16:22,720
And we associate the text with a video snippet that runs about 8 to 16 seconds each.

215
00:16:22,720 --> 00:16:27,560
And intuitively, Minecraft learns the association between the video and the transcript

216
00:16:27,560 --> 00:16:30,880
that describes the activity in the video.

217
00:16:30,880 --> 00:16:33,800
And Minecraft outputs a score between 0 and 1,

218
00:16:33,800 --> 00:16:37,120
where 1 means a perfect correlation between the text and the video,

219
00:16:37,160 --> 00:16:41,360
and 0 means the text is irrelevant to the activity.

220
00:16:41,360 --> 00:16:47,240
So you see this is effectively a language-prompted foundation reward model

221
00:16:47,240 --> 00:16:51,560
that knows the nuances of things like forests, animal behaviors,

222
00:16:51,560 --> 00:16:55,000
and architectures in Minecraft.

223
00:16:55,000 --> 00:16:57,440
So how do we use Minecraft in action?

224
00:16:57,440 --> 00:17:01,440
Here's an example of our agent interacting with a simulator.

225
00:17:01,440 --> 00:17:05,200
And here, the task is to share sheep to obtain wool.

226
00:17:05,200 --> 00:17:11,120
And as the agent explores in the simulator, it generates a video snippet

227
00:17:11,120 --> 00:17:15,400
as a moving window, which can be encoded and fed into Minecraft,

228
00:17:15,400 --> 00:17:19,400
along with an encoding of the text prompt here.

229
00:17:19,400 --> 00:17:22,240
And Minecraft computes the association.

230
00:17:22,240 --> 00:17:26,720
The higher the association is, the more the agent's behavior in this video

231
00:17:26,720 --> 00:17:30,920
aligns with the language, which is a task you want it to do.

232
00:17:30,960 --> 00:17:35,640
And that becomes a reward function to any reinforcement learning algorithm.

233
00:17:35,640 --> 00:17:38,280
So this looks very familiar, right?

234
00:17:38,280 --> 00:17:46,240
Because it's essentially RL from human feedback, or ROHF in Minecraft.

235
00:17:46,240 --> 00:17:50,600
And ROHF was the cornerstone algorithm that made chatGBT possible.

236
00:17:50,600 --> 00:17:53,840
And I believe it will play a critical role in Jonas agents as well.

237
00:17:56,040 --> 00:17:58,800
I'll quickly gloss over some quantitative results.

238
00:17:58,800 --> 00:18:02,800
I promise there won't be many tables of numbers here.

239
00:18:02,800 --> 00:18:08,120
For these eight tasks, we show the percentage success rate over 200 test episodes.

240
00:18:08,120 --> 00:18:12,480
And here, in the green circle, is two variants of our Minecraft method.

241
00:18:12,480 --> 00:18:14,840
And in the orange circles are the baselines.

242
00:18:15,920 --> 00:18:21,200
So I highlight one baseline, which is that we construct a dance reward function

243
00:18:21,200 --> 00:18:26,320
manually for each task using the Mindoge API, it's a Python API.

244
00:18:26,320 --> 00:18:31,040
And you can consider this column as a kind of oracle, the upper bound of the performance,

245
00:18:31,040 --> 00:18:34,720
because we put a lot of human efforts into designing these reward functions

246
00:18:34,720 --> 00:18:35,680
just for the tasks.

247
00:18:37,280 --> 00:18:42,160
And we can see that Minecraft is able to match the quality of many of these,

248
00:18:42,160 --> 00:18:46,000
not all of them, but many of these manual engineering rewards.

249
00:18:46,000 --> 00:18:49,640
It is important to highlight that Minecraft is open vocabulary.

250
00:18:49,640 --> 00:18:53,680
So we use a single model for all of these tasks instead of one model for each.

251
00:18:53,680 --> 00:18:58,400
And we simply prompt the reward model with different tasks.

252
00:18:58,400 --> 00:18:59,560
And that's the only variation.

253
00:19:03,360 --> 00:19:07,200
One major feature of Foundation Model is strong generalization out of box.

254
00:19:07,200 --> 00:19:12,600
So can our agent generalize to dramatic changes in the visual appearance?

255
00:19:12,600 --> 00:19:17,840
So we did this experiment where during training, we only train our agents on

256
00:19:17,840 --> 00:19:21,760
a default terrain at noon on a sunny day.

257
00:19:21,760 --> 00:19:25,400
But we tested zero shot in a diverse range of terrains,

258
00:19:25,400 --> 00:19:29,480
whether it's in daylight cycles, and you can customize everything in Mindoge.

259
00:19:29,480 --> 00:19:33,320
And in our paper, we have numbers showing that Minecraft significantly beats

260
00:19:33,320 --> 00:19:37,480
an off-the-shelf visual encoder when facing these kind of distribution shift

261
00:19:37,480 --> 00:19:38,960
out of box.

262
00:19:38,960 --> 00:19:40,200
And this is no surprise, right?

263
00:19:40,200 --> 00:19:44,000
Because Minecraft was trained on hundreds of thousands of clips

264
00:19:44,000 --> 00:19:50,160
from Minecraft videos on YouTube, which have a very good coverage of all the scenarios.

265
00:19:51,120 --> 00:19:56,240
And I think that is just a testament to the big advantage of using

266
00:19:56,240 --> 00:20:00,040
international data because you get robustness out of box.

267
00:20:01,440 --> 00:20:05,840
And here are some demos of our learned agent behaviors on various tasks.

268
00:20:05,840 --> 00:20:12,120
So you may notice that these tasks are relatively short, around 100 to 500 time steps.

269
00:20:12,120 --> 00:20:18,240
And that is because Minecraft is not able to plan over very long time horizons.

270
00:20:18,400 --> 00:20:22,280
And it is an inherent limitation in the training pipeline

271
00:20:22,280 --> 00:20:25,440
because we could only use 8 to 16 seconds of the video,

272
00:20:25,440 --> 00:20:28,560
so it's constrained to short actions.

273
00:20:28,560 --> 00:20:32,880
But our hope is to build an agent that can explore and make new discoveries

274
00:20:32,880 --> 00:20:36,240
autonomously, just all by itself, and it keeps going.

275
00:20:36,240 --> 00:20:39,920
And in 2022, this goal seems quite out of reach for us.

276
00:20:39,920 --> 00:20:43,200
Mindoge was June 2022.

277
00:20:43,200 --> 00:20:46,880
And this year, something happened, and that is G4.

278
00:20:46,880 --> 00:20:51,600
A language model that is so good at coding and long horizon planning,

279
00:20:51,600 --> 00:20:54,080
so we just cannot sit still, right?

280
00:20:54,080 --> 00:21:00,400
We built Voyager, the first large language model powered life on a learning agent.

281
00:21:00,400 --> 00:21:04,800
And when we said Voyager lose in Minecraft, we see that it just keeps going.

282
00:21:04,800 --> 00:21:09,840
And by the way, all these video snippets are from a single episode of Voyager.

283
00:21:09,840 --> 00:21:13,680
It's not from different episodes, it's a single one.

284
00:21:13,680 --> 00:21:18,000
And we see that Voyager is just able to keep exploring the terrains,

285
00:21:18,000 --> 00:21:22,320
mine all kinds of materials, fight monsters, craft hundreds of recipes,

286
00:21:22,320 --> 00:21:26,880
and unlock an ever-expanding tree of diverse skills.

287
00:21:26,880 --> 00:21:30,000
So how do we do this?

288
00:21:30,000 --> 00:21:32,400
If we want to use the full power of G4,

289
00:21:32,400 --> 00:21:35,360
a central question is how to stringify things,

290
00:21:35,360 --> 00:21:40,240
converting this 3D world into a textual representation.

291
00:21:40,240 --> 00:21:42,480
We need a magic box here.

292
00:21:42,480 --> 00:21:47,200
And thankfully, again, the crazy Minecraft community already built one for us,

293
00:21:47,200 --> 00:21:49,520
and it's been around for many years.

294
00:21:49,520 --> 00:21:52,960
It's called Mindflayer, a high-level JavaScript API

295
00:21:52,960 --> 00:21:57,120
that's actively maintained to work with any Minecraft version.

296
00:21:57,120 --> 00:22:01,600
And the beauty of Mindflayer is it has access to the game states

297
00:22:01,600 --> 00:22:05,760
surrounding the agent, like the nearby blocks, animals, and enemies.

298
00:22:05,760 --> 00:22:10,560
So we effectively have a ground truth perception module as textual input.

299
00:22:10,640 --> 00:22:14,480
At the same time, Mindflayer also supports action APIs

300
00:22:14,480 --> 00:22:18,320
that we can compose skills.

301
00:22:18,320 --> 00:22:20,560
And now that we can convert everything to text,

302
00:22:20,560 --> 00:22:24,400
we're ready to construct an agent on top of G4.

303
00:22:24,400 --> 00:22:27,040
So on a high level, there are three components.

304
00:22:27,040 --> 00:22:32,240
One is a coding module that writes JavaScript code to control the game bot,

305
00:22:32,240 --> 00:22:36,160
and it's the main module that generates the executable actions.

306
00:22:36,160 --> 00:22:40,960
And second, we have a code base to store the correctly written code

307
00:22:40,960 --> 00:22:44,560
and look it up in the future if the agent needs to record a skill.

308
00:22:44,560 --> 00:22:46,720
And in this way, we don't duplicate efforts,

309
00:22:46,720 --> 00:22:48,960
and whenever facing similar situations in the future,

310
00:22:48,960 --> 00:22:51,040
the agent knows what to do.

311
00:22:51,040 --> 00:22:56,000
And third, we have a curriculum that proposes what to do next,

312
00:22:56,000 --> 00:23:00,080
given the agent's current capabilities and also situation.

313
00:23:00,080 --> 00:23:04,160
And when you wire these components up together,

314
00:23:04,160 --> 00:23:06,720
you get a loop that drives the agent indefinitely

315
00:23:06,720 --> 00:23:09,920
and achieve something like lifelong learning.

316
00:23:09,920 --> 00:23:13,360
So let's zoom in the center module.

317
00:23:13,360 --> 00:23:16,800
We prompt GD4 with documentations and examples

318
00:23:16,800 --> 00:23:20,000
on how to use a subset of the Mindflayer API

319
00:23:20,000 --> 00:23:24,880
and GD4 writes code to take actions given the current assigned task.

320
00:23:24,880 --> 00:23:27,120
And because JavaScript runs a coding interpreter,

321
00:23:27,120 --> 00:23:32,160
GD4 is able to define functions on a fly and run it interactively.

322
00:23:32,160 --> 00:23:34,560
But the code that GD4 writes isn't always correct, right?

323
00:23:34,560 --> 00:23:35,520
Just like human engineers.

324
00:23:35,520 --> 00:23:37,520
You can't get everything correct on the first try.

325
00:23:38,240 --> 00:23:41,040
So we developed an iterative prompting mechanism

326
00:23:41,040 --> 00:23:42,240
to refine the program.

327
00:23:43,040 --> 00:23:45,520
And there are three types of feedback here.

328
00:23:45,520 --> 00:23:48,640
The environment feedback, like what are the new materials

329
00:23:48,640 --> 00:23:52,320
you got after taking an action or some enemies nearby.

330
00:23:52,960 --> 00:23:55,600
And the execution error from the JavaScript interpreter

331
00:23:55,600 --> 00:23:58,880
if you wrote some buggy code, like undefined variable,

332
00:23:58,880 --> 00:24:00,800
for example, if it hallucinates something.

333
00:24:01,600 --> 00:24:05,520
And another GD4 that provides critique

334
00:24:05,520 --> 00:24:09,120
through self-reflection from the agent state and the world state.

335
00:24:09,760 --> 00:24:12,000
And that also helps refine the program effectively.

336
00:24:13,440 --> 00:24:14,800
So I want to show some quick example

337
00:24:14,800 --> 00:24:17,360
of how the critic provides feedback

338
00:24:17,360 --> 00:24:19,040
on the task completion progress.

339
00:24:19,760 --> 00:24:21,120
So let's say in the first example,

340
00:24:21,120 --> 00:24:23,920
the task is to craft a spike mass

341
00:24:23,920 --> 00:24:26,720
and GD4 looks at the agent's inventory

342
00:24:26,720 --> 00:24:28,720
and decides that it has enough copper

343
00:24:28,720 --> 00:24:31,200
but not enough Amherst as a material.

344
00:24:32,400 --> 00:24:35,120
And the second task is to kill three sheeps to collect food.

345
00:24:35,760 --> 00:24:38,320
And each sheep drops one unit of wool,

346
00:24:38,320 --> 00:24:40,320
but there are only two units in inventory.

347
00:24:40,320 --> 00:24:42,560
So GD4 reasons and says that,

348
00:24:42,560 --> 00:24:44,960
okay, you have one more sheep to go, likewise.

349
00:24:46,960 --> 00:24:48,720
Now, moving on to the second part.

350
00:24:49,920 --> 00:24:52,560
Once Voyager implements a skill correctly,

351
00:24:52,560 --> 00:24:54,800
we save it to our persistent storage.

352
00:24:55,440 --> 00:24:57,360
And you can think of the skill library

353
00:24:57,440 --> 00:25:01,200
as a code repository written entirely by a language model

354
00:25:01,200 --> 00:25:03,280
through interaction with the 3D world.

355
00:25:04,400 --> 00:25:07,120
And the agent can record new skills

356
00:25:07,120 --> 00:25:09,200
and also retrieve skills from the library

357
00:25:09,200 --> 00:25:11,760
facing similar situations in the future.

358
00:25:11,760 --> 00:25:14,240
So it doesn't have to go through this whole program refinement

359
00:25:14,240 --> 00:25:15,440
that we just saw again,

360
00:25:15,440 --> 00:25:16,800
which is quite inefficient,

361
00:25:16,800 --> 00:25:19,360
but you do it once you save it to disk.

362
00:25:20,080 --> 00:25:23,120
And in this way, Voyager kind of bootstraps

363
00:25:23,120 --> 00:25:24,960
its own capabilities recursively

364
00:25:25,520 --> 00:25:28,240
as it explores and experiments in the game.

365
00:25:29,680 --> 00:25:30,880
And let's dive a little bit deeper

366
00:25:30,880 --> 00:25:32,800
into how the skill library is implemented.

367
00:25:33,680 --> 00:25:35,520
So this is how we insert a new skill.

368
00:25:36,080 --> 00:25:40,160
First, we use GPT 3.5 to summarize the program into plain English.

369
00:25:40,720 --> 00:25:43,760
And summarization is very easy and GD4 is expensive.

370
00:25:43,760 --> 00:25:46,240
So we just go for a cheaper tier.

371
00:25:47,440 --> 00:25:51,040
And then we embed this summary as the key

372
00:25:51,040 --> 00:25:53,680
and we save the program, which is a bunch of code,

373
00:25:53,760 --> 00:25:54,800
as the value.

374
00:25:54,800 --> 00:25:57,760
And we find that doing this makes retrieval better

375
00:25:57,760 --> 00:25:59,840
because the summary is more semantic

376
00:25:59,840 --> 00:26:03,200
and the code is a bit more discrete and you insert it.

377
00:26:06,720 --> 00:26:09,120
And now for the retrieval process,

378
00:26:09,120 --> 00:26:10,960
where Voyager is faced with a new task,

379
00:26:10,960 --> 00:26:12,560
let's say craft iron pickaxe.

380
00:26:13,120 --> 00:26:16,320
We again use GP3.5 to generate a hint

381
00:26:16,320 --> 00:26:17,840
on how to solve the task.

382
00:26:17,840 --> 00:26:19,920
And that is something like a natural language paragraph.

383
00:26:20,560 --> 00:26:24,640
And then we embed that and use that as the query

384
00:26:24,640 --> 00:26:26,320
into the vector database.

385
00:26:26,320 --> 00:26:29,840
And we retrieve the skill from the library.

386
00:26:30,720 --> 00:26:34,160
So you can think of it as a kind of in-context replay buffer

387
00:26:34,160 --> 00:26:36,000
in the reinforcement learning literature.

388
00:26:37,680 --> 00:26:39,440
And now moving on to the third part.

389
00:26:41,120 --> 00:26:44,000
We have another GP4 that proposes what task to do,

390
00:26:44,000 --> 00:26:47,040
given its own capabilities at the moment.

391
00:26:47,680 --> 00:26:50,400
And here we give GP4 a very high-level

392
00:26:50,400 --> 00:26:52,320
kind of unsupervised objective

393
00:26:52,320 --> 00:26:55,920
that is to obtain as many unique items as possible.

394
00:26:55,920 --> 00:26:57,280
That is our high-level directive.

395
00:26:57,840 --> 00:27:00,560
And then GP4 takes this directive and implements

396
00:27:01,440 --> 00:27:04,640
a curriculum of progressively harder challenges

397
00:27:04,640 --> 00:27:06,160
and more novel challenges to solve.

398
00:27:07,040 --> 00:27:10,720
So it's kind of like curiosity of exploration,

399
00:27:10,720 --> 00:27:13,760
where it is our novelty search in a prior literature,

400
00:27:14,320 --> 00:27:16,320
but implemented purely in context.

401
00:27:16,400 --> 00:27:19,280
Yeah, if you're listening to Zoom, the next example is fun.

402
00:27:21,520 --> 00:27:23,440
Let's go through this example together.

403
00:27:24,720 --> 00:27:27,120
Just to kind of show you how Voyager works,

404
00:27:27,120 --> 00:27:29,920
the whole complicated data flow that I just showed.

405
00:27:30,640 --> 00:27:32,720
So the agent finds itself hungry.

406
00:27:33,440 --> 00:27:35,520
It only has one out of 20 hunger bars.

407
00:27:35,520 --> 00:27:38,720
So it knows GP4 knows that it needs to find food ASAP.

408
00:27:39,280 --> 00:27:42,640
And then it senses there are four entities nearby.

409
00:27:43,280 --> 00:27:45,840
A cat, a villager, a pig, and some wheat seeds.

410
00:27:46,400 --> 00:27:48,320
And now GP4 starts a self-reflection.

411
00:27:49,040 --> 00:27:51,520
Like, do I kill the cat and the villager to get some meat?

412
00:27:52,080 --> 00:27:52,960
That sounds horrible.

413
00:27:54,080 --> 00:27:54,960
How about the wheat seeds?

414
00:27:55,760 --> 00:27:57,600
I can use the seeds to grow a farm,

415
00:27:57,600 --> 00:27:59,440
but that's going to take a very long time

416
00:27:59,440 --> 00:28:01,360
until I can generate some food.

417
00:28:01,360 --> 00:28:04,080
So sorry, Piggy, you are the one being chosen.

418
00:28:05,120 --> 00:28:08,640
So GP4 looks at the inventory, which is the agent state.

419
00:28:09,200 --> 00:28:11,920
There is a piece of iron in inventory.

420
00:28:11,920 --> 00:28:15,680
So it recalls, Voyager recalls a skill from the library

421
00:28:15,680 --> 00:28:17,760
that is to craft an iron sword

422
00:28:17,760 --> 00:28:20,400
and then use that skill to start pursuing,

423
00:28:20,400 --> 00:28:23,600
to start learning a new skill, and that is Hunt Pig.

424
00:28:23,600 --> 00:28:27,840
And once the Hunt Pig routine is successful,

425
00:28:27,840 --> 00:28:30,240
GP4 saves it to the skill library.

426
00:28:30,240 --> 00:28:31,280
That's roughly how it works.

427
00:28:32,960 --> 00:28:34,880
Yeah, and putting all of these together,

428
00:28:34,880 --> 00:28:36,800
we have this iterative prompting mechanism,

429
00:28:36,800 --> 00:28:39,680
the skill library, and an automatic curriculum.

430
00:28:40,320 --> 00:28:42,560
And all of these combine.

431
00:28:42,560 --> 00:28:45,600
It's Voyager's no-gradient architecture

432
00:28:45,600 --> 00:28:47,360
where we don't train any new models

433
00:28:47,360 --> 00:28:48,800
or fine tune any parameters,

434
00:28:49,520 --> 00:28:54,080
and allows Voyager to self-boostrap on top of GP4,

435
00:28:54,080 --> 00:28:56,800
even though we are treating the underlying language model

436
00:28:56,800 --> 00:28:57,600
as a black box.

437
00:29:00,480 --> 00:29:01,920
It looks like my example work,

438
00:29:01,920 --> 00:29:04,560
and they started to listen.

439
00:29:08,560 --> 00:29:10,160
So yeah, these are the tasks

440
00:29:10,160 --> 00:29:12,080
that Voyager picked up along the way.

441
00:29:12,560 --> 00:29:14,560
And we didn't pre-program any of these.

442
00:29:14,560 --> 00:29:16,000
It's all Voyager's idea.

443
00:29:16,560 --> 00:29:18,400
The agent is kind of forever curious

444
00:29:18,400 --> 00:29:21,680
and also forever pursuing new adventures just by itself.

445
00:29:24,000 --> 00:29:26,640
So to quickly show some quantitative results,

446
00:29:27,680 --> 00:29:29,760
here we have a learning curve,

447
00:29:29,760 --> 00:29:33,280
where the x-axis is a number of prompting iterations,

448
00:29:33,280 --> 00:29:36,080
and the y-axis is the number of unique items

449
00:29:36,080 --> 00:29:39,280
that Voyager discovered as it's exploring an environment.

450
00:29:40,240 --> 00:29:43,440
And these two curves are baselines,

451
00:29:43,440 --> 00:29:45,760
a react and reflexion.

452
00:29:47,600 --> 00:29:49,040
And this is auto-GPT,

453
00:29:49,040 --> 00:29:50,880
which is like a popular software repo.

454
00:29:50,880 --> 00:29:53,920
Basically, you can think of it as combining react

455
00:29:53,920 --> 00:29:55,760
and a task planner that decomposes

456
00:29:55,760 --> 00:29:57,600
an objective into sub-goals.

457
00:29:58,240 --> 00:29:59,200
And this is Voyager.

458
00:30:00,000 --> 00:30:03,200
We're able to obtain three times more novel items

459
00:30:03,200 --> 00:30:04,720
than the prior methods,

460
00:30:04,720 --> 00:30:07,600
and also unlock the entire texture significantly faster.

461
00:30:08,320 --> 00:30:11,600
And if you take away the skill library,

462
00:30:11,600 --> 00:30:13,440
you see that Voyager really suffers.

463
00:30:13,440 --> 00:30:15,120
The performance takes a hit,

464
00:30:15,120 --> 00:30:17,840
because every time it needs to kind of repeat

465
00:30:17,840 --> 00:30:19,840
and relearn every skill from scratch

466
00:30:19,840 --> 00:30:22,000
and starts to make a lot more mistakes,

467
00:30:22,000 --> 00:30:24,400
and that really degrades the exploration.

468
00:30:26,640 --> 00:30:30,720
Here, these two are the bird-eye views of the Minecraft map,

469
00:30:30,720 --> 00:30:34,000
and these circles are what the prior methods

470
00:30:34,000 --> 00:30:35,360
are able to explore,

471
00:30:36,240 --> 00:30:38,160
given the same prompting iteration budget.

472
00:30:39,040 --> 00:30:42,160
And we see that they tend to get stuck in local areas

473
00:30:42,160 --> 00:30:44,160
and kind of fail to explore more,

474
00:30:44,800 --> 00:30:48,320
but Voyager is able to navigate distances at least two times

475
00:30:49,280 --> 00:30:51,360
as much as the prior works.

476
00:30:52,480 --> 00:30:55,280
So it's able to visit a lot more places,

477
00:30:55,280 --> 00:30:58,160
because to satisfy this high-level directive

478
00:30:58,160 --> 00:31:00,640
of obtaining as many unique items as possible,

479
00:31:00,640 --> 00:31:02,320
you've got to travel.

480
00:31:02,320 --> 00:31:03,360
If you stay at one place,

481
00:31:03,440 --> 00:31:05,280
you will quickly exhaust the interesting things to do.

482
00:31:06,320 --> 00:31:07,920
And Voyager travels a lot,

483
00:31:07,920 --> 00:31:09,520
so that's how we came up with the name.

484
00:31:11,760 --> 00:31:14,800
So finally, one limitation is that Voyager

485
00:31:14,800 --> 00:31:17,680
does not currently support visual perception,

486
00:31:17,680 --> 00:31:20,720
because the GV4 that we used back then was text-only,

487
00:31:21,440 --> 00:31:23,120
but there's nothing stopping Voyager

488
00:31:23,120 --> 00:31:27,120
from adopting like multi-modal language models in the future.

489
00:31:27,120 --> 00:31:29,520
So here we have a little proof-of-concept demo,

490
00:31:29,520 --> 00:31:32,080
where we ask a human to basically function

491
00:31:32,160 --> 00:31:33,840
as the image captioner.

492
00:31:33,840 --> 00:31:36,320
And the human will tell Voyager

493
00:31:36,320 --> 00:31:38,160
that as you're building these houses,

494
00:31:38,160 --> 00:31:39,520
what are the things that are missing?

495
00:31:39,520 --> 00:31:41,440
Like you place a door incorrectly,

496
00:31:42,160 --> 00:31:45,040
like the roof is also not done correctly.

497
00:31:45,040 --> 00:31:48,720
So the human is acting as a critic module of the Voyager stack.

498
00:31:49,360 --> 00:31:52,080
And we see that with some of that help,

499
00:31:52,080 --> 00:31:55,520
Voyager is able to build a farmhouse and another portal,

500
00:31:55,520 --> 00:31:59,440
but it has a hard time understanding 3D spatial coordinates

501
00:31:59,440 --> 00:32:01,680
just by itself in a textual domain.

502
00:32:02,560 --> 00:32:11,360
Now, after doing Voyager, we're considering like, where else can we apply this idea

503
00:32:11,360 --> 00:32:14,000
of coding in an embodying environment,

504
00:32:14,000 --> 00:32:17,920
observe the feedback, and iteratively refine the program.

505
00:32:18,720 --> 00:32:22,720
So we came to realize that physics simulations themselves

506
00:32:22,720 --> 00:32:24,560
are also just Python code.

507
00:32:24,560 --> 00:32:27,760
So why not apply some of the principles for Voyager

508
00:32:27,760 --> 00:32:29,920
and do something in another domain?

509
00:32:30,160 --> 00:32:33,280
What if you apply Voyager in the space of this physics simulator API?

510
00:32:33,280 --> 00:32:38,640
And this is Eureka, which my team announced just like three days ago,

511
00:32:38,640 --> 00:32:39,760
fresh out of the oven.

512
00:32:40,800 --> 00:32:44,400
It is an open-ended agent that designs reward functions

513
00:32:44,400 --> 00:32:47,520
for robot dexterity at superhuman level.

514
00:32:47,520 --> 00:32:50,880
And it turns out that GD4-POS reinforcement learning

515
00:32:50,880 --> 00:32:54,160
can spin a pen much better than I do.

516
00:32:54,160 --> 00:32:58,480
I gave up on this task a long time ago from childhood.

517
00:32:58,800 --> 00:33:00,720
It's so hard for me.

518
00:33:03,440 --> 00:33:06,480
So Eureka's idea is very simple and intuitive.

519
00:33:07,120 --> 00:33:11,200
GD4 generates a bunch of possible reward function candidates

520
00:33:11,200 --> 00:33:12,240
implemented in Python.

521
00:33:12,800 --> 00:33:17,200
And then you just do a full reinforcement learning training loop

522
00:33:17,200 --> 00:33:20,320
for each candidate in a GPU accelerated simulator.

523
00:33:21,280 --> 00:33:25,280
And you get a performance metric and you take the best candidates

524
00:33:25,280 --> 00:33:27,120
and feedback to GD4.

525
00:33:27,120 --> 00:33:30,480
And it samples the next proposals of candidates

526
00:33:30,480 --> 00:33:33,600
and keeps improving the whole population on the reward functions.

527
00:33:34,320 --> 00:33:35,120
That's the whole idea.

528
00:33:35,680 --> 00:33:38,080
It's kind of like an in-context evolutionary search.

529
00:33:40,160 --> 00:33:42,320
So here's the initial reward generation,

530
00:33:42,320 --> 00:33:46,000
where Eureka takes as context the environment code

531
00:33:46,000 --> 00:33:50,320
of NVIDIA's ISAC sim and a task description

532
00:33:50,320 --> 00:33:53,280
and samples the initial reward function implementation.

533
00:33:54,240 --> 00:33:56,480
So we found that the simulator code itself

534
00:33:56,480 --> 00:33:58,480
is actually a very good reference manual

535
00:33:58,480 --> 00:34:01,520
because it tells you, Eureka, what are the variables you can use,

536
00:34:02,160 --> 00:34:05,520
like the hand positions here, the fingertip position,

537
00:34:05,520 --> 00:34:07,440
the fingertips have safe, the rotation,

538
00:34:07,440 --> 00:34:08,880
angular velocity, et cetera.

539
00:34:09,600 --> 00:34:13,120
So you know all of these variables from the simulator code

540
00:34:13,120 --> 00:34:16,320
and you know how they interact with each other.

541
00:34:16,320 --> 00:34:20,560
So that serves as a very good in-context instruction.

542
00:34:21,440 --> 00:34:23,120
So Eureka doesn't need to reference

543
00:34:23,120 --> 00:34:24,960
any human return reward functions.

544
00:34:26,560 --> 00:34:28,960
And then once you have the generated reward,

545
00:34:28,960 --> 00:34:31,600
you plug it into any reinforcement learning algorithm

546
00:34:31,600 --> 00:34:33,120
and just train it to completion.

547
00:34:33,680 --> 00:34:37,760
So this step is typically very costly and very slow

548
00:34:37,760 --> 00:34:39,360
because reinforcement learning is always slow.

549
00:34:39,920 --> 00:34:42,480
And we were only able to scale up Eureka

550
00:34:42,480 --> 00:34:44,080
because of NVIDIA's ISAC chain,

551
00:34:44,800 --> 00:34:48,560
which runs a thousand simulated environment copies

552
00:34:48,560 --> 00:34:49,520
on a single GPU.

553
00:34:50,160 --> 00:34:53,840
So basically, you can think of it as speeding up reality

554
00:34:53,840 --> 00:34:54,800
by a thousand lags.

555
00:34:57,120 --> 00:34:58,160
And then after training,

556
00:34:58,160 --> 00:35:00,160
you will get the performance metrics

557
00:35:00,160 --> 00:35:01,680
back on each reward component.

558
00:35:02,320 --> 00:35:03,680
And as we saw from Voyager,

559
00:35:04,240 --> 00:35:07,040
GBT4 is very good at self-reflection.

560
00:35:07,040 --> 00:35:08,960
So we leverage that capability.

561
00:35:13,440 --> 00:35:16,720
There's a software trial reminding you to activate a license.

562
00:35:17,520 --> 00:35:20,320
Yeah, so Voyager reflects on it

563
00:35:20,320 --> 00:35:23,920
and then proposes mutations on the code.

564
00:35:25,520 --> 00:35:28,960
So here, the mutations we found can be very diverse,

565
00:35:28,960 --> 00:35:30,880
ranging from something as simple as just changing

566
00:35:30,880 --> 00:35:33,520
a hyperparameter in the reward function weighting

567
00:35:33,520 --> 00:35:37,440
to all the way to adding completely novel components

568
00:35:37,440 --> 00:35:38,240
to the reward function.

569
00:35:40,000 --> 00:35:41,520
And in our experiments,

570
00:35:41,520 --> 00:35:45,920
Eureka turns out to be a superhuman reward engineer

571
00:35:46,160 --> 00:35:48,480
actually outperforming some of the functions

572
00:35:48,480 --> 00:35:51,280
implemented by the expert human engineers

573
00:35:51,280 --> 00:35:53,280
on NVIDIA's ISAC same team.

574
00:35:55,680 --> 00:35:58,080
So here are some more demos of how Eureka

575
00:35:58,080 --> 00:36:00,240
is able to write very complex rewards

576
00:36:00,240 --> 00:36:04,160
that lead to these extremely dexterous behaviors.

577
00:36:04,160 --> 00:36:07,200
And we can actually train the robot hand

578
00:36:07,200 --> 00:36:09,840
to rotate pens not just in one direction,

579
00:36:09,840 --> 00:36:12,320
but in different directions, along different 3D axes.

580
00:36:12,960 --> 00:36:15,360
I think one major contribution of Eureka,

581
00:36:15,360 --> 00:36:17,920
different from Voyager, is to bridge the gap

582
00:36:17,920 --> 00:36:22,080
between high-level reasoning and low-level model controls.

583
00:36:22,080 --> 00:36:24,400
So Eureka introduces a new paradigm

584
00:36:24,400 --> 00:36:27,760
that I'm calling hybrid gradient architecture.

585
00:36:27,760 --> 00:36:29,920
So recall Voyager is a no-gradient architecture.

586
00:36:29,920 --> 00:36:33,040
We don't touch anything and we don't train anything.

587
00:36:33,040 --> 00:36:34,880
But Eureka is a hybrid gradient,

588
00:36:34,880 --> 00:36:38,640
where a black box inference-only language model

589
00:36:38,640 --> 00:36:41,040
instructs a wide range of functions

590
00:36:41,200 --> 00:36:44,080
instructs a white box, learnable neural network.

591
00:36:45,040 --> 00:36:47,520
So you can think of it as two loops, right?

592
00:36:47,520 --> 00:36:49,040
The outer loop is great and free,

593
00:36:49,600 --> 00:36:52,000
and it was, it's driven by GV4,

594
00:36:52,000 --> 00:36:54,240
kind of selecting the reward functions.

595
00:36:54,240 --> 00:36:56,160
And the inner loop is great and based.

596
00:36:56,160 --> 00:36:59,520
You train like a full reinforcement learning episode from it

597
00:36:59,520 --> 00:37:03,200
to achieve extreme dexterity using a specialized,

598
00:37:03,200 --> 00:37:06,160
like training by training a special neural network controller.

599
00:37:07,040 --> 00:37:09,280
And you must have both loops to succeed

600
00:37:09,280 --> 00:37:10,880
to deliver this kind of dexterity.

601
00:37:11,600 --> 00:37:14,160
And I think it will be a very useful paradigm

602
00:37:14,160 --> 00:37:16,640
for training robot agents in the future.

603
00:37:18,880 --> 00:37:23,120
So these days, when I go on Twitter or X,

604
00:37:23,680 --> 00:37:26,800
I see AI conquering new lands every week.

605
00:37:28,000 --> 00:37:30,160
Chat, image generation, and music,

606
00:37:30,160 --> 00:37:32,000
they're all very well within reach.

607
00:37:32,960 --> 00:37:35,520
But my dojo, Voyager, and Eureka,

608
00:37:35,520 --> 00:37:37,120
these are just scratching the surface

609
00:37:37,120 --> 00:37:39,120
of open-ended journalist agents.

610
00:37:40,480 --> 00:37:41,600
And looking forward,

611
00:37:41,600 --> 00:37:44,720
I want to share two key research directions

612
00:37:44,720 --> 00:37:47,600
that I personally find extremely promising,

613
00:37:47,600 --> 00:37:49,200
and I'm also working on it myself.

614
00:37:50,080 --> 00:37:52,320
The first is a continuation of Minecraft,

615
00:37:52,320 --> 00:37:54,720
basically how to develop methods

616
00:37:54,720 --> 00:37:56,880
that learn from Internet-skilled videos.

617
00:37:57,440 --> 00:38:00,480
And the second is multimodal foundation models.

618
00:38:00,480 --> 00:38:02,480
Now that GV4V is coming,

619
00:38:02,480 --> 00:38:04,720
but it is just the beginning of an era.

620
00:38:05,440 --> 00:38:09,120
And I think it's important to have all of the modalities

621
00:38:09,120 --> 00:38:10,880
in a single foundation model.

622
00:38:12,560 --> 00:38:13,680
So first, about videos.

623
00:38:14,880 --> 00:38:17,200
We all know that videos are abundant, right?

624
00:38:17,200 --> 00:38:21,440
Like so many data on YouTube, way too many

625
00:38:21,440 --> 00:38:23,120
for our limited GPUs to process.

626
00:38:24,320 --> 00:38:27,200
They're extremely useful to train models

627
00:38:27,200 --> 00:38:30,960
that not only have dynamic perception and intuitive physics,

628
00:38:30,960 --> 00:38:34,560
but also capture the complexity of human creativity

629
00:38:34,640 --> 00:38:35,680
and human behaviors.

630
00:38:36,320 --> 00:38:40,880
It's all good, except that when you are using video

631
00:38:40,880 --> 00:38:42,640
to pre-training body nations,

632
00:38:42,640 --> 00:38:44,400
there is huge distribution shift.

633
00:38:44,400 --> 00:38:46,320
You also don't get action labels,

634
00:38:46,320 --> 00:38:47,920
and you don't get any of the groundings

635
00:38:47,920 --> 00:38:49,440
because you are a passive observer.

636
00:38:50,960 --> 00:38:52,240
So I think here is a demonstration

637
00:38:52,240 --> 00:38:53,840
of why learning from video is hard,

638
00:38:53,840 --> 00:38:55,120
even for natural intelligence.

639
00:38:57,040 --> 00:39:00,400
So Little Cat is seeing boxers shaking their head,

640
00:39:01,200 --> 00:39:02,640
and it thinks maybe shaking head

641
00:39:02,640 --> 00:39:04,320
is the best way to do fighting.

642
00:39:04,960 --> 00:39:10,080
All right, this is why learning from video is hard.

643
00:39:13,040 --> 00:39:15,520
You have no idea, like why...

644
00:39:18,000 --> 00:39:18,800
This is too good.

645
00:39:18,800 --> 00:39:20,000
Let's play this again.

646
00:39:20,000 --> 00:39:22,800
You have no idea why Tyson is doing this, right?

647
00:39:22,800 --> 00:39:24,240
Like the cat has no idea,

648
00:39:24,240 --> 00:39:28,240
and then it associates this with just wrong kind of policy.

649
00:39:30,640 --> 00:39:33,360
But for sure, it doesn't help the fighting,

650
00:39:33,360 --> 00:39:35,520
but it definitely boosts the cat's confidence.

651
00:39:38,960 --> 00:39:40,320
That's why learning from video is hard.

652
00:39:42,320 --> 00:39:44,960
Now, I want to point out a few kind of latest research

653
00:39:44,960 --> 00:39:48,000
in how to leverage so much video for journalist agents.

654
00:39:49,040 --> 00:39:50,720
There are a couple of approaches.

655
00:39:51,280 --> 00:39:52,800
The first is the simplest,

656
00:39:52,800 --> 00:39:56,240
just learn kind of a visual feature extractor from the videos.

657
00:39:56,800 --> 00:40:00,000
So this is R3M from Chelsea's group at Stanford,

658
00:40:00,960 --> 00:40:03,520
and this model is still an image-level representation,

659
00:40:03,520 --> 00:40:06,560
just that it uses a video-level loss function to train,

660
00:40:07,200 --> 00:40:09,360
more specifically, time-contrastive learning.

661
00:40:10,000 --> 00:40:13,600
And after that, you can use this as an image backbone

662
00:40:13,600 --> 00:40:14,880
for any agent,

663
00:40:14,880 --> 00:40:16,160
but you still need to kind of find

664
00:40:16,160 --> 00:40:18,960
to using domain-specific data for the agent.

665
00:40:20,800 --> 00:40:24,480
The second path is to learn reward functions from video,

666
00:40:24,480 --> 00:40:28,640
and MineClip is one model under this category.

667
00:40:29,200 --> 00:40:32,880
It uses a contrastive objective between the transfer and video.

668
00:40:32,880 --> 00:40:36,160
And here, this work, VIP, is another way

669
00:40:36,160 --> 00:40:38,640
to learn a similarity-based reward

670
00:40:38,640 --> 00:40:40,960
for goal-conditioned tasks in the image space.

671
00:40:41,680 --> 00:40:45,600
So this work, VIP, is led by also the first author of Eureka,

672
00:40:45,600 --> 00:40:48,400
and Eureka is his internship project with me.

673
00:40:50,240 --> 00:40:51,680
And the third idea is very interesting.

674
00:40:52,400 --> 00:40:55,920
Can we directly do imitation learning from video,

675
00:40:56,480 --> 00:40:59,120
but better than the cat that we just saw?

676
00:40:59,840 --> 00:41:03,440
So we just said, you know, the videos don't have the actions, right?

677
00:41:04,400 --> 00:41:06,880
We need to find some ways to pseudo-level the actions.

678
00:41:07,600 --> 00:41:11,440
And this is video portraying a VPT from OpenAI last year

679
00:41:11,440 --> 00:41:13,520
to solve long-range tasks in Minecraft.

680
00:41:14,640 --> 00:41:17,680
And here, the pipeline works like this.

681
00:41:18,240 --> 00:41:21,360
Basically, you use a keyboard and a mouse action space,

682
00:41:22,000 --> 00:41:25,200
so you can align this action space with the human actions.

683
00:41:26,000 --> 00:41:29,280
And OpenAI hires a bunch of Minecraft players

684
00:41:29,840 --> 00:41:31,760
and actually collect data in-house,

685
00:41:31,760 --> 00:41:34,400
so they record the episodes done by those gamers.

686
00:41:35,040 --> 00:41:39,200
And now you have a data set of video and action pairs, right?

687
00:41:39,920 --> 00:41:42,640
And you train something called an inverse dynamics model,

688
00:41:42,640 --> 00:41:46,880
which is to take the observation and then predict the actions

689
00:41:46,880 --> 00:41:49,040
that cost the observation to change.

690
00:41:49,040 --> 00:41:50,480
So that's the inverse dynamics model.

691
00:41:51,040 --> 00:41:55,840
And that becomes a labeler that you can apply

692
00:41:55,840 --> 00:41:58,640
to in-the-wild YouTube videos that don't have the actions.

693
00:41:59,200 --> 00:42:04,160
So you apply IDM to like 70K hours of in-the-wild YouTube videos,

694
00:42:04,160 --> 00:42:06,080
and you will get these pseudo-actions

695
00:42:06,080 --> 00:42:09,440
that are not always correct, but also way better than random.

696
00:42:10,080 --> 00:42:12,000
And then you're training imitation learning

697
00:42:12,000 --> 00:42:13,760
on top of this augmented data set.

698
00:42:14,320 --> 00:42:18,560
And in this way, OpenAI is able to greatly expand the data

699
00:42:18,640 --> 00:42:22,480
because the original data collected from the humans

700
00:42:22,480 --> 00:42:25,200
are high quality, but they're extremely expensive,

701
00:42:25,200 --> 00:42:27,360
while in-the-wild YouTube videos are very cheap,

702
00:42:27,360 --> 00:42:28,800
but you don't have the actions.

703
00:42:28,800 --> 00:42:31,920
So they kind of solved and got the best of those roles.

704
00:42:32,640 --> 00:42:35,040
But still, it's really expensive to hire these humans.

705
00:42:37,680 --> 00:42:39,600
Now, what's beyond the videos, right?

706
00:42:40,320 --> 00:42:43,520
I'm a firm believer that multimodal models will be the future.

707
00:42:44,240 --> 00:42:46,160
And I see text as a very lossy,

708
00:42:46,160 --> 00:42:48,560
kind of 1D projection of our physical world.

709
00:42:49,120 --> 00:42:52,560
So it's essential to include the other sensory modalities

710
00:42:52,560 --> 00:42:54,320
to provide a full in-body experience.

711
00:42:55,200 --> 00:42:57,440
And in the context of in-body relations,

712
00:42:57,440 --> 00:43:01,520
I think the input will be a mixture of text, images, videos,

713
00:43:01,520 --> 00:43:05,200
and even audio in the future, and the output will be actions.

714
00:43:06,560 --> 00:43:11,280
So here's a very early example of a multimodal language model

715
00:43:11,280 --> 00:43:12,160
for robot learning.

716
00:43:12,720 --> 00:43:14,400
So let's imagine a household robot.

717
00:43:15,200 --> 00:43:19,120
We can ask the robot to bring us a cup of tea from the kitchen,

718
00:43:19,120 --> 00:43:20,560
but if we want to be more specific,

719
00:43:21,200 --> 00:43:23,840
I want this particular cup that is my favorite cup.

720
00:43:23,840 --> 00:43:25,120
So show me this image.

721
00:43:26,320 --> 00:43:31,440
And we also provide a video demo of how we want to mop the floor

722
00:43:31,440 --> 00:43:35,120
and ask the robot to imitate the similar motion in context.

723
00:43:36,240 --> 00:43:38,960
And when a robot sees an unfamiliar object like a sweeper,

724
00:43:38,960 --> 00:43:41,120
we can explain it by providing an image

725
00:43:41,120 --> 00:43:42,400
and showing this is a sweeper.

726
00:43:42,400 --> 00:43:45,040
Now go ahead and do something with the tool.

727
00:43:45,600 --> 00:43:47,840
And finally, to ensure safety, we can say,

728
00:43:47,840 --> 00:43:50,160
take a picture of that room and just do not enter that room.

729
00:43:51,520 --> 00:43:55,120
To achieve this, back last year,

730
00:43:55,120 --> 00:43:57,360
we proposed a model called VIMA,

731
00:43:57,360 --> 00:43:59,040
which stands for Visual Model Attention.

732
00:43:59,680 --> 00:44:02,720
And in this work, we introduce a concept called multimodal prompting,

733
00:44:03,360 --> 00:44:07,440
where the prompt can be a mixture of text, image, and videos.

734
00:44:08,160 --> 00:44:10,640
And this provides a very expressive API

735
00:44:10,640 --> 00:44:14,000
that just unifies a bunch of different robot tasks

736
00:44:14,000 --> 00:44:17,280
that otherwise would require a very different pipeline

737
00:44:17,280 --> 00:44:19,680
or specialized models to solve in prior literature.

738
00:44:20,560 --> 00:44:23,360
And VIMA simply tokenizes everything,

739
00:44:25,120 --> 00:44:28,080
converting image and text into sequences of tokens,

740
00:44:28,080 --> 00:44:29,760
and train a transformer on top

741
00:44:29,760 --> 00:44:32,480
to output the robot arm actions

742
00:44:32,480 --> 00:44:36,080
autoregressively one step at a time during inference time.

743
00:44:37,440 --> 00:44:40,320
So just to look at some of the examples here,

744
00:44:40,960 --> 00:44:43,600
this prompt rearrange objects to match the scene.

745
00:44:43,600 --> 00:44:45,680
It is a classical task called Visual Go Reaching

746
00:44:45,680 --> 00:44:47,760
that has a big body of prior works on it.

747
00:44:48,960 --> 00:44:53,120
And that's how our robot does it, given this prompt.

748
00:44:54,160 --> 00:44:57,600
And we can also give it novel concepts in context.

749
00:44:57,600 --> 00:44:59,600
Like this is a blanket, this is a work,

750
00:44:59,600 --> 00:45:01,520
now put a work into a blanket.

751
00:45:01,520 --> 00:45:05,200
And both words are nonsensical, so it's not in the training data,

752
00:45:05,200 --> 00:45:06,960
but VIMA is able to generalize zero shot.

753
00:45:07,840 --> 00:45:10,640
And follow the motion to manipulate this object.

754
00:45:11,440 --> 00:45:13,520
So the bot understands what we want

755
00:45:13,520 --> 00:45:14,880
and then follow this trajectory.

756
00:45:15,440 --> 00:45:18,080
And finally, we can give it more complex prompt,

757
00:45:18,080 --> 00:45:20,240
like these are the safety constraints,

758
00:45:20,240 --> 00:45:23,680
sweep the box into this, but without exceeding that line.

759
00:45:23,680 --> 00:45:27,360
And we would do this using the interleaving image

760
00:45:27,360 --> 00:45:28,320
and text tokens.

761
00:45:30,480 --> 00:45:34,400
And recently, Google Brain Robotics followed up after VIMA

762
00:45:34,400 --> 00:45:38,000
with RT1 and RT2, robot transformer one and two.

763
00:45:38,960 --> 00:45:42,640
And RT2 is using a similar recipe, as I described,

764
00:45:42,640 --> 00:45:46,000
where they first kind of pre-train on internet scale data

765
00:45:46,560 --> 00:45:49,680
and then fine tune with some human collected demonstrations

766
00:45:49,680 --> 00:45:50,640
on the Google robots.

767
00:45:51,440 --> 00:45:53,920
And RoboCAD from DeepMind is another interesting work.

768
00:45:54,640 --> 00:45:58,880
They train a single unified policy that works not just on

769
00:45:59,600 --> 00:46:03,120
a single robot, but actually across different embodiments,

770
00:46:03,120 --> 00:46:06,320
different robot forms, and even generalize to a new hardware.

771
00:46:07,280 --> 00:46:10,560
So I think this is like a higher form of multimodal agent

772
00:46:10,560 --> 00:46:12,320
with a physical form factor.

773
00:46:12,320 --> 00:46:15,440
The morphology of the agent itself is another modality.

774
00:46:17,440 --> 00:46:20,640
So that concludes our looking forward section.

775
00:46:21,840 --> 00:46:25,280
And lastly, I want to kind of put all the links together

776
00:46:25,280 --> 00:46:27,120
of the works I described.

777
00:46:27,120 --> 00:46:28,960
So this is mindodger.org.

778
00:46:28,960 --> 00:46:31,360
We have open source everything.

779
00:46:31,360 --> 00:46:34,640
Well, for all the projects where big fans are open source,

780
00:46:34,640 --> 00:46:38,480
we open source as much as we can, including like the model code,

781
00:46:38,480 --> 00:46:43,120
checkpoints, simulator code, and training data.

782
00:46:44,560 --> 00:46:46,960
And this is Voyager.mindodger.org.

783
00:46:47,600 --> 00:46:48,560
This is Eureka.

784
00:46:49,600 --> 00:46:50,960
And this is VIMA.

785
00:46:53,280 --> 00:46:54,400
And one more thing, right?

786
00:46:54,960 --> 00:46:58,000
If you just want an excuse to play Minecraft at work,

787
00:46:58,720 --> 00:47:00,240
then mindodger is perfect for you

788
00:47:00,240 --> 00:47:02,800
because you are collecting human demonstration

789
00:47:02,800 --> 00:47:04,000
to train generalization.

790
00:47:04,000 --> 00:47:06,800
And there's one thing that you take away from this talk.

791
00:47:06,800 --> 00:47:07,680
It should be this slide.

792
00:47:09,920 --> 00:47:11,760
And lastly, I just want to remind all of us,

793
00:47:12,560 --> 00:47:15,200
despite all the progress I've shown, what we can do

794
00:47:15,760 --> 00:47:20,000
is still very far from human ingenuity as embodied agents.

795
00:47:21,120 --> 00:47:23,920
These are the videos from our dataset

796
00:47:23,920 --> 00:47:26,960
of people doing like decorating a winter wonderland

797
00:47:26,960 --> 00:47:29,600
or building the functioning CPU circuit within Minecraft.

798
00:47:30,480 --> 00:47:33,120
And we are very far from that as AI research.

799
00:47:33,680 --> 00:47:35,520
So here's a call to the community.

800
00:47:35,520 --> 00:47:38,480
If human can do these mind-blowing tasks,

801
00:47:38,480 --> 00:47:40,400
then why not our AI, right?

802
00:47:40,400 --> 00:47:41,520
Let's find out together.

