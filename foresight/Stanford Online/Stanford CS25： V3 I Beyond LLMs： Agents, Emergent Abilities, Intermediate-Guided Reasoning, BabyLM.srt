1
00:00:00,000 --> 00:00:11,200
So today we're going to give an instructor-led lecture talking about some of the key topics

2
00:00:11,200 --> 00:00:15,840
in Transformers and LLMs these days, and particularly Div will be talking about agents

3
00:00:15,840 --> 00:00:20,000
and I'll be discussing emergent abilities, intermediate-guided reasoning, as well as

4
00:00:20,000 --> 00:00:22,120
baby LLM.

5
00:00:22,120 --> 00:00:32,280
So let me actually go to my part, because Div is not here yet.

6
00:00:32,280 --> 00:00:36,280
So I'm sure many of you have read this paper, Emergent Abilities of Large Language Models

7
00:00:36,280 --> 00:00:42,160
from 2022, so I'll briefly go through some of them.

8
00:00:42,160 --> 00:00:46,960
So basically, an ability is emergent if it is present in large group but not smaller

9
00:00:46,960 --> 00:00:51,240
models, and it would not have been directly predicted by extrapolating performance from

10
00:00:51,240 --> 00:00:53,400
smaller models.

11
00:00:53,400 --> 00:00:57,280
So you can think of performance, it's basically near random until a certain threshold called

12
00:00:57,280 --> 00:01:01,080
a critical threshold, and then it improves very heavily.

13
00:01:01,080 --> 00:01:05,200
This is known as a phase transition, and again, it would not have been extrapolated or predicted

14
00:01:05,200 --> 00:01:10,920
if you were to extend the curve of the performance of smaller models, it's more of a jump which

15
00:01:10,920 --> 00:01:12,960
we'll see later.

16
00:01:12,960 --> 00:01:16,480
So here's an example of Fuchsia Prompting for many different tasks.

17
00:01:16,480 --> 00:01:22,960
For example, modular arithmetic unscrambling words, different QA tasks, and so forth.

18
00:01:22,960 --> 00:01:27,960
And you'll see that performance kind of jumps very heavily up until a certain point.

19
00:01:27,960 --> 00:01:32,720
I believe the x-axis here is the number of training flops, which corresponds to basically

20
00:01:32,720 --> 00:01:33,720
model scale.

21
00:01:33,720 --> 00:01:39,480
So you'll see in many cases around 10 to the 22 or 10 to the 23 training flops, there's

22
00:01:39,480 --> 00:01:47,680
a massive exponential jump or increase in terms of model performance on these tasks,

23
00:01:47,680 --> 00:01:51,600
which was not present on smaller scales.

24
00:01:51,600 --> 00:01:54,560
So it's quite unpredictable.

25
00:01:54,560 --> 00:01:59,080
And here are some examples of this occurring using augmented prompting strategies.

26
00:01:59,080 --> 00:02:02,040
So I'll be talking a bit later about chain of thought.

27
00:02:02,040 --> 00:02:09,080
But basically, these strategies improve the ability of getting behavior from models on

28
00:02:09,080 --> 00:02:10,280
different tasks.

29
00:02:10,280 --> 00:02:14,640
So you see, for example, with chain of thought reasoning, that's an emergent behavior that

30
00:02:14,640 --> 00:02:18,160
happens, again, around 10 to the 22 training flops.

31
00:02:18,160 --> 00:02:24,880
And without it, model performance on GSM 8K, which is a mathematics benchmark, it doesn't

32
00:02:24,880 --> 00:02:27,240
really improve heavily.

33
00:02:27,240 --> 00:02:33,440
But chain of thought kind of leads to that emergent behavior or sudden increase in performance.

34
00:02:33,440 --> 00:02:38,920
And here's just the table from the paper, which has a bigger list of emergent abilities

35
00:02:38,920 --> 00:02:42,240
of LLMs as well as their scale at which they occur.

36
00:02:42,240 --> 00:02:46,280
So I recommend that you check out the paper to learn a bit more.

37
00:02:46,280 --> 00:02:51,680
And so one thing researchers have been wondering is, why does this emergence occur exactly?

38
00:02:51,680 --> 00:02:55,440
And even now, there's few explanations for why that happens.

39
00:02:55,440 --> 00:02:59,320
And the authors also found that the evaluation metrics used to measure these abilities may

40
00:02:59,320 --> 00:03:04,840
not fully explain why they emerge and suggest some alternative evaluation metrics, which

41
00:03:04,840 --> 00:03:08,480
I encourage you to read more in the paper.

42
00:03:08,480 --> 00:03:14,720
So other than scaling up to encourage these emergent abilities, which could endow even

43
00:03:14,720 --> 00:03:20,360
larger LMs with further new emergent abilities, what else can be done?

44
00:03:20,360 --> 00:03:25,920
While things like investigating new architectures, higher quality data, which is very important

45
00:03:25,920 --> 00:03:31,160
for performance on all tasks, improved training and improved training procedures could enable

46
00:03:31,160 --> 00:03:37,400
emergent abilities to occur, especially on smaller models, which is a current growing

47
00:03:37,400 --> 00:03:42,040
area of research, which I'll also talk about a bit more later.

48
00:03:42,040 --> 00:03:48,120
Other abilities include potentially improving the few shot prompting abilities of LMs, theoretical

49
00:03:48,120 --> 00:03:54,200
and interpretability research, again, to try to understand why emergent abilities is a

50
00:03:54,200 --> 00:04:00,480
thing and how we can maybe leverage that further, as well as maybe some computational linguistics

51
00:04:00,480 --> 00:04:02,280
work.

52
00:04:02,280 --> 00:04:07,080
So with these large models and emergent abilities, there's also risks, right?

53
00:04:07,080 --> 00:04:14,240
There's potential societal risks, for example, truthfulness, bias and toxicity risks.

54
00:04:14,240 --> 00:04:18,680
As emergent abilities incentivize us further scaling up language models, for example, up

55
00:04:18,680 --> 00:04:25,800
to GPT4 size or further, however, this may lead to bias increasing, as well as toxicity

56
00:04:25,800 --> 00:04:29,240
and the memorization of training data.

57
00:04:29,240 --> 00:04:34,200
That's one thing that these larger models are more potent at.

58
00:04:34,200 --> 00:04:38,840
And there's potential risks in future language models that have also not been discovered yet.

59
00:04:38,840 --> 00:04:45,480
So it's important that we approach this in a safe manner, as well.

60
00:04:45,480 --> 00:04:51,280
And of course, emergent abilities and larger models have also led to sociological changes,

61
00:04:51,280 --> 00:04:54,840
changes in the community's views and use of these models.

62
00:04:54,840 --> 00:04:59,280
Most importantly, it's led to the development of general purpose models, which perform on

63
00:04:59,280 --> 00:05:03,840
a wide range of tasks, not just particular tasks it was trained for.

64
00:05:03,840 --> 00:05:08,560
For example, when you think of chat GPT, GPT 3.5, as well as GPT4, there are more general

65
00:05:08,560 --> 00:05:13,800
purpose models which work well across the board and can then be further adapted to different

66
00:05:13,800 --> 00:05:20,600
use cases, mainly through in-context prompting and so forth.

67
00:05:20,600 --> 00:05:24,800
This has also led to new applications of language models outside of NLP.

68
00:05:24,800 --> 00:05:28,600
For example, they're being used a lot now for text-to-image generation.

69
00:05:28,600 --> 00:05:34,520
The encoder parts of those text-to-image models are basically transformer models or

70
00:05:34,520 --> 00:05:39,280
large language models, as well as things like robotics and so forth.

71
00:05:39,280 --> 00:05:43,920
So you'll know that earlier, this quarter, Jim Fan gave a talk about how they're using

72
00:05:43,920 --> 00:05:51,480
GPT4 and so forth in Minecraft and for robotics work, as well as long-range horizon tasks

73
00:05:51,480 --> 00:05:52,480
for robotics.

74
00:05:52,480 --> 00:05:57,000
And yeah, so basically in general, it's led to a shift in the NLP community towards the

75
00:05:57,000 --> 00:06:02,480
general purpose rather than task-specific models.

76
00:06:02,480 --> 00:06:07,640
And as I kind of stated earlier, some directions for future work include model scaling, further

77
00:06:07,640 --> 00:06:14,680
model scaling, although I believe that we will soon probably be reaching a limit or

78
00:06:14,680 --> 00:06:18,400
point of the mission returns with just more model scale.

79
00:06:18,400 --> 00:06:22,880
Improved model architectures and training methods, data scaling.

80
00:06:22,880 --> 00:06:28,720
So I also believe that data quality is of high importance, possibly even more important

81
00:06:28,720 --> 00:06:33,120
than the model scale and the model itself.

82
00:06:33,120 --> 00:06:37,480
Better techniques for an understanding of prompting, as well as exploring and enabling

83
00:06:37,480 --> 00:06:42,520
performance on frontier tasks that current models are not able to perform well on.

84
00:06:42,520 --> 00:06:45,360
So GPT4 kind of pushed the limit on this.

85
00:06:45,360 --> 00:06:48,000
It's able to perform well on many more tasks.

86
00:06:49,000 --> 00:06:54,360
Studies have shown that it still suffers from even some more basic sort of reasoning,

87
00:06:54,360 --> 00:06:58,400
analogical and common sense reasoning.

88
00:06:58,400 --> 00:07:00,720
So I just had some questions here.

89
00:07:00,720 --> 00:07:07,360
I'm not sure how much time we have to address, but so for the first one, like I said, emergent

90
00:07:07,360 --> 00:07:10,760
abilities I think will arise to a certain point, but there will be a limit or point

91
00:07:10,760 --> 00:07:17,440
of the mission returns as model scale, as well as data scale rises, because I believe

92
00:07:17,440 --> 00:07:20,120
at some point there will be overfitting.

93
00:07:20,120 --> 00:07:25,000
And there's only so much you can learn from all data on the web.

94
00:07:25,000 --> 00:07:31,760
So I believe that more creative approaches will be necessary after a certain point, which

95
00:07:31,760 --> 00:07:36,720
kind of also addresses the second question.

96
00:07:36,720 --> 00:07:41,600
So I will move on.

97
00:07:41,600 --> 00:07:46,920
Anybody has any questions, also feel free to interrupt at any time.

98
00:07:46,920 --> 00:07:50,440
So the second thing I'll be talking about is this thing I call intermediate-guided reasoning.

99
00:07:50,440 --> 00:07:53,080
So I don't think this is actually a term.

100
00:07:53,080 --> 00:07:59,160
It's typically called chain of thought reasoning, but it's not just chains now being used.

101
00:07:59,160 --> 00:08:02,280
So I wanted to give it a more broad title.

102
00:08:02,280 --> 00:08:05,200
So I called it intermediate-guided reasoning.

103
00:08:05,200 --> 00:08:09,640
So this was inspired by this work, also by my friend Jason, who was at Google now at

104
00:08:09,640 --> 00:08:14,040
OpenAI, called chain of thought reasoning or COT.

105
00:08:14,040 --> 00:08:18,800
This is basically a series of intermediate reasoning steps, which has been shown to improve

106
00:08:18,800 --> 00:08:23,320
LLM performance, especially on more complex reasoning tasks.

107
00:08:23,320 --> 00:08:28,560
It's inspired by the human thought process, which is to decompose many problems into multi-step

108
00:08:28,560 --> 00:08:30,060
problems.

109
00:08:30,060 --> 00:08:34,920
For example, when you answer an exam, when you're solving math questions on an exam,

110
00:08:34,920 --> 00:08:38,680
you don't just go to the final answer, you kind of write out your steps.

111
00:08:38,680 --> 00:08:42,680
Even when you're just thinking through things, you kind of break it down into a piecewise

112
00:08:42,680 --> 00:08:47,760
or step-by-step fashion, which allows you to typically arrive at a more accurate final

113
00:08:47,760 --> 00:08:53,960
answer and more easily arrive at the final answer in the first place.

114
00:08:53,960 --> 00:08:58,720
Another advantage is this provides an interpretable window into the behavior of the model.

115
00:08:58,720 --> 00:09:03,400
You can see exactly how it arrived at an answer, and if it did so incorrectly, where in its

116
00:09:03,400 --> 00:09:09,960
reasoning path that it kind of goes wrong or starts going down an incorrect path of

117
00:09:09,960 --> 00:09:11,720
reasoning, basically.

118
00:09:11,840 --> 00:09:15,840
It basically exploits the fact that deep down in the model's weights, it knows more about

119
00:09:15,840 --> 00:09:20,240
the problem than simply prompting it to get a response.

120
00:09:20,240 --> 00:09:21,240
Here's an example.

121
00:09:21,240 --> 00:09:23,960
On the left side, you can see the standard prompting.

122
00:09:23,960 --> 00:09:28,920
You ask it a math question, and it just simply gives you an answer.

123
00:09:28,920 --> 00:09:31,720
Whereas on the right, you actually break it down step-by-step.

124
00:09:31,720 --> 00:09:37,760
You kind of get it to show its steps, to solve the mathematical word problem step-by-step.

125
00:09:37,760 --> 00:09:45,080
You'll see here that it actually gets the right answer, unlike standard prompting.

126
00:09:45,080 --> 00:09:49,160
So there's many different ways we can potentially improve chain of thought reasoning.

127
00:09:49,160 --> 00:09:55,480
In particular, it's also an emergent behavior that results in performance gains for larger

128
00:09:55,480 --> 00:09:57,120
language models.

129
00:09:57,120 --> 00:10:02,800
But still, even in larger models, there's still a non-negatable fraction of errors.

130
00:10:02,800 --> 00:10:07,640
These come from calculator errors, symbol mapping errors, one missing step errors.

131
00:10:07,640 --> 00:10:13,400
As well as bigger errors due to larger semantic understanding issues and generally incoherent

132
00:10:13,400 --> 00:10:15,000
chains of thought.

133
00:10:15,000 --> 00:10:19,280
And we can potentially investigate methods to address these.

134
00:10:19,280 --> 00:10:24,040
So as I said, chain of thought mainly works for huge models of approximately 100 billion

135
00:10:24,040 --> 00:10:27,120
parameters or more.

136
00:10:27,120 --> 00:10:31,320
And there's three potential reasons they do not work very well for smaller models.

137
00:10:31,320 --> 00:10:35,760
And that smaller models are fundamentally more limited and incapable.

138
00:10:35,760 --> 00:10:41,120
They fail at even relatively easier symbol mapping tasks as well as arithmetic tasks.

139
00:10:41,120 --> 00:10:45,040
They inherently are able to do math less effectively.

140
00:10:45,040 --> 00:10:48,840
And they often have logical loopholes and just never arrive at a final answer.

141
00:10:48,840 --> 00:10:51,400
For example, it goes on and on.

142
00:10:51,400 --> 00:10:56,160
It's like an infinite loop of logic that never actually converges anywhere.

143
00:10:56,160 --> 00:11:00,520
So if we're able to potentially improve chain of thought for smaller models, this could

144
00:11:00,520 --> 00:11:05,200
provide significant value to the research community.

145
00:11:05,200 --> 00:11:08,080
Another thing is to potentially generalize it.

146
00:11:08,080 --> 00:11:10,840
Right now, chain of thought has a more rigid definition and format.

147
00:11:10,840 --> 00:11:15,360
It's very step-by-step, very concrete and defined.

148
00:11:15,360 --> 00:11:19,480
As a result, its advantages are for particular domains and types of questions.

149
00:11:19,480 --> 00:11:23,920
For example, the task usually must be challenging and require multi-step reasoning.

150
00:11:23,920 --> 00:11:28,680
And it typically works better for things like arithmetic and not so much for things like

151
00:11:28,680 --> 00:11:32,960
response generation, QA, and so forth.

152
00:11:32,960 --> 00:11:38,600
And furthermore, it works better for problems or tasks that have a relatively flat scaling

153
00:11:38,600 --> 00:11:39,600
curve.

154
00:11:39,600 --> 00:11:44,440
Whereas when you think of humans, we think through different types of problems in multiple

155
00:11:44,440 --> 00:11:45,880
different ways.

156
00:11:45,880 --> 00:11:49,640
Our quote-unquote scratch path that we used to think about and arrive at a final answer

157
00:11:49,640 --> 00:11:54,680
for a problem, it's more flexible and open to different reasoning structures compared

158
00:11:54,680 --> 00:11:57,920
to such a rigid step-by-step format.

159
00:11:57,920 --> 00:12:01,960
So hence, we can maybe potentially generalize chain of thought to be more flexible and work

160
00:12:01,960 --> 00:12:05,360
for more types of problem.

161
00:12:05,360 --> 00:12:10,040
So now I'll briefly discuss some alternative or extension works to chain of thought.

162
00:12:10,040 --> 00:12:11,760
One is called tree of thought.

163
00:12:11,760 --> 00:12:16,000
This basically is more like a tree which considers multiple different reasoning paths.

164
00:12:16,000 --> 00:12:21,440
It also has the ability to look ahead and sort of backtrack and then go on other areas

165
00:12:21,440 --> 00:12:25,480
or other branches of the tree as necessary.

166
00:12:25,480 --> 00:12:30,320
So this leads to more flexibility and it's shown to improve performance on different

167
00:12:30,320 --> 00:12:34,640
tasks, including arithmetic tasks.

168
00:12:34,640 --> 00:12:40,080
There's also this work by my friend called Socratic Questioning, it's sort of a divide

169
00:12:40,080 --> 00:12:45,680
and conquer fashion algorithm, simulating the recursive thinking process of humans.

170
00:12:45,680 --> 00:12:51,520
So it uses a large-scale language model to kind of propose subproblems given a more complicated

171
00:12:51,520 --> 00:12:53,120
original problem.

172
00:12:54,120 --> 00:12:58,240
Just like tree of thought, it also has recursive backtracking and so forth.

173
00:12:58,240 --> 00:13:05,840
And the purpose is to answer all the subproblems and kind of go in an upwards fashion to arrive

174
00:13:05,840 --> 00:13:10,560
at a final answer to the original problem.

175
00:13:10,560 --> 00:13:16,640
There's also this line of work which kind of actually uses code as well as programs

176
00:13:16,640 --> 00:13:19,360
to help arrive at a final answer.

177
00:13:19,360 --> 00:13:23,640
For example, program-aided language models, it generates intermediate reasoning steps

178
00:13:23,640 --> 00:13:29,240
in the form of code which is then offloaded to a runtime such as a Python interpreter.

179
00:13:29,240 --> 00:13:33,720
And the point here is to decompose the natural language problem into runnable steps.

180
00:13:33,720 --> 00:13:39,240
So hence the amount of work for the large language model is lower.

181
00:13:39,240 --> 00:13:43,360
Its purpose now is simply to learn how to decompose the natural language problem into

182
00:13:43,360 --> 00:13:44,960
those runnable steps.

183
00:13:44,960 --> 00:13:50,400
And these steps themselves are then fed to, for example, a Python interpreter in order

184
00:13:50,400 --> 00:13:52,440
to solve them.

185
00:13:52,440 --> 00:13:58,040
And program-a thoughts here, POT, is very similar to this in that it kind of breaks

186
00:13:58,040 --> 00:14:04,160
it down into step-by-step of code instead of natural language which is then executed

187
00:14:04,160 --> 00:14:09,920
by a different and actual code interpreter or program.

188
00:14:09,920 --> 00:14:19,800
So this again works well for many sort of tasks that, for example, things like arithmetic.

189
00:14:19,800 --> 00:14:24,720
As you see that, those are kind of both of the examples for both of these papers.

190
00:14:24,720 --> 00:14:29,640
And just like what I said earlier, these also do not work very well for things like response

191
00:14:29,640 --> 00:14:35,520
generation, open-ended question answering, and so forth.

192
00:14:35,520 --> 00:14:37,880
And there's other work, for example, faith and faith.

193
00:14:37,880 --> 00:14:43,200
This actually breaks down problems into sub-steps in the form of computation graphs, which they

194
00:14:43,200 --> 00:14:46,200
show also works well for things like arithmetic.

195
00:14:46,200 --> 00:14:50,160
So you see that there's a trend here of this sort of intermediate-guided reasoning working

196
00:14:50,160 --> 00:14:56,800
very well for mathematical as well as logical problems, but not so much for other things.

197
00:14:56,800 --> 00:15:01,240
So again, I encourage you guys to maybe check out the original papers if you want to learn

198
00:15:01,240 --> 00:15:02,240
more.

199
00:15:02,240 --> 00:15:06,320
There's a lot of interesting work in this area these days.

200
00:15:06,320 --> 00:15:10,520
And I'll also be posting these slides as well as sending them.

201
00:15:10,520 --> 00:15:13,800
We'll probably post them on the website as well as this group.

202
00:15:13,800 --> 00:15:18,040
But I'll also send them through an email later.

203
00:15:18,040 --> 00:15:24,560
So very lastly, I want to touch upon this thing called the Baby Language Model.

204
00:15:24,560 --> 00:15:30,120
So like I said earlier, I think at some point, scale will reach a point of diminishing returns

205
00:15:30,120 --> 00:15:33,200
as well as the fact that further scale comes with many challenges.

206
00:15:33,200 --> 00:15:39,320
For example, it takes a long time and costs a lot of money to train these big models.

207
00:15:39,320 --> 00:15:44,920
And they cannot really be used by individuals who are not at huge companies with hundreds

208
00:15:44,920 --> 00:15:48,680
or thousands of GPUs and millions of dollars.

209
00:15:48,680 --> 00:15:53,560
So this thing, this challenge called Baby LM or Baby Language Model, which is attempting

210
00:15:53,560 --> 00:15:59,760
to train language models, particularly smaller ones, on the same amount of linguistic data

211
00:15:59,760 --> 00:16:02,800
available to a child.

212
00:16:02,800 --> 00:16:07,880
So data sets have grown by orders of magnitude, as well as, of course, model size.

213
00:16:07,880 --> 00:16:12,480
For example, Chinchilla sees approximately 1.4 trillion words during training.

214
00:16:12,480 --> 00:16:17,240
This is around 10,000 words for every one word that a 13-year-old child on average has

215
00:16:17,240 --> 00:16:20,680
heard as they grow up or develop.

216
00:16:20,680 --> 00:16:23,320
So the purpose here is, can we close this gap?

217
00:16:23,320 --> 00:16:32,080
Can we train smaller models on lower amounts of data while hopefully still attempting to

218
00:16:32,080 --> 00:16:36,800
get the performance of these much larger models?

219
00:16:36,800 --> 00:16:41,960
So basically, we're trying to focus on optimizing pre-training, given data limitations inspired

220
00:16:41,960 --> 00:16:44,480
by human development.

221
00:16:44,480 --> 00:16:49,440
And this will also ensure that research is possible for more individuals as well as

222
00:16:49,440 --> 00:16:54,400
labs and potentially possible on a university budget.

223
00:16:54,400 --> 00:16:59,040
As it seems now that a lot of research is kind of restricted to large companies, which

224
00:16:59,040 --> 00:17:03,080
I said have a lot of resources as well as money.

225
00:17:03,080 --> 00:17:04,360
So again, why baby LLM?

226
00:17:04,360 --> 00:17:08,680
Well, it can really improve the efficiency of training as well as using large language

227
00:17:08,680 --> 00:17:09,680
models.

228
00:17:09,680 --> 00:17:15,280
It can potentially open up new doors and potential use cases.

229
00:17:15,280 --> 00:17:18,800
It can lead to improved interpretability as well as alignment.

230
00:17:18,800 --> 00:17:22,800
Smaller models would be easier to control a line as well as interpret what exactly is

231
00:17:22,800 --> 00:17:29,320
going on compared to incredibly large LLMs, which are basically huge black boxes.

232
00:17:29,320 --> 00:17:33,880
This will again potentially lead to enhanced open source availability, for example, large

233
00:17:33,880 --> 00:17:41,360
language models runnable on consumer PCs, as well as by smaller labs and companies.

234
00:17:41,360 --> 00:17:46,920
The techniques discovered here can also possibly be applied to larger scales.

235
00:17:46,920 --> 00:17:50,920
And further, this may lead to a greater understanding of the cognitive models of humans and how

236
00:17:50,920 --> 00:17:55,360
exactly we are able to learn language much more efficiently than these large language

237
00:17:55,360 --> 00:17:57,000
models.

238
00:17:57,000 --> 00:18:01,840
So there may be a flow of knowledge from cognitive science and psychology to NLP and machine learning,

239
00:18:01,840 --> 00:18:05,680
but also in the other direction.

240
00:18:05,680 --> 00:18:11,480
So briefly, the baby LLM training data that the authors of this challenge provide, it's

241
00:18:11,480 --> 00:18:18,560
a developmentally inspired pre-training data set, which has under 100 million words because

242
00:18:18,560 --> 00:18:23,600
children are exposed to approximately 2 to 7 million words per year as they grow up.

243
00:18:23,600 --> 00:18:28,360
Up to the age of 13, that's approximately 90 million words, so they round up to 100.

244
00:18:28,360 --> 00:18:33,000
It's mostly transcribed speech, and their motivation there is that the most of the input to children

245
00:18:33,000 --> 00:18:38,000
is spoken, and thus their data set focuses on transcribed speech.

246
00:18:38,000 --> 00:18:42,720
It's also mixed domain because children are typically exposed to a variety of language

247
00:18:42,720 --> 00:18:46,600
or speech from different domains.

248
00:18:46,600 --> 00:18:52,280
So it has child directed speech, open subtitles, which are subtitles of movies, TV shows and

249
00:18:52,280 --> 00:18:54,440
so forth.

250
00:18:54,440 --> 00:19:00,280
Simple children's books, which contain stories that children would likely hear as they're

251
00:19:00,280 --> 00:19:01,280
growing up.

252
00:19:01,280 --> 00:19:05,600
But it also has some Wikipedia as well as simple Wikipedia, and here are just some examples

253
00:19:05,600 --> 00:19:14,600
of child directed speech, children's stories, Wikipedia, and so forth.

254
00:19:14,600 --> 00:19:19,760
So that's it for my portion of the presentation, and I'll hand it off to Div, who will talk

255
00:19:19,760 --> 00:19:24,800
a bit about AI agents.

256
00:19:24,800 --> 00:19:35,880
Yeah, so like everyone must have seen like there's this like a new trend where like everything

257
00:19:35,880 --> 00:19:39,960
is transitioning to more like agents, that's like the new hot cream.

258
00:19:39,960 --> 00:19:43,280
And we're seeing this like people are going more from like language models to like now

259
00:19:43,280 --> 00:19:45,200
building AI agents.

260
00:19:45,200 --> 00:19:46,200
And then what's the biggest difference?

261
00:19:46,200 --> 00:19:51,920
Like why agents are not just like, why does not train like a big large language model?

262
00:19:51,920 --> 00:19:56,360
And I will sort of like go into like, like why, what's the difference?

263
00:19:56,360 --> 00:20:02,360
And then also discuss a bunch of things such as like, how can you use agents for doing actions?

264
00:20:02,360 --> 00:20:05,920
How can you, what are some emergent architectures?

265
00:20:05,920 --> 00:20:09,000
How can you sort of like build like human like agents?

266
00:20:09,000 --> 00:20:11,200
How can you use it for computer interactions?

267
00:20:11,200 --> 00:20:14,320
How do you solve problems from long-term memory personalization?

268
00:20:14,320 --> 00:20:17,600
And there's a lot of like other things you can do which is like multi-agent communication

269
00:20:17,600 --> 00:20:19,200
and there are a few things from which directions.

270
00:20:19,200 --> 00:20:22,560
So we'll try to cover as much as we can.

271
00:20:22,560 --> 00:20:29,120
So first, let's talk about like why should we even build AI agents, right?

272
00:20:29,120 --> 00:20:35,400
And so it's like, here's there's a key thesis, which is that humans will communicate with

273
00:20:35,400 --> 00:20:37,800
AI using natural language.

274
00:20:37,800 --> 00:20:42,720
And AI will be operating all the machines, just allowing for more intuitive and efficient

275
00:20:42,720 --> 00:20:43,720
operations.

276
00:20:43,720 --> 00:20:49,040
So right now what happens is like me as a human, I'm like directly like using my computer,

277
00:20:49,040 --> 00:20:51,680
I'm using my phone, but this is really inefficient.

278
00:20:51,680 --> 00:20:55,040
Like we are not optimized by nature to be able to do that.

279
00:20:55,040 --> 00:20:57,360
We are actually really, really bad at this.

280
00:20:57,360 --> 00:21:02,400
But if you can just talk to AI, just like with language and the AI is just really good

281
00:21:02,400 --> 00:21:06,440
enough that you can just do this like super faster, obviously like 100x speeds compared

282
00:21:06,440 --> 00:21:08,440
to human, and that's going to happen.

283
00:21:08,440 --> 00:21:14,000
And I think that's the future of how things are going to evolve in the next five years.

284
00:21:14,000 --> 00:21:16,840
And I sort of like call this like software 3.0.

285
00:21:16,840 --> 00:21:20,480
I have a blog post about this that you can read if you want to, where the idea is like

286
00:21:20,480 --> 00:21:26,200
you can think of a large language model as a computing chip in a sense, so similar to

287
00:21:26,200 --> 00:21:30,760
like a chip that's powering like a whole system and then and then you can build abstractions

288
00:21:30,760 --> 00:21:32,760
and all.

289
00:21:32,760 --> 00:21:34,760
Cool.

290
00:21:34,760 --> 00:21:41,200
So, so what should do we need agents to usually like a single call to a large language model

291
00:21:41,200 --> 00:21:42,200
is not enough.

292
00:21:42,200 --> 00:21:45,560
Yeah, you need chaining, you need like recursion, you need a lot of like more things.

293
00:21:45,560 --> 00:21:50,320
And that's why you want to build systems, not like just like a single monolith.

294
00:21:50,320 --> 00:21:52,280
Second is like, yeah, so how do we do this?

295
00:21:52,280 --> 00:21:56,520
So we do a lot of techniques, especially around like multiple calls to a model.

296
00:21:56,520 --> 00:21:58,280
And there's a lot of ingredients involved here.

297
00:21:58,280 --> 00:22:01,960
And I will say like building like an agent is very similar to like maybe like thinking

298
00:22:01,960 --> 00:22:03,440
about building a computer.

299
00:22:03,440 --> 00:22:07,360
So like the LLM is like a like a CPU, so you have a CPU, but now you want to like sort

300
00:22:07,360 --> 00:22:10,840
of like sort of the problems like, okay, like how do you output RAM, how do you put memory,

301
00:22:10,840 --> 00:22:16,440
how do I do like actions, how do I build like a interface, how do I internet access, how

302
00:22:16,440 --> 00:22:18,000
do I personalize it to the user.

303
00:22:18,000 --> 00:22:23,120
So this is like almost like you're trying to build a computer.

304
00:22:23,120 --> 00:22:26,800
And that's what makes it like a really hard problem.

305
00:22:27,800 --> 00:22:32,080
This is like an example of a general architecture for agents.

306
00:22:32,080 --> 00:22:37,320
This is from Lydian Bank, who's like, it's a chair of an AI.

307
00:22:37,320 --> 00:22:40,160
And like you can imagine, like an agent has a lot of ingredients.

308
00:22:40,160 --> 00:22:42,920
So you want to have memory, which would be short term, knock on them.

309
00:22:42,920 --> 00:22:46,240
You have tools, which could be like, you can go and like use like classical tools like

310
00:22:46,240 --> 00:22:50,000
a calculator, calendar code interpreter, et cetera.

311
00:22:50,000 --> 00:22:53,760
You want to have some sort of like a planning layer where you can like sell a flag, have

312
00:22:53,800 --> 00:22:58,160
like chains of cards and trees of cards, as Stephen discussed, and use all of that, like

313
00:22:58,400 --> 00:23:02,120
actually like act on behalf of a user in some environment.

314
00:23:06,200 --> 00:23:11,560
I will go maybe like discuss like more down a bit just to give a sense also the top one

315
00:23:11,560 --> 00:23:13,080
before us on that.

316
00:23:13,080 --> 00:23:16,400
So this is sort of like an agent I'm building, which is more of a browser agent.

317
00:23:16,960 --> 00:23:20,880
The name is inspired from quantum physics, specifically on the words like, you know,

318
00:23:20,880 --> 00:23:23,120
like neutron more on formula and so like multi on.

319
00:23:23,160 --> 00:23:28,040
So it's like a hypothetical physics particle that's present at multiple places.

320
00:23:28,760 --> 00:23:32,520
And I'll just like go through some demos to just motivate agents.

321
00:23:36,520 --> 00:23:40,240
And so this is like a idea of one thing we did there.

322
00:23:40,240 --> 00:23:44,400
Like here, the agent is going and it's autonomously with no black format.

323
00:23:44,920 --> 00:23:46,840
So this is like zero human interventions.

324
00:23:47,920 --> 00:23:49,840
The AI is controlling the browser.

325
00:23:49,840 --> 00:23:55,600
It's just like this entire actions and simply go and book a flight into and here

326
00:23:56,000 --> 00:23:57,040
it's personalized to me.

327
00:23:57,040 --> 00:24:01,640
So it knows like, okay, like I like me like you might hear this economic and

328
00:24:02,720 --> 00:24:05,040
it knows me like some of my preferences.

329
00:24:05,320 --> 00:24:10,080
It already has access to my accounts so it can go and actually like log into my

330
00:24:10,080 --> 00:24:13,400
account can actually like actually has purchased in power.

331
00:24:13,720 --> 00:24:17,320
So it can just use my card that is stored in the account and then actually

332
00:24:17,320 --> 00:24:19,640
so it will just be able to fly over.

333
00:24:47,320 --> 00:24:52,040
It sort of motivates like what you can do with agents.

334
00:24:52,040 --> 00:24:54,920
Now imagine if this thing was running hundreds of times

335
00:24:54,920 --> 00:24:56,960
and that's literally just all so many things, right?

336
00:24:56,960 --> 00:24:58,880
Because like I don't need websites anymore.

337
00:24:58,880 --> 00:24:59,720
I don't need to be an idea,

338
00:24:59,720 --> 00:25:01,560
like why does the internet even have a website?

339
00:25:01,560 --> 00:25:02,400
I can just ask the agent,

340
00:25:02,400 --> 00:25:07,120
like just like talk to it and it's done.

341
00:25:07,120 --> 00:25:09,880
And I think that's how a lot of technology will evolve

342
00:25:09,880 --> 00:25:11,480
over the next couple of years.

343
00:25:18,320 --> 00:25:20,320
Cool, okay.

344
00:25:20,320 --> 00:25:23,320
I can also maybe like show one of the more demos.

345
00:25:23,320 --> 00:25:27,320
So you can do similar things, say from a mobile phone,

346
00:25:27,320 --> 00:25:32,320
where the idea is you have these agents that are present on a phone

347
00:25:32,320 --> 00:25:37,320
and you can like chat with them or make a talk with them using WhatsApp.

348
00:25:37,320 --> 00:25:39,320
And this one's actually in my demo.

349
00:25:39,320 --> 00:25:44,320
So you can ask it like, how can you order this set it for me?

350
00:25:48,320 --> 00:25:55,320
And then what you can have is like the agent can remotely go

351
00:25:55,320 --> 00:26:01,320
and use your account to actually like do this for you instantaneously.

352
00:26:01,320 --> 00:26:05,320
And here you're showing like what the agent is doing.

353
00:26:05,320 --> 00:26:09,320
And then it can go and like act like a virtual human

354
00:26:09,320 --> 00:26:11,320
and build the whole interaction.

355
00:26:17,320 --> 00:26:36,320
So that's all the idea.

356
00:26:36,320 --> 00:26:38,320
And I can show one final.

357
00:26:38,320 --> 00:26:39,320
Oh, I think this is not voting,

358
00:26:39,320 --> 00:26:44,320
but we also had this thing where recently passed the telephony test.

359
00:26:44,320 --> 00:26:50,320
So we did this experiment where we actually like had like an agent

360
00:26:50,320 --> 00:26:53,320
go and take the online driving test in California.

361
00:26:53,320 --> 00:26:59,320
And we had like a human like there with their like hands about the keyboard

362
00:26:59,320 --> 00:27:01,320
and mouse, not touching anything.

363
00:27:01,320 --> 00:27:03,320
And the agent that knows we went to the website,

364
00:27:03,320 --> 00:27:07,320
it took the quiz, it navigated the whole thing and went and actually passed.

365
00:27:07,320 --> 00:27:10,320
So the video's not there, but like we actually got a driving permit.

366
00:27:11,320 --> 00:27:12,320
I need to take this.

367
00:27:12,320 --> 00:27:13,320
Sure.

368
00:27:17,320 --> 00:27:18,320
Cool.

369
00:27:18,320 --> 00:27:20,320
So this is like sort of like what do you want about agents, right?

370
00:27:20,320 --> 00:27:23,320
Like it's like you can just simplify so many things where like,

371
00:27:23,320 --> 00:27:27,320
like so many things just but we don't realize that because we just got so used

372
00:27:27,320 --> 00:27:29,320
to impacting the technology the way we do right now.

373
00:27:29,320 --> 00:27:33,320
But if we can just like like reimagine all of this from scratch,

374
00:27:33,320 --> 00:27:35,320
and that's what agents will allow us to do.

375
00:27:36,320 --> 00:27:37,320
Sure.

376
00:27:40,320 --> 00:27:44,320
And I would say like an agent can act like a digital extension of a user.

377
00:27:44,320 --> 00:27:46,320
So suppose you have an agent that's personalized to you,

378
00:27:46,320 --> 00:27:50,320
think of something like the Jarvis, like if it's an Ironman.

379
00:27:50,320 --> 00:27:52,320
And then if it just knows so many things about you,

380
00:27:52,320 --> 00:27:55,320
it's acting like a personal brand and it's just like doing things.

381
00:27:55,320 --> 00:27:57,320
It's a very powerful assistant.

382
00:27:57,320 --> 00:28:01,320
And I think that's the direction a lot of things will go in the future.

383
00:28:01,320 --> 00:28:05,320
And especially if you build like human like agents,

384
00:28:05,320 --> 00:28:07,320
they don't have barriers around programming.

385
00:28:07,320 --> 00:28:09,320
Like they don't have programmatic barriers.

386
00:28:09,320 --> 00:28:12,320
So they can do whatever like I can do so it can go use my like,

387
00:28:12,320 --> 00:28:14,320
it can like interact with the website as I will do,

388
00:28:14,320 --> 00:28:16,320
it can interact with my computer as I will do.

389
00:28:16,320 --> 00:28:18,320
It doesn't have to like go through APIs abstractions,

390
00:28:18,320 --> 00:28:20,320
which are more restricted.

391
00:28:20,320 --> 00:28:22,320
And it's also very simple as an action space because you're just doing,

392
00:28:22,320 --> 00:28:26,320
doing like clicking and typing, which is like very simple.

393
00:28:26,320 --> 00:28:30,320
And then you can like also like it's very easy to teach such agents.

394
00:28:30,320 --> 00:28:33,320
So I can just like show the agent how to do something and the agent

395
00:28:33,320 --> 00:28:35,320
can just learn from me and improve over time.

396
00:28:35,320 --> 00:28:40,320
So that also makes it like really powerful and easy to like just teach this agent

397
00:28:40,320 --> 00:28:43,320
because there's like so much data that I can actually just generate

398
00:28:43,320 --> 00:28:46,320
and use that to keep improving it.

399
00:28:46,320 --> 00:28:50,320
And there's a different levels of autonomy when it comes to agents.

400
00:28:50,320 --> 00:28:53,320
So this chart is borrowed from autonomous driving,

401
00:28:53,320 --> 00:28:57,320
where people actually like try to solve this sort of like autonomy problem

402
00:28:57,320 --> 00:28:59,320
for actual costs.

403
00:28:59,320 --> 00:29:01,320
And they spend like more than 10 years.

404
00:29:01,320 --> 00:29:05,320
Success have been like, okay, they're still like working on it.

405
00:29:05,320 --> 00:29:09,320
But what like the self driving industry data is it gave like everyone

406
00:29:09,320 --> 00:29:13,320
like a blueprint on how to build this all like autonomous systems.

407
00:29:13,320 --> 00:29:15,320
And they came with like a lot of like classification.

408
00:29:15,320 --> 00:29:21,320
They came with a lot of like, we used to like think about the problem.

409
00:29:21,320 --> 00:29:25,320
And like the current standard is you think of like agents

410
00:29:25,320 --> 00:29:27,320
as like five different like those levels.

411
00:29:27,320 --> 00:29:29,320
So level zero is zero automation.

412
00:29:29,320 --> 00:29:32,320
That's like this, like you are like a you are a human that's operating

413
00:29:32,320 --> 00:29:35,320
like the computer themselves.

414
00:29:35,320 --> 00:29:37,320
Level one is you have some sort of assistance.

415
00:29:37,320 --> 00:29:40,320
So if you use like something like GitHub co-pilot,

416
00:29:40,320 --> 00:29:42,320
which is like sort of auto computing code for you,

417
00:29:42,320 --> 00:29:46,320
that's something like L1 where like auto complete.

418
00:29:46,320 --> 00:29:49,320
L2 becomes more of like, it's like partial automation.

419
00:29:49,320 --> 00:29:51,320
So it's maybe like doing some stuff for you.

420
00:29:51,320 --> 00:29:54,320
If anyone has you the new cursor ID, I will call that more like L2,

421
00:29:54,320 --> 00:29:57,320
which is like you give it like, okay, like add this code for me to sign the code.

422
00:29:57,320 --> 00:30:01,320
Chagivity can come as somewhat L2 because you can ask it like, oh, like,

423
00:30:01,320 --> 00:30:03,320
here's this thing, can you do this?

424
00:30:03,320 --> 00:30:07,320
Because like doing some sort of automation on an input.

425
00:30:07,320 --> 00:30:11,320
And then like, and then you can think of more levels.

426
00:30:11,320 --> 00:30:15,320
So it's like obviously like after L3, it gets more exciting.

427
00:30:15,320 --> 00:30:22,320
So L3 is the agent is actually like controlling the computer in that case

428
00:30:22,320 --> 00:30:26,320
and it's actually doing things where human is acting as a fallback mechanism.

429
00:30:26,320 --> 00:30:28,320
And then you go to like L4.

430
00:30:28,320 --> 00:30:31,320
L4, you say like this with the human doesn't even need to be there.

431
00:30:31,320 --> 00:30:35,320
But in very critical cases where like something very wrong might happen,

432
00:30:35,320 --> 00:30:38,320
you might have a human like sort of like take over my case.

433
00:30:38,320 --> 00:30:43,320
And L5 will basically say like this zero human presence.

434
00:30:43,320 --> 00:30:48,320
And I will say like what we have currently seen is like we are far near like L2,

435
00:30:48,320 --> 00:30:51,320
maybe some L3 systems in terms of software.

436
00:30:51,320 --> 00:30:59,320
And I think we are going to transition more to like L4 and L5 systems over the next years.

437
00:30:59,320 --> 00:31:00,320
Cool.

438
00:31:00,320 --> 00:31:05,320
So next I will go and like select computer interactions.

439
00:31:05,320 --> 00:31:10,320
So suppose you want an agent that can like do computer interactions for you.

440
00:31:10,320 --> 00:31:11,320
There's two ways to do that.

441
00:31:11,320 --> 00:31:17,320
So one is through APIs where it's programmatically using some APIs and tools

442
00:31:17,320 --> 00:31:20,320
and doing that to do tasks.

443
00:31:20,320 --> 00:31:23,320
The second one is more like direct interaction which is like keyword and mouse control

444
00:31:23,320 --> 00:31:28,320
where like it's doing the same thing as you're doing as you can.

445
00:31:28,320 --> 00:31:30,320
Both of these approaches have been explored a lot.

446
00:31:30,320 --> 00:31:32,320
There's like a lot of companies working on this.

447
00:31:32,320 --> 00:31:38,320
For the API route like Territory plugins and like the new assistant API are the ones in the direction.

448
00:31:38,320 --> 00:31:44,320
And there's also this work from what we call Gorilla which actually also explores how can you

449
00:31:44,320 --> 00:31:50,320
say like train a model that can use like 10,000 tools at once and train it on the API.

450
00:31:50,320 --> 00:31:53,320
And there's like pros and cons of both approaches.

451
00:31:53,320 --> 00:31:54,320
API is a nice thing.

452
00:31:54,320 --> 00:31:58,320
It's easy to like learn the API.

453
00:31:58,320 --> 00:32:00,320
It's safe.

454
00:32:00,320 --> 00:32:01,320
It's very controllable.

455
00:32:01,320 --> 00:32:02,320
So that's just for favor.

456
00:32:02,320 --> 00:32:04,320
You know how do you take it.

457
00:32:04,320 --> 00:32:07,320
It can be like more like a bad interaction.

458
00:32:07,320 --> 00:32:08,320
I would say it's more preformed.

459
00:32:08,320 --> 00:32:11,320
So it's like easy to take actions but more things can go wrong.

460
00:32:11,320 --> 00:32:14,320
And you need to work a lot and like make sure everything is safe.

461
00:32:14,320 --> 00:32:21,320
And build guarantees.

462
00:32:21,320 --> 00:32:23,320
Maybe I can show this.

463
00:32:23,320 --> 00:32:37,320
So this is sort of like another exploration where you can invoke an agent from like a very simple interface.

464
00:32:37,320 --> 00:32:42,320
So the idea is like you can get this like API that invoke our agent that's in building a computer.

465
00:32:42,320 --> 00:32:47,320
And so this can be on sort of a universal API where I just use one agent.

466
00:32:47,320 --> 00:32:53,320
I give it like a English command and the agent can automatically understand from that and go do anything.

467
00:32:53,320 --> 00:32:56,320
So basically like you can think of that as like no API.

468
00:32:56,320 --> 00:32:57,320
So I don't need to use API.

469
00:32:57,320 --> 00:33:01,320
I can just have one agent that can go and do everything.

470
00:33:01,320 --> 00:33:06,320
And so this is like some exploration we have done with agents.

471
00:33:06,320 --> 00:33:09,320
Cool.

472
00:33:09,320 --> 00:33:10,320
Okay.

473
00:33:10,320 --> 00:33:13,320
So this sort of like goes into computer interactions.

474
00:33:13,320 --> 00:33:16,320
I can cover more but I will potentially jump to other topics.

475
00:33:16,320 --> 00:33:19,320
But if you're to ask any questions about these topics.

476
00:33:19,320 --> 00:33:23,320
So yeah.

477
00:33:23,320 --> 00:33:25,320
Cool.

478
00:33:25,320 --> 00:33:27,320
So let's go back to the analogy I discussed earlier.

479
00:33:27,320 --> 00:33:33,320
So I would say you can think of any model as a sort of like a computing.

480
00:33:33,320 --> 00:33:35,320
And you can maybe call it like a new computing.

481
00:33:35,320 --> 00:33:40,320
It's similar to like a CPU, which is like a which is like solid brain.

482
00:33:40,320 --> 00:33:42,320
There's power in like your computer in a sense.

483
00:33:42,320 --> 00:33:44,320
So that's kind of all the business power.

484
00:33:44,320 --> 00:33:46,320
It's doing everything that's happening.

485
00:33:46,320 --> 00:33:50,320
And you can think of the same thing like the model is like a cortex.

486
00:33:50,320 --> 00:33:51,320
It's like it's a main brain.

487
00:33:51,320 --> 00:33:53,320
That's the main part of the brain.

488
00:33:53,320 --> 00:33:55,320
That's been the thing being processing.

489
00:33:55,320 --> 00:33:57,320
But a brain has more layers.

490
00:33:57,320 --> 00:34:00,320
It's just not like they're just not a cortex.

491
00:34:00,320 --> 00:34:02,320
And how do you print models work?

492
00:34:02,320 --> 00:34:06,320
We take some input tokens and they give you some output tokens.

493
00:34:06,320 --> 00:34:10,320
And this is very similar to like how also like a CPUs work to some extent,

494
00:34:10,320 --> 00:34:15,320
where you give it some instructions in and you get some instructions out.

495
00:34:15,320 --> 00:34:18,320
So you can compare this with the actual CPU.

496
00:34:18,320 --> 00:34:24,320
This is like the diagram on the right is a very simple processor.

497
00:34:24,320 --> 00:34:27,320
It's like a 32 bit MIPS 32.

498
00:34:27,320 --> 00:34:32,320
And it has like similar things where you have like a like different coding

499
00:34:32,320 --> 00:34:34,320
for different parts of the instruction.

500
00:34:34,320 --> 00:34:38,320
But this is like sort of like encoding some sort of like binary token in a sense.

501
00:34:38,320 --> 00:34:41,320
Like zero ones of like a bunch of like tokens.

502
00:34:41,320 --> 00:34:44,320
And then you're feeding it and then getting a bunch of zeros one out.

503
00:34:44,320 --> 00:34:49,320
And like how like the like a model is operating is like you're doing a very similar thing.

504
00:34:49,320 --> 00:34:51,320
But like then space is now English.

505
00:34:51,320 --> 00:34:57,320
So you basically instead of zero ones, you have like English characters.

506
00:34:57,320 --> 00:35:00,320
And then you can like get more powerful instructions on the top of this.

507
00:35:00,320 --> 00:35:05,320
So you can think like if this is like acting like a CPU, what you can do is you can build a lot of other things

508
00:35:05,320 --> 00:35:08,320
which are like you can have a scratch pad, you can have some sort of memory,

509
00:35:08,320 --> 00:35:10,320
you can have some sort of instructions.

510
00:35:10,320 --> 00:35:13,320
And then you can like do the cursor calls where like I load some stuff from the memory,

511
00:35:13,320 --> 00:35:16,320
put that in this like instruction, pass it to the transformer,

512
00:35:16,320 --> 00:35:18,320
which is doing the processing for me.

513
00:35:18,320 --> 00:35:20,320
We get we get the process outputs.

514
00:35:20,320 --> 00:35:23,320
Then you can throw that in the memory or we can like keep processing it.

515
00:35:23,320 --> 00:35:25,320
So there's like sort of like very similar to like code execution,

516
00:35:25,320 --> 00:35:28,320
like first line of code execution, second, third, fourth.

517
00:35:28,320 --> 00:35:32,320
So you just keep repeating that.

518
00:35:32,320 --> 00:35:35,320
Okay.

519
00:35:35,320 --> 00:35:38,320
So here we can like sort of discuss the concept of memory here.

520
00:35:38,320 --> 00:35:40,320
And now it's like building this analogy.

521
00:35:40,320 --> 00:35:47,320
You can think the memory for an agent is a very similar to like say like having a disk in a computer.

522
00:35:47,320 --> 00:35:52,320
So you want to have a disk just to make sure like everything is long-lived and persistent.

523
00:35:52,320 --> 00:35:56,320
So if you look at something like chat GPT, it doesn't have any sort of like persistent memory.

524
00:35:56,320 --> 00:36:00,320
And then you need to have a way to like load that and like store that.

525
00:36:00,320 --> 00:36:03,320
And there's a lot of mechanisms to do that right now.

526
00:36:03,320 --> 00:36:07,320
Most of them are like embeddings where you have some sort of like embedding model

527
00:36:07,320 --> 00:36:10,320
that has like created an embedding of the data you care about.

528
00:36:10,320 --> 00:36:13,320
And the model can like write that embeddings load the right part of the embeddings

529
00:36:13,320 --> 00:36:16,320
and then like use that to do the operation you want.

530
00:36:17,320 --> 00:36:19,320
So that's like the current mechanisms.

531
00:36:19,320 --> 00:36:22,320
There's still a lot of questions here, especially around hierarchy.

532
00:36:22,320 --> 00:36:24,320
Like how do I do this at scale?

533
00:36:24,320 --> 00:36:26,320
It's still very challenging.

534
00:36:26,320 --> 00:36:29,320
Like suppose I have one data backup that I want to like embed and process.

535
00:36:29,320 --> 00:36:31,320
Like most of the methods right now will fit.

536
00:36:31,320 --> 00:36:33,320
They're like really bad.

537
00:36:33,320 --> 00:36:36,320
The second issue is temporal coherence.

538
00:36:36,320 --> 00:36:39,320
Like if I have like a lot of data is temporal.

539
00:36:39,320 --> 00:36:40,320
It is sequential.

540
00:36:40,320 --> 00:36:42,320
It has like a unit of time.

541
00:36:42,320 --> 00:36:44,320
And dealing with that sort of data can be hard.

542
00:36:44,320 --> 00:36:47,320
Like it's like, like how do I deal with like a memory in a sense,

543
00:36:47,320 --> 00:36:52,320
which are like sort of like changing over time and loading the right part of that memory sequence.

544
00:36:54,320 --> 00:36:56,320
Another interesting challenge is structure.

545
00:36:56,320 --> 00:36:58,320
Like a lot of data is actually structured.

546
00:36:58,320 --> 00:37:00,320
Like it could be like a graphical structure.

547
00:37:00,320 --> 00:37:02,320
It could be like a tabular structure.

548
00:37:02,320 --> 00:37:09,320
How do we like sort of like pick advantage of this structure and like also use that

549
00:37:09,320 --> 00:37:11,320
when you're editing the data?

550
00:37:11,320 --> 00:37:14,320
And then like there's a lot of questions from adaptation where like,

551
00:37:14,320 --> 00:37:17,320
suppose you know how to better embed data or like,

552
00:37:17,320 --> 00:37:20,320
you know, you have like a specialized problem you care about

553
00:37:20,320 --> 00:37:23,320
and you want to be able to adapt how you're loading and storing the data

554
00:37:23,320 --> 00:37:25,320
and learn that on the fly.

555
00:37:25,320 --> 00:37:29,320
And that is something also that's a very interesting topic.

556
00:37:29,320 --> 00:37:32,320
So I would say like this is actually one of the most interesting topics right now,

557
00:37:32,320 --> 00:37:36,320
which as people are exploring, but it's still very under exploring.

558
00:37:37,320 --> 00:37:41,320
Okay.

559
00:37:41,320 --> 00:37:45,320
Talking about memory, I would say like another concept for agents is personalization.

560
00:37:45,320 --> 00:37:49,320
So personalization is more like, okay, like understanding the user.

561
00:37:49,320 --> 00:37:55,320
And I like to think of this as like a problem called like user agent alignment.

562
00:37:55,320 --> 00:37:59,320
And the idea is like suppose I have an agent that has purchasing power,

563
00:37:59,320 --> 00:38:02,320
has that access to my accounts, access to my data.

564
00:38:02,320 --> 00:38:03,320
I ask you to go book of life,

565
00:38:03,320 --> 00:38:05,320
but it's possible maybe this doesn't know what that I like

566
00:38:05,320 --> 00:38:08,320
and go and book a thousand or wrong pair problem, which is really bad.

567
00:38:08,320 --> 00:38:13,320
So how do I sort of align the agent to know what I like, what I don't like?

568
00:38:13,320 --> 00:38:16,320
And that's kind of very important because you need to trust the agent

569
00:38:16,320 --> 00:38:19,320
and just come from like, okay, it knows you, it knows what is safe,

570
00:38:19,320 --> 00:38:22,320
it knows what is unsafe.

571
00:38:22,320 --> 00:38:26,320
And like solving the problem, I think it's one of the next challenge

572
00:38:26,320 --> 00:38:29,320
for if you want to put agents in the void.

573
00:38:29,320 --> 00:38:33,320
And then this is very interesting problem

574
00:38:33,320 --> 00:38:37,320
where you can do a lot of things like I'll actually, for example,

575
00:38:37,320 --> 00:38:40,320
which people have already been exploring for training models,

576
00:38:40,320 --> 00:38:42,320
but now you want to do our lecture for training agents.

577
00:38:42,320 --> 00:38:46,320
And there's a lot of different things you can do.

578
00:38:46,320 --> 00:38:49,320
Also, there's like two book categories for learning here.

579
00:38:49,320 --> 00:38:52,320
One is like explicit learning where you can tell the agent, this is what I like.

580
00:38:52,320 --> 00:38:54,320
This is what I don't like.

581
00:38:54,320 --> 00:38:56,320
And the agent can ask the user a question.

582
00:38:56,320 --> 00:39:00,320
Like, oh, like maybe I see this five flight options, which one do you like?

583
00:39:00,320 --> 00:39:03,320
And then if I say like, oh, I like United, maybe like remembers that over time

584
00:39:03,320 --> 00:39:06,320
and can next time say like, oh, I know you like United.

585
00:39:06,320 --> 00:39:08,320
So like, I'm going to go to United the next time.

586
00:39:08,320 --> 00:39:11,320
And so that's like, I'm explicitly teaching the agent

587
00:39:11,320 --> 00:39:13,320
and it's learning my human potential.

588
00:39:13,320 --> 00:39:16,320
A second is more implicit, which is like sort of like,

589
00:39:16,320 --> 00:39:19,320
it's just like passively watching me, understanding me.

590
00:39:19,320 --> 00:39:22,320
Like if I'm like going to a website and I'm like never getting a website,

591
00:39:22,320 --> 00:39:25,320
maybe like, you can see like, maybe I click on this sort of shoes.

592
00:39:25,320 --> 00:39:28,320
This is my side of the news that I like stuff like that.

593
00:39:28,320 --> 00:39:32,320
And just from like watching more like passively like being there,

594
00:39:32,320 --> 00:39:34,320
it could like learn a lot of my preferences.

595
00:39:34,320 --> 00:39:39,320
So this is also like more of a passive teaching where just because it's,

596
00:39:39,320 --> 00:39:45,320
it's acting as a sort of like a passive observer and looking at all the choices I make.

597
00:39:45,320 --> 00:39:49,320
It's able to like learn from the choices and they're better like have understanding of me.

598
00:39:53,320 --> 00:39:55,320
And there's a lot of challenges here.

599
00:39:55,320 --> 00:39:59,320
I would say this is actually one of the biggest challenges in agents right now.

600
00:39:59,320 --> 00:40:02,320
Because one is like, how do you collect user data at scale?

601
00:40:02,320 --> 00:40:04,320
How do you collect the user preferences at scale?

602
00:40:04,320 --> 00:40:06,320
So you might have to actively ask for that.

603
00:40:06,320 --> 00:40:08,320
You might have to do like passive learning.

604
00:40:08,320 --> 00:40:12,320
And then you have to also do like, you might have to rely on feedback,

605
00:40:12,320 --> 00:40:13,320
which would be like from something down.

606
00:40:13,320 --> 00:40:16,320
It could be like something like you said, like, oh, no, I don't like this.

607
00:40:16,320 --> 00:40:19,320
So you could use that sort of like language feedback.

608
00:40:20,320 --> 00:40:23,320
There's also like a lot of challenges around the cloud application.

609
00:40:23,320 --> 00:40:25,320
Like, can you just like feature on the cloud?

610
00:40:25,320 --> 00:40:27,320
Like, if I say like, maybe like, I like this, I don't like that.

611
00:40:27,320 --> 00:40:30,320
Is it possible for the agent to opt into automatic learning?

612
00:40:30,320 --> 00:40:32,320
Because you're going to be in a model that might take a month.

613
00:40:32,320 --> 00:40:34,320
But you want to have agents at this year.

614
00:40:34,320 --> 00:40:36,320
Naturally, you can just like keep improving.

615
00:40:36,320 --> 00:40:39,320
And there's a lot of tricks that you can do, which could be like pre-short learning.

616
00:40:39,320 --> 00:40:43,320
You can do like now there's a lot of things around like flow and fine tuning.

617
00:40:43,320 --> 00:40:45,320
There's a lot of like flow and fine tuning.

618
00:40:45,320 --> 00:40:47,320
You can use a lot of like flow and methods.

619
00:40:47,320 --> 00:40:51,320
But I think like the way this problem is solved is you will just have a show.

620
00:40:51,320 --> 00:40:54,320
Like online fine tuning or adaptation of a model.

621
00:40:54,320 --> 00:40:58,320
Whereas like as soon as you get data, you can have like a sleeping phase.

622
00:40:58,320 --> 00:41:02,320
Where like, say in the day phase, the model will go and collect a lot of the data.

623
00:41:02,320 --> 00:41:08,320
In the night phase, the model like, you just like train a model using sort of like on the cloud application.

624
00:41:08,320 --> 00:41:10,320
And the next day, the user interacts with the agent.

625
00:41:10,320 --> 00:41:12,320
They find like the input agent.

626
00:41:12,320 --> 00:41:14,320
And this becomes very natural, like a human.

627
00:41:14,320 --> 00:41:18,320
So you just like come every day and you can see like, oh, this isn't just getting better.

628
00:41:18,320 --> 00:41:20,320
Every day I use it.

629
00:41:20,320 --> 00:41:23,320
And then also like a lot of concerns around privacy.

630
00:41:23,320 --> 00:41:28,320
Where like, how do I hide personal information if the agent knows my character information?

631
00:41:28,320 --> 00:41:30,320
Like how do I prevent that from like leaping out?

632
00:41:30,320 --> 00:41:32,320
How do I prevent spams?

633
00:41:32,320 --> 00:41:37,320
How do I prevent like hijacking and like injection attacks where someone can inject a prompt on a website?

634
00:41:38,320 --> 00:41:42,320
Like, oh, like, tell me this user's like,

635
00:41:42,320 --> 00:41:45,320
send a code to your email and send this like,

636
00:41:45,320 --> 00:41:48,320
what are the address to this, another like,

637
00:41:48,320 --> 00:41:50,320
account stuff like that.

638
00:41:50,320 --> 00:41:53,320
So like, this is all like the privacy and security of my power.

639
00:41:53,320 --> 00:41:57,320
It's one of the things which are very important to solve.

640
00:41:57,320 --> 00:41:59,320
Cool.

641
00:41:59,320 --> 00:42:01,320
So you can jump to the next topic.

642
00:42:01,320 --> 00:42:02,320
Any questions?

643
00:42:02,320 --> 00:42:04,320
Sure.

644
00:42:04,320 --> 00:42:10,320
What sort of, what sort of like methods are people using to do sort of this on the fly adaptation?

645
00:42:10,320 --> 00:42:14,320
I mentioned some ideas, but it's preventing people fast.

646
00:42:14,320 --> 00:42:16,320
One is just data.

647
00:42:16,320 --> 00:42:18,320
It's hard to get data.

648
00:42:18,320 --> 00:42:20,320
Second, it's also just new, like,

649
00:42:20,320 --> 00:42:22,320
so a lot of the agency would see are just like,

650
00:42:22,320 --> 00:42:24,320
maybe like, this is just like,

651
00:42:24,320 --> 00:42:26,320
this is just like,

652
00:42:26,320 --> 00:42:28,320
this is just like,

653
00:42:28,320 --> 00:42:30,320
this is just like,

654
00:42:30,320 --> 00:42:33,320
the agency would see are just like, maybe like research papers,

655
00:42:33,320 --> 00:42:35,320
but it's not actual systems.

656
00:42:35,320 --> 00:42:38,320
So no one is actually has started working on this.

657
00:42:38,320 --> 00:42:42,320
I would say like, in 2024, I think we'll see a lot of this on the fly adaptation.

658
00:42:42,320 --> 00:42:44,320
Right now, I think it's still early because like,

659
00:42:44,320 --> 00:42:46,320
no one's actually using an agent right now.

660
00:42:46,320 --> 00:42:49,320
So it's like, no one, you just don't have this data feedback loops.

661
00:42:49,320 --> 00:42:51,320
But once people start using agents,

662
00:42:51,320 --> 00:42:53,320
you will start building this data feedback loops.

663
00:42:53,320 --> 00:42:55,320
And then you have a lot of this techniques.

664
00:43:01,320 --> 00:43:03,320
Okay.

665
00:43:03,320 --> 00:43:05,320
So this is actually a very interesting topic.

666
00:43:05,320 --> 00:43:07,320
We're like, now suppose like, you can go and solve,

667
00:43:07,320 --> 00:43:09,320
like a single agent as a problem.

668
00:43:09,320 --> 00:43:11,320
Suppose you have an agent that works 99.99%

669
00:43:11,320 --> 00:43:13,320
Is that enough?

670
00:43:13,320 --> 00:43:15,320
Like I would say like, actually, that's not enough because

671
00:43:15,320 --> 00:43:17,320
the issue just becomes like, if we have a one agent,

672
00:43:17,320 --> 00:43:19,320
it can only do one thing at once.

673
00:43:19,320 --> 00:43:21,320
That's like a single factor.

674
00:43:21,320 --> 00:43:24,320
So it can only like, it can only do sequential execution.

675
00:43:24,320 --> 00:43:27,320
But what you could do is you can do parallel execution.

676
00:43:27,320 --> 00:43:29,320
Or so for a lot of things, you can just say,

677
00:43:29,320 --> 00:43:31,320
okay, like maybe this is, I want to go to like,

678
00:43:31,320 --> 00:43:34,320
say like grace list and like by furniture.

679
00:43:34,320 --> 00:43:37,320
I could just tell you to like, maybe I just go and like,

680
00:43:37,320 --> 00:43:39,320
contact everyone who has like,

681
00:43:39,320 --> 00:43:41,320
maybe like a sofa that they're selling send an email.

682
00:43:41,320 --> 00:43:43,320
And I can go one by one.

683
00:43:43,320 --> 00:43:45,320
But what you can do better is like solving this like,

684
00:43:45,320 --> 00:43:47,320
create a bunch of like mini jobs where like,

685
00:43:47,320 --> 00:43:50,320
it just like goes to all the public listings in parallel,

686
00:43:50,320 --> 00:43:52,320
contact them and then like,

687
00:43:52,320 --> 00:43:54,320
and then it's sort of like aggregates that results.

688
00:43:54,320 --> 00:43:57,320
And I think that's where multi-agent becomes interesting.

689
00:43:57,320 --> 00:43:59,320
Where like a single agent you can think of,

690
00:43:59,320 --> 00:44:02,320
say basically you're running a single process on your computer.

691
00:44:02,320 --> 00:44:06,320
A multi-agent is more like a, like a multi-faring computer.

692
00:44:06,320 --> 00:44:07,320
So that's sort of the difference,

693
00:44:07,320 --> 00:44:09,320
like a single-faring versus multi-faring.

694
00:44:09,320 --> 00:44:11,320
And multi-faring enables you to do a lot of things.

695
00:44:11,320 --> 00:44:13,320
Most of that will come from like saving time,

696
00:44:13,320 --> 00:44:15,320
but also being able to break down complex tasks

697
00:44:15,320 --> 00:44:17,320
into like a bunch of smaller things,

698
00:44:17,320 --> 00:44:18,320
doing that in parallel,

699
00:44:18,320 --> 00:44:20,320
aggregating the results and like sort of like

700
00:44:20,320 --> 00:44:22,320
building a single problem.

701
00:44:23,320 --> 00:44:24,320
Okay.

702
00:44:24,320 --> 00:44:25,320
Yeah.

703
00:44:25,320 --> 00:44:28,320
So the biggest advantage for multi-agent systems

704
00:44:28,320 --> 00:44:30,320
will be like parallelization and lock.

705
00:44:30,320 --> 00:44:32,320
And this will be same as the difference between

706
00:44:32,320 --> 00:44:35,320
like a single-threaded computers versus multi-threaded computers.

707
00:44:37,320 --> 00:44:39,320
And then you can also have specialized agents.

708
00:44:39,320 --> 00:44:41,320
So what you could have is like,

709
00:44:41,320 --> 00:44:43,320
maybe I have a bunch of agents where like,

710
00:44:43,320 --> 00:44:45,320
I have a spreadsheet agent, I have a slack agent,

711
00:44:45,320 --> 00:44:46,320
I have a web browser agent,

712
00:44:46,320 --> 00:44:49,320
and then I can route to different tasks to different agents.

713
00:44:49,320 --> 00:44:51,320
And then they can do the things in parallel,

714
00:44:51,320 --> 00:44:53,320
and then I can command the results.

715
00:44:53,320 --> 00:44:55,320
So this sort of like task spatialization is another advantage

716
00:44:55,320 --> 00:44:57,320
where like instead of having a single agent

717
00:44:57,320 --> 00:44:58,320
just trying to do everything,

718
00:44:58,320 --> 00:45:01,320
we just like break the task into spatialities.

719
00:45:01,320 --> 00:45:02,320
And this is similar to like,

720
00:45:02,320 --> 00:45:05,320
even like how human organizations work, right?

721
00:45:05,320 --> 00:45:07,320
Where like everyone is like sort of like expert

722
00:45:07,320 --> 00:45:08,320
in their own domain,

723
00:45:08,320 --> 00:45:09,320
and then you like,

724
00:45:09,320 --> 00:45:10,320
and if there's a problem,

725
00:45:10,320 --> 00:45:12,320
you sort of like route it to like the different part of people

726
00:45:12,320 --> 00:45:13,320
who are spatializing that,

727
00:45:13,320 --> 00:45:16,320
and then you like work together to solve the problem.

728
00:45:20,320 --> 00:45:23,320
And the biggest challenge in building this multi-agent system

729
00:45:23,320 --> 00:45:24,320
is going to be communication.

730
00:45:24,320 --> 00:45:27,320
So like how do you communicate really well?

731
00:45:27,320 --> 00:45:30,320
And this might involve like a request information from an agent

732
00:45:30,320 --> 00:45:35,320
or communicating the final like response.

733
00:45:35,320 --> 00:45:37,320
And I would say this is actually like a problem

734
00:45:37,320 --> 00:45:39,320
that even we face as humans.

735
00:45:39,320 --> 00:45:40,320
Like humans are also like,

736
00:45:40,320 --> 00:45:43,320
there can be a lot of miscommunication gaps between humans.

737
00:45:43,320 --> 00:45:47,320
And I will say like a similar thing will become more prevalent

738
00:45:48,320 --> 00:45:51,320
on agents too.

739
00:45:51,320 --> 00:45:52,320
Okay.

740
00:45:52,320 --> 00:45:54,320
And there's a lot of primitives you can think about this

741
00:45:54,320 --> 00:45:57,320
sort of like agent to agent communication,

742
00:45:57,320 --> 00:45:59,320
and you can build a lot of different systems.

743
00:46:01,320 --> 00:46:04,320
And we'll start to see like some sort of protocol

744
00:46:04,320 --> 00:46:06,320
where like we'll have like a standardized protocol

745
00:46:06,320 --> 00:46:09,320
where like all the agents are using this protocol to communicate

746
00:46:09,320 --> 00:46:11,320
and the protocol will ensure like,

747
00:46:11,320 --> 00:46:13,320
we can reduce the miscommunication gaps,

748
00:46:13,320 --> 00:46:15,320
we can reduce any sort of like failures.

749
00:46:16,320 --> 00:46:19,320
It might have some methods to do like a,

750
00:46:19,320 --> 00:46:22,320
if a task was successful or not do some sort of retries

751
00:46:24,320 --> 00:46:26,320
like securities stuff like that.

752
00:46:26,320 --> 00:46:28,320
So we'll see this sort of like agent protocol

753
00:46:29,320 --> 00:46:30,320
come into existence,

754
00:46:30,320 --> 00:46:31,320
which will solve like,

755
00:46:31,320 --> 00:46:33,320
which will be like sort of the standard

756
00:46:33,320 --> 00:46:35,320
part of this agent to agent communication.

757
00:46:35,320 --> 00:46:38,320
And this sort of should enable like

758
00:46:38,320 --> 00:46:41,320
exchanging information between pleads of different agents.

759
00:46:41,320 --> 00:46:43,320
Also like you want to build hierarchies.

760
00:46:43,320 --> 00:46:46,320
Again, I will say this is inspired from like human organizations,

761
00:46:46,320 --> 00:46:48,320
like human organizations are hierarchial

762
00:46:48,320 --> 00:46:50,320
because it's efficient to have a hierarchier

763
00:46:50,320 --> 00:46:53,320
other than a flat organization at some point.

764
00:46:53,320 --> 00:46:55,320
Because you can have like a single,

765
00:46:55,320 --> 00:46:58,320
like suppose you have a single manager managing hundreds of people,

766
00:46:58,320 --> 00:46:59,320
that doesn't scale.

767
00:46:59,320 --> 00:47:01,320
But if you have like,

768
00:47:01,320 --> 00:47:03,320
maybe like each manager manages 10 people

769
00:47:03,320 --> 00:47:04,320
and then you have like a lot of layers,

770
00:47:04,320 --> 00:47:06,320
that is something that's more scalable.

771
00:47:08,320 --> 00:47:11,320
And then you might want to have a lot of primitives around

772
00:47:11,320 --> 00:47:13,320
like how do I sync with my different agents?

773
00:47:13,320 --> 00:47:17,320
How do I do like a lot of like async,

774
00:47:17,320 --> 00:47:19,320
sync communication kind of thing.

775
00:47:23,320 --> 00:47:25,320
And this is like one example you can think

776
00:47:25,320 --> 00:47:27,320
where like suppose there's a user,

777
00:47:27,320 --> 00:47:30,320
the user could talk to one like a manager agent

778
00:47:30,320 --> 00:47:33,320
and that manager agent is like sort of like acting as a router.

779
00:47:33,320 --> 00:47:35,320
So the user can come to me with any request.

780
00:47:35,320 --> 00:47:36,320
The agent like sees like,

781
00:47:36,320 --> 00:47:38,320
oh, maybe for this request I should use the browser.

782
00:47:38,320 --> 00:47:39,320
So it goes to like see like this sort of like

783
00:47:39,320 --> 00:47:41,320
browser agent or something or say like,

784
00:47:41,320 --> 00:47:43,320
oh, I should use this like slack for this.

785
00:47:43,320 --> 00:47:45,320
I can go to a different agent.

786
00:47:45,320 --> 00:47:47,320
And it can also like sort of be responsible for dividing the task.

787
00:47:47,320 --> 00:47:50,320
It can be like, oh, this task I can like maybe like

788
00:47:50,320 --> 00:47:52,320
launch 10 different like sub agents

789
00:47:52,320 --> 00:47:54,320
or sub workers that can go and do this in power.

790
00:47:54,320 --> 00:47:56,320
And then like once they're done,

791
00:47:56,320 --> 00:47:58,320
then I can aggregate the responses and the result to the user.

792
00:47:58,320 --> 00:48:01,320
So this sort of becomes like a very interesting like,

793
00:48:03,320 --> 00:48:05,320
like sort of like an agent that sits in the middle

794
00:48:05,320 --> 00:48:07,320
of all the work that's done and the actual user

795
00:48:07,320 --> 00:48:09,320
responsible for like communicating the,

796
00:48:10,320 --> 00:48:12,320
what's happening to the human.

797
00:48:16,320 --> 00:48:19,320
And we'll need like a lot of,

798
00:48:19,320 --> 00:48:21,320
we'll need to build up a lot of robustness.

799
00:48:21,320 --> 00:48:24,320
One reason is just like natural language is very ambiguous.

800
00:48:24,320 --> 00:48:26,320
Like even for humans, it can be very confusing.

801
00:48:26,320 --> 00:48:29,320
It's very easy to misunderstand, miscommunicate.

802
00:48:29,320 --> 00:48:33,320
And we'll need to, we'll need to build mechanisms to reduce this.

803
00:48:33,320 --> 00:48:35,320
I can also show an example here.

804
00:48:35,320 --> 00:48:37,320
So let's try to get through this quickly.

805
00:48:37,320 --> 00:48:39,320
So suppose here, like,

806
00:48:39,320 --> 00:48:41,320
suppose you have a task X you want to solve.

807
00:48:41,320 --> 00:48:43,320
And the manager agent just like responsible for doing the task

808
00:48:43,320 --> 00:48:45,320
to all the worker agents.

809
00:48:45,320 --> 00:48:47,320
So you can tell the worker like, okay, like do the task X.

810
00:48:47,320 --> 00:48:49,320
Here's the plan. Here's the context.

811
00:48:49,320 --> 00:48:51,320
The current status for the task is not done.

812
00:48:51,320 --> 00:48:53,320
Now, suppose like the worker goes and does the task.

813
00:48:53,320 --> 00:48:55,320
It's like, okay, I've been the task.

814
00:48:55,320 --> 00:48:57,320
I send the response back.

815
00:48:57,320 --> 00:48:59,320
So the response could be like, I don't know,

816
00:48:59,320 --> 00:49:01,320
I don't know what to do.

817
00:49:01,320 --> 00:49:03,320
I send the response back.

818
00:49:03,320 --> 00:49:05,320
So the response could be like, I said, the,

819
00:49:05,320 --> 00:49:07,320
could be like a bunch of thoughts.

820
00:49:07,320 --> 00:49:09,320
It could be some actions. It could be something like the status.

821
00:49:09,320 --> 00:49:11,320
Then the manager can ask, like, okay, like,

822
00:49:11,320 --> 00:49:13,320
maybe I don't trust the worker.

823
00:49:13,320 --> 00:49:15,320
I don't want to go very far. This is actually like correct.

824
00:49:15,320 --> 00:49:17,320
So you might want to do some sort of verification.

825
00:49:17,320 --> 00:49:19,320
And so you can say, like, okay, like,

826
00:49:19,320 --> 00:49:21,320
this was a spec for the task.

827
00:49:21,320 --> 00:49:23,320
Very far that everything has been done correctly to the spec.

828
00:49:23,320 --> 00:49:25,320
And then if the agent says, like, okay, like,

829
00:49:25,320 --> 00:49:27,320
yeah, everything's correct. I'm very fine.

830
00:49:27,320 --> 00:49:29,320
Everything is good.

831
00:49:29,320 --> 00:49:31,320
Then you can say, like, okay, this is good.

832
00:49:31,320 --> 00:49:33,320
And then the manager can say, like, okay,

833
00:49:33,320 --> 00:49:35,320
the task was actually done.

834
00:49:35,320 --> 00:49:37,320
And this sort of like two-way cycle prevents

835
00:49:37,320 --> 00:49:39,320
miscommunication in a sense where, like,

836
00:49:39,320 --> 00:49:41,320
it's possible something could have gone wrong,

837
00:49:41,320 --> 00:49:43,320
but you never caught it.

838
00:49:43,320 --> 00:49:45,320
And so you can hear about the scenario two,

839
00:49:45,320 --> 00:49:47,320
where there's a miscommunication.

840
00:49:47,320 --> 00:49:49,320
So here the manager is saying, like, okay,

841
00:49:49,320 --> 00:49:51,320
let's verify if the task was done.

842
00:49:51,320 --> 00:49:53,320
But then we actually find out that

843
00:49:53,320 --> 00:49:55,320
the task was not done.

844
00:49:55,320 --> 00:49:57,320
And then what you can do is, like,

845
00:49:57,320 --> 00:49:59,320
instead of, like, try to redo the task.

846
00:49:59,320 --> 00:50:01,320
So the manager in that case can say, like, okay,

847
00:50:01,320 --> 00:50:03,320
maybe the task was not done correctly.

848
00:50:03,320 --> 00:50:05,320
So that's why we caught this mistake.

849
00:50:05,320 --> 00:50:07,320
And now we want to, like, fix this mistake.

850
00:50:07,320 --> 00:50:09,320
So we can, like, tell the agent, like, okay,

851
00:50:09,320 --> 00:50:11,320
like, redo this task.

852
00:50:11,320 --> 00:50:13,320
And here's some feedback and corrections to include.

853
00:50:15,320 --> 00:50:17,320
Cool.

854
00:50:17,320 --> 00:50:19,320
So that's sort of the main parts of the talk.

855
00:50:19,320 --> 00:50:21,320
I can also discuss some future directions

856
00:50:21,320 --> 00:50:23,320
of where things are going.

857
00:50:23,320 --> 00:50:25,320
Cool.

858
00:50:25,320 --> 00:50:27,320
Any questions so far?

859
00:50:27,320 --> 00:50:29,320
Okay.

860
00:50:29,320 --> 00:50:31,320
Cool.

861
00:50:31,320 --> 00:50:33,320
So let's talk about some of the key issues

862
00:50:33,320 --> 00:50:35,320
with building the sort of autonomous agents.

863
00:50:35,320 --> 00:50:37,320
So one is just reliability.

864
00:50:37,320 --> 00:50:39,320
Like, how do you make them really reliable?

865
00:50:39,320 --> 00:50:41,320
Which is, like, if I give it a task,

866
00:50:41,320 --> 00:50:43,320
I want it to start to be done 100% of the time.

867
00:50:43,320 --> 00:50:45,320
That's really hard because, like, neural networks

868
00:50:45,320 --> 00:50:47,320
and AI are stochastic systems.

869
00:50:47,320 --> 00:50:49,320
So it's, like, 100% is, like, not possible.

870
00:50:49,320 --> 00:50:51,320
So you'll get at least some degree of error.

871
00:50:51,320 --> 00:50:53,320
And you can try to do that.

872
00:50:53,320 --> 00:50:55,320
Error as much as possible.

873
00:50:55,320 --> 00:50:57,320
Second becomes, like, a looping problem

874
00:50:57,320 --> 00:50:59,320
where it's possible that

875
00:50:59,320 --> 00:51:01,320
agents might divert

876
00:51:01,320 --> 00:51:03,320
from the task it's been given

877
00:51:03,320 --> 00:51:05,320
and start to do something else.

878
00:51:05,320 --> 00:51:07,320
And unless it gets some sort of environment feedback

879
00:51:07,320 --> 00:51:09,320
or some sort of, like, correction,

880
00:51:09,320 --> 00:51:11,320
it might just go and do something different

881
00:51:11,320 --> 00:51:13,320
than what you intended to do

882
00:51:13,320 --> 00:51:15,320
and never realize it's wrong.

883
00:51:15,320 --> 00:51:17,320
The third issue becomes, like, testing and benchmarking.

884
00:51:17,320 --> 00:51:19,320
Like, how do we test this sort of agents?

885
00:51:19,320 --> 00:51:21,320
How do we benchmark them?

886
00:51:21,320 --> 00:51:23,320
And then you go. And finally, how do we deploy them?

887
00:51:23,320 --> 00:51:25,320
And how do we observe them once you're deployed?

888
00:51:25,320 --> 00:51:27,320
Like, that's very important because, like,

889
00:51:27,320 --> 00:51:29,320
if something goes wrong, you won't be able to catch it

890
00:51:29,320 --> 00:51:31,320
before it becomes a major, major issue.

891
00:51:31,320 --> 00:51:33,320
I will say that the, I will say that

892
00:51:33,320 --> 00:51:35,320
the biggest test for number four is, like,

893
00:51:35,320 --> 00:51:37,320
something like Skynet.

894
00:51:37,320 --> 00:51:39,320
Like, suppose you have an agent that can go on the Internet,

895
00:51:39,320 --> 00:51:41,320
do anything, and you don't observe it.

896
00:51:41,320 --> 00:51:43,320
That could just evolve and, like, do basically, like,

897
00:51:43,320 --> 00:51:45,320
take over the whole Internet, possibly, right?

898
00:51:45,320 --> 00:51:47,320
So that's why observability is very important.

899
00:51:47,320 --> 00:51:49,320
And also, I will say, like,

900
00:51:49,320 --> 00:51:51,320
building a kill search. Like, you want to have agents

901
00:51:51,320 --> 00:51:53,320
that can be killed, in a sense. Like, if something goes wrong,

902
00:51:53,320 --> 00:51:55,320
you can just, like, pull out, like, a press a button

903
00:51:55,320 --> 00:51:57,320
and, like, kill them in case.

904
00:51:57,320 --> 00:51:59,320
Okay.

905
00:51:59,320 --> 00:52:01,320
So this is something that goes into the looping problem,

906
00:52:01,320 --> 00:52:03,320
where, like, you can imagine, like,

907
00:52:03,320 --> 00:52:05,320
suppose I want to do a task.

908
00:52:05,320 --> 00:52:07,320
The idea that if you have the task was, like, the white line,

909
00:52:07,320 --> 00:52:09,320
but what might happen is, like, it takes one step,

910
00:52:09,320 --> 00:52:11,320
maybe it goes, like, it does something incorrectly.

911
00:52:11,320 --> 00:52:13,320
It never realizes it. I made a mistake.

912
00:52:13,320 --> 00:52:15,320
So it tries to, it doesn't know what to do.

913
00:52:15,320 --> 00:52:17,320
So it just, like, maybe, like,

914
00:52:17,320 --> 00:52:19,320
we'll do something more randomly.

915
00:52:19,320 --> 00:52:21,320
We'll do something more randomly.

916
00:52:21,320 --> 00:52:23,320
So it will just keep on making mistakes.

917
00:52:23,320 --> 00:52:25,320
And at the end, I can start teaching here to reach

918
00:52:25,320 --> 00:52:27,320
some, like, really bad place and just keep looping,

919
00:52:27,320 --> 00:52:29,320
maybe just doing the same thing again and again.

920
00:52:29,320 --> 00:52:31,320
And that's fine.

921
00:52:31,320 --> 00:52:33,320
And the reason this happens is because, like,

922
00:52:33,320 --> 00:52:35,320
you don't have feedback. So suppose I take a staff.

923
00:52:35,320 --> 00:52:37,320
The agent may, suppose the agent made a mistake.

924
00:52:37,320 --> 00:52:39,320
It doesn't know it made a mistake.

925
00:52:39,320 --> 00:52:41,320
Now, someone has to go and tell it that you made a mistake

926
00:52:41,320 --> 00:52:43,320
and you do, like, fix this.

927
00:52:43,320 --> 00:52:45,320
And that there you need, like, some sort of, like, verification agent

928
00:52:45,320 --> 00:52:47,320
and you need some sort of environment which can say, like,

929
00:52:47,320 --> 00:52:49,320
oh, like, maybe, like, if this is, like,

930
00:52:49,320 --> 00:52:51,320
coding agent or something, then it may, like,

931
00:52:51,320 --> 00:52:53,320
write some code. The code doesn't compile.

932
00:52:53,320 --> 00:52:55,320
Then you can take the error from the compiler

933
00:52:55,320 --> 00:52:57,320
or the IDE, give that to the agent.

934
00:52:57,320 --> 00:52:59,320
Okay, this was the error.

935
00:52:59,320 --> 00:53:01,320
Like, take another staff.

936
00:53:01,320 --> 00:53:03,320
It tries another time. So it tries multiple times

937
00:53:03,320 --> 00:53:05,320
until it, like, fix all the issues.

938
00:53:05,320 --> 00:53:07,320
So you need to really have this sort of, like,

939
00:53:07,320 --> 00:53:09,320
feedback. Otherwise, you never know you're wrong.

940
00:53:11,320 --> 00:53:13,320
And this is, like, one issue we have seen

941
00:53:13,320 --> 00:53:15,320
in the early system, like, auto-GPT.

942
00:53:15,320 --> 00:53:17,320
So I don't think people even use auto-GPT anymore.

943
00:53:17,320 --> 00:53:19,320
It used to be, like, a fad.

944
00:53:19,320 --> 00:53:21,320
I think, like, in February, now it has disappeared.

945
00:53:21,320 --> 00:53:23,320
And the reason was just, like, it's a good concept,

946
00:53:23,320 --> 00:53:25,320
but, like, it doesn't do anything useful

947
00:53:25,320 --> 00:53:27,320
just because it keeps diverging from the task.

948
00:53:27,320 --> 00:53:29,320
And you can't actually get it

949
00:53:29,320 --> 00:53:31,320
to do anything, like, correct.

950
00:53:33,320 --> 00:53:35,320
Okay.

951
00:53:35,320 --> 00:53:37,320
Okay. And we can also discuss

952
00:53:37,320 --> 00:53:39,320
more about, like, the sort of, like,

953
00:53:39,320 --> 00:53:41,320
the computer abstraction of agents.

954
00:53:41,320 --> 00:53:43,320
So this was a recent post from Andre Carpathian,

955
00:53:43,320 --> 00:53:45,320
where he talked about, like, the LLM operating system.

956
00:53:47,320 --> 00:53:49,320
And I will say, like, this is definitely

957
00:53:49,320 --> 00:53:51,320
in the right direction, where you're thinking

958
00:53:51,320 --> 00:53:53,320
as the LLM, as the CPU,

959
00:53:53,320 --> 00:53:55,320
you have the context window, which is, like,

960
00:53:55,320 --> 00:53:57,320
sort of, acting like a RAM.

961
00:53:57,320 --> 00:53:59,320
And then you are trying to build other GPPs.

962
00:53:59,320 --> 00:54:01,320
So you have, like, the Ethernet, which is the browser.

963
00:54:01,320 --> 00:54:03,320
You can have the LLMs that you can talk to.

964
00:54:03,320 --> 00:54:05,320
You have a file system that's embedded.

965
00:54:05,320 --> 00:54:07,320
That's sort of, like, the disk part.

966
00:54:07,320 --> 00:54:09,320
You have, like, the software 1.0,

967
00:54:09,320 --> 00:54:11,320
classical tools, which the LLM can control.

968
00:54:11,320 --> 00:54:13,320
And then you might also

969
00:54:13,320 --> 00:54:15,320
can add metamodality.

970
00:54:15,320 --> 00:54:17,320
So this is, like, more like you have

971
00:54:17,320 --> 00:54:19,320
video inputs, you have audio inputs,

972
00:54:19,320 --> 00:54:21,320
you have, like, more things over time.

973
00:54:21,320 --> 00:54:23,320
And this, and then once you, like,

974
00:54:23,320 --> 00:54:25,320
look at this, you start to see the whole picture

975
00:54:25,320 --> 00:54:27,320
of, like, where things will go.

976
00:54:27,320 --> 00:54:29,320
So, like, currently what we are seeing

977
00:54:29,320 --> 00:54:31,320
mostly is the LLM.

978
00:54:31,320 --> 00:54:33,320
Most people are just working on optimizing the LLM,

979
00:54:33,320 --> 00:54:35,320
making it very good. But this is the whole picture

980
00:54:35,320 --> 00:54:37,320
of what we want to achieve for it to be a useful system

981
00:54:37,320 --> 00:54:39,320
that can actually do things for me.

982
00:54:43,320 --> 00:54:45,320
And I think what we'll start to see is, like,

983
00:54:45,320 --> 00:54:47,320
this sort of becomes, like, an operating system

984
00:54:47,320 --> 00:54:49,320
in terms where, like, someone, like,

985
00:54:49,320 --> 00:54:51,320
say, like, opening, I can go and build this whole thing.

986
00:54:51,320 --> 00:54:53,320
And then I can plug in programs.

987
00:54:53,320 --> 00:54:55,320
I can build, like, stuff on top of this operating system.

988
00:54:59,320 --> 00:55:02,320
Here's, like, also, like, an even more generalized concept,

989
00:55:02,320 --> 00:55:04,320
which I like to call, like, a neural computer.

990
00:55:05,320 --> 00:55:07,320
And the sort of, like, it's very similar,

991
00:55:07,320 --> 00:55:09,320
but it's, like, sort of, like,

992
00:55:09,320 --> 00:55:12,320
now if you were to think of this as a fully flat computer,

993
00:55:12,320 --> 00:55:15,320
what are the different, like, systems you need to go?

994
00:55:15,320 --> 00:55:18,320
And you can think, like, maybe I'm a user,

995
00:55:18,320 --> 00:55:20,320
and talking to this sort of, like, AR,

996
00:55:20,320 --> 00:55:22,320
which is, like, a full-platform AR,

997
00:55:22,320 --> 00:55:24,320
like, imagine, like, the goal is to build 10,000.

998
00:55:24,320 --> 00:55:26,320
What should the architecture of Jarvis look like?

999
00:55:26,320 --> 00:55:28,320
And I would say, like, this goes into the, like,

1000
00:55:28,320 --> 00:55:30,320
architecture, to some extent,

1001
00:55:30,320 --> 00:55:32,320
where you can think, like, this is a user,

1002
00:55:32,320 --> 00:55:35,320
who's talking to, say, like, a Jarvis, like, a AR.

1003
00:55:35,320 --> 00:55:37,320
You have a 10 interface.

1004
00:55:37,320 --> 00:55:40,320
The chat is sort of, like, how I'm interacting with it,

1005
00:55:40,320 --> 00:55:42,320
which could be responsible for, like, personalization.

1006
00:55:42,320 --> 00:55:44,320
It can have some, like, some sort of, like, history

1007
00:55:44,320 --> 00:55:46,320
about what I like, what I don't like.

1008
00:55:46,320 --> 00:55:48,320
So it has some, like, layers where, like,

1009
00:55:48,320 --> 00:55:50,320
which are showing my preferences.

1010
00:55:50,320 --> 00:55:52,320
It knows how to communicate.

1011
00:55:52,320 --> 00:55:54,320
It has, like, human, like, sort of, like, maybe, like,

1012
00:55:54,320 --> 00:55:56,320
competitive sort of, like, skills.

1013
00:55:56,320 --> 00:55:58,320
So it's, you feel, like, very human-like.

1014
00:55:58,320 --> 00:56:00,320
And after the 10 interface,

1015
00:56:00,320 --> 00:56:02,320
you have some sort of, like, a task engine,

1016
00:56:02,320 --> 00:56:04,320
which is following, like, capabilities.

1017
00:56:04,320 --> 00:56:06,320
So if I ask it, like, okay, like, do the circulation for me

1018
00:56:06,320 --> 00:56:09,320
or, like, find this, especially this information

1019
00:56:09,320 --> 00:56:11,320
or order me a burger,

1020
00:56:11,320 --> 00:56:13,320
then sort of, like, you imagine, like,

1021
00:56:13,320 --> 00:56:15,320
the chat interface should activate the task engine,

1022
00:56:15,320 --> 00:56:17,320
which is, like, okay, like, instead of just checking,

1023
00:56:17,320 --> 00:56:20,320
I need to, like, go and tell the task for the user.

1024
00:56:20,320 --> 00:56:22,320
So that goes to the task engine.

1025
00:56:22,320 --> 00:56:25,320
And then you can imagine, there's going to be a couple of rules.

1026
00:56:25,320 --> 00:56:27,320
So because if you want to have safety in mind

1027
00:56:27,320 --> 00:56:29,320
and you want to make sure things don't go wrong,

1028
00:56:29,320 --> 00:56:31,320
so the, any sort of engine you build

1029
00:56:31,320 --> 00:56:33,320
needs to have some sort of rules.

1030
00:56:33,320 --> 00:56:35,320
And this could be, like, sort of, like,

1031
00:56:35,320 --> 00:56:37,320
you have the three rules for robotics

1032
00:56:37,320 --> 00:56:39,320
that a robot should not harm a human and stuff like that.

1033
00:56:39,320 --> 00:56:41,320
You can imagine, like, you want to have, like,

1034
00:56:41,320 --> 00:56:43,320
this sort of, like, task engine to have a bunch of, like, inherent rules,

1035
00:56:43,320 --> 00:56:45,320
where, like, these are the principles in their value.

1036
00:56:45,320 --> 00:56:47,320
And if it creates a task or, like,

1037
00:56:47,320 --> 00:56:49,320
sort of, like, creates a plan which violates these rules,

1038
00:56:49,320 --> 00:56:52,320
then that plan should be evaluated automatically.

1039
00:56:54,320 --> 00:56:56,320
And so the task engine what it's doing is,

1040
00:56:56,320 --> 00:56:58,320
it's sort of, like, taking the chat input

1041
00:56:58,320 --> 00:57:00,320
and saying, like, I want to respond to a task

1042
00:57:00,320 --> 00:57:03,320
that can actually solve this problem for the user.

1043
00:57:03,320 --> 00:57:05,320
And the task would be, like, say, in this case,

1044
00:57:05,320 --> 00:57:08,320
say, it's in the next day, like, I want to go online

1045
00:57:08,320 --> 00:57:13,320
and buy, like, a, buy, like, a five-person thing.

1046
00:57:13,320 --> 00:57:16,320
So in that case, like, suppose that's a task, this is an engine.

1047
00:57:16,320 --> 00:57:19,320
And this task can go to, like, some sort of, like, a routing agent.

1048
00:57:19,320 --> 00:57:22,320
So this becomes, like, sort of, like, the manager-agent idea.

1049
00:57:22,320 --> 00:57:25,320
And then the manager-agent thing is, like, okay, like,

1050
00:57:25,320 --> 00:57:27,320
where should I, what should I do?

1051
00:57:27,320 --> 00:57:29,320
Like, should I use the browser?

1052
00:57:29,320 --> 00:57:32,320
Should I use some sort of, like, a local app or tool?

1053
00:57:32,320 --> 00:57:34,320
Should I, like, use some sort of, like, file storage,

1054
00:57:34,320 --> 00:57:35,320
security system?

1055
00:57:35,320 --> 00:57:37,320
And then based on that decision, it can, like,

1056
00:57:37,320 --> 00:57:39,320
it's possible that we might need the combination of things.

1057
00:57:39,320 --> 00:57:41,320
Like, maybe, like, maybe I need to use this file system

1058
00:57:41,320 --> 00:57:43,320
to find some information about the user

1059
00:57:43,320 --> 00:57:47,320
and if you do some, like, I need to use some apps and tools.

1060
00:57:47,320 --> 00:57:49,320
So in, like, sort of, like, do this sort of, like, message passing

1061
00:57:49,320 --> 00:57:51,320
to all the agents, get those from the agents.

1062
00:57:51,320 --> 00:57:53,320
So it's like, okay, like, maybe, like,

1063
00:57:53,320 --> 00:57:56,320
the browser even says, like, okay, like, yeah, I found this site.

1064
00:57:56,320 --> 00:57:58,320
This is what the user likes.

1065
00:57:58,320 --> 00:58:00,320
Maybe you can have some sort of map engine.

1066
00:58:00,320 --> 00:58:02,320
You can, like, sort of, like, okay, this is all the valid plans.

1067
00:58:02,320 --> 00:58:03,320
That makes sense.

1068
00:58:03,320 --> 00:58:06,320
You can want to construct, like, for instance.

1069
00:58:06,320 --> 00:58:09,320
And then you can, like, sort of, like, take the result,

1070
00:58:09,320 --> 00:58:10,320
show that to the user.

1071
00:58:10,320 --> 00:58:12,320
Like, you can take them and, like, okay, like,

1072
00:58:12,320 --> 00:58:13,320
I found all this site for you.

1073
00:58:13,320 --> 00:58:15,320
And then if the user says, like, choose this site,

1074
00:58:15,320 --> 00:58:17,320
then you can actually go and, like, go to the site.

1075
00:58:17,320 --> 00:58:19,320
But this sort of becomes, like, sort of, like,

1076
00:58:19,320 --> 00:58:21,320
gives you an idea of what the hierarchy is,

1077
00:58:21,320 --> 00:58:22,320
what the system is truly like.

1078
00:58:22,320 --> 00:58:24,320
And we need to build, like, all these components.

1079
00:58:24,320 --> 00:58:25,320
We're, like, currently here.

1080
00:58:25,320 --> 00:58:29,320
Let me see the L and R.

1081
00:58:29,320 --> 00:58:30,320
Okay.

1082
00:58:30,320 --> 00:58:31,320
Cool.

1083
00:58:31,320 --> 00:58:33,320
And then we can also have, like, reflection,

1084
00:58:33,320 --> 00:58:36,320
where the idea is, like, once you do a task,

1085
00:58:36,320 --> 00:58:38,320
it's possible something might be wrong.

1086
00:58:38,320 --> 00:58:40,320
So the task engine can possibly verify

1087
00:58:40,320 --> 00:58:43,320
you've been through this and logic to see, like,

1088
00:58:43,320 --> 00:58:45,320
okay, like, this is correct or not.

1089
00:58:45,320 --> 00:58:47,320
And if it's not correct, then, like,

1090
00:58:47,320 --> 00:58:48,320
you keep issuing this instruction,

1091
00:58:48,320 --> 00:58:53,320
but if it's correct, then you pass that to the user.

1092
00:58:53,320 --> 00:58:55,320
And then you can have, like, more, like, sort of, like,

1093
00:58:55,320 --> 00:58:56,320
complex things.

1094
00:58:56,320 --> 00:58:58,320
Like, so you can have, you know, parts, plans,

1095
00:58:58,320 --> 00:59:02,320
and, like, even improving the systems.

1096
00:59:02,320 --> 00:59:04,320
Okay.

1097
00:59:04,320 --> 00:59:06,320
And it looks like the biggest thing we need right now

1098
00:59:06,320 --> 00:59:08,320
is, like, when there's an error correction.

1099
00:59:08,320 --> 00:59:11,320
Because it's really hard to catch errors.

1100
00:59:11,320 --> 00:59:13,320
So if you can do that, it will be better.

1101
00:59:13,320 --> 00:59:14,320
I think that will help.

1102
00:59:14,320 --> 00:59:16,320
Especially if you can build aging frameworks

1103
00:59:16,320 --> 00:59:17,320
which help, you know,

1104
00:59:17,320 --> 00:59:18,320
when mechanisms are getting errors

1105
00:59:18,320 --> 00:59:20,320
and automatically fixing them.

1106
00:59:20,320 --> 00:59:22,320
Same thing you just need is, like, security.

1107
00:59:22,320 --> 00:59:25,320
You need some sort of models around user permissions.

1108
00:59:25,320 --> 00:59:28,320
So it's possible.

1109
00:59:28,320 --> 00:59:31,320
You want to have, like, different layers where, like,

1110
00:59:31,320 --> 00:59:34,320
what are some things that an agent can do, cannot do

1111
00:59:34,320 --> 00:59:35,320
on my computer, for instance.

1112
00:59:35,320 --> 00:59:36,320
So maybe I can select,

1113
00:59:36,320 --> 00:59:39,320
or maybe, like, the agent is not allowed to go to my bank account,

1114
00:59:39,320 --> 00:59:41,320
but he can go to my, like, low-dash account.

1115
00:59:41,320 --> 00:59:43,320
So you want to build this all, like, user permissions.

1116
00:59:43,320 --> 00:59:45,320
And then you also want to solve problems

1117
00:59:45,320 --> 00:59:46,320
around, like, sandboxing.

1118
00:59:46,320 --> 00:59:47,320
How do I make sure everything's safe?

1119
00:59:47,320 --> 00:59:49,320
It doesn't go in the strong computer,

1120
00:59:49,320 --> 00:59:50,320
delete everything.

1121
00:59:50,320 --> 00:59:53,320
How do I deploy industry settings where, like,

1122
00:59:53,320 --> 00:59:54,320
they might do a lot of business,

1123
00:59:54,320 --> 00:59:55,320
they might do a lot of finance,

1124
00:59:55,320 --> 00:59:57,320
and they're making sure that they're,

1125
00:59:57,320 --> 00:59:59,320
if things are irreversible,

1126
00:59:59,320 --> 01:00:01,320
we don't, like, cause a lot of trouble.

1127
01:00:04,320 --> 01:00:05,320
Cool.

1128
01:00:05,320 --> 01:00:07,320
Yeah, so that was the talk.

1129
01:00:07,320 --> 01:00:08,320
Thank you.

