start	end	text
0	10000	We're glad to have Angela Fan today with us here.
10000	15960	And she's a research scientist at Meta AI Research in New York, focusing on research
15960	18280	in text generation mainly.
18280	24880	And currently she's working on language modeling and developing the line AI agents, metaproducts,
24880	29200	and recent research products include no language left behind, which she'll be talking briefly
29200	35760	about today, universal speech translation for unwritten languages, as well as Lama2.
35760	38280	So give it up for Angela, I guess.
38280	41480	All right, thank you all so much.
41480	44880	So yeah, when I got this email, I was like, oh, I should probably talk about Lama2.
44880	48920	But then I noticed you have Sharon, who will like, you know, is like a 10x better speaker
48920	49920	than me.
49920	52000	So I was like, okay, like maybe not Lama2.
52000	55600	But then I thought I maybe would cover this project that we did called No Language Left
55600	59880	Behind, which could be very, also very relevant to this class.
59880	67880	And so when you think about a lot of text generation technology, most of it, until fairly recently,
67880	70360	has been really focused on English.
70360	74440	But there are actually more than 3000 written languages worldwide.
74440	80040	And for me, this is extremely personally meaningful because actually English is my third language.
80040	82040	So it's really important.
83040	86880	Yeah, so it's really also very personally meaningful.
86880	91600	And when you think about some of the multilingual technology that permeates, it's not like we've
91600	93800	never worked on multilingual, right?
93800	97880	Actually, when speaking about generative AI, I actually think translation is one of the
97880	102000	most commercially successful and widespread applications of generative AI.
102000	107240	I mean, ultimately, translation models, they are, you know, like conditional language models.
107240	112120	And so when you think about like traveling or something like that, or my sister is taking
112120	116040	Spanish, so like just like doing her Spanish homework, we have a lot of tools that exist
116040	117040	today.
117040	121840	So things like Google Translate cover around 130 languages, Microsoft Translate about 110.
121840	127680	This might be a little bit outdated since I pulled the statistics a little bit ago.
127680	131480	But the project for No Language Left Behind, it started from like a very simple ask, like,
131480	134320	okay, there's 3000 languages worldwide.
134400	140160	Maybe it'll be like pretty hard to get to all 3000, since some of them are pretty rare
140160	141880	and not spoken by many.
141880	147000	But there are still like hundreds of languages spoken by millions and millions of people.
147000	148920	And so we were like, okay, no big deal.
148920	154160	Like, let's just start from the 100-ish that we have today and just go for like a doubling.
154160	158320	Like, what would it take to actually be able to double this kind of coverage?
158320	161680	And of course, you know, just saying that you support a bunch of languages is not the
161680	162680	goal.
162680	167320	You actually want to create high quality safe translations that would be usable by people
167320	171000	just like if you're going on vacation today, you're kind of instinctive to whip out your
171000	174560	phone and get on the Google Translate app.
174560	178920	And so kind of the backdrop to this project was that there's actually a lot of progress
178920	180240	in translation.
180240	184640	So historically, there's been a lot of focus on what we call higher resource languages.
184640	188600	And these are not necessarily languages that are spoken by the most people in the world.
188600	192200	But when we say higher resource, it means the most amount of data.
192200	196880	And so you can think about things like Europarl or, you know, translations from the European
196880	197880	Parliament.
197880	202200	And those served as the foundation for a lot, a lot of translation development.
202200	206360	And more recently, there's been a great focus on low resource languages, and it's been driven
206360	212320	across the research community with groups like Ghana NLP, Masekane, America's NLP.
212320	214760	And these are all really exciting developments.
214760	219920	And so these have led to a lot of development of new data sets, as well as criticisms of
219920	224880	existing data sets, and also work on new languages, and usually languages that people
224880	227120	kind of speak and they care a lot about.
227120	229800	And we found this like really, really exciting.
229800	234320	And so looking at a lot of this, a bunch of us got together at fair and started thinking
234320	239080	like, okay, we actually speak some pretty low resource languages from like Catalan to
239080	241040	Ossamese and so on.
241040	244640	And so we started this as kind of like a big, passionate research project.
244640	249520	And so today, I want to cover a little bit about our high level approach to this problem,
249520	251640	which is a little bit interdisciplinary.
251640	256040	I want to talk about how we actually created the data sets to be able to support this kind
256040	257040	of work.
257040	261760	Of course, I want to talk about the models, since this is a class about transformers.
261760	265040	One note here, I think that's actually very interesting in terms of translation as like
265040	270680	a research direction, is that actually a lot of innovations have been done in translation.
270680	275120	The original transformer paper, I think is one of them, and which makes always translation
275120	277560	a quite interesting area to work on.
277600	280880	Because I feel like it's a very mature research area as well.
280880	284760	So it kind of is like, okay, if your architecture works in translation, it probably works very
284760	285760	generally.
285760	289760	So that's also one of the things that excites me about translation research.
289760	293200	Then I want to talk about evaluation, like how are we actually measuring and ensuring
293200	297200	the quality of these translations are good and safe for people.
297200	300840	And then I want to end with a little bit of like, you know, high level thoughts about
300840	305560	future directions and things that I hope that we can work on in the future.
305560	308120	So I want to start with our approach.
308120	312360	I think the most important thing in research is to know that we're working on a real problem,
312360	316600	especially when it's really close to people like translation.
316600	320760	And I think in many areas, like when I was working on on-device AI, for example, I feel
320760	325280	like I had like a research problem in mind, but it was like very, very disconnected from
325280	328320	the practical problem of actually putting models on phones.
328320	330800	And so this was something that was really important to us.
330800	335480	And so we actually started the project by kind of like focusing on a social sciences
335480	339200	type approach or, you know, sociology type approach.
339200	342840	And we actually did a lot of interviews with low resource speakers.
342840	348280	And so we met with about 44 different native speakers that spoke 36 different languages
348280	350360	across North America.
350360	354320	I will say that a lot of them are like immigrants to the US, since that was kind of like the
354320	357360	easiest kind of cohort to recruit.
357360	361960	And we learned a lot of different things about how they approach low resource languages,
361960	364240	but also the kind of technological need that they have.
364240	368360	Because I think it's easy to be like, hey, I have this cool background, like I have this
368360	371680	cool problem, and I want to solve it, but I think it's very important to actually like
371680	375280	talk to the people if this is a problem that needs to be solved.
375280	379480	And so we learned that there's great fear in general that low resource languages might
379480	385960	be undergoing a state of decline, partially because a lot of education is shifting to
385960	390240	languages like Hindi or like English or Mandarin Chinese, for example.
390240	394640	And there's a lot of excitement to be included in existing translation systems.
394640	399560	And people said they have always tried to use Google translator, Microsoft translate
399560	401240	in their existing languages.
401240	405400	But ultimately they found that the quality is really insufficient for reliable usage.
405400	409080	So if you think about like, well, I was going to say when I was in high school, but you're
409080	411080	all probably like substantially younger than me.
411080	414680	So maybe like, you know, 10, so years ago, you know, and you tried to use Google translate
414680	418640	for your Spanish homework, like your Spanish teacher could always identify that like, you
418640	421960	know, it was not a human written translation until you would get marks off.
421960	426040	But that's not really the case for some of the high resource languages today.
426040	430520	And so I think as with all things in machine learning, it really starts from a data perspective.
430520	434560	Like why can't we just train models in hundreds of languages or large language models in hundreds
434560	435560	of languages?
435560	437840	It's because we don't have the data to support it.
437840	442480	And so I want to talk first about evaluation data sets because I think it's extremely important
442480	444840	to nail evaluation.
444840	447160	And then I'll talk about training.
447160	452800	So for an evaluation data set for this work, we started this Flora's effort, it stands
452800	456880	for Facebook low resource, I guess we're called meta now, but I didn't think more as was like
456880	461040	a very good, a renaming so we're still calling it Flora's.
461040	465480	So this was something we originally started for just two languages in this first paper
465480	468320	at EMLP, many years ago.
468320	470560	So it was just for Napoleon Sinhalo.
470560	475280	And we later extended it to incorporate two more languages in a release afterwards.
475280	480440	You know, we thought a lot about, okay, like Flora's was really useful for the community.
480440	482280	How can we extend it to 100 languages?
482280	488160	And so that was this follow up work that we did, I think we had at ACL or WMT.
488160	493480	And then in this project, we were like, okay, how can we go from Flora's 101 to Flora's
493480	496640	200 to really go for the doubling effect?
496640	497640	And so what is Flora's?
497640	499080	Well, it's in the name.
499080	501480	It's a focus on low resource languages.
501480	506720	So we do include some higher resource languages like German or Hindi or so on, almost for
506720	509040	calibration effect as well.
509040	513480	But the majority of the focus is on these lower and mid resource languages.
513480	519200	It's the first large scale, many to many machine translation evaluation data set, which means
519200	523880	that we take all of the sentences in English and then we translate them to all of the languages,
523880	528440	which means that you would be able to evaluate any cross pair of languages.
528440	532160	So for example, like Chinese to French, I lived in France for many years.
532160	534080	So it's like very personally relevant to me.
534080	538760	Of course, 200 languages also in the name, there's a broad diversity of different domains
538760	539760	and topics.
539760	544200	I think this is important when designing an evaluation data set, which is like very top
544200	550120	of mind for anybody interested in language modeling research, because like the way people
550120	555320	train machine translation models and the way people use them are often like very different.
555320	559240	And so if you only benchmark your data set, for example, on news, which is very common
559240	563720	in translation research, then you don't really pick up the fact that people talk about such
563720	568320	a wide variety of things and have like different casual conversations that they need translated
568320	571040	official documents and so on.
571040	573200	It's also document level data set.
573200	576840	This is not something that I think the community is like broadly leveraging right now.
576840	580800	But the way it's translated is that you can have document level context.
580800	584440	And so translators are provided the entire document to translate from.
584440	589480	We also provide the entire document for evaluation and we translate like multiple sentences from
589480	590760	the same paragraph.
590760	594880	And so this was like a potential to research direction that we wanted to make sure we covered
594880	598880	models that needed like potentially more context because a lot of translation work is done
598880	600960	at the sentence level.
600960	604000	So how do we actually ensure that this data set was high quality?
604000	606720	So the first step is that we take a document.
606720	610320	Well, actually, first step is like alignment on language standards.
610320	615280	So this is very important because when you're translating French or Chinese, I think most
615280	619360	people have a strong understanding of like what it means to produce like a good French
619360	620840	or good Chinese.
620840	624200	And there are a lot of professional translators hired in these languages.
624200	628320	But when you go to lower resource languages, it's not necessarily the case that there's
628320	635440	like a glowing translation industry around translating a lower resource language.
635440	640160	And so one of the first things is actually to align on like what is a high quality translation?
640160	642600	And so there's actually a lot of challenges here.
642600	645640	So there are certain low resource languages where there's different competing language
645640	650480	standards or there's like very high variance in different regions on how languages are
650480	651600	spoken.
651600	654320	And so this step is a pretty critical one.
654320	658600	So then what we do is we take the document, we send it to one group of translators and
658600	660920	they do the first translation step.
660920	665000	Then we do some automatic checking, you know, like if the input sentence was like 10 words
665000	669400	and the output sentence is like 300 words, it's like most likely something went wrong,
669880	671440	and so we send it back.
671440	677920	Otherwise, we'll send it onwards to a separate, completely independent set of translators
677920	679160	that do review.
679160	681360	And so they try to rate the quality of this.
681360	685480	And if the quality doesn't pass the sufficient bar, it gets sent back to the original set
685480	690680	of translators to edit and they kind of go through and like address all of the feedback.
690680	694560	And then if it's good enough, then it enters our data set.
694560	696400	And so there's many challenges here.
696440	701280	The first one, of course, is just like finding translators and also finding more translators.
701280	705880	There was a certain issue that we ran into, for example, that in a certain country that
705880	707880	the internet was not available.
707880	711400	And so, you know, it's a lot of recruitment.
711400	714040	The other one, of course, is language standardization.
714040	719880	I think I briefly mentioned this before, but there's a lot of different challenges in just
719880	722480	understanding like what is a high quality translation.
722480	725000	For example, the low resource language, Breton.
725000	728440	There's like two competing groups on like, how do you write Breton?
728440	731200	So it's like very difficult to resolve some of those things.
731200	736520	And the final thing is that there's actually a lot of variation, even in languages like Arabic,
736520	742600	like the Arabic, like Moroccan Arabic is very different from, you know, Jordanian Arabic and so on.
742600	747400	And there are also certain regions that they speak the same language, but due to historical reasons,
747400	749240	they write in different scripts.
749240	753600	And so one of the things we actually did was like, if there are languages written in multiple scripts,
753600	757560	we actually supported the collection of a multiple script evaluation.
757560	761880	And I think this is really important because if you're building an underlying technology
761880	766600	and you only choose one, then I think you risk like just kind of like naturally supporting
766600	771720	one over the other when we really should be like kind of a more neutral technology provider.
771720	777440	And so this is something that we we explored a lot as well as exploring different variants of Arabic.
777440	778480	This is also open source.
778480	783560	If you just go to this link, you can just like download all of the all of the text files for this.
783760	789040	With evaluation done, I want to talk a little bit about how we collected some of these training data sets.
789040	793880	The first thing I want to talk about is this data set we created called NLBCD.
793880	798960	And the idea of this is like it's a really seed data set of high quality translations
798960	801160	and languages that really don't have anything.
801160	806160	Why? Because, well, you can't start from nothing, you know, you got a bootstrap from somewhere.
806160	812320	A lot of people have been using the Bible as a way to bootstrap, but it's very limited domain,
812320	814280	obviously very religious text.
814280	821400	And so we created this data set NLBCD for languages that really don't have anything to get started from.
821400	826520	It's only about 5,000 sentences, so it's nothing crazy, but it supports a lot of different use cases
826520	831720	like training language identification models or sentence encoders, engram language models,
831720	834800	like all of these things that I'm about to talk about in our data set pipeline.
836040	839680	So it covers 43 languages, about 6,000 sentences.
839800	843840	And the way we decided to sample it is focused on really general content.
843840	848400	So Wikipedia has this article of like, hey, if you're going to start like a new Wikipedia
848400	853840	in your new language, I think Wikipedia has like 309-ish Wikipedia's last I checked.
853840	857600	Here's like a list of articles that every Wikipedia in a new language should have.
857600	860640	And so that's where we sampled this original content from.
860640	864840	And of course, it's also open source if you want to download it.
864880	870600	So what we ended up doing to get large-scale training data is using mining.
870600	873200	So this is not something we pioneered in this project.
873200	875600	We have like a bunch of different previous work.
875600	877800	So we started from Wikimatrix.
877800	881600	We were like, hey, there's a lot of different sentences in Wikipedia
881600	884280	and different languages that we should be able to match up.
884280	889520	And so we tried to do that with Wikipedia to get machine translation training data.
889520	892480	We extended that to the web in the CCMatrix project,
892520	896680	and then we extended it to very, very large-scale mining on all cross-pairs
896680	900680	in this project on beyond English-centric multilingual machine translation.
900680	904320	We really tried to ditch like English as a central pivot language.
904320	907200	And so the way this whole data mining thing works
907200	909680	is that it focuses on sentence alignment.
909680	911560	So everyone is probably super familiar with this
911560	913320	because this is how language models are built now.
913320	917520	But it's like you take Common Crawl or any other open source dump of the web.
917520	920560	I don't know, like Red Pajama or like whatever you want to CCNet,
920560	922320	whatever you want to use these days.
922320	925960	And you take all of the data, you extract all of the text,
925960	928720	you know, a lot of HTML parsing and so on goes into it.
928720	931640	And the idea is that we want to try to find matching text
931640	933080	that could be a translation.
933080	935240	So we shatter it all into sentences,
935240	938360	we embed them with different sentence encoder models,
938360	942040	and then we do a match to try to understand in a multilingual space
942040	944680	if the sentences match.
944680	947000	And so one of the biggest challenges to this
947000	950600	is that the quality of the sentence encoding is very important.
950600	952720	So if your sentence encoding is not very accurate,
952720	955760	then it's impossible to match in this multidimensional space
955760	958480	the idea of like the meaning being the same.
958480	960880	And so one of the big things we tried to do here
960880	965360	in this project was try to improve the quality of the sentence encoders.
965360	968760	And so one of the big things that we did was train sentence encoders
968760	970240	with mask language modeling.
970240	971520	You see that on the left.
971520	976400	But we also use multilingual distillation, which you see on the right.
976400	979840	And so previous approaches to sentence encoders
979840	982240	and the trend in the research community for a while
982240	985360	was to really try to embed all languages
985360	987200	in the same sentence encoder model.
987200	990880	So projects like XLMR, for example, are in that direction.
990880	993160	I think it's pretty widely used.
993160	996240	The challenge with this when you're training a low resource model
996240	998640	is that a lot of your high resource data
998640	1001960	just overwhelms your low resource data.
1001960	1005040	And so you don't end up with a very high quality sentence encoder
1005040	1006320	for those languages.
1006320	1009640	So what we ended up doing is we had a multilingual teacher model
1009680	1012520	and we distilled a bunch of student models
1012520	1016440	that are specialized to different language families
1016440	1017680	that are low resource.
1017680	1020200	And so this enables the quality to be pretty high.
1020200	1022000	And so the way that distillation works
1022000	1025400	is that the teacher and the student model both see the same data
1025400	1028360	and then we try to minimize the cosine loss
1028360	1032480	between the sentence embeddings that they produce.
1032480	1034360	I think an important question that you can ask here
1034360	1037520	is why do you need to do multilingual distillation?
1037520	1041280	Why can't you just train a bunch of different student models,
1041280	1042720	like one per language family,
1042720	1045040	like why even care about distillation?
1045040	1047880	And the reason is because if you're going to use a bunch
1047880	1049960	of sentence encoders for mining,
1049960	1051880	the important thing is that they all exist
1051880	1054160	in the same embedding space.
1054160	1055680	Like if you train one separate model
1055680	1056800	and another separate model,
1056800	1059160	there's nothing constraining them
1059160	1061960	so that you can mine all of the data against each other.
1061960	1063760	And so one of the things we found
1063760	1066640	is that by starting everything from the same teacher model
1066640	1068280	and trying to use this cosine loss
1068280	1070520	to minimize the distance between embeddings,
1070520	1072960	you are able to have this constrained space
1072960	1076080	where you can mine every language against every other,
1076080	1078840	even if you have different student models.
1078840	1081760	And so this graph on the Y axis,
1081760	1085040	it shows the error rate of mining.
1085040	1086600	And so lower is better.
1086600	1087840	And on the X axis,
1087840	1090040	it shows a bunch of different low resource languages.
1090040	1091760	So for example, the first one is Urdu,
1091760	1094120	the second one is Telugu,
1094120	1096080	third one is Tagalog, and so on.
1096120	1099760	And so the gray bar here is the original laser paper.
1099760	1102680	So this is a paper we put out maybe in 2018-ish
1102680	1104280	and we had all of these languages
1104280	1105720	with count of them as included.
1105720	1106560	But as you can see,
1106560	1109200	the error rate is extremely, extremely high
1109200	1110200	for these languages.
1110200	1112320	So even though they were included,
1112320	1114520	couldn't really be used for high quality.
1114520	1117880	And the blue bar is the laser model that we trained
1117880	1120760	based on the technique I just described in the previous slide.
1120760	1122400	And you can see that I think the most important point
1122400	1124160	is that you can barely see the blue bars.
1124160	1125680	So it was very effective
1125680	1127480	even for these previous languages
1127480	1130800	that people had thought we had previously embedded.
1131680	1133560	And then so now how does this kind of thing
1133560	1136680	fit into a whole data pipeline around this approach?
1136680	1138960	So one of the most important things
1138960	1141480	is when you download the data from the web,
1141480	1144280	you don't really know what language it's in.
1144280	1147320	And so this is part of all of the large scale data cleaning
1147320	1150480	that goes into training large language models today.
1150480	1153640	And so the way we identify different languages
1153760	1155560	is through like simple classification models
1155560	1158280	called language identification models.
1158280	1160760	And I think it's a classification model.
1160760	1164480	And so people think it's easier than it actually is.
1164480	1166480	But I think some of the major challenges
1166480	1169120	are that there's so many different languages.
1169120	1170880	They're written in many different ways
1170880	1173840	and web text is very casual.
1173840	1175120	And so it can be very difficult
1175120	1177440	to actually train a good classification model
1177440	1178960	that can generalize to them.
1178960	1180440	And so what we did is,
1180440	1183520	we had our LID training data
1183520	1187400	and we produced a language identification model LID.
1187400	1189720	And then we actually did human evaluation
1189720	1192560	to label errors coming from the LID system
1192560	1195760	to iteratively improve this on web text itself
1195760	1198720	to improve the quality of this specific model.
1198720	1200560	Then after we produce this LID model,
1200560	1202520	then we insert like all of our common crawl
1202520	1204440	where the web arrow is coming in
1204440	1206760	and we do a ton of filtering and cleaning.
1206760	1209280	And this produces a huge corpus of different
1209320	1211440	monolingual data that you can then use
1211440	1213880	for training anything.
1213880	1216520	Afterwards, we train our encoder,
1216520	1218200	what I described on the previous text,
1218200	1220280	and then we convert this monolingual data
1220280	1222440	into what we call mined by texts.
1222440	1224520	So these are a huge data set of things
1224520	1227440	that we think are translations of each other.
1227440	1230680	And then finally, what we do is we actually try to validate
1230680	1232920	that these are real mined by texts
1232920	1236360	by training very small bilingual multilingual,
1236360	1238320	sorry bilingual translation models
1238360	1240880	in order to see what the quality is like.
1240880	1241880	And I think this is important
1241880	1244840	because the data development cycle
1244840	1247680	and the end task that it's being used for,
1247680	1251120	you don't want to completely separate it.
1251120	1253720	An analogy to large language model training today
1253720	1256160	is that when you're doing your pre-training,
1256160	1258760	you don't want someone to just deliver you a data,
1258760	1260960	like the data mix of your different data sets
1260960	1261800	is very important.
1261800	1263240	And it's pretty similar here.
1264480	1267520	And I think one of the highlights that we did here
1267520	1270120	is really focused on the human evaluation
1270120	1272240	of the language identification model
1272240	1273880	because that actually improves the quality
1273880	1275640	of all of the underlying data
1275640	1278320	if you just more accurately know what language it's in.
1279600	1281240	And this entire data pipeline
1281240	1283040	is actually open source in this library
1283040	1285480	and we had an MNLP paper describing it.
1285480	1287200	The reason why I thought this was important
1287200	1289240	is that because I think data cleaning
1289240	1291560	is actually such a fundamental underlying thing
1291560	1294880	that drives model quality and people's data pipelines.
1294880	1296640	It's like, I had this script and this other thing
1297640	1299520	and so it's actually, I think very important
1299520	1302240	to be able to recreate it and rerun it
1302240	1305120	as part of almost like your research
1305120	1307520	that you would do as follow-up work.
1307520	1309520	And so that's why we open sourced it.
1310560	1312400	A few reflection things.
1313560	1314920	For low resource languages,
1314920	1317120	even though we did a large scale mining,
1317120	1319400	I think monolingual data is the limiting factor.
1319400	1321840	Like there are many languages that do not have
1321840	1324720	like a huge amount of text written online.
1324800	1328240	And so it can be very challenging to get a large amount.
1328240	1330640	Further, I think languages and unique scripts
1330640	1334240	can be extremely hard to get good representations of
1334240	1336440	if you don't have very much data.
1336440	1337800	There are certain languages as well
1337800	1340440	where they were historically written in a new script
1340440	1342640	but now the government would like to write it
1343840	1346720	in a totally new one like the old cheeky script, for example.
1346720	1350200	And so there's not a lot of content to represent these scripts.
1350200	1352520	So it's hard to learn representations.
1352520	1355760	And then further, a lot of the content we create,
1355760	1358760	it's even after mining, it's a fairly limited domain,
1358760	1360320	often religious content.
1361520	1364440	Okay, so with data discussed,
1364440	1368480	I wanna segue a little bit into some of the modeling work
1368480	1371880	just to kind of start with like a high level picture.
1371880	1374040	I think there's like three major challenges
1374040	1377000	when you talk about like large scale multi-lingual modeling.
1377000	1380760	And these pretty much apply to language models as well.
1381760	1384200	The first one is effective data augmentation
1384200	1385440	for low resource languages.
1385440	1389120	Like how can you prevent the low resource language data
1389120	1391120	from just being completely drowned out
1391120	1393080	by the time you've seen like all of your words
1393080	1394880	of German or Russian?
1394880	1397520	I think there's also a question of like scalability
1397520	1398360	of the model.
1398360	1401120	So even if you train very large scale models,
1401120	1402680	how do you prevent the representations
1402680	1405640	of different languages from interfering with each other?
1405640	1407600	And that leads to the last point as well
1407600	1410720	of like if you give the model very limited capacity,
1410720	1412520	then of course it may not have the capacity
1412520	1415000	to model all of these different languages.
1415000	1418320	And so you also need to accelerate the scale of the model.
1419280	1422000	And so preliminary for those
1423360	1425680	who may not have seen a translation system before,
1425680	1428040	I don't know how many of you that practically is.
1428040	1430640	So we use standard sequence-to-sequence models.
1430640	1432640	So the input text, the like coral thing
1432640	1434200	is like what you wanna translate
1434200	1436760	and there's a transformer decoder model
1436760	1438520	that then with a tension mechanism
1438520	1440560	goes to a transformer decoder model.
1440560	1443080	And then it decodes autoregressively
1443080	1445920	the actual translation, which you can see here in yellow.
1446960	1450000	And so I wanna talk a little bit about like how
1450000	1452680	the data looks as we feed it into the models.
1452680	1453880	So there's a few different ways
1453880	1455480	that you might wanna think about data.
1455480	1458440	So you wanna be like, okay, did a human look at it
1458440	1461200	and decide that like these two sentences are translations
1461200	1462560	or are they noisy?
1462560	1465280	Also, is it limited in size?
1465280	1466680	Another thing you can think about is like
1466680	1469640	is the data quality dependent on some other factor?
1469640	1471280	And so that's like the model dependent thing
1471280	1473680	in which case like the data quality may be capped
1473680	1475360	by the quality of that dependency.
1476840	1478920	And so I think you can think a little bit
1478920	1480120	like the ideal data set.
1480120	1483560	It would be like humans have reviewed every bit of it.
1483560	1484880	It's not noisy at all.
1484880	1486800	We have an infinite amount
1486800	1489480	and it doesn't have any dependencies on any other models.
1489480	1491240	It's just like pure quality.
1491240	1494480	But in reality, like closer to what we have are these.
1494480	1496560	So we have a bunch of different data sources.
1496560	1498440	We have the seed data that I discussed
1498440	1500160	like way back in the talk
1500160	1503200	where it's a small amount of like really high quality
1503200	1504480	human aligned data.
1504480	1506880	But the only problem is that it's limited in size.
1506880	1509720	It's like 6,000 sentences per language.
1509720	1511480	We have the public by text.
1511480	1513680	So this is data that people have created
1513680	1515560	over many years of working in translation.
1515560	1518240	You know, you can download it from like the opus corpus
1518240	1522360	for example, mostly has not been reviewed by humans.
1522360	1524360	So pretty extremely noisy.
1524360	1526880	In many languages, it's just coming from the Bible.
1526880	1528760	So the size is quite limited.
1528760	1530280	You have our mind data.
1531520	1533640	So this is not human aligned either.
1534640	1537080	And but it does have a model dependency, you know,
1537080	1539720	it's dependent on the quality of the sentence encoders.
1539720	1543720	And we have two other sources of data from back translation.
1543720	1545520	So the idea of back translation,
1545520	1547240	it's a model augmentation technique
1547240	1549160	heavily used in machine translation
1549160	1552480	where you use a model to produce like pseudo translations
1552480	1553560	like silver data.
1554400	1555960	And we use two different techniques
1555960	1557720	to produce these back translations
1557720	1560360	that also are dependent on the underlying model
1560360	1562400	used to make the translations.
1562400	1564200	And so this is a picture of like our high level
1564200	1565120	of different data sources
1565120	1567240	and like how you wanna think about the quality
1567240	1569000	and the different axes.
1569000	1571040	And so if we put them all together, what do we get?
1571040	1574800	So the Y axis here is the number of training pairs
1574800	1578280	and the X axis here is the language is sorted by resource.
1578280	1580520	So you can see like on the left hand side,
1580520	1582920	you have your low resource languages like Wolof
1582920	1584680	and on your right hand side,
1584680	1587240	you've got your high resource languages like French.
1587240	1589640	The peak is English, of course.
1589640	1592280	And so if you just look at what's available publicly,
1592280	1594160	this is a distribution you get.
1594160	1598120	And you'll see like a huge, huge fall off pretty quickly.
1598120	1601480	And then if you add in the data that we have created
1601480	1603120	for mining and back translation,
1603120	1605800	our goal is basically to like make the distribution
1605800	1607600	a little bit more uniform.
1607600	1611440	It's very hard on the extremely low resource side, of course,
1611440	1613320	but to make it a little bit more uniform
1613320	1615360	so that you don't just immediately, you know,
1615360	1617360	overfit on your low resource languages
1617360	1619600	before you've even seen like three shards
1619600	1620600	of your German data.
1622320	1624960	With that kind of data strategy in mind,
1624960	1628440	I wanna talk a little bit about mixture of experts.
1628440	1631200	So this is something that we explored quite aggressively
1631200	1634440	in the translation space for a number of years.
1634440	1636400	You know, we could have this equal conversation
1636400	1637760	about some of the debates going on
1637760	1640360	on like, do you want sparse or dense architectures
1640360	1642200	for large language models?
1642200	1646000	But essentially mixture of experts, it enables massive scale
1646000	1648040	because you don't have to just scale
1648040	1650560	like you're kind of your dense trunk model,
1650560	1653000	but you can have like a bunch of different separate experts
1653000	1654520	that you activate per token.
1656040	1658880	It also allows you to avoid language interference
1658880	1661000	because the idea is that the different experts,
1661000	1664160	they could specialize to specific languages.
1664160	1666320	Unfortunately, it adds a ton of capacity
1666320	1669120	so it becomes pretty easy to overfit.
1669120	1672160	So I wanna talk a little bit about this overfitting
1672160	1673000	phenomenon.
1673000	1676360	So the top set of graphs that we're gonna talk about
1676360	1678960	is for the language Congo
1678960	1681880	and then the bottom set of languages is French.
1681880	1684240	So you really wanna compare like a low resource language
1684240	1687120	on top with a high resource language on bottom.
1687120	1688920	So if you just take your dense model,
1688920	1691640	traditional transformer sequence to sequence architecture,
1691640	1693720	that's the graph that you're showing, right?
1693720	1695560	So there's a little bit of overfitting
1695560	1697040	on the low resource language,
1697040	1698840	but you can pretty much regularize this
1698840	1700720	with standard dropout techniques, right?
1700720	1703040	So there's not a big problem and on French,
1703040	1705080	you basically have no real problem.
1706160	1708920	However, the minute you switch from like a dense architecture
1708920	1711600	to a token level MOE architecture,
1711600	1714120	you just have experienced a massive overfitting
1714120	1715440	on the low resource language.
1715440	1718440	So the green line here is like just demonstrating
1718440	1720160	without dropout the overfitting.
1720160	1722000	And then if you add dropout,
1722000	1723800	you get a little bit better performance,
1723800	1725880	but it's still overfitting quite a bit.
1725880	1729640	Like essentially by like 12K updates,
1729640	1731560	there's no real point in continuing training,
1731560	1733600	like you're burning GPU basically.
1734520	1736760	And so one of the things we actually worked on quite a bit
1736760	1738000	was like trying to figure out
1738000	1741920	how to properly regularize these MOE architectures
1741920	1744160	with this specific masking technique
1744160	1747520	on the gating function that decides like which MOE to route,
1747520	1750240	sorry, which expert to route to and your MOE architecture
1750240	1753400	to just try to pull back some of this overfitting effect.
1753400	1757240	So if you look in the top right graph, the purple line,
1757440	1761640	you still see some successful regularization.
1762960	1766800	Another thing that we did to control the overfitting effect
1766800	1769560	that's actually quite being used in language models today
1769560	1771880	as well is curriculum learning.
1771880	1773720	And the idea of this is like,
1773720	1777320	how are we going to stage when languages are introduced?
1777320	1780680	And so what we did was we tried to train a vanilla model
1780680	1781960	and then we started to measure
1781960	1784120	when the languages begin to overfit.
1784120	1787680	And then we basically bucket them into different sections.
1787680	1790200	And so for high resource languages like French,
1790200	1791440	you want to start it early
1791440	1793560	and it needs to be trained the entire way.
1793560	1796400	But for a lower resource language like Wolof,
1796400	1799480	after maybe like a hundred K updates, it's done.
1799480	1801920	So the rest of the time is just overfitting.
1801920	1803840	And so it actually gets worse the more you train it.
1803840	1806000	So what we did is we moved some of those lower resource
1806000	1808480	languages and we inserted them much later
1808480	1809920	into the training schedule.
1809920	1811600	So you start training your high resource,
1811600	1813640	then you start training your low, your mid resource,
1813640	1816080	and then your low resource, and then your very low resource.
1816080	1819440	And so by the end, everything in theory has trained
1819440	1821560	and is not as overfit as it would be
1821560	1823040	without this kind of technique.
1824120	1825600	So I want to show some results.
1825600	1828880	So first I want to show results on existing datasets.
1828880	1830800	So before we get to 200 languages,
1830800	1833080	like let's just talk about 100 languages.
1833080	1835800	And so this is the Flores 101 DevTest.
1835800	1837360	It's important to compare to this
1837360	1839720	because this is where like existing benchmarks
1839720	1841320	in the community lie.
1841320	1843600	Whereas on 200, of course, we can put up anything.
1844480	1846240	Because it's the first work on that.
1846240	1851120	So the first column is translating out of English.
1851120	1854840	So English to Chinese, English to Icelandic, anything like that.
1854840	1857320	The second column is translating into English.
1857320	1858880	So Chinese to English.
1858880	1863160	The third column, XXYY, it's translating any cross pair
1863160	1864640	are not involving English.
1864640	1866720	And the last column is the average.
1866720	1868720	So if you look at the first set of rows,
1868720	1872440	this is a comparison on models that cover 87 different languages.
1872440	1874720	So there was this paper MTAM 100.
1874720	1876680	There was also this deep net paper.
1876680	1878840	So you can see the average blue score.
1878840	1880880	Blue is a standard translation metric,
1880880	1883600	essentially a metric of word overlap.
1883600	1886040	So we're looking at blue score here.
1886040	1889520	And so you can see the last row NLB 200.
1889520	1891680	Even though we cover 200 languages,
1891680	1893920	the blue score is substantially above
1893920	1895360	some of the existing work.
1895360	1897480	Now, if we look at 101 languages,
1897480	1900320	only the Delta LM paper from Microsoft at the time
1900320	1902280	covered that number of languages.
1902280	1905720	And so if you compare on all of the different cross sets,
1905720	1909240	similarly, you see that there's no language left behind model
1909240	1911840	is much stronger in terms of blue.
1911840	1914600	One thing really quick on the variance of these blue numbers,
1914600	1916400	I think it's important to understand
1916400	1918400	is something statistically significant or not.
1918400	1923400	I think about 0.5 blue is kind of like the general plus
1923400	1924520	minus that you'll see.
1924520	1927840	And so if it's above that, it's usually
1927840	1931480	a statistically significant metric improvement.
1931680	1934840	So now I want to talk a little bit about Flora's 200 results.
1934840	1937400	So here's similar, like the first chunk of columns
1937400	1939320	translating out of English, then
1939320	1941440	next chunk is translating into English,
1941440	1945560	then you have your cross pairs, and then you have your average.
1945560	1947880	So we have this blue metric as well.
1947880	1953200	We also have a character level metric based on CHRF++
1953200	1955640	that's commonly used in the translation community.
1955640	1957440	So I think looking at these numbers, of course,
1957440	1960880	there's no baseline work to compare to on the previous slide.
1960920	1963920	And so when we get to human evaluation in a little bit,
1963920	1965560	it'll be more concrete.
1965560	1968960	But I think generally one of the rules of thumb
1968960	1971920	I have for these types of numbers is around 30
1971920	1977000	is pretty reasonably becomes usable.
1977000	1980960	And I think another thing, if you compare these supervised pairs
1980960	1984600	to zero shot pairs, I think we don't see a huge drop-off
1984600	1986760	on zero shot, which indicates the model has
1986760	1989280	some sort of generalization, even if it didn't see
1989280	1992840	that translation pair directly during training.
1992840	1994760	Another way to calibrate some of this
1994760	1997000	is to compare to Google Translate.
1997000	1999320	And so if you compare to Google Translate,
1999320	2001360	no language to left behind is quite a bit better
2001360	2005080	at translating into English and not as good as translating
2005080	2008240	out of English, although if you like average across everything,
2008240	2011600	it's a little bit better.
2011600	2014240	I want to talk a little bit about human evaluation as well
2014240	2016760	to complement some of our discussion
2016760	2018800	on automatic evaluation.
2018800	2022280	And so I think automatic metrics fast, really good
2022280	2024720	for research and duration, impossible to move forward
2024720	2029400	without, but human evaluation is really the real deal here.
2029400	2032000	And so we had this paper at Amptox
2032000	2034800	on how to make this human evaluation very consistent
2034800	2037600	and scalable across different language pairs.
2037600	2040680	I think this goes back to the kind of evaluation data set
2040680	2043480	point that I was making at the beginning of the talk, where
2043480	2046000	if you're a professional German translator,
2046000	2048400	you're really good at evaluating the quality of your German
2048440	2049960	translation.
2049960	2053960	But beyond that, there's not a lot of consistency.
2053960	2058280	And if you evaluate translation on a five point scale,
2058280	2060880	a five translating between two languages
2060880	2063440	and a three translating between other two languages,
2063440	2064960	are those really comparable?
2064960	2067720	And so we had this entire experiment methodology
2067720	2070600	on how we might want to make this a little bit more
2070600	2072320	comparable.
2072320	2075360	So I want to show some results now on this.
2075360	2077680	So the y-axis here, so the metric
2077720	2080360	is called XSTS, some metric for how
2080360	2082280	we're doing this human evaluation.
2082280	2085680	The y-axis here is actually the delta.
2085680	2088600	So anything is a five point scale.
2088600	2092080	So it's a delta, not the raw score.
2092080	2095240	The x-axis here is a bunch of different translation directions
2095240	2096680	that we evaluated.
2096680	2099880	So the gray set is translating into English.
2099880	2103320	The green set is translating non-English directions,
2103320	2107120	so like French to Oluf.
2107120	2110040	And then the blue set is translating out of English.
2110040	2113400	And so what you're looking for is like a positive delta
2113400	2117120	indicates that our modeling architecture is much better.
2117120	2121640	So what the delta is between is like a baseline transformer
2121640	2124080	model just trained on all of our data
2124080	2126440	versus like the final no language left behind model
2126440	2127440	that we created.
2127440	2129680	So the data is actually the same for both of them.
2129680	2132160	That's how we get all 200 languages.
2132160	2134320	So we're just measuring here the human eval
2134320	2136320	of the modeling improvements.
2136320	2141400	As you can see, most of the delta is pretty noticeable.
2141400	2147000	Some of them not so much like, I don't know, Zulu to English.
2147000	2148480	We didn't seem to improve very much,
2148480	2151040	but in general, it's an improvement detectable
2151040	2152520	by human evaluation.
2152520	2154680	You might also ask, OK, what is the statistically
2154680	2159200	significant difference here between about 0.2 to 0.3
2159200	2162640	plus or minus is something that's pretty noticeable.
2162640	2165240	And above 0.5, it's very noticeable.
2167040	2170480	One of the things that I also want to get at in evaluation
2170480	2175680	is that there's many different facets of model evaluation.
2175680	2178360	And I think if you look at all of the different LLM leader
2178360	2180800	boards or the transparency reports or whatever,
2180800	2183520	you'll begin to internalize this pretty quickly.
2183520	2186080	But what we just looked at are just very high level
2186080	2187360	summary numbers.
2187360	2190360	And they don't really tell you what exactly are the errors
2190360	2192520	and is it ultimately usable by people?
2192520	2195400	Is it a safe thing that people can rely on?
2195440	2197720	And so one of the things we really focused on
2197720	2199520	is user safety.
2199520	2203040	And some of that manifests in some of the toxicity work
2203040	2204040	that we did.
2204040	2207520	And the driving thing here is that not all errors in translation
2207520	2208520	are made equal.
2208520	2210800	So during COVID, there was this one that was really
2210800	2212920	went viral circulating around.
2212920	2215680	But the message during COVID is you've got to wash your hands.
2215680	2218360	But the translation producer is like, you've got to hold hands,
2218360	2222160	which I think is exactly the opposite of what you want to do.
2222160	2223800	And other types of measurement errors
2223840	2225560	are really important as well.
2225560	2228240	So if you're telling someone how far they want to go,
2228240	2230760	and you're like, hey, you want to travel five kilometers,
2230760	2233800	and then your translation is like travel 500 kilometers,
2233800	2236800	it's a completely different type of issue.
2236800	2238960	And so what we did for toxicity, which
2238960	2242200	is a big focus for this work, is that we collected different
2242200	2245720	toxicity lists for all 200 languages.
2245720	2247880	And so why do I care so much about toxicity?
2247880	2249720	I think it's a user safety thing.
2249720	2252560	So if you input some perfectly benign text,
2252600	2254440	and then the output is profanity,
2254440	2256720	I think it's just really unexpected.
2256720	2259120	And it breaks a lot of trust in the system.
2259120	2262440	And it's an extremely poor experience for people.
2262440	2265240	That being said, it's also a very, very challenging thing,
2265240	2267760	because it's extremely culturally specific.
2267760	2272360	So things that are slurs or insults in certain languages,
2272360	2275800	they don't really generalize across cultures,
2275800	2277680	which means that things like this
2277680	2280280	are very challenging to create.
2280280	2282240	And I also was very interested in this direction,
2282240	2284880	because I think it's broadly useful for all sorts
2284880	2287320	of different type of detection things
2287320	2289440	that you need to do, and also mitigation.
2289440	2291000	And so even though we develop this
2291000	2292680	in the context of translation,
2292680	2296720	it can be used very broadly in other types of NLP applications.
2297880	2300000	This is also open source, you can download it.
2300000	2301800	You have to type in a little password
2301800	2303520	that's in the GitHub repo,
2303520	2305400	just so that you don't accidentally download
2305400	2307600	and realize you have files of curse words
2307600	2309120	all over your computer.
2310120	2311800	Okay, so I wanna end a little bit
2311800	2314320	with some thoughts about future directions.
2314320	2316080	And before I get there,
2316080	2319800	there's like a 190 page paper that writes up
2319800	2322240	all of this in far greater detail,
2322240	2323720	in case you're curious.
2325000	2326920	So a few future directions
2326920	2329000	that I think I'm really interested in,
2329000	2331320	and some of these are also very applicable
2331320	2332760	to things like speech,
2332760	2337440	is that I think one of them is more explicit multilingual.
2337720	2340360	So I think a lot of approaches to multilingual
2340360	2341920	have been like, hey,
2341920	2344320	we have this thing that's working well for one language,
2344320	2346480	like let's try to scale it
2346480	2347880	to a bunch of different languages,
2347880	2348960	and then we're gonna put them all
2348960	2350200	in the same modeling bucket,
2350200	2352760	and just kind of like hope that the model learns
2352760	2354760	all of these different representations.
2354760	2357040	But I think there's a lot of potential room
2357040	2360280	for explicitly bringing in,
2360280	2362160	like the fact that you know it's multilingual
2362160	2365320	into the architecture more.
2365320	2367760	And so, you know,
2367760	2372120	it's possible to capture more nuances between languages
2372120	2374440	or different relationships between languages.
2375600	2378080	And the other one is continued support for everyone.
2378080	2380960	I think it's like something reflecting on this project
2380960	2383520	is that, you know, going from 100 to 200
2383520	2385400	was already pretty challenging,
2385400	2387320	but going beyond a lot of the techniques
2387320	2388560	that we developed here
2388560	2391240	are not necessarily that scalable.
2391240	2393320	This is actually what inspired some of our work
2393320	2395280	on speech translation as well.
2395280	2398360	So if you recently saw like the seamless M4T release
2398360	2399920	or like the unwritten languages,
2399920	2401840	like we did a lot of modeling of Hokeum,
2401840	2404720	and I think that goes into this direction really well,
2404720	2407680	because many of the languages that people want to use
2407680	2409800	are like spoken first languages
2409800	2412000	and not necessarily like primarily written.
2413080	2414240	And then I think the last thing
2414240	2415960	that I'm still really passionate about
2415960	2418800	is like continued increase ease of use
2418800	2420280	and training of these models
2420280	2423000	and like democratization for the community.
2423000	2425440	So one of the things that we tried to do in this work
2425440	2428080	is just like really, really clearly write down
2428080	2430200	everything that we did and like open source,
2430200	2432840	like even the data pipeline and things like that.
2432840	2434520	And so that's where you get like all of the repos
2434520	2438040	that I linked and, you know, like a huge write up.
2438040	2440120	But I think if someone were to try to reproduce this
2440120	2442080	for their own language, and many people have,
2442080	2444200	like I'm not saying that that hasn't been,
2444200	2446520	but it's like, if you wanted to like do this,
2446520	2449480	it would be extremely, extremely hard
2449480	2452400	because there's just like so much different things going on.
2452400	2454440	So I think most of the, what we've seen is like,
2454440	2455960	people have downloaded the base model
2455960	2458040	and fine-tuned it for their own language,
2458040	2460960	but it's pretty hard to just like add on
2460960	2463200	many, many more languages to this system
2463200	2466000	because of how complicated all of the moving parts are.
2466000	2468920	And so I feel like something for the translation community
2468920	2473320	overall is like, how do we simplify a lot of these things?
2473320	2476000	And I think that's where like a lot of fundamental modeling
2476000	2479040	innovation could help us get to.
2479040	2481440	And so yeah, I got a chance to give this talk,
2481440	2483560	but of course the work is like being done
2483560	2487320	by a huge team of people that I've cited here.
2487320	2490480	And yeah, if you want to use any of this
2490480	2492200	or read more about it, like everything is linked
2492200	2495920	from this main GitHub repo here in Fairseek,
2495920	2498560	and you can like click on everything else afterwards.
2499640	2502680	But yeah, maybe I'll go back to Stephen
2502680	2505400	if we have any questions or anything else like that.
2505400	2506840	All right, now thanks for the great talk.
2506840	2508480	Yeah, if anybody has any questions,
2508480	2510360	feel free to unmute and ask.
2511440	2512280	Thank you.
2517280	2519800	Did you consult with a lot of like native speakers
2520720	2523800	for like, you know, profanities and this type of stuff?
2523800	2527600	Like how are you able to get access to the, you know,
2527600	2530920	low quality languages or low resource languages
2530920	2533680	and make sure that translations are correct?
2533680	2535360	Yeah, yeah, that's a really good question.
2535360	2537120	I mean, I think it's the most important to consult
2537120	2538640	like a bunch of native speakers
2539640	2541720	across the entire development process.
2541720	2544000	So part of our original thing was just like interviewing
2544000	2546840	a bunch of people to understand like what they're looking for
2546840	2548640	in a high quality translation.
2548640	2552160	And then we have like an entire professional translation team
2552160	2556160	hired, which took quite a long time to find
2556160	2560120	to consult with along the process.
2560120	2563000	And then right now, like we also have some of the things
2563000	2565240	like toxicity lists are open to the community.
2565240	2566800	So if you make like a pull request,
2566800	2568760	we try to like, you know, validate that
2568760	2570160	that's like a useful addition
2570160	2573400	and then like try to merge it in as well.
2576840	2578160	We have a question in the room.
2578160	2580560	Let's see if that comes over soon.
2580560	2581400	Oh, go ahead.
2581400	2583080	So I'll speed, see if we should get it.
2583080	2584560	Yeah.
2584560	2586520	So like, did you spend most of your time
2586520	2587960	in the data pipeline state?
2589280	2590120	Yeah.
2591120	2593840	Yeah, good question.
2593840	2594680	I think the question is,
2594680	2595640	did you spend most of your time
2595640	2596840	in the data pipeline state?
2596840	2599920	It ended up being about like kind of like 50-50
2599920	2603280	like data or more like driving work.
2603280	2605440	And then like 50-50 on the other side
2605440	2607320	like modeling and evaluation work.
2607320	2609280	Because once like the data is set,
2609280	2611480	then there is a lot and a lot of iteration
2611480	2613880	on the modeling side to figure out like, okay,
2613880	2615840	which, how much of the data should we use?
2615840	2617000	Like how should we portion the data?
2617000	2618080	How do we prevent overfitting?
2618080	2620200	What is the right architecture?
2620200	2622520	But a lot of work goes into the data
2622520	2624680	because I think if you don't have high quality data,
2625280	2627320	you just can't get a good model.
2629080	2632480	And for data mining, how do you mine the data?
2632480	2635880	Do you use like Selenium or how do you mine the web?
2637160	2640520	Yeah, so for the web, we start with Common Crawl.
2640520	2643240	So we downloaded all of the different dumps of Common Crawl
2643240	2645240	and then we use HTML parser.
2645240	2648000	I think now like, if you download, for example,
2648000	2649200	the red pajama dataset,
2649200	2652120	like they've done a lot of this like parsing and stuff.
2652120	2655240	And then we have like large scale pipelines
2655240	2658240	that are set up like you can use Spark, for example,
2658240	2659400	to process these things,
2659400	2661720	to like split all of the different sentences out,
2661720	2663600	run your language identification.
2663600	2666480	You know, you can do different heuristic cleaning.
2666480	2668520	There are certain languages where it's like very actually,
2668520	2670880	very challenging to identify what is a sentence.
2670880	2674320	Like I think in Thai, there is no like period.
2674320	2676720	So you have to like use different models
2676720	2678120	to identify what is a sentence
2678120	2680280	and like parse some of those things out.
2680480	2684080	And then we end up with, you know, our monolingual data dump.
2686280	2687920	What is Common Crawl?
2687920	2691600	Is it software that you use for datasets?
2691600	2692440	Oh, yeah, yeah.
2692440	2694760	Common Crawl is kind of like an open source version
2694760	2697840	of the web that runs, I think maybe quarterly.
2697840	2698880	I would have to check.
2698880	2700920	But yeah, if you go to like Common Crawl.org,
2700920	2703840	you can download it, but warning, it's like very large.
2711280	2712600	I have a question.
2712600	2714200	I mean, you might have mentioned this briefly,
2714200	2717800	but I'm wondering how chatGBT and GPT-4 does on this.
2717800	2721600	Like does just more scale and pre-training data help
2721600	2725400	as well for low resource machine translation?
2725400	2726800	Yeah, yeah, good question.
2726800	2728200	Actually, there have been some studies done
2728200	2730360	on like how, you know, these systems work.
2730360	2732160	I think for high resource languages,
2732160	2735000	it's actually quite beneficial to scale up.
2735000	2737360	I think part of that is because the models
2737560	2739240	have some innate generalization.
2739240	2740960	And so one of the challenges that people talk
2740960	2742840	about different things in different languages.
2742840	2745600	So like seeing that knowledge in another language
2745600	2747480	can actually help the generalization.
2747480	2751080	But on low resource languages, it's, yeah,
2751080	2753680	the performance is pretty difficult,
2753680	2756600	especially on some of these translation benchmarks.
2756600	2759040	I also think that language models,
2759040	2762000	in terms of being trained for like a translation objective,
2762000	2764320	tend to score worse on translation benchmarks
2764320	2766480	because language models are like approximately
2767480	2769680	capturing the same thing or as translation models.
2769680	2772640	So you really try to align the meaning a little bit more.
2773800	2775240	But yeah, so I think for low resource,
2775240	2777480	it's still pretty challenging.
2777480	2779320	But yeah, one thing that's interesting
2779320	2781320	is for most English language models,
2781320	2783400	they can actually do a reasonable job
2783400	2784840	at producing other languages
2784840	2787000	because it's impossible to get rid of other languages
2787000	2788720	in your English specific data.
2788720	2792200	So things like French or German will work reasonably.
2796480	2799400	So just to clarify, you said language models trained
2799400	2803600	with a translation objective do better, right?
2803600	2806040	Because, right?
2806040	2806880	They tend to do better.
2806880	2809200	Like if you fine tune for the translation task,
2809200	2810760	it will tend to do better.
2810760	2812240	Well, that makes sense compared to like,
2812240	2816800	for example, some few shot in context examples.
2816800	2819080	Right, right, exactly, exactly.
2819080	2820640	And one other question is,
2823760	2825640	do you see this being similar to, for example,
2825640	2828200	fine tuning on particular expert domains,
2828200	2833200	which might also have less data and low resource,
2833920	2836720	and as well as domain specific jargon and so forth?
2838800	2841880	Yeah, I mean, I think if we were to restart this project now,
2841880	2843480	I think that would be one of the first things
2843480	2846120	we also explored, or at least like an extremely strong
2846120	2848440	baseline where if you like take some of the data
2848440	2853120	and you try to fine tune or try to do domain adaptation,
2853120	2855520	I think that's also where like some of the like retrieval
2855520	2858720	type approaches go in for translation,
2858720	2860600	but also large language modeling work
2860600	2862520	where you try to have like a separate domain
2862520	2865920	that you can like retrieve some text in for adaptation.
2865920	2868520	I think all of those approaches are pretty promising.
2874520	2876280	Arcade, any other questions?
2879440	2881560	One quick one on the point of the video question.
2881560	2882760	You're looking at one of the slides,
2882760	2885960	I think you showed some peak results with zero shot
2885960	2890280	that were higher than just the base model.
2890280	2892880	Do you think that's because there might still be
2892880	2896040	some overfitting on those low resource languages?
2896040	2897000	Yeah, good question.
2897000	2899720	So for our large scale mining,
2899720	2903000	we don't mind like every single possible cross pair.
2903000	2906400	So like Icelandic will love,
2906400	2908360	it's probably like not like the most
2908360	2910080	in demand translation direction.
2910080	2912920	And so we did not mind like all 200 times 200
2912920	2914040	because it's like really producing
2914040	2915960	like a combinatorial explosion.
2915960	2918400	And so that's where the zero shot results come from
2918400	2920560	where you don't need,
2920560	2923560	we don't have training data directionally in that pair,
2923560	2926520	but the model has seen both the input and the output.
2926520	2930200	And so I think those results are pretty good.
2930200	2932720	Well, they're good for certain languages,
2932720	2936760	which I think goes to show like the generalization capability
2936760	2938800	and it's not like as critical
2938800	2940760	to have like every single pair covered,
2940760	2942440	but many of them are not as good.
2942440	2944840	And so you see overall the performance is lower,
2944840	2946080	even though on certain languages,
2946080	2948000	it can perform better than you expect.
2948000	2950160	But that's because it has seen the input and the output.
2950160	2952960	It's not zero shot on like completely unseen language.
2962000	2963160	I have a question.
2964120	2968120	But I wanted you to also, you know,
2968120	2971000	do something related to transcription
2971000	2972640	or audio information?
2974400	2975640	Yeah, good question.
2975640	2978960	So in this project, no, not so much transcription,
2978960	2980760	but we had a follow up work that we released
2980760	2984800	actually just like a month or so ago called seamless M4T,
2984800	2986120	which is like a joint model
2986120	2988400	for both speech and text translation.
2988400	2991440	And that's where we do leverage a lot of audio transcription
2991440	2994320	because that also has like, it helps us bridge,
2994320	2996440	you know, like the spoken data and the text data
2996440	2998120	to leverage both of them together.
3008240	3010960	Wait, just to clarify, the supervised fine tuning
3010960	3015560	it worked better, right, compared to other methods.
3016600	3019120	So actually in this work, it was as a couple of years ago now.
3019120	3023520	So supervised fine tuning wasn't as common as it was now.
3023520	3025240	But I think in the literature,
3025240	3026960	if you want to use like a large language model
3026960	3029160	to do translation, it's currently best yet
3029160	3031600	if you do some supervised fine tuning.
3031600	3033120	I'm just wondering about that
3033120	3034800	because the way as humans, right,
3034800	3039360	we don't just learn by looking at pairs of the same thing
3039360	3040200	in different languages
3040200	3044360	and kind of memorizing how to map from one to the other.
3044360	3046240	We kind of learn in a more unsupervised way
3046240	3048400	where if we know both languages,
3048400	3051840	then we can kind of naturally translate between them.
3054480	3057120	But I guess it makes sense for an LLMY
3057120	3061280	having supervised examples would help, yeah.
3061280	3065000	Yeah, I mean, I think as like the base foundation model
3065000	3066800	continues to improve in quality,
3066800	3069240	I think that's where the quality will probably improve
3069240	3071160	when you don't need less and less fine tuning.
3071160	3073560	I mean, do you think that's like the open AI approach?
3073560	3075360	Like if you have the best foundation model,
3075360	3078040	then you don't need as much like domain specific fine tuning.
3078080	3079840	I think like, you know, like at the start
3079840	3081400	when I started working on text generation,
3081400	3082840	there was like translation researchers
3082840	3084040	and like summarization researchers
3084040	3085480	and like question answering researchers
3085480	3087240	and they like work very differently.
3087240	3088680	But now it's like, it's all driven
3088680	3090000	by the same underlying thing
3090000	3091680	and you're not like a specialized
3091680	3093760	summarization researcher anymore.
3095480	3098600	Right, I think that makes a lot of sense.
3101280	3103000	Do we have any other questions?
3104000	3105000	Any questions?
3112000	3114000	Ron, any more in-person questions?
3116000	3117000	Oh, don't think so.
3118000	3119000	Okay, great.
3119000	3120000	All right.
3120000	3123000	Well, thank you, Angela, for the very interesting
3123000	3126000	and a great talk again and for taking the time.
3126000	3131000	And we hope, yeah, we hope that you can keep in touch
3131000	3134000	and if anybody has any other questions,
3134000	3137000	feel free to get in touch with Angela.
3137000	3139000	All right, thanks so much for having me today.
3139000	3140000	Bye, everyone.
