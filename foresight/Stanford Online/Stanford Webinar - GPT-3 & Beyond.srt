1
00:00:00,000 --> 00:00:15,360
So Chris Potts is a professor and actually also the chair of the Department of Linguistics

2
00:00:15,360 --> 00:00:20,400
and by courtesy also at the Department of Computer Science and he's a great expert in

3
00:00:20,400 --> 00:00:24,400
the area of natural language understanding so he's you know there would not be a better

4
00:00:24,400 --> 00:00:29,160
person to hear about a topic than him and we are so grateful that he could make the

5
00:00:29,160 --> 00:00:34,760
time and he's actually also teaching the graduate course CS22 for you natural language

6
00:00:34,760 --> 00:00:39,880
understanding that we actually transform into a professional course that is starting next week

7
00:00:39,880 --> 00:00:44,920
on the same topic so you know if you're interested in learning more we have some links included

8
00:00:45,560 --> 00:00:50,760
you know down below on your platform you can check it out and you know there's so many other

9
00:00:50,760 --> 00:00:55,320
things that can be said about Chris like he has a super interesting podcast he's running like so

10
00:00:55,320 --> 00:01:01,160
many interesting research papers like projects he worked on so you know go ahead and learn more

11
00:01:01,160 --> 00:01:07,240
about him like you should also have a little link I think without further ado I think we can kick it

12
00:01:07,240 --> 00:01:13,800
off Chris thank you so much once again oh thank you so much Petra for the kind words and welcome

13
00:01:13,800 --> 00:01:20,200
to everyone it's wonderful to be here with you all I do think that we live in a golden age for

14
00:01:20,280 --> 00:01:26,360
natural language understanding maybe also a disconcerting age a weird age but certainly

15
00:01:26,360 --> 00:01:31,480
a time of a lot of innovation and a lot of change it's sort of an interesting moment for

16
00:01:31,480 --> 00:01:39,640
reflection for me because I started teaching my NLU course at Stanford in 2012 about a decade ago

17
00:01:40,280 --> 00:01:46,920
that feels very recent in my lived experience but it feels like a completely different age

18
00:01:46,920 --> 00:01:53,320
when it comes to NLU and indeed all of artificial intelligence I I never would have guessed in 2012

19
00:01:53,320 --> 00:01:59,720
that we would have such an amazing array of technologies and scientific innovations and

20
00:01:59,720 --> 00:02:06,840
that we would have these models that were just so performant and also so widely deployed in the

21
00:02:06,840 --> 00:02:14,360
world this is also a story of again for better or worse increasing societal impact and so that

22
00:02:14,360 --> 00:02:19,160
does come together for me into a golden age and just to reflect on this a little bit it's really

23
00:02:19,160 --> 00:02:25,160
just amazing to think about how many of these models you can get hands on with if you want to

24
00:02:25,160 --> 00:02:31,800
right away right you can download or use via apis models like dolly 2 that do incredible text to

25
00:02:31,800 --> 00:02:37,720
image generation stable diffusion mid-journey they're all in that class we also have github

26
00:02:37,720 --> 00:02:43,640
co-pilot based in the codex model for doing code generation tons of people derive a lot of value

27
00:02:43,640 --> 00:02:50,520
from that system u.com is at the leading edge I would say of search technologies that are changing

28
00:02:50,520 --> 00:02:55,000
the search experience and also leading us to new and better results when we search on the web

29
00:02:56,040 --> 00:03:03,000
whisper ai is an incredible model from open ai this does speech to text and this model is a

30
00:03:03,000 --> 00:03:10,280
generic model that is better than the best user customized models that we had 10 years ago just

31
00:03:10,280 --> 00:03:14,840
astounding not something I would have predicted I think and then of course the star of our show

32
00:03:14,840 --> 00:03:22,200
for today is going to be these big language models gpt 3 is the famous one you can use it via an api

33
00:03:22,200 --> 00:03:28,840
we have all these open source ones as well that have come out opt bloom gpt neo x these are models

34
00:03:28,840 --> 00:03:33,800
that you can download and work with to your heart's content provided that you have all the computing

35
00:03:33,800 --> 00:03:39,560
resources necessary so just incredible and I'm sure you're familiar with this but let's just you

36
00:03:39,560 --> 00:03:44,600
know get this into our common ground here it's incredible what these models can do here's a quick

37
00:03:44,600 --> 00:03:53,960
demo of gpt 3 I asked the da Vinci 2 engine in which year was stanford university founded

38
00:03:53,960 --> 00:03:59,000
when did it enroll its first students who is its current president and what is its mascot

39
00:03:59,000 --> 00:04:06,200
and da Vinci 2 gave a fluent and complete answer that is correct on all counts just incredible

40
00:04:06,920 --> 00:04:12,680
that was with da Vinci 2 we got a big update to that model in late 2022 that's da Vinci 3

41
00:04:12,680 --> 00:04:18,360
and here I'm showing you that it reproduces that results exactly and I do think that da Vinci 3 is

42
00:04:18,360 --> 00:04:24,360
a big step forward over the previous engine here's actually an example of that you know I have like

43
00:04:24,360 --> 00:04:30,120
to play adversarial games with this model and so I asked da Vinci 2 would it be possible to hire a

44
00:04:30,120 --> 00:04:35,320
team of tamarins to help me paint my house assuming I'm willing to pay them in sufficient quantities

45
00:04:35,320 --> 00:04:40,440
of fruit to meet minimum wage requirements in california this is adversarial because I know

46
00:04:40,440 --> 00:04:45,960
that these models don't have a really rich understanding of the world we live in they're

47
00:04:45,960 --> 00:04:51,480
often distracted by details like this and sure enough da Vinci 2 got confused yes it would be

48
00:04:51,480 --> 00:04:55,320
possible to hire a team of tamarins to paint your house you would need to make sure that you're

49
00:04:55,320 --> 00:05:01,560
providing them with enough fruit to meet minimum wage requirements and so forth so easily distracted

50
00:05:01,560 --> 00:05:07,480
but I tried this again with da Vinci 3 and with the same question it gave a very sensible answer no

51
00:05:07,480 --> 00:05:12,280
it would not be possible to hire a team of tamarins to help you paint your house da Vinci 2 was not

52
00:05:12,280 --> 00:05:18,280
distracted by my adversarial game this is not to say that you can't trick da Vinci 2 just go on to

53
00:05:18,280 --> 00:05:24,120
twitter and you'll find examples of that but again I do think we're seeing a pretty remarkable rate

54
00:05:24,120 --> 00:05:31,480
of progress toward these models being robust and relatively trustworthy this is also a story

55
00:05:31,560 --> 00:05:36,760
of scientific innovation that was a brief anecdote but we're seeing this same level of progress

56
00:05:36,760 --> 00:05:41,640
in the tools that we use to measure system performance in the field I've put this under

57
00:05:41,640 --> 00:05:47,000
the heading of benchmark saturate faster than ever this is from a paper from 2021 that I was

58
00:05:47,000 --> 00:05:52,680
involved with Kila et al here's the framework along the x-axis I have time going back to the

59
00:05:52,680 --> 00:05:59,480
1990s and along the y-axis I have a normalized measure of our estimate of human performance

60
00:05:59,480 --> 00:06:06,520
that's the red line set at zero so MNIST digit recognition a grand old data set in the field

61
00:06:06,520 --> 00:06:11,720
that was launched in the 1990s and it took about 20 years for us to surpass this estimate of human

62
00:06:11,720 --> 00:06:17,640
performance switchboard is a similar story launched in the 90s this is the speech to text problem it

63
00:06:17,640 --> 00:06:24,200
took about 20 years for us to get up past this red line here image net is newer this was launched

64
00:06:24,200 --> 00:06:30,680
in 2009 it took about 10 years for us to reach this saturation point and from here the pace is

65
00:06:30,680 --> 00:06:36,040
really going to pick up so squad 1.1 is question answering that was solved in about three years

66
00:06:36,840 --> 00:06:43,240
the response was squad 2.0 that was solved in less than two years and then the glue benchmark

67
00:06:43,240 --> 00:06:47,720
if you were in the field you might recall back the glue benchmark is this big set of tasks that

68
00:06:47,720 --> 00:06:53,800
was meant to stress test our best models when it was announced a lot of us worried that it was just

69
00:06:53,800 --> 00:06:59,480
too hard for present day models but glue was saturated in less than a year the response was

70
00:06:59,480 --> 00:07:05,560
super glue meant to be much harder it was also saturated in less than a year a remarkable

71
00:07:05,560 --> 00:07:10,440
story of progress undoubtedly even if you're cynical about this measure of human performance

72
00:07:10,440 --> 00:07:17,640
we are still seeing a rapid increase in the rate of change here and you know 2021 was ages ago in

73
00:07:17,640 --> 00:07:23,640
the story of AI now I think this same thing carries over into the current era with our largest

74
00:07:23,640 --> 00:07:29,480
language models this is from a really nice post from Jason Wei he is assessing emergent abilities

75
00:07:29,480 --> 00:07:35,160
in large language models you see eight of them given here along the x-axis for these plots you

76
00:07:35,160 --> 00:07:41,640
have model size and on the y-axis you have accuracy and what Jason is showing is that at a certain

77
00:07:41,640 --> 00:07:47,160
point these really big models just attain these abilities to do these really hard tasks

78
00:07:47,960 --> 00:07:54,520
Jason estimates that for 137 tasks models are showing this kind of emergent ability and that

79
00:07:54,520 --> 00:08:00,760
includes tasks that were explicitly set up to help us stress test our largest language model

80
00:08:00,760 --> 00:08:08,520
they're just falling away one by one really incredible now we're going to talk a little

81
00:08:08,520 --> 00:08:13,640
bit later about the factors that are driving this enormous progress for large language models but I

82
00:08:13,640 --> 00:08:18,920
want to be upfront that one of the major factors here is just the raw size of these models you

83
00:08:18,920 --> 00:08:23,960
can see that in Jason's plots that's where the emergent ability kicks in and let me put that

84
00:08:23,960 --> 00:08:29,080
in context for you so this is from a famous plot from a paper that's actually about making models

85
00:08:29,080 --> 00:08:35,560
smaller and what they did is track the rise of you know increases in model size along the x-axis

86
00:08:35,560 --> 00:08:42,040
we have time depth it only goes back to 2018 it's not very long ago and in 2018 the largest of our

87
00:08:42,040 --> 00:08:50,920
models had around 100 million parameters seems small by current comparisons in late 2019 early 2020

88
00:08:50,920 --> 00:08:56,440
we start to see a rapid increase in the size of these models so that by the end of 2020 we have

89
00:08:56,440 --> 00:09:03,800
this megatron model at 8.3 billion parameters I remember when that came out it seemed like it

90
00:09:03,800 --> 00:09:09,640
must be some kind of typo I could not fathom that we had a model that was that large but now of

91
00:09:09,640 --> 00:09:14,760
course this is kind of on the small side soon after that we got an 11 billion parameter variant of

92
00:09:14,760 --> 00:09:22,840
that model and then gpd3 came out that says 175 billion parameters and that one too now looks

93
00:09:22,840 --> 00:09:28,440
small in comparison to these truly gargantuan megatron models and the palm model from google

94
00:09:28,440 --> 00:09:36,040
which surpassed 500 billion parameters I want to emphasize that this has made a complete mockery

95
00:09:36,040 --> 00:09:42,600
of the y-axis of this plot to capture the scale correctly we would need 5 000 of these slides

96
00:09:42,600 --> 00:09:48,040
stacked on top of each other again it still feels weird to say that but that is the truth

97
00:09:48,040 --> 00:09:54,440
the scale of this is absolutely enormous and not something I think that I would have anticipated

98
00:09:54,440 --> 00:09:59,960
way back when we were dealing with those 100 million parameter babies by comparison they seem

99
00:09:59,960 --> 00:10:07,560
large to me at that point so this brings us to our central question it's a golden age this is all

100
00:10:07,560 --> 00:10:12,760
undoubtedly exciting and the things that I've just described to you are going to have an impact on

101
00:10:12,760 --> 00:10:19,320
your lives positive and negative but certainly an impact but I take it that we are here today

102
00:10:19,320 --> 00:10:25,000
because we are researchers and we would like to participate in this research and that could leave

103
00:10:25,000 --> 00:10:31,720
you with a kind of worried feeling how can you contribute to nlu in this era of these gargantuan

104
00:10:31,720 --> 00:10:38,280
models I've set this up as a kind of flow chart first question do you have 50 million dollars

105
00:10:38,280 --> 00:10:44,760
and a love of deep learning infrastructure if the answer is yes to this question then I would

106
00:10:44,760 --> 00:10:49,240
encourage you to go off and build your own large language model you could change the world in this

107
00:10:49,240 --> 00:10:54,600
way I would also request that you get in touch with me maybe you could join my research group and

108
00:10:54,680 --> 00:11:00,840
maybe fund my research group that would be wonderful but I'm assuming that most of you

109
00:11:00,840 --> 00:11:06,920
cannot truthfully answer yes to this question I'm in the no camp right and on both counts I am both

110
00:11:06,920 --> 00:11:12,760
dramatically short of the funds and I also don't have a love of deep learning infrastructure so for

111
00:11:12,760 --> 00:11:18,520
those of us who have to answer no to this question how can you contribute even if the answer is no

112
00:11:18,520 --> 00:11:23,960
there are tons of things that you can be doing all right so just topics that are front of mind

113
00:11:23,960 --> 00:11:29,160
to me include retrieval augmented in-context learning this could be small models that are

114
00:11:29,160 --> 00:11:34,920
performant you could always contribute to creating better benchmarks this is a perennial challenge

115
00:11:34,920 --> 00:11:40,360
for the field and maybe the most significant thing that you can do is just create devices that allow

116
00:11:40,360 --> 00:11:45,960
us to accurately measure the performance of our systems you could also help us solve what I've

117
00:11:45,960 --> 00:11:51,320
called the last mile problem for productive applications these central developments in AI

118
00:11:51,320 --> 00:11:58,040
take us 95 percent of the way toward utility but that last five percent actually having a

119
00:11:58,040 --> 00:12:04,680
positive impact on people's lives often requires twice as much development twice as much innovation

120
00:12:04,680 --> 00:12:11,080
across domain experts people who are good at human computer interaction and AI experts right and

121
00:12:11,080 --> 00:12:15,800
there's so there's just a huge amount that has to be done to realize the potential of these technologies

122
00:12:16,520 --> 00:12:22,600
and then finally you could think about achieving faithful human interpretable explanations of how

123
00:12:22,600 --> 00:12:28,120
these models behave if we're going to trust them we need to understand how they work at a human level

124
00:12:28,120 --> 00:12:32,600
that is supremely challenging and therefore this is incredibly important work you could be doing

125
00:12:33,800 --> 00:12:38,360
now I would love to talk with you about all four of those things and really elaborate on them but

126
00:12:38,360 --> 00:12:44,680
our time is short and so what I've done is select one topic retrieval augmented in-context learning

127
00:12:44,680 --> 00:12:50,120
to focus on because it's it's intimately connected to this notion of in-context learning

128
00:12:50,120 --> 00:12:55,960
and it's a place where all of us can participate in lots of innovative ways so that's kind of the

129
00:12:55,960 --> 00:13:02,920
central plan for the day before I do that though I just want to help us get more common ground around

130
00:13:02,920 --> 00:13:08,760
what I take to be the really central change that's happening as a result of these large language

131
00:13:08,760 --> 00:13:15,800
models and I've put that under the heading of the rise of in-context learning again this is

132
00:13:15,800 --> 00:13:21,000
something we're all getting used to it really remarks a genuine paradigm shift I would say

133
00:13:22,440 --> 00:13:28,440
in-context learning really traces to the GPT-3 paper there are precedents earlier in the literature

134
00:13:28,440 --> 00:13:34,360
but it was the GPT-3 paper that really gave it a thorough initial investigation and showed that it

135
00:13:34,440 --> 00:13:40,840
had promised with the earliest GPT models here's how this works we have our big language model

136
00:13:40,840 --> 00:13:47,000
and we prompt it with a bunch of text so for example this is from that GPT-3 paper we might

137
00:13:47,000 --> 00:13:53,240
prompt the model with a context passage and a title we might follow that with one or more

138
00:13:53,240 --> 00:13:58,360
demonstrations here the demonstration is a question and an answer and the goal of the

139
00:13:58,360 --> 00:14:03,080
demonstration is to help the model learn in context that is from the prompt we've given it

140
00:14:03,080 --> 00:14:08,200
what behavior we're trying to elicit from it so here you might say we're trying to coax the model

141
00:14:08,200 --> 00:14:14,440
to do extractive question answering to find the answer as a substring of the passage we gave it

142
00:14:14,440 --> 00:14:19,480
you might have a few of those and then finally we have the actual question we want the model to

143
00:14:19,480 --> 00:14:25,240
answer we prompt the model with this prompt here that puts it in some state and then its

144
00:14:25,240 --> 00:14:30,120
generation is taken to be the prediction or response and that's how we assess its success

145
00:14:30,920 --> 00:14:35,080
and the whole idea is that the model can learn in context that is from this prompt

146
00:14:35,080 --> 00:14:39,880
what we want it to do so that gives you a sense for how this works you've probably all prompted

147
00:14:39,880 --> 00:14:44,760
language models like you like this yourself already i want to dwell on this for a second though

148
00:14:44,760 --> 00:14:50,200
this is a really different thing from what we used to do throughout artificial intelligence

149
00:14:50,200 --> 00:14:55,800
let me contrast in context learning with the standard paradigm of standard supervision

150
00:14:56,760 --> 00:15:03,000
back in the old days of 2017 or whatever we would typically set things up like this we would have

151
00:15:03,000 --> 00:15:07,720
say we wanted to solve a problem like classifying texts according to whether they express nervous

152
00:15:07,720 --> 00:15:12,680
anticipation a complex human emotion the first step would be that we would need to create a data

153
00:15:12,680 --> 00:15:18,520
set of positive and negative examples of that phenomenon and then we would train a custom

154
00:15:18,520 --> 00:15:24,760
built model to make the binary distinction reflected in the labels here it can be surprisingly

155
00:15:24,840 --> 00:15:29,720
powerful but you can start to see already how this isn't going to scale to the complexity of

156
00:15:29,720 --> 00:15:35,480
the human experience we're going to need separate data sets and maybe separate models for optimism

157
00:15:35,480 --> 00:15:41,080
and sadness and every other emotion you can think of and that's just a subset of all the

158
00:15:41,080 --> 00:15:45,880
problems we might want our models to solve for each one we're going to need data and maybe a

159
00:15:45,880 --> 00:15:54,200
custom built model the promise of in-context learning is that a single big frozen language model

160
00:15:54,200 --> 00:15:59,080
can serve all those goals and in this mode we do that prompting thing that I just described

161
00:15:59,080 --> 00:16:04,600
we're going to give the model examples just expressed in flat text of positive and negative

162
00:16:04,600 --> 00:16:09,240
instances and hope that that's enough for it to learn in context about the distinction we're

163
00:16:09,240 --> 00:16:14,760
trying to establish this is really really different consider that over here the phrase nervous

164
00:16:14,760 --> 00:16:20,200
anticipation has no special status the model doesn't really process it it's entirely structured to

165
00:16:20,200 --> 00:16:26,600
make a binary distinction and the label nervous anticipation is kind of for us on the right the

166
00:16:26,600 --> 00:16:32,680
model needs to learn essentially the meanings of all of these terms and our intentions and figure

167
00:16:32,680 --> 00:16:38,920
out how to make these distinctions on new examples all from a prompt it's just weird and wild that

168
00:16:38,920 --> 00:16:44,040
this works at all I think I used to be discouraging about this as an avenue and now we're seeing it

169
00:16:44,040 --> 00:16:52,120
bear so much fruit what are the mechanisms behind this I'm going to identify a few of them for you

170
00:16:52,120 --> 00:16:57,720
the first one is certainly the transformer architecture this is the basic building block

171
00:16:57,720 --> 00:17:03,000
of essentially all the language models that I've mentioned so far we have great coverage of the

172
00:17:03,000 --> 00:17:07,240
transformer in our course natural language understanding so I'm going to do this quickly

173
00:17:07,240 --> 00:17:12,920
the transformer starts with word embeddings and positional encodings on top of those we have a

174
00:17:12,920 --> 00:17:18,600
bunch of attention mechanisms these give the name to the famous paper attention is all you need

175
00:17:18,600 --> 00:17:23,880
which announced the transformer evidently attention is not all you need because we have these

176
00:17:23,880 --> 00:17:27,880
positional encodings at the bottom and then we have a bunch of feed forward layers and

177
00:17:27,880 --> 00:17:35,000
regularization steps at the top but attention really is the beating heart of this model and it

178
00:17:35,000 --> 00:17:41,240
really was a dramatic departure from the fancy mechanisms LSTMs and so forth that were characteristic

179
00:17:41,240 --> 00:17:46,840
of the pre-transformer era so that's essentially though on the diagram here the full model in the

180
00:17:46,840 --> 00:17:52,520
course we have a bunch of materials that help you get hands on with transformer representations

181
00:17:52,520 --> 00:17:58,280
and also dive deep into math into the mathematics so I'm just going to skip past this I will say

182
00:17:58,280 --> 00:18:02,360
that if you dive deep you're likely to go through the same journey we all go through

183
00:18:03,000 --> 00:18:08,440
where your first question is how on earth does this work this diagram looks very complicated

184
00:18:08,440 --> 00:18:14,120
but then you come to terms with it and you realize oh this is actually a bunch of very

185
00:18:14,120 --> 00:18:20,120
simple mechanisms but then you arrive at a question that is a burning question for all of us why does

186
00:18:20,120 --> 00:18:25,720
this work so well this remains an open question a lot of people are working on explaining why this

187
00:18:25,720 --> 00:18:31,400
is so effective and that is certainly an area in which all of us could participate analytic work

188
00:18:31,400 --> 00:18:39,640
understanding why this is so successful the second big innovation here is a realization

189
00:18:39,640 --> 00:18:44,920
that what I've called self supervision is an incredibly powerful mechanism for acquiring

190
00:18:44,920 --> 00:18:51,800
rich representations of form and meaning this is also very strange in self supervision the model's

191
00:18:51,800 --> 00:18:57,400
only objective is to learn from co-occurrence patterns in the sequences it's trained on this is

192
00:18:57,400 --> 00:19:03,400
purely distributional learning another way to put this is the model is just learning to assign

193
00:19:03,400 --> 00:19:10,040
high probability to attested sequences that is the fundamental mechanism we think about these

194
00:19:10,040 --> 00:19:15,400
models as generators but generation is just sampling from the model that's a kind of secondary

195
00:19:15,400 --> 00:19:21,240
or derivative process the main thing is learning from these co-occurrence patterns an enlightening

196
00:19:21,240 --> 00:19:25,640
thing about the current era is that it's fruitful for these sequences content to contain lots of

197
00:19:25,640 --> 00:19:31,560
symbols not just language but computer code sensor readings even images and so forth those

198
00:19:31,560 --> 00:19:37,640
are all just symbol streams and the model learns associations among them the core thing about

199
00:19:37,640 --> 00:19:42,360
self supervision though that really contrasts it with the standard supervised paradigm I mentioned

200
00:19:42,360 --> 00:19:48,520
before is that the objective doesn't mention any specific specific symbols or relations between

201
00:19:48,520 --> 00:19:55,400
them is entirely about learning these co-occurrence patterns and from this simple mechanism we get such

202
00:19:55,480 --> 00:20:02,280
rich results and that is incredibly empowering because you need hardly any human effort to train

203
00:20:02,280 --> 00:20:08,200
a model with self supervision you just need vast quantities of these symbol streams and so that has

204
00:20:08,200 --> 00:20:15,080
facilitated the rise of another important mechanism here large-scale pre-training and there are actually

205
00:20:15,080 --> 00:20:20,440
two innovations that are happening here right so we see the rise of large-scale pre-training in the

206
00:20:20,440 --> 00:20:27,800
earliest work on static word representations like word to vex and glove and what those teams realize

207
00:20:27,800 --> 00:20:33,400
is not only that it's powerful to train on vast quantities of data using just self supervision

208
00:20:33,400 --> 00:20:40,040
but also that it's empowering to the community to release those parameters not just data not just

209
00:20:40,040 --> 00:20:45,480
code but the actual learned representations for other people to build on that has been incredible

210
00:20:45,480 --> 00:20:51,640
in terms of building effective systems after those we get ELMO which was the first model to do this

211
00:20:51,640 --> 00:20:58,120
for contextual word representations truly large language models then we get BERT of course and

212
00:20:58,120 --> 00:21:05,240
GPT and then finally of course GPT-3 at a scale that was really previously unimagined and maybe

213
00:21:05,240 --> 00:21:13,800
kind of unimaginable for me a final piece that we should not overlook is the role of human feedback

214
00:21:13,800 --> 00:21:20,360
in all of this and I'm thinking in particular of the open AI models I've given a lot of coverage

215
00:21:20,360 --> 00:21:26,120
so far of this mechanism of self supervision but we have to acknowledge that our best models

216
00:21:26,120 --> 00:21:31,480
are what open AI calls the instruct models and those are trained with way more than just self

217
00:21:31,480 --> 00:21:38,280
supervision this is a diagram from the chat GPT blog post it has a lot of details I'm confident

218
00:21:38,280 --> 00:21:43,880
that there are really two pieces that are important first the language model is fine tuned

219
00:21:44,440 --> 00:21:50,520
on human level supervision just making binary distinctions about good generations and bad ones

220
00:21:50,520 --> 00:21:56,600
that's already beyond self supervision and then in a second phase the model generates outputs and

221
00:21:56,600 --> 00:22:02,600
humans rank all of the outputs the model has produced and that feedback goes into a lightweight

222
00:22:02,600 --> 00:22:09,000
reinforcement learning mechanism in both of those phases we have important human contributions

223
00:22:09,000 --> 00:22:14,600
that take us beyond that self supervision step and kind of reduce the magical feeling of how

224
00:22:14,600 --> 00:22:20,680
these models are achieving so much I'm emphasizing this because I think what we're seeing is a return

225
00:22:20,680 --> 00:22:26,280
to a familiar and kind of cynical sounding story about AI which is that many of the transformative

226
00:22:26,280 --> 00:22:33,080
step forwards are actually on the back of a lot of human effort behind the scenes expressed at the

227
00:22:33,080 --> 00:22:39,560
level of training data but on the positive side here it is incredible that this human feedback

228
00:22:39,560 --> 00:22:45,240
is having such an important impact instruct models are best in class in the field and we have a lot

229
00:22:45,240 --> 00:22:51,240
of evidence that that must be because of these human feedback steps happening at a scale that I

230
00:22:51,320 --> 00:22:56,840
assume is astounding they must have at open AI large teams of people providing very fine

231
00:22:56,840 --> 00:23:01,800
green feedback across lots of different domains with lots of different tasks in mind

232
00:23:04,280 --> 00:23:10,600
final piece by way of background prompting itself this has been a real journey for all of us I've

233
00:23:10,600 --> 00:23:16,200
described this as step by step and chain of thought reasoning to give you a feel for how

234
00:23:16,200 --> 00:23:20,920
this is happening let's just imagine that we've posed a question like can our models reason about

235
00:23:21,000 --> 00:23:28,600
negation that is if we didn't eat any food does the model know that we didn't eat any pizza in the

236
00:23:28,600 --> 00:23:36,280
old days of 2021 we were so naive we would prompt models with just that direct question like is it

237
00:23:36,280 --> 00:23:40,440
true that if we didn't eat any food then we didn't eat any pizza and we would see what the model

238
00:23:40,440 --> 00:23:49,240
said in return now in 2023 we know so much and we have learned that it can really help to design

239
00:23:49,240 --> 00:23:53,960
a prompt that helps the model reason in the intended ways this is often called step by step

240
00:23:53,960 --> 00:23:58,840
reasoning here's an example of a prompt that was given to me by Omar Khattab you start by telling

241
00:23:58,840 --> 00:24:04,040
it it's a logic and common sense reasoning exam for some reason that's helpful then you give it

242
00:24:04,040 --> 00:24:10,040
some specific instructions and then you use some special markup to give it an example of the kind

243
00:24:10,040 --> 00:24:16,120
of reasoning that you would like it to follow after that example comes the actual prompt and in

244
00:24:16,120 --> 00:24:22,040
this context what we essentially ask the model to do is express its own reasoning and then conditional

245
00:24:22,040 --> 00:24:28,520
on what it has produced create an answer and the eye-opening thing about the current era is that

246
00:24:28,520 --> 00:24:33,000
this can be transformative better I think if you wanted to put this poetically you'd say that these

247
00:24:33,000 --> 00:24:38,040
large language models are kind of like alien creatures and it's taking us some time to figure

248
00:24:38,040 --> 00:24:43,160
out how to communicate with them and together with all that instruct fine tuning with human

249
00:24:43,160 --> 00:24:48,760
supervision we're converging on prompts like this as the powerful device and this is exciting to me

250
00:24:48,760 --> 00:24:55,080
because what's really emerging is that this is a kind of very light way of programming an AI system

251
00:24:55,080 --> 00:24:59,720
using only prompts as opposed to all the deep learning code that we used to have to write

252
00:24:59,720 --> 00:25:04,360
and that's going to be incredibly empowering in terms of system development and experimentation

253
00:25:06,680 --> 00:25:11,240
all right so we have our background in place I'd like to move to my main topic here

254
00:25:11,240 --> 00:25:15,800
which is retrieval augmented in-context learning what you're going to see here is a

255
00:25:15,800 --> 00:25:21,640
combination of language models with retriever models which are themselves under the hood

256
00:25:21,640 --> 00:25:27,160
large language models as well but let me start with a bit of the backstory here I think we're

257
00:25:27,160 --> 00:25:33,000
all probably vaguely aware at this point that large language models have been revolutionizing

258
00:25:33,000 --> 00:25:39,400
search again the star of this is the transformer or maybe more specifically its famous spokesmodel

259
00:25:39,400 --> 00:25:45,400
Burt right after Burt was announced around 2018 Google announced that it was incorporating

260
00:25:45,400 --> 00:25:51,240
aspects of Burt into its core search technology and Microsoft made a similar announcement at

261
00:25:51,240 --> 00:25:57,800
about the same time and I think those are just two public facing stories of you know many instances

262
00:25:57,800 --> 00:26:04,120
of large search technologies having Burt elements incorporated into them in that era and then of

263
00:26:04,120 --> 00:26:09,800
course in the current era we have startups like you.com which have made large language models

264
00:26:09,800 --> 00:26:15,560
pretty central to the entire search experience in the form of you know delivering results but also

265
00:26:15,560 --> 00:26:23,000
interactive search with conversational agents so that's all exciting but I am an NLP at heart

266
00:26:23,000 --> 00:26:27,720
and so for me in a way the more exciting direction here is the fact that finally

267
00:26:28,360 --> 00:26:35,240
search is revolutionizing NLP by helping us bridge the gap into much more relevant

268
00:26:35,240 --> 00:26:40,760
knowledge intensive tasks to give you a feel for how that's happening let's just use question

269
00:26:40,760 --> 00:26:48,120
answering as an example so prior to this work in NLP we would pose question answering or QA in the

270
00:26:48,120 --> 00:26:55,640
following way you saw this already with the GPT-3 example we would have as given at test time a title

271
00:26:55,640 --> 00:27:01,880
and a context passage and then a question and the task of the model is to find the answer to that

272
00:27:01,880 --> 00:27:07,800
question as a literal substring of the context passage which was guaranteed by the nature of

273
00:27:07,800 --> 00:27:14,760
the data set as you can imagine models are really good at this task superhuman certainly at this

274
00:27:14,760 --> 00:27:20,440
task but it's also a very rarefied task this is not a natural form of question answering in the

275
00:27:20,440 --> 00:27:26,440
world and it's certainly unlike the scenario of for example doing web search so the promise of the

276
00:27:26,440 --> 00:27:31,560
open formulations of this task are that we're going to connect more directly with the real world

277
00:27:31,560 --> 00:27:39,400
in this formulation at test time we're just given a question and the standard strategy is to rely on

278
00:27:39,400 --> 00:27:45,880
some kind of retrieval mechanism to find relevant evidence in a large corpus or maybe even the web

279
00:27:46,440 --> 00:27:51,800
and then we proceed as before this is a much harder problem because we're not going to get

280
00:27:51,800 --> 00:27:56,280
the substring guarantee anymore because we're dependent on the retriever to find relevant

281
00:27:56,280 --> 00:28:02,200
evidence but of course it's a much more important task because this is much more like our experience

282
00:28:02,200 --> 00:28:08,600
of searching on the web now I've kind of biased already in describing things this way where I

283
00:28:08,600 --> 00:28:14,600
assume we're retrieving a passage but there is another narrative out there let me skip to this

284
00:28:14,600 --> 00:28:18,920
then you could call this like the llms for everything approach and this would be where

285
00:28:18,920 --> 00:28:24,600
there's no explicit retriever you just have a question come in you have a big opaque model

286
00:28:24,600 --> 00:28:30,200
process that question and out comes an answer voila you hope that the user's information

287
00:28:30,200 --> 00:28:36,520
need is met directly no separate retrieval mechanism just the language model doing everything I think

288
00:28:36,520 --> 00:28:41,800
this is an incredibly inspiring vision but we should be aware that there are lots of kind of

289
00:28:41,800 --> 00:28:48,680
danger zones here so the first is just efficiency one of the major factors driving that explosion

290
00:28:48,680 --> 00:28:53,960
in model size that I tracked before is that in this llms for everything approach we are asking

291
00:28:53,960 --> 00:29:00,200
this model to play the role of both knowledge store and language capability if we could separate

292
00:29:00,200 --> 00:29:07,720
those out we might get away with smaller models we have a related problem of update ability suppose

293
00:29:07,720 --> 00:29:13,080
a fact in the world changes that document on the web changes for example well you're going to have

294
00:29:13,080 --> 00:29:18,600
to update the parameters of this big opaque model somehow to conform to the change in reality

295
00:29:19,400 --> 00:29:23,800
there are people hard at work on that problem that's a very exciting problem but I think we're a

296
00:29:23,800 --> 00:29:29,480
long way from being able to offer guarantees that a change in the world is reflected in the model

297
00:29:29,480 --> 00:29:35,800
behavior and that plays into all sorts of issues of trustworthiness and explainability of behavior

298
00:29:35,800 --> 00:29:43,080
and so forth also we have an issue of provenance look at the answer at the bottom there is that the

299
00:29:43,080 --> 00:29:48,680
correct answer should you trust this model right in the standard web search experience we typically

300
00:29:48,680 --> 00:29:53,880
are given some web pages that we can click on to verify at least at the next level of detail

301
00:29:53,880 --> 00:29:59,560
whether the information is correct but here we're just given this response and if the model also

302
00:29:59,560 --> 00:30:03,640
generated a provenance string if it told us where it found the information we'd be left with the

303
00:30:03,640 --> 00:30:09,320
concern that that provenance string was also untrustworthy right and this is like a really

304
00:30:09,320 --> 00:30:14,440
breaking a fundamental contract that users expect to have with search technologies I believe

305
00:30:15,320 --> 00:30:19,880
so those are some things to worry about there are positives though of course these models are

306
00:30:19,880 --> 00:30:25,960
incredibly effective at meeting your information need directly and they're also outstanding at

307
00:30:25,960 --> 00:30:31,080
synthesizing information if your question can only be answered by 10 different web pages

308
00:30:31,080 --> 00:30:35,160
it's very likely that the language model will still be able to do it without you having to hunt

309
00:30:35,160 --> 00:30:42,360
through all those pages so exciting but lots of concerns here here is the alternative of retrieval

310
00:30:42,360 --> 00:30:49,320
augmented approaches right oh I can't resist this actually just to give you an example of how

311
00:30:49,320 --> 00:30:56,920
important this trustworthy thing can be so I used to be impressed by DaVinci 3 because it would give

312
00:30:57,000 --> 00:31:02,120
a correct answer to the question are professional baseball players allowed to glue small wings

313
00:31:02,120 --> 00:31:06,920
onto their caps this is a question that I got from a wonderful article by Hector Levec where he

314
00:31:06,920 --> 00:31:12,760
encourages us to stress test our models by asking them questions that would seem to run up against

315
00:31:12,760 --> 00:31:18,280
any simple distributional or statistical learning model and really get at whether they have a model

316
00:31:18,280 --> 00:31:23,720
of the world and for DaVinci 2 it gave what it looked like a really good Levec style answer

317
00:31:23,720 --> 00:31:31,000
there is no rule against it but it is not common that seems true so I was disappointed I guess

318
00:31:31,000 --> 00:31:35,160
or I'm actually not sure how to feel about this when I asked DaVinci 3 the same question and it

319
00:31:35,160 --> 00:31:40,440
said no professional baseball players are not allowed to glue small wings onto their caps major

320
00:31:40,440 --> 00:31:44,440
league baseball has strict rules about the appearance of players uniforms and caps in any

321
00:31:44,440 --> 00:31:51,560
modification to the caps are not allowed that also sounds reasonable to me is it true it would help

322
00:31:51,640 --> 00:31:57,400
enormously if the model could offer me at least a web page with with evidence that's relevant to

323
00:31:57,400 --> 00:32:03,480
these claims otherwise I'm simply left wondering and I think that shows you that we've kind of broken

324
00:32:03,480 --> 00:32:08,920
this implicit contract with the user that we expect from search so that'll bring me to my

325
00:32:08,920 --> 00:32:14,280
alternative here retrieval based or retrieval augmented NLP to give you a sense for this at

326
00:32:14,280 --> 00:32:18,680
the top here I have a standard search box and I've put in a very complicated question indeed

327
00:32:19,400 --> 00:32:24,440
the first step in this approach is familiar from the LLMs for everything one we're going to encode

328
00:32:24,440 --> 00:32:29,800
that query into a dense numerical representation capturing aspects of its form and meaning we

329
00:32:29,800 --> 00:32:35,880
use a language model for that the next step is new though we are also going to use a language model

330
00:32:35,880 --> 00:32:41,240
maybe the same one we use for the query to process all of the documents in our document collection

331
00:32:41,880 --> 00:32:48,040
so each one has some kind of numerical deep learning representation now on the basis of

332
00:32:48,040 --> 00:32:53,080
these represent representations we can now score documents with respect to queries just like we

333
00:32:53,080 --> 00:32:59,640
would in the standard good old days of information retrieval so we can reproduce every aspect of

334
00:32:59,640 --> 00:33:05,160
that familiar experience if we want to we're just doing it now in this very rich semantic space

335
00:33:05,960 --> 00:33:10,200
so we get some results back and we could offer those to the user as ranked results but we can

336
00:33:10,200 --> 00:33:16,840
also go further we can have another language model call it a reader or a generator slurp up

337
00:33:16,840 --> 00:33:22,680
those retrieved passages and synthesize them into a single answer maybe meeting the user's

338
00:33:22,680 --> 00:33:27,960
information need directly right so let's check in on how we're doing with respect to our goals

339
00:33:27,960 --> 00:33:32,760
here first efficiency I won't have time to substantiate this today but these systems in

340
00:33:32,760 --> 00:33:38,040
terms of parameter counts can be much smaller than the integrated approach I mentioned before

341
00:33:39,240 --> 00:33:45,160
we also have an easy path to update ability we have this index here so as pages change in our

342
00:33:45,160 --> 00:33:51,160
document store we simply use our frozen language model to reprocess and re-represent them and we

343
00:33:51,160 --> 00:33:56,280
can have a pretty good guarantee at this point that information changes will be reflected in

344
00:33:56,280 --> 00:34:01,560
the retrieved results down here we're also naturally tracking provenance because we have

345
00:34:01,560 --> 00:34:06,120
all these documents and they're used to deliver the results and we can have that carry through

346
00:34:06,120 --> 00:34:12,360
into the generation so we've kept that contract with the user these models are incredibly effective

347
00:34:12,360 --> 00:34:18,040
across lots of literature we're seeing that retrieval augmented approaches are just superior

348
00:34:18,040 --> 00:34:24,200
to the fully integrated llms for everything one and we've retained the benefit of llms for everything

349
00:34:24,200 --> 00:34:29,800
because we have this model down here the reader generator that can synthesize information into

350
00:34:29,800 --> 00:34:37,880
answers that meet the information need directly so that's my fundamental pitch now again things are

351
00:34:37,880 --> 00:34:44,280
changing fast and even the approach to designing these systems is also changing really fast so in

352
00:34:44,280 --> 00:34:51,720
the in the previous era of 2020 we would have these pre-trained components like we have our index and

353
00:34:51,720 --> 00:34:57,000
our retriever maybe we have a language model like reader generator and you might have other

354
00:34:57,000 --> 00:35:02,840
pre-trained components image processing and so forth so you have all these assets and the question is

355
00:35:02,840 --> 00:35:08,600
how are you going to bring them together into an integrated solution the standard deep learning

356
00:35:08,600 --> 00:35:14,840
answer to that question is to define a bunch of task specific parameters that are meant to tie

357
00:35:14,840 --> 00:35:19,560
together all those components and then you learn those parameters with respect to some task

358
00:35:19,560 --> 00:35:25,720
and you hope that that has kind of created an effective integrated system that's the modular

359
00:35:25,720 --> 00:35:31,960
vision of deep learning the truth in practice is that even for very experienced researchers

360
00:35:31,960 --> 00:35:38,680
and system designers this can often go really wrong and debugging these systems and figuring out

361
00:35:38,680 --> 00:35:44,440
how to improve them can be very difficult because they are so opaque and the scale is so large

362
00:35:46,520 --> 00:35:52,680
but maybe we're moving out of an era in which we have to do this at all so this will bring us back

363
00:35:52,680 --> 00:35:59,240
to in-context learning the fundamental insight here is that many of these models can in principle

364
00:35:59,240 --> 00:36:07,480
communicate in natural language right so a retriever is abstractly just a device for pulling in text

365
00:36:07,480 --> 00:36:14,200
and producing text with scores and a language model is also a device for pulling in text and

366
00:36:14,200 --> 00:36:20,600
producing text with scores and we have already seen in my basic picture of retrieval augmented

367
00:36:20,600 --> 00:36:25,320
approaches that we could have the retriever communicate with the language model via retrieve

368
00:36:25,320 --> 00:36:31,000
results well what if we just allow that to go in both directions now we've got a system

369
00:36:31,000 --> 00:36:36,840
that is essentially constructed by prompts that help these models do message passing between them

370
00:36:36,840 --> 00:36:42,680
in potentially very complicated ways an entirely new approach to system design that I think is going

371
00:36:42,680 --> 00:36:48,200
to have an incredible democratizing effect on who designs these systems and what they're for

372
00:36:49,000 --> 00:36:55,160
let me give you a deep sense for just how wide open the design space is here again to give you

373
00:36:55,240 --> 00:37:01,800
a sense for how much of this research is still left to be done even in this golden era let's

374
00:37:01,800 --> 00:37:07,320
imagine a search context the question is what course to take what we're going to do in this new

375
00:37:07,320 --> 00:37:14,520
mode is begin a prompt that contains that question just as before and now what we can do next is

376
00:37:14,520 --> 00:37:19,720
retrieve a context passage that'll be like the retrieval augmented approach that I showed you

377
00:37:19,720 --> 00:37:24,760
at the start of this section right you could just use our retriever for that but there's more

378
00:37:24,760 --> 00:37:29,000
that could be done what about demonstrations let's imagine that we have a little train set

379
00:37:29,000 --> 00:37:34,440
of qa pairs that kind of demonstrate for our system what the intended behavior is well we can add

380
00:37:34,440 --> 00:37:39,320
those into the prompt and now we're giving the system a lot of few shot guidance about how to

381
00:37:39,320 --> 00:37:46,280
learn in context right but that's also just the beginning I might have sampled these training

382
00:37:46,280 --> 00:37:52,840
examples randomly for my train set but I have a retriever remember and so what I could do instead

383
00:37:52,840 --> 00:37:58,680
is find the demonstrations that are the most similar to the user's question and put those

384
00:37:58,680 --> 00:38:04,040
in my prompt with the expectation that that will help it understand kind of topical coherence and

385
00:38:04,040 --> 00:38:10,280
lead to better results but I could go further right I could use my retriever again to find

386
00:38:10,280 --> 00:38:15,800
relevant context passages for each one of those demonstrations to further help it figure out

387
00:38:15,800 --> 00:38:21,720
how to reason in terms of evidence and that also opens up a huge design space we could do what we

388
00:38:21,720 --> 00:38:26,600
call hindsight retrieval where for each one of these we're using both the question and the answer

389
00:38:26,600 --> 00:38:33,000
to find relevant context passages to really give you integrated informational packets that the model

390
00:38:33,000 --> 00:38:38,200
can benefit from and there's lots more that we could do with these demonstrations you're probably

391
00:38:38,200 --> 00:38:44,120
starting to see it right we could do some rewriting and so forth really makes sophisticated use of

392
00:38:44,120 --> 00:38:50,040
the retriever and the language model interwoven we could also think about how we selected this

393
00:38:50,040 --> 00:38:56,440
background passage I was assuming that we would just retrieve the most relevant passage according

394
00:38:56,440 --> 00:39:02,680
to our question but we could also think about rewriting the user's query in terms of the

395
00:39:02,680 --> 00:39:07,480
demonstrations that we could construct it to get a new query that will help the model that's

396
00:39:07,480 --> 00:39:12,680
especially powerful if you have a kind of interactional mode where the demonstrations are actually

397
00:39:12,680 --> 00:39:18,520
part of like a dialogue history or something like that and then finally we could turn our

398
00:39:18,520 --> 00:39:23,160
attention to how we're actually generating the answer I was assuming we would take the top

399
00:39:23,160 --> 00:39:27,960
generation from the language model but we could do much more we could filter its generations

400
00:39:27,960 --> 00:39:33,800
to just those that match a substring of the passage reproducing some of the old mode of

401
00:39:33,800 --> 00:39:38,920
question answering but now in this completely open formulation that can be incredibly powerful

402
00:39:38,920 --> 00:39:44,600
if you know your model can retrieve good background passages here those are two simple steps you could

403
00:39:44,600 --> 00:39:51,640
also go all the way to the other extreme and use the full retrieval augmented generation or rag model

404
00:39:51,640 --> 00:39:56,840
which is essentially creates a full probability model that allows us to marginalize out the

405
00:39:56,840 --> 00:40:03,400
contribution of passages that can be incredibly powerful in terms of making maximal use of the

406
00:40:03,400 --> 00:40:11,080
capacity of this model to generate text conditional on all the work that we did up here I hope that's

407
00:40:11,080 --> 00:40:16,360
giving you a sense for just how much can happen here what we're starting to see I think is that

408
00:40:16,360 --> 00:40:22,040
there is a new programming mode emerging it's a programming mode that involves using these large

409
00:40:22,040 --> 00:40:29,720
pre-trained components to design in code prompts that are essentially full AI systems that are

410
00:40:29,720 --> 00:40:35,720
entirely about message passing between these frozen components we have a new paper out that's called

411
00:40:35,800 --> 00:40:40,840
demonstrate search predictor dsp this is a lightweight programming framework for doing

412
00:40:40,840 --> 00:40:46,600
exactly what I was just describing for you and one thing I want to call out is that our results

413
00:40:46,600 --> 00:40:53,960
are fantastic now you know we can pat ourselves on the back we have a very talented team and so it's

414
00:40:53,960 --> 00:40:59,080
no surprise the results are so good but I actually want to be upfront with you I think the real insight

415
00:40:59,080 --> 00:41:05,320
here is that it is such early days in terms of us figuring out how to construct these prompts how to

416
00:41:05,320 --> 00:41:11,400
program these systems that we've only just begun to understand what's optimal we have explored only

417
00:41:11,400 --> 00:41:16,840
a tiny part of the space and everything we're doing is suboptimal and that's just the kind of conditions

418
00:41:16,840 --> 00:41:22,600
where you get these huge leap forwards leaps forward in performance on these tasks so I suspect

419
00:41:22,600 --> 00:41:28,280
that the bold row that we have here will not be long-lived given how much innovation is happening

420
00:41:28,280 --> 00:41:33,960
in this space and I want to make a pitch for our course here right so we have in this course

421
00:41:34,600 --> 00:41:39,560
a bunch of assignment slash bake-offs and the way that works essentially is that you have an

422
00:41:39,560 --> 00:41:45,640
assignment that helps you build some baselines and then work toward an original system which you

423
00:41:45,640 --> 00:41:51,640
enter into a bake-off which is a kind of informal competition around data and modeling our newest

424
00:41:51,640 --> 00:41:57,160
of these is called few shot open qa with cobear retrieval it's a version of the problems that I've

425
00:41:57,160 --> 00:42:02,360
just been describing for you this is a problem that could not even have been meaningfully posed

426
00:42:02,440 --> 00:42:09,000
five years ago and now we are seeing students doing incredible cutting-edge things in this mode

427
00:42:09,000 --> 00:42:14,280
it's exactly what I was just describing for you and we're in the sort of moment where a student

428
00:42:14,280 --> 00:42:19,160
project could lead to a paper that you know really leaves leads to state-of-the-art performance in

429
00:42:19,160 --> 00:42:24,120
surprising ways again because there is just so much research that has to be done here

430
00:42:24,520 --> 00:42:34,440
I'm running out of time what I think I'll do is just briefly call out again those important other

431
00:42:34,440 --> 00:42:39,800
areas that I've given short drift to today but I think are just so important starting with data

432
00:42:39,800 --> 00:42:47,000
sets I've been talking about system design and task performance but it is now and will always be the

433
00:42:47,000 --> 00:42:53,320
case that contributing you new benchmark data sets is basically the most important thing you can do

434
00:42:53,400 --> 00:42:59,080
Jacques Cousteau said water and air the two essential fluids on which all life depends I would

435
00:42:59,080 --> 00:43:08,280
extend that NLP our data sets are the resource on which all progress depends now Cousteau extended

436
00:43:08,280 --> 00:43:13,320
this with have become global garbage cans I am not that cynical about our data sets I think we've

437
00:43:13,320 --> 00:43:18,520
learned a lot about how to create effective data sets we're getting better at this but we need to

438
00:43:18,520 --> 00:43:24,280
watch out for this metaphorical pollution and we need always to be pushing our systems with

439
00:43:24,280 --> 00:43:30,360
harder tasks that come closer to the human capabilities that we're actually actually trying to get them

440
00:43:30,360 --> 00:43:36,120
to achieve and without contributions of data sets we could be tricking ourselves when we think we're

441
00:43:36,120 --> 00:43:43,320
making a lot of progress the second thing that I wanted to call out relates to model explainability

442
00:43:43,320 --> 00:43:49,000
you know we're in an era of incredible impact and that has rightly turned researchers to questions

443
00:43:49,000 --> 00:43:57,560
of system reliability safety trust approved use and pernicious social biases we have to get serious

444
00:43:57,560 --> 00:44:03,640
about all these issues if we're gonna responsibly have all of the impact that we're achieving at this

445
00:44:03,640 --> 00:44:09,720
point all of these things are incredibly difficult because the systems we're talking about are these

446
00:44:09,720 --> 00:44:15,400
enormous opaque impossible to understand analytically devices like this that are just

447
00:44:15,400 --> 00:44:21,000
clouding our understanding of them and so to me that shines a light on the importance of

448
00:44:21,000 --> 00:44:26,920
achieving analytic guarantees about our model behaviors that seems to me to be a prerequisite

449
00:44:26,920 --> 00:44:32,520
for getting serious about any one of these topics and the goal there in our terms is to achieve

450
00:44:33,240 --> 00:44:39,400
faithful human interpretable explanations of model behavior we have great coverage of these

451
00:44:39,400 --> 00:44:45,080
methods in the course hands-on materials screencasts and other things that will help you

452
00:44:45,640 --> 00:44:51,240
participate in this research and also as a side effect write absolutely outstanding

453
00:44:51,240 --> 00:44:58,360
discussion and analysis sections for your papers and the final thing I wanted to call out is just

454
00:44:58,440 --> 00:45:05,800
that last mile problem fundamental advances in AI take us 95 percent of the way there but that last

455
00:45:05,800 --> 00:45:12,280
five percent is every bit as difficult as the first 95 in my group we've been looking a lot at

456
00:45:12,280 --> 00:45:19,960
image accessibility this is an incredibly important societal problem because images are so central

457
00:45:19,960 --> 00:45:26,440
to modern life across being on the web and in social media also in the news and in our scientific

458
00:45:26,440 --> 00:45:32,520
discourse and it's a sad fact about the current state of the world that almost none of these images

459
00:45:32,520 --> 00:45:38,840
are made non-visually accessible so blind and low vision users are basically unable to understand

460
00:45:38,840 --> 00:45:43,640
all this context and receive all of this information something has to change that

461
00:45:44,760 --> 00:45:50,920
image-based text generation has become incredibly good over the last 10 years that's another story

462
00:45:50,920 --> 00:45:57,480
of astounding progress but it has yet to take us to the point where we can actually write useful

463
00:45:57,480 --> 00:46:03,320
descriptions of these images that would help a BLB user and that last bit is going to require

464
00:46:03,320 --> 00:46:11,000
HCI research linguistic research and fundamental advances in AI and by the way lots of astounding

465
00:46:11,000 --> 00:46:17,880
new data sets and this is just one example of in the innumerable number of applied problems

466
00:46:17,880 --> 00:46:23,560
that fall into this mode and that can be very exciting for people who have domain expertise

467
00:46:23,560 --> 00:46:32,600
that can help us close that final mile so let me wrap up here I don't want to have a standard

468
00:46:32,600 --> 00:46:38,600
conclusion I think it's fun to close with some predictions about the future and I have put this

469
00:46:38,600 --> 00:46:43,800
under the heading of predictions for the text next 10 years or so although I'm about to retract

470
00:46:43,800 --> 00:46:50,280
that for reasons I will get to but here are the predictions first laggard industries that are rich

471
00:46:50,280 --> 00:46:56,040
in text data will be transformed in part by NLP technology and that's likely to happen from

472
00:46:56,040 --> 00:47:02,360
some disruptive newcomers coming out of left field second prediction artificial assistants

473
00:47:02,360 --> 00:47:07,240
will get dramatically better and become more ubiquitous with the side effect that you'll

474
00:47:07,240 --> 00:47:13,640
often be unsure in life whether this customer service representative is a person or an AI

475
00:47:13,640 --> 00:47:21,480
or some team combining the two many kinds of writing including student papers at universities

476
00:47:21,480 --> 00:47:26,440
will be done with AI writing assistants and this might be transparently true given how

477
00:47:26,440 --> 00:47:32,360
sophisticated autocomplete and other tools have gotten at this point and then finally

478
00:47:32,360 --> 00:47:37,560
the negative effects of NLP and of AI will be amplified along with the positives I'm thinking

479
00:47:37,560 --> 00:47:44,600
of things like disinformation spread market disruption systemic bias it's almost sure to

480
00:47:44,600 --> 00:47:49,160
be the case if it hasn't already happened already that there will be some calamitous world event

481
00:47:49,720 --> 00:47:56,040
that traces to the intentional or unintentional misuse of some AI technology that's in our future

482
00:47:56,920 --> 00:48:00,920
so I think these are reasonable predictions and I'm curious for yours but I have to tell you

483
00:48:01,720 --> 00:48:09,720
that I made these predictions in 2020 two years ago with the expectation that they would be good

484
00:48:09,720 --> 00:48:16,520
for 10 years but more than half of them probably have already come true two and three are definitely

485
00:48:16,520 --> 00:48:21,640
true about the world we live in and on the flip side I just failed to predict so many important

486
00:48:21,640 --> 00:48:26,360
things like the most prominent example is that I just failed to predict the progress we would see

487
00:48:26,440 --> 00:48:33,400
in text image models like dolly two and and stable diffusion in fact I'll be honest with you I might

488
00:48:33,400 --> 00:48:38,360
have bet against them I thought that was an area that was going to languish for a long time and yet

489
00:48:38,360 --> 00:48:43,320
nonetheless seemingly out of nowhere we had this incredible set of advances and there are probably

490
00:48:43,320 --> 00:48:50,600
lots of other areas where I would make similarly bad predictions so I said 10 years but I think

491
00:48:50,680 --> 00:48:57,320
my new rule is going to be that I'm going to predict only through 2024 at the very outside

492
00:48:57,320 --> 00:49:03,960
because in 10 years the only thing I can say with confidence is that we will be in a radically

493
00:49:03,960 --> 00:49:09,480
different place from where we are now but what that place will be like is anyone's guess I'm

494
00:49:09,480 --> 00:49:14,200
interested in your predictions about it but I think I will stop here thank you very much

495
00:49:14,760 --> 00:49:21,720
thank you so much Chris for the engaging and extremely interesting topic and presentation

496
00:49:21,720 --> 00:49:27,160
you have given I'm always amazed by all the new things you're mentioning every single time we

497
00:49:27,160 --> 00:49:33,080
talk I feel it is something new something exciting you know not you not me especially not me like

498
00:49:33,080 --> 00:49:39,000
expected if you'll be talking about it so soon many questions came in I must already see people

499
00:49:39,000 --> 00:49:43,720
unfortunately not be able to get to all of them because the time is limited and the audience is

500
00:49:43,800 --> 00:49:51,080
so active and so many people showed up so let me pick a few um Chris so the cost of the training

501
00:49:51,080 --> 00:49:55,960
model so it seems it really scales with the size and we are paying a lot of attention and like

502
00:49:55,960 --> 00:50:01,320
putting a lot of effort into the training uh so what does it mean for the energy requirements

503
00:50:01,320 --> 00:50:05,320
and I guess we are talking about predictions but like how does it look like now and like

504
00:50:05,320 --> 00:50:12,360
what do you recommend people to to pay attention to oh it's a wonderful set of questions to be

505
00:50:12,360 --> 00:50:20,280
answering and critically important I mean I ask myself you know you know if you think about

506
00:50:20,280 --> 00:50:25,000
industries in the world some of them are improving in terms of their environmental impacts some are

507
00:50:25,000 --> 00:50:31,320
getting much worse where is artificial intelligence in that is it getting better or is it getting worse

508
00:50:31,320 --> 00:50:37,720
I don't know the answer because on the one hand the expenditure for training and now serving for

509
00:50:37,720 --> 00:50:44,040
example GPT-3 to everyone who wants to use it is absolutely enormous and it has real costs

510
00:50:44,840 --> 00:50:51,720
like measured in emissions and things like that on the other hand this is a centralization of all

511
00:50:51,720 --> 00:50:57,960
of that and that can often bring real benefits and I want to not forget of the previous era

512
00:50:57,960 --> 00:51:05,080
where every single person trained every single model from scratch and so now a lot of our research

513
00:51:05,160 --> 00:51:11,960
is actually just using these frozen components they were expensive but the expenditure of our lab

514
00:51:11,960 --> 00:51:19,240
is probably going way down because we are not training these big models it kind of reminds

515
00:51:19,240 --> 00:51:24,040
me of that last mile problem again in the previous era it was like we were all driving to pick up our

516
00:51:24,040 --> 00:51:30,040
groceries everywhere huge expenditure with all those individual trips now it's much more like

517
00:51:30,120 --> 00:51:33,320
they're all brought to the end of the street and we walk to get them

518
00:51:34,200 --> 00:51:38,440
but of course that's done in big trucks and those have real consequences as well

519
00:51:38,440 --> 00:51:44,920
I don't know but I hope that a lot of smart people work continue to work on this problem

520
00:51:44,920 --> 00:51:48,840
and that'll lead to benefits in terms of us doing all these things more efficiently as well

521
00:51:50,840 --> 00:51:57,000
thank you so much the next question and you touched on that a few times but it might be

522
00:51:57,000 --> 00:52:02,200
good to summarize that a little bit because we got a lot of the questions about kind of the

523
00:52:02,200 --> 00:52:09,080
trustworthiness and if the model actually knows that it's wrong or correct and like how do how

524
00:52:09,080 --> 00:52:13,480
do we trust the model or like how do we achieve the trustworthiness of the model because right now

525
00:52:13,480 --> 00:52:19,240
it's a lot of the generation happening generative models happening so like how do we pass that

526
00:52:19,800 --> 00:52:29,160
it's an incredibly good question and it is the thing I have in mind when we're doing all our work

527
00:52:29,160 --> 00:52:35,720
on explaining models because I feel like offering faithful human interpretable explanations is the

528
00:52:35,720 --> 00:52:42,200
step we can take toward trustworthiness it's a very difficult problem I just want to add that it

529
00:52:42,200 --> 00:52:47,640
might be even harder than we've anticipated because people are also pretty untrustworthy

530
00:52:48,600 --> 00:52:56,120
it's just that individual people often don't have like a systemic effect right so if you're

531
00:52:56,120 --> 00:53:01,480
really doing a poor job at something you probably impact just a handful of people

532
00:53:01,480 --> 00:53:07,640
and other people say at your company do a much better job but these ai's are now it's like they're

533
00:53:07,640 --> 00:53:15,000
everyone and so any kind of small problem that they have is amplified across the entire population

534
00:53:15,080 --> 00:53:19,960
they interact with and that's going to probably mean that our standards for trustworthiness for

535
00:53:19,960 --> 00:53:25,400
them need to be higher than they are for humans and that's another sense in which they're going

536
00:53:25,400 --> 00:53:31,480
to have to be superhuman to achieve the jobs we're asking of them and the field cannot offer

537
00:53:31,480 --> 00:53:39,960
guarantees right now so come help us fascinating thank you so much and like I saw also some

538
00:53:39,960 --> 00:53:44,600
questions or comments about the bias in data and like you mentioned it also right like you

539
00:53:44,600 --> 00:53:52,120
like we are improving like there is a big improvement happening um last question for you um like a

540
00:53:52,120 --> 00:53:56,680
little bit of a thought experiment but like do you think that the large language models might be

541
00:53:56,680 --> 00:54:02,520
able to come up with answers to as yet unanswered important scientific questions like something

542
00:54:02,520 --> 00:54:08,920
we are not even sure that it even exists like in our minds right now oh it's a wonderful question

543
00:54:08,920 --> 00:54:13,720
yeah and people are asking this across multiple domains like they're producing incredible artwork

544
00:54:13,720 --> 00:54:19,320
but are we now trapped inside a feedback loop that's going to lead to less truly innovative art

545
00:54:19,320 --> 00:54:25,240
and and if we ask them to generate text are they going to do either weird irrelevant stuff or just

546
00:54:25,240 --> 00:54:32,840
more of the boring average case stuff um I don't know the answer I will say though that these models

547
00:54:32,840 --> 00:54:38,840
have an incredible capacity to synthesize information across sources and I feel like

548
00:54:39,480 --> 00:54:45,880
that is a source of innovation for humans as well simply making those connections and it might be

549
00:54:45,880 --> 00:54:50,920
true that there is nothing new under the sun but there are lots of new connections perspectives

550
00:54:50,920 --> 00:54:56,520
and so forth to be had and I actually do have faith that models are going to be able to at least

551
00:54:56,520 --> 00:55:04,280
simulate some of that and it might look to us like innovation but this is not to say that this

552
00:55:04,280 --> 00:55:09,960
is uh not a concern for us it should be something we think about especially because we might be

553
00:55:09,960 --> 00:55:14,360
heading into an era when whether we want them to or not mostly these models are trained on their own

554
00:55:14,360 --> 00:55:19,720
output which is being put on the web and then consumed when people create train sets and so

555
00:55:19,720 --> 00:55:28,840
forth and so on yeah great thank you so much and we are nearing the end so like last point um

556
00:55:29,400 --> 00:55:35,160
do you have any like last remarks any anything anything interesting you would suggest others to

557
00:55:35,160 --> 00:55:42,680
look at follow read um learn about to kind of get more acquainted with the subject well learn

558
00:55:42,680 --> 00:55:47,560
more about the NLU GPT-3 other large language models and their recommendations

559
00:55:50,680 --> 00:55:55,720
the thing that comes to mind based on all the interactions I have with the professional

560
00:55:55,720 --> 00:56:01,720
development students who have taken our course before is that a lot of you I'm guessing have

561
00:56:01,720 --> 00:56:08,760
incredibly valuable valuable domain expertise you work in an industry in a position that has taught

562
00:56:08,760 --> 00:56:15,560
you tons of things and given you lots of skills and my last mile problem shows you that that is

563
00:56:15,560 --> 00:56:21,320
relevant to AI and therefore you could bring it to bear on AI and we might all benefit where you

564
00:56:21,320 --> 00:56:25,800
would be taking all these innovations you can learn about in our course and other courses

565
00:56:25,800 --> 00:56:32,600
combining that with your domain expertise and maybe actually making progress in a meaningful way on a

566
00:56:32,600 --> 00:56:39,720
problem as opposed to merely having demos and things that our scientific community often produces

567
00:56:39,720 --> 00:56:45,160
real impact so often requires real domain expertise of the sort you all have

568
00:56:47,000 --> 00:56:53,160
great thank you so much um and yeah at the end thank you so much Chris for taking the time to do

569
00:56:53,160 --> 00:56:59,560
this I know beginning of the quarter hectic Stanford live and I appreciate you taking the time to do

570
00:56:59,560 --> 00:57:05,320
this to run this webinar thank you also everybody who had a chance to join us live or like who's

571
00:57:05,320 --> 00:57:11,240
watching this recording if you could please let us know what kind of other topics you might be

572
00:57:11,240 --> 00:57:18,440
interested in in this sort of a free webinar structure we have a little survey down on the console

573
00:57:19,640 --> 00:57:26,280
and yeah I hope you all have a great day a wonderful start of the of or like end of the

574
00:57:26,280 --> 00:57:31,000
winter start of the spring and yeah thank you everybody for joining us yeah Petra this is

575
00:57:31,000 --> 00:57:35,400
wonderful we got an astounding number of really great questions it's too bad we're out of time

576
00:57:35,400 --> 00:57:39,480
there's a lot to think about here and so that's just another thank you to the audience for all

577
00:57:39,480 --> 00:57:52,680
this food for thought thank you

