start	end	text
0	17960	This is definitely one of the shortest travel trips that I've ever had to give a talk because
17960	22960	my office is just across the street, and this is a familiar space because we ran the Human
22960	26440	Computer Interaction Seminar in here last year.
26440	31720	So it's great to be here today, and as Mark said, I'm one of the faculty members in the
31720	36880	mechanical engineering department where I run the Shape Lab, but I'm also very much involved
36880	40560	with the Stanford Human Computer Interaction Group as well.
40560	44640	And so today I'm going to be talking about our work sort of at the intersection between
44640	49560	yeah, human computer interaction and robotics and mechatronics systems and thinking about
49560	53800	how can we really enable people to interact in much richer ways.
53800	60920	I also have to apologize, I had a hand procedure done earlier today, and basically I thought
60920	66000	it was going to be not as painful as it was, and so I'm a little bit more under the weather
66000	72480	than normal, so you'll have to believe that this would be even more exciting normally.
72480	79520	So speaking of hands, I've long been impressed by the dexterous ability of our hands to allow
79560	85640	us to manipulate the world, and certainly having recently had some injury for my wrist,
85640	91920	that's become even more clear to me how important our hands are for manipulating things in the
91920	96840	world and really just the amazing complexity of our hands and the way in which we're able to
96840	103360	interact so gracefully and dexterously to achieve really stillful manipulation such as here,
103360	106760	sculpting, and clay, but all of our everyday tasks as well.
107320	114040	But to me what's really exciting about the hand and manipulation is the role of manipulation
114680	123320	in terms of our cognition as well. And so physical interaction is very important for not just
123320	128680	physical manipulation for the sake of changing the world, but also for us to express our thoughts
128680	134360	as you see me gesturing like this, but also for us to better understand information. So there's
134360	139880	been a lot of studies that have looked at how children when they learn with abacai, for example,
139880	146920	or abacus in plural, abacai, that they actually learn math concepts differently and better than if
146920	152120	they learn them just on paper and pencil. And so there's something behind this idea of embodied
152120	157720	cognition that's actually quite important. And we see that not just in terms of education,
157720	162440	but really in terms of very specialized domains as well. So for example, this is a photograph
163080	167960	of urban planners and architects. And what we see there is there's this, you know, basically
167960	173080	rich interplay between the physical models that they're building, space, and the ways in which
173080	179080	people want to interact as well. And so physical and spatial form can really help us better understand
179080	185080	and solve problems and very complex ones in the architecture domain, but many other domains as
185080	191320	well. And the big kind of reason behind this is that spatial manipulation, as I kind of highlighted
191320	197080	before, really aids in spatial cognition. These two things are very tightly coupled together.
197080	202680	And one of the great professors that we have had at Stanford, Barbara Tversky, has done a lot of work
202680	208040	as well, looking into the role that spatial manipulation plays in our cognition and kind
208040	213560	of the evolutionary basis for this as well. So she has a great new book out called Mind and
213560	218840	Motion that I highly recommend you take a look at. But in many ways, this is because cognition
218840	225160	and perception are very much coupled together through action. And we've evolved over a long
225160	230440	period of time to really benefit from the way that we can manipulate the world to better
230440	235640	understand it. And these things are tightly coupled together. This is even more important
235640	241800	when we think about access to information. And so for example, these physical representations,
241800	246680	you know, can be really important for people with different abilities. So here's a picture
246680	252760	of a blind student at the Perkins School for the Blind in the Boston area looking or feeling,
252760	258200	rather, a double helix model. And this is something that would be very hard for us to explain through
258200	263160	words, but by being able to touch, feel, and manipulate it, they can, you know, much more
263160	270200	easily understand. And this is a tactile model of Berlin's Museum Island. Again, this type of
270280	276920	spatial information might be very hard for us to convey through text alone, but by allowing
276920	283480	people to directly access it through touch and through our haptic sense, we can much more readily
283480	288840	access it. But if we look at the ways in which we traditionally interact with computing and with
288840	296040	information, not much has changed since 30 years ago in some ways for the ways in which we interact.
296040	300520	And historically, it's been the case that we really haven't been able to leverage our sense of
300520	306440	touch in that. And if we look towards, you know, basically newer interfaces for interaction such
306440	311720	as in virtual reality, a lot of times we really get these benefits of spatial interaction where
311720	317160	here I can move around and interact and collaborate with someone else. But when we reach out to feel
317160	322520	something or touch something, we're not really receiving any meaningful feedback. And so one of
322520	327000	the key questions that I've been interested in as well as many other people in the field of haptic
327000	332760	interfaces is, you know, what if we could reach out and touch the void? And so during my PhD at
332760	337720	the MIT Media Lab, we were really interested in trying to merge between the physical and the digital
337720	344680	world and bring that sense of physical touch into the real world so that if we were, you know, two
344680	349800	people working around a workstation, we could look at each other, see each other, and also directly
349800	356200	interact with a surface and be able to feel it. And so let me show you what that might look like.
356200	362760	So here's a picture of a new type of haptic display. We call this a shape display that we developed at
362760	367880	the MIT Media Lab with my advisor, PhD advisor Hiroshi Ishii and my colleague Daniel Lightinger,
367880	371960	where we could, you know, really reach out and sort of sculpt with digital clay,
371960	377400	or we could reach out and new interface elements could appear that have the correct affordances
377400	383480	and ergonomics for how we might want to interact. Or we could render different types of 3D models in
383480	390600	sort of real time and be able to, again, see them in 3D but also touch them. So this type of display
390600	395880	had been sort of considered before by a number of other researchers, but here we were able to really
395880	400600	look at what are these meaningful interaction techniques and start to look at some example
400600	405640	applications as well. So this was a project that we worked on with Tony Tain, who was a
405720	411160	formerly a practicing urban planner. And as I said before, you know, the spatial models that
411160	415960	urban planners and architects build are often very important for how they think and consider this
415960	420520	space. But they're, you know, basically they would make one set of full models and then not
420520	426280	be able to change anything. And so here we can in real time change the models, but also have other
426280	431640	simulations that we can load on top of that and be able to, you know, edit, manipulate and collaboratively
431640	437880	work together. So this sort of gives a hint for what this new type of interaction with what we
437880	443720	call shape changing displays might be like. And so when I came to Stanford, I guess now seven-ish
443720	449480	years ago or so, I started the Shape Lab to really deeper investigate these types of systems,
449480	454600	but also work on some other areas as well. And so in our group, we've been working on new types of
454600	458840	haptic displays, like the ones that I've shown here. And that's really what I'm going to spend
458840	463960	the most of today talking about. But I quickly wanted to mention two other areas that our group
463960	469320	has also been working on as well, and that are kind of related to the robotics areas. And so
469320	474760	the next area that we've also been thinking about is this notion of ubiquitous robotic interfaces.
474760	481000	And really this idea harkens back to someone who maybe some of you know of, but Mark Weiser,
481000	485880	who sort of started the field of ubiquitous computing that we really live in today, where
485960	491080	there's, you know, in this room around us, many different sensors and actual or sorry, many different
491080	498360	sensors and displays, there's probably, you know, at least 100, if not 1000 computers in this room
498360	503160	right now. And so the question that we had is sort of what would it be like in the future if we
503160	510920	could have actuation and robotics be embedded in our environment as easily as the computing and
510920	516760	displays that we have today. And so with my PhD student, who's now a professor at Simon Frazier
516760	522600	University, Lawrence Kim, we created this platform of small mobile robots that we could then explore.
522600	528200	And again, here we can have them display information like this. But we also looked at ways in which
528200	534440	they could be embedded in our environment, like on our desk, move around to display information when
534440	539480	we need, but also manipulate things in the environment or go about, you know, remotely
539480	544680	sensing different things as well. And we think this kind of opens up some interesting opportunities
544680	551160	to think about, again, where does this line between robot and environment end. And so that's
551160	556360	something that we're also quite interested in and kind of intersects a lot with human robot
556360	561400	interaction. And the last area that we're doing work in is in shape changing robotics, which is
561400	566600	something that Mark highlighted before. And a lot of that has to do with the enabling technologies
566600	572440	that we are looking at to make shape changing displays and also shape changing robots. And so
572440	576840	if you think back to that first example haptic display that I showed that is sort of like a
576840	583240	2.5D surface display, one of the things that we have been thinking about is how might we sort of
583240	590360	make full 3D shape changing displays. This is sort of the holy grail of some of our work in terms of
590360	595480	thinking about, oh, how could we actually, you know, feel a whole entire dolphin and actually have it
595480	601160	be able to change shape between different surfaces. And so as one of my four of our PhD students,
602280	607320	Zach Hammond and a collaborator of ours, Nathan Yusevich, from Allison Otelmore's group,
607880	612600	we're trying to think about how might we make this vision a reality. And so we got inspired by
613320	618520	basically balloon animals to think about new ways that we might approach this. And the interesting
618520	623320	thing about balloon animals is that you have started out with something very simple, like
623400	629000	essentially inflated tube. And then we can pinch it at certain points, knot it together,
629000	634360	and basically create many different shapes out of the same, you know, simple balloon idea.
634360	640520	And so we tried to bring this idea to the field of soft robotics where we could have basically an
640520	646440	inflated beam. And as opposed to relying on pumps to move and actuate different areas,
646440	652520	what if we created a pinch point in that beam? And then what if we could move that pinch point around?
653320	657320	And then basically what that allows us to do is create some type of system that can really
657320	664040	dramatically change shape by putting a robot roller node on that surface and driving it around and
664040	670040	then creating what we call sort of this idea of a isoparametric type of robot where we have a fixed
670040	677160	length of inflated tube, but then we can change kind of the overall basically geometry, even though
677160	683480	it has a fixed topology. And so here is, you know, one element inside that, but we can place
683480	689080	these together through other kinematic connections to create some type of tetrahedral robot that we
689080	694200	can then basically a truss robot that we can then really dramatically change shape. And again,
694200	698920	we get these benefits of having this constant volume of air that we don't need to really pump
698920	704360	around, but instead using motors to move these pinch points or buckle points around. And so
704360	711080	here's an example of what those types of basically truss robots could look like. And so this is,
711080	716840	you know, a fairly large device about, you know, this size and it's able to pretty dramatically
716840	721720	change shape and move around. And this is all shown in real time. And so it can, you know,
721720	728120	locomote by basically doing some type of punctuated rolling, change its shape and move around. But
728120	736440	it can also go ahead and manipulate objects as well. So it can use the geometry of itself to
736440	740920	basically grasp an object and then be able to pick it up and actually do some interesting
740920	747000	in-hand manipulation as well. And so we think that this kind of idea of having these large
747000	752040	shape-changing truss robots has some interesting applications in terms of thinking about new
752040	757480	ways that we could be able to locomote, manipulate, and also use the shape change to afford different
757480	763000	types of interaction in the environment. And so we've also done some work to look at the
763000	768040	modeling and kinematic control and planning of this as well. So Zach and our group did a lot of
768040	772840	work on grasp optimization planning with these types of robot. And we have a collaboration
772840	777240	with Matt Schwager looking at decentralized control of these types of systems as well.
778360	784280	So I won't go into too many details in that area today, but I just wanted to give you kind of an
784280	788200	overview of some of the different things that we're working on in our group. But today I'm really
788200	794040	going to focus on these types of shape-changing or haptic displays. And I'm going to start out by
794040	799560	talking a little bit about the way in which we approach these problems. And really the way in
799560	804200	which we like to think about this in our group is really focusing very deeply on specific
804200	810280	applications and needs, working with domain experts in those areas, and then trying to learn more
810280	815160	about what are the enabling technologies or the requirements for the enabling technologies
815720	821240	to actually make these systems work. And then that kind of feeds back into other applications
821240	827000	and needs. And we think this is kind of a nice paradigm for working on, you know, basically
827000	832520	new technology to make these things possible. So I wanted to start by talking about two vignettes
832520	837400	of specific applications that we've been working on. And the first is in car design. And the second
837400	842200	is in making, making accessible to sort of highlight some of the challenges in making these
842200	848040	systems actually useful. And so many of you might be familiar with, you know, car design in general
848040	854120	and have seen these types of clay models that basically industrial designers and human factors
854120	862200	experts create to prototype and test car designs. And so we had been reached or we have been working
862200	869080	with Volkswagen who's trying to transition from making these clay models, which end up costing,
869080	874600	you know, something on the order of $100,000 per clay model. So it's very expensive and time consuming
874600	881640	to make those and moving instead to using virtual reality to prototype and test at steel, especially
881640	886440	in terms of working with different stakeholders that they care about, as well as the human factors
886520	892120	aspects of like the interior of the car as well. And so what they're trying to do is basically
892120	897000	transition from this, you know, very physical style of doing things to now move into the virtual
897000	902280	reality systems. But what they found is that, for example, in the case of the interior of the car
902280	908120	design, what they call the seating buck and the human machine interaction, basically, they'll be
908120	913480	able to load up different car designs and virtual reality. And then they have these very fancy seating
913480	919080	butts to adjust the position and height and steering wheel position to basically create any
919080	924680	different kind of car. But then you go and reach out and try to touch the HMI, the human machine
924680	929640	interface part of it, and the dashboard. And basically, you, you know, again, reach out and
929640	934920	touch nothing or touch air. And so one thing that they end up doing is creating these, you know,
934920	941320	basically machined or milled foam models that they can then place inside this seating buck
941320	945640	to then be able to go ahead and test. But it turns out it takes a long time for them to actually be
945640	951000	able to create these full models. They can't switch something, change things on, on demand, etc.
951000	956680	And so we had this idea of what if we could create, you know, interactive seating buck,
957640	962200	basically simulator, where we could create this, you know, basically surface that we could change
962200	968120	in real time to allow us to explore basically the different HMI interactions and some of the
968120	973720	ergonomic issues as well. So we started out working with them on this concept and trying to create a
973720	979320	new generation of our tactile displays to be able to do this. So this was led by Alexa Sue.
980120	984600	And so she created this, you know, basically, this is about the smallest that you can do with
984600	990920	kind of like low cost off the shelf DC actuators. So this has a direct drive between them with
990920	995640	the lead screw and, you know, many, many actuators and we made this modular so you can stack them
995640	1000920	together. And we also, you know, looked at the integration with this as well as in virtual
1000920	1006600	reality. And then we brought it back to our collaborators at Volkswagen, particularly like
1006600	1012280	the industrial designers and the, you know, human factor specialists. And they sort of said, Oh,
1012280	1017480	hey, this is great. We like the idea. But, you know, wouldn't it be great if these could be a lot
1017480	1023160	higher resolution and much cheaper? So, you know, in terms of the types of things that they're looking
1023160	1029240	for, they were not still there with this technology. And again, Oh, wouldn't it be nice if, you know,
1029240	1033800	we could create many of these, not just, you know, basically, this device here cost something like
1033800	1039720	$6,000 in parts to create. So wouldn't it be nice if we could, you know, basically have this be
1039720	1045480	higher resolution and, you know, basically much larger in scale and lower in cost. So those were
1045480	1051400	some of the feedback that we got, which sort of makes sense in some ways. But then I wanted to
1051400	1056040	highlight another application area that we've been thinking about that also informs some of these
1056040	1061720	other challenges. And so this is in the area of still computerated design and this idea of making,
1061720	1066920	making accessible. And so many of you are probably very familiar with the, the making movement or
1066920	1074760	maker movement that kind of has gone on over the past 15 or 20 years. And basically, you know,
1074760	1079480	one of the great things about that is that it's really empowered a lot of people and, and served
1079560	1085320	as a great way to involve more people in STEM education. But that doesn't mean that all people
1085320	1091800	are unable to do that. And particularly a lot of the tools that we use for making, especially in
1091800	1096440	terms of computerated design are not accessible to people that are blind or visually impaired.
1096440	1102280	And so those people have historically been excluded from those areas. And so while 3D printing can
1102280	1107960	be very helpful in terms of supporting accessible education through the use of creating tactile
1107960	1113400	graphics or other types of materials that blind people can touch and feel, there's really this
1113400	1118840	lack of authoring tools for blind people for them to be able to be the designers and engineers
1118840	1124440	themselves. And so the big problem with these, essentially with these systems that exist for
1124440	1130040	computerated design is that they're all based on basically graphical user interfaces, where you
1130040	1135560	have to directly manipulate with a mouse and a keyboard, as well as the computer screen to be
1135640	1141000	able to select and control many features. And that's really great for people that are cited, right?
1141000	1146840	We've moved beyond command line interfaces to this direct, direct manipulation type of interface.
1146840	1150440	But for someone that's blind and visually impaired, basically the main way that they
1150440	1155640	interact with computers is through different types of screen readers, which basically provide audio
1155640	1160600	feedback. And so how might we think about ways in which they could still be able to use this?
1160600	1168280	So there is some work on using text-based editors for creating geometries. So for example, OpenSCAD
1168280	1174840	is a computed solid geometry modeler where you write basically in a declarative programming
1174840	1179800	language to define the geometry. And there's been some great work at the Dimensions Project at the
1179800	1184760	New York Public Library, Chansey Fleet there, in terms of basically using that for people that are
1184760	1189240	blind or visually impaired to write code to then be able to 3D print something. But if you can
1189240	1193800	imagine and you've used a 3D printer, you know they're not particularly fast. And so basically
1193800	1199480	you write some code. And then while we, you know, people that are cited would be able to, you know,
1199480	1203240	instantly see the changes that we're making, someone that's blind and visually impaired would
1203240	1209960	have to basically use a 3D printer, wait somewhere between an hour to, you know, 10 hours to find
1209960	1214600	out, oh, the change that I made was that exactly what I wanted. And so one of the questions we've
1214600	1220280	been asking is, how might dynamic tactile feedback, you know, support this type of interaction? And so
1220280	1226200	again, my former PhD student, Alexa Su, as well as some members from the blind and visually impaired
1226200	1231160	community in the Bay Area, Sun Kim, who's an Access Technology Specialist at the Vista Center for
1231160	1237800	the Blind, as well as Josh Miele, who is an amazing blind engineer and researcher who's now at
1238600	1244440	at Amazon working on accessibility there, we all work together to create a co-design with
1244440	1249640	other blind makers, a tool that allows people to use these types of tactile shape displays we've
1249640	1254920	been talking about to in real time have that feedback. So here, basically you're able to write
1254920	1261160	code in open SCAD, and then in real time be able to touch and feel the geometry and sort of be able
1261160	1267080	to understand what it is. And so one of the questions that we had, and then 3D print the design.
1267080	1272040	So one of the questions we had is, you know, what are the types of interactions that we need to port
1272040	1277560	from these direct manipulation interfaces that are really essential for basically helping blind
1277560	1282120	and visually impaired people understand that. So we did a lot of different co-design sessions to try
1282120	1286680	to understand what these challenges are. And one of the things that's interesting, it turns out that
1286680	1291880	section views are even more important, you know, basically for blind and visually impaired people
1291880	1298440	than for in our, you know, sighted base CAD systems. Because, you know, essentially, you know,
1298440	1304280	there's no notion of transparency in these types of displays. You're basically just feeling the top
1304280	1309000	surface of the convex hole or whatever. So you can't have any way to display, at least with these
1309000	1315720	displays, this notion of transparency, which we rely on a lot. So we have some, you know, basically
1315720	1320600	promising interactions that we think are possible with this and compared to like a single point haptic
1321160	1327480	device. We might have less high resolution in terms of the spatial component. But by allowing
1327480	1332680	people to touch it with their whole hands, they're actually much more easily able to understand what
1332680	1338040	it is and the shape. So we think this is a promising direction. But if we look at the types of geometries
1338040	1342600	that the people we were working with are able to create with this type of tactile display,
1342600	1349160	we can see they're quite limited. And so this was a big issue that we found. But there is this real
1349240	1354200	benefit from having this tight coupling of the iteration. And so we think this is a promising
1354200	1360920	direction and very promising in terms of this notion of real time iterative feedback. And also
1360920	1365000	that people in the blind and visually impaired community really do want to be designers and
1365000	1370920	makers of their own, you know, basically technology. And this type of system can be very empowering
1370920	1377240	for them. But, you know, there's still these big issues in terms of resolution. So if we think
1377240	1382760	about a computer display that we might use being very high resolution versus this pin display that
1382760	1388440	I'm showing here, you know, there's a big gap in terms of the resolution and potentially the
1388440	1393880	under understandability of that geometry. And then also in terms of access. So if each of those
1393880	1399400	tactile displays that still low resolution costs upwards of $6,000, that means that, you know,
1399400	1405080	many people can't have access to it, as opposed to, you know, $500 laptop that we could use to
1405080	1410840	teach cat. So across these different application areas, we really saw these big challenges in terms
1410840	1416280	of cost, steel and resolution, as well as interaction techniques. But I'm not going to talk as much
1416280	1421000	about that today. And so a lot of the work that we've done in the past couple of years has been
1421000	1427240	trying to address or mitigate some of these issues of cost, steel and resolution. And we've kind of
1427240	1433480	taken two different approaches to try to tackle that on our group. The first is in trying to create
1433560	1440360	new and novel technical solutions, hardware solutions to solve that. And the second area is on
1440360	1446360	kind of what we call kind of perceptual illusions or using basically perceptual engineering to think
1446360	1452280	about how do we basically use the existing hardware that we have, but maybe use some clever
1452280	1457480	tricks in terms of how we integrate information together to improve the perceived resolution.
1457480	1462840	And so I'm going to talk about both of these areas. So the first area that I'll talk about
1462840	1468840	is on creating higher resolution tactile displays. And this has been kind of a big challenge in the
1468840	1473880	field of haptics for a long time. And there's sort of this big tradeoff or dichotomy between people
1473880	1479720	that are trying to make really high bandwidth tactile displays versus people that are trying to
1479720	1485080	make kind of these shape displays or tactile displays that maybe don't need to move as fast
1485080	1491080	or maybe don't need to move at all. And how do we find this balance between the two? And so one of
1491080	1496040	my former PhD students, Ty Zane, started to think about ways in which we could really try to push
1496040	1502040	the resolution if we trade off on that bandwidth side of things. And so if we think about this
1502040	1507960	design space or list of design requirements for like the ideal or ultimate tactile display,
1507960	1512440	there's a lot of different things that we might consider, one of which is like, what is the necessary
1512440	1518040	resolution? And so we can look to the haptics and psychophysics literature for some intuition
1518040	1524440	around that. Basically, if we think about just statically touching a shape, something like that,
1524440	1531240	we need to be in that one or 1.25 millimeter range for us to be able to not really be able to feel
1531240	1535960	individual pins. So you could think about this idea of the retina display that you might be
1535960	1541960	familiar with from Apple's marketing, where if you look at an iPhone today, you can't see where one
1541960	1547640	pixel ends and the next begins. Basically, this two point discrimination threshold is kind of
1548040	1553320	the same concept. And so we want to be in that one to one point two five millimeter range. But
1553320	1558840	that's I would say a very generous, you know, basically ballpark estimate, because actually
1558840	1564120	if you start to move your finger, then you can basically feel, you know, down to, you know,
1564120	1569480	tens of microns or below. So you can feel, you know, a single hair very easily. But that's relying
1569480	1576200	much more on your, you know, basically cutaneous information from vibration and essentially texture
1576200	1582760	information. But if we just think about gross shape, this 1.25 millimeters gets us pretty close.
1583560	1589000	So as I said, people have been trying to work on this problem for a long time. And there's a lot
1589000	1593480	of different approaches in our own work. We've used like, you know, basically mechanical linear
1593480	1599080	actuators and also different types of pneumatic actuators, which have again, challenges in terms
1599080	1605080	of scaling these down, as well as, you know, basically the high cost of the actuators and
1605080	1610680	being able to steal those together. Other people have created like electromagnetic tactile displays.
1610680	1615720	But in general, those have some challenges as you scale them down, because the magnetic,
1615720	1620520	electromagnetic fields start to bleed in with each other. So there was some interesting work from
1621320	1626760	Juan Zarate and Herbert Shea in terms of thinking about electromagnetic shielding for this, but
1626760	1631480	they're still kind of on the centimeter steel type of size or other people using
1631480	1636440	electroactive polymers. And again, all of these are really thinking about this idea of, you know,
1636440	1643320	how do we make a really fast or high bandwidth tactile display. And so our, the intuition
1643320	1648760	that Kai had was, okay, let's not focus on the high bandwidth aspect. But instead, what if we had
1648760	1654280	something that's much more like e-ink, where, oh, we're not changing it very frequently, we're
1654280	1660520	going to kind of refresh the whole, the whole thing. How do we instead have maybe clutches or
1660520	1665960	brakes that we could engage or lock when we need them to, but then have one global actuator that
1665960	1671800	might change everything. And so by trading off on this high bandwidth or the temporal domain,
1672360	1677240	or frequency domain, and instead focusing on the spatial, what might we be able to do?
1677240	1683240	And so the technology that we sort of came to in terms of a good, good trade off between
1683240	1689160	this sort of high force density in terms of braking or clutching, and then also the steel
1689240	1695080	ability is electrostatic adhesion. And so many of you are probably familiar with the electrostatic
1695080	1698280	fact where, you know, basically you have a balloon, you rub it on your hair,
1698280	1703720	and it sticks to your head. And so this has long been used in different industries, for example,
1703720	1711560	in like wafer chucking in the, in the semiconductor industry, and paper handling and other things.
1711560	1718280	And then over the past 15 or so years, it's really come into vogue in the robotics community as well.
1718280	1723320	So probably many of you are familiar with it, but, and also very commonly used in the MEMS
1723880	1729640	steel devices as well. And so basically we started to think about how do we make these millimeter
1729640	1735720	steel, you know, basically high force density clutches, and what are the different techniques
1735720	1740840	that we might need to, to use to do that. And we think that this is a promising technology for
1740840	1746840	this type of, you know, refreshable display. And so basically here we can see kind of an example
1746840	1752440	of one of these displays where we have essentially a dielectric thin film that we've then patterned
1752440	1757240	with these interdigitated electrodes that are on the order of, you know, basically a millimeter
1757240	1764680	across. And then we can basically turn on an electric field that then basically induces some
1764680	1771160	charge on these brass or different types of metal pins that, and then locks them into place.
1771720	1781720	What does this actually look like in action? Okay. Yeah. So basically we can raise up,
1781720	1787160	so it's a refreshable display. So essentially we, now we unlock all the pins, we raise them up,
1787160	1793880	and then as we move it down, we lock them into place. And so the electrostatic clutches can turn
1793880	1798840	on and off very quickly, giving us very high precision in terms of linear positioning,
1799480	1805080	and are relatively high force compared to their size. So again, the basic operating principle
1805080	1810520	is that we have this, you know, interdigitated electrode that's serving as this clutch that's
1810520	1815640	on one side of a high dielectric constant thin film. And then on the other side, there's a pin,
1815640	1822200	and basically we induce opposite charge on the pin. And that basically creates this electrostatic
1822200	1828040	force. And so kind of very simple model of how this electrostatic force works is very similar
1828120	1833400	to the parallel plate capacitor equation, where essentially, you know, the things that matter
1833400	1840360	are essentially the dielectric constant of the thin film, the contact area between the metal
1840360	1847800	pin and the dielectric film and the electrodes on the other side, the voltage as well as the film
1847800	1853240	thickness. And so historically in kind of robotics and other applications, people have really pushed
1853240	1859480	on the voltage as the thing to kind of improve the the actuation force. Here we have some
1859480	1864600	challenges in doing that because one, we're dealing with people and, you know, basically some of these
1864600	1870600	very high, you know, 10 kilovolt types of range, a very small amount of current can actually be
1870600	1876200	not very good for people. So that's one issue. And then the other issue is we want to have,
1876200	1881080	you know, not just one of these actuators, we want to have tens of thousands of these actuators.
1881080	1887080	And so how do we have, you know, very small and low cost transistors that allow us to, you know,
1887080	1892440	steal this in terms of production? And so we ended up trying to push more on the thin,
1892440	1897880	thinness and also the high dielectric constant as things that we could push on as opposed to
1897880	1903480	the voltage because of those two constraints. So this is kind of, you know, basically what the
1903480	1910040	actual device looks like when we fabricate these and using the UV laser cutter that Mark's lab
1910040	1915400	or actually Alison Okamura's lab. Well, I forget where it is now, but basically between Mark and
1915400	1924200	Alison, exactly. Shared facility, which is great. Basically, that allows us to, you know, very easily
1924200	1929160	fabricate these and test out different patterns. And so we use PVDF, which is a high dielectric
1929160	1934360	constant in film and a very small sheet of it. And then we have gold that's sputtered directly
1934360	1939640	on it or some other aluminum, for example. And then we laser a blade off the parts that we don't
1939640	1945320	want. So the actuation principle is very simple in terms of, you know, we have these clutches
1945320	1950760	and then we lower down the platform lock when the pins get to the right place. We turn on the
1950760	1955960	clutch and then we create the whole pattern. And then when we want to erase it or refresh it,
1955960	1961080	we just turn them off and then move the pin up and down. And so we can get, again, very high
1961080	1966680	spatial accuracy in terms of the, you know, basically the linear positioning of the height
1966680	1973000	because we can turn them, turn on and off the clutches and about, you know, on the order of
1973000	1980840	10 milliseconds or so, which allows us to have pretty high spatial resolution. We've also done a
1980840	1986760	lot of work on quasi-static loading as well. And unfortunately, the, you know, basically other
1986760	1991400	people as well. So for example, Steve Collins lab has done a lot of work on these electrostatic
1991480	1996920	types of clutches and also found that, you know, basically the back of the envelope calculations
1996920	2001320	don't really end up matching very well with performance because of the effective contact
2001320	2006760	area. And so we've done a lot of work also on data-driven modeling for this. But basically,
2006760	2012920	we think these clutches, you know, on the order of, you know, 50 to 100 grams of force for the
2012920	2017720	areas that we're looking at, which, you know, is not that much force. But if we think about
2017720	2022120	the contact force that's necessary as you're exploring something, that's on the order of
2022120	2028680	51 grams of force. And that would likely be spread across multiple pins as well. So we think we're
2028680	2033560	in the right, right ballpark for this. And my current student, Ahad, who's there now, is trying
2033560	2039000	to work on improving the performance and thinking about other things. So we've done some user testing
2039000	2045000	as well to explore how well these tactile displays work. And it seems like it's a promising direction
2045000	2050120	and seems to be working very well. So we think this, you know, basically these electrostatic
2050760	2057080	pin displays are kind of a promising approach to really pushing the resolution and low-cost
2057080	2062280	aspects of these types of refreshable tactile displays. So that's something that we're quite
2062280	2068760	excited about. And we've been able to achieve sort of this 1.5 or 1.7 millimeter pitch as well.
2070040	2073720	One of the challenges with these types of displays, though, is that they end up creating
2073720	2078440	these very discrete types of shapes, right? And so it's possible that, you know, we might want to
2078440	2084600	have more continuous shapes that could be approximated better with some other method. And also,
2084600	2091320	maybe there's a way to do that where we trade off, you know, basically, and basically are able to have,
2091320	2097880	you know, basically more continuous shapes with fewer number of actuators. And so Ahad, who recently
2097880	2104600	had a paper that was accepted to ICRA, that's on thinking about how might we make these basically
2104600	2109960	more continuous shape displays and what are techniques we could use to sort of have sort of a
2109960	2115400	monolithic manufacturing process where we can kind of create these in one go. And so Ahad has been
2115400	2122360	working on thinking about kind of the combination of electrostatic and electroadhesive actuation
2122440	2128840	with auxetic materials to be able to create basically the shape-changing continuous displays
2128840	2136120	where we're able to vary the curvature of them by locking individual cells in an auxetic grid
2136120	2141800	or an auxetic network. And so here's an example of that shown here. And so basically, the idea is
2141800	2148040	that we have, you know, work in auxetic skins where we can basically have these different patterns that
2148040	2155720	can expand basically differently based upon, you know, how much, you know, strain there is in the
2155720	2161880	system. And what we're doing is then locking some of these cells in this auxetic pattern. And what
2161880	2167480	that does is it means that there's going to be very different local strain concentrations that end up
2167480	2173160	as you inflate this surface being able to create different geometries. And so basically, there's
2173160	2178600	been exciting work in the computer graphics field as well as in other areas on basically being able
2178600	2185480	to computationally design these auxetic patterns so that you can then create some arbitrary geometry
2185480	2191880	that's beyond sort of like what a develop surface could be. And our kind of contribution here is
2191880	2198600	to think about, oh, as opposed to being able to, you know, essentially pre-plan and create a custom
2198600	2204280	auxetic pattern that could create some given shape, could we essentially create a smart skin
2204280	2211480	where we can change that local amount that each cell can open and close in real time or at run time.
2211480	2218200	And so basically, the way that we do that is by having each auxetic cell essentially be electrostatic
2218920	2224520	break or clutch that can, you know, basically either open or close, depending on how much
2224520	2229000	voltage we're applying across them. And so what this means is that we want to have essentially
2229000	2235400	an auxetic pattern with a very large surface area, because as I said before, the electrostatic force
2235400	2240760	is basically proportional to the amount of surface area that we have. And so essentially, you know,
2240760	2246200	we've looked at different patterns, but this one here, you know, again, has a very large surface area.
2246200	2252360	And so what we end up doing is taking two of these sheets and then rotating them off phase
2252440	2258760	so that basically, essentially, there's a lot of overlap between them. And we can essentially lock
2258760	2264280	which parts will expand and which parts work. So here you get an idea of what that single cell
2264280	2273560	might look like. And so we have two. Yeah, so here you can see these, these sets expanding and
2273560	2279240	contracting. And so essentially, we can as we pull on them or inflate them, they're opening up and
2279240	2285720	closing. And we can basically turn on the electrostatic adhesion to lock them and allow them to not
2285720	2291480	open up, which means that basically there's less displacement and less strain in the system.
2291480	2295880	So we've looked at different types of combinations of layers and different materials as well.
2296760	2301320	And are looking at, you know, basically, how do we then make this into this monolithic system? So
2301320	2306280	again, there's these two sheets that are on top of each other. And then basically, we create them
2306280	2310760	out of this flexible printed circuit board. And so that's really nice, because we can just go
2310760	2315880	ahead and fabricate that using off the shelf, you know, printed circuit board techniques.
2315880	2321640	And then we have one sheet that's in this orientation, then a dielectric thin film in between,
2321640	2328760	and then the other flex PCB that's on the on the next side as well. So then that's what this sort
2328760	2334920	of 2D service here looks like. And here we're engaging and locking between them. And you can see
2334920	2339160	some of them start to fail as well. That I guess that was the unlocked one here,
2339160	2344840	this region below is locked and above is unlocked, you can sort of see. And then here's the locked
2344840	2350840	region. And at a certain point, it'll start to fail as well. And so basically, we can, again,
2350840	2355240	computationally control which of those areas we want to be locked and unlocked. And that allows
2355240	2360200	us to create these different shapes. And then we have a inflated bladder that's underneath it that
2360200	2366680	we can then basically inflate. And that will then create this global shape change. And so here's
2366680	2373640	kind of an example of this 100% locked, which sort of creates this, you know, basically a very
2373640	2378520	uniform shape. And then now we're locking less and less of the display to create different
2378520	2383720	curvatures, if you can see like that. So we think again, this is kind of a promising approach. We're
2383800	2390280	still looking for basically, exotic patterns where we can have higher, you know, amount of strain and
2390280	2395160	more basically curvature that we can create then. But we think this again has some benefits in terms
2395160	2401320	of being able to really create and manufacture this very quickly. A related project that was kind
2401320	2406680	of in the early stages is on kind of connecting this idea of these continuous shape displays
2406680	2410280	with some of the work that I showed at the beginning on these shape changing robots,
2411000	2415800	where basically we want to have these elastic grid shells. So if you think about in computer
2415800	2420600	graphics, we often have like NERBs, surfaces, which are kind of combinations of these,
2421960	2426600	you know, different splines that are connected to each other. What if we do that in the real world?
2426600	2431880	And so basically, Sophia Weitzner and Wingsum Lawn, our group, are trying to create these basically
2432520	2437720	robotic elastic grid shells that can then again change their geometry in real time.
2437720	2442680	This is a small one by one prototype, but here you can see we locked one part of it and then
2442680	2448120	we're able to inject more material into it and create this curvature there. And so this is what
2448120	2454200	we're kind of aiming to do on the left is create these, you know, basically interconnected grid
2454200	2459480	shells that we can in real time change their surface. So these are some of the things we've
2459480	2463640	been doing in our group to think about, you know, how do we push forward and make these higher
2463640	2469000	resolution, you know, surface displays. And I think we've had some great promise in looking
2469000	2473880	at electrostatic adhesion, as well as kind of new approaches to making more continuous
2473880	2478600	surface displays. But I think it's really clear to us and probably you as well that the hardware
2478600	2484200	will really never perfectly render the real world, right? The real world is so rich and very
2484200	2490920	complex. And so I think there's this big gap between that. But the interesting thing is that
2491000	2496920	our perception is also imperfect as well. And so maybe we don't need to have perfect hardware
2496920	2501640	when we're considering these types of displays. And so the last part of my talk, I want to talk
2501640	2506600	about some of the work we're doing in terms of using visual haptic illusions to improve the
2506600	2511400	perceived performance of these types of tactile displays or other types of shape displays.
2512360	2517720	So the first is work from my former PhD student, Parastu Abtaiki, who is now starting at Princeton
2517720	2524120	University next year, that's looking at this same problem of how we might use the fact that our
2525560	2530200	basically our visual perception is often dominates our haptic perception. And so
2530200	2535320	one way to illustrate this is basically our proprioceptive system is not very good. And so
2535320	2541320	if I am able to basically touch my fingers together in front of my face, I'm actually often
2541320	2546040	using my visual system to really help me with that. But if I try to do it above my head,
2546040	2551160	I can sometimes get it, but you'll find that it's not as accurate. And so again, that's because of
2551160	2555960	all the kind of errors along the line in terms of, you know, our different joints and different
2555960	2561320	mechanical receptors that we have. But our proprioceptive system has more noise and air
2561320	2564920	than our visual system. And therefore, our, you know, brains, when we're thinking about
2564920	2570360	integrating this multi sensory integration, sorry, integrating this multi sensory information,
2570360	2575720	tend to rely on our visual system. And so as I mentioned before, there's many of these limitations
2575800	2581160	of these shape displays that I talked about in terms of low spatial resolution, limited display
2581160	2587640	size or low actuation speed. And the intuition or insight that Parastu had was, oh, how do we
2587640	2593000	leverage this fact that our proprioceptive system isn't very good to sort of increase the perceived
2593000	2600040	resolution. And this really kind of essentially builds on a technique from the field of virtual
2600040	2604200	reality that's called redirected touch. Some of you might be familiar with the idea of redirected
2604280	2609800	walking, where again, we can kind of steer people in virtual reality by having some slight offset
2609800	2615480	between where you are in the real world and where you see you are, where you see yourself or where
2615480	2621160	you see your hand in the virtual scene. And so what we can do is essentially apply some small
2621160	2627080	virtual offset as I'm reaching that basically makes it seem like my hand as I move straight
2627080	2633560	is moving to the left. And my basically visual motor system will basically compensate for that
2633560	2639000	and make my hand move to the right in the real world to basically compensate for that
2639000	2644760	bias or shift to the left. And so we can basically computationally steer where a person's hand is
2644760	2651080	going by applying these offsets in the virtual world. And by leveraging that redirected touch
2651080	2656120	effect, we're able to address some of these different aspects of low spatial resolution
2656120	2661320	and low actuation speed. And so I'll talk about some of the ways in which we're doing that here
2662280	2668920	using angle redirection, scaling up and vertical redirection as well. So you might be familiar
2668920	2673320	with this idea of anti aliasing or the aliasing effect that happens when we look at
2673960	2678920	graphical display. This aliasing effect is actually also very pronounced in these tactile
2678920	2684120	displays as well. And so one way that we were trying to mitigate this low resolution is by
2684120	2690280	trying to essentially get rid of this aliasing that happens when you display a vertical or,
2690280	2697240	sorry, a diagonal line. And so if we see here, as we move along this surface here,
2697960	2702280	you know, basically we have this kind of, again, aliasing effect that you can feel these bumps
2702280	2709480	as well. And so what if instead we could redirect you so that you're moving along a straight line,
2709480	2714200	which is very doesn't have that aliasing problem. But in the virtual world, you think you're moving
2714200	2720120	along this diagonal line. And so again, by applying this slight offset between where your hand is
2720120	2725400	in the real world and where it is in the virtual scene, we can make you touch different areas of
2725400	2732200	the display and not be able to realize it or perceive it. In terms of the overall resolution
2732200	2737400	of the device, we can think about the number of pixels or taxles that you're in contact with as
2737400	2742920	you move over a surface. And so one of the challenges is if we have a small object that
2742920	2747400	we're rendering on a tactile display of a fixed resolution, then you're not going to encounter
2747400	2753640	that many taxles as you move along the surface. So again, what if we could change this essentially
2753640	2760120	control the display ratio, or utilize this offset between where my real hand is and my virtual hand
2760120	2767080	is. And in the virtual scene, render a small object, but in the real world, render a larger
2767080	2772600	object of the same version, and we can render it in higher resolution. So again, we can change the
2772600	2777960	ratio between where I'm interacting in the real world, where I'm interacting in the virtual world,
2777960	2783160	and leverage that to improve the perceived resolution. And we've done this also for things
2783160	2789000	like extending the height of the display, the workspace limitations of it, and other things
2789000	2794840	like that as well. So here, you can see as I'm moving up, I have some fixed amount of range
2794840	2800600	that I can move up. And then here, we're able to, again, offset where you think your hand is
2800680	2806440	in the virtual scene. And so you can think that basically the tactile display has
2807080	2814360	larger pins than we can actually create with these displays. We've run a lot of psychophysical
2814360	2822120	studies to find what these thresholds are, and also find that when we display an offset
2822120	2827560	underneath that threshold, essentially, people perceive it as being higher resolution without
2827560	2832360	noticing that it's there. But we've also found some interesting effects in the difference between
2832360	2837720	active versus passive touch, where in the active touch conditions, that's where you're
2837720	2844920	moving your hand as well. So for example, in that angle redirection, we can't offset people's hands
2844920	2850280	as much as we can in the passive touch condition, where for example, the motor is moving your hand,
2851000	2854280	where we can offset people more. And so again, that has to do with kind of the
2855240	2860520	you know, forward model that people have in their sensory motor system in terms of, you know,
2860520	2866040	basically, my predictions of where my hand might be. And so I'm more willing to allow
2866040	2872360	things to have noise and there to be more air when I'm being moved versus I'm moving myself
2872360	2877000	as well. So we've looked at ways in which we can leverage this to create different types of
2877000	2882520	applications such as, you know, improving the resolution or, or, you know, again, increasing
2882600	2888840	the vertical redirection as well. But as you can see from those types of systems, you know,
2888840	2894520	these illusions really only work in this small area of the display. And so another set of work
2894520	2899720	that we've been doing is trying to improve the kind of scalability of how we might apply these
2899720	2904680	to a much larger area and create essentially what are called encountered type haptic displays that
2904680	2910200	might operate over a larger region. So we put these types of haptic displays, tactile displays,
2910200	2915960	on, you know, robotic arm. And then when we reach out, the robotic device can be there in time.
2915960	2920440	But one of the challenges with this type of approach of encountered type haptic devices is
2920440	2927480	that oftentimes the device has some limitations as well, right? So the device may arrive late or
2927480	2932520	it might be out of the workspace of that robotic system. So I want to touch something up here,
2932520	2936920	but the robot is only down there. And so again, there's a number of challenges with these
2936920	2942360	encountered type haptic devices in terms of different reachability issues that lead to these
2942360	2947320	uncertain spatial discrepancies, which really kind of affect people's perceived performance
2947320	2953560	of these devices. So we've done a few things in this area to improve the essentially the ability
2953560	2959800	for these devices to work by essentially redirecting your hand to the reachable area of the robotic
2959800	2971000	system. Great. Okay, great. So anyway, basically, we can do some cool things in terms of this,
2971000	2975240	you know, redirection to guide people into different people's space. And we've also done
2975240	2982280	some interesting work on trying to apply basically model predictive control to basically run this
2982280	2987160	in real time to improve the perceived performance using a model of human reaching and sensory
2987160	2995560	integration. So I think that I'll conclude there with just one short statement, if I didn't find it,
2996280	3002920	which is that, you know, we started out this work really thinking about how real does haptics need
3002920	3008520	to be. And I think where our group is going is really trying to think more about how real
3008520	3014200	does haptics need to seem and really trying to leverage a sensory motor control perspective
3014200	3019480	to optimize both the hardware and the software together. So I'd like to thank, you know, my
3019480	3023960	PhD students and postdocs in my lab that contributed to this work, as well as our funding sources.
3023960	3030440	And I guess I'd be happy to answer any questions. Yeah, I thought it was till 130. So my apologies
3030440	3036760	on that mark. Yeah, so thanks so much.
