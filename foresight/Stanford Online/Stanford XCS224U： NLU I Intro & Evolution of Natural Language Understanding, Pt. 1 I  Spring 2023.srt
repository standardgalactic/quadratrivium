1
00:00:00,000 --> 00:00:07,120
Welcome everyone, this is natural language understanding.

2
00:00:07,120 --> 00:00:11,440
It is a weird and wonderful and maybe worrying moment

3
00:00:11,440 --> 00:00:13,920
to be doing natural language understanding.

4
00:00:13,920 --> 00:00:18,240
My goal for today is just to kind of immerse us in this moment

5
00:00:18,240 --> 00:00:23,120
and think about how we got here and what it's like to be doing research now.

6
00:00:23,120 --> 00:00:27,360
And I think that'll set us up well to think about what we're going to do in the course,

7
00:00:27,360 --> 00:00:32,960
and how that's going to set you up to participate in this moment in AI

8
00:00:32,960 --> 00:00:35,840
in many ways, in whichever ways you choose.

9
00:00:35,840 --> 00:00:38,480
And it's an especially impactful moment to be doing that.

10
00:00:38,480 --> 00:00:40,640
And this is a project oriented course.

11
00:00:40,640 --> 00:00:43,920
And I feel like we can get you all to the point where you are doing

12
00:00:43,920 --> 00:00:47,600
meaningful things that contribute to this ongoing moment

13
00:00:47,600 --> 00:00:50,560
in ways that are going to be exciting and impactful.

14
00:00:50,560 --> 00:00:53,360
That is the fundamental goal of the course.

15
00:00:53,360 --> 00:00:56,560
Let's now think about the current model of the course.

16
00:00:56,560 --> 00:00:57,920
Let's now think about the current moment.

17
00:00:57,920 --> 00:01:00,640
This is always a moment of reflection for me.

18
00:01:00,640 --> 00:01:06,880
I started teaching this course in 2012, which I guess is ages ago now.

19
00:01:06,880 --> 00:01:08,800
It feels recent in my lived experience,

20
00:01:08,800 --> 00:01:11,840
but it does feel like ages ago in terms of the content.

21
00:01:11,840 --> 00:01:15,600
In 2012, on the first day, I had a slide that looked like this.

22
00:01:15,600 --> 00:01:20,800
I said it was an exciting time to be doing natural language understanding research.

23
00:01:20,800 --> 00:01:24,000
I noted that there was a resurgence of interest in the area

24
00:01:24,000 --> 00:01:28,960
after a long period of people mainly focused on syntax and things like that.

25
00:01:28,960 --> 00:01:33,840
But there was a widespread perception that NLE was poised for a breakthrough

26
00:01:33,840 --> 00:01:37,360
and to have huge impact that was relating to business things

27
00:01:37,360 --> 00:01:40,400
and that there was a white-hot job market for Stanford grads.

28
00:01:40,400 --> 00:01:43,760
A lot of this language is coming from the fact that we were in this moment

29
00:01:43,760 --> 00:01:48,720
when Siri had just launched, Watson had just won on Jeopardy,

30
00:01:48,720 --> 00:01:51,760
and we had all of these in-home devices and all the tech giants

31
00:01:51,760 --> 00:01:56,320
of competing on what was emerging as the field of natural language understanding.

32
00:01:57,200 --> 00:01:59,120
Let's fast forward to 2022.

33
00:01:59,120 --> 00:02:01,920
I did feel like I should update that in 2022

34
00:02:01,920 --> 00:02:04,160
by saying this is the most exciting moment ever,

35
00:02:04,160 --> 00:02:06,320
as opposed to just being an exciting time.

36
00:02:06,880 --> 00:02:09,200
But I emphasize the same things.

37
00:02:09,200 --> 00:02:13,600
We were in this feeling that we had experienced a resurgence of interest

38
00:02:13,600 --> 00:02:17,280
in the area, although now it was hyper-intensified.

39
00:02:17,280 --> 00:02:18,480
Same thing with industry.

40
00:02:18,480 --> 00:02:24,000
The industry interest at this point makes the stuff from 2012 look like small potatoes.

41
00:02:24,960 --> 00:02:29,600
Systems were getting very impressive, but, and I maintain this here,

42
00:02:29,600 --> 00:02:32,080
they show their weaknesses very quickly,

43
00:02:32,080 --> 00:02:35,520
and the core things about NLE remain far from solved.

44
00:02:35,520 --> 00:02:38,000
So the big breakthroughs lie in the future.

45
00:02:38,000 --> 00:02:40,320
I will say that even since 2022,

46
00:02:40,320 --> 00:02:43,040
it has felt like there has been an acceleration

47
00:02:43,040 --> 00:02:48,400
and some problems that we used to focus on feel kind of like they're less pressing.

48
00:02:48,400 --> 00:02:51,920
I won't say solved, but they feel like we've made a lot of progress on them

49
00:02:51,920 --> 00:02:54,480
as a result of models getting better.

50
00:02:54,480 --> 00:02:59,120
But all that means for me is that there are more exciting things in the future

51
00:02:59,120 --> 00:03:01,600
that we can tackle even more ambitious things.

52
00:03:01,600 --> 00:03:04,160
And you'll see that I've tried to overhaul the course

53
00:03:04,160 --> 00:03:08,240
to be ever more ambitious about the kind of problems that we might take on.

54
00:03:09,200 --> 00:03:12,400
But we do kind of live in a golden age for all of this stuff.

55
00:03:12,400 --> 00:03:17,200
And even in 2022, I'm not sure what I would have predicted to say nothing of 2012,

56
00:03:17,200 --> 00:03:20,400
that we would have these incredible models like Dali2,

57
00:03:20,400 --> 00:03:23,680
which can take you from text into these incredible images,

58
00:03:23,680 --> 00:03:28,000
language models, which will more or less be the star of the quarter for us,

59
00:03:28,000 --> 00:03:31,920
but also models that can take you from natural language to code.

60
00:03:31,920 --> 00:03:35,680
And of course, we are all seeing right now as we speak

61
00:03:36,480 --> 00:03:43,200
that the entire industry related to web search is being reshaped around NLU technologies.

62
00:03:43,760 --> 00:03:48,560
So whereas this felt like a kind of niche area of NLP,

63
00:03:48,560 --> 00:03:51,040
when we started this course in 2012,

64
00:03:51,040 --> 00:03:54,080
now it feels like the entire field of NLP,

65
00:03:54,080 --> 00:03:57,200
certainly in some aspects all of AI,

66
00:03:57,200 --> 00:04:00,240
is focused on these questions of natural language understanding,

67
00:04:00,240 --> 00:04:02,000
which is exciting for us.

68
00:04:02,960 --> 00:04:04,800
One more moment of reflection here.

69
00:04:04,800 --> 00:04:07,600
You know, in this course throughout the years,

70
00:04:07,600 --> 00:04:12,560
we have used simple examples to kind of highlight the weaknesses of current models.

71
00:04:12,560 --> 00:04:16,320
And so a classic one for us was simply this question,

72
00:04:16,320 --> 00:04:19,280
which US states border no US states?

73
00:04:19,280 --> 00:04:22,320
And the idea here is that it's a simple question,

74
00:04:22,320 --> 00:04:24,880
but it can be hard for our language technologies

75
00:04:24,880 --> 00:04:27,120
because of that negation, the no there.

76
00:04:28,080 --> 00:04:31,840
In 1980, there was a famous system called Chat 80.

77
00:04:31,840 --> 00:04:37,520
It was a symbolic system representing the first major phase of research in NLP.

78
00:04:37,520 --> 00:04:40,080
You can see the fragment of the system here.

79
00:04:40,080 --> 00:04:44,240
And Chat 80 was an incredible system in that it could answer questions like,

80
00:04:44,240 --> 00:04:48,640
which country bordering the Mediterranean borders a country that is bordered by a country

81
00:04:48,640 --> 00:04:51,440
whose population exceeds the population of India?

82
00:04:51,440 --> 00:04:54,320
I've given you the answer here, Turkey,

83
00:04:55,120 --> 00:04:58,480
at least according to 1980s geography.

84
00:04:59,040 --> 00:05:01,840
But if you asked Chat 80 a simple question like,

85
00:05:01,840 --> 00:05:03,920
which US states border no US states,

86
00:05:03,920 --> 00:05:06,640
it would just say, I don't understand.

87
00:05:06,640 --> 00:05:10,800
It was an incredibly expressive system, but rigid.

88
00:05:10,800 --> 00:05:13,120
It could do some things very deeply,

89
00:05:13,120 --> 00:05:14,880
as you see from the first question,

90
00:05:14,880 --> 00:05:17,360
but things that fell outside of its capacity,

91
00:05:17,360 --> 00:05:18,880
it would just fall down flat.

92
00:05:20,240 --> 00:05:21,440
That was the 1980s.

93
00:05:21,440 --> 00:05:22,400
Let's fast forward.

94
00:05:22,400 --> 00:05:25,040
2009, around the time this course launched,

95
00:05:25,040 --> 00:05:26,960
Wolfram Alphra hit the scene.

96
00:05:26,960 --> 00:05:30,800
And this was meant to be a kind of revolutionary language technology.

97
00:05:30,800 --> 00:05:32,320
The website is still up.

98
00:05:32,320 --> 00:05:36,560
And to my amazement, it still gives the following behavior.

99
00:05:36,560 --> 00:05:40,080
If you search for which US states border no US states,

100
00:05:40,080 --> 00:05:42,800
it kind of just gives you a list of the US states.

101
00:05:43,440 --> 00:05:48,720
Revealing, I would say, that it has no capacity to understand the question posed.

102
00:05:49,680 --> 00:05:50,800
That was 2009.

103
00:05:50,800 --> 00:05:53,440
So we've gone from 1980 to 2009.

104
00:05:54,000 --> 00:05:55,440
Okay, let's go to 2020.

105
00:05:55,440 --> 00:05:58,800
This is the first of the open AI models, ADA,

106
00:05:59,360 --> 00:06:01,520
which US states border no US states?

107
00:06:02,080 --> 00:06:03,200
The answer is no.

108
00:06:03,840 --> 00:06:05,600
And then it sort of starts to babble.

109
00:06:05,600 --> 00:06:07,600
The US border is not a state border.

110
00:06:07,600 --> 00:06:09,280
It did that for a very long time.

111
00:06:10,320 --> 00:06:12,960
But about Babbage, this is still 2020.

112
00:06:13,840 --> 00:06:16,000
The US states border no US states.

113
00:06:16,000 --> 00:06:17,520
What is the name of the US state?

114
00:06:17,520 --> 00:06:19,920
And then it really went off the deep end from there.

115
00:06:19,920 --> 00:06:22,240
Again, for a very long time, that was Babbage.

116
00:06:22,240 --> 00:06:25,600
If you had seen this output, well, at least for me,

117
00:06:25,600 --> 00:06:29,360
it might have shaken my faith that this was a viable approach.

118
00:06:30,480 --> 00:06:32,000
But the team persisted, I guess.

119
00:06:32,000 --> 00:06:34,240
2021, this is the Curie model.

120
00:06:34,240 --> 00:06:36,400
Which US states border no US states?

121
00:06:37,120 --> 00:06:39,440
It had a problem that it started listing things,

122
00:06:39,440 --> 00:06:42,720
but it did say Alaska, Hawaii, and Puerto Rico,

123
00:06:42,720 --> 00:06:46,800
which is an interestingly more impressive answer than the first answer.

124
00:06:47,600 --> 00:06:50,560
It still has some problem understanding what it means to respond,

125
00:06:50,560 --> 00:06:52,960
but it's looking like we're seeing some signal.

126
00:06:54,160 --> 00:06:55,600
Da Vinci instruct beta.

127
00:06:55,600 --> 00:06:56,800
This is 2022.

128
00:06:56,800 --> 00:06:59,520
It's important, I think, that this is the first of the models

129
00:06:59,520 --> 00:07:01,040
that have been struck in the name.

130
00:07:01,040 --> 00:07:02,800
We'll talk about that in a minute.

131
00:07:02,800 --> 00:07:04,720
Which US states border no US states?

132
00:07:04,720 --> 00:07:06,320
Alaska and Hawaii.

133
00:07:06,320 --> 00:07:11,440
We've from 2020 to 2022, we have seen this astounding leap forward,

134
00:07:11,440 --> 00:07:14,560
making everything before then sort of pale in comparison.

135
00:07:14,560 --> 00:07:19,280
And then finally, text Da Vinci won one of the new best in class models

136
00:07:19,280 --> 00:07:21,040
at least until two months ago.

137
00:07:21,040 --> 00:07:23,120
Which US states border no US states?

138
00:07:23,120 --> 00:07:26,880
Alaska and Hawaii are the only US states that border no other US states.

139
00:07:26,880 --> 00:07:29,520
A very impressive answer indeed.

140
00:07:29,520 --> 00:07:32,000
And if you just think about the little history I've given,

141
00:07:32,720 --> 00:07:36,080
a kind of microcosm of what is happening in the field,

142
00:07:36,960 --> 00:07:42,080
a lot of time without much progress with some hype attached.

143
00:07:42,160 --> 00:07:45,920
And now in the last few years, this kind of rapid progress forward.

144
00:07:47,360 --> 00:07:51,280
And you know, that's just one example, but these examples multiply

145
00:07:51,280 --> 00:07:52,320
and we can quantify this.

146
00:07:52,320 --> 00:07:54,080
Here's another impressive case.

147
00:07:54,080 --> 00:07:59,040
I asked the Da Vinci 2 model in which year was Stanford University founded?

148
00:07:59,040 --> 00:08:00,960
When did it enroll its first students?

149
00:08:00,960 --> 00:08:03,680
Who is its current president and what is its mascot?

150
00:08:03,680 --> 00:08:05,760
A complicated question indeed.

151
00:08:05,760 --> 00:08:10,400
And it gave a fluent and factually correct answer on all counts.

152
00:08:11,360 --> 00:08:15,600
This is the Da Vinci 3 model, which was best in class until a few weeks ago.

153
00:08:16,160 --> 00:08:19,200
And it gave exactly the same answer, very impressive.

154
00:08:20,560 --> 00:08:23,600
Now in this course, and you'll see at the website,

155
00:08:23,600 --> 00:08:26,800
one of the readings we've suggested for the start of the course

156
00:08:26,800 --> 00:08:31,200
is this classic paper by Hector Levec called On Our Best Behavior.

157
00:08:31,200 --> 00:08:34,960
And the thrust of this article essentially channeling Terry Winograd

158
00:08:34,960 --> 00:08:37,200
and Terry Winograd's schema.

159
00:08:38,000 --> 00:08:40,880
The idea is that we should come up with examples

160
00:08:40,880 --> 00:08:44,000
that will test whether models deeply understand.

161
00:08:44,000 --> 00:08:47,600
And in particular, get past the kind of simple memorization

162
00:08:47,600 --> 00:08:50,560
of statistics and other things about the data they're trained on

163
00:08:50,560 --> 00:08:54,160
and really probe to see whether they understand what the world is like.

164
00:08:54,160 --> 00:08:57,680
And Levec and Winograd's technique for doing this

165
00:08:57,680 --> 00:09:01,120
is to pose very unlikely questions

166
00:09:01,120 --> 00:09:03,440
where humans have very natural answers.

167
00:09:03,440 --> 00:09:05,600
Like one of the ones Levec poses is,

168
00:09:05,600 --> 00:09:07,840
could a crocodile run the steeple chase?

169
00:09:08,960 --> 00:09:11,280
Maybe it's a question you've never thought about before,

170
00:09:11,280 --> 00:09:14,960
but you probably have a pretty consistent answer across this group.

171
00:09:14,960 --> 00:09:16,880
Could a crocodile run the steeple chase?

172
00:09:17,520 --> 00:09:19,600
Here I asked another one of Levec's questions.

173
00:09:19,600 --> 00:09:24,320
Are professional baseball players allowed to glue small wings onto their caps?

174
00:09:24,320 --> 00:09:26,160
You could think about that for a second.

175
00:09:26,160 --> 00:09:30,880
The Da Vinci 2 model said there is no rule against it, but it is not common.

176
00:09:30,880 --> 00:09:35,040
And that seemed like a very good answer to me at the time.

177
00:09:35,040 --> 00:09:39,200
When the Da Vinci 3 engine came out, though, this started to worry me.

178
00:09:39,200 --> 00:09:43,680
No, professional baseball players are not allowed to glue small wings onto their caps.

179
00:09:43,680 --> 00:09:45,520
Major League Baseball has strict rules

180
00:09:45,520 --> 00:09:48,080
about the appearance of players' uniforms and caps.

181
00:09:48,880 --> 00:09:51,600
And any modifications to the caps are not allowed.

182
00:09:52,480 --> 00:09:55,120
Okay, I thought I was feeling good about this,

183
00:09:55,120 --> 00:09:58,160
but now I don't even myself know what the answer is.

184
00:09:58,160 --> 00:10:01,440
Are professional baseball players allowed to glue small wings onto their caps?

185
00:10:01,440 --> 00:10:04,800
We have two confident answers that are contradictory

186
00:10:05,600 --> 00:10:08,880
across two models that are very closely related.

187
00:10:08,880 --> 00:10:11,680
It's starting to worry us a little bit, I hope.

188
00:10:11,680 --> 00:10:14,080
But still, it's impressive.

189
00:10:14,080 --> 00:10:14,560
What's that?

190
00:10:14,560 --> 00:10:15,840
You want me to ask a part?

191
00:10:15,840 --> 00:10:16,720
Yes.

192
00:10:16,720 --> 00:10:17,520
You could check.

193
00:10:17,520 --> 00:10:21,920
Yes, I have a few cases, and this is an interesting experiment for us to run for sure.

194
00:10:21,920 --> 00:10:24,160
Let me show you the responses I got a bit later.

195
00:10:24,160 --> 00:10:29,040
The point, though, I guess, if you've seen the movie Blade Runner,

196
00:10:29,040 --> 00:10:33,840
this is starting to feel like to figure out whether an agent we were interacting with

197
00:10:33,840 --> 00:10:36,000
was human or AI.

198
00:10:36,000 --> 00:10:41,120
We would need to get very sophisticated interview techniques indeed.

199
00:10:41,120 --> 00:10:43,920
The Turing test long forgotten here.

200
00:10:43,920 --> 00:10:49,520
Now we're into the mode of trying to figure out exactly what kind of agents we're interacting with

201
00:10:49,520 --> 00:10:54,160
by having to be extremely clever about the kinds of things that we do with them.

202
00:10:55,920 --> 00:10:58,560
Now, that's kind of anecdotal evidence,

203
00:10:58,560 --> 00:11:03,840
but I think that the picture of progress is also supported by what's happening in the field.

204
00:11:03,840 --> 00:11:06,800
Let me start this story with our benchmarks.

205
00:11:06,800 --> 00:11:10,000
And the headline here is that our benchmarks, the tasks,

206
00:11:10,000 --> 00:11:15,200
the data sets we use to probe our models are saturating faster than ever before.

207
00:11:15,200 --> 00:11:17,760
And I'll articulate what I mean by saturate.

208
00:11:17,760 --> 00:11:19,760
So we here have a little framework.

209
00:11:19,760 --> 00:11:24,640
Along the x-axis, I have time stretching back into, like, the 1990s.

210
00:11:24,640 --> 00:11:31,280
And along the y-axis, I have a normalized measure of distance from what we call human performance.

211
00:11:31,280 --> 00:11:33,840
That's the red line set at zero.

212
00:11:33,840 --> 00:11:37,040
Each one of these benchmarks has, in its own particular way,

213
00:11:37,040 --> 00:11:39,680
set a so-called estimate of human performance.

214
00:11:39,680 --> 00:11:41,760
I think we should be cynical about that.

215
00:11:41,760 --> 00:11:45,360
But nonetheless, this will be a kind of marker of progress for us.

216
00:11:45,760 --> 00:11:47,760
First data set, MNIST.

217
00:11:47,760 --> 00:11:50,800
This is like digit recognition, famous task in AI.

218
00:11:50,800 --> 00:11:56,000
It was launched in the 1990s, and it took about 20 years for us to see a system

219
00:11:56,000 --> 00:12:00,000
that surpassed human performance in this very loose sense.

220
00:12:00,880 --> 00:12:04,400
The switchboard corpus, this is going from speech to text.

221
00:12:04,400 --> 00:12:08,800
It's a very similar story, launched in the 90s, and it took about 20 years

222
00:12:08,800 --> 00:12:11,120
for us to see a superhuman system.

223
00:12:11,120 --> 00:12:14,880
ImageNet, this was launched, I believe, in 2009,

224
00:12:14,880 --> 00:12:20,400
and it took less than 10 years for us to see a system that surpassed that red line.

225
00:12:20,400 --> 00:12:23,280
And now progress is going to pick up really fast.

226
00:12:23,280 --> 00:12:28,320
Squad 1.1, the Stanford question answering data set, was launched in 2016,

227
00:12:28,320 --> 00:12:32,560
and it took about three years for it to be saturated in this sense.

228
00:12:32,560 --> 00:12:37,760
Squad 2.0 was the team's attempt to pose an even harder problem,

229
00:12:37,840 --> 00:12:41,840
one where there were unanswerable questions, but it took even less time

230
00:12:41,840 --> 00:12:44,640
for systems to get past that red line.

231
00:12:45,200 --> 00:12:47,200
Then we get the glue benchmark.

232
00:12:47,200 --> 00:12:52,480
This is a famous benchmark in natural language understanding, a multi-task benchmark.

233
00:12:52,480 --> 00:12:57,680
When this was launched, a lot of us thought that glue would be too difficult

234
00:12:57,680 --> 00:12:58,800
for present-day systems.

235
00:12:58,800 --> 00:13:02,720
It looked like this might be a challenge that would stand for a very long time,

236
00:13:02,720 --> 00:13:05,200
but it took like less than a year.

237
00:13:05,280 --> 00:13:10,560
But it took like less than a year for systems to pass human performance.

238
00:13:10,560 --> 00:13:15,920
The response was superglue, but it was saturated, if anything, even more quickly.

239
00:13:16,960 --> 00:13:20,800
Now we can be as cynical as we want about this notion of human performance,

240
00:13:20,800 --> 00:13:24,320
and I think we should dwell on whether or not it's fair to call it that.

241
00:13:24,320 --> 00:13:30,960
But even setting that aside, this looks like undeniably a story of progress.

242
00:13:31,040 --> 00:13:37,120
The systems that we had in 2012 would not even have been able to enter the glue benchmark

243
00:13:37,120 --> 00:13:39,760
to say nothing of achieving scores like this.

244
00:13:39,760 --> 00:13:41,920
So something meaningful has happened.

245
00:13:41,920 --> 00:13:46,320
Now you might think by the standards of AI, these data sets are kind of old.

246
00:13:46,320 --> 00:13:51,600
Here's a post from Jason Wei, where he evaluated our latest and greatest large language models

247
00:13:51,600 --> 00:13:55,360
on a bunch of mostly new tasks that were actually designed

248
00:13:55,360 --> 00:13:59,680
to stress test this new class of very large language models.

249
00:13:59,680 --> 00:14:03,280
And Jason's observation is that we see emergent abilities

250
00:14:03,280 --> 00:14:08,240
across more than 100 tasks for these models, especially for our largest models.

251
00:14:08,240 --> 00:14:13,120
The point, though, is that we again thought these tasks would stand for a very long time,

252
00:14:13,120 --> 00:14:17,840
and what we're seeing instead is that one by one systems are certainly getting traction

253
00:14:17,840 --> 00:14:22,080
and in some cases performing at the standard we had set for humans.

254
00:14:22,800 --> 00:14:26,160
Again, an incredible story of progress there.

255
00:14:26,320 --> 00:14:29,920
So I hope that is energizing, maybe a little intimidating,

256
00:14:29,920 --> 00:14:32,480
but I hope fundamentally energizing for you all.

257
00:14:33,840 --> 00:14:38,320
The next question that I want to ask for you is just what is going on?

258
00:14:38,320 --> 00:14:41,440
What is driving all of this sudden progress?

259
00:14:41,440 --> 00:14:46,320
Let's get a feel for that, and that'll kind of serve as the foundation for the course itself.

260
00:14:46,880 --> 00:14:52,000
Before I do that, though, are there questions or comments, things I could resolve,

261
00:14:52,720 --> 00:14:56,080
or things I left out about the current moment?

262
00:15:00,240 --> 00:15:01,760
We're running on bar to think very well.

263
00:15:02,800 --> 00:15:08,160
We should reflect, though, maybe as a group about what it means to do very well.

264
00:15:08,160 --> 00:15:11,200
My question for you, when you say it did well,

265
00:15:11,200 --> 00:15:15,280
what is the Major League Baseball rule about players gluing things onto their caps?

266
00:15:17,200 --> 00:15:18,480
You found the actual rule.

267
00:15:18,480 --> 00:15:21,680
No, this is what, well, I don't...

268
00:15:22,480 --> 00:15:23,280
I didn't find the rule.

269
00:15:23,280 --> 00:15:24,880
Bard found that rule and gave me that number.

270
00:15:24,880 --> 00:15:25,280
Okay.

271
00:15:26,400 --> 00:15:28,640
Yes, that is going to be the question for us.

272
00:15:32,640 --> 00:15:36,960
I'm going to show you the open AI models will offer me links, but the links go nowhere.

273
00:15:40,160 --> 00:15:43,760
What you're pointing out, I think, is an increasing societal problem.

274
00:15:43,760 --> 00:15:47,120
These models are offering us what looks like evidence,

275
00:15:47,120 --> 00:15:49,760
but a lot of the evidence is just fabricated.

276
00:15:49,760 --> 00:15:52,560
And this is worse than offering no evidence at all.

277
00:15:52,560 --> 00:15:56,160
What I really need is someone who knows Major League Baseball to tell me,

278
00:15:56,160 --> 00:15:59,200
what is the rule about players and their caps?

279
00:15:59,760 --> 00:16:03,360
I want it from an expert human, not an expert language model.

280
00:16:05,840 --> 00:16:06,240
What's that?

281
00:16:06,240 --> 00:16:07,040
Can we Google?

282
00:16:08,080 --> 00:16:09,440
Be careful how you Google, though.

283
00:16:09,440 --> 00:16:11,440
I guess that's the lesson of 2023.

284
00:16:14,960 --> 00:16:16,480
All right, what's going on?

285
00:16:16,480 --> 00:16:18,480
Let's start to make some progress on this.

286
00:16:18,560 --> 00:16:21,920
Again, first, a little bit of historical context.

287
00:16:21,920 --> 00:16:26,080
I've got a timeline going back to the 1960s along the x-axis.

288
00:16:26,080 --> 00:16:28,480
This is more or less the start of the field itself.

289
00:16:29,040 --> 00:16:35,920
And in that early era, essentially all of the approaches were based in symbolic algorithms,

290
00:16:35,920 --> 00:16:37,760
like the chat 81 that I showed you.

291
00:16:37,760 --> 00:16:41,200
In fact, that was kind of pioneered here at Stanford by people

292
00:16:41,200 --> 00:16:43,920
who were pioneering the very field of AI.

293
00:16:43,920 --> 00:16:47,680
And that paradigm of essentially programming these systems

294
00:16:47,680 --> 00:16:49,920
lasted well into the 1980s.

295
00:16:52,000 --> 00:16:56,960
In the 90s, early 2000s, we get the statistical revolution

296
00:16:56,960 --> 00:17:01,440
throughout artificial intelligence and then in turn in natural language processing.

297
00:17:01,440 --> 00:17:06,080
And the big change there is that instead of programming systems with all these rules,

298
00:17:06,080 --> 00:17:10,320
we're going to design machine learning systems that are going to try to learn from data.

299
00:17:10,320 --> 00:17:13,040
Under the hood, there was still a lot of programming involved

300
00:17:13,040 --> 00:17:15,600
because we would write a lot of feature functions

301
00:17:15,600 --> 00:17:19,200
that were little programs that would help us detect things about data.

302
00:17:19,200 --> 00:17:22,240
And we would hope that our machine learning systems could learn

303
00:17:22,240 --> 00:17:24,560
from the output of those feature functions.

304
00:17:24,560 --> 00:17:29,520
But in the end, this was the rise of the fully data-driven learning systems.

305
00:17:29,520 --> 00:17:34,560
And we just hope that some process of optimization leads us to new capabilities.

306
00:17:35,920 --> 00:17:39,200
The next big phase of this was the deep learning revolution.

307
00:17:39,280 --> 00:17:42,400
This happened starting around 2009, 2010.

308
00:17:42,400 --> 00:17:45,440
Again, Stanford was at the forefront of this, to be sure.

309
00:17:46,240 --> 00:17:49,520
It felt like a big change at the time, but in retrospect,

310
00:17:49,520 --> 00:17:52,160
this is kind of not so different from this mode here.

311
00:17:52,160 --> 00:17:58,160
It's just that we now replace that simple model with really big models,

312
00:17:58,160 --> 00:18:02,640
really deep models that have a tremendous capacity to learn things from data.

313
00:18:03,280 --> 00:18:08,480
We started also to see a shift even further away from those feature functions,

314
00:18:08,480 --> 00:18:10,160
from writing little programs.

315
00:18:10,160 --> 00:18:14,160
And more toward a more mode where we would just hope that the data

316
00:18:14,160 --> 00:18:17,520
and the optimization process could do all the work for us.

317
00:18:19,520 --> 00:18:25,120
Then the next big thing that happened, which I could take us I suppose until about 2018,

318
00:18:25,120 --> 00:18:28,240
would be this mode where we have a lot of pre-trained parameters.

319
00:18:28,240 --> 00:18:32,800
These are pictures of maybe big language models or computer vision models or something.

320
00:18:32,800 --> 00:18:36,800
And when we build systems, we build on those pre-trained components

321
00:18:36,880 --> 00:18:40,960
and stitch them together with these task-specific parameters.

322
00:18:40,960 --> 00:18:46,240
And we hope that when they're all combined and we do some learning on some task-specific data,

323
00:18:46,240 --> 00:18:49,520
we have something that's benefiting from all these pre-trained components.

324
00:18:51,120 --> 00:18:55,920
And then the mode that we seem to be in now that I want us to reflect critically on

325
00:18:56,480 --> 00:19:00,000
is this mode where we're going to replace everything with maybe one

326
00:19:00,560 --> 00:19:05,040
ginormous language model of some kind and hope that that thing,

327
00:19:05,040 --> 00:19:08,640
that enormous black box will do all the work for us.

328
00:19:08,640 --> 00:19:11,920
We should think critically about whether that's really the path forward,

329
00:19:11,920 --> 00:19:15,120
but it certainly feels like the zeitgeist to be sure.

330
00:19:16,160 --> 00:19:16,800
Question, yeah?

331
00:19:17,760 --> 00:19:20,000
If you think it's worth it, could you go back to the last slide

332
00:19:20,960 --> 00:19:25,360
and maybe explain a little bit, a more browner example of what that all means?

333
00:19:25,360 --> 00:19:26,320
I couldn't quite follow.

334
00:19:27,040 --> 00:19:28,480
Let's do that later.

335
00:19:28,480 --> 00:19:34,400
The point for now though is really this shift from here where we're mostly learning from

336
00:19:34,480 --> 00:19:39,680
scratch for our task, here we've got things like BERT in the mix.

337
00:19:39,680 --> 00:19:45,840
We've got pre-trained components, models that we hope begin in a state that gives us a leg up

338
00:19:45,840 --> 00:19:47,680
on the problem we're trying to solve.

339
00:19:47,680 --> 00:19:49,040
That's the big thing that happened.

340
00:19:49,040 --> 00:19:53,760
And you get this emphasis on people releasing model parameters.

341
00:19:53,760 --> 00:19:58,800
In this earlier phase like here, there was no talk of releasing model parameters

342
00:19:58,800 --> 00:20:03,520
because mostly the models people trained were just good for the task that they had set.

343
00:20:04,160 --> 00:20:09,440
As we move into this era and then certainly this one, these things are meant to be like

344
00:20:09,440 --> 00:20:14,800
general purpose language capabilities or maybe general purpose computer vision capabilities

345
00:20:14,800 --> 00:20:20,240
that we stitch together into a system that can do more than any previous system could do.

346
00:20:23,920 --> 00:20:25,680
Right, so then we have this big thing here.

347
00:20:26,640 --> 00:20:32,880
So that's the feeling now behind all of this certainly beginning in this final phase here

348
00:20:32,880 --> 00:20:35,040
is the transformer architecture.

349
00:20:35,040 --> 00:20:36,720
Just may take the temperature of the room.

350
00:20:36,720 --> 00:20:39,120
How many people have encountered the transformer before?

351
00:20:40,560 --> 00:20:43,600
Right, yeah, it's sort of unavoidable if you're doing this research.

352
00:20:43,600 --> 00:20:48,240
Here's a diagram of it, but I'm not going to go through this diagram now because

353
00:20:48,240 --> 00:20:53,840
starting on Wednesday, we are going to have an entire lecture essentially devoted to unpacking

354
00:20:53,840 --> 00:20:56,000
this thing and understanding it.

355
00:20:56,000 --> 00:21:01,840
All I can say for you now is that I expect you to go on the following journey, which all of us go on.

356
00:21:02,480 --> 00:21:04,880
How on earth does the transformer work?

357
00:21:04,880 --> 00:21:06,880
It looks very, very complicated.

358
00:21:07,440 --> 00:21:12,800
I hope can get you to the point where you feel, oh, this is actually pretty simple components

359
00:21:12,800 --> 00:21:15,920
that have been combined in a pretty straightforward way.

360
00:21:15,920 --> 00:21:17,520
That's your second step on the journey.

361
00:21:17,520 --> 00:21:22,480
The one, the true enlightenment comes from, wait a second, why does this work at all?

362
00:21:23,360 --> 00:21:28,000
And then you're with the entire field trying to understand why these simple things

363
00:21:28,000 --> 00:21:30,720
were brought together in this way have proved so powerful.

364
00:21:32,800 --> 00:21:38,400
The other major thing that happened, which is kind of latent going all the way back to the

365
00:21:38,400 --> 00:21:44,320
start of AI, especially as it relates to linguistics, is this notion of self-supervision,

366
00:21:44,320 --> 00:21:49,680
of distributional learning, because this is going to unlock the door to us just learning

367
00:21:49,680 --> 00:21:52,080
from the world in the most general sense.

368
00:21:52,960 --> 00:21:59,840
In self-supervision, your model's only goal is to learn from co-occurrence patterns in the

369
00:21:59,840 --> 00:22:01,440
sequences that it's trained on.

370
00:22:01,440 --> 00:22:04,960
And the sequences can be language, but they could be language plus

371
00:22:04,960 --> 00:22:10,080
sensor readings, computer code, maybe even images that you embed in this space,

372
00:22:10,080 --> 00:22:11,200
just symbols.

373
00:22:11,200 --> 00:22:15,520
And the model's only goal is to learn from the distributional patterns that they contain.

374
00:22:16,640 --> 00:22:20,000
Or for many of these models to assign high probability

375
00:22:20,000 --> 00:22:23,440
to the attested sequences in whatever data that you pour in.

376
00:22:24,160 --> 00:22:27,120
For this kind of learning, we don't need to do any labeling.

377
00:22:27,840 --> 00:22:32,160
All we need to do is have lots and lots of symbol streams.

378
00:22:33,600 --> 00:22:36,800
And then when we generate from these models, we're sampling from them.

379
00:22:36,800 --> 00:22:40,080
And that's what we all think of when we think of prompting and getting a response back.

380
00:22:40,080 --> 00:22:45,200
But the underlying mechanism is, at least in part, this notion of self-supervision.

381
00:22:45,200 --> 00:22:49,040
And I'll emphasize again, because I think this is really important for why these models are so

382
00:22:49,040 --> 00:22:52,640
powerful, the symbols do not need to be just language.

383
00:22:52,640 --> 00:22:57,840
They can include lots of other things that might help a model piece together,

384
00:22:57,840 --> 00:23:02,640
a full picture of the world we live in, and also the connections between language and

385
00:23:02,640 --> 00:23:06,160
those pieces of the world, just from this distributional learning.

386
00:23:07,680 --> 00:23:13,520
The result of this proving so powerful is the advent of large-scale pre-training.

387
00:23:13,520 --> 00:23:18,240
Because now we're not held back anymore by the need for labeled data.

388
00:23:18,240 --> 00:23:21,600
All we need is lots of data in unstructured format.

389
00:23:22,240 --> 00:23:27,840
This really begins in the era of static word representations like word-to-vec and glove.

390
00:23:28,640 --> 00:23:31,840
And in fact, those teams, and I would say especially the glove team,

391
00:23:31,840 --> 00:23:38,800
they were really visionary in the sense that they not only released a paper and code,

392
00:23:39,360 --> 00:23:41,600
but pre-trained parameters.

393
00:23:41,600 --> 00:23:45,760
This was really brand new for the field, this idea that you would empower people

394
00:23:46,320 --> 00:23:48,640
with model artifacts.

395
00:23:48,640 --> 00:23:54,480
And people started using them as the inputs to recurrent neural networks and other things.

396
00:23:54,480 --> 00:24:01,040
And you started to see pre-training as an important component to doing really well at hard things.

397
00:24:03,200 --> 00:24:06,160
There were some predecessors that I'll talk about next time.

398
00:24:06,160 --> 00:24:11,200
But the really big moment for contextual representations is the ELMO model.

399
00:24:11,200 --> 00:24:14,240
This is the paper, Deep Contextualized Word Representations.

400
00:24:14,320 --> 00:24:20,480
I can remember being at the North American ACL meeting in New Orleans in 2018

401
00:24:20,480 --> 00:24:22,320
at the best paper session.

402
00:24:22,320 --> 00:24:27,200
They had not announced which of the best papers was going to win the Outstanding Paper Award.

403
00:24:27,200 --> 00:24:33,360
But we all knew it was going to be the ELMO paper because the gains that they had reported

404
00:24:33,360 --> 00:24:38,640
from fine-tuning their ELMO parameters on hard tasks or the field were just mind-blowing.

405
00:24:38,640 --> 00:24:43,520
The sort of thing that you really only see once in a kind of generation of this research.

406
00:24:43,520 --> 00:24:44,800
Or so we thought.

407
00:24:44,800 --> 00:24:51,760
Because the next year, Burt came out, same thing, I think same best paper award thing.

408
00:24:51,760 --> 00:24:56,000
The paper already had had huge impact by the time it was even published.

409
00:24:57,120 --> 00:25:00,160
And they too released their model parameters.

410
00:25:00,160 --> 00:25:02,240
ELMO is not transformer-based.

411
00:25:02,240 --> 00:25:05,680
Burt is the first of the sequence of things that's based in the transformer.

412
00:25:05,680 --> 00:25:09,760
And again, lifting all boats even above where ELMO had brought us.

413
00:25:10,640 --> 00:25:12,240
Then we get GPT.

414
00:25:12,240 --> 00:25:14,160
This is the first GPT paper.

415
00:25:14,160 --> 00:25:17,120
And then fast forward a little bit, we get GPT-3.

416
00:25:17,120 --> 00:25:24,240
And that was pre-training at a scale that was previously kind of unimaginable.

417
00:25:24,240 --> 00:25:28,720
Because now we're talking about, you know, for the Burt model, 100 million parameters

418
00:25:28,720 --> 00:25:31,920
and for GPT-3, well north of 100 billion.

419
00:25:32,880 --> 00:25:34,800
Different order of magnitude.

420
00:25:34,800 --> 00:25:37,840
And what we started to see is emergent capabilities.

421
00:25:39,440 --> 00:25:41,120
That model size thing is important.

422
00:25:41,760 --> 00:25:45,360
Again, this is a sort of feeling of progress and maybe also despair.

423
00:25:45,360 --> 00:25:50,160
I think I can lift your spirits a little bit, but we should think about model size.

424
00:25:50,160 --> 00:25:52,640
So I have years along the x-axis again.

425
00:25:53,280 --> 00:25:58,800
And I have model size going from 100 million to 1 trillion here on a logarithmic scale.

426
00:25:59,520 --> 00:26:03,600
So 2018 GPT, that's like 100 million Burt.

427
00:26:03,600 --> 00:26:06,240
I think it's 300 million for the large one.

428
00:26:06,240 --> 00:26:08,160
Okay, GPT-2 even larger.

429
00:26:08,960 --> 00:26:10,640
Megatron 8.3 billion.

430
00:26:10,640 --> 00:26:13,920
I remember when this came out, I probably laughed.

431
00:26:13,920 --> 00:26:15,200
Maybe I thought it was a joke.

432
00:26:15,200 --> 00:26:19,760
I certainly thought it was some kind of typo because I couldn't imagine that it was actually

433
00:26:19,760 --> 00:26:21,440
billion like with a B there.

434
00:26:23,120 --> 00:26:26,160
But now that's, you know, we take that for granted.

435
00:26:26,160 --> 00:26:30,240
Megatron 11 billion, this is 20, 21 or so.

436
00:26:30,240 --> 00:26:34,880
Then we get GPT-3 reportedly at 175 billion parameters.

437
00:26:34,880 --> 00:26:38,240
And then we get this thing where it seems like we're doing typos again.

438
00:26:38,240 --> 00:26:45,040
Megatron Turing NLG was like 500, and then Palm is 540 billion parameters.

439
00:26:45,680 --> 00:26:50,640
And I guess there are rumors that we have gone upward all the way to a trillion, right?

440
00:26:51,440 --> 00:26:53,680
There's an undeniable trend here.

441
00:26:54,240 --> 00:26:59,120
I think there is something to this trend, but we should reflect on it a little bit.

442
00:26:59,120 --> 00:27:03,040
One thing I want to say is there's a noteworthy pattern.

443
00:27:03,040 --> 00:27:09,440
A very few entities have participated in this very large, in this race for very large models.

444
00:27:09,440 --> 00:27:13,520
We've got like Google, NVIDIA, Meta and OpenAI, right?

445
00:27:14,400 --> 00:27:16,640
And that was actually a real cause for concern.

446
00:27:16,640 --> 00:27:22,160
I remember being at a workshop between Stanford and OpenAI where the number one source of

447
00:27:22,160 --> 00:27:28,960
consternation was really that only OpenAI at that point had trained these really large models.

448
00:27:28,960 --> 00:27:32,880
And after that, predictably, these other large tech companies kind of caught up.

449
00:27:33,600 --> 00:27:38,320
But it was still for a while looking like a story of real centralization of power.

450
00:27:39,680 --> 00:27:42,400
That might still be happening, but I think there's a reason to be optimistic.

451
00:27:42,400 --> 00:27:47,200
So here at Stanford, the Helm Group, which is part of the Center for Research on Foundation

452
00:27:47,200 --> 00:27:52,560
Models, led this incredibly ambitious project of evaluating lots of language models.

453
00:27:52,560 --> 00:27:57,360
And one thing that emerges from that is that we have a more healthy ecosystem now.

454
00:27:57,360 --> 00:28:01,440
So we have these like loose collectives, big science and a Luther are both kind of fully

455
00:28:01,440 --> 00:28:07,600
open source groups of researchers. We've got, well, one academic institution represented.

456
00:28:07,600 --> 00:28:10,640
This could be a little bit embarrassing for Stanford, maybe we'll correct that.

457
00:28:11,280 --> 00:28:14,880
And then maybe the more important thing is that we have lots of startups represented.

458
00:28:14,880 --> 00:28:20,800
So these are well-funded but relatively small outfits that are producing outstanding language

459
00:28:20,800 --> 00:28:24,720
models. And so the result, I think we're going to see much more of this.

460
00:28:24,800 --> 00:28:28,400
And then we'll worry less about centralization of power.

461
00:28:28,400 --> 00:28:31,840
There's plenty of other things to worry about, so we shouldn't get sanguine about this.

462
00:28:31,840 --> 00:28:36,160
But this particular point, I think, is being alleviated by current trends.

463
00:28:36,720 --> 00:28:42,080
And there's another aspect of this too, which is you have the scary rise in model size.

464
00:28:42,080 --> 00:28:48,080
But what is happening right now as we speak in a very quick way is we're seeing a push

465
00:28:48,080 --> 00:28:53,360
towards smaller models. And in particular, we're seeing that models that are in the range of like

466
00:28:53,360 --> 00:28:58,240
10 billion parameters can be highly performant, right? So we have the Flan models.

467
00:28:59,280 --> 00:29:03,360
We have Lama. And then here at Stanford, they released the Alpaca thing.

468
00:29:03,360 --> 00:29:07,840
And then Databricks released Hello Dolly model. These are all models that are like

469
00:29:07,840 --> 00:29:12,960
8 to 10 billion parameters, which I know this sounds funny because I laughed a few years ago

470
00:29:12,960 --> 00:29:18,000
when the Megatron model had 8.3 billion. And now what I'm saying to you is that this is relatively

471
00:29:18,080 --> 00:29:24,640
small, but so it goes. And the point is that a 10 billion parameter model is one that could be run

472
00:29:24,640 --> 00:29:30,640
on regular old commercial hardware. Whereas these monsters up here, really, you have lots of

473
00:29:30,640 --> 00:29:35,600
pressures towards centralization of power there because almost no one can work with them.

474
00:29:35,600 --> 00:29:40,000
But anyone essentially can work with Alpaca. And it won't be long before we've got

475
00:29:40,000 --> 00:29:43,840
the ability to kind of work with it on small devices and things like that.

476
00:29:43,840 --> 00:29:48,880
And that, too, is really going to open the door to lots of innovation. I think that will

477
00:29:48,880 --> 00:29:53,680
bring some good. And I think it will bring some bad, but it is certainly a meaningful change

478
00:29:53,680 --> 00:29:57,760
from this scary trend that we were seeing until four months ago.

479
00:30:02,080 --> 00:30:08,160
As a result of these models being so powerful, people started to realize that you can get a

480
00:30:08,160 --> 00:30:14,080
lot of mileage out of them simply by prompting them. When you prompt one of these very large

481
00:30:14,080 --> 00:30:19,120
models, you put it in a temporary state by inputting some text, and then you generate a

482
00:30:19,120 --> 00:30:23,200
sample from the model using some technique and you see what comes out, right? So if you type

483
00:30:23,200 --> 00:30:29,920
into one of these models, better late than, it's going to probably spit out never. If you put in

484
00:30:29,920 --> 00:30:35,600
every day, I eat breakfast, lunch, and it will probably say dinner. And you might have an intuition

485
00:30:35,600 --> 00:30:40,320
that the reasons, the causes for that are kind of different. The first one is a sort of idiom,

486
00:30:40,320 --> 00:30:46,080
so that it could just learn from co-occurrence patterns in text transparently. For the second one,

487
00:30:46,080 --> 00:30:51,680
we kind of interpreted as humans as reflecting something about routines, but you should remind

488
00:30:51,680 --> 00:30:57,760
yourself that the mechanism is the same as in the first case. This was just a bunch of co-occurrence

489
00:30:57,760 --> 00:31:03,840
patterns. A lot of people described their routines in text and the model picked up on that. And carry

490
00:31:03,920 --> 00:31:08,720
that thought forward as you think about things like the president of the U.S. is. When it fills

491
00:31:08,720 --> 00:31:15,040
that in with Biden or whoever, it might look like it is offering us factual knowledge and maybe in

492
00:31:15,040 --> 00:31:20,960
some sense it is, but it's the same mechanism as for those first two examples. It is just learning

493
00:31:20,960 --> 00:31:26,160
from the fact that a lot of people have expressed a lot of text that look like the president of the

494
00:31:26,160 --> 00:31:32,640
U.S. is Joe Biden, and it is repeating that back to us. And so definitely, if you ask a model,

495
00:31:32,640 --> 00:31:38,720
something like the key to happiness is you should remember that this is just the aggregate of a lot

496
00:31:38,720 --> 00:31:44,800
of data that it was trained on. It has no particular wisdom to offer you necessarily beyond what was

497
00:31:44,800 --> 00:31:54,800
encoded latently in that giant sea of mostly unaudited unstructured text. Yeah, question.

498
00:31:56,000 --> 00:32:00,400
I guess, you know, it would be kind of hard to get something like this, but if we had like a corpus

499
00:32:00,400 --> 00:32:05,040
of just like, you know, although language is right, but literally all of the facts were wrong.

500
00:32:05,040 --> 00:32:10,800
Like, we just imagined like a very factually incorrect corpus. Like, I guess I'm getting

501
00:32:10,800 --> 00:32:18,480
how, like, how do we inject like truth into like these corpuses? It's a question that bears

502
00:32:18,480 --> 00:32:24,880
repeating. How do we inject truth? It's a question you all could think about. What is truth, of course.

503
00:32:24,960 --> 00:32:31,040
But also, what would that mean? And how would we achieve it? And even if we did back off to something

504
00:32:31,040 --> 00:32:36,560
like, how would we ensure self-consistency for a model? Or, you know, at the level of a worldview

505
00:32:36,560 --> 00:32:42,560
or a set of facts, even those questions which seem easier to pose are incredibly difficult

506
00:32:42,560 --> 00:32:47,600
questions in the current moment, where our only mechanisms are basically that self-supervision

507
00:32:47,600 --> 00:32:53,200
thing that I described, and then a little bit of what I'll talk about next. But none of the

508
00:32:53,200 --> 00:32:57,520
structure that we used to have where we would have a database of knowledge and things like that,

509
00:32:58,240 --> 00:32:59,600
that is posing problems.

510
00:33:05,200 --> 00:33:10,160
The prompting thing, we take this a step forward, right? So the GPT-3 paper, remember that's that

511
00:33:10,160 --> 00:33:17,120
175 billion parameter monster. The eye-opening thing about that is what we now call in-context

512
00:33:17,120 --> 00:33:22,400
learning, which was just the notion that for these very large, very capable models, you could

513
00:33:22,400 --> 00:33:28,320
input a bunch of text, like here's a passage, and maybe an example of the kind of behavior that

514
00:33:28,320 --> 00:33:34,080
you wanted, and then your actual question, and the model would do a pretty good job at answering

515
00:33:34,080 --> 00:33:39,360
the question. And what you're doing here is, with your context passage and your demonstration,

516
00:33:39,360 --> 00:33:45,360
pushing the model to be extractive, to find an answer to its question in the context passage,

517
00:33:46,080 --> 00:33:51,280
and then the observation of this paper is that they do a pretty good job at following that same

518
00:33:51,280 --> 00:33:57,360
behavior for the actual target question at the bottom here. Remember, this is all just prompting,

519
00:33:57,360 --> 00:34:02,320
putting the model in a temporary state, and seeing what comes out. You don't change the model,

520
00:34:02,320 --> 00:34:09,040
you just prompt it. This, in 2012, if you had asked me whether this was a viable path forward

521
00:34:09,040 --> 00:34:15,120
for a class project, I want to prompt a RNN or something. I would have advised you as best I

522
00:34:15,120 --> 00:34:20,240
could to choose some other topic, because I never would have guessed that this would work. So the

523
00:34:20,240 --> 00:34:26,720
mind-blowing thing about this paper and everything that's followed is that we might be nearing the

524
00:34:26,720 --> 00:34:33,760
point where we can design entire AI systems on the basis of this simple in-context learning mechanism,

525
00:34:33,760 --> 00:34:40,160
transformatively different from anything that we saw before. In fact, let me just emphasize this

526
00:34:40,160 --> 00:34:46,160
a little bit. It is worth dwelling on how strange this is. For those of you who have been in the

527
00:34:46,160 --> 00:34:52,240
field a little while, just contrast what I described in-context learning with the standard

528
00:34:52,240 --> 00:34:59,360
mode of supervision. Let's imagine for a case here that we want to train a model to detect

529
00:34:59,360 --> 00:35:05,280
nervous anticipation, and I have picked this because this is a very particular human emotion.

530
00:35:05,280 --> 00:35:10,240
And in the old mode, we would need an entire dedicated model to this, right? We would collect

531
00:35:10,240 --> 00:35:16,480
a little data set of positive and negative instances of nervous anticipation, and we would

532
00:35:16,480 --> 00:35:22,320
train a supervised classifier on feature representations of these examples over here,

533
00:35:22,320 --> 00:35:28,320
learning from this binary distinction. We would need custom data and a custom model

534
00:35:28,320 --> 00:35:35,440
for this particular task in all likelihood. In this new mode, few shot in-context learning,

535
00:35:35,440 --> 00:35:40,720
we essentially just prompt the model, hey, model, here's an example of nervous anticipation. My

536
00:35:40,720 --> 00:35:45,680
palms started to sweat as the Lotto numbers were read off. Hey, model, here's an example without

537
00:35:45,680 --> 00:35:52,880
nervous anticipation and so forth. And it learns from all those symbols that you put in and their

538
00:35:52,880 --> 00:36:00,320
co-occurrences something about nervous anticipation. On the left for this model here, I've written

539
00:36:00,320 --> 00:36:05,520
out nervous anticipation, but remember that has no special status. I've structured the model

540
00:36:05,520 --> 00:36:10,800
around the binary distinction, the one and the zero. And everything about the model is geared

541
00:36:10,800 --> 00:36:17,120
toward my learning goal. On the right, nervous anticipation is just more of the symbols that

542
00:36:17,120 --> 00:36:23,200
I've put into the model. And the eye-opening thing, again, about the GPT-3 paper and what's

543
00:36:23,200 --> 00:36:29,840
followed is that models can learn, be put in a temporary state and do well at tasks like this.

544
00:36:31,280 --> 00:36:38,720
Now, I talked about self-supervision before, and I think that is a major component to the success

545
00:36:38,720 --> 00:36:44,240
of these models. But it is increasingly clear that it is not the only thing that is driving

546
00:36:44,240 --> 00:36:50,960
learning in the best models in this class. The other thing that we should think about

547
00:36:50,960 --> 00:36:56,240
is what's called reinforcement learning with human feedback. This is a diagram from the

548
00:36:56,240 --> 00:37:02,000
chat GPT blog post. There are a lot of details here, but really two of them are important for us

549
00:37:02,000 --> 00:37:10,720
for right now. The first is that in a phase of training these models, people are given inputs

550
00:37:10,720 --> 00:37:16,960
and ask themselves to produce good outputs for those inputs. So you might be asked to do a little

551
00:37:16,960 --> 00:37:22,240
Python program, and you yourself as an annotator might write that Python program, for example.

552
00:37:22,240 --> 00:37:26,080
So that's highly skilled work that depends on a lot of human intelligence,

553
00:37:26,640 --> 00:37:32,560
and those examples, those pairs are part of how the model is trained. And that is so important

554
00:37:32,560 --> 00:37:37,840
because that takes us way beyond just learning from co-occurrence patterns of symbols and text.

555
00:37:38,400 --> 00:37:45,440
It is now back to a very familiar story from all of AI, which is that it's not magic. What is

556
00:37:45,440 --> 00:37:50,880
happening is that a lot of human intelligence is driving the behavior of these systems.

557
00:37:51,840 --> 00:37:56,640
And that happens again at step two here. So now the model produces different outputs,

558
00:37:56,640 --> 00:38:02,560
and humans come in and rank those outputs, again expressing direct human preferences

559
00:38:02,560 --> 00:38:08,240
that take us well beyond self-supervision. So we should remember, we had that brief moment where

560
00:38:08,240 --> 00:38:13,200
it looked like it was all unstructured unlabeled data, and that was important to unlocking these

561
00:38:13,200 --> 00:38:19,680
capacities, but now we are back at a very labor-intensive human capacity here, driving

562
00:38:20,480 --> 00:38:23,600
what looked like the really important behaviors for these models.

563
00:38:27,120 --> 00:38:31,520
Final step, which I think actually intimately relates to that instruct tuning that I just

564
00:38:31,520 --> 00:38:35,520
described. That's a kind of way of summarizing this reinforcement learning with human feedback.

565
00:38:36,240 --> 00:38:40,480
And this is what's called step by step or chain of thought reasoning. Now we're thinking about

566
00:38:40,480 --> 00:38:45,440
the prompts that we use for these models. So suppose we asked ourselves a question like,

567
00:38:45,440 --> 00:38:51,280
can models reason about negation? To give an example, does the model know that if the customer

568
00:38:51,280 --> 00:38:55,920
doesn't have any auto loan, sorry, doesn't have any loans, then the customer doesn't have any

569
00:38:55,920 --> 00:39:00,400
auto loans? It's a simple example. It's the sort of reasoning that you might have to do if you're

570
00:39:00,400 --> 00:39:05,760
thinking about a contract or something like that, whether a rule has been followed. And it just

571
00:39:05,760 --> 00:39:12,160
involves negation, our old friend from the start of the lecture. Now in the old school

572
00:39:12,160 --> 00:39:18,960
prompting style, all the way back in 2021, we would kind of naively just input, is it true that

573
00:39:18,960 --> 00:39:23,280
if the customer doesn't have any loans, then the customer doesn't have any auto loans into one of

574
00:39:23,280 --> 00:39:28,240
these models, and we would see what came back. And here it says, no, this is not necessarily

575
00:39:28,240 --> 00:39:32,800
true. A customer can have auto loans without having any other loans, which is the reverse

576
00:39:32,800 --> 00:39:38,240
of the question that I asked. Again, kind of showing it doesn't deeply understand what we've

577
00:39:38,240 --> 00:39:44,880
put in here. It just kind of does an act that looks like it did. And that's worrisome. But

578
00:39:44,880 --> 00:39:48,720
we're learning how to communicate with these very alien creatures. Now we do what's called

579
00:39:48,720 --> 00:39:53,200
step-by-step prompting. This is the cutting edge thing. You would just tell the model that it was

580
00:39:53,200 --> 00:39:58,800
in some kind of logical or common sense reasoning exam that matters to the model. Then you could

581
00:39:58,800 --> 00:40:03,840
give some instructions, and then you could give an example in your prompts of the kind of thing

582
00:40:03,840 --> 00:40:09,120
it was going to see. And then finally you could prompt it with your premise and then your question.

583
00:40:09,680 --> 00:40:14,240
And the model would spit out something that looked really good. Here I won't bother going

584
00:40:14,240 --> 00:40:20,880
through the details, but with that kind of prompt, the model now not only answers and reasons

585
00:40:20,880 --> 00:40:27,360
correctly, but also offers a really nice explanation of its own reasoning. The capacity was there,

586
00:40:27,360 --> 00:40:32,400
it was latent, and we didn't see it in the simple prompting mode, but the more sophisticated

587
00:40:32,400 --> 00:40:39,280
prompting mode elicited it. And I think this is in large part the result of the fact that this

588
00:40:39,280 --> 00:40:44,720
model was instruct tuned. And so people actually taught it about how that markup is supposed to

589
00:40:44,720 --> 00:40:49,360
work and how it's supposed to think about prompts like this. So the combination of all that human

590
00:40:49,360 --> 00:40:54,720
intelligence and the capacity of the model led to this really interesting and much better behavior.

591
00:40:55,040 --> 00:41:05,200
That is a glimpse of the foundations of all of this, I would say. Of course, we're going to unpack

592
00:41:05,200 --> 00:41:09,840
all of that stuff as we go through the quarter, but I hope you're getting a sense for it.

593
00:41:09,840 --> 00:41:13,520
Are there questions I can answer about it, things I could circle back on? Yes?

594
00:41:14,560 --> 00:41:21,360
The human brain has about 100 billion neurons, is my understanding. And I'm not sure how many

595
00:41:21,440 --> 00:41:25,120
parameters that might be, maybe like 10 trillion parameters or something like that.

596
00:41:25,920 --> 00:41:30,720
Are we approaching a point where these machines can start emulating the human brain or is there

597
00:41:30,720 --> 00:41:35,760
something to the language instinct or, you know, instincts of all kinds that maybe take

598
00:41:35,760 --> 00:41:43,200
into the human brain? Oh, it's nothing but big questions today. Right, so the question is kind

599
00:41:43,200 --> 00:41:47,600
of like, what is the relationship between the models we're talking about and the human brain?

600
00:41:47,600 --> 00:41:52,640
And you raise that in terms of the size, and I guess the upshot of your description was that

601
00:41:52,640 --> 00:41:59,040
these models remain smaller than the human brain. I think that's reasonable. It's tricky,

602
00:41:59,040 --> 00:42:04,160
though. On the one hand, they obviously have superhuman capabilities. On the other hand,

603
00:42:04,160 --> 00:42:10,080
they fall down in ways that humans don't. It's very interesting to ask why that difference exists.

604
00:42:11,040 --> 00:42:17,360
And maybe that would tell us something about the limitations of learning from scratch versus

605
00:42:17,360 --> 00:42:23,200
being initialized by evolution the way all of us were. I don't know, but I would say that

606
00:42:23,920 --> 00:42:29,600
underlying your whole line of questioning is the question, can we use these models to eliminate

607
00:42:29,600 --> 00:42:34,640
questions of neuroscience and cognitive science? And I think we should be careful,

608
00:42:34,640 --> 00:42:41,520
but that the answer is absolutely yes. And in fact, the increased ability of these models to learn

609
00:42:41,520 --> 00:42:47,520
from data has been really illuminating about certain kind of recalcitrant questions from

610
00:42:47,520 --> 00:42:52,080
cognitive science in particular. You have to be careful because they're so different from us,

611
00:42:52,080 --> 00:42:58,080
these models. On the other hand, I think they are helping us understand how to differentiate

612
00:42:58,080 --> 00:43:02,640
different theories of cognition. And ultimately, I think they will help us understand cognition

613
00:43:02,640 --> 00:43:09,600
itself. And I would, of course, welcome projects that were focused on those cognitive questions

614
00:43:09,600 --> 00:43:15,200
in here. This is a wonderful space in which to explore this kind of more speculative angle,

615
00:43:16,080 --> 00:43:18,960
connecting AI to the cognitive sciences.

616
00:43:23,520 --> 00:43:25,680
Other questions, comments? Yes, in the back.

617
00:43:26,240 --> 00:43:31,200
I would be curious to understand whether, I mean, partially following up on the brain thing,

618
00:43:31,200 --> 00:43:36,640
just to use a metaphor of our brain not being just one huge lump of neurons, but being separated

619
00:43:36,720 --> 00:43:42,160
into different areas. And then also thinking about the previous phase that you talked about,

620
00:43:42,160 --> 00:43:47,440
about breaking up the models and potentially having a model in the front that decides which

621
00:43:47,440 --> 00:43:53,200
domain our question falls into, and then having different sub-models. And I'm wondering whether

622
00:43:53,200 --> 00:43:58,480
that's arising, whether we're going to touch on an architecture like that. Because it just seems

623
00:43:58,480 --> 00:44:05,600
natural to me because prompting a huge model is just very expensive computationally. It feels like

624
00:44:06,240 --> 00:44:10,720
combining big models and logic trees could be a cool approach.

625
00:44:10,720 --> 00:44:15,280
I love it. Yeah, like one quick summary of what you said would relate directly to your question.

626
00:44:15,280 --> 00:44:22,160
The modularity of mind is an important old question about human cognition, to what extent

627
00:44:22,160 --> 00:44:30,320
are our abilities modularized in the mind-brain? With these current models, which have a capacity

628
00:44:30,320 --> 00:44:34,080
to do lots of different things if they have the right pre-training and the right structure,

629
00:44:34,080 --> 00:44:40,800
we could ask, does modularity emerge naturally? Or do they learn non-modular solutions? Both of

630
00:44:40,800 --> 00:44:46,080
those seem like they could be indirect evidence for how people work. Again, we have to be careful

631
00:44:46,080 --> 00:44:50,160
because these models are so different from us. But as a kind of existence proof, for example,

632
00:44:50,160 --> 00:44:55,200
that modularity was emergent from otherwise unstructured learning, that would be certainly

633
00:44:55,200 --> 00:45:00,560
eye-opening, right? I have no idea. Yeah, I don't know whether there are results for that.

634
00:45:00,640 --> 00:45:08,080
Are there results? No, just kind of a follow-up question on that as well. So, given how closed

635
00:45:08,080 --> 00:45:14,400
all these big models are, how could we interact with the models in such a way that helps us learn

636
00:45:14,400 --> 00:45:20,240
if there is modularity? Because we literally can only interact with the, how do we go about

637
00:45:20,240 --> 00:45:25,920
starting that? Right, so the question is, you know, the closed-off nature of a lot of these

638
00:45:26,000 --> 00:45:30,960
models has been a problem. We can access the open AI models, but only through an API. We don't get

639
00:45:30,960 --> 00:45:37,440
to look at their internal representations, and that has been a blocker. But I mentioned the rise of

640
00:45:37,440 --> 00:45:43,120
these 10 billion parameter models as being performant and interesting, and those are models that,

641
00:45:43,120 --> 00:45:47,520
with the right hardware, you can dissect a little bit. And I think that's just going to get better

642
00:45:47,520 --> 00:45:52,240
and better, and so we'll be able to, you know, peer inside them in ways that we haven't been able to

643
00:45:52,240 --> 00:45:59,200
until recently. Yeah. And in fact, like, we're going to talk a lot about explainability. That's

644
00:45:59,200 --> 00:46:04,160
a major unit of this course, and I think it's an increasingly important area of the whole field

645
00:46:04,160 --> 00:46:09,200
that we have techniques for understanding these models so that we know how they're going to behave

646
00:46:09,200 --> 00:46:13,920
when we deploy them. And it would be wonderfully exciting if you all wanted to try to scale the

647
00:46:13,920 --> 00:46:18,640
methods we talk about to a model that was as big as eight or 10 billion parameters.

648
00:46:18,720 --> 00:46:23,680
Ambitious just to do that, but then maybe a meaningful step forward. Yeah.

649
00:46:24,640 --> 00:46:29,440
I have a question back to, like, this baseball cap prompt that we were discussing. So I suppose,

650
00:46:29,440 --> 00:46:34,400
like, a part of the way that we discuss rules is, like, there is a little bit of ambiguity

651
00:46:34,400 --> 00:46:38,720
for, like, human interpretation, like, for example, in the honor code and the fundamental standard,

652
00:46:38,720 --> 00:46:44,240
like, it's intentionally ambiguous so that it's context dependent. And so, like, the idea is that

653
00:46:44,240 --> 00:46:49,280
there's, like, this inherent underlying value system that, like, affords whatever the rules

654
00:46:49,280 --> 00:46:55,040
that are written out are. And so that's, like, the primary form of evaluation. And so, I guess,

655
00:46:55,040 --> 00:46:59,280
like, how does that play into, then, how these language models are understanding? Like, is there

656
00:46:59,280 --> 00:47:04,240
some form of encoded or understanding, understood deeper value system that's encoded into them?

657
00:47:06,240 --> 00:47:10,800
You could certainly ask. I mean, the essence of your question is, could we, with analysis techniques,

658
00:47:10,880 --> 00:47:16,400
say, find out that a model had a particular belief system that was guiding its behavior?

659
00:47:17,040 --> 00:47:21,760
I think we can ask that question now. It sounds fantastically difficult, but maybe piecemeal

660
00:47:21,760 --> 00:47:26,560
we could get, make some progress on it for sure. Yeah, I want to return to the MLB one, though,

661
00:47:26,560 --> 00:47:31,600
because, well, as you'll see, and as I think we already saw, these models purport to offer

662
00:47:31,600 --> 00:47:34,960
evidence from a rule book. And that's where I feel stuck.

663
00:47:34,960 --> 00:47:43,760
You're keeping score at home. I posted the answer and some other stuff in the class discussion.

664
00:47:43,760 --> 00:47:49,680
Wonderful. Thank you. Yes?

665
00:47:49,680 --> 00:47:56,080
Can we just hook up these models to a large database of actually required information

666
00:47:56,080 --> 00:47:59,600
that's been encyclopedia and allow it to, you know, work stuff up?

667
00:48:00,560 --> 00:48:06,720
Oh, well, kind of yes. Actually, this is the sort of solution that I want to advocate for.

668
00:48:06,720 --> 00:48:08,240
I'm going to do this in a minute. Yeah.

669
00:48:11,120 --> 00:48:15,200
Here, let's, so we'll do this overview. I want to give you a feel for how the course will work

670
00:48:15,200 --> 00:48:20,480
and then dive into some of our major themes. So high-level overview. We've got these topics,

671
00:48:20,480 --> 00:48:25,200
contextual representations, transformers and stuff, multi-domain sentiment analysis.

672
00:48:25,200 --> 00:48:29,760
That will be the topic of the first homework and it's going to build on the first unit there.

673
00:48:30,560 --> 00:48:34,800
Retrieval augmented in context learning. This is where we might hook up to a database and get

674
00:48:34,800 --> 00:48:40,560
some guarantees about how these models will behave. Compositional generalization. In case

675
00:48:40,560 --> 00:48:44,800
you were worried that all the tasks were solved, I'm going to confront you with a task, a seemingly

676
00:48:44,800 --> 00:48:50,080
simple task about semantic interpretation that you will, well, I think it will not be solved.

677
00:48:50,080 --> 00:48:53,520
I mean, those could be famous last words because who knows what you all are capable of,

678
00:48:54,160 --> 00:48:58,960
but it's a very hard task that we will pose. We'll talk about benchmarking and adversarial

679
00:48:58,960 --> 00:49:03,360
training and testing. Increasingly important topics as we move into this mode where everyone

680
00:49:03,360 --> 00:49:08,400
is interacting with these large language models and feeling impressed by their behavior. We need

681
00:49:08,400 --> 00:49:13,680
to take a step back and rigorously assess whether they actually are behaving in good ways or whether

682
00:49:13,680 --> 00:49:17,280
we're just biased toward remembering the good things and forgetting the bad ones.

683
00:49:18,400 --> 00:49:22,160
We'll do model introspection. That's the explainability stuff that I mentioned and finally

684
00:49:22,160 --> 00:49:27,440
methods and metrics. And as you can see for the like five, six, and seven, that's going to be in

685
00:49:27,440 --> 00:49:32,560
the phase of the course where you're focused on final projects. And I'm hoping that that gives you

686
00:49:32,560 --> 00:49:38,640
tools to write really rich final papers that have great analysis in them and really excellent

687
00:49:38,640 --> 00:49:43,360
assessments. And then for the work that you'll do, we're going to have three assignments.

688
00:49:44,320 --> 00:49:48,480
And each one of the assignments is paired with what we call a bake-off, which is an informal

689
00:49:48,560 --> 00:49:54,000
competition around data and modeling. Essentially, the homework problems ask you to set up some

690
00:49:54,000 --> 00:49:59,360
baseline systems and get a feel for a problem. And then you write your own original system

691
00:49:59,920 --> 00:50:04,800
and you enter that into the bake-off. And we have a leaderboard on Gradescope and the team

692
00:50:04,800 --> 00:50:09,840
is going to look at all your submissions and give out some prizes for top-performing systems,

693
00:50:09,840 --> 00:50:15,280
but also systems that are really creative or interesting or ambitious or something like that.

694
00:50:15,280 --> 00:50:20,960
And that has always been a lot of fun and also really illuminating because it's like crowdsourcing

695
00:50:20,960 --> 00:50:26,720
a whole lot of different approaches to a problem. And then as a group, we can reflect on what worked

696
00:50:26,720 --> 00:50:31,680
and what didn't and look at the really ambitious things that you all try. So that's my favorite part.

697
00:50:31,680 --> 00:50:38,720
We have three offline quizzes and this is just as a way to make sure you have incentives to really

698
00:50:38,800 --> 00:50:44,880
immerse yourself in the course material. And those are done on Canvas. There's actually a

699
00:50:44,880 --> 00:50:49,680
fourth quiz, which I'll talk a little bit about probably next time, that is just making sure

700
00:50:49,680 --> 00:50:55,200
you understand the course policies. That's quiz zero. You can take it as many times as you want,

701
00:50:55,200 --> 00:51:00,880
but the idea is that you will have some incentive to learn about policies like due dates and so

702
00:51:00,880 --> 00:51:06,480
forth. And then the real action is in the final project and that will have a lit review phase,

703
00:51:06,560 --> 00:51:12,240
an experiment protocol, and a final paper. Those three components, you'll probably do those in teams

704
00:51:12,240 --> 00:51:16,720
and throughout all of that work, you'll be mentored by someone from the teaching team.

705
00:51:16,720 --> 00:51:23,040
And as I said before, we have this incredibly expert teaching team, lots of varied expertise,

706
00:51:23,040 --> 00:51:27,600
a lot of experience in the field. And so we hope to align you with the person,

707
00:51:28,240 --> 00:51:33,280
with someone who's really aligned with your project goals. And then I think you can go really,

708
00:51:33,280 --> 00:51:38,800
really far. Yeah? It looks like we've got one quarter already looking forward to Bake Offs.

709
00:51:38,800 --> 00:51:44,560
I don't understand what kids get obsessed about this stuff. On the final project,

710
00:51:44,560 --> 00:51:51,280
is this more of an academic paper or a rather about building working code and

711
00:51:52,480 --> 00:51:56,800
showing the state of the art? Great question. For the first one, the Bake Offs. Yes,

712
00:51:56,800 --> 00:52:02,160
it is easy to get obsessed with your Bake Off entry. I would say that if you get obsessed

713
00:52:02,160 --> 00:52:09,200
and you do really well, just make that into your final project. All three of them are really

714
00:52:09,200 --> 00:52:13,440
important problems. They are not idle work. I mean, one of them is on retrieval augmented

715
00:52:13,440 --> 00:52:17,600
in-context learning, which is one of my core research focuses right now. So is compositional

716
00:52:17,600 --> 00:52:21,680
generalization. If you do something really interesting for a Bake Off, make it your final

717
00:52:21,680 --> 00:52:26,960
paper and then go on to publish it. For the second part of your question, I would say that the core

718
00:52:26,960 --> 00:52:31,760
goal is to get you to produce something that could be a research contribution in the field.

719
00:52:32,320 --> 00:52:37,680
And we have lots of success stories. I've got links at the website to people who have gone on to

720
00:52:37,680 --> 00:52:43,840
publish their final paper as an NLP paper. I'm careful the way I say that. They didn't literally

721
00:52:43,840 --> 00:52:48,640
publish the final paper because in 10 weeks, almost no one can produce a publishable paper.

722
00:52:48,640 --> 00:52:53,760
It's just not enough time. But you could form the basis for then working a little bit more

723
00:52:53,760 --> 00:52:58,080
or a lot more and then getting a really outstanding publication out of it. And I would

724
00:52:58,080 --> 00:53:02,400
say that that's the default goal. The nature of the contribution though is highly varied.

725
00:53:02,400 --> 00:53:06,960
We have one requirement, which is that the final paper have some quantitative evaluation in it.

726
00:53:07,760 --> 00:53:11,840
But there are a lot of ways to satisfy that requirement and then you could be serving

727
00:53:11,840 --> 00:53:17,360
many different questions in the field for some expansive notion of the field as well.

728
00:53:18,320 --> 00:53:33,120
Background materials. So I should say that officially, we are presupposing CS224N or CS224S,

729
00:53:33,120 --> 00:53:38,080
as prerequisites for the course. And what that means is that I'm going to skip a lot of the

730
00:53:38,080 --> 00:53:44,800
fundamentals that we have covered in past years. If you need a refresher, check out the background

731
00:53:44,800 --> 00:53:51,440
page of the course site. It covers fundamentals of scientific computing, static vector representations

732
00:53:51,440 --> 00:53:57,680
like Word2Vec and Glove, and supervised learning. And I'm hoping that that's enough of a refresher.

733
00:53:57,680 --> 00:54:03,600
If you look at that material and find that it too is kind of beyond where you're at right now,

734
00:54:03,600 --> 00:54:08,000
then contact us on the teaching team and we can think about how to manage that.

735
00:54:08,960 --> 00:54:13,520
But officially, this is a course that presupposes CS224N.

736
00:54:16,080 --> 00:54:20,080
And then the core goals, this kind of relates to that previous question. Hands-on experience with

737
00:54:20,080 --> 00:54:26,080
a wide range of problems. Mentorship from the teaching team to guide you through projects and

738
00:54:26,080 --> 00:54:31,840
assignments. And then really the central goal here is to make you the best, that is most insightful,

739
00:54:31,840 --> 00:54:38,640
most responsible, most flexible NLU researcher and practitioner that you can be for whatever

740
00:54:38,640 --> 00:54:43,600
you decide to do next. And we're assuming that you have lots of diverse goals that somehow connect

741
00:54:43,600 --> 00:54:55,760
with NLU. All right, let's do some course themes unless there are questions. I have a whole final

742
00:54:55,760 --> 00:55:02,000
section of this slideshow that's about the course materials and requirements and stuff

743
00:55:02,000 --> 00:55:05,680
might save that for next time. And you can check it out at the website and you'll be

744
00:55:05,680 --> 00:55:11,600
forced to engage with it for quiz zero. I thought instead I would dive back into the

745
00:55:11,600 --> 00:55:14,080
content part of this unless there are questions or comments.

746
00:55:14,400 --> 00:55:25,920
All right, first course theme, transformer-based pre-training. So starting with the transformer,

747
00:55:25,920 --> 00:55:30,560
we want to talk about core concepts and goals, give you a sense for what these models are like,

748
00:55:31,200 --> 00:55:36,240
why they work, what they're supposed to do, all of that stuff. We'll talk about a bunch of different

749
00:55:36,240 --> 00:55:41,840
architectures. There are dozens and dozens of them, but I hope that I have picked enough of them

750
00:55:41,840 --> 00:55:47,120
with the right selection of them to give you a feel for how people are thinking about these models

751
00:55:47,120 --> 00:55:52,000
and the kind of innovations they've brought in that have led to real meaningful advancement,

752
00:55:52,000 --> 00:55:56,480
just at the level of architectures. We'll also talk about positional encoding, which I think

753
00:55:56,480 --> 00:56:01,680
maybe a lot of us have been surprised to see just how important that is as a differentiator for

754
00:56:01,680 --> 00:56:08,000
different approaches in this space. We'll talk about distillation, taking really large models

755
00:56:08,000 --> 00:56:13,920
and making them smaller. It's an important goal for lots of reasons and an exciting area of research.

756
00:56:13,920 --> 00:56:18,880
And then as I mentioned, it's going to do a little lecture for us on diffusion objectives for these

757
00:56:18,880 --> 00:56:25,360
models, and then it's going to talk about practical pre-training and fine-tuning. I'm going to enlist

758
00:56:25,360 --> 00:56:30,080
the entire teaching team to do guest lectures, and these are the two that I've lined up so far.

759
00:56:31,760 --> 00:56:36,320
And that will kind of culminate or be aligned with this first homework in Bakeoff, which has a

760
00:56:36,320 --> 00:56:40,720
multi-domain sentiment. I'm going to give you a bunch of different sentiment data sets,

761
00:56:40,720 --> 00:56:44,480
and you're going to have to design one system that can kind of succeed on all of them.

762
00:56:45,040 --> 00:56:50,000
And then for the Bakeoff, we have an unlabeled data set for you. We have the labels, but you won't.

763
00:56:50,640 --> 00:56:56,480
And that has data that's like what you developed on, and then some mystery examples that you will

764
00:56:56,480 --> 00:57:01,520
not really be able to anticipate. And we're going to see how well you do at handling all of these

765
00:57:01,520 --> 00:57:09,360
different domains with one system. And this is by way of kind of, again, a refresher on core

766
00:57:09,360 --> 00:57:14,160
concepts in supervised learning and really getting you to think about transformers, although we're

767
00:57:14,160 --> 00:57:18,240
not going to constrain the kind of solution that you offer for your original system.

768
00:57:22,480 --> 00:57:28,240
Our second major theme will be retrieval augmented in-context learning, a topic

769
00:57:29,120 --> 00:57:36,080
that I would not even have dreamt of five years ago and seemed kind of infeasible three years ago,

770
00:57:36,080 --> 00:57:41,680
and that we first did two years one year ago. Oh goodness. I think this is only the second time,

771
00:57:41,680 --> 00:57:45,440
but I had to redo it entirely because things have changed so much.

772
00:57:47,360 --> 00:57:53,040
Here's the idea. We have two characters so far in our kind of emerging narrative for NLU.

773
00:57:53,680 --> 00:57:57,840
On the one hand, we have this approach that I'm going to call LLMs for everything,

774
00:57:57,840 --> 00:58:03,520
large language models for everything. You input some kind of question. Here I've chosen a very

775
00:58:03,520 --> 00:58:09,360
complicated question. Which MVP of a game Red Flaherty umpired was elected to the Baseball

776
00:58:09,360 --> 00:58:13,920
Hall of Fame? And hats off to you if you know that the answer is Sandy Kofax.

777
00:58:16,000 --> 00:58:22,000
The LLMs for everything approach is that you just type that question in and the model gives you an

778
00:58:22,000 --> 00:58:28,480
answer. And hopefully you're happy with the answer. The other character that I'm going to

779
00:58:28,480 --> 00:58:33,600
introduce here is what I'm going to call retrieval augmented. So I have the same question at the top

780
00:58:33,600 --> 00:58:37,360
here, except now this is going to proceed differently. The first thing that we will do

781
00:58:37,920 --> 00:58:44,320
is take some large language model and encode that query into some numerical representation.

782
00:58:45,280 --> 00:58:50,080
That's sort of familiar. The new piece is that we're going to also have a knowledge store

783
00:58:50,640 --> 00:58:58,240
which you could think of as an old fashioned web index, right? Just a knowledge store of documents

784
00:58:58,240 --> 00:59:04,160
with the modern twist that now all of the documents are also represented by large language models.

785
00:59:04,160 --> 00:59:08,720
But fundamentally, this is an index of a sort that drives all web search right now.

786
00:59:09,680 --> 00:59:15,120
We can score documents with respect to queries on the basis of these numerical representations.

787
00:59:15,120 --> 00:59:21,280
And if we want to, we can reproduce the classic search experience. Here I've got a ranked list

788
00:59:21,280 --> 00:59:27,680
of documents that came back from my query, just like when you do Google as of the last time I googled.

789
00:59:28,880 --> 00:59:33,440
But in this mode, we can continue, right? We could have another language model slurp up those

790
00:59:33,440 --> 00:59:39,040
retrieved documents and synthesize them into an answer. And so here at the bottom I've got,

791
00:59:39,040 --> 00:59:43,760
it's kind of small, but it's the same answer over here, although notably this answer is now

792
00:59:43,760 --> 00:59:49,520
decorated with links that would allow you the user to track back to what documents actually

793
00:59:50,560 --> 00:59:56,480
provided that evidence. Whereas on the left, who knows where that information came from.

794
00:59:56,480 --> 00:59:58,560
And that's kind of what we were already grappling with.

795
01:00:00,640 --> 01:00:05,360
This is an important societal need because this is taking over web search. What are our goals

796
01:00:05,360 --> 01:00:10,400
for this kind of model here? So first, we want synthesis, fluency, right? We want to be able to

797
01:00:10,400 --> 01:00:16,160
take information from multiple documents and synthesize it down into a single answer. And I

798
01:00:16,160 --> 01:00:21,040
think both of the approaches that I just showed you are going to do really well on that. We also

799
01:00:21,040 --> 01:00:26,880
need these models to be efficient, to be updatable, because the world is changing all the time.

800
01:00:27,680 --> 01:00:33,040
We need it to track provenance and maybe invoke something like factuality, but certainly

801
01:00:33,040 --> 01:00:37,520
provenance. We need to know where the information came from. And we need some safety and security.

802
01:00:37,520 --> 01:00:42,240
We need to know that the model won't produce private information, and we might need to restrict

803
01:00:42,240 --> 01:00:46,880
access to parts of the model's knowledge to different groups, like different customers or

804
01:00:46,880 --> 01:00:51,120
different people with different privileges and so forth. That's what we're going to need if we're

805
01:00:51,120 --> 01:00:57,200
really going to deploy these models out into the world. As I said, I think both of the approaches

806
01:00:57,200 --> 01:01:01,200
that I sketched do well on the synthesis part, because they both use a language model and those

807
01:01:01,200 --> 01:01:06,960
are really good. They all have the gift of gab, so to speak. What about efficiency, right? On the

808
01:01:06,960 --> 01:01:14,720
LLM for everything approach, we had this undeniable rise in model size, and I pointed out models like

809
01:01:14,720 --> 01:01:21,600
an alpaca that are smaller. But I strongly suspect that if we are going to continue to ask these

810
01:01:21,600 --> 01:01:28,240
models to be both a knowledge store and a language capability, we're going to be dealing with these

811
01:01:28,240 --> 01:01:35,520
really large models. The hope of the retrieval augmented approach is that we could get by with

812
01:01:35,520 --> 01:01:40,800
the smaller models, and the reason we could do that is that we're going to factor out the knowledge

813
01:01:40,800 --> 01:01:46,480
store into that index and the language capability, which is going to be the language model. The only

814
01:01:46,480 --> 01:01:51,920
thing we're going to be asking the language model is to be good at that kind of in-context learning.

815
01:01:51,920 --> 01:01:57,600
It doesn't need to also store a full model of the world, and I think that means that these models

816
01:01:57,600 --> 01:02:02,960
could be smaller. So overall, a big gain in efficiency if we go retrieval augmented.

817
01:02:03,440 --> 01:02:06,160
People will make progress, but I think it's going to be tense.

818
01:02:08,000 --> 01:02:11,760
What about updateability? Again, this is a problem that people are working on very

819
01:02:11,760 --> 01:02:18,320
concertedly for the LLMs for everything approach, but these models persist in giving outdated answers

820
01:02:18,320 --> 01:02:22,880
to questions. And one pattern you see is that there's a lot of progress where you could like

821
01:02:22,880 --> 01:02:27,280
edit a model so that it gives the correct answer to who is the president of the US,

822
01:02:27,280 --> 01:02:33,760
but then you ask it about something related to the family of the president, and it reveals that it

823
01:02:33,760 --> 01:02:39,120
has outdated information stored in its parameters. And that's because all of this information is

824
01:02:39,120 --> 01:02:45,600
interconnected, and we don't at the present moment know how to reliably do that kind of systematic

825
01:02:45,600 --> 01:02:53,840
editing. Okay, on the retrieval augmented approach, we just re-index our data. If the world changes,

826
01:02:53,840 --> 01:02:59,040
we assume that the knowledge store changed like somebody updated a Wikipedia page. So we

827
01:02:59,040 --> 01:03:03,840
represent all the documents again, or at least just the ones that changed. And now we have a lot

828
01:03:03,840 --> 01:03:08,720
of guarantees that as that propagates forward into the retrieved results, which are consumed by the

829
01:03:08,720 --> 01:03:14,880
language model, it will reflect the changes we made to the underlying database in exactly the same

830
01:03:14,880 --> 01:03:22,960
way that a web search index is updated now. One forward pass of the large language model

831
01:03:22,960 --> 01:03:29,120
compared to maybe training from scratch over here on new data to get an absolute guarantee

832
01:03:29,120 --> 01:03:35,200
that the change will propagate. What about provenance? Okay, we have seen this already,

833
01:03:35,200 --> 01:03:42,880
this problem here. LLMs for everything. I asked GPT3, the DaVinci 3 model, my question,

834
01:03:42,880 --> 01:03:47,760
are professional baseball players allowed to glue small wings onto their caps? But I kind of cut it

835
01:03:47,760 --> 01:03:54,400
off, but at the top there I said provide me some links to the evidence. And it dutifully

836
01:03:54,400 --> 01:04:00,240
provided the links, but none of the links are real. If you copy them out and follow them,

837
01:04:00,240 --> 01:04:06,880
they all go to 404 pages. And I think that this is worse than providing no links at all,

838
01:04:06,880 --> 01:04:12,320
because I'm attuned as a human in the current moment to see links and think they're probably

839
01:04:12,400 --> 01:04:17,680
evidence. And I don't follow all the links. And here you might look and say, oh yeah,

840
01:04:17,680 --> 01:04:26,080
I see it found the relevant MLB pages and that's it, right? Over here, the kind of the point of this

841
01:04:26,080 --> 01:04:30,560
is that we are first doing a search phase where we're actually linked back to documents,

842
01:04:30,560 --> 01:04:35,040
and then we just need to solve the interesting, non-trivial question of how to link those

843
01:04:35,040 --> 01:04:40,480
documents into the synthesized answer. But all of the information we need is right there on the

844
01:04:40,480 --> 01:04:45,920
screen for us, and so this feels like a relatively tractable problem compared to what we are faced

845
01:04:45,920 --> 01:04:54,240
with on the left. I will say, I've been just amazed at the rollout, especially of the Bing

846
01:04:54,240 --> 01:04:59,840
search engine, which now incorporates open AI models at some level, because it is clear that it

847
01:04:59,840 --> 01:05:04,800
is doing web search, right? Because it's got information that comes from documents that only

848
01:05:04,800 --> 01:05:11,200
appeared on the web days before your query. But what it's doing with that information seems

849
01:05:11,200 --> 01:05:16,640
completely chaotic to me, so that it's kind of just getting mushed in with whatever else the

850
01:05:16,640 --> 01:05:23,440
model is doing, and you get this unpredictable combination of things that are grounded in

851
01:05:23,440 --> 01:05:28,560
documents and things that are completely fabricated. And again, I maintain this is worse than just

852
01:05:28,560 --> 01:05:36,160
giving an answer with no evidence attached to it. I don't know why these companies are not simply

853
01:05:36,160 --> 01:05:40,480
doing the retrieval augmented thing, but I'm sure they are going to wise up, and maybe your research

854
01:05:40,480 --> 01:05:46,560
could help them wise up a little bit about this. Finally, safety and security. This is relatively

855
01:05:46,560 --> 01:05:51,280
straightforward. On the LLMs for everything approach, we have a pressing problem, privacy

856
01:05:51,280 --> 01:05:55,760
challenges. We know that those models can memorize long strings in their training data, and that

857
01:05:55,840 --> 01:06:00,560
could include some very particular information about one of us, and that should be worrying us.

858
01:06:01,280 --> 01:06:06,720
We have no known way with a language model to compartmentalize LLM capabilities and say, like,

859
01:06:06,720 --> 01:06:12,720
you can see this kind of result, and you cannot. And similarly, we have no known way to restrict

860
01:06:12,720 --> 01:06:18,320
access to part of an LLMs capabilities. They just produce things based on their prompts,

861
01:06:18,320 --> 01:06:22,640
and you could try to have some prompt tuning that would tell them for this kind of person or setting

862
01:06:22,640 --> 01:06:29,120
do this and not that, but nobody could guarantee that that would succeed. Whereas for the retrieval

863
01:06:29,120 --> 01:06:35,440
augmented approach, again, we're thinking about accessing information from an index and access

864
01:06:35,440 --> 01:06:41,840
restrictions on an index is an old problem by now. Again, I don't want to say solved, but something

865
01:06:41,840 --> 01:06:47,760
that a lot of people have tackled for decades now, and so we can offer something like guarantees

866
01:06:48,320 --> 01:06:51,440
just from the fact that we have a separated knowledge store.

867
01:06:54,640 --> 01:06:59,600
Again, my smiley face. You can see where my feelings are. For the LLMs for everything approach,

868
01:07:00,160 --> 01:07:04,080
you know, people are working on these problems, and it's very exciting. And if you want a challenge,

869
01:07:04,640 --> 01:07:09,680
take, you know, take up one of these challenges here, but over here on the retrieval augmented side,

870
01:07:09,680 --> 01:07:14,000
I think we have lots of reasons to think it's not that they're completely solved. It's just

871
01:07:14,000 --> 01:07:20,160
that we can see the path to solving them. And this feels very urgent to me because of how suddenly

872
01:07:20,800 --> 01:07:25,440
this kind of technology is being deployed in a very user facing way for one of the core things

873
01:07:25,440 --> 01:07:31,120
we do in society, which is web search. So it's an urgent thing that we get good at this.

874
01:07:33,680 --> 01:07:39,040
Final things I want to say about this. So until recently, the way you would do even the

875
01:07:39,040 --> 01:07:45,040
retrieval augmented thing would be that you would have your index, and then you might train

876
01:07:45,040 --> 01:07:50,000
a custom purpose model to do the question answering part, and it could extract things from the text

877
01:07:50,000 --> 01:07:53,680
that you produced or maybe even generate some new things from the text that you produced.

878
01:07:54,720 --> 01:07:58,800
And that's kind of the mode that I mentioned before where you'd have like some language models,

879
01:07:58,800 --> 01:08:03,360
maybe a few of them, and you'd have an index, and you would stitch them together into a question

880
01:08:03,360 --> 01:08:08,400
answering system that you would probably train on question answering data. And you would hope

881
01:08:08,400 --> 01:08:13,920
that this whole big monster, maybe fine tuned on squad or natural questions or one of those data

882
01:08:13,920 --> 01:08:22,000
sets, gave you a general purpose question answering capability. That's the present, but I think it

883
01:08:22,000 --> 01:08:27,920
might actually be the recent past. And in fact, the way that you all will probably work when we do

884
01:08:27,920 --> 01:08:34,160
this unit, and certainly for the homework, is that we will just have frozen components. And this

885
01:08:34,160 --> 01:08:40,560
starts from the observation that the retriever model is really just a model that takes in text

886
01:08:40,560 --> 01:08:47,760
and produces text with scores. And a language model is also a device for taking in text and

887
01:08:47,760 --> 01:08:52,560
producing text with scores. And these are when these are frozen components, you can think of them

888
01:08:52,560 --> 01:08:57,680
as just black box devices that do this input output thing. And then you get into the intriguing

889
01:08:57,680 --> 01:09:03,360
mode of asking, well, what if we had them just talk to each other? And that is what you will do

890
01:09:03,360 --> 01:09:08,640
for the homework and bake off, you will have frozen retriever and a frozen large language model,

891
01:09:08,640 --> 01:09:15,040
and you will get them to work together to solve a very difficult open domain question answering

892
01:09:15,040 --> 01:09:21,120
problem. And that's kind of pushing us into a new mode for even thinking about how we design AI

893
01:09:21,120 --> 01:09:26,400
systems where it's not so much about fine tuning, it's much more about getting them to communicate

894
01:09:26,400 --> 01:09:33,600
with each other effectively to design a system from frozen components. Again, unanticipated,

895
01:09:33,600 --> 01:09:38,720
at least by me, as of a few years ago, and now an exciting new direction.

896
01:09:40,800 --> 01:09:44,960
So just to wrap out, I think what I'll do since we're near the end of the class here,

897
01:09:44,960 --> 01:09:49,200
I'll just finish up this one unit, and then we'll use some of our time next time to introduce

898
01:09:49,200 --> 01:09:55,600
a few other of these course themes, and that'll set us up well for diving into transformers.

899
01:09:55,680 --> 01:10:01,360
Final piece here just to inspire you, few shot open QA is kind of the task that you will tackle

900
01:10:01,360 --> 01:10:06,000
for homework too. And here's how you could think about this. Imagine that the question has come

901
01:10:06,000 --> 01:10:11,680
in, what is the course to take? The most standard thing we could do is just prompt the language

902
01:10:11,680 --> 01:10:16,720
model with that question, what is the course to take down here and see what answer it gave back,

903
01:10:16,720 --> 01:10:22,880
right? But the retrieval augmented insight is that we might also retrieve some kind of passage

904
01:10:22,880 --> 01:10:27,120
from a knowledge store. Here I have a very short passage, the course to take is natural language

905
01:10:27,120 --> 01:10:33,680
understanding, and that could be done with a retrieval mechanism. But why stop there? It might

906
01:10:33,680 --> 01:10:40,080
help the model as we saw going back to the GPT3 paper to have some examples of the kind of behavior

907
01:10:40,080 --> 01:10:44,960
that I'm hoping to get from the model. And so here I have retrieved from some data set,

908
01:10:44,960 --> 01:10:48,880
question answer pairs that will kind of give it a sense for what I want it to do in the end.

909
01:10:49,680 --> 01:10:56,400
But again, why stop there? We could also pick questions that were based very closely on the

910
01:10:56,400 --> 01:11:01,760
question that we posed. That would be like K nearest neighbor's approach where we use our retrieval

911
01:11:01,760 --> 01:11:08,560
mechanism to find similar questions to the one that we care about. I could also add in some context

912
01:11:08,560 --> 01:11:14,640
passages and I could do that by retrieval. So now we've used the retrieval model twice potentially,

913
01:11:14,640 --> 01:11:18,880
wants to get good demonstrations and wants to provide context for each one of them.

914
01:11:19,840 --> 01:11:24,640
But I could also use my retrieval mechanism with the questions and answers from the demonstration

915
01:11:24,640 --> 01:11:30,320
to get even richer connections between my demonstrations and the passages. I could even

916
01:11:30,320 --> 01:11:35,600
use a language model to rewrite aspects of those demonstrations to put them in a format

917
01:11:35,600 --> 01:11:41,920
that might help me with the final question that I want to pose. So now I have an interwoven use

918
01:11:41,920 --> 01:11:47,360
of the retrieval mechanism and the large language model to build up this prompt, right?

919
01:11:48,240 --> 01:11:52,640
Down at the retrieval thing, I could do the same thing. And then when you think about the model

920
01:11:52,640 --> 01:11:58,240
generation, again, we could just take the top response from the model, but we can do very

921
01:11:58,240 --> 01:12:05,040
sophisticated things on up to this full retrieval augmented generation model, which essentially

922
01:12:05,040 --> 01:12:10,800
marginalizes out the evidence passage and gives us a really powerful look at a good answer,

923
01:12:10,880 --> 01:12:17,440
conditional on that very complicated prompt that we constructed. I think what you're seeing on the

924
01:12:17,440 --> 01:12:23,520
left here is that we are going to move from an era where we just type in prompts into these models

925
01:12:23,520 --> 01:12:30,640
and hope for the best into an era where prompt construction is a kind of new programming mode

926
01:12:31,280 --> 01:12:37,120
where you're writing down computer code, could be Python code, that is doing traditional computing

927
01:12:37,120 --> 01:12:44,080
things, but also drawing on very powerful pre-trained components to assemble this kind

928
01:12:44,080 --> 01:12:49,920
of instruction kit for your large language model to do whatever task you have set for it.

929
01:12:50,480 --> 01:12:55,680
And so instead of designing these AI systems with all that fine tuning I described before,

930
01:12:55,680 --> 01:13:01,120
we might actually be moving back into a mode that's like that symbolic mode from the 80s

931
01:13:01,120 --> 01:13:06,560
where you type in a computer program. It's just that now the program that you type in

932
01:13:06,640 --> 01:13:13,360
is connected to these very powerful modern AI components and we're seeing right now

933
01:13:14,000 --> 01:13:18,560
that that is opening doors to all kinds of new capabilities for these systems

934
01:13:18,560 --> 01:13:22,640
and this first homework in Bake Off is going to give you a glimpse of that.

935
01:13:23,440 --> 01:13:28,080
And you're going to use a programming model we've developed called demonstrate search predict

936
01:13:28,080 --> 01:13:31,200
that I hope will give you a glimpse of just how powerful this can be.

937
01:13:32,080 --> 01:13:40,480
All right, we are out of time, right, 420? So next time I'll show you a few more units from the

938
01:13:40,480 --> 01:13:43,600
course and then we'll dive into transformers.

