1
00:00:00,000 --> 00:00:16,440
Just such a treat to be back, I spend many hours on that side of the room, so it's wild

2
00:00:16,440 --> 00:00:19,840
to be on this side of the room and going, whoa, there was actually like monitors up here,

3
00:00:19,840 --> 00:00:25,440
like that's how the speakers kept track of where in their talk they were, so that's good

4
00:00:25,440 --> 00:00:27,280
to know.

5
00:00:27,600 --> 00:00:31,440
This is sort of the first set of talks I've given since the pandemic, and so I thought

6
00:00:31,440 --> 00:00:36,720
it was a really great opportunity to talk about some new ideas that have been on my

7
00:00:36,720 --> 00:00:42,000
mind, and particularly with all of you as my captive audience, I thought that I would

8
00:00:42,000 --> 00:00:48,840
use this talk as an opportunity to think out loud about what the role of HCI should be

9
00:00:48,840 --> 00:00:55,760
in the face of all of this really incredible rapid progress that AI and ML have made, particularly

10
00:00:55,760 --> 00:01:01,920
kind of scoped in the last six months or so.

11
00:01:01,920 --> 00:01:08,480
As I was trying to think about what the role of HCI should be, I was reminded of this figure

12
00:01:08,480 --> 00:01:15,400
from Jonathan Gruden's 2009 article in triple AI about how AI and HCI are two fields that

13
00:01:15,400 --> 00:01:17,880
are divided by a common focus.

14
00:01:17,880 --> 00:01:23,080
As you can see in moments where AI makes a lot of progress, it's almost like the pendulum

15
00:01:23,120 --> 00:01:31,000
swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches

16
00:01:31,000 --> 00:01:37,960
of human intelligence augmentation or amplification, but also I think HCI is in a more established

17
00:01:37,960 --> 00:01:47,200
stronger position than it's ever been in the past, and so I really think it's our responsibility

18
00:01:47,280 --> 00:01:53,120
to think about what that counterbalance to ever-increased automation should be.

19
00:01:53,120 --> 00:01:58,240
So I often, in moments like this, like to sort of turn to history and ground myself,

20
00:01:58,240 --> 00:02:03,520
and so if we cast back to the first AI winter with Sutherland's sketchpad, right around

21
00:02:03,520 --> 00:02:09,320
that time there was this foundational paper written by Lick Leiter at MIT titled Man

22
00:02:09,320 --> 00:02:14,320
Computers in Biosas, and I think the gendering is unfortunate and unfortunately reflective

23
00:02:14,400 --> 00:02:20,400
of the times, but nevertheless in this paper, Lick Leiter put forth this really compelling

24
00:02:20,400 --> 00:02:27,900
vision about the ways in which a computer could interact with us through this intuitive,

25
00:02:27,900 --> 00:02:34,660
guided trial and error procedure, turning up solutions and revealing unexpected turns

26
00:02:34,660 --> 00:02:39,240
in the reasoning, and I was really tempted to put this sort of side-by-side with this

27
00:02:39,320 --> 00:02:45,560
very recent demo that OpenAI released with ChadGPT plus plugins where you can upload

28
00:02:45,560 --> 00:02:50,560
this music.csv data set and then start to have this very natural language interaction

29
00:02:50,560 --> 00:02:54,960
to ask what are the columns in the data set, how many rows in there in the data set, and

30
00:02:54,960 --> 00:03:00,240
then even say, can you give me some basic visualizations of this data set, and it thinks

31
00:03:00,240 --> 00:03:06,400
a little bit, it's working real hard, and there you go, it produces sort of three visualizations

32
00:03:06,440 --> 00:03:12,240
and even starts to give you maybe something that looks like an explanation, and I wonder,

33
00:03:12,240 --> 00:03:16,840
is it time to roll out our mission accomplished banners? Like have we achieved Lick Leiter's

34
00:03:16,840 --> 00:03:22,160
vision to think in interaction with a computer in the same way that we think with a colleague

35
00:03:22,160 --> 00:03:27,760
whose competence supplements our own? Now, I don't think it's time to roll out the mission

36
00:03:27,760 --> 00:03:31,840
accomplished banners, but I'm hopeful that the reason it's not that is not just my sort

37
00:03:31,880 --> 00:03:37,960
of hope that we haven't been put out of jobs, but rather that there is something more to do.

38
00:03:37,960 --> 00:03:44,960
So two years after Lick Leiter's man-computer symbiosis, Douglas Engelbart wrote up this

39
00:03:44,960 --> 00:03:50,240
really incredible framework called augmenting human intellect, and right in the introduction

40
00:03:50,240 --> 00:03:56,440
of this piece, we already start to see how Engelbart is defining a much more expansive

41
00:03:56,480 --> 00:04:02,680
role of human augmentation. So the idea is not just about problem solving, which he does

42
00:04:02,680 --> 00:04:07,640
mention right at the end there, to derive solutions to a problem, but it's also about

43
00:04:07,640 --> 00:04:14,640
using computers to help us think. It's to increase our capacity to approach a complex

44
00:04:15,680 --> 00:04:21,040
problem situation, to gain comprehension really about this thinking and not just the problem

45
00:04:21,040 --> 00:04:26,280
solving pieces, and really what I like is how he thinks we'll get there. Certainly there

46
00:04:26,280 --> 00:04:30,920
will be sophisticated methods, high powered electronic aids, but to me the part that really

47
00:04:30,920 --> 00:04:36,800
resonates in his prescription here is streamlined terminology and notation, and that's going

48
00:04:36,800 --> 00:04:43,440
to be a theme of my talk here, certainly one of the themes that underlies my group's work.

49
00:04:43,440 --> 00:04:49,000
And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work

50
00:04:49,000 --> 00:04:52,880
with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the

51
00:04:52,920 --> 00:04:58,440
top right-hand corner, on this system called B2. So this is a Jupyter notebook, it's a

52
00:04:58,440 --> 00:05:03,720
very commonly used data science environment where people can start to write code in the

53
00:05:03,720 --> 00:05:09,840
style of a Python REPL, but what B2 does is saying, well, in addition to that sort of

54
00:05:09,840 --> 00:05:15,720
linear style of data science analysis and programming, there's a lot of value in a more

55
00:05:15,720 --> 00:05:21,000
visual analysis dashboard style interface like Tableau. And so what B2 tries to do is

56
00:05:21,040 --> 00:05:26,160
bring these two pieces together. So you can see once I've invoked B2 it adds this on the

57
00:05:26,160 --> 00:05:31,960
sidebar, and I can start to issue regular sort of Python, you know, pandas commands

58
00:05:31,960 --> 00:05:36,960
like looking at the data frame, getting a, you know, a sense of how many rows there are,

59
00:05:36,960 --> 00:05:41,600
what the columns are, and now I can start to write some code to do a little bit of data

60
00:05:41,600 --> 00:05:46,120
transformation and visualization. Notice here in all of these steps, you know, when I'm

61
00:05:46,160 --> 00:05:50,800
authoring a visualization, I don't have to specify what that visualization should look

62
00:05:50,800 --> 00:05:56,800
like, right? I'm just calling these .viz methods on the data frame, and B2 behind the scenes

63
00:05:56,800 --> 00:06:02,000
is figuring out what sort of visualization actually makes sense based on the history

64
00:06:02,000 --> 00:06:07,520
of the transformations that were performed on the data frame. So in the case of year,

65
00:06:07,520 --> 00:06:12,400
for instance, if I've grouped by year, the most sensible visualization to produce is

66
00:06:12,440 --> 00:06:18,200
a histogram of the number of counts of data records across years. You might have also noticed

67
00:06:18,200 --> 00:06:23,000
in the video that if I click the fields on the right hand side there, that it automatically

68
00:06:23,000 --> 00:06:28,240
produces an equivalent visualization, but it doesn't stop there. It adds, you know, the code

69
00:06:28,240 --> 00:06:34,840
and tags them with these little, you know, yellow emojis to indicate that there's actually sort of

70
00:06:34,840 --> 00:06:41,080
a common shared representation here, right? Clicking on the sidebar not only produces the

71
00:06:41,080 --> 00:06:46,440
visualization, but produces the equivalent code as well. And what's interesting is that these

72
00:06:46,440 --> 00:06:51,920
visualizations aren't just output mechanisms, but I can start to interact with them to sort of do

73
00:06:51,920 --> 00:06:58,040
this cross filtering interaction. So all the other bars update to reflect the data shown in the

74
00:06:58,040 --> 00:07:04,640
highlighted bars, and B2 is keeping this as an interaction log that is semantically meaningful

75
00:07:04,640 --> 00:07:09,680
to me. So this interaction log doesn't comprise mouse clicks and keystrokes and things like that,

76
00:07:09,920 --> 00:07:15,040
but it's expressing data queries, right? Which states have been selected? And I can use that

77
00:07:15,040 --> 00:07:21,520
data query to perform subsequent sort of analyses based on my interactive results. So I can say,

78
00:07:21,520 --> 00:07:27,840
great, I'm gonna, you know, copy some code to the clipboard, paste it in as a data query to look

79
00:07:27,840 --> 00:07:33,280
at what the interactive selection should be, and then, you know, proceed with some other sort of

80
00:07:33,280 --> 00:07:39,440
visual analysis. And so as we're sort of, you know, looking at these two forms of interaction,

81
00:07:39,600 --> 00:07:44,720
I was trying to figure out, well, some things feel the same, right? I've got that kind of

82
00:07:44,720 --> 00:07:49,680
conversational back and forth. I'm sure on the left-hand side with ChatGPT, it's a more natural

83
00:07:49,680 --> 00:07:53,800
language conversation. On the right-hand side, it's more of a repel conversation. But also,

84
00:07:53,800 --> 00:07:59,480
things feel qualitatively different. And how do I actually kind of characterize what is the same

85
00:07:59,480 --> 00:08:04,000
and what is the difference? And I thought really hard about it. And I realized that actually,

86
00:08:04,480 --> 00:08:10,240
maybe what still matters is direct manipulation, right? And by direct manipulation, I don't mean

87
00:08:11,280 --> 00:08:15,440
just the sort of Ben Schneiderman version of the term, which is, you know, associated with

88
00:08:15,440 --> 00:08:20,960
graphical user interfaces and having a representation on screen that you can manipulate and undo

89
00:08:20,960 --> 00:08:26,240
redo and things like that. But what I mean here is the deeper treatment of direct manipulation

90
00:08:26,240 --> 00:08:31,680
that three cognitive scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about

91
00:08:31,680 --> 00:08:38,640
the mid-1980s. So in particular, in Hutchins et al's treatment of direct manipulation, they sort

92
00:08:38,640 --> 00:08:44,320
of, you know, imagine direct manipulation to be this cognitive process between a user's goals

93
00:08:44,320 --> 00:08:50,880
and the user interface. And, you know, they identify this gulf of execution that exists when a user

94
00:08:50,880 --> 00:08:56,800
has to translate their goals into commands that they execute on the user interface. And similarly,

95
00:08:56,880 --> 00:09:02,320
a return gulf of evaluation when a user has to figure out, well, did the UI do the thing that

96
00:09:02,320 --> 00:09:06,720
I was expecting it to do? Right? I'm seeing a lot of nods in the audience because, you know, if you've

97
00:09:06,720 --> 00:09:12,880
had experience in user interaction design, user experience, you've maybe experienced these terms

98
00:09:12,880 --> 00:09:18,880
gulf of evaluation and execution. But what I find interesting in this 1985 paper is that they went

99
00:09:18,880 --> 00:09:24,960
one level deeper. So in particular, they identified this idea of a semantic distance,

100
00:09:24,960 --> 00:09:32,640
which is basically how users take the fuzzy notions in their head and translate those into

101
00:09:32,640 --> 00:09:39,680
the nouns and verbs of the user interface, right? So going, doing that sort of sense-meaning operation

102
00:09:39,680 --> 00:09:46,320
of transforming your intentions into the particular actions that might exist in the user interface.

103
00:09:47,120 --> 00:09:51,600
And in addition to the semantic distance, they identified what I love. I love this term in

104
00:09:51,600 --> 00:09:56,720
articulatory distance, right? So it's not necessarily the meaning that we care about anymore,

105
00:09:56,720 --> 00:10:02,080
but the way in which we're conveying that meaning through the UI. And this is particularly important

106
00:10:02,080 --> 00:10:08,400
because you might have several user interfaces that all express the same semantics, right? You can

107
00:10:08,400 --> 00:10:14,720
conduct the same set of, you know, operations with them, the same nouns and verbs. But the way you

108
00:10:14,720 --> 00:10:19,600
do that might be different because one interface might be graphical, the other one might be textual,

109
00:10:19,600 --> 00:10:24,720
another one might be conversational, gesture-oriented, etc. And their claim in this paper was that

110
00:10:24,720 --> 00:10:30,800
that articulation, the form of that meaning is really, really important, just as important as

111
00:10:30,800 --> 00:10:36,160
the semantics. And of course, these distances exist on the Gulf of Evaluation as well. So the

112
00:10:36,160 --> 00:10:42,880
articulatory distance is how do I perceive the changes that occurred in the UI and start to bring

113
00:10:42,880 --> 00:10:50,640
meaning to that perceptual operation by interpreting and evaluating the degree to which

114
00:10:50,640 --> 00:10:56,640
they met my goals. So this is actually going to give us the kind of conceptual machinery for

115
00:10:56,640 --> 00:11:01,920
the rest of the talk. And it's a little bit dense. And so I want to return to sort of the prior two

116
00:11:01,920 --> 00:11:07,600
examples and think about how they manifest these two kinds of distances. So in the case of the

117
00:11:07,600 --> 00:11:12,560
chat GPT example, you know, if we start with semantic distance, I would say that, well, the

118
00:11:12,560 --> 00:11:17,680
semantics aren't really well defined, right? They're not really explicit. Because what these models

119
00:11:17,680 --> 00:11:23,840
have done is they've learned over, you know, vast corpuses of text, often just text that is present

120
00:11:23,840 --> 00:11:28,960
on the internet. And so what they've learned is this latent space that is very ambiguous in the

121
00:11:28,960 --> 00:11:34,240
semantics that are encoded in that latent space. So as a user, it's hard for me to know how to

122
00:11:34,320 --> 00:11:39,360
translate my intentions into something that the system can understand because I don't know what

123
00:11:39,360 --> 00:11:46,720
it is the system knows about the world. But as I'm sure many of us are aware, like prompt engineering

124
00:11:46,720 --> 00:11:52,400
is a thing, right? So if I figure out exactly how to craft my, you know, natural language

125
00:11:52,400 --> 00:11:57,760
expression, suddenly I can get the model to very rapidly almost zero shot adopt the semantics that

126
00:11:57,760 --> 00:12:03,120
I want. And that feels like a very powerful, you know, affordance that we've not necessarily had

127
00:12:03,120 --> 00:12:10,720
before. On the other side, you know, the semantic distance in the Jupiter notebook in B2 had explicitly

128
00:12:10,720 --> 00:12:15,600
defined semantics, right? We have the explicit semantics of pandas on the data frame of the

129
00:12:15,600 --> 00:12:20,960
visualization library of being able to click on the fields in a graphical user interface to produce

130
00:12:20,960 --> 00:12:27,040
visualizations. And every time I did that, I had the shared representation of the code. So either I

131
00:12:27,040 --> 00:12:30,480
would offer the code and it would produce a visualization or if the system produced some

132
00:12:30,480 --> 00:12:36,320
code, I could go in and comment and uncomment entries or tweak the code in a particular way

133
00:12:36,320 --> 00:12:41,440
and things like that. And so it gave me the shared representation that allowed me to bridge

134
00:12:41,440 --> 00:12:49,200
between input and output mechanisms really, really easily. With articulatory distance in chat

135
00:12:49,200 --> 00:12:56,000
GPT, right, natural language, it's been enormously powerful because it's reduced the sort of learning

136
00:12:56,080 --> 00:13:01,280
threshold for a lot of things, right? So if I don't know exactly what it is I want or how to

137
00:13:01,280 --> 00:13:07,440
sort of pose it to the question, I can lean into the ambiguity of natural language and chat GPT

138
00:13:07,440 --> 00:13:13,760
catches up to my intentions pretty rapidly, which is great. But conversely, sometimes I know exactly

139
00:13:13,760 --> 00:13:19,760
what it is I want. And it's really frustrating to have to express precise operations through the

140
00:13:19,760 --> 00:13:26,000
ambiguity of natural language. And then as a result, because of the fact that natural language

141
00:13:26,000 --> 00:13:30,320
is the only mechanism so far by which we can interact with many of these models, there's a

142
00:13:30,320 --> 00:13:35,760
disconnect if your output is visual, like the case of visualization. So I can't interact with

143
00:13:35,760 --> 00:13:41,040
the visualizations in any way to do subsequent back and forth interactions with the model. Now,

144
00:13:41,040 --> 00:13:45,280
I don't think the second point is sort of a fundamental limitation, but it's certainly, you

145
00:13:45,280 --> 00:13:51,280
know, the state of where we are today. And on the other hand, with, you know, Jupiter Notebook and

146
00:13:51,280 --> 00:13:56,480
B2, with the articulatory distance, we've got basically the inverse of this, right? We've got a

147
00:13:56,480 --> 00:14:02,160
nice precise programmatic syntax. So if I know that syntax, I can work really, really efficiently,

148
00:14:02,160 --> 00:14:08,560
right? Sort of a common affordance of many sort of command line style interfaces. But I really

149
00:14:08,560 --> 00:14:14,320
need to learn that syntax to be effective. And in some cases with pearly design syntaxes, which

150
00:14:14,400 --> 00:14:19,840
I might maybe argue, Pandas is an example of, right? I constantly have to look up the documentation

151
00:14:19,840 --> 00:14:24,800
for, right? There's a learning curve associated with it that slows people down. Yeah, Michael.

152
00:14:37,440 --> 00:14:42,000
Yeah, so the reason I put it, I think this is a great question, you know, what lies in semantic

153
00:14:42,000 --> 00:14:47,040
and articulatory. And oftentimes it is quite a fuzzy distinction. The reason I put this in

154
00:14:47,040 --> 00:14:53,120
articulatory is my experience with Pandas oftentimes is I know what it is I want to do, right? I know

155
00:14:53,120 --> 00:14:57,920
the sort of operation I want to perform on my, on my data frame. I just don't know the specific

156
00:14:57,920 --> 00:15:04,880
syntax that I need to look up. Exactly, exactly. But certainly, you know, if, if you don't know

157
00:15:04,880 --> 00:15:09,280
what it is you want to do, then the affordances of natural language absolutely help because you

158
00:15:09,280 --> 00:15:14,960
can kind of, you know, pose things in really fuzzy ways and, and kind of iterate towards your outcome.

159
00:15:17,600 --> 00:15:21,840
And, and I think you see some of this ambiguity in, in sort of, you know, the distinction between

160
00:15:21,840 --> 00:15:27,280
semantic and articulatory distance here with this, this last point where because there are consistent

161
00:15:27,280 --> 00:15:33,680
semantics that actually has this knock on effect on the articulation because now there's a shared

162
00:15:33,680 --> 00:15:38,720
representation of input and output and that simplifies that articulatory distance as well.

163
00:15:39,280 --> 00:15:44,960
So there's not quite that disconnect that we see on the chat GPT side. And so, you know, that's a,

164
00:15:44,960 --> 00:15:51,200
that's, you know, I found semantic and articulatory distances to be a really helpful sort of framework

165
00:15:51,200 --> 00:15:57,840
and I wanted to use it to sort of analyze the very last step in the output that that demo produced. So

166
00:15:57,840 --> 00:16:02,960
it, it, you know, it's basically this, this thing that masquerades as an explanation of the

167
00:16:02,960 --> 00:16:08,400
visualizations that chat GPT produced. But if you actually look at what it says, right, here's some

168
00:16:08,400 --> 00:16:13,520
basic visualizations. Number one, a histogram of song durations colon. This shows the distribution

169
00:16:13,520 --> 00:16:18,720
of song durations in seconds. All right, fair enough. Scatter plot of song hotness versus artist

170
00:16:18,720 --> 00:16:23,600
familiarity. This shows the relationship between song hotness and artist familiarity. Well, I would

171
00:16:23,600 --> 00:16:29,120
hope so. And then bar chart of the top 10 most frequent artist names. This shows the top 10 most

172
00:16:29,120 --> 00:16:35,280
frequent artist names in the data set, right. These are not really explanations, but they're

173
00:16:35,280 --> 00:16:40,720
pretty provocative or evocative in the potential that these models might have in allowing us to

174
00:16:40,720 --> 00:16:46,400
produce these textual descriptions of visual artifacts. And certainly, you know, a lot of,

175
00:16:46,400 --> 00:16:51,840
of, of people, certainly lots of big tech companies have thought about the ways in which you could use

176
00:16:52,640 --> 00:16:57,440
all kinds of machine learning models, not just LLMs to do the sort of rich description of visual

177
00:16:57,440 --> 00:17:03,120
content and particularly for this sort of these accessibility use cases, like how do you describe

178
00:17:03,200 --> 00:17:09,120
these kinds of artifacts to people who are blind or have low vision. And lots of people have studied

179
00:17:09,120 --> 00:17:13,760
the degree to which these models are effective and found maybe unsurprisingly that they're not

180
00:17:13,760 --> 00:17:19,120
terribly effective right now, right. So here is a quote from a participant from one of our studies

181
00:17:19,120 --> 00:17:23,680
who says, you know, the reader wouldn't get much insight from texts like this, which not only,

182
00:17:23,680 --> 00:17:28,640
you know, is problematic because it doesn't effectively convey information, but more troublingly,

183
00:17:28,640 --> 00:17:33,120
it actually increases the burden that readers face when they're trying to make sense

184
00:17:33,120 --> 00:17:38,320
of this output, right. There's a lot of sort of noise that gets added to that experience.

185
00:17:39,120 --> 00:17:43,840
Another participant, you know, says very, very interestingly, the problem with these textual

186
00:17:43,840 --> 00:17:50,560
descriptions is also that it robs me of control of consuming the data, right. A participant,

187
00:17:50,560 --> 00:17:55,280
another participant said, I want to have the time and space to interpret the numbers for myself

188
00:17:55,280 --> 00:18:01,760
before I read any kind of textual description that does the analysis for me. And so to me,

189
00:18:01,760 --> 00:18:07,280
these sound very similar to issues associated with a semantic and an articulatory distance,

190
00:18:07,280 --> 00:18:12,240
right. That first quote talking about, well, these texts aren't conveying anything interesting.

191
00:18:12,960 --> 00:18:17,360
The second set talking about, well, I want to have that time and space, I want to be able to

192
00:18:17,360 --> 00:18:24,240
control the form with which that text is conveyed to me. And so I want to dig into how we might

193
00:18:24,240 --> 00:18:29,040
address these two distances in the case of accessibility. But before I do that, I want to

194
00:18:29,040 --> 00:18:35,440
give us a sense of how people who are blind or have low vision experience, you know, the

195
00:18:36,240 --> 00:18:41,360
internet and graphical interfaces today. So I'm going to turn things over to my PhD student,

196
00:18:41,360 --> 00:18:46,720
Jonathan Zhang, who will give us a quick demo of an assistive technology called a screen reader

197
00:18:46,720 --> 00:18:49,120
that basically narrates on-screen content.

198
00:18:54,240 --> 00:19:11,120
So here I can demonstrate what the accessible HTML version of our paper looks like to a screen reader.

199
00:19:24,960 --> 00:19:35,840
So as you can see, what a screen reader does is it basically sort of linearizes the operation

200
00:19:35,840 --> 00:19:43,680
of, you know, reading, perceiving, understanding graphical content on a user interface. And in

201
00:19:43,680 --> 00:19:47,920
particular, you might notice that the narration was actually quite rapid, right. And this is

202
00:19:47,920 --> 00:19:54,960
actually a slowed down version of what, you know, proficient screen reader users use, which is often

203
00:19:54,960 --> 00:20:01,040
much, much faster. But what is interesting about the screen reader use case is that it forces that

204
00:20:01,040 --> 00:20:07,040
linearity, right. And the key challenge in figuring out the articulatory distance in the case of

205
00:20:07,040 --> 00:20:12,160
accessibility is how do you take visualizations that probably all of us in the audience have

206
00:20:12,160 --> 00:20:16,560
slightly subtly different ways of reading, right. Maybe some of you start by reading the title,

207
00:20:16,560 --> 00:20:21,600
then moving to the axes, then looking at, you know, the shapes, while others might start by

208
00:20:21,600 --> 00:20:27,280
looking at the most salient trend and then start to, you know, map out to what the axes and legends

209
00:20:27,280 --> 00:20:33,280
and stuff like that are. How do we take all of that rich diversity, but linearize it? So the

210
00:20:33,280 --> 00:20:38,800
people who use screen readers can nevertheless have that same, you know, choice in meeting a

211
00:20:38,800 --> 00:20:45,440
visualization, but under these conditions. And so the way we have chosen to do that is basically

212
00:20:45,440 --> 00:20:52,880
by restructuring the content of a visualization into a text-oriented hierarchy. So at the top,

213
00:20:52,880 --> 00:20:57,600
at the root of this hierarchy is just a summary of the chart, probably the trends that are shown

214
00:20:57,600 --> 00:21:03,760
in the chart. And then the hierarchy branches off into the individual sort of data fields

215
00:21:03,760 --> 00:21:08,960
or the encodings in this case, right. The x-axis, the y-axis, the legend and things like that.

216
00:21:08,960 --> 00:21:15,120
And then people can start to drill down in ways that maintain some correspondence with the visual

217
00:21:15,120 --> 00:21:21,600
artifacts. So one step below, you know, the x-axis is stepping through them by the major ticks,

218
00:21:21,600 --> 00:21:25,200
right. One step below the major ticks would be minor ticks and then ultimately you would get

219
00:21:25,200 --> 00:21:29,760
to the individual data points. So let me throw things back to Jonathan to give us a demo of how

220
00:21:29,760 --> 00:21:37,920
this works. A scatter plot of Penguin data. And to a screen reader, our system represents this

221
00:21:37,920 --> 00:21:44,480
scatter plot as a keyboard navigable data structure that contains text descriptions at

222
00:21:44,480 --> 00:21:51,200
varying levels of detail. So when a screen reader user first encounters this visualization on a page,

223
00:21:51,200 --> 00:21:56,720
they'll be able to read off a high-level alt text description of what the chart is. So

224
00:21:59,840 --> 00:22:04,960
and if they're interested in getting more detail about this visualization, they can dive in by

225
00:22:04,960 --> 00:22:11,840
pressing the down arrow key to descend one level in the hierarchy and access descriptions about

226
00:22:11,840 --> 00:22:15,760
the different encodings of the scatter plot. So I'm going to press the down arrow key.

227
00:22:19,760 --> 00:22:25,120
I can press the left and right arrow keys to flip through descriptions of the other axes and

228
00:22:25,120 --> 00:22:40,640
legends. Cool, so let's say I am interested in getting more information about the x-axis. I can

229
00:22:41,360 --> 00:22:46,800
use the left arrow key to navigate back to the x-axis description and then press down one more

230
00:22:46,800 --> 00:23:01,360
time to descend a level of detail into the x-axis. So on this level underneath the x-axis, I'm

231
00:23:01,360 --> 00:23:08,240
accessing descriptions of intervals along the x-axis and it's reading out to me how many data

232
00:23:08,240 --> 00:23:12,640
values are contained within each interval. So by pressing left and right, I can kind of get

233
00:23:12,640 --> 00:23:26,240
a sense of the distribution of data along the x-axis. So let's say I am interested in this

234
00:23:26,240 --> 00:23:32,800
range from 190 to 200. I can then press down arrow again to dive into the individual data points

235
00:23:32,800 --> 00:23:48,560
that are contained within this interval. So let's say that instead of moving up and down

236
00:23:48,560 --> 00:23:54,160
this hierarchical structure, I would rather just move around the x-y grid in the scatter plot,

237
00:23:54,160 --> 00:24:00,720
as if I were kind of feeling around a tactile graphic, for example. I can start by navigating

238
00:24:00,720 --> 00:24:11,200
over to the grid view of the scatter plot. And once I descend into this part of the hierarchy,

239
00:24:11,200 --> 00:24:17,760
I can use the WASD keys to move up and down different squares along the grid.

240
00:24:17,920 --> 00:24:31,600
And so similarly to before, it's starting off by giving me the number of data values that are

241
00:24:31,600 --> 00:24:36,320
contained in that square so that I can get a sense of the distribution of the data.

242
00:24:37,680 --> 00:24:45,920
And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges.

243
00:24:45,920 --> 00:24:52,720
And this was the first time he felt like he actually understood and could build a mental

244
00:24:52,720 --> 00:24:59,120
model of what it was that a scatter plot was representing. We saw these sorts of comments

245
00:24:59,120 --> 00:25:05,360
reflected in user studies that we ran about how the form of this textual output really

246
00:25:05,360 --> 00:25:10,720
influenced participants' mental model of what the data was, what the trends were, and things like that.

247
00:25:10,720 --> 00:25:15,520
So one participant, for instance, said, I now know how to drill down and up between different

248
00:25:15,600 --> 00:25:21,600
layers in the data to get an overall picture. And it gives me a different way of thinking.

249
00:25:21,600 --> 00:25:26,400
And another one said, I'm thinking more in spatial terms because this is just a new method

250
00:25:26,400 --> 00:25:32,320
for navigating and moving through the grid and drilling down to information and things like that.

251
00:25:32,320 --> 00:25:39,120
And so what I find interesting here is that at every step, the semantic content stayed exactly

252
00:25:39,120 --> 00:25:44,160
the same. And there wasn't even very rich semantic content. It was a range and then a count of the

253
00:25:44,160 --> 00:25:50,640
data values. All we manipulated was that articulation, that form, giving it a hierarchical

254
00:25:50,640 --> 00:25:56,800
nature, adding all of these different navigational affordances, and just manipulating the articulation

255
00:25:56,800 --> 00:26:02,640
had this huge impact on people's mental models of the data. And I think that we're really just at

256
00:26:02,640 --> 00:26:08,080
the tip of the iceberg of these more accessible structures. Currently, in my group, we're thinking

257
00:26:08,080 --> 00:26:14,240
about just the impact that token order has on how people using screen readers build up those

258
00:26:14,240 --> 00:26:19,680
mental models. If you're constantly prompting them with the range first rather than the actual data

259
00:26:19,680 --> 00:26:25,520
values, does that introduce friction to their capacity to build that mental model and things

260
00:26:25,520 --> 00:26:32,880
like that? But in all of this, where is semantic distance? How do we actually start to make that

261
00:26:32,880 --> 00:26:38,000
textual descriptions more interesting and meaningful? And this is where I think LLMs can

262
00:26:38,000 --> 00:26:44,240
really help us. For one reason, it's because there's just a sheer amount of textual content we need

263
00:26:44,240 --> 00:26:50,400
to be able to produce that is infeasible to expect a human to sort of manually author.

264
00:26:50,400 --> 00:26:54,880
But there are other sort of implications that we'll touch upon really shortly.

265
00:26:54,880 --> 00:26:59,360
But before we can get LLMs to actually sort of produce the content we want,

266
00:26:59,360 --> 00:27:05,840
what we need to do is shift from that very latent space with implicit semantics to a set of explicit

267
00:27:05,840 --> 00:27:12,720
semantics. We need to impose a conceptual model onto our LLMs. Or another way of saying that is

268
00:27:12,720 --> 00:27:17,760
we need to get the LLMs to understand what a good textual description of a visualization is.

269
00:27:18,560 --> 00:27:24,480
And so that's what my then PhD student, Alan Lungard, set out to do. We ran a crowdsource study

270
00:27:24,480 --> 00:27:31,520
where we got sort of 2,000 descriptions of charts. And through qualitative coding,

271
00:27:31,520 --> 00:27:36,000
we realized there are basically four kinds of semantic content that textual descriptions should

272
00:27:36,000 --> 00:27:42,080
convey. The first most primitive layer is basically just the sort of construction details of the chart.

273
00:27:42,080 --> 00:27:48,480
What are the titles, the labels, the scales, the units, etc. And accessibility best practices say

274
00:27:48,480 --> 00:27:53,680
that this is some of the most important content to convey because it gives people sort of important

275
00:27:53,760 --> 00:27:59,200
milestones and landmarks. One level above that are the sort of statistical properties like minimum,

276
00:27:59,200 --> 00:28:04,880
maximum, outliers, and things like that. And then one level above that is probably what is cited

277
00:28:04,880 --> 00:28:10,080
people we consider the real value of visualization to be. The perceptual and cognitive characteristics

278
00:28:10,080 --> 00:28:15,680
like complex trends and patterns, things that automated statistical methods we typically think

279
00:28:15,680 --> 00:28:21,520
of as not being sufficient at. And then finally, the fourth and highest level are what journalists

280
00:28:21,520 --> 00:28:26,240
often consider to be the real value of visualization, which is the narration that gets associated

281
00:28:26,240 --> 00:28:30,800
with it. What is the data story that you're able to tell through the visualization? Can you explain

282
00:28:30,800 --> 00:28:36,640
what you're seeing, the causal mechanisms, etc., etc. Now, another reason I think LLMs are really

283
00:28:36,640 --> 00:28:42,880
suited for this sort of semantic bridging task is because when we asked sighted and blind people

284
00:28:42,880 --> 00:28:48,560
what their preferences were, when it came to these four layers, four levels, we saw really

285
00:28:48,560 --> 00:28:55,600
distinct preferences. In the case of sighted people, because we've got our own visual perception

286
00:28:56,640 --> 00:29:03,120
doing that sort of bridging of the Gulf of Evaluation, sighted people tended to want higher

287
00:29:03,120 --> 00:29:08,240
and higher levels of content being conveyed through text. Blind readers, on the other hand,

288
00:29:08,240 --> 00:29:14,000
were pretty significantly divergent. For many of them, they didn't want those level three and four,

289
00:29:14,000 --> 00:29:18,800
particularly the level four captions at all, because they wanted that time and space to do

290
00:29:18,800 --> 00:29:28,080
the interpretation for themselves. And so here, this visualization to me conveys that LLMs can

291
00:29:28,080 --> 00:29:33,200
help us or machine learning models can help us think about sort of personalizing the semantics

292
00:29:33,200 --> 00:29:39,120
of a user interface in a way that maybe we haven't had the opportunity to study so far. There's been

293
00:29:39,120 --> 00:29:44,400
a lot of work in personalization, but it's often been at that level of the articulation,

294
00:29:44,400 --> 00:29:49,520
changing the sizes of buttons and adapting color palettes and things like that. And there's maybe

295
00:29:49,520 --> 00:29:57,760
an opportunity now to use LLMs to actually change what the nouns, the verbs, the concepts of a user

296
00:29:57,760 --> 00:30:03,120
interface are much more fundamentally. And so the way we're going about doing this in the case of

297
00:30:03,120 --> 00:30:08,960
textual descriptions is we're going to be releasing very soon a data set of about, actually now we're

298
00:30:08,960 --> 00:30:13,760
over 12,000 pairs of chart captions. And we've generated some of these captions and we've crowdsourced

299
00:30:13,760 --> 00:30:19,200
some of these captions. And we started to train baseline models to do this task. And one of the

300
00:30:19,200 --> 00:30:25,920
interesting features here is how do we represent the semantics of a chart to a large language model,

301
00:30:25,920 --> 00:30:30,640
right? One way could just be let's treat the chart as an image, right? This is just a set of pixels.

302
00:30:31,520 --> 00:30:36,960
And unsurprisingly, the baseline models don't do very well at that because a chart is a much richer

303
00:30:36,960 --> 00:30:41,920
kind of artifact than just an image, right? It's got all this rich structure. So then we said,

304
00:30:41,920 --> 00:30:46,160
great, let's look at a data table or let's look at a scene graph, which is just a fancy way of

305
00:30:46,160 --> 00:30:51,200
saying the SVG associated with the chart. And a priori, we would have thought, well, the scene

306
00:30:51,200 --> 00:30:56,400
graph is maybe like a good in-between between the computational affordances of data table and

307
00:30:56,400 --> 00:31:02,480
capturing some of those perceptual characteristics. Turns out for the LLMs we trained that were

308
00:31:02,480 --> 00:31:07,360
all transformer models, they did equivalently well on those two representations. And so one of

309
00:31:07,360 --> 00:31:12,800
the things my group is working on right now is a new way of representing visualizations that more

310
00:31:12,800 --> 00:31:19,120
directly encode some of those perceptual operations that are otherwise currently implicit in a scene

311
00:31:19,120 --> 00:31:26,400
graph that grammar of graphics libraries like VegaLite or GGplot perform. But what's interesting

312
00:31:26,400 --> 00:31:31,680
in all of this to me is that through these generative models, the goal has been how do we impose

313
00:31:31,760 --> 00:31:38,160
a conceptual model onto them, right? How do we bring some explicit semantics? And I think we're

314
00:31:38,160 --> 00:31:43,680
just scratching the surface here as well because I think the chart example case is a really great

315
00:31:43,680 --> 00:31:48,240
one where a lot of these representations of charts that we've got right now, the grammar of graphics,

316
00:31:48,240 --> 00:31:52,960
for instance, were designed for people to author, right? So we're really good at figuring out how

317
00:31:52,960 --> 00:31:58,080
to design programming languages, domain specific languages, to emphasize the cognitive characteristics

318
00:31:58,080 --> 00:32:02,560
that are important for human authors. Things like, you know, the cognitive dimensions of notation

319
00:32:02,560 --> 00:32:06,880
that cares about, you know, how viscous is the programming language? How many premature commitments

320
00:32:06,880 --> 00:32:11,840
is the programming language enforced? But I don't know what it means to design a representation

321
00:32:11,840 --> 00:32:18,240
to be suitable for an LLM to operate over, right? Do we restructure the programming language more

322
00:32:18,240 --> 00:32:24,720
fundamentally to make it tractable for an LLM? Maybe. So in addition to generative models,

323
00:32:24,720 --> 00:32:30,560
my group has also been working with predictive models. And here I think the bridging task is

324
00:32:30,560 --> 00:32:37,040
really not about imposing a conceptual model, but bridging it or aligning it to the ones that we

325
00:32:37,040 --> 00:32:42,640
already have. And often the way that a lot of this work happens is through the lens of model

326
00:32:42,640 --> 00:32:50,480
interpretability. So here is a very popular set of techniques called saliency maps. The idea behind

327
00:32:50,480 --> 00:32:58,000
saliency maps is they're trying to depict the most important input features for a particular

328
00:32:58,000 --> 00:33:03,280
outcome. So in this case, you know, this is an image, the label should be toy terrier, and here's

329
00:33:03,280 --> 00:33:08,560
what a variety of different kinds of saliency methods believe to be, you know, the most important

330
00:33:08,560 --> 00:33:17,840
pixels to produce that outcome. Now, I look at these visualizations and I go, well, you know,

331
00:33:17,840 --> 00:33:24,400
is it telling me something? Maybe, right? And maybe the reason I believe it's telling me something

332
00:33:24,400 --> 00:33:30,720
is because I'm the one doing the perception and interpretive tasks, right? Like if I look at some

333
00:33:30,720 --> 00:33:35,120
of those visualizations on the bottom, I go, oh, like, it looks like the dog snout is really

334
00:33:35,120 --> 00:33:40,560
important to the classification of a toy terrier or the spots. But it's not actually the saliency

335
00:33:40,560 --> 00:33:45,600
method that is doing that interpretation for me. I'm the one bringing meaning to those lit pixels,

336
00:33:46,160 --> 00:33:52,320
right? And so as a result, if we think about that gulf of evaluation, it's not the saliency

337
00:33:52,320 --> 00:33:58,080
method that's helping bridge that gulf in any way, which is why saliency maps for now have been

338
00:33:58,080 --> 00:34:03,840
these tools that we just use in a very ad hoc way that require a lot of manual effort to make sense

339
00:34:03,840 --> 00:34:10,800
of. And so a question that my student, Angie Boggast, has been focused on is how do we scaffold

340
00:34:10,800 --> 00:34:15,920
that semantic sense making operation, right? Providing some additional structure to help

341
00:34:16,560 --> 00:34:21,680
sort of scale it up to make it more reproducible and things like that. And what she's developed

342
00:34:21,680 --> 00:34:27,440
is these set of metrics that are very analogous to ideas of precision and recall, but are operating

343
00:34:27,440 --> 00:34:32,960
at the level of input features and interpretability. So in many data sets, right, you've got some set

344
00:34:32,960 --> 00:34:38,080
of ground truth human annotated features. And what shared interest is looking at is what is the

345
00:34:38,080 --> 00:34:43,200
overlap between what a saliency method considers as being important to the classification and what

346
00:34:43,200 --> 00:34:48,240
the humans, the human annotators thought was important. And there's actually three different

347
00:34:48,240 --> 00:34:54,240
ways that these overlaps can manifest. The first is a sort of ground truth coverage, which is very

348
00:34:54,240 --> 00:34:59,360
analogous to ideas of recall, right? It's how much of the ground truth does the model incorporate

349
00:34:59,360 --> 00:35:04,000
in its prediction or what is the proportion of the ground truth region that is covered by the

350
00:35:04,000 --> 00:35:09,200
saliency region. And if we look at some examples of low coverage on the top and high coverage at the

351
00:35:09,200 --> 00:35:14,640
bottom, we can see then the case of low ground truth coverage is actually very little overlap,

352
00:35:14,640 --> 00:35:20,400
right, between the ground truth, the yellow region, and the salient region in orange. But I often find

353
00:35:20,400 --> 00:35:25,600
that it's actually the high coverage regions that are more interesting to analyze. So if we compare,

354
00:35:25,600 --> 00:35:31,200
you know, cases where the model was correct on the right with the green label and cases where the

355
00:35:31,200 --> 00:35:36,640
model was incorrect with the red label, we can see in the case of correct high ground truth coverage,

356
00:35:36,640 --> 00:35:41,040
there are instances where the model relies not just on the object, like in this case with the

357
00:35:41,040 --> 00:35:47,840
cab, but a lot of contextual information as well to ultimately make that correct prediction. But

358
00:35:47,840 --> 00:35:53,920
on the flip side, right, with the laptop, the model is doing the same thing, but here the context

359
00:35:53,920 --> 00:35:58,000
is actually throwing it off, right? It's actually confusing the model because it's accounting for

360
00:35:58,000 --> 00:36:03,680
too much of that context in its decision making. Another kind of coverage is something we call

361
00:36:03,680 --> 00:36:08,480
saliency coverage, and this is more akin to precision, right, which is how strictly is the

362
00:36:08,480 --> 00:36:14,800
model relying only on ground truth features to make its sort of prediction. And again, you know,

363
00:36:14,800 --> 00:36:20,400
if we look at low and high coverage in the case of low coverage, we can see again pretty disjoint

364
00:36:21,120 --> 00:36:27,360
sorts of sets. But in the case of the high coverage regions, we can see that, you know,

365
00:36:27,360 --> 00:36:33,920
in the case of high saliency coverage, it basically means that the salient regions are a strict subset

366
00:36:33,920 --> 00:36:38,560
of the ground truth coverage. But the difference between a correct and incorrect prediction is

367
00:36:38,560 --> 00:36:45,360
whether that subset was sufficient to make the correct classification or not, right? So in the

368
00:36:45,360 --> 00:36:50,240
case of the Maltese dog, it did indeed only need to look at the head to make that correct prediction.

369
00:36:50,560 --> 00:36:56,960
But in the case of the Dalmatian, it probably should have accounted for more of that dog's head or

370
00:36:56,960 --> 00:37:01,280
some of the other characteristics associated with the dog. By focusing only on the snout,

371
00:37:01,280 --> 00:37:07,440
it ended up sort of arriving at the incorrect sort of classification. And finally, the last metric

372
00:37:07,440 --> 00:37:11,920
is something that is very familiar IOU, the intersection over the union. This is sort of

373
00:37:11,920 --> 00:37:16,880
the strictest shared interest metric. It's really measuring how aligned the model's behavior is

374
00:37:16,880 --> 00:37:21,760
with human reasoning. So if you look at some examples, again, you know, low coverage at the

375
00:37:21,760 --> 00:37:29,520
top, we can see, you know, in incorrect cases, totally distinct disjoint sets again. But in a

376
00:37:29,520 --> 00:37:34,720
correct instance, I actually find that pretty interesting, right? Low IOU coverage, but it

377
00:37:34,720 --> 00:37:39,920
got a correct classification. Now, one could say maybe it got lucky. But potentially, what

378
00:37:39,920 --> 00:37:46,320
the signal there is, is that maybe all the model needs is a tiny bit of a wheel associated with

379
00:37:46,320 --> 00:37:52,000
a horse, right, to make the prediction that is actually a horse cart and not just a horse, right?

380
00:37:52,000 --> 00:37:58,080
And on the flip side, with high coverage, you know, Newfoundland, great, you know, total,

381
00:37:58,080 --> 00:38:02,960
total alignment. But in this case, right, incorrect classification, even though there was high

382
00:38:02,960 --> 00:38:08,560
coverage, this might suggest, you know, genuinely difficult to classify images, right, even for

383
00:38:08,560 --> 00:38:13,600
people. Because if I look at that, a pickup truck seems a totally reasonable guess to have made

384
00:38:13,600 --> 00:38:17,680
about the image. I don't know that I've got enough sort of visual information there to call

385
00:38:17,680 --> 00:38:23,920
that a snowplow. So shared interest basically gives us a mechanism to start to scaffold and

386
00:38:23,920 --> 00:38:29,760
structure, bridge that semantic distance, right? People no longer necessarily need to manually

387
00:38:29,760 --> 00:38:35,840
start to analyze these things. And in fact, you know, we analyzed lots of different models across,

388
00:38:35,840 --> 00:38:40,800
you know, both vision and natural language and found that different combinations of these shared

389
00:38:40,800 --> 00:38:45,040
interest metrics, along with figuring out whether the prediction was correct or not,

390
00:38:45,040 --> 00:38:50,800
actually surfaced eight kinds of repeating patterns in model behavior. So we can see human

391
00:38:50,800 --> 00:38:56,240
aligned and some of these others we also looked at earlier, right, context confusion,

392
00:38:56,240 --> 00:39:01,760
context dependent and so forth. And all of these give us sort of semantics that we can start to

393
00:39:01,760 --> 00:39:07,040
play around with through different articulations. So one articulation of these semantics might be

394
00:39:07,040 --> 00:39:12,480
a very traditional visual analytics interface, right, where I've got all the different kinds of

395
00:39:13,200 --> 00:39:17,920
images that I care about. This is a system we built to help a board certified dermatologist

396
00:39:17,920 --> 00:39:22,560
make sense of this melanoma detection model. And you've got, you know, query widgets on the top

397
00:39:22,560 --> 00:39:28,720
to sort and filter. You can use, you know, these histograms of the shared interest metrics to really

398
00:39:28,720 --> 00:39:33,520
drill into the data. But what was maybe most interesting was what the dermatologist said

399
00:39:33,520 --> 00:39:41,680
when they started to analyze that recurring pattern of context dependent cases. So in particular,

400
00:39:41,680 --> 00:39:47,760
when they switched to these context dependent cases, the dermatologist started to wonder if the

401
00:39:47,760 --> 00:39:52,640
model is seeing something we are not truly appreciating in the clinical image. Maybe there

402
00:39:52,640 --> 00:39:59,200
are subtle changes we don't yet understand that the model does, right at the boundaries of the

403
00:39:59,280 --> 00:40:06,240
skin region and things like that. And so, you know, to me, this is alluding to the fact of,

404
00:40:06,240 --> 00:40:12,320
well, can we as domain experts learn something about our problem domain based on how it is

405
00:40:12,320 --> 00:40:17,760
models are operating? And I think we see this more clearly in another articulation of shared

406
00:40:17,760 --> 00:40:22,800
interest semantics. Here, what we're doing is basically using shared interest to interactively

407
00:40:22,800 --> 00:40:29,040
probe or query that latent space. So we're brushing and using that brushed region as ground truth

408
00:40:29,360 --> 00:40:35,840
and then calculating the IOU coverage to figure out what are all the classes that maximize IOU

409
00:40:35,840 --> 00:40:42,720
coverage for that brush ground truth. So we can see if I brush over hand, a lot of the classes

410
00:40:42,720 --> 00:40:47,760
that get returned are things that are often associated with hands like laptops and cleavers

411
00:40:47,760 --> 00:40:54,320
and interestingly enough, hen. So I guess a lot of the images in the ImageNet, you know, data set

412
00:40:54,400 --> 00:41:01,840
have people holding hens, right, which is, I guess, kind of interesting. But more maybe profoundly

413
00:41:01,840 --> 00:41:08,000
is we could ask a question like, what is the essence of a dog, right? What is the minimal

414
00:41:08,000 --> 00:41:14,080
amount of region that I would need to brush for the model to still be convinced that what it is

415
00:41:14,080 --> 00:41:19,280
classifying as a dog? So I could start with the whole dog and then brush just on its head and

416
00:41:19,280 --> 00:41:24,160
sure, you know, querying which shared interest still returns, you know, dog classes. But then I

417
00:41:24,160 --> 00:41:31,600
could use a smaller brush and brush just on the nose and it still returns, you know, German shepherd

418
00:41:31,600 --> 00:41:36,880
and sheepdog and Tibetan terrier and things like that. So it seems like according to the model,

419
00:41:37,440 --> 00:41:45,840
all it really needs to know about, you know, an object in the image is the sort of shape of its

420
00:41:45,840 --> 00:41:51,360
nose or something associated with its nose to be able to classify whether it is or is not a dog.

421
00:41:51,360 --> 00:41:57,040
And this seems like a really sort of toy example, but it reflects some of the things that real

422
00:41:57,040 --> 00:42:03,200
world scientists are doing. So in particular, you know, there's a researcher at the University of

423
00:42:03,200 --> 00:42:09,360
Washington, Julia Parrish that runs this grand crowdsource data collection project around seabird

424
00:42:09,360 --> 00:42:14,880
deaths. And the way they train their participants to figure out how to do bird classification

425
00:42:14,960 --> 00:42:19,840
is by asking them to measure, you know, the bird beaks and the bird feet and things like that.

426
00:42:19,840 --> 00:42:23,840
And so I think it's really interesting that we're seeing maybe some of those sorts of

427
00:42:23,840 --> 00:42:32,480
representations creep up in how a model is making its decisions as well. And so where I want to end

428
00:42:32,480 --> 00:42:38,480
is sort of being most speculative and where I think, you know, there's scope for HCI to sort of

429
00:42:38,480 --> 00:42:43,680
grow. And so, you know, we looked at generative models and imposing a conceptual model on them.

430
00:42:43,680 --> 00:42:49,120
We looked at predictive models where the idea was to align conceptual models. But what I think,

431
00:42:49,120 --> 00:42:53,840
you know, we're hearing from that dermatologist we're seeing in that last case study which shared

432
00:42:53,840 --> 00:43:01,440
interest is the potential to use machine learning models to basically discover new representations

433
00:43:01,440 --> 00:43:07,200
of particular problem domains, right? And, you know, again, at my most speculative, I don't know

434
00:43:07,200 --> 00:43:11,440
what I would call these, but I would maybe call them abstraction models, right, where the goal of

435
00:43:11,440 --> 00:43:16,880
these models is not to produce some particular outcome that I care about, but to maximize what

436
00:43:16,880 --> 00:43:21,440
are the different ways of representing the world, right? What are all the diverse abstractions that

437
00:43:21,440 --> 00:43:26,720
we could learn about a problem domain like classifying dogs or classifying seabirds or things

438
00:43:26,720 --> 00:43:32,000
like that. And I think this is a really interesting opportunity to use machine learning to essentially

439
00:43:32,000 --> 00:43:37,920
advance our understanding, advance our science. But I want to be careful here because we've already

440
00:43:38,000 --> 00:43:42,400
seen, you know, through this talk, but also in the broader discourse, how generative and predictive

441
00:43:42,400 --> 00:43:47,120
models can sort of muddy that, that gulf of evaluation, right? Lots of people are starting

442
00:43:47,120 --> 00:43:52,320
to anthropomorphize these models, you know, some people think these models are representing general

443
00:43:52,320 --> 00:43:57,120
intelligence or conscience or things like that. And there's a potential with, you know, these

444
00:43:57,120 --> 00:44:03,520
abstraction models to make this problem worse by muddying the question of, well, how do we know

445
00:44:03,600 --> 00:44:10,640
what we know, right? Like what counts as evidence? Is it evidence because, you know, the model has

446
00:44:10,640 --> 00:44:16,960
learned that representation? And how do we validate what that evidence is? In the case of

447
00:44:16,960 --> 00:44:22,240
representations that are designed or interpreted or theorized by people, we know how to consider that

448
00:44:22,240 --> 00:44:27,360
to be evidence, right? But I don't know what it means for a learned representation to count as

449
00:44:27,360 --> 00:44:32,240
evidence. And as all sorts of problems in machine learning, this is not necessarily a problem that

450
00:44:32,240 --> 00:44:39,920
is unique to machine learning. So here are three visualizations that were used to discuss the

451
00:44:39,920 --> 00:44:45,680
COVID-19 pandemic right at the, the, the peak of the first wave in the summer of 2020.

452
00:44:48,080 --> 00:44:55,280
And I'm curious if anything pops out at you, like any reason, you know, to be curious or

453
00:44:55,360 --> 00:45:05,840
suspect of, of these visualizations, right? Like no, right? Probably not. Like these seem pretty

454
00:45:05,840 --> 00:45:11,680
legitimate, right? Like our world and data, very legitimate data source, right? And if you look at,

455
00:45:11,680 --> 00:45:15,520
look at some of these two other visualizations, you might go, you know what, actually the one on the

456
00:45:15,520 --> 00:45:19,680
right, that looks like something in maybe a policy briefing or something, right? It looks very

457
00:45:19,680 --> 00:45:27,120
sophisticated, lots of good annotation, you know, a style and aesthetic that looks very sort of

458
00:45:27,120 --> 00:45:32,880
sophisticated. But you may be catching what I'm alluding to, which is the fact that all three

459
00:45:32,880 --> 00:45:40,800
visualizations were used by people on social media to advance the argument that, you know,

460
00:45:40,800 --> 00:45:47,200
our response to COVID was overblown. Not that COVID was a hoax, but that our reaction to it

461
00:45:47,200 --> 00:45:54,000
was, was way too extreme. That COVID wasn't as serious an issue as it might initially seem.

462
00:45:54,560 --> 00:46:00,080
And I want to be really careful about what I'm, when I'm doing here with these charts, because

463
00:46:00,080 --> 00:46:04,720
certainly some of the people that were distributing this were bad actors who were ideologically

464
00:46:04,720 --> 00:46:11,920
motivated. But through a very long, laborious ethnography, ethnographic process that we

465
00:46:11,920 --> 00:46:17,680
conducted, spending six months on five different Facebook groups, we found that a lot of people

466
00:46:17,680 --> 00:46:22,560
who were producing visualizations like that were actually displaying many hallmarks of citizen

467
00:46:22,560 --> 00:46:28,000
data science. So they were really many of them filling gaps in, in information sort of collection,

468
00:46:28,000 --> 00:46:32,400
because they were situated in rural parts of the country where, you know, there wasn't a lot of good

469
00:46:32,400 --> 00:46:37,920
data collection. So many members of these groups were hosting, you know, webcasts, live seminars of

470
00:46:37,920 --> 00:46:42,800
how to download data from the government website, how to clean it and excel, how to visualize it and

471
00:46:42,800 --> 00:46:48,880
things like that. And, and most surprisingly to us, many of them were engaged in discussion that

472
00:46:48,880 --> 00:46:54,560
looked like peer review, right? They were critically assessing data sources, discussing metrics,

473
00:46:55,200 --> 00:46:59,920
making arguments for which metrics were, were better or not. But all of this was sort of

474
00:46:59,920 --> 00:47:06,160
inflected through a sort of frustration with mainstream institutions and maybe even distrust

475
00:47:06,160 --> 00:47:12,640
of those institutions as well, right? But ultimately what these groups cared about was bolstering a

476
00:47:12,640 --> 00:47:18,720
sense of social unity and civic engagement, right? So this quote I, I find particularly sort of

477
00:47:18,720 --> 00:47:23,600
reflective of that sense of, you know, it's incumbent on all of us to hold our elected officials to

478
00:47:23,600 --> 00:47:28,560
account so that they make better decisions through data, right? I'm speaking to you as a neighbor,

479
00:47:28,560 --> 00:47:35,120
as a mama bear, right? So this is not some sort of ideologically motivated individual who is,

480
00:47:35,120 --> 00:47:39,520
who is, you know, trying to be a bad actor. This is just an engaged member of the citizenry.

481
00:47:40,400 --> 00:47:45,520
And similarly, you know, oftentimes they were actually more sophisticated than scientists

482
00:47:45,520 --> 00:47:50,720
can be. So many of these members were very reflexive about their own data analysis,

483
00:47:50,720 --> 00:47:55,760
data gathering process, right? So someone says, you know, I've never claimed to have no bias,

484
00:47:55,760 --> 00:48:00,560
right? I'm human, of course I'm biased, here are my biases. Whereas in science, often we like to

485
00:48:00,560 --> 00:48:06,240
portray ourselves as being very objective, you know, arbiters of truth. And so in many ways,

486
00:48:06,240 --> 00:48:12,720
you know, what was happening in these groups is, is perhaps more sophisticated than what was happening

487
00:48:12,720 --> 00:48:16,880
in science and public health at the time. But the question is, so what does this have to do with

488
00:48:16,880 --> 00:48:22,640
sort of bridging semantic distances and abstraction models? Well, I think what was happening in,

489
00:48:22,640 --> 00:48:28,400
in those groups was, you know, they, they, they disagreed with the definitions of some of these

490
00:48:28,480 --> 00:48:34,240
metrics, right? They were living in rural communities. And so the metrics that public

491
00:48:34,240 --> 00:48:39,920
health officials were using to, to, you know, define the, the state and scale of the pandemic

492
00:48:39,920 --> 00:48:43,760
was not reflected in their lived experience. They were turning around and well,

493
00:48:43,760 --> 00:48:48,480
it didn't seem like COVID was an issue, right? And so our colleagues in the humanities and

494
00:48:48,480 --> 00:48:54,720
social sciences often advocate for adopting what they call an interpretive, interpretivist lens,

495
00:48:54,720 --> 00:48:59,280
right? The idea that knowledge is subjective, it's socially constructed,

496
00:48:59,280 --> 00:49:03,920
and that it's composed of many diff, different diverse perspectives that we have to figure out

497
00:49:03,920 --> 00:49:09,680
ways to synthesize together. And while that idea has been adopted in pockets of visualization and

498
00:49:09,680 --> 00:49:15,520
HCI and CS, so far, I think it's largely been on the qualitative side, because if we think about

499
00:49:15,520 --> 00:49:22,080
how to do computation, we have to, you know, we're forced into making decisions about the world and

500
00:49:22,080 --> 00:49:26,960
how to represent that world and computational data structures. And what I think abstraction models

501
00:49:26,960 --> 00:49:32,480
allow us to do is start to push, but you know, push that boundary a little bit, right? Rather than

502
00:49:32,480 --> 00:49:38,000
being focused on developing a model that produces a single best outcome, we might instead be looking

503
00:49:38,000 --> 00:49:42,720
to a world in which we are training sort of ecosystems of abstraction models, where we're

504
00:49:42,720 --> 00:49:48,080
forcing them to learn really different representations of the world or of a problem domain,

505
00:49:48,080 --> 00:49:52,240
and then leaving it up to people to figure out how to synthesize between those learned

506
00:49:52,240 --> 00:49:57,760
representations for, you know, some particular policy goal or, you know,

507
00:49:59,840 --> 00:50:05,680
thing that they want to optimize for. So with that, I'm happy to take questions about any of

508
00:50:05,760 --> 00:50:07,280
what I talked about. Thank you very much.

509
00:50:31,280 --> 00:50:35,120
This image that made me make this decision, so do you think the results would be different

510
00:50:35,120 --> 00:50:42,720
if you used, like, iFixations in that comparison? That's an interesting question. We haven't

511
00:50:42,720 --> 00:50:51,200
considered iFixations for the salency map work, but certainly I think your intuition is right in

512
00:50:51,200 --> 00:50:57,040
the sense that, you know, the current way that we've modeled shared interest is pretty brittle,

513
00:50:57,040 --> 00:51:01,680
right? It's operating at the level of abstraction of, like, pixels in an image, and how meaningful

514
00:51:01,680 --> 00:51:07,600
are pixels really? And so what Angie is working on right now is a way to raise the

515
00:51:07,600 --> 00:51:12,160
level of abstraction that shared interest is working on. So in many of these domains, like,

516
00:51:12,160 --> 00:51:18,080
you know, ImageNet, the task that we're asking models to do, the labeling task,

517
00:51:18,080 --> 00:51:24,240
actually inherits from a much richer knowledge graph or taxonomy or hierarchy or things like that.

518
00:51:24,240 --> 00:51:28,000
But right now, at least, you know, there's a little bit of work in hierarchical learning,

519
00:51:28,000 --> 00:51:33,600
but most of the predictive models are just learning at the finest level of detail, right?

520
00:51:33,600 --> 00:51:39,840
So we're throwing away all that rich information that might be really relevant to how a person is

521
00:51:39,840 --> 00:51:44,880
making a decision. So maybe what I care about is not whether it's a Chihuahua or a golden retriever

522
00:51:44,880 --> 00:51:50,320
or a laboratory retriever. I might care, is it a dog or really sometimes is it just an object,

523
00:51:50,320 --> 00:51:55,920
right? And so what does it look like to do shared interest in more meaningful abstraction space

524
00:51:55,920 --> 00:52:00,240
rather than pixels is something we're working on. Yeah, great question. Thanks. Yeah, Will.

525
00:52:01,680 --> 00:52:07,040
Thank you for the great talk, Arvin. So going back to Jupyter notebooks and ChatGPT,

526
00:52:07,920 --> 00:52:14,160
you talked about how, right, ChatGPT can shell out to some of these nice plugins like for Excel or

527
00:52:14,160 --> 00:52:19,440
whatever to try and help people do natural language data science and that there's this

528
00:52:19,440 --> 00:52:24,160
articulatory distance due to the difficulty of learning an API. But conversely, you could say

529
00:52:24,160 --> 00:52:29,680
tools like co-pilot are sort of the parallel to overcoming that articulatory distance by

530
00:52:29,680 --> 00:52:33,600
almost in some sense, what is the same interface expressing a natural language but just in a

531
00:52:33,600 --> 00:52:38,720
code comment and then getting back code, right? But just I guess the only difference is its code

532
00:52:38,720 --> 00:52:42,560
you can see as opposed to code that's running in some back end that you don't see. And I'm curious

533
00:52:42,560 --> 00:52:47,520
if you think there's sort of a synthesis of these two poles, an interface that can take the best of

534
00:52:47,520 --> 00:52:52,480
both worlds and offers conversation but still provides access to the code or encourages people

535
00:52:52,480 --> 00:52:58,800
to understand the annoying representations. Yeah, absolutely. I thought really hard about

536
00:52:59,520 --> 00:53:06,720
which of those examples I wanted to use as the kind of foil to B2. So I did very seriously

537
00:53:06,720 --> 00:53:13,760
consider a co-pilot and I sort of agree with your analysis that it's, I think, a much better

538
00:53:13,760 --> 00:53:20,880
example of how to integrate the capacity of these LLMs. And I think there's opportunity

539
00:53:20,880 --> 00:53:27,200
to push that even further where what I would often want is really targeted mechanisms to

540
00:53:27,200 --> 00:53:33,520
introduce ambiguity, right? Right now, the little that I've used co-pilot, it's almost at the level

541
00:53:33,520 --> 00:53:40,720
of, well, it's going to produce the whole function, the whole whatever. And often what I want is it

542
00:53:40,720 --> 00:53:46,640
to be the sort of parallel prototyper for me, right? I want to introduce, say, a hole in my program

543
00:53:46,640 --> 00:53:51,040
and then go, I don't know that I want that hole to be filled in with just one specific

544
00:53:52,080 --> 00:53:56,960
outcome, but I want it to produce the whole space and for me to go, well, I want a little bit of

545
00:53:56,960 --> 00:54:03,520
this and a little bit of that and so on and so forth. So yeah, I totally agree with there being

546
00:54:03,520 --> 00:54:08,080
some really interesting medium of these things. Cool. Yeah, I like that idea.

547
00:54:10,000 --> 00:54:16,400
Yeah. Hey, really exciting talk. I'm wondering towards your kind of vision for these abstraction

548
00:54:16,480 --> 00:54:22,400
models, I'm wondering like obviously kind of from a human-interpreter interaction perspective,

549
00:54:22,400 --> 00:54:28,320
we know like representation matters so much, right? Like isomorphs of representation very much

550
00:54:28,320 --> 00:54:34,080
change how people can approach a problem or understand it. But I guess the ways in which

551
00:54:34,080 --> 00:54:40,480
they vary and the benefits of these different representations are tied very much to human

552
00:54:40,480 --> 00:54:45,520
cognition and perception. And I'm wondering, you know, in some of the examples you're showing and

553
00:54:45,520 --> 00:54:50,640
a lot of work in machine learning, we're sort of training things based upon that output. Yeah.

554
00:54:50,640 --> 00:54:55,440
And I'm wondering like, you know, are there ways that we can get at more of how people

555
00:54:55,440 --> 00:55:01,200
are thinking versus just how they output and how do we get there? Yeah, I love this question.

556
00:55:02,000 --> 00:55:08,160
And the reason I love it is also the reason I love sort of that Hutchins et al description of

557
00:55:08,160 --> 00:55:12,480
direct manipulation, right? I find the terms that they use there, particularly these two's

558
00:55:12,480 --> 00:55:18,240
distances, really evocative terms. Because to me, a distance is something that I would want to

559
00:55:18,240 --> 00:55:24,800
measure, right? But so far at least, as far as I know, those terms have largely been descriptive,

560
00:55:24,800 --> 00:55:30,560
right? As you saw in my talk, like I use them to be very analytic, but I'm not able to be generative

561
00:55:30,560 --> 00:55:38,080
with them in, you know, a very systematic way. So certainly a lot of the work that my group is

562
00:55:38,080 --> 00:55:43,440
trying to do right now is in visualization, you know, there's a lot of work that we've

563
00:55:44,240 --> 00:55:49,680
inherited in methods from sort of vision science. So we run these studies of human perception.

564
00:55:51,360 --> 00:55:56,560
And increasingly, the field is starting to get to, well, how do we start to measure cognition,

565
00:55:56,560 --> 00:56:02,320
right? Can we model sort of a decision making task and start to, you know, operationalize

566
00:56:02,320 --> 00:56:06,800
that through experimental design? And so we're starting to push in some of those directions

567
00:56:06,800 --> 00:56:11,440
as well, but scope to sort of, you know, interaction in a Jupyter notebook, but then

568
00:56:11,440 --> 00:56:15,840
starting to see, you know, the impact that interaction has on sort of the downstream

569
00:56:15,840 --> 00:56:21,520
analyses people would do, and then see if that actually maps to, you know, their goals or things

570
00:56:21,520 --> 00:56:34,480
like that. Absolutely. Yeah. So I'm curious about the, just continue on this line of

571
00:56:34,480 --> 00:56:40,080
perception up through cognition, you know, going back to the sort of like,

572
00:56:40,800 --> 00:56:47,840
or 10 Cleveland McGill kinds of stuff, the automatic processing was very key to the design

573
00:56:47,840 --> 00:56:53,680
of visualizations, especially early on, that the notion was that my encodings were supposed to

574
00:56:53,680 --> 00:57:00,960
map on to almost like system one interpretation, right? Like when I see the scatterplot and

575
00:57:00,960 --> 00:57:04,560
you know, encoding distance in the following way, I'm gonna draw the correct conclusion.

576
00:57:04,560 --> 00:57:11,520
And it's interesting to me that sort of through the transformations you've started to pursue,

577
00:57:11,520 --> 00:57:19,600
we're not trying to like encode those into a similar mapping for audio, but instead

578
00:57:19,600 --> 00:57:25,360
directly doing the cognition on behalf of the individual. And those seem like orthogonal

579
00:57:25,360 --> 00:57:30,880
directions one could go. I'm curious how we find the right point in the design space.

580
00:57:31,840 --> 00:57:37,280
I think this is a fantastic question. So the way my group is starting to think about this

581
00:57:37,920 --> 00:57:41,760
of like, how do we find the sort of right balance of who is doing the perception,

582
00:57:41,760 --> 00:57:46,720
who is doing the interpretation is starting to consider some of these modalities and concert

583
00:57:46,720 --> 00:57:50,560
to better understand what the relative affordances of these modalities are.

584
00:57:50,560 --> 00:57:55,280
So in particular, Jonathan, who you saw in the demos is leading some really,

585
00:57:55,280 --> 00:58:03,040
really cool work right now around what if I'm sort of specifying the visual, the audio,

586
00:58:03,040 --> 00:58:07,840
the sort of sonified audio and the textual audio side by side, and then I'm playing them

587
00:58:07,840 --> 00:58:13,680
sort of simultaneously through. Do I want, you know, there to be sort of perceptual redundancy

588
00:58:13,680 --> 00:58:19,520
where the sonification is sort of emphasizing what is, you know, described in the texture?

589
00:58:19,520 --> 00:58:26,560
Do I want these modalities to be complementary? And, you know, sort of TBD, but I think there's

590
00:58:26,560 --> 00:58:31,600
some really exciting sort of questions for us to sort of dig into space.

591
00:58:31,600 --> 00:58:34,720
Are there similar pre attentive principles for audio? There must be.

592
00:58:36,000 --> 00:58:40,480
As far as I know, so I'm, you know, we're just starting to look in the sort of sonification

593
00:58:40,480 --> 00:58:46,720
literature. Yeah, as far as we can tell sound is a very, very different perceptual sense

594
00:58:47,520 --> 00:58:53,440
than vision. And so even the sort of, you know, basic sort of visual encoding paradigm where I

595
00:58:53,440 --> 00:58:59,600
take a data field, I map it to, you know, position color size that breaks down very rapidly for audio.

596
00:59:00,320 --> 00:59:06,400
So oftentimes really all the people are able to sort of, you know, detect differences in our sort

597
00:59:06,400 --> 00:59:13,600
of pitch and loudness. And even then our fidelity at that is very, very low. And so there might be

598
00:59:13,600 --> 00:59:19,120
some pre attentive characteristics. We're certainly looking at some early work in HCI.

599
00:59:19,120 --> 00:59:27,120
I think Stephen Brewster had done around sort of ear cons, you know, discreet sort of representations

600
00:59:27,120 --> 00:59:31,040
of icons, but through audio and things like that. So there may be some of that there.

601
00:59:32,000 --> 00:59:35,920
But at least so far we're so early in our own work that we don't know.

602
00:59:35,920 --> 00:59:37,520
Interesting. Okay, thank you. Yeah.

603
00:59:39,600 --> 00:59:43,280
I think we're about at time. So if you have additional questions, please mob him after

604
00:59:43,280 --> 00:59:52,800
the talk. Thank you, Arvin for joining us. Thank you very much.

