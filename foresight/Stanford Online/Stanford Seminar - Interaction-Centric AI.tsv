start	end	text
0	16560	Being a speaker at this seminar series is, I mean, it means a lot to me personally.
16560	22040	When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but
22040	26420	I didn't really have that many resources, so I had to rely on online resources.
26940	31340	This seminar series recordings have been posted online, and I think I've watched
31340	34860	pretty much everything to really learn about HCI.
34860	38740	And then I came as a master's student here in 2008 and
38740	42940	took 547 pretty much for the entire two years I was here.
42940	49300	And now I feel great that I get a chance to speak as a speaker, so this is great.
50340	54020	Today I want to talk about interaction centric AI.
54020	60340	This is a reprise of the New Europe's keynote talk that I gave two weeks ago
60340	62700	in front of thousands of AI researchers.
62700	66860	I tried to reframe it a little bit so that it's more customized for
66860	70020	an HCI audience rather than an AI audience.
70020	75300	But the idea is that I want to think about using human AI interaction
75300	78060	at the center of developing AI technologies.
78060	81180	And of course, I don't have to preach to the choir that human AI interaction
81180	86340	actually matters, but diving deeper based on my experience of building
86340	90300	these interactive systems in different contexts, like education, discussion,
90300	94740	decision making, I want to dive deeper and report some of the detailed
94740	98900	interactions that we've been observing and learning from.
98900	104540	And think about what it means to design human AI interaction in various contexts
104540	108460	and what are some action items moving forward as a community.
109460	113060	So let's first start with some definitions and terms.
113060	120300	I would say the dominant paradigm for developing AI technologies has been model centric.
120300	125180	The idea is to build a model with high accuracy and we want to evaluate it
125180	128740	against unseen examples for its generalizability.
128740	133820	And benchmarks have been great in that they could help us competitively
134220	138940	compare different models' performances, which could be useful in making
138940	142260	scientific advances possible.
142260	145900	And more recently, people have been talking a lot about data centric AI,
145900	150780	where the idea is using this nicely performing model, what is a good,
150780	155460	sort of robust and efficient data pipeline around it in terms of collection
155460	160820	of the data, processing of it, cleaning of it, so that the model actually performs
160820	163060	really well in different contexts.
163100	168500	And here the focus is acquiring quality data and setting the pipeline in a way
168500	171340	that really helps the machine perform its best.
172980	179660	And these two paradigms are great, but then what is slightly missing is the user
179660	184580	who's using these AI technologies and those who are affected by what the AI
184580	186020	systems give you.
186020	192140	So interaction centric AI is sort of my term in some contrast to model
192140	196300	centric and data centric AI, where the goal would be basically what HCI
196300	200780	researchers do in this context, like improving the user experience by
200780	203980	building usable and useful applications.
203980	208860	And the unit that we often grapple with is a human AI interaction.
208860	212340	And you might be wondering, is this some sort of like marketing term?
212340	217260	Like how is it different from human centric AI we've been talking all about?
217260	221340	It's largely similar, so I'm not trying to say I invented this new term or
221340	225820	anything, but I want to focus our attention to the interaction that is
225820	230540	happening between humans and AI and the complex relationships and the dynamics
230540	234620	that are happening between the two, rather than focusing on just the humans
234620	236140	or machine alone.
236140	241100	So I can say that that's the focus of where my discussion will be today.
243260	249780	So let's say you're this AI researcher and your team has built this amazing model.
249780	254900	So this is actually something that I copied and pasted from one of the
254900	256500	diffusion models papers.
256500	260540	I don't know what they actually mean, some of them I understand.
260540	265300	But basically, this is what you have as an AI researcher.
265300	269700	But what would a person using this kind of AI want to do with it?
269700	271140	Here's an example.
271140	276340	So this is a Twitch streamer in South Korea who was trying to use this
276340	281940	diffusion-based text to image generation model to create this image of an
281940	286420	animated character eating ramen with chopsticks with noodles around the
286420	287300	character.
287300	292500	So this is the roughly sketched out goal that the user company has.
292500	299220	And he ended up spending two hours fiddling with text-based prompts to get at
299220	301460	the final image that he wants.
301460	304820	And this is somewhat similar to what Manish shared a couple of weeks ago at
304820	310580	the HAI conference in terms of what he had to do with the prompt-based interface.
310580	314500	And here, the entire two-hour journey was live streamed.
314500	319060	So I want to kind of share a quick summary of what happened in that stream.
319060	322740	And of course, we need something in the middle to bridge between the technology
323380	324740	and the human user.
324740	329540	And that's what we have, the prompt-based interface and interaction that's
329540	330500	happening between the two.
331380	335140	So the streamer started by something simple and obvious.
335140	337780	The prompt says, eating ramen, and this is what he got.
337780	342740	It's okay, it's kind of there, but the bowl is perhaps too large.
342740	344820	The chopsticks are all to be placed.
344820	349940	And he heard from somewhere that adding a full sentence might make things better.
349940	352500	So he goes, she is eating ramen.
353780	358900	She is eating ramen, for sure, but you can see that something's not quite right.
359220	363220	So he keeps going on by adding more descriptions.
363220	366660	And the prompt is definitely getting longer.
367620	372340	And it seems that the AI is not quite getting how chopsticks should be used
372340	374900	and how many should be used.
374900	380020	So he keeps adding these descriptions to really explain what it means to use chopsticks.
380340	390900	And to be fair, there's hair, there's chopsticks, there's noodles.
390900	393060	So it's in computer graphics.
393060	395860	Dealing with human hair, I heard, is a really tough challenge.
395860	401860	And maybe for AI, it's also kind of struggling to deal with all these similar looking objects.
401860	407620	And it doesn't really seem to get how to differentiate between chopsticks and noodles.
408580	412340	And another interesting aspect was that since it was a live stream,
412340	419060	the viewers were actively participating in recommending new prompts to try out,
419060	420900	sharing their interpretations.
420900	425860	And this is somewhat of a collaborative mental model construction process,
425860	428260	if you will, as a group of people.
428260	430660	They are really trying to figure out what's going on.
430660	434180	And now the prompt is five lines long.
434740	439140	And the service that this streamer was using was supporting variations,
439140	442340	where you could pick an image and say create some variations.
442340	446100	And he was referring to this interaction as variation gacha.
446100	450340	So gacha is a Japanese word for like a random box or blind box.
450340	455140	And this kind of tells us that how unpredictable this sort of interface is.
455140	460740	Once you hit the generate button, the user doesn't really have a good sense of knowing what to expect.
461220	465140	And this is the actual stream, as you can see, like his praying,
465140	470020	and which also tells us about the usability of this sort of system.
470020	474660	He doesn't have a good way of knowing what to expect, so that he actually has to pray.
476660	481140	After two hours of hard work, this is the final image that he landed.
481140	484580	And it looks pretty good, and he claims victory.
484580	489380	But then look at what he had to do at the top, right?
489700	494740	At the top, there are seven lines of prompt that he had to write.
495620	499060	And arguably, this is natural language.
499780	502420	But I would say this is really pseudo-natural language.
503700	507380	And so this is basically the experience that he had to go through.
507380	509220	So is this a good interface?
509220	514180	And I sort of got inspired by Manish's discussion of discussing the usability
514180	516900	of these text prompt-based interfaces.
517540	518740	There are some good elements, right?
518740	519780	It's quite intuitive.
519780	523940	You can use natural language, or you believe natural language could be used.
524500	527300	And the output is presented in a visual manner,
527300	531540	which helps you kind of understand whether you got the image that you like or not,
531540	532980	so that you can sort of debug.
533780	536980	And there are some interactions that are supported,
536980	542580	like variations and seeds and like words that should not be used and things like that.
543540	547140	But there are many ways in which this interface actually fails
547140	548820	to support what the actual user wants.
549700	551860	He had to rely on trial and error.
551860	556420	And just the fact that he had to spend two hours to get that image
556420	559060	seems to suggest that something is really wrong.
559940	561940	And of course, it was not really predictable
561940	566180	and lack of specific feedback on the effect of what specific words
566180	569860	in the prompt had influenced on the final outcome.
569940	573060	These links were often missing, which made it really difficult.
574340	578660	So is this really just a problem for these text-based prompt-based systems?
579300	584340	I would say every AI application faces these interaction challenges.
584340	587540	On the user side, when they first encounter these systems,
587540	591700	they often have to struggle to kind of figure out how to make it work.
591700	597140	Often people resort to misusing it, abusing it, and learning takes a long time.
597700	600100	And part of it is really a design challenge.
601380	603220	And we've seen other examples like this,
603220	606260	where people don't have a good sense of what's happening
606260	611060	in this algorithmically-generated systems and AI-powered systems.
611060	614980	Like in the famous study of Facebook newsfeed users,
614980	620020	more than half of the participants were not aware of the newsfeed curation algorithm's existence
620020	622100	at all, which is far from being true.
623060	629540	And on the right, what you see is in the pathologists' diagnosis scenario,
630260	633140	often they would rely on some notion of similarity.
633140	637380	So there are these algorithms that are designed to help people find similar images,
637940	644900	but then the realization that the researchers had was that people had different notions of similarity.
644900	650100	So a singular notion of similarity that was used in building an algorithm would not really suffice.
650100	655700	So what they ended up doing was to support three different types of similarity interaction,
655700	660900	and the user was able to kind of transition between these different terms in a fluid manner,
660900	664580	which really gives more control and agency on the user side.
667460	672420	And these, you know, put in a more simple sort of diagram manner,
672420	676900	whether you are a creator or Facebook user pathologist,
677460	680740	you seem to have some kind of a mental model of how the system works,
680740	685300	a very sort of a classical sort of gap between what the user wants and the system wants.
685300	689620	And obviously, the system is not behaving in a way that you really want.
689620	696900	And this gap arguably seems to be larger with these more complex black box and deep learning based systems.
698740	702020	And AI community has been tackling this problem as well.
702100	707540	And, you know, some of the folks have been framing this as an alignment problem,
707540	712260	which is about aligning the model's behavior with human intent.
712900	720340	And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open
720340	726100	AI's response to the alignment problem, where their idea is, in addition to the, you know,
726100	731860	basic large language model that they have, they would add this fine tuning layer with human feedback,
731860	737060	which often involves asking people whether, you know, they were happy with the results they got.
737060	743300	And the system kind of uses that feedback to train a reinforcement learning agent to
743300	748660	do the fine tuning so that the resulting text aligns better with what the user wants.
748660	754100	And they were seeing some success from it. And a quote from the paper is that
754100	759460	making language models bigger does not inherently make them better at following a user's intent.
760420	766580	And aligning language models with user intent on a wide range of tasks by fine tuning with human feedback.
766580	771940	And of course, there's been a lot of discussion about whether this is really the most promising way
771940	777940	to, you know, involve humans or alignment problem. But I think this is some progress towards that direction.
778820	786900	But all of these examples, I would say, basically lead us to revisit these classical notions of
786900	793300	Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s, right?
793300	799300	As a user, they want to know what's going on with the system, and they want to have more control and agency.
799300	804900	And on the evaluation side, when AI gives you some kind of result, they want to be able to
804900	811700	understand it, interpret it, and want to get some explanation of it. And as an HCI researcher who's
811700	817140	building these interactive systems, I feel like in often cases, I try to bridge these gaps.
817140	823060	I come up with new ways of designing these social interactions and human AI interactions
823060	828500	in a way that tries to bridge these gaps. And these are just some of the systems that I've been
828500	835140	developing in different application domains. And I think many of them have somewhat succeeded
835140	839220	in bridging these gaps. But other times, to be honest, we haven't done a good job of doing that.
840180	845700	So what I want to do for the remaining time for this talk is to share some of these lessons,
845700	852580	and some of them from positive experiences, but other times, bitter experiences by something
852580	858260	that we haven't really done a good job of. And the main message that I want to send across
858260	863860	is that beyond these point solutions for this system that works in this particular context,
863860	868500	we've seen some success, I think as a field, we really need to start thinking about,
869140	874900	can we do something more systematic and sustainable? Or empower designers and developers in thinking
874900	882500	about can we develop these AI applications that are more usable and useful for more groups of people
882500	888340	rather than having to reinvent the wheel each time someone has to develop these applications.
888340	891540	And I think we're seeing too many of these cases where people are like,
892260	896580	there's this cool model, let's build something around it, and it just gets released in a few
896580	901780	days and realizes that people want it in a completely different way, people abuse it,
901780	905860	a few days later, it goes down. We're seeing too many of these failure cases.
907460	912100	So from the HCI point of view, I think HCI research can really advance this
913460	918580	interaction-centric AI by contributing these generalizable building blocks for designing
918580	925540	these systems and interface affordances. And AI research can also advance by embracing the idea
925540	931540	of interaction-centric AI by rethinking models, architecture design, benchmarks, metrics, and
931540	939380	research process. The part of it has to involve sort of broadening the perspective beyond just
939380	946020	thinking about the model and the output that it generates to think about the users behind those
946020	951220	and their mental models. And often there's not just a single user, but a group of user,
951220	956020	community of user, a society of users. And there's also the temporal dimension,
956020	961780	like before the user comes in and tries to use a system, we should be asking the questions about
961780	967380	like, what's the task and who are these users and why and how. And during the interaction,
967380	972420	we need to be thinking about presentation visualization. And the other way around as
972420	976500	well, like interpretable results are being presented to the user. Do they have a way to
976500	982260	provide feedback to the system? And also, it's never going to be just a single use, right? People
982260	987940	would want to come back and use a system for a sustained amount of time. In those cases, people's
987940	995220	mental model would evolve. And what does it mean for the system? And so I think this is sort of
995220	1003540	the ecosystem that I have in mind. And with these, I want to dive into these specific examples
1003540	1010180	where we designed human-AI interactions. And I identify four major challenges
1012340	1017940	in terms of human-AI interaction. The first one is about bridging the accuracy gap.
1019620	1024660	So I'm on my sabbatical now. I'm working with this startup called Ringle, where they are
1024660	1030260	basically Uber for language learning. They are matching tutors and tuties, and they have this
1030260	1036500	video-based language tutoring session. So what we try to do here is to build this diagnostic
1036500	1041940	service based on analyzing the chat-based tutoring session to give people personalized
1041940	1048580	feedback and suggestions for improvement. But instead of going into the details of the service
1048580	1055300	itself, I want to touch upon the case that we ran into when we were trying to run this automated
1055300	1062100	speech recognition AI, which is crucial in sort of turning the video-based chat into text format,
1062660	1066980	which is really required for us to run all these diagnostic algorithms on top of.
1068420	1076020	And the standard metric of success in ASR would be word error rate, how correctly it can
1076580	1084180	recover the original text. And on the tutor side, when we ran ASR on like hundreds and
1084180	1091060	thousands of sessions, the average word error rate was around 8%. Can you take a guess as to what
1091060	1098980	the number would have been for students? Obviously there's this white margin that's quite high,
1099060	1108340	so you can imagine, 30. Yeah, we're seeing 23. So there's quite a bit of a gap. And this is an
1108340	1114740	example of an accuracy gap where different groups of users are getting disproportionate results from
1114740	1121620	the same AI. And the gap actually widens if we look at like the best tutor and the worst student
1121620	1127380	when it comes to the performance of these models. But in terms of thinking about the interaction
1127380	1133620	that these people are trying to have with this AI, I would argue that the students are the ones
1133620	1139620	who really need this AI to work. Based on the accuracy of this AI, they want to kind of look
1139620	1146420	at where they succeeded and failed and they want to learn and reflect. And with this low accuracy,
1146420	1151940	they would really be struggling to come up with good action items and they might be frustrated,
1151940	1157220	they might lose trust on the system. But interestingly, a lot of focus when it comes to
1157220	1164180	model development is that we seem to be focusing on the 6%, like making the 6% better instead of
1164180	1170420	narrowing the gap between 6 and 36%. And we have to really be asking like, what is the most important
1170420	1174660	question in this context? And are we really focusing on the most important question here?
1175460	1182580	And we see these other examples too, where Tyra and others have studied the machine translation
1182580	1188100	that is being used in emergency rooms when it comes to discharge statements that are presented
1188100	1194660	to patients and patients' families. And we see a huge disparity between different languages.
1194660	1201380	And in the natural language processing community, this support for low resource languages has been
1202100	1207700	a topic for research and there has been great efforts. And on the right is a famous example
1207700	1215700	of gender shades, where the gender classification algorithm shows, again, an accuracy disparity
1215700	1224900	between darker skin female versus lighter scale male. And of course, these diversity and inclusion
1225700	1230980	efforts and low resource language support research in the AI community and in the community
1230980	1236580	have been tackling these issues of accuracy gap, of course. But then I would argue that they could
1237380	1243940	advance further by embracing more interaction-centric approach in trying to really see how in the real
1243940	1249380	world people are interacting with these results and what kind of actual struggles that they have
1249380	1257460	because of poor or good AI accuracy and what, as a community, how can we define the problem that's
1257460	1265780	most important. And conceptually speaking, I feel like a good analogy might be the ceiling and floor
1265780	1272740	analogy. The ceiling would be this primary user group who gets the best part of AI. And floor
1272740	1279540	would be secondary user group who is disproportionately getting more negative impact of
1279540	1285780	the same AI. And there's this accuracy gap. And often I feel like taking a model-centric approach
1285780	1292340	incentivizes people and researchers to work on raising the ceiling. There could be a couple
1292340	1297460	reasons for this. First of all, that's the sota number you get, which might be what you need to
1297460	1303060	publish a paper out of it. Or the benchmarks that you're working with do not really have much data
1303940	1308500	on the floor side. It's maybe more focused on the ceiling side. And that's why the ceiling is there
1308500	1314500	in the first place. So it might be just incentivizing people to continue to push the boundaries of
1314500	1322580	ceiling. And as a result, what we see is a lot of a widened accuracy gap. And if we take a more
1322580	1327380	interaction-centric approach, I would argue that if we identify that narrowing this gap is a more
1327380	1333620	important problem, we can narrow this accuracy gap. And it's not just a matter of accuracy,
1333620	1338180	if you think about it. It's about experience, benefit, and value that people get out of
1338180	1346420	interacting with this AI. So there was a first challenge about the accuracy gap and how thinking
1346420	1351860	about how people interact with this AI can help us identify what problems are worth tackling.
1352660	1358420	And second of all, I want to talk about when people actually use AI. And one of the
1358980	1365460	anti-patterns of human-AI interaction is that people just stop using AI altogether or abandon it,
1366020	1372500	which is something you might want to avoid as a system designer. And that's why it's important
1372500	1379380	to think about how do we incentivize people to work with AI? And in most cases, people abandon
1379380	1385460	using AI because it's not really giving them concrete value that they expect. And we explore
1385460	1391860	this in the context of online education in this system called XS. So the problem that we wanted
1391860	1397380	to focus here is that in online, let's say you want to learn some new concept like probability,
1397380	1402980	there are lots of problems and answers you can find. But finding good explanations is
1402980	1409140	surprisingly difficult. And generating high-quality explanations is costly and resource-intensive
1409140	1416020	console. So we wanted to tackle this problem by building this online education platform,
1416020	1421780	where people are presented with a problem and they solve this problem, they submit an answer,
1423540	1429700	and they see an example that's presented by the system and they get a chance to rate how helpful
1429700	1437460	the explanation that they saw was. And then they are getting a chance to sort of self-explain
1437460	1443540	their own answer. So this is a pedagogically meaningful activity to be able to sort of explain
1443540	1449940	your thought process, externalize it, and lots of research supports doing self-explanation.
1450740	1456420	Okay, so fairly simple sort of front end in terms of the learner's experience. So what's
1456420	1463380	happening behind the scene is that the system is collecting these explanations and ratings
1463380	1469780	from learners, right? Since it's a live system, new learners keep coming in and provide new ratings
1469780	1476020	and explanations. And we formulate this in a multi-armed bandit manner, which means that
1476020	1481860	as a new explanation comes into the system, as a byproduct of humans' learning activity,
1481860	1487860	a new arm gets added to the system. And what the system is doing is to determine this dynamic
1487860	1494180	policy for what the most effective explanation would be for the next learner coming into the system.
1495780	1501380	So if you're familiar with the reinforcement learning of concepts, we are navigating
1501380	1507220	exploitation and exploration trade-off. Exploitation in the sense that the system wants to present the
1507220	1512980	best explanations to the next learner coming into the system, but the system doesn't really know what
1512980	1518980	the best explanations are until it collects some amount of ratings from people. So it has to do some
1518980	1525220	exploration where it should collect this data. And to solve that, we use a technique called
1525220	1531780	Thomson sampling. So what happens is the system keeps track of these policies and when a new
1531780	1536740	explanation comes in and ratings come in, these things get updated and the policy
1537140	1544260	of probabilistic policy gets updated so that it uses this distribution to determine
1544260	1554340	what explanation to show to the next learner. So when we ran a study, these access-generated
1554340	1561780	explanations were helpful in terms of helping people learn better. So when we compared against
1561860	1567220	presenting no explanation at all and measured differences between pre-test and post-test
1567220	1573860	results, we were seeing that people were gaining 3% increase in their scores. So just getting a
1574660	1581060	chance to rethink the problem, I think still gave them some increase in their scores. And when they
1581060	1588020	were seeing the instructor-generated explanation, which is, I guess, somewhat of an ideal case or
1588020	1594420	the standard case, we're seeing 9% increase and with access, we're seeing 12% increase. So between
1594420	1599620	these two conditions, it was not statistically significantly different, but there were certainly
1599620	1607220	cases where access was picking explanations from learners that were even more powerful than the
1607220	1614740	instructor-generated ones. So in this system, if we were to take a more model-centric approach,
1614740	1620900	I think we might have built an AI that automatically generates high quality explanations.
1622340	1628260	But instead, in taking an interaction-centric approach, I think the system we created is basically
1628260	1635540	this co-learning system where the user, the learner, and AI are learning at the same time in a single
1635540	1642420	system. So it's sort of an education-focused system of the game-with-the-purpose kind of setting,
1642420	1647380	where organic benefits are provided to people who are interacting with the system,
1647380	1653380	and the system is learning something useful out of it. And this is basically the mechanism that
1653380	1658820	we have in that both sides are learning and explanation and feedback are establishing this
1658820	1665460	loop. And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing,
1665460	1671780	where learners as a crowd coming into the system are basically doing this by getting their individual
1671780	1677620	benefits while they're providing something useful for the system to learn and do its thing better.
1678580	1685380	So since then, I've been expanding this idea to a broad array of applications. So for example,
1685380	1691860	can we use this kind of co-learning ideas to summarize how-to videos in terms of steps and
1691860	1697780	sub-steps, or building a concept map out of an instructional video that shows relationships
1697780	1702740	between different concepts, or helping learners come up with the solution plans
1703380	1708500	in algorithmic problem-solving settings. And other researchers have taken on this idea
1708500	1713540	in different application contexts as well. So I think we can try to really generalize this kind
1713540	1722980	of idea of co-learning system design in different contexts. Moving on to the third challenge,
1723940	1730740	is about beyond a single user. And often we think about a single user, a single AI interacting
1730740	1735940	with each other. In real life, it would be much more complex and there would be diverse configurations.
1736980	1743220	So how can we consider these social dynamics? And there could be various types of social dynamics,
1743780	1750900	but one specific instance that we did in was group-based, chat-based discussion in a group.
1751780	1758020	So we built this system called Solution Chat, where the idea is what if this AI agent could
1758740	1765060	recommend real-time moderation messages to a group. So let's say a group is discussing,
1765060	1770020	you know, what to do for the company retreat next week, and they're having a discussion.
1770020	1775060	The system, in real time, based on the understanding of the discussion context,
1775060	1778820	and also knowing what kind of messages would be useful for the group,
1778820	1784020	based on our sort of literature survey of discussion and discussion-based education,
1784740	1792340	it presents these recommendation messages, like any more ideas, or can this person share their
1792340	1798100	opinions, you have been quiet for a while, or should we try to move on to the next stage,
1798100	1803780	or thank you for your opinion. So these kinds of moderation messages are presented by the system
1803780	1810020	in real time, just like what you get in smart replies in Gmail, for example. And as a moderator,
1810020	1816500	you can just choose to accept any of the messages that you like, and discard the ones that you don't
1816500	1828500	like. So a quick summary of the results of what we saw was that in our lab study with 55 users
1828500	1834740	in 12 different groups, when we compared how many moderation messages were used in different groups,
1834740	1839540	when we compared the baseline condition without these real-time recommendations versus
1839540	1845300	solution chat or system, we're seeing a significant increase in the number of moderation messages
1845300	1850340	that were present in the chat stream in the solution chat condition. But interestingly,
1850340	1855700	you can see that the users manually typed moderation messages were actually decreasing
1855700	1864980	in solution chat, but many of them were replaced by the accepting AI-generated recommendations.
1867380	1873140	And furthermore, we had this great opportunity to actually release this system to over 2,000
1873140	1878180	real-world users in a corporate education setting. So during COVID, a lot of these corporate
1878180	1884580	education programs moved online, and this company that we worked with wanted to use these kinds of
1884580	1891620	system to moderate hundreds of chat rooms that were doing discussion-based activity.
1892980	1899860	And not surprisingly, just like the very first live stream prompt example that I mentioned,
1899860	1905540	here again, people were collaboratively trying to understand the capabilities and limitations of
1905540	1911540	AI when they were first presented with the system. So they were using the chat to test
1911540	1917940	different messages, often things that they believe would be not working, and they would be
1917940	1923620	sharing the results of, oh, this is working, this is not working, I think this does this well,
1923620	1930420	but not that well. And it seems as a group does this kind of testing in the very first phase of
1930420	1936660	their usage of the system, people have this shared expectation of the system, and that seems to sort
1936660	1943540	of determine their further interactions with the system. And it was also notable how different
1943540	1948500	groups had different expectations based on their limited experimentation that they did in the beginning.
1950340	1953620	And there were some interesting social dynamics that we observed as well,
1954500	1961380	like in how people use these AI recommendations to socially interact with each other.
1962340	1966420	Some people were using AI as proxy. So one of the quotes that we had was,
1966420	1971220	I didn't want to directly ask the person to stop talking. So the person relied on the AI
1971220	1978820	recommended message to kind of send it. They still chose to send it, but it was their way of kind of
1978820	1987540	softening the potential sort of dispute with the person. Other people were using AI as a reference.
1987620	1992820	So what we were seeing is that it was a fairly simple technical pipeline that we had. So it was
1992820	2000580	just a canned response. So people were sometimes not really fond of the tone of the message,
2000580	2005300	style of the message that we showed. So the person said, I found no fun in the recommended
2005300	2010900	messages because all the messages look the same. So in those cases, what people did was they still
2011460	2019380	adopted the idea from the recommendation, but then rewrote it so that it feels more personal,
2019380	2026820	and it feels more like it's coming from them, not AI. In other cases, AI seems to be adding
2026820	2033220	a social burden. So in this excerpt, so one of the people said, I'm doubtful about the
2033780	2039460	credibility of AI. And then the moderator picks this AI recommendation. Thanks for your opinion.
2039540	2044100	Another person says, I also think negatively. Thanks for your opinion. Thanks for sharing a good
2044100	2049460	opinion. Shall we go to the next topic? And then the moderator realizes he might have clicked,
2049460	2054820	accept way too many times, and it was a little unnatural. So he stopped to kind of
2054820	2060420	clarify and apologize for my unnatural words as I'm using AI recommendations.
2061220	2067380	So while we were seeing how people were saving their time and cognitive effort in moderation
2067380	2072500	could have decreased, it might have actually introduced other types of burden at the same time.
2075460	2079700	Again, so if we were to build this kind of system in a more model-centric manner,
2079700	2084580	I think a good alternative might have been automated discussion moderation, where AI
2084580	2092500	would actually do all the moderation by itself. But instead, we chose to take a more AI-assisted
2092500	2098340	moderation for obvious reasons. Users want to have more agency and control, and they wanted to
2098340	2104340	keep their style of communication. So instead of handing over the entire control to AI,
2104900	2112420	we still sort of gave that control to the human moderator who could kind of use it as an additional
2112420	2120900	resource. Okay, so there was a third challenge. And moving on to the final challenge of supporting
2120900	2127060	sustainable engagement. Here, the concern is that we want to think beyond this single
2127060	2134020	session usage. And over time, how people react to these systems might change, their mental model
2134020	2138260	might change, and how AI actually works might change. So we need to really think about this
2138260	2147060	temporal dimension more carefully. And for this thread, we investigated in the context of
2147860	2154260	novices making changes to websites that they're seeing. So for example, you might have a case
2154260	2159220	where you visited this website that colors hurt your eyes, or you couldn't really find this button
2159220	2165860	or tap it because it's too small, maybe you want to make it larger. But then people without
2165860	2172580	expertise in HTML and CSS have difficulty doing this. So we thought by leveraging the power of
2172580	2178420	large language models and so on, maybe we can support more natural language queries. So if a
2178420	2184660	person says tone down the text, the system can kind of display these style recommendations that they
2184660	2193300	can explore and select from that are about toning down the text. So the way the system works is
2193300	2200260	if the user clicks and says make this larger, the system presents a set of design attributes that
2200260	2210820	are about making something larger. And the user can say emphasize this part. It's somewhat ambiguous.
2210820	2216180	There isn't a clear single design attribute that is about emphasis. So it presents these
2217220	2225460	few recommendations that are about emphasizing something. So we built this by establishing
2225460	2232660	this NLP pipeline and computer vision pipeline. On the NLP side, what it does is analyzing the
2232660	2239700	user's query and mapping them with the style attributes that seem to be connected to what
2239700	2246740	the user's intent is about. In terms of computer vision, we collected millions of web design
2247620	2254020	elements to determine a good set of recommendations to show to the learner. So by combining those,
2254020	2259940	we built this system. Again, so instead of going deep into the technical details of the system,
2259940	2266660	I want to focus on the interaction dynamics. So we ran this user study with 40 people where we
2266660	2272660	presented them with either stylet, which is the name of our system, versus the baseline, the
2272660	2277220	Chrome developer tool, which is sort of the standard tool for making these style changes.
2278180	2284340	So we compared these two groups. And we gave people two tasks. One is a well-defined task
2284340	2290900	where we ask people to turn this before image into an after image. And then secondly, we had this
2290900	2296100	open-ended task where we gave this blank slate and people were able to make any kind of change
2296100	2304900	that they want. First, I want to share success stories. People were more successful in completing
2304980	2312180	these design tasks when using stylet. 80% of the stylet users completed the task as opposed to only
2312180	2318180	35% in Chrome developer tools. And these were complete novices in web design, no experience at all.
2319060	2324500	And people completed the task in 35% less times. It was efficient to use stylet.
2325380	2331300	Another interesting observation was that people were making same similar number of changes in
2331300	2338740	both conditions. But in stylet condition, people were making more diverse changes, which means that
2338740	2344740	it probably had to do with how stylet shows these multiple options for people to explore. And there
2344740	2350100	was a conscious decision to not just show the most obvious one, but show somewhat related ones as
2350100	2356260	well so that people could explore and tinker around different options. But then an unexpected
2356820	2362260	finding was when we looked at people's self-confidence. Because we thought this kind of system would be
2362260	2367860	useful for people's learning of the skills and confidence that they have about the skills,
2367860	2373540	we asked people's self-confidence after each task. What we noted was that after the first task,
2374100	2378980	in both conditions, people's self-confidence increased. But then in the second task,
2378980	2385220	after the second task, users' self-confidence decreased for stylet while in the developer
2385220	2391220	tool, it kept increasing. Why would that be the case? And we were seeing many cases where
2391220	2396420	stylet users were frustrated that the only control that they had was natural language.
2397060	2401220	Now they have some grasp of how it works. They wanted to do more fine-grained control more
2401220	2406180	directly. And they wanted more specific things. But because they only had natural language,
2406740	2411780	they sometimes just got frustrated. Whereas in the Chrome Developer Tools condition,
2411780	2416260	people were just happy that they accomplished something with their own hands.
2416260	2422020	And I think that is presented as a continued increase in self-confidence.
2423220	2431860	And we know from HCI and CS147 that people's expertise and learnability really matters. And as
2431860	2439140	they have more knowledge of the domain and the skill, they might need to get more advanced
2439140	2445060	controls or being able to more directly manipulate what they are working on. So I think this had
2445060	2450020	some interesting lessons in terms of thinking about the temporal dimension in that learners are
2450020	2457060	changing. And other researchers have been reporting that considering these temporal dynamics is
2457060	2462900	important. On the left, what you see is design researchers who have shown that there are these
2462900	2468900	different stages of relationship that people have in technologies like self-tracking devices.
2469220	2474420	First, they would start with initiation and experimentation, followed by intensifying and
2474420	2479620	integration, and then stagnation and termination. And one of the design lessons might be that
2480180	2486340	these might be more meta-level factors that really should be considered in design systems,
2486340	2491780	in that even the same kind of intervention might need to be presented in different
2491780	2496340	manners depending on what stage you are or what your expectation is with the system.
2496900	2502260	On the right, what you're seeing is the guidelines for human-AI interaction, really influential
2502260	2508420	work from Emershi et al. And they organize these guidelines for human-AI interaction
2508420	2513860	in different categories but are organized in the temporal sort of aspect, like initially
2513860	2520740	encounter with AI during interaction, when things go wrong, and over time. So taking into account
2520820	2526180	this temporal dimension can really be powerful in supporting more sustainable engagement.
2527620	2534100	And the related question might be, as people are relying more on these AI tools, like grammar
2534100	2540660	fixes or even generating text, it's important to think about how people's mental model would
2540660	2547540	change over time, and AI also changes over time too. And do we hit a point where people become
2547540	2554100	maybe overly reliant in that maybe their grammar skills or writing skills do not improve anymore,
2554660	2560100	but then without the tool, they actually might perform worse? And what is that dynamic? Or maybe
2560100	2564580	over reliance is perfectly fine because if we believe these tools will be around the user all
2564580	2570260	the time, maybe it's just the final outcome that matters. And I think we need more studies and
2570340	2575780	analysis of the long-term engagement of users using these kind of technologies.
2579220	2583700	And to kind of sum up, if we were to take a more model-centric approach here, I think we might
2583700	2591220	have built a system that makes automatic design fixes to optimize a web page directly, and the
2591220	2597380	system makes a fix and user can just use it. But instead, we took a more sort of interaction-centric
2597380	2603780	route where we asked people to do sort of, you know, style change by themselves as the system
2603780	2609460	was presenting these recommendations, and they still had to do the fix by themselves. But what
2609460	2615460	we expected here was that people can then customize by seeing these attributes, they can learn,
2615460	2620100	they can discover new ways of doing things, they can think around, which can empower them,
2620100	2625140	especially in the more learning context, although the temporal dimension has to be more carefully
2625140	2631780	taken into account. So these were the four challenges that I wanted to
2631780	2637140	share today. And to kind of wrap up, I just wanted to pose two questions moving forward
2638020	2645300	from the interaction-centric perspective as HCI researchers. So first is, how might we design
2645300	2652820	these building blocks and interface affordances for new and upcoming AI models? Okay, so I think
2652900	2658180	part of it is that instead of building these point solutions, I think we need to think about,
2658180	2663700	are there any sort of generalizable frameworks, libraries, widgets, or interface affordances
2664340	2669860	that we could come up with as a community that is really good at these kinds of things?
2670580	2676420	And the second question is, does AI really require us to have these new things? I mean,
2676420	2682020	can we just use existing design elements and frameworks to build AI applications?
2682820	2690260	And I tend to think that we might need something new for these new and upcoming AI models,
2690260	2695460	especially because they have these very different characteristics than the conventional systems
2695460	2701060	that we have been building. They're more probabilistic, harder to predict, more black box in nature,
2701620	2707700	yet seemingly more impactful and powerful in terms of what they do, hallucinating. All these
2707700	2713460	properties packed together, I think we might really need to think about, what are the types
2713460	2720340	of interaction affordances that are really built for supporting the usability of these
2720340	2728660	AI-powered applications? So in this, I think as a community, we are making all these great
2728660	2733300	advances, like making different types of contributions. And I tend to focus on more
2733860	2739700	interactive systems and techniques, whereas other people focus on introducing new design
2739700	2744820	processes and understandings. And I think all this work is needed. And some of the interesting
2744820	2751860	examples of adding an interaction layer to these new types of models is in this example,
2751860	2758260	Tailbrush, where the user can draw the level of fortune that they won in the character to have
2758260	2768100	when they use generative models to generate a story. Or this AI chains work, which presents these
2768100	2775620	primitives and workflows for putting together this workflow that can accomplish more complex tasks
2775620	2783540	with these LLM prompts that a single prompt cannot really perform. And in my research group,
2783540	2789220	with my PhD student, Tesu Kim, we have been investigating this idea of what would be more
2789220	2794820	generalizable design framework. And thinking about input, model, and output, we have been
2794820	2801860	thinking about the concepts of cells, generators, and lenses, and tried to introduce this standardized
2801860	2807300	libraries and widgets that people can easily adopt in their AI applications. So for example,
2807300	2814100	using this kind of framework, people can build a copywriting app, email app, or story writing
2814100	2819860	app using pretty much the same kind of framework, which can save people's time while supporting
2819860	2825220	the types of interactions like iterations and comparison and experimenting different outputs.
2827540	2832980	And the second question, and the final question that I want to ask today is, how might we as an
2832980	2838980	HCI community collaborate better with the AI community on these various things? And it was also
2838980	2845140	the discussion that I was having a lot with today's meetings, and also with various AI researchers,
2845140	2850660	especially in Europe. And in terms of community collaboration, of course, one of the important
2850660	2855780	things is metrics. And there was also a great discussion at the HCI conference a couple weeks
2855780	2862900	ago, hosted here at Stanford. And in the AI community, it cares a lot about model performance
2862900	2868260	and generalization errors, where in HCI, we tend to focus on the human experience. So how do we
2868260	2874900	really bridge the gap between the metrics? And what it means to do AI research with more human
2874900	2880260	side metrics incorporated? What's the incentive for people to do that? And how do we encourage
2880260	2886980	poor AI people to use these metrics, too? In terms of human input design, a lot of the comments
2886980	2892500	that I was getting in terms of interaction-centric AI from AI researchers is that these ideas are
2892500	2898740	great, but then I don't really know how to actually take action about it. And part of it is, in their
2898740	2905540	model-building kind of work, how can I incorporate human feedback? And how do I use it in a meaningful
2905540	2911220	way to really change the way the model actually works, rather than just getting more high-level
2911220	2920500	design guidance? So one great direction for this might be, think about more making human feedback,
2920500	2924980	more computationally feasible, so that this compatibility is actually satisfied.
2926500	2930820	And lastly, we need to think about the change in design process as well. And in a lot of,
2931540	2937460	this is Stanford D-School's user-centered design cycle. And I think in a lot of the AI research,
2937460	2942500	what we're seeing is this prototype test kind of culture. You try something new, test it,
2942500	2948820	iteratively improve it. But then one of the frustrations is that interaction often comes
2948820	2955460	too late, right? There's this new cool model, and can you build an UI on top of it, is sort of the
2955540	2960820	kind of discourse we get a lot. And I think interaction should not just be like an icing
2960820	2966660	on the cake, but really something that can guide the entire design process or help people determine,
2966660	2972260	is this the right problem to tackle in the first place? Or what kind of interaction should we try
2972260	2977620	to support with AI? And based on that, think of what AI should do and should not do and how much
2977620	2984980	AI should be used in a particular context. So that's all I wanted to share. And here's a summary
2984980	2989140	of what I mentioned today, and I'd be happy to take any questions. Thank you.
2994660	2999220	All right, so I'll check my recommendations of facilitating messages. If there are any more ideas.
3001140	3005460	No? What do you think?
3006100	3009620	It really sounded like an AI.
3011220	3021060	I'll just click them all. I want to pull the mic on. I want to pull the thread a little bit on
3021060	3028900	this notion of how to connect human feedback with the objective functions that you touched on near
3028900	3033700	the end, because that's been rattling around in my head in much of the talk that you're giving,
3033700	3037380	that if I think about what should AI researchers be doing differently,
3038980	3044180	then you're asking, well, what's the proper model of the person in their system?
3045460	3049300	And traditionally, the problem has been that human interaction is really expensive,
3050260	3054420	just to collect annotated data. Or once you have it to be able to tune the model,
3056260	3062500	you don't get that much of it. And so they often fall back on self-supervision, or as you've been
3062500	3068420	talking about in the value alignment, they train an RL model to mimic a human and then let that go
3068420	3074020	loose. And it seems like until, I think they're kind of, I want you to take a position on one of
3074020	3079380	the two positions. One either is to say, look, we need to find strategies like that where we can
3079380	3086900	create proxy humans, and that's how we hook into the objective functions, the loss functions, etc.
3086900	3092100	The other alternative would be to say, no, we're going to find some other way to actually
3092100	3100020	make human feedback at a scale and in a form that they can directly use in the models. I'm
3100020	3104260	just curious, like, if you want to take a bet, where's your bet on that? Where should we be heading?
3105460	3111700	Yeah, that's an excellent question. I would say, I mean, you asked me to take a position,
3111700	3118580	but I would say both will be prevalent. And I like the letter much more. And I think that's more
3118580	3123540	promising and sustainable. And for example, the reason I'm really interested in this, like,
3123540	3129620	co-learning feedback loop between the human and the machine is that, you know, even if this super
3129620	3136020	advanced AI comes along and let's say it presents this, like, super accurate explanations, people's
3136020	3141780	self-explanation activity is still meaningful, right? Because that's how they could learn.
3141780	3147620	And so I feel like, you know, we can really try to find these compatible
3149700	3155940	mechanisms in which the human can get the benefit and get the incentive for doing what
3155940	3160260	they are really good at and what is helpful for them, not necessarily trying to help the system
3160260	3166340	or, you know, getting paid to system, paid to support the system per se. And at the same time,
3166340	3170900	the system can use it for something meaningful. And on the system side, I think in the system,
3170900	3177860	like, access that I presented, I was really happy when we landed at this technical solution where
3177860	3185860	people's rating data could be almost directly piped into the feedback for the RL agent to kind
3185860	3191300	of use as meaningful feedback. So I think that's just one example where this kind of worked out
3191300	3196900	for this kind of context. And I think we need to really investigate more and think about, are there
3197460	3202740	any generalizable mechanisms that this kind of approach could work in different contexts?
3202740	3207380	This assumes that you have a large set of users you can draw on, like there are learners that
3207380	3212100	are coming through your system. If I'm early on in the pipeline and I just kind of have V0,
3212100	3215220	I don't have the users yet, are there strategies you would recommend?
3216020	3220740	Yeah, yeah, excellent. So in that same access system, for instance, what we did was to
3221620	3229780	insert the instructor-generated explanations as sort of the initial seed. And I was also imagining
3229780	3236260	maybe using LLMs, for instance, we can plug in AI-generated ones to kind of avoid the cold
3236260	3242340	start problem. And it would be interesting to see how, you know, in the same system, like AI-generated
3242340	3247860	ones, instructor-generated ones, and learner-generated ones can kind of compete against each other
3247860	3251780	until the system ultimately just focuses on what is best for learners.
3252820	3259060	This is kind of a two-part question, going back to the like third challenge or like project you
3259060	3265380	talked about, where there was that note about AI as proxy, like people kind of using that as
3266100	3270740	like an excuse to make points, where maybe they wanted to do something but didn't want it to come
3270740	3278180	off as them. So the first part of the question is like, in that case, did people want to,
3279140	3283700	later it says people wanted the message to kind of sound like them, but in the case of the AI
3283700	3289780	as proxy, did they want that to sound like them? Or were they wanting it to sound more artificial?
3289780	3295940	And then second part of the question is, do you think there are more situations than just this
3296020	3302500	where maybe we don't want the AI to feel super personable and maybe want the interaction to
3302500	3307620	feel slightly more kind of mechanical or unnatural? Yeah, that's an excellent question. And I would
3307620	3314900	say these were somewhat different use cases, and both I think are valuable and smite. And
3316340	3321860	that again, I think in a more model-centric approach, we also kind of focus on trying to
3322660	3328500	create these messages that are more like humans. And that could be effective in certain cases,
3328500	3334260	but as you said, that might not really be what the users want, because in a proxy kind of setting,
3334260	3340340	you might not actually want it to sound too personalized, because maybe the more canned
3340340	3348980	message might actually work better in that context. And vice versa. So I think just being able to
3348980	3354420	identify all these different needs that people have and expectations that people have and being
3354420	3360820	able to somewhat fluidly support those, I think was really an interesting kind of observation
3360820	3366580	that we had. And I think moving forward, one of the lessons was that this more personalizable
3367860	3372100	message generation could be an interesting technology that could be potentially integrated,
3372100	3377140	but that's not going to solve everything, because there are these other types of needs that will
3377140	3383780	not be supported, even with the perfect personalizable style transfer. So yeah.
3386020	3391780	Explaining stuff, I kept thinking about how what you described and sort of the challenges
3391780	3398180	that we see with this new deep networks and models and how we interact with them are
3398820	3404260	similar to how people used to interact with search engines, right? At the beginning, people were
3404340	3411140	not as good as sort of figuring out how to query the search engine right. And over time, both
3412420	3417220	we became better at querying the search engines, and then the search engines became better at
3417220	3423540	sort of understanding how to interpret user queries. Do you see any similarities there? Is
3423540	3430900	there something that's very unique to the challenges we face with this new models? Or is it just that
3430980	3437060	we haven't had enough time to sort of adopt to each other in a way? Yeah.
3437060	3444100	Excellent. Yeah. And I think it's a recurring theme as these new technologies come in. Initially,
3444100	3449060	people would kind of struggle and they would need to learn how it actually works through trial and
3449060	3454420	error and lots of like failed attempts. And that's what we're seeing with these like
3454420	3458980	chat GPT, for instance, a lot of people are trying things out, reporting success and failure cases.
3459940	3466660	So I do think there are certain similarities. What's more unique about what we're seeing right
3466660	3473940	now is that due to the nature of like how black box, complex, unpredictable these models are,
3473940	3480980	I think it just confuses people much more. And there's a question of, you know, is this really
3480980	3488020	like a human learning problem to begin with, right? So if people take, do it more, and you know,
3488020	3493220	if they had more time, will people be actually able to really get to a point where they could
3493220	3498020	really easily create something that they like? Probably not. Right? So that's why I think we
3498020	3504180	need both on the model side to kind of think about what are more interactable and learnable ways of,
3504180	3509140	you know, architecting this kind of models in the first place. And also from the HCI point of view,
3509140	3515060	what are these interaction mechanisms that could be added to these models in a way that
3515060	3520180	it is actually more understandable and usable on the user side? Yeah. Thanks so much. Yeah.
3521140	3524340	I think we're at about the time, but Duho will be here for a couple minutes after the talk for
3524340	3528180	further questions. So let's thank him for speaking. Thank you.
