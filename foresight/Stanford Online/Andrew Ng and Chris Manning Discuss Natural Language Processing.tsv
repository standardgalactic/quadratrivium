start	end	text
0	13020	Hi, I'm delighted to be here with my old friend and collaborator, Professor Chris Manning.
13020	17080	Chris has a very long and impressive bio, but just briefly, he is Professor of Computer
17080	22080	Science at Stanford University and also the Director of the Stanford AI Lab.
22080	27520	And he also has a distinction of being the most highly cited researcher in NLP or natural
27520	28520	language processing.
28920	30880	So really good to be here with you, Chris.
30880	32560	Oh, good to get a chance to chat, Andrew.
33640	37480	So, you know, we've known each other, collaborated for many years.
37480	41880	And one interesting part of your background, I always thought, was that even though today
41880	46360	you're a distinguished researcher in machine learning and NLP, you actually started off
46360	48080	in a very different area.
48080	55120	Your PhD, if I'm not correctly, was in linguistics, and you were studying the syntax of language.
55560	60080	So how did you go from studying syntax to being an NLP researcher?
60680	62480	So I can certainly tell you about that.
62480	66720	But I should also point out that, you know, I'm still actually a professor of linguistics
66720	67120	as well.
67120	68920	I have a joint appointment at Stanford.
69880	74160	And, you know, once in a blue moon, not very often, I do actually still teach some real
74160	78320	linguistics as well as computer-involved natural language processing.
79160	87040	So, you know, so starting out, I was very interested in human languages and how they
87040	92240	work, how people understand them, how they are required.
92240	97960	So I had this sort of appeal, I saw this appeal in human languages.
98560	106680	But that equally led me to think about ideas that we now very much think about as machine
106680	108960	learning or computational ideas.
108960	117080	So two of the central ideas in human language, how do little children acquire human language?
117080	121920	And for adults, well, we're just talking to each other now, and we pretty much understand
121920	122640	each other.
122640	126040	And, you know, that's actually an amazing thing how we managed to do that.
126040	128200	So what kind of processing allows that?
128200	133120	And so that early on got me interested in looking at machine learning.
133120	138560	In fact, even before I'd made it to grad school, I'd started, you know, baby steps
138560	141240	and learning machine learning coming off of those interests.
141680	143960	Yeah, in fact, all human languages learn.
143960	146560	You know, we had learned at some point in our lives to speak English.
146560	148280	We'd grown up in a different place.
148280	150360	We would learn a totally different language.
150360	155880	So it's amazing to think how humans do that and now maybe machines learn language too.
156240	160360	But so, so just, you know, tell us more about your journey.
160360	164920	So you had a PhD in linguistics, and then, and then how did you?
165320	167480	So there's some stuff before that as well.
167640	173680	So, I mean, you know, when I was an undergrad, well, officially, I actually did three majors.
173680	178800	This was in Australia, one in math, one in computer science and one in linguistics.
179000	184480	Now, people get a slightly exaggerated sense of what that means if you're in an American
184480	189000	context, because, you know, it'd be, I think, impossible to complete three majors
189000	191080	or not undergrad at Stanford.
191200	195840	But, you know, actually, where I was as an undergrad doing, I did an arts degree.
195840	198040	So I could do whatever I wanted, like linguistics.
198200	201720	You had to complete two majors to complete the arts degree.
201720	205400	So, you know, it's sort of more like double majoring, maybe in US terms.
206280	210160	You probably don't know this about me, but at Carnegie Mellon, I actually was a triple major.
210760	213560	Math CS was once in the statistics and economics.
214600	216560	That's great, we're both fellow triple majors.
217040	223080	Yeah, so anyway, I did have background in interest in doing things with computer science.
224040	227360	And so my interests were kind of mixed.
227360	231080	And I mean, actually, you know, when I applied to grad schools, I mean, one of the places
231080	235840	I applied to was Carnegie Mellon, because they were strong in computational linguistics,
235840	239760	you know, and if I'd gone there, I would have been enrolled as a CS student.
240200	244720	But I ended up at Stanford as a linguistics student, because at that time there wasn't
244720	248480	any natural language processing in the CS department.
249040	254280	But, you know, I was still interested in pursuing ideas in natural language processing.
254280	260880	But at that point in the early 90s, things were just starting to change.
260880	269600	But the bulk of natural language processing was rule-based, logical, declarative systems.
269840	275480	But it was also in those years, at the beginning of the 90s, when there first started to be
275480	280360	lots of human language material, text and speech available digitally.
280360	283920	So this was really actually just before the World Wide Web exploded.
283920	289640	But there had already started to be things like legal materials and newspaper articles
289640	295600	and parliamentary handsaws, where you could at last get your hands on millions of words
295720	297160	of human language.
297160	302200	And it just seemed really clear that there had to be exciting things that you could do
302200	305240	by working empirically from lots of human language.
305240	311320	And that's what really sort of got me involved in a new kind of natural language processing
311320	313440	that then led into my subsequent career.
314000	317280	It sounds like your career was initially more linguistics.
317280	323000	And if the rise of data and machine learning and empirical methods, it shifted to what NLP
323000	324800	and machine learning and NLP?
325360	327720	Yeah, I mean, it absolutely certainly shifted.
327720	333200	And I've certainly sort of shifted much more to doing both natural language processing
333200	335240	and machine learning models.
335240	338840	But to some extent, the balances varied.
338840	341280	But I've sort of been with that as of while.
341280	343800	You know, actually, there's an undergrad.
343800	350280	For my undergrad on this thesis, it was sort of learning the forms of words.
350280	355520	So how you can, which became a famous problem of sort of learning past tense of English
355520	358560	verbs in the early connectionist literature.
358560	362720	And I was trying to sort of learn paradigms of forms of verbs.
362720	369720	And I was learning rules for the different forms using the C4.5 decision tree learning
369720	371200	algorithm, if you remember that.
372440	372760	Yes.
373800	374240	Right.
374400	375000	Good times.
375000	375200	Yeah.
375200	377520	And it's surprisingly non-intuitive, right?
377520	379880	How going from present tense to past tense.
380960	385320	From, I don't know, run to run and all the other weird special cases can be.
385320	385680	Yeah.
387400	391200	Hey, so we talked a bunch about NLP, natural language processing.
391200	398680	So for some of the learners, pick up machine learning for the first time, can you say, what is NLP?
398680	399960	Sure, absolutely.
399960	403400	Yes, NLP stands for natural language processing.
403400	407920	Another word this term that's sometimes used for that is computational linguistics.
407920	409000	It's the same thing.
409040	412800	I mean, natural language processing is actually a weird term, right?
412800	416160	So it means that we're doing things with human languages.
416160	420760	So you have to have the conception that you're enough of a computer scientist that when you
420760	425360	say language, you think in your brain programming language, and therefore you need to say natural
425360	429320	language to mean that you're talking about the languages that human beings use.
430040	435680	So overall natural language processing is doing anything intelligent with human languages.
435680	443480	So in one sense, that breaks down into understanding human languages, producing human languages,
443480	445600	acquiring human languages.
445600	449560	Though people also often think about it in terms of different applications.
449560	455560	And so then you might think about things like machine translation or doing question answering
455560	462120	or generating advertising copy or summarization.
462120	467880	There are so many different tasks that people work on with particular goals in mind where
467880	469640	you do things with human language.
469640	475280	And there's a lot of natural language processing because so much of what the world works on
475280	481560	our human world is dealt with and transmitted in terms of human language material.
482880	489680	So because of all of these applications or even a web stage, most of us use NLP many, many times.
489680	495600	Yeah, you're right. In some sense, the biggest application of natural language is web search,
495600	498440	right? That's really the big one.
498440	502520	I mean, traditionally, it was a kind of a simple one, right?
502520	507600	But in the good old days, it was, you know, there were various waiting factors and so on,
507600	513440	but it was mainly sort of matching keywords than your search terms and then some factors
513440	515120	about the quality of the page.
515120	519520	It didn't really feel like language understanding, but that's really been changing
519520	525840	over the years. So these days, you'll often, if you ask a question to a search engine,
525840	531760	it'll give you, you know, an answer box where it has extracted a piece of text and puts
531760	535720	what it thinks is the answer in bold or color or something like that, which is then this
535720	537720	task of question answering.
537720	540440	And then it's really a natural language understanding task.
540440	545600	Yeah, yeah. And I feel like in addition to web searches, maybe the big one, you know,
545680	551080	when we're going to a online shopping website or a movie website and typing in what we want
551080	556440	and doing a website search on a much smaller website than, you know, the big search engines,
556440	562240	that also increasingly uses sophisticated NLP algorithms and it's also creating quite
562240	567680	a lot of value. Maybe to you is not, you know, the real NLP, but it still seems very valuable.
567680	572400	I agree. It's very valuable. And there are, you know, lots of interesting problems in
572400	577560	any e-commerce website with search, very difficult problems, actually, when people
577560	581840	describe the kind of goods they want and you need to be trying to match it to products
581840	585640	that are available. That isn't an easy problem at all, it turns out.
585640	592640	Yeah, that's true. Yeah. So over the last couple of decades, NLP has gone through a major
592640	598440	shift from more of the rule-based techniques that you alluded to just now to using really
598480	605480	machine learning much more pervasively. And so you were one of the people at, you know,
605480	610240	leading parts of that charge and seeing every step of the way of creating some of the steps
610240	614880	as it happened. Can you say a bit about that process and what you saw?
614880	621000	Sure, absolutely. Yeah. So when I started off as an undergrad and grad student, really
621040	628040	most of natural language processing was done by hand-built systems, which variously used
629720	636720	rules and inference procedures to sort of try and build up paths and an understanding
637040	642280	of a piece of text. What's an example of a rule or an inference system?
642280	649280	So, you know, a rule could be part of the structure of human language, like English
649680	654400	sentence normally consists of a subject noun phrase followed by a verb and an object noun
654400	660040	phrase, and that gives you some ideas to how to understand the meaning of the sentence.
660040	667040	But it might also be saying something about how to interpret a word, so that a lot of
667160	674160	words in English are very ambiguous. But if you have something like the word star and
675160	680280	it's in the context of a movie, then it's probably referring to a human being on this
680280	686240	astronomical object. And in those days, people tried to deal with things like that using
686240	693240	rules of that sort. That doesn't seem very likely to work to us these days, but, you
694240	701240	know, once upon a time that was pretty standard. And so it was only when lots of digital text
702160	706480	and speech started to become available that it really seemed like there was this different
706480	713240	way that instead we could start calculating statistics over human language material and
713240	720240	building machine learning models. And so that was the first thing that I got into in the
722200	729200	sort of mid to late 1990s. And so, you know, the first area where I started doing lots
729320	734600	of research and publishing papers and getting well known is building what in the early days
734600	740440	we often called statistical natural language processing. But it later merged into in general
740440	746040	probabilistic approaches to artificial intelligence and machine learning. And that sort of took
746040	753040	us through to approximately 2010, let's say. And that's roughly when the new interest in
754040	761040	deep learning using large artificial neural networks started to take off. For my interest
762360	767840	in that, I really have you to thank Andrew, because at this stage, Andrew was still full
767840	773840	time at Stanford, and he was in the office next door to me, and he was really excited
773840	779160	about the new things that were happening in the area of deep learning. I guess anyone
779200	783200	who walked into his office, he'd tell them, oh, it's so exciting what's happening now,
783200	788720	and neural networks should start looking at that. And so, you know, that was really the
788720	795720	impetus that got me pretty early on involved in looking at things in neural networks. I
797520	803000	had actually seen a bit of it before. So while I was a grad student here, actually Dave Ruhmelhardt
803000	808200	was at Stanford and Psych, and I'd taken his neural networks class. And so, you know,
808240	813560	I'd seen some of that, but it hadn't actually really been what I'd gotten into for my own
813560	816880	research. So around.
816880	819880	I didn't know that. Thank you.
819880	822880	And then we wound up, you know, supervising some students together.
822880	823880	Yeah, absolutely.
823880	828880	But I'd love to hear the rise of also deep learning in NLP. What are the GCs?
828880	835880	Yeah. So starting about 2010, yeah, me, students started to do the first papers in
838880	843920	deep learning aimed at NLP conferences. You know, it's always hard when you're trying
843920	850680	to do something new. We had exactly the same experiences that people 15 or so years earlier
850680	855760	had had when they started trying to do statistical NLP of when there's an established way of
855760	860360	doing things. It's really hard to push out new ideas. So really some of our first papers
860360	867160	were rejected from conferences and instead appeared at machine learning conferences or
867160	872240	deep learning workshops. But very quickly that started to change and people got super
872240	879240	interested in neural network ideas. But I sort of feel like the neural network period
879600	886600	which started effectively about 2010 itself divides in two. Because for the first period,
887600	894680	let's basically say it's till 2018, we showed a lot of success at building neural networks
894720	900720	for all sorts of tasks. We built them for syntactic parsing and sentiment analysis and
900720	907720	what else to question answering. But it was sort of like we were doing the same thing
908600	914480	that we used to do with other kinds of machine learning models, except we now had a better
914480	919760	machine learning model. And we were sort of instead of training up a logistic regression
919800	924400	or a support vector machine, we were still doing the same kind of sentiment analysis
924400	930640	task but now we're doing it with a neural network. So I think looking back now in some
930640	937640	sense, the bigger change came around 2018 because that was when the idea of well we
939040	946040	could just start with a large amount of human language material and build large self-supervised
947040	954040	models. So that was models then like BERT and GPT and successor models to that. And
954960	960760	they could just sort of acquire from word prediction over a huge amount of text this
960760	967760	amazing knowledge of human languages. I think really probably that's going to be viewed
967760	974280	in retrospect as the bigger kind of cut point where the way things were done really changed.
974760	979760	Yeah, I think there is that trend for the large language models, learning from math
979760	984640	and the mouse and data. I think even to lead up to that, there was one of your research
984640	991640	papers that really slightly blew my mind, which is a glove paper. So because with word
992160	997720	embeddings where you learn a vector of numbers to represent a word using a neural network,
997720	1004120	that was quite mind blowing for me. And then the glove work that you did really
1004160	1008000	cleaned up the math, made it so much simpler. And then I remember reading I said, oh, that's
1008000	1013160	all there is to it. And then you can learn these really surprisingly detailed representations
1013160	1017440	of the computer learns real nuances of what words mean.
1017440	1022840	Absolutely. Yeah, so I should give a little bit of credit to others. Other people also
1022840	1029800	worked on some similar ideas, including Renan Colbert and Jace Weston and Tom Ostermeek
1030000	1036760	and colleagues at Google. But the glove word vectors is one of the very prominent systems
1036760	1041960	of word vectors. So these word vectors already did, yeah, you're right, illustrate this idea
1041960	1046560	of using self-supervised learning that we just took massive amounts of text, and then
1046560	1053160	we could build these models that knew an enormous amount about the meaning of words. I mean,
1053160	1060160	it's still something I sort of show people every year in the first lecture of my NLP class,
1061440	1068440	because it's something simple, but it actually just works so surprisingly well. You can do
1068440	1074600	this sort of simple modeling of trying to predict a word given the words in the context.
1074600	1079560	And simply by sort of running the math of learning to do those predictions well, you
1079600	1084200	learn all these things about word meaning, and you can do these really nice patterns
1084200	1091200	of similar word meaning or analogies of something like, you know, pencil is to drawing as paint
1092800	1099800	brush is to, and it'll say painting, right? That it's sort of already showing just a lot
1100000	1107000	of successful learning. So that was the precursor to what then got developed to the next stage
1108000	1114000	with things like Burton GPT, where it wasn't just meanings of individual words, but meanings
1114760	1116880	of whole pieces of text and context.
1116880	1121880	Yeah, so I thought it was amazing that you can, you know, take a small neural network
1121880	1127720	or some model and then give it lots of English sentences or some of the language and hide
1127720	1132240	the word, ask it to predict what is the word that I just hit, and that allows it to learn
1132280	1138780	these analogies and these very deep, what you would think are really deep things behind
1138780	1145280	the meaning of the words. And then, you know, 2018, maybe there's other infection point.
1145280	1146800	What happened after that?
1146800	1153800	Yeah, so I mean, in 2018, that was the point in which, well, sort of really two things happened.
1154000	1161000	One thing is that people, well, really in 2017, had developed this new neural architecture,
1162760	1169760	which was much more scalable onto modern parallel GPUs, and so that was the transformer architecture.
1171000	1176360	The second part of it, though, was, you know, maybe people rediscovered because it was using
1176360	1181960	the same trick as the glove model, that if you have the task of say, of just predicting
1181960	1188200	a word given a context, either a context on both sides of it or the preceding words,
1188200	1195200	that that just turns out to be an amazing learning task. And that surprises a lot of
1195200	1202040	people, and a lot of the time you see discussions where people say disparaging things of, you
1202040	1206760	know, this is nothing interesting is happening, all it's doing is statistics to predict which
1206840	1213840	word is most likely to come after the preceding words. And I think the really interesting
1213840	1220680	thing is that that's true, but it's not true. I mean, because, yes, what the task is, is
1220680	1227440	you're predicting the next word given preceding words. But the really interesting thing is,
1227440	1234440	if you want to do that task really as well as possible, then it actually helps to understand
1235200	1241200	the whole of the rest of the sentence and know who's doing what to who in what's in the sentence.
1241200	1248200	But more than that, it also helps to understand the world. Because if your, your text is going
1248800	1255800	something along the lines of, you know, the currency used in Fiji is that, well, you need
1257360	1262920	to have some world knowledge to know what the right answer to that is. And so good models
1262960	1269960	at doing this learn both to follow the structure of sentences and their meaning and to know
1271400	1276800	facts about the world also that they can predict. And therefore this turns into what
1276800	1282240	sometimes referred to as an AI complete task, right? That you really need, there's nothing
1282240	1289080	that can't actually be useful in answering this, what word comes next sense, right? You
1289080	1296080	know, you can be in the World Cup semifinals, the teams are, and you need to know something
1296600	1299520	about soccer to be giving the right answer.
1299520	1304720	AI completes this funny concept, or is this idea that you can solve this one problem,
1304720	1310400	you can solve, you know, everything in AI or kind of make an analogy to NP complete problems
1310400	1315640	from the theory of computing. What do you think? Do you think predicting the next word
1315680	1318640	is AI complete? I have very mixed feelings about that myself.
1318640	1323640	I shall, I shall go ahead and say, I don't think it's true. So just what do you think?
1323640	1330640	I think it's not quite true because I think there are other kinds of things that human
1333200	1339080	beings manage to work out. You know, there are human beings that have clever insights
1339080	1345600	in mathematics, or there are human beings who are looking at something that's much more
1345760	1352560	you know, three dimensional, real world puzzle of sort of figuring out how to do something
1352560	1359560	mechanical or something like that, and that's just not a language problem. But on the other
1360960	1367960	hand, I mean, I think language gets closer to universality than some people think as
1368920	1375920	well, because, you know, we live in this 3D world and operate in it with our bodies and
1378320	1385320	our feelings and other creatures and artifacts around it, and you could think, well, not
1385520	1392520	much of that is in language at all. But actually, just about all of this stuff, we think about,
1393400	1399440	we talk about, we write about it in language, we can describe the positions of things relative
1399440	1404680	to each other in language. So a surprising amount of the other parts of the world are
1404680	1409640	seen in reflection in language, and therefore you're learning about all of them too when
1409640	1411600	you learn about language use.
1411600	1417600	You learn about, you know, one aspect of a lot of things, even if things like, how do
1418280	1425280	you ride a bicycle? You don't really learn how to ride a bicycle, but you learn some
1425480	1429880	aspects of what it involves, that you need to balance and you have to have your feet
1429880	1434680	on the pedals and push them and all of that kind of things, yeah.
1434680	1441200	And so with this trend in NLP, the large language models has been very exciting for the last
1441200	1446160	several years. What are your thoughts on where all this will go?
1446520	1453520	Well, I mean, yeah, so it's just been amazingly successful and exciting, right? So we haven't
1455440	1458760	really explained all the details, right? So there's a first stage of learning these
1458760	1465760	large language models where the task is just to predict the next word and you do that billions
1465760	1472360	of times over a very large piece of text, and behold, you get this large neural network,
1472400	1477240	which is just a really useful artifact for all sorts of natural language processing tasks.
1477240	1481520	But then you still actually have to do something with it if you want to do a particular task,
1481520	1488520	whether that's question answering or summarization or detecting toxic content in social media
1488520	1492480	or something like that. And at that point, there's a choice of things that you could
1492480	1498840	do with it. The traditional answer was then you had a particular task, like say, detecting
1498840	1504480	toxic comments in social media, and you'd take some supervised data for that, and then
1504480	1511480	you'd fine tune the language model to answer that classification task. But you were enormously
1511480	1517520	helped by having this base of this large self-supervised model because it meant that the model had
1517520	1523280	enormous knowledge of language and it could generalize very quickly. So unlike the sort
1523320	1530320	of the standard old days of supervised learning where it was kind of, well, if you give me
1531000	1536840	10,000 labeled examples, I might be able to produce a halfway decent model for you, but
1536840	1542520	if you give me 50,000 labeled examples, it'll be a lot better. It's sort of turned into this
1542520	1549040	world of, well, if you give me 100 labeled examples and I'm fine tuning a large language
1549040	1553960	model, I'll be able to do great, better than I would be able to do with the 50,000 examples
1553960	1559480	in the old world. Some of the more recent, exciting works now even going beyond that,
1559480	1563360	it's now, well, maybe you don't actually have to fine tune the model at all. So people have
1563360	1569360	done a lot of work using methods sometimes referred to as prompting or instruction where
1569360	1576360	you can simply in natural language, perhaps with examples, perhaps with explicit instructions,
1576440	1582240	just tell the model what you want it to do and it does it. Which, even as someone who's
1582240	1588560	been working in natural language processing for 30 years, it actually just blows my mind
1588560	1595560	how well this works. I guess I wasn't a decade ago thinking that in now we'd be able to
1597600	1604600	just tell the model, I want you to summarize this piece of text here and there will be
1606360	1613360	then summarize it. I think that's incredible. So we're in this very exciting time where
1613920	1620600	a lot of new natural language capabilities are unfolding. I think there's just no doubt
1620600	1627600	at all for the next couple of years the future of that is extremely bright as people work
1628080	1633400	out different things and different ways to do things and people start to apply in different
1633440	1639560	application areas, the kind of capabilities that have been unlocked with recent technological
1639560	1645560	developments. There's always a question in technology is to sort of whether the curve
1645560	1650400	keeps on heading steeply upwards or whether there's then some new things we have to discover
1650400	1657240	how to do. It's been going up for quite a while. So hopefully extrapolation is always
1657240	1664240	dangerous but we'll see. You mentioned writing prompts. It's still the NLP system, the large
1665400	1670120	language model, what you want and it seems to magically do it. I'm curious, do you think
1670120	1676320	prompt engineering is the path of the future where actually when I write these prompts
1676320	1681360	I sometimes find it works miraculously and sometimes it's frustrating. The process of
1681360	1686160	re-wording my instructions to tweak the wording to get it just right to generate the result
1686200	1690880	I want. So do you think prompt engineering is the way of the future or do you think it's
1690880	1696880	an intermediate hack until someone invents a better way to control the outputs of these
1696880	1703880	systems? I think it's both. I think it will be the way of the future but I also think
1706840	1713840	at the moment people are doing a lot of hacking around and re-wording to try and get things
1714760	1721760	to work better. With any luck with a few more years of development that will start to go
1722600	1729600	away. One way to think about the difference is in comparison to the kind of voice assistance
1731800	1738800	or virtual assistance that are available on phones and speaker devices like Amazon Alexa
1739120	1746120	these days. I think all of us have had the experience that present those devices aren't
1747200	1751720	always great but if you know the right way to word things it will do something but if
1751720	1758720	you use the wrong wording it won't. The difference with human beings is by and large you don't
1758720	1762520	have to think about that. You can say what you want and it doesn't matter what word you
1762520	1768720	choose. The other human being assuming it's someone who knows the same language etc. Well
1768920	1775920	understand you and do what you want. I think and would hope that we'll start to see the
1776200	1781040	same kind of progression with these models that at the moment fiddling around with the
1781040	1786440	particular wording you use can make a very big difference to how well it works but hopefully
1786440	1791880	in a few years time that just won't be true. You'll be able to use different wordings and
1791920	1798920	it'll still work but the basic idea that we're moving into this age where actually human
1799440	1806440	language will be able to be used as an instruction language to tell your computer what to do so
1806720	1813720	instead of having to use menus and radio buttons and things like that or writing Python code
1815680	1820240	instead of either of those things that you'll be able to say what you want in the computer
1820280	1826800	or do it. I think that age is opening up in front of us that will continue to build and
1826800	1833800	that will be hugely transformative. It feels like come a long ways but only much more to
1834720	1839920	come and much more to go. Absolutely. In the development of NLP technology there's one
1839920	1843640	thing I want to ask you and I suspect you and I may have different perspectives on this
1843720	1850720	but in the last couple of decades the trend has been to rely less on rule based engineering
1850720	1857240	and more on machine learning on data sometimes lots of data. Looking to the future where
1857240	1864240	do you think that mix of hand coded constraints or other constraints explicit constraints versus
1864240	1868040	you know let's get a neural network and throw lots of data at it. Where do you think that
1868120	1875120	balance will fall? I think that there's no doubt that using learning from data is the
1878320	1885320	way forward and what we're going to continue to do but I think there's still a space for
1885600	1892600	models that have more structure, more inductive bias that have some kind of basis of exploiting
1893360	1900200	the nature of language. So in recent years the model that's been enormously successful
1900200	1907200	is the transform in your network and the transform in your network is essentially this huge association
1907200	1913720	machine so it'll just suck associations from anywhere and look at two words and figure
1913720	1917760	out which words release and which other words for all words. Yes so you use everything to
1917760	1923520	predict anything and do it over and over again and you'll get anything you want and you know
1923520	1930520	that's been incredibly, incredibly successful but it's been incredibly successful in the
1930560	1937560	domain where you have humongous, humongous amounts of data right so that these transformer
1937800	1942880	models for these large language models are now being trained on tens of billions of words
1942960	1949400	of text. When I started off in statistical natural language processing and some of the
1949400	1955640	traditional linguists used to complain about the fact that I was collecting statistics
1955640	1962560	from 30 million words of newswire and building a predictive model and thought that was just
1962560	1969560	not what linguistics was about. I felt I had a perfectly good answer which is that a human
1970560	1977560	kid as their learning language they're exposed to actually well more than 30 million words
1977920	1982880	of data but you know that kind of amount of data so you know the kind of amount of data
1982880	1989640	we were using were perfectly reasonable amounts of data to be using to be you know not exactly
1989640	1994240	trying to model human language acquisition but to be thinking about how we can learn
1994240	2001240	about language from lots of data. But you know these modern transformers are now you
2003480	2010240	know using already at least two orders of magnitude more data and you know most people
2010240	2017000	think the way to get things to the next level is to use more still and make it three orders
2017000	2022520	of magnitude and you know in one sense that scaling up strategy has been hugely effective
2022640	2027080	so you know I don't blame anybody for saying let's make another order magnitude bigger and
2027080	2034080	see what amazing things we can do but it also shows that human learning is just way way
2037160	2042920	better in being able to extract a lot more information out of a quite limited amount
2042920	2048840	of data and at that point you can have various hypotheses but I think it's reasonable to
2048880	2055880	assume that human learning is somewhat structured towards the structure of the world and things
2057440	2062440	that sees in the world and that allows it to learn more quickly from less data.
2062440	2066840	Alright I'll move you on that. I think better learning algorithms, our current machine learning
2066840	2071760	algorithms are much less efficient or makes much less efficient use of data and so there's
2071760	2077440	way more data than any you know child and then I think whether the improved learning
2077480	2083520	algorithms will be from linguistic like rules or whether it'll just be engineers engineering
2083520	2090520	much more efficient versions of the transform or whatever comes after it. That will be traditional.
2090880	2097880	I don't think it'll be by people explicitly putting traditional linguistic rules into
2098200	2105200	the system. I don't think that's the way forward. On the other hand I mean you know I think
2105320	2112320	what we're starting to see is models like these transformer models are actually discovering
2112320	2119320	the structure of language themselves right so you know the broad facts of you know human
2119320	2124080	language that you know English has the subject before the verb and the object afterwards
2124080	2129040	whereas you know in Japanese that the verb at the end of the sentence and the subject
2129040	2133360	and object are normally in that order before it that could be in the other order you know
2133360	2139280	actually transformer models are learning these facts you can interrogate them and see
2139280	2143360	that even though they were never explicitly told about subjects and objects that they
2143360	2149360	know these notions so I think they you know they're discovering a lot else as well about
2149360	2154960	language use and context and the meanings and senses of words and what is and isn't you
2154960	2160840	know unpleasant language but part of what they're learning is the same kind of structure
2160840	2165680	that linguists have laid out as the sort of structure of different human languages.
2165680	2171800	So does it over many decades linguists discover certain things and by training on billions
2171800	2175800	of words transformers are discovering the same things that linguists discovered in human
2175800	2181680	language that's that's that's cool. So all this is really exciting progress in NLP driven
2181680	2186880	by machine learning and by other things. To someone entering the field entering machine
2186920	2192960	learning or AI or NLP there's just a lot going on. What advice would you have for someone
2192960	2201080	wanting to break into machine learning? Yeah well it's a great time to break in. I think
2201080	2206840	there's just no doubt at all that we're still in the early stages of seeing the impact of
2206840	2216000	this new approach where effectively software computer science is being reinvented in on
2216000	2221000	the basis of much more use of machine learning and the various other things that come away
2221000	2226360	from that and then more generally across industries there are just lots of opportunities for more
2226360	2233040	automation making more use of you know interpretation of human language material for me or in other
2233040	2242120	areas like vision and robotics or the same kinds of things. So lots of possibilities.
2242240	2247200	So you know at that point there's lots to do obviously and you want to get some kind
2247200	2253840	of good foundation right so knowing some of the core technical methods of machine learning
2253840	2260840	understanding ideas of how to build models from data look at losses do training diagnose
2260840	2268200	errors all of these core things I mean that's definitely useful for natural language processing
2268280	2273840	in particular some of those skills are completely relevant but then there are particular kinds
2273840	2279200	of models that are commonly used including the transformer that we've talked about a lot
2279200	2283400	today you definitely should know about transformers and indeed they're increasingly being used
2283400	2289320	in every other part of machine learning as well for vision bioinformatics even robotics
2289320	2295320	is now using transformers but beyond that I think it's also useful to learn something
2295360	2301440	about human language and the nature of the problems that involves because I mean even
2301440	2307920	though people aren't directly going to be encoding rules of human language into their
2307920	2315400	computing system a sensitivity to sort of what kind of things happen in language and what to
2315400	2320720	look out for and what you might want to model that's still a useful skill to have.
2321720	2327920	And then in terms of learning the foundations learning about these concepts you had entered
2327920	2335520	AI from a linguistic background and we now see people from you know all walks of life wanting
2335520	2341360	to to start doing work in AI what are your thoughts on the preparation one should have
2341360	2346800	or any thoughts on how to start from something other than computer science or AI so there are
2346840	2355360	lots of places you can come from and vector across in different ways and we're seeing
2355360	2361600	tons of people doing that that they're people who started off in different areas whether
2361600	2369200	you know it was chemistry physics or even much further in field and people you know history
2369200	2374160	whatever have started to look at machine learning I mean I think there are sort of two levels of
2374160	2381480	answer there I mean one level of answer is you know one of the amazing transformations is that
2381480	2389320	there's now these very good software packages for doing things with neural network models I mean
2389320	2395400	this these software is really easy to use you don't actually need to understand a lot of highly
2395400	2401360	technical stuff you've got need to have some kind of high-level conception about what is the idea of
2401360	2406560	machine learning and how do I go about training a model and what should I look at in the numbers
2406560	2411520	that are being printed out to see if it's working right but you know you don't actually have to
2411520	2416960	have a higher degree to be able to build these models I mean and indeed what we're seeing is you
2416960	2422360	know lots of high school students are getting into doing this because it's actually something that if
2422360	2429640	you have some basic computer skills and a bit of programming you can pick up and do it's just way
2429680	2436720	more accessible than lots of stuff that preceded a weather in AI or outside of AI and other areas
2436720	2442280	you know like operating systems or security but you know if you want to get to a deeper level than
2442280	2449320	that and actually want to understand more of what's going on I think you can't really get there if you
2449320	2457400	don't have a certain mathematics foundation like at the end of the day that deep learning is based
2457520	2465840	on calculus and you need to be optimizing functions and if you sort of don't have any background in
2465840	2473520	that I think that sort of ends up as a wall at some point so you know. The math for machine learning
2473520	2480960	and data science it does come in handy for some of the work we're going to do. Yeah so I think at
2480960	2488720	some level if you're at the major in history or you know non-mathematical parts of psychology I
2488720	2494360	actually have a good friend who yeah he you know learnt calculus in grad school because he was a
2494360	2499200	psychologist and he'd never done it before and decided he wanted to start learning about these
2499200	2506040	new kinds of models and decided it wasn't too late to be able to go and take a Cal course and so he
2506040	2513800	did right so you know you do need to know some of that stuff but for lots of people if they've seen
2513800	2521760	some of that before even if you're kind of rusty I think you can kind of get back in the zone and
2521760	2528440	it doesn't really matter that you haven't you know done AI as an undergrad or machine learnings and
2528440	2533560	things like that that you can really start to learn how to build these models and do things and you
2533560	2539480	know really that's my own story right that despite the fact that they let me sit in the school of
2539480	2546400	engineering at Stanford these days you know my background isn't as an engineer you know my PhDs
2546400	2554880	and linguistics but you know I've sort of largely vectored across from having some knowledge of
2554880	2561120	mathematics and linguistics and knowing some programming into sort of getting much more into
2561160	2563120	building AI models.
2563120	2568160	I was curious about something do you think the improved libraries and abstractions that are now
2568160	2573680	available like coding frameworks like TensorFlow or PyTorch do you think that reduces the need to
2573680	2579360	understand calculus because boy it's been it's been a while since I had to actually take a derivative
2579360	2584720	in order to even implement or create a new neural network architecture because of automatic
2584720	2594000	differentiation. Yeah I mean absolutely I mean so in the early days when we were doing things sort
2594000	2601120	of 2010 to 2015 right for every model we built we were working out the derivatives by hand and then
2601120	2606480	you know writing some code and whatever it was you know sometimes it was Python but sometimes it
2606480	2613320	might have been Java or C to calculate these derivatives and checking that we got them right
2613320	2621960	and so on where you know these days you actually don't need to know any of that to build deep
2621960	2627000	learning models I mean this is actually something I think about been thinking about even with respect
2627000	2633080	to my own natural language processing with deep learning class that I teach you know at the beginning
2633720	2641960	we do still go through doing you know matrix calculus and making sure people know about
2642040	2649640	Jacobians and things like that so that they understand what's being done in back propagation
2649640	2655640	deep learning but you know there's sort of this sense in which that means that we just give them
2655640	2661800	hell for two weeks you know sort of like boot camp or something to make them suffer and then we say
2661800	2666760	oh but you do the rest of the class with PyTorch and they sort of never have to know any of that
2666760	2673400	again right I you know there's always a question of how deep you want to go in technical foundations
2673400	2680840	right you can keep on going right like does a computer scientist in the 2020s need to understand
2680840	2689640	you know electronics and transistors or what happens in you know CPU well you know it's complicated
2689640	2694840	I mean in various ways it is helpful to know some of that stuff I mean you know I know Andrew you
2694840	2701080	were one of the pioneers and getting machine learning onto GPUs and well you know that sort of
2701080	2706600	means you had to have some sense that there's this new hardware out there and it has some attributes
2706600	2713080	of parallelism that means there's likely to be able to do something exciting so you know it is useful
2713080	2717880	to have some broader knowledge and understanding and you know sometimes something breaks and if
2717880	2723080	you have some deep knowledge you can understand why it broke but there's another sense in which you
2723080	2730040	know most people have to take some things on trust and you can do most of what you want to do in
2731160	2737160	neural network modeling these days without knowing calculus at all yeah that's a great point I feel
2737160	2742360	like sometimes the reliability of the abstraction determines how often you need to go in to fix
2742360	2747880	something that's broken so I actually my understanding of quantum physics is very weak I barely
2747880	2752440	understand it so you could argue I don't understand how computers work because transistors are built
2752440	2757800	in quantum physics but fortunately you know if something went wrong with transistors I've never
2757800	2766040	had to go in to try to fix it so they're a bit hard to fix I think and so I think I think well
2766040	2771240	another example you know the sort function their libraries are sort things and sometimes they actually
2771240	2775960	don't work right swap in the memory or whatever and that's when if you really understand how the
2775960	2782200	sort function works you can go in and fix it but then sometimes if we have abstractions libraries
2782280	2788600	APIs are reliable enough then that is nice to those abstractions then diminishes them to understand
2788600	2793720	some of the things that happen so it's an exciting world feels like you know we have giants building
2793720	2798440	on the shoulders of giants and and all of these things are becoming more complex and more exciting
2798440	2805960	every every every month yeah absolutely so thanks Chris that was really um interesting and inspiring
2805960	2812040	and and I hope that to everyone watching this hearing Chris's own journey um to become a
2812040	2817560	computer scientist and to become a leading maybe the leading NLP computer scientists as well as all
2817560	2824520	of this exciting work having an NLP right now I hope that inspires you to jump into the sphere and
2824520	2830360	take a go at it there's just a lot more work to be done collectively by our community than still
2830360	2833880	so I think the more of us are working on this the better off the world will be
2833880	2845720	so thanks a lot Chris it was really great having you thanks a lot Andrew it's been fun chatting
