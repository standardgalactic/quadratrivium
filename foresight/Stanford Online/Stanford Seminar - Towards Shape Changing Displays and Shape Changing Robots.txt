This is definitely one of the shortest travel trips that I've ever had to give a talk because
my office is just across the street, and this is a familiar space because we ran the Human
Computer Interaction Seminar in here last year.
So it's great to be here today, and as Mark said, I'm one of the faculty members in the
mechanical engineering department where I run the Shape Lab, but I'm also very much involved
with the Stanford Human Computer Interaction Group as well.
And so today I'm going to be talking about our work sort of at the intersection between
yeah, human computer interaction and robotics and mechatronics systems and thinking about
how can we really enable people to interact in much richer ways.
I also have to apologize, I had a hand procedure done earlier today, and basically I thought
it was going to be not as painful as it was, and so I'm a little bit more under the weather
than normal, so you'll have to believe that this would be even more exciting normally.
So speaking of hands, I've long been impressed by the dexterous ability of our hands to allow
us to manipulate the world, and certainly having recently had some injury for my wrist,
that's become even more clear to me how important our hands are for manipulating things in the
world and really just the amazing complexity of our hands and the way in which we're able to
interact so gracefully and dexterously to achieve really stillful manipulation such as here,
sculpting, and clay, but all of our everyday tasks as well.
But to me what's really exciting about the hand and manipulation is the role of manipulation
in terms of our cognition as well. And so physical interaction is very important for not just
physical manipulation for the sake of changing the world, but also for us to express our thoughts
as you see me gesturing like this, but also for us to better understand information. So there's
been a lot of studies that have looked at how children when they learn with abacai, for example,
or abacus in plural, abacai, that they actually learn math concepts differently and better than if
they learn them just on paper and pencil. And so there's something behind this idea of embodied
cognition that's actually quite important. And we see that not just in terms of education,
but really in terms of very specialized domains as well. So for example, this is a photograph
of urban planners and architects. And what we see there is there's this, you know, basically
rich interplay between the physical models that they're building, space, and the ways in which
people want to interact as well. And so physical and spatial form can really help us better understand
and solve problems and very complex ones in the architecture domain, but many other domains as
well. And the big kind of reason behind this is that spatial manipulation, as I kind of highlighted
before, really aids in spatial cognition. These two things are very tightly coupled together.
And one of the great professors that we have had at Stanford, Barbara Tversky, has done a lot of work
as well, looking into the role that spatial manipulation plays in our cognition and kind
of the evolutionary basis for this as well. So she has a great new book out called Mind and
Motion that I highly recommend you take a look at. But in many ways, this is because cognition
and perception are very much coupled together through action. And we've evolved over a long
period of time to really benefit from the way that we can manipulate the world to better
understand it. And these things are tightly coupled together. This is even more important
when we think about access to information. And so for example, these physical representations,
you know, can be really important for people with different abilities. So here's a picture
of a blind student at the Perkins School for the Blind in the Boston area looking or feeling,
rather, a double helix model. And this is something that would be very hard for us to explain through
words, but by being able to touch, feel, and manipulate it, they can, you know, much more
easily understand. And this is a tactile model of Berlin's Museum Island. Again, this type of
spatial information might be very hard for us to convey through text alone, but by allowing
people to directly access it through touch and through our haptic sense, we can much more readily
access it. But if we look at the ways in which we traditionally interact with computing and with
information, not much has changed since 30 years ago in some ways for the ways in which we interact.
And historically, it's been the case that we really haven't been able to leverage our sense of
touch in that. And if we look towards, you know, basically newer interfaces for interaction such
as in virtual reality, a lot of times we really get these benefits of spatial interaction where
here I can move around and interact and collaborate with someone else. But when we reach out to feel
something or touch something, we're not really receiving any meaningful feedback. And so one of
the key questions that I've been interested in as well as many other people in the field of haptic
interfaces is, you know, what if we could reach out and touch the void? And so during my PhD at
the MIT Media Lab, we were really interested in trying to merge between the physical and the digital
world and bring that sense of physical touch into the real world so that if we were, you know, two
people working around a workstation, we could look at each other, see each other, and also directly
interact with a surface and be able to feel it. And so let me show you what that might look like.
So here's a picture of a new type of haptic display. We call this a shape display that we developed at
the MIT Media Lab with my advisor, PhD advisor Hiroshi Ishii and my colleague Daniel Lightinger,
where we could, you know, really reach out and sort of sculpt with digital clay,
or we could reach out and new interface elements could appear that have the correct affordances
and ergonomics for how we might want to interact. Or we could render different types of 3D models in
sort of real time and be able to, again, see them in 3D but also touch them. So this type of display
had been sort of considered before by a number of other researchers, but here we were able to really
look at what are these meaningful interaction techniques and start to look at some example
applications as well. So this was a project that we worked on with Tony Tain, who was a
formerly a practicing urban planner. And as I said before, you know, the spatial models that
urban planners and architects build are often very important for how they think and consider this
space. But they're, you know, basically they would make one set of full models and then not
be able to change anything. And so here we can in real time change the models, but also have other
simulations that we can load on top of that and be able to, you know, edit, manipulate and collaboratively
work together. So this sort of gives a hint for what this new type of interaction with what we
call shape changing displays might be like. And so when I came to Stanford, I guess now seven-ish
years ago or so, I started the Shape Lab to really deeper investigate these types of systems,
but also work on some other areas as well. And so in our group, we've been working on new types of
haptic displays, like the ones that I've shown here. And that's really what I'm going to spend
the most of today talking about. But I quickly wanted to mention two other areas that our group
has also been working on as well, and that are kind of related to the robotics areas. And so
the next area that we've also been thinking about is this notion of ubiquitous robotic interfaces.
And really this idea harkens back to someone who maybe some of you know of, but Mark Weiser,
who sort of started the field of ubiquitous computing that we really live in today, where
there's, you know, in this room around us, many different sensors and actual or sorry, many different
sensors and displays, there's probably, you know, at least 100, if not 1000 computers in this room
right now. And so the question that we had is sort of what would it be like in the future if we
could have actuation and robotics be embedded in our environment as easily as the computing and
displays that we have today. And so with my PhD student, who's now a professor at Simon Frazier
University, Lawrence Kim, we created this platform of small mobile robots that we could then explore.
And again, here we can have them display information like this. But we also looked at ways in which
they could be embedded in our environment, like on our desk, move around to display information when
we need, but also manipulate things in the environment or go about, you know, remotely
sensing different things as well. And we think this kind of opens up some interesting opportunities
to think about, again, where does this line between robot and environment end. And so that's
something that we're also quite interested in and kind of intersects a lot with human robot
interaction. And the last area that we're doing work in is in shape changing robotics, which is
something that Mark highlighted before. And a lot of that has to do with the enabling technologies
that we are looking at to make shape changing displays and also shape changing robots. And so
if you think back to that first example haptic display that I showed that is sort of like a
2.5D surface display, one of the things that we have been thinking about is how might we sort of
make full 3D shape changing displays. This is sort of the holy grail of some of our work in terms of
thinking about, oh, how could we actually, you know, feel a whole entire dolphin and actually have it
be able to change shape between different surfaces. And so as one of my four of our PhD students,
Zach Hammond and a collaborator of ours, Nathan Yusevich, from Allison Otelmore's group,
we're trying to think about how might we make this vision a reality. And so we got inspired by
basically balloon animals to think about new ways that we might approach this. And the interesting
thing about balloon animals is that you have started out with something very simple, like
essentially inflated tube. And then we can pinch it at certain points, knot it together,
and basically create many different shapes out of the same, you know, simple balloon idea.
And so we tried to bring this idea to the field of soft robotics where we could have basically an
inflated beam. And as opposed to relying on pumps to move and actuate different areas,
what if we created a pinch point in that beam? And then what if we could move that pinch point around?
And then basically what that allows us to do is create some type of system that can really
dramatically change shape by putting a robot roller node on that surface and driving it around and
then creating what we call sort of this idea of a isoparametric type of robot where we have a fixed
length of inflated tube, but then we can change kind of the overall basically geometry, even though
it has a fixed topology. And so here is, you know, one element inside that, but we can place
these together through other kinematic connections to create some type of tetrahedral robot that we
can then basically a truss robot that we can then really dramatically change shape. And again,
we get these benefits of having this constant volume of air that we don't need to really pump
around, but instead using motors to move these pinch points or buckle points around. And so
here's an example of what those types of basically truss robots could look like. And so this is,
you know, a fairly large device about, you know, this size and it's able to pretty dramatically
change shape and move around. And this is all shown in real time. And so it can, you know,
locomote by basically doing some type of punctuated rolling, change its shape and move around. But
it can also go ahead and manipulate objects as well. So it can use the geometry of itself to
basically grasp an object and then be able to pick it up and actually do some interesting
in-hand manipulation as well. And so we think that this kind of idea of having these large
shape-changing truss robots has some interesting applications in terms of thinking about new
ways that we could be able to locomote, manipulate, and also use the shape change to afford different
types of interaction in the environment. And so we've also done some work to look at the
modeling and kinematic control and planning of this as well. So Zach and our group did a lot of
work on grasp optimization planning with these types of robot. And we have a collaboration
with Matt Schwager looking at decentralized control of these types of systems as well.
So I won't go into too many details in that area today, but I just wanted to give you kind of an
overview of some of the different things that we're working on in our group. But today I'm really
going to focus on these types of shape-changing or haptic displays. And I'm going to start out by
talking a little bit about the way in which we approach these problems. And really the way in
which we like to think about this in our group is really focusing very deeply on specific
applications and needs, working with domain experts in those areas, and then trying to learn more
about what are the enabling technologies or the requirements for the enabling technologies
to actually make these systems work. And then that kind of feeds back into other applications
and needs. And we think this is kind of a nice paradigm for working on, you know, basically
new technology to make these things possible. So I wanted to start by talking about two vignettes
of specific applications that we've been working on. And the first is in car design. And the second
is in making, making accessible to sort of highlight some of the challenges in making these
systems actually useful. And so many of you might be familiar with, you know, car design in general
and have seen these types of clay models that basically industrial designers and human factors
experts create to prototype and test car designs. And so we had been reached or we have been working
with Volkswagen who's trying to transition from making these clay models, which end up costing,
you know, something on the order of $100,000 per clay model. So it's very expensive and time consuming
to make those and moving instead to using virtual reality to prototype and test at steel, especially
in terms of working with different stakeholders that they care about, as well as the human factors
aspects of like the interior of the car as well. And so what they're trying to do is basically
transition from this, you know, very physical style of doing things to now move into the virtual
reality systems. But what they found is that, for example, in the case of the interior of the car
design, what they call the seating buck and the human machine interaction, basically, they'll be
able to load up different car designs and virtual reality. And then they have these very fancy seating
butts to adjust the position and height and steering wheel position to basically create any
different kind of car. But then you go and reach out and try to touch the HMI, the human machine
interface part of it, and the dashboard. And basically, you, you know, again, reach out and
touch nothing or touch air. And so one thing that they end up doing is creating these, you know,
basically machined or milled foam models that they can then place inside this seating buck
to then be able to go ahead and test. But it turns out it takes a long time for them to actually be
able to create these full models. They can't switch something, change things on, on demand, etc.
And so we had this idea of what if we could create, you know, interactive seating buck,
basically simulator, where we could create this, you know, basically surface that we could change
in real time to allow us to explore basically the different HMI interactions and some of the
ergonomic issues as well. So we started out working with them on this concept and trying to create a
new generation of our tactile displays to be able to do this. So this was led by Alexa Sue.
And so she created this, you know, basically, this is about the smallest that you can do with
kind of like low cost off the shelf DC actuators. So this has a direct drive between them with
the lead screw and, you know, many, many actuators and we made this modular so you can stack them
together. And we also, you know, looked at the integration with this as well as in virtual
reality. And then we brought it back to our collaborators at Volkswagen, particularly like
the industrial designers and the, you know, human factor specialists. And they sort of said, Oh,
hey, this is great. We like the idea. But, you know, wouldn't it be great if these could be a lot
higher resolution and much cheaper? So, you know, in terms of the types of things that they're looking
for, they were not still there with this technology. And again, Oh, wouldn't it be nice if, you know,
we could create many of these, not just, you know, basically, this device here cost something like
$6,000 in parts to create. So wouldn't it be nice if we could, you know, basically have this be
higher resolution and, you know, basically much larger in scale and lower in cost. So those were
some of the feedback that we got, which sort of makes sense in some ways. But then I wanted to
highlight another application area that we've been thinking about that also informs some of these
other challenges. And so this is in the area of still computerated design and this idea of making,
making accessible. And so many of you are probably very familiar with the, the making movement or
maker movement that kind of has gone on over the past 15 or 20 years. And basically, you know,
one of the great things about that is that it's really empowered a lot of people and, and served
as a great way to involve more people in STEM education. But that doesn't mean that all people
are unable to do that. And particularly a lot of the tools that we use for making, especially in
terms of computerated design are not accessible to people that are blind or visually impaired.
And so those people have historically been excluded from those areas. And so while 3D printing can
be very helpful in terms of supporting accessible education through the use of creating tactile
graphics or other types of materials that blind people can touch and feel, there's really this
lack of authoring tools for blind people for them to be able to be the designers and engineers
themselves. And so the big problem with these, essentially with these systems that exist for
computerated design is that they're all based on basically graphical user interfaces, where you
have to directly manipulate with a mouse and a keyboard, as well as the computer screen to be
able to select and control many features. And that's really great for people that are cited, right?
We've moved beyond command line interfaces to this direct, direct manipulation type of interface.
But for someone that's blind and visually impaired, basically the main way that they
interact with computers is through different types of screen readers, which basically provide audio
feedback. And so how might we think about ways in which they could still be able to use this?
So there is some work on using text-based editors for creating geometries. So for example, OpenSCAD
is a computed solid geometry modeler where you write basically in a declarative programming
language to define the geometry. And there's been some great work at the Dimensions Project at the
New York Public Library, Chansey Fleet there, in terms of basically using that for people that are
blind or visually impaired to write code to then be able to 3D print something. But if you can
imagine and you've used a 3D printer, you know they're not particularly fast. And so basically
you write some code. And then while we, you know, people that are cited would be able to, you know,
instantly see the changes that we're making, someone that's blind and visually impaired would
have to basically use a 3D printer, wait somewhere between an hour to, you know, 10 hours to find
out, oh, the change that I made was that exactly what I wanted. And so one of the questions we've
been asking is, how might dynamic tactile feedback, you know, support this type of interaction? And so
again, my former PhD student, Alexa Su, as well as some members from the blind and visually impaired
community in the Bay Area, Sun Kim, who's an Access Technology Specialist at the Vista Center for
the Blind, as well as Josh Miele, who is an amazing blind engineer and researcher who's now at
at Amazon working on accessibility there, we all work together to create a co-design with
other blind makers, a tool that allows people to use these types of tactile shape displays we've
been talking about to in real time have that feedback. So here, basically you're able to write
code in open SCAD, and then in real time be able to touch and feel the geometry and sort of be able
to understand what it is. And so one of the questions that we had, and then 3D print the design.
So one of the questions we had is, you know, what are the types of interactions that we need to port
from these direct manipulation interfaces that are really essential for basically helping blind
and visually impaired people understand that. So we did a lot of different co-design sessions to try
to understand what these challenges are. And one of the things that's interesting, it turns out that
section views are even more important, you know, basically for blind and visually impaired people
than for in our, you know, sighted base CAD systems. Because, you know, essentially, you know,
there's no notion of transparency in these types of displays. You're basically just feeling the top
surface of the convex hole or whatever. So you can't have any way to display, at least with these
displays, this notion of transparency, which we rely on a lot. So we have some, you know, basically
promising interactions that we think are possible with this and compared to like a single point haptic
device. We might have less high resolution in terms of the spatial component. But by allowing
people to touch it with their whole hands, they're actually much more easily able to understand what
it is and the shape. So we think this is a promising direction. But if we look at the types of geometries
that the people we were working with are able to create with this type of tactile display,
we can see they're quite limited. And so this was a big issue that we found. But there is this real
benefit from having this tight coupling of the iteration. And so we think this is a promising
direction and very promising in terms of this notion of real time iterative feedback. And also
that people in the blind and visually impaired community really do want to be designers and
makers of their own, you know, basically technology. And this type of system can be very empowering
for them. But, you know, there's still these big issues in terms of resolution. So if we think
about a computer display that we might use being very high resolution versus this pin display that
I'm showing here, you know, there's a big gap in terms of the resolution and potentially the
under understandability of that geometry. And then also in terms of access. So if each of those
tactile displays that still low resolution costs upwards of $6,000, that means that, you know,
many people can't have access to it, as opposed to, you know, $500 laptop that we could use to
teach cat. So across these different application areas, we really saw these big challenges in terms
of cost, steel and resolution, as well as interaction techniques. But I'm not going to talk as much
about that today. And so a lot of the work that we've done in the past couple of years has been
trying to address or mitigate some of these issues of cost, steel and resolution. And we've kind of
taken two different approaches to try to tackle that on our group. The first is in trying to create
new and novel technical solutions, hardware solutions to solve that. And the second area is on
kind of what we call kind of perceptual illusions or using basically perceptual engineering to think
about how do we basically use the existing hardware that we have, but maybe use some clever
tricks in terms of how we integrate information together to improve the perceived resolution.
And so I'm going to talk about both of these areas. So the first area that I'll talk about
is on creating higher resolution tactile displays. And this has been kind of a big challenge in the
field of haptics for a long time. And there's sort of this big tradeoff or dichotomy between people
that are trying to make really high bandwidth tactile displays versus people that are trying to
make kind of these shape displays or tactile displays that maybe don't need to move as fast
or maybe don't need to move at all. And how do we find this balance between the two? And so one of
my former PhD students, Ty Zane, started to think about ways in which we could really try to push
the resolution if we trade off on that bandwidth side of things. And so if we think about this
design space or list of design requirements for like the ideal or ultimate tactile display,
there's a lot of different things that we might consider, one of which is like, what is the necessary
resolution? And so we can look to the haptics and psychophysics literature for some intuition
around that. Basically, if we think about just statically touching a shape, something like that,
we need to be in that one or 1.25 millimeter range for us to be able to not really be able to feel
individual pins. So you could think about this idea of the retina display that you might be
familiar with from Apple's marketing, where if you look at an iPhone today, you can't see where one
pixel ends and the next begins. Basically, this two point discrimination threshold is kind of
the same concept. And so we want to be in that one to one point two five millimeter range. But
that's I would say a very generous, you know, basically ballpark estimate, because actually
if you start to move your finger, then you can basically feel, you know, down to, you know,
tens of microns or below. So you can feel, you know, a single hair very easily. But that's relying
much more on your, you know, basically cutaneous information from vibration and essentially texture
information. But if we just think about gross shape, this 1.25 millimeters gets us pretty close.
So as I said, people have been trying to work on this problem for a long time. And there's a lot
of different approaches in our own work. We've used like, you know, basically mechanical linear
actuators and also different types of pneumatic actuators, which have again, challenges in terms
of scaling these down, as well as, you know, basically the high cost of the actuators and
being able to steal those together. Other people have created like electromagnetic tactile displays.
But in general, those have some challenges as you scale them down, because the magnetic,
electromagnetic fields start to bleed in with each other. So there was some interesting work from
Juan Zarate and Herbert Shea in terms of thinking about electromagnetic shielding for this, but
they're still kind of on the centimeter steel type of size or other people using
electroactive polymers. And again, all of these are really thinking about this idea of, you know,
how do we make a really fast or high bandwidth tactile display. And so our, the intuition
that Kai had was, okay, let's not focus on the high bandwidth aspect. But instead, what if we had
something that's much more like e-ink, where, oh, we're not changing it very frequently, we're
going to kind of refresh the whole, the whole thing. How do we instead have maybe clutches or
brakes that we could engage or lock when we need them to, but then have one global actuator that
might change everything. And so by trading off on this high bandwidth or the temporal domain,
or frequency domain, and instead focusing on the spatial, what might we be able to do?
And so the technology that we sort of came to in terms of a good, good trade off between
this sort of high force density in terms of braking or clutching, and then also the steel
ability is electrostatic adhesion. And so many of you are probably familiar with the electrostatic
fact where, you know, basically you have a balloon, you rub it on your hair,
and it sticks to your head. And so this has long been used in different industries, for example,
in like wafer chucking in the, in the semiconductor industry, and paper handling and other things.
And then over the past 15 or so years, it's really come into vogue in the robotics community as well.
So probably many of you are familiar with it, but, and also very commonly used in the MEMS
steel devices as well. And so basically we started to think about how do we make these millimeter
steel, you know, basically high force density clutches, and what are the different techniques
that we might need to, to use to do that. And we think that this is a promising technology for
this type of, you know, refreshable display. And so basically here we can see kind of an example
of one of these displays where we have essentially a dielectric thin film that we've then patterned
with these interdigitated electrodes that are on the order of, you know, basically a millimeter
across. And then we can basically turn on an electric field that then basically induces some
charge on these brass or different types of metal pins that, and then locks them into place.
What does this actually look like in action? Okay. Yeah. So basically we can raise up,
so it's a refreshable display. So essentially we, now we unlock all the pins, we raise them up,
and then as we move it down, we lock them into place. And so the electrostatic clutches can turn
on and off very quickly, giving us very high precision in terms of linear positioning,
and are relatively high force compared to their size. So again, the basic operating principle
is that we have this, you know, interdigitated electrode that's serving as this clutch that's
on one side of a high dielectric constant thin film. And then on the other side, there's a pin,
and basically we induce opposite charge on the pin. And that basically creates this electrostatic
force. And so kind of very simple model of how this electrostatic force works is very similar
to the parallel plate capacitor equation, where essentially, you know, the things that matter
are essentially the dielectric constant of the thin film, the contact area between the metal
pin and the dielectric film and the electrodes on the other side, the voltage as well as the film
thickness. And so historically in kind of robotics and other applications, people have really pushed
on the voltage as the thing to kind of improve the the actuation force. Here we have some
challenges in doing that because one, we're dealing with people and, you know, basically some of these
very high, you know, 10 kilovolt types of range, a very small amount of current can actually be
not very good for people. So that's one issue. And then the other issue is we want to have,
you know, not just one of these actuators, we want to have tens of thousands of these actuators.
And so how do we have, you know, very small and low cost transistors that allow us to, you know,
steal this in terms of production? And so we ended up trying to push more on the thin,
thinness and also the high dielectric constant as things that we could push on as opposed to
the voltage because of those two constraints. So this is kind of, you know, basically what the
actual device looks like when we fabricate these and using the UV laser cutter that Mark's lab
or actually Alison Okamura's lab. Well, I forget where it is now, but basically between Mark and
Alison, exactly. Shared facility, which is great. Basically, that allows us to, you know, very easily
fabricate these and test out different patterns. And so we use PVDF, which is a high dielectric
constant in film and a very small sheet of it. And then we have gold that's sputtered directly
on it or some other aluminum, for example. And then we laser a blade off the parts that we don't
want. So the actuation principle is very simple in terms of, you know, we have these clutches
and then we lower down the platform lock when the pins get to the right place. We turn on the
clutch and then we create the whole pattern. And then when we want to erase it or refresh it,
we just turn them off and then move the pin up and down. And so we can get, again, very high
spatial accuracy in terms of the, you know, basically the linear positioning of the height
because we can turn them, turn on and off the clutches and about, you know, on the order of
10 milliseconds or so, which allows us to have pretty high spatial resolution. We've also done a
lot of work on quasi-static loading as well. And unfortunately, the, you know, basically other
people as well. So for example, Steve Collins lab has done a lot of work on these electrostatic
types of clutches and also found that, you know, basically the back of the envelope calculations
don't really end up matching very well with performance because of the effective contact
area. And so we've done a lot of work also on data-driven modeling for this. But basically,
we think these clutches, you know, on the order of, you know, 50 to 100 grams of force for the
areas that we're looking at, which, you know, is not that much force. But if we think about
the contact force that's necessary as you're exploring something, that's on the order of
51 grams of force. And that would likely be spread across multiple pins as well. So we think we're
in the right, right ballpark for this. And my current student, Ahad, who's there now, is trying
to work on improving the performance and thinking about other things. So we've done some user testing
as well to explore how well these tactile displays work. And it seems like it's a promising direction
and seems to be working very well. So we think this, you know, basically these electrostatic
pin displays are kind of a promising approach to really pushing the resolution and low-cost
aspects of these types of refreshable tactile displays. So that's something that we're quite
excited about. And we've been able to achieve sort of this 1.5 or 1.7 millimeter pitch as well.
One of the challenges with these types of displays, though, is that they end up creating
these very discrete types of shapes, right? And so it's possible that, you know, we might want to
have more continuous shapes that could be approximated better with some other method. And also,
maybe there's a way to do that where we trade off, you know, basically, and basically are able to have,
you know, basically more continuous shapes with fewer number of actuators. And so Ahad, who recently
had a paper that was accepted to ICRA, that's on thinking about how might we make these basically
more continuous shape displays and what are techniques we could use to sort of have sort of a
monolithic manufacturing process where we can kind of create these in one go. And so Ahad has been
working on thinking about kind of the combination of electrostatic and electroadhesive actuation
with auxetic materials to be able to create basically the shape-changing continuous displays
where we're able to vary the curvature of them by locking individual cells in an auxetic grid
or an auxetic network. And so here's an example of that shown here. And so basically, the idea is
that we have, you know, work in auxetic skins where we can basically have these different patterns that
can expand basically differently based upon, you know, how much, you know, strain there is in the
system. And what we're doing is then locking some of these cells in this auxetic pattern. And what
that does is it means that there's going to be very different local strain concentrations that end up
as you inflate this surface being able to create different geometries. And so basically, there's
been exciting work in the computer graphics field as well as in other areas on basically being able
to computationally design these auxetic patterns so that you can then create some arbitrary geometry
that's beyond sort of like what a develop surface could be. And our kind of contribution here is
to think about, oh, as opposed to being able to, you know, essentially pre-plan and create a custom
auxetic pattern that could create some given shape, could we essentially create a smart skin
where we can change that local amount that each cell can open and close in real time or at run time.
And so basically, the way that we do that is by having each auxetic cell essentially be electrostatic
break or clutch that can, you know, basically either open or close, depending on how much
voltage we're applying across them. And so what this means is that we want to have essentially
an auxetic pattern with a very large surface area, because as I said before, the electrostatic force
is basically proportional to the amount of surface area that we have. And so essentially, you know,
we've looked at different patterns, but this one here, you know, again, has a very large surface area.
And so what we end up doing is taking two of these sheets and then rotating them off phase
so that basically, essentially, there's a lot of overlap between them. And we can essentially lock
which parts will expand and which parts work. So here you get an idea of what that single cell
might look like. And so we have two. Yeah, so here you can see these, these sets expanding and
contracting. And so essentially, we can as we pull on them or inflate them, they're opening up and
closing. And we can basically turn on the electrostatic adhesion to lock them and allow them to not
open up, which means that basically there's less displacement and less strain in the system.
So we've looked at different types of combinations of layers and different materials as well.
And are looking at, you know, basically, how do we then make this into this monolithic system? So
again, there's these two sheets that are on top of each other. And then basically, we create them
out of this flexible printed circuit board. And so that's really nice, because we can just go
ahead and fabricate that using off the shelf, you know, printed circuit board techniques.
And then we have one sheet that's in this orientation, then a dielectric thin film in between,
and then the other flex PCB that's on the on the next side as well. So then that's what this sort
of 2D service here looks like. And here we're engaging and locking between them. And you can see
some of them start to fail as well. That I guess that was the unlocked one here,
this region below is locked and above is unlocked, you can sort of see. And then here's the locked
region. And at a certain point, it'll start to fail as well. And so basically, we can, again,
computationally control which of those areas we want to be locked and unlocked. And that allows
us to create these different shapes. And then we have a inflated bladder that's underneath it that
we can then basically inflate. And that will then create this global shape change. And so here's
kind of an example of this 100% locked, which sort of creates this, you know, basically a very
uniform shape. And then now we're locking less and less of the display to create different
curvatures, if you can see like that. So we think again, this is kind of a promising approach. We're
still looking for basically, exotic patterns where we can have higher, you know, amount of strain and
more basically curvature that we can create then. But we think this again has some benefits in terms
of being able to really create and manufacture this very quickly. A related project that was kind
of in the early stages is on kind of connecting this idea of these continuous shape displays
with some of the work that I showed at the beginning on these shape changing robots,
where basically we want to have these elastic grid shells. So if you think about in computer
graphics, we often have like NERBs, surfaces, which are kind of combinations of these,
you know, different splines that are connected to each other. What if we do that in the real world?
And so basically, Sophia Weitzner and Wingsum Lawn, our group, are trying to create these basically
robotic elastic grid shells that can then again change their geometry in real time.
This is a small one by one prototype, but here you can see we locked one part of it and then
we're able to inject more material into it and create this curvature there. And so this is what
we're kind of aiming to do on the left is create these, you know, basically interconnected grid
shells that we can in real time change their surface. So these are some of the things we've
been doing in our group to think about, you know, how do we push forward and make these higher
resolution, you know, surface displays. And I think we've had some great promise in looking
at electrostatic adhesion, as well as kind of new approaches to making more continuous
surface displays. But I think it's really clear to us and probably you as well that the hardware
will really never perfectly render the real world, right? The real world is so rich and very
complex. And so I think there's this big gap between that. But the interesting thing is that
our perception is also imperfect as well. And so maybe we don't need to have perfect hardware
when we're considering these types of displays. And so the last part of my talk, I want to talk
about some of the work we're doing in terms of using visual haptic illusions to improve the
perceived performance of these types of tactile displays or other types of shape displays.
So the first is work from my former PhD student, Parastu Abtaiki, who is now starting at Princeton
University next year, that's looking at this same problem of how we might use the fact that our
basically our visual perception is often dominates our haptic perception. And so
one way to illustrate this is basically our proprioceptive system is not very good. And so
if I am able to basically touch my fingers together in front of my face, I'm actually often
using my visual system to really help me with that. But if I try to do it above my head,
I can sometimes get it, but you'll find that it's not as accurate. And so again, that's because of
all the kind of errors along the line in terms of, you know, our different joints and different
mechanical receptors that we have. But our proprioceptive system has more noise and air
than our visual system. And therefore, our, you know, brains, when we're thinking about
integrating this multi sensory integration, sorry, integrating this multi sensory information,
tend to rely on our visual system. And so as I mentioned before, there's many of these limitations
of these shape displays that I talked about in terms of low spatial resolution, limited display
size or low actuation speed. And the intuition or insight that Parastu had was, oh, how do we
leverage this fact that our proprioceptive system isn't very good to sort of increase the perceived
resolution. And this really kind of essentially builds on a technique from the field of virtual
reality that's called redirected touch. Some of you might be familiar with the idea of redirected
walking, where again, we can kind of steer people in virtual reality by having some slight offset
between where you are in the real world and where you see you are, where you see yourself or where
you see your hand in the virtual scene. And so what we can do is essentially apply some small
virtual offset as I'm reaching that basically makes it seem like my hand as I move straight
is moving to the left. And my basically visual motor system will basically compensate for that
and make my hand move to the right in the real world to basically compensate for that
bias or shift to the left. And so we can basically computationally steer where a person's hand is
going by applying these offsets in the virtual world. And by leveraging that redirected touch
effect, we're able to address some of these different aspects of low spatial resolution
and low actuation speed. And so I'll talk about some of the ways in which we're doing that here
using angle redirection, scaling up and vertical redirection as well. So you might be familiar
with this idea of anti aliasing or the aliasing effect that happens when we look at
graphical display. This aliasing effect is actually also very pronounced in these tactile
displays as well. And so one way that we were trying to mitigate this low resolution is by
trying to essentially get rid of this aliasing that happens when you display a vertical or,
sorry, a diagonal line. And so if we see here, as we move along this surface here,
you know, basically we have this kind of, again, aliasing effect that you can feel these bumps
as well. And so what if instead we could redirect you so that you're moving along a straight line,
which is very doesn't have that aliasing problem. But in the virtual world, you think you're moving
along this diagonal line. And so again, by applying this slight offset between where your hand is
in the real world and where it is in the virtual scene, we can make you touch different areas of
the display and not be able to realize it or perceive it. In terms of the overall resolution
of the device, we can think about the number of pixels or taxles that you're in contact with as
you move over a surface. And so one of the challenges is if we have a small object that
we're rendering on a tactile display of a fixed resolution, then you're not going to encounter
that many taxles as you move along the surface. So again, what if we could change this essentially
control the display ratio, or utilize this offset between where my real hand is and my virtual hand
is. And in the virtual scene, render a small object, but in the real world, render a larger
object of the same version, and we can render it in higher resolution. So again, we can change the
ratio between where I'm interacting in the real world, where I'm interacting in the virtual world,
and leverage that to improve the perceived resolution. And we've done this also for things
like extending the height of the display, the workspace limitations of it, and other things
like that as well. So here, you can see as I'm moving up, I have some fixed amount of range
that I can move up. And then here, we're able to, again, offset where you think your hand is
in the virtual scene. And so you can think that basically the tactile display has
larger pins than we can actually create with these displays. We've run a lot of psychophysical
studies to find what these thresholds are, and also find that when we display an offset
underneath that threshold, essentially, people perceive it as being higher resolution without
noticing that it's there. But we've also found some interesting effects in the difference between
active versus passive touch, where in the active touch conditions, that's where you're
moving your hand as well. So for example, in that angle redirection, we can't offset people's hands
as much as we can in the passive touch condition, where for example, the motor is moving your hand,
where we can offset people more. And so again, that has to do with kind of the
you know, forward model that people have in their sensory motor system in terms of, you know,
basically, my predictions of where my hand might be. And so I'm more willing to allow
things to have noise and there to be more air when I'm being moved versus I'm moving myself
as well. So we've looked at ways in which we can leverage this to create different types of
applications such as, you know, improving the resolution or, or, you know, again, increasing
the vertical redirection as well. But as you can see from those types of systems, you know,
these illusions really only work in this small area of the display. And so another set of work
that we've been doing is trying to improve the kind of scalability of how we might apply these
to a much larger area and create essentially what are called encountered type haptic displays that
might operate over a larger region. So we put these types of haptic displays, tactile displays,
on, you know, robotic arm. And then when we reach out, the robotic device can be there in time.
But one of the challenges with this type of approach of encountered type haptic devices is
that oftentimes the device has some limitations as well, right? So the device may arrive late or
it might be out of the workspace of that robotic system. So I want to touch something up here,
but the robot is only down there. And so again, there's a number of challenges with these
encountered type haptic devices in terms of different reachability issues that lead to these
uncertain spatial discrepancies, which really kind of affect people's perceived performance
of these devices. So we've done a few things in this area to improve the essentially the ability
for these devices to work by essentially redirecting your hand to the reachable area of the robotic
system. Great. Okay, great. So anyway, basically, we can do some cool things in terms of this,
you know, redirection to guide people into different people's space. And we've also done
some interesting work on trying to apply basically model predictive control to basically run this
in real time to improve the perceived performance using a model of human reaching and sensory
integration. So I think that I'll conclude there with just one short statement, if I didn't find it,
which is that, you know, we started out this work really thinking about how real does haptics need
to be. And I think where our group is going is really trying to think more about how real
does haptics need to seem and really trying to leverage a sensory motor control perspective
to optimize both the hardware and the software together. So I'd like to thank, you know, my
PhD students and postdocs in my lab that contributed to this work, as well as our funding sources.
And I guess I'd be happy to answer any questions. Yeah, I thought it was till 130. So my apologies
on that mark. Yeah, so thanks so much.
