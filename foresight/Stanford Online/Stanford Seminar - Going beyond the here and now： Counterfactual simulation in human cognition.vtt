WEBVTT

00:00.000 --> 00:15.720
Well, thank you. Thank you so much for having me. It's a pleasure to be here. And I hope

00:15.720 --> 00:21.840
maybe some of the things that I talk about may give some inspiration for you, HCI guys.

00:21.840 --> 00:26.040
So I lead the causality and cognition lab in the psychology department. I'm interested

00:26.040 --> 00:30.240
in how people understand causality and basically how the world works and how they understand

00:30.240 --> 00:34.920
each other. And we're interested in how people learn about the causal structure of the world,

00:34.920 --> 00:38.680
how they, once they have it in their mind, how they can use it to reason about the world,

00:38.680 --> 00:42.320
make predictions, make inferences about the past, or think about maybe also how things

00:42.320 --> 00:46.120
could have played out differently from how they actually did. And how those capacities

00:46.120 --> 00:50.520
also allow us to make the kind of judgments we do in our everyday lives, like for example,

00:50.520 --> 00:55.600
assigning responsibility to one another. And that's in fact one of the bigger sort of overarching

00:55.680 --> 00:59.560
goals that my lab is working toward, namely developing a computational framework for

00:59.560 --> 01:04.000
understanding responsibility. And I think to get there, we have to be able to answer at

01:04.000 --> 01:08.040
least two questions, namely one being what causal role somebody's action played in bringing

01:08.040 --> 01:12.360
about the outcome. And the other one being what the action that the person took tells

01:12.360 --> 01:16.640
us about the kind of person that they are. For this first one, we need some intuitive

01:16.640 --> 01:20.600
theory of how the world works. So we can relate the actions that somebody took to the kind

01:20.600 --> 01:24.680
of outcomes that resulted from those actions. And for the second question, we need some

01:24.760 --> 01:28.480
intuitive theory of how people work. So we can go backwards from the actions that we've

01:28.480 --> 01:32.080
observed to the mental states that may have given rise to those actions. So what were

01:32.080 --> 01:35.360
the person's intentions, what did they believe, what were the kinds of things maybe that they

01:35.360 --> 01:41.200
were able to do as well. And so I studied psychology, like in my undergrad, and I was

01:41.200 --> 01:44.680
most excited about social psychology, because I felt sort of most applicable, I guess, to

01:44.680 --> 01:49.240
my everyday life, and somehow also got into responsibility, like back then already. Maybe

01:49.240 --> 01:52.920
it was because I was in some group project where I felt I was doing all the heavy lifting

01:53.000 --> 01:56.720
and maybe I wasn't getting all the credit for it. So that was sort of what interested

01:56.720 --> 02:02.160
me initially. And when I read around in that work in social psychology, a lot of the theories

02:02.160 --> 02:07.160
that I saw took a form sort of like this. So I'll just give you a few examples. So basically

02:07.160 --> 02:11.680
sort of like boxes and arrows theories, where they identified important concepts that were

02:11.680 --> 02:16.040
related to how we assign responsibility, and maybe also roughly how they were related to

02:16.040 --> 02:22.600
one another, but still left a lot in a certain way unspecified. So this is a quote from

02:22.960 --> 02:26.680
Bertrand Molle from a while ago. He says, like, an important limitation of many of these

02:26.680 --> 02:31.200
models of moral judgment or assigning responsibility is they don't really generate any quantitative

02:31.200 --> 02:35.360
predictions. And you might say, like, oh, what do you need quantitative predictions for? Well,

02:35.360 --> 02:39.480
one thing that they're useful for is sort of, you know, laying your cards out and making

02:39.480 --> 02:44.280
it concrete, what your model does also allows it then to be falsified more easily. And I

02:44.280 --> 02:48.440
remember this one instance, it was like me, I think maybe first day of my PhD, I went to

02:48.440 --> 02:52.200
this conference and had dinner, you know, with one of the, one of the people who had made

02:52.200 --> 02:56.720
one of these sort of boxes and arrows and diagrams. And I told them about some experiment

02:56.720 --> 03:01.320
that I thought, that I thought of and thought, like, oh, this would happen. And I think that

03:01.320 --> 03:05.840
would be the result of that experiment. And, and I was very, very smart. I thought, like,

03:05.840 --> 03:09.200
oh, this would totally kind of disprove your theory, right? And he said, no, no, that would

03:09.200 --> 03:13.600
be totally consistent with my theory. And I thought, oh, that's weird. I maybe I really

03:13.600 --> 03:18.240
tried to understand the theory very well. And, and so, so that also was a sort of little

03:18.240 --> 03:22.080
bit of a moment for me that I felt like, okay, maybe it's important to try to make these

03:22.120 --> 03:25.400
theories even more precise. So we know what it is that they're predicting, so we can go

03:25.400 --> 03:29.640
about and, you know, falsify them and sort of improve them. And so that's been very much

03:29.640 --> 03:34.560
kind of an inspiration for me, what I've been trying to do it a little bit. And so one of

03:34.560 --> 03:39.840
the starting points in almost all of these theories of responsibility is there's causality,

03:39.840 --> 03:43.600
always causality comes first. So I thought, okay, let me try, let me try that one. So

03:43.600 --> 03:49.200
can we get more specific about what it means, you know, what people, what it takes for people

03:49.240 --> 03:55.320
to say that one thing caused another thing to happen. And so I think that three key ingredients

03:55.320 --> 03:59.480
that we need, like in order to get a theory for how people think that one thing caused

03:59.480 --> 04:04.720
another thing to happen. And those are starting with a mental model that people have of a

04:04.720 --> 04:09.800
particular domain, a mental model that allows us to conceive of counterfactual interventions.

04:09.840 --> 04:13.480
So, and I'll flush it out a little bit more in a moment. So imagining how things could

04:13.480 --> 04:17.560
have been different from how they actually were. And that allows us then to mentally

04:17.560 --> 04:23.560
simulate what the consequences of this counterfactual intervention would have been. And so the idea

04:23.560 --> 04:27.520
of mental models has been around, you know, for quite some time and has recently gotten

04:27.520 --> 04:32.560
a little bit more attention again, also in AI. And, but, but yeah, some of the credit

04:32.560 --> 04:37.040
at least in modern times that go to the philosopher Kenneth Craig and his book, The Nature of

04:37.040 --> 04:40.600
Explanation, who said something along the lines, well, he said exactly that, but I'm

04:40.600 --> 04:44.760
going to say along the lines, so that we have something like a small scale model. Oh, wouldn't

04:44.760 --> 04:49.520
it be very helpful if we had something like a small scale model of the world in our minds

04:49.520 --> 04:52.920
that we can then use for all sorts of things, like predicting what was going to happen if

04:52.920 --> 04:57.160
I did this, rather than actually having to carry out the action and then, you know, dying

04:57.160 --> 05:02.400
maybe if it was a bad one. And, and yeah, that that would be really helpful for decision

05:02.400 --> 05:06.200
making. And as I will say in a moment also really helpful for explaining kind of why

05:06.200 --> 05:11.400
something happened. So this idea of mental models has been around for a very long time.

05:11.400 --> 05:15.160
And then in somewhat more recent years, at least in cognitive science, has been made

05:15.160 --> 05:18.640
a little bit more concrete, particularly as it pertains to our mental model of the physical

05:18.640 --> 05:23.400
world. And so the idea is was here to say like, well, maybe our mental model of the

05:23.400 --> 05:28.360
physical world is in certain respects, similar to the kinds of physics engines that we use

05:28.360 --> 05:32.640
to make realistic computer games. That's a common move, right? You have some, some tool

05:32.640 --> 05:36.040
and then you think like, okay, maybe the mind is a little bit like that tool. So this was

05:36.040 --> 05:39.760
just, you know, psychologists playing Angry Birds and then thinking like, okay, maybe

05:39.800 --> 05:46.560
the mind is a little bit like Angry Birds. So here, the basic idea, right, is that we

05:46.560 --> 05:50.360
take in the world, you know, through our perceptual senses, and that we then build this internal

05:50.360 --> 05:54.320
representation of the world. That's now the physics engine kind of representation. So

05:54.320 --> 05:58.280
that we pass the world, for example, into objects and the properties of those objects

05:58.280 --> 06:02.880
and then the interactions between those objects. So here, this child maybe passes the world

06:02.880 --> 06:06.720
into the ball and then the eagle on top of the tower and then the tower or the blocks

06:06.760 --> 06:11.840
that make up the tower. And now that you have this internal representation of the world,

06:11.840 --> 06:15.640
you can use it, for example, for planning. So if this child, for example, wants to topple

06:15.640 --> 06:20.040
over that tower, they can think about what's going to happen if they roll the ball like

06:20.040 --> 06:26.000
in different kinds of ways. So I can run simulations using this internal engine in my mind. So

06:26.000 --> 06:29.240
having this would be very useful because I could make predictions about the future. I

06:29.240 --> 06:32.960
could pay sort of Sherlock and infer from the current state of the world what must have

06:32.960 --> 06:38.120
happened in the past. And as I'll show in a moment, this would also be useful for explaining

06:38.120 --> 06:43.960
something that happened in the present. Okay, so what I'll do is in this in the remaining

06:43.960 --> 06:49.200
time, right? I'll basically want to cover these two different aspects of working towards

06:49.200 --> 06:53.320
this computational framework. So in part one, I'm going to focus on the physical domain.

06:53.320 --> 06:58.120
And then in part two, I'm going to expand it to just start to think about people. I should

06:58.120 --> 07:02.360
say, obviously, feel free to ask questions like anytime throughout. Otherwise, I'll try

07:02.560 --> 07:08.760
and end around 12.20 so that we have a little bit of time also for Q&A at the end. Feel

07:08.760 --> 07:13.080
free to ask and throughout if anything's unclear. Okay, so let's start with this part

07:13.080 --> 07:17.160
one. And I should also warn you, there is a little bit of audience participation required.

07:17.160 --> 07:21.960
So get ready for that. So the first, we started really simple, right? I was saying, okay,

07:21.960 --> 07:25.440
I want to understand causality a little bit better. What's the simplest possible setting

07:25.440 --> 07:29.560
maybe in which you could think about causality? Well, it's two billion balls colliding with

07:29.560 --> 07:34.160
one another. And here's the first audience participation part. So there's going to be

07:34.160 --> 07:37.720
these two balls coming in on the right side of the screen. And I'm going to ask you whether

07:37.720 --> 07:41.640
you think that ball A caused ball B to go through the gate. And if you think so, maybe

07:41.640 --> 07:46.640
just raise your arm like at the end of the video clip. So here's what's happening. Okay,

07:46.640 --> 07:53.640
so who thinks that A caused B to go through the gate in this case? Okay, a lot of people

07:54.320 --> 08:00.320
do anyone think that it didn't? No one dares? Okay, cool. So you're in line with what most

08:00.320 --> 08:06.320
people say in this case. And here's what I think was going on in your minds. Not the

08:13.760 --> 08:18.760
motor part of raising your hand, but the kind of judgment, the part of, yeah, was there

08:18.760 --> 08:22.480
causation happening in this case? And the first part is very kind of uncontroversial.

08:22.480 --> 08:25.720
So you looked at what actually happened. You saw that they collided with one another

08:25.720 --> 08:29.440
and then ball B ended up going through the gate. And now the somewhat more controversial

08:29.440 --> 08:34.320
part is to say that, well, that's in itself is not sufficient. That does not contain all

08:34.320 --> 08:37.560
the information you need in order to say that A caused ball B to go through the gate in

08:37.560 --> 08:42.360
this case. But you also need something like this. You need the capacity to simulate in

08:42.360 --> 08:47.640
your mind, in this case, that removing basically ball A from the scene, kind of in your mind.

08:47.640 --> 08:50.880
And then simulating where ball B would have gone if ball A hadn't been present in the

08:50.920 --> 08:55.440
scene. Maybe you all sort of naturally and spontaneously did that. And of course, I already

08:55.440 --> 08:59.520
talked a little bit about counterfactuals and stuff like that in the experiment. Of course,

08:59.520 --> 09:04.720
I don't do that. I just ask people to make causal judgments. So the simple idea here

09:04.720 --> 09:08.920
is then to say, when do you say that A caused me to go through the gate? It's really sort

09:08.920 --> 09:12.920
of like an epistemic notion. So your subjective degree of belief, well, to the extent that

09:12.920 --> 09:18.120
you think that what would have happened in the actual, so that what would have happened

09:18.160 --> 09:21.280
in the counterfactual situation would have been different from the thing that actually

09:21.280 --> 09:26.080
happened, that determines your extent to which you say that A caused B to go through the

09:26.080 --> 09:30.440
gate. And here you're probably pretty sure in this instance that B would have missed

09:30.440 --> 09:35.520
if A hadn't been there. So you say, yes, A caused it to go through. And just a little

09:35.520 --> 09:39.960
bit in terms of sort of background, a lot of the inspiration for this kind of work comes

09:39.960 --> 09:46.840
from Judea Pearl's work on causality. Some of you may have heard of his work. And there

09:46.880 --> 09:50.680
they use different kinds of generative models to capture people's causal knowledge of the

09:50.680 --> 09:55.040
world. So this could be something like causal base nets or structural equations that you

09:55.040 --> 09:59.960
may also remember from your stats class if you had one. And then you define some kind

09:59.960 --> 10:05.400
of operations on these models to support things like counterfactual reasoning. So imagining

10:05.400 --> 10:10.280
that some variable had been replaced with another one, for example. And so I'm doing

10:10.280 --> 10:14.120
something quite similar here, only in that I'm assuming that the generative model that

10:14.120 --> 10:18.160
people have of the world in this case is somewhat richer than what can be represented

10:18.160 --> 10:22.320
with these causal basis or structural equations. So in my case, the generative model that I

10:22.320 --> 10:25.960
assume people have in their mind is something like the physics engine that I actually use

10:25.960 --> 10:30.440
to generate them in the stimuli. And I'll make that noisy. I'll show you in a second

10:30.440 --> 10:35.520
how I'm making it noisy. And then I also have to think about, okay, what are now the counterfactual

10:35.520 --> 10:39.760
intervention operators that you might have over a representation like this one? And in

10:39.760 --> 10:43.680
this case, it could be something like imagining that an object wouldn't have been, would not

10:43.680 --> 10:48.520
have been there, for example. Okay, so you might think now, okay, well, maybe that's

10:48.520 --> 10:52.720
the only game in town. Like what else could you possibly be doing in a setting like this?

10:52.720 --> 10:56.240
And at least luckily for me, there has been a lot of philosophers and psychologists that

10:56.240 --> 11:00.520
have argued for what I called these actualist theories of causation. And they basically

11:00.520 --> 11:04.520
just say you don't need that part, right? All you need, all the information you need

11:04.520 --> 11:09.840
to give causal judgments or causal explanations for what happened is there in the actual situation

11:09.880 --> 11:14.800
in some sense. And so one of the best kind of worked out accounts of that in psychology

11:14.800 --> 11:19.400
comes from a psychologist called Philip Wolff and he calls it the force dynamics model

11:19.400 --> 11:24.080
of causation. And the idea is that all you need to pay attention to is the forces that

11:24.080 --> 11:29.600
are associated with the agent and the patient. That's the sort of lingo they use. And you

11:29.600 --> 11:33.080
then you just look, need to look at how these forces are configured. And that helps you

11:33.080 --> 11:37.680
to say what in this case here, there's different causal expressions is appropriate to use in

11:37.680 --> 11:42.480
a particular situation. And I'll just apply it to this example here. So we have the patient

11:42.480 --> 11:46.520
which is ball B that has a force that is associated with it. Then we have an agent

11:46.520 --> 11:50.520
that applies a force to the patient in this case. As a function of these two forces, we

11:50.520 --> 11:54.360
have some resulting force here. And then in this case, the patient also ended up reaching

11:54.360 --> 12:00.000
the end state. And because this configuration looks like that and that maps onto this force

12:00.000 --> 12:04.440
configuration, Philip Wolff's account would also here say, yes, a cause B to go through

12:04.440 --> 12:08.240
the gate. So this clip would not help us actually tease apart this other model that

12:08.240 --> 12:14.160
I've been kind of promoting. So just to make this distinction sort of clear or clearer.

12:14.160 --> 12:17.480
So in the force dynamics model, you start with some intuitive theory of how the world

12:17.480 --> 12:22.080
works, which in this case are these little force vectors that apply to agents and patients.

12:22.080 --> 12:25.960
And you can then directly go from there to making causal judgments. So there's this direct

12:25.960 --> 12:31.120
route from this kind of intuitive theory to causal judgment. He also says that you can

12:31.120 --> 12:35.720
do counterfactuals too by imagining, for example, if one of the forces hadn't been there, what

12:35.720 --> 12:40.200
would have happened in the situation, but that it's not necessary to figure out whether

12:40.200 --> 12:44.920
something caused something to happen. And sort of what I'm arguing for is sort of a slightly

12:44.920 --> 12:48.360
different picture. Where I'm saying, well, first of all, I start with a slightly different

12:48.360 --> 12:52.520
theory of the domain. In this case, again, using the physics engine rather than using

12:52.520 --> 12:56.880
these force vectors. But then saying that you have to go through this process of counterfactual

12:56.880 --> 13:01.880
simulation to say that something caused something to happen. And what I'm going to try and do

13:01.880 --> 13:07.080
in the next two slides is sort of motivate that account.

13:07.080 --> 13:10.760
One way to motivate at first is that I started off saying like I want to have this model

13:10.760 --> 13:15.160
of responsibility. And that means that I want to have a model of causation that not only

13:15.160 --> 13:19.320
narrowly applies to the physical world, but that can also be applied to, for example, the

13:19.320 --> 13:23.760
kind of causation that happens between people. And here's just some examples of causal statements

13:23.760 --> 13:27.960
that you could hear at the fall of Lehman Brothers, caused the financial crisis. My

13:27.960 --> 13:32.440
housemates failed to water my plants, caused them to die. Realizing that he forgot his

13:32.440 --> 13:36.280
wallet at home, caused him to go back. You probably wouldn't say that exactly in English,

13:36.280 --> 13:40.560
but they all seem to find sort of causal things, like to say. And it's probably a little bit

13:40.560 --> 13:44.960
tricky, or at least I would find it tricky, to think of how would I explain these sorts

13:44.960 --> 13:48.960
of causations with force vectors. And the hope is that the account that I'm developing

13:48.960 --> 13:53.880
is a little bit more flexible so that it can apply to these sorts of situations as well.

13:53.880 --> 13:59.680
But now, and another kind of key advantage, I think, of the model that we've been developing

13:59.680 --> 14:04.640
is that it actually allows us to derive quantitative predictions. And it's hence more easily falsifiable

14:04.640 --> 14:09.120
that some of the prior work. And so you can falsify it if you like, write a paper until

14:09.120 --> 14:15.560
we was wrong. And then I have to go back to the office and improve the account. So here's

14:15.600 --> 14:19.360
a way in which we're getting quantitative predictions out of this model. But I was saying

14:19.360 --> 14:24.400
that how you make causal judgments is by comparing what actually happened with what

14:24.400 --> 14:29.440
would have happened in the relevant counterfactual situation. But now you don't know that. The

14:29.440 --> 14:33.240
thing that I'm showing here on the right-hand side, I guess, that's in some sense the ground

14:33.240 --> 14:37.480
to truth. But you don't get to see that. You only see what actually happens. You don't

14:37.480 --> 14:41.600
get to see what would have happened if ball A hadn't been there. So you have to use your

14:41.600 --> 14:44.880
intuitive understanding, again, of this domain to simulate what would have happened in this

14:44.880 --> 14:50.160
counterfactual. And so one way for us to capture this uncertainty that you may have

14:50.160 --> 14:55.480
about exactly what would have happened if ball A hadn't been there is by generating simulations

14:55.480 --> 14:59.640
from our physics engine, but now injecting a little bit of noise into that engine. So

14:59.640 --> 15:04.120
now it becomes sort of like a probabilistic program because it's now not a deterministic

15:04.120 --> 15:09.240
outcome anymore if ball A hadn't been there. But rather what I'm doing is I'm generating

15:09.240 --> 15:14.800
a simulated sample from my model. And now in this case there's many different ways in

15:14.800 --> 15:18.960
which you could make your model kind of random or uncertain. Here what we did is we just

15:18.960 --> 15:24.600
took the actual ground truth, that ball B, velocity that ball B would have had, and applied

15:24.600 --> 15:28.880
a small perturbation to the velocity vector at each point in time. So now it's sort of

15:28.880 --> 15:33.120
like in your simulation, when you're imagining where ball B would have gone, it sort of jiggles

15:33.120 --> 15:38.640
a little bit along the way. And so this might be now one outcome, like off such a sample.

15:38.640 --> 15:41.520
So if you think like, oh, oh, I think it would have missed. But let me try again. Like,

15:41.520 --> 15:44.440
oh, yeah, I think it would have missed. Yeah, I'm pretty sure it would have missed. So this

15:44.440 --> 15:47.400
is just multiple times sampling in your mind of what would have happened if ball A hadn't

15:47.400 --> 15:50.840
been there. And here, since all of them, you're pretty sure that it would have missed, you

15:50.840 --> 15:55.160
said, yeah, A caused it to go through. But you can probably already anticipate, we can

15:55.160 --> 16:00.320
now do a slightly different case, right, where in the actual situation, again, still A collides

16:00.320 --> 16:03.800
with B and B goes in. But this time it's sort of less clear what would have happened

16:03.800 --> 16:07.600
if ball A hadn't been present in the scene. Because that ball B is headed like right to

16:07.640 --> 16:12.120
the goal post, essentially. And now if you apply the same idea of simulating with noise

16:12.120 --> 16:16.040
what would have happened, you know, in some cases, maybe ball B would have missed. But

16:16.040 --> 16:20.000
it's also possible that ball B would have gone in anyhow, even if A hadn't been there.

16:20.000 --> 16:23.560
And that accordingly, you might say like, yeah, I'm less sure, you know, that A caused

16:23.560 --> 16:28.400
ball B to go through the gate in this case. So that's what we did now in our experiment

16:28.400 --> 16:32.720
where we showed people a bunch of clips like this one. So here's just three different ones,

16:32.960 --> 16:39.120
one clip in which, you know, it's pretty clear here at the top that ball B would have missed

16:39.120 --> 16:43.120
if ball A hadn't been there. The one in the middle is like one, this kind of close call.

16:43.120 --> 16:46.400
And then the one on the right hand side is one in which it was pretty clear that ball

16:46.400 --> 16:50.880
B would have gone in anyhow, even if A hadn't been there. And then between experiments,

16:50.880 --> 16:54.160
we either asked them some counterfactual question. So that's the one here at the bottom,

16:54.160 --> 16:57.760
the blue one. Do you think that ball B would have missed if ball A hadn't been there?

16:58.400 --> 17:02.160
And then we see that in this case, they're pretty sure, yeah, I think it would have missed.

17:02.160 --> 17:06.160
Here, they're right at the midpoint of the scale, not sure, right, whether it would have

17:06.160 --> 17:11.600
missed or not. So we give them some slider where they can just evaluate their degree of belief.

17:11.600 --> 17:17.360
And then in this case, they're pretty sure that it would not have missed, even if ball A hadn't

17:17.360 --> 17:22.480
been there. And then we take a separate group of participants and we ask them a causal question.

17:22.480 --> 17:26.240
So those participants don't hear anything about counterfactuals. We just asked them in a clip

17:26.240 --> 17:30.720
like that, what do you think that ball A caused ball B to go through the gate? And we see that

17:31.120 --> 17:36.480
judgments align very closely with those of the ones in the counterfactual question condition.

17:37.600 --> 17:41.520
And we can also use that model that I described that draws these samples and tries to simulate

17:41.520 --> 17:46.400
what would have happened. And it also yields very similar judgments in this, or makes predictions

17:46.400 --> 17:51.360
in this case. These were just three of the video clips. We had like 18 different clips in that

17:51.360 --> 17:56.080
experiment. And if we just line up here on the x-axis, the average counterfactual judgments

17:56.160 --> 18:00.880
that participants made, and on the y-axis, the average causal ratings that participants gave,

18:00.880 --> 18:04.800
you see that they're very closely aligned with one another, at least suggesting a strong

18:04.800 --> 18:10.720
relationship between these kinds of judgments. But when we published this work as a coxide paper,

18:10.720 --> 18:15.280
so for the cognitive science proceedings, one of the reviewers, they were mostly happy with it,

18:15.280 --> 18:18.880
but one of the reviewers was saying, yeah, but all of the clips that you showed participants,

18:18.880 --> 18:22.880
something slightly different was going on. So maybe you just didn't try hard enough to come

18:22.880 --> 18:26.800
up with an actualistic count, like one that only looks at what actually happened. And if you

18:26.800 --> 18:34.240
tried a little bit harder, then you could have explained it away. So we did try, and we didn't

18:34.240 --> 18:38.160
succeed, but it's also sort of a weird position that you're in when you kind of don't want to

18:38.160 --> 18:43.840
succeed, right? So we thought, okay, maybe the better thing, rather than being crappy at modeling,

18:44.480 --> 18:48.080
you know, just let's come up with an experiment where it feels like if it comes up in the way that

18:48.080 --> 18:53.040
we think it will, there's no way you could possibly explain it with an actualistic count.

18:53.040 --> 18:56.960
And so that's the route we took. So just to really think like, oh, are these counterfactors

18:56.960 --> 19:01.600
really necessary for understanding causal judgments? So second round of audience participation,

19:01.600 --> 19:05.360
get ready. I'm just going to show you a slightly different clip, and this time I'm going to ask

19:05.360 --> 19:08.240
you whether you think that ball A prevented Bobby from going through the gate.

19:08.240 --> 19:20.880
Okay, what do you think? If you think that ball A prevented Bobby from going through

19:20.880 --> 19:28.960
the gate, you can raise your hand. Okay, a few people think so in this case. Okay, I'll show you

19:28.960 --> 19:38.640
another one. Okay, this was not some kind of, you know, glitch. I was having fun, you know,

19:38.640 --> 19:43.600
doing the physics engine and sort of playing portal, right, by turning these things into a

19:43.600 --> 19:46.640
Taylor port, right? And I didn't tell you anything about them, of course, when I showed you the

19:46.640 --> 19:50.800
first clip, but maybe just seeing that one clip, you already have like one shot learning, yeah,

19:50.800 --> 19:55.200
okay, maybe that's a Taylor port. And the Taylor port, it works only for ball B, you know, it

19:55.280 --> 19:59.280
doesn't work for ball A, and the yellow thing is the entry of the Taylor port, and the blue

19:59.280 --> 20:04.640
thing is the exit of the Taylor port. And now that I've shown you that, if I now show you exactly

20:04.640 --> 20:09.680
the same clip again, you're going to say, at least if you're like my participants, yes, it prevented

20:09.680 --> 20:13.760
it from going through, right? Because now what changed is basically your belief about how the

20:13.760 --> 20:17.760
world works, such that your counterfactual looks a little bit more like that now, right? What would

20:17.760 --> 20:21.680
have happened is that it would have gone through the Taylor port and into the goal, right? So the

20:21.680 --> 20:26.880
fact that I can show you exactly the same clip twice, right? And, and all I've changed was your

20:26.880 --> 20:31.040
belief about how the world works. And that makes a big difference to your causal judgment, sort of

20:31.040 --> 20:36.480
shows that it's, it cannot be sufficient to explain causal judgments just in terms of what

20:36.480 --> 20:40.480
actually happened, because actually what actually happened was exactly the same in both of the

20:40.480 --> 20:45.040
times that I showed you the clip. I don't need to do the Taylor port thing. The Taylor port thing

20:45.040 --> 20:49.040
is cute because I can show you exactly the same clip, but I can also move some obstacle in and

20:49.040 --> 20:53.600
out of the way, right here on the left hand side, you're not going to say that A prevented B from

20:53.600 --> 20:58.000
going through the gate. On the right hand side, you are because the block is out of the way, right?

20:58.000 --> 21:02.240
And a similar way for causation. And on the left hand side, you're going to say, yeah, A caused it

21:02.240 --> 21:06.240
because the block would have blocked it. And on the right hand side, you're not really going to say

21:06.240 --> 21:10.800
that it caused it because it would have gone in anyhow, right? Same idea. I'm doing exactly the

21:10.800 --> 21:14.320
same interactions between the balls. I'm just changing something kind of in the background

21:14.320 --> 21:18.400
that affects the counterfactual and, and thereby also affects people's causal judgments.

21:20.000 --> 21:26.080
Okay. So another thing that's sort of neat about this model is that it doesn't only kind of predict

21:26.080 --> 21:30.880
basically the judgment that people should give at the end of it, but also says something about

21:30.880 --> 21:35.680
the cognitive process by which they arrive at the judgment, right? In this case is maybe this

21:35.680 --> 21:39.680
process of mental simulation, that you kind of generating these samples and thinking about what

21:39.680 --> 21:45.520
would have happened, and that those drive the causal judgment. And one way we can do that,

21:45.600 --> 21:49.360
or can sort of get more direct evidence on that is to use eye tracking, right? To see, okay,

21:49.360 --> 21:52.880
where is it that you're looking at when you're asked to make causal judgments in these kinds

21:52.880 --> 21:58.880
of video clips. So we went back to the really simple ones again. And now also between experiments,

21:58.880 --> 22:03.440
just ask participants a different question about the video that, that, that they would see. And

22:03.440 --> 22:07.120
they knew at the beginning what question they would be asked. So we had one condition here that

22:07.120 --> 22:10.880
we call the outcome condition where they'd watch the video and we would just ask them at the end,

22:10.960 --> 22:15.760
in this case, if it ended up missing, did be completely miss the gate. And so I'll show you

22:15.760 --> 22:19.200
the eye movements of one of the participants in this condition. And I'm going to play the

22:19.200 --> 22:24.960
video at half speed and I'll do some sort of life narration as it unfolds. So the blue dot is the

22:24.960 --> 22:28.800
eye movement, right? So the participant here is looking back and forth between ball A and ball B.

22:29.760 --> 22:34.880
So looking, looking at ball B, sort of now trying to extrapolate where ball B will end up hitting

22:34.880 --> 22:43.600
the wall. And then mostly looking at ball B. Not very exciting, but also that's all they need to

22:43.600 --> 22:48.560
know in order to answer this question in this case. So now if you take a different participant

22:49.120 --> 22:54.320
who was asked to make a causal question, or asked to answer a causal question in the video,

22:54.320 --> 22:58.480
but otherwise saw exactly the same video clips as other participants did, you're going to see that

22:58.480 --> 23:01.920
the eye movements look quite different. And they look different in a way that made me very happy

23:01.920 --> 23:07.760
at the time. So you see they're not just looking at ball B, they're trying to anticipate where

23:07.760 --> 23:12.800
ball B would have gone, you know, if ball A wasn't present in the scene. And it's quite likely that

23:12.800 --> 23:17.680
when you guys, when I showed you this first video clip that you did that, right? And may not even

23:17.680 --> 23:22.400
been super, you know, aware to you that you did do that, like I haven't really checked, you know,

23:23.120 --> 23:26.800
yeah, how, well, at some point at the beginning when I ran this on the laptop, I would sometimes

23:26.800 --> 23:29.680
see that people would use their finger, or they would use their, you know, kind of

23:30.320 --> 23:35.520
pen or something. And that's of course pretty aware, I guess, right? But it's possible that with

23:35.520 --> 23:39.520
the eye movements, this sort of comes so natural to us that we don't even realize that we're engaging

23:39.520 --> 23:45.200
kind of in this kind of process. But yeah, I was very happy, you know, when I saw this happening.

23:46.320 --> 23:50.880
And so this is anecdotal in a sense, it's just one video clip, right? But we can also look at

23:50.880 --> 23:55.200
more generally, sort of analyzing the differences in the eye movements that people are producing

23:55.280 --> 23:58.800
between these different experiments. And what I'm showing here is just looking at

23:58.800 --> 24:02.560
the saccades that participants are producing. So those are fast eye movements jumping from

24:02.560 --> 24:08.240
one point to another. And then I look at the endpoints of those saccades. And I look at where

24:08.240 --> 24:13.600
those fall, right? And I took into account only the time between ball A and ball B coming into the

24:13.600 --> 24:21.280
scene. And before basically, when they collide with one another, that time window. And then we

24:21.280 --> 24:27.360
see that on this, for the causal question, a lot of those saccades basically end up along the path

24:27.360 --> 24:31.360
right that ball B would have taken if ball A hadn't been there. Whereas in the other condition,

24:31.360 --> 24:40.160
we see very few of these kinds of eye movements. So nice, I guess, even more direct evidence

24:40.160 --> 24:43.840
that people are engaging in this kind of process and that they're doing it specifically

24:43.840 --> 24:48.160
when asked to answer a causal question about the clip and sort of spontaneously.

24:48.400 --> 24:56.400
There is this other part to it. But I think I will skip, so I have a little bit more time to,

24:57.520 --> 25:02.400
let me see. Well, actually, I'll share it. Sorry about that. So there was another,

25:02.400 --> 25:07.280
after we published this paper, there was another reviewer number two, as there often is.

25:09.280 --> 25:14.480
And they were basically still saying, okay, well, this was for the eye tracking data. And they said,

25:14.880 --> 25:19.920
that's nice. You're showing us these sort of eye movements. But they basically said that, okay,

25:19.920 --> 25:24.320
these eye movements, they're happening before the balls are colliding with one another. And you're

25:24.320 --> 25:28.240
calling it sort of counterfactual simulation. Counterfactual should mean it should be back

25:28.240 --> 25:32.240
in the past. Going back in the past, evaluating that something would have been different,

25:32.240 --> 25:38.000
and then seeing what difference that would have made. And they were saying, oh, what,

25:38.000 --> 25:43.280
you should just call it the hypothetical simulation model instead, and not that. So we were able to

25:43.440 --> 25:51.600
push back. But the reviewer also was right to some extent, I think. So this is a paper that I've

25:51.600 --> 25:56.720
published quite recently, where I was trying to say that, no, you really need the counterfactuals.

25:56.720 --> 26:00.160
So a lot of this has been like, yeah, you really need the counterfactuals. And then you just keep

26:00.160 --> 26:04.720
getting some pushback, and you try to convince people even more so. So this was this reviewer

26:04.720 --> 26:08.800
number two here. You haven't really shown us counterfactual simulation. Those looks are happening

26:08.800 --> 26:12.800
before the balls are colliding. So his idea was, well, maybe what people are doing is they're kind

26:12.800 --> 26:17.200
of simulating some hypothetical future. In this case, the hypothetical future is like,

26:17.200 --> 26:21.760
what would happen if ball A wasn't there? And then they're storing that in their mind,

26:21.760 --> 26:25.840
and comparing that to what actually happened at the end. And that's a slightly different

26:25.840 --> 26:31.120
computation from the one that I think they're carrying out. And this relates to something,

26:31.120 --> 26:38.800
again, here's Judea Pearl, this climbing on this kind of virtual letter here. Because he has argued

26:38.800 --> 26:43.840
that there are these three different ways of thinking about the extent to which people have

26:43.840 --> 26:49.440
causal knowledge of how the world works. On the lowest rung of the letter, and he often accuses

26:49.440 --> 26:53.040
a lot of deep learning and so on to be on that rung, although it's a little unclear,

26:54.160 --> 26:57.760
he calls that rung the level of association. So that's what you learn in the stats classes

26:57.760 --> 27:03.040
correlation. When two things are associated with one another, and you can infer one variable from

27:03.120 --> 27:09.280
the presence of another, so the normal conditional probability, PY given, I would say PY given X.

27:09.280 --> 27:14.160
So what does some symptom tell me about the disease, for example? On the next level,

27:14.160 --> 27:17.840
it's the level of interventional reasoning. That's the kind of when I do a randomized control

27:17.840 --> 27:22.720
trial, for example, or if I'm, again, hypothetically reasoning, oh, what would happen if I were to do

27:22.720 --> 27:28.560
this? What would happen if I were to do that? And that's sort of when your stats teacher tells you

27:29.120 --> 27:33.040
causation and correlation aren't the same thing, that's often the thing that they then think about,

27:33.040 --> 27:36.080
right? That like, oh, on the level of an experiment, now I'm performing an intervention,

27:36.080 --> 27:40.720
randomly assigning people to different groups, and I can draw different kinds of causal inferences

27:40.720 --> 27:46.960
from that information than when I just have observations. But then process ultimately,

27:46.960 --> 27:52.800
the kind of the highest rung on the letter is reserved for counterfactual reasoning,

27:52.800 --> 27:57.360
and that allows you to give specific answers essentially to why questions. So why did this

27:57.360 --> 28:02.000
happen in this particular case? Like, you know, was it the aspirin that stopped my headache,

28:02.000 --> 28:06.240
or would it have stopped anyhow, even if it hadn't taken the aspirin? Or, you know,

28:06.960 --> 28:10.560
was Kennedy shot? Would Kennedy still have been alive if it hadn't been shot by

28:10.560 --> 28:16.080
very heavy-ass world? And so essentially, now the question boils down to, do we need that third

28:16.080 --> 28:23.200
level, like to explain people's causal judgments, or is the second one enough, right? So just to kind

28:23.200 --> 28:26.560
of try and make it a little bit more clear, right? So the hypothetical, luckily in English,

28:26.640 --> 28:30.560
also we have sort of a way of marking the difference between them. So here's an English

28:30.560 --> 28:35.680
hypothetical. Would B go into the goal if A was removed? So what you'd be doing is taking the

28:35.680 --> 28:41.920
time into account until they collide, simulating like a possible future, and then computing the

28:41.920 --> 28:47.840
probability of that. Versus the counterfactual, what I'm doing, slightly different in English,

28:47.840 --> 28:54.400
right, would B have gone into the goal if A had been removed? I sometimes, you know, regret

28:54.400 --> 28:58.080
having gotten into counterfactual so much, so obviously not a native speaker, and the

28:58.080 --> 29:01.760
counterfactuals are sometimes a little complicated, right, that you get the tenses right and so on,

29:01.760 --> 29:06.720
but I think I've mostly gotten it down by now after like 20 years. So would B have gone into

29:06.720 --> 29:10.400
the goal if ball A had been removed? So you're doing slightly different here now, right? You're

29:10.400 --> 29:15.440
taking into account everything until the end, and you're now going back in time to do this

29:15.440 --> 29:19.360
intervention and then think about how the world could have unfolded differently from how it actually

29:19.360 --> 29:25.520
did. So now it turns out in this very simple setting here, that makes no difference. The

29:25.520 --> 29:29.360
hypothetical probability and the counterfactual probability is the same because there's nothing,

29:29.360 --> 29:34.480
there's only this one causal event happening, so it doesn't really come apart. So in a very simple

29:34.480 --> 29:40.640
setting where you have one cause and one effect, essentially, you cannot tease the two apart,

29:40.640 --> 29:44.640
but you don't need to make it much more complicated. It's sufficient if you just have one other

29:44.640 --> 29:49.200
alternative event that you are initially uncertain about, and that will make it such that

29:49.280 --> 29:53.280
now the hypothetical probability and the counterfactual probability will be different from one another.

29:54.480 --> 29:59.200
So here was the genius invention, just putting like a little block again that you've seen earlier,

29:59.200 --> 30:04.960
but this time the block is on rails into the scene, and that will now make it such that we

30:04.960 --> 30:08.720
can tease these two different things apart. So here's an example. I'm not going to ask for

30:08.720 --> 30:12.240
audience petition this time, but let's say that this was happening in the clip,

30:13.760 --> 30:17.120
and now if you were asked to say, oh, did it prevent it from going into the goal?

30:17.680 --> 30:22.240
My participants say in this case, yes, it did. And the idea is, why is it? Well,

30:22.240 --> 30:27.120
because the block moved out of the way in time, such that Balbi would have gone through the goal

30:27.120 --> 30:31.520
if Ball A hadn't been there. But you may have also noticed that the movement of the block

30:31.520 --> 30:36.160
is happening after the balls collided with one another. So not something that you could have

30:36.160 --> 30:40.160
sort of anticipated at this earlier moment in time, or at least had some uncertainty about.

30:40.960 --> 30:46.000
So the basic idea here is to say like, oh, my hypothetical probability at the time

30:46.000 --> 30:49.920
would Ball B go into the goal if Ball A wasn't there? Well, that's unsure. That depends on

30:49.920 --> 30:53.920
whether or not the block's going to move. So I should give it like a 0.5 or something. I told

30:53.920 --> 30:58.400
participants it's just as likely to move as it's not. Whereas for the counterfactual probability,

30:59.120 --> 31:02.880
well, I know that it moved in this case. So I should be pretty certain that it would have

31:02.880 --> 31:07.280
gone in if Ball A had been removed. So now I have a way basically of teasing the two apart

31:07.280 --> 31:11.920
and can see which one better explains the causal judgments. Is it the hypothetical judgments

31:11.920 --> 31:15.680
that I ask participants to do, or is it the counterfactual judgments that I ask another group

31:15.680 --> 31:20.800
to do? And then I ask one group to give causal judgments and then just try to relate them to

31:20.800 --> 31:28.000
one another. And what I find is when I look at the hypothetical, so maybe I should say a little

31:28.000 --> 31:32.400
bit more about that plot here, at the bottom, it basically shows you the initial configuration

31:32.400 --> 31:38.320
of the block. Was it in the way or not? And then did it move yes or no? So in this example here,

31:38.320 --> 31:42.400
it's one where it was initially in the way, but it moved. But in the hypothetical condition,

31:42.880 --> 31:46.800
you don't know that because you only see it until they pause. And then if you look at the

31:46.800 --> 31:50.160
hypothetical judgments, they think when it's initially in the way, they think it's a little

31:50.160 --> 31:53.440
less likely that it's going to go in. And when it's initially out of the way, they think it's a

31:53.440 --> 31:57.840
little bit more likely. So they're sort of a little bit sticky in terms of what actually happened.

31:59.040 --> 32:03.520
For the counterfactual probabilities, pretty much only the final state is what matters.

32:03.520 --> 32:07.920
If it was out of the way at the end, you think, yeah, it would have gone in. If it was in the way

32:07.920 --> 32:12.640
at the end, you think it would have missed. And now if you ask people to make causal judgments

32:12.640 --> 32:16.960
in this case, we see that they align very closely with the counterfactual ones and not with the

32:16.960 --> 32:22.080
hypothetical ones. And this was for the kind of missed cases, but the same story again holds

32:22.080 --> 32:27.360
essentially for the causal cases too. So they think that it caused it when the block would have been

32:27.360 --> 32:31.200
in the way at the end, and they don't think that it caused it when the block was would have been

32:31.200 --> 32:36.000
out of the way at the end. So enough to make this review too happy, but maybe not Michael.

32:38.160 --> 32:43.680
I'm a happy guy. I'm curious. Can you go back one slide? Just to make sure I understand.

32:45.360 --> 32:50.240
There were two things that changed in that intervention. There was the question you asked,

32:50.240 --> 32:55.600
the hypothetical versus the counterfactual. And it also sounds like the changes in how far they

32:55.600 --> 33:01.680
saw into the video. That's right. That's right. And I'm picturing the counterfactual situation

33:01.680 --> 33:06.640
where if you ask me the hypothetical question, but showed me the full video, so I see a whole video

33:06.720 --> 33:12.080
and then you say, would be going into the goal if A was removed? I don't know.

33:12.960 --> 33:16.720
Yeah. Yeah. It's a tricky one. I mean, I guess, you know, you'd have to ask them, like, what did

33:16.720 --> 33:21.280
you think? I guess sort of at the time, right? Like before it happened, did you think, and people

33:21.280 --> 33:24.800
are often bad at that, right? We know that from all the hindsight research and so on, that they

33:24.800 --> 33:29.040
have difficulty putting themselves back into the epistemic state, I guess that they had at an

33:29.040 --> 33:33.680
earlier point in time, right? So I'm not exactly, I haven't tried that one. I haven't tried showing

33:33.920 --> 33:38.000
it until the end, but then asking them the hypothetical question, it's possible, of course,

33:38.000 --> 33:44.880
that they will confuse it like as a counterfactual question, right? And, but for me, it was still

33:44.880 --> 33:48.960
sufficient, I guess, at least to address this reviewer's concern, because his idea was really,

33:48.960 --> 33:53.200
yeah, that computation is happening earlier, right? It's happening before the causal event of

33:53.200 --> 33:57.040
interest, and then you're storing the output of that computation, in this case, the hypothetical

33:57.040 --> 34:02.000
probability, and then just comparing that to what actually happens at the end, right? So it still

34:02.000 --> 34:11.920
felt that it's addressing that, but yeah. Okay, so having these two things helps teasing them apart.

34:12.720 --> 34:16.160
Okay, I'll sum up the first part, and then the second part will be short, but that's okay.

34:17.200 --> 34:20.800
So for this counterfactual simulation model, what I've showed you that there's this sort of nice

34:20.800 --> 34:25.920
correspondence between people's beliefs about the relevant counterfactual and the causal judgments

34:25.920 --> 34:29.920
that they make, that it looks like that these things are necessary, which you can show with the

34:29.920 --> 34:35.040
teleport or with the, with the brick in and out of the way, that people spontaneously engage in

34:35.040 --> 34:39.680
this kind of counterfactual simulation as evidence to the eye movements, and that it's

34:39.680 --> 34:44.240
counterfactuals really and not hypotheticals that seem to be important for expanding causal judgments.

34:45.440 --> 34:49.120
We've played around with this model like a little bit more. Once you have a hammer, right,

34:49.120 --> 34:53.360
you find all the nails. So this one is just like looking at slightly more complex cases.

34:53.360 --> 34:57.120
This one here, philosophers love, because it's a case of, let me show it again, maybe a case of

34:57.120 --> 35:03.040
double prevention, where B prevents ball A from preventing ball E from going through the gate,

35:03.040 --> 35:07.920
right, because knocks it out. It happens in, maybe in football, probably happens often when

35:07.920 --> 35:11.680
one tackles like another person that would have tackled the person running with the ball, right.

35:11.680 --> 35:15.280
And so you might say, oh, to what extent did that cause it? You can also look at omissions when

35:15.280 --> 35:20.240
nothing is happening. So ball A is just chilling here in the corner, and you might still ask,

35:20.240 --> 35:23.840
oh, did it go in because ball A didn't hit it, right? And now you could imagine, well, if it had hit

35:23.920 --> 35:27.600
it, what would have happened in this case? And we can also look at cases where really

35:27.600 --> 35:32.080
nothing is happening at all. So here's just a tower of blocks, right, and you might still wonder,

35:32.080 --> 35:35.600
oh, to what extent is this black one here responsible for the other one staying on the table?

35:36.160 --> 35:39.280
And even though, yeah, there's nothing happening, right, you might still say, well,

35:39.280 --> 35:43.280
how do you answer this question? Maybe by doing something like playing Jenga in your mind, right,

35:43.280 --> 35:47.440
imagining it being removed, and then what would have happened to the scene? So that even just

35:47.440 --> 35:52.800
physical support in some sense is very closely related to ideas of causation, right. What it means

35:52.800 --> 35:59.440
to support is essentially to prevent something from falling. Okay, so that was part one. Now a

35:59.440 --> 36:05.120
sort of short version of part two. And so responsibility attribution was something that I've

36:05.120 --> 36:10.320
been into for quite a while and was also my motivating thing. And then I drifted off into

36:10.320 --> 36:15.040
causality world mostly just because physics engines were around at the time. So it was like, oh,

36:15.040 --> 36:19.680
now I can use those. And with around at the time, I mean, I was a postdoc with Josh Tenenbaum back

36:19.680 --> 36:24.240
then and physics engines were all the rage at the time. And I said, okay, now I'll also use them.

36:24.240 --> 36:29.360
And there aren't really yet, although I guess Michael is working on it, psychology engines,

36:29.360 --> 36:32.480
right, that is easy where you could just have agents and think about what they would have done.

36:33.760 --> 36:38.320
So this work that I had done on responsibility attribution wasn't particularly social, also

36:38.320 --> 36:41.840
didn't really involve simulation, I think. And there was one experiment that got a little bit

36:41.840 --> 36:47.200
closer that I'll briefly share with you here on a paper called Moral Dynamics. And it will look

36:47.200 --> 36:51.760
very billiard ball world like I haven't moved too far away from the billiard balls, but this kind

36:51.760 --> 36:55.360
of that's somewhat agentive, right? So and so we could show people like a video clip like this

36:55.360 --> 36:59.840
and then ask them, what about extent do you think that blue was responsible that the green one got

36:59.840 --> 37:05.360
harmed in this case here? And our inspiration here came from a paper called Moral Kinematics where

37:05.360 --> 37:09.920
they basically argued, again, it's somewhat more actualist view and saying, okay, there's certain

37:09.920 --> 37:13.840
features that people are picking up on in these scenes, like the duration of contact,

37:13.840 --> 37:18.160
how far things moved and things like that. And then they directly mapped from these features

37:18.160 --> 37:24.640
of the scene to the moral judgment in this case. And we liked the general setup, but didn't really

37:24.640 --> 37:30.480
like that model like as much. So we proposed another model that has a slightly different title,

37:30.480 --> 37:34.960
Moral Dynamics instead. And we thought, okay, these features are important, but the features are

37:34.960 --> 37:40.640
important in that they give us evidence for the latent variables and that those are ultimately

37:40.640 --> 37:44.880
the ones that I care about. And in this case, what are the latent ones that we thought one,

37:44.880 --> 37:48.880
not very surprisingly here on the right hand side causality, but did you think that it actually

37:48.880 --> 37:53.840
caused it, you know, to for this negative outcome to happen. And then the left side,

37:53.840 --> 37:57.600
the intuitive psychology part, very kind of minimal in this case here. But it's basically

37:57.600 --> 38:02.160
saying like, well, maybe these features give you some evidence about like how much the agent

38:02.160 --> 38:06.800
actually wanted to bring about this negative outcome. So if you think, for example, if somebody

38:06.800 --> 38:10.480
really wants something to happen, then they're willing to incur a larger cost to make it happen.

38:11.600 --> 38:15.440
Putting a lot of effort, for example. So if somebody puts in a lot of effort into something,

38:15.440 --> 38:19.920
you know that they must have really valued it. And if somebody really valued some negative outcome,

38:19.920 --> 38:24.880
well, that's a bad thing. That was roughly the idea here. And we could then show that if we have

38:24.880 --> 38:30.400
a model that just basically infers the amount of effort that some agent exerted and tried to map

38:30.400 --> 38:35.840
that onto the responsibility that worked kind of, you know, okay-ish. If we only took into account

38:35.840 --> 38:40.160
the causal role that some agent played and tried to use that to explain the extent to which

38:40.160 --> 38:44.960
they're held responsible, that worked okay-ish. But if we now took a model that takes both of

38:44.960 --> 38:49.200
these components into account, that worked pretty well, which was roughly in line with this kind

38:49.200 --> 38:54.400
of unsurprisingly, now this framework that I laid out at the beginning, right, that when we assign

38:54.400 --> 38:58.640
responsibility to others, we don't just care about the causal role that they played, but also what

38:58.640 --> 39:02.720
the action tells me about the kind of person that they are. In this case, the action tells me

39:02.720 --> 39:05.840
something about the desire that they had to bring about this negative outcome.

39:06.480 --> 39:13.040
Okay. But still, we didn't really have a real model of agents in this case. We still sort of

39:13.040 --> 39:18.240
basically just use the physics engine. Also, we weren't able to talk about intentions, and it's

39:18.240 --> 39:23.120
clearly important often when people talk about responsibility. And also still our kind of factual

39:23.120 --> 39:26.880
simulation here was basically purely physical, just seeing how this thing would have moved

39:26.880 --> 39:34.880
without the other one. So I don't have the skills to make it happen with sort of more

39:34.880 --> 39:39.120
agentive agents, but luckily now that I'm here, I get to work out with all these smart people,

39:39.840 --> 39:43.760
and here's my PhD student, Sarah Wu, and our research assistant, Shruti Sreeta,

39:43.760 --> 39:48.000
and they've looked into cases now that are a little bit more agentive. They're still kind of in

39:48.000 --> 39:53.760
in grid world, but at least now planning and intentions and things like that are involved.

39:54.480 --> 39:59.040
And here's the basic setup. So this is inspired by some previous work that has looked into helping

39:59.040 --> 40:05.840
and hindering as a case study. And what they did is essentially they said, well, what it means

40:05.840 --> 40:11.520
for somebody to intend to help someone is that their utility function includes the other person's

40:11.520 --> 40:17.440
utility with a positive sign. Intending to help just means wanting to bring positive utility,

40:17.440 --> 40:21.840
at least in this framework, to the other person. And intending to hinder puts a negative sign,

40:21.840 --> 40:27.760
like now I want it that the other person is a low utility. So it turns out though that

40:27.760 --> 40:32.640
intending to help or hinder versus actually helping or hindering is not necessarily the same

40:32.640 --> 40:38.400
thing. So here's an example. I don't have a child yet, but at some point maybe we'll have a child,

40:38.400 --> 40:42.000
and then if I go grocery shopping with the child, there probably will be a period of time where

40:42.000 --> 40:49.280
they're not actually helping. They're sort of like trying to help, but kind of making it worse,

40:49.280 --> 40:52.880
at least in terms of efficiency and so on. It's going to take longer. Of course, it's useful

40:52.880 --> 40:57.200
because eventually they will be helpful. I have to go through that process just like a PhD student.

40:58.320 --> 41:12.320
So yeah, so you go through that process, and then you might intend to be helpful,

41:12.320 --> 41:18.320
but it might take a little bit of time to actually be helpful. And the claim is to evaluate that,

41:18.320 --> 41:22.000
you need counterfactuals again to tell, oh, is the person actually helpful? Well,

41:22.000 --> 41:26.080
how would it have happened without them, essentially? Or there's different counterfactuals

41:26.160 --> 41:30.080
to consider, but that's one of them. So here's our grid world that we played with,

41:30.080 --> 41:35.040
with the helping and hindering setup. So we have this red guy here who wants to get to the star,

41:35.040 --> 41:39.840
has a pure physical goal in this case, just to get to that location. Then we have this blue one

41:39.840 --> 41:44.400
who has a pure social goal. They either want to help or hinder the red one from getting there.

41:44.400 --> 41:48.560
And then there are these walls here that you can't do anything about, but there's also these blocks,

41:48.560 --> 41:53.920
and only the blue one can interact with these blocks. They can push, pull them out of the way.

41:53.920 --> 41:57.680
So here's our Hollywood clip of what's happening in this situation.

42:02.160 --> 42:08.160
Okay, so in this case, happy end, like a Hollywood movie, red made it,

42:08.160 --> 42:11.200
and then we can show people these clips and we can ask them, oh, how responsible was the

42:11.200 --> 42:15.520
blue player for the red player's success, for example, in this trial? We can also ask them

42:15.520 --> 42:19.600
a counterfactual question, right, would the red player still have succeeded even if the blue player

42:19.600 --> 42:25.040
hadn't been there? And we can ask them to make an inference about the intention of the blue one

42:25.040 --> 42:28.800
in this case. What was the blue player intending to do? Were they trying to help or were they

42:28.800 --> 42:34.480
trying to hinder? Definitely help, definitely hinder. So the idea is now basically the same as earlier,

42:34.480 --> 42:38.480
by just saying, okay, again, we need some kind of generative model of the domain. In this domain,

42:38.480 --> 42:43.840
now it's a model of agents basically planning and recursively reasoning about one another, right?

42:44.880 --> 42:49.120
And that's now our probabilistic program. And we can again compute counterfactuals over that,

42:49.120 --> 42:52.960
maybe in this case thinking, well, what would have happened if the blue one hadn't been there?

42:52.960 --> 42:57.360
And then thinking how the red one would have planned their path differently, but without the

42:57.360 --> 43:02.880
presence of blue, that's a rough idea. So again, we take some actual situation and we can then

43:02.880 --> 43:06.800
simulate what would have happened in the relevant counterfactual situation in this case where blue

43:06.800 --> 43:10.960
hadn't been there. We can talk later if you like about other counterfactuals you might consider,

43:10.960 --> 43:14.960
but we just went with this one here, but what if they hadn't been there? In this case, yeah,

43:14.960 --> 43:19.360
they wouldn't have made it because the block was in the way, right? We also have a model of

43:19.360 --> 43:23.120
intention inference, but I'll sort of skip that. It's basically just saying, okay, if you have a

43:23.120 --> 43:27.760
generative model about what an helping or hindering agent would do, you can then condition on the

43:27.760 --> 43:32.880
observations that you see them acting and see what's more likely that they were helping or

43:32.880 --> 43:37.360
hindering given the actions that they carried out. So I'll just give you a few more examples of the

43:37.360 --> 43:42.160
sort of video clips that we showed to participants. That's a diagram of the one that you've just seen.

43:42.160 --> 43:49.520
Here's another one where kind of, you know, blue is sort of extra mean, you might say. There was

43:49.520 --> 43:54.880
already a block in the way, but they put another block in the way. What the heck? Yeah, really trying

43:54.880 --> 44:03.840
to be helpful through adversarial actions. So here's another one here where blue is sort of

44:03.840 --> 44:08.800
laudably helpful, but like, you know, was not really necessary, but maybe looks nice.

44:09.760 --> 44:12.480
Here's a case in which sort of things go wrong.

44:15.840 --> 44:19.520
Where blue was maybe trying to be helpful, but actually sort of made it worse, you know,

44:19.520 --> 44:24.320
the reactions that they took. And then here's another one. We had a large number, so I'm just

44:24.320 --> 44:29.760
showing like a subset of them. So this is one where blue could have easily hindered if they had

44:29.760 --> 44:36.400
wanted to, but didn't, because they could have just pushed it into the way. And so then we now

44:36.480 --> 44:41.520
have to again, yeah, try to capture whether we can, with our model, capture the counterfactual

44:41.520 --> 44:45.440
judgments that people are making. And we sort of can, there's not as much kind of variance here,

44:46.000 --> 44:50.080
at least in the predictions of the model. So this model is sort of okay-ish. It captures the trends

44:50.080 --> 44:53.600
overall, but there's more variance in people's judgments that is not quite captured by the

44:53.600 --> 44:59.520
model yet. So we're still, this is sort of more ongoing work. In terms of intention inference,

44:59.520 --> 45:03.760
it's fine. So it can also kind of infer whether the person was helping or hindering,

45:03.760 --> 45:07.440
but also here, what you see is stuff are bunched up that the model all gives a hundred to,

45:08.480 --> 45:13.280
where there's still some differentiation that people make, but sort of mostly captures what's

45:13.280 --> 45:18.400
going on. And if we now look at the responsibility judgments, and we try to do the same thing

45:18.400 --> 45:22.240
initially that we did with the billiard balls earlier, that we just take the counterfactuals,

45:22.240 --> 45:29.040
like on the x-axis, and try to predict the responsibility here on the y-axis, it's okay-ish,

45:29.040 --> 45:32.560
but not, you always want, when you do computational modeling, you always want them

45:32.560 --> 45:37.200
nicely line up on the diagonal. And that's not really what was happening in this case,

45:37.200 --> 45:40.800
whereas for the billiard balls, we have this very simple counterfactuals nicely predict

45:41.440 --> 45:47.040
the causal ratings. But if again, if you have a model that incorporates also the intention

45:47.040 --> 45:53.040
inferences, like into the predictions, now they do sort of more nicely line up on the diagonal.

45:53.680 --> 45:57.440
Again, suggesting that when it comes to assigning responsibility for agents,

45:57.440 --> 46:01.840
it's not just the causal role that matters. It also matters what the actions that they took

46:01.840 --> 46:05.440
tell me about the kind of person that they are. In this case, it tells me something about

46:05.440 --> 46:09.760
their intentions, like they try to be helpful, or that they try to be hindering. So the both of

46:09.760 --> 46:15.040
these components. And just to give you a sense of an example where we need this kind of intention

46:15.040 --> 46:19.360
part, like that's back to that mean one where the blue one pushes another one into the way, right?

46:19.920 --> 46:24.240
And so just to help you kind of interpret the bars here, the counterfactual, that's the condition

46:24.240 --> 46:28.400
where we asked them, would red have succeeded if blue hadn't been there? That's basically our

46:28.400 --> 46:35.440
causal model. And they don't think so, right? The pink, pink, purplish one is like very low,

46:35.440 --> 46:39.600
right? But also when we asked them what the intention of the blue one is, they think, yeah,

46:39.600 --> 46:43.440
was really hindering. So here zero means hindering and 100 means helping. So they think, yeah,

46:43.440 --> 46:48.400
they were hindering. So even though they say that, yeah, the blue one didn't really play a causal

46:48.400 --> 46:51.680
role, they still give them quite a bit of responsibility, like in the blue one on the

46:51.680 --> 46:56.720
right hand side. So that's one case, at least, where currently we need this other part. So they

46:56.720 --> 46:59.520
think, yeah, blue blue's actually make no difference, but they were definitely trying to

46:59.520 --> 47:03.120
hinder. And so, yeah, I still give them some responsibility for this outcome.

47:04.480 --> 47:10.640
Okay, so sort of almost last slide. Because we have these agents like recursively thinking about

47:10.640 --> 47:15.120
one another, an interesting setting that also can happen here is that you can actually hinder or

47:15.120 --> 47:20.400
help one another, again, maybe also like in the, in the advisor, advisor setting, not by actually

47:20.400 --> 47:24.880
making any change to the physical world, but changing somebody else's belief. So I just want to

47:25.360 --> 47:28.880
show you that example. And maybe you'll get that intuition from the setting here.

47:34.960 --> 47:40.160
So very, very mean, very, very sad. Because it looked really like blue was going to help,

47:40.160 --> 47:47.520
right? And then they didn't, right? And here's just one participant, what they're saying,

47:47.520 --> 47:50.960
oh, blue tricked red into thinking she was going to move the box to help. But then

47:50.960 --> 47:56.080
once red was stuck on the side of the wall, blue left the box where it was, very sad, you know.

47:56.080 --> 47:59.840
And a lot of people say something along those lines. We also had one condition where we just

47:59.840 --> 48:03.600
have them give explanations of what happened, right? And here the interesting part, right,

48:03.600 --> 48:07.520
is that the hindering is not happening because blue changed anything about the world. They didn't

48:07.520 --> 48:11.280
move a block in the way or something, but they hindered because they made red believe that

48:11.280 --> 48:15.120
they were going to be helpful and then they weren't, right? Here, if blue hadn't been there,

48:15.120 --> 48:18.960
red would have just walked along on the outside and they might have made it, you know, anyhow,

48:19.040 --> 48:23.360
even without blue. And this happens because they're recursively thinking about one another,

48:23.360 --> 48:26.320
right? And red things like, oh, blue is taking actions that are going to help me

48:26.320 --> 48:32.400
so I can take the shortcut. And then it turns out I couldn't in this case. Okay, wrapping up.

48:33.200 --> 48:37.120
So this was the second part where we, I guess, applied this model now to at least a simple

48:37.120 --> 48:41.360
setting where agents are interacting with one another, helping and hindering one another,

48:42.320 --> 48:46.320
that in order to judge whether somebody helped or hindered, I again think that you need this

48:46.320 --> 48:52.480
process of counterfactual simulation and that responsibility judgments are sensitive both

48:52.480 --> 48:56.640
to the cause of the world that somebody played and what the actions tell us about the kind of

48:56.640 --> 49:02.160
mental state that they had. Just to conclude, so together, hopefully, this sort of set of studies

49:02.160 --> 49:06.400
gets some evidence that people seem to be constructing these rich mental models of the

49:06.400 --> 49:10.080
world that we can get evidence for in different kinds of ways, like through eye tracking and other

49:10.080 --> 49:16.400
tools. By imagining interventions like on these mental models, those allow us to compute the

49:16.400 --> 49:20.960
counterfactuals, which I think are important for assigning responsibility, giving explanations

49:20.960 --> 49:26.400
and so on. And that this counterfactual simulation model that I've been kind of developing can then

49:26.400 --> 49:30.640
be relatively flexibly applied to physical and social events, where you think that the main

49:30.640 --> 49:34.400
thing that's happening is that your model of the world changes and maybe the exact

49:34.400 --> 49:38.400
counterfactual cooperation that you're carrying out changes, but otherwise the framework sort of

49:38.400 --> 49:43.520
holds. So with that, I want to thank the main people who helped me do this kind of work,

49:43.520 --> 49:47.680
and then maybe you for your attention. And there's a little bit of time for questions.

49:59.520 --> 50:04.960
So one thing I'm curious is, I assume notions of causality are probably somewhat universal,

50:05.040 --> 50:09.760
but especially issues of moral judgment, intention are likely dependent to some extent on

50:09.760 --> 50:13.680
environmental factors, cultural factors, those kinds of things. And so I'm curious if you've

50:13.680 --> 50:17.040
either observed those in your experiments or if you have some way of controlling for those

50:17.040 --> 50:23.440
factors when you recruit participants. Yeah, so that's an interesting question. And I think

50:23.440 --> 50:30.400
even notions of causality actually, there are cultural effects like who you see there. So

50:31.280 --> 50:36.240
when making causal judgments, there's often, there's basically like in many cases,

50:37.040 --> 50:40.880
what's called the problem of causal selection, how do I even decide what thing to pick out of

50:40.880 --> 50:45.680
as the cause in the first place. In my setting, very often I've kind of made it pretty easy,

50:45.680 --> 50:50.480
and I've sort of constrained it because I already told you like these are the possible causes,

50:51.520 --> 50:55.440
but in the real world it's not like that. And it's sometimes, we may see something,

50:55.440 --> 50:59.680
we may see a person as a cause, or we may see a system as a cause, or we may also

50:59.680 --> 51:04.400
see the kind of counterfactuals that may come to mind to us may also depend on what our background

51:04.400 --> 51:09.680
is. And it often tells us something about, oh, when somebody then gives a certain counterfactual,

51:09.680 --> 51:15.040
it tells us quite a bit about them. So this comes up in the context, for example, also of victim

51:15.040 --> 51:19.840
blaming. Like if that's the counterfactual that came to mind to you, oh, that tells me something

51:19.840 --> 51:26.240
like about you. So I would say that even in that context, there are strong kind of interpersonal

51:26.320 --> 51:32.480
and cultural effects that affect how we attribute causality. Now when it comes to intention inferences,

51:32.480 --> 51:36.800
I'm not sure that that process in and of itself, that at least to me feels relatively

51:38.800 --> 51:43.280
whatever universal, that we kind of, we have to engage in that all the time by trying to

51:43.280 --> 51:48.880
predict what other people are intending in the way that we interact with them. But then again,

51:48.880 --> 51:55.040
how maybe then judgments in this case of responsibility or morality like draw on these

51:55.040 --> 52:00.080
different components, for example, that I've laid out here, no claim that that is in any sense

52:00.080 --> 52:06.640
sort of universal. But it might very well be that in certain cultures like this kind of what I take

52:06.640 --> 52:11.280
here to be more the person component, right, may have a stronger influence on responsibility

52:11.280 --> 52:16.560
judgments and in others, it might mostly be about causality. I certainly in my experiments

52:17.840 --> 52:21.520
for individual participants see a lot of variance along the lines. But there's some people that

52:21.520 --> 52:25.600
don't care about even the intention part at all. They just say like, oh, when it's about responsible,

52:25.600 --> 52:30.400
I just look at what would have happened if they hadn't been there. And then other participants,

52:30.400 --> 52:37.120
judgments are suggesting that they care about the intention part much more. But I have not yet

52:37.760 --> 52:41.840
engaged in the kind of work that then tries to explain why is it, why is it that this person

52:41.840 --> 52:46.480
casts so much about causality and why is it that this person casts so much about the intention

52:46.480 --> 52:55.840
part, for example. Thank you. I'm going to hug the mic actually. I'm interested, like, do you think

52:55.840 --> 53:01.920
this model applies to other settings? Because all of the examples were sort of like physical or

53:01.920 --> 53:07.360
agents taking physical actions. So if you had just like a verbal description of some social

53:07.360 --> 53:11.840
scenario where there's like speech acts that are causing things, do you think it would work the same?

53:12.320 --> 53:18.080
Yeah. Yeah, that's a great question. So would it work the same? So my sense is like, yeah, in a

53:18.080 --> 53:28.080
similar way, so there's a number of things here, I think. So we have applied the model a little bit,

53:28.080 --> 53:32.880
like this kind of counterfactual simulation model in the physical world, also two speech acts.

53:32.880 --> 53:37.280
And there it's in the context of like, we were basically jealous of, you know,

53:38.240 --> 53:41.840
for those of you who remember full wolf, you had these different words, right? And we were like,

53:41.840 --> 53:46.080
oh, our model can only do like cause and prevent. That's kind of sad. But there's other causal

53:46.080 --> 53:52.720
expressions, of course, right? Enabling, affecting, letting, allowing, and so on. And it's going to be

53:52.720 --> 53:57.120
a little bit of a of doing, but I'll get there. So we were trying to see to what extent this

53:57.120 --> 54:01.200
framework that we have could also allow us to explain differences between these different

54:02.000 --> 54:07.680
expressions, right? And, and this also comes up, you know, in philosophy, like even questions,

54:07.680 --> 54:12.080
so the question versus killing versus causing to die, even people like in cases of abortion,

54:12.080 --> 54:15.920
you know, the way that you talk about it, right? Again, reveals something, you know,

54:15.920 --> 54:20.720
how you think about it. And in general, right, like this distinction also, when you have that as

54:20.720 --> 54:25.280
an alternative that you could have said killing, but you chose causing to die, it suggests maybe

54:25.280 --> 54:28.240
a more roundabout way in which something happened, right? Like the person killed it,

54:28.240 --> 54:32.880
caused them to die. You think, yeah, it would be weird to say that someone caused them to die

54:32.880 --> 54:36.720
when they like, you know, directly walked up to them and, you know, shot them. This also came

54:36.720 --> 54:42.000
up recently or still coming up these days, actually, with the case of Alec Baldwin, Rust,

54:42.000 --> 54:46.080
like in the movie, right? The way that people talk about it was it will hold the gun that

54:46.080 --> 54:50.560
discharged or something rather than, you know, shot the person, right? So it matters a lot,

54:50.560 --> 54:55.200
basically, like in this case, the choice of word, right? And the, and the, in some sense,

54:55.200 --> 54:59.520
the counterfactual alternatives you could have had, right, for them, the image that it's creating

54:59.520 --> 55:04.160
in the listener in this case, right? So the fact that, oh, you chose this expression suggests to me

55:04.160 --> 55:08.880
that the scenario must have been such, like rather than such. So that's at least the minimal way,

55:08.880 --> 55:13.040
I think, in which it applies also to, to thinking about speech acts, right? And thinking like,

55:13.040 --> 55:17.200
yeah. And of course, you could think like, oh, you know, again, take the advice example, would the

55:17.200 --> 55:22.160
students still have done that if I had not said that, right? So we are obviously causing each other

55:22.240 --> 55:26.960
a lot in the way that we talk to each other. And sometimes, you know, yeah. Also, of course,

55:26.960 --> 55:31.920
after talking, I might think like, oh, I wish I had, I had answered this question from differently

55:32.480 --> 55:35.680
than what I actually did. And I regret it, right? And things like that.

55:38.480 --> 55:43.360
Yeah. So on a similar note, I'm wondering if you have thoughts on how possible it would be to

55:44.240 --> 55:49.680
use this model on society, large scale societal events, their divisive, such as what cost a

55:49.680 --> 55:56.640
person to be elected, what cost code outbreaks, or what causes climate change, like how possible

55:56.640 --> 56:01.360
would it be to apply this to those events and also what challenges you foresee? Yeah. Yeah,

56:01.360 --> 56:06.800
that's a great question. And, and so, so I had the example of example at the beginning with like,

56:06.800 --> 56:10.080
oh, did the fall of Lehman Brothers caused the financial crisis, right? That's sort of like,

56:10.080 --> 56:14.560
large scale. And I don't know, right? And, and, and partly it might, so, and there's a few options,

56:14.560 --> 56:19.040
right? One is like, okay, just like totally punting, right? And saying like, okay, well, if the system

56:19.040 --> 56:23.600
gets sufficiently complex, such that I cannot carry out the relevant counterfactual computation

56:23.600 --> 56:28.800
anymore, well, I just don't know, right? I cannot give that causal answer. That's, that's one version,

56:28.800 --> 56:33.360
right? And there's another version where you say like, okay, well, to the extent that I can maybe,

56:33.360 --> 56:38.880
you know, abstract away from a lot of the lower level details that say of some, so if I'm, if I'm,

56:38.880 --> 56:44.000
if I have the capacity to build maybe a more abstract model, which, which I can now simulate,

56:44.000 --> 56:48.560
right, then I might be giving you an answer sort of on that level, right? And so, but then it's

56:48.560 --> 56:52.960
also half punting, right? Because now you have to kind of come up with a good model of how people

56:52.960 --> 56:58.080
generate the right kind of causal abstractions for some situation that then allow them to compute

56:58.080 --> 57:03.200
the counterfactual, because now it's not messy anymore, right? And another thing that I should

57:03.200 --> 57:07.280
mention, and that quite a lot of the work on responsibility that I've, that I've looked at

57:07.280 --> 57:12.320
particularly in groups, the sort of situations that you pointed out, like elections and, you know,

57:12.320 --> 57:16.960
global warming, they're, they're characterized by, by large degrees of over determination,

57:16.960 --> 57:23.520
right? Like in election, you hardly ever cast a pivotal vote, right? And, and so those also

57:23.520 --> 57:27.680
traditionally were problems for counterfactual accounts, right? Because everyone can say like,

57:27.680 --> 57:32.320
I made no difference, like if I fly every day, you know, that's not really going to make a difference.

57:32.320 --> 57:37.280
And so, and there you can, and similar with election, why should I go vote, right? Because

57:37.280 --> 57:40.720
if my vote's not going to make any difference, right? And there at least models have been built

57:40.720 --> 57:45.600
that then say like, okay, well, it's not, you're not off the hook, right? It's basically saying,

57:45.600 --> 57:49.280
even if you would not have made a difference in this particular situation,

57:49.280 --> 57:52.800
maybe the degree of responsibility that you have for some election, for example,

57:52.800 --> 57:57.120
maybe related to how close you were to making a difference to the outcome, right? If it's like,

57:57.120 --> 58:02.160
if the outcome is 6-5, you feel very responsible. If it's like 7-4, a little less. If it's like

58:02.160 --> 58:07.840
8-3, a little less, right? But not, but it shouldn't go to zero, right? And then, and then as it,

58:07.840 --> 58:11.520
maybe now relates to kind of, you know, global warming and so on, part of the challenge then

58:11.520 --> 58:16.960
from the more like, you know, what do we do about it? Side might be like, okay, how do we make it

58:16.960 --> 58:23.360
such that people don't perceive a sort of, you know, going to zero sense of responsibility,

58:23.360 --> 58:28.800
right? Such that you feel like actually the actions that you do make a difference to the outcome.

58:28.800 --> 58:36.160
And so, yeah, so that's, so I think a mix of thoughts, I guess, in response to your question.

58:37.120 --> 58:42.160
So we're about it, time. Is there a reminder if you are here? If you're logging attendance,

58:42.160 --> 58:46.400
make sure to grab one of these code words up at the front and give Toby a compliment on his

58:46.400 --> 58:50.800
talk on your way out, maybe make you come up next door. Let's thank our speaker.

