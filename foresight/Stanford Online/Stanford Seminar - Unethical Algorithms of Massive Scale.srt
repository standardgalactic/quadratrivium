1
00:00:00,000 --> 00:00:14,880
Who was here last time I was here?

2
00:00:14,880 --> 00:00:17,000
I know Hal was.

3
00:00:17,000 --> 00:00:20,040
Oh, I know that guy, Rick.

4
00:00:20,040 --> 00:00:21,040
And oh, I remember you two.

5
00:00:21,040 --> 00:00:22,040
What's your first name?

6
00:00:22,040 --> 00:00:23,040
Eugene.

7
00:00:23,040 --> 00:00:24,040
Eugene, of course.

8
00:00:24,040 --> 00:00:25,040
How are you?

9
00:00:25,040 --> 00:00:26,040
Okay.

10
00:00:26,040 --> 00:00:27,040
Good.

11
00:00:27,080 --> 00:00:31,400
When I recall, when I was here last time, I came with a lot of slides and I decided

12
00:00:31,400 --> 00:00:33,480
not to use them.

13
00:00:33,480 --> 00:00:38,680
And I'm going to do the same thing again, except I think I'm going to show you one slide

14
00:00:38,680 --> 00:00:40,680
because it just blows my mind.

15
00:00:40,680 --> 00:00:43,520
So when we get to that one point, I'm going to turn this on and I'm going to show you

16
00:00:43,520 --> 00:00:45,440
one slide.

17
00:00:45,440 --> 00:00:47,320
And by then, I hope you'll be prepared.

18
00:00:47,320 --> 00:00:51,040
Oh, dear.

19
00:00:51,040 --> 00:00:53,720
Well, that shouldn't be.

20
00:00:53,720 --> 00:00:54,720
How do we get rid of that?

21
00:00:54,720 --> 00:00:56,960
We're just going to go like that.

22
00:00:56,960 --> 00:00:59,320
So well, that's only half of it anyway.

23
00:00:59,320 --> 00:01:01,560
That's not the important half of the slide.

24
00:01:01,560 --> 00:01:06,680
So I want to take us on a journey.

25
00:01:06,680 --> 00:01:14,080
And the journey is going to take us from the hypothetical to the somewhat real, to the

26
00:01:14,080 --> 00:01:19,280
real, to the surreal.

27
00:01:19,280 --> 00:01:21,920
So we're going to move in that direction.

28
00:01:21,920 --> 00:01:24,680
So let's start with the hypothetical.

29
00:01:24,680 --> 00:01:28,560
And in fact, I liked Andy's introduction, short, sweet.

30
00:01:28,560 --> 00:01:36,960
He mentioned something about influence, perhaps on the internet, perhaps using new technologies.

31
00:01:36,960 --> 00:01:41,400
Let's consider a few hypothetical situations.

32
00:01:41,400 --> 00:01:42,400
Here we go.

33
00:01:42,400 --> 00:01:44,760
We'll start with Facebook.

34
00:01:44,760 --> 00:01:57,440
Let's say that last spring or summer, Facebook sent out, this is hypothetical now, sent out

35
00:01:57,440 --> 00:02:01,280
reminders to go register to vote.

36
00:02:01,280 --> 00:02:08,320
Now at the moment, they have nearly two billion members.

37
00:02:08,400 --> 00:02:15,640
Among those members, there are 72% of the adult population of the US.

38
00:02:15,640 --> 00:02:23,760
So if Facebook chose to send out, go out and register to vote reminders to their people,

39
00:02:23,760 --> 00:02:25,280
they could reach a lot of people.

40
00:02:25,280 --> 00:02:31,000
They could reach a lot of people because in fact, a lot of Americans who are eligible

41
00:02:31,000 --> 00:02:33,760
to vote are not registered.

42
00:02:33,760 --> 00:02:40,800
In fact, I think it's roughly 70 million Americans who are eligible to vote out of the 220 or

43
00:02:40,800 --> 00:02:44,760
so who could vote are not even registered.

44
00:02:44,760 --> 00:02:51,120
So if you take roughly 0.7 times 0.7, if you see where I got those numbers from, then

45
00:02:51,120 --> 00:02:59,680
that means Facebook could reach 50 million Americans who are not registered to vote and

46
00:02:59,680 --> 00:03:04,480
could send out reminders to them.

47
00:03:04,480 --> 00:03:07,560
Now here's the hypothetical part.

48
00:03:07,560 --> 00:03:13,200
What if they sent out those reminders only to people in certain demographics?

49
00:03:13,200 --> 00:03:19,000
What if they chose to send out reminders just to Democrats, let's say, or just to Republicans,

50
00:03:19,000 --> 00:03:23,920
or just to supporters of Donald Trump, or just to supporters of Hillary Clinton?

51
00:03:23,920 --> 00:03:26,840
Would anyone know that?

52
00:03:26,840 --> 00:03:33,600
I don't think anyone would know that because what Facebook sends out is always targeted.

53
00:03:33,600 --> 00:03:41,920
No one keeps track of which groups are receiving those messages or those items on the newsfeed.

54
00:03:41,920 --> 00:03:44,000
No one keeps track.

55
00:03:44,000 --> 00:03:50,000
So in fact, hypothetically, Facebook could have sent out those reminders to register

56
00:03:50,000 --> 00:03:58,000
to vote, reaching 50 million unregistered adults who are eligible to vote, and no one

57
00:03:58,000 --> 00:04:04,080
would have known they could have done so selectively just to supporters of one candidate.

58
00:04:04,080 --> 00:04:08,640
Now how many additional people would then have registered to vote?

59
00:04:08,640 --> 00:04:10,920
We'll get to that.

60
00:04:10,920 --> 00:04:18,600
Okay, now we're still on the hypothetical, but let me just mention on this one issue

61
00:04:18,600 --> 00:04:26,160
that in fact, Facebook did send out reminders to go register to vote.

62
00:04:26,160 --> 00:04:34,780
They sent them out en masse, and the New York Times concluded that because, very clever

63
00:04:34,780 --> 00:04:39,280
and simple what they came up with, the New York Times concluded that because Facebook,

64
00:04:39,280 --> 00:04:45,160
generally speaking, reaches a younger audience, and because younger people tend to be more

65
00:04:45,160 --> 00:04:51,480
Democrat than Republican, then in fact, Facebook, even if it did broadcast these messages to

66
00:04:51,480 --> 00:04:57,520
everyone, that what they did was advantageous to Democrats.

67
00:04:57,520 --> 00:05:00,840
That was the New York Times conclusion, and they came up with some numbers, they came

68
00:05:00,840 --> 00:05:08,400
up with some estimates, but in fact, we don't really know to whom those reminders were sent.

69
00:05:08,400 --> 00:05:09,400
Okay, there's example one.

70
00:05:09,400 --> 00:05:12,360
Now let's go to example two.

71
00:05:12,480 --> 00:05:19,760
Election day, if on election day, Facebook chose to send out go out and vote reminders

72
00:05:19,760 --> 00:05:26,240
just to people of one political party or just to supporters of one candidate, how many additional

73
00:05:26,240 --> 00:05:36,400
people might they stimulate to get off of their sofas and go vote?

74
00:05:37,400 --> 00:05:44,200
Well, as it happens, there's actually an answer to this question, because in 2012, Facebook,

75
00:05:44,200 --> 00:05:50,320
with some people I know from the University of California, San Diego, published an article

76
00:05:50,320 --> 00:05:55,600
about a manipulation that they did in 2010 during the 2010 election.

77
00:05:55,600 --> 00:06:00,920
They in fact sent out go out and vote reminders to 60 million of their members on election

78
00:06:00,920 --> 00:06:08,240
day in 2010, and they had a control group, and they did some surveying to try to figure

79
00:06:08,240 --> 00:06:16,560
out who went out and voted in the experimental group, when the group that was getting the

80
00:06:16,560 --> 00:06:21,560
reminders and in the control group, and they concluded that their reminders on election

81
00:06:21,560 --> 00:06:30,520
day in 2010 caused an additional 340,000 people to go out and vote.

82
00:06:30,520 --> 00:06:37,400
So if you extrapolate from that published experiment, if you extrapolate from that to

83
00:06:37,400 --> 00:06:49,920
election day in 2016, that would tell you conservatively that if on election day Facebook

84
00:06:49,920 --> 00:06:58,280
had sent out go out and vote reminders selectively to supporters of one party or another, conservatively

85
00:06:58,280 --> 00:07:05,240
speaking, they could easily have caused an additional six or 700,000 people to go out

86
00:07:05,240 --> 00:07:11,680
and vote who otherwise would have stayed home, and no one would have known.

87
00:07:11,680 --> 00:07:16,560
So did they actually do that on election day?

88
00:07:16,560 --> 00:07:23,080
I don't know, but as you'll see, we're going to move from the hypothetical to the real

89
00:07:23,080 --> 00:07:27,240
and a little bit in the direction of the surreal, so we'll get back to this.

90
00:07:27,240 --> 00:07:32,640
Now there are other things hypothetically that Facebook could have done to manipulate

91
00:07:32,640 --> 00:07:34,000
people last year.

92
00:07:34,000 --> 00:07:39,880
You may recall that a whistleblower turned up, a former Facebook employee who had been

93
00:07:39,880 --> 00:07:47,040
one of the news curators, and in fact I met another of the news curators for Facebook

94
00:07:47,040 --> 00:07:52,280
sometime later in New York, and who told me that no, they weren't sitting in a basement

95
00:07:52,280 --> 00:07:58,640
as the press had reported, no, that was false, but yes, they really were a bunch of recent

96
00:07:58,640 --> 00:08:06,560
college graduates with very liberal leanings, and yes, they were selectively removing conservative

97
00:08:06,560 --> 00:08:13,040
news stories from the news feed that Facebook shows to people, and Facebook now seems to

98
00:08:13,040 --> 00:08:17,800
be, according to some reports, the main place people are getting their news, or at least

99
00:08:17,800 --> 00:08:21,560
getting links to their news stories.

100
00:08:21,560 --> 00:08:31,920
So the curators, human curators, were indeed messing with the news feed in a way that favored

101
00:08:31,920 --> 00:08:33,520
the Democrats.

102
00:08:33,520 --> 00:08:35,720
So this is an actual whistleblower who came forth.

103
00:08:35,720 --> 00:08:41,260
So this is, but let's keep it hypothetical, because first of all, we don't know that the

104
00:08:41,260 --> 00:08:45,840
management of Facebook had anything to do that, it could have just been that they happened

105
00:08:45,840 --> 00:08:50,280
to hire these particular people with these particular political leanings, right?

106
00:08:50,280 --> 00:08:56,720
So just an error, really a kind of, you know, an oversight, you could consider, however,

107
00:08:56,720 --> 00:09:02,160
let's look at this hypothetically, what if they deliberately wanted to do that?

108
00:09:02,160 --> 00:09:06,680
What if it wasn't just an accident of hiring, what if they deliberately wanted to alter

109
00:09:06,680 --> 00:09:11,200
our opinions on things by altering news feeds?

110
00:09:11,200 --> 00:09:12,200
Could they do that?

111
00:09:12,200 --> 00:09:14,560
Yes, are they doing that?

112
00:09:14,560 --> 00:09:18,960
Well, there was that whistleblower, but you know, all those people got fired, and they

113
00:09:18,960 --> 00:09:24,280
said they were moving to an algorithm, and so are they doing this now?

114
00:09:24,280 --> 00:09:26,400
I don't know.

115
00:09:26,400 --> 00:09:27,960
So it's still hypothetical.

116
00:09:27,960 --> 00:09:31,600
So you've got the news feeds, you've got the trending stories, same thing could be

117
00:09:31,600 --> 00:09:33,120
done there, right?

118
00:09:33,120 --> 00:09:41,580
So Facebook has a bunch of different ways to alter opinions without people knowing.

119
00:09:41,580 --> 00:09:43,840
That's what's important.

120
00:09:43,840 --> 00:09:46,180
That's what's important.

121
00:09:46,180 --> 00:09:52,260
They even have a fifth way, because are any of you here Facebook users and willing to

122
00:09:52,260 --> 00:09:53,260
admit it?

123
00:09:53,260 --> 00:09:56,860
Okay, well, that's most of you, wow.

124
00:09:56,860 --> 00:10:06,820
You know they have a search bar, but what you may or may not know is that although the

125
00:10:06,820 --> 00:10:12,620
search bar is usually used to find your friends and family, it can be used for other purposes

126
00:10:12,620 --> 00:10:21,660
too, and before the election, Facebook posted a video urging people to search for election

127
00:10:21,660 --> 00:10:30,820
2016 in the search bar, and then that would give you a list, again, a feed of information

128
00:10:30,820 --> 00:10:33,500
which of course they had complete control over.

129
00:10:33,500 --> 00:10:36,580
So they've got the search bar, they've got the trending stories, they've got the news

130
00:10:36,580 --> 00:10:44,940
feed, they've got the possibility of sending out selective reminders, lots of things, none

131
00:10:44,940 --> 00:10:46,940
of which would be visible to people.

132
00:10:46,940 --> 00:10:53,460
Let's shift over to the big guy, Google.

133
00:10:53,460 --> 00:11:02,900
So Google controls lists also, news feeds are lists, and Google controls lists.

134
00:11:02,900 --> 00:11:08,980
Two very, very important lists, among others, but two extremely important lists, both of

135
00:11:08,980 --> 00:11:15,820
which are generated on the fly when you get onto Google search and start to conduct a

136
00:11:15,820 --> 00:11:17,060
search.

137
00:11:17,060 --> 00:11:21,020
The first list is the little list up at the top.

138
00:11:21,020 --> 00:11:25,420
Those are the search suggestions in what is sometimes called autocomplete.

139
00:11:25,420 --> 00:11:32,140
Google invented autocomplete, 2004 I believe, and when they invented it, it had I believe

140
00:11:32,260 --> 00:11:38,860
10 search suggestions, and it was an opt-in feature.

141
00:11:38,860 --> 00:11:46,580
But a couple of years later, it was no longer an opt-in, in fact, you couldn't opt out.

142
00:11:46,580 --> 00:11:52,580
And then sometime later, the list began to change, initially it appeared that the list

143
00:11:52,580 --> 00:11:56,700
was just showing you what other people were searching for.

144
00:11:56,700 --> 00:12:01,700
And if you, right now, for example, if you go and conduct a search on Yahoo, don't waste

145
00:12:01,740 --> 00:12:07,380
your time because their search results are terrible, by the way, and they're pulling

146
00:12:07,380 --> 00:12:13,660
almost all of their search results from Google under an arrangement that was signed between

147
00:12:13,660 --> 00:12:18,260
the two companies in late 2015, however, let's put that aside.

148
00:12:18,260 --> 00:12:24,860
The point is, when you try to do a search on Yahoo, you get 10 search results, and they

149
00:12:24,860 --> 00:12:28,140
appear to be what people are searching for.

150
00:12:28,140 --> 00:12:33,340
If you use Bing, which I also wouldn't bother with, but you get eight search results, which

151
00:12:33,340 --> 00:12:35,700
also appear to be what people are searching for.

152
00:12:35,700 --> 00:12:41,700
You can confirm that, in fact, the suggestions you're getting on Bing and Yahoo are really,

153
00:12:41,700 --> 00:12:44,060
generally speaking, just what people are searching for.

154
00:12:44,060 --> 00:12:45,660
How do you confirm that?

155
00:12:45,660 --> 00:12:47,020
It's very simple.

156
00:12:47,020 --> 00:12:52,220
You go over to Google, Trends, and you can see what people are searching for.

157
00:12:52,300 --> 00:12:59,540
So, meanwhile, Google's list of suggestions over the years, for some reason, got smaller

158
00:12:59,540 --> 00:13:06,460
and smaller and smaller, so that, at least on most devices, they only show you four items.

159
00:13:06,460 --> 00:13:11,540
Occasionally, you'll get three, two, or one.

160
00:13:11,540 --> 00:13:16,500
There are some circumstances under which you can get five, six, seven, but in fact, we actually

161
00:13:16,500 --> 00:13:21,140
did an extensive survey to figure out how likely those possibilities are.

162
00:13:21,220 --> 00:13:22,620
Very unlikely.

163
00:13:22,620 --> 00:13:25,980
You generally speaking, on most devices, get four.

164
00:13:25,980 --> 00:13:29,820
On some mobile devices, you get five, but the point is, it's a short list.

165
00:13:29,820 --> 00:13:36,900
And the items on that list no longer have any obvious correspondence to what people

166
00:13:36,900 --> 00:13:40,260
are searching for, so what are they showing you?

167
00:13:40,260 --> 00:13:47,620
Well, the point is, here's a list that Google controls that we all see, perhaps, every day

168
00:13:47,620 --> 00:13:52,300
or perhaps many times a day, and it's the search suggestions list.

169
00:13:52,300 --> 00:13:54,700
And generally speaking, it has four items.

170
00:13:54,700 --> 00:13:59,100
And generally speaking, those four items don't have any obvious correspondence anymore to

171
00:13:59,100 --> 00:14:00,940
what people are searching for.

172
00:14:00,940 --> 00:14:02,140
Very easy to show that.

173
00:14:02,140 --> 00:14:04,900
You just look on Google Trends.

174
00:14:04,900 --> 00:14:11,660
More and more, the items that appear on that list also are customized for the individual.

175
00:14:11,660 --> 00:14:16,020
So that's one reason why, of course, they're not going to have much correspondence to what

176
00:14:16,020 --> 00:14:19,820
people in general are searching for, because they're going to have to do with you and your

177
00:14:19,820 --> 00:14:24,020
history and what Google's algorithm perceives as your needs.

178
00:14:24,020 --> 00:14:25,420
So there's a list.

179
00:14:25,420 --> 00:14:28,660
And then they've got this other list, because once you click on something, sometimes even

180
00:14:28,660 --> 00:14:36,460
if you don't click on something, in the suggestions, the search results populate.

181
00:14:36,460 --> 00:14:39,300
Sometimes you don't even have to click, and the results populate below.

182
00:14:39,300 --> 00:14:41,300
So now you've got a second list.

183
00:14:41,300 --> 00:14:44,620
So I've got a short list, and I've got a long list.

184
00:14:44,620 --> 00:14:49,380
Now the long list, as you know, goes on forever.

185
00:14:49,380 --> 00:14:54,740
But most people don't go beyond the first page of results, which shows you 10 results.

186
00:14:54,740 --> 00:14:59,900
In fact, 50% of all clicks go to the top two results.

187
00:14:59,900 --> 00:15:05,780
And more than 90% of all clicks are on the first page.

188
00:15:05,780 --> 00:15:14,820
So what's on that first page is very, very important, extremely important, two lists.

189
00:15:14,820 --> 00:15:18,660
Now let's get hypothetical here.

190
00:15:18,660 --> 00:15:28,780
What if Google were using search suggestions not to help you do your search, although that's

191
00:15:28,780 --> 00:15:31,540
what they would claim, of course, but they're actually a business.

192
00:15:31,540 --> 00:15:36,060
They're not really the public library, like they pretend to be, they're actually a business.

193
00:15:36,060 --> 00:15:42,100
And so what if Google were actually using search suggestions in a way that makes them

194
00:15:42,100 --> 00:15:44,380
more money?

195
00:15:44,380 --> 00:15:46,140
How could they use search suggestions?

196
00:15:46,140 --> 00:15:51,460
It's hypothetical, I'm not claiming anything.

197
00:15:51,460 --> 00:15:54,540
But how would they do that?

198
00:15:54,540 --> 00:16:00,340
Well, as a matter of fact, we know a lot about this, that's when I get to the real part of

199
00:16:00,340 --> 00:16:03,040
my talk, I'll explain to you how we know.

200
00:16:03,040 --> 00:16:07,180
We know a lot about this, but just staying hypothetical for the moment, how could they

201
00:16:07,180 --> 00:16:08,180
do this?

202
00:16:08,180 --> 00:16:12,220
Well, first of all, if they know about you and your search history and your interests

203
00:16:12,220 --> 00:16:17,420
and so on, in fact, they know a lot about all of us, well, they could put items there

204
00:16:17,420 --> 00:16:21,420
that they think you're likely to click on.

205
00:16:21,420 --> 00:16:26,700
And that they think you want to see, maybe, but more importantly, they could put items

206
00:16:26,700 --> 00:16:34,500
there on that list that you're very likely to click on rather than doing what.

207
00:16:34,500 --> 00:16:36,580
What do they not want you to do?

208
00:16:36,580 --> 00:16:44,980
Yes, they don't want you to type your search term, your full search term.

209
00:16:44,980 --> 00:16:48,940
This is hypothetical, of course.

210
00:16:48,940 --> 00:16:55,220
So hypothetically, if I were running Google and I wanted to make a lot of money, I would

211
00:16:55,220 --> 00:16:59,860
figure out how to show people search terms that made it very likely that people are going

212
00:16:59,860 --> 00:17:03,980
to click on one of the search terms.

213
00:17:03,980 --> 00:17:08,860
And I would also, since I'm now controlling what people are clicking on, I would make sure

214
00:17:08,860 --> 00:17:14,260
that the results that populate as a result of a click or even without a click, I would

215
00:17:14,260 --> 00:17:19,300
make darn sure that those results are results I want to show people.

216
00:17:19,300 --> 00:17:20,300
See?

217
00:17:20,300 --> 00:17:21,700
Yes, what is your name?

218
00:17:21,700 --> 00:17:22,700
Hi, John.

219
00:17:22,700 --> 00:17:30,300
Why don't you think they want you to type out what it is you really want?

220
00:17:30,300 --> 00:17:34,620
You know, I don't really want to speculate about the motives, particular motives.

221
00:17:34,620 --> 00:17:42,560
I guess what I'm saying is if I were in charge, then I would lose control over what people

222
00:17:42,560 --> 00:17:45,140
were searching for.

223
00:17:45,140 --> 00:17:46,140
Think about this.

224
00:17:46,140 --> 00:17:53,620
If I let you go ahead and type your whole search term, and then I let you hit Enter,

225
00:17:53,620 --> 00:17:59,260
I'm in a tough position now because my credibility always depends on giving you exactly what

226
00:17:59,260 --> 00:18:00,260
you want.

227
00:18:00,260 --> 00:18:04,260
So you always have to have that feeling that they answered your question, right?

228
00:18:04,260 --> 00:18:08,480
So I'm really kind of stuck because depending on what you typed, I've actually got to give

229
00:18:08,480 --> 00:18:14,260
you more or less what it is you wanted or what it is you thought you wanted.

230
00:18:14,260 --> 00:18:15,260
See what I'm saying?

231
00:18:15,260 --> 00:18:19,500
If I lose some control, whereas if I can get you to click on one of the suggestions that

232
00:18:19,500 --> 00:18:22,740
I make, I'm in complete control.

233
00:18:22,740 --> 00:18:26,460
That's just better from a business perspective.

234
00:18:26,460 --> 00:18:33,660
So we've got this list of here, and again, hypothetically, Google could show people lists

235
00:18:33,660 --> 00:18:37,540
that give them more control over what people search for, and another way to put that is

236
00:18:37,540 --> 00:18:43,620
Google could show people search terms that would nudge people's searches.

237
00:18:43,620 --> 00:18:48,340
You know that great book from 2008 by Thaler and the other guy called Nudge?

238
00:18:48,340 --> 00:18:50,500
It's a great, great book.

239
00:18:50,500 --> 00:18:56,620
And search suggestions hypothetically could be used to nudge people's searches in directions

240
00:18:56,620 --> 00:18:58,580
that are advantageous for the company.

241
00:18:58,580 --> 00:19:01,260
It's hypothetical, right?

242
00:19:01,260 --> 00:19:03,940
Now let's get to the search results.

243
00:19:03,940 --> 00:19:08,820
Search results, 10, those are the key ones.

244
00:19:08,820 --> 00:19:14,260
Google has total control over the order in which these items are presented.

245
00:19:14,260 --> 00:19:22,940
Hypothetically, they could put items near the top that they want you to click on, and

246
00:19:22,940 --> 00:19:27,940
again, where doing so would be advantageous to the company.

247
00:19:27,940 --> 00:19:33,180
And that could have to do with making money, that could be one goal, possibly, but could

248
00:19:33,180 --> 00:19:35,260
have to do with other things too.

249
00:19:35,260 --> 00:19:40,540
People agendas have to do with anything, really.

250
00:19:40,540 --> 00:19:46,300
I mean, again, I don't know people's specific motives, I'm just saying one could hypothetically

251
00:19:46,300 --> 00:19:55,900
use search results in a way to manipulate people's thinking, beliefs, purchases, certainly,

252
00:19:55,900 --> 00:19:57,900
and possibly even their votes.

253
00:19:57,900 --> 00:20:02,380
Okay, so we've got a few different hypotheticals here for Facebook.

254
00:20:02,380 --> 00:20:07,780
We've got a couple for Google, and I could go on all day about other options Google has.

255
00:20:07,780 --> 00:20:12,100
If you want to look at a cool article, it's a piece I wrote for US News and World Report

256
00:20:12,100 --> 00:20:17,580
called The News Censorship, and that goes through lots of different crazy lists that

257
00:20:17,580 --> 00:20:23,740
Google controls, and if you can control lists, wow, you can control what people think.

258
00:20:23,740 --> 00:20:37,220
No, the Great Firewall is trivial by comparison.

259
00:20:37,220 --> 00:20:43,300
The Great Firewall cuts off access to a lot of information, although all of the Chinese

260
00:20:43,300 --> 00:20:51,500
students who work with me say that in China it's very common for people to use proxies

261
00:20:51,500 --> 00:20:54,500
to get around the firewall.

262
00:20:54,500 --> 00:21:00,540
But all it does is restrict access, where I'm talking about a much finer degree of control.

263
00:21:00,540 --> 00:21:03,340
Also, the Great Firewall is visible.

264
00:21:03,340 --> 00:21:05,780
Do you see what I'm saying?

265
00:21:05,780 --> 00:21:09,420
It's completely different than the kinds of hypothetical situations I'm discussing so

266
00:21:09,420 --> 00:21:15,860
far because I'm talking about methods of influence that are completely invisible.

267
00:21:15,860 --> 00:21:21,580
If someone is showing you a list of search suggestions that have been carefully prepared

268
00:21:21,580 --> 00:21:25,780
to make sure, make it likely anyway, that you're going to click on one of them and when you

269
00:21:25,780 --> 00:21:30,300
do, that serves my corporate needs, you can't see that.

270
00:21:30,300 --> 00:21:31,300
It's impossible.

271
00:21:31,300 --> 00:21:33,300
And the same is true, of course, with the search results.

272
00:21:33,300 --> 00:21:39,180
If I'm putting the results in an order that suits my corporate needs, serves my corporate

273
00:21:39,180 --> 00:21:40,340
needs, you can't see that.

274
00:21:40,340 --> 00:21:42,100
It's impossible.

275
00:21:43,100 --> 00:21:46,060
Okay, so we got a few possibilities for Facebook.

276
00:21:46,060 --> 00:21:47,980
We got a couple for Google.

277
00:21:47,980 --> 00:21:52,820
Now let's go into crazy land here because remember, we're moving gradually toward the

278
00:21:52,820 --> 00:22:10,660
surreal, but still, speaking hypothetically, Tinder, Tinder, Tinder, Tinder is a kind of

279
00:22:11,660 --> 00:22:16,540
it's a matching service mainly for people who want to have sex with each other.

280
00:22:16,540 --> 00:22:20,900
Very popular sex, I mean, and Tinder too.

281
00:22:20,900 --> 00:22:28,860
So Tinder normally just shows you pictures of people and then you swipe left or swipe

282
00:22:28,860 --> 00:22:32,780
right indicating whether you're hot or not, right?

283
00:22:32,780 --> 00:22:37,620
This brings us right back to Bill Gates and his original app at Harvard that got him into

284
00:22:37,620 --> 00:22:38,700
trouble.

285
00:22:38,700 --> 00:22:40,580
So that's really what Tinder is.

286
00:22:40,580 --> 00:22:45,500
It's just a hot or not, you swipe and so on, and if someone else who you said was hot

287
00:22:45,500 --> 00:22:50,340
swipes you as hot, then you get connected with that person and that's basically what

288
00:22:50,340 --> 00:22:51,340
Tinder is.

289
00:22:51,340 --> 00:22:57,540
Oh, I'm sorry, Zuckerberg, absolutely, sorry.

290
00:22:57,540 --> 00:22:58,540
Thank you.

291
00:22:58,540 --> 00:22:59,540
I appreciate that.

292
00:22:59,540 --> 00:23:04,900
Okay, so, but before the election, a couple months before the election, Tinder announced

293
00:23:04,900 --> 00:23:07,180
a very odd application.

294
00:23:07,180 --> 00:23:13,220
It was called Swipe the Vote and Tinder offered to help you figure out which candidate you're

295
00:23:13,220 --> 00:23:18,220
better suited for, which candidate in other words better serves your values and your needs

296
00:23:18,220 --> 00:23:21,460
as a voter, Tinder.

297
00:23:21,460 --> 00:23:27,140
So sure enough, when people got onto Tinder, one of the options they had was to swipe the

298
00:23:27,140 --> 00:23:31,580
vote, you know, let us help you figure out who to vote for.

299
00:23:31,580 --> 00:23:36,540
So of course the people who are most likely to do this are going to be undecided voters.

300
00:23:37,140 --> 00:23:41,580
That's gorgeous, that's beautiful because we know a lot about undecided voters and those

301
00:23:41,580 --> 00:23:44,740
are the people who are easiest to influence.

302
00:23:44,740 --> 00:23:51,740
So over time, I don't know how many people use Swipe the Vote because how would I know,

303
00:23:51,740 --> 00:23:52,940
right?

304
00:23:52,940 --> 00:23:54,580
But here's the way it worked.

305
00:23:54,580 --> 00:23:59,020
It asks a question about immigration and it says, you know, you agree or disagree and

306
00:23:59,020 --> 00:24:03,020
you swipe one way or the other and then it asks another question, you know, about taxes

307
00:24:03,020 --> 00:24:05,260
and you swipe one way or the other and so on.

308
00:24:05,260 --> 00:24:10,660
And it's just pretty much five questions and then it says, you're a perfect match for Donald

309
00:24:10,660 --> 00:24:11,660
Trump.

310
00:24:11,660 --> 00:24:17,460
So we're still in hypothetical land.

311
00:24:17,460 --> 00:24:27,740
What if hypothetically the company had some bias in favor of one candidate or the other?

312
00:24:27,740 --> 00:24:32,900
Couldn't they make it, couldn't they tell every single person who swipes the vote?

313
00:24:33,380 --> 00:24:36,940
That they should vote for Donald Trump or Hillary Clinton?

314
00:24:36,940 --> 00:24:37,940
Couldn't they do that?

315
00:24:37,940 --> 00:24:38,940
Would anyone know?

316
00:24:38,940 --> 00:24:39,940
Would anyone even notice?

317
00:24:39,940 --> 00:24:46,180
You know, if they wanted to mask the effect, as we've done a lot of experiments on masking,

318
00:24:46,180 --> 00:24:49,780
you know, and they didn't want it to be that obvious, believe me, that's trivially easy

319
00:24:49,780 --> 00:24:51,580
to do.

320
00:24:51,580 --> 00:24:59,980
But the point is, here's a matching service at a website used by tens of millions of Americans

321
00:25:00,060 --> 00:25:03,660
which is advising people on how to vote.

322
00:25:03,660 --> 00:25:07,300
But we don't know what that algorithm is doing.

323
00:25:07,300 --> 00:25:09,660
We don't know whether they favor one candidate or the other.

324
00:25:09,660 --> 00:25:16,660
Well, it turns out that Tinder is not the only matching service, vote matching service on

325
00:25:16,660 --> 00:25:17,660
the internet.

326
00:25:17,660 --> 00:25:20,620
They're popping up more and more.

327
00:25:20,620 --> 00:25:25,380
Some look really, really credible, you know, they're nonprofits and this and that.

328
00:25:25,380 --> 00:25:27,300
But you don't really know who's running them.

329
00:25:27,300 --> 00:25:31,060
You don't know what their motives are and you don't know what the algorithm is doing.

330
00:25:31,060 --> 00:25:36,900
So hypothetically, you could put matching services on the internet, including on big

331
00:25:36,900 --> 00:25:45,500
websites like Tinder, to help people make up their minds about voters or about abortion

332
00:25:45,500 --> 00:25:49,140
or about taxes or about homosexuality or about anything.

333
00:25:49,140 --> 00:25:54,580
You could put matching services up on the internet and algorithms could be shifting

334
00:25:54,580 --> 00:25:58,380
opinions literally by the millions because we know the numbers.

335
00:25:58,380 --> 00:25:59,380
This is what we do.

336
00:25:59,380 --> 00:26:00,380
This is all we do.

337
00:26:00,380 --> 00:26:02,500
We just quantify these effects.

338
00:26:02,500 --> 00:26:09,180
And no one would know because it's invisible.

339
00:26:09,180 --> 00:26:14,060
Okay, I could go on with hypotheticals but you get the idea.

340
00:26:14,060 --> 00:26:21,020
Oh, no, no, was Tinder's algorithm biased toward one candidate or the other?

341
00:26:21,020 --> 00:26:23,540
I don't know.

342
00:26:23,540 --> 00:26:28,820
Remember all those, I don't know, that's very important.

343
00:26:28,820 --> 00:26:32,740
Okay.

344
00:26:32,740 --> 00:26:41,020
To put this in perspective, what kinds of manipulations are making the news every day?

345
00:26:41,020 --> 00:26:44,140
They're not what I just told you.

346
00:26:44,140 --> 00:26:52,220
What are the manipulations that are in the news constantly, especially lately?

347
00:26:52,220 --> 00:26:58,380
Fake news, that's number one, absolutely, positively number one, fake news.

348
00:26:58,380 --> 00:27:02,460
And number two is Cambridge Analytica.

349
00:27:02,460 --> 00:27:08,460
And Cambridge Analytica using massive amounts of data, some of which they kind of, you could

350
00:27:08,460 --> 00:27:13,860
say stole or obtained unethically anyway, and a lot of which they bought.

351
00:27:13,860 --> 00:27:22,940
Cambridge Analytica funded by that Mercer billionaire fellow who is a staunch Trump supporter.

352
00:27:22,940 --> 00:27:32,980
And Cambridge Analytica supposedly helped shift the vote toward Brexit in the Brexit referendum.

353
00:27:32,980 --> 00:27:37,260
And Cambridge Analytica supposedly also helped to put Trump in office.

354
00:27:37,260 --> 00:27:39,660
That makes the news not as much as fake news does though.

355
00:27:39,660 --> 00:27:45,820
Fake news in the news all the time.

356
00:27:45,820 --> 00:27:49,420
We don't study, we could easily study the impact.

357
00:27:49,420 --> 00:27:53,460
We could quantify the impact of fake news stories on people.

358
00:27:53,460 --> 00:27:55,820
We don't, we don't bother.

359
00:27:55,820 --> 00:28:00,220
We could quantify the impact of the kinds of manipulations Cambridge Analytica was using,

360
00:28:00,220 --> 00:28:07,500
which by the way is just plain old marketing stuff, because all they were doing was customizing

361
00:28:07,740 --> 00:28:12,700
basically images and language and ads to get people to click.

362
00:28:12,700 --> 00:28:15,820
That's exactly what marketers do, right?

363
00:28:15,820 --> 00:28:19,100
You use multivariate analysis, you keep changing things.

364
00:28:19,100 --> 00:28:24,940
Now they, they had access to lots of data about people, about supposedly all the voting,

365
00:28:24,940 --> 00:28:26,860
all the voters in America.

366
00:28:26,860 --> 00:28:31,500
They claim to have more than 5,000 data points for every single voter in the United States.

367
00:28:32,220 --> 00:28:40,220
So what I'm saying is that I'm not, we're not studying that.

368
00:28:40,220 --> 00:28:45,100
Why are we not studying these incredibly, because they're trivial.

369
00:28:45,100 --> 00:28:51,020
By comparison to the hypotheticals that I just described, they're completely trivial.

370
00:28:51,020 --> 00:28:53,740
Why is fake news trivial by comparison?

371
00:28:53,740 --> 00:28:56,300
First of all, you can see fake news.

372
00:28:57,180 --> 00:29:01,500
You know there's fake news in front of your eyeballs, or you know there's news anyway.

373
00:29:01,500 --> 00:29:05,340
You're not sure whether it's fake enough, but you sure as heck know that there's a human element

374
00:29:05,340 --> 00:29:09,420
there, because it looks like a newspaper, and so a human must have written it.

375
00:29:09,420 --> 00:29:13,020
And some, and usually there's an aim of a human who wrote the article.

376
00:29:13,020 --> 00:29:15,020
You can see it.

377
00:29:16,220 --> 00:29:20,220
That's very different than kinds of influence, which are invisible to people.

378
00:29:20,220 --> 00:29:23,180
There's extensive research on this and social psychology.

379
00:29:23,260 --> 00:29:28,140
If you influence people using methods, subtle methods that they can't see,

380
00:29:30,380 --> 00:29:34,060
people end up believing that they made up their own minds.

381
00:29:34,060 --> 00:29:38,300
You can still shift people's opinions and actions and so on,

382
00:29:38,300 --> 00:29:42,620
but if they can't see the source of influence, they end up believing that they made up their own minds.

383
00:29:42,620 --> 00:29:45,260
They have no idea that they've even been influenced.

384
00:29:46,540 --> 00:29:52,060
So what's the earliest, obvious example of fake news?

385
00:29:52,060 --> 00:30:00,940
What's the earliest example of that kind of influence in the United States that made big news back in the late 50s, I think, a long time ago?

386
00:30:06,140 --> 00:30:09,100
Exactly, subliminal stimulation, that's right.

387
00:30:09,100 --> 00:30:16,140
And this made big news in the U.S. a long time ago, because supposedly a movie theater in New Jersey was,

388
00:30:16,460 --> 00:30:21,900
you know, had cut in these little frames into their film saying,

389
00:30:23,260 --> 00:30:27,020
go buy a soda, you know, you're thirsty, go buy a soda, go buy our popcorn.

390
00:30:27,580 --> 00:30:30,860
And that got into the news and there was a big uproar.

391
00:30:30,860 --> 00:30:39,100
And as a matter of fact, the association that controlled television standards at the time, they made it supposedly,

392
00:30:39,100 --> 00:30:48,060
they prohibited the use of subliminal stimuli, at least on television.

393
00:30:48,060 --> 00:30:54,700
Some countries ended up passing very strict laws prohibiting it in all kinds of public situations.

394
00:30:54,700 --> 00:30:57,980
The UK subliminal stimulation is unlawful, period.

395
00:30:58,540 --> 00:31:00,140
We never made it unlawful here.

396
00:31:00,140 --> 00:31:05,500
It's still probably used, but, you know, it doesn't really have that much of an impact.

397
00:31:06,140 --> 00:31:07,500
It's just scary, though.

398
00:31:07,500 --> 00:31:12,620
The idea that there's some stimulus that's affecting you and you can't really see it and

399
00:31:12,620 --> 00:31:18,060
it's caused you to buy a drink, but it turns out subliminal stimuli do have an impact on people,

400
00:31:18,060 --> 00:31:20,060
but it's very, very, very, very small.

401
00:31:21,500 --> 00:31:26,860
So, you know, if you're building a business, that's probably not where you want to put your marketing money.

402
00:31:28,140 --> 00:31:34,460
But the point is, invisible stimuli that affect people, you know, they've been around for a while.

403
00:31:34,860 --> 00:31:36,300
There are lots of examples.

404
00:31:36,300 --> 00:31:38,060
There's a body of research on this.

405
00:31:39,020 --> 00:31:43,820
But what I'm trying to tell you is that looking at those hypotheticals,

406
00:31:43,820 --> 00:31:46,140
we have now moved into a very different world,

407
00:31:48,300 --> 00:31:55,980
where there are, hypothetically, means of influencing people by the billions online,

408
00:31:57,180 --> 00:32:01,340
invisibly, without any awareness on people's part that they're being manipulated.

409
00:32:01,340 --> 00:32:07,820
And, you know, the word ethics is in the title of my talk today.

410
00:32:09,580 --> 00:32:13,820
Although the use of these methods is currently perfectly legal,

411
00:32:15,980 --> 00:32:18,380
not because I think anyone would say they should be legal,

412
00:32:18,380 --> 00:32:21,660
but simply because the law hasn't caught up with the technologies.

413
00:32:22,780 --> 00:32:26,620
But I think most of us would agree that these are unethical, at least,

414
00:32:27,500 --> 00:32:28,860
even if they're legal at the moment.

415
00:32:29,420 --> 00:32:32,780
I think most of us would agree that they are unethical.

416
00:32:33,820 --> 00:32:41,260
So, more and more, I think we need to be thinking about the ethics of what new technology is bringing to people.

417
00:32:44,140 --> 00:32:47,100
All right, let's move now, more in the realm of real.

418
00:32:49,420 --> 00:32:52,460
I don't want to spend too much time on this because I could go on forever.

419
00:32:52,460 --> 00:32:54,300
And by the way, what time do I need to stop?

420
00:32:55,260 --> 00:33:00,940
I know it's five something, but okay, good, we're running schedule.

421
00:33:00,940 --> 00:33:05,100
So, let's move a little bit toward the real.

422
00:33:07,500 --> 00:33:15,580
In early 2012, in fact, it happened to be New Year's Day, come to think of it, January for 2012,

423
00:33:16,220 --> 00:33:21,420
I got a bunch of emails all from Google saying that my website had been hacked

424
00:33:21,740 --> 00:33:27,260
and that they were blocking access through their search engine.

425
00:33:28,140 --> 00:33:33,740
So, until that day, I had never given any thought to Google at all.

426
00:33:33,740 --> 00:33:35,180
I just thought it was a great search engine.

427
00:33:35,980 --> 00:33:41,340
And I started to learn some things about Google that made me more concerned about the company.

428
00:33:41,340 --> 00:33:44,380
For one thing, I learned that they had no customer service department,

429
00:33:44,380 --> 00:33:45,900
which I thought was odd, and they still don't.

430
00:33:46,780 --> 00:33:57,980
No, actually, they don't, but we can talk about it more later if you like.

431
00:33:58,540 --> 00:34:04,460
But it's not like lots of other companies where you just call an 800 number and someone answers

432
00:34:04,460 --> 00:34:06,220
the phone and they help you solve your problem.

433
00:34:06,220 --> 00:34:07,580
They don't have anything like that.

434
00:34:07,580 --> 00:34:11,580
In fact, at one point when I did get someone on the phone from Google,

435
00:34:11,580 --> 00:34:14,060
she basically said, I'm really sorry I can't answer that question.

436
00:34:14,060 --> 00:34:15,580
I'm really sorry I can't answer that question.

437
00:34:15,580 --> 00:34:17,020
I said, can you help me at all?

438
00:34:17,020 --> 00:34:18,780
She goes, no, I'm really not allowed to help you.

439
00:34:19,500 --> 00:34:25,660
So, that's the closest I got to a human being who was not a bot and it was an actual person.

440
00:34:25,660 --> 00:34:26,940
I don't think she was a bot anyway.

441
00:34:27,580 --> 00:34:32,620
And the point is I started to learn some things about the company, which bothered me.

442
00:34:32,620 --> 00:34:37,180
It only took me five or six days to get my website taken care of, cleaned up, and so on.

443
00:34:37,180 --> 00:34:40,460
And it took much longer to get it through Google sensors,

444
00:34:41,420 --> 00:34:47,340
in other words, to get their algorithm to okay my website again.

445
00:34:47,340 --> 00:34:49,820
But all right, it was just a hack, no big deal.

446
00:34:51,500 --> 00:34:56,060
However, there were a couple things about this because I've been coding since I was 13 years

447
00:34:56,060 --> 00:35:00,220
old and there were a couple things about what happened that bothered me and made me want to

448
00:35:00,220 --> 00:35:01,820
look more closely at this company.

449
00:35:02,620 --> 00:35:10,220
One was that not only did the search engine block people, warn people away from going to my website.

450
00:35:10,220 --> 00:35:11,820
I understand that, right?

451
00:35:11,820 --> 00:35:16,460
Google's crawlers found malware, I get that, right?

452
00:35:16,460 --> 00:35:19,980
So, their search engine should warn people away, makes perfect sense.

453
00:35:21,500 --> 00:35:28,860
But also, if you tried to get to my website or any of the 20 psychological tests that are

454
00:35:28,860 --> 00:35:32,540
actually based there, so through other URLs you tried to get into those tests,

455
00:35:33,340 --> 00:35:37,340
using Firefox, you couldn't get there.

456
00:35:37,340 --> 00:35:38,940
And that doesn't make sense.

457
00:35:38,940 --> 00:35:44,940
Firefox is a product of Mozilla, which is a non-profit corporation, and I don't get that.

458
00:35:44,940 --> 00:35:48,940
I don't see how Google's crawler would have anything to do with Firefox.

459
00:35:49,900 --> 00:35:53,740
And then I found the same was true with Safari, which is owned by Apple.

460
00:35:54,780 --> 00:35:57,740
So, there were things like this that were bugging me.

461
00:35:57,740 --> 00:35:59,020
I don't want to go into details.

462
00:35:59,020 --> 00:36:03,740
I just want to point out that I started to think a little bit more critically about Google as a

463
00:36:03,740 --> 00:36:12,300
company. Later that year, chatting with some people about search results and search rankings,

464
00:36:13,660 --> 00:36:18,380
I got interested in that, not just on Google, but on search engines in general.

465
00:36:19,820 --> 00:36:25,500
We're all constantly wondering about the search algorithm that they use and how they do this

466
00:36:25,500 --> 00:36:29,420
ordering and how every once in a while they change the ordering, which puts another

467
00:36:30,220 --> 00:36:34,780
thousand businesses out of business, and everyone's always wondering about those things.

468
00:36:36,060 --> 00:36:43,500
It turns out that by late 2012, there was a growing scientific literature looking at

469
00:36:44,140 --> 00:36:50,380
the impact of search rankings, search position, in other words, on people's behavior.

470
00:36:51,340 --> 00:37:00,700
This was being done primarily in the field of marketing because where you are in the search

471
00:37:00,700 --> 00:37:06,620
results depends a lot on whether your business is going to succeed or fail.

472
00:37:06,620 --> 00:37:10,780
If you can get up one more notch, depending on your industry, that might be worth another

473
00:37:10,780 --> 00:37:15,980
million dollars in revenue. So, in fact, there was a growing literature looking at

474
00:37:16,060 --> 00:37:19,660
those little notches and how they impacted people. Among other things,

475
00:37:20,620 --> 00:37:26,140
eye tracking studies showed that people's eyes would go up to the top of the list,

476
00:37:26,140 --> 00:37:31,580
even when you deliberately constructed lists in which superior results were down at the bottom.

477
00:37:32,380 --> 00:37:37,900
In other words, people were just hung up on the top stuff. It's as if people generally believed

478
00:37:38,700 --> 00:37:41,900
that what's at the top is better and what's at the top is truer.

479
00:37:42,380 --> 00:37:50,140
Well, Eugene, I was there. The two books I've seen about the history of Google,

480
00:37:51,660 --> 00:37:54,300
all of them and all the other articles I've seen,

481
00:37:55,820 --> 00:38:00,300
missed the fact that Larry and Sergey had a classmate named Luis Gravano, who is now

482
00:38:00,860 --> 00:38:05,660
a computer science professor at Columbia University. And upstairs in Route 104,

483
00:38:06,460 --> 00:38:13,820
Luis assembled the most amazing quarter or two seminars. Everybody who wrote a search engine

484
00:38:13,820 --> 00:38:19,500
up through the 1990s was invited to come, the guys who wrote Alta Vista, Steve Kersh, who wrote

485
00:38:19,500 --> 00:38:26,700
InfoSeq, and so forth. You go down the list and Larry and Sergey sat on the side of the room

486
00:38:26,700 --> 00:38:32,940
and they started giggling because Larry had developed page rank with Terry and Hector and

487
00:38:33,260 --> 00:38:40,140
other people. And I mean, relevance ranking, and this information retrieval is not my area,

488
00:38:40,140 --> 00:38:45,820
but I certainly used dialogue back in 1976-76. I don't want us to get too much off track because

489
00:38:45,820 --> 00:38:52,460
my time is limited. The thing is this, relevance ranking has been around for decades. And you

490
00:38:52,460 --> 00:38:57,900
should talk to contact Luis Gravano and find out. I can go beyond that, Eugene, because as a matter

491
00:38:57,900 --> 00:39:03,980
of fact, list effects have been around for centuries. And they have been studied in detail

492
00:39:03,980 --> 00:39:11,340
for at least 100 years. So it has long been known that items at the top of a list have more impact

493
00:39:11,340 --> 00:39:16,540
than items in the middle. Under some conditions, items at the bottom of a list also have more impact,

494
00:39:16,540 --> 00:39:21,420
their names for all these things. And all this stuff has been well studied. But when I look at

495
00:39:21,420 --> 00:39:30,380
this literature, when I looked at this literature, I was finding numbers that were just off the scale.

496
00:39:31,580 --> 00:39:37,580
And it wasn't until quite some, I mean, sometime much, much later that we actually figured out

497
00:39:38,460 --> 00:39:46,220
why items near the top of these search results are so impactful. They're incredibly impactful.

498
00:39:46,220 --> 00:39:55,580
In other words, search results produced list effects that are orders of magnitude greater

499
00:39:55,580 --> 00:40:03,900
than any other list effects ever studied. Okay? Well, with luck, I'll be able to tell you why

500
00:40:03,900 --> 00:40:08,940
shortly. But let me just explain what happened. I got interested in this because I saw these big

501
00:40:08,940 --> 00:40:15,260
numbers and I thought, well, if people have this trust for what's at the top, could you use search

502
00:40:15,260 --> 00:40:21,260
results deliberately? I asked to alter people's opinions about things, not just alter their

503
00:40:21,260 --> 00:40:27,980
purchases. In other words, obviously, purchasing was the main issue in these studies. Clicks,

504
00:40:28,780 --> 00:40:34,940
click throughs, conversion rates, that kind of thing. But I was asking a different question. I

505
00:40:34,940 --> 00:40:41,260
was saying, if people have enormous trust for these items near the top, could we use search

506
00:40:41,260 --> 00:40:46,620
results to alter people's opinions? And I thought, what kinds of opinions could we alter? Could we

507
00:40:46,620 --> 00:40:54,060
alter people's voting preferences, for example? That was a question that I raised. So early 2013,

508
00:40:54,060 --> 00:40:59,980
working with a former student of mine, he was working for me at the time, Ronald Robertson,

509
00:40:59,980 --> 00:41:05,660
who's now getting his PhD in a network science program at Northeastern University.

510
00:41:06,300 --> 00:41:15,020
We decided to test this idea by randomly assigning eligible voters to one of three groups. In one

511
00:41:15,020 --> 00:41:21,500
group, they saw search results which were ordered in such a way that favored one political candidate.

512
00:41:21,500 --> 00:41:26,300
In other words, if you clicked on an item near the top of that list, you'd get to a web page,

513
00:41:26,300 --> 00:41:30,300
which said awesome things about that candidate or terrible things about the opposing candidate.

514
00:41:30,620 --> 00:41:38,140
Some people are randomly assigned to a second group in which the opposite is the case. They're

515
00:41:38,140 --> 00:41:42,220
seeing search results that favor the opposing candidate. And the third group is the control

516
00:41:42,220 --> 00:41:49,740
group. They're seeing the search results mixed up. Now, I thought that using this kind of research

517
00:41:49,740 --> 00:41:56,220
design that we could shift, I predicted, two to three percent of the people in these, we call them

518
00:41:56,220 --> 00:42:03,340
bias groups. I figured we could shift two to three percent of them using this technique. And I thought,

519
00:42:03,340 --> 00:42:08,140
okay, that's not a big number, but still a lot of elections are very close. So if you could shift

520
00:42:08,140 --> 00:42:14,300
two to three percent of your undecided voters reliably using, you know, search rankings, I thought,

521
00:42:14,300 --> 00:42:20,700
well, that could have an impact on very close elections. First experiment we ran, the shift was

522
00:42:21,420 --> 00:42:28,380
over forty-eight percent. Second one we ran, the shift was sixty-three percent. Third one we ran,

523
00:42:28,380 --> 00:42:32,860
I think it was thirty-nine percent. These were all pretty small studies. Then we did a national

524
00:42:32,860 --> 00:42:37,340
study in the U.S. with more than two thousand people. Shift we got was, again, about thirty-nine

525
00:42:37,340 --> 00:42:43,900
percent. We also discovered very quickly that we could mask what we were doing. We could hide it.

526
00:42:44,860 --> 00:42:49,340
Even in the first experiment we ran, where people were seeing highly, highly, highly

527
00:42:49,340 --> 00:42:56,620
biased search results, a quarter of the people in the study seemed to have no awareness. I'm sorry,

528
00:42:56,620 --> 00:43:00,780
a quarter of the people in the study, only a quarter of the people in the study, seemed to be

529
00:43:00,780 --> 00:43:06,220
aware of the bias. Three quarters seemed to have no awareness. We found that just by mixing things

530
00:43:06,220 --> 00:43:12,620
up a little bit, okay, so you, so I've got, you know, Trump, Trump, Trump, Clinton, Trump,

531
00:43:12,620 --> 00:43:17,820
Trump, Trump, Trump, just by mixing things up a little bit, adding in a mask, we could easily

532
00:43:17,820 --> 00:43:23,420
boost the number of people who were unaware that they were seeing bias search rankings to one hundred

533
00:43:23,420 --> 00:43:30,380
percent. There was very, very simple manipulation to do and a very simple manipulation to hide.

534
00:43:31,740 --> 00:43:38,060
Producing outrageous shifts in voting preferences as high as eighty percent

535
00:43:39,340 --> 00:43:44,940
in one of the demographic groups that we looked at. We were, we're running experience that were

536
00:43:44,940 --> 00:43:49,420
all speaking of hypothetical and we're moving toward the real, though we're not there yet,

537
00:43:49,420 --> 00:43:55,980
because this is still all kind of science-y stuff, right? But then we did a big experiment in India

538
00:43:55,980 --> 00:44:01,580
during their 2014 election there with more than 2,000 voters from throughout India,

539
00:44:01,580 --> 00:44:06,460
right smack in the middle of the campaign. In fact, even after the voting process started,

540
00:44:06,460 --> 00:44:10,540
we were still bringing in people who hadn't voted yet, because in India they have so many voters

541
00:44:10,540 --> 00:44:14,140
that in fact they, they stretch out the voting process over a period of several weeks and we

542
00:44:14,140 --> 00:44:20,940
were still conducting our study. And there in India where we got real voters and they're being

543
00:44:20,940 --> 00:44:25,260
bombarded with information and they have high familiarity with the candidates, I was saying,

544
00:44:25,260 --> 00:44:30,860
I think we'll still get an effect, but I think it'll be really small, one to two percent or zero.

545
00:44:31,500 --> 00:44:36,060
I thought, I thought maybe the, the reality of the campaigning and the pressure and all that

546
00:44:36,060 --> 00:44:44,140
would just overwhelm what search results could do. What we learned was that search,

547
00:44:44,140 --> 00:44:49,740
bias search results could easily shift voting preferences by more than 20 percent with real

548
00:44:49,740 --> 00:44:56,620
voters and over 60 percent in some demographic groups. In other words, here was a kind of

549
00:44:56,620 --> 00:45:03,820
manipulation, oh by the way, 99.5 percent of the people in that study showed no awareness whatsoever

550
00:45:03,820 --> 00:45:11,340
that they were seeing bias search rankings, 99.5 percent. So this is very different than fake news

551
00:45:11,340 --> 00:45:15,020
and it's very different than even what Cambridge Analytica was doing, you know, where they're,

552
00:45:15,020 --> 00:45:20,300
they're just coming up with good clickbait for people, because no one can see this occurring,

553
00:45:20,300 --> 00:45:25,340
no one's aware that they're being manipulated at all, and yet we're manipulating them. We're

554
00:45:25,340 --> 00:45:33,100
manipulating them sufficiently to, we figured out looking at, in fact, election statistics from

555
00:45:33,100 --> 00:45:41,020
around the world, we were manipulating people sufficiently so that biased search rankings,

556
00:45:41,020 --> 00:45:48,220
could, we calculated, be currently determining the outcomes of upwards of 25 percent of the

557
00:45:48,220 --> 00:45:52,700
national elections in the world. That's mainly because a lot of elections are very close.

558
00:45:55,820 --> 00:46:00,380
And depending on the country and depending on how, what the internet penetration is in that

559
00:46:00,380 --> 00:46:05,020
country, what the percentages of undecided voters and some other things, you know, you can

560
00:46:05,020 --> 00:46:12,300
actually calculate fairly precisely whether or not search rankings can be used to flip an election.

561
00:46:14,140 --> 00:46:17,820
In some of our experiments, we were using web pages and search results from the

562
00:46:18,460 --> 00:46:24,860
2010 election for Prime Minister of Australia. I mentioned this because the winner of that

563
00:46:24,860 --> 00:46:31,500
election won by a margin of 0.24 percent. There are a lot of very close elections.

564
00:46:32,460 --> 00:46:38,780
And even in our last election, Hillary won the popular vote by approximately 2.9 million votes,

565
00:46:39,580 --> 00:46:46,460
but what percentage is that of the total vote of close to 140 million people?

566
00:46:48,940 --> 00:46:53,900
Can you do that in your head? The point is, that's a pretty close election. A lot of elections are

567
00:46:53,900 --> 00:47:00,700
close. Search rankings are very powerful. And so that's what we began to learn. So we since have

568
00:47:00,700 --> 00:47:08,620
done many, many experiments on search results and their power to influence people. We've done

569
00:47:08,620 --> 00:47:15,340
experiments showing that you can influence people's attitudes about things like abortion, fracking,

570
00:47:16,300 --> 00:47:26,620
homosexuality. We've learned that if you let people do multiple searches and which they're seeing

571
00:47:27,500 --> 00:47:32,620
mainly different sets of web pages, but in each case they're seeing a biased set of web pages,

572
00:47:33,420 --> 00:47:40,780
that with each additional search, there's an increase in that shift that we call manipulation

573
00:47:40,780 --> 00:47:47,340
power. So people who conduct multiple searches on the same topic, if they're seeing search results

574
00:47:47,340 --> 00:47:52,380
that are biased in a particular way, that has more and more of an impact on them over time.

575
00:47:54,460 --> 00:48:01,500
We've also done some cool work on the role that operant conditioning seems to play

576
00:48:02,060 --> 00:48:05,740
in this effect, which we eventually called SEEM, the search engine manipulation effect.

577
00:48:06,220 --> 00:48:10,700
I won't go into the details of the experiment, but basically what we figured out, what we confirmed,

578
00:48:11,820 --> 00:48:20,060
is that the reason why search rankings have such an enormous effect, again much larger than most

579
00:48:20,060 --> 00:48:26,460
list effects, is because there's something very peculiar about the way we use search engines.

580
00:48:28,460 --> 00:48:34,860
The vast majority of searches that we conduct are of a routine sort. We're looking for facts.

581
00:48:36,700 --> 00:48:44,140
Right? I go, tell me about where Rick Lozanski went to school.

582
00:48:46,540 --> 00:48:53,180
Or I say, what is the capital of Massachusetts? And sure enough, over and over and over again,

583
00:48:53,180 --> 00:48:59,580
the correct answer appears where? Right at the top. And of course, these days, it even gets up into

584
00:48:59,580 --> 00:49:04,380
the so-called featured snippet or the Google box. So you don't even have to look at the search results.

585
00:49:04,380 --> 00:49:08,620
We're actually doing experiments on the impact of the featured snippet. Right now,

586
00:49:08,620 --> 00:49:13,580
they're running literally right now. The point is, in the experiment we did in operant conditioning,

587
00:49:13,580 --> 00:49:20,780
what we figured out was that the reason why people believe that what's at the top is truer and better

588
00:49:20,780 --> 00:49:26,940
is because there's this daily regimen of operant conditioning. We're like rats in a skinner box

589
00:49:26,940 --> 00:49:30,540
in which we're learning over and over and over again what's at the top is better,

590
00:49:30,540 --> 00:49:35,340
what's at the top is truer. So when the day comes, when we want to put in something a little

591
00:49:35,340 --> 00:49:40,220
different, like something we're really unsure about, like what's the best vacation spot in the

592
00:49:40,220 --> 00:49:46,140
United States? So there's no clear answer, right? It doesn't matter. We're going to trust because

593
00:49:46,140 --> 00:49:50,940
of all that conditioning that never stops. We're going to trust what's at the top more than we

594
00:49:50,940 --> 00:49:55,980
trust things that are down lower. It's really that simple. In the experiment we did, we actually

595
00:49:56,060 --> 00:50:02,300
manipulated people's trust level to show that if you interfere with that operant conditioning,

596
00:50:02,300 --> 00:50:06,860
in fact, people don't trust what's at the top so much and they start looking lower and they

597
00:50:06,860 --> 00:50:14,700
start going to subsequent pages of search results. So we also have done a series of experiments that

598
00:50:14,700 --> 00:50:21,980
will be published soon and looking at the way SIEM can be suppressed with various kinds of alerts

599
00:50:22,620 --> 00:50:27,020
and there are people interested in this because, for example, you may be familiar with the project

600
00:50:27,020 --> 00:50:35,180
called FindX. Anyone know FindX? It's a new project based in Europe. In fact, the man who

601
00:50:35,180 --> 00:50:39,980
started this is now working with me and some other people and something I'll mention at the end of

602
00:50:39,980 --> 00:50:51,660
the talk. FindX is meant to be a search engine which is transparent and fair in which the

603
00:50:51,660 --> 00:51:02,620
rules for ordering are made public and in which even users have a say actually in determining

604
00:51:02,620 --> 00:51:07,660
what the algorithm is doing and how it's computing search results. Another thing they are thinking

605
00:51:07,660 --> 00:51:17,660
about is adding alerts. If you add alerts saying this set of results appears to favor Hillary

606
00:51:17,660 --> 00:51:24,700
Clinton, that has an impact on the way people treat what's in the search results. You can also

607
00:51:24,700 --> 00:51:30,540
add alerts to particular items in the list. So we have work coming out on that.

608
00:51:32,060 --> 00:51:38,540
So we're moving gradually toward the real here. We also started last year studying

609
00:51:38,540 --> 00:51:42,700
search suggestions. Now, again, we're studying things that are invisible. We're not interested in

610
00:51:42,700 --> 00:51:52,860
fake news. Yeah. Yes, I have a question. Yes. Is there a group of people who are somewhat immune

611
00:51:52,860 --> 00:52:00,140
to these effects you describe? In other words, everybody is affected by those?

612
00:52:00,940 --> 00:52:05,980
Well, in the studies we've done in the United States, we've never found any, well, I mean,

613
00:52:05,980 --> 00:52:10,060
there's going to be individuals who are immune, of course, but we have never found a demographic

614
00:52:10,060 --> 00:52:15,340
group that's immune. I'm talking individualism. Is there an individual biologically possible

615
00:52:15,340 --> 00:52:19,980
who would not be affected by it? Well, of course, anyone who is very, by nature, very skeptical

616
00:52:19,980 --> 00:52:25,740
or anyone who's had a bad experience with Google or something, of course, individuals.

617
00:52:25,740 --> 00:52:31,340
So there is a group of people who are immune. Because I don't think so. We are all the same.

618
00:52:31,340 --> 00:52:38,940
Well, as I say, in the United States, we have never found a demographic group that was immune.

619
00:52:39,660 --> 00:52:45,660
Never. Right. So it's just a question of how much people are swayed. Is it this much or is it this

620
00:52:45,660 --> 00:52:50,380
much? But I mean, it's crazy. Everybody's a little bit. And if everybody's swayed a little bit,

621
00:52:50,380 --> 00:52:57,500
how can we talk about facts and fake news? If the content producers and content consumers

622
00:52:57,500 --> 00:53:04,060
also are influenced by other people's opinion, then everything is relative. Well, that's Abraham Lincoln.

623
00:53:04,140 --> 00:53:07,740
You can fool all the people some of the time. Some of the people all the time.

624
00:53:07,740 --> 00:53:13,260
No, no, I'm not talking about this. Let me just point out that we know how to suppress

625
00:53:14,060 --> 00:53:18,780
these effects. So in the paper that we have coming out, we actually show how you can suppress the

626
00:53:18,780 --> 00:53:23,420
in our control groups, we suppress the effect completely, 100%. So we know how to suppress

627
00:53:24,860 --> 00:53:32,060
this kind of effect. If you mix things up, then people don't shift one way or the other.

628
00:53:32,380 --> 00:53:35,980
And there are other ways to suppress the effect in varying degrees. So we're learning about that.

629
00:53:35,980 --> 00:53:40,540
Let me just shift over to search suggestions quickly, because we now have learned a lot.

630
00:53:40,540 --> 00:53:46,540
Okay, room, nice to meet you. We've now learned a lot about search suggestions and why we're

631
00:53:46,540 --> 00:53:50,460
seeing the search suggestions we're seeing, why generally speaking, we're only seeing four.

632
00:53:51,340 --> 00:53:58,540
We actually, I mean, really have learned so much about this that again, it's a whole scary area.

633
00:53:58,540 --> 00:54:05,100
So we've named a new effect called the search suggestion effect, because search suggestions

634
00:54:05,100 --> 00:54:11,100
can in fact be used easily to manipulate people's opinions, attitudes, beliefs, behavior, voting

635
00:54:11,100 --> 00:54:17,740
preferences. We've even figured out where the number four comes from. And the key there to put

636
00:54:17,740 --> 00:54:24,300
it just briefly, the key to that number four has to do with what happens with negative search

637
00:54:24,300 --> 00:54:29,020
suggestions. You've heard of negativity bias, I'm sure, because people study it in a half a

638
00:54:29,020 --> 00:54:34,220
dozen different fields. Sometimes it's called the cockroach and the salad phenomenon. When

639
00:54:34,220 --> 00:54:37,500
there's something negative, and then when a stimulus is negative, like a cockroach in your

640
00:54:37,500 --> 00:54:42,700
salad, your attention is drawn to it and it ruins the whole salad, and you send the salad back.

641
00:54:44,620 --> 00:54:50,300
Now, if I put a piece of chocolate into a plate of sewage, that does not upgrade the plate of

642
00:54:50,300 --> 00:54:56,380
sewage at all. So something particular about negatives, well, it turns out our new experiments

643
00:54:56,380 --> 00:55:01,340
show that there's something very special about those negatives in this list of search suggestions too.

644
00:55:01,340 --> 00:55:09,180
And guess who knows that? Google. So in June, July and August of last year, we documented the fact,

645
00:55:09,180 --> 00:55:15,260
this was partly based on a video that had gone viral in June, then in fact, Google was systematically

646
00:55:15,260 --> 00:55:21,100
suppressing negative search suggestions for Hillary Clinton. Now, when we went public with that,

647
00:55:21,100 --> 00:55:25,420
and others went public with that, so it wasn't just us, but when we went public with our findings,

648
00:55:25,420 --> 00:55:30,540
Google flipped the switch. They literally just turned off the manipulation, just like that.

649
00:55:31,420 --> 00:55:35,980
And from that day on, you could start to see negatives when you did searches for anything

650
00:55:35,980 --> 00:55:40,460
related to Hillary Clinton. But June, July and August, it was virtually impossible to get any

651
00:55:40,540 --> 00:55:45,660
negative search suggestions. We've learned that when there's a negative in that list of four,

652
00:55:46,300 --> 00:55:54,540
it can draw 10 to 15 times as many clicks. And the more undecided someone is on an issue,

653
00:55:54,540 --> 00:56:01,100
the more clicks the negative draws. So one of the simplest ways to manipulate people's opinions

654
00:56:01,100 --> 00:56:06,140
invisibly is through the differential suppression of negative search suggestions. That is to say,

655
00:56:06,540 --> 00:56:10,700
suppress the negative search suggestions for the position I'm supporting,

656
00:56:10,700 --> 00:56:14,940
and I allow negative search suggestions to appear for the other position, the one I'm not supporting.

657
00:56:16,300 --> 00:56:22,380
And what we're now doing is quantifying what that does to people's searches and what that does

658
00:56:22,380 --> 00:56:26,620
to people's opinions and voting preferences. We're doing that right now. But we even figured

659
00:56:26,620 --> 00:56:33,980
out that number four, because it turns out that if there is a negative in the list,

660
00:56:34,940 --> 00:56:39,340
and I, as I add more and more alternatives to that negative, because I want people clicking

661
00:56:39,340 --> 00:56:42,700
on that negative, believe me, I'm allowing that negative to be there because I want people to

662
00:56:42,700 --> 00:56:47,900
click on it and I know it attracts attention and I know it attracts clicks. But the more alternatives

663
00:56:47,900 --> 00:56:55,420
I add, the fewer people will click on the negative. It dilutes the impact of the negativity bias.

664
00:56:56,060 --> 00:57:02,860
Right? Now, by the same token, I want to keep adding more items to my list. Why?

665
00:57:04,780 --> 00:57:10,700
Because I don't want people finishing their own search term. I don't want them doing that.

666
00:57:10,700 --> 00:57:16,380
So on the one hand, I want my list to be long. On the other hand, I want it to be short. Well,

667
00:57:16,380 --> 00:57:22,780
it turns out those two distributions overlap perfectly with one optimal value.

668
00:57:25,420 --> 00:57:33,100
Guess what it is? Four. Four is the magic number. We didn't know if this was going to come out of

669
00:57:33,100 --> 00:57:40,700
our data, but it popped right out. Okay. So we're learning more and more about how these things

670
00:57:40,700 --> 00:57:51,180
work. Now we get to the cereal. You know, all this stuff, even the experiments, even the experiment

671
00:57:51,260 --> 00:57:57,580
in India, in some sense, it's all hypothetical, isn't it? Because you don't know what people

672
00:57:57,580 --> 00:58:02,460
are really seeing. I mean, to see what people are really seeing, I'd have to creep up behind

673
00:58:02,460 --> 00:58:07,900
Eugene like this and I'd have to look over his shoulder and go, are there any European nations

674
00:58:07,900 --> 00:58:14,780
that we're practically unaffected by the two world wars? Now I know because I crept up on him

675
00:58:14,780 --> 00:58:20,060
and I looked over his shoulder and wouldn't we have to do that to really see what people are seeing?

676
00:58:21,900 --> 00:58:29,260
Okay. So I tell the story, the full story. It's coming out in a couple months in a piece called

677
00:58:29,260 --> 00:58:34,780
Haming Big Tech and I recommend it to you highly because I'm told by friends who read it that it

678
00:58:34,780 --> 00:58:41,100
reads like a spy novel. And I tell the story of what this crazy thing we did. So now we're in the

679
00:58:41,100 --> 00:58:51,980
surreal realm here. Starting in late 2015, early 2016, we set up a Nielsen-type network of field

680
00:58:51,980 --> 00:59:02,220
agents around the country. All these people were recruited in a clandestine manner. We took incredible

681
00:59:02,220 --> 00:59:08,460
steps to make sure that they could not be identified, which Nielsen does too. Nielsen does the same

682
00:59:08,460 --> 00:59:13,020
with the families they used to rate television shows. They've been doing that since the 1950s.

683
00:59:14,380 --> 00:59:21,660
So we recruited these people. We developed a custom add-on for both Firefox and Chrome

684
00:59:22,620 --> 00:59:27,980
that all of these are field agents installed on their computers. That gave us control over

685
00:59:28,700 --> 00:59:33,020
information that we would be collecting automatically when they conducted searches.

686
00:59:33,580 --> 00:59:39,580
In particular, searches using any one of 500 different election-related search terms

687
00:59:39,580 --> 00:59:43,580
that we control. We control that list. Sometimes we could collect whatever we wanted.

688
00:59:44,140 --> 00:59:47,900
But as it happens, we were only collecting information about election-related searches.

689
00:59:48,860 --> 00:59:55,500
And we got our first data starting to come in on May 19, 2016, and we kept going up to the

690
00:59:55,500 --> 01:00:03,100
election. And as we worked out the kinks in our system, the rate of data flow increased. And

691
01:00:03,100 --> 01:00:15,260
ultimately, we preserved 13,207 searches on Google Bing and Yahoo and the 98,044 web pages to which

692
01:00:15,260 --> 01:00:24,140
the search results linked. And of course, we knew what search positions the links were appearing in.

693
01:00:25,500 --> 01:00:31,180
So in other words, we had the ability to determine. We were not looking over the shoulders of our field

694
01:00:31,180 --> 01:00:36,300
agents as they were conducting searches and preserving their actual search results and

695
01:00:36,300 --> 01:00:40,700
preserving the web pages to which all 10 search results on the first page linked.

696
01:00:41,980 --> 01:00:47,500
So this had never been done before, apparently. And it was tremendously exciting. And it was very

697
01:00:47,500 --> 01:00:53,580
nerve-wracking. We then used crowdsourcing to determine whether the web pages were

698
01:00:54,380 --> 01:01:00,940
favored Hillary Clinton or Donald Trump. And we concluded that, in fact, for roughly,

699
01:01:00,940 --> 01:01:10,140
for merely six months before the election, Google's search rankings were biased in favor of Hillary

700
01:01:10,140 --> 01:01:18,140
Clinton. We also determined that the bias in Google's search results was larger than the bias in

701
01:01:18,140 --> 01:01:23,740
Yahoo's search results, which was much, much smaller. And of course, the fact that they

702
01:01:23,740 --> 01:01:27,500
have a bias shouldn't be too surprising since they're pulling almost all of their

703
01:01:27,500 --> 01:01:34,460
search results from Google. And then what about Bing? Well, it turns out we couldn't use our Bing

704
01:01:34,460 --> 01:01:40,380
data. There were a bunch of data we couldn't use. Why? Because some of our field agents were commuting,

705
01:01:41,100 --> 01:01:47,900
we're communicating with us using Gmail. We deliberately recruited a few field agents

706
01:01:47,900 --> 01:01:53,500
deliberately who used Gmail because we knew that if Google took an interest in what we were doing,

707
01:01:54,300 --> 01:01:58,780
it would be very easy for them to identify those people. So that was kind of our control group.

708
01:01:59,740 --> 01:02:02,300
So that brings me finally to the one slide I'm going to show you.

709
01:02:02,300 --> 01:02:07,340
Now, is this real or surreal?

710
01:02:18,620 --> 01:02:25,900
This is showing you over a 25-day period before November 8 and including November 8.

711
01:02:25,900 --> 01:02:31,820
This is showing you the bias, if any of those points above the line is showing bias

712
01:02:32,620 --> 01:02:37,180
or favoritism for Hillary Clinton, this pro-Clinton, below the line that would be

713
01:02:38,380 --> 01:02:44,540
web pages favored Donald Trump. So this is showing you 25 days before the election

714
01:02:45,340 --> 01:02:52,220
and you see there are pretty clear favoritism for Hillary Clinton in search results. And by the

715
01:02:52,220 --> 01:02:56,140
way, it said nothing to do with the search terms because if you look at the search terms people

716
01:02:56,140 --> 01:03:03,180
were using, the search terms actually slightly favored Donald Trump. So this was not an effective

717
01:03:03,180 --> 01:03:13,260
search terms. This is an algorithmic effect. And now this is what's cool. These are all

718
01:03:14,140 --> 01:03:21,500
non-Gmail Google users. What about that control group we had? What about the Google users who

719
01:03:21,500 --> 01:03:25,020
were also communicating with us during all these months using Gmail?

720
01:03:34,860 --> 01:03:44,700
Now, to my eye, those graphs look different. Statistically, those numbers are dramatically

721
01:03:44,780 --> 01:03:52,060
different. At the point 001 level, they're dramatically different. You can draw whatever

722
01:03:52,060 --> 01:03:58,220
conclusions you like regarding why we got this difference, but what this tells me is when you're

723
01:03:58,220 --> 01:04:03,340
going to conduct a study like this, you should be very cautious about how you conduct the study.

724
01:04:04,620 --> 01:04:10,540
What we realized at the end of all this was not so much that our numbers really were very

725
01:04:10,540 --> 01:04:18,620
important. What we realized is that we have the ability now to look over people's shoulders as

726
01:04:18,620 --> 01:04:26,460
they're looking at Tinder and they're swiping as they're using Facebook, as they're looking at

727
01:04:26,460 --> 01:04:31,660
Facebook feeds, as they're looking at not just search rankings, as they're looking at search

728
01:04:31,660 --> 01:04:38,860
suggestions, you can use the same add-on technology that we successfully developed here in this project

729
01:04:39,580 --> 01:04:44,700
to look over people's shoulders around the world. You can scale up what we did

730
01:04:45,660 --> 01:04:52,620
and set it up in country after country after country. When we realized that this was possible,

731
01:04:52,620 --> 01:04:58,380
what we really had here, a way of keeping tabs on these big tech companies and what they're

732
01:04:58,380 --> 01:05:04,620
showing people, then I called up some people I knew, including one of these guys over here

733
01:05:05,420 --> 01:05:10,860
and a guy some of you know named Dennis Allison and some other nice folks, Jake Shapiro from

734
01:05:10,860 --> 01:05:17,900
Princeton University and Martin Moore from King's College London and on and on and on. The list now

735
01:05:17,900 --> 01:05:23,260
is growing and growing and growing of people at major institutions who've become part of a group

736
01:05:24,220 --> 01:05:28,860
that is working to set up a new organization. It's called the Sunlight Society.

737
01:05:29,820 --> 01:05:40,780
And the Sunlight Society will serve as a kind of monitor of technologies that are being developed

738
01:05:41,340 --> 01:05:47,740
which could in fact influence people's behaviors, people's opinions, people's votes,

739
01:05:47,740 --> 01:05:54,860
people's purchases, perhaps without them even knowing. This kind of system, whether we do it

740
01:05:54,860 --> 01:06:01,180
successfully or not, it needs to exist. There's definitely a need for this at this point because

741
01:06:01,740 --> 01:06:06,700
everything that I've been saying up until just a few minutes ago was all hypothetical, wasn't it?

742
01:06:07,820 --> 01:06:13,900
This is not so hypothetical anymore. This is much closer to real or even surreal. This is weird.

743
01:06:17,100 --> 01:06:24,300
You can use this technology to look at demographic effects, to look at what these

744
01:06:24,300 --> 01:06:34,380
companies are showing people, how individualized these stimuli are that people are being subjected

745
01:06:34,380 --> 01:06:41,820
to. You could look at anything that people are seeing on their screens, have it instantly transmitted

746
01:06:42,940 --> 01:06:49,340
to servers which is what we did, have your servers do an analysis and we're now working on

747
01:06:49,340 --> 01:06:54,860
automating the analysis of bias ratings, for example. And all of that stuff can be automated.

748
01:06:54,860 --> 01:07:02,380
You can train algorithms to evaluate text in much the same way that humans evaluate text

749
01:07:02,380 --> 01:07:06,780
and those algorithms are getting better and better and better. And in fact, both Google and Facebook

750
01:07:06,780 --> 01:07:12,140
right now are using algorithms like that to try to identify fake news stories. The point is you

751
01:07:12,140 --> 01:07:21,980
could be collecting data in real time on many different platforms, analyzing the data in real

752
01:07:21,980 --> 01:07:30,460
time and finding the problems maybe before they get out of hand. You could share these findings

753
01:07:30,460 --> 01:07:36,060
as appropriate with the media. You could share them as appropriate with law enforcement agencies.

754
01:07:37,020 --> 01:07:45,260
Courts, I had a conference call a couple of weeks ago with the two top investigators in the three

755
01:07:45,260 --> 01:07:52,940
antitrust actions that the EU has brought against Google. They're very interested in this kind of

756
01:07:52,940 --> 01:07:59,500
technology because the evidence they have to support some of the claims that they have made

757
01:07:59,500 --> 01:08:08,620
against Google is actually pretty weak compared to what we have. So I think we're there. We've

758
01:08:08,620 --> 01:08:15,900
gone from hypothetical to a little bit more real, but still somewhat hypothetical, to a lot more real

759
01:08:15,900 --> 01:08:24,220
and then the possibility of really finally being able to make companies like this accountable to

760
01:08:24,220 --> 01:08:31,100
the public. If this interests you and you'd like to help, you know, join this effort,

761
01:08:31,900 --> 01:08:36,700
again, whether we do it successfully or not, we know it's going to happen and we know it needs

762
01:08:36,700 --> 01:08:51,740
to happen. And that's my story. Thank you. So, given the two examples that gave of Facebook and

763
01:08:51,740 --> 01:09:00,940
Google and given the powerful effect, I'm sorry, and Tinder and given that this effect is so powerful

764
01:09:02,060 --> 01:09:10,620
as you state, why did you do it from when? Well, he won because of the peculiarities of the electoral

765
01:09:10,620 --> 01:09:18,300
college and the American people didn't choose him. I mean, Hillary Clinton won by almost 2.9

766
01:09:18,300 --> 01:09:24,620
million votes. She won the popular vote. Maybe in California with nothing about it. That's irrelevant.

767
01:09:24,620 --> 01:09:31,020
I mean, if we had a direct vote kind of system, which they have in many countries, then she would

768
01:09:31,020 --> 01:09:40,300
have won. You know, the analysis of that election, and people are going to be analyzing that election

769
01:09:40,300 --> 01:09:46,860
for a hundred years, and we all know there's a long list of reasons why Donald Trump won.

770
01:09:48,300 --> 01:09:52,620
But I will tell you, and I guess it's on the record because I'm being recorded, I will tell you

771
01:09:52,620 --> 01:09:57,660
that then I'm a friend of some people in the Trump family, and then I was in touch with them on

772
01:09:57,660 --> 01:10:04,540
election eve, and that there came a certain moment in time, I won't tell you the exact time,

773
01:10:05,580 --> 01:10:13,660
where I got a text, and the text said, we are all shocked here. And when this person said we,

774
01:10:14,460 --> 01:10:26,700
this person meant we. See what I'm saying? Not only did they not believe they were going to win,

775
01:10:27,580 --> 01:10:34,940
I personally don't think based on, again, my personal knowledge of some of the people involved,

776
01:10:34,940 --> 01:10:40,540
I personally do not believe he had any intention of becoming president, which is why he is still

777
01:10:40,540 --> 01:10:50,700
looking very much like a deer in the headlights. He was trying to increase his celebrity status,

778
01:10:50,700 --> 01:10:56,940
he was trying to lay the foundations for setting up a huge media network, and he was putting all

779
01:10:56,940 --> 01:11:03,260
those pieces in place. And this is not what these people had in mind, which is why they're kind of

780
01:11:03,260 --> 01:11:10,940
all scarring around, and there's just complete chaos, and you know. So he won because of what

781
01:11:10,940 --> 01:11:15,580
historians will tell us 50 years from now, that's why he won. See what I'm saying?

782
01:11:18,220 --> 01:11:23,740
Well, you know, the question is, again, hypotheticals, right? The question is,

783
01:11:23,740 --> 01:11:29,180
was Google using, and was Facebook using these manipulations to the full extent that they had,

784
01:11:29,180 --> 01:11:34,060
that was possible? And we have reason to believe that they were not. I mean, we know, for example,

785
01:11:34,060 --> 01:11:39,340
that Google turned off that negative search suggestion manipulation in early September.

786
01:11:39,340 --> 01:11:45,340
We know they just turned it right off, like that. So I think that these companies that,

787
01:11:45,340 --> 01:11:52,300
you know, behind the scenes, or in some cases very openly were, you know, wanted Hillary Clinton

788
01:11:52,300 --> 01:11:58,380
to win and were supporting her in all kinds of ways. I mean, Dustin Moskowitz, am I pronouncing

789
01:11:58,940 --> 01:12:04,540
that correct? One of the co-founders of Facebook, he donated just a couple of months before the

790
01:12:04,540 --> 01:12:11,100
election. He donated $25 million to the Democrats. So, you know, these companies, and the people who

791
01:12:11,100 --> 01:12:14,780
worked for these companies, they were very strong supporters of the Democrats. But I think, number

792
01:12:14,780 --> 01:12:22,700
one, I think that they held back a little bit on the manipulations. And number two, I think that

793
01:12:22,780 --> 01:12:30,140
they were just overconfident. I think they were overconfident. The polls said consistently that

794
01:12:30,140 --> 01:12:34,300
she had it in the bag. And I think they just got overconfident. And maybe we're being a little

795
01:12:34,300 --> 01:12:38,220
cautious. And I don't think that they used all the tools that they had available to them.

796
01:12:40,380 --> 01:12:47,740
But that's why you have to have a monitoring system in place. Because the historians are

797
01:12:47,740 --> 01:12:53,580
just going to be speculating. We don't need to speculate. We can monitor. We can track. They

798
01:12:53,580 --> 01:13:03,260
track us. We can track them. It's that simple. And then this won't be speculation. Why is this

799
01:13:03,260 --> 01:13:07,820
all speculation? There was an article that came out in The Guardian, which has done a very, very

800
01:13:07,820 --> 01:13:12,860
good series on high tech and very skeptical about, you know, what high tech is serving up to the

801
01:13:12,860 --> 01:13:19,740
world. It was a very good piece in early December talking about the Brexit issue. And this was by

802
01:13:19,740 --> 01:13:26,540
Carol Cadwalader. I've spoken with a number of times. Very, very, very, very smart journalists

803
01:13:26,540 --> 01:13:33,020
and good investigative journalists. And in this article, she laments the fact that what

804
01:13:34,380 --> 01:13:39,740
people in the UK were seeing on their computer screens, you know, back in June when the Brexit

805
01:13:40,060 --> 01:13:46,860
book occurred, that it was all lost, that we can't know what Cambridge Analytica was showing people.

806
01:13:46,860 --> 01:13:51,180
Because it's all lost. It's all ephemeral, right? Most of what we see on screens is ephemeral,

807
01:13:51,820 --> 01:13:56,460
especially when we're generating search results. That's ephemeral. It exists for a couple seconds,

808
01:13:56,460 --> 01:14:01,660
and it has an impact on us, and it goes away, and it's gone. So what she was saying was,

809
01:14:01,660 --> 01:14:05,980
if only, if only we could go back in time and see what people were seeing on their screens.

810
01:14:06,780 --> 01:14:14,940
Okay? We could do that. I have an appointment on Friday with folks from the Internet Archive,

811
01:14:14,940 --> 01:14:21,660
which is not far from here, which is, you know, Brewster Kale's project. And, you know, they've

812
01:14:21,660 --> 01:14:25,340
been following what we've been doing since almost the beginning, because at some point the Internet

813
01:14:25,340 --> 01:14:31,660
Archive is going to post our database for everyone to, you know, pour through. But I mean, that's

814
01:14:31,740 --> 01:14:36,620
what we need. We need organizations like the Internet Archive working with people who develop

815
01:14:36,620 --> 01:14:42,620
monitoring systems, an organization like the Sunlight Society that not only develops and scales

816
01:14:42,620 --> 01:14:47,500
up these systems, but that looks around the world for other people developing similar systems,

817
01:14:47,500 --> 01:14:52,940
and kind of coordinates, coordinates the efforts. And this, that we don't have to speculate. We'll

818
01:14:52,940 --> 01:14:59,420
know what's happening. And this could result in something wonderful. It could actually get

819
01:15:00,060 --> 01:15:05,900
some of these manipulations to disappear. It's possible if a good monitoring system were in

820
01:15:05,900 --> 01:15:10,460
place that some of these companies who are doing some of these crazy things, think about swipe the

821
01:15:10,460 --> 01:15:17,740
boat, will stop because they'll realize this is being recorded. This is being recorded.

822
01:15:21,900 --> 01:15:27,020
Yes, what is your name? Brad. Now, are you one of the famous or infamous actual students?

823
01:15:27,740 --> 01:15:32,300
I guess I am. Wow. Well, I will say I have not been here for that many times.

824
01:15:33,420 --> 01:15:38,300
Well, thank you for coming, person. Thank you. Yeah, this is definitely quite interesting. I'm

825
01:15:38,300 --> 01:15:41,900
like curious to see more of this data and the need for it is like, it's definitely very clear.

826
01:15:42,540 --> 01:15:46,540
It's less clear to me like how we can actually be interpreting these results

827
01:15:47,660 --> 01:15:52,300
right now anyway, without having more of it. Just because like for one difference right off the

828
01:15:52,300 --> 01:15:57,020
bat with like gmail users, you're always logged into your Google search results. Like that's

829
01:15:57,020 --> 01:16:01,180
a completely different set of personalization and development going on at Google, right to be

830
01:16:01,180 --> 01:16:05,820
showing you that you know which results those are. Right. So that's just like already a difference

831
01:16:05,820 --> 01:16:09,500
there. And then just thinking about how do you measure I think the hardest part there more

832
01:16:09,500 --> 01:16:12,940
fundamentally is like how do you measure what is a bias set of search results. Let's see, Brad,

833
01:16:12,940 --> 01:16:17,020
I can see this in your head. I can see the gears because you're not just asking these questions,

834
01:16:17,020 --> 01:16:19,900
you're answering them in your head at the same time that you're asking the questions.

835
01:16:20,620 --> 01:16:28,220
Deny that. The first. Deny it. Yes or no. I think I have two different questions. One I have like

836
01:16:28,220 --> 01:16:33,260
one imagine to answer for which is the personalization components. Yeah. Which is,

837
01:16:33,260 --> 01:16:37,900
but I think I could be dissuaded at that one. Right. But the second one. It's easy, right?

838
01:16:39,500 --> 01:16:44,780
You know how to do that, right? Oh, how to change the data. Oh, I'm saying though that you know how

839
01:16:44,860 --> 01:16:50,140
to track the personalization. It's very easy. Yeah. I'm just saying it's harder to get. I can

840
01:16:50,140 --> 01:16:53,980
also imagine coming up with algorithms that seem very neutral and are tempted to be very

841
01:16:53,980 --> 01:17:00,460
neutral that would give identical effects. And that's why I think it's very interesting to study

842
01:17:00,460 --> 01:17:05,340
more and see if you can like tease out and but B, I'm not ready to like leave to the conclusion

843
01:17:05,340 --> 01:17:10,460
that Google is manipulating its results to influencer. I don't know what they're doing,

844
01:17:10,460 --> 01:17:17,100
but I know how to track what they're doing. And I'm learning more and more about how to

845
01:17:17,100 --> 01:17:22,060
automate the analysis of the data that we're collecting. And I know how to set up systems

846
01:17:22,060 --> 01:17:27,900
like this. So I think one thing I am curious about is how do you figure out what's a bias

847
01:17:27,900 --> 01:17:34,380
set of search results? Well, we just thought we that's a very good question. This question

848
01:17:34,380 --> 01:17:40,300
comes up all the time. And I sometimes I regret using the term bias because it's a loaded term.

849
01:17:40,300 --> 01:17:45,820
And I'm not using it in a loaded way, believe it or not. I realized bias sounds like prejudice

850
01:17:45,820 --> 01:17:51,820
and things like that. And I don't I'm using it in the way psychology researchers use the term,

851
01:17:51,820 --> 01:17:59,100
which is that that favors one perspective over another. And so when we have when we do use

852
01:17:59,100 --> 01:18:05,740
crowdsourcing to rate, you know, whether a page is pro one candidate or another, we just we give

853
01:18:05,740 --> 01:18:11,180
them an 11 point scale goes from five to zero to five, right? And here's candidate a and here's

854
01:18:11,180 --> 01:18:16,460
candidate b. And we say, read the webpage and just tell us whether it favored and on this 11

855
01:18:16,460 --> 01:18:21,260
point scale, whether it favors candidate a or candidate b. So bias is an unfortunate term.

856
01:18:22,460 --> 01:18:27,020
The point is you can you can take terms like that operationalize them until you're satisfied.

857
01:18:28,060 --> 01:18:32,940
So I don't think that's what the problem is. I don't even think that bias per se is the problem.

858
01:18:32,940 --> 01:18:39,340
I think we're talking about a whole new world that is emerging. And this is I'm actually working

859
01:18:39,340 --> 01:18:44,700
on a book on this subject trying to think ahead 10 or 20 years. Wow, is that impossible these days?

860
01:18:45,340 --> 01:18:51,420
But I think a whole new world is emerging in which effects of the sort that I've been telling you

861
01:18:51,420 --> 01:18:56,700
about are simply going to multiply. And so, you know, it's going to be a game of catch up. And

862
01:18:56,700 --> 01:19:01,180
that's one of the reasons why you have to have monitoring systems in place, because even if you

863
01:19:01,180 --> 01:19:07,420
don't understand how those apparently neutral kinds of stimuli, right, but that you mentioned,

864
01:19:07,420 --> 01:19:10,620
even if you don't understand how that's being used to manipulate people,

865
01:19:10,620 --> 01:19:14,380
well, if you at least if you capture the information, you can go back and analyze the

866
01:19:14,380 --> 01:19:19,180
crap out of it. And maybe you can figure it out. And I think that's the world that we're headed

867
01:19:19,180 --> 01:19:25,420
toward one in which new technologies, I mean, imagine how Google Home could be used to manipulate

868
01:19:25,900 --> 01:19:31,820
or the new product that Apple just announced. But Apple has never had this motive, by the way,

869
01:19:32,460 --> 01:19:38,780
because Apple has a different business model. Apple actually sells products. Microsoft actually

870
01:19:38,780 --> 01:19:42,700
sells products. I realize more and more companies are moving in the direction of Google's business

871
01:19:42,700 --> 01:19:48,540
model. But Google doesn't sell any products. Not really. More than 90% of their revenue is

872
01:19:48,540 --> 01:19:56,780
still advertising revenue. They're they use cool looking data collection platforms to collect data,

873
01:19:57,340 --> 01:20:01,820
and then they leverage that data to make this this year, they're going to make over they'll

874
01:20:01,820 --> 01:20:09,740
have revenues of over $100 billion. So Google is still the place you have to watch. And

875
01:20:10,620 --> 01:20:18,620
secondarily, Facebook, Google currently controls five out of the six billion platform

876
01:20:20,300 --> 01:20:24,220
applications in the world. And there's only one left. And that's Facebook controls the other one,

877
01:20:24,220 --> 01:20:29,180
which is social media. But what I'm saying is Google, you have to keep an eye on Google,

878
01:20:29,180 --> 01:20:32,460
but they're going to be other companies. It's not just Google. They're going to be other companies

879
01:20:32,460 --> 01:20:37,900
doing other things that have never been done before. You know, these these effects, we're

880
01:20:37,900 --> 01:20:43,660
now studying four effects that have never existed before in human history, completely

881
01:20:43,660 --> 01:20:51,580
unprecedented, almost entirely invisible, with which produce enormous shifts in people's thinking.

882
01:20:53,020 --> 01:20:56,220
Our if we're if we have identified and are studying four,

883
01:20:59,020 --> 01:21:00,460
what do you think, could there be five?

884
01:21:00,780 --> 01:21:09,340
How many are there? I don't know. You know, we've we've found and are studying four.

885
01:21:10,060 --> 01:21:13,900
So that does that mean they're actually 10? Does that mean they're actually 100?

886
01:21:13,900 --> 01:21:19,180
I don't know. But I do know this. Next year, there'll be more of those kinds of effects than

887
01:21:19,180 --> 01:21:29,180
there are this year. Hal? So let me let me shift to the advertising world. A product is to try and

888
01:21:29,260 --> 01:21:34,060
give us some of the emotions associated with politics. There's an incredible battle between

889
01:21:36,940 --> 01:21:41,980
search engines trying to give people what they want, and advertisers trying to

890
01:21:43,100 --> 01:21:48,060
bump their stuff up there, whether they deserve it or not. Yes, that's right. So how do you decide

891
01:21:48,060 --> 01:21:55,740
what's fair in this war? And in terms of if I translate this back into politics, how am I going

892
01:21:55,740 --> 01:22:03,100
to decide what's fair in that world? You know, is your crowdsource evaluation? I can tell you

893
01:22:03,100 --> 01:22:09,340
what I am, and I can tell you what I'm not. Okay, I'll start with the knots. I am not a lawyer.

894
01:22:09,340 --> 01:22:17,180
I'm not a public policymaker. I am not a thought leader. I am not. You know what I am? I'm a

895
01:22:17,180 --> 01:22:21,980
researcher. I'm a really, really good researcher. The more I've done research over the years,

896
01:22:21,980 --> 01:22:28,300
the more I realized I'm good. I know how to figure these things out, and I love doing it. And that's

897
01:22:29,340 --> 01:22:33,980
about as far as I could go. No, I think that's cool though. I think that's pretty far. I think

898
01:22:33,980 --> 01:22:42,540
you're on a wonderful project. Okay. The one thing you haven't mentioned yet is keeping track of the

899
01:22:42,540 --> 01:22:49,980
stuff that wasn't displayed so you know when you try and go back a year and evaluate something

900
01:22:49,980 --> 01:22:56,940
as to whether, you know, the stuff that you got was the right sample. Well, that's why we want to

901
01:22:56,940 --> 01:23:03,420
scale up the kind of thing that we did. If you scale it up large enough, you can keep track of

902
01:23:03,420 --> 01:23:08,220
all kinds of stuff. Look what Brewster Cale's organization does. You've heard of the Wayback

903
01:23:08,220 --> 01:23:16,620
Machine? I mean, the Internet Archive takes snapshots of the entire Internet pretty much.

904
01:23:17,340 --> 01:23:20,860
I don't know if they do the dark net, but I mean, at least, you know, the Internet we most of us

905
01:23:20,860 --> 01:23:26,860
can see. So, I mean, if you have the resources, you could capture lots of different, and what is it

906
01:23:26,860 --> 01:23:32,540
we want to capture ephemeral information? That's what we want to capture. That's what is normally

907
01:23:32,540 --> 01:23:38,780
completely lost. Fantastic. Yeah. And what I'm saying is we can capture ephemeral information.

908
01:23:38,780 --> 01:23:44,940
Hold on to it. Analyze it now or analyze it later.

