{"text": " So this is E380 for May 10th, 2023. And our speaker is Ken Kahn, who is retired in Oxford after being at Oxford. And soon we'll be back at Oxford. Ken was back in the Dark Ages at Xerox Park for a bit. And there he worked on a project which became a lifetime session, I guess, called Toon Talk, which was the first really configurable, programmable animation system. And in its day, golly, I hate to think of how long ago it was. It was really the most amazing thing. And rather than go the commercial route and build video games, it can keep poking away at the methodology and the underlying structures of which you tell stories through pictures and voice. He recently did some work collaborating with the chat GPT-4. And unlike a lot of people who looked at it as using it as a tool, he appears to believe, as I do, that chat GPT-4 is just another programming companion, a worker, a colleague, and used it in a very interesting way. So I'm going to let you go on from there and feel free, by the way, if you're in the audience, to interrupt with questions and so forth. And if it gets to be a diversionary thing, we'll either accept it or not. Go on, Ken. Good. Let me move this. The Zoom stuff was sort of in the way. All right. Thanks, everybody, for coming and feel free to interrupt, ask questions at any time. And this is what I'll be talking about is some experiments I've done trying to create apps, just having conversations with a language model. But first, maybe I'll give you a tiny bit of background. So I was doing good old-fashioned AI for about 20 years first at the MIT Lab and then Sir Ox Park and so on. And then I got into the idea of creating a programming language for children that was completely based around animation. There was no text involved. And then in the middle of that, I also got involved in Oxford, in a big project trying to make it possible for people to make agent-based simulations of, you know, epidemics or ant colonies or whatever. And then I got involved in 2016 in a big European research project where my role was to make it possible for the students, and these were 11 to 14 world to be able to incorporate machine learning in their projects. So I built upon the SNAP programming language, which is more adult version of Scratch, and added lots of blocks for doing computer vision or natural language processing or to be even able to just create neural networks and train them right there in the browser. And that was my focus until ChatGPT came along. And I've been focused on treating ChatGPT as a colleague here. So I'm going to give a really detailed story about one conversation I had with ChatGPT, which involved building a fireworks application. And then I'm going to touch on some of these other examples that I've also done with ChatGPT. And I keep saying ChatGPT, and I started it with the older one, but as soon as I had access to GPT-4, I only used the GPT-4 version of it, which is just so much more capable than any of the other ones. And I've done attempts to see if I could do it with Microsoft Bing or Google's Barred or Hugging Chat or Claude from Anthropic or Pi, which just came out. And all of them did pretty poorly, though I did just read that today Google said they're going to be announcing a new version of Barred that's more capable in generating code. So maybe it would be an equally good alternative. And I didn't really explore very carefully the older version of ChatGPT. So the first example I'll give you is where the, I asked ChatGPT to be more like a tutor and I myself pretended to be a student. But then the other six examples, it's much more of pair programming where the chatbot is really doing all of the programming. And I'm kind of the navigator kind of saying, oh, we should try this or that's not working or what about this problem. I also have to just because of the current technology do a little bit of copy and paste, but that's very mechanical and not very interesting. So first thing to understand is that these are much more than just code generators that they, let me move this. They could, just by their nature will, will not just give you code but give you some explanations of what's going on in the bigger picture and how that works. And then if you ask, it'll always put some comments in the code, but if you don't really understand some function, you could just ask it to comment it and it'll, every little section, it'll put in comments and not just sort of meaning, I mean, redundant comments, but ones that are actually quite helpful. And the other thing is it's, it often makes, constructs buggy code, but if you tell it about the bugs, especially if you just copy and paste the error messages into the chat, it could fix them. And then if you ask it, and you say, okay, could you think of ways to improve this app, it'll come up with a lot of sensible suggestions. And then there's been cases where it couldn't generate some code because it was trained two years ago, and they're, it was using an API that's much newer than that. And all I did was copy and paste the documentation into the chat GPT and, you know, read it in the chat. And it could also generate documentation. And it's also important, especially when you're thinking about children using this that you could set, you could tell it from the beginning, you know, you're talking to a young child, and it'll use a different kind of language, you know, language, you know, you can use it as a, you know, a language that's much newer than that. And all I did was copy and paste the documentation into the chat GPT and, you know, read it in seconds and was able, you know, use a different kind of vocabulary and avoid more technical things and so on. So I, all of these experiments I decided to use, ask it to create a web app in JavaScript, you know, with some HTML and CSS. And the reason for that is these three reasons is that there's nothing anyone would need to install because everybody's got a browser. And browsers are so capable these days, you know, they've got speech synthesis, speech recognition, they, you know, they could easily communicate with all sorts of APIs and so on. And, and then it's so much easier for people to share, you could actually just send the index.html file on the script.js file to your friends, or you could host it any number of places for free, just as static web pages. You don't have to run a server or anything. And what professional programmers are using, I recently read a survey that said something like about 75% of people that are using large language models to help with their programming are using the Visual Studio copilot, but 20 or 25% are doing what I'm doing, which is just to have a conversation with the chatbot that can program rather than have it nicely integrated. And of course, being nicely integrated, you know, you don't have to do the copy and paste and it maybe understands more of the bigger context. It's, it's, it's, but it's much too complicated to imagine a typical school student using it. Okay, so the example that I want to start with is this fireworks one, which as I said before, this one's a little different than the other ones I'm going to talk about because I, I, I asked it from the beginning to be more like a tutor than just a colleague. As you'll see, there's myself or my persona was pretending to be this. Yeah, I needed to know something about JavaScript, but not that much. And I'll show you the result in a minute, but let's just go through what this conversation was all about. So, um, I assume everybody could read this, that's France big enough and all that. The, the, um, I, I started off by just saying you're a tutor for beginner programmers, first learning JavaScript, you provide hints and explanations, but never provide complete solutions. Instead, you nudge the learner towards solutions. And it says, sure, I'll be happy to help. And then I said, can you, how can I program a display of fireworks? And it says, oh, well, using a canvas is probably the best way to go. And then it actually comes up with this, whatever it is, eight point plan for how to actually build this, including, you know, making different classes and having an animate and so on. So this is one of the few places where this very nice tool called share GPT.com fell down, which is, it makes for a really nice way of sharing your, um, uh, your conversation with, with GPT, you just push a button and it creates a URL that has the entire thing. But for some reason this, because I copy and pasted here, this tiny little index HTML file ahead. But what it was, I'll tell you right now is simply, it said here to create a, create a canvas. So it was just basically the rear bones of a HTML file with a canvas and nothing really else. And, you know, it's, it responds always in this real nice, encouraging, friendly way that a good tutor maybe should be, you know, well, that was a good starting point of our recommending a few more things you could add these more features. And then it generates this, and you could just simply in the live version of this, I just click on copy code and then it's on the clipboard. And then I just create a folder called fireworks and I create a file called index dot HTML and just paste this into it. You know, so I don't really need to be very technically competent to, to have this kind of experience. So then, remember it said also here, that I should use this get element by ID and get context. So I'm assuming I knew a little bit here. So I said, you know, oh, I, I just started, I did these two lines of code. And it's, you know, great start and all that. So then it suggests that I size the canvas and have it adjust whenever the browser windows is resized so that you always have a canvas that's the right size. So I just again, you know, copy code and just pasted it in. But it said the next thing it said here was to create a class for fireworks. So I pretended not to know how to do that. So I said, how do I define a class? And it gives a nice explanation and gives a nice little generic example of a class and how you can make an instance. And then I actually made this mistake. I meant to say how's this and paste in my little bit of code. But it noticed that I forgot to include whether the code that it wanted me to review. So here. So then I just pasted it in. So this is, you know, my attempt to be a beginning student who kind of figured out from what it was saying that this is what, what I should do. And again, you know, it's encouraging. It's on the right track. But here's a few suggestions, you know, the convention is this should be kept. Start with a capital letter. You know, I should have a update method and maybe on a gravity be good. So, so it suggests this update and then I should define these two methods. And then I was thinking that I, you know, if I'm assuming that a student at least understands a little bit about velocity and gravity is not the same, I'm sure they could have asked chat GPT here, but, but I didn't know whether it would matter if I update the vertical position and then update the velocity or update the velocity and then the vertical position. And it says differently minimal, but most people do it this way. And it's a little bit better to do it in this order. And then it gives me the code for it. At that point, I pretend not to know how to, how to actually draw the fireworks rocket. And, you know, it says, Oh, well, you know, we could do it. The simplest thing would be to make it be a circle. And here's the code and notice it generated all these comments and so on. And then it gives a nice explanation of how this works and what it's doing. So now I have the bare minimum of this program, but I guess I'm pretending that I have no idea how to even test this. So it says, Oh, well, you could create a sample fireworks with these parameters and then create an animation loop that just continually calls update and draw. And again, it explains what's going on and how to get this to work. And then it says, you know, this code should be in the file called fireworks.js. And then just click on the index.html file in the browser. And you'll see it working. And then it doesn't working. So I was smart enough to say, Well, how could I see if there's an error message? And then it tells me about different ways I could get to the console and see what an error message is. And then I copy and pasted the error message in here. And it realized that the problem was that the JavaScript was running before the page had fully loaded. And it gives two ways of solving this. You use the Windows on load, or you could simply move the script later in the HTML. And I did the letter. So at that point, I saw the rocket go up. But it wasn't exploding yet, of course. So I didn't have a good idea how to do the explosion. And it comes up with this plan for how to do it and a basic outline of how to implement it. You know, where it makes 50 particles and they all go off in random directions and with some random speeds. And so then I thought, well, it wasn't clear if I should just, because the fireworks rocket and the particles had very similar ways of being drawn and being updated, whether I should really just copy the same code and it says, oh, well, you know, you could create a base class and then have a called moving object and then fireworks and particles could be subclasses that extend it. And again, it's just giving you the sketch of how to do it. It doesn't matter if it's filling everything for you right now. So, so I kind of follow this advice, but I didn't understand again, how I could test it, because remember the test before was just whether you could get a rocket to go up and how do I test the thing. And it comes up with, again, a nice little plan for doing it. And then it gives the sort of sketch of the code and leaves some things for me to do. And again, it explains what to do. So it was giving me explanations and I said, how about this for creating a test fireworks. And it said it looks good. And it'll create one, you know, explains what it'll do. It'll be at the bottom and somewhere, but it'll be red and initial velocity and with gravity or something. And then it's trying to be kind of, again, I think it's pretty good that it doesn't just make me feel like I'm, you know, ignorant and it's so knowledgeable. It says, well, here's a small suggestion, you know, be nice to make the colors random rather than just all of them being red and provides a very simple little function for coming up with a random color and then shows how I could use it. So I tried all that. And now remember, it doesn't have any idea with what's going on. I have to always tell it how things are going. And I said, I think the fireworks would only go about a third of the way up of the screen and blow up and they weren't using the full canvas. And it gives a nice explanation that depending on the initial velocity and the gravity value, they may not be in balance. So it suggests that I increased the velocity and decreased gravity. And I tried it and now they flew off the top of the canvas. But because it kind of explained what was going on, I just only increased the velocity and left gravity alone and it was working just fine. And it approved my changes to its suggestions. And, you know, it's saying there's no correct value, you could play with different ones, you could explore until you get something you're happy with or something. So then I ask, well, how can I add effects to this? And it explains that there's the audio API built into the browser and it can read file, I mean, play files with these extensions. And it creates a tiny little function for doing it. And it suggests that, you know, you put play sound of the launch when the fireworks are created and then you play the explosion when it explodes. And then I ask it, how would you suggest I search for the two sounds that we need? And it says, certainly there's a lot of websites with four year paid ones. And it says this one is a collaborative one with Creative Commons license sound. And then it even suggests, maybe I was up here, yeah, the keywords I searched for fireworks launch or rocket launch or fireworks explosion. So I put them in just as it suggested. And then I thought, it'd be nice to have a button that when you push starts everything up rather than it just happens. So it says, well, you could just add this button to your HTML. And then, and then in your JavaScript, get hold of that button and put a listener that and it had this idea that you don't start the fireworks until some global variable is set. And I could have just copied that but I said, you know, I understood that if all I want to do is when the buttons pushed that it starts animating why not just start animating. So I said, can't I just do that. And it says, oh, you're right that more direct and, you know, straightforward. And, you know, said don't don't add this start fireworks flag but instead, you know, is this kind of code or change the code, you know, with this aspect. And then I noticed that the the request animation frame used to be that the first thing it did and that was the last thing and I just got curious and said, does it matter whether it's in the beginning of the end and it says it can have an impact but the difference is usually subtle and explains how there's a risk that if you do it in the beginning you might be you might lead to some performance issues and so on. So everything was working and then in one of some of the suggested code there was a comment saying that you should I should implement something that would remove the particles. And I just said, you know, you said I should remove the particles but I didn't do it and everything's working fine anyways. And it explains that you know, basically, you're, I might end up consuming a lot more memory and processing on particles that are no longer visible or that can lead to making everything go slow or use a lot of memory. And it suggests a life span, but I said I'd rather have it just remove when they're no longer visible when it said sure that's reasonable and generated the condition here for when particles off the outside, not visible anymore. So everything was working but there was an annoying little scroll bars and horizontally and vertical. And so I said I see these scroll bars and it said well maybe it's because the canvas and the page on the right size and it suggested all this but actually that that's very close to what it had before. And I said it didn't help. And then it came up with some more CSS to kind of deal with it. And I said, yeah, a little bit better but because the scroll bars were less of an issue but there still were there I mean there was very little scrolling involved. So then it said well maybe we just have to remove a few pixels from the window dimensions of the canvas. And it suggested removing, you know, four pixels, and I tried it and it didn't work but when I did 10 it was fine. So everything was fine. And then I said well, you know, fireworks should happen at night it should be a black background so it just generated, you know these two lines of code here rather than clearing it. And then I said, I can't think of any other improvements can you, and it came up with what is this nine different improvements, you know, a couple of them actually were already done and it didn't seem to remember that it already randomized the colors and had a variable radius but but it could have you know we could add wind or air resistance we could have more different shapes we could have a trail, you know, more user activity, you know. So I thought well I'll just pick one of this number five. So it suggested adding, you know, an alpha value and and then the update uses it. But notice that there's this thing called global alpha and I would have thought somehow alpha itself was all. This I never heard of global alpha what is it and gives me nice explanation and how it works and why he's using it and so on. So then what when I tried doing the change that it suggested. I got this error message that that you must call the super constructor in a derived class because since I was altering the some class without calling the super. And it said oh you just have to add this line. And so that fix that problem. And I said, you know, it'd be nice if the app actually had a little note saying this was created with the help of GPT for and having a link to the log. And it created all this nice CSS and it created this div with the description and the link and so on. And then it generated all this code but it actually forgot to add the include the JavaScript script that the script element had been there and it just forgot it when I was putting this in so I just said you. But again this is just, this wasn't a problem originally this is a problem with the shared GPT that sometimes when I paste things. So they don't somehow make it into this history. So then I said that you forgot this but I added it. And then we want to remove the button after it's clicked and everything's working up if this is a problem that happens on many of the occasions which is it used the word and knit here. Well, earlier it was using the name of the function was animate. I told it, you know, I fixed it but you know. But I think because these conversations get long in the context is limited to, I don't know, I think it's up to 8,000 tokens for track GPT for and there's a 32 K version that access to but. And as a result it's kind of forgetting some of the earlier stuff that it told me and it was just guessing. And this is a common problem. I'll show you where it forgets exactly what a variable name or function name isn't in there. Generates code with a similar name with the same kind of concept but obviously one that won't work because computer programming languages need the exact same name. I'll show you the results of all of this. So if I click here. Happy. Yeah. And that took, I don't know, less than two hours I probably should have kept track of the timing, the whole experience but I thought, you know, it was a very promising kind of experience in terms of how helpful it was how much it was suggesting things how good the suggestions were, how much and understood about things and how much a good explain things. So, that's what I want to say about fireworks. But one thing that everybody's been talking about with these chatbots is how important it is to do prompt engineering and there are many contexts in which it really is important, but I've been surprised for almost all my experiments. I very, very rarely did any attempt to make good prompts, you know, I would copy and paste an error instead of saying, Well, this is the error I found in the console or so. It was the only one where I wanted it to be more of a tutor so I had this, this, sorry, prompt here but that was pretty much the only exception where I wasn't just having an ordinary conversation with it. I didn't bother to make it use speech instead of typing these things but that's straightforward to do especially because, you know, browsers have speech recognition API. Okay, so let me tell you a little bit about these six other experiments I did. Okay, so, right. So here, I wasn't playing the house key into play the role of a tutor I was treating it very much like pair programming. And I'm more than navigator suggesting things and chat GPT is the, the one actually generating the code or something. So I said how can I create a web page with a button that when clicked speaks a random integer, less than 1000 in a random language pitch rate and voice. So it, you know, told me here's some HTML. Here's the JavaScript. It had just three languages but said you can add more. And it understood all about the speech synthesis API that the, you know, HTML five, all the browsers have. And it's picking a random pitch or random rate and learning the language and reporting errors and stuff. So, so just with what the first pass like this, when I clicked on the button. I saw that there was this error or not no voices available but then if I clicked again, everything worked fine from then on. It understood exactly what was going on which is that the get voices is asynchronous and they may not have been loaded when the first time you clicked on the button but then the second time it has is loaded. So you could program around this by on voices changed so that you, you don't try to say anything until this flag has been set that the voices have been loaded. So that worked. And then I said, Oh, it'll be nice to display what the number is and the language and I said etc and then understood perfectly well that meant the pitch and the rate and so on. And also, I didn't really like the idea of having to go to the console to see an error message so could you report it some other way. And, you know, it updated the HTML updated the JavaScript. And then it was pretty ugly so I just simply said add CSS I didn't, you know, say more than that, and it generated some reasonable CSS to make it all look nicer and explaining what it was and stuff. And then, rather than have just those three languages baked into it. I said why don't you add all the languages that are available in Chrome. And I said in Chrome but it actually comes up and it says Chrome here too but it actually comes up with a solution that'll work for any browser, because it's just collecting all the languages that the browser has available, and then pulling out the language code. The languages, the voices. So, so that worked. And I had a working version of this but then I thought it'd be nice to have an additional capability in this app. So I said, add a cartoon image of a parrot and when you click on it repeats what the user says with a random language pitch rate and place. So it, you know, it has this parrot that thing and it says you know I should put replace that with my path to whatever I put a parrot image, and then it knew perfectly well how to use the speech recognition. API to pick up what somebody was saying. And that worked and I said well, give the user feedback as to what text it was actually had heard, along with the other things that were there. And and then I said, you know we had a button for speaking random number but I said, use this file again and you can see this is basically, I went to Dolly and said, you know, a colorful collection of random integers. And it generated that image and I just asked it to incorporated in the app. And then it didn't look so nice they were vertical and they were kind of small so I said make them larger and lay them out horizontally. And it had no problems doing that. And then I just thought, you know, how would you explain what this app does to a young child. It says, the app head has two pictures one of colorful numbers another parrot when you click on the picture of colorful numbers the apple. Make your computer or phone say a random number, it's like picking a number out of a hat but the computer does it for you so it's really trying to you know explain this at a to a young child pretty well I think you know it's even more fun is the number will be spoken in different language voice speed each time. So if you click on the parrot picture, the app listens to what you say just like a parrot listening to talk, and then the parrot, actually the computer pretending to be the pair will repeat which is really nice, I thought explanation. And then I said, well, can it translate what was heard. So you could use Google translate and you need an API key and here's the way to talk to that API and you should, you know, put the key here and goes on and on. And I asked, you know, so the API key shouldn't be picked in and should ask for it. I have a, I thought it might be better to use hugging face instead of Google translate so I said, how about using hugging face instead and it was perfectly fine to use hugging face. But then when I tried it as kept getting these error messages. And what was really happening was that when you ask hugging face to use a particular model. If it wasn't already loaded, it'll load it and give you back a response saying it's loading please wait an estimated, you know, 10 seconds or something. But the code wasn't paying attention to that and also I didn't really want it to wait. So, even though it, it explained what was going on and how to fix it. I just went and got a Google translate key. And so let me show you the result. So I'm not going to bother with the translate because I have to get my key and paste it and all that, but it works perfectly fine without it. So if I click here. So it picked Italian a very high pitch and low rate 408. And then if I do this, it should be listening. It should be. Oh, yeah. It should be listening to me as I speak. And again, if I if we had the API key would have translated that to French inside it. And again, I had this link to the dialogues you could actually see how this was created. I have to mention that this parrot, my two year old granddaughter just giggled and giggled and giggled when we'd say things to it and it would repeat it. And it would repeat it in a funny voice or something. And would repeat it in a funny voice or something. Anyway, she thought that was just so fun. Okay. So let me talk about the next one. I said, how can I make a web page that detects which way I'm pointing my finger and uses it to draw on a canvas. So my original concept was that if I pointed this way would draw that way if I pointed this way draw that way and so on. But it misunderstood me but actually came up with an equally good scheme that I wasn't didn't bother pursuing. So what's so interesting about this is just from this. You know, does generates all this HTML CSS but the JavaScript. Actually, I think it says it here too that it. Yeah, it's actually going to use a machine learning model that's a hand pose that'll figure out, you know, 33 different locations on your finger, you know, the each joint and so on. And knows how to load it and use it and so on and also of course how to get the video feed going into it. And then here how to load the TensorFlow.js library and then how to load the hand pose model, and how to use it and then how to use it to detect if I'm my fingers pointing upward. And this is why an interesting thing that sometimes it's generating code and just stops in the middle, or the case the explanation. And that's because, you know, it only does so much of a completion it's limited to 1000 or 2000 tokens or something but if you just say please continue it just apologizes and just starts right where pretty much where left off and, you know, continues generating code. And then I said, well, nothing's being drawn and it says oh there's some issue about the position on the canvas, and it turns out it was a bit confused about the to the coordinate system of the hand pose and the coordinate system of the canvas, but it tried to fix it and then I said well now it's only drawing in small area and it finally, you know, gets it right with some changes. And then this is this problem where it uses some variable called predictions but but it was, in this case I think it was a scoping error but sometimes it's because it had a different name and some other part of the code. So it suggests how to pass the make it available here. And then I said, well, how about if you say the color it'll change to the color that the user spoke and it will again use a speech recognition and only accepts, you know, these 10 different colors and if you say any of it, if what you said includes any of the these 10 colors then it'll switch to that color. And there's a default color of black and so on. And it turned out, it worked the first time but only the first time and it said, oops, sorry for the confusion I should be using the API bill differently and it regenerates the code. And everything's working and I said well let's display what what the last thing spoken in the current color so that people know that it's here hanging it. And then this is very nice I just said add instructions on how to use the app, and it generates, you know, the, the HTML to explain how to use it, and make some CSS to make that look nice. And then somehow the display didn't get right. Again, because I think it was forgetting what it was calling things before it was using different similar but different names here. And again the same problem that it could picked up the wrong variable name or something. So it's not the problem might have been that we were doing things before the page was loaded but that wasn't really the problem. So I said that that didn't fix it and then I said just I didn't have the same much more same problem. And that didn't help. So I had an idea what was wrong but I wasn't right either and it said, it thought maybe that was the issue. So, and also notice this problem like you know, whether it was calling it video or webcam it wasn't consistently naming things. So it was finally got it all working. And I didn't understand what happened so somehow it started to work even though these changes didn't seem, I don't know. And then there was a really minor problem that took forever to fix which was, it was showing the last thing that it heard, and then the color, all in the same line and would look nicer if they're on separate lines and it just keeps not getting it right and finally, I said, maybe it's something to do with the overlay. And then it comes up with this solution that I didn't even know about there was some fancy CSS to deal with the problem. So everything's working fine and then I said, you know, could you explain to a 10 year old how it work and once again it does a really nice job of explaining how this thing works. And let me show you how what it's like. Oh, but I have to turn off the video. Turn it off the video right. I'll be trying to load it again. So, notice that it generated these instructions all by itself and say I turned off the video. Oh here comes good. So now, if I move my finger like this and I say red, red, blue, blue, blue. It's kind of, you know, it's working. But I'm able to draw all over it with my finger. A five year old or a 10 year old who really like. Yeah, no, actually, my granddaughter, this was funny too that I can draw on it. Okay, so, but there was what I thought was interesting was, you know, had no troubles. Loading a machine learning model using it appropriately to get the hand poses just knew how to do all those sort of things. So the next example I wanted to talk about is if some, some person wanted to actually do a little bit of machine learning on some of their own data. So, I had this spreadsheet which was had just 60 examples 20 examples of some text that shows a lot of confidence 20 examples that shows a real lack of confidence and 20 that were kind of neutral. And I said how can I make a web page that would predict the level of confidence of some new text. You know, and give some suggestions and, and but oh, but, but I didn't say anything about JavaScript so it thought oh I should just do this all in Python. And then I said no no I don't want to do it in Python how could I do in JavaScript and then it says well here's how you could do it using node and I just wanted a web page. So I say, no I don't want to use node I only want the browser and so well here's how we could do it. Um, you know it's loading TensorFlow.js it creates the model, the very small model and compiles it and does the training and and then does a prediction and shows the shows what's going on. And again it stopped right in the middle of the generating I just said continue and it continued generating. So I should have these two files. And then I have to put the CSV file somewhere in that same folder or something. So it actually worked but yeah there was no feedback that it while it was training so I said can you make a graph of the training loss. And so it loads some library and it knows perfectly well how to integrate this library. With the callback of the training and it worked but the plot was much bigger than the page and it looked pretty ugly so I said it was too big and CSS I made it nice makes it look nicer. And I said, you know why don't why don't you show the likelihood scores for the predictions and it doesn't do that and it does. And then I asked how can I make this more accurate because it wasn't very accurate. And it says well you could have more data you could do some pre processing rotation you could change the model architecture, you could, you know all these things. And I said well how about you improve the model architecture and it says sure and it makes a bigger model with more layers and and then I said well maybe the user wants to explore how many epochs that give a interface so the user could enter how many epochs they want. And the graph came up after it was finished training so I said could you make the graph, the updated while it's doing the training, and it's a chair and I heard that. And then you know, finish. And then I said, could the training function was pretty complicated so I said could you have comments to it. And notice, you know for each section it says okay you know getting the inputs, preparing the data creating the model. Setting up the graph, you know, setting up the call back and it just goes on and on explaining it nicely. And then I happened to notice that it was gave very different answers depending on whether I had uppercase or not, and it understood exactly what was going on that it was using the universal sentence encoder. It was very sensitive, and it can. It can easily fix this by, of course, just taking one of the user input and sending it to lower case so there wasn't that difference. And there it was so let's see it in action. So, I'm going to say, I don't know. That's just because it. So, it says training, but it's actually coming up with the embeddings for the 60 sentences and there is that now we get the training and see the validation losses going up and so it's overfitting or something, but I could type something here like I say, predict. And it says 99.7% sure that it's confident. And I say, And then Well, didn't do so good. Should have been neutral, but thought that was maybe it's showing some confidence. Anyways, so that I thought was pretty impressive that it could do something like that. So, let's see. So, in terms of timing. I don't want to show all of these because it'll take too long but I wanted to. I think I'll show two more but I'll try to go quickly is that. Okay, so this one is using asking chat GPT to use GPT to make a conversation between personas. personas. And you know who knows how to. Well actually didn't know how to call the API. Oh, so various points I had to copy and paste the documentation from their website and then was able to do it fine. And again, it was trying to use no and I said no, no, I don't want to. And this is interesting that this is why we're using the API key in the front end that's a little bit not recommended because the key could get. Somebody might be able to get your key or whatever. Let me maybe this one. I could just, oh, I do want to show you one bug that was so funny. Yeah, here it is. So there's trying to be a conversation between Aristotle and Galileo, but what, but at this point in the progress at Aristotle says I am Galileo and Galileo says I am Aristotle I am not Galileo and then Aristotle says I am Galileo I am Galileo I'm both there so it just was totally crazy. It understood what was wrong here and how to, you know, change the roles and the right messages to the chat system, such that. Let me show you. Well actually just an interest of time I'll just show you a screenshot. So, you could enter any two people here. And then you could join in by just typing a message here and saying send. So, Galileo greets Aristotle, and they're both really polite to each other, and they appreciate each kind words and blah, and then I jump in and say well how fast objects fall and Aristotle says to Galileo, you know, the challenge seems to know things that happened every died but you know that his motion is a motion of a challenge but he still maintains the speeds proportional to the weight. And Galileo says, you know, Oh, and then if there's some weird bug words, it actually repeats this twice and I mentioned it to chat GPT but I made the mistake of mentioning that and another problem and it dealt with the other problem and I never came back to figuring out why. Sometimes it repeats. Anyways, Aristotle's respectfully disagreeing and explaining this and so it's a nice little, you know, app. And so, let me show you the most complicated one. I'll tell you real quick. Well, I'll just show you this one. This one. Flowers are all fading away and shrinking, but I could drop water balloons on them and they get bigger or something. So that's. So, you know, it's hard to keep watering all these flowers, you know, so that's that. That's what it, you know, we did with a little conversation. But this was a funny one because it actually was partially working and then in trying to add another feature it broke some of the earlier stuff and then it just wasn't it. This was about the only time when I've really had some not very good results. But when I started over again, it actually did well and also I have links here for how you know bar did trying this or something. Okay, so this is the, the really big ambitious one that I did. And it's a counter to the thing 111 exchanges back and forth. In the beginning, what I asking you to do is to create, generate a story, and I picked this because you know, Gaiman in Sandman, there's this story where this character has kidnapped amused and for punishment. It's getting so many ideas for stories that it's that he's going crazy can't think of anything other than stories and there's like a lot of crazy stories including a man who falls in love with paper dolls so I actually did all 15 that came from that story, plus some other ones that I invented. So it generates, you know, three paragraphs like, like I asked, and then I said, can you criticize them and it says oh well we could do this character development or better setting. And it's not generic ones it's saying the town the theater was only briefly mentioned maybe you could improve that or something. So it's able to apply suggestions one and four and it does that and then I say well to Chris, you know, come up with criticism for the second paragraph. So it's able to criticize its own generation, its own creations and then apply changes. So then I said put it all together into a single story, and then I said, the version of an illustration for each paragraph include the medium the artistic style mood point of view lighting and the like, and it comes up with these kinds of. So then, at that point I said well I'd like to make a webpage in which you could do this interactively. And then we get it works, but then eventually works, I should say, and this is a place where it didn't know how to use the. It's the Dolly API, but I just pasted the documentation and you know it's really just copy and paste it with all the, you know, crazy stuff that happens when you just copy and paste it still understood it well enough to generate the right code. And so then I, it goes on and on very long this one I, I, once I got it working for that I said well let's make it work for any story and let's make it so that you could criticize the illustrations and generate new ones and so on. Maybe I'll show you the final result here. Well, by the way, I even if you want to get it. An idea of how I did this without going through the entire 111 things I kind of highlighted the main things of what I did and what happened and what was going on here. And the, here it is. So we, by the way, it generated all of the instructions here. So it tells you again this is, I just said add in and instructions in the beginning with a button to close them and it explains how this whole thing works. So then, you know, needs my key there and then I say, can. I didn't try this before three, three is probably good. So one thing you could do here was I added this button that could actually show you the communications with GPT for and with Dolly so you could see what actually it's, you know, it does. So this shouldn't take more than another less than a minute I think. So this is going to have all of those features that I kind of experimented with in the dialogue so you could ask for, ask it to generate criticism you could decide to apply the criticism to rewrite a paragraph, and you could also get criticism about the illustrations. And you could even enter your own criticism of an illustration and it'll regenerate that will update the illustration prompt and then generate a new story and it should interleave the text and the images. So, one of the problems with using GPT for is that it's so much slower than 3.5 3.5 would have, you know, done this long ago but for just just as such a nicer job that I tend not to use it obviously I could have asked GPT for to make an interface where you could decide which, what role do you want to use for doing this. Maybe what I should do. Oh, here comes finally. So, I stood nervously at the podium with the prestigious Stanford University and elite seminar series in electrical engineering, he had spent countless hours perfecting his presentation and sustainable energy systems and was the culmination of his hard work as he looked at the learning of intelligent faces eager to hear what he had to say, he felt a sense of pride also mounting pressure to deliver amounts and shoes, and here I am giving the speech. And if I, throw over it we could see that it says the illustrations of watercolor painting with realistic and softly blended colors depicting the anxious Ken standing at the podium the setting as well. Lit hall with the iconic Stanford University seal. That's true. It's getting enhancing it intensity the point of view is from the audience looking. So it's a bit confused because the audience is there and it's looking at the audience but this is a dolly problem it's it's actually generated. So if I click on this. Here's the, the description. And Oh, that's interesting. Well, maybe it's still thinking, because I think it's going to, it's going to show some suggested changes to the to the thing. I don't know. So maybe I'll just say, I don't like changes. So rewrite the description. And then once it's the descriptions were written I could hit. So now it should say an oil painting was realistic when I say replace the image and generate a new one or something. And even while this is happening I could, I think I could click. Oh, here it comes. I'll close this. And there I am giving a lecture. It looks just like you. Yeah. And then if I click here, I'll get some criticism that it'll generate for the first paragraph here. By the way, we can see what the other paragraphs look like well the audience got excited here. This is the pro AI group. And then. Wow. Looks like everybody wanted my autograph at the end. It's pretty good. As he concluded the talk and any questions it was clear that he made an impact professors and students like approached him praising. This is pretty good. Anyways, if we go here. So it generates seven suggestions for how to change this paragraph. The opening sentence. Introduce more background on Ken so that the readers understand my qualifications and reasons for giving the talk. Whatever. And I could even say here you know change it from energy systems to chat. So that's the, the most ambitious. That I created with chat. So I'm practically finished here. So one question in my mind is doing this does it automate away the fun of programming. Or is it better to think of this as empowering people with very little programming skills to create all sorts of be very creative and creating apps. And see more pampered often wrote about hard fun because one of the children talked about how programming and logo was was hard fun that it was, or he probably said it was fun but hard. So, that's still the case but it's also for a lot of children they find it frustrating or too difficult and, you know, just doesn't always work to introduce programming to children. And also there's sometimes the teachers don't are much support because they don't understand very well how to program either. So the, the plus side is that by having a collaboration like this anybody with just an idea for an app that's reasonable obviously, if you wanted to make, you know, a triple A rated game or something, you're not likely to succeed. But it could come to life through this kind of process. Of course, GPT for is GPT for there will be a five and a six and a seven and eight. Yeah, exactly. That's exactly what I'm talking about the future here so the short term we could expect it just to get, you know, more competent and the GPT for can look at images and understand them, but that's only been released to a to a very small number of testers and some company that's producing software for blind people. But that's coming so you could you should be able to just like take a screenshot of of a problem with your app and then say well here's here's what what's going on and it'll interpret the image and maybe figure out how to fix things. And this copy and paste thing is, you know, obviously can be automated. And, you know, the, probably as many of you know about the, the leaked Google Docs that claims that the open source alternatives to barred and GPT for are getting so good that maybe the big players don't really have any particular edge over what's going on in the open source rule. So maybe that will be something that near term could be done but the long term who knows it's really going to keep getting better and more capable and so on. And so I, I asked Dolly to create a water color painting for the question and answer portion of the talk. And I said no text should be included in it. So, hopefully there are some questions. How do you like working with an AI as a, as a colleague. The, the first thing, the fireworks thing was the most pleasant one because it was, it was trying to be, you know, had this role more of a tutor, the, the other ones that was still, you know, really interesting but I felt like it was kind of writing this program and asking for a lot of copy and pasting, you know, and to report about errors, you know, that that was sort of my role. I mean, of course, that's not quite fair because what I also did was, and this is maybe an important aspect of this is that I always started with the simplest version of the app that I had in mind. And once I got that, I would ask for a single enhancement and once that works another one so I was, I was, I did have a, an incremental plan for how the app should get build that was purely mine and it was, you know, my assistant in that role but, but often it was more like, you know, just telling me okay here's some code replace this here's some code, you know, copy and paste this and. But it was still something exciting about the fact that you know it understood me so well and I was, and I was able to do so many things and knew all these different APIs and was better at CSS than I am for sure. It was hard, at least in my experience CSS is not transferred. So, I attended a talk at Stanford, a couple weeks ago, and they did a study of the security of the code generated I think it was by co pilot co generator, and they found that it was dismal. In fact, programmers who use co pilot produced less secure code than those who did not, but the ones who did thought their code was more secure. And the conclusion of that study was that the assistant lacks context. Why you were showing some of the examples I was just scrolling through the text and it looks like it's the same problem there that the assistant is losing context. Yeah, no, that was the other annoying thing was that it was losing context and there were a few times when I realized that it just had completely forgotten what the source code for for a part of it or so I would copy and paste something much earlier and said, you know, and here's the dysfunction or here's this HTML that we're using and that often would would help because now it was in the I wasn't thinking so much of forgetting as it seemed that in some cases, it, it misinterpreted the context of what you were asking, because it didn't know the environment in which you were working. Yeah. Well, generally, I mean like that example where with the video one where I had originally intended. Oh, I should put my video back on. I originally intended it, you know that I can point with my finger in different directions and that would indicate which way the pen would move or something, as opposed to the way it came up with. But I was just as happy with that rather than trying to get it to interpret it the way I intended, but in terms of security I thought it was pretty impressive that it really tried hard to talk me out of using the API key in a static web page like this. On the other hand, I, you know, I said, you know, make it a password field and, you know, it's, it's not really a problem that that a user is putting the key into a web page like this but of course, once it's hosted somewhere, somebody has to trust that you're not stealing their key. But of course the, the, in every one of these cases you can look at the source and the code is actually quite clear and nice. So somebody could see that the API key isn't sent off to somebody or something. In this kind of context, I don't think the security is such an issue, but obviously, if you're, if you're making some kind of web service and, you know, there's a lot more things to think about in terms of security. Yeah, I was, I wasn't so worried about security for these apps as much as the context issue. There's so much implicit context that the programmer has that the assistant does not. Right. Yeah, I mentioned it'll be quite different if I was doing some really large project. Because, you know, so the, this last one, the illustrated story generator, it did, it was a little more than 500 lines of JavaScript. So, and, and that's actually too large for the, at least the default version of chat GBT I can't say, you know, paste in this entire 500 lines and say now let's modify it in some way it's just too much context. I mean, too many tokens for it to be fit its context window. So there are these limitations of course, the, like I just saw today some startup saying that they've got a context window of 50,000 tokens. I've been, you know, using I think 8000 or something. So that, and there's even some research paper that claimed that they had a million token context that was using some clever algorithms because you know that the context normally is a quadratic complexity but if you're clever you can make it and log in or something. Well, I think I really blown away again this is this is a very nice expression to how people seem to be using the, the artificial intelligence devices or personal persona. This is as a component in a larger system. And I guess now the ads we see for, you know, doctors who use AI and get us to use AI and people who play with AI, driven games and so forth, has, has more context in terms of the current AI systems. I think that's a very useful thing. Yeah, you're back. Yeah. I think I really appreciate the introduction because I had not thought about this part of it I always thought the hard part was twiddling the knobs on the, the data used to do on the AI but talking to the AI turns out to be complex enough. Yeah. You know, if you go back a year you had to give it, you know, several examples of what you were intending and had to engineer your prompt kind of carefully to get it to be pretty competent but GPT for as I mentioned I really just seem much more of a natural conversation. And I'm really I'm curious to see how, you know, a 12 year old deals with such things, you know, better. Probably better than you and I do. Yeah, that's right. My, my, my favorite story is watching a non speaking child sort of order six months old stealing his mother's phone and the per password and then using it to play his video. This is this is adaptation at a very low rate or very high rate. Somebody's got a somebody's got a question you watching the hands up Dennis. I'm not watching it at all. When you want to you want to call people and put them up. Yeah, go ahead you got your hand up. Yeah, I was just, I had a question about the Edges of the models understanding of programming. Is there any indication or what was your experience in finding the edges of the, what the model knew about a particular API or programming in general. I mean, I was specifically thinking, I use a prototypical inheritance over the newer class inheritance. And I was just kind of curious there's probably a lot less, a lot fewer code examples of that on the, on the internet. So probably less training data for that. I was just curious if you encountered, if you looked at how the model degrades. Yeah. Well, I didn't really but I did notice something about the, it's coding styles. And it wasn't always consistent like a tiny example but maybe illustrative one is that this problem about, you know, where the JavaScript's trying to get an element from the down, but the page hasn't loaded yet. It was using sometimes this unload as a, you know, and sometimes it was using a listener to on DOM content loaded which is what I've read is the proper better way of doing it that on load is an older way that isn't as good but it's still kind of works. So, so there is this kind of. Two different issues one is whether it's on load or on DOM, DOM content load but also as to whether it's using, it's actually using at event listener, or whether it's just setting the listener directly. And it kind of wasn't consistent sometimes we'll do one sometimes the other. You know, again, because I guess most of the programs that it looked at were consistent about which way they would deal with that. It was like a real program. No, but it's just a bit of a real programmer tends to have us one style in which they stick to for those kinds of tasks, but it was I thought it was a bit odd that it was kind of flip flopping about whether it was, you know, which way would deal with that problem. So real programmers coming back six months later will not be consistent with what. Thank you. Very cool things that can do. It's a program and you mentioned the prompt engineering. Before, so that it may not require any more prompt engineering almost a row. So that is recognized usable and you also mentioned role playing. And one of the techniques. So now, there are more developing stress on techniques rather than engineering of prompt itself. But maybe from your experience of this project, what these techniques would you found helpful. Maybe you found something of your own that helped you to navigate the chat activity for Well, the problem a little bit was that I was always really the underlying motivation for a lot of this was thinking about, you know, what kind of learning materials or experience what I imagine like a high school student having. So I was rarely kind of role playing myself I was often, you know, pretending in my head, you know, to be maybe a high school student that knew a little bit about this and not much about that or something. So, maybe I'll try this soon, which is, especially if it's a younger child. I think if you set up the context better and I say, you know, you're going to be helping a nine year old who doesn't know much about programming to build the apps that they want, you know, be sure to explain things carefully and not use too much technical jar. And he's set up set up a context like that, that it would tend to, I believe it but I even tried this that it would tend to use language much more appropriate to communicate with a nine year old because it does. There's two examples of where I said, after built an app I said, what could you explain to a nine year old, or it's young child how this app would work it. I thought I did a great job of explaining it in, in, you know, the right kind of vocabulary and right kind of analogies. But I should really come up with a task that actually I want to do and just stop pretending to be a student and just see, you know, what I could accomplish as well. And I'm doing it this way is I've always been interested in kids and programming, but also there's, there's an awful lot of real programmers that are using chat GPT to get accomplished their day to day tasks so I thought this would be a more experience to think in terms of, of how children might deal with this. And when I say children of course it could be an adult that knows no programming a hobbyist or something but I mean, a non expert programmer. That's very interesting. Thank you. It's also recently I listened to podcast is Lee chasen, which was an educator who also worked now on the content strategies, and he used role playing in the beginning like you did with the child to actually create a content for the So in terms of how they can study the material because he was a teacher before, I think, teaching history, and uses this prompts like overall playing to actually create the tasks that the children would understand or certain output that would be better suited for the audience so levels of understanding so I think what you also showed here is how well it works. In terms of this first example that you show so that it really stands you nicely as if you were indeed a younger adult or child so my question may be also additionally meant to ask what other techniques you found helpful like maybe not only role playing, but something else maybe some structuring or given examples. Yeah, so part of the problem is that at the same time I was doing this I was also curious about, could it really understand me if I just say, you know, maybe, etc, and I don't really list everything out or if I say, you know, that was it didn't go high enough without giving much detail so I was kind of really wanting to explore how how well I could do if I was not being very careful to give it detailed instructions or large amount of context just to see, you know, could it still respond appropriately. One thing I should say too is that, you know, the Khan Academy has been working with chat gpt to make a tutor that they gave a demo of teaching algebra, and it was really very very good at giving hints and giving feedback when the student did things wrong, but the student said well what's the answer what's how do I solve this it'll keep saying. I'm not going to give you the answer but here's a hint or something so it was they had set up the context appropriately so it would not. You know, it would be a good pedagogical tutor and not be just a chat bot that would just give you answers when asked when asked for it but in my my case I thought. I thought that it was, I could have maybe tried to early on say something like well you're giving me too much. Code you know I want to be able to do more of it myself or something I could have done that and it would have taken me longer but it would have given me more hints and more little pieces to put together, but. It was the way I did it I think was plausible for some students that you know would be happy with these pretty high level things and some copying pasting but also some creating things on their own. Thank you. Any more questions. And thank you very, very much. I love being educated in this in this area, and I qualify as a barely marginal student, I guess. And what happens is I think that we see examples of reverse aging that the young kids are much more successful than the older people. And, you know, I'd like to claim that I know how to do things, but oftentimes don't. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 19.0, "text": " So this is E380 for May 10th, 2023.", "tokens": [50364, 407, 341, 307, 462, 18, 4702, 337, 1891, 1266, 392, 11, 44377, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3514884742530617, "compression_ratio": 1.0666666666666667, "no_speech_prob": 0.07318638265132904}, {"id": 1, "seek": 0, "start": 19.0, "end": 27.0, "text": " And our speaker is Ken Kahn, who is retired in Oxford after being at Oxford.", "tokens": [51314, 400, 527, 8145, 307, 8273, 591, 12140, 11, 567, 307, 16776, 294, 24786, 934, 885, 412, 24786, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3514884742530617, "compression_ratio": 1.0666666666666667, "no_speech_prob": 0.07318638265132904}, {"id": 2, "seek": 2700, "start": 27.0, "end": 32.0, "text": " And soon we'll be back at Oxford.", "tokens": [50364, 400, 2321, 321, 603, 312, 646, 412, 24786, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18798537687821823, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.027886195108294487}, {"id": 3, "seek": 2700, "start": 32.0, "end": 40.0, "text": " Ken was back in the Dark Ages at Xerox Park for a bit.", "tokens": [50614, 8273, 390, 646, 294, 264, 9563, 37362, 412, 1783, 2032, 87, 4964, 337, 257, 857, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18798537687821823, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.027886195108294487}, {"id": 4, "seek": 2700, "start": 40.0, "end": 48.0, "text": " And there he worked on a project which became a lifetime session, I guess,", "tokens": [51014, 400, 456, 415, 2732, 322, 257, 1716, 597, 3062, 257, 11364, 5481, 11, 286, 2041, 11, 51414], "temperature": 0.0, "avg_logprob": -0.18798537687821823, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.027886195108294487}, {"id": 5, "seek": 2700, "start": 48.0, "end": 53.0, "text": " called Toon Talk, which was the first really configurable, programmable", "tokens": [51414, 1219, 1407, 266, 8780, 11, 597, 390, 264, 700, 534, 22192, 712, 11, 37648, 712, 51664], "temperature": 0.0, "avg_logprob": -0.18798537687821823, "compression_ratio": 1.3742690058479532, "no_speech_prob": 0.027886195108294487}, {"id": 6, "seek": 5300, "start": 53.0, "end": 55.0, "text": " animation system.", "tokens": [50364, 9603, 1185, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1654834169330019, "compression_ratio": 1.4941176470588236, "no_speech_prob": 0.09375445544719696}, {"id": 7, "seek": 5300, "start": 55.0, "end": 63.0, "text": " And in its day, golly, I hate to think of how long ago it was.", "tokens": [50464, 400, 294, 1080, 786, 11, 352, 13020, 11, 286, 4700, 281, 519, 295, 577, 938, 2057, 309, 390, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1654834169330019, "compression_ratio": 1.4941176470588236, "no_speech_prob": 0.09375445544719696}, {"id": 8, "seek": 5300, "start": 63.0, "end": 67.0, "text": " It was really the most amazing thing.", "tokens": [50864, 467, 390, 534, 264, 881, 2243, 551, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1654834169330019, "compression_ratio": 1.4941176470588236, "no_speech_prob": 0.09375445544719696}, {"id": 9, "seek": 5300, "start": 67.0, "end": 73.0, "text": " And rather than go the commercial route and build video games,", "tokens": [51064, 400, 2831, 813, 352, 264, 6841, 7955, 293, 1322, 960, 2813, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1654834169330019, "compression_ratio": 1.4941176470588236, "no_speech_prob": 0.09375445544719696}, {"id": 10, "seek": 5300, "start": 73.0, "end": 80.0, "text": " it can keep poking away at the methodology and the underlying structures", "tokens": [51364, 309, 393, 1066, 42684, 1314, 412, 264, 24850, 293, 264, 14217, 9227, 51714], "temperature": 0.0, "avg_logprob": -0.1654834169330019, "compression_ratio": 1.4941176470588236, "no_speech_prob": 0.09375445544719696}, {"id": 11, "seek": 8000, "start": 80.0, "end": 86.0, "text": " of which you tell stories through pictures and voice.", "tokens": [50364, 295, 597, 291, 980, 3676, 807, 5242, 293, 3177, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13577478759142816, "compression_ratio": 1.2836879432624113, "no_speech_prob": 0.004672984126955271}, {"id": 12, "seek": 8000, "start": 86.0, "end": 92.0, "text": " He recently did some work collaborating with the chat GPT-4.", "tokens": [50664, 634, 3938, 630, 512, 589, 30188, 365, 264, 5081, 26039, 51, 12, 19, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13577478759142816, "compression_ratio": 1.2836879432624113, "no_speech_prob": 0.004672984126955271}, {"id": 13, "seek": 8000, "start": 92.0, "end": 99.0, "text": " And unlike a lot of people who looked at it as using it as a tool,", "tokens": [50964, 400, 8343, 257, 688, 295, 561, 567, 2956, 412, 309, 382, 1228, 309, 382, 257, 2290, 11, 51314], "temperature": 0.0, "avg_logprob": -0.13577478759142816, "compression_ratio": 1.2836879432624113, "no_speech_prob": 0.004672984126955271}, {"id": 14, "seek": 9900, "start": 99.0, "end": 107.0, "text": " he appears to believe, as I do, that chat GPT-4 is just another programming", "tokens": [50364, 415, 7038, 281, 1697, 11, 382, 286, 360, 11, 300, 5081, 26039, 51, 12, 19, 307, 445, 1071, 9410, 50764], "temperature": 0.0, "avg_logprob": -0.09594146609306335, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.0494467131793499}, {"id": 15, "seek": 9900, "start": 107.0, "end": 113.0, "text": " companion, a worker, a colleague, and used it in a very interesting way.", "tokens": [50764, 22363, 11, 257, 11346, 11, 257, 13532, 11, 293, 1143, 309, 294, 257, 588, 1880, 636, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09594146609306335, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.0494467131793499}, {"id": 16, "seek": 9900, "start": 113.0, "end": 119.0, "text": " So I'm going to let you go on from there and feel free, by the way,", "tokens": [51064, 407, 286, 478, 516, 281, 718, 291, 352, 322, 490, 456, 293, 841, 1737, 11, 538, 264, 636, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09594146609306335, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.0494467131793499}, {"id": 17, "seek": 9900, "start": 119.0, "end": 123.0, "text": " if you're in the audience, to interrupt with questions and so forth.", "tokens": [51364, 498, 291, 434, 294, 264, 4034, 11, 281, 12729, 365, 1651, 293, 370, 5220, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09594146609306335, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.0494467131793499}, {"id": 18, "seek": 12300, "start": 123.0, "end": 131.0, "text": " And if it gets to be a diversionary thing, we'll either accept it or not.", "tokens": [50364, 400, 498, 309, 2170, 281, 312, 257, 49422, 822, 551, 11, 321, 603, 2139, 3241, 309, 420, 406, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16924450315278153, "compression_ratio": 1.2325581395348837, "no_speech_prob": 0.0022511999122798443}, {"id": 19, "seek": 12300, "start": 131.0, "end": 133.0, "text": " Go on, Ken.", "tokens": [50764, 1037, 322, 11, 8273, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16924450315278153, "compression_ratio": 1.2325581395348837, "no_speech_prob": 0.0022511999122798443}, {"id": 20, "seek": 12300, "start": 133.0, "end": 135.0, "text": " Good.", "tokens": [50864, 2205, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16924450315278153, "compression_ratio": 1.2325581395348837, "no_speech_prob": 0.0022511999122798443}, {"id": 21, "seek": 12300, "start": 135.0, "end": 143.0, "text": " Let me move this.", "tokens": [50964, 961, 385, 1286, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16924450315278153, "compression_ratio": 1.2325581395348837, "no_speech_prob": 0.0022511999122798443}, {"id": 22, "seek": 12300, "start": 143.0, "end": 145.0, "text": " The Zoom stuff was sort of in the way.", "tokens": [51364, 440, 13453, 1507, 390, 1333, 295, 294, 264, 636, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16924450315278153, "compression_ratio": 1.2325581395348837, "no_speech_prob": 0.0022511999122798443}, {"id": 23, "seek": 12300, "start": 145.0, "end": 147.0, "text": " All right.", "tokens": [51464, 1057, 558, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16924450315278153, "compression_ratio": 1.2325581395348837, "no_speech_prob": 0.0022511999122798443}, {"id": 24, "seek": 14700, "start": 147.0, "end": 151.0, "text": " Thanks, everybody, for coming and feel free to interrupt,", "tokens": [50364, 2561, 11, 2201, 11, 337, 1348, 293, 841, 1737, 281, 12729, 11, 50564], "temperature": 0.0, "avg_logprob": -0.14397904607984754, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.027545470744371414}, {"id": 25, "seek": 14700, "start": 151.0, "end": 154.0, "text": " ask questions at any time.", "tokens": [50564, 1029, 1651, 412, 604, 565, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14397904607984754, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.027545470744371414}, {"id": 26, "seek": 14700, "start": 154.0, "end": 160.0, "text": " And this is what I'll be talking about is some experiments I've done trying", "tokens": [50714, 400, 341, 307, 437, 286, 603, 312, 1417, 466, 307, 512, 12050, 286, 600, 1096, 1382, 51014], "temperature": 0.0, "avg_logprob": -0.14397904607984754, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.027545470744371414}, {"id": 27, "seek": 14700, "start": 160.0, "end": 164.0, "text": " to create apps, just having conversations with a language model.", "tokens": [51014, 281, 1884, 7733, 11, 445, 1419, 7315, 365, 257, 2856, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14397904607984754, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.027545470744371414}, {"id": 28, "seek": 14700, "start": 164.0, "end": 167.0, "text": " But first, maybe I'll give you a tiny bit of background.", "tokens": [51214, 583, 700, 11, 1310, 286, 603, 976, 291, 257, 5870, 857, 295, 3678, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14397904607984754, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.027545470744371414}, {"id": 29, "seek": 16700, "start": 167.0, "end": 173.0, "text": " So I was doing good old-fashioned AI for about 20 years first at the MIT", "tokens": [50364, 407, 286, 390, 884, 665, 1331, 12, 37998, 7318, 337, 466, 945, 924, 700, 412, 264, 13100, 50664], "temperature": 0.0, "avg_logprob": -0.19929793943841773, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.020312491804361343}, {"id": 30, "seek": 16700, "start": 173.0, "end": 176.0, "text": " Lab and then Sir Ox Park and so on.", "tokens": [50664, 10137, 293, 550, 6144, 16489, 4964, 293, 370, 322, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19929793943841773, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.020312491804361343}, {"id": 31, "seek": 16700, "start": 176.0, "end": 180.0, "text": " And then I got into the idea of creating a programming language for", "tokens": [50814, 400, 550, 286, 658, 666, 264, 1558, 295, 4084, 257, 9410, 2856, 337, 51014], "temperature": 0.0, "avg_logprob": -0.19929793943841773, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.020312491804361343}, {"id": 32, "seek": 16700, "start": 180.0, "end": 183.0, "text": " children that was completely based around animation.", "tokens": [51014, 2227, 300, 390, 2584, 2361, 926, 9603, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19929793943841773, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.020312491804361343}, {"id": 33, "seek": 16700, "start": 183.0, "end": 186.0, "text": " There was no text involved.", "tokens": [51164, 821, 390, 572, 2487, 3288, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19929793943841773, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.020312491804361343}, {"id": 34, "seek": 16700, "start": 186.0, "end": 191.0, "text": " And then in the middle of that, I also got involved in Oxford,", "tokens": [51314, 400, 550, 294, 264, 2808, 295, 300, 11, 286, 611, 658, 3288, 294, 24786, 11, 51564], "temperature": 0.0, "avg_logprob": -0.19929793943841773, "compression_ratio": 1.509433962264151, "no_speech_prob": 0.020312491804361343}, {"id": 35, "seek": 19100, "start": 191.0, "end": 197.0, "text": " in a big project trying to make it possible for", "tokens": [50364, 294, 257, 955, 1716, 1382, 281, 652, 309, 1944, 337, 50664], "temperature": 0.0, "avg_logprob": -0.14438958363990262, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0026714454870671034}, {"id": 36, "seek": 19100, "start": 197.0, "end": 201.0, "text": " people to make agent-based simulations of, you know,", "tokens": [50664, 561, 281, 652, 9461, 12, 6032, 35138, 295, 11, 291, 458, 11, 50864], "temperature": 0.0, "avg_logprob": -0.14438958363990262, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0026714454870671034}, {"id": 37, "seek": 19100, "start": 201.0, "end": 206.0, "text": " epidemics or ant colonies or whatever.", "tokens": [50864, 13510, 38014, 420, 2511, 27981, 420, 2035, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14438958363990262, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0026714454870671034}, {"id": 38, "seek": 19100, "start": 206.0, "end": 212.0, "text": " And then I got involved in 2016 in a big European research project where my", "tokens": [51114, 400, 550, 286, 658, 3288, 294, 6549, 294, 257, 955, 6473, 2132, 1716, 689, 452, 51414], "temperature": 0.0, "avg_logprob": -0.14438958363990262, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0026714454870671034}, {"id": 39, "seek": 19100, "start": 212.0, "end": 218.0, "text": " role was to make it possible for the students, and these were 11 to 14", "tokens": [51414, 3090, 390, 281, 652, 309, 1944, 337, 264, 1731, 11, 293, 613, 645, 2975, 281, 3499, 51714], "temperature": 0.0, "avg_logprob": -0.14438958363990262, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0026714454870671034}, {"id": 40, "seek": 21800, "start": 218.0, "end": 223.0, "text": " world to be able to incorporate machine learning in their projects.", "tokens": [50364, 1002, 281, 312, 1075, 281, 16091, 3479, 2539, 294, 641, 4455, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15453578948974608, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.007567159365862608}, {"id": 41, "seek": 21800, "start": 223.0, "end": 228.0, "text": " So I built upon the SNAP programming language, which is more adult version", "tokens": [50614, 407, 286, 3094, 3564, 264, 13955, 4715, 9410, 2856, 11, 597, 307, 544, 5075, 3037, 50864], "temperature": 0.0, "avg_logprob": -0.15453578948974608, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.007567159365862608}, {"id": 42, "seek": 21800, "start": 228.0, "end": 236.0, "text": " of Scratch, and added lots of blocks for doing computer vision or", "tokens": [50864, 295, 34944, 852, 11, 293, 3869, 3195, 295, 8474, 337, 884, 3820, 5201, 420, 51264], "temperature": 0.0, "avg_logprob": -0.15453578948974608, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.007567159365862608}, {"id": 43, "seek": 21800, "start": 236.0, "end": 243.0, "text": " natural language processing or to be even able to just create", "tokens": [51264, 3303, 2856, 9007, 420, 281, 312, 754, 1075, 281, 445, 1884, 51614], "temperature": 0.0, "avg_logprob": -0.15453578948974608, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.007567159365862608}, {"id": 44, "seek": 21800, "start": 243.0, "end": 246.0, "text": " neural networks and train them right there in the browser.", "tokens": [51614, 18161, 9590, 293, 3847, 552, 558, 456, 294, 264, 11185, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15453578948974608, "compression_ratio": 1.5741626794258374, "no_speech_prob": 0.007567159365862608}, {"id": 45, "seek": 24600, "start": 246.0, "end": 252.0, "text": " And that was my focus until ChatGPT came along.", "tokens": [50364, 400, 300, 390, 452, 1879, 1826, 27503, 38, 47, 51, 1361, 2051, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1278263348252026, "compression_ratio": 1.4034090909090908, "no_speech_prob": 0.0030729747377336025}, {"id": 46, "seek": 24600, "start": 252.0, "end": 261.0, "text": " And I've been focused on treating ChatGPT as a colleague here.", "tokens": [50664, 400, 286, 600, 668, 5178, 322, 15083, 27503, 38, 47, 51, 382, 257, 13532, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1278263348252026, "compression_ratio": 1.4034090909090908, "no_speech_prob": 0.0030729747377336025}, {"id": 47, "seek": 24600, "start": 261.0, "end": 267.0, "text": " So I'm going to give a really detailed story about one conversation I had", "tokens": [51114, 407, 286, 478, 516, 281, 976, 257, 534, 9942, 1657, 466, 472, 3761, 286, 632, 51414], "temperature": 0.0, "avg_logprob": -0.1278263348252026, "compression_ratio": 1.4034090909090908, "no_speech_prob": 0.0030729747377336025}, {"id": 48, "seek": 24600, "start": 267.0, "end": 272.0, "text": " with ChatGPT, which involved building a fireworks application.", "tokens": [51414, 365, 27503, 38, 47, 51, 11, 597, 3288, 2390, 257, 28453, 3861, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1278263348252026, "compression_ratio": 1.4034090909090908, "no_speech_prob": 0.0030729747377336025}, {"id": 49, "seek": 27200, "start": 272.0, "end": 279.0, "text": " And then I'm going to touch on some of these other examples that I've also", "tokens": [50364, 400, 550, 286, 478, 516, 281, 2557, 322, 512, 295, 613, 661, 5110, 300, 286, 600, 611, 50714], "temperature": 0.0, "avg_logprob": -0.09038187621475814, "compression_ratio": 1.406060606060606, "no_speech_prob": 0.0034801126457750797}, {"id": 50, "seek": 27200, "start": 279.0, "end": 285.0, "text": " done with ChatGPT.", "tokens": [50714, 1096, 365, 27503, 38, 47, 51, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09038187621475814, "compression_ratio": 1.406060606060606, "no_speech_prob": 0.0034801126457750797}, {"id": 51, "seek": 27200, "start": 285.0, "end": 292.0, "text": " And I keep saying ChatGPT, and I started it with the older one,", "tokens": [51014, 400, 286, 1066, 1566, 27503, 38, 47, 51, 11, 293, 286, 1409, 309, 365, 264, 4906, 472, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09038187621475814, "compression_ratio": 1.406060606060606, "no_speech_prob": 0.0034801126457750797}, {"id": 52, "seek": 27200, "start": 292.0, "end": 299.0, "text": " but as soon as I had access to GPT-4, I only used the GPT-4 version of it,", "tokens": [51364, 457, 382, 2321, 382, 286, 632, 2105, 281, 26039, 51, 12, 19, 11, 286, 787, 1143, 264, 26039, 51, 12, 19, 3037, 295, 309, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09038187621475814, "compression_ratio": 1.406060606060606, "no_speech_prob": 0.0034801126457750797}, {"id": 53, "seek": 29900, "start": 299.0, "end": 302.0, "text": " which is just so much more capable than any of the other ones.", "tokens": [50364, 597, 307, 445, 370, 709, 544, 8189, 813, 604, 295, 264, 661, 2306, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1514528475309673, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0022505929227918386}, {"id": 54, "seek": 29900, "start": 302.0, "end": 309.0, "text": " And I've done attempts to see if I could do it with Microsoft Bing or", "tokens": [50514, 400, 286, 600, 1096, 15257, 281, 536, 498, 286, 727, 360, 309, 365, 8116, 30755, 420, 50864], "temperature": 0.0, "avg_logprob": -0.1514528475309673, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0022505929227918386}, {"id": 55, "seek": 29900, "start": 309.0, "end": 314.0, "text": " Google's Barred or Hugging Chat or Claude from Anthropic or Pi,", "tokens": [50864, 3329, 311, 4156, 986, 420, 46892, 3249, 27503, 420, 12947, 2303, 490, 12727, 39173, 420, 17741, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1514528475309673, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0022505929227918386}, {"id": 56, "seek": 29900, "start": 314.0, "end": 316.0, "text": " which just came out.", "tokens": [51114, 597, 445, 1361, 484, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1514528475309673, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0022505929227918386}, {"id": 57, "seek": 29900, "start": 316.0, "end": 321.0, "text": " And all of them did pretty poorly, though I did just read that today", "tokens": [51214, 400, 439, 295, 552, 630, 1238, 22271, 11, 1673, 286, 630, 445, 1401, 300, 965, 51464], "temperature": 0.0, "avg_logprob": -0.1514528475309673, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0022505929227918386}, {"id": 58, "seek": 29900, "start": 321.0, "end": 325.0, "text": " Google said they're going to be announcing a new version of Barred", "tokens": [51464, 3329, 848, 436, 434, 516, 281, 312, 28706, 257, 777, 3037, 295, 4156, 986, 51664], "temperature": 0.0, "avg_logprob": -0.1514528475309673, "compression_ratio": 1.5281385281385282, "no_speech_prob": 0.0022505929227918386}, {"id": 59, "seek": 32500, "start": 325.0, "end": 329.0, "text": " that's more capable in generating code.", "tokens": [50364, 300, 311, 544, 8189, 294, 17746, 3089, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11572318077087403, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.0017536712111905217}, {"id": 60, "seek": 32500, "start": 329.0, "end": 333.0, "text": " So maybe it would be an equally good alternative.", "tokens": [50564, 407, 1310, 309, 576, 312, 364, 12309, 665, 8535, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11572318077087403, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.0017536712111905217}, {"id": 61, "seek": 32500, "start": 333.0, "end": 340.0, "text": " And I didn't really explore very carefully the older version of ChatGPT.", "tokens": [50764, 400, 286, 994, 380, 534, 6839, 588, 7500, 264, 4906, 3037, 295, 27503, 38, 47, 51, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11572318077087403, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.0017536712111905217}, {"id": 62, "seek": 32500, "start": 340.0, "end": 347.0, "text": " So the first example I'll give you is where the, I asked ChatGPT to be more", "tokens": [51114, 407, 264, 700, 1365, 286, 603, 976, 291, 307, 689, 264, 11, 286, 2351, 27503, 38, 47, 51, 281, 312, 544, 51464], "temperature": 0.0, "avg_logprob": -0.11572318077087403, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.0017536712111905217}, {"id": 63, "seek": 32500, "start": 347.0, "end": 351.0, "text": " like a tutor and I myself pretended to be a student.", "tokens": [51464, 411, 257, 35613, 293, 286, 2059, 45056, 281, 312, 257, 3107, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11572318077087403, "compression_ratio": 1.4477611940298507, "no_speech_prob": 0.0017536712111905217}, {"id": 64, "seek": 35100, "start": 351.0, "end": 357.0, "text": " But then the other six examples, it's much more of pair programming where", "tokens": [50364, 583, 550, 264, 661, 2309, 5110, 11, 309, 311, 709, 544, 295, 6119, 9410, 689, 50664], "temperature": 0.0, "avg_logprob": -0.14025243898717368, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.0013661823468282819}, {"id": 65, "seek": 35100, "start": 357.0, "end": 363.0, "text": " the chatbot is really doing all of the programming.", "tokens": [50664, 264, 5081, 18870, 307, 534, 884, 439, 295, 264, 9410, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14025243898717368, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.0013661823468282819}, {"id": 66, "seek": 35100, "start": 363.0, "end": 368.0, "text": " And I'm kind of the navigator kind of saying, oh, we should try this or", "tokens": [50964, 400, 286, 478, 733, 295, 264, 7407, 1639, 733, 295, 1566, 11, 1954, 11, 321, 820, 853, 341, 420, 51214], "temperature": 0.0, "avg_logprob": -0.14025243898717368, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.0013661823468282819}, {"id": 67, "seek": 35100, "start": 368.0, "end": 371.0, "text": " that's not working or what about this problem.", "tokens": [51214, 300, 311, 406, 1364, 420, 437, 466, 341, 1154, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14025243898717368, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.0013661823468282819}, {"id": 68, "seek": 35100, "start": 371.0, "end": 375.0, "text": " I also have to just because of the current technology do a little bit of", "tokens": [51364, 286, 611, 362, 281, 445, 570, 295, 264, 2190, 2899, 360, 257, 707, 857, 295, 51564], "temperature": 0.0, "avg_logprob": -0.14025243898717368, "compression_ratio": 1.553921568627451, "no_speech_prob": 0.0013661823468282819}, {"id": 69, "seek": 37500, "start": 375.0, "end": 382.0, "text": " copy and paste, but that's very mechanical and not very interesting.", "tokens": [50364, 5055, 293, 9163, 11, 457, 300, 311, 588, 12070, 293, 406, 588, 1880, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1763090193271637, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.005727224517613649}, {"id": 70, "seek": 37500, "start": 382.0, "end": 388.0, "text": " So first thing to understand is that these are much more than just code", "tokens": [50714, 407, 700, 551, 281, 1223, 307, 300, 613, 366, 709, 544, 813, 445, 3089, 51014], "temperature": 0.0, "avg_logprob": -0.1763090193271637, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.005727224517613649}, {"id": 71, "seek": 37500, "start": 388.0, "end": 394.0, "text": " generators that they, let me move this.", "tokens": [51014, 38662, 300, 436, 11, 718, 385, 1286, 341, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1763090193271637, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.005727224517613649}, {"id": 72, "seek": 37500, "start": 394.0, "end": 400.0, "text": " They could, just by their nature will, will not just give you code but give", "tokens": [51314, 814, 727, 11, 445, 538, 641, 3687, 486, 11, 486, 406, 445, 976, 291, 3089, 457, 976, 51614], "temperature": 0.0, "avg_logprob": -0.1763090193271637, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.005727224517613649}, {"id": 73, "seek": 40000, "start": 400.0, "end": 406.0, "text": " you some explanations of what's going on in the bigger picture and how that works.", "tokens": [50364, 291, 512, 28708, 295, 437, 311, 516, 322, 294, 264, 3801, 3036, 293, 577, 300, 1985, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14986728392925458, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.17743149399757385}, {"id": 74, "seek": 40000, "start": 406.0, "end": 411.0, "text": " And then if you ask, it'll always put some comments in the code, but if you", "tokens": [50664, 400, 550, 498, 291, 1029, 11, 309, 603, 1009, 829, 512, 3053, 294, 264, 3089, 11, 457, 498, 291, 50914], "temperature": 0.0, "avg_logprob": -0.14986728392925458, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.17743149399757385}, {"id": 75, "seek": 40000, "start": 411.0, "end": 416.0, "text": " don't really understand some function, you could just ask it to comment it and", "tokens": [50914, 500, 380, 534, 1223, 512, 2445, 11, 291, 727, 445, 1029, 309, 281, 2871, 309, 293, 51164], "temperature": 0.0, "avg_logprob": -0.14986728392925458, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.17743149399757385}, {"id": 76, "seek": 40000, "start": 416.0, "end": 422.0, "text": " it'll, every little section, it'll put in comments and not just sort of meaning,", "tokens": [51164, 309, 603, 11, 633, 707, 3541, 11, 309, 603, 829, 294, 3053, 293, 406, 445, 1333, 295, 3620, 11, 51464], "temperature": 0.0, "avg_logprob": -0.14986728392925458, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.17743149399757385}, {"id": 77, "seek": 40000, "start": 422.0, "end": 426.0, "text": " I mean, redundant comments, but ones that are actually quite helpful.", "tokens": [51464, 286, 914, 11, 40997, 3053, 11, 457, 2306, 300, 366, 767, 1596, 4961, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14986728392925458, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.17743149399757385}, {"id": 78, "seek": 42600, "start": 426.0, "end": 431.0, "text": " And the other thing is it's, it often makes, constructs buggy code, but if you", "tokens": [50364, 400, 264, 661, 551, 307, 309, 311, 11, 309, 2049, 1669, 11, 7690, 82, 7426, 1480, 3089, 11, 457, 498, 291, 50614], "temperature": 0.0, "avg_logprob": -0.2261226695516835, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.045992057770490646}, {"id": 79, "seek": 42600, "start": 431.0, "end": 435.0, "text": " tell it about the bugs, especially if you just copy and paste the error messages", "tokens": [50614, 980, 309, 466, 264, 15120, 11, 2318, 498, 291, 445, 5055, 293, 9163, 264, 6713, 7897, 50814], "temperature": 0.0, "avg_logprob": -0.2261226695516835, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.045992057770490646}, {"id": 80, "seek": 42600, "start": 435.0, "end": 438.0, "text": " into the chat, it could fix them.", "tokens": [50814, 666, 264, 5081, 11, 309, 727, 3191, 552, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2261226695516835, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.045992057770490646}, {"id": 81, "seek": 42600, "start": 438.0, "end": 442.0, "text": " And then if you ask it, and you say, okay, could you think of ways to improve", "tokens": [50964, 400, 550, 498, 291, 1029, 309, 11, 293, 291, 584, 11, 1392, 11, 727, 291, 519, 295, 2098, 281, 3470, 51164], "temperature": 0.0, "avg_logprob": -0.2261226695516835, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.045992057770490646}, {"id": 82, "seek": 42600, "start": 442.0, "end": 447.0, "text": " this app, it'll come up with a lot of sensible suggestions.", "tokens": [51164, 341, 724, 11, 309, 603, 808, 493, 365, 257, 688, 295, 25380, 13396, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2261226695516835, "compression_ratio": 1.5913461538461537, "no_speech_prob": 0.045992057770490646}, {"id": 83, "seek": 44700, "start": 447.0, "end": 452.0, "text": " And then there's been cases where it couldn't generate some code because it", "tokens": [50364, 400, 550, 456, 311, 668, 3331, 689, 309, 2809, 380, 8460, 512, 3089, 570, 309, 50614], "temperature": 0.0, "avg_logprob": -0.21770703302670832, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.02515602298080921}, {"id": 84, "seek": 44700, "start": 452.0, "end": 457.0, "text": " was trained two years ago, and they're, it was using an API that's much newer", "tokens": [50614, 390, 8895, 732, 924, 2057, 11, 293, 436, 434, 11, 309, 390, 1228, 364, 9362, 300, 311, 709, 17628, 50864], "temperature": 0.0, "avg_logprob": -0.21770703302670832, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.02515602298080921}, {"id": 85, "seek": 44700, "start": 457.0, "end": 463.0, "text": " than that. And all I did was copy and paste the documentation into the chat", "tokens": [50864, 813, 300, 13, 400, 439, 286, 630, 390, 5055, 293, 9163, 264, 14333, 666, 264, 5081, 51164], "temperature": 0.0, "avg_logprob": -0.21770703302670832, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.02515602298080921}, {"id": 86, "seek": 44700, "start": 463.0, "end": 467.0, "text": " GPT and, you know, read it in the chat.", "tokens": [51164, 26039, 51, 293, 11, 291, 458, 11, 1401, 309, 294, 264, 5081, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21770703302670832, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.02515602298080921}, {"id": 87, "seek": 46700, "start": 467.0, "end": 471.0, "text": " And it could also generate documentation.", "tokens": [50364, 400, 309, 727, 611, 8460, 14333, 13, 50564], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 88, "seek": 46700, "start": 471.0, "end": 475.0, "text": " And it's also important, especially when you're thinking about children using", "tokens": [50564, 400, 309, 311, 611, 1021, 11, 2318, 562, 291, 434, 1953, 466, 2227, 1228, 50764], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 89, "seek": 46700, "start": 475.0, "end": 479.0, "text": " this that you could set, you could tell it from the beginning, you know,", "tokens": [50764, 341, 300, 291, 727, 992, 11, 291, 727, 980, 309, 490, 264, 2863, 11, 291, 458, 11, 50964], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 90, "seek": 46700, "start": 479.0, "end": 483.0, "text": " you're talking to a young child, and it'll use a different kind of", "tokens": [50964, 291, 434, 1417, 281, 257, 2037, 1440, 11, 293, 309, 603, 764, 257, 819, 733, 295, 51164], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 91, "seek": 46700, "start": 483.0, "end": 487.0, "text": " language, you know, language, you know, you can use it as a, you know,", "tokens": [51164, 2856, 11, 291, 458, 11, 2856, 11, 291, 458, 11, 291, 393, 764, 309, 382, 257, 11, 291, 458, 11, 51364], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 92, "seek": 46700, "start": 487.0, "end": 491.0, "text": " a language that's much newer than that. And all I did was copy and paste the", "tokens": [51364, 257, 2856, 300, 311, 709, 17628, 813, 300, 13, 400, 439, 286, 630, 390, 5055, 293, 9163, 264, 51564], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 93, "seek": 46700, "start": 491.0, "end": 495.0, "text": " documentation into the chat GPT and, you know, read it in seconds and was", "tokens": [51564, 14333, 666, 264, 5081, 26039, 51, 293, 11, 291, 458, 11, 1401, 309, 294, 3949, 293, 390, 51764], "temperature": 0.4, "avg_logprob": -0.41515069580078123, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.18438035249710083}, {"id": 94, "seek": 49500, "start": 495.0, "end": 499.0, "text": " able, you know, use a different kind of vocabulary and avoid more technical", "tokens": [50364, 1075, 11, 291, 458, 11, 764, 257, 819, 733, 295, 19864, 293, 5042, 544, 6191, 50564], "temperature": 0.0, "avg_logprob": -0.17286782148407726, "compression_ratio": 1.5071090047393365, "no_speech_prob": 0.0028870089445263147}, {"id": 95, "seek": 49500, "start": 499.0, "end": 503.0, "text": " things and so on.", "tokens": [50564, 721, 293, 370, 322, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17286782148407726, "compression_ratio": 1.5071090047393365, "no_speech_prob": 0.0028870089445263147}, {"id": 96, "seek": 49500, "start": 503.0, "end": 510.0, "text": " So I, all of these experiments I decided to use, ask it to create a web app", "tokens": [50764, 407, 286, 11, 439, 295, 613, 12050, 286, 3047, 281, 764, 11, 1029, 309, 281, 1884, 257, 3670, 724, 51114], "temperature": 0.0, "avg_logprob": -0.17286782148407726, "compression_ratio": 1.5071090047393365, "no_speech_prob": 0.0028870089445263147}, {"id": 97, "seek": 49500, "start": 510.0, "end": 516.0, "text": " in JavaScript, you know, with some HTML and CSS. And the reason for that is", "tokens": [51114, 294, 15778, 11, 291, 458, 11, 365, 512, 17995, 293, 24387, 13, 400, 264, 1778, 337, 300, 307, 51414], "temperature": 0.0, "avg_logprob": -0.17286782148407726, "compression_ratio": 1.5071090047393365, "no_speech_prob": 0.0028870089445263147}, {"id": 98, "seek": 49500, "start": 516.0, "end": 521.0, "text": " these three reasons is that there's nothing anyone would need to install", "tokens": [51414, 613, 1045, 4112, 307, 300, 456, 311, 1825, 2878, 576, 643, 281, 3625, 51664], "temperature": 0.0, "avg_logprob": -0.17286782148407726, "compression_ratio": 1.5071090047393365, "no_speech_prob": 0.0028870089445263147}, {"id": 99, "seek": 52100, "start": 521.0, "end": 525.0, "text": " because everybody's got a browser. And browsers are so capable these days,", "tokens": [50364, 570, 2201, 311, 658, 257, 11185, 13, 400, 36069, 366, 370, 8189, 613, 1708, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11065297541411026, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.014481023885309696}, {"id": 100, "seek": 52100, "start": 525.0, "end": 529.0, "text": " you know, they've got speech synthesis, speech recognition, they, you know,", "tokens": [50564, 291, 458, 11, 436, 600, 658, 6218, 30252, 11, 6218, 11150, 11, 436, 11, 291, 458, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11065297541411026, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.014481023885309696}, {"id": 101, "seek": 52100, "start": 529.0, "end": 535.0, "text": " they could easily communicate with all sorts of APIs and so on.", "tokens": [50764, 436, 727, 3612, 7890, 365, 439, 7527, 295, 21445, 293, 370, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11065297541411026, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.014481023885309696}, {"id": 102, "seek": 52100, "start": 535.0, "end": 540.0, "text": " And, and then it's so much easier for people to share, you could actually", "tokens": [51064, 400, 11, 293, 550, 309, 311, 370, 709, 3571, 337, 561, 281, 2073, 11, 291, 727, 767, 51314], "temperature": 0.0, "avg_logprob": -0.11065297541411026, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.014481023885309696}, {"id": 103, "seek": 52100, "start": 540.0, "end": 547.0, "text": " just send the index.html file on the script.js file to your friends,", "tokens": [51314, 445, 2845, 264, 8186, 13, 357, 15480, 3991, 322, 264, 5755, 13, 25530, 3991, 281, 428, 1855, 11, 51664], "temperature": 0.0, "avg_logprob": -0.11065297541411026, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.014481023885309696}, {"id": 104, "seek": 54700, "start": 547.0, "end": 553.0, "text": " or you could host it any number of places for free, just as static web pages.", "tokens": [50364, 420, 291, 727, 3975, 309, 604, 1230, 295, 3190, 337, 1737, 11, 445, 382, 13437, 3670, 7183, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0944228312548469, "compression_ratio": 1.4484536082474226, "no_speech_prob": 0.004979007411748171}, {"id": 105, "seek": 54700, "start": 553.0, "end": 558.0, "text": " You don't have to run a server or anything.", "tokens": [50664, 509, 500, 380, 362, 281, 1190, 257, 7154, 420, 1340, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0944228312548469, "compression_ratio": 1.4484536082474226, "no_speech_prob": 0.004979007411748171}, {"id": 106, "seek": 54700, "start": 558.0, "end": 565.0, "text": " And what professional programmers are using, I recently read a survey that said", "tokens": [50914, 400, 437, 4843, 41504, 366, 1228, 11, 286, 3938, 1401, 257, 8984, 300, 848, 51264], "temperature": 0.0, "avg_logprob": -0.0944228312548469, "compression_ratio": 1.4484536082474226, "no_speech_prob": 0.004979007411748171}, {"id": 107, "seek": 54700, "start": 565.0, "end": 571.0, "text": " something like about 75% of people that are using large language models to help", "tokens": [51264, 746, 411, 466, 9562, 4, 295, 561, 300, 366, 1228, 2416, 2856, 5245, 281, 854, 51564], "temperature": 0.0, "avg_logprob": -0.0944228312548469, "compression_ratio": 1.4484536082474226, "no_speech_prob": 0.004979007411748171}, {"id": 108, "seek": 57100, "start": 571.0, "end": 577.0, "text": " with their programming are using the Visual Studio copilot, but 20 or 25% are", "tokens": [50364, 365, 641, 9410, 366, 1228, 264, 23187, 13500, 2971, 31516, 11, 457, 945, 420, 3552, 4, 366, 50664], "temperature": 0.0, "avg_logprob": -0.13669625003780939, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.003704990493133664}, {"id": 109, "seek": 57100, "start": 577.0, "end": 581.0, "text": " doing what I'm doing, which is just to have a conversation with the chatbot", "tokens": [50664, 884, 437, 286, 478, 884, 11, 597, 307, 445, 281, 362, 257, 3761, 365, 264, 5081, 18870, 50864], "temperature": 0.0, "avg_logprob": -0.13669625003780939, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.003704990493133664}, {"id": 110, "seek": 57100, "start": 581.0, "end": 585.0, "text": " that can program rather than have it nicely integrated. And of course,", "tokens": [50864, 300, 393, 1461, 2831, 813, 362, 309, 9594, 10919, 13, 400, 295, 1164, 11, 51064], "temperature": 0.0, "avg_logprob": -0.13669625003780939, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.003704990493133664}, {"id": 111, "seek": 57100, "start": 585.0, "end": 589.0, "text": " being nicely integrated, you know, you don't have to do the copy and paste", "tokens": [51064, 885, 9594, 10919, 11, 291, 458, 11, 291, 500, 380, 362, 281, 360, 264, 5055, 293, 9163, 51264], "temperature": 0.0, "avg_logprob": -0.13669625003780939, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.003704990493133664}, {"id": 112, "seek": 57100, "start": 589.0, "end": 593.0, "text": " and it maybe understands more of the bigger context. It's, it's, it's, but it's", "tokens": [51264, 293, 309, 1310, 15146, 544, 295, 264, 3801, 4319, 13, 467, 311, 11, 309, 311, 11, 309, 311, 11, 457, 309, 311, 51464], "temperature": 0.0, "avg_logprob": -0.13669625003780939, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.003704990493133664}, {"id": 113, "seek": 57100, "start": 593.0, "end": 600.0, "text": " much too complicated to imagine a typical school student using it.", "tokens": [51464, 709, 886, 6179, 281, 3811, 257, 7476, 1395, 3107, 1228, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13669625003780939, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.003704990493133664}, {"id": 114, "seek": 60000, "start": 601.0, "end": 612.0, "text": " Okay, so the example that I want to start with is this fireworks one, which", "tokens": [50414, 1033, 11, 370, 264, 1365, 300, 286, 528, 281, 722, 365, 307, 341, 28453, 472, 11, 597, 50964], "temperature": 0.0, "avg_logprob": -0.1460116113935198, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.0022501875646412373}, {"id": 115, "seek": 60000, "start": 612.0, "end": 615.0, "text": " as I said before, this one's a little different than the other ones I'm going to", "tokens": [50964, 382, 286, 848, 949, 11, 341, 472, 311, 257, 707, 819, 813, 264, 661, 2306, 286, 478, 516, 281, 51114], "temperature": 0.0, "avg_logprob": -0.1460116113935198, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.0022501875646412373}, {"id": 116, "seek": 60000, "start": 615.0, "end": 621.0, "text": " talk about because I, I, I asked it from the beginning to be more like a tutor", "tokens": [51114, 751, 466, 570, 286, 11, 286, 11, 286, 2351, 309, 490, 264, 2863, 281, 312, 544, 411, 257, 35613, 51414], "temperature": 0.0, "avg_logprob": -0.1460116113935198, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.0022501875646412373}, {"id": 117, "seek": 60000, "start": 621.0, "end": 624.0, "text": " than just a colleague.", "tokens": [51414, 813, 445, 257, 13532, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1460116113935198, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.0022501875646412373}, {"id": 118, "seek": 62400, "start": 625.0, "end": 632.0, "text": " As you'll see, there's myself or my persona was pretending to be this. Yeah,", "tokens": [50414, 1018, 291, 603, 536, 11, 456, 311, 2059, 420, 452, 12184, 390, 22106, 281, 312, 341, 13, 865, 11, 50764], "temperature": 0.0, "avg_logprob": -0.16565058448097922, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.003480875166133046}, {"id": 119, "seek": 62400, "start": 632.0, "end": 637.0, "text": " I needed to know something about JavaScript, but not that much.", "tokens": [50764, 286, 2978, 281, 458, 746, 466, 15778, 11, 457, 406, 300, 709, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16565058448097922, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.003480875166133046}, {"id": 120, "seek": 62400, "start": 637.0, "end": 643.0, "text": " And I'll show you the result in a minute, but let's just go through what this", "tokens": [51014, 400, 286, 603, 855, 291, 264, 1874, 294, 257, 3456, 11, 457, 718, 311, 445, 352, 807, 437, 341, 51314], "temperature": 0.0, "avg_logprob": -0.16565058448097922, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.003480875166133046}, {"id": 121, "seek": 62400, "start": 643.0, "end": 645.0, "text": " conversation was all about.", "tokens": [51314, 3761, 390, 439, 466, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16565058448097922, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.003480875166133046}, {"id": 122, "seek": 64500, "start": 646.0, "end": 652.0, "text": " So, um, I assume everybody could read this, that's France big enough and all", "tokens": [50414, 407, 11, 1105, 11, 286, 6552, 2201, 727, 1401, 341, 11, 300, 311, 6190, 955, 1547, 293, 439, 50714], "temperature": 0.0, "avg_logprob": -0.2198212272242496, "compression_ratio": 1.5, "no_speech_prob": 0.007571839727461338}, {"id": 123, "seek": 64500, "start": 652.0, "end": 660.0, "text": " that. The, the, um, I, I started off by just saying you're a tutor for beginner", "tokens": [50714, 300, 13, 440, 11, 264, 11, 1105, 11, 286, 11, 286, 1409, 766, 538, 445, 1566, 291, 434, 257, 35613, 337, 22080, 51114], "temperature": 0.0, "avg_logprob": -0.2198212272242496, "compression_ratio": 1.5, "no_speech_prob": 0.007571839727461338}, {"id": 124, "seek": 64500, "start": 660.0, "end": 664.0, "text": " programmers, first learning JavaScript, you provide hints and explanations, but", "tokens": [51114, 41504, 11, 700, 2539, 15778, 11, 291, 2893, 27271, 293, 28708, 11, 457, 51314], "temperature": 0.0, "avg_logprob": -0.2198212272242496, "compression_ratio": 1.5, "no_speech_prob": 0.007571839727461338}, {"id": 125, "seek": 64500, "start": 664.0, "end": 669.0, "text": " never provide complete solutions. Instead, you nudge the learner towards", "tokens": [51314, 1128, 2893, 3566, 6547, 13, 7156, 11, 291, 297, 16032, 264, 33347, 3030, 51564], "temperature": 0.0, "avg_logprob": -0.2198212272242496, "compression_ratio": 1.5, "no_speech_prob": 0.007571839727461338}, {"id": 126, "seek": 66900, "start": 669.0, "end": 675.0, "text": " solutions. And it says, sure, I'll be happy to help. And then I said, can you,", "tokens": [50364, 6547, 13, 400, 309, 1619, 11, 988, 11, 286, 603, 312, 2055, 281, 854, 13, 400, 550, 286, 848, 11, 393, 291, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11553467021268957, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.07571489363908768}, {"id": 127, "seek": 66900, "start": 675.0, "end": 681.0, "text": " how can I program a display of fireworks? And it says, oh, well, using a canvas", "tokens": [50664, 577, 393, 286, 1461, 257, 4674, 295, 28453, 30, 400, 309, 1619, 11, 1954, 11, 731, 11, 1228, 257, 16267, 50964], "temperature": 0.0, "avg_logprob": -0.11553467021268957, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.07571489363908768}, {"id": 128, "seek": 66900, "start": 681.0, "end": 685.0, "text": " is probably the best way to go. And then it actually comes up with this, whatever", "tokens": [50964, 307, 1391, 264, 1151, 636, 281, 352, 13, 400, 550, 309, 767, 1487, 493, 365, 341, 11, 2035, 51164], "temperature": 0.0, "avg_logprob": -0.11553467021268957, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.07571489363908768}, {"id": 129, "seek": 66900, "start": 685.0, "end": 689.0, "text": " it is, eight point plan for how to actually build this, including, you know,", "tokens": [51164, 309, 307, 11, 3180, 935, 1393, 337, 577, 281, 767, 1322, 341, 11, 3009, 11, 291, 458, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11553467021268957, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.07571489363908768}, {"id": 130, "seek": 66900, "start": 689.0, "end": 694.0, "text": " making different classes and having an animate and so on.", "tokens": [51364, 1455, 819, 5359, 293, 1419, 364, 36439, 293, 370, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11553467021268957, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.07571489363908768}, {"id": 131, "seek": 69400, "start": 694.0, "end": 700.0, "text": " So this is one of the few places where this very nice tool called share GPT.com", "tokens": [50364, 407, 341, 307, 472, 295, 264, 1326, 3190, 689, 341, 588, 1481, 2290, 1219, 2073, 26039, 51, 13, 1112, 50664], "temperature": 0.0, "avg_logprob": -0.1668428564971348, "compression_ratio": 1.544, "no_speech_prob": 0.007455643266439438}, {"id": 132, "seek": 69400, "start": 700.0, "end": 707.0, "text": " fell down, which is, it makes for a really nice way of sharing your, um,", "tokens": [50664, 5696, 760, 11, 597, 307, 11, 309, 1669, 337, 257, 534, 1481, 636, 295, 5414, 428, 11, 1105, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1668428564971348, "compression_ratio": 1.544, "no_speech_prob": 0.007455643266439438}, {"id": 133, "seek": 69400, "start": 707.0, "end": 713.0, "text": " uh, your conversation with, with GPT, you just push a button and it creates a", "tokens": [51014, 2232, 11, 428, 3761, 365, 11, 365, 26039, 51, 11, 291, 445, 2944, 257, 2960, 293, 309, 7829, 257, 51314], "temperature": 0.0, "avg_logprob": -0.1668428564971348, "compression_ratio": 1.544, "no_speech_prob": 0.007455643266439438}, {"id": 134, "seek": 69400, "start": 713.0, "end": 718.0, "text": " URL that has the entire thing. But for some reason this, because I copy and", "tokens": [51314, 12905, 300, 575, 264, 2302, 551, 13, 583, 337, 512, 1778, 341, 11, 570, 286, 5055, 293, 51564], "temperature": 0.0, "avg_logprob": -0.1668428564971348, "compression_ratio": 1.544, "no_speech_prob": 0.007455643266439438}, {"id": 135, "seek": 69400, "start": 718.0, "end": 723.0, "text": " pasted here, this tiny little index HTML file ahead. But what it was, I'll tell", "tokens": [51564, 1791, 292, 510, 11, 341, 5870, 707, 8186, 17995, 3991, 2286, 13, 583, 437, 309, 390, 11, 286, 603, 980, 51814], "temperature": 0.0, "avg_logprob": -0.1668428564971348, "compression_ratio": 1.544, "no_speech_prob": 0.007455643266439438}, {"id": 136, "seek": 72300, "start": 723.0, "end": 729.0, "text": " you right now is simply, it said here to create a, create a canvas.", "tokens": [50364, 291, 558, 586, 307, 2935, 11, 309, 848, 510, 281, 1884, 257, 11, 1884, 257, 16267, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19897331159139417, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0029794019646942616}, {"id": 137, "seek": 72300, "start": 729.0, "end": 736.0, "text": " So it was just basically the rear bones of a HTML file with a canvas and nothing", "tokens": [50664, 407, 309, 390, 445, 1936, 264, 8250, 10491, 295, 257, 17995, 3991, 365, 257, 16267, 293, 1825, 51014], "temperature": 0.0, "avg_logprob": -0.19897331159139417, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0029794019646942616}, {"id": 138, "seek": 72300, "start": 736.0, "end": 742.0, "text": " really else. And, you know, it's, it responds always in this real nice,", "tokens": [51014, 534, 1646, 13, 400, 11, 291, 458, 11, 309, 311, 11, 309, 27331, 1009, 294, 341, 957, 1481, 11, 51314], "temperature": 0.0, "avg_logprob": -0.19897331159139417, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0029794019646942616}, {"id": 139, "seek": 72300, "start": 742.0, "end": 746.0, "text": " encouraging, friendly way that a good tutor maybe should be, you know, well,", "tokens": [51314, 14580, 11, 9208, 636, 300, 257, 665, 35613, 1310, 820, 312, 11, 291, 458, 11, 731, 11, 51514], "temperature": 0.0, "avg_logprob": -0.19897331159139417, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0029794019646942616}, {"id": 140, "seek": 72300, "start": 746.0, "end": 750.0, "text": " that was a good starting point of our recommending a few more things you could", "tokens": [51514, 300, 390, 257, 665, 2891, 935, 295, 527, 30559, 257, 1326, 544, 721, 291, 727, 51714], "temperature": 0.0, "avg_logprob": -0.19897331159139417, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0029794019646942616}, {"id": 141, "seek": 75000, "start": 750.0, "end": 754.0, "text": " add these more features. And then it generates this, and you could just", "tokens": [50364, 909, 613, 544, 4122, 13, 400, 550, 309, 23815, 341, 11, 293, 291, 727, 445, 50564], "temperature": 0.0, "avg_logprob": -0.11535919706026714, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.00831195991486311}, {"id": 142, "seek": 75000, "start": 754.0, "end": 760.0, "text": " simply in the live version of this, I just click on copy code and then it's on", "tokens": [50564, 2935, 294, 264, 1621, 3037, 295, 341, 11, 286, 445, 2052, 322, 5055, 3089, 293, 550, 309, 311, 322, 50864], "temperature": 0.0, "avg_logprob": -0.11535919706026714, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.00831195991486311}, {"id": 143, "seek": 75000, "start": 760.0, "end": 766.0, "text": " the clipboard. And then I just create a folder called fireworks and I create a", "tokens": [50864, 264, 7353, 3787, 13, 400, 550, 286, 445, 1884, 257, 10820, 1219, 28453, 293, 286, 1884, 257, 51164], "temperature": 0.0, "avg_logprob": -0.11535919706026714, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.00831195991486311}, {"id": 144, "seek": 75000, "start": 766.0, "end": 771.0, "text": " file called index dot HTML and just paste this into it. You know, so I don't", "tokens": [51164, 3991, 1219, 8186, 5893, 17995, 293, 445, 9163, 341, 666, 309, 13, 509, 458, 11, 370, 286, 500, 380, 51414], "temperature": 0.0, "avg_logprob": -0.11535919706026714, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.00831195991486311}, {"id": 145, "seek": 75000, "start": 771.0, "end": 777.0, "text": " really need to be very technically competent to, to have this kind of", "tokens": [51414, 534, 643, 281, 312, 588, 12120, 29998, 281, 11, 281, 362, 341, 733, 295, 51714], "temperature": 0.0, "avg_logprob": -0.11535919706026714, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.00831195991486311}, {"id": 146, "seek": 77700, "start": 777.0, "end": 785.0, "text": " experience. So then, remember it said also here, that I should use this get", "tokens": [50364, 1752, 13, 407, 550, 11, 1604, 309, 848, 611, 510, 11, 300, 286, 820, 764, 341, 483, 50764], "temperature": 0.0, "avg_logprob": -0.1061827023824056, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.0017538656247779727}, {"id": 147, "seek": 77700, "start": 785.0, "end": 790.0, "text": " element by ID and get context. So I'm assuming I knew a little bit here. So I", "tokens": [50764, 4478, 538, 7348, 293, 483, 4319, 13, 407, 286, 478, 11926, 286, 2586, 257, 707, 857, 510, 13, 407, 286, 51014], "temperature": 0.0, "avg_logprob": -0.1061827023824056, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.0017538656247779727}, {"id": 148, "seek": 77700, "start": 790.0, "end": 796.0, "text": " said, you know, oh, I, I just started, I did these two lines of code. And it's,", "tokens": [51014, 848, 11, 291, 458, 11, 1954, 11, 286, 11, 286, 445, 1409, 11, 286, 630, 613, 732, 3876, 295, 3089, 13, 400, 309, 311, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1061827023824056, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.0017538656247779727}, {"id": 149, "seek": 77700, "start": 796.0, "end": 803.0, "text": " you know, great start and all that. So then it suggests that I size the canvas", "tokens": [51314, 291, 458, 11, 869, 722, 293, 439, 300, 13, 407, 550, 309, 13409, 300, 286, 2744, 264, 16267, 51664], "temperature": 0.0, "avg_logprob": -0.1061827023824056, "compression_ratio": 1.5522388059701493, "no_speech_prob": 0.0017538656247779727}, {"id": 150, "seek": 80300, "start": 803.0, "end": 812.0, "text": " and have it adjust whenever the browser windows is resized so that you always", "tokens": [50364, 293, 362, 309, 4369, 5699, 264, 11185, 9309, 307, 725, 1602, 370, 300, 291, 1009, 50814], "temperature": 0.0, "avg_logprob": -0.0830726286944221, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003272028174251318}, {"id": 151, "seek": 80300, "start": 812.0, "end": 818.0, "text": " have a canvas that's the right size. So I just again, you know, copy code and", "tokens": [50814, 362, 257, 16267, 300, 311, 264, 558, 2744, 13, 407, 286, 445, 797, 11, 291, 458, 11, 5055, 3089, 293, 51114], "temperature": 0.0, "avg_logprob": -0.0830726286944221, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003272028174251318}, {"id": 152, "seek": 80300, "start": 818.0, "end": 823.0, "text": " just pasted it in. But it said the next thing it said here was to create a", "tokens": [51114, 445, 1791, 292, 309, 294, 13, 583, 309, 848, 264, 958, 551, 309, 848, 510, 390, 281, 1884, 257, 51364], "temperature": 0.0, "avg_logprob": -0.0830726286944221, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003272028174251318}, {"id": 153, "seek": 80300, "start": 823.0, "end": 829.0, "text": " class for fireworks. So I pretended not to know how to do that. So I said, how do", "tokens": [51364, 1508, 337, 28453, 13, 407, 286, 45056, 406, 281, 458, 577, 281, 360, 300, 13, 407, 286, 848, 11, 577, 360, 51664], "temperature": 0.0, "avg_logprob": -0.0830726286944221, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003272028174251318}, {"id": 154, "seek": 82900, "start": 829.0, "end": 836.0, "text": " I define a class? And it gives a nice explanation and gives a nice little generic", "tokens": [50364, 286, 6964, 257, 1508, 30, 400, 309, 2709, 257, 1481, 10835, 293, 2709, 257, 1481, 707, 19577, 50714], "temperature": 0.0, "avg_logprob": -0.12003304318683904, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0011691258987411857}, {"id": 155, "seek": 82900, "start": 836.0, "end": 844.0, "text": " example of a class and how you can make an instance. And then I actually made this", "tokens": [50714, 1365, 295, 257, 1508, 293, 577, 291, 393, 652, 364, 5197, 13, 400, 550, 286, 767, 1027, 341, 51114], "temperature": 0.0, "avg_logprob": -0.12003304318683904, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0011691258987411857}, {"id": 156, "seek": 82900, "start": 844.0, "end": 851.0, "text": " mistake. I meant to say how's this and paste in my little bit of code. But it", "tokens": [51114, 6146, 13, 286, 4140, 281, 584, 577, 311, 341, 293, 9163, 294, 452, 707, 857, 295, 3089, 13, 583, 309, 51464], "temperature": 0.0, "avg_logprob": -0.12003304318683904, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0011691258987411857}, {"id": 157, "seek": 82900, "start": 851.0, "end": 856.0, "text": " noticed that I forgot to include whether the code that it wanted me to review. So", "tokens": [51464, 5694, 300, 286, 5298, 281, 4090, 1968, 264, 3089, 300, 309, 1415, 385, 281, 3131, 13, 407, 51714], "temperature": 0.0, "avg_logprob": -0.12003304318683904, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0011691258987411857}, {"id": 158, "seek": 85600, "start": 856.0, "end": 861.0, "text": " here. So then I just pasted it in. So this is, you know, my attempt to be a", "tokens": [50364, 510, 13, 407, 550, 286, 445, 1791, 292, 309, 294, 13, 407, 341, 307, 11, 291, 458, 11, 452, 5217, 281, 312, 257, 50614], "temperature": 0.0, "avg_logprob": -0.09067828566939742, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.006286703981459141}, {"id": 159, "seek": 85600, "start": 861.0, "end": 865.0, "text": " beginning student who kind of figured out from what it was saying that this is", "tokens": [50614, 2863, 3107, 567, 733, 295, 8932, 484, 490, 437, 309, 390, 1566, 300, 341, 307, 50814], "temperature": 0.0, "avg_logprob": -0.09067828566939742, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.006286703981459141}, {"id": 160, "seek": 85600, "start": 865.0, "end": 871.0, "text": " what, what I should do. And again, you know, it's encouraging. It's on the right", "tokens": [50814, 437, 11, 437, 286, 820, 360, 13, 400, 797, 11, 291, 458, 11, 309, 311, 14580, 13, 467, 311, 322, 264, 558, 51114], "temperature": 0.0, "avg_logprob": -0.09067828566939742, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.006286703981459141}, {"id": 161, "seek": 85600, "start": 871.0, "end": 876.0, "text": " track. But here's a few suggestions, you know, the convention is this should be", "tokens": [51114, 2837, 13, 583, 510, 311, 257, 1326, 13396, 11, 291, 458, 11, 264, 10286, 307, 341, 820, 312, 51364], "temperature": 0.0, "avg_logprob": -0.09067828566939742, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.006286703981459141}, {"id": 162, "seek": 85600, "start": 876.0, "end": 883.0, "text": " kept. Start with a capital letter. You know, I should have a update method and", "tokens": [51364, 4305, 13, 6481, 365, 257, 4238, 5063, 13, 509, 458, 11, 286, 820, 362, 257, 5623, 3170, 293, 51714], "temperature": 0.0, "avg_logprob": -0.09067828566939742, "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.006286703981459141}, {"id": 163, "seek": 88300, "start": 883.0, "end": 891.0, "text": " maybe on a gravity be good. So, so it suggests this update and then I should", "tokens": [50364, 1310, 322, 257, 12110, 312, 665, 13, 407, 11, 370, 309, 13409, 341, 5623, 293, 550, 286, 820, 50764], "temperature": 0.0, "avg_logprob": -0.2036430952978916, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.0024715696927160025}, {"id": 164, "seek": 88300, "start": 891.0, "end": 904.0, "text": " define these two methods. And then I was thinking that I, you know, if I'm assuming", "tokens": [50764, 6964, 613, 732, 7150, 13, 400, 550, 286, 390, 1953, 300, 286, 11, 291, 458, 11, 498, 286, 478, 11926, 51414], "temperature": 0.0, "avg_logprob": -0.2036430952978916, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.0024715696927160025}, {"id": 165, "seek": 88300, "start": 904.0, "end": 909.0, "text": " that a student at least understands a little bit about velocity and gravity is not", "tokens": [51414, 300, 257, 3107, 412, 1935, 15146, 257, 707, 857, 466, 9269, 293, 12110, 307, 406, 51664], "temperature": 0.0, "avg_logprob": -0.2036430952978916, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.0024715696927160025}, {"id": 166, "seek": 90900, "start": 909.0, "end": 914.0, "text": " the same, I'm sure they could have asked chat GPT here, but, but I didn't know", "tokens": [50364, 264, 912, 11, 286, 478, 988, 436, 727, 362, 2351, 5081, 26039, 51, 510, 11, 457, 11, 457, 286, 994, 380, 458, 50614], "temperature": 0.0, "avg_logprob": -0.21630308800136921, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.012048893608152866}, {"id": 167, "seek": 90900, "start": 914.0, "end": 919.0, "text": " whether it would matter if I update the vertical position and then update the", "tokens": [50614, 1968, 309, 576, 1871, 498, 286, 5623, 264, 9429, 2535, 293, 550, 5623, 264, 50864], "temperature": 0.0, "avg_logprob": -0.21630308800136921, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.012048893608152866}, {"id": 168, "seek": 90900, "start": 919.0, "end": 924.0, "text": " velocity or update the velocity and then the vertical position. And it says", "tokens": [50864, 9269, 420, 5623, 264, 9269, 293, 550, 264, 9429, 2535, 13, 400, 309, 1619, 51114], "temperature": 0.0, "avg_logprob": -0.21630308800136921, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.012048893608152866}, {"id": 169, "seek": 90900, "start": 924.0, "end": 929.0, "text": " differently minimal, but most people do it this way. And it's a little bit better", "tokens": [51114, 7614, 13206, 11, 457, 881, 561, 360, 309, 341, 636, 13, 400, 309, 311, 257, 707, 857, 1101, 51364], "temperature": 0.0, "avg_logprob": -0.21630308800136921, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.012048893608152866}, {"id": 170, "seek": 90900, "start": 929.0, "end": 937.0, "text": " to do it in this order. And then it gives me the code for it.", "tokens": [51364, 281, 360, 309, 294, 341, 1668, 13, 400, 550, 309, 2709, 385, 264, 3089, 337, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21630308800136921, "compression_ratio": 1.7488372093023257, "no_speech_prob": 0.012048893608152866}, {"id": 171, "seek": 93700, "start": 938.0, "end": 944.0, "text": " At that point, I pretend not to know how to, how to actually draw the fireworks", "tokens": [50414, 1711, 300, 935, 11, 286, 11865, 406, 281, 458, 577, 281, 11, 577, 281, 767, 2642, 264, 28453, 50714], "temperature": 0.0, "avg_logprob": -0.10200698955638988, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.0030738767236471176}, {"id": 172, "seek": 93700, "start": 944.0, "end": 950.0, "text": " rocket. And, you know, it says, Oh, well, you know, we could do it. The simplest", "tokens": [50714, 13012, 13, 400, 11, 291, 458, 11, 309, 1619, 11, 876, 11, 731, 11, 291, 458, 11, 321, 727, 360, 309, 13, 440, 22811, 51014], "temperature": 0.0, "avg_logprob": -0.10200698955638988, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.0030738767236471176}, {"id": 173, "seek": 93700, "start": 950.0, "end": 954.0, "text": " thing would be to make it be a circle. And here's the code and notice it generated", "tokens": [51014, 551, 576, 312, 281, 652, 309, 312, 257, 6329, 13, 400, 510, 311, 264, 3089, 293, 3449, 309, 10833, 51214], "temperature": 0.0, "avg_logprob": -0.10200698955638988, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.0030738767236471176}, {"id": 174, "seek": 93700, "start": 954.0, "end": 959.0, "text": " all these comments and so on. And then it gives a nice explanation of how this works", "tokens": [51214, 439, 613, 3053, 293, 370, 322, 13, 400, 550, 309, 2709, 257, 1481, 10835, 295, 577, 341, 1985, 51464], "temperature": 0.0, "avg_logprob": -0.10200698955638988, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.0030738767236471176}, {"id": 175, "seek": 93700, "start": 959.0, "end": 966.0, "text": " and what it's doing. So now I have the bare minimum of this program, but I guess", "tokens": [51464, 293, 437, 309, 311, 884, 13, 407, 586, 286, 362, 264, 6949, 7285, 295, 341, 1461, 11, 457, 286, 2041, 51814], "temperature": 0.0, "avg_logprob": -0.10200698955638988, "compression_ratio": 1.623015873015873, "no_speech_prob": 0.0030738767236471176}, {"id": 176, "seek": 96600, "start": 966.0, "end": 972.0, "text": " I'm pretending that I have no idea how to even test this. So it says, Oh, well,", "tokens": [50364, 286, 478, 22106, 300, 286, 362, 572, 1558, 577, 281, 754, 1500, 341, 13, 407, 309, 1619, 11, 876, 11, 731, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10114009380340576, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0007319134892895818}, {"id": 177, "seek": 96600, "start": 972.0, "end": 978.0, "text": " you could create a sample fireworks with these parameters and then create an", "tokens": [50664, 291, 727, 1884, 257, 6889, 28453, 365, 613, 9834, 293, 550, 1884, 364, 50964], "temperature": 0.0, "avg_logprob": -0.10114009380340576, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0007319134892895818}, {"id": 178, "seek": 96600, "start": 978.0, "end": 986.0, "text": " animation loop that just continually calls update and draw. And again, it", "tokens": [50964, 9603, 6367, 300, 445, 22277, 5498, 5623, 293, 2642, 13, 400, 797, 11, 309, 51364], "temperature": 0.0, "avg_logprob": -0.10114009380340576, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0007319134892895818}, {"id": 179, "seek": 96600, "start": 986.0, "end": 991.0, "text": " explains what's going on and how to get this to work. And then it says, you know,", "tokens": [51364, 13948, 437, 311, 516, 322, 293, 577, 281, 483, 341, 281, 589, 13, 400, 550, 309, 1619, 11, 291, 458, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10114009380340576, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.0007319134892895818}, {"id": 180, "seek": 99100, "start": 992.0, "end": 997.0, "text": " this code should be in the file called fireworks.js. And then just click on the", "tokens": [50414, 341, 3089, 820, 312, 294, 264, 3991, 1219, 28453, 13, 25530, 13, 400, 550, 445, 2052, 322, 264, 50664], "temperature": 0.0, "avg_logprob": -0.1278989592263865, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.004197549540549517}, {"id": 181, "seek": 99100, "start": 997.0, "end": 1003.0, "text": " index.html file in the browser. And you'll see it working. And then it doesn't", "tokens": [50664, 8186, 13, 357, 15480, 3991, 294, 264, 11185, 13, 400, 291, 603, 536, 309, 1364, 13, 400, 550, 309, 1177, 380, 50964], "temperature": 0.0, "avg_logprob": -0.1278989592263865, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.004197549540549517}, {"id": 182, "seek": 99100, "start": 1003.0, "end": 1010.0, "text": " working. So I was smart enough to say, Well, how could I see if there's an error", "tokens": [50964, 1364, 13, 407, 286, 390, 4069, 1547, 281, 584, 11, 1042, 11, 577, 727, 286, 536, 498, 456, 311, 364, 6713, 51314], "temperature": 0.0, "avg_logprob": -0.1278989592263865, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.004197549540549517}, {"id": 183, "seek": 99100, "start": 1010.0, "end": 1015.0, "text": " message? And then it tells me about different ways I could get to the console", "tokens": [51314, 3636, 30, 400, 550, 309, 5112, 385, 466, 819, 2098, 286, 727, 483, 281, 264, 11076, 51564], "temperature": 0.0, "avg_logprob": -0.1278989592263865, "compression_ratio": 1.601010101010101, "no_speech_prob": 0.004197549540549517}, {"id": 184, "seek": 101500, "start": 1015.0, "end": 1020.0, "text": " and see what an error message is. And then I copy and pasted the error message in", "tokens": [50364, 293, 536, 437, 364, 6713, 3636, 307, 13, 400, 550, 286, 5055, 293, 1791, 292, 264, 6713, 3636, 294, 50614], "temperature": 0.0, "avg_logprob": -0.11267156954164859, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.010813429020345211}, {"id": 185, "seek": 101500, "start": 1020.0, "end": 1027.0, "text": " here. And it realized that the problem was that the JavaScript was running before", "tokens": [50614, 510, 13, 400, 309, 5334, 300, 264, 1154, 390, 300, 264, 15778, 390, 2614, 949, 50964], "temperature": 0.0, "avg_logprob": -0.11267156954164859, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.010813429020345211}, {"id": 186, "seek": 101500, "start": 1027.0, "end": 1032.0, "text": " the page had fully loaded. And it gives two ways of solving this. You use the", "tokens": [50964, 264, 3028, 632, 4498, 13210, 13, 400, 309, 2709, 732, 2098, 295, 12606, 341, 13, 509, 764, 264, 51214], "temperature": 0.0, "avg_logprob": -0.11267156954164859, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.010813429020345211}, {"id": 187, "seek": 101500, "start": 1032.0, "end": 1041.0, "text": " Windows on load, or you could simply move the script later in the HTML. And I did", "tokens": [51214, 8591, 322, 3677, 11, 420, 291, 727, 2935, 1286, 264, 5755, 1780, 294, 264, 17995, 13, 400, 286, 630, 51664], "temperature": 0.0, "avg_logprob": -0.11267156954164859, "compression_ratio": 1.599009900990099, "no_speech_prob": 0.010813429020345211}, {"id": 188, "seek": 104100, "start": 1041.0, "end": 1052.0, "text": " the letter. So at that point, I saw the rocket go up. But it wasn't exploding yet,", "tokens": [50364, 264, 5063, 13, 407, 412, 300, 935, 11, 286, 1866, 264, 13012, 352, 493, 13, 583, 309, 2067, 380, 35175, 1939, 11, 50914], "temperature": 0.0, "avg_logprob": -0.12567065102713448, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.0010158952791243792}, {"id": 189, "seek": 104100, "start": 1052.0, "end": 1057.0, "text": " of course. So I didn't have a good idea how to do the explosion. And it comes up", "tokens": [50914, 295, 1164, 13, 407, 286, 994, 380, 362, 257, 665, 1558, 577, 281, 360, 264, 15673, 13, 400, 309, 1487, 493, 51164], "temperature": 0.0, "avg_logprob": -0.12567065102713448, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.0010158952791243792}, {"id": 190, "seek": 104100, "start": 1057.0, "end": 1065.0, "text": " with this plan for how to do it and a basic outline of how to implement it. You", "tokens": [51164, 365, 341, 1393, 337, 577, 281, 360, 309, 293, 257, 3875, 16387, 295, 577, 281, 4445, 309, 13, 509, 51564], "temperature": 0.0, "avg_logprob": -0.12567065102713448, "compression_ratio": 1.4550898203592815, "no_speech_prob": 0.0010158952791243792}, {"id": 191, "seek": 106500, "start": 1065.0, "end": 1073.0, "text": " know, where it makes 50 particles and they all go off in random directions and", "tokens": [50364, 458, 11, 689, 309, 1669, 2625, 10007, 293, 436, 439, 352, 766, 294, 4974, 11095, 293, 50764], "temperature": 0.0, "avg_logprob": -0.16978926494203764, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.006690214388072491}, {"id": 192, "seek": 106500, "start": 1073.0, "end": 1087.0, "text": " with some random speeds. And so then I thought, well, it wasn't clear if I should", "tokens": [50764, 365, 512, 4974, 16411, 13, 400, 370, 550, 286, 1194, 11, 731, 11, 309, 2067, 380, 1850, 498, 286, 820, 51464], "temperature": 0.0, "avg_logprob": -0.16978926494203764, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.006690214388072491}, {"id": 193, "seek": 106500, "start": 1087.0, "end": 1093.0, "text": " just, because the fireworks rocket and the particles had very similar ways of", "tokens": [51464, 445, 11, 570, 264, 28453, 13012, 293, 264, 10007, 632, 588, 2531, 2098, 295, 51764], "temperature": 0.0, "avg_logprob": -0.16978926494203764, "compression_ratio": 1.4424242424242424, "no_speech_prob": 0.006690214388072491}, {"id": 194, "seek": 109300, "start": 1093.0, "end": 1098.0, "text": " being drawn and being updated, whether I should really just copy the same code", "tokens": [50364, 885, 10117, 293, 885, 10588, 11, 1968, 286, 820, 534, 445, 5055, 264, 912, 3089, 50614], "temperature": 0.0, "avg_logprob": -0.1306256899019567, "compression_ratio": 1.59, "no_speech_prob": 0.010978867299854755}, {"id": 195, "seek": 109300, "start": 1098.0, "end": 1103.0, "text": " and it says, oh, well, you know, you could create a base class and then have a", "tokens": [50614, 293, 309, 1619, 11, 1954, 11, 731, 11, 291, 458, 11, 291, 727, 1884, 257, 3096, 1508, 293, 550, 362, 257, 50864], "temperature": 0.0, "avg_logprob": -0.1306256899019567, "compression_ratio": 1.59, "no_speech_prob": 0.010978867299854755}, {"id": 196, "seek": 109300, "start": 1103.0, "end": 1109.0, "text": " called moving object and then fireworks and particles could be subclasses that", "tokens": [50864, 1219, 2684, 2657, 293, 550, 28453, 293, 10007, 727, 312, 1422, 11665, 279, 300, 51164], "temperature": 0.0, "avg_logprob": -0.1306256899019567, "compression_ratio": 1.59, "no_speech_prob": 0.010978867299854755}, {"id": 197, "seek": 109300, "start": 1109.0, "end": 1114.0, "text": " extend it. And again, it's just giving you the sketch of how to do it. It doesn't", "tokens": [51164, 10101, 309, 13, 400, 797, 11, 309, 311, 445, 2902, 291, 264, 12325, 295, 577, 281, 360, 309, 13, 467, 1177, 380, 51414], "temperature": 0.0, "avg_logprob": -0.1306256899019567, "compression_ratio": 1.59, "no_speech_prob": 0.010978867299854755}, {"id": 198, "seek": 111400, "start": 1114.0, "end": 1124.0, "text": " matter if it's filling everything for you right now. So, so I kind of follow this", "tokens": [50364, 1871, 498, 309, 311, 10623, 1203, 337, 291, 558, 586, 13, 407, 11, 370, 286, 733, 295, 1524, 341, 50864], "temperature": 0.0, "avg_logprob": -0.2543637454509735, "compression_ratio": 1.467065868263473, "no_speech_prob": 0.256547212600708}, {"id": 199, "seek": 111400, "start": 1124.0, "end": 1132.0, "text": " advice, but I didn't understand again, how I could test it, because remember the", "tokens": [50864, 5192, 11, 457, 286, 994, 380, 1223, 797, 11, 577, 286, 727, 1500, 309, 11, 570, 1604, 264, 51264], "temperature": 0.0, "avg_logprob": -0.2543637454509735, "compression_ratio": 1.467065868263473, "no_speech_prob": 0.256547212600708}, {"id": 200, "seek": 111400, "start": 1132.0, "end": 1135.0, "text": " test before was just whether you could get a rocket to go up and how do I test the", "tokens": [51264, 1500, 949, 390, 445, 1968, 291, 727, 483, 257, 13012, 281, 352, 493, 293, 577, 360, 286, 1500, 264, 51414], "temperature": 0.0, "avg_logprob": -0.2543637454509735, "compression_ratio": 1.467065868263473, "no_speech_prob": 0.256547212600708}, {"id": 201, "seek": 113500, "start": 1135.0, "end": 1145.0, "text": " thing. And it comes up with, again, a nice little plan for doing it. And then it", "tokens": [50364, 551, 13, 400, 309, 1487, 493, 365, 11, 797, 11, 257, 1481, 707, 1393, 337, 884, 309, 13, 400, 550, 309, 50864], "temperature": 0.0, "avg_logprob": -0.25126113313617127, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.40276241302490234}, {"id": 202, "seek": 113500, "start": 1145.0, "end": 1153.0, "text": " gives the sort of sketch of the code and leaves some things for me to do. And", "tokens": [50864, 2709, 264, 1333, 295, 12325, 295, 264, 3089, 293, 5510, 512, 721, 337, 385, 281, 360, 13, 400, 51264], "temperature": 0.0, "avg_logprob": -0.25126113313617127, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.40276241302490234}, {"id": 203, "seek": 113500, "start": 1153.0, "end": 1163.0, "text": " again, it explains what to do. So it was giving me explanations and I said, how", "tokens": [51264, 797, 11, 309, 13948, 437, 281, 360, 13, 407, 309, 390, 2902, 385, 28708, 293, 286, 848, 11, 577, 51764], "temperature": 0.0, "avg_logprob": -0.25126113313617127, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.40276241302490234}, {"id": 204, "seek": 116300, "start": 1163.0, "end": 1175.0, "text": " about this for creating a test fireworks. And it said it looks good. And it'll create", "tokens": [50364, 466, 341, 337, 4084, 257, 1500, 28453, 13, 400, 309, 848, 309, 1542, 665, 13, 400, 309, 603, 1884, 50964], "temperature": 0.0, "avg_logprob": -0.16608521143595378, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.004466192796826363}, {"id": 205, "seek": 116300, "start": 1175.0, "end": 1181.0, "text": " one, you know, explains what it'll do. It'll be at the bottom and somewhere, but", "tokens": [50964, 472, 11, 291, 458, 11, 13948, 437, 309, 603, 360, 13, 467, 603, 312, 412, 264, 2767, 293, 4079, 11, 457, 51264], "temperature": 0.0, "avg_logprob": -0.16608521143595378, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.004466192796826363}, {"id": 206, "seek": 116300, "start": 1181.0, "end": 1186.0, "text": " it'll be red and initial velocity and with gravity or something. And then it's trying", "tokens": [51264, 309, 603, 312, 2182, 293, 5883, 9269, 293, 365, 12110, 420, 746, 13, 400, 550, 309, 311, 1382, 51514], "temperature": 0.0, "avg_logprob": -0.16608521143595378, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.004466192796826363}, {"id": 207, "seek": 116300, "start": 1186.0, "end": 1192.0, "text": " to be kind of, again, I think it's pretty good that it doesn't just make me feel like", "tokens": [51514, 281, 312, 733, 295, 11, 797, 11, 286, 519, 309, 311, 1238, 665, 300, 309, 1177, 380, 445, 652, 385, 841, 411, 51814], "temperature": 0.0, "avg_logprob": -0.16608521143595378, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.004466192796826363}, {"id": 208, "seek": 119200, "start": 1192.0, "end": 1197.0, "text": " I'm, you know, ignorant and it's so knowledgeable. It says, well, here's a small", "tokens": [50364, 286, 478, 11, 291, 458, 11, 29374, 293, 309, 311, 370, 33800, 13, 467, 1619, 11, 731, 11, 510, 311, 257, 1359, 50614], "temperature": 0.0, "avg_logprob": -0.14866897671721702, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.006483136676251888}, {"id": 209, "seek": 119200, "start": 1197.0, "end": 1204.0, "text": " suggestion, you know, be nice to make the colors random rather than just all of them", "tokens": [50614, 16541, 11, 291, 458, 11, 312, 1481, 281, 652, 264, 4577, 4974, 2831, 813, 445, 439, 295, 552, 50964], "temperature": 0.0, "avg_logprob": -0.14866897671721702, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.006483136676251888}, {"id": 210, "seek": 119200, "start": 1204.0, "end": 1211.0, "text": " being red and provides a very simple little function for coming up with a random", "tokens": [50964, 885, 2182, 293, 6417, 257, 588, 2199, 707, 2445, 337, 1348, 493, 365, 257, 4974, 51314], "temperature": 0.0, "avg_logprob": -0.14866897671721702, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.006483136676251888}, {"id": 211, "seek": 119200, "start": 1211.0, "end": 1218.0, "text": " color and then shows how I could use it. So I tried all that. And now remember, it", "tokens": [51314, 2017, 293, 550, 3110, 577, 286, 727, 764, 309, 13, 407, 286, 3031, 439, 300, 13, 400, 586, 1604, 11, 309, 51664], "temperature": 0.0, "avg_logprob": -0.14866897671721702, "compression_ratio": 1.5302325581395348, "no_speech_prob": 0.006483136676251888}, {"id": 212, "seek": 121800, "start": 1218.0, "end": 1223.0, "text": " doesn't have any idea with what's going on. I have to always tell it how things are", "tokens": [50364, 1177, 380, 362, 604, 1558, 365, 437, 311, 516, 322, 13, 286, 362, 281, 1009, 980, 309, 577, 721, 366, 50614], "temperature": 0.0, "avg_logprob": -0.11009653876809512, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00271308864466846}, {"id": 213, "seek": 121800, "start": 1223.0, "end": 1229.0, "text": " going. And I said, I think the fireworks would only go about a third of the way up", "tokens": [50614, 516, 13, 400, 286, 848, 11, 286, 519, 264, 28453, 576, 787, 352, 466, 257, 2636, 295, 264, 636, 493, 50914], "temperature": 0.0, "avg_logprob": -0.11009653876809512, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00271308864466846}, {"id": 214, "seek": 121800, "start": 1229.0, "end": 1234.0, "text": " of the screen and blow up and they weren't using the full canvas. And it gives a nice", "tokens": [50914, 295, 264, 2568, 293, 6327, 493, 293, 436, 4999, 380, 1228, 264, 1577, 16267, 13, 400, 309, 2709, 257, 1481, 51164], "temperature": 0.0, "avg_logprob": -0.11009653876809512, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00271308864466846}, {"id": 215, "seek": 121800, "start": 1234.0, "end": 1240.0, "text": " explanation that depending on the initial velocity and the gravity value, they may not", "tokens": [51164, 10835, 300, 5413, 322, 264, 5883, 9269, 293, 264, 12110, 2158, 11, 436, 815, 406, 51464], "temperature": 0.0, "avg_logprob": -0.11009653876809512, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00271308864466846}, {"id": 216, "seek": 124000, "start": 1240.0, "end": 1249.0, "text": " be in balance. So it suggests that I increased the velocity and decreased gravity. And I", "tokens": [50364, 312, 294, 4772, 13, 407, 309, 13409, 300, 286, 6505, 264, 9269, 293, 24436, 12110, 13, 400, 286, 50814], "temperature": 0.0, "avg_logprob": -0.12696094361562577, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.030196011066436768}, {"id": 217, "seek": 124000, "start": 1249.0, "end": 1255.0, "text": " tried it and now they flew off the top of the canvas. But because it kind of explained", "tokens": [50814, 3031, 309, 293, 586, 436, 15728, 766, 264, 1192, 295, 264, 16267, 13, 583, 570, 309, 733, 295, 8825, 51114], "temperature": 0.0, "avg_logprob": -0.12696094361562577, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.030196011066436768}, {"id": 218, "seek": 124000, "start": 1255.0, "end": 1261.0, "text": " what was going on, I just only increased the velocity and left gravity alone and it was", "tokens": [51114, 437, 390, 516, 322, 11, 286, 445, 787, 6505, 264, 9269, 293, 1411, 12110, 3312, 293, 309, 390, 51414], "temperature": 0.0, "avg_logprob": -0.12696094361562577, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.030196011066436768}, {"id": 219, "seek": 126100, "start": 1261.0, "end": 1275.0, "text": " working just fine. And it approved my changes to its suggestions. And, you know, it's", "tokens": [50364, 1364, 445, 2489, 13, 400, 309, 10826, 452, 2962, 281, 1080, 13396, 13, 400, 11, 291, 458, 11, 309, 311, 51064], "temperature": 0.0, "avg_logprob": -0.13452944115026674, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.08027005195617676}, {"id": 220, "seek": 126100, "start": 1275.0, "end": 1280.0, "text": " saying there's no correct value, you could play with different ones, you could explore", "tokens": [51064, 1566, 456, 311, 572, 3006, 2158, 11, 291, 727, 862, 365, 819, 2306, 11, 291, 727, 6839, 51314], "temperature": 0.0, "avg_logprob": -0.13452944115026674, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.08027005195617676}, {"id": 221, "seek": 126100, "start": 1280.0, "end": 1285.0, "text": " until you get something you're happy with or something. So then I ask, well, how can I add", "tokens": [51314, 1826, 291, 483, 746, 291, 434, 2055, 365, 420, 746, 13, 407, 550, 286, 1029, 11, 731, 11, 577, 393, 286, 909, 51564], "temperature": 0.0, "avg_logprob": -0.13452944115026674, "compression_ratio": 1.5202312138728324, "no_speech_prob": 0.08027005195617676}, {"id": 222, "seek": 128500, "start": 1285.0, "end": 1292.0, "text": " effects to this? And it explains that there's the audio API built into the browser and it", "tokens": [50364, 5065, 281, 341, 30, 400, 309, 13948, 300, 456, 311, 264, 6278, 9362, 3094, 666, 264, 11185, 293, 309, 50714], "temperature": 0.0, "avg_logprob": -0.1326936833998736, "compression_ratio": 1.536723163841808, "no_speech_prob": 0.10799790918827057}, {"id": 223, "seek": 128500, "start": 1292.0, "end": 1299.0, "text": " can read file, I mean, play files with these extensions. And it creates a tiny little function", "tokens": [50714, 393, 1401, 3991, 11, 286, 914, 11, 862, 7098, 365, 613, 25129, 13, 400, 309, 7829, 257, 5870, 707, 2445, 51064], "temperature": 0.0, "avg_logprob": -0.1326936833998736, "compression_ratio": 1.536723163841808, "no_speech_prob": 0.10799790918827057}, {"id": 224, "seek": 128500, "start": 1299.0, "end": 1307.0, "text": " for doing it. And it suggests that, you know, you put play sound of the launch when the", "tokens": [51064, 337, 884, 309, 13, 400, 309, 13409, 300, 11, 291, 458, 11, 291, 829, 862, 1626, 295, 264, 4025, 562, 264, 51464], "temperature": 0.0, "avg_logprob": -0.1326936833998736, "compression_ratio": 1.536723163841808, "no_speech_prob": 0.10799790918827057}, {"id": 225, "seek": 130700, "start": 1307.0, "end": 1317.0, "text": " fireworks are created and then you play the explosion when it explodes. And then I ask", "tokens": [50364, 28453, 366, 2942, 293, 550, 291, 862, 264, 15673, 562, 309, 42610, 13, 400, 550, 286, 1029, 50864], "temperature": 0.0, "avg_logprob": -0.16327308786326442, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.38430655002593994}, {"id": 226, "seek": 130700, "start": 1317.0, "end": 1323.0, "text": " it, how would you suggest I search for the two sounds that we need? And it says, certainly", "tokens": [50864, 309, 11, 577, 576, 291, 3402, 286, 3164, 337, 264, 732, 3263, 300, 321, 643, 30, 400, 309, 1619, 11, 3297, 51164], "temperature": 0.0, "avg_logprob": -0.16327308786326442, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.38430655002593994}, {"id": 227, "seek": 130700, "start": 1323.0, "end": 1328.0, "text": " there's a lot of websites with four year paid ones. And it says this one is a collaborative", "tokens": [51164, 456, 311, 257, 688, 295, 12891, 365, 1451, 1064, 4835, 2306, 13, 400, 309, 1619, 341, 472, 307, 257, 16555, 51414], "temperature": 0.0, "avg_logprob": -0.16327308786326442, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.38430655002593994}, {"id": 228, "seek": 130700, "start": 1328.0, "end": 1333.0, "text": " one with Creative Commons license sound. And then it even suggests, maybe I was up here,", "tokens": [51414, 472, 365, 26598, 34894, 10476, 1626, 13, 400, 550, 309, 754, 13409, 11, 1310, 286, 390, 493, 510, 11, 51664], "temperature": 0.0, "avg_logprob": -0.16327308786326442, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.38430655002593994}, {"id": 229, "seek": 133300, "start": 1333.0, "end": 1341.0, "text": " yeah, the keywords I searched for fireworks launch or rocket launch or fireworks explosion.", "tokens": [50364, 1338, 11, 264, 21009, 286, 22961, 337, 28453, 4025, 420, 13012, 4025, 420, 28453, 15673, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16690066323351505, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.038425054401159286}, {"id": 230, "seek": 133300, "start": 1341.0, "end": 1347.0, "text": " So I put them in just as it suggested. And then I thought, it'd be nice to have a button", "tokens": [50764, 407, 286, 829, 552, 294, 445, 382, 309, 10945, 13, 400, 550, 286, 1194, 11, 309, 1116, 312, 1481, 281, 362, 257, 2960, 51064], "temperature": 0.0, "avg_logprob": -0.16690066323351505, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.038425054401159286}, {"id": 231, "seek": 133300, "start": 1347.0, "end": 1355.0, "text": " that when you push starts everything up rather than it just happens. So it says, well, you", "tokens": [51064, 300, 562, 291, 2944, 3719, 1203, 493, 2831, 813, 309, 445, 2314, 13, 407, 309, 1619, 11, 731, 11, 291, 51464], "temperature": 0.0, "avg_logprob": -0.16690066323351505, "compression_ratio": 1.4972375690607735, "no_speech_prob": 0.038425054401159286}, {"id": 232, "seek": 135500, "start": 1355.0, "end": 1368.0, "text": " could just add this button to your HTML. And then, and then in your JavaScript, get hold", "tokens": [50364, 727, 445, 909, 341, 2960, 281, 428, 17995, 13, 400, 550, 11, 293, 550, 294, 428, 15778, 11, 483, 1797, 51014], "temperature": 0.0, "avg_logprob": -0.10944316698157269, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.15187346935272217}, {"id": 233, "seek": 135500, "start": 1368.0, "end": 1376.0, "text": " of that button and put a listener that and it had this idea that you don't start the fireworks", "tokens": [51014, 295, 300, 2960, 293, 829, 257, 31569, 300, 293, 309, 632, 341, 1558, 300, 291, 500, 380, 722, 264, 28453, 51414], "temperature": 0.0, "avg_logprob": -0.10944316698157269, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.15187346935272217}, {"id": 234, "seek": 135500, "start": 1376.0, "end": 1381.0, "text": " until some global variable is set. And I could have just copied that but I said, you know,", "tokens": [51414, 1826, 512, 4338, 7006, 307, 992, 13, 400, 286, 727, 362, 445, 25365, 300, 457, 286, 848, 11, 291, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10944316698157269, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.15187346935272217}, {"id": 235, "seek": 138100, "start": 1381.0, "end": 1386.0, "text": " I understood that if all I want to do is when the buttons pushed that it starts animating", "tokens": [50364, 286, 7320, 300, 498, 439, 286, 528, 281, 360, 307, 562, 264, 9905, 9152, 300, 309, 3719, 2383, 990, 50614], "temperature": 0.0, "avg_logprob": -0.15031498908996582, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.0053828200325369835}, {"id": 236, "seek": 138100, "start": 1386.0, "end": 1391.0, "text": " why not just start animating. So I said, can't I just do that. And it says, oh, you're right", "tokens": [50614, 983, 406, 445, 722, 2383, 990, 13, 407, 286, 848, 11, 393, 380, 286, 445, 360, 300, 13, 400, 309, 1619, 11, 1954, 11, 291, 434, 558, 50864], "temperature": 0.0, "avg_logprob": -0.15031498908996582, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.0053828200325369835}, {"id": 237, "seek": 138100, "start": 1391.0, "end": 1398.0, "text": " that more direct and, you know, straightforward. And, you know, said don't don't add this start", "tokens": [50864, 300, 544, 2047, 293, 11, 291, 458, 11, 15325, 13, 400, 11, 291, 458, 11, 848, 500, 380, 500, 380, 909, 341, 722, 51214], "temperature": 0.0, "avg_logprob": -0.15031498908996582, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.0053828200325369835}, {"id": 238, "seek": 138100, "start": 1398.0, "end": 1406.0, "text": " fireworks flag but instead, you know, is this kind of code or change the code, you know,", "tokens": [51214, 28453, 7166, 457, 2602, 11, 291, 458, 11, 307, 341, 733, 295, 3089, 420, 1319, 264, 3089, 11, 291, 458, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15031498908996582, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.0053828200325369835}, {"id": 239, "seek": 140600, "start": 1406.0, "end": 1421.0, "text": " with this aspect. And then I noticed that the the request animation frame used to be that", "tokens": [50364, 365, 341, 4171, 13, 400, 550, 286, 5694, 300, 264, 264, 5308, 9603, 3920, 1143, 281, 312, 300, 51114], "temperature": 0.0, "avg_logprob": -0.1567412434202252, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.025931542739272118}, {"id": 240, "seek": 140600, "start": 1421.0, "end": 1425.0, "text": " the first thing it did and that was the last thing and I just got curious and said, does", "tokens": [51114, 264, 700, 551, 309, 630, 293, 300, 390, 264, 1036, 551, 293, 286, 445, 658, 6369, 293, 848, 11, 775, 51314], "temperature": 0.0, "avg_logprob": -0.1567412434202252, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.025931542739272118}, {"id": 241, "seek": 140600, "start": 1425.0, "end": 1430.0, "text": " it matter whether it's in the beginning of the end and it says it can have an impact but", "tokens": [51314, 309, 1871, 1968, 309, 311, 294, 264, 2863, 295, 264, 917, 293, 309, 1619, 309, 393, 362, 364, 2712, 457, 51564], "temperature": 0.0, "avg_logprob": -0.1567412434202252, "compression_ratio": 1.608433734939759, "no_speech_prob": 0.025931542739272118}, {"id": 242, "seek": 143000, "start": 1430.0, "end": 1436.0, "text": " the difference is usually subtle and explains how there's a risk that if you do it in the", "tokens": [50364, 264, 2649, 307, 2673, 13743, 293, 13948, 577, 456, 311, 257, 3148, 300, 498, 291, 360, 309, 294, 264, 50664], "temperature": 0.0, "avg_logprob": -0.23236633936564127, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.18685846030712128}, {"id": 243, "seek": 143000, "start": 1436.0, "end": 1447.0, "text": " beginning you might be you might lead to some performance issues and so on.", "tokens": [50664, 2863, 291, 1062, 312, 291, 1062, 1477, 281, 512, 3389, 2663, 293, 370, 322, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23236633936564127, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.18685846030712128}, {"id": 244, "seek": 143000, "start": 1447.0, "end": 1455.0, "text": " So everything was working and then in one of some of the suggested code there was a comment", "tokens": [51214, 407, 1203, 390, 1364, 293, 550, 294, 472, 295, 512, 295, 264, 10945, 3089, 456, 390, 257, 2871, 51614], "temperature": 0.0, "avg_logprob": -0.23236633936564127, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.18685846030712128}, {"id": 245, "seek": 145500, "start": 1455.0, "end": 1464.0, "text": " saying that you should I should implement something that would remove the particles.", "tokens": [50364, 1566, 300, 291, 820, 286, 820, 4445, 746, 300, 576, 4159, 264, 10007, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13405960726450725, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.21969851851463318}, {"id": 246, "seek": 145500, "start": 1464.0, "end": 1469.0, "text": " And I just said, you know, you said I should remove the particles but I didn't do it and", "tokens": [50814, 400, 286, 445, 848, 11, 291, 458, 11, 291, 848, 286, 820, 4159, 264, 10007, 457, 286, 994, 380, 360, 309, 293, 51064], "temperature": 0.0, "avg_logprob": -0.13405960726450725, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.21969851851463318}, {"id": 247, "seek": 145500, "start": 1469.0, "end": 1478.0, "text": " everything's working fine anyways. And it explains that you know, basically, you're, I might end up", "tokens": [51064, 1203, 311, 1364, 2489, 13448, 13, 400, 309, 13948, 300, 291, 458, 11, 1936, 11, 291, 434, 11, 286, 1062, 917, 493, 51514], "temperature": 0.0, "avg_logprob": -0.13405960726450725, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.21969851851463318}, {"id": 248, "seek": 145500, "start": 1478.0, "end": 1484.0, "text": " consuming a lot more memory and processing on particles that are no longer visible or", "tokens": [51514, 19867, 257, 688, 544, 4675, 293, 9007, 322, 10007, 300, 366, 572, 2854, 8974, 420, 51814], "temperature": 0.0, "avg_logprob": -0.13405960726450725, "compression_ratio": 1.7342995169082125, "no_speech_prob": 0.21969851851463318}, {"id": 249, "seek": 148400, "start": 1484.0, "end": 1494.0, "text": " that can lead to making everything go slow or use a lot of memory. And it suggests a life span, but I said", "tokens": [50364, 300, 393, 1477, 281, 1455, 1203, 352, 2964, 420, 764, 257, 688, 295, 4675, 13, 400, 309, 13409, 257, 993, 16174, 11, 457, 286, 848, 50864], "temperature": 0.0, "avg_logprob": -0.22002077793729477, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.013215635903179646}, {"id": 250, "seek": 148400, "start": 1494.0, "end": 1500.0, "text": " I'd rather have it just remove when they're no longer visible when it said sure that's reasonable and", "tokens": [50864, 286, 1116, 2831, 362, 309, 445, 4159, 562, 436, 434, 572, 2854, 8974, 562, 309, 848, 988, 300, 311, 10585, 293, 51164], "temperature": 0.0, "avg_logprob": -0.22002077793729477, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.013215635903179646}, {"id": 251, "seek": 148400, "start": 1500.0, "end": 1510.0, "text": " generated the condition here for when particles off the outside, not visible anymore.", "tokens": [51164, 10833, 264, 4188, 510, 337, 562, 10007, 766, 264, 2380, 11, 406, 8974, 3602, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22002077793729477, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.013215635903179646}, {"id": 252, "seek": 151000, "start": 1510.0, "end": 1516.0, "text": " So everything was working but there was an annoying little scroll bars and horizontally and vertical.", "tokens": [50364, 407, 1203, 390, 1364, 457, 456, 390, 364, 11304, 707, 11369, 10228, 293, 33796, 293, 9429, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13690318231997284, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.006687125191092491}, {"id": 253, "seek": 151000, "start": 1516.0, "end": 1526.0, "text": " And so I said I see these scroll bars and it said well maybe it's because the canvas and the page on the right", "tokens": [50664, 400, 370, 286, 848, 286, 536, 613, 11369, 10228, 293, 309, 848, 731, 1310, 309, 311, 570, 264, 16267, 293, 264, 3028, 322, 264, 558, 51164], "temperature": 0.0, "avg_logprob": -0.13690318231997284, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.006687125191092491}, {"id": 254, "seek": 151000, "start": 1526.0, "end": 1532.0, "text": " size and it suggested all this but actually that that's very close to what it had before.", "tokens": [51164, 2744, 293, 309, 10945, 439, 341, 457, 767, 300, 300, 311, 588, 1998, 281, 437, 309, 632, 949, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13690318231997284, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.006687125191092491}, {"id": 255, "seek": 153200, "start": 1532.0, "end": 1540.0, "text": " And I said it didn't help. And then it came up with some more CSS to kind of deal with it.", "tokens": [50364, 400, 286, 848, 309, 994, 380, 854, 13, 400, 550, 309, 1361, 493, 365, 512, 544, 24387, 281, 733, 295, 2028, 365, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.139616283369653, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.01362843532115221}, {"id": 256, "seek": 153200, "start": 1540.0, "end": 1548.0, "text": " And I said, yeah, a little bit better but because the scroll bars were less of an issue but there still were", "tokens": [50764, 400, 286, 848, 11, 1338, 11, 257, 707, 857, 1101, 457, 570, 264, 11369, 10228, 645, 1570, 295, 364, 2734, 457, 456, 920, 645, 51164], "temperature": 0.0, "avg_logprob": -0.139616283369653, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.01362843532115221}, {"id": 257, "seek": 153200, "start": 1548.0, "end": 1557.0, "text": " there I mean there was very little scrolling involved. So then it said well maybe we just have to remove a few pixels from", "tokens": [51164, 456, 286, 914, 456, 390, 588, 707, 29053, 3288, 13, 407, 550, 309, 848, 731, 1310, 321, 445, 362, 281, 4159, 257, 1326, 18668, 490, 51614], "temperature": 0.0, "avg_logprob": -0.139616283369653, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.01362843532115221}, {"id": 258, "seek": 155700, "start": 1557.0, "end": 1569.0, "text": " the window dimensions of the canvas. And it suggested removing, you know, four pixels, and I tried it and it didn't work but", "tokens": [50364, 264, 4910, 12819, 295, 264, 16267, 13, 400, 309, 10945, 12720, 11, 291, 458, 11, 1451, 18668, 11, 293, 286, 3031, 309, 293, 309, 994, 380, 589, 457, 50964], "temperature": 0.0, "avg_logprob": -0.15117892999758667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.05028272047638893}, {"id": 259, "seek": 155700, "start": 1569.0, "end": 1578.0, "text": " when I did 10 it was fine. So everything was fine. And then I said well, you know, fireworks should happen at night it should be a", "tokens": [50964, 562, 286, 630, 1266, 309, 390, 2489, 13, 407, 1203, 390, 2489, 13, 400, 550, 286, 848, 731, 11, 291, 458, 11, 28453, 820, 1051, 412, 1818, 309, 820, 312, 257, 51414], "temperature": 0.0, "avg_logprob": -0.15117892999758667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.05028272047638893}, {"id": 260, "seek": 155700, "start": 1578.0, "end": 1584.0, "text": " black background so it just generated, you know these two lines of code here rather than clearing it.", "tokens": [51414, 2211, 3678, 370, 309, 445, 10833, 11, 291, 458, 613, 732, 3876, 295, 3089, 510, 2831, 813, 23937, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15117892999758667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.05028272047638893}, {"id": 261, "seek": 158400, "start": 1584.0, "end": 1593.0, "text": " And then I said, I can't think of any other improvements can you, and it came up with what is this nine different improvements, you know, a couple of them actually were already done", "tokens": [50364, 400, 550, 286, 848, 11, 286, 393, 380, 519, 295, 604, 661, 13797, 393, 291, 11, 293, 309, 1361, 493, 365, 437, 307, 341, 4949, 819, 13797, 11, 291, 458, 11, 257, 1916, 295, 552, 767, 645, 1217, 1096, 50814], "temperature": 0.0, "avg_logprob": -0.13648978318318283, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.002630587201565504}, {"id": 262, "seek": 158400, "start": 1593.0, "end": 1601.0, "text": " and it didn't seem to remember that it already randomized the colors and had a variable radius but", "tokens": [50814, 293, 309, 994, 380, 1643, 281, 1604, 300, 309, 1217, 38513, 264, 4577, 293, 632, 257, 7006, 15845, 457, 51214], "temperature": 0.0, "avg_logprob": -0.13648978318318283, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.002630587201565504}, {"id": 263, "seek": 158400, "start": 1601.0, "end": 1611.0, "text": " but it could have you know we could add wind or air resistance we could have more different shapes we could have a trail, you know, more user activity, you know.", "tokens": [51214, 457, 309, 727, 362, 291, 458, 321, 727, 909, 2468, 420, 1988, 7335, 321, 727, 362, 544, 819, 10854, 321, 727, 362, 257, 9924, 11, 291, 458, 11, 544, 4195, 5191, 11, 291, 458, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13648978318318283, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.002630587201565504}, {"id": 264, "seek": 161100, "start": 1611.0, "end": 1616.0, "text": " So I thought well I'll just pick one of this number five.", "tokens": [50364, 407, 286, 1194, 731, 286, 603, 445, 1888, 472, 295, 341, 1230, 1732, 13, 50614], "temperature": 0.0, "avg_logprob": -0.137078657746315, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.0033751865848898888}, {"id": 265, "seek": 161100, "start": 1616.0, "end": 1623.0, "text": " So it suggested adding, you know, an alpha value and", "tokens": [50614, 407, 309, 10945, 5127, 11, 291, 458, 11, 364, 8961, 2158, 293, 50964], "temperature": 0.0, "avg_logprob": -0.137078657746315, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.0033751865848898888}, {"id": 266, "seek": 161100, "start": 1623.0, "end": 1627.0, "text": " and then the update uses it.", "tokens": [50964, 293, 550, 264, 5623, 4960, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.137078657746315, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.0033751865848898888}, {"id": 267, "seek": 161100, "start": 1627.0, "end": 1635.0, "text": " But notice that there's this thing called global alpha and I would have thought somehow alpha itself was all.", "tokens": [51164, 583, 3449, 300, 456, 311, 341, 551, 1219, 4338, 8961, 293, 286, 576, 362, 1194, 6063, 8961, 2564, 390, 439, 13, 51564], "temperature": 0.0, "avg_logprob": -0.137078657746315, "compression_ratio": 1.447674418604651, "no_speech_prob": 0.0033751865848898888}, {"id": 268, "seek": 163500, "start": 1635.0, "end": 1644.0, "text": " This I never heard of global alpha what is it and gives me nice explanation and how it works and why he's using it and so on.", "tokens": [50364, 639, 286, 1128, 2198, 295, 4338, 8961, 437, 307, 309, 293, 2709, 385, 1481, 10835, 293, 577, 309, 1985, 293, 983, 415, 311, 1228, 309, 293, 370, 322, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18299249649047852, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.006689304020255804}, {"id": 269, "seek": 163500, "start": 1644.0, "end": 1648.0, "text": " So then", "tokens": [50814, 407, 550, 51014], "temperature": 0.0, "avg_logprob": -0.18299249649047852, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.006689304020255804}, {"id": 270, "seek": 163500, "start": 1648.0, "end": 1652.0, "text": " what when I tried doing the change that it suggested.", "tokens": [51014, 437, 562, 286, 3031, 884, 264, 1319, 300, 309, 10945, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18299249649047852, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.006689304020255804}, {"id": 271, "seek": 165200, "start": 1652.0, "end": 1667.0, "text": " I got this error message that that you must call the super constructor in a derived class because since I was altering the some class without calling the super.", "tokens": [50364, 286, 658, 341, 6713, 3636, 300, 300, 291, 1633, 818, 264, 1687, 47479, 294, 257, 18949, 1508, 570, 1670, 286, 390, 11337, 278, 264, 512, 1508, 1553, 5141, 264, 1687, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15862532319693728, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.08031782507896423}, {"id": 272, "seek": 165200, "start": 1667.0, "end": 1672.0, "text": " And it said oh you just have to add this line.", "tokens": [51114, 400, 309, 848, 1954, 291, 445, 362, 281, 909, 341, 1622, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15862532319693728, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.08031782507896423}, {"id": 273, "seek": 165200, "start": 1672.0, "end": 1677.0, "text": " And so that fix that problem.", "tokens": [51364, 400, 370, 300, 3191, 300, 1154, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15862532319693728, "compression_ratio": 1.5490196078431373, "no_speech_prob": 0.08031782507896423}, {"id": 274, "seek": 167700, "start": 1677.0, "end": 1687.0, "text": " And I said, you know, it'd be nice if the app actually had a little note saying this was created with the help of GPT for", "tokens": [50364, 400, 286, 848, 11, 291, 458, 11, 309, 1116, 312, 1481, 498, 264, 724, 767, 632, 257, 707, 3637, 1566, 341, 390, 2942, 365, 264, 854, 295, 26039, 51, 337, 50864], "temperature": 0.0, "avg_logprob": -0.1271513630362118, "compression_ratio": 1.5240963855421688, "no_speech_prob": 0.01168170478194952}, {"id": 275, "seek": 167700, "start": 1687.0, "end": 1691.0, "text": " and having a link to the log.", "tokens": [50864, 293, 1419, 257, 2113, 281, 264, 3565, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1271513630362118, "compression_ratio": 1.5240963855421688, "no_speech_prob": 0.01168170478194952}, {"id": 276, "seek": 167700, "start": 1691.0, "end": 1701.0, "text": " And it created all this nice CSS and it created this div with the description and the link and so on.", "tokens": [51064, 400, 309, 2942, 439, 341, 1481, 24387, 293, 309, 2942, 341, 3414, 365, 264, 3855, 293, 264, 2113, 293, 370, 322, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1271513630362118, "compression_ratio": 1.5240963855421688, "no_speech_prob": 0.01168170478194952}, {"id": 277, "seek": 170100, "start": 1701.0, "end": 1708.0, "text": " And then it generated all this code but it actually forgot to add the", "tokens": [50364, 400, 550, 309, 10833, 439, 341, 3089, 457, 309, 767, 5298, 281, 909, 264, 50714], "temperature": 0.0, "avg_logprob": -0.14566464301867363, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.000910800532437861}, {"id": 278, "seek": 170100, "start": 1708.0, "end": 1719.0, "text": " include the JavaScript script that the script element had been there and it just forgot it when I was putting this in so I just said you.", "tokens": [50714, 4090, 264, 15778, 5755, 300, 264, 5755, 4478, 632, 668, 456, 293, 309, 445, 5298, 309, 562, 286, 390, 3372, 341, 294, 370, 286, 445, 848, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14566464301867363, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.000910800532437861}, {"id": 279, "seek": 170100, "start": 1719.0, "end": 1727.0, "text": " But again this is just, this wasn't a problem originally this is a problem with the shared GPT that sometimes when I paste things.", "tokens": [51264, 583, 797, 341, 307, 445, 11, 341, 2067, 380, 257, 1154, 7993, 341, 307, 257, 1154, 365, 264, 5507, 26039, 51, 300, 2171, 562, 286, 9163, 721, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14566464301867363, "compression_ratio": 1.6407766990291262, "no_speech_prob": 0.000910800532437861}, {"id": 280, "seek": 172700, "start": 1727.0, "end": 1733.0, "text": " So they don't somehow make it into this history.", "tokens": [50364, 407, 436, 500, 380, 6063, 652, 309, 666, 341, 2503, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1692909390738841, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00475331861525774}, {"id": 281, "seek": 172700, "start": 1733.0, "end": 1737.0, "text": " So then I said that you forgot this but I added it.", "tokens": [50664, 407, 550, 286, 848, 300, 291, 5298, 341, 457, 286, 3869, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1692909390738841, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00475331861525774}, {"id": 282, "seek": 172700, "start": 1737.0, "end": 1747.0, "text": " And then we want to remove the button after it's clicked and everything's working up if this is a problem that happens on many of the occasions which is", "tokens": [50864, 400, 550, 321, 528, 281, 4159, 264, 2960, 934, 309, 311, 23370, 293, 1203, 311, 1364, 493, 498, 341, 307, 257, 1154, 300, 2314, 322, 867, 295, 264, 20641, 597, 307, 51364], "temperature": 0.0, "avg_logprob": -0.1692909390738841, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00475331861525774}, {"id": 283, "seek": 172700, "start": 1747.0, "end": 1750.0, "text": " it used the word and knit here.", "tokens": [51364, 309, 1143, 264, 1349, 293, 15594, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1692909390738841, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00475331861525774}, {"id": 284, "seek": 172700, "start": 1750.0, "end": 1756.0, "text": " Well, earlier it was using the name of the function was animate.", "tokens": [51514, 1042, 11, 3071, 309, 390, 1228, 264, 1315, 295, 264, 2445, 390, 36439, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1692909390738841, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.00475331861525774}, {"id": 285, "seek": 175600, "start": 1756.0, "end": 1760.0, "text": " I told it, you know, I fixed it but you know.", "tokens": [50364, 286, 1907, 309, 11, 291, 458, 11, 286, 6806, 309, 457, 291, 458, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21835913586972364, "compression_ratio": 1.3988095238095237, "no_speech_prob": 0.007807034999132156}, {"id": 286, "seek": 175600, "start": 1760.0, "end": 1777.0, "text": " But I think because these conversations get long in the context is limited to, I don't know, I think it's up to 8,000 tokens for track GPT for and there's a 32 K version that access to but.", "tokens": [50564, 583, 286, 519, 570, 613, 7315, 483, 938, 294, 264, 4319, 307, 5567, 281, 11, 286, 500, 380, 458, 11, 286, 519, 309, 311, 493, 281, 1649, 11, 1360, 22667, 337, 2837, 26039, 51, 337, 293, 456, 311, 257, 8858, 591, 3037, 300, 2105, 281, 457, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21835913586972364, "compression_ratio": 1.3988095238095237, "no_speech_prob": 0.007807034999132156}, {"id": 287, "seek": 177700, "start": 1777.0, "end": 1790.0, "text": " And as a result it's kind of forgetting some of the earlier stuff that it told me and it was just guessing. And this is a common problem. I'll show you where it forgets exactly what a variable name or function name isn't in there.", "tokens": [50364, 400, 382, 257, 1874, 309, 311, 733, 295, 25428, 512, 295, 264, 3071, 1507, 300, 309, 1907, 385, 293, 309, 390, 445, 17939, 13, 400, 341, 307, 257, 2689, 1154, 13, 286, 603, 855, 291, 689, 309, 2870, 82, 2293, 437, 257, 7006, 1315, 420, 2445, 1315, 1943, 380, 294, 456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11669436368075284, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.03730098158121109}, {"id": 288, "seek": 177700, "start": 1790.0, "end": 1802.0, "text": " Generates code with a similar name with the same kind of concept but obviously one that won't work because computer programming languages need the exact same name.", "tokens": [51014, 15409, 1024, 3089, 365, 257, 2531, 1315, 365, 264, 912, 733, 295, 3410, 457, 2745, 472, 300, 1582, 380, 589, 570, 3820, 9410, 8650, 643, 264, 1900, 912, 1315, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11669436368075284, "compression_ratio": 1.628099173553719, "no_speech_prob": 0.03730098158121109}, {"id": 289, "seek": 180200, "start": 1802.0, "end": 1807.0, "text": " I'll show you the results of all of this.", "tokens": [50364, 286, 603, 855, 291, 264, 3542, 295, 439, 295, 341, 13, 50614], "temperature": 0.0, "avg_logprob": -0.31013947946053966, "compression_ratio": 0.9444444444444444, "no_speech_prob": 0.503123939037323}, {"id": 290, "seek": 180200, "start": 1807.0, "end": 1818.0, "text": " So if I click here.", "tokens": [50614, 407, 498, 286, 2052, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.31013947946053966, "compression_ratio": 0.9444444444444444, "no_speech_prob": 0.503123939037323}, {"id": 291, "seek": 180200, "start": 1818.0, "end": 1821.0, "text": " Happy.", "tokens": [51164, 8277, 13, 51314], "temperature": 0.0, "avg_logprob": -0.31013947946053966, "compression_ratio": 0.9444444444444444, "no_speech_prob": 0.503123939037323}, {"id": 292, "seek": 182100, "start": 1821.0, "end": 1834.0, "text": " Yeah.", "tokens": [50364, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24128804604212442, "compression_ratio": 1.292857142857143, "no_speech_prob": 0.0367230661213398}, {"id": 293, "seek": 182100, "start": 1834.0, "end": 1849.0, "text": " And that took, I don't know, less than two hours I probably should have kept track of the timing, the whole experience but I thought, you know, it was a very promising kind of", "tokens": [51014, 400, 300, 1890, 11, 286, 500, 380, 458, 11, 1570, 813, 732, 2496, 286, 1391, 820, 362, 4305, 2837, 295, 264, 10822, 11, 264, 1379, 1752, 457, 286, 1194, 11, 291, 458, 11, 309, 390, 257, 588, 20257, 733, 295, 51764], "temperature": 0.0, "avg_logprob": -0.24128804604212442, "compression_ratio": 1.292857142857143, "no_speech_prob": 0.0367230661213398}, {"id": 294, "seek": 184900, "start": 1849.0, "end": 1861.0, "text": " experience in terms of how helpful it was how much it was suggesting things how good the suggestions were, how much and understood about things and how much a good explain things.", "tokens": [50364, 1752, 294, 2115, 295, 577, 4961, 309, 390, 577, 709, 309, 390, 18094, 721, 577, 665, 264, 13396, 645, 11, 577, 709, 293, 7320, 466, 721, 293, 577, 709, 257, 665, 2903, 721, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17671772149892953, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02795000746846199}, {"id": 295, "seek": 184900, "start": 1861.0, "end": 1866.0, "text": " So, that's what I want to say about fireworks.", "tokens": [50964, 407, 11, 300, 311, 437, 286, 528, 281, 584, 466, 28453, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17671772149892953, "compression_ratio": 1.5804195804195804, "no_speech_prob": 0.02795000746846199}, {"id": 296, "seek": 186600, "start": 1866.0, "end": 1879.0, "text": " But one thing that everybody's been talking about with these chatbots is how important it is to do prompt engineering and there are many contexts in which it really is important, but I've been surprised for almost all my experiments.", "tokens": [50364, 583, 472, 551, 300, 2201, 311, 668, 1417, 466, 365, 613, 5081, 65, 1971, 307, 577, 1021, 309, 307, 281, 360, 12391, 7043, 293, 456, 366, 867, 30628, 294, 597, 309, 534, 307, 1021, 11, 457, 286, 600, 668, 6100, 337, 1920, 439, 452, 12050, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12716266673098328, "compression_ratio": 1.592156862745098, "no_speech_prob": 0.028833720833063126}, {"id": 297, "seek": 186600, "start": 1879.0, "end": 1893.0, "text": " I very, very rarely did any attempt to make good prompts, you know, I would copy and paste an error instead of saying, Well, this is the error I found in the console or so.", "tokens": [51014, 286, 588, 11, 588, 13752, 630, 604, 5217, 281, 652, 665, 41095, 11, 291, 458, 11, 286, 576, 5055, 293, 9163, 364, 6713, 2602, 295, 1566, 11, 1042, 11, 341, 307, 264, 6713, 286, 1352, 294, 264, 11076, 420, 370, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12716266673098328, "compression_ratio": 1.592156862745098, "no_speech_prob": 0.028833720833063126}, {"id": 298, "seek": 189300, "start": 1893.0, "end": 1910.0, "text": " It was the only one where I wanted it to be more of a tutor so I had this, this, sorry, prompt here but that was pretty much the only exception where I wasn't just having an ordinary conversation with it.", "tokens": [50364, 467, 390, 264, 787, 472, 689, 286, 1415, 309, 281, 312, 544, 295, 257, 35613, 370, 286, 632, 341, 11, 341, 11, 2597, 11, 12391, 510, 457, 300, 390, 1238, 709, 264, 787, 11183, 689, 286, 2067, 380, 445, 1419, 364, 10547, 3761, 365, 309, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14561216354370118, "compression_ratio": 1.4676258992805755, "no_speech_prob": 0.03111734613776207}, {"id": 299, "seek": 191000, "start": 1910.0, "end": 1929.0, "text": " I didn't bother to make it use speech instead of typing these things but that's straightforward to do especially because, you know, browsers have speech recognition API.", "tokens": [50364, 286, 994, 380, 8677, 281, 652, 309, 764, 6218, 2602, 295, 18444, 613, 721, 457, 300, 311, 15325, 281, 360, 2318, 570, 11, 291, 458, 11, 36069, 362, 6218, 11150, 9362, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12058402810777936, "compression_ratio": 1.417142857142857, "no_speech_prob": 0.1621226668357849}, {"id": 300, "seek": 191000, "start": 1929.0, "end": 1939.0, "text": " Okay, so let me tell you a little bit about these six other experiments I did.", "tokens": [51314, 1033, 11, 370, 718, 385, 980, 291, 257, 707, 857, 466, 613, 2309, 661, 12050, 286, 630, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12058402810777936, "compression_ratio": 1.417142857142857, "no_speech_prob": 0.1621226668357849}, {"id": 301, "seek": 193900, "start": 1939.0, "end": 1946.0, "text": " Okay, so, right.", "tokens": [50364, 1033, 11, 370, 11, 558, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1916792898467093, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.005550095811486244}, {"id": 302, "seek": 193900, "start": 1946.0, "end": 1955.0, "text": " So here, I wasn't playing the house key into play the role of a tutor I was treating it very much like pair programming.", "tokens": [50714, 407, 510, 11, 286, 2067, 380, 2433, 264, 1782, 2141, 666, 862, 264, 3090, 295, 257, 35613, 286, 390, 15083, 309, 588, 709, 411, 6119, 9410, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1916792898467093, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.005550095811486244}, {"id": 303, "seek": 193900, "start": 1955.0, "end": 1965.0, "text": " And I'm more than navigator suggesting things and chat GPT is the, the one actually generating the code or something.", "tokens": [51164, 400, 286, 478, 544, 813, 7407, 1639, 18094, 721, 293, 5081, 26039, 51, 307, 264, 11, 264, 472, 767, 17746, 264, 3089, 420, 746, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1916792898467093, "compression_ratio": 1.4406779661016949, "no_speech_prob": 0.005550095811486244}, {"id": 304, "seek": 196500, "start": 1965.0, "end": 1976.0, "text": " So I said how can I create a web page with a button that when clicked speaks a random integer, less than 1000 in a random language pitch rate and voice.", "tokens": [50364, 407, 286, 848, 577, 393, 286, 1884, 257, 3670, 3028, 365, 257, 2960, 300, 562, 23370, 10789, 257, 4974, 24922, 11, 1570, 813, 9714, 294, 257, 4974, 2856, 7293, 3314, 293, 3177, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13006951516134696, "compression_ratio": 1.3540372670807452, "no_speech_prob": 0.006093917414546013}, {"id": 305, "seek": 196500, "start": 1976.0, "end": 1982.0, "text": " So it, you know, told me here's some HTML. Here's the JavaScript.", "tokens": [50914, 407, 309, 11, 291, 458, 11, 1907, 385, 510, 311, 512, 17995, 13, 1692, 311, 264, 15778, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13006951516134696, "compression_ratio": 1.3540372670807452, "no_speech_prob": 0.006093917414546013}, {"id": 306, "seek": 198200, "start": 1982.0, "end": 2001.0, "text": " It had just three languages but said you can add more. And it understood all about the speech synthesis API that the, you know, HTML five, all the browsers have.", "tokens": [50364, 467, 632, 445, 1045, 8650, 457, 848, 291, 393, 909, 544, 13, 400, 309, 7320, 439, 466, 264, 6218, 30252, 9362, 300, 264, 11, 291, 458, 11, 17995, 1732, 11, 439, 264, 36069, 362, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18848753757164127, "compression_ratio": 1.4696132596685083, "no_speech_prob": 0.23350346088409424}, {"id": 307, "seek": 198200, "start": 2001.0, "end": 2010.0, "text": " And it's picking a random pitch or random rate and learning the language and reporting errors and stuff.", "tokens": [51314, 400, 309, 311, 8867, 257, 4974, 7293, 420, 4974, 3314, 293, 2539, 264, 2856, 293, 10031, 13603, 293, 1507, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18848753757164127, "compression_ratio": 1.4696132596685083, "no_speech_prob": 0.23350346088409424}, {"id": 308, "seek": 201000, "start": 2010.0, "end": 2022.0, "text": " So, so just with what the first pass like this, when I clicked on the button.", "tokens": [50364, 407, 11, 370, 445, 365, 437, 264, 700, 1320, 411, 341, 11, 562, 286, 23370, 322, 264, 2960, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15053972543454638, "compression_ratio": 1.4532374100719425, "no_speech_prob": 0.0038224810268729925}, {"id": 309, "seek": 201000, "start": 2022.0, "end": 2029.0, "text": " I saw that there was this error or not no voices available but then if I clicked again, everything worked fine from then on.", "tokens": [50964, 286, 1866, 300, 456, 390, 341, 6713, 420, 406, 572, 9802, 2435, 457, 550, 498, 286, 23370, 797, 11, 1203, 2732, 2489, 490, 550, 322, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15053972543454638, "compression_ratio": 1.4532374100719425, "no_speech_prob": 0.0038224810268729925}, {"id": 310, "seek": 202900, "start": 2030.0, "end": 2043.0, "text": " It understood exactly what was going on which is that the get voices is asynchronous and they may not have been loaded when the first time you clicked on the button but then the second time it has is loaded.", "tokens": [50414, 467, 7320, 2293, 437, 390, 516, 322, 597, 307, 300, 264, 483, 9802, 307, 49174, 293, 436, 815, 406, 362, 668, 13210, 562, 264, 700, 565, 291, 23370, 322, 264, 2960, 457, 550, 264, 1150, 565, 309, 575, 307, 13210, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14487598560474538, "compression_ratio": 1.5477707006369428, "no_speech_prob": 0.20659314095973969}, {"id": 311, "seek": 202900, "start": 2043.0, "end": 2054.0, "text": " So you could program around this by", "tokens": [51064, 407, 291, 727, 1461, 926, 341, 538, 51614], "temperature": 0.0, "avg_logprob": -0.14487598560474538, "compression_ratio": 1.5477707006369428, "no_speech_prob": 0.20659314095973969}, {"id": 312, "seek": 205400, "start": 2055.0, "end": 2068.0, "text": " on voices changed so that you, you don't try to say anything until this flag has been set that the voices have been loaded.", "tokens": [50414, 322, 9802, 3105, 370, 300, 291, 11, 291, 500, 380, 853, 281, 584, 1340, 1826, 341, 7166, 575, 668, 992, 300, 264, 9802, 362, 668, 13210, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18861103057861328, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.01639075018465519}, {"id": 313, "seek": 205400, "start": 2068.0, "end": 2079.0, "text": " So that worked. And then I said, Oh, it'll be nice to display what the number is and the language and I said etc and then understood perfectly well that meant the pitch and the rate and so on.", "tokens": [51064, 407, 300, 2732, 13, 400, 550, 286, 848, 11, 876, 11, 309, 603, 312, 1481, 281, 4674, 437, 264, 1230, 307, 293, 264, 2856, 293, 286, 848, 5183, 293, 550, 7320, 6239, 731, 300, 4140, 264, 7293, 293, 264, 3314, 293, 370, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18861103057861328, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.01639075018465519}, {"id": 314, "seek": 207900, "start": 2079.0, "end": 2088.0, "text": " And also, I didn't really like the idea of having to go to the console to see an error message so could you report it some other way.", "tokens": [50364, 400, 611, 11, 286, 994, 380, 534, 411, 264, 1558, 295, 1419, 281, 352, 281, 264, 11076, 281, 536, 364, 6713, 3636, 370, 727, 291, 2275, 309, 512, 661, 636, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08287972450256348, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.009406010620296001}, {"id": 315, "seek": 207900, "start": 2088.0, "end": 2097.0, "text": " And, you know, it updated the HTML updated the JavaScript.", "tokens": [50814, 400, 11, 291, 458, 11, 309, 10588, 264, 17995, 10588, 264, 15778, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08287972450256348, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.009406010620296001}, {"id": 316, "seek": 209700, "start": 2097.0, "end": 2113.0, "text": " And then it was pretty ugly so I just simply said add CSS I didn't, you know, say more than that, and it generated some reasonable CSS to make it all look nicer and explaining what it was and stuff.", "tokens": [50364, 400, 550, 309, 390, 1238, 12246, 370, 286, 445, 2935, 848, 909, 24387, 286, 994, 380, 11, 291, 458, 11, 584, 544, 813, 300, 11, 293, 309, 10833, 512, 10585, 24387, 281, 652, 309, 439, 574, 22842, 293, 13468, 437, 309, 390, 293, 1507, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12228984248881437, "compression_ratio": 1.346938775510204, "no_speech_prob": 0.036730196326971054}, {"id": 317, "seek": 211300, "start": 2113.0, "end": 2124.0, "text": " And then, rather than have just those three languages baked into it. I said why don't you add all the languages that are available in Chrome.", "tokens": [50364, 400, 550, 11, 2831, 813, 362, 445, 729, 1045, 8650, 19453, 666, 309, 13, 286, 848, 983, 500, 380, 291, 909, 439, 264, 8650, 300, 366, 2435, 294, 15327, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10319529758410508, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.07577590644359589}, {"id": 318, "seek": 211300, "start": 2124.0, "end": 2142.0, "text": " And I said in Chrome but it actually comes up and it says Chrome here too but it actually comes up with a solution that'll work for any browser, because it's just collecting all the languages that the browser has available, and then pulling out the language code.", "tokens": [50914, 400, 286, 848, 294, 15327, 457, 309, 767, 1487, 493, 293, 309, 1619, 15327, 510, 886, 457, 309, 767, 1487, 493, 365, 257, 3827, 300, 603, 589, 337, 604, 11185, 11, 570, 309, 311, 445, 12510, 439, 264, 8650, 300, 264, 11185, 575, 2435, 11, 293, 550, 8407, 484, 264, 2856, 3089, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10319529758410508, "compression_ratio": 1.8161434977578474, "no_speech_prob": 0.07577590644359589}, {"id": 319, "seek": 214200, "start": 2142.0, "end": 2147.0, "text": " The languages, the voices.", "tokens": [50364, 440, 8650, 11, 264, 9802, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18665159665621245, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.010002823546528816}, {"id": 320, "seek": 214200, "start": 2147.0, "end": 2150.0, "text": " So,", "tokens": [50614, 407, 11, 50764], "temperature": 0.0, "avg_logprob": -0.18665159665621245, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.010002823546528816}, {"id": 321, "seek": 214200, "start": 2150.0, "end": 2162.0, "text": " so that worked. And I had a working version of this but then I thought it'd be nice to have an additional capability in this app.", "tokens": [50764, 370, 300, 2732, 13, 400, 286, 632, 257, 1364, 3037, 295, 341, 457, 550, 286, 1194, 309, 1116, 312, 1481, 281, 362, 364, 4497, 13759, 294, 341, 724, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18665159665621245, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.010002823546528816}, {"id": 322, "seek": 214200, "start": 2162.0, "end": 2171.0, "text": " So I said, add a cartoon image of a parrot and when you click on it repeats what the user says with a random language pitch rate and place.", "tokens": [51364, 407, 286, 848, 11, 909, 257, 18569, 3256, 295, 257, 42462, 293, 562, 291, 2052, 322, 309, 35038, 437, 264, 4195, 1619, 365, 257, 4974, 2856, 7293, 3314, 293, 1081, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18665159665621245, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.010002823546528816}, {"id": 323, "seek": 217100, "start": 2171.0, "end": 2190.0, "text": " So it, you know, it has this parrot that thing and it says you know I should put replace that with my path to whatever I put a parrot image, and then it knew perfectly well how to use the speech recognition.", "tokens": [50364, 407, 309, 11, 291, 458, 11, 309, 575, 341, 42462, 300, 551, 293, 309, 1619, 291, 458, 286, 820, 829, 7406, 300, 365, 452, 3100, 281, 2035, 286, 829, 257, 42462, 3256, 11, 293, 550, 309, 2586, 6239, 731, 577, 281, 764, 264, 6218, 11150, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1479301296296667, "compression_ratio": 1.4588235294117646, "no_speech_prob": 0.005905750673264265}, {"id": 324, "seek": 217100, "start": 2190.0, "end": 2195.0, "text": " API to pick up what somebody was saying.", "tokens": [51314, 9362, 281, 1888, 493, 437, 2618, 390, 1566, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1479301296296667, "compression_ratio": 1.4588235294117646, "no_speech_prob": 0.005905750673264265}, {"id": 325, "seek": 219500, "start": 2195.0, "end": 2205.0, "text": " And that worked and I said well, give the user feedback as to what text it was actually had heard, along with the other things that were there.", "tokens": [50364, 400, 300, 2732, 293, 286, 848, 731, 11, 976, 264, 4195, 5824, 382, 281, 437, 2487, 309, 390, 767, 632, 2198, 11, 2051, 365, 264, 661, 721, 300, 645, 456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14150569144259678, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.006483948323875666}, {"id": 326, "seek": 219500, "start": 2205.0, "end": 2208.0, "text": " And", "tokens": [50864, 400, 51014], "temperature": 0.0, "avg_logprob": -0.14150569144259678, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.006483948323875666}, {"id": 327, "seek": 219500, "start": 2208.0, "end": 2222.0, "text": " and then I said, you know we had a button for speaking random number but I said, use this file again and you can see this is basically, I went to Dolly and said, you know, a colorful collection of random integers.", "tokens": [51014, 293, 550, 286, 848, 11, 291, 458, 321, 632, 257, 2960, 337, 4124, 4974, 1230, 457, 286, 848, 11, 764, 341, 3991, 797, 293, 291, 393, 536, 341, 307, 1936, 11, 286, 1437, 281, 1144, 13020, 293, 848, 11, 291, 458, 11, 257, 18506, 5765, 295, 4974, 41674, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14150569144259678, "compression_ratio": 1.6712962962962963, "no_speech_prob": 0.006483948323875666}, {"id": 328, "seek": 222200, "start": 2222.0, "end": 2230.0, "text": " And it generated that image and I just asked it to incorporated in the app.", "tokens": [50364, 400, 309, 10833, 300, 3256, 293, 286, 445, 2351, 309, 281, 21654, 294, 264, 724, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10835115769330193, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0027136190328747034}, {"id": 329, "seek": 222200, "start": 2230.0, "end": 2239.0, "text": " And then it didn't look so nice they were vertical and they were kind of small so I said make them larger and lay them out horizontally.", "tokens": [50764, 400, 550, 309, 994, 380, 574, 370, 1481, 436, 645, 9429, 293, 436, 645, 733, 295, 1359, 370, 286, 848, 652, 552, 4833, 293, 2360, 552, 484, 33796, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10835115769330193, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0027136190328747034}, {"id": 330, "seek": 222200, "start": 2239.0, "end": 2242.0, "text": " And it had no problems doing that.", "tokens": [51214, 400, 309, 632, 572, 2740, 884, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10835115769330193, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0027136190328747034}, {"id": 331, "seek": 222200, "start": 2242.0, "end": 2248.0, "text": " And then I just thought, you know, how would you explain what this app does to a young child.", "tokens": [51364, 400, 550, 286, 445, 1194, 11, 291, 458, 11, 577, 576, 291, 2903, 437, 341, 724, 775, 281, 257, 2037, 1440, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10835115769330193, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.0027136190328747034}, {"id": 332, "seek": 224800, "start": 2248.0, "end": 2256.0, "text": " It says, the app head has two pictures one of colorful numbers another parrot when you click on the picture of colorful numbers the apple.", "tokens": [50364, 467, 1619, 11, 264, 724, 1378, 575, 732, 5242, 472, 295, 18506, 3547, 1071, 42462, 562, 291, 2052, 322, 264, 3036, 295, 18506, 3547, 264, 10606, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1427494345359432, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.02031024917960167}, {"id": 333, "seek": 224800, "start": 2256.0, "end": 2266.0, "text": " Make your computer or phone say a random number, it's like picking a number out of a hat but the computer does it for you so it's really trying to you know explain this at a", "tokens": [50764, 4387, 428, 3820, 420, 2593, 584, 257, 4974, 1230, 11, 309, 311, 411, 8867, 257, 1230, 484, 295, 257, 2385, 457, 264, 3820, 775, 309, 337, 291, 370, 309, 311, 534, 1382, 281, 291, 458, 2903, 341, 412, 257, 51264], "temperature": 0.0, "avg_logprob": -0.1427494345359432, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.02031024917960167}, {"id": 334, "seek": 224800, "start": 2266.0, "end": 2273.0, "text": " to a young child pretty well I think you know it's even more fun is the number will be spoken in different language voice speed each time.", "tokens": [51264, 281, 257, 2037, 1440, 1238, 731, 286, 519, 291, 458, 309, 311, 754, 544, 1019, 307, 264, 1230, 486, 312, 10759, 294, 819, 2856, 3177, 3073, 1184, 565, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1427494345359432, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.02031024917960167}, {"id": 335, "seek": 227300, "start": 2273.0, "end": 2288.0, "text": " So if you click on the parrot picture, the app listens to what you say just like a parrot listening to talk, and then the parrot, actually the computer pretending to be the pair will repeat which is really nice, I thought explanation.", "tokens": [50364, 407, 498, 291, 2052, 322, 264, 42462, 3036, 11, 264, 724, 35959, 281, 437, 291, 584, 445, 411, 257, 42462, 4764, 281, 751, 11, 293, 550, 264, 42462, 11, 767, 264, 3820, 22106, 281, 312, 264, 6119, 486, 7149, 597, 307, 534, 1481, 11, 286, 1194, 10835, 13, 51114], "temperature": 0.0, "avg_logprob": -0.25854820363661823, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.14787417650222778}, {"id": 336, "seek": 227300, "start": 2288.0, "end": 2295.0, "text": " And then I said, well, can it translate what was heard.", "tokens": [51114, 400, 550, 286, 848, 11, 731, 11, 393, 309, 13799, 437, 390, 2198, 13, 51464], "temperature": 0.0, "avg_logprob": -0.25854820363661823, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.14787417650222778}, {"id": 337, "seek": 229500, "start": 2295.0, "end": 2309.0, "text": " So you could use Google translate and you need an API key and here's the way to talk to that API and you should, you know, put the key here and goes on and on.", "tokens": [50364, 407, 291, 727, 764, 3329, 13799, 293, 291, 643, 364, 9362, 2141, 293, 510, 311, 264, 636, 281, 751, 281, 300, 9362, 293, 291, 820, 11, 291, 458, 11, 829, 264, 2141, 510, 293, 1709, 322, 293, 322, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20030885667943243, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.14208555221557617}, {"id": 338, "seek": 229500, "start": 2309.0, "end": 2322.0, "text": " And I asked, you know, so the API key shouldn't be picked in and should ask for it.", "tokens": [51064, 400, 286, 2351, 11, 291, 458, 11, 370, 264, 9362, 2141, 4659, 380, 312, 6183, 294, 293, 820, 1029, 337, 309, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20030885667943243, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.14208555221557617}, {"id": 339, "seek": 232200, "start": 2323.0, "end": 2337.0, "text": " I have a, I thought it might be better to use hugging face instead of Google translate so I said, how about using hugging face instead and it was perfectly fine to use hugging face.", "tokens": [50414, 286, 362, 257, 11, 286, 1194, 309, 1062, 312, 1101, 281, 764, 41706, 1851, 2602, 295, 3329, 13799, 370, 286, 848, 11, 577, 466, 1228, 41706, 1851, 2602, 293, 309, 390, 6239, 2489, 281, 764, 41706, 1851, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15705163855301707, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.022260567173361778}, {"id": 340, "seek": 232200, "start": 2337.0, "end": 2341.0, "text": " But then when I tried it as kept getting these error messages.", "tokens": [51114, 583, 550, 562, 286, 3031, 309, 382, 4305, 1242, 613, 6713, 7897, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15705163855301707, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.022260567173361778}, {"id": 341, "seek": 234100, "start": 2341.0, "end": 2352.0, "text": " And what was really happening was that when you ask hugging face to use a particular model.", "tokens": [50364, 400, 437, 390, 534, 2737, 390, 300, 562, 291, 1029, 41706, 1851, 281, 764, 257, 1729, 2316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13455320226735082, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.13829688727855682}, {"id": 342, "seek": 234100, "start": 2352.0, "end": 2362.0, "text": " If it wasn't already loaded, it'll load it and give you back a response saying it's loading please wait an estimated, you know, 10 seconds or something.", "tokens": [50914, 759, 309, 2067, 380, 1217, 13210, 11, 309, 603, 3677, 309, 293, 976, 291, 646, 257, 4134, 1566, 309, 311, 15114, 1767, 1699, 364, 14109, 11, 291, 458, 11, 1266, 3949, 420, 746, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13455320226735082, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.13829688727855682}, {"id": 343, "seek": 236200, "start": 2362.0, "end": 2377.0, "text": " But the code wasn't paying attention to that and also I didn't really want it to wait. So, even though it, it explained what was going on and how to fix it. I just went and got a Google translate key.", "tokens": [50364, 583, 264, 3089, 2067, 380, 6229, 3202, 281, 300, 293, 611, 286, 994, 380, 534, 528, 309, 281, 1699, 13, 407, 11, 754, 1673, 309, 11, 309, 8825, 437, 390, 516, 322, 293, 577, 281, 3191, 309, 13, 286, 445, 1437, 293, 658, 257, 3329, 13799, 2141, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0947070802961077, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.06182767450809479}, {"id": 344, "seek": 236200, "start": 2377.0, "end": 2385.0, "text": " And so let me show you the result.", "tokens": [51114, 400, 370, 718, 385, 855, 291, 264, 1874, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0947070802961077, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.06182767450809479}, {"id": 345, "seek": 238500, "start": 2385.0, "end": 2396.0, "text": " So I'm not going to bother with the translate because I have to get my key and paste it and all that, but it works perfectly fine without it. So if I click here.", "tokens": [50364, 407, 286, 478, 406, 516, 281, 8677, 365, 264, 13799, 570, 286, 362, 281, 483, 452, 2141, 293, 9163, 309, 293, 439, 300, 11, 457, 309, 1985, 6239, 2489, 1553, 309, 13, 407, 498, 286, 2052, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1681456764539083, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.24782665073871613}, {"id": 346, "seek": 238500, "start": 2396.0, "end": 2403.0, "text": " So it picked Italian a very high pitch and low rate 408.", "tokens": [50914, 407, 309, 6183, 10003, 257, 588, 1090, 7293, 293, 2295, 3314, 3356, 23, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1681456764539083, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.24782665073871613}, {"id": 347, "seek": 238500, "start": 2403.0, "end": 2407.0, "text": " And then if I do this, it should be listening.", "tokens": [51264, 400, 550, 498, 286, 360, 341, 11, 309, 820, 312, 4764, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1681456764539083, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.24782665073871613}, {"id": 348, "seek": 238500, "start": 2407.0, "end": 2409.0, "text": " It should be.", "tokens": [51464, 467, 820, 312, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1681456764539083, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.24782665073871613}, {"id": 349, "seek": 238500, "start": 2409.0, "end": 2411.0, "text": " Oh, yeah.", "tokens": [51564, 876, 11, 1338, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1681456764539083, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.24782665073871613}, {"id": 350, "seek": 238500, "start": 2411.0, "end": 2414.0, "text": " It should be listening to me as I speak.", "tokens": [51664, 467, 820, 312, 4764, 281, 385, 382, 286, 1710, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1681456764539083, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.24782665073871613}, {"id": 351, "seek": 241400, "start": 2415.0, "end": 2423.0, "text": " And again, if I if we had the API key would have translated that to French inside it.", "tokens": [50414, 400, 797, 11, 498, 286, 498, 321, 632, 264, 9362, 2141, 576, 362, 16805, 300, 281, 5522, 1854, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16290757921006943, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.017780911177396774}, {"id": 352, "seek": 241400, "start": 2423.0, "end": 2432.0, "text": " And again, I had this link to the dialogues you could actually see how this was created.", "tokens": [50814, 400, 797, 11, 286, 632, 341, 2113, 281, 264, 45551, 291, 727, 767, 536, 577, 341, 390, 2942, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16290757921006943, "compression_ratio": 1.3488372093023255, "no_speech_prob": 0.017780911177396774}, {"id": 353, "seek": 243200, "start": 2432.0, "end": 2445.0, "text": " I have to mention that this parrot, my two year old granddaughter just giggled and giggled and giggled when we'd say things to it and it would repeat it. And it would repeat it in a funny voice or something.", "tokens": [50364, 286, 362, 281, 2152, 300, 341, 42462, 11, 452, 732, 1064, 1331, 44411, 445, 290, 6249, 1493, 293, 290, 6249, 1493, 293, 290, 6249, 1493, 562, 321, 1116, 584, 721, 281, 309, 293, 309, 576, 7149, 309, 13, 400, 309, 576, 7149, 309, 294, 257, 4074, 3177, 420, 746, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11792587197345236, "compression_ratio": 1.8736263736263736, "no_speech_prob": 0.09526994824409485}, {"id": 354, "seek": 243200, "start": 2445.0, "end": 2448.0, "text": " And would repeat it in a funny voice or something.", "tokens": [51014, 400, 576, 7149, 309, 294, 257, 4074, 3177, 420, 746, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11792587197345236, "compression_ratio": 1.8736263736263736, "no_speech_prob": 0.09526994824409485}, {"id": 355, "seek": 243200, "start": 2448.0, "end": 2453.0, "text": " Anyway, she thought that was just so fun. Okay.", "tokens": [51164, 5684, 11, 750, 1194, 300, 390, 445, 370, 1019, 13, 1033, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11792587197345236, "compression_ratio": 1.8736263736263736, "no_speech_prob": 0.09526994824409485}, {"id": 356, "seek": 243200, "start": 2453.0, "end": 2459.0, "text": " So let me talk about the next one.", "tokens": [51414, 407, 718, 385, 751, 466, 264, 958, 472, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11792587197345236, "compression_ratio": 1.8736263736263736, "no_speech_prob": 0.09526994824409485}, {"id": 357, "seek": 245900, "start": 2459.0, "end": 2472.0, "text": " I said, how can I make a web page that detects which way I'm pointing my finger and uses it to draw on a canvas. So my original concept was that if I pointed this way would draw that way if I pointed this way draw that way and so on.", "tokens": [50364, 286, 848, 11, 577, 393, 286, 652, 257, 3670, 3028, 300, 5531, 82, 597, 636, 286, 478, 12166, 452, 5984, 293, 4960, 309, 281, 2642, 322, 257, 16267, 13, 407, 452, 3380, 3410, 390, 300, 498, 286, 10932, 341, 636, 576, 2642, 300, 636, 498, 286, 10932, 341, 636, 2642, 300, 636, 293, 370, 322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1414210455758231, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.0060900975950062275}, {"id": 358, "seek": 245900, "start": 2472.0, "end": 2481.0, "text": " But it misunderstood me but actually came up with an equally good scheme that I wasn't didn't bother pursuing.", "tokens": [51014, 583, 309, 33870, 385, 457, 767, 1361, 493, 365, 364, 12309, 665, 12232, 300, 286, 2067, 380, 994, 380, 8677, 20222, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1414210455758231, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.0060900975950062275}, {"id": 359, "seek": 245900, "start": 2481.0, "end": 2485.0, "text": " So what's so interesting about this is just from this.", "tokens": [51464, 407, 437, 311, 370, 1880, 466, 341, 307, 445, 490, 341, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1414210455758231, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.0060900975950062275}, {"id": 360, "seek": 248500, "start": 2486.0, "end": 2492.0, "text": " You know, does generates all this HTML CSS but the JavaScript.", "tokens": [50414, 509, 458, 11, 775, 23815, 439, 341, 17995, 24387, 457, 264, 15778, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16556859996220838, "compression_ratio": 1.4696969696969697, "no_speech_prob": 0.009403307922184467}, {"id": 361, "seek": 248500, "start": 2492.0, "end": 2510.0, "text": " Actually, I think it says it here too that it. Yeah, it's actually going to use a machine learning model that's a hand pose that'll figure out, you know, 33 different locations on your finger, you know, the each joint and so on.", "tokens": [50714, 5135, 11, 286, 519, 309, 1619, 309, 510, 886, 300, 309, 13, 865, 11, 309, 311, 767, 516, 281, 764, 257, 3479, 2539, 2316, 300, 311, 257, 1011, 10774, 300, 603, 2573, 484, 11, 291, 458, 11, 11816, 819, 9253, 322, 428, 5984, 11, 291, 458, 11, 264, 1184, 7225, 293, 370, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16556859996220838, "compression_ratio": 1.4696969696969697, "no_speech_prob": 0.009403307922184467}, {"id": 362, "seek": 251000, "start": 2510.0, "end": 2517.0, "text": " And knows how to load it and use it and so on and also of course how to get the video feed going into it.", "tokens": [50364, 400, 3255, 577, 281, 3677, 309, 293, 764, 309, 293, 370, 322, 293, 611, 295, 1164, 577, 281, 483, 264, 960, 3154, 516, 666, 309, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11671999338510874, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.029729455709457397}, {"id": 363, "seek": 251000, "start": 2517.0, "end": 2532.0, "text": " And then here how to load the TensorFlow.js library and then how to load the hand pose model, and how to use it and then how to use it to detect if I'm my fingers pointing upward.", "tokens": [50714, 400, 550, 510, 577, 281, 3677, 264, 37624, 13, 25530, 6405, 293, 550, 577, 281, 3677, 264, 1011, 10774, 2316, 11, 293, 577, 281, 764, 309, 293, 550, 577, 281, 764, 309, 281, 5531, 498, 286, 478, 452, 7350, 12166, 23452, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11671999338510874, "compression_ratio": 1.7065868263473054, "no_speech_prob": 0.029729455709457397}, {"id": 364, "seek": 253200, "start": 2533.0, "end": 2542.0, "text": " And this is why an interesting thing that sometimes it's generating code and just stops in the middle, or the case the explanation.", "tokens": [50414, 400, 341, 307, 983, 364, 1880, 551, 300, 2171, 309, 311, 17746, 3089, 293, 445, 10094, 294, 264, 2808, 11, 420, 264, 1389, 264, 10835, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17017158594998447, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.023308537900447845}, {"id": 365, "seek": 253200, "start": 2542.0, "end": 2561.0, "text": " And that's because, you know, it only does so much of a completion it's limited to 1000 or 2000 tokens or something but if you just say please continue it just apologizes and just starts right where pretty much where left off and, you know, continues generating code.", "tokens": [50864, 400, 300, 311, 570, 11, 291, 458, 11, 309, 787, 775, 370, 709, 295, 257, 19372, 309, 311, 5567, 281, 9714, 420, 8132, 22667, 420, 746, 457, 498, 291, 445, 584, 1767, 2354, 309, 445, 9472, 5660, 293, 445, 3719, 558, 689, 1238, 709, 689, 1411, 766, 293, 11, 291, 458, 11, 6515, 17746, 3089, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17017158594998447, "compression_ratio": 1.7124463519313304, "no_speech_prob": 0.023308537900447845}, {"id": 366, "seek": 256100, "start": 2562.0, "end": 2577.0, "text": " And then I said, well, nothing's being drawn and it says oh there's some issue about the position on the canvas, and it turns out it was a bit confused about the to the coordinate system of the", "tokens": [50414, 400, 550, 286, 848, 11, 731, 11, 1825, 311, 885, 10117, 293, 309, 1619, 1954, 456, 311, 512, 2734, 466, 264, 2535, 322, 264, 16267, 11, 293, 309, 4523, 484, 309, 390, 257, 857, 9019, 466, 264, 281, 264, 15670, 1185, 295, 264, 51164], "temperature": 0.0, "avg_logprob": -0.16130419994922393, "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.004975937306880951}, {"id": 367, "seek": 257700, "start": 2577.0, "end": 2592.0, "text": " hand pose and the coordinate system of the canvas, but it tried to fix it and then I said well now it's only drawing in small area and it finally, you know, gets it right with some changes.", "tokens": [50364, 1011, 10774, 293, 264, 15670, 1185, 295, 264, 16267, 11, 457, 309, 3031, 281, 3191, 309, 293, 550, 286, 848, 731, 586, 309, 311, 787, 6316, 294, 1359, 1859, 293, 309, 2721, 11, 291, 458, 11, 2170, 309, 558, 365, 512, 2962, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11839708368828956, "compression_ratio": 1.4, "no_speech_prob": 0.10078272223472595}, {"id": 368, "seek": 259200, "start": 2592.0, "end": 2601.0, "text": " And then this is this problem where it uses some variable called predictions but", "tokens": [50364, 400, 550, 341, 307, 341, 1154, 689, 309, 4960, 512, 7006, 1219, 21264, 457, 50814], "temperature": 0.0, "avg_logprob": -0.08668635496452673, "compression_ratio": 1.5625, "no_speech_prob": 0.007691384758800268}, {"id": 369, "seek": 259200, "start": 2601.0, "end": 2607.0, "text": " but it was, in this case I think it was a scoping error but sometimes it's because it had a different name and some other part of the code.", "tokens": [50814, 457, 309, 390, 11, 294, 341, 1389, 286, 519, 309, 390, 257, 795, 26125, 6713, 457, 2171, 309, 311, 570, 309, 632, 257, 819, 1315, 293, 512, 661, 644, 295, 264, 3089, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08668635496452673, "compression_ratio": 1.5625, "no_speech_prob": 0.007691384758800268}, {"id": 370, "seek": 259200, "start": 2607.0, "end": 2614.0, "text": " So it suggests how to pass the make it available here.", "tokens": [51114, 407, 309, 13409, 577, 281, 1320, 264, 652, 309, 2435, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08668635496452673, "compression_ratio": 1.5625, "no_speech_prob": 0.007691384758800268}, {"id": 371, "seek": 261400, "start": 2614.0, "end": 2624.0, "text": " And then I said, well, how about if you say the color it'll change to the color that the user spoke and it", "tokens": [50364, 400, 550, 286, 848, 11, 731, 11, 577, 466, 498, 291, 584, 264, 2017, 309, 603, 1319, 281, 264, 2017, 300, 264, 4195, 7179, 293, 309, 50864], "temperature": 0.0, "avg_logprob": -0.1116905927658081, "compression_ratio": 1.1910112359550562, "no_speech_prob": 0.006093588657677174}, {"id": 372, "seek": 262400, "start": 2624.0, "end": 2643.0, "text": " will again use a speech recognition and only accepts, you know, these 10 different colors and if you say any of it, if what you said includes any of the these 10 colors then it'll switch to that color.", "tokens": [50364, 486, 797, 764, 257, 6218, 11150, 293, 787, 33538, 11, 291, 458, 11, 613, 1266, 819, 4577, 293, 498, 291, 584, 604, 295, 309, 11, 498, 437, 291, 848, 5974, 604, 295, 264, 613, 1266, 4577, 550, 309, 603, 3679, 281, 300, 2017, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19603850764612998, "compression_ratio": 1.546583850931677, "no_speech_prob": 0.12927265465259552}, {"id": 373, "seek": 262400, "start": 2643.0, "end": 2647.0, "text": " And there's a default color of black and so on.", "tokens": [51314, 400, 456, 311, 257, 7576, 2017, 295, 2211, 293, 370, 322, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19603850764612998, "compression_ratio": 1.546583850931677, "no_speech_prob": 0.12927265465259552}, {"id": 374, "seek": 264700, "start": 2647.0, "end": 2659.0, "text": " And it turned out, it worked the first time but only the first time and it said, oops, sorry for the confusion I should be using the API bill differently and it regenerates the code.", "tokens": [50364, 400, 309, 3574, 484, 11, 309, 2732, 264, 700, 565, 457, 787, 264, 700, 565, 293, 309, 848, 11, 34166, 11, 2597, 337, 264, 15075, 286, 820, 312, 1228, 264, 9362, 2961, 7614, 293, 309, 26358, 1024, 264, 3089, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18030808522151068, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.1939622312784195}, {"id": 375, "seek": 264700, "start": 2659.0, "end": 2667.0, "text": " And everything's working and I said well let's display what what the last thing spoken in the current color so that people know that it's here hanging it.", "tokens": [50964, 400, 1203, 311, 1364, 293, 286, 848, 731, 718, 311, 4674, 437, 437, 264, 1036, 551, 10759, 294, 264, 2190, 2017, 370, 300, 561, 458, 300, 309, 311, 510, 8345, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18030808522151068, "compression_ratio": 1.6280193236714975, "no_speech_prob": 0.1939622312784195}, {"id": 376, "seek": 266700, "start": 2667.0, "end": 2682.0, "text": " And then this is very nice I just said add instructions on how to use the app, and it generates, you know, the, the HTML to explain how to use it, and make some CSS to make that look nice.", "tokens": [50364, 400, 550, 341, 307, 588, 1481, 286, 445, 848, 909, 9415, 322, 577, 281, 764, 264, 724, 11, 293, 309, 23815, 11, 291, 458, 11, 264, 11, 264, 17995, 281, 2903, 577, 281, 764, 309, 11, 293, 652, 512, 24387, 281, 652, 300, 574, 1481, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11450131372971968, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.07790762186050415}, {"id": 377, "seek": 266700, "start": 2682.0, "end": 2687.0, "text": " And then somehow the display didn't get right.", "tokens": [51114, 400, 550, 6063, 264, 4674, 994, 380, 483, 558, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11450131372971968, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.07790762186050415}, {"id": 378, "seek": 266700, "start": 2687.0, "end": 2695.0, "text": " Again, because I think it was forgetting what it was calling things before it was using different similar but different names here.", "tokens": [51364, 3764, 11, 570, 286, 519, 309, 390, 25428, 437, 309, 390, 5141, 721, 949, 309, 390, 1228, 819, 2531, 457, 819, 5288, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11450131372971968, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.07790762186050415}, {"id": 379, "seek": 269500, "start": 2695.0, "end": 2704.0, "text": " And again the same problem that it could picked up the wrong variable name or something.", "tokens": [50364, 400, 797, 264, 912, 1154, 300, 309, 727, 6183, 493, 264, 2085, 7006, 1315, 420, 746, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14848501032049005, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.011677833274006844}, {"id": 380, "seek": 269500, "start": 2704.0, "end": 2712.0, "text": " So it's not the problem might have been that we were doing things before the page was loaded but that wasn't really the problem.", "tokens": [50814, 407, 309, 311, 406, 264, 1154, 1062, 362, 668, 300, 321, 645, 884, 721, 949, 264, 3028, 390, 13210, 457, 300, 2067, 380, 534, 264, 1154, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14848501032049005, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.011677833274006844}, {"id": 381, "seek": 269500, "start": 2712.0, "end": 2718.0, "text": " So I said that that didn't fix it and then I said just I didn't have the same much more same problem.", "tokens": [51214, 407, 286, 848, 300, 300, 994, 380, 3191, 309, 293, 550, 286, 848, 445, 286, 994, 380, 362, 264, 912, 709, 544, 912, 1154, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14848501032049005, "compression_ratio": 1.6789473684210525, "no_speech_prob": 0.011677833274006844}, {"id": 382, "seek": 271800, "start": 2718.0, "end": 2732.0, "text": " And that didn't help. So I had an idea what was wrong but I wasn't right either and it said, it thought maybe that was the issue.", "tokens": [50364, 400, 300, 994, 380, 854, 13, 407, 286, 632, 364, 1558, 437, 390, 2085, 457, 286, 2067, 380, 558, 2139, 293, 309, 848, 11, 309, 1194, 1310, 300, 390, 264, 2734, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22298288345336914, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.2015739232301712}, {"id": 383, "seek": 271800, "start": 2732.0, "end": 2742.0, "text": " So, and also notice this problem like you know, whether it was calling it video or webcam it wasn't consistently naming things.", "tokens": [51064, 407, 11, 293, 611, 3449, 341, 1154, 411, 291, 458, 11, 1968, 309, 390, 5141, 309, 960, 420, 39490, 309, 2067, 380, 14961, 25290, 721, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22298288345336914, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.2015739232301712}, {"id": 384, "seek": 274200, "start": 2742.0, "end": 2748.0, "text": " So it was finally got it all working.", "tokens": [50364, 407, 309, 390, 2721, 658, 309, 439, 1364, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19639231519001285, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.016395719721913338}, {"id": 385, "seek": 274200, "start": 2748.0, "end": 2756.0, "text": " And I didn't understand what happened so somehow it started to work even though these changes didn't seem, I don't know.", "tokens": [50664, 400, 286, 994, 380, 1223, 437, 2011, 370, 6063, 309, 1409, 281, 589, 754, 1673, 613, 2962, 994, 380, 1643, 11, 286, 500, 380, 458, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19639231519001285, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.016395719721913338}, {"id": 386, "seek": 275600, "start": 2756.0, "end": 2774.0, "text": " And then there was a really minor problem that took forever to fix which was, it was showing the last thing that it heard, and then the color, all in the same line and would look nicer if they're on separate lines and it just keeps not getting it right and finally,", "tokens": [50364, 400, 550, 456, 390, 257, 534, 6696, 1154, 300, 1890, 5680, 281, 3191, 597, 390, 11, 309, 390, 4099, 264, 1036, 551, 300, 309, 2198, 11, 293, 550, 264, 2017, 11, 439, 294, 264, 912, 1622, 293, 576, 574, 22842, 498, 436, 434, 322, 4994, 3876, 293, 309, 445, 5965, 406, 1242, 309, 558, 293, 2721, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1459274761012343, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.04882698878645897}, {"id": 387, "seek": 277400, "start": 2774.0, "end": 2797.0, "text": " I said, maybe it's something to do with the overlay. And then it comes up with this solution that I didn't even know about there was some fancy CSS to deal with the problem.", "tokens": [50364, 286, 848, 11, 1310, 309, 311, 746, 281, 360, 365, 264, 31741, 13, 400, 550, 309, 1487, 493, 365, 341, 3827, 300, 286, 994, 380, 754, 458, 466, 456, 390, 512, 10247, 24387, 281, 2028, 365, 264, 1154, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14770137432009675, "compression_ratio": 1.3307692307692307, "no_speech_prob": 0.012428202666342258}, {"id": 388, "seek": 279700, "start": 2797.0, "end": 2807.0, "text": " So everything's working fine and then I said, you know, could you explain to a 10 year old how it work and once again it does a really nice job of explaining how this thing works.", "tokens": [50364, 407, 1203, 311, 1364, 2489, 293, 550, 286, 848, 11, 291, 458, 11, 727, 291, 2903, 281, 257, 1266, 1064, 1331, 577, 309, 589, 293, 1564, 797, 309, 775, 257, 534, 1481, 1691, 295, 13468, 577, 341, 551, 1985, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10892131669180734, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.13454891741275787}, {"id": 389, "seek": 279700, "start": 2807.0, "end": 2814.0, "text": " And let me show you how what it's like.", "tokens": [50864, 400, 718, 385, 855, 291, 577, 437, 309, 311, 411, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10892131669180734, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.13454891741275787}, {"id": 390, "seek": 279700, "start": 2814.0, "end": 2821.0, "text": " Oh, but I have to turn off the video.", "tokens": [51214, 876, 11, 457, 286, 362, 281, 1261, 766, 264, 960, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10892131669180734, "compression_ratio": 1.4519774011299436, "no_speech_prob": 0.13454891741275787}, {"id": 391, "seek": 282100, "start": 2821.0, "end": 2824.0, "text": " Turn it off the video right.", "tokens": [50364, 7956, 309, 766, 264, 960, 558, 13, 50514], "temperature": 0.0, "avg_logprob": -0.22392666507774675, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.022271310910582542}, {"id": 392, "seek": 282100, "start": 2824.0, "end": 2831.0, "text": " I'll be trying to load it again.", "tokens": [50514, 286, 603, 312, 1382, 281, 3677, 309, 797, 13, 50864], "temperature": 0.0, "avg_logprob": -0.22392666507774675, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.022271310910582542}, {"id": 393, "seek": 282100, "start": 2831.0, "end": 2850.0, "text": " So, notice that it generated these instructions all by itself and say I turned off the video. Oh here comes good. So now, if I move my finger like this and I say red, red, blue, blue, blue.", "tokens": [50864, 407, 11, 3449, 300, 309, 10833, 613, 9415, 439, 538, 2564, 293, 584, 286, 3574, 766, 264, 960, 13, 876, 510, 1487, 665, 13, 407, 586, 11, 498, 286, 1286, 452, 5984, 411, 341, 293, 286, 584, 2182, 11, 2182, 11, 3344, 11, 3344, 11, 3344, 13, 51814], "temperature": 0.0, "avg_logprob": -0.22392666507774675, "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.022271310910582542}, {"id": 394, "seek": 285000, "start": 2850.0, "end": 2854.0, "text": " It's kind of, you know, it's working.", "tokens": [50364, 467, 311, 733, 295, 11, 291, 458, 11, 309, 311, 1364, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3027809871716446, "compression_ratio": 1.515, "no_speech_prob": 0.025158556178212166}, {"id": 395, "seek": 285000, "start": 2854.0, "end": 2859.0, "text": " But I'm able to draw all over it with my finger.", "tokens": [50564, 583, 286, 478, 1075, 281, 2642, 439, 670, 309, 365, 452, 5984, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3027809871716446, "compression_ratio": 1.515, "no_speech_prob": 0.025158556178212166}, {"id": 396, "seek": 285000, "start": 2859.0, "end": 2863.0, "text": " A five year old or a 10 year old who really like.", "tokens": [50814, 316, 1732, 1064, 1331, 420, 257, 1266, 1064, 1331, 567, 534, 411, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3027809871716446, "compression_ratio": 1.515, "no_speech_prob": 0.025158556178212166}, {"id": 397, "seek": 285000, "start": 2863.0, "end": 2874.0, "text": " Yeah, no, actually, my granddaughter, this was funny too that I can draw on it. Okay, so, but there was what I thought was interesting was, you know, had no troubles.", "tokens": [51014, 865, 11, 572, 11, 767, 11, 452, 44411, 11, 341, 390, 4074, 886, 300, 286, 393, 2642, 322, 309, 13, 1033, 11, 370, 11, 457, 456, 390, 437, 286, 1194, 390, 1880, 390, 11, 291, 458, 11, 632, 572, 15379, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3027809871716446, "compression_ratio": 1.515, "no_speech_prob": 0.025158556178212166}, {"id": 398, "seek": 287400, "start": 2874.0, "end": 2882.0, "text": " Loading a machine learning model using it appropriately to get the hand poses just knew how to do all those sort of things.", "tokens": [50364, 6130, 8166, 257, 3479, 2539, 2316, 1228, 309, 23505, 281, 483, 264, 1011, 26059, 445, 2586, 577, 281, 360, 439, 729, 1333, 295, 721, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11847313623579722, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0032716351561248302}, {"id": 399, "seek": 287400, "start": 2882.0, "end": 2895.0, "text": " So the next example I wanted to talk about is if some, some person wanted to actually do a little bit of machine learning on some of their own data.", "tokens": [50764, 407, 264, 958, 1365, 286, 1415, 281, 751, 466, 307, 498, 512, 11, 512, 954, 1415, 281, 767, 360, 257, 707, 857, 295, 3479, 2539, 322, 512, 295, 641, 1065, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11847313623579722, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0032716351561248302}, {"id": 400, "seek": 289500, "start": 2895.0, "end": 2911.0, "text": " So, I had this spreadsheet which was had just 60 examples 20 examples of some text that shows a lot of confidence 20 examples that shows a real lack of confidence and 20 that were kind of neutral.", "tokens": [50364, 407, 11, 286, 632, 341, 27733, 597, 390, 632, 445, 4060, 5110, 945, 5110, 295, 512, 2487, 300, 3110, 257, 688, 295, 6687, 945, 5110, 300, 3110, 257, 957, 5011, 295, 6687, 293, 945, 300, 645, 733, 295, 10598, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10371175233055563, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.0420549139380455}, {"id": 401, "seek": 289500, "start": 2911.0, "end": 2919.0, "text": " And I said how can I make a web page that would predict the level of confidence of some new text.", "tokens": [51164, 400, 286, 848, 577, 393, 286, 652, 257, 3670, 3028, 300, 576, 6069, 264, 1496, 295, 6687, 295, 512, 777, 2487, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10371175233055563, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.0420549139380455}, {"id": 402, "seek": 291900, "start": 2919.0, "end": 2928.0, "text": " You know, and give some suggestions and, and but oh, but, but I didn't say anything about JavaScript so it thought oh I should just do this all in Python.", "tokens": [50364, 509, 458, 11, 293, 976, 512, 13396, 293, 11, 293, 457, 1954, 11, 457, 11, 457, 286, 994, 380, 584, 1340, 466, 15778, 370, 309, 1194, 1954, 286, 820, 445, 360, 341, 439, 294, 15329, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16456826251486073, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.06555657088756561}, {"id": 403, "seek": 291900, "start": 2928.0, "end": 2937.0, "text": " And then I said no no I don't want to do it in Python how could I do in JavaScript and then it says well here's how you could do it using node and I just wanted a web page.", "tokens": [50814, 400, 550, 286, 848, 572, 572, 286, 500, 380, 528, 281, 360, 309, 294, 15329, 577, 727, 286, 360, 294, 15778, 293, 550, 309, 1619, 731, 510, 311, 577, 291, 727, 360, 309, 1228, 9984, 293, 286, 445, 1415, 257, 3670, 3028, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16456826251486073, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.06555657088756561}, {"id": 404, "seek": 291900, "start": 2937.0, "end": 2943.0, "text": " So I say, no I don't want to use node I only want the browser and so well here's how we could do it.", "tokens": [51264, 407, 286, 584, 11, 572, 286, 500, 380, 528, 281, 764, 9984, 286, 787, 528, 264, 11185, 293, 370, 731, 510, 311, 577, 321, 727, 360, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16456826251486073, "compression_ratio": 1.8135593220338984, "no_speech_prob": 0.06555657088756561}, {"id": 405, "seek": 294300, "start": 2944.0, "end": 2947.0, "text": " Um,", "tokens": [50414, 3301, 11, 50564], "temperature": 0.0, "avg_logprob": -0.22451606478009906, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.006484121084213257}, {"id": 406, "seek": 294300, "start": 2947.0, "end": 2957.0, "text": " you know it's loading TensorFlow.js it creates the model, the very small model and compiles it and does the training and", "tokens": [50564, 291, 458, 309, 311, 15114, 37624, 13, 25530, 309, 7829, 264, 2316, 11, 264, 588, 1359, 2316, 293, 715, 4680, 309, 293, 775, 264, 3097, 293, 51064], "temperature": 0.0, "avg_logprob": -0.22451606478009906, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.006484121084213257}, {"id": 407, "seek": 294300, "start": 2957.0, "end": 2969.0, "text": " and then does a prediction and shows the shows what's going on. And again it stopped right in the middle of the generating I just said continue and it continued generating.", "tokens": [51064, 293, 550, 775, 257, 17630, 293, 3110, 264, 3110, 437, 311, 516, 322, 13, 400, 797, 309, 5936, 558, 294, 264, 2808, 295, 264, 17746, 286, 445, 848, 2354, 293, 309, 7014, 17746, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22451606478009906, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.006484121084213257}, {"id": 408, "seek": 296900, "start": 2969.0, "end": 2975.0, "text": " So I should have these two files.", "tokens": [50364, 407, 286, 820, 362, 613, 732, 7098, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1486922848609186, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.0037052822299301624}, {"id": 409, "seek": 296900, "start": 2975.0, "end": 2981.0, "text": " And then I have to put the CSV file somewhere in that same folder or something.", "tokens": [50664, 400, 550, 286, 362, 281, 829, 264, 48814, 3991, 4079, 294, 300, 912, 10820, 420, 746, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1486922848609186, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.0037052822299301624}, {"id": 410, "seek": 296900, "start": 2981.0, "end": 2988.0, "text": " So it actually worked but yeah there was no feedback that it while it was training so I said can you make a graph of the training loss.", "tokens": [50964, 407, 309, 767, 2732, 457, 1338, 456, 390, 572, 5824, 300, 309, 1339, 309, 390, 3097, 370, 286, 848, 393, 291, 652, 257, 4295, 295, 264, 3097, 4470, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1486922848609186, "compression_ratio": 1.4821428571428572, "no_speech_prob": 0.0037052822299301624}, {"id": 411, "seek": 298800, "start": 2988.0, "end": 2997.0, "text": " And so it loads some library and it knows perfectly well how to integrate this library.", "tokens": [50364, 400, 370, 309, 12668, 512, 6405, 293, 309, 3255, 6239, 731, 577, 281, 13365, 341, 6405, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16958418626051683, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.026740701869130135}, {"id": 412, "seek": 298800, "start": 2997.0, "end": 3013.0, "text": " With the callback of the training and it worked but the plot was much bigger than the page and it looked pretty ugly so I said it was too big and CSS I made it nice makes it look nicer.", "tokens": [50814, 2022, 264, 818, 3207, 295, 264, 3097, 293, 309, 2732, 457, 264, 7542, 390, 709, 3801, 813, 264, 3028, 293, 309, 2956, 1238, 12246, 370, 286, 848, 309, 390, 886, 955, 293, 24387, 286, 1027, 309, 1481, 1669, 309, 574, 22842, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16958418626051683, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.026740701869130135}, {"id": 413, "seek": 301300, "start": 3013.0, "end": 3021.0, "text": " And I said, you know why don't why don't you show the likelihood scores for the predictions and it doesn't do that and it does.", "tokens": [50364, 400, 286, 848, 11, 291, 458, 983, 500, 380, 983, 500, 380, 291, 855, 264, 22119, 13444, 337, 264, 21264, 293, 309, 1177, 380, 360, 300, 293, 309, 775, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17110819166356867, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.01032312586903572}, {"id": 414, "seek": 301300, "start": 3021.0, "end": 3025.0, "text": " And then I asked how can I make this more accurate because it wasn't very accurate.", "tokens": [50764, 400, 550, 286, 2351, 577, 393, 286, 652, 341, 544, 8559, 570, 309, 2067, 380, 588, 8559, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17110819166356867, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.01032312586903572}, {"id": 415, "seek": 301300, "start": 3025.0, "end": 3035.0, "text": " And it says well you could have more data you could do some pre processing rotation you could change the model architecture, you could, you know all these things.", "tokens": [50964, 400, 309, 1619, 731, 291, 727, 362, 544, 1412, 291, 727, 360, 512, 659, 9007, 12447, 291, 727, 1319, 264, 2316, 9482, 11, 291, 727, 11, 291, 458, 439, 613, 721, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17110819166356867, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.01032312586903572}, {"id": 416, "seek": 303500, "start": 3035.0, "end": 3048.0, "text": " And I said well how about you improve the model architecture and it says sure and it makes a bigger model with more layers and", "tokens": [50364, 400, 286, 848, 731, 577, 466, 291, 3470, 264, 2316, 9482, 293, 309, 1619, 988, 293, 309, 1669, 257, 3801, 2316, 365, 544, 7914, 293, 51014], "temperature": 0.0, "avg_logprob": -0.13328744519141414, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.03356889635324478}, {"id": 417, "seek": 303500, "start": 3048.0, "end": 3058.0, "text": " and then I said well maybe the user wants to explore how many epochs that give a interface so the user could enter how many epochs they want.", "tokens": [51014, 293, 550, 286, 848, 731, 1310, 264, 4195, 2738, 281, 6839, 577, 867, 30992, 28346, 300, 976, 257, 9226, 370, 264, 4195, 727, 3242, 577, 867, 30992, 28346, 436, 528, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13328744519141414, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.03356889635324478}, {"id": 418, "seek": 305800, "start": 3058.0, "end": 3069.0, "text": " And the graph came up after it was finished training so I said could you make the graph, the updated while it's doing the training, and it's a chair and I heard that.", "tokens": [50364, 400, 264, 4295, 1361, 493, 934, 309, 390, 4335, 3097, 370, 286, 848, 727, 291, 652, 264, 4295, 11, 264, 10588, 1339, 309, 311, 884, 264, 3097, 11, 293, 309, 311, 257, 6090, 293, 286, 2198, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2305482543341004, "compression_ratio": 1.8711111111111112, "no_speech_prob": 0.02159758470952511}, {"id": 419, "seek": 305800, "start": 3069.0, "end": 3071.0, "text": " And then you know, finish.", "tokens": [50914, 400, 550, 291, 458, 11, 2413, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2305482543341004, "compression_ratio": 1.8711111111111112, "no_speech_prob": 0.02159758470952511}, {"id": 420, "seek": 305800, "start": 3071.0, "end": 3077.0, "text": " And then I said, could the training function was pretty complicated so I said could you have comments to it.", "tokens": [51014, 400, 550, 286, 848, 11, 727, 264, 3097, 2445, 390, 1238, 6179, 370, 286, 848, 727, 291, 362, 3053, 281, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2305482543341004, "compression_ratio": 1.8711111111111112, "no_speech_prob": 0.02159758470952511}, {"id": 421, "seek": 305800, "start": 3077.0, "end": 3084.0, "text": " And notice, you know for each section it says okay you know getting the inputs, preparing the data creating the model.", "tokens": [51314, 400, 3449, 11, 291, 458, 337, 1184, 3541, 309, 1619, 1392, 291, 458, 1242, 264, 15743, 11, 10075, 264, 1412, 4084, 264, 2316, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2305482543341004, "compression_ratio": 1.8711111111111112, "no_speech_prob": 0.02159758470952511}, {"id": 422, "seek": 308400, "start": 3084.0, "end": 3091.0, "text": " Setting up the graph, you know, setting up the call back and it just goes on and on explaining it nicely.", "tokens": [50364, 21063, 493, 264, 4295, 11, 291, 458, 11, 3287, 493, 264, 818, 646, 293, 309, 445, 1709, 322, 293, 322, 13468, 309, 9594, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1981719864739312, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.07578020542860031}, {"id": 423, "seek": 308400, "start": 3091.0, "end": 3106.0, "text": " And then I happened to notice that it was gave very different answers depending on whether I had uppercase or not, and it understood exactly what was going on that it was using the universal sentence encoder.", "tokens": [50714, 400, 550, 286, 2011, 281, 3449, 300, 309, 390, 2729, 588, 819, 6338, 5413, 322, 1968, 286, 632, 11775, 2869, 651, 420, 406, 11, 293, 309, 7320, 2293, 437, 390, 516, 322, 300, 309, 390, 1228, 264, 11455, 8174, 2058, 19866, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1981719864739312, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.07578020542860031}, {"id": 424, "seek": 310600, "start": 3106.0, "end": 3112.0, "text": " It was very sensitive, and it can.", "tokens": [50364, 467, 390, 588, 9477, 11, 293, 309, 393, 13, 50664], "temperature": 0.0, "avg_logprob": -0.24731908904181588, "compression_ratio": 1.4939759036144578, "no_speech_prob": 0.15593402087688446}, {"id": 425, "seek": 310600, "start": 3112.0, "end": 3125.0, "text": " It can easily fix this by, of course, just taking one of the user input and sending it to lower case so there wasn't that difference. And there it was so let's see it in action.", "tokens": [50664, 467, 393, 3612, 3191, 341, 538, 11, 295, 1164, 11, 445, 1940, 472, 295, 264, 4195, 4846, 293, 7750, 309, 281, 3126, 1389, 370, 456, 2067, 380, 300, 2649, 13, 400, 456, 309, 390, 370, 718, 311, 536, 309, 294, 3069, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24731908904181588, "compression_ratio": 1.4939759036144578, "no_speech_prob": 0.15593402087688446}, {"id": 426, "seek": 310600, "start": 3125.0, "end": 3128.0, "text": " So, I'm going to say, I don't know.", "tokens": [51314, 407, 11, 286, 478, 516, 281, 584, 11, 286, 500, 380, 458, 13, 51464], "temperature": 0.0, "avg_logprob": -0.24731908904181588, "compression_ratio": 1.4939759036144578, "no_speech_prob": 0.15593402087688446}, {"id": 427, "seek": 312800, "start": 3128.0, "end": 3130.0, "text": " That's just because it.", "tokens": [50364, 663, 311, 445, 570, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2216653112155288, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.016647670418024063}, {"id": 428, "seek": 312800, "start": 3130.0, "end": 3137.0, "text": " So, it says training, but it's actually", "tokens": [50464, 407, 11, 309, 1619, 3097, 11, 457, 309, 311, 767, 50814], "temperature": 0.0, "avg_logprob": -0.2216653112155288, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.016647670418024063}, {"id": 429, "seek": 312800, "start": 3137.0, "end": 3153.0, "text": " coming up with the embeddings for the 60 sentences and there is that now we get the training and see the validation losses going up and so it's overfitting or something, but I could type something here like", "tokens": [50814, 1348, 493, 365, 264, 12240, 29432, 337, 264, 4060, 16579, 293, 456, 307, 300, 586, 321, 483, 264, 3097, 293, 536, 264, 24071, 15352, 516, 493, 293, 370, 309, 311, 670, 69, 2414, 420, 746, 11, 457, 286, 727, 2010, 746, 510, 411, 51614], "temperature": 0.0, "avg_logprob": -0.2216653112155288, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.016647670418024063}, {"id": 430, "seek": 315300, "start": 3153.0, "end": 3163.0, "text": " I say, predict.", "tokens": [50364, 286, 584, 11, 6069, 13, 50864], "temperature": 0.0, "avg_logprob": -0.30749661127726235, "compression_ratio": 1.0606060606060606, "no_speech_prob": 0.12569719552993774}, {"id": 431, "seek": 315300, "start": 3163.0, "end": 3169.0, "text": " And it says 99.7% sure that it's confident.", "tokens": [50864, 400, 309, 1619, 11803, 13, 22, 4, 988, 300, 309, 311, 6679, 13, 51164], "temperature": 0.0, "avg_logprob": -0.30749661127726235, "compression_ratio": 1.0606060606060606, "no_speech_prob": 0.12569719552993774}, {"id": 432, "seek": 315300, "start": 3169.0, "end": 3182.0, "text": " And I say,", "tokens": [51164, 400, 286, 584, 11, 51814], "temperature": 0.0, "avg_logprob": -0.30749661127726235, "compression_ratio": 1.0606060606060606, "no_speech_prob": 0.12569719552993774}, {"id": 433, "seek": 318200, "start": 3182.0, "end": 3195.0, "text": " And then", "tokens": [50364, 400, 550, 51014], "temperature": 0.0, "avg_logprob": -0.2669096532857643, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.02126195654273033}, {"id": 434, "seek": 318200, "start": 3195.0, "end": 3197.0, "text": " Well, didn't do so good.", "tokens": [51014, 1042, 11, 994, 380, 360, 370, 665, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2669096532857643, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.02126195654273033}, {"id": 435, "seek": 318200, "start": 3197.0, "end": 3203.0, "text": " Should have been neutral, but thought that was maybe it's showing some confidence.", "tokens": [51114, 6454, 362, 668, 10598, 11, 457, 1194, 300, 390, 1310, 309, 311, 4099, 512, 6687, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2669096532857643, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.02126195654273033}, {"id": 436, "seek": 318200, "start": 3203.0, "end": 3211.0, "text": " Anyways, so that I thought was pretty impressive that it could do something like that.", "tokens": [51414, 15585, 11, 370, 300, 286, 1194, 390, 1238, 8992, 300, 309, 727, 360, 746, 411, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2669096532857643, "compression_ratio": 1.3716216216216217, "no_speech_prob": 0.02126195654273033}, {"id": 437, "seek": 321100, "start": 3211.0, "end": 3215.0, "text": " So, let's see.", "tokens": [50364, 407, 11, 718, 311, 536, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18377017974853516, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.01149514876306057}, {"id": 438, "seek": 321100, "start": 3215.0, "end": 3218.0, "text": " So, in terms of timing.", "tokens": [50564, 407, 11, 294, 2115, 295, 10822, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18377017974853516, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.01149514876306057}, {"id": 439, "seek": 321100, "start": 3218.0, "end": 3223.0, "text": " I don't want to show all of these because it'll take too long but I wanted to.", "tokens": [50714, 286, 500, 380, 528, 281, 855, 439, 295, 613, 570, 309, 603, 747, 886, 938, 457, 286, 1415, 281, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18377017974853516, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.01149514876306057}, {"id": 440, "seek": 321100, "start": 3223.0, "end": 3226.0, "text": " I think I'll show two more but I'll try to go quickly is that.", "tokens": [50964, 286, 519, 286, 603, 855, 732, 544, 457, 286, 603, 853, 281, 352, 2661, 307, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18377017974853516, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.01149514876306057}, {"id": 441, "seek": 321100, "start": 3226.0, "end": 3240.0, "text": " Okay, so this one is using asking chat GPT to use GPT to make a conversation between personas.", "tokens": [51114, 1033, 11, 370, 341, 472, 307, 1228, 3365, 5081, 26039, 51, 281, 764, 26039, 51, 281, 652, 257, 3761, 1296, 12019, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18377017974853516, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.01149514876306057}, {"id": 442, "seek": 324000, "start": 3240.0, "end": 3243.0, "text": " personas.", "tokens": [50364, 12019, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20448958079020182, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.01064209919422865}, {"id": 443, "seek": 324000, "start": 3243.0, "end": 3246.0, "text": " And", "tokens": [50514, 400, 50664], "temperature": 0.0, "avg_logprob": -0.20448958079020182, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.01064209919422865}, {"id": 444, "seek": 324000, "start": 3246.0, "end": 3249.0, "text": " you know who knows how to.", "tokens": [50664, 291, 458, 567, 3255, 577, 281, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20448958079020182, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.01064209919422865}, {"id": 445, "seek": 324000, "start": 3249.0, "end": 3254.0, "text": " Well actually didn't know how to call the", "tokens": [50814, 1042, 767, 994, 380, 458, 577, 281, 818, 264, 51064], "temperature": 0.0, "avg_logprob": -0.20448958079020182, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.01064209919422865}, {"id": 446, "seek": 324000, "start": 3254.0, "end": 3258.0, "text": " API.", "tokens": [51064, 9362, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20448958079020182, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.01064209919422865}, {"id": 447, "seek": 324000, "start": 3258.0, "end": 3268.0, "text": " Oh, so various points I had to copy and paste the documentation from their website and then was able to do it fine.", "tokens": [51264, 876, 11, 370, 3683, 2793, 286, 632, 281, 5055, 293, 9163, 264, 14333, 490, 641, 3144, 293, 550, 390, 1075, 281, 360, 309, 2489, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20448958079020182, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.01064209919422865}, {"id": 448, "seek": 326800, "start": 3268.0, "end": 3282.0, "text": " And again, it was trying to use no and I said no, no, I don't want to. And this is interesting that this is why we're using the API key in the front end that's a little bit not recommended because the key could get.", "tokens": [50364, 400, 797, 11, 309, 390, 1382, 281, 764, 572, 293, 286, 848, 572, 11, 572, 11, 286, 500, 380, 528, 281, 13, 400, 341, 307, 1880, 300, 341, 307, 983, 321, 434, 1228, 264, 9362, 2141, 294, 264, 1868, 917, 300, 311, 257, 707, 857, 406, 9628, 570, 264, 2141, 727, 483, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21422049011846986, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0049031684175133705}, {"id": 449, "seek": 326800, "start": 3282.0, "end": 3287.0, "text": " Somebody might be able to get your key or whatever.", "tokens": [51064, 13463, 1062, 312, 1075, 281, 483, 428, 2141, 420, 2035, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21422049011846986, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0049031684175133705}, {"id": 450, "seek": 326800, "start": 3287.0, "end": 3289.0, "text": " Let me maybe this one.", "tokens": [51314, 961, 385, 1310, 341, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21422049011846986, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0049031684175133705}, {"id": 451, "seek": 326800, "start": 3289.0, "end": 3296.0, "text": " I could just, oh, I do want to show you one bug that was so funny.", "tokens": [51414, 286, 727, 445, 11, 1954, 11, 286, 360, 528, 281, 855, 291, 472, 7426, 300, 390, 370, 4074, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21422049011846986, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0049031684175133705}, {"id": 452, "seek": 329600, "start": 3296.0, "end": 3299.0, "text": " Yeah, here it is.", "tokens": [50364, 865, 11, 510, 309, 307, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20893123822334486, "compression_ratio": 1.8323353293413174, "no_speech_prob": 0.017420819029211998}, {"id": 453, "seek": 329600, "start": 3299.0, "end": 3318.0, "text": " So there's trying to be a conversation between Aristotle and Galileo, but what, but at this point in the progress at Aristotle says I am Galileo and Galileo says I am Aristotle I am not Galileo and then Aristotle says I am Galileo I am Galileo I'm both there so it just was totally crazy.", "tokens": [50514, 407, 456, 311, 1382, 281, 312, 257, 3761, 1296, 42368, 293, 46576, 78, 11, 457, 437, 11, 457, 412, 341, 935, 294, 264, 4205, 412, 42368, 1619, 286, 669, 46576, 78, 293, 46576, 78, 1619, 286, 669, 42368, 286, 669, 406, 46576, 78, 293, 550, 42368, 1619, 286, 669, 46576, 78, 286, 669, 46576, 78, 286, 478, 1293, 456, 370, 309, 445, 390, 3879, 3219, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20893123822334486, "compression_ratio": 1.8323353293413174, "no_speech_prob": 0.017420819029211998}, {"id": 454, "seek": 331800, "start": 3318.0, "end": 3331.0, "text": " It understood what was wrong here and how to, you know, change the roles and the right messages to the chat system, such that.", "tokens": [50364, 467, 7320, 437, 390, 2085, 510, 293, 577, 281, 11, 291, 458, 11, 1319, 264, 9604, 293, 264, 558, 7897, 281, 264, 5081, 1185, 11, 1270, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15460769335428873, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.021898537874221802}, {"id": 455, "seek": 331800, "start": 3331.0, "end": 3333.0, "text": " Let me show you.", "tokens": [51014, 961, 385, 855, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15460769335428873, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.021898537874221802}, {"id": 456, "seek": 331800, "start": 3333.0, "end": 3340.0, "text": " Well actually just an interest of time I'll just show you a screenshot.", "tokens": [51114, 1042, 767, 445, 364, 1179, 295, 565, 286, 603, 445, 855, 291, 257, 27712, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15460769335428873, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.021898537874221802}, {"id": 457, "seek": 331800, "start": 3340.0, "end": 3343.0, "text": " So,", "tokens": [51464, 407, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15460769335428873, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.021898537874221802}, {"id": 458, "seek": 334300, "start": 3343.0, "end": 3346.0, "text": " you could enter any two people here.", "tokens": [50364, 291, 727, 3242, 604, 732, 561, 510, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1570503682266047, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.017152532935142517}, {"id": 459, "seek": 334300, "start": 3346.0, "end": 3351.0, "text": " And then you could join in by just typing a message here and saying send.", "tokens": [50514, 400, 550, 291, 727, 3917, 294, 538, 445, 18444, 257, 3636, 510, 293, 1566, 2845, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1570503682266047, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.017152532935142517}, {"id": 460, "seek": 334300, "start": 3351.0, "end": 3368.0, "text": " So, Galileo greets Aristotle, and they're both really polite to each other, and they appreciate each kind words and blah, and then I jump in and say well how fast objects fall and Aristotle says to Galileo, you know,", "tokens": [50764, 407, 11, 46576, 78, 6066, 1385, 42368, 11, 293, 436, 434, 1293, 534, 25171, 281, 1184, 661, 11, 293, 436, 4449, 1184, 733, 2283, 293, 12288, 11, 293, 550, 286, 3012, 294, 293, 584, 731, 577, 2370, 6565, 2100, 293, 42368, 1619, 281, 46576, 78, 11, 291, 458, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1570503682266047, "compression_ratio": 1.6108374384236452, "no_speech_prob": 0.017152532935142517}, {"id": 461, "seek": 336800, "start": 3368.0, "end": 3378.0, "text": " the challenge seems to know things that happened every died but you know that his motion is a motion of a challenge but he still maintains the speeds proportional to the weight.", "tokens": [50364, 264, 3430, 2544, 281, 458, 721, 300, 2011, 633, 4539, 457, 291, 458, 300, 702, 5394, 307, 257, 5394, 295, 257, 3430, 457, 415, 920, 33385, 264, 16411, 24969, 281, 264, 3364, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25879393232629655, "compression_ratio": 1.4963503649635037, "no_speech_prob": 0.20635868608951569}, {"id": 462, "seek": 336800, "start": 3378.0, "end": 3381.0, "text": " And Galileo says, you know,", "tokens": [50864, 400, 46576, 78, 1619, 11, 291, 458, 11, 51014], "temperature": 0.0, "avg_logprob": -0.25879393232629655, "compression_ratio": 1.4963503649635037, "no_speech_prob": 0.20635868608951569}, {"id": 463, "seek": 338100, "start": 3381.0, "end": 3395.0, "text": " Oh, and then if there's some weird bug words, it actually repeats this twice and I mentioned it to chat GPT but I made the mistake of mentioning that and another problem and it dealt with the other problem and I never came back to figuring out why.", "tokens": [50364, 876, 11, 293, 550, 498, 456, 311, 512, 3657, 7426, 2283, 11, 309, 767, 35038, 341, 6091, 293, 286, 2835, 309, 281, 5081, 26039, 51, 457, 286, 1027, 264, 6146, 295, 18315, 300, 293, 1071, 1154, 293, 309, 15991, 365, 264, 661, 1154, 293, 286, 1128, 1361, 646, 281, 15213, 484, 983, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2154936356977983, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.049553655087947845}, {"id": 464, "seek": 338100, "start": 3395.0, "end": 3397.0, "text": " Sometimes it repeats.", "tokens": [51064, 4803, 309, 35038, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2154936356977983, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.049553655087947845}, {"id": 465, "seek": 338100, "start": 3397.0, "end": 3400.0, "text": " Anyways,", "tokens": [51164, 15585, 11, 51314], "temperature": 0.0, "avg_logprob": -0.2154936356977983, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.049553655087947845}, {"id": 466, "seek": 338100, "start": 3400.0, "end": 3407.0, "text": " Aristotle's respectfully disagreeing and explaining this and so it's a nice little, you know,", "tokens": [51314, 42368, 311, 45201, 14091, 278, 293, 13468, 341, 293, 370, 309, 311, 257, 1481, 707, 11, 291, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.2154936356977983, "compression_ratio": 1.5805084745762712, "no_speech_prob": 0.049553655087947845}, {"id": 467, "seek": 340700, "start": 3407.0, "end": 3415.0, "text": " app.", "tokens": [50364, 724, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3012471849268133, "compression_ratio": 1.25, "no_speech_prob": 0.024392977356910706}, {"id": 468, "seek": 340700, "start": 3415.0, "end": 3422.0, "text": " And so, let me show you the most complicated one.", "tokens": [50764, 400, 370, 11, 718, 385, 855, 291, 264, 881, 6179, 472, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3012471849268133, "compression_ratio": 1.25, "no_speech_prob": 0.024392977356910706}, {"id": 469, "seek": 340700, "start": 3422.0, "end": 3426.0, "text": " I'll tell you real quick. Well, I'll just show you this one.", "tokens": [51114, 286, 603, 980, 291, 957, 1702, 13, 1042, 11, 286, 603, 445, 855, 291, 341, 472, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3012471849268133, "compression_ratio": 1.25, "no_speech_prob": 0.024392977356910706}, {"id": 470, "seek": 340700, "start": 3426.0, "end": 3429.0, "text": " This one.", "tokens": [51314, 639, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3012471849268133, "compression_ratio": 1.25, "no_speech_prob": 0.024392977356910706}, {"id": 471, "seek": 342900, "start": 3429.0, "end": 3438.0, "text": " Flowers are all fading away and shrinking, but I could drop water balloons on them and they get bigger or something.", "tokens": [50364, 48194, 366, 439, 38644, 1314, 293, 41684, 11, 457, 286, 727, 3270, 1281, 26193, 322, 552, 293, 436, 483, 3801, 420, 746, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1602581944958917, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.009705553762614727}, {"id": 472, "seek": 342900, "start": 3438.0, "end": 3447.0, "text": " So that's.", "tokens": [50814, 407, 300, 311, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1602581944958917, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.009705553762614727}, {"id": 473, "seek": 342900, "start": 3447.0, "end": 3453.0, "text": " So, you know, it's hard to keep watering all these flowers, you know, so that's that.", "tokens": [51264, 407, 11, 291, 458, 11, 309, 311, 1152, 281, 1066, 33028, 439, 613, 8085, 11, 291, 458, 11, 370, 300, 311, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1602581944958917, "compression_ratio": 1.4589041095890412, "no_speech_prob": 0.009705553762614727}, {"id": 474, "seek": 345300, "start": 3453.0, "end": 3469.0, "text": " That's what it, you know, we did with a little conversation. But this was a funny one because it actually was partially working and then in trying to add another feature it broke some of the earlier stuff and then it just wasn't it.", "tokens": [50364, 663, 311, 437, 309, 11, 291, 458, 11, 321, 630, 365, 257, 707, 3761, 13, 583, 341, 390, 257, 4074, 472, 570, 309, 767, 390, 18886, 1364, 293, 550, 294, 1382, 281, 909, 1071, 4111, 309, 6902, 512, 295, 264, 3071, 1507, 293, 550, 309, 445, 2067, 380, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16443189856124252, "compression_ratio": 1.5346534653465347, "no_speech_prob": 0.06003699451684952}, {"id": 475, "seek": 345300, "start": 3469.0, "end": 3475.0, "text": " This was about the only time when I've really had some not very good results.", "tokens": [51164, 639, 390, 466, 264, 787, 565, 562, 286, 600, 534, 632, 512, 406, 588, 665, 3542, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16443189856124252, "compression_ratio": 1.5346534653465347, "no_speech_prob": 0.06003699451684952}, {"id": 476, "seek": 347500, "start": 3475.0, "end": 3484.0, "text": " But when I started over again, it actually did well and also I have links here for how you know bar did trying this or something.", "tokens": [50364, 583, 562, 286, 1409, 670, 797, 11, 309, 767, 630, 731, 293, 611, 286, 362, 6123, 510, 337, 577, 291, 458, 2159, 630, 1382, 341, 420, 746, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2266769693858588, "compression_ratio": 1.4189944134078212, "no_speech_prob": 0.24459680914878845}, {"id": 477, "seek": 347500, "start": 3484.0, "end": 3491.0, "text": " Okay, so this is the, the really big ambitious one that I did.", "tokens": [50814, 1033, 11, 370, 341, 307, 264, 11, 264, 534, 955, 20239, 472, 300, 286, 630, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2266769693858588, "compression_ratio": 1.4189944134078212, "no_speech_prob": 0.24459680914878845}, {"id": 478, "seek": 347500, "start": 3491.0, "end": 3499.0, "text": " And it's a counter to the thing 111 exchanges back and forth.", "tokens": [51164, 400, 309, 311, 257, 5682, 281, 264, 551, 2975, 16, 27374, 646, 293, 5220, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2266769693858588, "compression_ratio": 1.4189944134078212, "no_speech_prob": 0.24459680914878845}, {"id": 479, "seek": 349900, "start": 3499.0, "end": 3508.0, "text": " In the beginning, what I asking you to do is to create,", "tokens": [50364, 682, 264, 2863, 11, 437, 286, 3365, 291, 281, 360, 307, 281, 1884, 11, 50814], "temperature": 0.0, "avg_logprob": -0.22958167682994496, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.029287395998835564}, {"id": 480, "seek": 349900, "start": 3508.0, "end": 3515.0, "text": " generate a story, and I picked this because", "tokens": [50814, 8460, 257, 1657, 11, 293, 286, 6183, 341, 570, 51164], "temperature": 0.0, "avg_logprob": -0.22958167682994496, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.029287395998835564}, {"id": 481, "seek": 349900, "start": 3515.0, "end": 3525.0, "text": " you know, Gaiman in Sandman, there's this story where this character has kidnapped amused and for punishment.", "tokens": [51164, 291, 458, 11, 10384, 25504, 294, 7985, 1601, 11, 456, 311, 341, 1657, 689, 341, 2517, 575, 29300, 669, 4717, 293, 337, 14133, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22958167682994496, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.029287395998835564}, {"id": 482, "seek": 352500, "start": 3526.0, "end": 3543.0, "text": " It's getting so many ideas for stories that it's that he's going crazy can't think of anything other than stories and there's like a lot of crazy stories including a man who falls in love with paper dolls so I actually did all 15 that came from that story, plus some", "tokens": [50414, 467, 311, 1242, 370, 867, 3487, 337, 3676, 300, 309, 311, 300, 415, 311, 516, 3219, 393, 380, 519, 295, 1340, 661, 813, 3676, 293, 456, 311, 411, 257, 688, 295, 3219, 3676, 3009, 257, 587, 567, 8804, 294, 959, 365, 3035, 29134, 370, 286, 767, 630, 439, 2119, 300, 1361, 490, 300, 1657, 11, 1804, 512, 51264], "temperature": 0.0, "avg_logprob": -0.17249846803969232, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.02840595319867134}, {"id": 483, "seek": 352500, "start": 3543.0, "end": 3546.0, "text": " other ones that I invented.", "tokens": [51264, 661, 2306, 300, 286, 14479, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17249846803969232, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.02840595319867134}, {"id": 484, "seek": 354600, "start": 3547.0, "end": 3558.0, "text": " So it generates, you know, three paragraphs like, like I asked, and then I said, can you criticize them and it says oh well we could do this character development or better setting.", "tokens": [50414, 407, 309, 23815, 11, 291, 458, 11, 1045, 48910, 411, 11, 411, 286, 2351, 11, 293, 550, 286, 848, 11, 393, 291, 31010, 552, 293, 309, 1619, 1954, 731, 321, 727, 360, 341, 2517, 3250, 420, 1101, 3287, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16726491110665456, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.08497554063796997}, {"id": 485, "seek": 354600, "start": 3558.0, "end": 3564.0, "text": " And it's not generic ones it's saying the town the theater was only briefly mentioned maybe you could improve that or something.", "tokens": [50964, 400, 309, 311, 406, 19577, 2306, 309, 311, 1566, 264, 3954, 264, 10612, 390, 787, 10515, 2835, 1310, 291, 727, 3470, 300, 420, 746, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16726491110665456, "compression_ratio": 1.5577889447236182, "no_speech_prob": 0.08497554063796997}, {"id": 486, "seek": 356400, "start": 3564.0, "end": 3574.0, "text": " So it's able to apply suggestions one and four and it does that and then I say well to Chris, you know, come up with criticism for the second paragraph.", "tokens": [50364, 407, 309, 311, 1075, 281, 3079, 13396, 472, 293, 1451, 293, 309, 775, 300, 293, 550, 286, 584, 731, 281, 6688, 11, 291, 458, 11, 808, 493, 365, 15835, 337, 264, 1150, 18865, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1780565946530073, "compression_ratio": 1.6473684210526316, "no_speech_prob": 0.08749308437108994}, {"id": 487, "seek": 356400, "start": 3574.0, "end": 3580.0, "text": " So it's able to criticize its own generation, its own creations and then apply changes.", "tokens": [50864, 407, 309, 311, 1075, 281, 31010, 1080, 1065, 5125, 11, 1080, 1065, 37836, 293, 550, 3079, 2962, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1780565946530073, "compression_ratio": 1.6473684210526316, "no_speech_prob": 0.08749308437108994}, {"id": 488, "seek": 356400, "start": 3580.0, "end": 3586.0, "text": " So then I said put it all together into a single story, and then I said,", "tokens": [51164, 407, 550, 286, 848, 829, 309, 439, 1214, 666, 257, 2167, 1657, 11, 293, 550, 286, 848, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1780565946530073, "compression_ratio": 1.6473684210526316, "no_speech_prob": 0.08749308437108994}, {"id": 489, "seek": 358600, "start": 3586.0, "end": 3598.0, "text": " the version of an illustration for each paragraph include the medium the artistic style mood point of view lighting and the like, and it comes up with these kinds of.", "tokens": [50364, 264, 3037, 295, 364, 22645, 337, 1184, 18865, 4090, 264, 6399, 264, 17090, 3758, 9268, 935, 295, 1910, 9577, 293, 264, 411, 11, 293, 309, 1487, 493, 365, 613, 3685, 295, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1807895302772522, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.06270898878574371}, {"id": 490, "seek": 358600, "start": 3598.0, "end": 3603.0, "text": " So then, at that point I said well I'd like to make a webpage in which you could do this interactively.", "tokens": [50964, 407, 550, 11, 412, 300, 935, 286, 848, 731, 286, 1116, 411, 281, 652, 257, 37852, 294, 597, 291, 727, 360, 341, 4648, 3413, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1807895302772522, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.06270898878574371}, {"id": 491, "seek": 358600, "start": 3603.0, "end": 3615.0, "text": " And then we get it works, but then eventually works, I should say, and this is a place where it didn't know how to use the.", "tokens": [51214, 400, 550, 321, 483, 309, 1985, 11, 457, 550, 4728, 1985, 11, 286, 820, 584, 11, 293, 341, 307, 257, 1081, 689, 309, 994, 380, 458, 577, 281, 764, 264, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1807895302772522, "compression_ratio": 1.6016260162601625, "no_speech_prob": 0.06270898878574371}, {"id": 492, "seek": 361500, "start": 3615.0, "end": 3637.0, "text": " It's the Dolly API, but I just pasted the documentation and you know it's really just copy and paste it with all the, you know, crazy stuff that happens when you just copy and paste it still understood it well enough to generate the right code.", "tokens": [50364, 467, 311, 264, 1144, 13020, 9362, 11, 457, 286, 445, 1791, 292, 264, 14333, 293, 291, 458, 309, 311, 534, 445, 5055, 293, 9163, 309, 365, 439, 264, 11, 291, 458, 11, 3219, 1507, 300, 2314, 562, 291, 445, 5055, 293, 9163, 309, 920, 7320, 309, 731, 1547, 281, 8460, 264, 558, 3089, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20406410611909012, "compression_ratio": 1.5541401273885351, "no_speech_prob": 0.006899259518831968}, {"id": 493, "seek": 363700, "start": 3637.0, "end": 3657.0, "text": " And so then I, it goes on and on very long this one I, I, once I got it working for that I said well let's make it work for any story and let's make it so that you could criticize the illustrations and generate new ones and so on.", "tokens": [50364, 400, 370, 550, 286, 11, 309, 1709, 322, 293, 322, 588, 938, 341, 472, 286, 11, 286, 11, 1564, 286, 658, 309, 1364, 337, 300, 286, 848, 731, 718, 311, 652, 309, 589, 337, 604, 1657, 293, 718, 311, 652, 309, 370, 300, 291, 727, 31010, 264, 34540, 293, 8460, 777, 2306, 293, 370, 322, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12607589032914904, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.05028846859931946}, {"id": 494, "seek": 363700, "start": 3657.0, "end": 3662.0, "text": " Maybe I'll show you the final result here.", "tokens": [51364, 2704, 286, 603, 855, 291, 264, 2572, 1874, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12607589032914904, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.05028846859931946}, {"id": 495, "seek": 366200, "start": 3663.0, "end": 3667.0, "text": " Well, by the way, I even if you want to get it.", "tokens": [50414, 1042, 11, 538, 264, 636, 11, 286, 754, 498, 291, 528, 281, 483, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1671009565654554, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.0675022229552269}, {"id": 496, "seek": 366200, "start": 3667.0, "end": 3683.0, "text": " An idea of how I did this without going through the entire 111 things I kind of highlighted the main things of what I did and what happened and what was going on here.", "tokens": [50614, 1107, 1558, 295, 577, 286, 630, 341, 1553, 516, 807, 264, 2302, 2975, 16, 721, 286, 733, 295, 17173, 264, 2135, 721, 295, 437, 286, 630, 293, 437, 2011, 293, 437, 390, 516, 322, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1671009565654554, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.0675022229552269}, {"id": 497, "seek": 368300, "start": 3683.0, "end": 3692.0, "text": " And the, here it is.", "tokens": [50364, 400, 264, 11, 510, 309, 307, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1942947781275189, "compression_ratio": 1.5354838709677419, "no_speech_prob": 0.006901630666106939}, {"id": 498, "seek": 368300, "start": 3692.0, "end": 3697.0, "text": " So we, by the way, it generated all of the instructions here.", "tokens": [50814, 407, 321, 11, 538, 264, 636, 11, 309, 10833, 439, 295, 264, 9415, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1942947781275189, "compression_ratio": 1.5354838709677419, "no_speech_prob": 0.006901630666106939}, {"id": 499, "seek": 368300, "start": 3697.0, "end": 3707.0, "text": " So it tells you again this is, I just said add in and instructions in the beginning with a button to close them and it explains how this whole thing works.", "tokens": [51064, 407, 309, 5112, 291, 797, 341, 307, 11, 286, 445, 848, 909, 294, 293, 9415, 294, 264, 2863, 365, 257, 2960, 281, 1998, 552, 293, 309, 13948, 577, 341, 1379, 551, 1985, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1942947781275189, "compression_ratio": 1.5354838709677419, "no_speech_prob": 0.006901630666106939}, {"id": 500, "seek": 370700, "start": 3707.0, "end": 3717.0, "text": " So then, you know, needs my key there and then I say,", "tokens": [50364, 407, 550, 11, 291, 458, 11, 2203, 452, 2141, 456, 293, 550, 286, 584, 11, 50864], "temperature": 0.0, "avg_logprob": -0.21528417185733192, "compression_ratio": 1.1875, "no_speech_prob": 0.0021823057904839516}, {"id": 501, "seek": 370700, "start": 3717.0, "end": 3727.0, "text": " can.", "tokens": [50864, 393, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21528417185733192, "compression_ratio": 1.1875, "no_speech_prob": 0.0021823057904839516}, {"id": 502, "seek": 370700, "start": 3727.0, "end": 3732.0, "text": " I didn't try this before three, three is probably good.", "tokens": [51364, 286, 994, 380, 853, 341, 949, 1045, 11, 1045, 307, 1391, 665, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21528417185733192, "compression_ratio": 1.1875, "no_speech_prob": 0.0021823057904839516}, {"id": 503, "seek": 373200, "start": 3732.0, "end": 3751.0, "text": " So one thing you could do here was I added this button that could actually show you the communications with GPT for and with Dolly so you could see what actually it's, you know, it does.", "tokens": [50364, 407, 472, 551, 291, 727, 360, 510, 390, 286, 3869, 341, 2960, 300, 727, 767, 855, 291, 264, 15163, 365, 26039, 51, 337, 293, 365, 1144, 13020, 370, 291, 727, 536, 437, 767, 309, 311, 11, 291, 458, 11, 309, 775, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15432138291616287, "compression_ratio": 1.536144578313253, "no_speech_prob": 0.0014547668397426605}, {"id": 504, "seek": 373200, "start": 3751.0, "end": 3758.0, "text": " So this shouldn't take more than another less than a minute I think.", "tokens": [51314, 407, 341, 4659, 380, 747, 544, 813, 1071, 1570, 813, 257, 3456, 286, 519, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15432138291616287, "compression_ratio": 1.536144578313253, "no_speech_prob": 0.0014547668397426605}, {"id": 505, "seek": 375800, "start": 3758.0, "end": 3778.0, "text": " So this is going to have all of those features that I kind of experimented with in the dialogue so you could ask for, ask it to generate criticism you could decide to apply the criticism to rewrite a paragraph, and you could also get criticism about the illustrations.", "tokens": [50364, 407, 341, 307, 516, 281, 362, 439, 295, 729, 4122, 300, 286, 733, 295, 5120, 292, 365, 294, 264, 10221, 370, 291, 727, 1029, 337, 11, 1029, 309, 281, 8460, 15835, 291, 727, 4536, 281, 3079, 264, 15835, 281, 28132, 257, 18865, 11, 293, 291, 727, 611, 483, 15835, 466, 264, 34540, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11058930346840307, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0010983918327838182}, {"id": 506, "seek": 377800, "start": 3778.0, "end": 3794.0, "text": " And you could even enter your own criticism of an illustration and it'll regenerate that will update the illustration prompt and then generate a new story and it should", "tokens": [50364, 400, 291, 727, 754, 3242, 428, 1065, 15835, 295, 364, 22645, 293, 309, 603, 26358, 473, 300, 486, 5623, 264, 22645, 12391, 293, 550, 8460, 257, 777, 1657, 293, 309, 820, 51164], "temperature": 0.0, "avg_logprob": -0.1585042787634808, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.07044573128223419}, {"id": 507, "seek": 377800, "start": 3794.0, "end": 3801.0, "text": " interleave the text and the images.", "tokens": [51164, 728, 306, 946, 264, 2487, 293, 264, 5267, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1585042787634808, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.07044573128223419}, {"id": 508, "seek": 380100, "start": 3801.0, "end": 3805.0, "text": " So,", "tokens": [50364, 407, 11, 50564], "temperature": 0.0, "avg_logprob": -0.13265963236490885, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.009409207850694656}, {"id": 509, "seek": 380100, "start": 3805.0, "end": 3828.0, "text": " one of the problems with using GPT for is that it's so much slower than 3.5 3.5 would have, you know, done this long ago but for just just as such a nicer job that I tend not to use it obviously I could have asked GPT for to make an interface where you could decide which,", "tokens": [50564, 472, 295, 264, 2740, 365, 1228, 26039, 51, 337, 307, 300, 309, 311, 370, 709, 14009, 813, 805, 13, 20, 805, 13, 20, 576, 362, 11, 291, 458, 11, 1096, 341, 938, 2057, 457, 337, 445, 445, 382, 1270, 257, 22842, 1691, 300, 286, 3928, 406, 281, 764, 309, 2745, 286, 727, 362, 2351, 26039, 51, 337, 281, 652, 364, 9226, 689, 291, 727, 4536, 597, 11, 51714], "temperature": 0.0, "avg_logprob": -0.13265963236490885, "compression_ratio": 1.4680851063829787, "no_speech_prob": 0.009409207850694656}, {"id": 510, "seek": 382800, "start": 3828.0, "end": 3832.0, "text": " what role do you want to use for doing this.", "tokens": [50364, 437, 3090, 360, 291, 528, 281, 764, 337, 884, 341, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23250460013365135, "compression_ratio": 1.5381355932203389, "no_speech_prob": 0.06176305189728737}, {"id": 511, "seek": 382800, "start": 3832.0, "end": 3836.0, "text": " Maybe what I should do. Oh, here comes finally.", "tokens": [50564, 2704, 437, 286, 820, 360, 13, 876, 11, 510, 1487, 2721, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23250460013365135, "compression_ratio": 1.5381355932203389, "no_speech_prob": 0.06176305189728737}, {"id": 512, "seek": 382800, "start": 3836.0, "end": 3852.0, "text": " So, I stood nervously at the podium with the prestigious Stanford University and elite seminar series in electrical engineering, he had spent countless hours perfecting his presentation and sustainable energy systems and was the culmination of his hard work as he looked", "tokens": [50764, 407, 11, 286, 9371, 5724, 5098, 412, 264, 26827, 365, 264, 33510, 20374, 3535, 293, 17801, 29235, 2638, 294, 12147, 7043, 11, 415, 632, 4418, 19223, 2496, 2176, 278, 702, 5860, 293, 11235, 2281, 3652, 293, 390, 264, 28583, 399, 295, 702, 1152, 589, 382, 415, 2956, 51564], "temperature": 0.0, "avg_logprob": -0.23250460013365135, "compression_ratio": 1.5381355932203389, "no_speech_prob": 0.06176305189728737}, {"id": 513, "seek": 385200, "start": 3852.0, "end": 3862.0, "text": " at the learning of intelligent faces eager to hear what he had to say, he felt a sense of pride also mounting pressure to deliver amounts and shoes, and here I am giving the speech.", "tokens": [50364, 412, 264, 2539, 295, 13232, 8475, 18259, 281, 1568, 437, 415, 632, 281, 584, 11, 415, 2762, 257, 2020, 295, 10936, 611, 22986, 3321, 281, 4239, 11663, 293, 6654, 11, 293, 510, 286, 669, 2902, 264, 6218, 13, 50864], "temperature": 0.0, "avg_logprob": -0.35336577892303467, "compression_ratio": 1.4148148148148147, "no_speech_prob": 0.12573757767677307}, {"id": 514, "seek": 385200, "start": 3862.0, "end": 3865.0, "text": " And if I,", "tokens": [50864, 400, 498, 286, 11, 51014], "temperature": 0.0, "avg_logprob": -0.35336577892303467, "compression_ratio": 1.4148148148148147, "no_speech_prob": 0.12573757767677307}, {"id": 515, "seek": 386500, "start": 3865.0, "end": 3875.0, "text": " throw over it we could see that it says the illustrations of watercolor painting with realistic and softly blended colors depicting the anxious Ken standing at the podium the setting as well.", "tokens": [50364, 3507, 670, 309, 321, 727, 536, 300, 309, 1619, 264, 34540, 295, 31727, 5370, 365, 12465, 293, 30832, 27048, 4577, 1367, 21490, 264, 15166, 8273, 4877, 412, 264, 26827, 264, 3287, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2715225566517223, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.2939041256904602}, {"id": 516, "seek": 386500, "start": 3875.0, "end": 3880.0, "text": " Lit hall with the iconic Stanford University seal.", "tokens": [50864, 41841, 6500, 365, 264, 15762, 20374, 3535, 12185, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2715225566517223, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.2939041256904602}, {"id": 517, "seek": 386500, "start": 3880.0, "end": 3883.0, "text": " That's true.", "tokens": [51114, 663, 311, 2074, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2715225566517223, "compression_ratio": 1.4825581395348837, "no_speech_prob": 0.2939041256904602}, {"id": 518, "seek": 388300, "start": 3883.0, "end": 3895.0, "text": " It's getting enhancing it intensity the point of view is from the audience looking. So it's a bit confused because the audience is there and it's looking at the audience but this is a dolly problem it's it's actually generated.", "tokens": [50364, 467, 311, 1242, 36579, 309, 13749, 264, 935, 295, 1910, 307, 490, 264, 4034, 1237, 13, 407, 309, 311, 257, 857, 9019, 570, 264, 4034, 307, 456, 293, 309, 311, 1237, 412, 264, 4034, 457, 341, 307, 257, 2722, 88, 1154, 309, 311, 309, 311, 767, 10833, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2673840858566929, "compression_ratio": 1.6358381502890174, "no_speech_prob": 0.32042714953422546}, {"id": 519, "seek": 388300, "start": 3895.0, "end": 3900.0, "text": " So if I click on this. Here's the, the description.", "tokens": [50964, 407, 498, 286, 2052, 322, 341, 13, 1692, 311, 264, 11, 264, 3855, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2673840858566929, "compression_ratio": 1.6358381502890174, "no_speech_prob": 0.32042714953422546}, {"id": 520, "seek": 388300, "start": 3900.0, "end": 3905.0, "text": " And", "tokens": [51214, 400, 51464], "temperature": 0.0, "avg_logprob": -0.2673840858566929, "compression_ratio": 1.6358381502890174, "no_speech_prob": 0.32042714953422546}, {"id": 521, "seek": 390500, "start": 3905.0, "end": 3908.0, "text": " Oh, that's interesting.", "tokens": [50364, 876, 11, 300, 311, 1880, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18090473810831706, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.007226318586617708}, {"id": 522, "seek": 390500, "start": 3908.0, "end": 3921.0, "text": " Well, maybe it's still thinking, because I think it's going to, it's going to show some suggested changes to the", "tokens": [50514, 1042, 11, 1310, 309, 311, 920, 1953, 11, 570, 286, 519, 309, 311, 516, 281, 11, 309, 311, 516, 281, 855, 512, 10945, 2962, 281, 264, 51164], "temperature": 0.0, "avg_logprob": -0.18090473810831706, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.007226318586617708}, {"id": 523, "seek": 390500, "start": 3921.0, "end": 3924.0, "text": " to the thing.", "tokens": [51164, 281, 264, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18090473810831706, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.007226318586617708}, {"id": 524, "seek": 390500, "start": 3924.0, "end": 3925.0, "text": " I don't know.", "tokens": [51314, 286, 500, 380, 458, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18090473810831706, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.007226318586617708}, {"id": 525, "seek": 390500, "start": 3925.0, "end": 3933.0, "text": " So maybe I'll just say,", "tokens": [51364, 407, 1310, 286, 603, 445, 584, 11, 51764], "temperature": 0.0, "avg_logprob": -0.18090473810831706, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.007226318586617708}, {"id": 526, "seek": 393300, "start": 3933.0, "end": 3948.0, "text": " I don't like changes. So rewrite the description.", "tokens": [50364, 286, 500, 380, 411, 2962, 13, 407, 28132, 264, 3855, 13, 51114], "temperature": 0.0, "avg_logprob": -0.23705613171612774, "compression_ratio": 1.5, "no_speech_prob": 0.02159038558602333}, {"id": 527, "seek": 393300, "start": 3948.0, "end": 3961.0, "text": " And then once it's the descriptions were written I could hit. So now it should say an oil painting was realistic when I say replace the image and generate a new one or something.", "tokens": [51114, 400, 550, 1564, 309, 311, 264, 24406, 645, 3720, 286, 727, 2045, 13, 407, 586, 309, 820, 584, 364, 3184, 5370, 390, 12465, 562, 286, 584, 7406, 264, 3256, 293, 8460, 257, 777, 472, 420, 746, 13, 51764], "temperature": 0.0, "avg_logprob": -0.23705613171612774, "compression_ratio": 1.5, "no_speech_prob": 0.02159038558602333}, {"id": 528, "seek": 396100, "start": 3961.0, "end": 3968.0, "text": " And even while this is happening I could, I think I could click. Oh, here it comes. I'll close this.", "tokens": [50364, 400, 754, 1339, 341, 307, 2737, 286, 727, 11, 286, 519, 286, 727, 2052, 13, 876, 11, 510, 309, 1487, 13, 286, 603, 1998, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15398826097187243, "compression_ratio": 1.502857142857143, "no_speech_prob": 0.009118761867284775}, {"id": 529, "seek": 396100, "start": 3968.0, "end": 3972.0, "text": " And there I am giving a lecture.", "tokens": [50714, 400, 456, 286, 669, 2902, 257, 7991, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15398826097187243, "compression_ratio": 1.502857142857143, "no_speech_prob": 0.009118761867284775}, {"id": 530, "seek": 396100, "start": 3972.0, "end": 3974.0, "text": " It looks just like you.", "tokens": [50914, 467, 1542, 445, 411, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15398826097187243, "compression_ratio": 1.502857142857143, "no_speech_prob": 0.009118761867284775}, {"id": 531, "seek": 396100, "start": 3974.0, "end": 3976.0, "text": " Yeah.", "tokens": [51014, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15398826097187243, "compression_ratio": 1.502857142857143, "no_speech_prob": 0.009118761867284775}, {"id": 532, "seek": 396100, "start": 3976.0, "end": 3986.0, "text": " And then if I click here, I'll get some criticism that it'll generate for the first paragraph here.", "tokens": [51114, 400, 550, 498, 286, 2052, 510, 11, 286, 603, 483, 512, 15835, 300, 309, 603, 8460, 337, 264, 700, 18865, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15398826097187243, "compression_ratio": 1.502857142857143, "no_speech_prob": 0.009118761867284775}, {"id": 533, "seek": 398600, "start": 3986.0, "end": 3994.0, "text": " By the way, we can see what the other paragraphs look like well the audience got excited here.", "tokens": [50364, 3146, 264, 636, 11, 321, 393, 536, 437, 264, 661, 48910, 574, 411, 731, 264, 4034, 658, 2919, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1716877266212746, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.010642534121870995}, {"id": 534, "seek": 398600, "start": 3994.0, "end": 3997.0, "text": " This is the pro AI group.", "tokens": [50764, 639, 307, 264, 447, 7318, 1594, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1716877266212746, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.010642534121870995}, {"id": 535, "seek": 398600, "start": 3997.0, "end": 3999.0, "text": " And then.", "tokens": [50914, 400, 550, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1716877266212746, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.010642534121870995}, {"id": 536, "seek": 398600, "start": 3999.0, "end": 4001.0, "text": " Wow.", "tokens": [51014, 3153, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1716877266212746, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.010642534121870995}, {"id": 537, "seek": 398600, "start": 4001.0, "end": 4006.0, "text": " Looks like everybody wanted my autograph at the end.", "tokens": [51114, 10027, 411, 2201, 1415, 452, 36660, 412, 264, 917, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1716877266212746, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.010642534121870995}, {"id": 538, "seek": 400600, "start": 4006.0, "end": 4015.0, "text": " It's pretty good.", "tokens": [50364, 467, 311, 1238, 665, 13, 50814], "temperature": 0.0, "avg_logprob": -0.35652333214169457, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.25034812092781067}, {"id": 539, "seek": 400600, "start": 4015.0, "end": 4024.0, "text": " As he concluded the talk and any questions it was clear that he made an impact professors and students like approached him praising. This is pretty good.", "tokens": [50814, 1018, 415, 22960, 264, 751, 293, 604, 1651, 309, 390, 1850, 300, 415, 1027, 364, 2712, 15924, 293, 1731, 411, 17247, 796, 42941, 13, 639, 307, 1238, 665, 13, 51264], "temperature": 0.0, "avg_logprob": -0.35652333214169457, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.25034812092781067}, {"id": 540, "seek": 400600, "start": 4024.0, "end": 4027.0, "text": " Anyways, if we go here.", "tokens": [51264, 15585, 11, 498, 321, 352, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.35652333214169457, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.25034812092781067}, {"id": 541, "seek": 400600, "start": 4027.0, "end": 4032.0, "text": " So it generates seven suggestions for how to change this paragraph.", "tokens": [51414, 407, 309, 23815, 3407, 13396, 337, 577, 281, 1319, 341, 18865, 13, 51664], "temperature": 0.0, "avg_logprob": -0.35652333214169457, "compression_ratio": 1.461111111111111, "no_speech_prob": 0.25034812092781067}, {"id": 542, "seek": 403200, "start": 4032.0, "end": 4036.0, "text": " The opening sentence.", "tokens": [50364, 440, 5193, 8174, 13, 50564], "temperature": 0.0, "avg_logprob": -0.21437652527339876, "compression_ratio": 1.408839779005525, "no_speech_prob": 0.022268768399953842}, {"id": 543, "seek": 403200, "start": 4036.0, "end": 4043.0, "text": " Introduce more background on Ken so that the readers understand my qualifications and reasons for giving the talk.", "tokens": [50564, 27193, 384, 544, 3678, 322, 8273, 370, 300, 264, 17147, 1223, 452, 33223, 293, 4112, 337, 2902, 264, 751, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21437652527339876, "compression_ratio": 1.408839779005525, "no_speech_prob": 0.022268768399953842}, {"id": 544, "seek": 403200, "start": 4043.0, "end": 4044.0, "text": " Whatever.", "tokens": [50914, 8541, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21437652527339876, "compression_ratio": 1.408839779005525, "no_speech_prob": 0.022268768399953842}, {"id": 545, "seek": 403200, "start": 4044.0, "end": 4049.0, "text": " And I could even say here you know change it from energy systems to chat.", "tokens": [50964, 400, 286, 727, 754, 584, 510, 291, 458, 1319, 309, 490, 2281, 3652, 281, 5081, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21437652527339876, "compression_ratio": 1.408839779005525, "no_speech_prob": 0.022268768399953842}, {"id": 546, "seek": 403200, "start": 4049.0, "end": 4055.0, "text": " So that's the, the most ambitious.", "tokens": [51214, 407, 300, 311, 264, 11, 264, 881, 20239, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21437652527339876, "compression_ratio": 1.408839779005525, "no_speech_prob": 0.022268768399953842}, {"id": 547, "seek": 405500, "start": 4055.0, "end": 4060.0, "text": " That I created with chat.", "tokens": [50364, 663, 286, 2942, 365, 5081, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17575858295827673, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.012046463787555695}, {"id": 548, "seek": 405500, "start": 4060.0, "end": 4064.0, "text": " So I'm practically finished here.", "tokens": [50614, 407, 286, 478, 15667, 4335, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17575858295827673, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.012046463787555695}, {"id": 549, "seek": 405500, "start": 4064.0, "end": 4071.0, "text": " So one question in my mind is doing this does it automate away the fun of programming.", "tokens": [50814, 407, 472, 1168, 294, 452, 1575, 307, 884, 341, 775, 309, 31605, 1314, 264, 1019, 295, 9410, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17575858295827673, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.012046463787555695}, {"id": 550, "seek": 405500, "start": 4071.0, "end": 4081.0, "text": " Or is it better to think of this as empowering people with very little programming skills to create all sorts of be very creative and creating apps.", "tokens": [51164, 1610, 307, 309, 1101, 281, 519, 295, 341, 382, 28261, 561, 365, 588, 707, 9410, 3942, 281, 1884, 439, 7527, 295, 312, 588, 5880, 293, 4084, 7733, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17575858295827673, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.012046463787555695}, {"id": 551, "seek": 408100, "start": 4081.0, "end": 4096.0, "text": " And see more pampered often wrote about hard fun because one of the children talked about how programming and logo was was hard fun that it was, or he probably said it was fun but hard.", "tokens": [50364, 400, 536, 544, 30738, 40004, 2049, 4114, 466, 1152, 1019, 570, 472, 295, 264, 2227, 2825, 466, 577, 9410, 293, 9699, 390, 390, 1152, 1019, 300, 309, 390, 11, 420, 415, 1391, 848, 309, 390, 1019, 457, 1152, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21129195634708847, "compression_ratio": 1.4682539682539681, "no_speech_prob": 0.002888672286644578}, {"id": 552, "seek": 409600, "start": 4096.0, "end": 4099.0, "text": " So,", "tokens": [50364, 407, 11, 50514], "temperature": 0.0, "avg_logprob": -0.12967381007234816, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.017162833362817764}, {"id": 553, "seek": 409600, "start": 4099.0, "end": 4112.0, "text": " that's still the case but it's also for a lot of children they find it frustrating or too difficult and, you know, just doesn't always work to introduce programming to children.", "tokens": [50514, 300, 311, 920, 264, 1389, 457, 309, 311, 611, 337, 257, 688, 295, 2227, 436, 915, 309, 16522, 420, 886, 2252, 293, 11, 291, 458, 11, 445, 1177, 380, 1009, 589, 281, 5366, 9410, 281, 2227, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12967381007234816, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.017162833362817764}, {"id": 554, "seek": 409600, "start": 4112.0, "end": 4120.0, "text": " And also there's sometimes the teachers don't are much support because they don't understand very well how to program either.", "tokens": [51164, 400, 611, 456, 311, 2171, 264, 6023, 500, 380, 366, 709, 1406, 570, 436, 500, 380, 1223, 588, 731, 577, 281, 1461, 2139, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12967381007234816, "compression_ratio": 1.527363184079602, "no_speech_prob": 0.017162833362817764}, {"id": 555, "seek": 412000, "start": 4120.0, "end": 4136.0, "text": " So the, the plus side is that by having a collaboration like this anybody with just an idea for an app that's reasonable obviously, if you wanted to make, you know, a triple A rated game or something, you're not likely to succeed.", "tokens": [50364, 407, 264, 11, 264, 1804, 1252, 307, 300, 538, 1419, 257, 9363, 411, 341, 4472, 365, 445, 364, 1558, 337, 364, 724, 300, 311, 10585, 2745, 11, 498, 291, 1415, 281, 652, 11, 291, 458, 11, 257, 15508, 316, 22103, 1216, 420, 746, 11, 291, 434, 406, 3700, 281, 7754, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13736031020897022, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.01132592000067234}, {"id": 556, "seek": 412000, "start": 4136.0, "end": 4140.0, "text": " But it could come to life through this kind of process.", "tokens": [51164, 583, 309, 727, 808, 281, 993, 807, 341, 733, 295, 1399, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13736031020897022, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.01132592000067234}, {"id": 557, "seek": 414000, "start": 4140.0, "end": 4160.0, "text": " Of course, GPT for is GPT for there will be a five and a six and a seven and eight. Yeah, exactly. That's exactly what I'm talking about the future here so the short term we could expect it just to get, you know, more competent and the GPT for can", "tokens": [50364, 2720, 1164, 11, 26039, 51, 337, 307, 26039, 51, 337, 456, 486, 312, 257, 1732, 293, 257, 2309, 293, 257, 3407, 293, 3180, 13, 865, 11, 2293, 13, 663, 311, 2293, 437, 286, 478, 1417, 466, 264, 2027, 510, 370, 264, 2099, 1433, 321, 727, 2066, 309, 445, 281, 483, 11, 291, 458, 11, 544, 29998, 293, 264, 26039, 51, 337, 393, 51364], "temperature": 0.0, "avg_logprob": -0.230006810390588, "compression_ratio": 1.4702380952380953, "no_speech_prob": 0.23041106760501862}, {"id": 558, "seek": 416000, "start": 4160.0, "end": 4176.0, "text": " look at images and understand them, but that's only been released to a to a very small number of testers and some company that's producing software for blind people.", "tokens": [50364, 574, 412, 5267, 293, 1223, 552, 11, 457, 300, 311, 787, 668, 4736, 281, 257, 281, 257, 588, 1359, 1230, 295, 1500, 433, 293, 512, 2237, 300, 311, 10501, 4722, 337, 6865, 561, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1175020368475663, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.3270949423313141}, {"id": 559, "seek": 417600, "start": 4176.0, "end": 4194.0, "text": " But that's coming so you could you should be able to just like take a screenshot of of a problem with your app and then say well here's here's what what's going on and it'll interpret the image and maybe figure out how to fix things.", "tokens": [50364, 583, 300, 311, 1348, 370, 291, 727, 291, 820, 312, 1075, 281, 445, 411, 747, 257, 27712, 295, 295, 257, 1154, 365, 428, 724, 293, 550, 584, 731, 510, 311, 510, 311, 437, 437, 311, 516, 322, 293, 309, 603, 7302, 264, 3256, 293, 1310, 2573, 484, 577, 281, 3191, 721, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13871734482901438, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.13651983439922333}, {"id": 560, "seek": 419400, "start": 4194.0, "end": 4216.0, "text": " And this copy and paste thing is, you know, obviously can be automated. And, you know, the, probably as many of you know about the, the leaked Google Docs that claims that the open source alternatives to barred and GPT for are getting so good that maybe", "tokens": [50364, 400, 341, 5055, 293, 9163, 551, 307, 11, 291, 458, 11, 2745, 393, 312, 18473, 13, 400, 11, 291, 458, 11, 264, 11, 1391, 382, 867, 295, 291, 458, 466, 264, 11, 264, 31779, 3329, 16024, 82, 300, 9441, 300, 264, 1269, 4009, 20478, 281, 2159, 986, 293, 26039, 51, 337, 366, 1242, 370, 665, 300, 1310, 51464], "temperature": 0.0, "avg_logprob": -0.18656841653292297, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.15587303042411804}, {"id": 561, "seek": 421600, "start": 4216.0, "end": 4224.0, "text": " the big players don't really have any particular edge over what's going on in the open source rule.", "tokens": [50364, 264, 955, 4150, 500, 380, 534, 362, 604, 1729, 4691, 670, 437, 311, 516, 322, 294, 264, 1269, 4009, 4978, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07342644532521565, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.05028315633535385}, {"id": 562, "seek": 421600, "start": 4224.0, "end": 4238.0, "text": " So maybe that will be something that near term could be done but the long term who knows it's really going to keep getting better and more capable and so on.", "tokens": [50764, 407, 1310, 300, 486, 312, 746, 300, 2651, 1433, 727, 312, 1096, 457, 264, 938, 1433, 567, 3255, 309, 311, 534, 516, 281, 1066, 1242, 1101, 293, 544, 8189, 293, 370, 322, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07342644532521565, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.05028315633535385}, {"id": 563, "seek": 423800, "start": 4238.0, "end": 4254.0, "text": " And so I, I asked Dolly to create a water color painting for the question and answer portion of the talk. And I said no text should be included in it.", "tokens": [50364, 400, 370, 286, 11, 286, 2351, 1144, 13020, 281, 1884, 257, 1281, 2017, 5370, 337, 264, 1168, 293, 1867, 8044, 295, 264, 751, 13, 400, 286, 848, 572, 2487, 820, 312, 5556, 294, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2418505595280574, "compression_ratio": 1.2605042016806722, "no_speech_prob": 0.04140113666653633}, {"id": 564, "seek": 425400, "start": 4255.0, "end": 4264.0, "text": " So, hopefully there are some questions.", "tokens": [50414, 407, 11, 4696, 456, 366, 512, 1651, 13, 50864], "temperature": 0.0, "avg_logprob": -0.26017753009138433, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.21175318956375122}, {"id": 565, "seek": 425400, "start": 4264.0, "end": 4270.0, "text": " How do you like working with an AI as a, as a colleague.", "tokens": [50864, 1012, 360, 291, 411, 1364, 365, 364, 7318, 382, 257, 11, 382, 257, 13532, 13, 51164], "temperature": 0.0, "avg_logprob": -0.26017753009138433, "compression_ratio": 1.0909090909090908, "no_speech_prob": 0.21175318956375122}, {"id": 566, "seek": 427000, "start": 4271.0, "end": 4294.0, "text": " The, the first thing, the fireworks thing was the most pleasant one because it was, it was trying to be, you know, had this role more of a tutor, the, the other ones that was still, you know, really interesting but I felt like it was kind of writing this program and asking", "tokens": [50414, 440, 11, 264, 700, 551, 11, 264, 28453, 551, 390, 264, 881, 16232, 472, 570, 309, 390, 11, 309, 390, 1382, 281, 312, 11, 291, 458, 11, 632, 341, 3090, 544, 295, 257, 35613, 11, 264, 11, 264, 661, 2306, 300, 390, 920, 11, 291, 458, 11, 534, 1880, 457, 286, 2762, 411, 309, 390, 733, 295, 3579, 341, 1461, 293, 3365, 51564], "temperature": 0.0, "avg_logprob": -0.15225745692397608, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.07361514121294022}, {"id": 567, "seek": 429400, "start": 4294.0, "end": 4313.0, "text": " for a lot of copy and pasting, you know, and to report about errors, you know, that that was sort of my role. I mean, of course, that's not quite fair because what I also did was, and this is maybe an important aspect of this is that I always started with the simplest version of the app that I had in mind.", "tokens": [50364, 337, 257, 688, 295, 5055, 293, 1791, 278, 11, 291, 458, 11, 293, 281, 2275, 466, 13603, 11, 291, 458, 11, 300, 300, 390, 1333, 295, 452, 3090, 13, 286, 914, 11, 295, 1164, 11, 300, 311, 406, 1596, 3143, 570, 437, 286, 611, 630, 390, 11, 293, 341, 307, 1310, 364, 1021, 4171, 295, 341, 307, 300, 286, 1009, 1409, 365, 264, 22811, 3037, 295, 264, 724, 300, 286, 632, 294, 1575, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1599303392263559, "compression_ratio": 1.6329787234042554, "no_speech_prob": 0.1401003748178482}, {"id": 568, "seek": 431300, "start": 4313.0, "end": 4336.0, "text": " And once I got that, I would ask for a single enhancement and once that works another one so I was, I was, I did have a, an incremental plan for how the app should get build that was purely mine and it was, you know, my assistant in that role but, but often it was more like, you know,", "tokens": [50364, 400, 1564, 286, 658, 300, 11, 286, 576, 1029, 337, 257, 2167, 40776, 293, 1564, 300, 1985, 1071, 472, 370, 286, 390, 11, 286, 390, 11, 286, 630, 362, 257, 11, 364, 35759, 1393, 337, 577, 264, 724, 820, 483, 1322, 300, 390, 17491, 3892, 293, 309, 390, 11, 291, 458, 11, 452, 10994, 294, 300, 3090, 457, 11, 457, 2049, 309, 390, 544, 411, 11, 291, 458, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1456255847460603, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.11104726046323776}, {"id": 569, "seek": 433600, "start": 4336.0, "end": 4344.0, "text": " just telling me okay here's some code replace this here's some code, you know, copy and paste this and.", "tokens": [50364, 445, 3585, 385, 1392, 510, 311, 512, 3089, 7406, 341, 510, 311, 512, 3089, 11, 291, 458, 11, 5055, 293, 9163, 341, 293, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15067826308213272, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.05997360497713089}, {"id": 570, "seek": 433600, "start": 4344.0, "end": 4362.0, "text": " But it was still something exciting about the fact that you know it understood me so well and I was, and I was able to do so many things and knew all these different APIs and was better at CSS than I am for sure.", "tokens": [50764, 583, 309, 390, 920, 746, 4670, 466, 264, 1186, 300, 291, 458, 309, 7320, 385, 370, 731, 293, 286, 390, 11, 293, 286, 390, 1075, 281, 360, 370, 867, 721, 293, 2586, 439, 613, 819, 21445, 293, 390, 1101, 412, 24387, 813, 286, 669, 337, 988, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15067826308213272, "compression_ratio": 1.5879396984924623, "no_speech_prob": 0.05997360497713089}, {"id": 571, "seek": 436200, "start": 4362.0, "end": 4370.0, "text": " It was hard, at least in my experience CSS is not transferred.", "tokens": [50364, 467, 390, 1152, 11, 412, 1935, 294, 452, 1752, 24387, 307, 406, 15809, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17954805814302885, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.084962397813797}, {"id": 572, "seek": 436200, "start": 4370.0, "end": 4386.0, "text": " So, I attended a talk at Stanford, a couple weeks ago, and they did a study of the security of the code generated I think it was by co pilot co generator, and they found that it was dismal.", "tokens": [50764, 407, 11, 286, 15990, 257, 751, 412, 20374, 11, 257, 1916, 3259, 2057, 11, 293, 436, 630, 257, 2979, 295, 264, 3825, 295, 264, 3089, 10833, 286, 519, 309, 390, 538, 598, 9691, 598, 19265, 11, 293, 436, 1352, 300, 309, 390, 717, 5579, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17954805814302885, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.084962397813797}, {"id": 573, "seek": 438600, "start": 4386.0, "end": 4395.0, "text": " In fact, programmers who use co pilot produced less secure code than those who did not, but the ones who did thought their code was more secure.", "tokens": [50364, 682, 1186, 11, 41504, 567, 764, 598, 9691, 7126, 1570, 7144, 3089, 813, 729, 567, 630, 406, 11, 457, 264, 2306, 567, 630, 1194, 641, 3089, 390, 544, 7144, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07829602367906685, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.040798112750053406}, {"id": 574, "seek": 438600, "start": 4395.0, "end": 4402.0, "text": " And the conclusion of that study was that the assistant lacks context.", "tokens": [50814, 400, 264, 10063, 295, 300, 2979, 390, 300, 264, 10994, 31132, 4319, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07829602367906685, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.040798112750053406}, {"id": 575, "seek": 438600, "start": 4402.0, "end": 4414.0, "text": " Why you were showing some of the examples I was just scrolling through the text and it looks like it's the same problem there that the assistant is losing context.", "tokens": [51164, 1545, 291, 645, 4099, 512, 295, 264, 5110, 286, 390, 445, 29053, 807, 264, 2487, 293, 309, 1542, 411, 309, 311, 264, 912, 1154, 456, 300, 264, 10994, 307, 7027, 4319, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07829602367906685, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.040798112750053406}, {"id": 576, "seek": 441400, "start": 4415.0, "end": 4432.0, "text": " Yeah, no, that was the other annoying thing was that it was losing context and there were a few times when I realized that it just had completely forgotten what the source code for for a part of it or so I would copy and paste something", "tokens": [50414, 865, 11, 572, 11, 300, 390, 264, 661, 11304, 551, 390, 300, 309, 390, 7027, 4319, 293, 456, 645, 257, 1326, 1413, 562, 286, 5334, 300, 309, 445, 632, 2584, 11832, 437, 264, 4009, 3089, 337, 337, 257, 644, 295, 309, 420, 370, 286, 576, 5055, 293, 9163, 746, 51264], "temperature": 0.0, "avg_logprob": -0.14690821125822248, "compression_ratio": 1.5324675324675325, "no_speech_prob": 0.0100092189386487}, {"id": 577, "seek": 443200, "start": 4433.0, "end": 4445.0, "text": " much earlier and said, you know, and here's the dysfunction or here's this HTML that we're using and that often would would help because now it was in the", "tokens": [50414, 709, 3071, 293, 848, 11, 291, 458, 11, 293, 510, 311, 264, 32002, 420, 510, 311, 341, 17995, 300, 321, 434, 1228, 293, 300, 2049, 576, 576, 854, 570, 586, 309, 390, 294, 264, 51014], "temperature": 0.0, "avg_logprob": -0.18133716123649873, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.042038578540086746}, {"id": 578, "seek": 443200, "start": 4445.0, "end": 4460.0, "text": " I wasn't thinking so much of forgetting as it seemed that in some cases, it, it misinterpreted the context of what you were asking, because it didn't know the environment in which you were working.", "tokens": [51014, 286, 2067, 380, 1953, 370, 709, 295, 25428, 382, 309, 6576, 300, 294, 512, 3331, 11, 309, 11, 309, 3346, 41935, 292, 264, 4319, 295, 437, 291, 645, 3365, 11, 570, 309, 994, 380, 458, 264, 2823, 294, 597, 291, 645, 1364, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18133716123649873, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.042038578540086746}, {"id": 579, "seek": 446000, "start": 4460.0, "end": 4463.0, "text": " Yeah.", "tokens": [50364, 865, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20468730926513673, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.007340035866945982}, {"id": 580, "seek": 446000, "start": 4463.0, "end": 4467.0, "text": " Well,", "tokens": [50514, 1042, 11, 50714], "temperature": 0.0, "avg_logprob": -0.20468730926513673, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.007340035866945982}, {"id": 581, "seek": 446000, "start": 4467.0, "end": 4483.0, "text": " generally, I mean like that example where with the video one where I had originally intended. Oh, I should put my video back on.", "tokens": [50714, 5101, 11, 286, 914, 411, 300, 1365, 689, 365, 264, 960, 472, 689, 286, 632, 7993, 10226, 13, 876, 11, 286, 820, 829, 452, 960, 646, 322, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20468730926513673, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.007340035866945982}, {"id": 582, "seek": 448300, "start": 4483.0, "end": 4495.0, "text": " I originally intended it, you know that I can point with my finger in different directions and that would indicate which way the pen would move or something, as opposed to the way it came up with.", "tokens": [50364, 286, 7993, 10226, 309, 11, 291, 458, 300, 286, 393, 935, 365, 452, 5984, 294, 819, 11095, 293, 300, 576, 13330, 597, 636, 264, 3435, 576, 1286, 420, 746, 11, 382, 8851, 281, 264, 636, 309, 1361, 493, 365, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1113582675376635, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.021595465019345284}, {"id": 583, "seek": 448300, "start": 4495.0, "end": 4511.0, "text": " But I was just as happy with that rather than trying to get it to interpret it the way I intended, but in terms of security I thought it was pretty impressive that it really tried hard to talk me out of", "tokens": [50964, 583, 286, 390, 445, 382, 2055, 365, 300, 2831, 813, 1382, 281, 483, 309, 281, 7302, 309, 264, 636, 286, 10226, 11, 457, 294, 2115, 295, 3825, 286, 1194, 309, 390, 1238, 8992, 300, 309, 534, 3031, 1152, 281, 751, 385, 484, 295, 51764], "temperature": 0.0, "avg_logprob": -0.1113582675376635, "compression_ratio": 1.6694560669456067, "no_speech_prob": 0.021595465019345284}, {"id": 584, "seek": 451100, "start": 4511.0, "end": 4516.0, "text": " using the API key in a static web page like this.", "tokens": [50364, 1228, 264, 9362, 2141, 294, 257, 13437, 3670, 3028, 411, 341, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13527152907680456, "compression_ratio": 1.6, "no_speech_prob": 0.017411476001143456}, {"id": 585, "seek": 451100, "start": 4516.0, "end": 4533.0, "text": " On the other hand, I, you know, I said, you know, make it a password field and, you know, it's, it's not really a problem that that a user is putting the key into a web page like this but of course,", "tokens": [50614, 1282, 264, 661, 1011, 11, 286, 11, 291, 458, 11, 286, 848, 11, 291, 458, 11, 652, 309, 257, 11524, 2519, 293, 11, 291, 458, 11, 309, 311, 11, 309, 311, 406, 534, 257, 1154, 300, 300, 257, 4195, 307, 3372, 264, 2141, 666, 257, 3670, 3028, 411, 341, 457, 295, 1164, 11, 51464], "temperature": 0.0, "avg_logprob": -0.13527152907680456, "compression_ratio": 1.6, "no_speech_prob": 0.017411476001143456}, {"id": 586, "seek": 453300, "start": 4533.0, "end": 4547.0, "text": " once it's hosted somewhere, somebody has to trust that you're not stealing their key. But of course the, the, in every one of these cases you can look at the source and the code is actually quite clear and nice.", "tokens": [50364, 1564, 309, 311, 19204, 4079, 11, 2618, 575, 281, 3361, 300, 291, 434, 406, 19757, 641, 2141, 13, 583, 295, 1164, 264, 11, 264, 11, 294, 633, 472, 295, 613, 3331, 291, 393, 574, 412, 264, 4009, 293, 264, 3089, 307, 767, 1596, 1850, 293, 1481, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1215848786490304, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.010981070809066296}, {"id": 587, "seek": 453300, "start": 4547.0, "end": 4556.0, "text": " So somebody could see that the API key isn't sent off to somebody or something.", "tokens": [51064, 407, 2618, 727, 536, 300, 264, 9362, 2141, 1943, 380, 2279, 766, 281, 2618, 420, 746, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1215848786490304, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.010981070809066296}, {"id": 588, "seek": 455600, "start": 4556.0, "end": 4572.0, "text": " In this kind of context, I don't think the security is such an issue, but obviously, if you're, if you're making some kind of web service and, you know, there's a lot more things to think about in terms of security.", "tokens": [50364, 682, 341, 733, 295, 4319, 11, 286, 500, 380, 519, 264, 3825, 307, 1270, 364, 2734, 11, 457, 2745, 11, 498, 291, 434, 11, 498, 291, 434, 1455, 512, 733, 295, 3670, 2643, 293, 11, 291, 458, 11, 456, 311, 257, 688, 544, 721, 281, 519, 466, 294, 2115, 295, 3825, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14068130935941422, "compression_ratio": 1.4625850340136055, "no_speech_prob": 0.011321128346025944}, {"id": 589, "seek": 457200, "start": 4572.0, "end": 4586.0, "text": " Yeah, I was, I wasn't so worried about security for these apps as much as the context issue. There's so much implicit context that the programmer has that the assistant does not.", "tokens": [50364, 865, 11, 286, 390, 11, 286, 2067, 380, 370, 5804, 466, 3825, 337, 613, 7733, 382, 709, 382, 264, 4319, 2734, 13, 821, 311, 370, 709, 26947, 4319, 300, 264, 32116, 575, 300, 264, 10994, 775, 406, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12626470625400543, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.008055628277361393}, {"id": 590, "seek": 457200, "start": 4586.0, "end": 4595.0, "text": " Right. Yeah, I mentioned it'll be quite different if I was doing some really large project.", "tokens": [51064, 1779, 13, 865, 11, 286, 2835, 309, 603, 312, 1596, 819, 498, 286, 390, 884, 512, 534, 2416, 1716, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12626470625400543, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.008055628277361393}, {"id": 591, "seek": 459500, "start": 4595.0, "end": 4619.0, "text": " Because, you know, so the, this last one, the illustrated story generator, it did, it was a little more than 500 lines of JavaScript. So, and, and that's actually too large for the, at least the default version of chat GBT I can't say, you know, paste in this entire 500 lines and say", "tokens": [50364, 1436, 11, 291, 458, 11, 370, 264, 11, 341, 1036, 472, 11, 264, 33875, 1657, 19265, 11, 309, 630, 11, 309, 390, 257, 707, 544, 813, 5923, 3876, 295, 15778, 13, 407, 11, 293, 11, 293, 300, 311, 767, 886, 2416, 337, 264, 11, 412, 1935, 264, 7576, 3037, 295, 5081, 26809, 51, 286, 393, 380, 584, 11, 291, 458, 11, 9163, 294, 341, 2302, 5923, 3876, 293, 584, 51564], "temperature": 0.0, "avg_logprob": -0.20983387672737852, "compression_ratio": 1.4947368421052631, "no_speech_prob": 0.03959973156452179}, {"id": 592, "seek": 461900, "start": 4619.0, "end": 4625.0, "text": " now let's modify it in some way it's just too much context.", "tokens": [50364, 586, 718, 311, 16927, 309, 294, 512, 636, 309, 311, 445, 886, 709, 4319, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15768496669940094, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.07792286574840546}, {"id": 593, "seek": 461900, "start": 4625.0, "end": 4642.0, "text": " I mean, too many tokens for it to be fit its context window. So there are these limitations of course, the, like I just saw today some startup saying that they've got a context window of 50,000 tokens.", "tokens": [50664, 286, 914, 11, 886, 867, 22667, 337, 309, 281, 312, 3318, 1080, 4319, 4910, 13, 407, 456, 366, 613, 15705, 295, 1164, 11, 264, 11, 411, 286, 445, 1866, 965, 512, 18578, 1566, 300, 436, 600, 658, 257, 4319, 4910, 295, 2625, 11, 1360, 22667, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15768496669940094, "compression_ratio": 1.5086705202312138, "no_speech_prob": 0.07792286574840546}, {"id": 594, "seek": 464200, "start": 4642.0, "end": 4646.0, "text": " I've been, you know, using I think 8000 or something.", "tokens": [50364, 286, 600, 668, 11, 291, 458, 11, 1228, 286, 519, 1649, 1360, 420, 746, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1503140926361084, "compression_ratio": 1.595, "no_speech_prob": 0.014266682788729668}, {"id": 595, "seek": 464200, "start": 4646.0, "end": 4671.0, "text": " So that, and there's even some research paper that claimed that they had a million token context that was using some clever algorithms because you know that the context normally is a quadratic complexity but if you're clever you can make it and log in or something.", "tokens": [50564, 407, 300, 11, 293, 456, 311, 754, 512, 2132, 3035, 300, 12941, 300, 436, 632, 257, 2459, 14862, 4319, 300, 390, 1228, 512, 13494, 14642, 570, 291, 458, 300, 264, 4319, 5646, 307, 257, 37262, 14024, 457, 498, 291, 434, 13494, 291, 393, 652, 309, 293, 3565, 294, 420, 746, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1503140926361084, "compression_ratio": 1.595, "no_speech_prob": 0.014266682788729668}, {"id": 596, "seek": 467100, "start": 4672.0, "end": 4697.0, "text": " Well, I think I really blown away again this is this is a very nice expression to how people seem to be using the, the artificial intelligence devices or personal persona.", "tokens": [50414, 1042, 11, 286, 519, 286, 534, 16479, 1314, 797, 341, 307, 341, 307, 257, 588, 1481, 6114, 281, 577, 561, 1643, 281, 312, 1228, 264, 11, 264, 11677, 7599, 5759, 420, 2973, 12184, 13, 51664], "temperature": 0.0, "avg_logprob": -0.28661401648270457, "compression_ratio": 1.368, "no_speech_prob": 0.01221736054867506}, {"id": 597, "seek": 469700, "start": 4697.0, "end": 4724.0, "text": " This is as a component in a larger system. And I guess now the ads we see for, you know, doctors who use AI and get us to use AI and people who play with AI, driven games and so forth, has, has more context in terms of the current AI systems.", "tokens": [50364, 639, 307, 382, 257, 6542, 294, 257, 4833, 1185, 13, 400, 286, 2041, 586, 264, 10342, 321, 536, 337, 11, 291, 458, 11, 8778, 567, 764, 7318, 293, 483, 505, 281, 764, 7318, 293, 561, 567, 862, 365, 7318, 11, 9555, 2813, 293, 370, 5220, 11, 575, 11, 575, 544, 4319, 294, 2115, 295, 264, 2190, 7318, 3652, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19251557001991879, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.0479704886674881}, {"id": 598, "seek": 472400, "start": 4724.0, "end": 4728.0, "text": " I think that's a very useful thing.", "tokens": [50364, 286, 519, 300, 311, 257, 588, 4420, 551, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13718987913692698, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.049481675028800964}, {"id": 599, "seek": 472400, "start": 4728.0, "end": 4730.0, "text": " Yeah, you're back.", "tokens": [50564, 865, 11, 291, 434, 646, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13718987913692698, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.049481675028800964}, {"id": 600, "seek": 472400, "start": 4730.0, "end": 4732.0, "text": " Yeah.", "tokens": [50664, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13718987913692698, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.049481675028800964}, {"id": 601, "seek": 472400, "start": 4732.0, "end": 4752.0, "text": " I think I really appreciate the introduction because I had not thought about this part of it I always thought the hard part was twiddling the knobs on the, the data used to do on the AI but", "tokens": [50764, 286, 519, 286, 534, 4449, 264, 9339, 570, 286, 632, 406, 1194, 466, 341, 644, 295, 309, 286, 1009, 1194, 264, 1152, 644, 390, 683, 14273, 1688, 264, 46999, 322, 264, 11, 264, 1412, 1143, 281, 360, 322, 264, 7318, 457, 51764], "temperature": 0.0, "avg_logprob": -0.13718987913692698, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.049481675028800964}, {"id": 602, "seek": 475200, "start": 4752.0, "end": 4757.0, "text": " talking to the AI turns out to be complex enough.", "tokens": [50364, 1417, 281, 264, 7318, 4523, 484, 281, 312, 3997, 1547, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11347409743296949, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.025147374719381332}, {"id": 603, "seek": 475200, "start": 4757.0, "end": 4759.0, "text": " Yeah.", "tokens": [50614, 865, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11347409743296949, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.025147374719381332}, {"id": 604, "seek": 475200, "start": 4759.0, "end": 4776.0, "text": " You know, if you go back a year you had to give it, you know, several examples of what you were intending and had to engineer your prompt kind of carefully to get it to be pretty competent but GPT for as I mentioned I really just seem much more of a natural", "tokens": [50714, 509, 458, 11, 498, 291, 352, 646, 257, 1064, 291, 632, 281, 976, 309, 11, 291, 458, 11, 2940, 5110, 295, 437, 291, 645, 560, 2029, 293, 632, 281, 11403, 428, 12391, 733, 295, 7500, 281, 483, 309, 281, 312, 1238, 29998, 457, 26039, 51, 337, 382, 286, 2835, 286, 534, 445, 1643, 709, 544, 295, 257, 3303, 51564], "temperature": 0.0, "avg_logprob": -0.11347409743296949, "compression_ratio": 1.5048076923076923, "no_speech_prob": 0.025147374719381332}, {"id": 605, "seek": 477600, "start": 4776.0, "end": 4779.0, "text": " conversation.", "tokens": [50364, 3761, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15758705139160156, "compression_ratio": 1.3106060606060606, "no_speech_prob": 0.13638894259929657}, {"id": 606, "seek": 477600, "start": 4779.0, "end": 4788.0, "text": " And I'm really I'm curious to see how, you know, a 12 year old deals with such things, you know, better.", "tokens": [50514, 400, 286, 478, 534, 286, 478, 6369, 281, 536, 577, 11, 291, 458, 11, 257, 2272, 1064, 1331, 11215, 365, 1270, 721, 11, 291, 458, 11, 1101, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15758705139160156, "compression_ratio": 1.3106060606060606, "no_speech_prob": 0.13638894259929657}, {"id": 607, "seek": 477600, "start": 4788.0, "end": 4790.0, "text": " Probably better than you and I do.", "tokens": [50964, 9210, 1101, 813, 291, 293, 286, 360, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15758705139160156, "compression_ratio": 1.3106060606060606, "no_speech_prob": 0.13638894259929657}, {"id": 608, "seek": 477600, "start": 4790.0, "end": 4794.0, "text": " Yeah, that's right.", "tokens": [51064, 865, 11, 300, 311, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15758705139160156, "compression_ratio": 1.3106060606060606, "no_speech_prob": 0.13638894259929657}, {"id": 609, "seek": 479400, "start": 4794.0, "end": 4812.0, "text": " My, my, my favorite story is watching a non speaking child sort of order six months old stealing his mother's phone and the per password and then using it to play his video.", "tokens": [50364, 1222, 11, 452, 11, 452, 2954, 1657, 307, 1976, 257, 2107, 4124, 1440, 1333, 295, 1668, 2309, 2493, 1331, 19757, 702, 2895, 311, 2593, 293, 264, 680, 11524, 293, 550, 1228, 309, 281, 862, 702, 960, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1398310661315918, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.10219588875770569}, {"id": 610, "seek": 479400, "start": 4812.0, "end": 4819.0, "text": " This is this is adaptation at a very low rate or very high rate.", "tokens": [51264, 639, 307, 341, 307, 21549, 412, 257, 588, 2295, 3314, 420, 588, 1090, 3314, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1398310661315918, "compression_ratio": 1.4968553459119496, "no_speech_prob": 0.10219588875770569}, {"id": 611, "seek": 481900, "start": 4819.0, "end": 4823.0, "text": " Somebody's got a somebody's got a question you watching the hands up Dennis.", "tokens": [50364, 13463, 311, 658, 257, 2618, 311, 658, 257, 1168, 291, 1976, 264, 2377, 493, 23376, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17991866835628648, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.041432175785303116}, {"id": 612, "seek": 481900, "start": 4823.0, "end": 4826.0, "text": " I'm not watching it at all.", "tokens": [50564, 286, 478, 406, 1976, 309, 412, 439, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17991866835628648, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.041432175785303116}, {"id": 613, "seek": 481900, "start": 4826.0, "end": 4831.0, "text": " When you want to you want to call people and put them up.", "tokens": [50714, 1133, 291, 528, 281, 291, 528, 281, 818, 561, 293, 829, 552, 493, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17991866835628648, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.041432175785303116}, {"id": 614, "seek": 481900, "start": 4831.0, "end": 4834.0, "text": " Yeah, go ahead you got your hand up.", "tokens": [50964, 865, 11, 352, 2286, 291, 658, 428, 1011, 493, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17991866835628648, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.041432175785303116}, {"id": 615, "seek": 481900, "start": 4834.0, "end": 4841.0, "text": " Yeah, I was just, I had a question about the", "tokens": [51114, 865, 11, 286, 390, 445, 11, 286, 632, 257, 1168, 466, 264, 51464], "temperature": 0.0, "avg_logprob": -0.17991866835628648, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.041432175785303116}, {"id": 616, "seek": 481900, "start": 4841.0, "end": 4847.0, "text": " Edges of the models understanding of programming.", "tokens": [51464, 3977, 2880, 295, 264, 5245, 3701, 295, 9410, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17991866835628648, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.041432175785303116}, {"id": 617, "seek": 484700, "start": 4847.0, "end": 4861.0, "text": " Is there any indication or what was your experience in finding the edges of the, what the model knew about a particular API or", "tokens": [50364, 1119, 456, 604, 18877, 420, 437, 390, 428, 1752, 294, 5006, 264, 8819, 295, 264, 11, 437, 264, 2316, 2586, 466, 257, 1729, 9362, 420, 51064], "temperature": 0.0, "avg_logprob": -0.11742475937152731, "compression_ratio": 1.2115384615384615, "no_speech_prob": 0.01589265838265419}, {"id": 618, "seek": 486100, "start": 4861.0, "end": 4870.0, "text": " programming in general. I mean, I was specifically thinking, I use a prototypical inheritance over the newer class inheritance.", "tokens": [50364, 9410, 294, 2674, 13, 286, 914, 11, 286, 390, 4682, 1953, 11, 286, 764, 257, 46219, 34061, 32122, 670, 264, 17628, 1508, 32122, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12185141097667605, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.5995498895645142}, {"id": 619, "seek": 486100, "start": 4870.0, "end": 4878.0, "text": " And I was just kind of curious there's probably a lot less, a lot fewer code examples of that on the, on the internet.", "tokens": [50814, 400, 286, 390, 445, 733, 295, 6369, 456, 311, 1391, 257, 688, 1570, 11, 257, 688, 13366, 3089, 5110, 295, 300, 322, 264, 11, 322, 264, 4705, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12185141097667605, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.5995498895645142}, {"id": 620, "seek": 486100, "start": 4878.0, "end": 4887.0, "text": " So probably less training data for that. I was just curious if you encountered, if you looked at how the model degrades.", "tokens": [51214, 407, 1391, 1570, 3097, 1412, 337, 300, 13, 286, 390, 445, 6369, 498, 291, 20381, 11, 498, 291, 2956, 412, 577, 264, 2316, 368, 22626, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12185141097667605, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.5995498895645142}, {"id": 621, "seek": 488700, "start": 4887.0, "end": 4889.0, "text": " Yeah.", "tokens": [50364, 865, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15421307714361893, "compression_ratio": 1.485, "no_speech_prob": 0.0052976240403950214}, {"id": 622, "seek": 488700, "start": 4889.0, "end": 4897.0, "text": " Well, I didn't really but I did notice something about the, it's coding styles.", "tokens": [50464, 1042, 11, 286, 994, 380, 534, 457, 286, 630, 3449, 746, 466, 264, 11, 309, 311, 17720, 13273, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15421307714361893, "compression_ratio": 1.485, "no_speech_prob": 0.0052976240403950214}, {"id": 623, "seek": 488700, "start": 4897.0, "end": 4908.0, "text": " And it wasn't always consistent like a tiny example but maybe illustrative one is that this problem about, you know, where the JavaScript's trying to", "tokens": [50864, 400, 309, 2067, 380, 1009, 8398, 411, 257, 5870, 1365, 457, 1310, 8490, 30457, 472, 307, 300, 341, 1154, 466, 11, 291, 458, 11, 689, 264, 15778, 311, 1382, 281, 51414], "temperature": 0.0, "avg_logprob": -0.15421307714361893, "compression_ratio": 1.485, "no_speech_prob": 0.0052976240403950214}, {"id": 624, "seek": 488700, "start": 4908.0, "end": 4916.0, "text": " get an element from the down, but the page hasn't loaded yet.", "tokens": [51414, 483, 364, 4478, 490, 264, 760, 11, 457, 264, 3028, 6132, 380, 13210, 1939, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15421307714361893, "compression_ratio": 1.485, "no_speech_prob": 0.0052976240403950214}, {"id": 625, "seek": 491600, "start": 4916.0, "end": 4922.0, "text": " It was using sometimes this unload as a, you know,", "tokens": [50364, 467, 390, 1228, 2171, 341, 32165, 382, 257, 11, 291, 458, 11, 50664], "temperature": 0.0, "avg_logprob": -0.18256019592285155, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0078097153455019}, {"id": 626, "seek": 491600, "start": 4922.0, "end": 4935.0, "text": " and sometimes it was using a listener to on DOM content loaded which is what I've read is the proper better way of doing it that on load is an older way that isn't as good but it's still kind of works.", "tokens": [50664, 293, 2171, 309, 390, 1228, 257, 31569, 281, 322, 35727, 2701, 13210, 597, 307, 437, 286, 600, 1401, 307, 264, 2296, 1101, 636, 295, 884, 309, 300, 322, 3677, 307, 364, 4906, 636, 300, 1943, 380, 382, 665, 457, 309, 311, 920, 733, 295, 1985, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18256019592285155, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0078097153455019}, {"id": 627, "seek": 491600, "start": 4935.0, "end": 4938.0, "text": " So, so there is this kind of.", "tokens": [51314, 407, 11, 370, 456, 307, 341, 733, 295, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18256019592285155, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0078097153455019}, {"id": 628, "seek": 493800, "start": 4938.0, "end": 4951.0, "text": " Two different issues one is whether it's on load or on DOM, DOM content load but also as to whether it's using, it's actually using at event listener, or whether it's just setting the listener directly.", "tokens": [50364, 4453, 819, 2663, 472, 307, 1968, 309, 311, 322, 3677, 420, 322, 35727, 11, 35727, 2701, 3677, 457, 611, 382, 281, 1968, 309, 311, 1228, 11, 309, 311, 767, 1228, 412, 2280, 31569, 11, 420, 1968, 309, 311, 445, 3287, 264, 31569, 3838, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1518041309557463, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.09262602031230927}, {"id": 629, "seek": 493800, "start": 4951.0, "end": 4955.0, "text": " And it kind of wasn't consistent sometimes we'll do one sometimes the other.", "tokens": [51014, 400, 309, 733, 295, 2067, 380, 8398, 2171, 321, 603, 360, 472, 2171, 264, 661, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1518041309557463, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.09262602031230927}, {"id": 630, "seek": 493800, "start": 4955.0, "end": 4964.0, "text": " You know, again, because I guess most of the programs that it looked at were consistent about which way they would deal with that.", "tokens": [51214, 509, 458, 11, 797, 11, 570, 286, 2041, 881, 295, 264, 4268, 300, 309, 2956, 412, 645, 8398, 466, 597, 636, 436, 576, 2028, 365, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1518041309557463, "compression_ratio": 1.680327868852459, "no_speech_prob": 0.09262602031230927}, {"id": 631, "seek": 496400, "start": 4964.0, "end": 4970.0, "text": " It was like a real program.", "tokens": [50364, 467, 390, 411, 257, 957, 1461, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18941423767491392, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.049523357301950455}, {"id": 632, "seek": 496400, "start": 4970.0, "end": 4987.0, "text": " No, but it's just a bit of a real programmer tends to have us one style in which they stick to for those kinds of tasks, but it was I thought it was a bit odd that it was kind of flip flopping about whether it was, you know, which way would deal with that problem.", "tokens": [50664, 883, 11, 457, 309, 311, 445, 257, 857, 295, 257, 957, 32116, 12258, 281, 362, 505, 472, 3758, 294, 597, 436, 2897, 281, 337, 729, 3685, 295, 9608, 11, 457, 309, 390, 286, 1194, 309, 390, 257, 857, 7401, 300, 309, 390, 733, 295, 7929, 25343, 3381, 466, 1968, 309, 390, 11, 291, 458, 11, 597, 636, 576, 2028, 365, 300, 1154, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18941423767491392, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.049523357301950455}, {"id": 633, "seek": 498700, "start": 4987.0, "end": 4993.0, "text": " So real programmers coming back six months later will not be consistent with what.", "tokens": [50364, 407, 957, 41504, 1348, 646, 2309, 2493, 1780, 486, 406, 312, 8398, 365, 437, 13, 50664], "temperature": 0.0, "avg_logprob": -0.4621705506977282, "compression_ratio": 1.0789473684210527, "no_speech_prob": 0.10431800782680511}, {"id": 634, "seek": 499300, "start": 4993.0, "end": 4996.0, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.3917191715563758, "compression_ratio": 1.5286624203821657, "no_speech_prob": 0.03609520569443703}, {"id": 635, "seek": 499300, "start": 4996.0, "end": 4999.0, "text": " Very cool things that can do.", "tokens": [50514, 4372, 1627, 721, 300, 393, 360, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3917191715563758, "compression_ratio": 1.5286624203821657, "no_speech_prob": 0.03609520569443703}, {"id": 636, "seek": 499300, "start": 4999.0, "end": 5003.0, "text": " It's a program and you mentioned the prompt engineering.", "tokens": [50664, 467, 311, 257, 1461, 293, 291, 2835, 264, 12391, 7043, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3917191715563758, "compression_ratio": 1.5286624203821657, "no_speech_prob": 0.03609520569443703}, {"id": 637, "seek": 499300, "start": 5003.0, "end": 5013.0, "text": " Before, so that it may not require any more prompt engineering almost a row. So that is recognized usable and you also mentioned role playing.", "tokens": [50864, 4546, 11, 370, 300, 309, 815, 406, 3651, 604, 544, 12391, 7043, 1920, 257, 5386, 13, 407, 300, 307, 9823, 29975, 293, 291, 611, 2835, 3090, 2433, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3917191715563758, "compression_ratio": 1.5286624203821657, "no_speech_prob": 0.03609520569443703}, {"id": 638, "seek": 501300, "start": 5013.0, "end": 5021.0, "text": " And one of the techniques. So now, there are more developing stress on techniques rather than engineering of prompt itself.", "tokens": [50364, 400, 472, 295, 264, 7512, 13, 407, 586, 11, 456, 366, 544, 6416, 4244, 322, 7512, 2831, 813, 7043, 295, 12391, 2564, 13, 50764], "temperature": 0.0, "avg_logprob": -0.28309794395200666, "compression_ratio": 1.59375, "no_speech_prob": 0.12355747073888779}, {"id": 639, "seek": 501300, "start": 5021.0, "end": 5035.0, "text": " But maybe from your experience of this project, what these techniques would you found helpful. Maybe you found something of your own that helped you to navigate the chat activity for", "tokens": [50764, 583, 1310, 490, 428, 1752, 295, 341, 1716, 11, 437, 613, 7512, 576, 291, 1352, 4961, 13, 2704, 291, 1352, 746, 295, 428, 1065, 300, 4254, 291, 281, 12350, 264, 5081, 5191, 337, 51464], "temperature": 0.0, "avg_logprob": -0.28309794395200666, "compression_ratio": 1.59375, "no_speech_prob": 0.12355747073888779}, {"id": 640, "seek": 503500, "start": 5035.0, "end": 5056.0, "text": " Well, the problem a little bit was that I was always really the underlying motivation for a lot of this was thinking about, you know, what kind of learning materials or experience what I imagine like a high school student having.", "tokens": [50364, 1042, 11, 264, 1154, 257, 707, 857, 390, 300, 286, 390, 1009, 534, 264, 14217, 12335, 337, 257, 688, 295, 341, 390, 1953, 466, 11, 291, 458, 11, 437, 733, 295, 2539, 5319, 420, 1752, 437, 286, 3811, 411, 257, 1090, 1395, 3107, 1419, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14581850596836635, "compression_ratio": 1.4585987261146496, "no_speech_prob": 0.0185249000787735}, {"id": 641, "seek": 505600, "start": 5056.0, "end": 5070.0, "text": " So I was rarely kind of role playing myself I was often, you know, pretending in my head, you know, to be maybe a high school student that knew a little bit about this and not much about that or something.", "tokens": [50364, 407, 286, 390, 13752, 733, 295, 3090, 2433, 2059, 286, 390, 2049, 11, 291, 458, 11, 22106, 294, 452, 1378, 11, 291, 458, 11, 281, 312, 1310, 257, 1090, 1395, 3107, 300, 2586, 257, 707, 857, 466, 341, 293, 406, 709, 466, 300, 420, 746, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10290012536225496, "compression_ratio": 1.4413793103448276, "no_speech_prob": 0.01341327652335167}, {"id": 642, "seek": 505600, "start": 5070.0, "end": 5073.0, "text": " So,", "tokens": [51064, 407, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10290012536225496, "compression_ratio": 1.4413793103448276, "no_speech_prob": 0.01341327652335167}, {"id": 643, "seek": 507300, "start": 5074.0, "end": 5085.0, "text": " maybe I'll try this soon, which is, especially if it's a younger child. I think if you set up the context better and I say, you know,", "tokens": [50414, 1310, 286, 603, 853, 341, 2321, 11, 597, 307, 11, 2318, 498, 309, 311, 257, 7037, 1440, 13, 286, 519, 498, 291, 992, 493, 264, 4319, 1101, 293, 286, 584, 11, 291, 458, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1404167601936742, "compression_ratio": 1.2314814814814814, "no_speech_prob": 0.053335558623075485}, {"id": 644, "seek": 508500, "start": 5086.0, "end": 5100.0, "text": " you're going to be helping a nine year old who doesn't know much about programming to build the apps that they want, you know, be sure to explain things carefully and not use too much technical jar.", "tokens": [50414, 291, 434, 516, 281, 312, 4315, 257, 4949, 1064, 1331, 567, 1177, 380, 458, 709, 466, 9410, 281, 1322, 264, 7733, 300, 436, 528, 11, 291, 458, 11, 312, 988, 281, 2903, 721, 7500, 293, 406, 764, 886, 709, 6191, 15181, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11590290069580078, "compression_ratio": 1.4244604316546763, "no_speech_prob": 0.07908206433057785}, {"id": 645, "seek": 510000, "start": 5100.0, "end": 5115.0, "text": " And he's set up set up a context like that, that it would tend to, I believe it but I even tried this that it would tend to use language much more appropriate to communicate with a nine year old because it does.", "tokens": [50364, 400, 415, 311, 992, 493, 992, 493, 257, 4319, 411, 300, 11, 300, 309, 576, 3928, 281, 11, 286, 1697, 309, 457, 286, 754, 3031, 341, 300, 309, 576, 3928, 281, 764, 2856, 709, 544, 6854, 281, 7890, 365, 257, 4949, 1064, 1331, 570, 309, 775, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22458936653885186, "compression_ratio": 1.4859154929577465, "no_speech_prob": 0.1274998039007187}, {"id": 646, "seek": 511500, "start": 5115.0, "end": 5133.0, "text": " There's two examples of where I said, after built an app I said, what could you explain to a nine year old, or it's young child how this app would work it. I thought I did a great job of explaining it in, in, you know, the right kind of vocabulary and right kind of analogies.", "tokens": [50364, 821, 311, 732, 5110, 295, 689, 286, 848, 11, 934, 3094, 364, 724, 286, 848, 11, 437, 727, 291, 2903, 281, 257, 4949, 1064, 1331, 11, 420, 309, 311, 2037, 1440, 577, 341, 724, 576, 589, 309, 13, 286, 1194, 286, 630, 257, 869, 1691, 295, 13468, 309, 294, 11, 294, 11, 291, 458, 11, 264, 558, 733, 295, 19864, 293, 558, 733, 295, 16660, 530, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1435021010922714, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.020944755524396896}, {"id": 647, "seek": 513300, "start": 5134.0, "end": 5150.0, "text": " But I should really come up with a task that actually I want to do and just stop pretending to be a student and just see, you know, what I could accomplish as well.", "tokens": [50414, 583, 286, 820, 534, 808, 493, 365, 257, 5633, 300, 767, 286, 528, 281, 360, 293, 445, 1590, 22106, 281, 312, 257, 3107, 293, 445, 536, 11, 291, 458, 11, 437, 286, 727, 9021, 382, 731, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1279962469891804, "compression_ratio": 1.3442622950819672, "no_speech_prob": 0.04664197191596031}, {"id": 648, "seek": 515000, "start": 5151.0, "end": 5166.0, "text": " And I'm doing it this way is I've always been interested in kids and programming, but also there's, there's an awful lot of real programmers that are using chat GPT to get accomplished their day to day tasks so I thought this would be a more", "tokens": [50414, 400, 286, 478, 884, 309, 341, 636, 307, 286, 600, 1009, 668, 3102, 294, 2301, 293, 9410, 11, 457, 611, 456, 311, 11, 456, 311, 364, 11232, 688, 295, 957, 41504, 300, 366, 1228, 5081, 26039, 51, 281, 483, 15419, 641, 786, 281, 786, 9608, 370, 286, 1194, 341, 576, 312, 257, 544, 51164], "temperature": 0.0, "avg_logprob": -0.19479048879523025, "compression_ratio": 1.478527607361963, "no_speech_prob": 0.28734126687049866}, {"id": 649, "seek": 516600, "start": 5167.0, "end": 5182.0, "text": " experience to think in terms of, of how children might deal with this. And when I say children of course it could be an adult that knows no programming a hobbyist or something but I mean, a non expert programmer.", "tokens": [50414, 1752, 281, 519, 294, 2115, 295, 11, 295, 577, 2227, 1062, 2028, 365, 341, 13, 400, 562, 286, 584, 2227, 295, 1164, 309, 727, 312, 364, 5075, 300, 3255, 572, 9410, 257, 18240, 468, 420, 746, 457, 286, 914, 11, 257, 2107, 5844, 32116, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1833913180292869, "compression_ratio": 1.4228187919463087, "no_speech_prob": 0.05479368194937706}, {"id": 650, "seek": 518200, "start": 5182.0, "end": 5201.0, "text": " That's very interesting. Thank you. It's also recently I listened to podcast is Lee chasen, which was an educator who also worked now on the content strategies, and he used role playing in the beginning like you did with the child to actually create a content for the", "tokens": [50364, 663, 311, 588, 1880, 13, 1044, 291, 13, 467, 311, 611, 3938, 286, 13207, 281, 7367, 307, 6957, 417, 296, 268, 11, 597, 390, 364, 31237, 567, 611, 2732, 586, 322, 264, 2701, 9029, 11, 293, 415, 1143, 3090, 2433, 294, 264, 2863, 411, 291, 630, 365, 264, 1440, 281, 767, 1884, 257, 2701, 337, 264, 51314], "temperature": 0.0, "avg_logprob": -0.23770891825358073, "compression_ratio": 1.5, "no_speech_prob": 0.09199374169111252}, {"id": 651, "seek": 520100, "start": 5201.0, "end": 5217.0, "text": " So in terms of how they can study the material because he was a teacher before, I think, teaching history, and uses this prompts like overall playing to actually create the tasks that the children would understand or certain", "tokens": [50364, 407, 294, 2115, 295, 577, 436, 393, 2979, 264, 2527, 570, 415, 390, 257, 5027, 949, 11, 286, 519, 11, 4571, 2503, 11, 293, 4960, 341, 41095, 411, 4787, 2433, 281, 767, 1884, 264, 9608, 300, 264, 2227, 576, 1223, 420, 1629, 51164], "temperature": 0.0, "avg_logprob": -0.20059417641681174, "compression_ratio": 1.4834437086092715, "no_speech_prob": 0.018169548362493515}, {"id": 652, "seek": 521700, "start": 5218.0, "end": 5228.0, "text": " output that would be better suited for the audience so levels of understanding so I think what you also showed here is how well it works.", "tokens": [50414, 5598, 300, 576, 312, 1101, 24736, 337, 264, 4034, 370, 4358, 295, 3701, 370, 286, 519, 437, 291, 611, 4712, 510, 307, 577, 731, 309, 1985, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19452726046244304, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.06102759391069412}, {"id": 653, "seek": 521700, "start": 5228.0, "end": 5241.0, "text": " In terms of this first example that you show so that it really stands you nicely as if you were indeed a younger adult or child so", "tokens": [50914, 682, 2115, 295, 341, 700, 1365, 300, 291, 855, 370, 300, 309, 534, 7382, 291, 9594, 382, 498, 291, 645, 6451, 257, 7037, 5075, 420, 1440, 370, 51564], "temperature": 0.0, "avg_logprob": -0.19452726046244304, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.06102759391069412}, {"id": 654, "seek": 524100, "start": 5242.0, "end": 5256.0, "text": " my question may be also additionally meant to ask what other techniques you found helpful like maybe not only role playing, but something else maybe some structuring or given examples.", "tokens": [50414, 452, 1168, 815, 312, 611, 43181, 4140, 281, 1029, 437, 661, 7512, 291, 1352, 4961, 411, 1310, 406, 787, 3090, 2433, 11, 457, 746, 1646, 1310, 512, 6594, 1345, 420, 2212, 5110, 13, 51114], "temperature": 0.0, "avg_logprob": -0.33226337948360957, "compression_ratio": 1.4153846153846155, "no_speech_prob": 0.0949287861585617}, {"id": 655, "seek": 525600, "start": 5257.0, "end": 5273.0, "text": " Yeah, so part of the problem is that at the same time I was doing this I was also curious about, could it really understand me if I just say, you know,", "tokens": [50414, 865, 11, 370, 644, 295, 264, 1154, 307, 300, 412, 264, 912, 565, 286, 390, 884, 341, 286, 390, 611, 6369, 466, 11, 727, 309, 534, 1223, 385, 498, 286, 445, 584, 11, 291, 458, 11, 51214], "temperature": 0.0, "avg_logprob": -0.16478495597839354, "compression_ratio": 1.2905982905982907, "no_speech_prob": 0.3320412337779999}, {"id": 656, "seek": 527300, "start": 5273.0, "end": 5290.0, "text": " maybe, etc, and I don't really list everything out or if I say, you know, that was it didn't go high enough without giving much detail so I was kind of really wanting to explore how how well I could do if I was", "tokens": [50364, 1310, 11, 5183, 11, 293, 286, 500, 380, 534, 1329, 1203, 484, 420, 498, 286, 584, 11, 291, 458, 11, 300, 390, 309, 994, 380, 352, 1090, 1547, 1553, 2902, 709, 2607, 370, 286, 390, 733, 295, 534, 7935, 281, 6839, 577, 577, 731, 286, 727, 360, 498, 286, 390, 51214], "temperature": 0.0, "avg_logprob": -0.13407208546098456, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.27763617038726807}, {"id": 657, "seek": 527300, "start": 5290.0, "end": 5302.0, "text": " not being very careful to give it detailed instructions or large amount of context just to see, you know, could it still respond appropriately.", "tokens": [51214, 406, 885, 588, 5026, 281, 976, 309, 9942, 9415, 420, 2416, 2372, 295, 4319, 445, 281, 536, 11, 291, 458, 11, 727, 309, 920, 4196, 23505, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13407208546098456, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.27763617038726807}, {"id": 658, "seek": 530200, "start": 5302.0, "end": 5324.0, "text": " One thing I should say too is that, you know, the Khan Academy has been working with chat gpt to make a tutor that they gave a demo of teaching algebra, and it was really very very good at giving hints and giving feedback when the", "tokens": [50364, 1485, 551, 286, 820, 584, 886, 307, 300, 11, 291, 458, 11, 264, 18136, 11735, 575, 668, 1364, 365, 5081, 290, 662, 281, 652, 257, 35613, 300, 436, 2729, 257, 10723, 295, 4571, 21989, 11, 293, 309, 390, 534, 588, 588, 665, 412, 2902, 27271, 293, 2902, 5824, 562, 264, 51464], "temperature": 0.0, "avg_logprob": -0.1775203914177127, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.03508692607283592}, {"id": 659, "seek": 530200, "start": 5324.0, "end": 5331.0, "text": " student did things wrong, but the student said well what's the answer what's how do I solve this it'll keep saying.", "tokens": [51464, 3107, 630, 721, 2085, 11, 457, 264, 3107, 848, 731, 437, 311, 264, 1867, 437, 311, 577, 360, 286, 5039, 341, 309, 603, 1066, 1566, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1775203914177127, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.03508692607283592}, {"id": 660, "seek": 533100, "start": 5331.0, "end": 5343.0, "text": " I'm not going to give you the answer but here's a hint or something so it was they had set up the context appropriately so it would not.", "tokens": [50364, 286, 478, 406, 516, 281, 976, 291, 264, 1867, 457, 510, 311, 257, 12075, 420, 746, 370, 309, 390, 436, 632, 992, 493, 264, 4319, 23505, 370, 309, 576, 406, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10658364977155413, "compression_ratio": 1.2830188679245282, "no_speech_prob": 0.012811345048248768}, {"id": 661, "seek": 534300, "start": 5343.0, "end": 5356.0, "text": " You know, it would be a good pedagogical tutor and not be just a chat bot that would just give you answers when asked when asked for it but in my my case I thought.", "tokens": [50364, 509, 458, 11, 309, 576, 312, 257, 665, 5670, 31599, 804, 35613, 293, 406, 312, 445, 257, 5081, 10592, 300, 576, 445, 976, 291, 6338, 562, 2351, 562, 2351, 337, 309, 457, 294, 452, 452, 1389, 286, 1194, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16889253343854632, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.21714386343955994}, {"id": 662, "seek": 534300, "start": 5356.0, "end": 5365.0, "text": " I thought that it was, I could have maybe tried to early on say something like well you're giving me too much.", "tokens": [51014, 286, 1194, 300, 309, 390, 11, 286, 727, 362, 1310, 3031, 281, 2440, 322, 584, 746, 411, 731, 291, 434, 2902, 385, 886, 709, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16889253343854632, "compression_ratio": 1.5449438202247192, "no_speech_prob": 0.21714386343955994}, {"id": 663, "seek": 536500, "start": 5365.0, "end": 5379.0, "text": " Code you know I want to be able to do more of it myself or something I could have done that and it would have taken me longer but it would have given me more hints and more little pieces to put together, but.", "tokens": [50364, 15549, 291, 458, 286, 528, 281, 312, 1075, 281, 360, 544, 295, 309, 2059, 420, 746, 286, 727, 362, 1096, 300, 293, 309, 576, 362, 2726, 385, 2854, 457, 309, 576, 362, 2212, 385, 544, 27271, 293, 544, 707, 3755, 281, 829, 1214, 11, 457, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10345710754394531, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.09658253937959671}, {"id": 664, "seek": 537900, "start": 5379.0, "end": 5395.0, "text": " It was the way I did it I think was plausible for some students that you know would be happy with these pretty high level things and some copying pasting but also some creating things on their own.", "tokens": [50364, 467, 390, 264, 636, 286, 630, 309, 286, 519, 390, 39925, 337, 512, 1731, 300, 291, 458, 576, 312, 2055, 365, 613, 1238, 1090, 1496, 721, 293, 512, 27976, 1791, 278, 457, 611, 512, 4084, 721, 322, 641, 1065, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11917716806585138, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.04877617582678795}, {"id": 665, "seek": 537900, "start": 5395.0, "end": 5399.0, "text": " Thank you.", "tokens": [51164, 1044, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11917716806585138, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.04877617582678795}, {"id": 666, "seek": 537900, "start": 5399.0, "end": 5404.0, "text": " Any more questions.", "tokens": [51364, 2639, 544, 1651, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11917716806585138, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.04877617582678795}, {"id": 667, "seek": 540400, "start": 5404.0, "end": 5407.0, "text": " And thank you very, very much.", "tokens": [50364, 400, 1309, 291, 588, 11, 588, 709, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16669873396555582, "compression_ratio": 1.2095238095238094, "no_speech_prob": 0.019664539024233818}, {"id": 668, "seek": 540400, "start": 5407.0, "end": 5419.0, "text": " I love being educated in this in this area, and I qualify as a barely marginal student, I guess.", "tokens": [50514, 286, 959, 885, 15872, 294, 341, 294, 341, 1859, 11, 293, 286, 20276, 382, 257, 10268, 16885, 3107, 11, 286, 2041, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16669873396555582, "compression_ratio": 1.2095238095238094, "no_speech_prob": 0.019664539024233818}, {"id": 669, "seek": 541900, "start": 5419.0, "end": 5428.0, "text": " And what happens is I think that we see examples of reverse aging that the young kids are much more successful than the older people.", "tokens": [50364, 400, 437, 2314, 307, 286, 519, 300, 321, 536, 5110, 295, 9943, 19090, 300, 264, 2037, 2301, 366, 709, 544, 4406, 813, 264, 4906, 561, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14591957439075817, "compression_ratio": 1.4064516129032258, "no_speech_prob": 0.159376859664917}, {"id": 670, "seek": 541900, "start": 5428.0, "end": 5434.0, "text": " And, you know, I'd like to claim that I know how to do things, but oftentimes don't.", "tokens": [50814, 400, 11, 291, 458, 11, 286, 1116, 411, 281, 3932, 300, 286, 458, 577, 281, 360, 721, 11, 457, 18349, 500, 380, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14591957439075817, "compression_ratio": 1.4064516129032258, "no_speech_prob": 0.159376859664917}, {"id": 671, "seek": 543400, "start": 5434.0, "end": 5437.0, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 50514], "temperature": 0.0, "avg_logprob": -0.3992537260055542, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.5501560568809509}], "language": "en"}