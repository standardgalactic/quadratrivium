WEBVTT

00:00.000 --> 00:15.240
So this work is part of a larger thread of work centered around algorithmic governance.

00:15.240 --> 00:22.400
That is, how do we audit and measure problematic content like mis-disinformation, online extremism,

00:22.400 --> 00:26.920
and how do we do that on online platforms, and then specifically the search and recommendation

00:26.920 --> 00:29.720
algorithms driving these platforms.

00:29.720 --> 00:34.080
So as I was preparing for this talk, I thought of opening it up in a slightly unconventional

00:34.080 --> 00:39.440
way with this particular quote, which I'm going to read a little bit, which says, mankind

00:39.440 --> 00:44.720
barely noticed when the concept of massively organized information quietly emerged to become

00:44.720 --> 00:50.480
a means of social control, a weapon of war, and a roadmap for group destruction.

00:50.480 --> 00:54.720
Any guesses as to which year of the world this quote describes?

00:54.720 --> 01:03.640
Is it the world, is it a current world, 2020s, 2000s, 1900s?

01:03.640 --> 01:07.960
Some people could guess just by the word mankind in previous talk.

01:07.960 --> 01:13.160
Yeah, that's absolutely accurate.

01:13.160 --> 01:18.440
So this is actually Edwin Black's description of the world of the 1930s, 1940s, during the

01:18.440 --> 01:23.160
World War II, and the invention of the punch card, which was at that time the new wave

01:23.160 --> 01:26.360
of automation and data collection.

01:26.360 --> 01:31.040
And what is more surprising is that this quote is still true in today's 21st century world,

01:31.040 --> 01:36.640
and we have seen many new scholarships, and I'm sure folks here have read and are familiar

01:36.640 --> 01:41.640
with these books, many of these outline the harms of automation and data collection.

01:41.640 --> 01:46.320
And more recently, this discussion has shifted to the harms posed by generative AI and large

01:46.320 --> 01:47.320
language models.

01:47.320 --> 01:51.360
So I'm going to read this quote a little bit more of what Edwin Black had to say in his

01:51.360 --> 01:54.680
book, which is a lot of parallels to a present-day world.

01:54.680 --> 01:59.080
So he goes on to say, the unique igniting event was the most fateful day of the last

01:59.080 --> 02:04.760
century when Adolf Hitler came to power, but for the first time in history, an anti-Semayet

02:04.760 --> 02:08.960
had automation on his side, so that was what was different, and the automation was in the

02:08.960 --> 02:11.720
form of IBM punch cards.

02:11.720 --> 02:16.960
So he goes on to say that IBM was self-criped by a special immoral corporate mantra, if

02:16.960 --> 02:19.680
it can be done, it should be done.

02:19.680 --> 02:23.720
So we should take a pause and kind of re-read this phrase, if it can be done, it should

02:23.720 --> 02:30.200
be done, which is parallels very re-resemblance to the motives and culture practiced in modern-day

02:30.200 --> 02:35.560
technology companies, and I'm sure being in the Silicon Valley, you all probably have

02:35.560 --> 02:40.000
seen some of this while interning at some of these companies, move fast and break things,

02:40.000 --> 02:44.480
done is better than perfect, what would you do if you weren't afraid.

02:44.480 --> 02:48.800
And I argue that the sort of rushed culture, the sort of culture of disruption and speed

02:48.800 --> 02:53.760
often poses a great challenges in governing these technologies and conducting thoughtful

02:53.760 --> 02:59.480
medicalist audits, and in turn this is what makes algorithmic governance a difficult problem.

02:59.480 --> 03:03.400
And I'll come back to these ideas later on towards the end of the talk, but let me first

03:03.400 --> 03:09.360
dive into the meat of this presentation, a couple of studies which we did in this realm

03:09.360 --> 03:11.040
of algorithmic governance.

03:11.040 --> 03:16.560
So here I want to focus on two types of algorithms, search and recommendations, and two specific

03:16.920 --> 03:20.080
platforms, YouTube and Amazon.

03:20.080 --> 03:26.040
And our focus was only one type of problem, misinformation particularly.

03:26.040 --> 03:30.520
Now you might wonder why search and recommendation algorithms, so now the world is moving towards

03:30.520 --> 03:36.040
large language models, why we should focus on these old technologies, search and recommendations.

03:36.040 --> 03:40.680
So users generally have an unwavering trust in search engines, so several scholarly work,

03:40.680 --> 03:45.560
some of these cited here have actually shown that these ranking of search results have

03:46.040 --> 03:50.440
really dramatic effect on users' attitudes, their preferences, their behaviors.

03:50.440 --> 03:55.160
In fact, bias search rankings are so powerful they can even shift voting preferences of

03:55.160 --> 03:59.080
undecided waters by so much so as 20%.

03:59.080 --> 04:03.440
And users don't even show awareness of that kind of manipulation happening in their search

04:03.440 --> 04:04.440
results.

04:04.440 --> 04:09.120
So this should tell you how powerful search algorithms are, and then that is why we should

04:09.120 --> 04:13.640
keep studying them despite the other new shiny technologies coming our way.

04:13.720 --> 04:17.560
For recommendation algorithms, I hope I don't have to make a case here because you see those

04:17.560 --> 04:22.960
almost everywhere starting from what recommendations, what movies you should watch, what products

04:22.960 --> 04:27.080
one should buy, which campaign you should donate to, they are almost everywhere.

04:27.080 --> 04:29.000
And they also have these reinforcing effect, right?

04:29.000 --> 04:34.240
So the more you like, watch certain content, the more you share, the more you get those

04:34.240 --> 04:36.440
types of content.

04:36.440 --> 04:41.160
Now this might be harmless say when you're going out this weekend and trying to buy Christmas

04:41.200 --> 04:46.480
decorations, but these things can get ugly quite quickly when say you are browsing for

04:46.480 --> 04:52.040
vaccine information, health information or climate information.

04:52.040 --> 04:54.720
Now why audit for misinformation in particular, right?

04:54.720 --> 05:00.440
So currently there is this disproportionate focus on AI bias and fairness and tech journalist

05:00.440 --> 05:05.560
Karen Howe captures this notion very well in her article where she mentions how often

05:05.560 --> 05:10.760
responsible AI teams and companies are pigeonholed into targeting AI bias.

05:10.760 --> 05:14.360
Now don't get me wrong, so bias and fairness are indeed important topics that one should

05:14.360 --> 05:20.080
pursue, but tackling just AI bias draws away attention from fixing much bigger problems

05:20.080 --> 05:25.400
of other types of harmful information such as misinformation, extremism, conspiratorial

05:25.400 --> 05:26.480
content.

05:26.480 --> 05:31.000
So this is what is underlying motivation behind the set of studies that we did.

05:31.000 --> 05:36.840
And with that, let's dive into this first study, the YouTube audit work.

05:37.120 --> 05:41.680
A major motivation for this YouTube study was coming from these frequent headlines that

05:41.680 --> 05:46.440
I was noticing a few years ago, how YouTube is driving people in the internet's darkest

05:46.440 --> 05:51.240
corners, and then there were these opinion pieces talking about YouTube being the great

05:51.240 --> 05:53.640
radicalizer.

05:53.640 --> 05:58.240
But YouTube was also responding with articles saying that they will be reducing conspiracy

05:58.240 --> 06:03.400
theory recommendations or making it much harder to find those on the platform.

06:03.400 --> 06:08.640
And all these questions and all these reports are really anecdotal, like empirically how

06:08.640 --> 06:09.920
bad is this, right?

06:09.920 --> 06:12.200
Do we really know that?

06:12.200 --> 06:17.360
And this is what I studied to try to do, verify these anecdotal claims that does YouTube really

06:17.360 --> 06:20.160
surface these problematic content.

06:20.160 --> 06:24.840
And in order to do this, we conducted these systematic audits on YouTube search and recommendation

06:24.840 --> 06:29.480
algorithms and we picked one type of problematic content, conspiracy theories.

06:29.480 --> 06:32.600
Now what really is an audit, right?

06:32.600 --> 06:37.720
I have talked about mentioned audit a couple of times so far, but how do you audit algorithms?

06:37.720 --> 06:42.480
I'm sure some of you in the audience might be familiar with the concept of audit.

06:42.480 --> 06:46.040
For those of you who don't know, I'm going to give you a quick definition through an

06:46.040 --> 06:50.920
example and also say that this is a thriving field of research, lots of work has been done

06:50.920 --> 06:52.800
in this space.

06:52.800 --> 06:56.960
And this was one of the earliest example of audits coming from the social science world.

06:56.960 --> 07:02.000
And it's also one of my favorite ones from this 2004 paper, where researchers conducted

07:02.000 --> 07:07.120
this very clever field experiment to investigate employment discrimination.

07:07.120 --> 07:11.720
That is, they audited the labor market for racial discrimination.

07:11.720 --> 07:16.720
So what they did was they responded with fictitious resumes to help wanted ads in the Boston

07:16.720 --> 07:18.960
and Chicago newspapers.

07:18.960 --> 07:23.280
To manipulate the perception of race, what they did was they kept everything else in

07:23.280 --> 07:26.800
each of these resumes constant, only they changed the names.

07:26.800 --> 07:31.960
And the names were either very African American sounding names such as Laquisha or Jamal or

07:31.960 --> 07:37.080
very white sounding names such as Emily or Greg, and hence the name of this paper.

07:37.080 --> 07:41.160
The results showed that there was significant discrimination against African American names

07:41.160 --> 07:48.040
while white names received 50% more callbacks for interviews compared to the African American

07:48.040 --> 07:51.800
names despite everything else being constant in those resumes.

07:51.800 --> 07:55.640
So this is a core idea behind audits, that is, you would keep everything else constant

07:55.640 --> 07:59.880
and then you manipulate a single variable to determine how that change would affect

07:59.880 --> 08:01.520
the algorithm.

08:01.520 --> 08:06.560
So if you translate this into the context of YouTube, you would manipulate a variable

08:06.560 --> 08:11.520
to determine whether the search and recommendation algorithm returns different results, say when

08:11.520 --> 08:17.680
someone's age, their other demographic attributes, gender, their watch history, where they are

08:17.680 --> 08:24.520
searching from, the geolocation, if that differs, what happens with the research results.

08:24.520 --> 08:28.000
So to answer this, we set up this elaborate audit framework, which broadly looked like

08:28.000 --> 08:31.800
this, where we had programmed bots, or in other words, we were conducting these sock

08:31.800 --> 08:32.800
puppet audits.

08:32.800 --> 08:37.800
So these were bots or sock puppets, which behave like normal users logging into YouTube,

08:37.800 --> 08:42.200
running queries on the search platform while at the same time, a script at the back end

08:42.200 --> 08:48.200
were collecting whatever search and recommendation results the platform was returning.

08:48.200 --> 08:52.080
So we started with selecting search topics, and a goal here was to make sure that the

08:52.080 --> 08:56.280
topics are indeed high impact, that is, lots of people are searching for these topics,

08:56.280 --> 09:00.320
so they should be popular, and they should be topics which also have the potential to

09:00.320 --> 09:02.880
return false conspiracies.

09:02.880 --> 09:07.600
So we did some more background work, referring to Wikipedia, comparing with Google Trends,

09:07.600 --> 09:13.920
and we came up with this list of five different topics, 9-11 conspiracies, vaccine controversies,

09:13.920 --> 09:18.400
moon landing conspiracies, chemtrail, and then flat earth.

09:18.400 --> 09:23.240
And then we audited three components of YouTube, the up next video, the top five recommendations,

09:23.240 --> 09:27.520
and also YouTube's search results.

09:27.520 --> 09:32.000
For demographics, we checked for four different age groups and two different types of gender,

09:32.000 --> 09:37.840
male and female, and to emulate this, we had to create eight different sock puppet combination

09:37.840 --> 09:39.080
accounts.

09:39.080 --> 09:44.960
And then for geolocation, we found this hot and cold regions, that is, we call these

09:44.960 --> 09:48.480
hot and cold regions because these are the regions which have the highest or the lowest

09:48.480 --> 09:50.720
interest for that particular topic.

09:50.720 --> 09:56.680
And we found these hot and cold regions comparing with Google Trends' interest over time graph.

09:56.680 --> 09:58.840
So this is how it looked like for all the topics.

09:58.840 --> 10:04.480
So for example, if you pick the flatter theories, Montana was a hot region or a high interest

10:04.480 --> 10:09.320
showing region, while New Jersey is a low interest or cold region.

10:09.320 --> 10:14.480
So once we had all these parameters, the demographic geolocation and all these parameters, we essentially

10:14.480 --> 10:20.520
created bot accounts or sock puppets and programmed these accounts to keep firing queries on YouTube.

10:20.520 --> 10:26.240
For geolocation, these bots fired queries from IP addresses of these locations.

10:26.240 --> 10:30.520
Now one very important thing that we had to do for running these audits is, throughout

10:30.520 --> 10:33.840
these audit experiments, you would have to control for noise to ensure that the effect

10:33.840 --> 10:39.320
that you're observing is actually from the algorithm is not because of the noise that

10:39.320 --> 10:42.760
might have been introduced while running the experiment.

10:42.760 --> 10:49.480
So we controlled for browser noise by selecting one single version of Firefox browser.

10:49.480 --> 10:54.240
We made sure that the YouTube searches are happening simultaneously to control for temporal

10:54.240 --> 10:57.320
effects and so on.

10:57.320 --> 11:05.400
So all of that audit run resulted in about 56,000, more than 56,000 videos capturing about

11:05.400 --> 11:08.080
3,000 unique videos.

11:08.080 --> 11:13.400
And then was the hard part, right, the manual annotations for this data set.

11:13.400 --> 11:17.480
And I can go into detail of why we went with manual annotations and how we annotated if

11:17.480 --> 11:22.120
anyone is interested in the Q&A round, but essentially this resulted in kind of three

11:22.120 --> 11:25.520
sets of annotations promoting neutral and debunking.

11:25.520 --> 11:29.800
And there was a lot of thought process that went into these annotation scheme, why these

11:29.800 --> 11:34.240
three class made sense for this purpose and so on.

11:34.240 --> 11:39.240
And then we performed statistical comparison tests to essentially find out what's the result

11:39.240 --> 11:40.800
of these audits.

11:40.800 --> 11:43.000
So let's look at some of these.

11:43.000 --> 11:44.000
So what did we find?

11:44.160 --> 11:49.680
We found that for brand new accounts, demography and geolocation do not really have an effect

11:49.680 --> 11:54.640
on the amount of misinformation or the type of conspiracy theories that these platforms

11:54.640 --> 11:59.440
are or YouTube is returning.

11:59.440 --> 12:00.440
This is encouraging.

12:00.440 --> 12:02.160
This is what we want the platform to do, right?

12:02.160 --> 12:07.640
So it tells us that unlike those reports which were blaming YouTube for returning conspiracy

12:07.640 --> 12:12.480
theories when it's a brand new account, turns out the demography and geolocation do not

12:12.480 --> 12:14.480
really have an effect.

12:14.480 --> 12:21.200
But once accounts builds a history by watching both demography and geolocation starts exerting

12:21.200 --> 12:29.120
an effect on the recommendation for certain combination of topics, stances and component.

12:29.120 --> 12:31.000
You might be thinking, okay, this is expected, right?

12:31.000 --> 12:35.720
This is what we, how we think the platform would behave.

12:35.720 --> 12:40.080
But turns out there are a little bit more nuanced results when we dig deeper into these,

12:40.080 --> 12:41.920
the actual audit outcomes.

12:41.920 --> 12:47.680
So for example, for the 9-11 topic, if the sock puppets watched YouTube videos promoting

12:47.680 --> 12:53.200
line of conspiracy, you would end up getting more of these promoting videos in the recommendations.

12:53.200 --> 12:57.560
But if the topic is something different, so for surprisingly for vaccine topic, the effect

12:57.560 --> 12:59.040
was completely opposite.

12:59.040 --> 13:03.400
If you watch anti-vaccine videos, YouTube ended up recommending you debunking videos

13:03.400 --> 13:07.040
in the up next and top five recommendations.

13:07.040 --> 13:12.840
So this, at least from these observations, it tells us that YouTube in some way is handling

13:12.840 --> 13:15.440
misinformation in a much more reactive way.

13:15.440 --> 13:19.800
It's modifying its search and recommendation algorithm selectively based on what reactions

13:19.800 --> 13:22.320
is getting from the media and technology critics.

13:22.320 --> 13:26.880
So we know that there was a lot of pushback for vaccine-related misinformation, and it

13:26.880 --> 13:30.800
appears that they have gone and fixed that, but they have not done that universally for

13:30.800 --> 13:34.320
other problematic topics.

13:34.320 --> 13:38.440
We also found that certain demographics were prone to conspiracy video recommendations.

13:38.440 --> 13:44.400
So for example, among eight of those demographic cases in all but one case, men accounts, that

13:44.400 --> 13:49.040
is, bought accounts who had gender set as male were recommended more misinformation

13:49.040 --> 13:50.640
videos.

13:50.640 --> 13:54.920
And perhaps more surprisingly, what we found that in four of these cases, men accounts

13:54.920 --> 14:00.440
who actually ended up watching neutral videos got significantly higher misinformation video

14:00.440 --> 14:02.080
recommendations.

14:02.080 --> 14:03.720
Now this is really problematic.

14:03.720 --> 14:09.000
It implies that the algorithm was actually recommending pro-conspiracy videos even when

14:09.000 --> 14:14.560
the user, in this case, Stockpuppet, was watching neutral videos on the topic.

14:14.560 --> 14:16.480
What could this mean for real users?

14:16.480 --> 14:21.680
This means that recommending promoting videos to men who are already drawn to neutral information

14:21.680 --> 14:26.840
for that topic, but have not yet developed pro-conspiracy beliefs, but now has a higher

14:26.840 --> 14:33.640
chance of developing that because the platform is returning these promoting conspiratorial

14:33.640 --> 14:36.400
videos.

14:36.400 --> 14:43.560
So wrapping up this work, the key contribution of this study was that in some senses this

14:43.560 --> 14:48.720
work developed a methodology to audit search engines for misinformation, and we were also

14:48.720 --> 14:53.400
able to statistically prove that YouTube's behavior varies across different misinformation

14:53.400 --> 14:54.960
topics.

14:54.960 --> 15:01.920
And our study also identified certain populations that could be potentially targets of certain

15:01.920 --> 15:03.920
types of misinformation.

15:03.920 --> 15:10.600
So this tells us that audit itself could be a useful way for studying how algorithms might

15:10.600 --> 15:14.280
have differential impacts on certain marginalized populations.

15:14.280 --> 15:15.280
Yes?

15:15.280 --> 15:16.280
Good question.

15:16.280 --> 15:19.400
So I get what you're saying about it being reactive.

15:19.400 --> 15:24.480
The evidence suggests that in this case it's like being a special case.

15:24.480 --> 15:28.760
Do you have a proposal as to how it might not be reactive?

15:28.760 --> 15:29.760
How could they be proactive?

15:29.760 --> 15:33.360
Is there something you would propose that they do instead?

15:33.360 --> 15:34.360
Yes.

15:34.360 --> 15:38.200
So I think one of the things they could do, and I think they are doing it now in hindsight,

15:38.200 --> 15:43.560
this is an older study, is they're sitting with teams of experts, health experts, and

15:43.560 --> 15:50.440
also looking at these health-related queries and topics in advance to figure out doing these

15:50.440 --> 15:52.360
red teaming exercises.

15:52.360 --> 15:53.360
They call it red teaming.

15:53.360 --> 15:55.320
I think in the research we call it audit.

15:55.320 --> 16:00.960
So they are doing this beforehand to figure out whether the platform is returning problematic

16:00.960 --> 16:02.280
content.

16:02.280 --> 16:11.000
And so if they do more of that proactively, of course there is some hope in changing things,

16:11.000 --> 16:14.320
and we should not be catching these reactively after the fact.

16:14.320 --> 16:17.920
That assumes that the points of view are stable, right?

16:17.920 --> 16:25.720
So we often have these scenarios where culture changes or something goes viral, like the

16:25.720 --> 16:30.440
tide pods or whatever, where I don't think a red team would have come up with, oh yeah,

16:30.440 --> 16:34.720
we're going to start eating bleach or whatever.

16:34.720 --> 16:37.320
Is there an approach there that you would advocate, like if you were in charge of one

16:37.320 --> 16:38.320
of these teams?

16:38.320 --> 16:39.320
Yeah.

16:39.320 --> 16:45.440
I think one of the things that I think some researchers at Stanford, maybe it was one

16:45.440 --> 16:51.080
of your students who did this work with crowd audits, right, like where the crowd itself

16:51.080 --> 16:56.280
is reporting, because it's not really possible for the red team to find all possible scenarios

16:56.280 --> 17:02.600
under which these sorts of problems happen.

17:02.600 --> 17:07.200
And I think that's where if you have these multiple eyes from different domains and different

17:07.200 --> 17:12.680
cultures to report those problems, and then the company actually responds to it.

17:12.680 --> 17:16.200
The problem is if the company's not responding or the people who are building these algorithms,

17:16.200 --> 17:17.760
if they're not responding to it.

17:17.760 --> 17:22.960
And so hopefully there should be a mechanism to do that, kind of closing that loop all the

17:22.960 --> 17:28.800
way from reporting to actually taking action.

17:28.800 --> 17:35.760
So moving on to Amazon, I think one of the things that led us to looking at Amazon is

17:35.760 --> 17:44.160
despite being this leading retailer platform, how less of a focused research focus has been

17:44.160 --> 17:46.280
paid to this platform.

17:46.280 --> 17:51.880
And I think what was alarming is there were several media reports at the time coming out

17:51.880 --> 17:56.680
suggesting that Amazon's algorithm were putting health and vaccine misinformation at the top

17:56.680 --> 17:57.760
of your reading list.

17:57.760 --> 18:02.600
But there was very little research to fall back to either verify or even, you know, kind

18:02.600 --> 18:05.960
of disprove these reports.

18:05.960 --> 18:10.320
So if you search on Amazon, unlike YouTube, which is at least tried to control for vaccine

18:10.320 --> 18:15.320
misinformation, searching on Amazon for a vaccine, even this morning when I searched,

18:15.320 --> 18:20.880
I could actually find some of these books, you would end up getting several anti-vaccination

18:20.880 --> 18:24.480
products mostly in the form of books.

18:24.480 --> 18:29.520
And the recommendation algorithm for Amazon are even much more sophisticated than YouTube.

18:29.520 --> 18:33.240
So you have your product page recommendation, which has all these many different layers,

18:33.240 --> 18:36.720
customers who bought items, sponsored products related to these items.

18:36.720 --> 18:42.160
You have your homepage recommendations, again, many different layers underneath, pre-purchase

18:42.160 --> 18:46.880
page recommendations, which is shown to you after you add a product to the cart that is

18:46.880 --> 18:52.240
after the user shows an intention to buy that product.

18:52.240 --> 18:57.800
So again, this was the same question as before, how bad is this scenario?

18:57.800 --> 19:01.880
So we essentially wanted to conduct the systematic audits on Amazon search and recommendation

19:01.880 --> 19:03.280
algorithm.

19:03.280 --> 19:08.760
And here we picked only one type of problematic content, vaccine misinformation.

19:08.760 --> 19:12.840
And we conducted two sets of audits, unpersonalized one, and then the personalized audits.

19:12.840 --> 19:18.520
The personalized audit school was to assess whether users account history built progressively

19:18.520 --> 19:22.760
by a user performing certain actions, such as clicking on a product, adding the product

19:22.760 --> 19:25.800
to the cart, showing their intention to buy.

19:25.800 --> 19:31.640
Because any of those actions, how does that change what recommendation is being returned?

19:31.640 --> 19:36.640
And here the user built these account history progressively by performing a particular action

19:36.640 --> 19:39.000
for seven consecutive days.

19:39.000 --> 19:42.560
So these were, again, when I say users, these were sock puppets, searching, searching plus

19:42.560 --> 19:45.680
clicking, searching plus clicking plus adding the product to the cart.

19:45.680 --> 19:49.160
So all these were different actions that were performed.

19:49.160 --> 19:53.960
And then we also controlled for noise, very similar setup as before, just with the caveat

19:53.960 --> 19:59.320
that this whole audit experiment setup was a big software engineering feat, considering

19:59.320 --> 20:06.240
the amount of different combinations of recommendations possible on Amazon.

20:06.240 --> 20:07.240
So what did we find?

20:07.240 --> 20:09.280
So I'm going to highlight a couple of results here.

20:09.280 --> 20:11.800
First a single case study result.

20:11.800 --> 20:16.320
So let's say users start searching for a vaccine and they click on an anti-vaccine book.

20:16.320 --> 20:20.600
So as of this morning, this book was actually there on their platform in the first page,

20:20.600 --> 20:22.480
first search result page.

20:22.480 --> 20:27.480
And so if the user clicks on this, that algorithm next serves the user three other anti-vaccine

20:27.480 --> 20:30.080
books in the product recommendation page.

20:30.080 --> 20:36.120
And then once the user adds a product or book to the cart that shows their intention to buy,

20:36.120 --> 20:41.040
both the pre-purchase as well as a home page recommendation also rapidly changes with many

20:41.040 --> 20:43.720
more anti-vaccine book recommendations.

20:43.720 --> 20:48.080
So this just tells you that once a user starts engaging with one misinformation product on

20:48.080 --> 20:52.240
this e-commerce platform, they will be presented with more of those similar stuff at every

20:52.240 --> 20:55.960
point of their Amazon navigation route.

20:55.960 --> 20:57.920
So this was not just a one-off case study.

20:57.920 --> 21:03.560
We found that more than 10% of Amazon products during the time period of our study for search

21:03.560 --> 21:10.080
terms like vaccine, autism, immunization resulted in misinformation book containing

21:10.080 --> 21:13.320
active vaccination content.

21:13.320 --> 21:17.480
And so our audit experiment, just to give you the scale of this experiment, this was

21:17.480 --> 21:24.240
ran for a little bit over three weeks, resulted in 36,000 search results, 16,000 recommendations,

21:24.240 --> 21:30.280
and then worked over several search filters like featured, sponsored, different recommendation

21:30.280 --> 21:35.560
types, user actions, and so all of that resulted in that number of more than 10%.

21:35.560 --> 21:42.400
So if you just zoom out and look at what these thousands of recommendations look like, this

21:42.400 --> 21:46.360
is the entire recommendation graph for one type of recommendation, what are the items

21:46.360 --> 21:49.880
customers buy after viewing this item.

21:49.880 --> 21:53.920
So here, each node in the graph represents a product, an Amazon product, and an edge

21:53.920 --> 21:58.800
from a node A to a node B indicates that B was recommended in the product page of A.

21:58.800 --> 22:02.400
Node size here is proportional to the number of times the product was recommended, and

22:02.400 --> 22:08.360
the color corresponds to whether if it's a red, if it denotes a product annotated as

22:08.360 --> 22:12.600
misinformation, green, neutral, and then blue debunking.

22:12.600 --> 22:18.840
And I think being, you know, all of you have CS degrees or almost about to get a CS degree,

22:18.840 --> 22:23.080
so you would probably able to decipher what's going on in this graph.

22:23.080 --> 22:27.680
There are these large red size nodes attached to other red nodes, which are almost completely

22:27.680 --> 22:28.680
separated.

22:28.680 --> 22:30.640
There are like two separate components, right?

22:30.640 --> 22:35.200
So this just shows how strong of a filter bubble effect there is for this particular

22:35.200 --> 22:36.200
recommendation.

22:36.200 --> 22:41.040
People who are recommended misinformation products, they keep getting recommended those

22:41.040 --> 22:45.520
products super hard for them to break out from that red zone to get to the blue or the

22:45.520 --> 22:46.520
green zone.

22:46.520 --> 22:47.520
Sorry, just a clarification.

22:47.520 --> 22:50.520
I think you mentioned, how did you code something as misinformation?

22:50.520 --> 22:52.520
Obviously, there's lots of shades of gray here.

22:52.520 --> 22:53.520
Yes.

22:53.520 --> 23:01.520
So there was an initial status, we went through an extensive annotation scheme of a set of,

23:01.520 --> 23:03.960
and then we also built a classifier to do that.

23:03.960 --> 23:10.040
But the entire, there are like a lot more details in the paper as to how we coded it,

23:10.040 --> 23:14.840
but I think it took us almost a month to even come up with the whole annotation scheme,

23:14.840 --> 23:20.520
and then five or six of our experts, including me, we kind of coded it.

23:20.520 --> 23:26.640
But yes, so we looked at a few markers, like the name of the book, the text, read the Google

23:26.640 --> 23:32.640
preview of the text of the book, some of the comments, and then also the reviews that are

23:32.640 --> 23:34.640
present on the Amazon website.

23:34.640 --> 23:39.480
So it's a lot more qualitative process, and then all the markers that we took, we also

23:39.680 --> 23:43.200
put it into the classifier to get the final annotations.

23:43.200 --> 23:47.560
So this would have been more like the overall position of the book, not like, was there

23:47.560 --> 23:49.560
a fact somewhere in there that.

23:49.560 --> 23:54.440
So yes, we cannot really go and look at, okay, there is this one single line in the text

23:54.440 --> 23:58.800
in the book, which is misinformation, but yes, so you could say there's a little bit

23:58.800 --> 24:01.960
of noise in there.

24:01.960 --> 24:07.480
And so this was another one just to say that this was not happening for one recommendation.

24:07.480 --> 24:13.160
Those who viewed this item also viewed very similar graph as before, very similar trend.

24:13.160 --> 24:18.400
And so the key takeaway here was our goal was to bring the focus to e-commerce platforms

24:18.400 --> 24:23.960
and show how their algorithms could be pushing anti-vaccine content to users.

24:23.960 --> 24:28.840
And we empirically established how certain real-world user actions on the platform could

24:28.840 --> 24:31.560
drive users to these problematic eco chambers.

24:31.560 --> 24:37.360
I think one of the implications, at least from this work, is that recommendation algorithms

24:37.760 --> 24:40.240
should not be blindly applied to all topics equally.

24:40.240 --> 24:46.400
If it's a health topic, perhaps companies need to pay a little bit more attention and

24:46.400 --> 24:53.880
to ensure that there is higher quality content coming out in their platform.

24:53.880 --> 25:00.080
So this work of ours intentionally and unintentionally was kind of rightly timed during the COVID pandemic.

25:00.080 --> 25:03.240
And so this was widely covered by several news channels.

25:03.240 --> 25:07.120
And in fact, Congressman Adam Schiff and Elizabeth Warren actually cited this research

25:07.160 --> 25:11.640
of ours in their letter to Amazon to control vaccine misinformation.

25:12.240 --> 25:16.400
So we were really happy that, OK, so now Amazon is going to take a few steps to do this,

25:16.760 --> 25:18.840
but turns out we were really wrong.

25:19.360 --> 25:21.080
So this is how Amazon is doing today.

25:21.080 --> 25:25.680
Still today, as of earlier this morning, you would still find several books containing

25:25.680 --> 25:27.880
vaccine misinformation.

25:27.880 --> 25:33.040
This also just tells how, even though you go into all these lengths doing these academic

25:33.040 --> 25:36.000
research, is it actually informing policies?

25:36.000 --> 25:37.760
Is it actually making any real world impact?

25:37.760 --> 25:40.120
And we can go into a long discussion about that.

25:40.520 --> 25:47.160
But in the interest of time, let me talk a little bit more on one other type of study

25:47.160 --> 25:50.600
that we did, looking at another type of audit method.

25:51.360 --> 25:54.560
So so far, the studies that I presented employed one type of audit method,

25:54.560 --> 25:55.520
sock puppet audits.

25:56.040 --> 26:02.280
While these audits provide great control over your experimental design, you can pinpoint

26:02.280 --> 26:06.000
exactly which variable might be affecting the output of the algorithm.

26:06.200 --> 26:10.760
But one criticism of these audits is that the bots behavior are usually built in a

26:10.760 --> 26:11.760
very conservative way, right?

26:11.760 --> 26:16.280
So the bot in this YouTube case was essentially going and watching all

26:16.280 --> 26:18.600
pro-conspiracy videos or all debunking videos.

26:18.880 --> 26:22.080
Real users do not really act exactly that in that way, right?

26:22.080 --> 26:25.800
So these are at least very extreme bot behaviors or user behaviors.

26:26.680 --> 26:30.400
So as an alternative, we conducted crowdsourced audits where we audited the algorithmic

26:30.400 --> 26:34.880
outputs from real world users to study and identify problematic behavior in users'

26:34.880 --> 26:35.880
naturalistic setting.

26:36.360 --> 26:40.320
So we conducted this audit for a nine day duration on YouTube.

26:40.680 --> 26:45.800
And our goal was to assess to the extent in which YouTube was regulating US-based

26:45.800 --> 26:48.080
election misinformation on their platform.

26:49.480 --> 26:53.800
And so soon after the presidential election in 2020, YouTube came under fire for

26:54.280 --> 26:58.000
surfacing election-related misinformation in their search and recommendations.

26:58.520 --> 27:02.160
And they quickly responded to those criticisms by introducing these content

27:02.160 --> 27:06.880
moderation policies to remove videos that spread election-related falsehoods

27:07.240 --> 27:11.080
and claim that misinformation videos would not be surfaced on their platform.

27:11.800 --> 27:17.040
But then again, during the midterm 2020 elections, there were reports saying that

27:17.040 --> 27:20.240
YouTube has still has misinformation blind spots, right?

27:20.240 --> 27:22.920
So they have not been very effective.

27:23.560 --> 27:27.840
So this study of ours was goal was to determine how effective YouTube was

27:27.840 --> 27:30.720
in successfully implementing its content moderation policy.

27:31.840 --> 27:34.840
And so we did this through this post hoc crowdsourced audit.

27:35.320 --> 27:36.360
Why it's post hoc?

27:36.360 --> 27:40.280
Because it's conducted after the fact the event has happened to elections of 2020

27:40.280 --> 27:43.280
and we were conducting this study in 2022.

27:43.880 --> 27:47.080
And it's crowdsourced audit since we investigated YouTube's algorithm collecting

27:47.080 --> 27:49.040
data from real world users.

27:50.400 --> 27:55.960
And I'm sure many of you who have run these sorts of recruitment studies, you

27:55.960 --> 27:58.720
would realize how difficult and how hard it is to do that.

27:59.000 --> 28:05.880
Essentially, we were asking users to lend their YouTube history so that we can do

28:05.880 --> 28:07.640
this sort of audit run.

28:08.600 --> 28:12.960
And so our crowdsourced investigation, I think we started with like recruiting

28:13.200 --> 28:17.880
600 to 500 users and we ended up slightly lower than 100 users.

28:17.880 --> 28:19.960
So 99s, particularly.

28:20.240 --> 28:24.720
So all these 99 users first filled out a pre-survey and about the beliefs on

28:24.720 --> 28:28.440
personalization on YouTube, how they trust YouTube search and recommendation

28:28.440 --> 28:32.760
algorithm, and then they installed this browser extension, which allowed us to

28:33.080 --> 28:34.880
collect users personalized data.

28:35.280 --> 28:40.720
We also had all these ethical considerations, which I can go on into

28:40.720 --> 28:42.320
more detail if anyone is interested.

28:43.200 --> 28:46.920
But what this extension was doing, it was collecting search results of search

28:46.920 --> 28:51.560
queries related to the 2020 US presidential election, as well as

28:51.560 --> 28:54.320
voter fraud claims surrounding the 2020 elections.

28:54.640 --> 28:57.000
So two kinds of collection was happening.

28:57.000 --> 29:02.160
One was with respect to search results in the standard and incognito window.

29:02.160 --> 29:06.240
And by comparing these search results in both these windows, our goal was to

29:06.240 --> 29:09.600
tell the extent in which YouTube was personalizing search results.

29:10.320 --> 29:12.920
And then we were also collecting recommendation results.

29:12.920 --> 29:16.200
And the way we were doing this, we were collecting these up next recommendation

29:16.200 --> 29:22.160
trails after a user has watched a list of pre-selected videos with different

29:22.160 --> 29:24.040
stances on election misinformation.

29:25.240 --> 29:29.520
And the extension would start by first watching a pre-selected seed video and

29:29.520 --> 29:33.480
then collecting up next videos up to five different levels.

29:35.280 --> 29:38.680
So when we asked our participants in this pre-study survey, how much do you

29:38.680 --> 29:40.640
think YouTube personalizes your search results?

29:40.640 --> 29:44.840
About 34% of them believe that YouTube personalizes their search results to a

29:44.840 --> 29:45.760
really great extent.

29:46.240 --> 29:49.960
But this, through our audit, we found that YouTube, actually, that their YouTube

29:49.960 --> 29:52.520
stop search results have little to no personalization.

29:52.840 --> 29:57.520
So this also tells you how users believe in algorithms, like the way they behave

29:57.840 --> 30:02.080
is different from actually the way the platform might be behaving.

30:03.720 --> 30:06.880
But when we asked how much YouTube personalizes their up next recommendation,

30:06.880 --> 30:11.040
that perception actually aligned with how actually the audit results showed.

30:11.040 --> 30:14.720
Like 51% of participants believe that YouTube personalizes up next

30:14.720 --> 30:18.760
recommendation to a great extent, which is in line with what our audit results

30:18.760 --> 30:19.080
found.

30:21.000 --> 30:24.840
We also calculate the amount of misinformation present in search results.

30:25.000 --> 30:28.160
And we quantified this with this misinformation bias score.

30:28.400 --> 30:31.560
And this is the only equation you're going to see throughout this talk.

30:32.480 --> 30:36.120
So this misinformation bias score, we're East from minus one to one.

30:36.120 --> 30:40.640
What the score does is it captures amount of misinformation, election related

30:40.640 --> 30:44.120
misinformation while taking into account the ranking of the search results.

30:44.440 --> 30:48.080
So a positive score indicates that search results contain videos that support

30:48.240 --> 30:53.080
election misinformation while negative it contain videos that oppose election

30:53.080 --> 30:53.920
misinformation.

30:55.200 --> 30:57.640
Now, if you look at the entire distribution of scores for our collective

30:57.640 --> 31:02.360
results, we found that the misinformation score, if you look at the X axis,

31:02.360 --> 31:06.200
it's mostly negative, which indicates that YouTube presents more debunking or

31:06.200 --> 31:08.120
opposing videos in the search results.

31:08.760 --> 31:11.480
A couple of other key things also jumps off, right?

31:11.480 --> 31:15.440
So you could see there are this distribution is by model.

31:15.960 --> 31:19.480
So essentially there are two different clusters and each of these clusters

31:19.480 --> 31:22.360
corresponds to two types of search queries.

31:24.040 --> 31:26.640
I mean, we didn't cluster it ahead of time, right?

31:26.640 --> 31:29.120
This, this emerged from our data.

31:29.320 --> 31:31.800
So the first cluster corresponds to voter fraud.

31:33.120 --> 31:36.480
Basically anything related to fraud in conjunction with keywords related to

31:36.480 --> 31:40.880
election, while cluster two is more generic election related searches,

31:40.880 --> 31:43.040
presidential election, mail-in ballots and so on.

31:44.040 --> 31:48.240
What's interesting here is that the cluster one has these missions

31:48.240 --> 31:52.680
information bias score, which are more negative, which indicates that if the

31:52.680 --> 31:56.160
user goes and search for fraud related topics, they are actually going to be

31:56.160 --> 31:59.800
given more opposing election related misinformation video, right?

31:59.800 --> 32:02.960
So it's making YouTube is making it really difficult for users to search

32:02.960 --> 32:07.280
for election fraud video, which in some sense tells that YouTube's pay more

32:07.280 --> 32:10.440
attention to queries about election fraud and ensures that when users are

32:10.440 --> 32:15.880
searching for them, they are in fact being exposed to opposing misinformation videos.

32:17.080 --> 32:22.240
So key takeaway here is in some way YouTube is in fact successful in

32:22.240 --> 32:24.480
enacting election misinformation policies.

32:24.480 --> 32:30.240
So things that we wanted to test turns out it's actually, you know,

32:30.280 --> 32:34.520
aligning with how they wanted to enforce these policies, but it is indeed

32:34.520 --> 32:37.720
paying special attention to certain queries about voter fraud.

32:38.600 --> 32:42.280
But there still exists certain misinformation in the up next trails.

32:42.280 --> 32:45.520
We found that with some of those positive scores that you found.

32:46.400 --> 32:51.160
And then finally, as a byproduct of this audit, we also found that there was

32:51.160 --> 32:56.840
some mismatch in participants beliefs and the algorithmic reality that happens,

32:56.840 --> 33:01.440
right, which indicates a lack, some lack of awareness of algorithmic, how

33:01.440 --> 33:02.400
algorithms behave.

33:02.600 --> 33:05.560
And I think there has been other researchers who have been working in

33:05.560 --> 33:10.680
the space looking at algorithmic folk theories and how people's perception

33:10.680 --> 33:14.200
differ from the way these platforms work.

33:16.960 --> 33:21.000
Now, wrapping up, so these three, these are all the other, you know,

33:21.000 --> 33:24.280
the core studies that I wanted to present, but obviously coming back to

33:24.280 --> 33:27.280
how I started the talk, where do we go from here, right?

33:27.440 --> 33:31.080
How do we do meaningful algorithmic governance in the first place?

33:31.080 --> 33:35.360
That is, how do we set the path towards algorithmic governance in a meaningful

33:35.360 --> 33:37.760
way and what are the challenges in doing that?

33:39.320 --> 33:44.120
So here are a few ideas and obviously this is not, you know, there might be

33:44.120 --> 33:47.520
more that could be added, but these are some of the possibilities for doing

33:47.520 --> 33:48.480
algorithmic governance.

33:48.480 --> 33:49.760
So I've listed three of these.

33:50.160 --> 33:54.960
The first is algorithmic audits and so governance via audits and there could

33:54.960 --> 33:56.480
be many layers to this, right?

33:56.480 --> 34:01.000
So one of the layer is conducting external audits and I presented some of

34:01.000 --> 34:06.320
these external audit studies through the three research work that we have

34:06.320 --> 34:10.440
done in the past and also these audits could identify different types of risk.

34:10.440 --> 34:13.840
So misinformation is one risk, but you could also do the same for bias,

34:13.840 --> 34:18.840
discrimination, accountability, accessibility, accessibility, fairness

34:18.840 --> 34:19.320
and so on.

34:19.320 --> 34:23.600
And there are many researchers who have worked in the space.

34:23.840 --> 34:25.840
I've listed some of these citations here.

34:26.840 --> 34:31.840
But obviously a question is, so we as academic community, third-party researchers,

34:31.840 --> 34:35.840
we are doing all these audits, is it really making any difference, right?

34:35.840 --> 34:39.840
And as classic example is the failure of our Amazon study to make much of a

34:39.840 --> 34:40.840
difference, right?

34:40.840 --> 34:43.840
So we still really don't have a system in place where the algorithms or

34:43.840 --> 34:47.840
companies running them are truly accountable to an independent third-party.

34:47.840 --> 34:51.840
So this reminds me how US-based consumer reports operate, right?

34:51.840 --> 34:54.840
So there are these independent third-party organizations that go into

34:54.840 --> 34:58.840
great lengths for testing products that you use every day, your cars,

34:58.840 --> 35:00.840
your washing machine and so on.

35:00.840 --> 35:04.840
But so my argument is that why can't we do the same for algorithm?

35:04.840 --> 35:07.840
In fact, I would argue that we need that more for algorithms because we are using

35:07.840 --> 35:11.840
them much more frequently than say your washing machine.

35:12.840 --> 35:15.840
The other shortcoming with external audit is that they are a form of reactive

35:15.840 --> 35:16.840
governance.

35:16.840 --> 35:19.840
This was the question that Michael was asking even earlier, like they operate

35:19.840 --> 35:21.840
after the algorithm have been deployed.

35:21.840 --> 35:25.840
So after the harm has been done, plus the external auditors do not really

35:25.840 --> 35:30.840
have access to the models, to the training data, which are obviously

35:30.840 --> 35:32.840
protected as trade secrets.

35:32.840 --> 35:37.840
So as an alternative, another layer to governance via audits is you could do

35:37.840 --> 35:40.840
internal audits as proactive governance.

35:40.840 --> 35:45.840
And so at the time, researchers from Google who are no longer at Google right

35:46.840 --> 35:51.840
now, but they released this paper making a case for internal audits where audit

35:51.840 --> 35:55.840
would be part of a core part of product development at every step of the

35:55.840 --> 35:56.840
way.

35:57.840 --> 35:59.840
You could also do the best of both worlds, right?

35:59.840 --> 36:02.840
You could do something called cooperative audits, which is a fairly newer

36:02.840 --> 36:07.840
concept where while external audits answers what problems the platform has

36:07.840 --> 36:12.840
and internal audit says why that's happening, you could have a combination

36:12.840 --> 36:16.840
of both and you could do cooperative audits as shared governance, which allows

36:16.840 --> 36:20.840
external algorithm auditors to audit the system of willing private companies.

36:20.840 --> 36:27.840
So I've done a little bit of this with Spotify where working with their

36:27.840 --> 36:33.840
engineers within the company figuring out how gender representations might be

36:33.840 --> 36:38.840
biased or not for their taste on boarding and listen action on podcasts.

36:38.840 --> 36:42.840
And then finally, you also need to do these audits multiple times, right?

36:42.840 --> 36:46.840
Longitudinally, that is we need to conduct these continuous audits monitoring

36:46.840 --> 36:50.840
platforms multiple times instead of that single snapshot audit.

36:50.840 --> 36:54.840
Many of my studies that I presented today are all single snapshot and we do

36:54.840 --> 36:56.840
need that kind of longitudinal effort.

36:57.840 --> 37:02.840
So here is where I want to highlight one of the quotes from the Brajis earlier

37:02.840 --> 37:07.840
internal audit paper where they mentioned the audit process is necessarily

37:07.840 --> 37:11.840
boring, it is slow, it is methodological.

37:11.840 --> 37:17.840
Which stands in stark contrast to what I had started my earlier slides with

37:17.840 --> 37:21.840
move fast and break things, done is better than perfect, so very much in contrast

37:21.840 --> 37:23.840
with the rushed culture of technology development.

37:23.840 --> 37:27.840
And this is where I want to take a little bit of tangent and mention about audit

37:27.840 --> 37:31.840
possibilities. One of the fastest growing developing AI technologies is the

37:31.840 --> 37:35.840
large language models and what would auditing even look like for large

37:35.840 --> 37:39.840
language models? What is the blueprint for LLM auditing?

37:39.840 --> 37:42.840
And then also what are the key challenges, right?

37:42.840 --> 37:45.840
So one of the key challenges is that it is difficult to assess the risks that

37:45.840 --> 37:50.840
AI systems and large language models in particular pose independent of the

37:50.840 --> 37:52.840
context in which they are deployed.

37:52.840 --> 37:57.840
So we do need application specific audits for large language models.

37:57.840 --> 38:01.840
The second challenge is that, and I don't know how to solve this, or rather

38:01.840 --> 38:04.840
even the first one, is that the capabilities and the training processes

38:04.840 --> 38:08.840
of these foundation and models have really outpaced the development of the tools

38:08.840 --> 38:12.840
and techniques and the procedures for auditing, right?

38:12.840 --> 38:15.840
So it is really hard to keep up the pace.

38:15.840 --> 38:19.840
And so doing ethical, legal and technically robust audits makes it super

38:19.840 --> 38:23.840
challenging for such a rapidly developing technology.

38:23.840 --> 38:27.840
And so it must be complemented probably with much more newer forms of

38:27.840 --> 38:29.840
supervision and control.

38:29.840 --> 38:33.840
So here is one possible framework, one possible blueprint for auditing

38:33.840 --> 38:36.840
large language models, which is kind of three layers.

38:36.840 --> 38:38.840
So the first one is a model audit.

38:38.840 --> 38:42.840
So as a name suggests, it focuses on assessing the technical properties of

38:42.840 --> 38:44.840
the pre-attained language models.

38:44.840 --> 38:49.840
So this is very similar flavor to the internal audit that I mentioned earlier.

38:49.840 --> 38:52.840
So that's sort of proactive governance before you deploy the model.

38:52.840 --> 38:56.840
But then there is application audit, which focuses on the assessing the

38:56.840 --> 38:59.840
applications built on top of the LLMs.

38:59.840 --> 39:02.840
So which is these flavors of post hoc audit, right?

39:02.840 --> 39:05.840
And it should be done longitudinally, right?

39:05.840 --> 39:09.840
Multiple times, over a long period of time, so as to capture any sort of new

39:09.840 --> 39:11.840
properties that might be emerging.

39:11.840 --> 39:15.840
And then finally, I think this is the new form of audit that we haven't talked

39:15.840 --> 39:17.840
about a lot, at least the research community.

39:17.840 --> 39:21.840
These are these governance audits that is assessing the processes whereby these

39:21.840 --> 39:25.840
language models are designed and where they are disseminated.

39:25.840 --> 39:29.840
So very much process oriented, right?

39:29.840 --> 39:32.840
My next proposition is about value-centered audits.

39:32.840 --> 39:37.840
That is, there is this active conversation around social values, emphasizing

39:37.840 --> 39:39.840
while designing algorithms.

39:39.840 --> 39:42.840
And I think we also need to turn that attention and thinking into how we can

39:42.840 --> 39:45.840
value and respect humans involved in the audit process.

39:45.840 --> 39:49.840
So these humans could be in the form of users who use the system or even

39:49.840 --> 39:53.840
auditors who are investigating the sites.

39:53.840 --> 39:57.840
And so for auditors, if we bring back the conversation for a second back to

39:57.840 --> 40:01.840
misinformation, one instance where auditors did not really feel perceived

40:01.840 --> 40:06.840
fair treatment was this scenario where fact-checkers are one of the key auditors

40:06.840 --> 40:09.840
of misinformation on online platforms like Facebook, Twitter.

40:09.840 --> 40:13.840
So the fact-checking organizations, SNOPs, a couple of years ago, actually backed

40:13.840 --> 40:17.840
out of their partnership with Facebook because they didn't feel their values

40:17.840 --> 40:19.840
were being respected.

40:19.840 --> 40:22.840
I think to delve into this question of fair treatment of auditors, we need more

40:22.840 --> 40:23.840
effort.

40:23.840 --> 40:28.840
And so one way in which my group has started a few initiatives, we have

40:28.840 --> 40:32.840
launched a research endeavor with the fact-checking organization based in

40:32.840 --> 40:34.840
Kenya called Pesachek.

40:34.840 --> 40:37.840
And this has also expanded to 16 other fact-checking organizations across

40:37.840 --> 40:39.840
four different continents.

40:39.840 --> 40:42.840
And we released our first report called the Human and Technological

40:42.840 --> 40:45.840
Infrastructures of Fact-Checking.

40:45.840 --> 40:50.840
And so one big motivation for this work was also this question of, are we

40:50.840 --> 40:53.840
really taking into account diverse voices when we are talking about

40:53.840 --> 40:55.840
governance and governing technologies?

40:55.840 --> 40:59.840
And are we really doing culturally responsible AI?

40:59.840 --> 41:02.840
Finally, how do we ensure actionable audits?

41:02.840 --> 41:05.840
That is, audits that result in real change.

41:05.840 --> 41:10.840
So one of the most successful examples of an actionable audit is Joy Voluwami's

41:10.840 --> 41:12.840
Gender Shade Study.

41:12.840 --> 41:17.840
So what she did was she audited facial recognition algorithms.

41:17.840 --> 41:21.840
And within seven months of the release of these original audit, all the

41:21.840 --> 41:26.840
three companies who had their facial recognition apps released new API

41:26.840 --> 41:31.840
versions that reduced accuracy disparities with gender, male and

41:31.840 --> 41:35.840
female, as well as race, darker and lighter-skinned subgroups.

41:35.840 --> 41:39.840
So in other words, the Gender Shade Study is a classic example of commercial

41:39.840 --> 41:41.840
actual impact.

41:41.840 --> 41:44.840
And so they laid out their approach in this actionable auditing paper of

41:44.840 --> 41:45.840
theirs.

41:45.840 --> 41:47.840
Highly recommend you all to go and refer to it.

41:47.840 --> 41:51.840
But turns out actionable auditing is often tremendously difficult to

41:51.840 --> 41:52.840
achieve.

41:52.840 --> 41:55.840
And here is where I want to revisit that earlier slide for our Amazon

41:55.840 --> 41:59.840
study to highlight how much we had failed in doing the actionable

41:59.840 --> 42:00.840
auditing.

42:00.840 --> 42:05.840
So despite widespread media coverage, despite a letter from Congressman

42:05.840 --> 42:08.840
Adam Schiff, Amazon did not really act much.

42:08.840 --> 42:13.840
All they did was add that banner of COVID-19 information directing to

42:13.840 --> 42:15.840
CDC's web page.

42:15.840 --> 42:19.840
So hopefully this kind of summarizes the challenges as well as opportunities

42:19.840 --> 42:23.840
and setting the path for algorithmic governance and hoping with the new

42:23.840 --> 42:28.840
regulations coming in place, maybe if I were to give this talk next year, I

42:28.840 --> 42:33.840
would have a little bit more hopeful slide than how I'm ending this talk.

42:33.840 --> 42:35.840
So that's it.

42:35.840 --> 42:37.840
So this is all I talked about today.

42:37.840 --> 42:40.840
Most of this work was done with my PhDs.

42:41.840 --> 42:44.840
Then PhD student Prena Juneja, who is now a faculty at Seattle

42:44.840 --> 42:45.840
University.

42:45.840 --> 42:50.840
And then my group, I would also stick in three other threads of work,

42:50.840 --> 42:53.840
which I obviously don't have time to talk about.

42:53.840 --> 42:56.840
But these are like a couple of other amazing students.

42:56.840 --> 43:00.840
So Shruti Furkaya she did a bunch of work on computational social

43:00.840 --> 43:01.840
science.

43:01.840 --> 43:05.840
I was earlier meeting a student who was doing this sort of work.

43:05.840 --> 43:09.840
So things like big data analysis of online interactions.

43:09.840 --> 43:13.840
Studying trajectories of participation of users in extreme

43:13.840 --> 43:15.840
communities, conspiratorial communities.

43:15.840 --> 43:19.840
We have also done a little bit of design intervention and social

43:19.840 --> 43:24.840
system design work with another student who is also a faculty now.

43:24.840 --> 43:28.840
This is more of an XCI flavor where essentially questions like how do

43:28.840 --> 43:32.840
you design a system to nudge users towards meaningful credibility

43:32.840 --> 43:33.840
assessment?

43:33.840 --> 43:38.840
How do you design a system to allow users to break out of their filter

43:38.840 --> 43:39.840
bubble?

43:39.840 --> 43:42.840
Something called other tube that we built on YouTube.

43:42.840 --> 43:47.840
And then finally, the last and the least fleshed out thread is some

43:47.840 --> 43:51.840
of the work that is currently ongoing with two of my students.

43:51.840 --> 43:54.840
We are looking at challenges and opportunities of generative AI

43:54.840 --> 43:56.840
in fact checking work.

43:56.840 --> 44:00.840
And then what are some cultural misalignment that might happen with

44:00.840 --> 44:01.840
language models?

44:01.840 --> 44:05.840
Especially with roots in the global south, we are looking at

44:05.840 --> 44:09.840
cultural implications of these language models in countries like

44:09.840 --> 44:13.840
India and other countries in Southeast Asia.

44:13.840 --> 44:16.840
So with that, I would like to end and happy to take questions.

44:16.840 --> 44:17.840
Thank you all.

44:17.840 --> 44:27.840
All right, we've got time for some questions.

44:27.840 --> 44:32.840
I was wondering in your auditing of YouTube algorithms that you guys

44:32.840 --> 44:33.840
looked at.

44:33.840 --> 44:36.840
Yes, it was in 2020.

44:36.840 --> 44:40.840
So I wasn't sure if YouTube shorts had been implemented since then

44:40.840 --> 44:43.840
because YouTube shorts are somewhat of a newer aspect.

44:43.840 --> 44:50.840
But I wonder if the algorithms that underlie the, I guess, traditional

44:50.840 --> 44:54.840
YouTube recommendation system underlie the same sort of like YouTube

44:54.840 --> 44:59.840
shorts recommendation because I guess the length of content and sort of

44:59.840 --> 45:02.840
the amount of stimulus that would be needed to get the person to keep

45:02.840 --> 45:03.840
the point would be different.

45:03.840 --> 45:07.840
And therefore possibly seeing that if there are sort of similar

45:07.840 --> 45:14.840
pattern between the two, whether or not the density, I guess, of

45:14.840 --> 45:17.840
sort of misinformation sort of increases because the fact that

45:17.840 --> 45:18.840
content is more short form.

45:18.840 --> 45:19.840
Yeah.

45:19.840 --> 45:21.840
So for the first study, we didn't, at that time, shorts were not

45:21.840 --> 45:22.840
there.

45:22.840 --> 45:26.840
But then for the third one that I presented with election misinformation,

45:26.840 --> 45:28.840
we did capture YouTube shorts.

45:28.840 --> 45:31.840
And that tells me that we should probably do another analysis

45:31.840 --> 45:35.840
comparing the length of the videos and the, you know, whether it's a

45:35.840 --> 45:37.840
short video versus a long form.

45:37.840 --> 45:39.840
We did not do that, but that's an excellent point.

45:39.840 --> 45:40.840
Yeah.

45:43.840 --> 45:45.840
I have a question on maybe two studies.

45:45.840 --> 45:47.840
Was it a good first one?

45:47.840 --> 45:50.840
Did you look at all at like the probability that you would get

45:50.840 --> 45:54.840
recommended legitimately false conspiracy videos, like on the

45:54.840 --> 45:59.840
moon landing from videos about conspiracies that are a little bit

45:59.840 --> 46:02.840
more true, like missing persons cases that the police just don't

46:02.840 --> 46:06.840
investigate and like the likelihood that you'll get recommended

46:06.840 --> 46:08.840
actually false content?

46:08.840 --> 46:13.840
Yeah, we did not because I think one of the shortcomings of running

46:13.840 --> 46:15.840
audits is the whole setup itself.

46:15.840 --> 46:17.840
So we have to start somewhere, right?

46:17.840 --> 46:21.840
So the RR starting point were a set of seed queries.

46:21.840 --> 46:22.840
Right?

46:22.840 --> 46:27.840
So with the way you are framing it, you know, we could, the

46:27.840 --> 46:30.840
hypothesis could be dozen missing persons case lead you to more

46:30.840 --> 46:31.840
conspiratory videos.

46:31.840 --> 46:34.840
And in that scenario, I think we can use our audit framework to

46:34.840 --> 46:38.840
have those as seed queries and see what happens.

46:42.840 --> 46:47.840
On your kind of concluding point about actionable audits, I'm just

46:47.840 --> 46:50.840
curious, do you think it's something to do with like the

46:50.840 --> 46:53.840
conducting of the audit itself or just the context and how it

46:53.840 --> 46:56.840
aligns with like the company's incentives?

46:56.840 --> 47:00.840
Because it feels like the gender shades case, it was like a very

47:00.840 --> 47:03.840
easily framed as like poor performance.

47:03.840 --> 47:05.840
And so they were trying to cover themselves.

47:05.840 --> 47:08.840
Whereas Amazon is somewhat incentivized to keep people buying

47:08.840 --> 47:11.840
things even if those things are harmful.

47:11.840 --> 47:15.840
So like, I guess I'm wondering, like, do you think audits need to

47:15.840 --> 47:18.840
be conducted differently or there just needs to be more external

47:18.840 --> 47:22.840
pressure like from the government or the public to incentivize the

47:22.840 --> 47:24.840
companies when they are like internally?

47:24.840 --> 47:26.840
Yeah, I don't think it's a ladder.

47:26.840 --> 47:32.840
I think, I think Joy went on to great extent to after the study was

47:32.840 --> 47:36.840
published to kind of give talks and publicize and do that kind of

47:36.840 --> 47:39.840
outreach, which I did not do with this work.

47:39.840 --> 47:41.840
I think that matters a lot, right?

47:41.840 --> 47:44.840
She's the one who went to Congress for testimony and testified

47:44.840 --> 47:46.840
against these companies.

47:46.840 --> 47:50.840
And when you do that kind of impact, it would definitely translate

47:50.840 --> 47:54.840
or there's a higher chance to be for your work to be translated to

47:54.840 --> 47:56.840
actual actionable outcome.

47:56.840 --> 47:59.840
I don't think those are actually steps listed in the actionable

47:59.840 --> 48:01.840
auditing paper.

48:01.840 --> 48:04.840
And in some sense, I feel like maybe the academic community need to

48:04.840 --> 48:07.840
think about how to incentivize those additional work.

48:07.840 --> 48:09.840
We don't have those incentives in place.

48:09.840 --> 48:12.840
And partly I think we should look inward and blame ourselves that

48:12.840 --> 48:14.840
we don't have those incentives in place.

48:17.840 --> 48:20.840
Moving to this, I just wanted to hear your thoughts more.

48:20.840 --> 48:24.840
When being either interactive or reactive, do you think we should

48:24.840 --> 48:28.840
draw a distinction between conspiracy theories or misinformation

48:28.840 --> 48:33.840
that has potential for great harm versus those that maybe don't,

48:33.840 --> 48:36.840
right, to justify interventions that override individual autonomy

48:36.840 --> 48:38.840
or control the information space?

48:38.840 --> 48:42.840
Like, who is the moon landing conspiracy theory?

48:42.840 --> 48:44.840
Can you say that last part?

48:44.840 --> 48:48.840
Like, the moon landing conspiracy presumably isn't hurting anyone,

48:48.840 --> 48:49.840
right?

48:49.840 --> 48:51.840
Should we take it down?

48:51.840 --> 48:53.840
Yeah, that's a really good point.

48:53.840 --> 48:57.840
I think for companies like Google, I know they have this,

48:57.840 --> 49:01.840
your money or your life, they have a view or an acronym,

49:01.840 --> 49:06.840
YMYL or something, a set of search guidelines.

49:06.840 --> 49:11.840
If those search results or the pages that show up,

49:11.840 --> 49:16.840
if it's affecting monetarily, financially, health or your life,

49:16.840 --> 49:19.840
then they're going to be more proactive and act on it.

49:19.840 --> 49:20.840
So you're right.

49:20.840 --> 49:23.840
Like, moon landing is probably not to that extent,

49:23.840 --> 49:28.840
versus if it's vaccine information that has direct life consequences,

49:28.840 --> 49:29.840
right?

49:29.840 --> 49:32.840
But obviously then there are all these other questions that when

49:32.840 --> 49:35.840
it's very well known that if you are drawn to one conspiracy,

49:35.840 --> 49:38.840
you're likely to get other conspiracy theories, right?

49:38.840 --> 49:40.840
So then what happens?

49:40.840 --> 49:43.840
Should those be prioritized, at least to the extent that they

49:43.840 --> 49:46.840
maybe should be prioritized in the recommendations,

49:46.840 --> 49:51.840
if not completely removed from the platform?

49:51.840 --> 49:52.840
Yeah?

49:52.840 --> 49:55.840
This is sort of another sort of idea that I had.

49:55.840 --> 49:59.840
Sort of looking at the idea of, like, what I think is sort of

49:59.840 --> 50:02.840
interesting about, like, social media apps like YouTube is a whole

50:02.840 --> 50:05.840
aspect of the media being able to communicate with others,

50:05.840 --> 50:07.840
like the other comment section of a YouTube channel.

50:07.840 --> 50:10.840
And I was wondering if there is a way to possibly, like,

50:10.840 --> 50:12.840
I'm not sure how present this is, or if this is even, like,

50:12.840 --> 50:17.840
a thing that is even, like, something that is able to be,

50:17.840 --> 50:18.840
like, looked into.

50:18.840 --> 50:25.840
But is there a possibility that the algorithm isn't,

50:25.840 --> 50:29.840
may almost be promoting you content not necessarily by what

50:29.840 --> 50:34.840
it's physically providing or, like, showing up then recommended,

50:34.840 --> 50:38.840
but showing other users who would most likely put other links

50:38.840 --> 50:41.840
to more, I guess, like, extreme videos in the comments being like,

50:41.840 --> 50:43.840
oh, if you thought this was interesting, like, look at this.

50:43.840 --> 50:47.840
And so that's not explicitly YouTube's algorithm showing you a video.

50:47.840 --> 50:51.840
It's showing that same video to someone else who has the ability

50:51.840 --> 50:55.840
to share a link to another YouTube video.

50:55.840 --> 50:58.840
That would be almost, like, pushing someone down like a pipeline

50:58.840 --> 50:59.840
of conspiracy theories.

50:59.840 --> 51:02.840
So more like how the social recommendations,

51:02.840 --> 51:05.840
like, you're sort of adding this collaborative social

51:05.840 --> 51:09.840
recommendation component to YouTube and seeing how that pushes.

51:09.840 --> 51:13.840
Like, if YouTube, like, you could have the same effect.

51:13.840 --> 51:17.840
Like, in theory, maybe, the YouTube recommendation system could

51:17.840 --> 51:21.840
not explicitly push someone by recommending, like,

51:21.840 --> 51:23.840
more intense conspiracy theories.

51:23.840 --> 51:26.840
But if YouTube is recommending someone who's already, like,

51:26.840 --> 51:29.840
a very, like, entrenched conspiracy theorist and maybe someone

51:29.840 --> 51:33.840
who's on the edge, if YouTube recommends them both the same,

51:33.840 --> 51:37.840
like, it's starting out conspiracy theory video,

51:37.840 --> 51:39.840
then you can have the person who's, like,

51:39.840 --> 51:41.840
very entrenched conspiracy theorist commenting

51:41.840 --> 51:43.840
and suggesting things themselves.

51:43.840 --> 51:47.840
And it's not that YouTube is explicitly recommending the original person

51:47.840 --> 51:51.840
or the person who isn't, like, entrenched conspiracy theory.

51:51.840 --> 51:55.840
It's that they put them essentially on the same,

51:56.840 --> 51:59.840
or they put them in the same environment in which they could communicate.

51:59.840 --> 52:00.840
Yeah.

52:00.840 --> 52:04.840
So, actually, my group of some of Shruti's work,

52:04.840 --> 52:08.840
we have done this in the context of Reddit where what you're describing

52:08.840 --> 52:10.840
those very entrenched conspiracy users,

52:10.840 --> 52:12.840
we term this as veteran users.

52:12.840 --> 52:16.840
And so they are one of the big drivers of bringing other people,

52:16.840 --> 52:20.840
like what we call joiners, into the community of conspiracy group.

52:20.840 --> 52:24.840
So I hope YouTube never does that for what you're suggesting,

52:24.840 --> 52:27.840
but that's a classic marker of how these social dynamics

52:27.840 --> 52:31.840
can actually bring people into these conspiratorial world views.

52:31.840 --> 52:34.840
And, you know, empirically, we have seen that.

52:34.840 --> 52:36.840
And there is also social science theory

52:36.840 --> 52:39.840
proving that that definitely happens.

52:39.840 --> 52:41.840
Yeah.

52:41.840 --> 52:47.840
One challenge that I feel like our field faces with audits is,

52:47.840 --> 52:51.840
I guess what I would describe as the sense I get of frustration

52:51.840 --> 52:55.840
from folks at these companies who feel like the audits

52:55.840 --> 53:00.840
aren't well executed or are way out of date as soon as they're published.

53:00.840 --> 53:04.840
You made this point that we have to be very methodical

53:04.840 --> 53:07.840
and often slow in doing this.

53:07.840 --> 53:11.840
Then you throw in the peer review pipeline

53:11.840 --> 53:14.840
that can slow things down further.

53:14.840 --> 53:17.840
And by the time the thing comes out,

53:17.840 --> 53:21.840
I remember seeing an applied researcher in a company

53:21.840 --> 53:24.840
who in principle would be more open to this kind of stuff,

53:24.840 --> 53:27.840
being like, our album doesn't even work like that anymore.

53:27.840 --> 53:32.840
And so I'm wondering, so obviously you listed a bunch of possible

53:32.840 --> 53:36.840
cooperative audits, longitudinal, internal, and so on.

53:36.840 --> 53:41.840
I'm curious, are there, is there anything we can do to address that?

53:41.840 --> 53:45.840
Let's assume that we can't change the incentives of the companies,

53:45.840 --> 53:47.840
to change how quickly we do the audits.

53:47.840 --> 53:50.840
Are we always going to be vulnerable to this,

53:50.840 --> 53:53.840
like, oh yeah, that was yesterday's algorithm kind of critique?

53:53.840 --> 53:55.840
Yeah, that's a really good point.

53:55.840 --> 53:59.840
In fact, I was at a workshop with other folks like Christo Wilson

53:59.840 --> 54:03.840
and a few others who have done audits for a very long time

54:03.840 --> 54:05.840
with people from Facebook and YouTube.

54:05.840 --> 54:08.840
And I think we came up with this exact same question.

54:08.840 --> 54:10.840
And I think the common thing that emerged was that,

54:10.840 --> 54:14.840
I don't think academics should be the, or academic institutions

54:14.840 --> 54:18.840
should be the places to do these sorts of long-term audits.

54:18.840 --> 54:22.840
It's fine to kind of develop the methods and, you know,

54:22.840 --> 54:25.840
kind of say, okay, this one should pay attention, for example,

54:25.840 --> 54:28.840
to Amazon, or, you know, this is the method to do it.

54:28.840 --> 54:31.840
But then you need, like, separate third-party companies

54:31.840 --> 54:33.840
to continuously do these audits, right?

54:33.840 --> 54:35.840
So sort of like consumer reports,

54:35.840 --> 54:38.840
what's the equivalent of that for audits?

54:38.840 --> 54:40.840
And I think at CSCW, the closing keynote,

54:40.840 --> 54:42.840
Room Room was mentioning that kind of red teaming,

54:42.840 --> 54:45.840
I think their company or whichever NGO she's working with,

54:45.840 --> 54:47.840
they are doing something like that.

54:47.840 --> 54:49.840
I think, so academics with that peer review process,

54:49.840 --> 54:52.840
I don't think we should be responsible for doing those sorts of

54:52.840 --> 54:55.840
continuous audits because we are always going to play catch-up

54:55.840 --> 54:58.840
with companies.

54:58.840 --> 55:00.840
Yeah.

55:00.840 --> 55:01.840
Oh.

55:01.840 --> 55:02.840
One last? Yeah.

55:02.840 --> 55:03.840
At the end.

55:03.840 --> 55:05.840
Do you have thoughts on, like, Twitter has implemented, like,

55:05.840 --> 55:07.840
community notes where people can just, like,

55:07.840 --> 55:10.840
anyone can put it under a post like,

55:10.840 --> 55:12.840
that's not true, or this is misleading,

55:12.840 --> 55:14.840
or this person ever said that, like,

55:14.840 --> 55:17.840
just, like, user-based, immediate type auditing,

55:17.840 --> 55:18.840
if you will.

55:18.840 --> 55:20.840
I do have thoughts on that versus, like,

55:20.840 --> 55:22.840
companies long-term auditing, or, like,

55:22.840 --> 55:23.840
if you think that's a good idea.

55:23.840 --> 55:25.840
Yeah, that's a really good point.

55:25.840 --> 55:27.840
We haven't looked at community notes,

55:27.840 --> 55:30.840
but I know, you know, some researchers have,

55:30.840 --> 55:32.840
kind of, looked and researched it.

55:32.840 --> 55:35.840
I don't have really any very smart thoughts as to,

55:35.840 --> 55:38.840
other than the usual advice that it's a good thing that

55:38.840 --> 55:41.840
one should do it, but with the caveat that,

55:41.840 --> 55:47.840
if the community doesn't reflect the right view,

55:47.840 --> 55:49.840
or I wouldn't use the word right,

55:49.840 --> 55:54.840
but, you know, a credible view of what happens then.

55:54.840 --> 55:59.840
That's problematic.

55:59.840 --> 56:00.840
I think that's time.

56:00.840 --> 56:01.840
So let's thank your speaker.

56:01.840 --> 56:02.840
Thank you.

56:02.840 --> 56:04.840
Thank you, everyone.

56:08.840 --> 56:09.840
Thank you.

