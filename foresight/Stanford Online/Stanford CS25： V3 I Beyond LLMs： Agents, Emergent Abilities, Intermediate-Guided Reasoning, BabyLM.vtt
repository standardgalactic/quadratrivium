WEBVTT

00:00.000 --> 00:11.200
So today we're going to give an instructor-led lecture talking about some of the key topics

00:11.200 --> 00:15.840
in Transformers and LLMs these days, and particularly Div will be talking about agents

00:15.840 --> 00:20.000
and I'll be discussing emergent abilities, intermediate-guided reasoning, as well as

00:20.000 --> 00:22.120
baby LLM.

00:22.120 --> 00:32.280
So let me actually go to my part, because Div is not here yet.

00:32.280 --> 00:36.280
So I'm sure many of you have read this paper, Emergent Abilities of Large Language Models

00:36.280 --> 00:42.160
from 2022, so I'll briefly go through some of them.

00:42.160 --> 00:46.960
So basically, an ability is emergent if it is present in large group but not smaller

00:46.960 --> 00:51.240
models, and it would not have been directly predicted by extrapolating performance from

00:51.240 --> 00:53.400
smaller models.

00:53.400 --> 00:57.280
So you can think of performance, it's basically near random until a certain threshold called

00:57.280 --> 01:01.080
a critical threshold, and then it improves very heavily.

01:01.080 --> 01:05.200
This is known as a phase transition, and again, it would not have been extrapolated or predicted

01:05.200 --> 01:10.920
if you were to extend the curve of the performance of smaller models, it's more of a jump which

01:10.920 --> 01:12.960
we'll see later.

01:12.960 --> 01:16.480
So here's an example of Fuchsia Prompting for many different tasks.

01:16.480 --> 01:22.960
For example, modular arithmetic unscrambling words, different QA tasks, and so forth.

01:22.960 --> 01:27.960
And you'll see that performance kind of jumps very heavily up until a certain point.

01:27.960 --> 01:32.720
I believe the x-axis here is the number of training flops, which corresponds to basically

01:32.720 --> 01:33.720
model scale.

01:33.720 --> 01:39.480
So you'll see in many cases around 10 to the 22 or 10 to the 23 training flops, there's

01:39.480 --> 01:47.680
a massive exponential jump or increase in terms of model performance on these tasks,

01:47.680 --> 01:51.600
which was not present on smaller scales.

01:51.600 --> 01:54.560
So it's quite unpredictable.

01:54.560 --> 01:59.080
And here are some examples of this occurring using augmented prompting strategies.

01:59.080 --> 02:02.040
So I'll be talking a bit later about chain of thought.

02:02.040 --> 02:09.080
But basically, these strategies improve the ability of getting behavior from models on

02:09.080 --> 02:10.280
different tasks.

02:10.280 --> 02:14.640
So you see, for example, with chain of thought reasoning, that's an emergent behavior that

02:14.640 --> 02:18.160
happens, again, around 10 to the 22 training flops.

02:18.160 --> 02:24.880
And without it, model performance on GSM 8K, which is a mathematics benchmark, it doesn't

02:24.880 --> 02:27.240
really improve heavily.

02:27.240 --> 02:33.440
But chain of thought kind of leads to that emergent behavior or sudden increase in performance.

02:33.440 --> 02:38.920
And here's just the table from the paper, which has a bigger list of emergent abilities

02:38.920 --> 02:42.240
of LLMs as well as their scale at which they occur.

02:42.240 --> 02:46.280
So I recommend that you check out the paper to learn a bit more.

02:46.280 --> 02:51.680
And so one thing researchers have been wondering is, why does this emergence occur exactly?

02:51.680 --> 02:55.440
And even now, there's few explanations for why that happens.

02:55.440 --> 02:59.320
And the authors also found that the evaluation metrics used to measure these abilities may

02:59.320 --> 03:04.840
not fully explain why they emerge and suggest some alternative evaluation metrics, which

03:04.840 --> 03:08.480
I encourage you to read more in the paper.

03:08.480 --> 03:14.720
So other than scaling up to encourage these emergent abilities, which could endow even

03:14.720 --> 03:20.360
larger LMs with further new emergent abilities, what else can be done?

03:20.360 --> 03:25.920
While things like investigating new architectures, higher quality data, which is very important

03:25.920 --> 03:31.160
for performance on all tasks, improved training and improved training procedures could enable

03:31.160 --> 03:37.400
emergent abilities to occur, especially on smaller models, which is a current growing

03:37.400 --> 03:42.040
area of research, which I'll also talk about a bit more later.

03:42.040 --> 03:48.120
Other abilities include potentially improving the few shot prompting abilities of LMs, theoretical

03:48.120 --> 03:54.200
and interpretability research, again, to try to understand why emergent abilities is a

03:54.200 --> 04:00.480
thing and how we can maybe leverage that further, as well as maybe some computational linguistics

04:00.480 --> 04:02.280
work.

04:02.280 --> 04:07.080
So with these large models and emergent abilities, there's also risks, right?

04:07.080 --> 04:14.240
There's potential societal risks, for example, truthfulness, bias and toxicity risks.

04:14.240 --> 04:18.680
As emergent abilities incentivize us further scaling up language models, for example, up

04:18.680 --> 04:25.800
to GPT4 size or further, however, this may lead to bias increasing, as well as toxicity

04:25.800 --> 04:29.240
and the memorization of training data.

04:29.240 --> 04:34.200
That's one thing that these larger models are more potent at.

04:34.200 --> 04:38.840
And there's potential risks in future language models that have also not been discovered yet.

04:38.840 --> 04:45.480
So it's important that we approach this in a safe manner, as well.

04:45.480 --> 04:51.280
And of course, emergent abilities and larger models have also led to sociological changes,

04:51.280 --> 04:54.840
changes in the community's views and use of these models.

04:54.840 --> 04:59.280
Most importantly, it's led to the development of general purpose models, which perform on

04:59.280 --> 05:03.840
a wide range of tasks, not just particular tasks it was trained for.

05:03.840 --> 05:08.560
For example, when you think of chat GPT, GPT 3.5, as well as GPT4, there are more general

05:08.560 --> 05:13.800
purpose models which work well across the board and can then be further adapted to different

05:13.800 --> 05:20.600
use cases, mainly through in-context prompting and so forth.

05:20.600 --> 05:24.800
This has also led to new applications of language models outside of NLP.

05:24.800 --> 05:28.600
For example, they're being used a lot now for text-to-image generation.

05:28.600 --> 05:34.520
The encoder parts of those text-to-image models are basically transformer models or

05:34.520 --> 05:39.280
large language models, as well as things like robotics and so forth.

05:39.280 --> 05:43.920
So you'll know that earlier, this quarter, Jim Fan gave a talk about how they're using

05:43.920 --> 05:51.480
GPT4 and so forth in Minecraft and for robotics work, as well as long-range horizon tasks

05:51.480 --> 05:52.480
for robotics.

05:52.480 --> 05:57.000
And yeah, so basically in general, it's led to a shift in the NLP community towards the

05:57.000 --> 06:02.480
general purpose rather than task-specific models.

06:02.480 --> 06:07.640
And as I kind of stated earlier, some directions for future work include model scaling, further

06:07.640 --> 06:14.680
model scaling, although I believe that we will soon probably be reaching a limit or

06:14.680 --> 06:18.400
point of the mission returns with just more model scale.

06:18.400 --> 06:22.880
Improved model architectures and training methods, data scaling.

06:22.880 --> 06:28.720
So I also believe that data quality is of high importance, possibly even more important

06:28.720 --> 06:33.120
than the model scale and the model itself.

06:33.120 --> 06:37.480
Better techniques for an understanding of prompting, as well as exploring and enabling

06:37.480 --> 06:42.520
performance on frontier tasks that current models are not able to perform well on.

06:42.520 --> 06:45.360
So GPT4 kind of pushed the limit on this.

06:45.360 --> 06:48.000
It's able to perform well on many more tasks.

06:49.000 --> 06:54.360
Studies have shown that it still suffers from even some more basic sort of reasoning,

06:54.360 --> 06:58.400
analogical and common sense reasoning.

06:58.400 --> 07:00.720
So I just had some questions here.

07:00.720 --> 07:07.360
I'm not sure how much time we have to address, but so for the first one, like I said, emergent

07:07.360 --> 07:10.760
abilities I think will arise to a certain point, but there will be a limit or point

07:10.760 --> 07:17.440
of the mission returns as model scale, as well as data scale rises, because I believe

07:17.440 --> 07:20.120
at some point there will be overfitting.

07:20.120 --> 07:25.000
And there's only so much you can learn from all data on the web.

07:25.000 --> 07:31.760
So I believe that more creative approaches will be necessary after a certain point, which

07:31.760 --> 07:36.720
kind of also addresses the second question.

07:36.720 --> 07:41.600
So I will move on.

07:41.600 --> 07:46.920
Anybody has any questions, also feel free to interrupt at any time.

07:46.920 --> 07:50.440
So the second thing I'll be talking about is this thing I call intermediate-guided reasoning.

07:50.440 --> 07:53.080
So I don't think this is actually a term.

07:53.080 --> 07:59.160
It's typically called chain of thought reasoning, but it's not just chains now being used.

07:59.160 --> 08:02.280
So I wanted to give it a more broad title.

08:02.280 --> 08:05.200
So I called it intermediate-guided reasoning.

08:05.200 --> 08:09.640
So this was inspired by this work, also by my friend Jason, who was at Google now at

08:09.640 --> 08:14.040
OpenAI, called chain of thought reasoning or COT.

08:14.040 --> 08:18.800
This is basically a series of intermediate reasoning steps, which has been shown to improve

08:18.800 --> 08:23.320
LLM performance, especially on more complex reasoning tasks.

08:23.320 --> 08:28.560
It's inspired by the human thought process, which is to decompose many problems into multi-step

08:28.560 --> 08:30.060
problems.

08:30.060 --> 08:34.920
For example, when you answer an exam, when you're solving math questions on an exam,

08:34.920 --> 08:38.680
you don't just go to the final answer, you kind of write out your steps.

08:38.680 --> 08:42.680
Even when you're just thinking through things, you kind of break it down into a piecewise

08:42.680 --> 08:47.760
or step-by-step fashion, which allows you to typically arrive at a more accurate final

08:47.760 --> 08:53.960
answer and more easily arrive at the final answer in the first place.

08:53.960 --> 08:58.720
Another advantage is this provides an interpretable window into the behavior of the model.

08:58.720 --> 09:03.400
You can see exactly how it arrived at an answer, and if it did so incorrectly, where in its

09:03.400 --> 09:09.960
reasoning path that it kind of goes wrong or starts going down an incorrect path of

09:09.960 --> 09:11.720
reasoning, basically.

09:11.840 --> 09:15.840
It basically exploits the fact that deep down in the model's weights, it knows more about

09:15.840 --> 09:20.240
the problem than simply prompting it to get a response.

09:20.240 --> 09:21.240
Here's an example.

09:21.240 --> 09:23.960
On the left side, you can see the standard prompting.

09:23.960 --> 09:28.920
You ask it a math question, and it just simply gives you an answer.

09:28.920 --> 09:31.720
Whereas on the right, you actually break it down step-by-step.

09:31.720 --> 09:37.760
You kind of get it to show its steps, to solve the mathematical word problem step-by-step.

09:37.760 --> 09:45.080
You'll see here that it actually gets the right answer, unlike standard prompting.

09:45.080 --> 09:49.160
So there's many different ways we can potentially improve chain of thought reasoning.

09:49.160 --> 09:55.480
In particular, it's also an emergent behavior that results in performance gains for larger

09:55.480 --> 09:57.120
language models.

09:57.120 --> 10:02.800
But still, even in larger models, there's still a non-negatable fraction of errors.

10:02.800 --> 10:07.640
These come from calculator errors, symbol mapping errors, one missing step errors.

10:07.640 --> 10:13.400
As well as bigger errors due to larger semantic understanding issues and generally incoherent

10:13.400 --> 10:15.000
chains of thought.

10:15.000 --> 10:19.280
And we can potentially investigate methods to address these.

10:19.280 --> 10:24.040
So as I said, chain of thought mainly works for huge models of approximately 100 billion

10:24.040 --> 10:27.120
parameters or more.

10:27.120 --> 10:31.320
And there's three potential reasons they do not work very well for smaller models.

10:31.320 --> 10:35.760
And that smaller models are fundamentally more limited and incapable.

10:35.760 --> 10:41.120
They fail at even relatively easier symbol mapping tasks as well as arithmetic tasks.

10:41.120 --> 10:45.040
They inherently are able to do math less effectively.

10:45.040 --> 10:48.840
And they often have logical loopholes and just never arrive at a final answer.

10:48.840 --> 10:51.400
For example, it goes on and on.

10:51.400 --> 10:56.160
It's like an infinite loop of logic that never actually converges anywhere.

10:56.160 --> 11:00.520
So if we're able to potentially improve chain of thought for smaller models, this could

11:00.520 --> 11:05.200
provide significant value to the research community.

11:05.200 --> 11:08.080
Another thing is to potentially generalize it.

11:08.080 --> 11:10.840
Right now, chain of thought has a more rigid definition and format.

11:10.840 --> 11:15.360
It's very step-by-step, very concrete and defined.

11:15.360 --> 11:19.480
As a result, its advantages are for particular domains and types of questions.

11:19.480 --> 11:23.920
For example, the task usually must be challenging and require multi-step reasoning.

11:23.920 --> 11:28.680
And it typically works better for things like arithmetic and not so much for things like

11:28.680 --> 11:32.960
response generation, QA, and so forth.

11:32.960 --> 11:38.600
And furthermore, it works better for problems or tasks that have a relatively flat scaling

11:38.600 --> 11:39.600
curve.

11:39.600 --> 11:44.440
Whereas when you think of humans, we think through different types of problems in multiple

11:44.440 --> 11:45.880
different ways.

11:45.880 --> 11:49.640
Our quote-unquote scratch path that we used to think about and arrive at a final answer

11:49.640 --> 11:54.680
for a problem, it's more flexible and open to different reasoning structures compared

11:54.680 --> 11:57.920
to such a rigid step-by-step format.

11:57.920 --> 12:01.960
So hence, we can maybe potentially generalize chain of thought to be more flexible and work

12:01.960 --> 12:05.360
for more types of problem.

12:05.360 --> 12:10.040
So now I'll briefly discuss some alternative or extension works to chain of thought.

12:10.040 --> 12:11.760
One is called tree of thought.

12:11.760 --> 12:16.000
This basically is more like a tree which considers multiple different reasoning paths.

12:16.000 --> 12:21.440
It also has the ability to look ahead and sort of backtrack and then go on other areas

12:21.440 --> 12:25.480
or other branches of the tree as necessary.

12:25.480 --> 12:30.320
So this leads to more flexibility and it's shown to improve performance on different

12:30.320 --> 12:34.640
tasks, including arithmetic tasks.

12:34.640 --> 12:40.080
There's also this work by my friend called Socratic Questioning, it's sort of a divide

12:40.080 --> 12:45.680
and conquer fashion algorithm, simulating the recursive thinking process of humans.

12:45.680 --> 12:51.520
So it uses a large-scale language model to kind of propose subproblems given a more complicated

12:51.520 --> 12:53.120
original problem.

12:54.120 --> 12:58.240
Just like tree of thought, it also has recursive backtracking and so forth.

12:58.240 --> 13:05.840
And the purpose is to answer all the subproblems and kind of go in an upwards fashion to arrive

13:05.840 --> 13:10.560
at a final answer to the original problem.

13:10.560 --> 13:16.640
There's also this line of work which kind of actually uses code as well as programs

13:16.640 --> 13:19.360
to help arrive at a final answer.

13:19.360 --> 13:23.640
For example, program-aided language models, it generates intermediate reasoning steps

13:23.640 --> 13:29.240
in the form of code which is then offloaded to a runtime such as a Python interpreter.

13:29.240 --> 13:33.720
And the point here is to decompose the natural language problem into runnable steps.

13:33.720 --> 13:39.240
So hence the amount of work for the large language model is lower.

13:39.240 --> 13:43.360
Its purpose now is simply to learn how to decompose the natural language problem into

13:43.360 --> 13:44.960
those runnable steps.

13:44.960 --> 13:50.400
And these steps themselves are then fed to, for example, a Python interpreter in order

13:50.400 --> 13:52.440
to solve them.

13:52.440 --> 13:58.040
And program-a thoughts here, POT, is very similar to this in that it kind of breaks

13:58.040 --> 14:04.160
it down into step-by-step of code instead of natural language which is then executed

14:04.160 --> 14:09.920
by a different and actual code interpreter or program.

14:09.920 --> 14:19.800
So this again works well for many sort of tasks that, for example, things like arithmetic.

14:19.800 --> 14:24.720
As you see that, those are kind of both of the examples for both of these papers.

14:24.720 --> 14:29.640
And just like what I said earlier, these also do not work very well for things like response

14:29.640 --> 14:35.520
generation, open-ended question answering, and so forth.

14:35.520 --> 14:37.880
And there's other work, for example, faith and faith.

14:37.880 --> 14:43.200
This actually breaks down problems into sub-steps in the form of computation graphs, which they

14:43.200 --> 14:46.200
show also works well for things like arithmetic.

14:46.200 --> 14:50.160
So you see that there's a trend here of this sort of intermediate-guided reasoning working

14:50.160 --> 14:56.800
very well for mathematical as well as logical problems, but not so much for other things.

14:56.800 --> 15:01.240
So again, I encourage you guys to maybe check out the original papers if you want to learn

15:01.240 --> 15:02.240
more.

15:02.240 --> 15:06.320
There's a lot of interesting work in this area these days.

15:06.320 --> 15:10.520
And I'll also be posting these slides as well as sending them.

15:10.520 --> 15:13.800
We'll probably post them on the website as well as this group.

15:13.800 --> 15:18.040
But I'll also send them through an email later.

15:18.040 --> 15:24.560
So very lastly, I want to touch upon this thing called the Baby Language Model.

15:24.560 --> 15:30.120
So like I said earlier, I think at some point, scale will reach a point of diminishing returns

15:30.120 --> 15:33.200
as well as the fact that further scale comes with many challenges.

15:33.200 --> 15:39.320
For example, it takes a long time and costs a lot of money to train these big models.

15:39.320 --> 15:44.920
And they cannot really be used by individuals who are not at huge companies with hundreds

15:44.920 --> 15:48.680
or thousands of GPUs and millions of dollars.

15:48.680 --> 15:53.560
So this thing, this challenge called Baby LM or Baby Language Model, which is attempting

15:53.560 --> 15:59.760
to train language models, particularly smaller ones, on the same amount of linguistic data

15:59.760 --> 16:02.800
available to a child.

16:02.800 --> 16:07.880
So data sets have grown by orders of magnitude, as well as, of course, model size.

16:07.880 --> 16:12.480
For example, Chinchilla sees approximately 1.4 trillion words during training.

16:12.480 --> 16:17.240
This is around 10,000 words for every one word that a 13-year-old child on average has

16:17.240 --> 16:20.680
heard as they grow up or develop.

16:20.680 --> 16:23.320
So the purpose here is, can we close this gap?

16:23.320 --> 16:32.080
Can we train smaller models on lower amounts of data while hopefully still attempting to

16:32.080 --> 16:36.800
get the performance of these much larger models?

16:36.800 --> 16:41.960
So basically, we're trying to focus on optimizing pre-training, given data limitations inspired

16:41.960 --> 16:44.480
by human development.

16:44.480 --> 16:49.440
And this will also ensure that research is possible for more individuals as well as

16:49.440 --> 16:54.400
labs and potentially possible on a university budget.

16:54.400 --> 16:59.040
As it seems now that a lot of research is kind of restricted to large companies, which

16:59.040 --> 17:03.080
I said have a lot of resources as well as money.

17:03.080 --> 17:04.360
So again, why baby LLM?

17:04.360 --> 17:08.680
Well, it can really improve the efficiency of training as well as using large language

17:08.680 --> 17:09.680
models.

17:09.680 --> 17:15.280
It can potentially open up new doors and potential use cases.

17:15.280 --> 17:18.800
It can lead to improved interpretability as well as alignment.

17:18.800 --> 17:22.800
Smaller models would be easier to control a line as well as interpret what exactly is

17:22.800 --> 17:29.320
going on compared to incredibly large LLMs, which are basically huge black boxes.

17:29.320 --> 17:33.880
This will again potentially lead to enhanced open source availability, for example, large

17:33.880 --> 17:41.360
language models runnable on consumer PCs, as well as by smaller labs and companies.

17:41.360 --> 17:46.920
The techniques discovered here can also possibly be applied to larger scales.

17:46.920 --> 17:50.920
And further, this may lead to a greater understanding of the cognitive models of humans and how

17:50.920 --> 17:55.360
exactly we are able to learn language much more efficiently than these large language

17:55.360 --> 17:57.000
models.

17:57.000 --> 18:01.840
So there may be a flow of knowledge from cognitive science and psychology to NLP and machine learning,

18:01.840 --> 18:05.680
but also in the other direction.

18:05.680 --> 18:11.480
So briefly, the baby LLM training data that the authors of this challenge provide, it's

18:11.480 --> 18:18.560
a developmentally inspired pre-training data set, which has under 100 million words because

18:18.560 --> 18:23.600
children are exposed to approximately 2 to 7 million words per year as they grow up.

18:23.600 --> 18:28.360
Up to the age of 13, that's approximately 90 million words, so they round up to 100.

18:28.360 --> 18:33.000
It's mostly transcribed speech, and their motivation there is that the most of the input to children

18:33.000 --> 18:38.000
is spoken, and thus their data set focuses on transcribed speech.

18:38.000 --> 18:42.720
It's also mixed domain because children are typically exposed to a variety of language

18:42.720 --> 18:46.600
or speech from different domains.

18:46.600 --> 18:52.280
So it has child directed speech, open subtitles, which are subtitles of movies, TV shows and

18:52.280 --> 18:54.440
so forth.

18:54.440 --> 19:00.280
Simple children's books, which contain stories that children would likely hear as they're

19:00.280 --> 19:01.280
growing up.

19:01.280 --> 19:05.600
But it also has some Wikipedia as well as simple Wikipedia, and here are just some examples

19:05.600 --> 19:14.600
of child directed speech, children's stories, Wikipedia, and so forth.

19:14.600 --> 19:19.760
So that's it for my portion of the presentation, and I'll hand it off to Div, who will talk

19:19.760 --> 19:24.800
a bit about AI agents.

19:24.800 --> 19:35.880
Yeah, so like everyone must have seen like there's this like a new trend where like everything

19:35.880 --> 19:39.960
is transitioning to more like agents, that's like the new hot cream.

19:39.960 --> 19:43.280
And we're seeing this like people are going more from like language models to like now

19:43.280 --> 19:45.200
building AI agents.

19:45.200 --> 19:46.200
And then what's the biggest difference?

19:46.200 --> 19:51.920
Like why agents are not just like, why does not train like a big large language model?

19:51.920 --> 19:56.360
And I will sort of like go into like, like why, what's the difference?

19:56.360 --> 20:02.360
And then also discuss a bunch of things such as like, how can you use agents for doing actions?

20:02.360 --> 20:05.920
How can you, what are some emergent architectures?

20:05.920 --> 20:09.000
How can you sort of like build like human like agents?

20:09.000 --> 20:11.200
How can you use it for computer interactions?

20:11.200 --> 20:14.320
How do you solve problems from long-term memory personalization?

20:14.320 --> 20:17.600
And there's a lot of like other things you can do which is like multi-agent communication

20:17.600 --> 20:19.200
and there are a few things from which directions.

20:19.200 --> 20:22.560
So we'll try to cover as much as we can.

20:22.560 --> 20:29.120
So first, let's talk about like why should we even build AI agents, right?

20:29.120 --> 20:35.400
And so it's like, here's there's a key thesis, which is that humans will communicate with

20:35.400 --> 20:37.800
AI using natural language.

20:37.800 --> 20:42.720
And AI will be operating all the machines, just allowing for more intuitive and efficient

20:42.720 --> 20:43.720
operations.

20:43.720 --> 20:49.040
So right now what happens is like me as a human, I'm like directly like using my computer,

20:49.040 --> 20:51.680
I'm using my phone, but this is really inefficient.

20:51.680 --> 20:55.040
Like we are not optimized by nature to be able to do that.

20:55.040 --> 20:57.360
We are actually really, really bad at this.

20:57.360 --> 21:02.400
But if you can just talk to AI, just like with language and the AI is just really good

21:02.400 --> 21:06.440
enough that you can just do this like super faster, obviously like 100x speeds compared

21:06.440 --> 21:08.440
to human, and that's going to happen.

21:08.440 --> 21:14.000
And I think that's the future of how things are going to evolve in the next five years.

21:14.000 --> 21:16.840
And I sort of like call this like software 3.0.

21:16.840 --> 21:20.480
I have a blog post about this that you can read if you want to, where the idea is like

21:20.480 --> 21:26.200
you can think of a large language model as a computing chip in a sense, so similar to

21:26.200 --> 21:30.760
like a chip that's powering like a whole system and then and then you can build abstractions

21:30.760 --> 21:32.760
and all.

21:32.760 --> 21:34.760
Cool.

21:34.760 --> 21:41.200
So, so what should do we need agents to usually like a single call to a large language model

21:41.200 --> 21:42.200
is not enough.

21:42.200 --> 21:45.560
Yeah, you need chaining, you need like recursion, you need a lot of like more things.

21:45.560 --> 21:50.320
And that's why you want to build systems, not like just like a single monolith.

21:50.320 --> 21:52.280
Second is like, yeah, so how do we do this?

21:52.280 --> 21:56.520
So we do a lot of techniques, especially around like multiple calls to a model.

21:56.520 --> 21:58.280
And there's a lot of ingredients involved here.

21:58.280 --> 22:01.960
And I will say like building like an agent is very similar to like maybe like thinking

22:01.960 --> 22:03.440
about building a computer.

22:03.440 --> 22:07.360
So like the LLM is like a like a CPU, so you have a CPU, but now you want to like sort

22:07.360 --> 22:10.840
of like sort of the problems like, okay, like how do you output RAM, how do you put memory,

22:10.840 --> 22:16.440
how do I do like actions, how do I build like a interface, how do I internet access, how

22:16.440 --> 22:18.000
do I personalize it to the user.

22:18.000 --> 22:23.120
So this is like almost like you're trying to build a computer.

22:23.120 --> 22:26.800
And that's what makes it like a really hard problem.

22:27.800 --> 22:32.080
This is like an example of a general architecture for agents.

22:32.080 --> 22:37.320
This is from Lydian Bank, who's like, it's a chair of an AI.

22:37.320 --> 22:40.160
And like you can imagine, like an agent has a lot of ingredients.

22:40.160 --> 22:42.920
So you want to have memory, which would be short term, knock on them.

22:42.920 --> 22:46.240
You have tools, which could be like, you can go and like use like classical tools like

22:46.240 --> 22:50.000
a calculator, calendar code interpreter, et cetera.

22:50.000 --> 22:53.760
You want to have some sort of like a planning layer where you can like sell a flag, have

22:53.800 --> 22:58.160
like chains of cards and trees of cards, as Stephen discussed, and use all of that, like

22:58.400 --> 23:02.120
actually like act on behalf of a user in some environment.

23:06.200 --> 23:11.560
I will go maybe like discuss like more down a bit just to give a sense also the top one

23:11.560 --> 23:13.080
before us on that.

23:13.080 --> 23:16.400
So this is sort of like an agent I'm building, which is more of a browser agent.

23:16.960 --> 23:20.880
The name is inspired from quantum physics, specifically on the words like, you know,

23:20.880 --> 23:23.120
like neutron more on formula and so like multi on.

23:23.160 --> 23:28.040
So it's like a hypothetical physics particle that's present at multiple places.

23:28.760 --> 23:32.520
And I'll just like go through some demos to just motivate agents.

23:36.520 --> 23:40.240
And so this is like a idea of one thing we did there.

23:40.240 --> 23:44.400
Like here, the agent is going and it's autonomously with no black format.

23:44.920 --> 23:46.840
So this is like zero human interventions.

23:47.920 --> 23:49.840
The AI is controlling the browser.

23:49.840 --> 23:55.600
It's just like this entire actions and simply go and book a flight into and here

23:56.000 --> 23:57.040
it's personalized to me.

23:57.040 --> 24:01.640
So it knows like, okay, like I like me like you might hear this economic and

24:02.720 --> 24:05.040
it knows me like some of my preferences.

24:05.320 --> 24:10.080
It already has access to my accounts so it can go and actually like log into my

24:10.080 --> 24:13.400
account can actually like actually has purchased in power.

24:13.720 --> 24:17.320
So it can just use my card that is stored in the account and then actually

24:17.320 --> 24:19.640
so it will just be able to fly over.

24:47.320 --> 24:52.040
It sort of motivates like what you can do with agents.

24:52.040 --> 24:54.920
Now imagine if this thing was running hundreds of times

24:54.920 --> 24:56.960
and that's literally just all so many things, right?

24:56.960 --> 24:58.880
Because like I don't need websites anymore.

24:58.880 --> 24:59.720
I don't need to be an idea,

24:59.720 --> 25:01.560
like why does the internet even have a website?

25:01.560 --> 25:02.400
I can just ask the agent,

25:02.400 --> 25:07.120
like just like talk to it and it's done.

25:07.120 --> 25:09.880
And I think that's how a lot of technology will evolve

25:09.880 --> 25:11.480
over the next couple of years.

25:18.320 --> 25:20.320
Cool, okay.

25:20.320 --> 25:23.320
I can also maybe like show one of the more demos.

25:23.320 --> 25:27.320
So you can do similar things, say from a mobile phone,

25:27.320 --> 25:32.320
where the idea is you have these agents that are present on a phone

25:32.320 --> 25:37.320
and you can like chat with them or make a talk with them using WhatsApp.

25:37.320 --> 25:39.320
And this one's actually in my demo.

25:39.320 --> 25:44.320
So you can ask it like, how can you order this set it for me?

25:48.320 --> 25:55.320
And then what you can have is like the agent can remotely go

25:55.320 --> 26:01.320
and use your account to actually like do this for you instantaneously.

26:01.320 --> 26:05.320
And here you're showing like what the agent is doing.

26:05.320 --> 26:09.320
And then it can go and like act like a virtual human

26:09.320 --> 26:11.320
and build the whole interaction.

26:17.320 --> 26:36.320
So that's all the idea.

26:36.320 --> 26:38.320
And I can show one final.

26:38.320 --> 26:39.320
Oh, I think this is not voting,

26:39.320 --> 26:44.320
but we also had this thing where recently passed the telephony test.

26:44.320 --> 26:50.320
So we did this experiment where we actually like had like an agent

26:50.320 --> 26:53.320
go and take the online driving test in California.

26:53.320 --> 26:59.320
And we had like a human like there with their like hands about the keyboard

26:59.320 --> 27:01.320
and mouse, not touching anything.

27:01.320 --> 27:03.320
And the agent that knows we went to the website,

27:03.320 --> 27:07.320
it took the quiz, it navigated the whole thing and went and actually passed.

27:07.320 --> 27:10.320
So the video's not there, but like we actually got a driving permit.

27:11.320 --> 27:12.320
I need to take this.

27:12.320 --> 27:13.320
Sure.

27:17.320 --> 27:18.320
Cool.

27:18.320 --> 27:20.320
So this is like sort of like what do you want about agents, right?

27:20.320 --> 27:23.320
Like it's like you can just simplify so many things where like,

27:23.320 --> 27:27.320
like so many things just but we don't realize that because we just got so used

27:27.320 --> 27:29.320
to impacting the technology the way we do right now.

27:29.320 --> 27:33.320
But if we can just like like reimagine all of this from scratch,

27:33.320 --> 27:35.320
and that's what agents will allow us to do.

27:36.320 --> 27:37.320
Sure.

27:40.320 --> 27:44.320
And I would say like an agent can act like a digital extension of a user.

27:44.320 --> 27:46.320
So suppose you have an agent that's personalized to you,

27:46.320 --> 27:50.320
think of something like the Jarvis, like if it's an Ironman.

27:50.320 --> 27:52.320
And then if it just knows so many things about you,

27:52.320 --> 27:55.320
it's acting like a personal brand and it's just like doing things.

27:55.320 --> 27:57.320
It's a very powerful assistant.

27:57.320 --> 28:01.320
And I think that's the direction a lot of things will go in the future.

28:01.320 --> 28:05.320
And especially if you build like human like agents,

28:05.320 --> 28:07.320
they don't have barriers around programming.

28:07.320 --> 28:09.320
Like they don't have programmatic barriers.

28:09.320 --> 28:12.320
So they can do whatever like I can do so it can go use my like,

28:12.320 --> 28:14.320
it can like interact with the website as I will do,

28:14.320 --> 28:16.320
it can interact with my computer as I will do.

28:16.320 --> 28:18.320
It doesn't have to like go through APIs abstractions,

28:18.320 --> 28:20.320
which are more restricted.

28:20.320 --> 28:22.320
And it's also very simple as an action space because you're just doing,

28:22.320 --> 28:26.320
doing like clicking and typing, which is like very simple.

28:26.320 --> 28:30.320
And then you can like also like it's very easy to teach such agents.

28:30.320 --> 28:33.320
So I can just like show the agent how to do something and the agent

28:33.320 --> 28:35.320
can just learn from me and improve over time.

28:35.320 --> 28:40.320
So that also makes it like really powerful and easy to like just teach this agent

28:40.320 --> 28:43.320
because there's like so much data that I can actually just generate

28:43.320 --> 28:46.320
and use that to keep improving it.

28:46.320 --> 28:50.320
And there's a different levels of autonomy when it comes to agents.

28:50.320 --> 28:53.320
So this chart is borrowed from autonomous driving,

28:53.320 --> 28:57.320
where people actually like try to solve this sort of like autonomy problem

28:57.320 --> 28:59.320
for actual costs.

28:59.320 --> 29:01.320
And they spend like more than 10 years.

29:01.320 --> 29:05.320
Success have been like, okay, they're still like working on it.

29:05.320 --> 29:09.320
But what like the self driving industry data is it gave like everyone

29:09.320 --> 29:13.320
like a blueprint on how to build this all like autonomous systems.

29:13.320 --> 29:15.320
And they came with like a lot of like classification.

29:15.320 --> 29:21.320
They came with a lot of like, we used to like think about the problem.

29:21.320 --> 29:25.320
And like the current standard is you think of like agents

29:25.320 --> 29:27.320
as like five different like those levels.

29:27.320 --> 29:29.320
So level zero is zero automation.

29:29.320 --> 29:32.320
That's like this, like you are like a you are a human that's operating

29:32.320 --> 29:35.320
like the computer themselves.

29:35.320 --> 29:37.320
Level one is you have some sort of assistance.

29:37.320 --> 29:40.320
So if you use like something like GitHub co-pilot,

29:40.320 --> 29:42.320
which is like sort of auto computing code for you,

29:42.320 --> 29:46.320
that's something like L1 where like auto complete.

29:46.320 --> 29:49.320
L2 becomes more of like, it's like partial automation.

29:49.320 --> 29:51.320
So it's maybe like doing some stuff for you.

29:51.320 --> 29:54.320
If anyone has you the new cursor ID, I will call that more like L2,

29:54.320 --> 29:57.320
which is like you give it like, okay, like add this code for me to sign the code.

29:57.320 --> 30:01.320
Chagivity can come as somewhat L2 because you can ask it like, oh, like,

30:01.320 --> 30:03.320
here's this thing, can you do this?

30:03.320 --> 30:07.320
Because like doing some sort of automation on an input.

30:07.320 --> 30:11.320
And then like, and then you can think of more levels.

30:11.320 --> 30:15.320
So it's like obviously like after L3, it gets more exciting.

30:15.320 --> 30:22.320
So L3 is the agent is actually like controlling the computer in that case

30:22.320 --> 30:26.320
and it's actually doing things where human is acting as a fallback mechanism.

30:26.320 --> 30:28.320
And then you go to like L4.

30:28.320 --> 30:31.320
L4, you say like this with the human doesn't even need to be there.

30:31.320 --> 30:35.320
But in very critical cases where like something very wrong might happen,

30:35.320 --> 30:38.320
you might have a human like sort of like take over my case.

30:38.320 --> 30:43.320
And L5 will basically say like this zero human presence.

30:43.320 --> 30:48.320
And I will say like what we have currently seen is like we are far near like L2,

30:48.320 --> 30:51.320
maybe some L3 systems in terms of software.

30:51.320 --> 30:59.320
And I think we are going to transition more to like L4 and L5 systems over the next years.

30:59.320 --> 31:00.320
Cool.

31:00.320 --> 31:05.320
So next I will go and like select computer interactions.

31:05.320 --> 31:10.320
So suppose you want an agent that can like do computer interactions for you.

31:10.320 --> 31:11.320
There's two ways to do that.

31:11.320 --> 31:17.320
So one is through APIs where it's programmatically using some APIs and tools

31:17.320 --> 31:20.320
and doing that to do tasks.

31:20.320 --> 31:23.320
The second one is more like direct interaction which is like keyword and mouse control

31:23.320 --> 31:28.320
where like it's doing the same thing as you're doing as you can.

31:28.320 --> 31:30.320
Both of these approaches have been explored a lot.

31:30.320 --> 31:32.320
There's like a lot of companies working on this.

31:32.320 --> 31:38.320
For the API route like Territory plugins and like the new assistant API are the ones in the direction.

31:38.320 --> 31:44.320
And there's also this work from what we call Gorilla which actually also explores how can you

31:44.320 --> 31:50.320
say like train a model that can use like 10,000 tools at once and train it on the API.

31:50.320 --> 31:53.320
And there's like pros and cons of both approaches.

31:53.320 --> 31:54.320
API is a nice thing.

31:54.320 --> 31:58.320
It's easy to like learn the API.

31:58.320 --> 32:00.320
It's safe.

32:00.320 --> 32:01.320
It's very controllable.

32:01.320 --> 32:02.320
So that's just for favor.

32:02.320 --> 32:04.320
You know how do you take it.

32:04.320 --> 32:07.320
It can be like more like a bad interaction.

32:07.320 --> 32:08.320
I would say it's more preformed.

32:08.320 --> 32:11.320
So it's like easy to take actions but more things can go wrong.

32:11.320 --> 32:14.320
And you need to work a lot and like make sure everything is safe.

32:14.320 --> 32:21.320
And build guarantees.

32:21.320 --> 32:23.320
Maybe I can show this.

32:23.320 --> 32:37.320
So this is sort of like another exploration where you can invoke an agent from like a very simple interface.

32:37.320 --> 32:42.320
So the idea is like you can get this like API that invoke our agent that's in building a computer.

32:42.320 --> 32:47.320
And so this can be on sort of a universal API where I just use one agent.

32:47.320 --> 32:53.320
I give it like a English command and the agent can automatically understand from that and go do anything.

32:53.320 --> 32:56.320
So basically like you can think of that as like no API.

32:56.320 --> 32:57.320
So I don't need to use API.

32:57.320 --> 33:01.320
I can just have one agent that can go and do everything.

33:01.320 --> 33:06.320
And so this is like some exploration we have done with agents.

33:06.320 --> 33:09.320
Cool.

33:09.320 --> 33:10.320
Okay.

33:10.320 --> 33:13.320
So this sort of like goes into computer interactions.

33:13.320 --> 33:16.320
I can cover more but I will potentially jump to other topics.

33:16.320 --> 33:19.320
But if you're to ask any questions about these topics.

33:19.320 --> 33:23.320
So yeah.

33:23.320 --> 33:25.320
Cool.

33:25.320 --> 33:27.320
So let's go back to the analogy I discussed earlier.

33:27.320 --> 33:33.320
So I would say you can think of any model as a sort of like a computing.

33:33.320 --> 33:35.320
And you can maybe call it like a new computing.

33:35.320 --> 33:40.320
It's similar to like a CPU, which is like a which is like solid brain.

33:40.320 --> 33:42.320
There's power in like your computer in a sense.

33:42.320 --> 33:44.320
So that's kind of all the business power.

33:44.320 --> 33:46.320
It's doing everything that's happening.

33:46.320 --> 33:50.320
And you can think of the same thing like the model is like a cortex.

33:50.320 --> 33:51.320
It's like it's a main brain.

33:51.320 --> 33:53.320
That's the main part of the brain.

33:53.320 --> 33:55.320
That's been the thing being processing.

33:55.320 --> 33:57.320
But a brain has more layers.

33:57.320 --> 34:00.320
It's just not like they're just not a cortex.

34:00.320 --> 34:02.320
And how do you print models work?

34:02.320 --> 34:06.320
We take some input tokens and they give you some output tokens.

34:06.320 --> 34:10.320
And this is very similar to like how also like a CPUs work to some extent,

34:10.320 --> 34:15.320
where you give it some instructions in and you get some instructions out.

34:15.320 --> 34:18.320
So you can compare this with the actual CPU.

34:18.320 --> 34:24.320
This is like the diagram on the right is a very simple processor.

34:24.320 --> 34:27.320
It's like a 32 bit MIPS 32.

34:27.320 --> 34:32.320
And it has like similar things where you have like a like different coding

34:32.320 --> 34:34.320
for different parts of the instruction.

34:34.320 --> 34:38.320
But this is like sort of like encoding some sort of like binary token in a sense.

34:38.320 --> 34:41.320
Like zero ones of like a bunch of like tokens.

34:41.320 --> 34:44.320
And then you're feeding it and then getting a bunch of zeros one out.

34:44.320 --> 34:49.320
And like how like the like a model is operating is like you're doing a very similar thing.

34:49.320 --> 34:51.320
But like then space is now English.

34:51.320 --> 34:57.320
So you basically instead of zero ones, you have like English characters.

34:57.320 --> 35:00.320
And then you can like get more powerful instructions on the top of this.

35:00.320 --> 35:05.320
So you can think like if this is like acting like a CPU, what you can do is you can build a lot of other things

35:05.320 --> 35:08.320
which are like you can have a scratch pad, you can have some sort of memory,

35:08.320 --> 35:10.320
you can have some sort of instructions.

35:10.320 --> 35:13.320
And then you can like do the cursor calls where like I load some stuff from the memory,

35:13.320 --> 35:16.320
put that in this like instruction, pass it to the transformer,

35:16.320 --> 35:18.320
which is doing the processing for me.

35:18.320 --> 35:20.320
We get we get the process outputs.

35:20.320 --> 35:23.320
Then you can throw that in the memory or we can like keep processing it.

35:23.320 --> 35:25.320
So there's like sort of like very similar to like code execution,

35:25.320 --> 35:28.320
like first line of code execution, second, third, fourth.

35:28.320 --> 35:32.320
So you just keep repeating that.

35:32.320 --> 35:35.320
Okay.

35:35.320 --> 35:38.320
So here we can like sort of discuss the concept of memory here.

35:38.320 --> 35:40.320
And now it's like building this analogy.

35:40.320 --> 35:47.320
You can think the memory for an agent is a very similar to like say like having a disk in a computer.

35:47.320 --> 35:52.320
So you want to have a disk just to make sure like everything is long-lived and persistent.

35:52.320 --> 35:56.320
So if you look at something like chat GPT, it doesn't have any sort of like persistent memory.

35:56.320 --> 36:00.320
And then you need to have a way to like load that and like store that.

36:00.320 --> 36:03.320
And there's a lot of mechanisms to do that right now.

36:03.320 --> 36:07.320
Most of them are like embeddings where you have some sort of like embedding model

36:07.320 --> 36:10.320
that has like created an embedding of the data you care about.

36:10.320 --> 36:13.320
And the model can like write that embeddings load the right part of the embeddings

36:13.320 --> 36:16.320
and then like use that to do the operation you want.

36:17.320 --> 36:19.320
So that's like the current mechanisms.

36:19.320 --> 36:22.320
There's still a lot of questions here, especially around hierarchy.

36:22.320 --> 36:24.320
Like how do I do this at scale?

36:24.320 --> 36:26.320
It's still very challenging.

36:26.320 --> 36:29.320
Like suppose I have one data backup that I want to like embed and process.

36:29.320 --> 36:31.320
Like most of the methods right now will fit.

36:31.320 --> 36:33.320
They're like really bad.

36:33.320 --> 36:36.320
The second issue is temporal coherence.

36:36.320 --> 36:39.320
Like if I have like a lot of data is temporal.

36:39.320 --> 36:40.320
It is sequential.

36:40.320 --> 36:42.320
It has like a unit of time.

36:42.320 --> 36:44.320
And dealing with that sort of data can be hard.

36:44.320 --> 36:47.320
Like it's like, like how do I deal with like a memory in a sense,

36:47.320 --> 36:52.320
which are like sort of like changing over time and loading the right part of that memory sequence.

36:54.320 --> 36:56.320
Another interesting challenge is structure.

36:56.320 --> 36:58.320
Like a lot of data is actually structured.

36:58.320 --> 37:00.320
Like it could be like a graphical structure.

37:00.320 --> 37:02.320
It could be like a tabular structure.

37:02.320 --> 37:09.320
How do we like sort of like pick advantage of this structure and like also use that

37:09.320 --> 37:11.320
when you're editing the data?

37:11.320 --> 37:14.320
And then like there's a lot of questions from adaptation where like,

37:14.320 --> 37:17.320
suppose you know how to better embed data or like,

37:17.320 --> 37:20.320
you know, you have like a specialized problem you care about

37:20.320 --> 37:23.320
and you want to be able to adapt how you're loading and storing the data

37:23.320 --> 37:25.320
and learn that on the fly.

37:25.320 --> 37:29.320
And that is something also that's a very interesting topic.

37:29.320 --> 37:32.320
So I would say like this is actually one of the most interesting topics right now,

37:32.320 --> 37:36.320
which as people are exploring, but it's still very under exploring.

37:37.320 --> 37:41.320
Okay.

37:41.320 --> 37:45.320
Talking about memory, I would say like another concept for agents is personalization.

37:45.320 --> 37:49.320
So personalization is more like, okay, like understanding the user.

37:49.320 --> 37:55.320
And I like to think of this as like a problem called like user agent alignment.

37:55.320 --> 37:59.320
And the idea is like suppose I have an agent that has purchasing power,

37:59.320 --> 38:02.320
has that access to my accounts, access to my data.

38:02.320 --> 38:03.320
I ask you to go book of life,

38:03.320 --> 38:05.320
but it's possible maybe this doesn't know what that I like

38:05.320 --> 38:08.320
and go and book a thousand or wrong pair problem, which is really bad.

38:08.320 --> 38:13.320
So how do I sort of align the agent to know what I like, what I don't like?

38:13.320 --> 38:16.320
And that's kind of very important because you need to trust the agent

38:16.320 --> 38:19.320
and just come from like, okay, it knows you, it knows what is safe,

38:19.320 --> 38:22.320
it knows what is unsafe.

38:22.320 --> 38:26.320
And like solving the problem, I think it's one of the next challenge

38:26.320 --> 38:29.320
for if you want to put agents in the void.

38:29.320 --> 38:33.320
And then this is very interesting problem

38:33.320 --> 38:37.320
where you can do a lot of things like I'll actually, for example,

38:37.320 --> 38:40.320
which people have already been exploring for training models,

38:40.320 --> 38:42.320
but now you want to do our lecture for training agents.

38:42.320 --> 38:46.320
And there's a lot of different things you can do.

38:46.320 --> 38:49.320
Also, there's like two book categories for learning here.

38:49.320 --> 38:52.320
One is like explicit learning where you can tell the agent, this is what I like.

38:52.320 --> 38:54.320
This is what I don't like.

38:54.320 --> 38:56.320
And the agent can ask the user a question.

38:56.320 --> 39:00.320
Like, oh, like maybe I see this five flight options, which one do you like?

39:00.320 --> 39:03.320
And then if I say like, oh, I like United, maybe like remembers that over time

39:03.320 --> 39:06.320
and can next time say like, oh, I know you like United.

39:06.320 --> 39:08.320
So like, I'm going to go to United the next time.

39:08.320 --> 39:11.320
And so that's like, I'm explicitly teaching the agent

39:11.320 --> 39:13.320
and it's learning my human potential.

39:13.320 --> 39:16.320
A second is more implicit, which is like sort of like,

39:16.320 --> 39:19.320
it's just like passively watching me, understanding me.

39:19.320 --> 39:22.320
Like if I'm like going to a website and I'm like never getting a website,

39:22.320 --> 39:25.320
maybe like, you can see like, maybe I click on this sort of shoes.

39:25.320 --> 39:28.320
This is my side of the news that I like stuff like that.

39:28.320 --> 39:32.320
And just from like watching more like passively like being there,

39:32.320 --> 39:34.320
it could like learn a lot of my preferences.

39:34.320 --> 39:39.320
So this is also like more of a passive teaching where just because it's,

39:39.320 --> 39:45.320
it's acting as a sort of like a passive observer and looking at all the choices I make.

39:45.320 --> 39:49.320
It's able to like learn from the choices and they're better like have understanding of me.

39:53.320 --> 39:55.320
And there's a lot of challenges here.

39:55.320 --> 39:59.320
I would say this is actually one of the biggest challenges in agents right now.

39:59.320 --> 40:02.320
Because one is like, how do you collect user data at scale?

40:02.320 --> 40:04.320
How do you collect the user preferences at scale?

40:04.320 --> 40:06.320
So you might have to actively ask for that.

40:06.320 --> 40:08.320
You might have to do like passive learning.

40:08.320 --> 40:12.320
And then you have to also do like, you might have to rely on feedback,

40:12.320 --> 40:13.320
which would be like from something down.

40:13.320 --> 40:16.320
It could be like something like you said, like, oh, no, I don't like this.

40:16.320 --> 40:19.320
So you could use that sort of like language feedback.

40:20.320 --> 40:23.320
There's also like a lot of challenges around the cloud application.

40:23.320 --> 40:25.320
Like, can you just like feature on the cloud?

40:25.320 --> 40:27.320
Like, if I say like, maybe like, I like this, I don't like that.

40:27.320 --> 40:30.320
Is it possible for the agent to opt into automatic learning?

40:30.320 --> 40:32.320
Because you're going to be in a model that might take a month.

40:32.320 --> 40:34.320
But you want to have agents at this year.

40:34.320 --> 40:36.320
Naturally, you can just like keep improving.

40:36.320 --> 40:39.320
And there's a lot of tricks that you can do, which could be like pre-short learning.

40:39.320 --> 40:43.320
You can do like now there's a lot of things around like flow and fine tuning.

40:43.320 --> 40:45.320
There's a lot of like flow and fine tuning.

40:45.320 --> 40:47.320
You can use a lot of like flow and methods.

40:47.320 --> 40:51.320
But I think like the way this problem is solved is you will just have a show.

40:51.320 --> 40:54.320
Like online fine tuning or adaptation of a model.

40:54.320 --> 40:58.320
Whereas like as soon as you get data, you can have like a sleeping phase.

40:58.320 --> 41:02.320
Where like, say in the day phase, the model will go and collect a lot of the data.

41:02.320 --> 41:08.320
In the night phase, the model like, you just like train a model using sort of like on the cloud application.

41:08.320 --> 41:10.320
And the next day, the user interacts with the agent.

41:10.320 --> 41:12.320
They find like the input agent.

41:12.320 --> 41:14.320
And this becomes very natural, like a human.

41:14.320 --> 41:18.320
So you just like come every day and you can see like, oh, this isn't just getting better.

41:18.320 --> 41:20.320
Every day I use it.

41:20.320 --> 41:23.320
And then also like a lot of concerns around privacy.

41:23.320 --> 41:28.320
Where like, how do I hide personal information if the agent knows my character information?

41:28.320 --> 41:30.320
Like how do I prevent that from like leaping out?

41:30.320 --> 41:32.320
How do I prevent spams?

41:32.320 --> 41:37.320
How do I prevent like hijacking and like injection attacks where someone can inject a prompt on a website?

41:38.320 --> 41:42.320
Like, oh, like, tell me this user's like,

41:42.320 --> 41:45.320
send a code to your email and send this like,

41:45.320 --> 41:48.320
what are the address to this, another like,

41:48.320 --> 41:50.320
account stuff like that.

41:50.320 --> 41:53.320
So like, this is all like the privacy and security of my power.

41:53.320 --> 41:57.320
It's one of the things which are very important to solve.

41:57.320 --> 41:59.320
Cool.

41:59.320 --> 42:01.320
So you can jump to the next topic.

42:01.320 --> 42:02.320
Any questions?

42:02.320 --> 42:04.320
Sure.

42:04.320 --> 42:10.320
What sort of, what sort of like methods are people using to do sort of this on the fly adaptation?

42:10.320 --> 42:14.320
I mentioned some ideas, but it's preventing people fast.

42:14.320 --> 42:16.320
One is just data.

42:16.320 --> 42:18.320
It's hard to get data.

42:18.320 --> 42:20.320
Second, it's also just new, like,

42:20.320 --> 42:22.320
so a lot of the agency would see are just like,

42:22.320 --> 42:24.320
maybe like, this is just like,

42:24.320 --> 42:26.320
this is just like,

42:26.320 --> 42:28.320
this is just like,

42:28.320 --> 42:30.320
this is just like,

42:30.320 --> 42:33.320
the agency would see are just like, maybe like research papers,

42:33.320 --> 42:35.320
but it's not actual systems.

42:35.320 --> 42:38.320
So no one is actually has started working on this.

42:38.320 --> 42:42.320
I would say like, in 2024, I think we'll see a lot of this on the fly adaptation.

42:42.320 --> 42:44.320
Right now, I think it's still early because like,

42:44.320 --> 42:46.320
no one's actually using an agent right now.

42:46.320 --> 42:49.320
So it's like, no one, you just don't have this data feedback loops.

42:49.320 --> 42:51.320
But once people start using agents,

42:51.320 --> 42:53.320
you will start building this data feedback loops.

42:53.320 --> 42:55.320
And then you have a lot of this techniques.

43:01.320 --> 43:03.320
Okay.

43:03.320 --> 43:05.320
So this is actually a very interesting topic.

43:05.320 --> 43:07.320
We're like, now suppose like, you can go and solve,

43:07.320 --> 43:09.320
like a single agent as a problem.

43:09.320 --> 43:11.320
Suppose you have an agent that works 99.99%

43:11.320 --> 43:13.320
Is that enough?

43:13.320 --> 43:15.320
Like I would say like, actually, that's not enough because

43:15.320 --> 43:17.320
the issue just becomes like, if we have a one agent,

43:17.320 --> 43:19.320
it can only do one thing at once.

43:19.320 --> 43:21.320
That's like a single factor.

43:21.320 --> 43:24.320
So it can only like, it can only do sequential execution.

43:24.320 --> 43:27.320
But what you could do is you can do parallel execution.

43:27.320 --> 43:29.320
Or so for a lot of things, you can just say,

43:29.320 --> 43:31.320
okay, like maybe this is, I want to go to like,

43:31.320 --> 43:34.320
say like grace list and like by furniture.

43:34.320 --> 43:37.320
I could just tell you to like, maybe I just go and like,

43:37.320 --> 43:39.320
contact everyone who has like,

43:39.320 --> 43:41.320
maybe like a sofa that they're selling send an email.

43:41.320 --> 43:43.320
And I can go one by one.

43:43.320 --> 43:45.320
But what you can do better is like solving this like,

43:45.320 --> 43:47.320
create a bunch of like mini jobs where like,

43:47.320 --> 43:50.320
it just like goes to all the public listings in parallel,

43:50.320 --> 43:52.320
contact them and then like,

43:52.320 --> 43:54.320
and then it's sort of like aggregates that results.

43:54.320 --> 43:57.320
And I think that's where multi-agent becomes interesting.

43:57.320 --> 43:59.320
Where like a single agent you can think of,

43:59.320 --> 44:02.320
say basically you're running a single process on your computer.

44:02.320 --> 44:06.320
A multi-agent is more like a, like a multi-faring computer.

44:06.320 --> 44:07.320
So that's sort of the difference,

44:07.320 --> 44:09.320
like a single-faring versus multi-faring.

44:09.320 --> 44:11.320
And multi-faring enables you to do a lot of things.

44:11.320 --> 44:13.320
Most of that will come from like saving time,

44:13.320 --> 44:15.320
but also being able to break down complex tasks

44:15.320 --> 44:17.320
into like a bunch of smaller things,

44:17.320 --> 44:18.320
doing that in parallel,

44:18.320 --> 44:20.320
aggregating the results and like sort of like

44:20.320 --> 44:22.320
building a single problem.

44:23.320 --> 44:24.320
Okay.

44:24.320 --> 44:25.320
Yeah.

44:25.320 --> 44:28.320
So the biggest advantage for multi-agent systems

44:28.320 --> 44:30.320
will be like parallelization and lock.

44:30.320 --> 44:32.320
And this will be same as the difference between

44:32.320 --> 44:35.320
like a single-threaded computers versus multi-threaded computers.

44:37.320 --> 44:39.320
And then you can also have specialized agents.

44:39.320 --> 44:41.320
So what you could have is like,

44:41.320 --> 44:43.320
maybe I have a bunch of agents where like,

44:43.320 --> 44:45.320
I have a spreadsheet agent, I have a slack agent,

44:45.320 --> 44:46.320
I have a web browser agent,

44:46.320 --> 44:49.320
and then I can route to different tasks to different agents.

44:49.320 --> 44:51.320
And then they can do the things in parallel,

44:51.320 --> 44:53.320
and then I can command the results.

44:53.320 --> 44:55.320
So this sort of like task spatialization is another advantage

44:55.320 --> 44:57.320
where like instead of having a single agent

44:57.320 --> 44:58.320
just trying to do everything,

44:58.320 --> 45:01.320
we just like break the task into spatialities.

45:01.320 --> 45:02.320
And this is similar to like,

45:02.320 --> 45:05.320
even like how human organizations work, right?

45:05.320 --> 45:07.320
Where like everyone is like sort of like expert

45:07.320 --> 45:08.320
in their own domain,

45:08.320 --> 45:09.320
and then you like,

45:09.320 --> 45:10.320
and if there's a problem,

45:10.320 --> 45:12.320
you sort of like route it to like the different part of people

45:12.320 --> 45:13.320
who are spatializing that,

45:13.320 --> 45:16.320
and then you like work together to solve the problem.

45:20.320 --> 45:23.320
And the biggest challenge in building this multi-agent system

45:23.320 --> 45:24.320
is going to be communication.

45:24.320 --> 45:27.320
So like how do you communicate really well?

45:27.320 --> 45:30.320
And this might involve like a request information from an agent

45:30.320 --> 45:35.320
or communicating the final like response.

45:35.320 --> 45:37.320
And I would say this is actually like a problem

45:37.320 --> 45:39.320
that even we face as humans.

45:39.320 --> 45:40.320
Like humans are also like,

45:40.320 --> 45:43.320
there can be a lot of miscommunication gaps between humans.

45:43.320 --> 45:47.320
And I will say like a similar thing will become more prevalent

45:48.320 --> 45:51.320
on agents too.

45:51.320 --> 45:52.320
Okay.

45:52.320 --> 45:54.320
And there's a lot of primitives you can think about this

45:54.320 --> 45:57.320
sort of like agent to agent communication,

45:57.320 --> 45:59.320
and you can build a lot of different systems.

46:01.320 --> 46:04.320
And we'll start to see like some sort of protocol

46:04.320 --> 46:06.320
where like we'll have like a standardized protocol

46:06.320 --> 46:09.320
where like all the agents are using this protocol to communicate

46:09.320 --> 46:11.320
and the protocol will ensure like,

46:11.320 --> 46:13.320
we can reduce the miscommunication gaps,

46:13.320 --> 46:15.320
we can reduce any sort of like failures.

46:16.320 --> 46:19.320
It might have some methods to do like a,

46:19.320 --> 46:22.320
if a task was successful or not do some sort of retries

46:24.320 --> 46:26.320
like securities stuff like that.

46:26.320 --> 46:28.320
So we'll see this sort of like agent protocol

46:29.320 --> 46:30.320
come into existence,

46:30.320 --> 46:31.320
which will solve like,

46:31.320 --> 46:33.320
which will be like sort of the standard

46:33.320 --> 46:35.320
part of this agent to agent communication.

46:35.320 --> 46:38.320
And this sort of should enable like

46:38.320 --> 46:41.320
exchanging information between pleads of different agents.

46:41.320 --> 46:43.320
Also like you want to build hierarchies.

46:43.320 --> 46:46.320
Again, I will say this is inspired from like human organizations,

46:46.320 --> 46:48.320
like human organizations are hierarchial

46:48.320 --> 46:50.320
because it's efficient to have a hierarchier

46:50.320 --> 46:53.320
other than a flat organization at some point.

46:53.320 --> 46:55.320
Because you can have like a single,

46:55.320 --> 46:58.320
like suppose you have a single manager managing hundreds of people,

46:58.320 --> 46:59.320
that doesn't scale.

46:59.320 --> 47:01.320
But if you have like,

47:01.320 --> 47:03.320
maybe like each manager manages 10 people

47:03.320 --> 47:04.320
and then you have like a lot of layers,

47:04.320 --> 47:06.320
that is something that's more scalable.

47:08.320 --> 47:11.320
And then you might want to have a lot of primitives around

47:11.320 --> 47:13.320
like how do I sync with my different agents?

47:13.320 --> 47:17.320
How do I do like a lot of like async,

47:17.320 --> 47:19.320
sync communication kind of thing.

47:23.320 --> 47:25.320
And this is like one example you can think

47:25.320 --> 47:27.320
where like suppose there's a user,

47:27.320 --> 47:30.320
the user could talk to one like a manager agent

47:30.320 --> 47:33.320
and that manager agent is like sort of like acting as a router.

47:33.320 --> 47:35.320
So the user can come to me with any request.

47:35.320 --> 47:36.320
The agent like sees like,

47:36.320 --> 47:38.320
oh, maybe for this request I should use the browser.

47:38.320 --> 47:39.320
So it goes to like see like this sort of like

47:39.320 --> 47:41.320
browser agent or something or say like,

47:41.320 --> 47:43.320
oh, I should use this like slack for this.

47:43.320 --> 47:45.320
I can go to a different agent.

47:45.320 --> 47:47.320
And it can also like sort of be responsible for dividing the task.

47:47.320 --> 47:50.320
It can be like, oh, this task I can like maybe like

47:50.320 --> 47:52.320
launch 10 different like sub agents

47:52.320 --> 47:54.320
or sub workers that can go and do this in power.

47:54.320 --> 47:56.320
And then like once they're done,

47:56.320 --> 47:58.320
then I can aggregate the responses and the result to the user.

47:58.320 --> 48:01.320
So this sort of becomes like a very interesting like,

48:03.320 --> 48:05.320
like sort of like an agent that sits in the middle

48:05.320 --> 48:07.320
of all the work that's done and the actual user

48:07.320 --> 48:09.320
responsible for like communicating the,

48:10.320 --> 48:12.320
what's happening to the human.

48:16.320 --> 48:19.320
And we'll need like a lot of,

48:19.320 --> 48:21.320
we'll need to build up a lot of robustness.

48:21.320 --> 48:24.320
One reason is just like natural language is very ambiguous.

48:24.320 --> 48:26.320
Like even for humans, it can be very confusing.

48:26.320 --> 48:29.320
It's very easy to misunderstand, miscommunicate.

48:29.320 --> 48:33.320
And we'll need to, we'll need to build mechanisms to reduce this.

48:33.320 --> 48:35.320
I can also show an example here.

48:35.320 --> 48:37.320
So let's try to get through this quickly.

48:37.320 --> 48:39.320
So suppose here, like,

48:39.320 --> 48:41.320
suppose you have a task X you want to solve.

48:41.320 --> 48:43.320
And the manager agent just like responsible for doing the task

48:43.320 --> 48:45.320
to all the worker agents.

48:45.320 --> 48:47.320
So you can tell the worker like, okay, like do the task X.

48:47.320 --> 48:49.320
Here's the plan. Here's the context.

48:49.320 --> 48:51.320
The current status for the task is not done.

48:51.320 --> 48:53.320
Now, suppose like the worker goes and does the task.

48:53.320 --> 48:55.320
It's like, okay, I've been the task.

48:55.320 --> 48:57.320
I send the response back.

48:57.320 --> 48:59.320
So the response could be like, I don't know,

48:59.320 --> 49:01.320
I don't know what to do.

49:01.320 --> 49:03.320
I send the response back.

49:03.320 --> 49:05.320
So the response could be like, I said, the,

49:05.320 --> 49:07.320
could be like a bunch of thoughts.

49:07.320 --> 49:09.320
It could be some actions. It could be something like the status.

49:09.320 --> 49:11.320
Then the manager can ask, like, okay, like,

49:11.320 --> 49:13.320
maybe I don't trust the worker.

49:13.320 --> 49:15.320
I don't want to go very far. This is actually like correct.

49:15.320 --> 49:17.320
So you might want to do some sort of verification.

49:17.320 --> 49:19.320
And so you can say, like, okay, like,

49:19.320 --> 49:21.320
this was a spec for the task.

49:21.320 --> 49:23.320
Very far that everything has been done correctly to the spec.

49:23.320 --> 49:25.320
And then if the agent says, like, okay, like,

49:25.320 --> 49:27.320
yeah, everything's correct. I'm very fine.

49:27.320 --> 49:29.320
Everything is good.

49:29.320 --> 49:31.320
Then you can say, like, okay, this is good.

49:31.320 --> 49:33.320
And then the manager can say, like, okay,

49:33.320 --> 49:35.320
the task was actually done.

49:35.320 --> 49:37.320
And this sort of like two-way cycle prevents

49:37.320 --> 49:39.320
miscommunication in a sense where, like,

49:39.320 --> 49:41.320
it's possible something could have gone wrong,

49:41.320 --> 49:43.320
but you never caught it.

49:43.320 --> 49:45.320
And so you can hear about the scenario two,

49:45.320 --> 49:47.320
where there's a miscommunication.

49:47.320 --> 49:49.320
So here the manager is saying, like, okay,

49:49.320 --> 49:51.320
let's verify if the task was done.

49:51.320 --> 49:53.320
But then we actually find out that

49:53.320 --> 49:55.320
the task was not done.

49:55.320 --> 49:57.320
And then what you can do is, like,

49:57.320 --> 49:59.320
instead of, like, try to redo the task.

49:59.320 --> 50:01.320
So the manager in that case can say, like, okay,

50:01.320 --> 50:03.320
maybe the task was not done correctly.

50:03.320 --> 50:05.320
So that's why we caught this mistake.

50:05.320 --> 50:07.320
And now we want to, like, fix this mistake.

50:07.320 --> 50:09.320
So we can, like, tell the agent, like, okay,

50:09.320 --> 50:11.320
like, redo this task.

50:11.320 --> 50:13.320
And here's some feedback and corrections to include.

50:15.320 --> 50:17.320
Cool.

50:17.320 --> 50:19.320
So that's sort of the main parts of the talk.

50:19.320 --> 50:21.320
I can also discuss some future directions

50:21.320 --> 50:23.320
of where things are going.

50:23.320 --> 50:25.320
Cool.

50:25.320 --> 50:27.320
Any questions so far?

50:27.320 --> 50:29.320
Okay.

50:29.320 --> 50:31.320
Cool.

50:31.320 --> 50:33.320
So let's talk about some of the key issues

50:33.320 --> 50:35.320
with building the sort of autonomous agents.

50:35.320 --> 50:37.320
So one is just reliability.

50:37.320 --> 50:39.320
Like, how do you make them really reliable?

50:39.320 --> 50:41.320
Which is, like, if I give it a task,

50:41.320 --> 50:43.320
I want it to start to be done 100% of the time.

50:43.320 --> 50:45.320
That's really hard because, like, neural networks

50:45.320 --> 50:47.320
and AI are stochastic systems.

50:47.320 --> 50:49.320
So it's, like, 100% is, like, not possible.

50:49.320 --> 50:51.320
So you'll get at least some degree of error.

50:51.320 --> 50:53.320
And you can try to do that.

50:53.320 --> 50:55.320
Error as much as possible.

50:55.320 --> 50:57.320
Second becomes, like, a looping problem

50:57.320 --> 50:59.320
where it's possible that

50:59.320 --> 51:01.320
agents might divert

51:01.320 --> 51:03.320
from the task it's been given

51:03.320 --> 51:05.320
and start to do something else.

51:05.320 --> 51:07.320
And unless it gets some sort of environment feedback

51:07.320 --> 51:09.320
or some sort of, like, correction,

51:09.320 --> 51:11.320
it might just go and do something different

51:11.320 --> 51:13.320
than what you intended to do

51:13.320 --> 51:15.320
and never realize it's wrong.

51:15.320 --> 51:17.320
The third issue becomes, like, testing and benchmarking.

51:17.320 --> 51:19.320
Like, how do we test this sort of agents?

51:19.320 --> 51:21.320
How do we benchmark them?

51:21.320 --> 51:23.320
And then you go. And finally, how do we deploy them?

51:23.320 --> 51:25.320
And how do we observe them once you're deployed?

51:25.320 --> 51:27.320
Like, that's very important because, like,

51:27.320 --> 51:29.320
if something goes wrong, you won't be able to catch it

51:29.320 --> 51:31.320
before it becomes a major, major issue.

51:31.320 --> 51:33.320
I will say that the, I will say that

51:33.320 --> 51:35.320
the biggest test for number four is, like,

51:35.320 --> 51:37.320
something like Skynet.

51:37.320 --> 51:39.320
Like, suppose you have an agent that can go on the Internet,

51:39.320 --> 51:41.320
do anything, and you don't observe it.

51:41.320 --> 51:43.320
That could just evolve and, like, do basically, like,

51:43.320 --> 51:45.320
take over the whole Internet, possibly, right?

51:45.320 --> 51:47.320
So that's why observability is very important.

51:47.320 --> 51:49.320
And also, I will say, like,

51:49.320 --> 51:51.320
building a kill search. Like, you want to have agents

51:51.320 --> 51:53.320
that can be killed, in a sense. Like, if something goes wrong,

51:53.320 --> 51:55.320
you can just, like, pull out, like, a press a button

51:55.320 --> 51:57.320
and, like, kill them in case.

51:57.320 --> 51:59.320
Okay.

51:59.320 --> 52:01.320
So this is something that goes into the looping problem,

52:01.320 --> 52:03.320
where, like, you can imagine, like,

52:03.320 --> 52:05.320
suppose I want to do a task.

52:05.320 --> 52:07.320
The idea that if you have the task was, like, the white line,

52:07.320 --> 52:09.320
but what might happen is, like, it takes one step,

52:09.320 --> 52:11.320
maybe it goes, like, it does something incorrectly.

52:11.320 --> 52:13.320
It never realizes it. I made a mistake.

52:13.320 --> 52:15.320
So it tries to, it doesn't know what to do.

52:15.320 --> 52:17.320
So it just, like, maybe, like,

52:17.320 --> 52:19.320
we'll do something more randomly.

52:19.320 --> 52:21.320
We'll do something more randomly.

52:21.320 --> 52:23.320
So it will just keep on making mistakes.

52:23.320 --> 52:25.320
And at the end, I can start teaching here to reach

52:25.320 --> 52:27.320
some, like, really bad place and just keep looping,

52:27.320 --> 52:29.320
maybe just doing the same thing again and again.

52:29.320 --> 52:31.320
And that's fine.

52:31.320 --> 52:33.320
And the reason this happens is because, like,

52:33.320 --> 52:35.320
you don't have feedback. So suppose I take a staff.

52:35.320 --> 52:37.320
The agent may, suppose the agent made a mistake.

52:37.320 --> 52:39.320
It doesn't know it made a mistake.

52:39.320 --> 52:41.320
Now, someone has to go and tell it that you made a mistake

52:41.320 --> 52:43.320
and you do, like, fix this.

52:43.320 --> 52:45.320
And that there you need, like, some sort of, like, verification agent

52:45.320 --> 52:47.320
and you need some sort of environment which can say, like,

52:47.320 --> 52:49.320
oh, like, maybe, like, if this is, like,

52:49.320 --> 52:51.320
coding agent or something, then it may, like,

52:51.320 --> 52:53.320
write some code. The code doesn't compile.

52:53.320 --> 52:55.320
Then you can take the error from the compiler

52:55.320 --> 52:57.320
or the IDE, give that to the agent.

52:57.320 --> 52:59.320
Okay, this was the error.

52:59.320 --> 53:01.320
Like, take another staff.

53:01.320 --> 53:03.320
It tries another time. So it tries multiple times

53:03.320 --> 53:05.320
until it, like, fix all the issues.

53:05.320 --> 53:07.320
So you need to really have this sort of, like,

53:07.320 --> 53:09.320
feedback. Otherwise, you never know you're wrong.

53:11.320 --> 53:13.320
And this is, like, one issue we have seen

53:13.320 --> 53:15.320
in the early system, like, auto-GPT.

53:15.320 --> 53:17.320
So I don't think people even use auto-GPT anymore.

53:17.320 --> 53:19.320
It used to be, like, a fad.

53:19.320 --> 53:21.320
I think, like, in February, now it has disappeared.

53:21.320 --> 53:23.320
And the reason was just, like, it's a good concept,

53:23.320 --> 53:25.320
but, like, it doesn't do anything useful

53:25.320 --> 53:27.320
just because it keeps diverging from the task.

53:27.320 --> 53:29.320
And you can't actually get it

53:29.320 --> 53:31.320
to do anything, like, correct.

53:33.320 --> 53:35.320
Okay.

53:35.320 --> 53:37.320
Okay. And we can also discuss

53:37.320 --> 53:39.320
more about, like, the sort of, like,

53:39.320 --> 53:41.320
the computer abstraction of agents.

53:41.320 --> 53:43.320
So this was a recent post from Andre Carpathian,

53:43.320 --> 53:45.320
where he talked about, like, the LLM operating system.

53:47.320 --> 53:49.320
And I will say, like, this is definitely

53:49.320 --> 53:51.320
in the right direction, where you're thinking

53:51.320 --> 53:53.320
as the LLM, as the CPU,

53:53.320 --> 53:55.320
you have the context window, which is, like,

53:55.320 --> 53:57.320
sort of, acting like a RAM.

53:57.320 --> 53:59.320
And then you are trying to build other GPPs.

53:59.320 --> 54:01.320
So you have, like, the Ethernet, which is the browser.

54:01.320 --> 54:03.320
You can have the LLMs that you can talk to.

54:03.320 --> 54:05.320
You have a file system that's embedded.

54:05.320 --> 54:07.320
That's sort of, like, the disk part.

54:07.320 --> 54:09.320
You have, like, the software 1.0,

54:09.320 --> 54:11.320
classical tools, which the LLM can control.

54:11.320 --> 54:13.320
And then you might also

54:13.320 --> 54:15.320
can add metamodality.

54:15.320 --> 54:17.320
So this is, like, more like you have

54:17.320 --> 54:19.320
video inputs, you have audio inputs,

54:19.320 --> 54:21.320
you have, like, more things over time.

54:21.320 --> 54:23.320
And this, and then once you, like,

54:23.320 --> 54:25.320
look at this, you start to see the whole picture

54:25.320 --> 54:27.320
of, like, where things will go.

54:27.320 --> 54:29.320
So, like, currently what we are seeing

54:29.320 --> 54:31.320
mostly is the LLM.

54:31.320 --> 54:33.320
Most people are just working on optimizing the LLM,

54:33.320 --> 54:35.320
making it very good. But this is the whole picture

54:35.320 --> 54:37.320
of what we want to achieve for it to be a useful system

54:37.320 --> 54:39.320
that can actually do things for me.

54:43.320 --> 54:45.320
And I think what we'll start to see is, like,

54:45.320 --> 54:47.320
this sort of becomes, like, an operating system

54:47.320 --> 54:49.320
in terms where, like, someone, like,

54:49.320 --> 54:51.320
say, like, opening, I can go and build this whole thing.

54:51.320 --> 54:53.320
And then I can plug in programs.

54:53.320 --> 54:55.320
I can build, like, stuff on top of this operating system.

54:59.320 --> 55:02.320
Here's, like, also, like, an even more generalized concept,

55:02.320 --> 55:04.320
which I like to call, like, a neural computer.

55:05.320 --> 55:07.320
And the sort of, like, it's very similar,

55:07.320 --> 55:09.320
but it's, like, sort of, like,

55:09.320 --> 55:12.320
now if you were to think of this as a fully flat computer,

55:12.320 --> 55:15.320
what are the different, like, systems you need to go?

55:15.320 --> 55:18.320
And you can think, like, maybe I'm a user,

55:18.320 --> 55:20.320
and talking to this sort of, like, AR,

55:20.320 --> 55:22.320
which is, like, a full-platform AR,

55:22.320 --> 55:24.320
like, imagine, like, the goal is to build 10,000.

55:24.320 --> 55:26.320
What should the architecture of Jarvis look like?

55:26.320 --> 55:28.320
And I would say, like, this goes into the, like,

55:28.320 --> 55:30.320
architecture, to some extent,

55:30.320 --> 55:32.320
where you can think, like, this is a user,

55:32.320 --> 55:35.320
who's talking to, say, like, a Jarvis, like, a AR.

55:35.320 --> 55:37.320
You have a 10 interface.

55:37.320 --> 55:40.320
The chat is sort of, like, how I'm interacting with it,

55:40.320 --> 55:42.320
which could be responsible for, like, personalization.

55:42.320 --> 55:44.320
It can have some, like, some sort of, like, history

55:44.320 --> 55:46.320
about what I like, what I don't like.

55:46.320 --> 55:48.320
So it has some, like, layers where, like,

55:48.320 --> 55:50.320
which are showing my preferences.

55:50.320 --> 55:52.320
It knows how to communicate.

55:52.320 --> 55:54.320
It has, like, human, like, sort of, like, maybe, like,

55:54.320 --> 55:56.320
competitive sort of, like, skills.

55:56.320 --> 55:58.320
So it's, you feel, like, very human-like.

55:58.320 --> 56:00.320
And after the 10 interface,

56:00.320 --> 56:02.320
you have some sort of, like, a task engine,

56:02.320 --> 56:04.320
which is following, like, capabilities.

56:04.320 --> 56:06.320
So if I ask it, like, okay, like, do the circulation for me

56:06.320 --> 56:09.320
or, like, find this, especially this information

56:09.320 --> 56:11.320
or order me a burger,

56:11.320 --> 56:13.320
then sort of, like, you imagine, like,

56:13.320 --> 56:15.320
the chat interface should activate the task engine,

56:15.320 --> 56:17.320
which is, like, okay, like, instead of just checking,

56:17.320 --> 56:20.320
I need to, like, go and tell the task for the user.

56:20.320 --> 56:22.320
So that goes to the task engine.

56:22.320 --> 56:25.320
And then you can imagine, there's going to be a couple of rules.

56:25.320 --> 56:27.320
So because if you want to have safety in mind

56:27.320 --> 56:29.320
and you want to make sure things don't go wrong,

56:29.320 --> 56:31.320
so the, any sort of engine you build

56:31.320 --> 56:33.320
needs to have some sort of rules.

56:33.320 --> 56:35.320
And this could be, like, sort of, like,

56:35.320 --> 56:37.320
you have the three rules for robotics

56:37.320 --> 56:39.320
that a robot should not harm a human and stuff like that.

56:39.320 --> 56:41.320
You can imagine, like, you want to have, like,

56:41.320 --> 56:43.320
this sort of, like, task engine to have a bunch of, like, inherent rules,

56:43.320 --> 56:45.320
where, like, these are the principles in their value.

56:45.320 --> 56:47.320
And if it creates a task or, like,

56:47.320 --> 56:49.320
sort of, like, creates a plan which violates these rules,

56:49.320 --> 56:52.320
then that plan should be evaluated automatically.

56:54.320 --> 56:56.320
And so the task engine what it's doing is,

56:56.320 --> 56:58.320
it's sort of, like, taking the chat input

56:58.320 --> 57:00.320
and saying, like, I want to respond to a task

57:00.320 --> 57:03.320
that can actually solve this problem for the user.

57:03.320 --> 57:05.320
And the task would be, like, say, in this case,

57:05.320 --> 57:08.320
say, it's in the next day, like, I want to go online

57:08.320 --> 57:13.320
and buy, like, a, buy, like, a five-person thing.

57:13.320 --> 57:16.320
So in that case, like, suppose that's a task, this is an engine.

57:16.320 --> 57:19.320
And this task can go to, like, some sort of, like, a routing agent.

57:19.320 --> 57:22.320
So this becomes, like, sort of, like, the manager-agent idea.

57:22.320 --> 57:25.320
And then the manager-agent thing is, like, okay, like,

57:25.320 --> 57:27.320
where should I, what should I do?

57:27.320 --> 57:29.320
Like, should I use the browser?

57:29.320 --> 57:32.320
Should I use some sort of, like, a local app or tool?

57:32.320 --> 57:34.320
Should I, like, use some sort of, like, file storage,

57:34.320 --> 57:35.320
security system?

57:35.320 --> 57:37.320
And then based on that decision, it can, like,

57:37.320 --> 57:39.320
it's possible that we might need the combination of things.

57:39.320 --> 57:41.320
Like, maybe, like, maybe I need to use this file system

57:41.320 --> 57:43.320
to find some information about the user

57:43.320 --> 57:47.320
and if you do some, like, I need to use some apps and tools.

57:47.320 --> 57:49.320
So in, like, sort of, like, do this sort of, like, message passing

57:49.320 --> 57:51.320
to all the agents, get those from the agents.

57:51.320 --> 57:53.320
So it's like, okay, like, maybe, like,

57:53.320 --> 57:56.320
the browser even says, like, okay, like, yeah, I found this site.

57:56.320 --> 57:58.320
This is what the user likes.

57:58.320 --> 58:00.320
Maybe you can have some sort of map engine.

58:00.320 --> 58:02.320
You can, like, sort of, like, okay, this is all the valid plans.

58:02.320 --> 58:03.320
That makes sense.

58:03.320 --> 58:06.320
You can want to construct, like, for instance.

58:06.320 --> 58:09.320
And then you can, like, sort of, like, take the result,

58:09.320 --> 58:10.320
show that to the user.

58:10.320 --> 58:12.320
Like, you can take them and, like, okay, like,

58:12.320 --> 58:13.320
I found all this site for you.

58:13.320 --> 58:15.320
And then if the user says, like, choose this site,

58:15.320 --> 58:17.320
then you can actually go and, like, go to the site.

58:17.320 --> 58:19.320
But this sort of becomes, like, sort of, like,

58:19.320 --> 58:21.320
gives you an idea of what the hierarchy is,

58:21.320 --> 58:22.320
what the system is truly like.

58:22.320 --> 58:24.320
And we need to build, like, all these components.

58:24.320 --> 58:25.320
We're, like, currently here.

58:25.320 --> 58:29.320
Let me see the L and R.

58:29.320 --> 58:30.320
Okay.

58:30.320 --> 58:31.320
Cool.

58:31.320 --> 58:33.320
And then we can also have, like, reflection,

58:33.320 --> 58:36.320
where the idea is, like, once you do a task,

58:36.320 --> 58:38.320
it's possible something might be wrong.

58:38.320 --> 58:40.320
So the task engine can possibly verify

58:40.320 --> 58:43.320
you've been through this and logic to see, like,

58:43.320 --> 58:45.320
okay, like, this is correct or not.

58:45.320 --> 58:47.320
And if it's not correct, then, like,

58:47.320 --> 58:48.320
you keep issuing this instruction,

58:48.320 --> 58:53.320
but if it's correct, then you pass that to the user.

58:53.320 --> 58:55.320
And then you can have, like, more, like, sort of, like,

58:55.320 --> 58:56.320
complex things.

58:56.320 --> 58:58.320
Like, so you can have, you know, parts, plans,

58:58.320 --> 59:02.320
and, like, even improving the systems.

59:02.320 --> 59:04.320
Okay.

59:04.320 --> 59:06.320
And it looks like the biggest thing we need right now

59:06.320 --> 59:08.320
is, like, when there's an error correction.

59:08.320 --> 59:11.320
Because it's really hard to catch errors.

59:11.320 --> 59:13.320
So if you can do that, it will be better.

59:13.320 --> 59:14.320
I think that will help.

59:14.320 --> 59:16.320
Especially if you can build aging frameworks

59:16.320 --> 59:17.320
which help, you know,

59:17.320 --> 59:18.320
when mechanisms are getting errors

59:18.320 --> 59:20.320
and automatically fixing them.

59:20.320 --> 59:22.320
Same thing you just need is, like, security.

59:22.320 --> 59:25.320
You need some sort of models around user permissions.

59:25.320 --> 59:28.320
So it's possible.

59:28.320 --> 59:31.320
You want to have, like, different layers where, like,

59:31.320 --> 59:34.320
what are some things that an agent can do, cannot do

59:34.320 --> 59:35.320
on my computer, for instance.

59:35.320 --> 59:36.320
So maybe I can select,

59:36.320 --> 59:39.320
or maybe, like, the agent is not allowed to go to my bank account,

59:39.320 --> 59:41.320
but he can go to my, like, low-dash account.

59:41.320 --> 59:43.320
So you want to build this all, like, user permissions.

59:43.320 --> 59:45.320
And then you also want to solve problems

59:45.320 --> 59:46.320
around, like, sandboxing.

59:46.320 --> 59:47.320
How do I make sure everything's safe?

59:47.320 --> 59:49.320
It doesn't go in the strong computer,

59:49.320 --> 59:50.320
delete everything.

59:50.320 --> 59:53.320
How do I deploy industry settings where, like,

59:53.320 --> 59:54.320
they might do a lot of business,

59:54.320 --> 59:55.320
they might do a lot of finance,

59:55.320 --> 59:57.320
and they're making sure that they're,

59:57.320 --> 59:59.320
if things are irreversible,

59:59.320 --> 01:00:01.320
we don't, like, cause a lot of trouble.

01:00:04.320 --> 01:00:05.320
Cool.

01:00:05.320 --> 01:00:07.320
Yeah, so that was the talk.

01:00:07.320 --> 01:00:08.320
Thank you.

