{"text": " So Chris Potts is a professor and actually also the chair of the Department of Linguistics and by courtesy also at the Department of Computer Science and he's a great expert in the area of natural language understanding so he's you know there would not be a better person to hear about a topic than him and we are so grateful that he could make the time and he's actually also teaching the graduate course CS22 for you natural language understanding that we actually transform into a professional course that is starting next week on the same topic so you know if you're interested in learning more we have some links included you know down below on your platform you can check it out and you know there's so many other things that can be said about Chris like he has a super interesting podcast he's running like so many interesting research papers like projects he worked on so you know go ahead and learn more about him like you should also have a little link I think without further ado I think we can kick it off Chris thank you so much once again oh thank you so much Petra for the kind words and welcome to everyone it's wonderful to be here with you all I do think that we live in a golden age for natural language understanding maybe also a disconcerting age a weird age but certainly a time of a lot of innovation and a lot of change it's sort of an interesting moment for reflection for me because I started teaching my NLU course at Stanford in 2012 about a decade ago that feels very recent in my lived experience but it feels like a completely different age when it comes to NLU and indeed all of artificial intelligence I I never would have guessed in 2012 that we would have such an amazing array of technologies and scientific innovations and that we would have these models that were just so performant and also so widely deployed in the world this is also a story of again for better or worse increasing societal impact and so that does come together for me into a golden age and just to reflect on this a little bit it's really just amazing to think about how many of these models you can get hands on with if you want to right away right you can download or use via apis models like dolly 2 that do incredible text to image generation stable diffusion mid-journey they're all in that class we also have github co-pilot based in the codex model for doing code generation tons of people derive a lot of value from that system u.com is at the leading edge I would say of search technologies that are changing the search experience and also leading us to new and better results when we search on the web whisper ai is an incredible model from open ai this does speech to text and this model is a generic model that is better than the best user customized models that we had 10 years ago just astounding not something I would have predicted I think and then of course the star of our show for today is going to be these big language models gpt 3 is the famous one you can use it via an api we have all these open source ones as well that have come out opt bloom gpt neo x these are models that you can download and work with to your heart's content provided that you have all the computing resources necessary so just incredible and I'm sure you're familiar with this but let's just you know get this into our common ground here it's incredible what these models can do here's a quick demo of gpt 3 I asked the da Vinci 2 engine in which year was stanford university founded when did it enroll its first students who is its current president and what is its mascot and da Vinci 2 gave a fluent and complete answer that is correct on all counts just incredible that was with da Vinci 2 we got a big update to that model in late 2022 that's da Vinci 3 and here I'm showing you that it reproduces that results exactly and I do think that da Vinci 3 is a big step forward over the previous engine here's actually an example of that you know I have like to play adversarial games with this model and so I asked da Vinci 2 would it be possible to hire a team of tamarins to help me paint my house assuming I'm willing to pay them in sufficient quantities of fruit to meet minimum wage requirements in california this is adversarial because I know that these models don't have a really rich understanding of the world we live in they're often distracted by details like this and sure enough da Vinci 2 got confused yes it would be possible to hire a team of tamarins to paint your house you would need to make sure that you're providing them with enough fruit to meet minimum wage requirements and so forth so easily distracted but I tried this again with da Vinci 3 and with the same question it gave a very sensible answer no it would not be possible to hire a team of tamarins to help you paint your house da Vinci 2 was not distracted by my adversarial game this is not to say that you can't trick da Vinci 2 just go on to twitter and you'll find examples of that but again I do think we're seeing a pretty remarkable rate of progress toward these models being robust and relatively trustworthy this is also a story of scientific innovation that was a brief anecdote but we're seeing this same level of progress in the tools that we use to measure system performance in the field I've put this under the heading of benchmark saturate faster than ever this is from a paper from 2021 that I was involved with Kila et al here's the framework along the x-axis I have time going back to the 1990s and along the y-axis I have a normalized measure of our estimate of human performance that's the red line set at zero so MNIST digit recognition a grand old data set in the field that was launched in the 1990s and it took about 20 years for us to surpass this estimate of human performance switchboard is a similar story launched in the 90s this is the speech to text problem it took about 20 years for us to get up past this red line here image net is newer this was launched in 2009 it took about 10 years for us to reach this saturation point and from here the pace is really going to pick up so squad 1.1 is question answering that was solved in about three years the response was squad 2.0 that was solved in less than two years and then the glue benchmark if you were in the field you might recall back the glue benchmark is this big set of tasks that was meant to stress test our best models when it was announced a lot of us worried that it was just too hard for present day models but glue was saturated in less than a year the response was super glue meant to be much harder it was also saturated in less than a year a remarkable story of progress undoubtedly even if you're cynical about this measure of human performance we are still seeing a rapid increase in the rate of change here and you know 2021 was ages ago in the story of AI now I think this same thing carries over into the current era with our largest language models this is from a really nice post from Jason Wei he is assessing emergent abilities in large language models you see eight of them given here along the x-axis for these plots you have model size and on the y-axis you have accuracy and what Jason is showing is that at a certain point these really big models just attain these abilities to do these really hard tasks Jason estimates that for 137 tasks models are showing this kind of emergent ability and that includes tasks that were explicitly set up to help us stress test our largest language model they're just falling away one by one really incredible now we're going to talk a little bit later about the factors that are driving this enormous progress for large language models but I want to be upfront that one of the major factors here is just the raw size of these models you can see that in Jason's plots that's where the emergent ability kicks in and let me put that in context for you so this is from a famous plot from a paper that's actually about making models smaller and what they did is track the rise of you know increases in model size along the x-axis we have time depth it only goes back to 2018 it's not very long ago and in 2018 the largest of our models had around 100 million parameters seems small by current comparisons in late 2019 early 2020 we start to see a rapid increase in the size of these models so that by the end of 2020 we have this megatron model at 8.3 billion parameters I remember when that came out it seemed like it must be some kind of typo I could not fathom that we had a model that was that large but now of course this is kind of on the small side soon after that we got an 11 billion parameter variant of that model and then gpd3 came out that says 175 billion parameters and that one too now looks small in comparison to these truly gargantuan megatron models and the palm model from google which surpassed 500 billion parameters I want to emphasize that this has made a complete mockery of the y-axis of this plot to capture the scale correctly we would need 5 000 of these slides stacked on top of each other again it still feels weird to say that but that is the truth the scale of this is absolutely enormous and not something I think that I would have anticipated way back when we were dealing with those 100 million parameter babies by comparison they seem large to me at that point so this brings us to our central question it's a golden age this is all undoubtedly exciting and the things that I've just described to you are going to have an impact on your lives positive and negative but certainly an impact but I take it that we are here today because we are researchers and we would like to participate in this research and that could leave you with a kind of worried feeling how can you contribute to nlu in this era of these gargantuan models I've set this up as a kind of flow chart first question do you have 50 million dollars and a love of deep learning infrastructure if the answer is yes to this question then I would encourage you to go off and build your own large language model you could change the world in this way I would also request that you get in touch with me maybe you could join my research group and maybe fund my research group that would be wonderful but I'm assuming that most of you cannot truthfully answer yes to this question I'm in the no camp right and on both counts I am both dramatically short of the funds and I also don't have a love of deep learning infrastructure so for those of us who have to answer no to this question how can you contribute even if the answer is no there are tons of things that you can be doing all right so just topics that are front of mind to me include retrieval augmented in-context learning this could be small models that are performant you could always contribute to creating better benchmarks this is a perennial challenge for the field and maybe the most significant thing that you can do is just create devices that allow us to accurately measure the performance of our systems you could also help us solve what I've called the last mile problem for productive applications these central developments in AI take us 95 percent of the way toward utility but that last five percent actually having a positive impact on people's lives often requires twice as much development twice as much innovation across domain experts people who are good at human computer interaction and AI experts right and there's so there's just a huge amount that has to be done to realize the potential of these technologies and then finally you could think about achieving faithful human interpretable explanations of how these models behave if we're going to trust them we need to understand how they work at a human level that is supremely challenging and therefore this is incredibly important work you could be doing now I would love to talk with you about all four of those things and really elaborate on them but our time is short and so what I've done is select one topic retrieval augmented in-context learning to focus on because it's it's intimately connected to this notion of in-context learning and it's a place where all of us can participate in lots of innovative ways so that's kind of the central plan for the day before I do that though I just want to help us get more common ground around what I take to be the really central change that's happening as a result of these large language models and I've put that under the heading of the rise of in-context learning again this is something we're all getting used to it really remarks a genuine paradigm shift I would say in-context learning really traces to the GPT-3 paper there are precedents earlier in the literature but it was the GPT-3 paper that really gave it a thorough initial investigation and showed that it had promised with the earliest GPT models here's how this works we have our big language model and we prompt it with a bunch of text so for example this is from that GPT-3 paper we might prompt the model with a context passage and a title we might follow that with one or more demonstrations here the demonstration is a question and an answer and the goal of the demonstration is to help the model learn in context that is from the prompt we've given it what behavior we're trying to elicit from it so here you might say we're trying to coax the model to do extractive question answering to find the answer as a substring of the passage we gave it you might have a few of those and then finally we have the actual question we want the model to answer we prompt the model with this prompt here that puts it in some state and then its generation is taken to be the prediction or response and that's how we assess its success and the whole idea is that the model can learn in context that is from this prompt what we want it to do so that gives you a sense for how this works you've probably all prompted language models like you like this yourself already i want to dwell on this for a second though this is a really different thing from what we used to do throughout artificial intelligence let me contrast in context learning with the standard paradigm of standard supervision back in the old days of 2017 or whatever we would typically set things up like this we would have say we wanted to solve a problem like classifying texts according to whether they express nervous anticipation a complex human emotion the first step would be that we would need to create a data set of positive and negative examples of that phenomenon and then we would train a custom built model to make the binary distinction reflected in the labels here it can be surprisingly powerful but you can start to see already how this isn't going to scale to the complexity of the human experience we're going to need separate data sets and maybe separate models for optimism and sadness and every other emotion you can think of and that's just a subset of all the problems we might want our models to solve for each one we're going to need data and maybe a custom built model the promise of in-context learning is that a single big frozen language model can serve all those goals and in this mode we do that prompting thing that I just described we're going to give the model examples just expressed in flat text of positive and negative instances and hope that that's enough for it to learn in context about the distinction we're trying to establish this is really really different consider that over here the phrase nervous anticipation has no special status the model doesn't really process it it's entirely structured to make a binary distinction and the label nervous anticipation is kind of for us on the right the model needs to learn essentially the meanings of all of these terms and our intentions and figure out how to make these distinctions on new examples all from a prompt it's just weird and wild that this works at all I think I used to be discouraging about this as an avenue and now we're seeing it bear so much fruit what are the mechanisms behind this I'm going to identify a few of them for you the first one is certainly the transformer architecture this is the basic building block of essentially all the language models that I've mentioned so far we have great coverage of the transformer in our course natural language understanding so I'm going to do this quickly the transformer starts with word embeddings and positional encodings on top of those we have a bunch of attention mechanisms these give the name to the famous paper attention is all you need which announced the transformer evidently attention is not all you need because we have these positional encodings at the bottom and then we have a bunch of feed forward layers and regularization steps at the top but attention really is the beating heart of this model and it really was a dramatic departure from the fancy mechanisms LSTMs and so forth that were characteristic of the pre-transformer era so that's essentially though on the diagram here the full model in the course we have a bunch of materials that help you get hands on with transformer representations and also dive deep into math into the mathematics so I'm just going to skip past this I will say that if you dive deep you're likely to go through the same journey we all go through where your first question is how on earth does this work this diagram looks very complicated but then you come to terms with it and you realize oh this is actually a bunch of very simple mechanisms but then you arrive at a question that is a burning question for all of us why does this work so well this remains an open question a lot of people are working on explaining why this is so effective and that is certainly an area in which all of us could participate analytic work understanding why this is so successful the second big innovation here is a realization that what I've called self supervision is an incredibly powerful mechanism for acquiring rich representations of form and meaning this is also very strange in self supervision the model's only objective is to learn from co-occurrence patterns in the sequences it's trained on this is purely distributional learning another way to put this is the model is just learning to assign high probability to attested sequences that is the fundamental mechanism we think about these models as generators but generation is just sampling from the model that's a kind of secondary or derivative process the main thing is learning from these co-occurrence patterns an enlightening thing about the current era is that it's fruitful for these sequences content to contain lots of symbols not just language but computer code sensor readings even images and so forth those are all just symbol streams and the model learns associations among them the core thing about self supervision though that really contrasts it with the standard supervised paradigm I mentioned before is that the objective doesn't mention any specific specific symbols or relations between them is entirely about learning these co-occurrence patterns and from this simple mechanism we get such rich results and that is incredibly empowering because you need hardly any human effort to train a model with self supervision you just need vast quantities of these symbol streams and so that has facilitated the rise of another important mechanism here large-scale pre-training and there are actually two innovations that are happening here right so we see the rise of large-scale pre-training in the earliest work on static word representations like word to vex and glove and what those teams realize is not only that it's powerful to train on vast quantities of data using just self supervision but also that it's empowering to the community to release those parameters not just data not just code but the actual learned representations for other people to build on that has been incredible in terms of building effective systems after those we get ELMO which was the first model to do this for contextual word representations truly large language models then we get BERT of course and GPT and then finally of course GPT-3 at a scale that was really previously unimagined and maybe kind of unimaginable for me a final piece that we should not overlook is the role of human feedback in all of this and I'm thinking in particular of the open AI models I've given a lot of coverage so far of this mechanism of self supervision but we have to acknowledge that our best models are what open AI calls the instruct models and those are trained with way more than just self supervision this is a diagram from the chat GPT blog post it has a lot of details I'm confident that there are really two pieces that are important first the language model is fine tuned on human level supervision just making binary distinctions about good generations and bad ones that's already beyond self supervision and then in a second phase the model generates outputs and humans rank all of the outputs the model has produced and that feedback goes into a lightweight reinforcement learning mechanism in both of those phases we have important human contributions that take us beyond that self supervision step and kind of reduce the magical feeling of how these models are achieving so much I'm emphasizing this because I think what we're seeing is a return to a familiar and kind of cynical sounding story about AI which is that many of the transformative step forwards are actually on the back of a lot of human effort behind the scenes expressed at the level of training data but on the positive side here it is incredible that this human feedback is having such an important impact instruct models are best in class in the field and we have a lot of evidence that that must be because of these human feedback steps happening at a scale that I assume is astounding they must have at open AI large teams of people providing very fine green feedback across lots of different domains with lots of different tasks in mind final piece by way of background prompting itself this has been a real journey for all of us I've described this as step by step and chain of thought reasoning to give you a feel for how this is happening let's just imagine that we've posed a question like can our models reason about negation that is if we didn't eat any food does the model know that we didn't eat any pizza in the old days of 2021 we were so naive we would prompt models with just that direct question like is it true that if we didn't eat any food then we didn't eat any pizza and we would see what the model said in return now in 2023 we know so much and we have learned that it can really help to design a prompt that helps the model reason in the intended ways this is often called step by step reasoning here's an example of a prompt that was given to me by Omar Khattab you start by telling it it's a logic and common sense reasoning exam for some reason that's helpful then you give it some specific instructions and then you use some special markup to give it an example of the kind of reasoning that you would like it to follow after that example comes the actual prompt and in this context what we essentially ask the model to do is express its own reasoning and then conditional on what it has produced create an answer and the eye-opening thing about the current era is that this can be transformative better I think if you wanted to put this poetically you'd say that these large language models are kind of like alien creatures and it's taking us some time to figure out how to communicate with them and together with all that instruct fine tuning with human supervision we're converging on prompts like this as the powerful device and this is exciting to me because what's really emerging is that this is a kind of very light way of programming an AI system using only prompts as opposed to all the deep learning code that we used to have to write and that's going to be incredibly empowering in terms of system development and experimentation all right so we have our background in place I'd like to move to my main topic here which is retrieval augmented in-context learning what you're going to see here is a combination of language models with retriever models which are themselves under the hood large language models as well but let me start with a bit of the backstory here I think we're all probably vaguely aware at this point that large language models have been revolutionizing search again the star of this is the transformer or maybe more specifically its famous spokesmodel Burt right after Burt was announced around 2018 Google announced that it was incorporating aspects of Burt into its core search technology and Microsoft made a similar announcement at about the same time and I think those are just two public facing stories of you know many instances of large search technologies having Burt elements incorporated into them in that era and then of course in the current era we have startups like you.com which have made large language models pretty central to the entire search experience in the form of you know delivering results but also interactive search with conversational agents so that's all exciting but I am an NLP at heart and so for me in a way the more exciting direction here is the fact that finally search is revolutionizing NLP by helping us bridge the gap into much more relevant knowledge intensive tasks to give you a feel for how that's happening let's just use question answering as an example so prior to this work in NLP we would pose question answering or QA in the following way you saw this already with the GPT-3 example we would have as given at test time a title and a context passage and then a question and the task of the model is to find the answer to that question as a literal substring of the context passage which was guaranteed by the nature of the data set as you can imagine models are really good at this task superhuman certainly at this task but it's also a very rarefied task this is not a natural form of question answering in the world and it's certainly unlike the scenario of for example doing web search so the promise of the open formulations of this task are that we're going to connect more directly with the real world in this formulation at test time we're just given a question and the standard strategy is to rely on some kind of retrieval mechanism to find relevant evidence in a large corpus or maybe even the web and then we proceed as before this is a much harder problem because we're not going to get the substring guarantee anymore because we're dependent on the retriever to find relevant evidence but of course it's a much more important task because this is much more like our experience of searching on the web now I've kind of biased already in describing things this way where I assume we're retrieving a passage but there is another narrative out there let me skip to this then you could call this like the llms for everything approach and this would be where there's no explicit retriever you just have a question come in you have a big opaque model process that question and out comes an answer voila you hope that the user's information need is met directly no separate retrieval mechanism just the language model doing everything I think this is an incredibly inspiring vision but we should be aware that there are lots of kind of danger zones here so the first is just efficiency one of the major factors driving that explosion in model size that I tracked before is that in this llms for everything approach we are asking this model to play the role of both knowledge store and language capability if we could separate those out we might get away with smaller models we have a related problem of update ability suppose a fact in the world changes that document on the web changes for example well you're going to have to update the parameters of this big opaque model somehow to conform to the change in reality there are people hard at work on that problem that's a very exciting problem but I think we're a long way from being able to offer guarantees that a change in the world is reflected in the model behavior and that plays into all sorts of issues of trustworthiness and explainability of behavior and so forth also we have an issue of provenance look at the answer at the bottom there is that the correct answer should you trust this model right in the standard web search experience we typically are given some web pages that we can click on to verify at least at the next level of detail whether the information is correct but here we're just given this response and if the model also generated a provenance string if it told us where it found the information we'd be left with the concern that that provenance string was also untrustworthy right and this is like a really breaking a fundamental contract that users expect to have with search technologies I believe so those are some things to worry about there are positives though of course these models are incredibly effective at meeting your information need directly and they're also outstanding at synthesizing information if your question can only be answered by 10 different web pages it's very likely that the language model will still be able to do it without you having to hunt through all those pages so exciting but lots of concerns here here is the alternative of retrieval augmented approaches right oh I can't resist this actually just to give you an example of how important this trustworthy thing can be so I used to be impressed by DaVinci 3 because it would give a correct answer to the question are professional baseball players allowed to glue small wings onto their caps this is a question that I got from a wonderful article by Hector Levec where he encourages us to stress test our models by asking them questions that would seem to run up against any simple distributional or statistical learning model and really get at whether they have a model of the world and for DaVinci 2 it gave what it looked like a really good Levec style answer there is no rule against it but it is not common that seems true so I was disappointed I guess or I'm actually not sure how to feel about this when I asked DaVinci 3 the same question and it said no professional baseball players are not allowed to glue small wings onto their caps major league baseball has strict rules about the appearance of players uniforms and caps in any modification to the caps are not allowed that also sounds reasonable to me is it true it would help enormously if the model could offer me at least a web page with with evidence that's relevant to these claims otherwise I'm simply left wondering and I think that shows you that we've kind of broken this implicit contract with the user that we expect from search so that'll bring me to my alternative here retrieval based or retrieval augmented NLP to give you a sense for this at the top here I have a standard search box and I've put in a very complicated question indeed the first step in this approach is familiar from the LLMs for everything one we're going to encode that query into a dense numerical representation capturing aspects of its form and meaning we use a language model for that the next step is new though we are also going to use a language model maybe the same one we use for the query to process all of the documents in our document collection so each one has some kind of numerical deep learning representation now on the basis of these represent representations we can now score documents with respect to queries just like we would in the standard good old days of information retrieval so we can reproduce every aspect of that familiar experience if we want to we're just doing it now in this very rich semantic space so we get some results back and we could offer those to the user as ranked results but we can also go further we can have another language model call it a reader or a generator slurp up those retrieved passages and synthesize them into a single answer maybe meeting the user's information need directly right so let's check in on how we're doing with respect to our goals here first efficiency I won't have time to substantiate this today but these systems in terms of parameter counts can be much smaller than the integrated approach I mentioned before we also have an easy path to update ability we have this index here so as pages change in our document store we simply use our frozen language model to reprocess and re-represent them and we can have a pretty good guarantee at this point that information changes will be reflected in the retrieved results down here we're also naturally tracking provenance because we have all these documents and they're used to deliver the results and we can have that carry through into the generation so we've kept that contract with the user these models are incredibly effective across lots of literature we're seeing that retrieval augmented approaches are just superior to the fully integrated llms for everything one and we've retained the benefit of llms for everything because we have this model down here the reader generator that can synthesize information into answers that meet the information need directly so that's my fundamental pitch now again things are changing fast and even the approach to designing these systems is also changing really fast so in the in the previous era of 2020 we would have these pre-trained components like we have our index and our retriever maybe we have a language model like reader generator and you might have other pre-trained components image processing and so forth so you have all these assets and the question is how are you going to bring them together into an integrated solution the standard deep learning answer to that question is to define a bunch of task specific parameters that are meant to tie together all those components and then you learn those parameters with respect to some task and you hope that that has kind of created an effective integrated system that's the modular vision of deep learning the truth in practice is that even for very experienced researchers and system designers this can often go really wrong and debugging these systems and figuring out how to improve them can be very difficult because they are so opaque and the scale is so large but maybe we're moving out of an era in which we have to do this at all so this will bring us back to in-context learning the fundamental insight here is that many of these models can in principle communicate in natural language right so a retriever is abstractly just a device for pulling in text and producing text with scores and a language model is also a device for pulling in text and producing text with scores and we have already seen in my basic picture of retrieval augmented approaches that we could have the retriever communicate with the language model via retrieve results well what if we just allow that to go in both directions now we've got a system that is essentially constructed by prompts that help these models do message passing between them in potentially very complicated ways an entirely new approach to system design that I think is going to have an incredible democratizing effect on who designs these systems and what they're for let me give you a deep sense for just how wide open the design space is here again to give you a sense for how much of this research is still left to be done even in this golden era let's imagine a search context the question is what course to take what we're going to do in this new mode is begin a prompt that contains that question just as before and now what we can do next is retrieve a context passage that'll be like the retrieval augmented approach that I showed you at the start of this section right you could just use our retriever for that but there's more that could be done what about demonstrations let's imagine that we have a little train set of qa pairs that kind of demonstrate for our system what the intended behavior is well we can add those into the prompt and now we're giving the system a lot of few shot guidance about how to learn in context right but that's also just the beginning I might have sampled these training examples randomly for my train set but I have a retriever remember and so what I could do instead is find the demonstrations that are the most similar to the user's question and put those in my prompt with the expectation that that will help it understand kind of topical coherence and lead to better results but I could go further right I could use my retriever again to find relevant context passages for each one of those demonstrations to further help it figure out how to reason in terms of evidence and that also opens up a huge design space we could do what we call hindsight retrieval where for each one of these we're using both the question and the answer to find relevant context passages to really give you integrated informational packets that the model can benefit from and there's lots more that we could do with these demonstrations you're probably starting to see it right we could do some rewriting and so forth really makes sophisticated use of the retriever and the language model interwoven we could also think about how we selected this background passage I was assuming that we would just retrieve the most relevant passage according to our question but we could also think about rewriting the user's query in terms of the demonstrations that we could construct it to get a new query that will help the model that's especially powerful if you have a kind of interactional mode where the demonstrations are actually part of like a dialogue history or something like that and then finally we could turn our attention to how we're actually generating the answer I was assuming we would take the top generation from the language model but we could do much more we could filter its generations to just those that match a substring of the passage reproducing some of the old mode of question answering but now in this completely open formulation that can be incredibly powerful if you know your model can retrieve good background passages here those are two simple steps you could also go all the way to the other extreme and use the full retrieval augmented generation or rag model which is essentially creates a full probability model that allows us to marginalize out the contribution of passages that can be incredibly powerful in terms of making maximal use of the capacity of this model to generate text conditional on all the work that we did up here I hope that's giving you a sense for just how much can happen here what we're starting to see I think is that there is a new programming mode emerging it's a programming mode that involves using these large pre-trained components to design in code prompts that are essentially full AI systems that are entirely about message passing between these frozen components we have a new paper out that's called demonstrate search predictor dsp this is a lightweight programming framework for doing exactly what I was just describing for you and one thing I want to call out is that our results are fantastic now you know we can pat ourselves on the back we have a very talented team and so it's no surprise the results are so good but I actually want to be upfront with you I think the real insight here is that it is such early days in terms of us figuring out how to construct these prompts how to program these systems that we've only just begun to understand what's optimal we have explored only a tiny part of the space and everything we're doing is suboptimal and that's just the kind of conditions where you get these huge leap forwards leaps forward in performance on these tasks so I suspect that the bold row that we have here will not be long-lived given how much innovation is happening in this space and I want to make a pitch for our course here right so we have in this course a bunch of assignment slash bake-offs and the way that works essentially is that you have an assignment that helps you build some baselines and then work toward an original system which you enter into a bake-off which is a kind of informal competition around data and modeling our newest of these is called few shot open qa with cobear retrieval it's a version of the problems that I've just been describing for you this is a problem that could not even have been meaningfully posed five years ago and now we are seeing students doing incredible cutting-edge things in this mode it's exactly what I was just describing for you and we're in the sort of moment where a student project could lead to a paper that you know really leaves leads to state-of-the-art performance in surprising ways again because there is just so much research that has to be done here I'm running out of time what I think I'll do is just briefly call out again those important other areas that I've given short drift to today but I think are just so important starting with data sets I've been talking about system design and task performance but it is now and will always be the case that contributing you new benchmark data sets is basically the most important thing you can do Jacques Cousteau said water and air the two essential fluids on which all life depends I would extend that NLP our data sets are the resource on which all progress depends now Cousteau extended this with have become global garbage cans I am not that cynical about our data sets I think we've learned a lot about how to create effective data sets we're getting better at this but we need to watch out for this metaphorical pollution and we need always to be pushing our systems with harder tasks that come closer to the human capabilities that we're actually actually trying to get them to achieve and without contributions of data sets we could be tricking ourselves when we think we're making a lot of progress the second thing that I wanted to call out relates to model explainability you know we're in an era of incredible impact and that has rightly turned researchers to questions of system reliability safety trust approved use and pernicious social biases we have to get serious about all these issues if we're gonna responsibly have all of the impact that we're achieving at this point all of these things are incredibly difficult because the systems we're talking about are these enormous opaque impossible to understand analytically devices like this that are just clouding our understanding of them and so to me that shines a light on the importance of achieving analytic guarantees about our model behaviors that seems to me to be a prerequisite for getting serious about any one of these topics and the goal there in our terms is to achieve faithful human interpretable explanations of model behavior we have great coverage of these methods in the course hands-on materials screencasts and other things that will help you participate in this research and also as a side effect write absolutely outstanding discussion and analysis sections for your papers and the final thing I wanted to call out is just that last mile problem fundamental advances in AI take us 95 percent of the way there but that last five percent is every bit as difficult as the first 95 in my group we've been looking a lot at image accessibility this is an incredibly important societal problem because images are so central to modern life across being on the web and in social media also in the news and in our scientific discourse and it's a sad fact about the current state of the world that almost none of these images are made non-visually accessible so blind and low vision users are basically unable to understand all this context and receive all of this information something has to change that image-based text generation has become incredibly good over the last 10 years that's another story of astounding progress but it has yet to take us to the point where we can actually write useful descriptions of these images that would help a BLB user and that last bit is going to require HCI research linguistic research and fundamental advances in AI and by the way lots of astounding new data sets and this is just one example of in the innumerable number of applied problems that fall into this mode and that can be very exciting for people who have domain expertise that can help us close that final mile so let me wrap up here I don't want to have a standard conclusion I think it's fun to close with some predictions about the future and I have put this under the heading of predictions for the text next 10 years or so although I'm about to retract that for reasons I will get to but here are the predictions first laggard industries that are rich in text data will be transformed in part by NLP technology and that's likely to happen from some disruptive newcomers coming out of left field second prediction artificial assistants will get dramatically better and become more ubiquitous with the side effect that you'll often be unsure in life whether this customer service representative is a person or an AI or some team combining the two many kinds of writing including student papers at universities will be done with AI writing assistants and this might be transparently true given how sophisticated autocomplete and other tools have gotten at this point and then finally the negative effects of NLP and of AI will be amplified along with the positives I'm thinking of things like disinformation spread market disruption systemic bias it's almost sure to be the case if it hasn't already happened already that there will be some calamitous world event that traces to the intentional or unintentional misuse of some AI technology that's in our future so I think these are reasonable predictions and I'm curious for yours but I have to tell you that I made these predictions in 2020 two years ago with the expectation that they would be good for 10 years but more than half of them probably have already come true two and three are definitely true about the world we live in and on the flip side I just failed to predict so many important things like the most prominent example is that I just failed to predict the progress we would see in text image models like dolly two and and stable diffusion in fact I'll be honest with you I might have bet against them I thought that was an area that was going to languish for a long time and yet nonetheless seemingly out of nowhere we had this incredible set of advances and there are probably lots of other areas where I would make similarly bad predictions so I said 10 years but I think my new rule is going to be that I'm going to predict only through 2024 at the very outside because in 10 years the only thing I can say with confidence is that we will be in a radically different place from where we are now but what that place will be like is anyone's guess I'm interested in your predictions about it but I think I will stop here thank you very much thank you so much Chris for the engaging and extremely interesting topic and presentation you have given I'm always amazed by all the new things you're mentioning every single time we talk I feel it is something new something exciting you know not you not me especially not me like expected if you'll be talking about it so soon many questions came in I must already see people unfortunately not be able to get to all of them because the time is limited and the audience is so active and so many people showed up so let me pick a few um Chris so the cost of the training model so it seems it really scales with the size and we are paying a lot of attention and like putting a lot of effort into the training uh so what does it mean for the energy requirements and I guess we are talking about predictions but like how does it look like now and like what do you recommend people to to pay attention to oh it's a wonderful set of questions to be answering and critically important I mean I ask myself you know you know if you think about industries in the world some of them are improving in terms of their environmental impacts some are getting much worse where is artificial intelligence in that is it getting better or is it getting worse I don't know the answer because on the one hand the expenditure for training and now serving for example GPT-3 to everyone who wants to use it is absolutely enormous and it has real costs like measured in emissions and things like that on the other hand this is a centralization of all of that and that can often bring real benefits and I want to not forget of the previous era where every single person trained every single model from scratch and so now a lot of our research is actually just using these frozen components they were expensive but the expenditure of our lab is probably going way down because we are not training these big models it kind of reminds me of that last mile problem again in the previous era it was like we were all driving to pick up our groceries everywhere huge expenditure with all those individual trips now it's much more like they're all brought to the end of the street and we walk to get them but of course that's done in big trucks and those have real consequences as well I don't know but I hope that a lot of smart people work continue to work on this problem and that'll lead to benefits in terms of us doing all these things more efficiently as well thank you so much the next question and you touched on that a few times but it might be good to summarize that a little bit because we got a lot of the questions about kind of the trustworthiness and if the model actually knows that it's wrong or correct and like how do how do we trust the model or like how do we achieve the trustworthiness of the model because right now it's a lot of the generation happening generative models happening so like how do we pass that it's an incredibly good question and it is the thing I have in mind when we're doing all our work on explaining models because I feel like offering faithful human interpretable explanations is the step we can take toward trustworthiness it's a very difficult problem I just want to add that it might be even harder than we've anticipated because people are also pretty untrustworthy it's just that individual people often don't have like a systemic effect right so if you're really doing a poor job at something you probably impact just a handful of people and other people say at your company do a much better job but these ai's are now it's like they're everyone and so any kind of small problem that they have is amplified across the entire population they interact with and that's going to probably mean that our standards for trustworthiness for them need to be higher than they are for humans and that's another sense in which they're going to have to be superhuman to achieve the jobs we're asking of them and the field cannot offer guarantees right now so come help us fascinating thank you so much and like I saw also some questions or comments about the bias in data and like you mentioned it also right like you like we are improving like there is a big improvement happening um last question for you um like a little bit of a thought experiment but like do you think that the large language models might be able to come up with answers to as yet unanswered important scientific questions like something we are not even sure that it even exists like in our minds right now oh it's a wonderful question yeah and people are asking this across multiple domains like they're producing incredible artwork but are we now trapped inside a feedback loop that's going to lead to less truly innovative art and and if we ask them to generate text are they going to do either weird irrelevant stuff or just more of the boring average case stuff um I don't know the answer I will say though that these models have an incredible capacity to synthesize information across sources and I feel like that is a source of innovation for humans as well simply making those connections and it might be true that there is nothing new under the sun but there are lots of new connections perspectives and so forth to be had and I actually do have faith that models are going to be able to at least simulate some of that and it might look to us like innovation but this is not to say that this is uh not a concern for us it should be something we think about especially because we might be heading into an era when whether we want them to or not mostly these models are trained on their own output which is being put on the web and then consumed when people create train sets and so forth and so on yeah great thank you so much and we are nearing the end so like last point um do you have any like last remarks any anything anything interesting you would suggest others to look at follow read um learn about to kind of get more acquainted with the subject well learn more about the NLU GPT-3 other large language models and their recommendations the thing that comes to mind based on all the interactions I have with the professional development students who have taken our course before is that a lot of you I'm guessing have incredibly valuable valuable domain expertise you work in an industry in a position that has taught you tons of things and given you lots of skills and my last mile problem shows you that that is relevant to AI and therefore you could bring it to bear on AI and we might all benefit where you would be taking all these innovations you can learn about in our course and other courses combining that with your domain expertise and maybe actually making progress in a meaningful way on a problem as opposed to merely having demos and things that our scientific community often produces real impact so often requires real domain expertise of the sort you all have great thank you so much um and yeah at the end thank you so much Chris for taking the time to do this I know beginning of the quarter hectic Stanford live and I appreciate you taking the time to do this to run this webinar thank you also everybody who had a chance to join us live or like who's watching this recording if you could please let us know what kind of other topics you might be interested in in this sort of a free webinar structure we have a little survey down on the console and yeah I hope you all have a great day a wonderful start of the of or like end of the winter start of the spring and yeah thank you everybody for joining us yeah Petra this is wonderful we got an astounding number of really great questions it's too bad we're out of time there's a lot to think about here and so that's just another thank you to the audience for all this food for thought thank you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.36, "text": " So Chris Potts is a professor and actually also the chair of the Department of Linguistics", "tokens": [50364, 407, 6688, 9145, 1373, 307, 257, 8304, 293, 767, 611, 264, 6090, 295, 264, 5982, 295, 441, 7050, 6006, 51132], "temperature": 0.0, "avg_logprob": -0.2022546132405599, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.139369934797287}, {"id": 1, "seek": 0, "start": 15.36, "end": 20.400000000000002, "text": " and by courtesy also at the Department of Computer Science and he's a great expert in", "tokens": [51132, 293, 538, 41704, 611, 412, 264, 5982, 295, 22289, 8976, 293, 415, 311, 257, 869, 5844, 294, 51384], "temperature": 0.0, "avg_logprob": -0.2022546132405599, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.139369934797287}, {"id": 2, "seek": 0, "start": 20.400000000000002, "end": 24.400000000000002, "text": " the area of natural language understanding so he's you know there would not be a better", "tokens": [51384, 264, 1859, 295, 3303, 2856, 3701, 370, 415, 311, 291, 458, 456, 576, 406, 312, 257, 1101, 51584], "temperature": 0.0, "avg_logprob": -0.2022546132405599, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.139369934797287}, {"id": 3, "seek": 0, "start": 24.400000000000002, "end": 29.16, "text": " person to hear about a topic than him and we are so grateful that he could make the", "tokens": [51584, 954, 281, 1568, 466, 257, 4829, 813, 796, 293, 321, 366, 370, 7941, 300, 415, 727, 652, 264, 51822], "temperature": 0.0, "avg_logprob": -0.2022546132405599, "compression_ratio": 1.6415094339622642, "no_speech_prob": 0.139369934797287}, {"id": 4, "seek": 2916, "start": 29.16, "end": 34.76, "text": " time and he's actually also teaching the graduate course CS22 for you natural language", "tokens": [50364, 565, 293, 415, 311, 767, 611, 4571, 264, 8080, 1164, 9460, 7490, 337, 291, 3303, 2856, 50644], "temperature": 0.0, "avg_logprob": -0.11725176811218262, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.05610346049070358}, {"id": 5, "seek": 2916, "start": 34.76, "end": 39.88, "text": " understanding that we actually transform into a professional course that is starting next week", "tokens": [50644, 3701, 300, 321, 767, 4088, 666, 257, 4843, 1164, 300, 307, 2891, 958, 1243, 50900], "temperature": 0.0, "avg_logprob": -0.11725176811218262, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.05610346049070358}, {"id": 6, "seek": 2916, "start": 39.88, "end": 44.92, "text": " on the same topic so you know if you're interested in learning more we have some links included", "tokens": [50900, 322, 264, 912, 4829, 370, 291, 458, 498, 291, 434, 3102, 294, 2539, 544, 321, 362, 512, 6123, 5556, 51152], "temperature": 0.0, "avg_logprob": -0.11725176811218262, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.05610346049070358}, {"id": 7, "seek": 2916, "start": 45.56, "end": 50.760000000000005, "text": " you know down below on your platform you can check it out and you know there's so many other", "tokens": [51184, 291, 458, 760, 2507, 322, 428, 3663, 291, 393, 1520, 309, 484, 293, 291, 458, 456, 311, 370, 867, 661, 51444], "temperature": 0.0, "avg_logprob": -0.11725176811218262, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.05610346049070358}, {"id": 8, "seek": 2916, "start": 50.760000000000005, "end": 55.32, "text": " things that can be said about Chris like he has a super interesting podcast he's running like so", "tokens": [51444, 721, 300, 393, 312, 848, 466, 6688, 411, 415, 575, 257, 1687, 1880, 7367, 415, 311, 2614, 411, 370, 51672], "temperature": 0.0, "avg_logprob": -0.11725176811218262, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.05610346049070358}, {"id": 9, "seek": 5532, "start": 55.32, "end": 61.16, "text": " many interesting research papers like projects he worked on so you know go ahead and learn more", "tokens": [50364, 867, 1880, 2132, 10577, 411, 4455, 415, 2732, 322, 370, 291, 458, 352, 2286, 293, 1466, 544, 50656], "temperature": 0.0, "avg_logprob": -0.08081154770903536, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.013204599730670452}, {"id": 10, "seek": 5532, "start": 61.16, "end": 67.24, "text": " about him like you should also have a little link I think without further ado I think we can kick it", "tokens": [50656, 466, 796, 411, 291, 820, 611, 362, 257, 707, 2113, 286, 519, 1553, 3052, 22450, 286, 519, 321, 393, 4437, 309, 50960], "temperature": 0.0, "avg_logprob": -0.08081154770903536, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.013204599730670452}, {"id": 11, "seek": 5532, "start": 67.24, "end": 73.8, "text": " off Chris thank you so much once again oh thank you so much Petra for the kind words and welcome", "tokens": [50960, 766, 6688, 1309, 291, 370, 709, 1564, 797, 1954, 1309, 291, 370, 709, 10472, 424, 337, 264, 733, 2283, 293, 2928, 51288], "temperature": 0.0, "avg_logprob": -0.08081154770903536, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.013204599730670452}, {"id": 12, "seek": 5532, "start": 73.8, "end": 80.2, "text": " to everyone it's wonderful to be here with you all I do think that we live in a golden age for", "tokens": [51288, 281, 1518, 309, 311, 3715, 281, 312, 510, 365, 291, 439, 286, 360, 519, 300, 321, 1621, 294, 257, 9729, 3205, 337, 51608], "temperature": 0.0, "avg_logprob": -0.08081154770903536, "compression_ratio": 1.6581196581196582, "no_speech_prob": 0.013204599730670452}, {"id": 13, "seek": 8020, "start": 80.28, "end": 86.36, "text": " natural language understanding maybe also a disconcerting age a weird age but certainly", "tokens": [50368, 3303, 2856, 3701, 1310, 611, 257, 717, 1671, 1776, 783, 3205, 257, 3657, 3205, 457, 3297, 50672], "temperature": 0.0, "avg_logprob": -0.056468434450103015, "compression_ratio": 1.5732758620689655, "no_speech_prob": 0.3590393364429474}, {"id": 14, "seek": 8020, "start": 86.36, "end": 91.48, "text": " a time of a lot of innovation and a lot of change it's sort of an interesting moment for", "tokens": [50672, 257, 565, 295, 257, 688, 295, 8504, 293, 257, 688, 295, 1319, 309, 311, 1333, 295, 364, 1880, 1623, 337, 50928], "temperature": 0.0, "avg_logprob": -0.056468434450103015, "compression_ratio": 1.5732758620689655, "no_speech_prob": 0.3590393364429474}, {"id": 15, "seek": 8020, "start": 91.48, "end": 99.64, "text": " reflection for me because I started teaching my NLU course at Stanford in 2012 about a decade ago", "tokens": [50928, 12914, 337, 385, 570, 286, 1409, 4571, 452, 426, 43, 52, 1164, 412, 20374, 294, 9125, 466, 257, 10378, 2057, 51336], "temperature": 0.0, "avg_logprob": -0.056468434450103015, "compression_ratio": 1.5732758620689655, "no_speech_prob": 0.3590393364429474}, {"id": 16, "seek": 8020, "start": 100.28, "end": 106.92, "text": " that feels very recent in my lived experience but it feels like a completely different age", "tokens": [51368, 300, 3417, 588, 5162, 294, 452, 5152, 1752, 457, 309, 3417, 411, 257, 2584, 819, 3205, 51700], "temperature": 0.0, "avg_logprob": -0.056468434450103015, "compression_ratio": 1.5732758620689655, "no_speech_prob": 0.3590393364429474}, {"id": 17, "seek": 10692, "start": 106.92, "end": 113.32000000000001, "text": " when it comes to NLU and indeed all of artificial intelligence I I never would have guessed in 2012", "tokens": [50364, 562, 309, 1487, 281, 426, 43, 52, 293, 6451, 439, 295, 11677, 7599, 286, 286, 1128, 576, 362, 21852, 294, 9125, 50684], "temperature": 0.0, "avg_logprob": -0.040194915562141234, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.02712583728134632}, {"id": 18, "seek": 10692, "start": 113.32000000000001, "end": 119.72, "text": " that we would have such an amazing array of technologies and scientific innovations and", "tokens": [50684, 300, 321, 576, 362, 1270, 364, 2243, 10225, 295, 7943, 293, 8134, 24283, 293, 51004], "temperature": 0.0, "avg_logprob": -0.040194915562141234, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.02712583728134632}, {"id": 19, "seek": 10692, "start": 119.72, "end": 126.84, "text": " that we would have these models that were just so performant and also so widely deployed in the", "tokens": [51004, 300, 321, 576, 362, 613, 5245, 300, 645, 445, 370, 2042, 394, 293, 611, 370, 13371, 17826, 294, 264, 51360], "temperature": 0.0, "avg_logprob": -0.040194915562141234, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.02712583728134632}, {"id": 20, "seek": 10692, "start": 126.84, "end": 134.36, "text": " world this is also a story of again for better or worse increasing societal impact and so that", "tokens": [51360, 1002, 341, 307, 611, 257, 1657, 295, 797, 337, 1101, 420, 5324, 5662, 33472, 2712, 293, 370, 300, 51736], "temperature": 0.0, "avg_logprob": -0.040194915562141234, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.02712583728134632}, {"id": 21, "seek": 13436, "start": 134.36, "end": 139.16000000000003, "text": " does come together for me into a golden age and just to reflect on this a little bit it's really", "tokens": [50364, 775, 808, 1214, 337, 385, 666, 257, 9729, 3205, 293, 445, 281, 5031, 322, 341, 257, 707, 857, 309, 311, 534, 50604], "temperature": 0.0, "avg_logprob": -0.0777257804212899, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.049549613147974014}, {"id": 22, "seek": 13436, "start": 139.16000000000003, "end": 145.16000000000003, "text": " just amazing to think about how many of these models you can get hands on with if you want to", "tokens": [50604, 445, 2243, 281, 519, 466, 577, 867, 295, 613, 5245, 291, 393, 483, 2377, 322, 365, 498, 291, 528, 281, 50904], "temperature": 0.0, "avg_logprob": -0.0777257804212899, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.049549613147974014}, {"id": 23, "seek": 13436, "start": 145.16000000000003, "end": 151.8, "text": " right away right you can download or use via apis models like dolly 2 that do incredible text to", "tokens": [50904, 558, 1314, 558, 291, 393, 5484, 420, 764, 5766, 1882, 271, 5245, 411, 2722, 88, 568, 300, 360, 4651, 2487, 281, 51236], "temperature": 0.0, "avg_logprob": -0.0777257804212899, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.049549613147974014}, {"id": 24, "seek": 13436, "start": 151.8, "end": 157.72000000000003, "text": " image generation stable diffusion mid-journey they're all in that class we also have github", "tokens": [51236, 3256, 5125, 8351, 25242, 2062, 12, 8696, 2397, 436, 434, 439, 294, 300, 1508, 321, 611, 362, 290, 355, 836, 51532], "temperature": 0.0, "avg_logprob": -0.0777257804212899, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.049549613147974014}, {"id": 25, "seek": 13436, "start": 157.72000000000003, "end": 163.64000000000001, "text": " co-pilot based in the codex model for doing code generation tons of people derive a lot of value", "tokens": [51532, 598, 12, 79, 31516, 2361, 294, 264, 3089, 87, 2316, 337, 884, 3089, 5125, 9131, 295, 561, 28446, 257, 688, 295, 2158, 51828], "temperature": 0.0, "avg_logprob": -0.0777257804212899, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.049549613147974014}, {"id": 26, "seek": 16364, "start": 163.64, "end": 170.51999999999998, "text": " from that system u.com is at the leading edge I would say of search technologies that are changing", "tokens": [50364, 490, 300, 1185, 344, 13, 1112, 307, 412, 264, 5775, 4691, 286, 576, 584, 295, 3164, 7943, 300, 366, 4473, 50708], "temperature": 0.0, "avg_logprob": -0.06190803752225988, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.009404513984918594}, {"id": 27, "seek": 16364, "start": 170.51999999999998, "end": 175.0, "text": " the search experience and also leading us to new and better results when we search on the web", "tokens": [50708, 264, 3164, 1752, 293, 611, 5775, 505, 281, 777, 293, 1101, 3542, 562, 321, 3164, 322, 264, 3670, 50932], "temperature": 0.0, "avg_logprob": -0.06190803752225988, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.009404513984918594}, {"id": 28, "seek": 16364, "start": 176.04, "end": 183.0, "text": " whisper ai is an incredible model from open ai this does speech to text and this model is a", "tokens": [50984, 26018, 9783, 307, 364, 4651, 2316, 490, 1269, 9783, 341, 775, 6218, 281, 2487, 293, 341, 2316, 307, 257, 51332], "temperature": 0.0, "avg_logprob": -0.06190803752225988, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.009404513984918594}, {"id": 29, "seek": 16364, "start": 183.0, "end": 190.27999999999997, "text": " generic model that is better than the best user customized models that we had 10 years ago just", "tokens": [51332, 19577, 2316, 300, 307, 1101, 813, 264, 1151, 4195, 30581, 5245, 300, 321, 632, 1266, 924, 2057, 445, 51696], "temperature": 0.0, "avg_logprob": -0.06190803752225988, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.009404513984918594}, {"id": 30, "seek": 19028, "start": 190.28, "end": 194.84, "text": " astounding not something I would have predicted I think and then of course the star of our show", "tokens": [50364, 5357, 24625, 406, 746, 286, 576, 362, 19147, 286, 519, 293, 550, 295, 1164, 264, 3543, 295, 527, 855, 50592], "temperature": 0.0, "avg_logprob": -0.08853891160753039, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.24177780747413635}, {"id": 31, "seek": 19028, "start": 194.84, "end": 202.2, "text": " for today is going to be these big language models gpt 3 is the famous one you can use it via an api", "tokens": [50592, 337, 965, 307, 516, 281, 312, 613, 955, 2856, 5245, 290, 662, 805, 307, 264, 4618, 472, 291, 393, 764, 309, 5766, 364, 1882, 72, 50960], "temperature": 0.0, "avg_logprob": -0.08853891160753039, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.24177780747413635}, {"id": 32, "seek": 19028, "start": 202.2, "end": 208.84, "text": " we have all these open source ones as well that have come out opt bloom gpt neo x these are models", "tokens": [50960, 321, 362, 439, 613, 1269, 4009, 2306, 382, 731, 300, 362, 808, 484, 277, 662, 26899, 290, 662, 41977, 2031, 613, 366, 5245, 51292], "temperature": 0.0, "avg_logprob": -0.08853891160753039, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.24177780747413635}, {"id": 33, "seek": 19028, "start": 208.84, "end": 213.8, "text": " that you can download and work with to your heart's content provided that you have all the computing", "tokens": [51292, 300, 291, 393, 5484, 293, 589, 365, 281, 428, 1917, 311, 2701, 5649, 300, 291, 362, 439, 264, 15866, 51540], "temperature": 0.0, "avg_logprob": -0.08853891160753039, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.24177780747413635}, {"id": 34, "seek": 19028, "start": 213.8, "end": 219.56, "text": " resources necessary so just incredible and I'm sure you're familiar with this but let's just you", "tokens": [51540, 3593, 4818, 370, 445, 4651, 293, 286, 478, 988, 291, 434, 4963, 365, 341, 457, 718, 311, 445, 291, 51828], "temperature": 0.0, "avg_logprob": -0.08853891160753039, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.24177780747413635}, {"id": 35, "seek": 21956, "start": 219.56, "end": 224.6, "text": " know get this into our common ground here it's incredible what these models can do here's a quick", "tokens": [50364, 458, 483, 341, 666, 527, 2689, 2727, 510, 309, 311, 4651, 437, 613, 5245, 393, 360, 510, 311, 257, 1702, 50616], "temperature": 0.0, "avg_logprob": -0.09157748606013155, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0050520021468400955}, {"id": 36, "seek": 21956, "start": 224.6, "end": 233.96, "text": " demo of gpt 3 I asked the da Vinci 2 engine in which year was stanford university founded", "tokens": [50616, 10723, 295, 290, 662, 805, 286, 2351, 264, 1120, 15011, 537, 568, 2848, 294, 597, 1064, 390, 27984, 7404, 5454, 13234, 51084], "temperature": 0.0, "avg_logprob": -0.09157748606013155, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0050520021468400955}, {"id": 37, "seek": 21956, "start": 233.96, "end": 239.0, "text": " when did it enroll its first students who is its current president and what is its mascot", "tokens": [51084, 562, 630, 309, 12266, 1080, 700, 1731, 567, 307, 1080, 2190, 3868, 293, 437, 307, 1080, 42339, 51336], "temperature": 0.0, "avg_logprob": -0.09157748606013155, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0050520021468400955}, {"id": 38, "seek": 21956, "start": 239.0, "end": 246.2, "text": " and da Vinci 2 gave a fluent and complete answer that is correct on all counts just incredible", "tokens": [51336, 293, 1120, 15011, 537, 568, 2729, 257, 40799, 293, 3566, 1867, 300, 307, 3006, 322, 439, 14893, 445, 4651, 51696], "temperature": 0.0, "avg_logprob": -0.09157748606013155, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0050520021468400955}, {"id": 39, "seek": 24620, "start": 246.92, "end": 252.67999999999998, "text": " that was with da Vinci 2 we got a big update to that model in late 2022 that's da Vinci 3", "tokens": [50400, 300, 390, 365, 1120, 15011, 537, 568, 321, 658, 257, 955, 5623, 281, 300, 2316, 294, 3469, 20229, 300, 311, 1120, 15011, 537, 805, 50688], "temperature": 0.0, "avg_logprob": -0.07494255003890371, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.007117571774870157}, {"id": 40, "seek": 24620, "start": 252.67999999999998, "end": 258.36, "text": " and here I'm showing you that it reproduces that results exactly and I do think that da Vinci 3 is", "tokens": [50688, 293, 510, 286, 478, 4099, 291, 300, 309, 11408, 887, 300, 3542, 2293, 293, 286, 360, 519, 300, 1120, 15011, 537, 805, 307, 50972], "temperature": 0.0, "avg_logprob": -0.07494255003890371, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.007117571774870157}, {"id": 41, "seek": 24620, "start": 258.36, "end": 264.36, "text": " a big step forward over the previous engine here's actually an example of that you know I have like", "tokens": [50972, 257, 955, 1823, 2128, 670, 264, 3894, 2848, 510, 311, 767, 364, 1365, 295, 300, 291, 458, 286, 362, 411, 51272], "temperature": 0.0, "avg_logprob": -0.07494255003890371, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.007117571774870157}, {"id": 42, "seek": 24620, "start": 264.36, "end": 270.12, "text": " to play adversarial games with this model and so I asked da Vinci 2 would it be possible to hire a", "tokens": [51272, 281, 862, 17641, 44745, 2813, 365, 341, 2316, 293, 370, 286, 2351, 1120, 15011, 537, 568, 576, 309, 312, 1944, 281, 11158, 257, 51560], "temperature": 0.0, "avg_logprob": -0.07494255003890371, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.007117571774870157}, {"id": 43, "seek": 24620, "start": 270.12, "end": 275.32, "text": " team of tamarins to help me paint my house assuming I'm willing to pay them in sufficient quantities", "tokens": [51560, 1469, 295, 7677, 289, 1292, 281, 854, 385, 4225, 452, 1782, 11926, 286, 478, 4950, 281, 1689, 552, 294, 11563, 22927, 51820], "temperature": 0.0, "avg_logprob": -0.07494255003890371, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.007117571774870157}, {"id": 44, "seek": 27532, "start": 275.32, "end": 280.44, "text": " of fruit to meet minimum wage requirements in california this is adversarial because I know", "tokens": [50364, 295, 6773, 281, 1677, 7285, 15444, 7728, 294, 2104, 5203, 654, 341, 307, 17641, 44745, 570, 286, 458, 50620], "temperature": 0.0, "avg_logprob": -0.02683874766031901, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0035923742689192295}, {"id": 45, "seek": 27532, "start": 280.44, "end": 285.96, "text": " that these models don't have a really rich understanding of the world we live in they're", "tokens": [50620, 300, 613, 5245, 500, 380, 362, 257, 534, 4593, 3701, 295, 264, 1002, 321, 1621, 294, 436, 434, 50896], "temperature": 0.0, "avg_logprob": -0.02683874766031901, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0035923742689192295}, {"id": 46, "seek": 27532, "start": 285.96, "end": 291.48, "text": " often distracted by details like this and sure enough da Vinci 2 got confused yes it would be", "tokens": [50896, 2049, 21658, 538, 4365, 411, 341, 293, 988, 1547, 1120, 15011, 537, 568, 658, 9019, 2086, 309, 576, 312, 51172], "temperature": 0.0, "avg_logprob": -0.02683874766031901, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0035923742689192295}, {"id": 47, "seek": 27532, "start": 291.48, "end": 295.32, "text": " possible to hire a team of tamarins to paint your house you would need to make sure that you're", "tokens": [51172, 1944, 281, 11158, 257, 1469, 295, 7677, 289, 1292, 281, 4225, 428, 1782, 291, 576, 643, 281, 652, 988, 300, 291, 434, 51364], "temperature": 0.0, "avg_logprob": -0.02683874766031901, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0035923742689192295}, {"id": 48, "seek": 27532, "start": 295.32, "end": 301.56, "text": " providing them with enough fruit to meet minimum wage requirements and so forth so easily distracted", "tokens": [51364, 6530, 552, 365, 1547, 6773, 281, 1677, 7285, 15444, 7728, 293, 370, 5220, 370, 3612, 21658, 51676], "temperature": 0.0, "avg_logprob": -0.02683874766031901, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0035923742689192295}, {"id": 49, "seek": 30156, "start": 301.56, "end": 307.48, "text": " but I tried this again with da Vinci 3 and with the same question it gave a very sensible answer no", "tokens": [50364, 457, 286, 3031, 341, 797, 365, 1120, 15011, 537, 805, 293, 365, 264, 912, 1168, 309, 2729, 257, 588, 25380, 1867, 572, 50660], "temperature": 0.0, "avg_logprob": -0.05317606764324641, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0021817893721163273}, {"id": 50, "seek": 30156, "start": 307.48, "end": 312.28000000000003, "text": " it would not be possible to hire a team of tamarins to help you paint your house da Vinci 2 was not", "tokens": [50660, 309, 576, 406, 312, 1944, 281, 11158, 257, 1469, 295, 7677, 289, 1292, 281, 854, 291, 4225, 428, 1782, 1120, 15011, 537, 568, 390, 406, 50900], "temperature": 0.0, "avg_logprob": -0.05317606764324641, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0021817893721163273}, {"id": 51, "seek": 30156, "start": 312.28000000000003, "end": 318.28, "text": " distracted by my adversarial game this is not to say that you can't trick da Vinci 2 just go on to", "tokens": [50900, 21658, 538, 452, 17641, 44745, 1216, 341, 307, 406, 281, 584, 300, 291, 393, 380, 4282, 1120, 15011, 537, 568, 445, 352, 322, 281, 51200], "temperature": 0.0, "avg_logprob": -0.05317606764324641, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0021817893721163273}, {"id": 52, "seek": 30156, "start": 318.28, "end": 324.12, "text": " twitter and you'll find examples of that but again I do think we're seeing a pretty remarkable rate", "tokens": [51200, 21439, 293, 291, 603, 915, 5110, 295, 300, 457, 797, 286, 360, 519, 321, 434, 2577, 257, 1238, 12802, 3314, 51492], "temperature": 0.0, "avg_logprob": -0.05317606764324641, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0021817893721163273}, {"id": 53, "seek": 30156, "start": 324.12, "end": 331.48, "text": " of progress toward these models being robust and relatively trustworthy this is also a story", "tokens": [51492, 295, 4205, 7361, 613, 5245, 885, 13956, 293, 7226, 39714, 341, 307, 611, 257, 1657, 51860], "temperature": 0.0, "avg_logprob": -0.05317606764324641, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0021817893721163273}, {"id": 54, "seek": 33148, "start": 331.56, "end": 336.76, "text": " of scientific innovation that was a brief anecdote but we're seeing this same level of progress", "tokens": [50368, 295, 8134, 8504, 300, 390, 257, 5353, 49845, 457, 321, 434, 2577, 341, 912, 1496, 295, 4205, 50628], "temperature": 0.0, "avg_logprob": -0.061745656984988774, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.009403591975569725}, {"id": 55, "seek": 33148, "start": 336.76, "end": 341.64000000000004, "text": " in the tools that we use to measure system performance in the field I've put this under", "tokens": [50628, 294, 264, 3873, 300, 321, 764, 281, 3481, 1185, 3389, 294, 264, 2519, 286, 600, 829, 341, 833, 50872], "temperature": 0.0, "avg_logprob": -0.061745656984988774, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.009403591975569725}, {"id": 56, "seek": 33148, "start": 341.64000000000004, "end": 347.0, "text": " the heading of benchmark saturate faster than ever this is from a paper from 2021 that I was", "tokens": [50872, 264, 9864, 295, 18927, 21160, 473, 4663, 813, 1562, 341, 307, 490, 257, 3035, 490, 7201, 300, 286, 390, 51140], "temperature": 0.0, "avg_logprob": -0.061745656984988774, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.009403591975569725}, {"id": 57, "seek": 33148, "start": 347.0, "end": 352.68, "text": " involved with Kila et al here's the framework along the x-axis I have time going back to the", "tokens": [51140, 3288, 365, 591, 7371, 1030, 419, 510, 311, 264, 8388, 2051, 264, 2031, 12, 24633, 286, 362, 565, 516, 646, 281, 264, 51424], "temperature": 0.0, "avg_logprob": -0.061745656984988774, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.009403591975569725}, {"id": 58, "seek": 33148, "start": 352.68, "end": 359.48, "text": " 1990s and along the y-axis I have a normalized measure of our estimate of human performance", "tokens": [51424, 13384, 82, 293, 2051, 264, 288, 12, 24633, 286, 362, 257, 48704, 3481, 295, 527, 12539, 295, 1952, 3389, 51764], "temperature": 0.0, "avg_logprob": -0.061745656984988774, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.009403591975569725}, {"id": 59, "seek": 35948, "start": 359.48, "end": 366.52000000000004, "text": " that's the red line set at zero so MNIST digit recognition a grand old data set in the field", "tokens": [50364, 300, 311, 264, 2182, 1622, 992, 412, 4018, 370, 376, 45, 19756, 14293, 11150, 257, 2697, 1331, 1412, 992, 294, 264, 2519, 50716], "temperature": 0.0, "avg_logprob": -0.05903126331085854, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0028864420019090176}, {"id": 60, "seek": 35948, "start": 366.52000000000004, "end": 371.72, "text": " that was launched in the 1990s and it took about 20 years for us to surpass this estimate of human", "tokens": [50716, 300, 390, 8730, 294, 264, 13384, 82, 293, 309, 1890, 466, 945, 924, 337, 505, 281, 27650, 341, 12539, 295, 1952, 50976], "temperature": 0.0, "avg_logprob": -0.05903126331085854, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0028864420019090176}, {"id": 61, "seek": 35948, "start": 371.72, "end": 377.64000000000004, "text": " performance switchboard is a similar story launched in the 90s this is the speech to text problem it", "tokens": [50976, 3389, 3679, 3787, 307, 257, 2531, 1657, 8730, 294, 264, 4289, 82, 341, 307, 264, 6218, 281, 2487, 1154, 309, 51272], "temperature": 0.0, "avg_logprob": -0.05903126331085854, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0028864420019090176}, {"id": 62, "seek": 35948, "start": 377.64000000000004, "end": 384.20000000000005, "text": " took about 20 years for us to get up past this red line here image net is newer this was launched", "tokens": [51272, 1890, 466, 945, 924, 337, 505, 281, 483, 493, 1791, 341, 2182, 1622, 510, 3256, 2533, 307, 17628, 341, 390, 8730, 51600], "temperature": 0.0, "avg_logprob": -0.05903126331085854, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0028864420019090176}, {"id": 63, "seek": 38420, "start": 384.2, "end": 390.68, "text": " in 2009 it took about 10 years for us to reach this saturation point and from here the pace is", "tokens": [50364, 294, 11453, 309, 1890, 466, 1266, 924, 337, 505, 281, 2524, 341, 27090, 935, 293, 490, 510, 264, 11638, 307, 50688], "temperature": 0.0, "avg_logprob": -0.04667187581020119, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.0541318915784359}, {"id": 64, "seek": 38420, "start": 390.68, "end": 396.03999999999996, "text": " really going to pick up so squad 1.1 is question answering that was solved in about three years", "tokens": [50688, 534, 516, 281, 1888, 493, 370, 15310, 502, 13, 16, 307, 1168, 13430, 300, 390, 13041, 294, 466, 1045, 924, 50956], "temperature": 0.0, "avg_logprob": -0.04667187581020119, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.0541318915784359}, {"id": 65, "seek": 38420, "start": 396.84, "end": 403.24, "text": " the response was squad 2.0 that was solved in less than two years and then the glue benchmark", "tokens": [50996, 264, 4134, 390, 15310, 568, 13, 15, 300, 390, 13041, 294, 1570, 813, 732, 924, 293, 550, 264, 8998, 18927, 51316], "temperature": 0.0, "avg_logprob": -0.04667187581020119, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.0541318915784359}, {"id": 66, "seek": 38420, "start": 403.24, "end": 407.71999999999997, "text": " if you were in the field you might recall back the glue benchmark is this big set of tasks that", "tokens": [51316, 498, 291, 645, 294, 264, 2519, 291, 1062, 9901, 646, 264, 8998, 18927, 307, 341, 955, 992, 295, 9608, 300, 51540], "temperature": 0.0, "avg_logprob": -0.04667187581020119, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.0541318915784359}, {"id": 67, "seek": 38420, "start": 407.71999999999997, "end": 413.8, "text": " was meant to stress test our best models when it was announced a lot of us worried that it was just", "tokens": [51540, 390, 4140, 281, 4244, 1500, 527, 1151, 5245, 562, 309, 390, 7548, 257, 688, 295, 505, 5804, 300, 309, 390, 445, 51844], "temperature": 0.0, "avg_logprob": -0.04667187581020119, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.0541318915784359}, {"id": 68, "seek": 41380, "start": 413.8, "end": 419.48, "text": " too hard for present day models but glue was saturated in less than a year the response was", "tokens": [50364, 886, 1152, 337, 1974, 786, 5245, 457, 8998, 390, 25408, 294, 1570, 813, 257, 1064, 264, 4134, 390, 50648], "temperature": 0.0, "avg_logprob": -0.054443138401682786, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0020815939642488956}, {"id": 69, "seek": 41380, "start": 419.48, "end": 425.56, "text": " super glue meant to be much harder it was also saturated in less than a year a remarkable", "tokens": [50648, 1687, 8998, 4140, 281, 312, 709, 6081, 309, 390, 611, 25408, 294, 1570, 813, 257, 1064, 257, 12802, 50952], "temperature": 0.0, "avg_logprob": -0.054443138401682786, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0020815939642488956}, {"id": 70, "seek": 41380, "start": 425.56, "end": 430.44, "text": " story of progress undoubtedly even if you're cynical about this measure of human performance", "tokens": [50952, 1657, 295, 4205, 35211, 754, 498, 291, 434, 46345, 466, 341, 3481, 295, 1952, 3389, 51196], "temperature": 0.0, "avg_logprob": -0.054443138401682786, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0020815939642488956}, {"id": 71, "seek": 41380, "start": 430.44, "end": 437.64, "text": " we are still seeing a rapid increase in the rate of change here and you know 2021 was ages ago in", "tokens": [51196, 321, 366, 920, 2577, 257, 7558, 3488, 294, 264, 3314, 295, 1319, 510, 293, 291, 458, 7201, 390, 12357, 2057, 294, 51556], "temperature": 0.0, "avg_logprob": -0.054443138401682786, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0020815939642488956}, {"id": 72, "seek": 43764, "start": 437.64, "end": 443.64, "text": " the story of AI now I think this same thing carries over into the current era with our largest", "tokens": [50364, 264, 1657, 295, 7318, 586, 286, 519, 341, 912, 551, 16402, 670, 666, 264, 2190, 4249, 365, 527, 6443, 50664], "temperature": 0.0, "avg_logprob": -0.06824333100091844, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.3626236915588379}, {"id": 73, "seek": 43764, "start": 443.64, "end": 449.47999999999996, "text": " language models this is from a really nice post from Jason Wei he is assessing emergent abilities", "tokens": [50664, 2856, 5245, 341, 307, 490, 257, 534, 1481, 2183, 490, 11181, 21174, 415, 307, 34348, 4345, 6930, 11582, 50956], "temperature": 0.0, "avg_logprob": -0.06824333100091844, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.3626236915588379}, {"id": 74, "seek": 43764, "start": 449.47999999999996, "end": 455.15999999999997, "text": " in large language models you see eight of them given here along the x-axis for these plots you", "tokens": [50956, 294, 2416, 2856, 5245, 291, 536, 3180, 295, 552, 2212, 510, 2051, 264, 2031, 12, 24633, 337, 613, 28609, 291, 51240], "temperature": 0.0, "avg_logprob": -0.06824333100091844, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.3626236915588379}, {"id": 75, "seek": 43764, "start": 455.15999999999997, "end": 461.64, "text": " have model size and on the y-axis you have accuracy and what Jason is showing is that at a certain", "tokens": [51240, 362, 2316, 2744, 293, 322, 264, 288, 12, 24633, 291, 362, 14170, 293, 437, 11181, 307, 4099, 307, 300, 412, 257, 1629, 51564], "temperature": 0.0, "avg_logprob": -0.06824333100091844, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.3626236915588379}, {"id": 76, "seek": 43764, "start": 461.64, "end": 467.15999999999997, "text": " point these really big models just attain these abilities to do these really hard tasks", "tokens": [51564, 935, 613, 534, 955, 5245, 445, 23766, 613, 11582, 281, 360, 613, 534, 1152, 9608, 51840], "temperature": 0.0, "avg_logprob": -0.06824333100091844, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.3626236915588379}, {"id": 77, "seek": 46764, "start": 467.96, "end": 474.52, "text": " Jason estimates that for 137 tasks models are showing this kind of emergent ability and that", "tokens": [50380, 11181, 20561, 300, 337, 3705, 22, 9608, 5245, 366, 4099, 341, 733, 295, 4345, 6930, 3485, 293, 300, 50708], "temperature": 0.0, "avg_logprob": -0.048909902572631836, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0001535051123937592}, {"id": 78, "seek": 46764, "start": 474.52, "end": 480.76, "text": " includes tasks that were explicitly set up to help us stress test our largest language model", "tokens": [50708, 5974, 9608, 300, 645, 20803, 992, 493, 281, 854, 505, 4244, 1500, 527, 6443, 2856, 2316, 51020], "temperature": 0.0, "avg_logprob": -0.048909902572631836, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0001535051123937592}, {"id": 79, "seek": 46764, "start": 480.76, "end": 488.52, "text": " they're just falling away one by one really incredible now we're going to talk a little", "tokens": [51020, 436, 434, 445, 7440, 1314, 472, 538, 472, 534, 4651, 586, 321, 434, 516, 281, 751, 257, 707, 51408], "temperature": 0.0, "avg_logprob": -0.048909902572631836, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0001535051123937592}, {"id": 80, "seek": 46764, "start": 488.52, "end": 493.64, "text": " bit later about the factors that are driving this enormous progress for large language models but I", "tokens": [51408, 857, 1780, 466, 264, 6771, 300, 366, 4840, 341, 11322, 4205, 337, 2416, 2856, 5245, 457, 286, 51664], "temperature": 0.0, "avg_logprob": -0.048909902572631836, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.0001535051123937592}, {"id": 81, "seek": 49364, "start": 493.64, "end": 498.91999999999996, "text": " want to be upfront that one of the major factors here is just the raw size of these models you", "tokens": [50364, 528, 281, 312, 30264, 300, 472, 295, 264, 2563, 6771, 510, 307, 445, 264, 8936, 2744, 295, 613, 5245, 291, 50628], "temperature": 0.0, "avg_logprob": -0.03548047888992179, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.004197327885776758}, {"id": 82, "seek": 49364, "start": 498.91999999999996, "end": 503.96, "text": " can see that in Jason's plots that's where the emergent ability kicks in and let me put that", "tokens": [50628, 393, 536, 300, 294, 11181, 311, 28609, 300, 311, 689, 264, 4345, 6930, 3485, 21293, 294, 293, 718, 385, 829, 300, 50880], "temperature": 0.0, "avg_logprob": -0.03548047888992179, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.004197327885776758}, {"id": 83, "seek": 49364, "start": 503.96, "end": 509.08, "text": " in context for you so this is from a famous plot from a paper that's actually about making models", "tokens": [50880, 294, 4319, 337, 291, 370, 341, 307, 490, 257, 4618, 7542, 490, 257, 3035, 300, 311, 767, 466, 1455, 5245, 51136], "temperature": 0.0, "avg_logprob": -0.03548047888992179, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.004197327885776758}, {"id": 84, "seek": 49364, "start": 509.08, "end": 515.56, "text": " smaller and what they did is track the rise of you know increases in model size along the x-axis", "tokens": [51136, 4356, 293, 437, 436, 630, 307, 2837, 264, 6272, 295, 291, 458, 8637, 294, 2316, 2744, 2051, 264, 2031, 12, 24633, 51460], "temperature": 0.0, "avg_logprob": -0.03548047888992179, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.004197327885776758}, {"id": 85, "seek": 49364, "start": 515.56, "end": 522.04, "text": " we have time depth it only goes back to 2018 it's not very long ago and in 2018 the largest of our", "tokens": [51460, 321, 362, 565, 7161, 309, 787, 1709, 646, 281, 6096, 309, 311, 406, 588, 938, 2057, 293, 294, 6096, 264, 6443, 295, 527, 51784], "temperature": 0.0, "avg_logprob": -0.03548047888992179, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.004197327885776758}, {"id": 86, "seek": 52204, "start": 522.04, "end": 530.92, "text": " models had around 100 million parameters seems small by current comparisons in late 2019 early 2020", "tokens": [50364, 5245, 632, 926, 2319, 2459, 9834, 2544, 1359, 538, 2190, 33157, 294, 3469, 6071, 2440, 4808, 50808], "temperature": 0.0, "avg_logprob": -0.05572837971626444, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0049802628345787525}, {"id": 87, "seek": 52204, "start": 530.92, "end": 536.4399999999999, "text": " we start to see a rapid increase in the size of these models so that by the end of 2020 we have", "tokens": [50808, 321, 722, 281, 536, 257, 7558, 3488, 294, 264, 2744, 295, 613, 5245, 370, 300, 538, 264, 917, 295, 4808, 321, 362, 51084], "temperature": 0.0, "avg_logprob": -0.05572837971626444, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0049802628345787525}, {"id": 88, "seek": 52204, "start": 536.4399999999999, "end": 543.8, "text": " this megatron model at 8.3 billion parameters I remember when that came out it seemed like it", "tokens": [51084, 341, 10816, 267, 2044, 2316, 412, 1649, 13, 18, 5218, 9834, 286, 1604, 562, 300, 1361, 484, 309, 6576, 411, 309, 51452], "temperature": 0.0, "avg_logprob": -0.05572837971626444, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0049802628345787525}, {"id": 89, "seek": 52204, "start": 543.8, "end": 549.64, "text": " must be some kind of typo I could not fathom that we had a model that was that large but now of", "tokens": [51452, 1633, 312, 512, 733, 295, 2125, 78, 286, 727, 406, 283, 998, 298, 300, 321, 632, 257, 2316, 300, 390, 300, 2416, 457, 586, 295, 51744], "temperature": 0.0, "avg_logprob": -0.05572837971626444, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0049802628345787525}, {"id": 90, "seek": 54964, "start": 549.64, "end": 554.76, "text": " course this is kind of on the small side soon after that we got an 11 billion parameter variant of", "tokens": [50364, 1164, 341, 307, 733, 295, 322, 264, 1359, 1252, 2321, 934, 300, 321, 658, 364, 2975, 5218, 13075, 17501, 295, 50620], "temperature": 0.0, "avg_logprob": -0.0666317403986213, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.007813750766217709}, {"id": 91, "seek": 54964, "start": 554.76, "end": 562.84, "text": " that model and then gpd3 came out that says 175 billion parameters and that one too now looks", "tokens": [50620, 300, 2316, 293, 550, 290, 79, 67, 18, 1361, 484, 300, 1619, 41165, 5218, 9834, 293, 300, 472, 886, 586, 1542, 51024], "temperature": 0.0, "avg_logprob": -0.0666317403986213, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.007813750766217709}, {"id": 92, "seek": 54964, "start": 562.84, "end": 568.4399999999999, "text": " small in comparison to these truly gargantuan megatron models and the palm model from google", "tokens": [51024, 1359, 294, 9660, 281, 613, 4908, 3691, 70, 394, 6139, 10816, 267, 2044, 5245, 293, 264, 17018, 2316, 490, 20742, 51304], "temperature": 0.0, "avg_logprob": -0.0666317403986213, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.007813750766217709}, {"id": 93, "seek": 54964, "start": 568.4399999999999, "end": 576.04, "text": " which surpassed 500 billion parameters I want to emphasize that this has made a complete mockery", "tokens": [51304, 597, 27650, 292, 5923, 5218, 9834, 286, 528, 281, 16078, 300, 341, 575, 1027, 257, 3566, 17362, 2109, 51684], "temperature": 0.0, "avg_logprob": -0.0666317403986213, "compression_ratio": 1.7130044843049328, "no_speech_prob": 0.007813750766217709}, {"id": 94, "seek": 57604, "start": 576.04, "end": 582.5999999999999, "text": " of the y-axis of this plot to capture the scale correctly we would need 5 000 of these slides", "tokens": [50364, 295, 264, 288, 12, 24633, 295, 341, 7542, 281, 7983, 264, 4373, 8944, 321, 576, 643, 1025, 13711, 295, 613, 9788, 50692], "temperature": 0.0, "avg_logprob": -0.052102674920874906, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.05026881769299507}, {"id": 95, "seek": 57604, "start": 582.5999999999999, "end": 588.04, "text": " stacked on top of each other again it still feels weird to say that but that is the truth", "tokens": [50692, 28867, 322, 1192, 295, 1184, 661, 797, 309, 920, 3417, 3657, 281, 584, 300, 457, 300, 307, 264, 3494, 50964], "temperature": 0.0, "avg_logprob": -0.052102674920874906, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.05026881769299507}, {"id": 96, "seek": 57604, "start": 588.04, "end": 594.4399999999999, "text": " the scale of this is absolutely enormous and not something I think that I would have anticipated", "tokens": [50964, 264, 4373, 295, 341, 307, 3122, 11322, 293, 406, 746, 286, 519, 300, 286, 576, 362, 23267, 51284], "temperature": 0.0, "avg_logprob": -0.052102674920874906, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.05026881769299507}, {"id": 97, "seek": 57604, "start": 594.4399999999999, "end": 599.9599999999999, "text": " way back when we were dealing with those 100 million parameter babies by comparison they seem", "tokens": [51284, 636, 646, 562, 321, 645, 6260, 365, 729, 2319, 2459, 13075, 10917, 538, 9660, 436, 1643, 51560], "temperature": 0.0, "avg_logprob": -0.052102674920874906, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.05026881769299507}, {"id": 98, "seek": 59996, "start": 599.96, "end": 607.5600000000001, "text": " large to me at that point so this brings us to our central question it's a golden age this is all", "tokens": [50364, 2416, 281, 385, 412, 300, 935, 370, 341, 5607, 505, 281, 527, 5777, 1168, 309, 311, 257, 9729, 3205, 341, 307, 439, 50744], "temperature": 0.0, "avg_logprob": -0.035912150560423385, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.15177583694458008}, {"id": 99, "seek": 59996, "start": 607.5600000000001, "end": 612.76, "text": " undoubtedly exciting and the things that I've just described to you are going to have an impact on", "tokens": [50744, 35211, 4670, 293, 264, 721, 300, 286, 600, 445, 7619, 281, 291, 366, 516, 281, 362, 364, 2712, 322, 51004], "temperature": 0.0, "avg_logprob": -0.035912150560423385, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.15177583694458008}, {"id": 100, "seek": 59996, "start": 612.76, "end": 619.32, "text": " your lives positive and negative but certainly an impact but I take it that we are here today", "tokens": [51004, 428, 2909, 3353, 293, 3671, 457, 3297, 364, 2712, 457, 286, 747, 309, 300, 321, 366, 510, 965, 51332], "temperature": 0.0, "avg_logprob": -0.035912150560423385, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.15177583694458008}, {"id": 101, "seek": 59996, "start": 619.32, "end": 625.0, "text": " because we are researchers and we would like to participate in this research and that could leave", "tokens": [51332, 570, 321, 366, 10309, 293, 321, 576, 411, 281, 8197, 294, 341, 2132, 293, 300, 727, 1856, 51616], "temperature": 0.0, "avg_logprob": -0.035912150560423385, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.15177583694458008}, {"id": 102, "seek": 62500, "start": 625.0, "end": 631.72, "text": " you with a kind of worried feeling how can you contribute to nlu in this era of these gargantuan", "tokens": [50364, 291, 365, 257, 733, 295, 5804, 2633, 577, 393, 291, 10586, 281, 297, 2781, 294, 341, 4249, 295, 613, 3691, 70, 394, 6139, 50700], "temperature": 0.0, "avg_logprob": -0.04555011647088187, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.3907148838043213}, {"id": 103, "seek": 62500, "start": 631.72, "end": 638.28, "text": " models I've set this up as a kind of flow chart first question do you have 50 million dollars", "tokens": [50700, 5245, 286, 600, 992, 341, 493, 382, 257, 733, 295, 3095, 6927, 700, 1168, 360, 291, 362, 2625, 2459, 3808, 51028], "temperature": 0.0, "avg_logprob": -0.04555011647088187, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.3907148838043213}, {"id": 104, "seek": 62500, "start": 638.28, "end": 644.76, "text": " and a love of deep learning infrastructure if the answer is yes to this question then I would", "tokens": [51028, 293, 257, 959, 295, 2452, 2539, 6896, 498, 264, 1867, 307, 2086, 281, 341, 1168, 550, 286, 576, 51352], "temperature": 0.0, "avg_logprob": -0.04555011647088187, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.3907148838043213}, {"id": 105, "seek": 62500, "start": 644.76, "end": 649.24, "text": " encourage you to go off and build your own large language model you could change the world in this", "tokens": [51352, 5373, 291, 281, 352, 766, 293, 1322, 428, 1065, 2416, 2856, 2316, 291, 727, 1319, 264, 1002, 294, 341, 51576], "temperature": 0.0, "avg_logprob": -0.04555011647088187, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.3907148838043213}, {"id": 106, "seek": 62500, "start": 649.24, "end": 654.6, "text": " way I would also request that you get in touch with me maybe you could join my research group and", "tokens": [51576, 636, 286, 576, 611, 5308, 300, 291, 483, 294, 2557, 365, 385, 1310, 291, 727, 3917, 452, 2132, 1594, 293, 51844], "temperature": 0.0, "avg_logprob": -0.04555011647088187, "compression_ratio": 1.7749077490774907, "no_speech_prob": 0.3907148838043213}, {"id": 107, "seek": 65460, "start": 654.6800000000001, "end": 660.84, "text": " maybe fund my research group that would be wonderful but I'm assuming that most of you", "tokens": [50368, 1310, 2374, 452, 2132, 1594, 300, 576, 312, 3715, 457, 286, 478, 11926, 300, 881, 295, 291, 50676], "temperature": 0.0, "avg_logprob": -0.04101360166395033, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.0012054683174937963}, {"id": 108, "seek": 65460, "start": 660.84, "end": 666.9200000000001, "text": " cannot truthfully answer yes to this question I'm in the no camp right and on both counts I am both", "tokens": [50676, 2644, 3494, 2277, 1867, 2086, 281, 341, 1168, 286, 478, 294, 264, 572, 2255, 558, 293, 322, 1293, 14893, 286, 669, 1293, 50980], "temperature": 0.0, "avg_logprob": -0.04101360166395033, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.0012054683174937963}, {"id": 109, "seek": 65460, "start": 666.9200000000001, "end": 672.76, "text": " dramatically short of the funds and I also don't have a love of deep learning infrastructure so for", "tokens": [50980, 17548, 2099, 295, 264, 8271, 293, 286, 611, 500, 380, 362, 257, 959, 295, 2452, 2539, 6896, 370, 337, 51272], "temperature": 0.0, "avg_logprob": -0.04101360166395033, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.0012054683174937963}, {"id": 110, "seek": 65460, "start": 672.76, "end": 678.52, "text": " those of us who have to answer no to this question how can you contribute even if the answer is no", "tokens": [51272, 729, 295, 505, 567, 362, 281, 1867, 572, 281, 341, 1168, 577, 393, 291, 10586, 754, 498, 264, 1867, 307, 572, 51560], "temperature": 0.0, "avg_logprob": -0.04101360166395033, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.0012054683174937963}, {"id": 111, "seek": 65460, "start": 678.52, "end": 683.96, "text": " there are tons of things that you can be doing all right so just topics that are front of mind", "tokens": [51560, 456, 366, 9131, 295, 721, 300, 291, 393, 312, 884, 439, 558, 370, 445, 8378, 300, 366, 1868, 295, 1575, 51832], "temperature": 0.0, "avg_logprob": -0.04101360166395033, "compression_ratio": 1.7843866171003717, "no_speech_prob": 0.0012054683174937963}, {"id": 112, "seek": 68396, "start": 683.96, "end": 689.1600000000001, "text": " to me include retrieval augmented in-context learning this could be small models that are", "tokens": [50364, 281, 385, 4090, 19817, 3337, 36155, 294, 12, 9000, 3828, 2539, 341, 727, 312, 1359, 5245, 300, 366, 50624], "temperature": 0.0, "avg_logprob": -0.05406799912452698, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.008298222906887531}, {"id": 113, "seek": 68396, "start": 689.1600000000001, "end": 694.9200000000001, "text": " performant you could always contribute to creating better benchmarks this is a perennial challenge", "tokens": [50624, 2042, 394, 291, 727, 1009, 10586, 281, 4084, 1101, 43751, 341, 307, 257, 680, 29705, 3430, 50912], "temperature": 0.0, "avg_logprob": -0.05406799912452698, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.008298222906887531}, {"id": 114, "seek": 68396, "start": 694.9200000000001, "end": 700.36, "text": " for the field and maybe the most significant thing that you can do is just create devices that allow", "tokens": [50912, 337, 264, 2519, 293, 1310, 264, 881, 4776, 551, 300, 291, 393, 360, 307, 445, 1884, 5759, 300, 2089, 51184], "temperature": 0.0, "avg_logprob": -0.05406799912452698, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.008298222906887531}, {"id": 115, "seek": 68396, "start": 700.36, "end": 705.96, "text": " us to accurately measure the performance of our systems you could also help us solve what I've", "tokens": [51184, 505, 281, 20095, 3481, 264, 3389, 295, 527, 3652, 291, 727, 611, 854, 505, 5039, 437, 286, 600, 51464], "temperature": 0.0, "avg_logprob": -0.05406799912452698, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.008298222906887531}, {"id": 116, "seek": 68396, "start": 705.96, "end": 711.32, "text": " called the last mile problem for productive applications these central developments in AI", "tokens": [51464, 1219, 264, 1036, 12620, 1154, 337, 13304, 5821, 613, 5777, 20862, 294, 7318, 51732], "temperature": 0.0, "avg_logprob": -0.05406799912452698, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.008298222906887531}, {"id": 117, "seek": 71132, "start": 711.32, "end": 718.0400000000001, "text": " take us 95 percent of the way toward utility but that last five percent actually having a", "tokens": [50364, 747, 505, 13420, 3043, 295, 264, 636, 7361, 14877, 457, 300, 1036, 1732, 3043, 767, 1419, 257, 50700], "temperature": 0.0, "avg_logprob": -0.05246238708496094, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.008055011741816998}, {"id": 118, "seek": 71132, "start": 718.0400000000001, "end": 724.6800000000001, "text": " positive impact on people's lives often requires twice as much development twice as much innovation", "tokens": [50700, 3353, 2712, 322, 561, 311, 2909, 2049, 7029, 6091, 382, 709, 3250, 6091, 382, 709, 8504, 51032], "temperature": 0.0, "avg_logprob": -0.05246238708496094, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.008055011741816998}, {"id": 119, "seek": 71132, "start": 724.6800000000001, "end": 731.08, "text": " across domain experts people who are good at human computer interaction and AI experts right and", "tokens": [51032, 2108, 9274, 8572, 561, 567, 366, 665, 412, 1952, 3820, 9285, 293, 7318, 8572, 558, 293, 51352], "temperature": 0.0, "avg_logprob": -0.05246238708496094, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.008055011741816998}, {"id": 120, "seek": 71132, "start": 731.08, "end": 735.8000000000001, "text": " there's so there's just a huge amount that has to be done to realize the potential of these technologies", "tokens": [51352, 456, 311, 370, 456, 311, 445, 257, 2603, 2372, 300, 575, 281, 312, 1096, 281, 4325, 264, 3995, 295, 613, 7943, 51588], "temperature": 0.0, "avg_logprob": -0.05246238708496094, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.008055011741816998}, {"id": 121, "seek": 73580, "start": 736.52, "end": 742.5999999999999, "text": " and then finally you could think about achieving faithful human interpretable explanations of how", "tokens": [50400, 293, 550, 2721, 291, 727, 519, 466, 19626, 17808, 1952, 7302, 712, 28708, 295, 577, 50704], "temperature": 0.0, "avg_logprob": -0.03653134939805517, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.013218222185969353}, {"id": 122, "seek": 73580, "start": 742.5999999999999, "end": 748.12, "text": " these models behave if we're going to trust them we need to understand how they work at a human level", "tokens": [50704, 613, 5245, 15158, 498, 321, 434, 516, 281, 3361, 552, 321, 643, 281, 1223, 577, 436, 589, 412, 257, 1952, 1496, 50980], "temperature": 0.0, "avg_logprob": -0.03653134939805517, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.013218222185969353}, {"id": 123, "seek": 73580, "start": 748.12, "end": 752.5999999999999, "text": " that is supremely challenging and therefore this is incredibly important work you could be doing", "tokens": [50980, 300, 307, 23710, 736, 7595, 293, 4412, 341, 307, 6252, 1021, 589, 291, 727, 312, 884, 51204], "temperature": 0.0, "avg_logprob": -0.03653134939805517, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.013218222185969353}, {"id": 124, "seek": 73580, "start": 753.8, "end": 758.3599999999999, "text": " now I would love to talk with you about all four of those things and really elaborate on them but", "tokens": [51264, 586, 286, 576, 959, 281, 751, 365, 291, 466, 439, 1451, 295, 729, 721, 293, 534, 20945, 322, 552, 457, 51492], "temperature": 0.0, "avg_logprob": -0.03653134939805517, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.013218222185969353}, {"id": 125, "seek": 73580, "start": 758.3599999999999, "end": 764.68, "text": " our time is short and so what I've done is select one topic retrieval augmented in-context learning", "tokens": [51492, 527, 565, 307, 2099, 293, 370, 437, 286, 600, 1096, 307, 3048, 472, 4829, 19817, 3337, 36155, 294, 12, 9000, 3828, 2539, 51808], "temperature": 0.0, "avg_logprob": -0.03653134939805517, "compression_ratio": 1.7093425605536332, "no_speech_prob": 0.013218222185969353}, {"id": 126, "seek": 76468, "start": 764.68, "end": 770.12, "text": " to focus on because it's it's intimately connected to this notion of in-context learning", "tokens": [50364, 281, 1879, 322, 570, 309, 311, 309, 311, 560, 5401, 4582, 281, 341, 10710, 295, 294, 12, 9000, 3828, 2539, 50636], "temperature": 0.0, "avg_logprob": -0.03357104374014813, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.001263305195607245}, {"id": 127, "seek": 76468, "start": 770.12, "end": 775.9599999999999, "text": " and it's a place where all of us can participate in lots of innovative ways so that's kind of the", "tokens": [50636, 293, 309, 311, 257, 1081, 689, 439, 295, 505, 393, 8197, 294, 3195, 295, 12999, 2098, 370, 300, 311, 733, 295, 264, 50928], "temperature": 0.0, "avg_logprob": -0.03357104374014813, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.001263305195607245}, {"id": 128, "seek": 76468, "start": 775.9599999999999, "end": 782.92, "text": " central plan for the day before I do that though I just want to help us get more common ground around", "tokens": [50928, 5777, 1393, 337, 264, 786, 949, 286, 360, 300, 1673, 286, 445, 528, 281, 854, 505, 483, 544, 2689, 2727, 926, 51276], "temperature": 0.0, "avg_logprob": -0.03357104374014813, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.001263305195607245}, {"id": 129, "seek": 76468, "start": 782.92, "end": 788.76, "text": " what I take to be the really central change that's happening as a result of these large language", "tokens": [51276, 437, 286, 747, 281, 312, 264, 534, 5777, 1319, 300, 311, 2737, 382, 257, 1874, 295, 613, 2416, 2856, 51568], "temperature": 0.0, "avg_logprob": -0.03357104374014813, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.001263305195607245}, {"id": 130, "seek": 78876, "start": 788.76, "end": 795.8, "text": " models and I've put that under the heading of the rise of in-context learning again this is", "tokens": [50364, 5245, 293, 286, 600, 829, 300, 833, 264, 9864, 295, 264, 6272, 295, 294, 12, 9000, 3828, 2539, 797, 341, 307, 50716], "temperature": 0.0, "avg_logprob": -0.06230156762259347, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.05829259753227234}, {"id": 131, "seek": 78876, "start": 795.8, "end": 801.0, "text": " something we're all getting used to it really remarks a genuine paradigm shift I would say", "tokens": [50716, 746, 321, 434, 439, 1242, 1143, 281, 309, 534, 19151, 257, 16699, 24709, 5513, 286, 576, 584, 50976], "temperature": 0.0, "avg_logprob": -0.06230156762259347, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.05829259753227234}, {"id": 132, "seek": 78876, "start": 802.4399999999999, "end": 808.4399999999999, "text": " in-context learning really traces to the GPT-3 paper there are precedents earlier in the literature", "tokens": [51048, 294, 12, 9000, 3828, 2539, 534, 26076, 281, 264, 26039, 51, 12, 18, 3035, 456, 366, 16969, 791, 3071, 294, 264, 10394, 51348], "temperature": 0.0, "avg_logprob": -0.06230156762259347, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.05829259753227234}, {"id": 133, "seek": 78876, "start": 808.4399999999999, "end": 814.36, "text": " but it was the GPT-3 paper that really gave it a thorough initial investigation and showed that it", "tokens": [51348, 457, 309, 390, 264, 26039, 51, 12, 18, 3035, 300, 534, 2729, 309, 257, 12934, 5883, 9627, 293, 4712, 300, 309, 51644], "temperature": 0.0, "avg_logprob": -0.06230156762259347, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.05829259753227234}, {"id": 134, "seek": 81436, "start": 814.44, "end": 820.84, "text": " had promised with the earliest GPT models here's how this works we have our big language model", "tokens": [50368, 632, 10768, 365, 264, 20573, 26039, 51, 5245, 510, 311, 577, 341, 1985, 321, 362, 527, 955, 2856, 2316, 50688], "temperature": 0.0, "avg_logprob": -0.04038585027058919, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.03959948942065239}, {"id": 135, "seek": 81436, "start": 820.84, "end": 827.0, "text": " and we prompt it with a bunch of text so for example this is from that GPT-3 paper we might", "tokens": [50688, 293, 321, 12391, 309, 365, 257, 3840, 295, 2487, 370, 337, 1365, 341, 307, 490, 300, 26039, 51, 12, 18, 3035, 321, 1062, 50996], "temperature": 0.0, "avg_logprob": -0.04038585027058919, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.03959948942065239}, {"id": 136, "seek": 81436, "start": 827.0, "end": 833.24, "text": " prompt the model with a context passage and a title we might follow that with one or more", "tokens": [50996, 12391, 264, 2316, 365, 257, 4319, 11497, 293, 257, 4876, 321, 1062, 1524, 300, 365, 472, 420, 544, 51308], "temperature": 0.0, "avg_logprob": -0.04038585027058919, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.03959948942065239}, {"id": 137, "seek": 81436, "start": 833.24, "end": 838.36, "text": " demonstrations here the demonstration is a question and an answer and the goal of the", "tokens": [51308, 34714, 510, 264, 16520, 307, 257, 1168, 293, 364, 1867, 293, 264, 3387, 295, 264, 51564], "temperature": 0.0, "avg_logprob": -0.04038585027058919, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.03959948942065239}, {"id": 138, "seek": 81436, "start": 838.36, "end": 843.08, "text": " demonstration is to help the model learn in context that is from the prompt we've given it", "tokens": [51564, 16520, 307, 281, 854, 264, 2316, 1466, 294, 4319, 300, 307, 490, 264, 12391, 321, 600, 2212, 309, 51800], "temperature": 0.0, "avg_logprob": -0.04038585027058919, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.03959948942065239}, {"id": 139, "seek": 84308, "start": 843.08, "end": 848.2, "text": " what behavior we're trying to elicit from it so here you might say we're trying to coax the model", "tokens": [50364, 437, 5223, 321, 434, 1382, 281, 806, 8876, 490, 309, 370, 510, 291, 1062, 584, 321, 434, 1382, 281, 598, 2797, 264, 2316, 50620], "temperature": 0.0, "avg_logprob": -0.047048178586092865, "compression_ratio": 1.9338842975206612, "no_speech_prob": 0.0029775393195450306}, {"id": 140, "seek": 84308, "start": 848.2, "end": 854.44, "text": " to do extractive question answering to find the answer as a substring of the passage we gave it", "tokens": [50620, 281, 360, 8947, 488, 1168, 13430, 281, 915, 264, 1867, 382, 257, 4594, 2937, 295, 264, 11497, 321, 2729, 309, 50932], "temperature": 0.0, "avg_logprob": -0.047048178586092865, "compression_ratio": 1.9338842975206612, "no_speech_prob": 0.0029775393195450306}, {"id": 141, "seek": 84308, "start": 854.44, "end": 859.48, "text": " you might have a few of those and then finally we have the actual question we want the model to", "tokens": [50932, 291, 1062, 362, 257, 1326, 295, 729, 293, 550, 2721, 321, 362, 264, 3539, 1168, 321, 528, 264, 2316, 281, 51184], "temperature": 0.0, "avg_logprob": -0.047048178586092865, "compression_ratio": 1.9338842975206612, "no_speech_prob": 0.0029775393195450306}, {"id": 142, "seek": 84308, "start": 859.48, "end": 865.24, "text": " answer we prompt the model with this prompt here that puts it in some state and then its", "tokens": [51184, 1867, 321, 12391, 264, 2316, 365, 341, 12391, 510, 300, 8137, 309, 294, 512, 1785, 293, 550, 1080, 51472], "temperature": 0.0, "avg_logprob": -0.047048178586092865, "compression_ratio": 1.9338842975206612, "no_speech_prob": 0.0029775393195450306}, {"id": 143, "seek": 84308, "start": 865.24, "end": 870.12, "text": " generation is taken to be the prediction or response and that's how we assess its success", "tokens": [51472, 5125, 307, 2726, 281, 312, 264, 17630, 420, 4134, 293, 300, 311, 577, 321, 5877, 1080, 2245, 51716], "temperature": 0.0, "avg_logprob": -0.047048178586092865, "compression_ratio": 1.9338842975206612, "no_speech_prob": 0.0029775393195450306}, {"id": 144, "seek": 87012, "start": 870.92, "end": 875.08, "text": " and the whole idea is that the model can learn in context that is from this prompt", "tokens": [50404, 293, 264, 1379, 1558, 307, 300, 264, 2316, 393, 1466, 294, 4319, 300, 307, 490, 341, 12391, 50612], "temperature": 0.0, "avg_logprob": -0.05537907282511393, "compression_ratio": 1.812, "no_speech_prob": 0.0027131906244903803}, {"id": 145, "seek": 87012, "start": 875.08, "end": 879.88, "text": " what we want it to do so that gives you a sense for how this works you've probably all prompted", "tokens": [50612, 437, 321, 528, 309, 281, 360, 370, 300, 2709, 291, 257, 2020, 337, 577, 341, 1985, 291, 600, 1391, 439, 31042, 50852], "temperature": 0.0, "avg_logprob": -0.05537907282511393, "compression_ratio": 1.812, "no_speech_prob": 0.0027131906244903803}, {"id": 146, "seek": 87012, "start": 879.88, "end": 884.76, "text": " language models like you like this yourself already i want to dwell on this for a second though", "tokens": [50852, 2856, 5245, 411, 291, 411, 341, 1803, 1217, 741, 528, 281, 24355, 322, 341, 337, 257, 1150, 1673, 51096], "temperature": 0.0, "avg_logprob": -0.05537907282511393, "compression_ratio": 1.812, "no_speech_prob": 0.0027131906244903803}, {"id": 147, "seek": 87012, "start": 884.76, "end": 890.2, "text": " this is a really different thing from what we used to do throughout artificial intelligence", "tokens": [51096, 341, 307, 257, 534, 819, 551, 490, 437, 321, 1143, 281, 360, 3710, 11677, 7599, 51368], "temperature": 0.0, "avg_logprob": -0.05537907282511393, "compression_ratio": 1.812, "no_speech_prob": 0.0027131906244903803}, {"id": 148, "seek": 87012, "start": 890.2, "end": 895.8, "text": " let me contrast in context learning with the standard paradigm of standard supervision", "tokens": [51368, 718, 385, 8712, 294, 4319, 2539, 365, 264, 3832, 24709, 295, 3832, 32675, 51648], "temperature": 0.0, "avg_logprob": -0.05537907282511393, "compression_ratio": 1.812, "no_speech_prob": 0.0027131906244903803}, {"id": 149, "seek": 89580, "start": 896.76, "end": 903.0, "text": " back in the old days of 2017 or whatever we would typically set things up like this we would have", "tokens": [50412, 646, 294, 264, 1331, 1708, 295, 6591, 420, 2035, 321, 576, 5850, 992, 721, 493, 411, 341, 321, 576, 362, 50724], "temperature": 0.0, "avg_logprob": -0.04763004514906141, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0037051760591566563}, {"id": 150, "seek": 89580, "start": 903.0, "end": 907.7199999999999, "text": " say we wanted to solve a problem like classifying texts according to whether they express nervous", "tokens": [50724, 584, 321, 1415, 281, 5039, 257, 1154, 411, 1508, 5489, 15765, 4650, 281, 1968, 436, 5109, 6296, 50960], "temperature": 0.0, "avg_logprob": -0.04763004514906141, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0037051760591566563}, {"id": 151, "seek": 89580, "start": 907.7199999999999, "end": 912.68, "text": " anticipation a complex human emotion the first step would be that we would need to create a data", "tokens": [50960, 35979, 257, 3997, 1952, 8913, 264, 700, 1823, 576, 312, 300, 321, 576, 643, 281, 1884, 257, 1412, 51208], "temperature": 0.0, "avg_logprob": -0.04763004514906141, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0037051760591566563}, {"id": 152, "seek": 89580, "start": 912.68, "end": 918.52, "text": " set of positive and negative examples of that phenomenon and then we would train a custom", "tokens": [51208, 992, 295, 3353, 293, 3671, 5110, 295, 300, 14029, 293, 550, 321, 576, 3847, 257, 2375, 51500], "temperature": 0.0, "avg_logprob": -0.04763004514906141, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0037051760591566563}, {"id": 153, "seek": 89580, "start": 918.52, "end": 924.76, "text": " built model to make the binary distinction reflected in the labels here it can be surprisingly", "tokens": [51500, 3094, 2316, 281, 652, 264, 17434, 16844, 15502, 294, 264, 16949, 510, 309, 393, 312, 17600, 51812], "temperature": 0.0, "avg_logprob": -0.04763004514906141, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0037051760591566563}, {"id": 154, "seek": 92476, "start": 924.84, "end": 929.72, "text": " powerful but you can start to see already how this isn't going to scale to the complexity of", "tokens": [50368, 4005, 457, 291, 393, 722, 281, 536, 1217, 577, 341, 1943, 380, 516, 281, 4373, 281, 264, 14024, 295, 50612], "temperature": 0.0, "avg_logprob": -0.04541098396733122, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003170573618263006}, {"id": 155, "seek": 92476, "start": 929.72, "end": 935.48, "text": " the human experience we're going to need separate data sets and maybe separate models for optimism", "tokens": [50612, 264, 1952, 1752, 321, 434, 516, 281, 643, 4994, 1412, 6352, 293, 1310, 4994, 5245, 337, 31074, 50900], "temperature": 0.0, "avg_logprob": -0.04541098396733122, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003170573618263006}, {"id": 156, "seek": 92476, "start": 935.48, "end": 941.08, "text": " and sadness and every other emotion you can think of and that's just a subset of all the", "tokens": [50900, 293, 22462, 293, 633, 661, 8913, 291, 393, 519, 295, 293, 300, 311, 445, 257, 25993, 295, 439, 264, 51180], "temperature": 0.0, "avg_logprob": -0.04541098396733122, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003170573618263006}, {"id": 157, "seek": 92476, "start": 941.08, "end": 945.88, "text": " problems we might want our models to solve for each one we're going to need data and maybe a", "tokens": [51180, 2740, 321, 1062, 528, 527, 5245, 281, 5039, 337, 1184, 472, 321, 434, 516, 281, 643, 1412, 293, 1310, 257, 51420], "temperature": 0.0, "avg_logprob": -0.04541098396733122, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003170573618263006}, {"id": 158, "seek": 92476, "start": 945.88, "end": 954.2, "text": " custom built model the promise of in-context learning is that a single big frozen language model", "tokens": [51420, 2375, 3094, 2316, 264, 6228, 295, 294, 12, 9000, 3828, 2539, 307, 300, 257, 2167, 955, 12496, 2856, 2316, 51836], "temperature": 0.0, "avg_logprob": -0.04541098396733122, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.003170573618263006}, {"id": 159, "seek": 95420, "start": 954.2, "end": 959.08, "text": " can serve all those goals and in this mode we do that prompting thing that I just described", "tokens": [50364, 393, 4596, 439, 729, 5493, 293, 294, 341, 4391, 321, 360, 300, 12391, 278, 551, 300, 286, 445, 7619, 50608], "temperature": 0.0, "avg_logprob": -0.04544347646285077, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.002471264684572816}, {"id": 160, "seek": 95420, "start": 959.08, "end": 964.6, "text": " we're going to give the model examples just expressed in flat text of positive and negative", "tokens": [50608, 321, 434, 516, 281, 976, 264, 2316, 5110, 445, 12675, 294, 4962, 2487, 295, 3353, 293, 3671, 50884], "temperature": 0.0, "avg_logprob": -0.04544347646285077, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.002471264684572816}, {"id": 161, "seek": 95420, "start": 964.6, "end": 969.24, "text": " instances and hope that that's enough for it to learn in context about the distinction we're", "tokens": [50884, 14519, 293, 1454, 300, 300, 311, 1547, 337, 309, 281, 1466, 294, 4319, 466, 264, 16844, 321, 434, 51116], "temperature": 0.0, "avg_logprob": -0.04544347646285077, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.002471264684572816}, {"id": 162, "seek": 95420, "start": 969.24, "end": 974.76, "text": " trying to establish this is really really different consider that over here the phrase nervous", "tokens": [51116, 1382, 281, 8327, 341, 307, 534, 534, 819, 1949, 300, 670, 510, 264, 9535, 6296, 51392], "temperature": 0.0, "avg_logprob": -0.04544347646285077, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.002471264684572816}, {"id": 163, "seek": 95420, "start": 974.76, "end": 980.2, "text": " anticipation has no special status the model doesn't really process it it's entirely structured to", "tokens": [51392, 35979, 575, 572, 2121, 6558, 264, 2316, 1177, 380, 534, 1399, 309, 309, 311, 7696, 18519, 281, 51664], "temperature": 0.0, "avg_logprob": -0.04544347646285077, "compression_ratio": 1.7938931297709924, "no_speech_prob": 0.002471264684572816}, {"id": 164, "seek": 98020, "start": 980.2, "end": 986.6, "text": " make a binary distinction and the label nervous anticipation is kind of for us on the right the", "tokens": [50364, 652, 257, 17434, 16844, 293, 264, 7645, 6296, 35979, 307, 733, 295, 337, 505, 322, 264, 558, 264, 50684], "temperature": 0.0, "avg_logprob": -0.02949437934361147, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.00180928991176188}, {"id": 165, "seek": 98020, "start": 986.6, "end": 992.6800000000001, "text": " model needs to learn essentially the meanings of all of these terms and our intentions and figure", "tokens": [50684, 2316, 2203, 281, 1466, 4476, 264, 28138, 295, 439, 295, 613, 2115, 293, 527, 19354, 293, 2573, 50988], "temperature": 0.0, "avg_logprob": -0.02949437934361147, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.00180928991176188}, {"id": 166, "seek": 98020, "start": 992.6800000000001, "end": 998.9200000000001, "text": " out how to make these distinctions on new examples all from a prompt it's just weird and wild that", "tokens": [50988, 484, 577, 281, 652, 613, 1483, 49798, 322, 777, 5110, 439, 490, 257, 12391, 309, 311, 445, 3657, 293, 4868, 300, 51300], "temperature": 0.0, "avg_logprob": -0.02949437934361147, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.00180928991176188}, {"id": 167, "seek": 98020, "start": 998.9200000000001, "end": 1004.0400000000001, "text": " this works at all I think I used to be discouraging about this as an avenue and now we're seeing it", "tokens": [51300, 341, 1985, 412, 439, 286, 519, 286, 1143, 281, 312, 21497, 3568, 466, 341, 382, 364, 39230, 293, 586, 321, 434, 2577, 309, 51556], "temperature": 0.0, "avg_logprob": -0.02949437934361147, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.00180928991176188}, {"id": 168, "seek": 100404, "start": 1004.04, "end": 1012.12, "text": " bear so much fruit what are the mechanisms behind this I'm going to identify a few of them for you", "tokens": [50364, 6155, 370, 709, 6773, 437, 366, 264, 15902, 2261, 341, 286, 478, 516, 281, 5876, 257, 1326, 295, 552, 337, 291, 50768], "temperature": 0.0, "avg_logprob": -0.05335259437561035, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.024021930992603302}, {"id": 169, "seek": 100404, "start": 1012.12, "end": 1017.7199999999999, "text": " the first one is certainly the transformer architecture this is the basic building block", "tokens": [50768, 264, 700, 472, 307, 3297, 264, 31782, 9482, 341, 307, 264, 3875, 2390, 3461, 51048], "temperature": 0.0, "avg_logprob": -0.05335259437561035, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.024021930992603302}, {"id": 170, "seek": 100404, "start": 1017.7199999999999, "end": 1023.0, "text": " of essentially all the language models that I've mentioned so far we have great coverage of the", "tokens": [51048, 295, 4476, 439, 264, 2856, 5245, 300, 286, 600, 2835, 370, 1400, 321, 362, 869, 9645, 295, 264, 51312], "temperature": 0.0, "avg_logprob": -0.05335259437561035, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.024021930992603302}, {"id": 171, "seek": 100404, "start": 1023.0, "end": 1027.24, "text": " transformer in our course natural language understanding so I'm going to do this quickly", "tokens": [51312, 31782, 294, 527, 1164, 3303, 2856, 3701, 370, 286, 478, 516, 281, 360, 341, 2661, 51524], "temperature": 0.0, "avg_logprob": -0.05335259437561035, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.024021930992603302}, {"id": 172, "seek": 100404, "start": 1027.24, "end": 1032.92, "text": " the transformer starts with word embeddings and positional encodings on top of those we have a", "tokens": [51524, 264, 31782, 3719, 365, 1349, 12240, 29432, 293, 2535, 304, 2058, 378, 1109, 322, 1192, 295, 729, 321, 362, 257, 51808], "temperature": 0.0, "avg_logprob": -0.05335259437561035, "compression_ratio": 1.7961538461538462, "no_speech_prob": 0.024021930992603302}, {"id": 173, "seek": 103292, "start": 1032.92, "end": 1038.6000000000001, "text": " bunch of attention mechanisms these give the name to the famous paper attention is all you need", "tokens": [50364, 3840, 295, 3202, 15902, 613, 976, 264, 1315, 281, 264, 4618, 3035, 3202, 307, 439, 291, 643, 50648], "temperature": 0.0, "avg_logprob": -0.05253523349761963, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.0021137031726539135}, {"id": 174, "seek": 103292, "start": 1038.6000000000001, "end": 1043.88, "text": " which announced the transformer evidently attention is not all you need because we have these", "tokens": [50648, 597, 7548, 264, 31782, 16371, 356, 3202, 307, 406, 439, 291, 643, 570, 321, 362, 613, 50912], "temperature": 0.0, "avg_logprob": -0.05253523349761963, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.0021137031726539135}, {"id": 175, "seek": 103292, "start": 1043.88, "end": 1047.88, "text": " positional encodings at the bottom and then we have a bunch of feed forward layers and", "tokens": [50912, 2535, 304, 2058, 378, 1109, 412, 264, 2767, 293, 550, 321, 362, 257, 3840, 295, 3154, 2128, 7914, 293, 51112], "temperature": 0.0, "avg_logprob": -0.05253523349761963, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.0021137031726539135}, {"id": 176, "seek": 103292, "start": 1047.88, "end": 1055.0, "text": " regularization steps at the top but attention really is the beating heart of this model and it", "tokens": [51112, 3890, 2144, 4439, 412, 264, 1192, 457, 3202, 534, 307, 264, 13497, 1917, 295, 341, 2316, 293, 309, 51468], "temperature": 0.0, "avg_logprob": -0.05253523349761963, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.0021137031726539135}, {"id": 177, "seek": 103292, "start": 1055.0, "end": 1061.24, "text": " really was a dramatic departure from the fancy mechanisms LSTMs and so forth that were characteristic", "tokens": [51468, 534, 390, 257, 12023, 25866, 490, 264, 10247, 15902, 441, 6840, 26386, 293, 370, 5220, 300, 645, 16282, 51780], "temperature": 0.0, "avg_logprob": -0.05253523349761963, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.0021137031726539135}, {"id": 178, "seek": 106124, "start": 1061.24, "end": 1066.84, "text": " of the pre-transformer era so that's essentially though on the diagram here the full model in the", "tokens": [50364, 295, 264, 659, 12, 24999, 837, 260, 4249, 370, 300, 311, 4476, 1673, 322, 264, 10686, 510, 264, 1577, 2316, 294, 264, 50644], "temperature": 0.0, "avg_logprob": -0.06390726906912668, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.005723370239138603}, {"id": 179, "seek": 106124, "start": 1066.84, "end": 1072.52, "text": " course we have a bunch of materials that help you get hands on with transformer representations", "tokens": [50644, 1164, 321, 362, 257, 3840, 295, 5319, 300, 854, 291, 483, 2377, 322, 365, 31782, 33358, 50928], "temperature": 0.0, "avg_logprob": -0.06390726906912668, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.005723370239138603}, {"id": 180, "seek": 106124, "start": 1072.52, "end": 1078.28, "text": " and also dive deep into math into the mathematics so I'm just going to skip past this I will say", "tokens": [50928, 293, 611, 9192, 2452, 666, 5221, 666, 264, 18666, 370, 286, 478, 445, 516, 281, 10023, 1791, 341, 286, 486, 584, 51216], "temperature": 0.0, "avg_logprob": -0.06390726906912668, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.005723370239138603}, {"id": 181, "seek": 106124, "start": 1078.28, "end": 1082.36, "text": " that if you dive deep you're likely to go through the same journey we all go through", "tokens": [51216, 300, 498, 291, 9192, 2452, 291, 434, 3700, 281, 352, 807, 264, 912, 4671, 321, 439, 352, 807, 51420], "temperature": 0.0, "avg_logprob": -0.06390726906912668, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.005723370239138603}, {"id": 182, "seek": 106124, "start": 1083.0, "end": 1088.44, "text": " where your first question is how on earth does this work this diagram looks very complicated", "tokens": [51452, 689, 428, 700, 1168, 307, 577, 322, 4120, 775, 341, 589, 341, 10686, 1542, 588, 6179, 51724], "temperature": 0.0, "avg_logprob": -0.06390726906912668, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.005723370239138603}, {"id": 183, "seek": 108844, "start": 1088.44, "end": 1094.1200000000001, "text": " but then you come to terms with it and you realize oh this is actually a bunch of very", "tokens": [50364, 457, 550, 291, 808, 281, 2115, 365, 309, 293, 291, 4325, 1954, 341, 307, 767, 257, 3840, 295, 588, 50648], "temperature": 0.0, "avg_logprob": -0.06857111842133277, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.002285607159137726}, {"id": 184, "seek": 108844, "start": 1094.1200000000001, "end": 1100.1200000000001, "text": " simple mechanisms but then you arrive at a question that is a burning question for all of us why does", "tokens": [50648, 2199, 15902, 457, 550, 291, 8881, 412, 257, 1168, 300, 307, 257, 9488, 1168, 337, 439, 295, 505, 983, 775, 50948], "temperature": 0.0, "avg_logprob": -0.06857111842133277, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.002285607159137726}, {"id": 185, "seek": 108844, "start": 1100.1200000000001, "end": 1105.72, "text": " this work so well this remains an open question a lot of people are working on explaining why this", "tokens": [50948, 341, 589, 370, 731, 341, 7023, 364, 1269, 1168, 257, 688, 295, 561, 366, 1364, 322, 13468, 983, 341, 51228], "temperature": 0.0, "avg_logprob": -0.06857111842133277, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.002285607159137726}, {"id": 186, "seek": 108844, "start": 1105.72, "end": 1111.4, "text": " is so effective and that is certainly an area in which all of us could participate analytic work", "tokens": [51228, 307, 370, 4942, 293, 300, 307, 3297, 364, 1859, 294, 597, 439, 295, 505, 727, 8197, 40358, 589, 51512], "temperature": 0.0, "avg_logprob": -0.06857111842133277, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.002285607159137726}, {"id": 187, "seek": 111140, "start": 1111.4, "end": 1119.64, "text": " understanding why this is so successful the second big innovation here is a realization", "tokens": [50364, 3701, 983, 341, 307, 370, 4406, 264, 1150, 955, 8504, 510, 307, 257, 25138, 50776], "temperature": 0.0, "avg_logprob": -0.05647680006529156, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.034071434289216995}, {"id": 188, "seek": 111140, "start": 1119.64, "end": 1124.92, "text": " that what I've called self supervision is an incredibly powerful mechanism for acquiring", "tokens": [50776, 300, 437, 286, 600, 1219, 2698, 32675, 307, 364, 6252, 4005, 7513, 337, 37374, 51040], "temperature": 0.0, "avg_logprob": -0.05647680006529156, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.034071434289216995}, {"id": 189, "seek": 111140, "start": 1124.92, "end": 1131.8000000000002, "text": " rich representations of form and meaning this is also very strange in self supervision the model's", "tokens": [51040, 4593, 33358, 295, 1254, 293, 3620, 341, 307, 611, 588, 5861, 294, 2698, 32675, 264, 2316, 311, 51384], "temperature": 0.0, "avg_logprob": -0.05647680006529156, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.034071434289216995}, {"id": 190, "seek": 111140, "start": 1131.8000000000002, "end": 1137.4, "text": " only objective is to learn from co-occurrence patterns in the sequences it's trained on this is", "tokens": [51384, 787, 10024, 307, 281, 1466, 490, 598, 12, 905, 14112, 10760, 8294, 294, 264, 22978, 309, 311, 8895, 322, 341, 307, 51664], "temperature": 0.0, "avg_logprob": -0.05647680006529156, "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.034071434289216995}, {"id": 191, "seek": 113740, "start": 1137.4, "end": 1143.4, "text": " purely distributional learning another way to put this is the model is just learning to assign", "tokens": [50364, 17491, 7316, 304, 2539, 1071, 636, 281, 829, 341, 307, 264, 2316, 307, 445, 2539, 281, 6269, 50664], "temperature": 0.0, "avg_logprob": -0.04711845942905971, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.00831135269254446}, {"id": 192, "seek": 113740, "start": 1143.4, "end": 1150.0400000000002, "text": " high probability to attested sequences that is the fundamental mechanism we think about these", "tokens": [50664, 1090, 8482, 281, 951, 21885, 22978, 300, 307, 264, 8088, 7513, 321, 519, 466, 613, 50996], "temperature": 0.0, "avg_logprob": -0.04711845942905971, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.00831135269254446}, {"id": 193, "seek": 113740, "start": 1150.0400000000002, "end": 1155.4, "text": " models as generators but generation is just sampling from the model that's a kind of secondary", "tokens": [50996, 5245, 382, 38662, 457, 5125, 307, 445, 21179, 490, 264, 2316, 300, 311, 257, 733, 295, 11396, 51264], "temperature": 0.0, "avg_logprob": -0.04711845942905971, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.00831135269254446}, {"id": 194, "seek": 113740, "start": 1155.4, "end": 1161.24, "text": " or derivative process the main thing is learning from these co-occurrence patterns an enlightening", "tokens": [51264, 420, 13760, 1399, 264, 2135, 551, 307, 2539, 490, 613, 598, 12, 905, 14112, 10760, 8294, 364, 18690, 4559, 51556], "temperature": 0.0, "avg_logprob": -0.04711845942905971, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.00831135269254446}, {"id": 195, "seek": 113740, "start": 1161.24, "end": 1165.64, "text": " thing about the current era is that it's fruitful for these sequences content to contain lots of", "tokens": [51556, 551, 466, 264, 2190, 4249, 307, 300, 309, 311, 49795, 337, 613, 22978, 2701, 281, 5304, 3195, 295, 51776], "temperature": 0.0, "avg_logprob": -0.04711845942905971, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.00831135269254446}, {"id": 196, "seek": 116564, "start": 1165.64, "end": 1171.5600000000002, "text": " symbols not just language but computer code sensor readings even images and so forth those", "tokens": [50364, 16944, 406, 445, 2856, 457, 3820, 3089, 10200, 27319, 754, 5267, 293, 370, 5220, 729, 50660], "temperature": 0.0, "avg_logprob": -0.06214512548139019, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003170251613482833}, {"id": 197, "seek": 116564, "start": 1171.5600000000002, "end": 1177.64, "text": " are all just symbol streams and the model learns associations among them the core thing about", "tokens": [50660, 366, 439, 445, 5986, 15842, 293, 264, 2316, 27152, 26597, 3654, 552, 264, 4965, 551, 466, 50964], "temperature": 0.0, "avg_logprob": -0.06214512548139019, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003170251613482833}, {"id": 198, "seek": 116564, "start": 1177.64, "end": 1182.3600000000001, "text": " self supervision though that really contrasts it with the standard supervised paradigm I mentioned", "tokens": [50964, 2698, 32675, 1673, 300, 534, 8712, 82, 309, 365, 264, 3832, 46533, 24709, 286, 2835, 51200], "temperature": 0.0, "avg_logprob": -0.06214512548139019, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003170251613482833}, {"id": 199, "seek": 116564, "start": 1182.3600000000001, "end": 1188.5200000000002, "text": " before is that the objective doesn't mention any specific specific symbols or relations between", "tokens": [51200, 949, 307, 300, 264, 10024, 1177, 380, 2152, 604, 2685, 2685, 16944, 420, 2299, 1296, 51508], "temperature": 0.0, "avg_logprob": -0.06214512548139019, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003170251613482833}, {"id": 200, "seek": 116564, "start": 1188.5200000000002, "end": 1195.4, "text": " them is entirely about learning these co-occurrence patterns and from this simple mechanism we get such", "tokens": [51508, 552, 307, 7696, 466, 2539, 613, 598, 12, 905, 14112, 10760, 8294, 293, 490, 341, 2199, 7513, 321, 483, 1270, 51852], "temperature": 0.0, "avg_logprob": -0.06214512548139019, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.003170251613482833}, {"id": 201, "seek": 119540, "start": 1195.48, "end": 1202.2800000000002, "text": " rich results and that is incredibly empowering because you need hardly any human effort to train", "tokens": [50368, 4593, 3542, 293, 300, 307, 6252, 28261, 570, 291, 643, 13572, 604, 1952, 4630, 281, 3847, 50708], "temperature": 0.0, "avg_logprob": -0.07504719911619674, "compression_ratio": 1.7434782608695651, "no_speech_prob": 0.0022503973450511694}, {"id": 202, "seek": 119540, "start": 1202.2800000000002, "end": 1208.2, "text": " a model with self supervision you just need vast quantities of these symbol streams and so that has", "tokens": [50708, 257, 2316, 365, 2698, 32675, 291, 445, 643, 8369, 22927, 295, 613, 5986, 15842, 293, 370, 300, 575, 51004], "temperature": 0.0, "avg_logprob": -0.07504719911619674, "compression_ratio": 1.7434782608695651, "no_speech_prob": 0.0022503973450511694}, {"id": 203, "seek": 119540, "start": 1208.2, "end": 1215.0800000000002, "text": " facilitated the rise of another important mechanism here large-scale pre-training and there are actually", "tokens": [51004, 10217, 18266, 264, 6272, 295, 1071, 1021, 7513, 510, 2416, 12, 20033, 659, 12, 17227, 1760, 293, 456, 366, 767, 51348], "temperature": 0.0, "avg_logprob": -0.07504719911619674, "compression_ratio": 1.7434782608695651, "no_speech_prob": 0.0022503973450511694}, {"id": 204, "seek": 119540, "start": 1215.0800000000002, "end": 1220.44, "text": " two innovations that are happening here right so we see the rise of large-scale pre-training in the", "tokens": [51348, 732, 24283, 300, 366, 2737, 510, 558, 370, 321, 536, 264, 6272, 295, 2416, 12, 20033, 659, 12, 17227, 1760, 294, 264, 51616], "temperature": 0.0, "avg_logprob": -0.07504719911619674, "compression_ratio": 1.7434782608695651, "no_speech_prob": 0.0022503973450511694}, {"id": 205, "seek": 122044, "start": 1220.44, "end": 1227.8, "text": " earliest work on static word representations like word to vex and glove and what those teams realize", "tokens": [50364, 20573, 589, 322, 13437, 1349, 33358, 411, 1349, 281, 1241, 87, 293, 26928, 293, 437, 729, 5491, 4325, 50732], "temperature": 0.0, "avg_logprob": -0.05373286604881287, "compression_ratio": 1.7377777777777779, "no_speech_prob": 0.020011339336633682}, {"id": 206, "seek": 122044, "start": 1227.8, "end": 1233.4, "text": " is not only that it's powerful to train on vast quantities of data using just self supervision", "tokens": [50732, 307, 406, 787, 300, 309, 311, 4005, 281, 3847, 322, 8369, 22927, 295, 1412, 1228, 445, 2698, 32675, 51012], "temperature": 0.0, "avg_logprob": -0.05373286604881287, "compression_ratio": 1.7377777777777779, "no_speech_prob": 0.020011339336633682}, {"id": 207, "seek": 122044, "start": 1233.4, "end": 1240.04, "text": " but also that it's empowering to the community to release those parameters not just data not just", "tokens": [51012, 457, 611, 300, 309, 311, 28261, 281, 264, 1768, 281, 4374, 729, 9834, 406, 445, 1412, 406, 445, 51344], "temperature": 0.0, "avg_logprob": -0.05373286604881287, "compression_ratio": 1.7377777777777779, "no_speech_prob": 0.020011339336633682}, {"id": 208, "seek": 122044, "start": 1240.04, "end": 1245.48, "text": " code but the actual learned representations for other people to build on that has been incredible", "tokens": [51344, 3089, 457, 264, 3539, 3264, 33358, 337, 661, 561, 281, 1322, 322, 300, 575, 668, 4651, 51616], "temperature": 0.0, "avg_logprob": -0.05373286604881287, "compression_ratio": 1.7377777777777779, "no_speech_prob": 0.020011339336633682}, {"id": 209, "seek": 124548, "start": 1245.48, "end": 1251.64, "text": " in terms of building effective systems after those we get ELMO which was the first model to do this", "tokens": [50364, 294, 2115, 295, 2390, 4942, 3652, 934, 729, 321, 483, 14426, 18976, 597, 390, 264, 700, 2316, 281, 360, 341, 50672], "temperature": 0.0, "avg_logprob": -0.06871060265435112, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.02714664489030838}, {"id": 210, "seek": 124548, "start": 1251.64, "end": 1258.1200000000001, "text": " for contextual word representations truly large language models then we get BERT of course and", "tokens": [50672, 337, 35526, 1349, 33358, 4908, 2416, 2856, 5245, 550, 321, 483, 363, 31479, 295, 1164, 293, 50996], "temperature": 0.0, "avg_logprob": -0.06871060265435112, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.02714664489030838}, {"id": 211, "seek": 124548, "start": 1258.1200000000001, "end": 1265.24, "text": " GPT and then finally of course GPT-3 at a scale that was really previously unimagined and maybe", "tokens": [50996, 26039, 51, 293, 550, 2721, 295, 1164, 26039, 51, 12, 18, 412, 257, 4373, 300, 390, 534, 8046, 517, 25228, 2001, 293, 1310, 51352], "temperature": 0.0, "avg_logprob": -0.06871060265435112, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.02714664489030838}, {"id": 212, "seek": 124548, "start": 1265.24, "end": 1273.8, "text": " kind of unimaginable for me a final piece that we should not overlook is the role of human feedback", "tokens": [51352, 733, 295, 517, 44976, 712, 337, 385, 257, 2572, 2522, 300, 321, 820, 406, 37826, 307, 264, 3090, 295, 1952, 5824, 51780], "temperature": 0.0, "avg_logprob": -0.06871060265435112, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.02714664489030838}, {"id": 213, "seek": 127380, "start": 1273.8, "end": 1280.36, "text": " in all of this and I'm thinking in particular of the open AI models I've given a lot of coverage", "tokens": [50364, 294, 439, 295, 341, 293, 286, 478, 1953, 294, 1729, 295, 264, 1269, 7318, 5245, 286, 600, 2212, 257, 688, 295, 9645, 50692], "temperature": 0.0, "avg_logprob": -0.06276564706455577, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.12727554142475128}, {"id": 214, "seek": 127380, "start": 1280.36, "end": 1286.12, "text": " so far of this mechanism of self supervision but we have to acknowledge that our best models", "tokens": [50692, 370, 1400, 295, 341, 7513, 295, 2698, 32675, 457, 321, 362, 281, 10692, 300, 527, 1151, 5245, 50980], "temperature": 0.0, "avg_logprob": -0.06276564706455577, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.12727554142475128}, {"id": 215, "seek": 127380, "start": 1286.12, "end": 1291.48, "text": " are what open AI calls the instruct models and those are trained with way more than just self", "tokens": [50980, 366, 437, 1269, 7318, 5498, 264, 7232, 5245, 293, 729, 366, 8895, 365, 636, 544, 813, 445, 2698, 51248], "temperature": 0.0, "avg_logprob": -0.06276564706455577, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.12727554142475128}, {"id": 216, "seek": 127380, "start": 1291.48, "end": 1298.28, "text": " supervision this is a diagram from the chat GPT blog post it has a lot of details I'm confident", "tokens": [51248, 32675, 341, 307, 257, 10686, 490, 264, 5081, 26039, 51, 6968, 2183, 309, 575, 257, 688, 295, 4365, 286, 478, 6679, 51588], "temperature": 0.0, "avg_logprob": -0.06276564706455577, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.12727554142475128}, {"id": 217, "seek": 129828, "start": 1298.28, "end": 1303.8799999999999, "text": " that there are really two pieces that are important first the language model is fine tuned", "tokens": [50364, 300, 456, 366, 534, 732, 3755, 300, 366, 1021, 700, 264, 2856, 2316, 307, 2489, 10870, 50644], "temperature": 0.0, "avg_logprob": -0.04494476636250814, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.34794148802757263}, {"id": 218, "seek": 129828, "start": 1304.44, "end": 1310.52, "text": " on human level supervision just making binary distinctions about good generations and bad ones", "tokens": [50672, 322, 1952, 1496, 32675, 445, 1455, 17434, 1483, 49798, 466, 665, 10593, 293, 1578, 2306, 50976], "temperature": 0.0, "avg_logprob": -0.04494476636250814, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.34794148802757263}, {"id": 219, "seek": 129828, "start": 1310.52, "end": 1316.6, "text": " that's already beyond self supervision and then in a second phase the model generates outputs and", "tokens": [50976, 300, 311, 1217, 4399, 2698, 32675, 293, 550, 294, 257, 1150, 5574, 264, 2316, 23815, 23930, 293, 51280], "temperature": 0.0, "avg_logprob": -0.04494476636250814, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.34794148802757263}, {"id": 220, "seek": 129828, "start": 1316.6, "end": 1322.6, "text": " humans rank all of the outputs the model has produced and that feedback goes into a lightweight", "tokens": [51280, 6255, 6181, 439, 295, 264, 23930, 264, 2316, 575, 7126, 293, 300, 5824, 1709, 666, 257, 22052, 51580], "temperature": 0.0, "avg_logprob": -0.04494476636250814, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.34794148802757263}, {"id": 221, "seek": 132260, "start": 1322.6, "end": 1329.0, "text": " reinforcement learning mechanism in both of those phases we have important human contributions", "tokens": [50364, 29280, 2539, 7513, 294, 1293, 295, 729, 18764, 321, 362, 1021, 1952, 15725, 50684], "temperature": 0.0, "avg_logprob": -0.03833879874302791, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.031100699678063393}, {"id": 222, "seek": 132260, "start": 1329.0, "end": 1334.6, "text": " that take us beyond that self supervision step and kind of reduce the magical feeling of how", "tokens": [50684, 300, 747, 505, 4399, 300, 2698, 32675, 1823, 293, 733, 295, 5407, 264, 12066, 2633, 295, 577, 50964], "temperature": 0.0, "avg_logprob": -0.03833879874302791, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.031100699678063393}, {"id": 223, "seek": 132260, "start": 1334.6, "end": 1340.6799999999998, "text": " these models are achieving so much I'm emphasizing this because I think what we're seeing is a return", "tokens": [50964, 613, 5245, 366, 19626, 370, 709, 286, 478, 45550, 341, 570, 286, 519, 437, 321, 434, 2577, 307, 257, 2736, 51268], "temperature": 0.0, "avg_logprob": -0.03833879874302791, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.031100699678063393}, {"id": 224, "seek": 132260, "start": 1340.6799999999998, "end": 1346.28, "text": " to a familiar and kind of cynical sounding story about AI which is that many of the transformative", "tokens": [51268, 281, 257, 4963, 293, 733, 295, 46345, 24931, 1657, 466, 7318, 597, 307, 300, 867, 295, 264, 36070, 51548], "temperature": 0.0, "avg_logprob": -0.03833879874302791, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.031100699678063393}, {"id": 225, "seek": 134628, "start": 1346.28, "end": 1353.08, "text": " step forwards are actually on the back of a lot of human effort behind the scenes expressed at the", "tokens": [50364, 1823, 30126, 366, 767, 322, 264, 646, 295, 257, 688, 295, 1952, 4630, 2261, 264, 8026, 12675, 412, 264, 50704], "temperature": 0.0, "avg_logprob": -0.03616178887231009, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.11110323667526245}, {"id": 226, "seek": 134628, "start": 1353.08, "end": 1359.56, "text": " level of training data but on the positive side here it is incredible that this human feedback", "tokens": [50704, 1496, 295, 3097, 1412, 457, 322, 264, 3353, 1252, 510, 309, 307, 4651, 300, 341, 1952, 5824, 51028], "temperature": 0.0, "avg_logprob": -0.03616178887231009, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.11110323667526245}, {"id": 227, "seek": 134628, "start": 1359.56, "end": 1365.24, "text": " is having such an important impact instruct models are best in class in the field and we have a lot", "tokens": [51028, 307, 1419, 1270, 364, 1021, 2712, 7232, 5245, 366, 1151, 294, 1508, 294, 264, 2519, 293, 321, 362, 257, 688, 51312], "temperature": 0.0, "avg_logprob": -0.03616178887231009, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.11110323667526245}, {"id": 228, "seek": 134628, "start": 1365.24, "end": 1371.24, "text": " of evidence that that must be because of these human feedback steps happening at a scale that I", "tokens": [51312, 295, 4467, 300, 300, 1633, 312, 570, 295, 613, 1952, 5824, 4439, 2737, 412, 257, 4373, 300, 286, 51612], "temperature": 0.0, "avg_logprob": -0.03616178887231009, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.11110323667526245}, {"id": 229, "seek": 137124, "start": 1371.32, "end": 1376.84, "text": " assume is astounding they must have at open AI large teams of people providing very fine", "tokens": [50368, 6552, 307, 5357, 24625, 436, 1633, 362, 412, 1269, 7318, 2416, 5491, 295, 561, 6530, 588, 2489, 50644], "temperature": 0.0, "avg_logprob": -0.05640007245658648, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0541239008307457}, {"id": 230, "seek": 137124, "start": 1376.84, "end": 1381.8, "text": " green feedback across lots of different domains with lots of different tasks in mind", "tokens": [50644, 3092, 5824, 2108, 3195, 295, 819, 25514, 365, 3195, 295, 819, 9608, 294, 1575, 50892], "temperature": 0.0, "avg_logprob": -0.05640007245658648, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0541239008307457}, {"id": 231, "seek": 137124, "start": 1384.28, "end": 1390.6, "text": " final piece by way of background prompting itself this has been a real journey for all of us I've", "tokens": [51016, 2572, 2522, 538, 636, 295, 3678, 12391, 278, 2564, 341, 575, 668, 257, 957, 4671, 337, 439, 295, 505, 286, 600, 51332], "temperature": 0.0, "avg_logprob": -0.05640007245658648, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0541239008307457}, {"id": 232, "seek": 137124, "start": 1390.6, "end": 1396.2, "text": " described this as step by step and chain of thought reasoning to give you a feel for how", "tokens": [51332, 7619, 341, 382, 1823, 538, 1823, 293, 5021, 295, 1194, 21577, 281, 976, 291, 257, 841, 337, 577, 51612], "temperature": 0.0, "avg_logprob": -0.05640007245658648, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0541239008307457}, {"id": 233, "seek": 137124, "start": 1396.2, "end": 1400.92, "text": " this is happening let's just imagine that we've posed a question like can our models reason about", "tokens": [51612, 341, 307, 2737, 718, 311, 445, 3811, 300, 321, 600, 31399, 257, 1168, 411, 393, 527, 5245, 1778, 466, 51848], "temperature": 0.0, "avg_logprob": -0.05640007245658648, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0541239008307457}, {"id": 234, "seek": 140092, "start": 1401.0, "end": 1408.6000000000001, "text": " negation that is if we didn't eat any food does the model know that we didn't eat any pizza in the", "tokens": [50368, 2485, 399, 300, 307, 498, 321, 994, 380, 1862, 604, 1755, 775, 264, 2316, 458, 300, 321, 994, 380, 1862, 604, 8298, 294, 264, 50748], "temperature": 0.0, "avg_logprob": -0.04105673030931122, "compression_ratio": 1.9073170731707316, "no_speech_prob": 0.0024296827614307404}, {"id": 235, "seek": 140092, "start": 1408.6000000000001, "end": 1416.28, "text": " old days of 2021 we were so naive we would prompt models with just that direct question like is it", "tokens": [50748, 1331, 1708, 295, 7201, 321, 645, 370, 29052, 321, 576, 12391, 5245, 365, 445, 300, 2047, 1168, 411, 307, 309, 51132], "temperature": 0.0, "avg_logprob": -0.04105673030931122, "compression_ratio": 1.9073170731707316, "no_speech_prob": 0.0024296827614307404}, {"id": 236, "seek": 140092, "start": 1416.28, "end": 1420.44, "text": " true that if we didn't eat any food then we didn't eat any pizza and we would see what the model", "tokens": [51132, 2074, 300, 498, 321, 994, 380, 1862, 604, 1755, 550, 321, 994, 380, 1862, 604, 8298, 293, 321, 576, 536, 437, 264, 2316, 51340], "temperature": 0.0, "avg_logprob": -0.04105673030931122, "compression_ratio": 1.9073170731707316, "no_speech_prob": 0.0024296827614307404}, {"id": 237, "seek": 140092, "start": 1420.44, "end": 1429.24, "text": " said in return now in 2023 we know so much and we have learned that it can really help to design", "tokens": [51340, 848, 294, 2736, 586, 294, 44377, 321, 458, 370, 709, 293, 321, 362, 3264, 300, 309, 393, 534, 854, 281, 1715, 51780], "temperature": 0.0, "avg_logprob": -0.04105673030931122, "compression_ratio": 1.9073170731707316, "no_speech_prob": 0.0024296827614307404}, {"id": 238, "seek": 142924, "start": 1429.24, "end": 1433.96, "text": " a prompt that helps the model reason in the intended ways this is often called step by step", "tokens": [50364, 257, 12391, 300, 3665, 264, 2316, 1778, 294, 264, 10226, 2098, 341, 307, 2049, 1219, 1823, 538, 1823, 50600], "temperature": 0.0, "avg_logprob": -0.06480341220120771, "compression_ratio": 1.8858267716535433, "no_speech_prob": 0.10641736537218094}, {"id": 239, "seek": 142924, "start": 1433.96, "end": 1438.84, "text": " reasoning here's an example of a prompt that was given to me by Omar Khattab you start by telling", "tokens": [50600, 21577, 510, 311, 364, 1365, 295, 257, 12391, 300, 390, 2212, 281, 385, 538, 33784, 11681, 1591, 455, 291, 722, 538, 3585, 50844], "temperature": 0.0, "avg_logprob": -0.06480341220120771, "compression_ratio": 1.8858267716535433, "no_speech_prob": 0.10641736537218094}, {"id": 240, "seek": 142924, "start": 1438.84, "end": 1444.04, "text": " it it's a logic and common sense reasoning exam for some reason that's helpful then you give it", "tokens": [50844, 309, 309, 311, 257, 9952, 293, 2689, 2020, 21577, 1139, 337, 512, 1778, 300, 311, 4961, 550, 291, 976, 309, 51104], "temperature": 0.0, "avg_logprob": -0.06480341220120771, "compression_ratio": 1.8858267716535433, "no_speech_prob": 0.10641736537218094}, {"id": 241, "seek": 142924, "start": 1444.04, "end": 1450.04, "text": " some specific instructions and then you use some special markup to give it an example of the kind", "tokens": [51104, 512, 2685, 9415, 293, 550, 291, 764, 512, 2121, 1491, 1010, 281, 976, 309, 364, 1365, 295, 264, 733, 51404], "temperature": 0.0, "avg_logprob": -0.06480341220120771, "compression_ratio": 1.8858267716535433, "no_speech_prob": 0.10641736537218094}, {"id": 242, "seek": 142924, "start": 1450.04, "end": 1456.1200000000001, "text": " of reasoning that you would like it to follow after that example comes the actual prompt and in", "tokens": [51404, 295, 21577, 300, 291, 576, 411, 309, 281, 1524, 934, 300, 1365, 1487, 264, 3539, 12391, 293, 294, 51708], "temperature": 0.0, "avg_logprob": -0.06480341220120771, "compression_ratio": 1.8858267716535433, "no_speech_prob": 0.10641736537218094}, {"id": 243, "seek": 145612, "start": 1456.12, "end": 1462.04, "text": " this context what we essentially ask the model to do is express its own reasoning and then conditional", "tokens": [50364, 341, 4319, 437, 321, 4476, 1029, 264, 2316, 281, 360, 307, 5109, 1080, 1065, 21577, 293, 550, 27708, 50660], "temperature": 0.0, "avg_logprob": -0.059968362535749165, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.006787626072764397}, {"id": 244, "seek": 145612, "start": 1462.04, "end": 1468.52, "text": " on what it has produced create an answer and the eye-opening thing about the current era is that", "tokens": [50660, 322, 437, 309, 575, 7126, 1884, 364, 1867, 293, 264, 3313, 12, 404, 4559, 551, 466, 264, 2190, 4249, 307, 300, 50984], "temperature": 0.0, "avg_logprob": -0.059968362535749165, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.006787626072764397}, {"id": 245, "seek": 145612, "start": 1468.52, "end": 1473.0, "text": " this can be transformative better I think if you wanted to put this poetically you'd say that these", "tokens": [50984, 341, 393, 312, 36070, 1101, 286, 519, 498, 291, 1415, 281, 829, 341, 20874, 984, 291, 1116, 584, 300, 613, 51208], "temperature": 0.0, "avg_logprob": -0.059968362535749165, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.006787626072764397}, {"id": 246, "seek": 145612, "start": 1473.0, "end": 1478.04, "text": " large language models are kind of like alien creatures and it's taking us some time to figure", "tokens": [51208, 2416, 2856, 5245, 366, 733, 295, 411, 12319, 12281, 293, 309, 311, 1940, 505, 512, 565, 281, 2573, 51460], "temperature": 0.0, "avg_logprob": -0.059968362535749165, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.006787626072764397}, {"id": 247, "seek": 145612, "start": 1478.04, "end": 1483.1599999999999, "text": " out how to communicate with them and together with all that instruct fine tuning with human", "tokens": [51460, 484, 577, 281, 7890, 365, 552, 293, 1214, 365, 439, 300, 7232, 2489, 15164, 365, 1952, 51716], "temperature": 0.0, "avg_logprob": -0.059968362535749165, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.006787626072764397}, {"id": 248, "seek": 148316, "start": 1483.16, "end": 1488.76, "text": " supervision we're converging on prompts like this as the powerful device and this is exciting to me", "tokens": [50364, 32675, 321, 434, 9652, 3249, 322, 41095, 411, 341, 382, 264, 4005, 4302, 293, 341, 307, 4670, 281, 385, 50644], "temperature": 0.0, "avg_logprob": -0.05618813431378707, "compression_ratio": 1.75, "no_speech_prob": 0.005999368149787188}, {"id": 249, "seek": 148316, "start": 1488.76, "end": 1495.0800000000002, "text": " because what's really emerging is that this is a kind of very light way of programming an AI system", "tokens": [50644, 570, 437, 311, 534, 14989, 307, 300, 341, 307, 257, 733, 295, 588, 1442, 636, 295, 9410, 364, 7318, 1185, 50960], "temperature": 0.0, "avg_logprob": -0.05618813431378707, "compression_ratio": 1.75, "no_speech_prob": 0.005999368149787188}, {"id": 250, "seek": 148316, "start": 1495.0800000000002, "end": 1499.72, "text": " using only prompts as opposed to all the deep learning code that we used to have to write", "tokens": [50960, 1228, 787, 41095, 382, 8851, 281, 439, 264, 2452, 2539, 3089, 300, 321, 1143, 281, 362, 281, 2464, 51192], "temperature": 0.0, "avg_logprob": -0.05618813431378707, "compression_ratio": 1.75, "no_speech_prob": 0.005999368149787188}, {"id": 251, "seek": 148316, "start": 1499.72, "end": 1504.3600000000001, "text": " and that's going to be incredibly empowering in terms of system development and experimentation", "tokens": [51192, 293, 300, 311, 516, 281, 312, 6252, 28261, 294, 2115, 295, 1185, 3250, 293, 37142, 51424], "temperature": 0.0, "avg_logprob": -0.05618813431378707, "compression_ratio": 1.75, "no_speech_prob": 0.005999368149787188}, {"id": 252, "seek": 148316, "start": 1506.68, "end": 1511.24, "text": " all right so we have our background in place I'd like to move to my main topic here", "tokens": [51540, 439, 558, 370, 321, 362, 527, 3678, 294, 1081, 286, 1116, 411, 281, 1286, 281, 452, 2135, 4829, 510, 51768], "temperature": 0.0, "avg_logprob": -0.05618813431378707, "compression_ratio": 1.75, "no_speech_prob": 0.005999368149787188}, {"id": 253, "seek": 151124, "start": 1511.24, "end": 1515.8, "text": " which is retrieval augmented in-context learning what you're going to see here is a", "tokens": [50364, 597, 307, 19817, 3337, 36155, 294, 12, 9000, 3828, 2539, 437, 291, 434, 516, 281, 536, 510, 307, 257, 50592], "temperature": 0.0, "avg_logprob": -0.07503853422222716, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.00413173995912075}, {"id": 254, "seek": 151124, "start": 1515.8, "end": 1521.64, "text": " combination of language models with retriever models which are themselves under the hood", "tokens": [50592, 6562, 295, 2856, 5245, 365, 19817, 331, 5245, 597, 366, 2969, 833, 264, 13376, 50884], "temperature": 0.0, "avg_logprob": -0.07503853422222716, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.00413173995912075}, {"id": 255, "seek": 151124, "start": 1521.64, "end": 1527.16, "text": " large language models as well but let me start with a bit of the backstory here I think we're", "tokens": [50884, 2416, 2856, 5245, 382, 731, 457, 718, 385, 722, 365, 257, 857, 295, 264, 36899, 510, 286, 519, 321, 434, 51160], "temperature": 0.0, "avg_logprob": -0.07503853422222716, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.00413173995912075}, {"id": 256, "seek": 151124, "start": 1527.16, "end": 1533.0, "text": " all probably vaguely aware at this point that large language models have been revolutionizing", "tokens": [51160, 439, 1391, 13501, 48863, 3650, 412, 341, 935, 300, 2416, 2856, 5245, 362, 668, 8894, 3319, 51452], "temperature": 0.0, "avg_logprob": -0.07503853422222716, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.00413173995912075}, {"id": 257, "seek": 151124, "start": 1533.0, "end": 1539.4, "text": " search again the star of this is the transformer or maybe more specifically its famous spokesmodel", "tokens": [51452, 3164, 797, 264, 3543, 295, 341, 307, 264, 31782, 420, 1310, 544, 4682, 1080, 4618, 25378, 8014, 338, 51772], "temperature": 0.0, "avg_logprob": -0.07503853422222716, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.00413173995912075}, {"id": 258, "seek": 153940, "start": 1539.4, "end": 1545.4, "text": " Burt right after Burt was announced around 2018 Google announced that it was incorporating", "tokens": [50364, 363, 6224, 558, 934, 363, 6224, 390, 7548, 926, 6096, 3329, 7548, 300, 309, 390, 33613, 50664], "temperature": 0.0, "avg_logprob": -0.06340542817727113, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007572412956506014}, {"id": 259, "seek": 153940, "start": 1545.4, "end": 1551.24, "text": " aspects of Burt into its core search technology and Microsoft made a similar announcement at", "tokens": [50664, 7270, 295, 363, 6224, 666, 1080, 4965, 3164, 2899, 293, 8116, 1027, 257, 2531, 12847, 412, 50956], "temperature": 0.0, "avg_logprob": -0.06340542817727113, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007572412956506014}, {"id": 260, "seek": 153940, "start": 1551.24, "end": 1557.8000000000002, "text": " about the same time and I think those are just two public facing stories of you know many instances", "tokens": [50956, 466, 264, 912, 565, 293, 286, 519, 729, 366, 445, 732, 1908, 7170, 3676, 295, 291, 458, 867, 14519, 51284], "temperature": 0.0, "avg_logprob": -0.06340542817727113, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007572412956506014}, {"id": 261, "seek": 153940, "start": 1557.8000000000002, "end": 1564.1200000000001, "text": " of large search technologies having Burt elements incorporated into them in that era and then of", "tokens": [51284, 295, 2416, 3164, 7943, 1419, 363, 6224, 4959, 21654, 666, 552, 294, 300, 4249, 293, 550, 295, 51600], "temperature": 0.0, "avg_logprob": -0.06340542817727113, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007572412956506014}, {"id": 262, "seek": 156412, "start": 1564.12, "end": 1569.8, "text": " course in the current era we have startups like you.com which have made large language models", "tokens": [50364, 1164, 294, 264, 2190, 4249, 321, 362, 28041, 411, 291, 13, 1112, 597, 362, 1027, 2416, 2856, 5245, 50648], "temperature": 0.0, "avg_logprob": -0.05934999607227467, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002933745039626956}, {"id": 263, "seek": 156412, "start": 1569.8, "end": 1575.56, "text": " pretty central to the entire search experience in the form of you know delivering results but also", "tokens": [50648, 1238, 5777, 281, 264, 2302, 3164, 1752, 294, 264, 1254, 295, 291, 458, 14666, 3542, 457, 611, 50936], "temperature": 0.0, "avg_logprob": -0.05934999607227467, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002933745039626956}, {"id": 264, "seek": 156412, "start": 1575.56, "end": 1583.0, "text": " interactive search with conversational agents so that's all exciting but I am an NLP at heart", "tokens": [50936, 15141, 3164, 365, 2615, 1478, 12554, 370, 300, 311, 439, 4670, 457, 286, 669, 364, 426, 45196, 412, 1917, 51308], "temperature": 0.0, "avg_logprob": -0.05934999607227467, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002933745039626956}, {"id": 265, "seek": 156412, "start": 1583.0, "end": 1587.7199999999998, "text": " and so for me in a way the more exciting direction here is the fact that finally", "tokens": [51308, 293, 370, 337, 385, 294, 257, 636, 264, 544, 4670, 3513, 510, 307, 264, 1186, 300, 2721, 51544], "temperature": 0.0, "avg_logprob": -0.05934999607227467, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002933745039626956}, {"id": 266, "seek": 158772, "start": 1588.3600000000001, "end": 1595.24, "text": " search is revolutionizing NLP by helping us bridge the gap into much more relevant", "tokens": [50396, 3164, 307, 8894, 3319, 426, 45196, 538, 4315, 505, 7283, 264, 7417, 666, 709, 544, 7340, 50740], "temperature": 0.0, "avg_logprob": -0.09264988845653749, "compression_ratio": 1.625, "no_speech_prob": 0.48786234855651855}, {"id": 267, "seek": 158772, "start": 1595.24, "end": 1600.76, "text": " knowledge intensive tasks to give you a feel for how that's happening let's just use question", "tokens": [50740, 3601, 18957, 9608, 281, 976, 291, 257, 841, 337, 577, 300, 311, 2737, 718, 311, 445, 764, 1168, 51016], "temperature": 0.0, "avg_logprob": -0.09264988845653749, "compression_ratio": 1.625, "no_speech_prob": 0.48786234855651855}, {"id": 268, "seek": 158772, "start": 1600.76, "end": 1608.1200000000001, "text": " answering as an example so prior to this work in NLP we would pose question answering or QA in the", "tokens": [51016, 13430, 382, 364, 1365, 370, 4059, 281, 341, 589, 294, 426, 45196, 321, 576, 10774, 1168, 13430, 420, 1249, 32, 294, 264, 51384], "temperature": 0.0, "avg_logprob": -0.09264988845653749, "compression_ratio": 1.625, "no_speech_prob": 0.48786234855651855}, {"id": 269, "seek": 158772, "start": 1608.1200000000001, "end": 1615.64, "text": " following way you saw this already with the GPT-3 example we would have as given at test time a title", "tokens": [51384, 3480, 636, 291, 1866, 341, 1217, 365, 264, 26039, 51, 12, 18, 1365, 321, 576, 362, 382, 2212, 412, 1500, 565, 257, 4876, 51760], "temperature": 0.0, "avg_logprob": -0.09264988845653749, "compression_ratio": 1.625, "no_speech_prob": 0.48786234855651855}, {"id": 270, "seek": 161564, "start": 1615.64, "end": 1621.88, "text": " and a context passage and then a question and the task of the model is to find the answer to that", "tokens": [50364, 293, 257, 4319, 11497, 293, 550, 257, 1168, 293, 264, 5633, 295, 264, 2316, 307, 281, 915, 264, 1867, 281, 300, 50676], "temperature": 0.0, "avg_logprob": -0.052733951740050584, "compression_ratio": 1.8238095238095238, "no_speech_prob": 0.006089997012168169}, {"id": 271, "seek": 161564, "start": 1621.88, "end": 1627.8000000000002, "text": " question as a literal substring of the context passage which was guaranteed by the nature of", "tokens": [50676, 1168, 382, 257, 20411, 4594, 2937, 295, 264, 4319, 11497, 597, 390, 18031, 538, 264, 3687, 295, 50972], "temperature": 0.0, "avg_logprob": -0.052733951740050584, "compression_ratio": 1.8238095238095238, "no_speech_prob": 0.006089997012168169}, {"id": 272, "seek": 161564, "start": 1627.8000000000002, "end": 1634.76, "text": " the data set as you can imagine models are really good at this task superhuman certainly at this", "tokens": [50972, 264, 1412, 992, 382, 291, 393, 3811, 5245, 366, 534, 665, 412, 341, 5633, 1687, 18796, 3297, 412, 341, 51320], "temperature": 0.0, "avg_logprob": -0.052733951740050584, "compression_ratio": 1.8238095238095238, "no_speech_prob": 0.006089997012168169}, {"id": 273, "seek": 161564, "start": 1634.76, "end": 1640.44, "text": " task but it's also a very rarefied task this is not a natural form of question answering in the", "tokens": [51320, 5633, 457, 309, 311, 611, 257, 588, 5892, 69, 1091, 5633, 341, 307, 406, 257, 3303, 1254, 295, 1168, 13430, 294, 264, 51604], "temperature": 0.0, "avg_logprob": -0.052733951740050584, "compression_ratio": 1.8238095238095238, "no_speech_prob": 0.006089997012168169}, {"id": 274, "seek": 164044, "start": 1640.44, "end": 1646.44, "text": " world and it's certainly unlike the scenario of for example doing web search so the promise of the", "tokens": [50364, 1002, 293, 309, 311, 3297, 8343, 264, 9005, 295, 337, 1365, 884, 3670, 3164, 370, 264, 6228, 295, 264, 50664], "temperature": 0.0, "avg_logprob": -0.05294776504689997, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.06944897770881653}, {"id": 275, "seek": 164044, "start": 1646.44, "end": 1651.56, "text": " open formulations of this task are that we're going to connect more directly with the real world", "tokens": [50664, 1269, 1254, 4136, 295, 341, 5633, 366, 300, 321, 434, 516, 281, 1745, 544, 3838, 365, 264, 957, 1002, 50920], "temperature": 0.0, "avg_logprob": -0.05294776504689997, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.06944897770881653}, {"id": 276, "seek": 164044, "start": 1651.56, "end": 1659.4, "text": " in this formulation at test time we're just given a question and the standard strategy is to rely on", "tokens": [50920, 294, 341, 37642, 412, 1500, 565, 321, 434, 445, 2212, 257, 1168, 293, 264, 3832, 5206, 307, 281, 10687, 322, 51312], "temperature": 0.0, "avg_logprob": -0.05294776504689997, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.06944897770881653}, {"id": 277, "seek": 164044, "start": 1659.4, "end": 1665.88, "text": " some kind of retrieval mechanism to find relevant evidence in a large corpus or maybe even the web", "tokens": [51312, 512, 733, 295, 19817, 3337, 7513, 281, 915, 7340, 4467, 294, 257, 2416, 1181, 31624, 420, 1310, 754, 264, 3670, 51636], "temperature": 0.0, "avg_logprob": -0.05294776504689997, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.06944897770881653}, {"id": 278, "seek": 166588, "start": 1666.44, "end": 1671.8000000000002, "text": " and then we proceed as before this is a much harder problem because we're not going to get", "tokens": [50392, 293, 550, 321, 8991, 382, 949, 341, 307, 257, 709, 6081, 1154, 570, 321, 434, 406, 516, 281, 483, 50660], "temperature": 0.0, "avg_logprob": -0.05546283721923828, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.02227315492928028}, {"id": 279, "seek": 166588, "start": 1671.8000000000002, "end": 1676.2800000000002, "text": " the substring guarantee anymore because we're dependent on the retriever to find relevant", "tokens": [50660, 264, 4594, 2937, 10815, 3602, 570, 321, 434, 12334, 322, 264, 19817, 331, 281, 915, 7340, 50884], "temperature": 0.0, "avg_logprob": -0.05546283721923828, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.02227315492928028}, {"id": 280, "seek": 166588, "start": 1676.2800000000002, "end": 1682.2, "text": " evidence but of course it's a much more important task because this is much more like our experience", "tokens": [50884, 4467, 457, 295, 1164, 309, 311, 257, 709, 544, 1021, 5633, 570, 341, 307, 709, 544, 411, 527, 1752, 51180], "temperature": 0.0, "avg_logprob": -0.05546283721923828, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.02227315492928028}, {"id": 281, "seek": 166588, "start": 1682.2, "end": 1688.6000000000001, "text": " of searching on the web now I've kind of biased already in describing things this way where I", "tokens": [51180, 295, 10808, 322, 264, 3670, 586, 286, 600, 733, 295, 28035, 1217, 294, 16141, 721, 341, 636, 689, 286, 51500], "temperature": 0.0, "avg_logprob": -0.05546283721923828, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.02227315492928028}, {"id": 282, "seek": 166588, "start": 1688.6000000000001, "end": 1694.6000000000001, "text": " assume we're retrieving a passage but there is another narrative out there let me skip to this", "tokens": [51500, 6552, 321, 434, 19817, 798, 257, 11497, 457, 456, 307, 1071, 9977, 484, 456, 718, 385, 10023, 281, 341, 51800], "temperature": 0.0, "avg_logprob": -0.05546283721923828, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.02227315492928028}, {"id": 283, "seek": 169460, "start": 1694.6, "end": 1698.9199999999998, "text": " then you could call this like the llms for everything approach and this would be where", "tokens": [50364, 550, 291, 727, 818, 341, 411, 264, 287, 75, 2592, 337, 1203, 3109, 293, 341, 576, 312, 689, 50580], "temperature": 0.0, "avg_logprob": -0.054891138076782225, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.014701138250529766}, {"id": 284, "seek": 169460, "start": 1698.9199999999998, "end": 1704.6, "text": " there's no explicit retriever you just have a question come in you have a big opaque model", "tokens": [50580, 456, 311, 572, 13691, 19817, 331, 291, 445, 362, 257, 1168, 808, 294, 291, 362, 257, 955, 42687, 2316, 50864], "temperature": 0.0, "avg_logprob": -0.054891138076782225, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.014701138250529766}, {"id": 285, "seek": 169460, "start": 1704.6, "end": 1710.1999999999998, "text": " process that question and out comes an answer voila you hope that the user's information", "tokens": [50864, 1399, 300, 1168, 293, 484, 1487, 364, 1867, 45565, 291, 1454, 300, 264, 4195, 311, 1589, 51144], "temperature": 0.0, "avg_logprob": -0.054891138076782225, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.014701138250529766}, {"id": 286, "seek": 169460, "start": 1710.1999999999998, "end": 1716.52, "text": " need is met directly no separate retrieval mechanism just the language model doing everything I think", "tokens": [51144, 643, 307, 1131, 3838, 572, 4994, 19817, 3337, 7513, 445, 264, 2856, 2316, 884, 1203, 286, 519, 51460], "temperature": 0.0, "avg_logprob": -0.054891138076782225, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.014701138250529766}, {"id": 287, "seek": 169460, "start": 1716.52, "end": 1721.8, "text": " this is an incredibly inspiring vision but we should be aware that there are lots of kind of", "tokens": [51460, 341, 307, 364, 6252, 15883, 5201, 457, 321, 820, 312, 3650, 300, 456, 366, 3195, 295, 733, 295, 51724], "temperature": 0.0, "avg_logprob": -0.054891138076782225, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.014701138250529766}, {"id": 288, "seek": 172180, "start": 1721.8, "end": 1728.68, "text": " danger zones here so the first is just efficiency one of the major factors driving that explosion", "tokens": [50364, 4330, 16025, 510, 370, 264, 700, 307, 445, 10493, 472, 295, 264, 2563, 6771, 4840, 300, 15673, 50708], "temperature": 0.0, "avg_logprob": -0.041163810869542565, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.011682392098009586}, {"id": 289, "seek": 172180, "start": 1728.68, "end": 1733.96, "text": " in model size that I tracked before is that in this llms for everything approach we are asking", "tokens": [50708, 294, 2316, 2744, 300, 286, 31703, 949, 307, 300, 294, 341, 287, 75, 2592, 337, 1203, 3109, 321, 366, 3365, 50972], "temperature": 0.0, "avg_logprob": -0.041163810869542565, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.011682392098009586}, {"id": 290, "seek": 172180, "start": 1733.96, "end": 1740.2, "text": " this model to play the role of both knowledge store and language capability if we could separate", "tokens": [50972, 341, 2316, 281, 862, 264, 3090, 295, 1293, 3601, 3531, 293, 2856, 13759, 498, 321, 727, 4994, 51284], "temperature": 0.0, "avg_logprob": -0.041163810869542565, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.011682392098009586}, {"id": 291, "seek": 172180, "start": 1740.2, "end": 1747.72, "text": " those out we might get away with smaller models we have a related problem of update ability suppose", "tokens": [51284, 729, 484, 321, 1062, 483, 1314, 365, 4356, 5245, 321, 362, 257, 4077, 1154, 295, 5623, 3485, 7297, 51660], "temperature": 0.0, "avg_logprob": -0.041163810869542565, "compression_ratio": 1.634453781512605, "no_speech_prob": 0.011682392098009586}, {"id": 292, "seek": 174772, "start": 1747.72, "end": 1753.08, "text": " a fact in the world changes that document on the web changes for example well you're going to have", "tokens": [50364, 257, 1186, 294, 264, 1002, 2962, 300, 4166, 322, 264, 3670, 2962, 337, 1365, 731, 291, 434, 516, 281, 362, 50632], "temperature": 0.0, "avg_logprob": -0.04466561736347519, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.011324298568069935}, {"id": 293, "seek": 174772, "start": 1753.08, "end": 1758.6000000000001, "text": " to update the parameters of this big opaque model somehow to conform to the change in reality", "tokens": [50632, 281, 5623, 264, 9834, 295, 341, 955, 42687, 2316, 6063, 281, 18975, 281, 264, 1319, 294, 4103, 50908], "temperature": 0.0, "avg_logprob": -0.04466561736347519, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.011324298568069935}, {"id": 294, "seek": 174772, "start": 1759.4, "end": 1763.8, "text": " there are people hard at work on that problem that's a very exciting problem but I think we're a", "tokens": [50948, 456, 366, 561, 1152, 412, 589, 322, 300, 1154, 300, 311, 257, 588, 4670, 1154, 457, 286, 519, 321, 434, 257, 51168], "temperature": 0.0, "avg_logprob": -0.04466561736347519, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.011324298568069935}, {"id": 295, "seek": 174772, "start": 1763.8, "end": 1769.48, "text": " long way from being able to offer guarantees that a change in the world is reflected in the model", "tokens": [51168, 938, 636, 490, 885, 1075, 281, 2626, 32567, 300, 257, 1319, 294, 264, 1002, 307, 15502, 294, 264, 2316, 51452], "temperature": 0.0, "avg_logprob": -0.04466561736347519, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.011324298568069935}, {"id": 296, "seek": 174772, "start": 1769.48, "end": 1775.8, "text": " behavior and that plays into all sorts of issues of trustworthiness and explainability of behavior", "tokens": [51452, 5223, 293, 300, 5749, 666, 439, 7527, 295, 2663, 295, 3361, 13136, 1324, 293, 2903, 2310, 295, 5223, 51768], "temperature": 0.0, "avg_logprob": -0.04466561736347519, "compression_ratio": 1.8066914498141264, "no_speech_prob": 0.011324298568069935}, {"id": 297, "seek": 177580, "start": 1775.8, "end": 1783.08, "text": " and so forth also we have an issue of provenance look at the answer at the bottom there is that the", "tokens": [50364, 293, 370, 5220, 611, 321, 362, 364, 2734, 295, 12785, 719, 574, 412, 264, 1867, 412, 264, 2767, 456, 307, 300, 264, 50728], "temperature": 0.0, "avg_logprob": -0.041990805555272986, "compression_ratio": 1.8, "no_speech_prob": 0.002979714423418045}, {"id": 298, "seek": 177580, "start": 1783.08, "end": 1788.68, "text": " correct answer should you trust this model right in the standard web search experience we typically", "tokens": [50728, 3006, 1867, 820, 291, 3361, 341, 2316, 558, 294, 264, 3832, 3670, 3164, 1752, 321, 5850, 51008], "temperature": 0.0, "avg_logprob": -0.041990805555272986, "compression_ratio": 1.8, "no_speech_prob": 0.002979714423418045}, {"id": 299, "seek": 177580, "start": 1788.68, "end": 1793.8799999999999, "text": " are given some web pages that we can click on to verify at least at the next level of detail", "tokens": [51008, 366, 2212, 512, 3670, 7183, 300, 321, 393, 2052, 322, 281, 16888, 412, 1935, 412, 264, 958, 1496, 295, 2607, 51268], "temperature": 0.0, "avg_logprob": -0.041990805555272986, "compression_ratio": 1.8, "no_speech_prob": 0.002979714423418045}, {"id": 300, "seek": 177580, "start": 1793.8799999999999, "end": 1799.56, "text": " whether the information is correct but here we're just given this response and if the model also", "tokens": [51268, 1968, 264, 1589, 307, 3006, 457, 510, 321, 434, 445, 2212, 341, 4134, 293, 498, 264, 2316, 611, 51552], "temperature": 0.0, "avg_logprob": -0.041990805555272986, "compression_ratio": 1.8, "no_speech_prob": 0.002979714423418045}, {"id": 301, "seek": 177580, "start": 1799.56, "end": 1803.6399999999999, "text": " generated a provenance string if it told us where it found the information we'd be left with the", "tokens": [51552, 10833, 257, 12785, 719, 6798, 498, 309, 1907, 505, 689, 309, 1352, 264, 1589, 321, 1116, 312, 1411, 365, 264, 51756], "temperature": 0.0, "avg_logprob": -0.041990805555272986, "compression_ratio": 1.8, "no_speech_prob": 0.002979714423418045}, {"id": 302, "seek": 180364, "start": 1803.64, "end": 1809.3200000000002, "text": " concern that that provenance string was also untrustworthy right and this is like a really", "tokens": [50364, 3136, 300, 300, 12785, 719, 6798, 390, 611, 1701, 22326, 23727, 558, 293, 341, 307, 411, 257, 534, 50648], "temperature": 0.0, "avg_logprob": -0.055590057373046876, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0012063514441251755}, {"id": 303, "seek": 180364, "start": 1809.3200000000002, "end": 1814.44, "text": " breaking a fundamental contract that users expect to have with search technologies I believe", "tokens": [50648, 7697, 257, 8088, 4364, 300, 5022, 2066, 281, 362, 365, 3164, 7943, 286, 1697, 50904], "temperature": 0.0, "avg_logprob": -0.055590057373046876, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0012063514441251755}, {"id": 304, "seek": 180364, "start": 1815.3200000000002, "end": 1819.88, "text": " so those are some things to worry about there are positives though of course these models are", "tokens": [50948, 370, 729, 366, 512, 721, 281, 3292, 466, 456, 366, 35127, 1673, 295, 1164, 613, 5245, 366, 51176], "temperature": 0.0, "avg_logprob": -0.055590057373046876, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0012063514441251755}, {"id": 305, "seek": 180364, "start": 1819.88, "end": 1825.96, "text": " incredibly effective at meeting your information need directly and they're also outstanding at", "tokens": [51176, 6252, 4942, 412, 3440, 428, 1589, 643, 3838, 293, 436, 434, 611, 14485, 412, 51480], "temperature": 0.0, "avg_logprob": -0.055590057373046876, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0012063514441251755}, {"id": 306, "seek": 180364, "start": 1825.96, "end": 1831.0800000000002, "text": " synthesizing information if your question can only be answered by 10 different web pages", "tokens": [51480, 26617, 3319, 1589, 498, 428, 1168, 393, 787, 312, 10103, 538, 1266, 819, 3670, 7183, 51736], "temperature": 0.0, "avg_logprob": -0.055590057373046876, "compression_ratio": 1.6702898550724639, "no_speech_prob": 0.0012063514441251755}, {"id": 307, "seek": 183108, "start": 1831.08, "end": 1835.1599999999999, "text": " it's very likely that the language model will still be able to do it without you having to hunt", "tokens": [50364, 309, 311, 588, 3700, 300, 264, 2856, 2316, 486, 920, 312, 1075, 281, 360, 309, 1553, 291, 1419, 281, 12454, 50568], "temperature": 0.0, "avg_logprob": -0.05861771243742143, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.002799169160425663}, {"id": 308, "seek": 183108, "start": 1835.1599999999999, "end": 1842.36, "text": " through all those pages so exciting but lots of concerns here here is the alternative of retrieval", "tokens": [50568, 807, 439, 729, 7183, 370, 4670, 457, 3195, 295, 7389, 510, 510, 307, 264, 8535, 295, 19817, 3337, 50928], "temperature": 0.0, "avg_logprob": -0.05861771243742143, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.002799169160425663}, {"id": 309, "seek": 183108, "start": 1842.36, "end": 1849.32, "text": " augmented approaches right oh I can't resist this actually just to give you an example of how", "tokens": [50928, 36155, 11587, 558, 1954, 286, 393, 380, 4597, 341, 767, 445, 281, 976, 291, 364, 1365, 295, 577, 51276], "temperature": 0.0, "avg_logprob": -0.05861771243742143, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.002799169160425663}, {"id": 310, "seek": 183108, "start": 1849.32, "end": 1856.9199999999998, "text": " important this trustworthy thing can be so I used to be impressed by DaVinci 3 because it would give", "tokens": [51276, 1021, 341, 39714, 551, 393, 312, 370, 286, 1143, 281, 312, 11679, 538, 3933, 53, 21961, 805, 570, 309, 576, 976, 51656], "temperature": 0.0, "avg_logprob": -0.05861771243742143, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.002799169160425663}, {"id": 311, "seek": 185692, "start": 1857.0, "end": 1862.1200000000001, "text": " a correct answer to the question are professional baseball players allowed to glue small wings", "tokens": [50368, 257, 3006, 1867, 281, 264, 1168, 366, 4843, 14323, 4150, 4350, 281, 8998, 1359, 11405, 50624], "temperature": 0.0, "avg_logprob": -0.06410727545479748, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.11895719170570374}, {"id": 312, "seek": 185692, "start": 1862.1200000000001, "end": 1866.92, "text": " onto their caps this is a question that I got from a wonderful article by Hector Levec where he", "tokens": [50624, 3911, 641, 13855, 341, 307, 257, 1168, 300, 286, 658, 490, 257, 3715, 7222, 538, 389, 20814, 1456, 303, 66, 689, 415, 50864], "temperature": 0.0, "avg_logprob": -0.06410727545479748, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.11895719170570374}, {"id": 313, "seek": 185692, "start": 1866.92, "end": 1872.76, "text": " encourages us to stress test our models by asking them questions that would seem to run up against", "tokens": [50864, 28071, 505, 281, 4244, 1500, 527, 5245, 538, 3365, 552, 1651, 300, 576, 1643, 281, 1190, 493, 1970, 51156], "temperature": 0.0, "avg_logprob": -0.06410727545479748, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.11895719170570374}, {"id": 314, "seek": 185692, "start": 1872.76, "end": 1878.28, "text": " any simple distributional or statistical learning model and really get at whether they have a model", "tokens": [51156, 604, 2199, 7316, 304, 420, 22820, 2539, 2316, 293, 534, 483, 412, 1968, 436, 362, 257, 2316, 51432], "temperature": 0.0, "avg_logprob": -0.06410727545479748, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.11895719170570374}, {"id": 315, "seek": 185692, "start": 1878.28, "end": 1883.72, "text": " of the world and for DaVinci 2 it gave what it looked like a really good Levec style answer", "tokens": [51432, 295, 264, 1002, 293, 337, 3933, 53, 21961, 568, 309, 2729, 437, 309, 2956, 411, 257, 534, 665, 1456, 303, 66, 3758, 1867, 51704], "temperature": 0.0, "avg_logprob": -0.06410727545479748, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.11895719170570374}, {"id": 316, "seek": 188372, "start": 1883.72, "end": 1891.0, "text": " there is no rule against it but it is not common that seems true so I was disappointed I guess", "tokens": [50364, 456, 307, 572, 4978, 1970, 309, 457, 309, 307, 406, 2689, 300, 2544, 2074, 370, 286, 390, 13856, 286, 2041, 50728], "temperature": 0.0, "avg_logprob": -0.05112666629609607, "compression_ratio": 1.7, "no_speech_prob": 0.0655582845211029}, {"id": 317, "seek": 188372, "start": 1891.0, "end": 1895.16, "text": " or I'm actually not sure how to feel about this when I asked DaVinci 3 the same question and it", "tokens": [50728, 420, 286, 478, 767, 406, 988, 577, 281, 841, 466, 341, 562, 286, 2351, 3933, 53, 21961, 805, 264, 912, 1168, 293, 309, 50936], "temperature": 0.0, "avg_logprob": -0.05112666629609607, "compression_ratio": 1.7, "no_speech_prob": 0.0655582845211029}, {"id": 318, "seek": 188372, "start": 1895.16, "end": 1900.44, "text": " said no professional baseball players are not allowed to glue small wings onto their caps major", "tokens": [50936, 848, 572, 4843, 14323, 4150, 366, 406, 4350, 281, 8998, 1359, 11405, 3911, 641, 13855, 2563, 51200], "temperature": 0.0, "avg_logprob": -0.05112666629609607, "compression_ratio": 1.7, "no_speech_prob": 0.0655582845211029}, {"id": 319, "seek": 188372, "start": 1900.44, "end": 1904.44, "text": " league baseball has strict rules about the appearance of players uniforms and caps in any", "tokens": [51200, 14957, 14323, 575, 10910, 4474, 466, 264, 8967, 295, 4150, 37235, 293, 13855, 294, 604, 51400], "temperature": 0.0, "avg_logprob": -0.05112666629609607, "compression_ratio": 1.7, "no_speech_prob": 0.0655582845211029}, {"id": 320, "seek": 188372, "start": 1904.44, "end": 1911.56, "text": " modification to the caps are not allowed that also sounds reasonable to me is it true it would help", "tokens": [51400, 26747, 281, 264, 13855, 366, 406, 4350, 300, 611, 3263, 10585, 281, 385, 307, 309, 2074, 309, 576, 854, 51756], "temperature": 0.0, "avg_logprob": -0.05112666629609607, "compression_ratio": 1.7, "no_speech_prob": 0.0655582845211029}, {"id": 321, "seek": 191156, "start": 1911.6399999999999, "end": 1917.3999999999999, "text": " enormously if the model could offer me at least a web page with with evidence that's relevant to", "tokens": [50368, 39669, 498, 264, 2316, 727, 2626, 385, 412, 1935, 257, 3670, 3028, 365, 365, 4467, 300, 311, 7340, 281, 50656], "temperature": 0.0, "avg_logprob": -0.06785366468340437, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.005056384019553661}, {"id": 322, "seek": 191156, "start": 1917.3999999999999, "end": 1923.48, "text": " these claims otherwise I'm simply left wondering and I think that shows you that we've kind of broken", "tokens": [50656, 613, 9441, 5911, 286, 478, 2935, 1411, 6359, 293, 286, 519, 300, 3110, 291, 300, 321, 600, 733, 295, 5463, 50960], "temperature": 0.0, "avg_logprob": -0.06785366468340437, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.005056384019553661}, {"id": 323, "seek": 191156, "start": 1923.48, "end": 1928.9199999999998, "text": " this implicit contract with the user that we expect from search so that'll bring me to my", "tokens": [50960, 341, 26947, 4364, 365, 264, 4195, 300, 321, 2066, 490, 3164, 370, 300, 603, 1565, 385, 281, 452, 51232], "temperature": 0.0, "avg_logprob": -0.06785366468340437, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.005056384019553661}, {"id": 324, "seek": 191156, "start": 1928.9199999999998, "end": 1934.28, "text": " alternative here retrieval based or retrieval augmented NLP to give you a sense for this at", "tokens": [51232, 8535, 510, 19817, 3337, 2361, 420, 19817, 3337, 36155, 426, 45196, 281, 976, 291, 257, 2020, 337, 341, 412, 51500], "temperature": 0.0, "avg_logprob": -0.06785366468340437, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.005056384019553661}, {"id": 325, "seek": 191156, "start": 1934.28, "end": 1938.6799999999998, "text": " the top here I have a standard search box and I've put in a very complicated question indeed", "tokens": [51500, 264, 1192, 510, 286, 362, 257, 3832, 3164, 2424, 293, 286, 600, 829, 294, 257, 588, 6179, 1168, 6451, 51720], "temperature": 0.0, "avg_logprob": -0.06785366468340437, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.005056384019553661}, {"id": 326, "seek": 193868, "start": 1939.4, "end": 1944.44, "text": " the first step in this approach is familiar from the LLMs for everything one we're going to encode", "tokens": [50400, 264, 700, 1823, 294, 341, 3109, 307, 4963, 490, 264, 441, 43, 26386, 337, 1203, 472, 321, 434, 516, 281, 2058, 1429, 50652], "temperature": 0.0, "avg_logprob": -0.061081736428397045, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0030735109467059374}, {"id": 327, "seek": 193868, "start": 1944.44, "end": 1949.8, "text": " that query into a dense numerical representation capturing aspects of its form and meaning we", "tokens": [50652, 300, 14581, 666, 257, 18011, 29054, 10290, 23384, 7270, 295, 1080, 1254, 293, 3620, 321, 50920], "temperature": 0.0, "avg_logprob": -0.061081736428397045, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0030735109467059374}, {"id": 328, "seek": 193868, "start": 1949.8, "end": 1955.88, "text": " use a language model for that the next step is new though we are also going to use a language model", "tokens": [50920, 764, 257, 2856, 2316, 337, 300, 264, 958, 1823, 307, 777, 1673, 321, 366, 611, 516, 281, 764, 257, 2856, 2316, 51224], "temperature": 0.0, "avg_logprob": -0.061081736428397045, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0030735109467059374}, {"id": 329, "seek": 193868, "start": 1955.88, "end": 1961.24, "text": " maybe the same one we use for the query to process all of the documents in our document collection", "tokens": [51224, 1310, 264, 912, 472, 321, 764, 337, 264, 14581, 281, 1399, 439, 295, 264, 8512, 294, 527, 4166, 5765, 51492], "temperature": 0.0, "avg_logprob": -0.061081736428397045, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0030735109467059374}, {"id": 330, "seek": 193868, "start": 1961.88, "end": 1968.04, "text": " so each one has some kind of numerical deep learning representation now on the basis of", "tokens": [51524, 370, 1184, 472, 575, 512, 733, 295, 29054, 2452, 2539, 10290, 586, 322, 264, 5143, 295, 51832], "temperature": 0.0, "avg_logprob": -0.061081736428397045, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0030735109467059374}, {"id": 331, "seek": 196804, "start": 1968.04, "end": 1973.08, "text": " these represent representations we can now score documents with respect to queries just like we", "tokens": [50364, 613, 2906, 33358, 321, 393, 586, 6175, 8512, 365, 3104, 281, 24109, 445, 411, 321, 50616], "temperature": 0.0, "avg_logprob": -0.05213799059969708, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.000282339722616598}, {"id": 332, "seek": 196804, "start": 1973.08, "end": 1979.6399999999999, "text": " would in the standard good old days of information retrieval so we can reproduce every aspect of", "tokens": [50616, 576, 294, 264, 3832, 665, 1331, 1708, 295, 1589, 19817, 3337, 370, 321, 393, 29501, 633, 4171, 295, 50944], "temperature": 0.0, "avg_logprob": -0.05213799059969708, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.000282339722616598}, {"id": 333, "seek": 196804, "start": 1979.6399999999999, "end": 1985.1599999999999, "text": " that familiar experience if we want to we're just doing it now in this very rich semantic space", "tokens": [50944, 300, 4963, 1752, 498, 321, 528, 281, 321, 434, 445, 884, 309, 586, 294, 341, 588, 4593, 47982, 1901, 51220], "temperature": 0.0, "avg_logprob": -0.05213799059969708, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.000282339722616598}, {"id": 334, "seek": 196804, "start": 1985.96, "end": 1990.2, "text": " so we get some results back and we could offer those to the user as ranked results but we can", "tokens": [51260, 370, 321, 483, 512, 3542, 646, 293, 321, 727, 2626, 729, 281, 264, 4195, 382, 20197, 3542, 457, 321, 393, 51472], "temperature": 0.0, "avg_logprob": -0.05213799059969708, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.000282339722616598}, {"id": 335, "seek": 196804, "start": 1990.2, "end": 1996.84, "text": " also go further we can have another language model call it a reader or a generator slurp up", "tokens": [51472, 611, 352, 3052, 321, 393, 362, 1071, 2856, 2316, 818, 309, 257, 15149, 420, 257, 19265, 1061, 20130, 493, 51804], "temperature": 0.0, "avg_logprob": -0.05213799059969708, "compression_ratio": 1.781954887218045, "no_speech_prob": 0.000282339722616598}, {"id": 336, "seek": 199684, "start": 1996.84, "end": 2002.6799999999998, "text": " those retrieved passages and synthesize them into a single answer maybe meeting the user's", "tokens": [50364, 729, 19817, 937, 31589, 293, 26617, 1125, 552, 666, 257, 2167, 1867, 1310, 3440, 264, 4195, 311, 50656], "temperature": 0.0, "avg_logprob": -0.044036237320097364, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0023219308350235224}, {"id": 337, "seek": 199684, "start": 2002.6799999999998, "end": 2007.9599999999998, "text": " information need directly right so let's check in on how we're doing with respect to our goals", "tokens": [50656, 1589, 643, 3838, 558, 370, 718, 311, 1520, 294, 322, 577, 321, 434, 884, 365, 3104, 281, 527, 5493, 50920], "temperature": 0.0, "avg_logprob": -0.044036237320097364, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0023219308350235224}, {"id": 338, "seek": 199684, "start": 2007.9599999999998, "end": 2012.76, "text": " here first efficiency I won't have time to substantiate this today but these systems in", "tokens": [50920, 510, 700, 10493, 286, 1582, 380, 362, 565, 281, 4594, 11520, 473, 341, 965, 457, 613, 3652, 294, 51160], "temperature": 0.0, "avg_logprob": -0.044036237320097364, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0023219308350235224}, {"id": 339, "seek": 199684, "start": 2012.76, "end": 2018.04, "text": " terms of parameter counts can be much smaller than the integrated approach I mentioned before", "tokens": [51160, 2115, 295, 13075, 14893, 393, 312, 709, 4356, 813, 264, 10919, 3109, 286, 2835, 949, 51424], "temperature": 0.0, "avg_logprob": -0.044036237320097364, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0023219308350235224}, {"id": 340, "seek": 199684, "start": 2019.24, "end": 2025.1599999999999, "text": " we also have an easy path to update ability we have this index here so as pages change in our", "tokens": [51484, 321, 611, 362, 364, 1858, 3100, 281, 5623, 3485, 321, 362, 341, 8186, 510, 370, 382, 7183, 1319, 294, 527, 51780], "temperature": 0.0, "avg_logprob": -0.044036237320097364, "compression_ratio": 1.6464285714285714, "no_speech_prob": 0.0023219308350235224}, {"id": 341, "seek": 202516, "start": 2025.16, "end": 2031.16, "text": " document store we simply use our frozen language model to reprocess and re-represent them and we", "tokens": [50364, 4166, 3531, 321, 2935, 764, 527, 12496, 2856, 2316, 281, 35257, 780, 293, 319, 12, 19919, 11662, 552, 293, 321, 50664], "temperature": 0.0, "avg_logprob": -0.04032798246903853, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0004043052904307842}, {"id": 342, "seek": 202516, "start": 2031.16, "end": 2036.28, "text": " can have a pretty good guarantee at this point that information changes will be reflected in", "tokens": [50664, 393, 362, 257, 1238, 665, 10815, 412, 341, 935, 300, 1589, 2962, 486, 312, 15502, 294, 50920], "temperature": 0.0, "avg_logprob": -0.04032798246903853, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0004043052904307842}, {"id": 343, "seek": 202516, "start": 2036.28, "end": 2041.5600000000002, "text": " the retrieved results down here we're also naturally tracking provenance because we have", "tokens": [50920, 264, 19817, 937, 3542, 760, 510, 321, 434, 611, 8195, 11603, 12785, 719, 570, 321, 362, 51184], "temperature": 0.0, "avg_logprob": -0.04032798246903853, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0004043052904307842}, {"id": 344, "seek": 202516, "start": 2041.5600000000002, "end": 2046.1200000000001, "text": " all these documents and they're used to deliver the results and we can have that carry through", "tokens": [51184, 439, 613, 8512, 293, 436, 434, 1143, 281, 4239, 264, 3542, 293, 321, 393, 362, 300, 3985, 807, 51412], "temperature": 0.0, "avg_logprob": -0.04032798246903853, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0004043052904307842}, {"id": 345, "seek": 202516, "start": 2046.1200000000001, "end": 2052.36, "text": " into the generation so we've kept that contract with the user these models are incredibly effective", "tokens": [51412, 666, 264, 5125, 370, 321, 600, 4305, 300, 4364, 365, 264, 4195, 613, 5245, 366, 6252, 4942, 51724], "temperature": 0.0, "avg_logprob": -0.04032798246903853, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0004043052904307842}, {"id": 346, "seek": 205236, "start": 2052.36, "end": 2058.04, "text": " across lots of literature we're seeing that retrieval augmented approaches are just superior", "tokens": [50364, 2108, 3195, 295, 10394, 321, 434, 2577, 300, 19817, 3337, 36155, 11587, 366, 445, 13028, 50648], "temperature": 0.0, "avg_logprob": -0.07272877693176269, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0010982871754094958}, {"id": 347, "seek": 205236, "start": 2058.04, "end": 2064.2000000000003, "text": " to the fully integrated llms for everything one and we've retained the benefit of llms for everything", "tokens": [50648, 281, 264, 4498, 10919, 287, 75, 2592, 337, 1203, 472, 293, 321, 600, 33438, 264, 5121, 295, 287, 75, 2592, 337, 1203, 50956], "temperature": 0.0, "avg_logprob": -0.07272877693176269, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0010982871754094958}, {"id": 348, "seek": 205236, "start": 2064.2000000000003, "end": 2069.8, "text": " because we have this model down here the reader generator that can synthesize information into", "tokens": [50956, 570, 321, 362, 341, 2316, 760, 510, 264, 15149, 19265, 300, 393, 26617, 1125, 1589, 666, 51236], "temperature": 0.0, "avg_logprob": -0.07272877693176269, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0010982871754094958}, {"id": 349, "seek": 205236, "start": 2069.8, "end": 2077.88, "text": " answers that meet the information need directly so that's my fundamental pitch now again things are", "tokens": [51236, 6338, 300, 1677, 264, 1589, 643, 3838, 370, 300, 311, 452, 8088, 7293, 586, 797, 721, 366, 51640], "temperature": 0.0, "avg_logprob": -0.07272877693176269, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.0010982871754094958}, {"id": 350, "seek": 207788, "start": 2077.88, "end": 2084.28, "text": " changing fast and even the approach to designing these systems is also changing really fast so in", "tokens": [50364, 4473, 2370, 293, 754, 264, 3109, 281, 14685, 613, 3652, 307, 611, 4473, 534, 2370, 370, 294, 50684], "temperature": 0.0, "avg_logprob": -0.052028656005859375, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.04080735519528389}, {"id": 351, "seek": 207788, "start": 2084.28, "end": 2091.7200000000003, "text": " the in the previous era of 2020 we would have these pre-trained components like we have our index and", "tokens": [50684, 264, 294, 264, 3894, 4249, 295, 4808, 321, 576, 362, 613, 659, 12, 17227, 2001, 6677, 411, 321, 362, 527, 8186, 293, 51056], "temperature": 0.0, "avg_logprob": -0.052028656005859375, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.04080735519528389}, {"id": 352, "seek": 207788, "start": 2091.7200000000003, "end": 2097.0, "text": " our retriever maybe we have a language model like reader generator and you might have other", "tokens": [51056, 527, 19817, 331, 1310, 321, 362, 257, 2856, 2316, 411, 15149, 19265, 293, 291, 1062, 362, 661, 51320], "temperature": 0.0, "avg_logprob": -0.052028656005859375, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.04080735519528389}, {"id": 353, "seek": 207788, "start": 2097.0, "end": 2102.84, "text": " pre-trained components image processing and so forth so you have all these assets and the question is", "tokens": [51320, 659, 12, 17227, 2001, 6677, 3256, 9007, 293, 370, 5220, 370, 291, 362, 439, 613, 9769, 293, 264, 1168, 307, 51612], "temperature": 0.0, "avg_logprob": -0.052028656005859375, "compression_ratio": 1.7782805429864252, "no_speech_prob": 0.04080735519528389}, {"id": 354, "seek": 210284, "start": 2102.84, "end": 2108.6000000000004, "text": " how are you going to bring them together into an integrated solution the standard deep learning", "tokens": [50364, 577, 366, 291, 516, 281, 1565, 552, 1214, 666, 364, 10919, 3827, 264, 3832, 2452, 2539, 50652], "temperature": 0.0, "avg_logprob": -0.04380291251726048, "compression_ratio": 1.82421875, "no_speech_prob": 0.020942699164152145}, {"id": 355, "seek": 210284, "start": 2108.6000000000004, "end": 2114.84, "text": " answer to that question is to define a bunch of task specific parameters that are meant to tie", "tokens": [50652, 1867, 281, 300, 1168, 307, 281, 6964, 257, 3840, 295, 5633, 2685, 9834, 300, 366, 4140, 281, 7582, 50964], "temperature": 0.0, "avg_logprob": -0.04380291251726048, "compression_ratio": 1.82421875, "no_speech_prob": 0.020942699164152145}, {"id": 356, "seek": 210284, "start": 2114.84, "end": 2119.56, "text": " together all those components and then you learn those parameters with respect to some task", "tokens": [50964, 1214, 439, 729, 6677, 293, 550, 291, 1466, 729, 9834, 365, 3104, 281, 512, 5633, 51200], "temperature": 0.0, "avg_logprob": -0.04380291251726048, "compression_ratio": 1.82421875, "no_speech_prob": 0.020942699164152145}, {"id": 357, "seek": 210284, "start": 2119.56, "end": 2125.7200000000003, "text": " and you hope that that has kind of created an effective integrated system that's the modular", "tokens": [51200, 293, 291, 1454, 300, 300, 575, 733, 295, 2942, 364, 4942, 10919, 1185, 300, 311, 264, 31111, 51508], "temperature": 0.0, "avg_logprob": -0.04380291251726048, "compression_ratio": 1.82421875, "no_speech_prob": 0.020942699164152145}, {"id": 358, "seek": 210284, "start": 2125.7200000000003, "end": 2131.96, "text": " vision of deep learning the truth in practice is that even for very experienced researchers", "tokens": [51508, 5201, 295, 2452, 2539, 264, 3494, 294, 3124, 307, 300, 754, 337, 588, 6751, 10309, 51820], "temperature": 0.0, "avg_logprob": -0.04380291251726048, "compression_ratio": 1.82421875, "no_speech_prob": 0.020942699164152145}, {"id": 359, "seek": 213196, "start": 2131.96, "end": 2138.68, "text": " and system designers this can often go really wrong and debugging these systems and figuring out", "tokens": [50364, 293, 1185, 16196, 341, 393, 2049, 352, 534, 2085, 293, 45592, 613, 3652, 293, 15213, 484, 50700], "temperature": 0.0, "avg_logprob": -0.06379418481479991, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0008825979894027114}, {"id": 360, "seek": 213196, "start": 2138.68, "end": 2144.44, "text": " how to improve them can be very difficult because they are so opaque and the scale is so large", "tokens": [50700, 577, 281, 3470, 552, 393, 312, 588, 2252, 570, 436, 366, 370, 42687, 293, 264, 4373, 307, 370, 2416, 50988], "temperature": 0.0, "avg_logprob": -0.06379418481479991, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0008825979894027114}, {"id": 361, "seek": 213196, "start": 2146.52, "end": 2152.68, "text": " but maybe we're moving out of an era in which we have to do this at all so this will bring us back", "tokens": [51092, 457, 1310, 321, 434, 2684, 484, 295, 364, 4249, 294, 597, 321, 362, 281, 360, 341, 412, 439, 370, 341, 486, 1565, 505, 646, 51400], "temperature": 0.0, "avg_logprob": -0.06379418481479991, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0008825979894027114}, {"id": 362, "seek": 213196, "start": 2152.68, "end": 2159.2400000000002, "text": " to in-context learning the fundamental insight here is that many of these models can in principle", "tokens": [51400, 281, 294, 12, 9000, 3828, 2539, 264, 8088, 11269, 510, 307, 300, 867, 295, 613, 5245, 393, 294, 8665, 51728], "temperature": 0.0, "avg_logprob": -0.06379418481479991, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0008825979894027114}, {"id": 363, "seek": 215924, "start": 2159.24, "end": 2167.4799999999996, "text": " communicate in natural language right so a retriever is abstractly just a device for pulling in text", "tokens": [50364, 7890, 294, 3303, 2856, 558, 370, 257, 19817, 331, 307, 12649, 356, 445, 257, 4302, 337, 8407, 294, 2487, 50776], "temperature": 0.0, "avg_logprob": -0.0732451933848707, "compression_ratio": 2.0594594594594593, "no_speech_prob": 0.023302586749196053}, {"id": 364, "seek": 215924, "start": 2167.4799999999996, "end": 2174.2, "text": " and producing text with scores and a language model is also a device for pulling in text and", "tokens": [50776, 293, 10501, 2487, 365, 13444, 293, 257, 2856, 2316, 307, 611, 257, 4302, 337, 8407, 294, 2487, 293, 51112], "temperature": 0.0, "avg_logprob": -0.0732451933848707, "compression_ratio": 2.0594594594594593, "no_speech_prob": 0.023302586749196053}, {"id": 365, "seek": 215924, "start": 2174.2, "end": 2180.6, "text": " producing text with scores and we have already seen in my basic picture of retrieval augmented", "tokens": [51112, 10501, 2487, 365, 13444, 293, 321, 362, 1217, 1612, 294, 452, 3875, 3036, 295, 19817, 3337, 36155, 51432], "temperature": 0.0, "avg_logprob": -0.0732451933848707, "compression_ratio": 2.0594594594594593, "no_speech_prob": 0.023302586749196053}, {"id": 366, "seek": 215924, "start": 2180.6, "end": 2185.3199999999997, "text": " approaches that we could have the retriever communicate with the language model via retrieve", "tokens": [51432, 11587, 300, 321, 727, 362, 264, 19817, 331, 7890, 365, 264, 2856, 2316, 5766, 30254, 51668], "temperature": 0.0, "avg_logprob": -0.0732451933848707, "compression_ratio": 2.0594594594594593, "no_speech_prob": 0.023302586749196053}, {"id": 367, "seek": 218532, "start": 2185.32, "end": 2191.0, "text": " results well what if we just allow that to go in both directions now we've got a system", "tokens": [50364, 3542, 731, 437, 498, 321, 445, 2089, 300, 281, 352, 294, 1293, 11095, 586, 321, 600, 658, 257, 1185, 50648], "temperature": 0.0, "avg_logprob": -0.058819246764230254, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.02000340446829796}, {"id": 368, "seek": 218532, "start": 2191.0, "end": 2196.84, "text": " that is essentially constructed by prompts that help these models do message passing between them", "tokens": [50648, 300, 307, 4476, 17083, 538, 41095, 300, 854, 613, 5245, 360, 3636, 8437, 1296, 552, 50940], "temperature": 0.0, "avg_logprob": -0.058819246764230254, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.02000340446829796}, {"id": 369, "seek": 218532, "start": 2196.84, "end": 2202.6800000000003, "text": " in potentially very complicated ways an entirely new approach to system design that I think is going", "tokens": [50940, 294, 7263, 588, 6179, 2098, 364, 7696, 777, 3109, 281, 1185, 1715, 300, 286, 519, 307, 516, 51232], "temperature": 0.0, "avg_logprob": -0.058819246764230254, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.02000340446829796}, {"id": 370, "seek": 218532, "start": 2202.6800000000003, "end": 2208.2000000000003, "text": " to have an incredible democratizing effect on who designs these systems and what they're for", "tokens": [51232, 281, 362, 364, 4651, 37221, 3319, 1802, 322, 567, 11347, 613, 3652, 293, 437, 436, 434, 337, 51508], "temperature": 0.0, "avg_logprob": -0.058819246764230254, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.02000340446829796}, {"id": 371, "seek": 218532, "start": 2209.0, "end": 2215.1600000000003, "text": " let me give you a deep sense for just how wide open the design space is here again to give you", "tokens": [51548, 718, 385, 976, 291, 257, 2452, 2020, 337, 445, 577, 4874, 1269, 264, 1715, 1901, 307, 510, 797, 281, 976, 291, 51856], "temperature": 0.0, "avg_logprob": -0.058819246764230254, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.02000340446829796}, {"id": 372, "seek": 221516, "start": 2215.24, "end": 2221.7999999999997, "text": " a sense for how much of this research is still left to be done even in this golden era let's", "tokens": [50368, 257, 2020, 337, 577, 709, 295, 341, 2132, 307, 920, 1411, 281, 312, 1096, 754, 294, 341, 9729, 4249, 718, 311, 50696], "temperature": 0.0, "avg_logprob": -0.04166128852150657, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008036161307245493}, {"id": 373, "seek": 221516, "start": 2221.7999999999997, "end": 2227.3199999999997, "text": " imagine a search context the question is what course to take what we're going to do in this new", "tokens": [50696, 3811, 257, 3164, 4319, 264, 1168, 307, 437, 1164, 281, 747, 437, 321, 434, 516, 281, 360, 294, 341, 777, 50972], "temperature": 0.0, "avg_logprob": -0.04166128852150657, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008036161307245493}, {"id": 374, "seek": 221516, "start": 2227.3199999999997, "end": 2234.52, "text": " mode is begin a prompt that contains that question just as before and now what we can do next is", "tokens": [50972, 4391, 307, 1841, 257, 12391, 300, 8306, 300, 1168, 445, 382, 949, 293, 586, 437, 321, 393, 360, 958, 307, 51332], "temperature": 0.0, "avg_logprob": -0.04166128852150657, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008036161307245493}, {"id": 375, "seek": 221516, "start": 2234.52, "end": 2239.72, "text": " retrieve a context passage that'll be like the retrieval augmented approach that I showed you", "tokens": [51332, 30254, 257, 4319, 11497, 300, 603, 312, 411, 264, 19817, 3337, 36155, 3109, 300, 286, 4712, 291, 51592], "temperature": 0.0, "avg_logprob": -0.04166128852150657, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008036161307245493}, {"id": 376, "seek": 221516, "start": 2239.72, "end": 2244.7599999999998, "text": " at the start of this section right you could just use our retriever for that but there's more", "tokens": [51592, 412, 264, 722, 295, 341, 3541, 558, 291, 727, 445, 764, 527, 19817, 331, 337, 300, 457, 456, 311, 544, 51844], "temperature": 0.0, "avg_logprob": -0.04166128852150657, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008036161307245493}, {"id": 377, "seek": 224476, "start": 2244.76, "end": 2249.0, "text": " that could be done what about demonstrations let's imagine that we have a little train set", "tokens": [50364, 300, 727, 312, 1096, 437, 466, 34714, 718, 311, 3811, 300, 321, 362, 257, 707, 3847, 992, 50576], "temperature": 0.0, "avg_logprob": -0.046405302029903804, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012060042936354876}, {"id": 378, "seek": 224476, "start": 2249.0, "end": 2254.44, "text": " of qa pairs that kind of demonstrate for our system what the intended behavior is well we can add", "tokens": [50576, 295, 9505, 64, 15494, 300, 733, 295, 11698, 337, 527, 1185, 437, 264, 10226, 5223, 307, 731, 321, 393, 909, 50848], "temperature": 0.0, "avg_logprob": -0.046405302029903804, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012060042936354876}, {"id": 379, "seek": 224476, "start": 2254.44, "end": 2259.32, "text": " those into the prompt and now we're giving the system a lot of few shot guidance about how to", "tokens": [50848, 729, 666, 264, 12391, 293, 586, 321, 434, 2902, 264, 1185, 257, 688, 295, 1326, 3347, 10056, 466, 577, 281, 51092], "temperature": 0.0, "avg_logprob": -0.046405302029903804, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012060042936354876}, {"id": 380, "seek": 224476, "start": 2259.32, "end": 2266.28, "text": " learn in context right but that's also just the beginning I might have sampled these training", "tokens": [51092, 1466, 294, 4319, 558, 457, 300, 311, 611, 445, 264, 2863, 286, 1062, 362, 3247, 15551, 613, 3097, 51440], "temperature": 0.0, "avg_logprob": -0.046405302029903804, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012060042936354876}, {"id": 381, "seek": 224476, "start": 2266.28, "end": 2272.84, "text": " examples randomly for my train set but I have a retriever remember and so what I could do instead", "tokens": [51440, 5110, 16979, 337, 452, 3847, 992, 457, 286, 362, 257, 19817, 331, 1604, 293, 370, 437, 286, 727, 360, 2602, 51768], "temperature": 0.0, "avg_logprob": -0.046405302029903804, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0012060042936354876}, {"id": 382, "seek": 227284, "start": 2272.84, "end": 2278.6800000000003, "text": " is find the demonstrations that are the most similar to the user's question and put those", "tokens": [50364, 307, 915, 264, 34714, 300, 366, 264, 881, 2531, 281, 264, 4195, 311, 1168, 293, 829, 729, 50656], "temperature": 0.0, "avg_logprob": -0.03569738147328201, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0012638899497687817}, {"id": 383, "seek": 227284, "start": 2278.6800000000003, "end": 2284.04, "text": " in my prompt with the expectation that that will help it understand kind of topical coherence and", "tokens": [50656, 294, 452, 12391, 365, 264, 14334, 300, 300, 486, 854, 309, 1223, 733, 295, 1192, 804, 26528, 655, 293, 50924], "temperature": 0.0, "avg_logprob": -0.03569738147328201, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0012638899497687817}, {"id": 384, "seek": 227284, "start": 2284.04, "end": 2290.28, "text": " lead to better results but I could go further right I could use my retriever again to find", "tokens": [50924, 1477, 281, 1101, 3542, 457, 286, 727, 352, 3052, 558, 286, 727, 764, 452, 19817, 331, 797, 281, 915, 51236], "temperature": 0.0, "avg_logprob": -0.03569738147328201, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0012638899497687817}, {"id": 385, "seek": 227284, "start": 2290.28, "end": 2295.8, "text": " relevant context passages for each one of those demonstrations to further help it figure out", "tokens": [51236, 7340, 4319, 31589, 337, 1184, 472, 295, 729, 34714, 281, 3052, 854, 309, 2573, 484, 51512], "temperature": 0.0, "avg_logprob": -0.03569738147328201, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0012638899497687817}, {"id": 386, "seek": 227284, "start": 2295.8, "end": 2301.7200000000003, "text": " how to reason in terms of evidence and that also opens up a huge design space we could do what we", "tokens": [51512, 577, 281, 1778, 294, 2115, 295, 4467, 293, 300, 611, 9870, 493, 257, 2603, 1715, 1901, 321, 727, 360, 437, 321, 51808], "temperature": 0.0, "avg_logprob": -0.03569738147328201, "compression_ratio": 1.7969348659003832, "no_speech_prob": 0.0012638899497687817}, {"id": 387, "seek": 230172, "start": 2301.72, "end": 2306.6, "text": " call hindsight retrieval where for each one of these we're using both the question and the answer", "tokens": [50364, 818, 44357, 19817, 3337, 689, 337, 1184, 472, 295, 613, 321, 434, 1228, 1293, 264, 1168, 293, 264, 1867, 50608], "temperature": 0.0, "avg_logprob": -0.024781068166097004, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.001753872144035995}, {"id": 388, "seek": 230172, "start": 2306.6, "end": 2313.0, "text": " to find relevant context passages to really give you integrated informational packets that the model", "tokens": [50608, 281, 915, 7340, 4319, 31589, 281, 534, 976, 291, 10919, 49391, 30364, 300, 264, 2316, 50928], "temperature": 0.0, "avg_logprob": -0.024781068166097004, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.001753872144035995}, {"id": 389, "seek": 230172, "start": 2313.0, "end": 2318.2, "text": " can benefit from and there's lots more that we could do with these demonstrations you're probably", "tokens": [50928, 393, 5121, 490, 293, 456, 311, 3195, 544, 300, 321, 727, 360, 365, 613, 34714, 291, 434, 1391, 51188], "temperature": 0.0, "avg_logprob": -0.024781068166097004, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.001753872144035995}, {"id": 390, "seek": 230172, "start": 2318.2, "end": 2324.12, "text": " starting to see it right we could do some rewriting and so forth really makes sophisticated use of", "tokens": [51188, 2891, 281, 536, 309, 558, 321, 727, 360, 512, 319, 19868, 293, 370, 5220, 534, 1669, 16950, 764, 295, 51484], "temperature": 0.0, "avg_logprob": -0.024781068166097004, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.001753872144035995}, {"id": 391, "seek": 230172, "start": 2324.12, "end": 2330.04, "text": " the retriever and the language model interwoven we could also think about how we selected this", "tokens": [51484, 264, 19817, 331, 293, 264, 2856, 2316, 728, 6120, 553, 321, 727, 611, 519, 466, 577, 321, 8209, 341, 51780], "temperature": 0.0, "avg_logprob": -0.024781068166097004, "compression_ratio": 1.7689530685920578, "no_speech_prob": 0.001753872144035995}, {"id": 392, "seek": 233004, "start": 2330.04, "end": 2336.44, "text": " background passage I was assuming that we would just retrieve the most relevant passage according", "tokens": [50364, 3678, 11497, 286, 390, 11926, 300, 321, 576, 445, 30254, 264, 881, 7340, 11497, 4650, 50684], "temperature": 0.0, "avg_logprob": -0.028164244189704816, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.00136621855199337}, {"id": 393, "seek": 233004, "start": 2336.44, "end": 2342.68, "text": " to our question but we could also think about rewriting the user's query in terms of the", "tokens": [50684, 281, 527, 1168, 457, 321, 727, 611, 519, 466, 319, 19868, 264, 4195, 311, 14581, 294, 2115, 295, 264, 50996], "temperature": 0.0, "avg_logprob": -0.028164244189704816, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.00136621855199337}, {"id": 394, "seek": 233004, "start": 2342.68, "end": 2347.48, "text": " demonstrations that we could construct it to get a new query that will help the model that's", "tokens": [50996, 34714, 300, 321, 727, 7690, 309, 281, 483, 257, 777, 14581, 300, 486, 854, 264, 2316, 300, 311, 51236], "temperature": 0.0, "avg_logprob": -0.028164244189704816, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.00136621855199337}, {"id": 395, "seek": 233004, "start": 2347.48, "end": 2352.68, "text": " especially powerful if you have a kind of interactional mode where the demonstrations are actually", "tokens": [51236, 2318, 4005, 498, 291, 362, 257, 733, 295, 4648, 1966, 4391, 689, 264, 34714, 366, 767, 51496], "temperature": 0.0, "avg_logprob": -0.028164244189704816, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.00136621855199337}, {"id": 396, "seek": 233004, "start": 2352.68, "end": 2358.52, "text": " part of like a dialogue history or something like that and then finally we could turn our", "tokens": [51496, 644, 295, 411, 257, 10221, 2503, 420, 746, 411, 300, 293, 550, 2721, 321, 727, 1261, 527, 51788], "temperature": 0.0, "avg_logprob": -0.028164244189704816, "compression_ratio": 1.8352941176470587, "no_speech_prob": 0.00136621855199337}, {"id": 397, "seek": 235852, "start": 2358.52, "end": 2363.16, "text": " attention to how we're actually generating the answer I was assuming we would take the top", "tokens": [50364, 3202, 281, 577, 321, 434, 767, 17746, 264, 1867, 286, 390, 11926, 321, 576, 747, 264, 1192, 50596], "temperature": 0.0, "avg_logprob": -0.029073563714822132, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.005907925311475992}, {"id": 398, "seek": 235852, "start": 2363.16, "end": 2367.96, "text": " generation from the language model but we could do much more we could filter its generations", "tokens": [50596, 5125, 490, 264, 2856, 2316, 457, 321, 727, 360, 709, 544, 321, 727, 6608, 1080, 10593, 50836], "temperature": 0.0, "avg_logprob": -0.029073563714822132, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.005907925311475992}, {"id": 399, "seek": 235852, "start": 2367.96, "end": 2373.8, "text": " to just those that match a substring of the passage reproducing some of the old mode of", "tokens": [50836, 281, 445, 729, 300, 2995, 257, 4594, 2937, 295, 264, 11497, 11408, 2175, 512, 295, 264, 1331, 4391, 295, 51128], "temperature": 0.0, "avg_logprob": -0.029073563714822132, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.005907925311475992}, {"id": 400, "seek": 235852, "start": 2373.8, "end": 2378.92, "text": " question answering but now in this completely open formulation that can be incredibly powerful", "tokens": [51128, 1168, 13430, 457, 586, 294, 341, 2584, 1269, 37642, 300, 393, 312, 6252, 4005, 51384], "temperature": 0.0, "avg_logprob": -0.029073563714822132, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.005907925311475992}, {"id": 401, "seek": 235852, "start": 2378.92, "end": 2384.6, "text": " if you know your model can retrieve good background passages here those are two simple steps you could", "tokens": [51384, 498, 291, 458, 428, 2316, 393, 30254, 665, 3678, 31589, 510, 729, 366, 732, 2199, 4439, 291, 727, 51668], "temperature": 0.0, "avg_logprob": -0.029073563714822132, "compression_ratio": 1.7832699619771863, "no_speech_prob": 0.005907925311475992}, {"id": 402, "seek": 238460, "start": 2384.6, "end": 2391.64, "text": " also go all the way to the other extreme and use the full retrieval augmented generation or rag model", "tokens": [50364, 611, 352, 439, 264, 636, 281, 264, 661, 8084, 293, 764, 264, 1577, 19817, 3337, 36155, 5125, 420, 17539, 2316, 50716], "temperature": 0.0, "avg_logprob": -0.06310961619917169, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.003822512924671173}, {"id": 403, "seek": 238460, "start": 2391.64, "end": 2396.8399999999997, "text": " which is essentially creates a full probability model that allows us to marginalize out the", "tokens": [50716, 597, 307, 4476, 7829, 257, 1577, 8482, 2316, 300, 4045, 505, 281, 16885, 1125, 484, 264, 50976], "temperature": 0.0, "avg_logprob": -0.06310961619917169, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.003822512924671173}, {"id": 404, "seek": 238460, "start": 2396.8399999999997, "end": 2403.4, "text": " contribution of passages that can be incredibly powerful in terms of making maximal use of the", "tokens": [50976, 13150, 295, 31589, 300, 393, 312, 6252, 4005, 294, 2115, 295, 1455, 49336, 764, 295, 264, 51304], "temperature": 0.0, "avg_logprob": -0.06310961619917169, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.003822512924671173}, {"id": 405, "seek": 238460, "start": 2403.4, "end": 2411.08, "text": " capacity of this model to generate text conditional on all the work that we did up here I hope that's", "tokens": [51304, 6042, 295, 341, 2316, 281, 8460, 2487, 27708, 322, 439, 264, 589, 300, 321, 630, 493, 510, 286, 1454, 300, 311, 51688], "temperature": 0.0, "avg_logprob": -0.06310961619917169, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.003822512924671173}, {"id": 406, "seek": 241108, "start": 2411.08, "end": 2416.36, "text": " giving you a sense for just how much can happen here what we're starting to see I think is that", "tokens": [50364, 2902, 291, 257, 2020, 337, 445, 577, 709, 393, 1051, 510, 437, 321, 434, 2891, 281, 536, 286, 519, 307, 300, 50628], "temperature": 0.0, "avg_logprob": -0.04751181027975427, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.020949000492691994}, {"id": 407, "seek": 241108, "start": 2416.36, "end": 2422.04, "text": " there is a new programming mode emerging it's a programming mode that involves using these large", "tokens": [50628, 456, 307, 257, 777, 9410, 4391, 14989, 309, 311, 257, 9410, 4391, 300, 11626, 1228, 613, 2416, 50912], "temperature": 0.0, "avg_logprob": -0.04751181027975427, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.020949000492691994}, {"id": 408, "seek": 241108, "start": 2422.04, "end": 2429.72, "text": " pre-trained components to design in code prompts that are essentially full AI systems that are", "tokens": [50912, 659, 12, 17227, 2001, 6677, 281, 1715, 294, 3089, 41095, 300, 366, 4476, 1577, 7318, 3652, 300, 366, 51296], "temperature": 0.0, "avg_logprob": -0.04751181027975427, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.020949000492691994}, {"id": 409, "seek": 241108, "start": 2429.72, "end": 2435.72, "text": " entirely about message passing between these frozen components we have a new paper out that's called", "tokens": [51296, 7696, 466, 3636, 8437, 1296, 613, 12496, 6677, 321, 362, 257, 777, 3035, 484, 300, 311, 1219, 51596], "temperature": 0.0, "avg_logprob": -0.04751181027975427, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.020949000492691994}, {"id": 410, "seek": 243572, "start": 2435.7999999999997, "end": 2440.8399999999997, "text": " demonstrate search predictor dsp this is a lightweight programming framework for doing", "tokens": [50368, 11698, 3164, 6069, 284, 274, 4952, 341, 307, 257, 22052, 9410, 8388, 337, 884, 50620], "temperature": 0.0, "avg_logprob": -0.059026033228093924, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.20165292918682098}, {"id": 411, "seek": 243572, "start": 2440.8399999999997, "end": 2446.6, "text": " exactly what I was just describing for you and one thing I want to call out is that our results", "tokens": [50620, 2293, 437, 286, 390, 445, 16141, 337, 291, 293, 472, 551, 286, 528, 281, 818, 484, 307, 300, 527, 3542, 50908], "temperature": 0.0, "avg_logprob": -0.059026033228093924, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.20165292918682098}, {"id": 412, "seek": 243572, "start": 2446.6, "end": 2453.9599999999996, "text": " are fantastic now you know we can pat ourselves on the back we have a very talented team and so it's", "tokens": [50908, 366, 5456, 586, 291, 458, 321, 393, 1947, 4175, 322, 264, 646, 321, 362, 257, 588, 13467, 1469, 293, 370, 309, 311, 51276], "temperature": 0.0, "avg_logprob": -0.059026033228093924, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.20165292918682098}, {"id": 413, "seek": 243572, "start": 2453.9599999999996, "end": 2459.08, "text": " no surprise the results are so good but I actually want to be upfront with you I think the real insight", "tokens": [51276, 572, 6365, 264, 3542, 366, 370, 665, 457, 286, 767, 528, 281, 312, 30264, 365, 291, 286, 519, 264, 957, 11269, 51532], "temperature": 0.0, "avg_logprob": -0.059026033228093924, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.20165292918682098}, {"id": 414, "seek": 243572, "start": 2459.08, "end": 2465.3199999999997, "text": " here is that it is such early days in terms of us figuring out how to construct these prompts how to", "tokens": [51532, 510, 307, 300, 309, 307, 1270, 2440, 1708, 294, 2115, 295, 505, 15213, 484, 577, 281, 7690, 613, 41095, 577, 281, 51844], "temperature": 0.0, "avg_logprob": -0.059026033228093924, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.20165292918682098}, {"id": 415, "seek": 246532, "start": 2465.32, "end": 2471.4, "text": " program these systems that we've only just begun to understand what's optimal we have explored only", "tokens": [50364, 1461, 613, 3652, 300, 321, 600, 787, 445, 16009, 281, 1223, 437, 311, 16252, 321, 362, 24016, 787, 50668], "temperature": 0.0, "avg_logprob": -0.05427307741982596, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.0037025620695203543}, {"id": 416, "seek": 246532, "start": 2471.4, "end": 2476.84, "text": " a tiny part of the space and everything we're doing is suboptimal and that's just the kind of conditions", "tokens": [50668, 257, 5870, 644, 295, 264, 1901, 293, 1203, 321, 434, 884, 307, 1422, 5747, 10650, 293, 300, 311, 445, 264, 733, 295, 4487, 50940], "temperature": 0.0, "avg_logprob": -0.05427307741982596, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.0037025620695203543}, {"id": 417, "seek": 246532, "start": 2476.84, "end": 2482.6000000000004, "text": " where you get these huge leap forwards leaps forward in performance on these tasks so I suspect", "tokens": [50940, 689, 291, 483, 613, 2603, 19438, 30126, 476, 2382, 2128, 294, 3389, 322, 613, 9608, 370, 286, 9091, 51228], "temperature": 0.0, "avg_logprob": -0.05427307741982596, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.0037025620695203543}, {"id": 418, "seek": 246532, "start": 2482.6000000000004, "end": 2488.28, "text": " that the bold row that we have here will not be long-lived given how much innovation is happening", "tokens": [51228, 300, 264, 11928, 5386, 300, 321, 362, 510, 486, 406, 312, 938, 12, 46554, 2212, 577, 709, 8504, 307, 2737, 51512], "temperature": 0.0, "avg_logprob": -0.05427307741982596, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.0037025620695203543}, {"id": 419, "seek": 246532, "start": 2488.28, "end": 2493.96, "text": " in this space and I want to make a pitch for our course here right so we have in this course", "tokens": [51512, 294, 341, 1901, 293, 286, 528, 281, 652, 257, 7293, 337, 527, 1164, 510, 558, 370, 321, 362, 294, 341, 1164, 51796], "temperature": 0.0, "avg_logprob": -0.05427307741982596, "compression_ratio": 1.7919708029197081, "no_speech_prob": 0.0037025620695203543}, {"id": 420, "seek": 249396, "start": 2494.6, "end": 2499.56, "text": " a bunch of assignment slash bake-offs and the way that works essentially is that you have an", "tokens": [50396, 257, 3840, 295, 15187, 17330, 16562, 12, 19231, 293, 264, 636, 300, 1985, 4476, 307, 300, 291, 362, 364, 50644], "temperature": 0.0, "avg_logprob": -0.08742791797042987, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.00572329293936491}, {"id": 421, "seek": 249396, "start": 2499.56, "end": 2505.64, "text": " assignment that helps you build some baselines and then work toward an original system which you", "tokens": [50644, 15187, 300, 3665, 291, 1322, 512, 987, 9173, 293, 550, 589, 7361, 364, 3380, 1185, 597, 291, 50948], "temperature": 0.0, "avg_logprob": -0.08742791797042987, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.00572329293936491}, {"id": 422, "seek": 249396, "start": 2505.64, "end": 2511.64, "text": " enter into a bake-off which is a kind of informal competition around data and modeling our newest", "tokens": [50948, 3242, 666, 257, 16562, 12, 4506, 597, 307, 257, 733, 295, 24342, 6211, 926, 1412, 293, 15983, 527, 17569, 51248], "temperature": 0.0, "avg_logprob": -0.08742791797042987, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.00572329293936491}, {"id": 423, "seek": 249396, "start": 2511.64, "end": 2517.16, "text": " of these is called few shot open qa with cobear retrieval it's a version of the problems that I've", "tokens": [51248, 295, 613, 307, 1219, 1326, 3347, 1269, 9505, 64, 365, 598, 26738, 19817, 3337, 309, 311, 257, 3037, 295, 264, 2740, 300, 286, 600, 51524], "temperature": 0.0, "avg_logprob": -0.08742791797042987, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.00572329293936491}, {"id": 424, "seek": 249396, "start": 2517.16, "end": 2522.36, "text": " just been describing for you this is a problem that could not even have been meaningfully posed", "tokens": [51524, 445, 668, 16141, 337, 291, 341, 307, 257, 1154, 300, 727, 406, 754, 362, 668, 3620, 2277, 31399, 51784], "temperature": 0.0, "avg_logprob": -0.08742791797042987, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.00572329293936491}, {"id": 425, "seek": 252236, "start": 2522.44, "end": 2529.0, "text": " five years ago and now we are seeing students doing incredible cutting-edge things in this mode", "tokens": [50368, 1732, 924, 2057, 293, 586, 321, 366, 2577, 1731, 884, 4651, 6492, 12, 12203, 721, 294, 341, 4391, 50696], "temperature": 0.0, "avg_logprob": -0.07790564394545281, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0031707952730357647}, {"id": 426, "seek": 252236, "start": 2529.0, "end": 2534.28, "text": " it's exactly what I was just describing for you and we're in the sort of moment where a student", "tokens": [50696, 309, 311, 2293, 437, 286, 390, 445, 16141, 337, 291, 293, 321, 434, 294, 264, 1333, 295, 1623, 689, 257, 3107, 50960], "temperature": 0.0, "avg_logprob": -0.07790564394545281, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0031707952730357647}, {"id": 427, "seek": 252236, "start": 2534.28, "end": 2539.1600000000003, "text": " project could lead to a paper that you know really leaves leads to state-of-the-art performance in", "tokens": [50960, 1716, 727, 1477, 281, 257, 3035, 300, 291, 458, 534, 5510, 6689, 281, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 294, 51204], "temperature": 0.0, "avg_logprob": -0.07790564394545281, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0031707952730357647}, {"id": 428, "seek": 252236, "start": 2539.1600000000003, "end": 2544.1200000000003, "text": " surprising ways again because there is just so much research that has to be done here", "tokens": [51204, 8830, 2098, 797, 570, 456, 307, 445, 370, 709, 2132, 300, 575, 281, 312, 1096, 510, 51452], "temperature": 0.0, "avg_logprob": -0.07790564394545281, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0031707952730357647}, {"id": 429, "seek": 254412, "start": 2544.52, "end": 2554.44, "text": " I'm running out of time what I think I'll do is just briefly call out again those important other", "tokens": [50384, 286, 478, 2614, 484, 295, 565, 437, 286, 519, 286, 603, 360, 307, 445, 10515, 818, 484, 797, 729, 1021, 661, 50880], "temperature": 0.0, "avg_logprob": -0.15080573641020675, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.043321385979652405}, {"id": 430, "seek": 254412, "start": 2554.44, "end": 2559.7999999999997, "text": " areas that I've given short drift to today but I think are just so important starting with data", "tokens": [50880, 3179, 300, 286, 600, 2212, 2099, 19699, 281, 965, 457, 286, 519, 366, 445, 370, 1021, 2891, 365, 1412, 51148], "temperature": 0.0, "avg_logprob": -0.15080573641020675, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.043321385979652405}, {"id": 431, "seek": 254412, "start": 2559.7999999999997, "end": 2567.0, "text": " sets I've been talking about system design and task performance but it is now and will always be the", "tokens": [51148, 6352, 286, 600, 668, 1417, 466, 1185, 1715, 293, 5633, 3389, 457, 309, 307, 586, 293, 486, 1009, 312, 264, 51508], "temperature": 0.0, "avg_logprob": -0.15080573641020675, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.043321385979652405}, {"id": 432, "seek": 254412, "start": 2567.0, "end": 2573.3199999999997, "text": " case that contributing you new benchmark data sets is basically the most important thing you can do", "tokens": [51508, 1389, 300, 19270, 291, 777, 18927, 1412, 6352, 307, 1936, 264, 881, 1021, 551, 291, 393, 360, 51824], "temperature": 0.0, "avg_logprob": -0.15080573641020675, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.043321385979652405}, {"id": 433, "seek": 257332, "start": 2573.4, "end": 2579.0800000000004, "text": " Jacques Cousteau said water and air the two essential fluids on which all life depends I would", "tokens": [50368, 42691, 26180, 2941, 1459, 848, 1281, 293, 1988, 264, 732, 7115, 33033, 322, 597, 439, 993, 5946, 286, 576, 50652], "temperature": 0.0, "avg_logprob": -0.07051295912667607, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.008833590894937515}, {"id": 434, "seek": 257332, "start": 2579.0800000000004, "end": 2588.28, "text": " extend that NLP our data sets are the resource on which all progress depends now Cousteau extended", "tokens": [50652, 10101, 300, 426, 45196, 527, 1412, 6352, 366, 264, 7684, 322, 597, 439, 4205, 5946, 586, 26180, 2941, 1459, 10913, 51112], "temperature": 0.0, "avg_logprob": -0.07051295912667607, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.008833590894937515}, {"id": 435, "seek": 257332, "start": 2588.28, "end": 2593.32, "text": " this with have become global garbage cans I am not that cynical about our data sets I think we've", "tokens": [51112, 341, 365, 362, 1813, 4338, 14150, 21835, 286, 669, 406, 300, 46345, 466, 527, 1412, 6352, 286, 519, 321, 600, 51364], "temperature": 0.0, "avg_logprob": -0.07051295912667607, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.008833590894937515}, {"id": 436, "seek": 257332, "start": 2593.32, "end": 2598.52, "text": " learned a lot about how to create effective data sets we're getting better at this but we need to", "tokens": [51364, 3264, 257, 688, 466, 577, 281, 1884, 4942, 1412, 6352, 321, 434, 1242, 1101, 412, 341, 457, 321, 643, 281, 51624], "temperature": 0.0, "avg_logprob": -0.07051295912667607, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.008833590894937515}, {"id": 437, "seek": 259852, "start": 2598.52, "end": 2604.28, "text": " watch out for this metaphorical pollution and we need always to be pushing our systems with", "tokens": [50364, 1159, 484, 337, 341, 19157, 804, 16727, 293, 321, 643, 1009, 281, 312, 7380, 527, 3652, 365, 50652], "temperature": 0.0, "avg_logprob": -0.04761222184422505, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.0062859938479959965}, {"id": 438, "seek": 259852, "start": 2604.28, "end": 2610.36, "text": " harder tasks that come closer to the human capabilities that we're actually actually trying to get them", "tokens": [50652, 6081, 9608, 300, 808, 4966, 281, 264, 1952, 10862, 300, 321, 434, 767, 767, 1382, 281, 483, 552, 50956], "temperature": 0.0, "avg_logprob": -0.04761222184422505, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.0062859938479959965}, {"id": 439, "seek": 259852, "start": 2610.36, "end": 2616.12, "text": " to achieve and without contributions of data sets we could be tricking ourselves when we think we're", "tokens": [50956, 281, 4584, 293, 1553, 15725, 295, 1412, 6352, 321, 727, 312, 4282, 278, 4175, 562, 321, 519, 321, 434, 51244], "temperature": 0.0, "avg_logprob": -0.04761222184422505, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.0062859938479959965}, {"id": 440, "seek": 259852, "start": 2616.12, "end": 2623.32, "text": " making a lot of progress the second thing that I wanted to call out relates to model explainability", "tokens": [51244, 1455, 257, 688, 295, 4205, 264, 1150, 551, 300, 286, 1415, 281, 818, 484, 16155, 281, 2316, 2903, 2310, 51604], "temperature": 0.0, "avg_logprob": -0.04761222184422505, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.0062859938479959965}, {"id": 441, "seek": 262332, "start": 2623.32, "end": 2629.0, "text": " you know we're in an era of incredible impact and that has rightly turned researchers to questions", "tokens": [50364, 291, 458, 321, 434, 294, 364, 4249, 295, 4651, 2712, 293, 300, 575, 32879, 3574, 10309, 281, 1651, 50648], "temperature": 0.0, "avg_logprob": -0.040684793676648824, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.23580431938171387}, {"id": 442, "seek": 262332, "start": 2629.0, "end": 2637.56, "text": " of system reliability safety trust approved use and pernicious social biases we have to get serious", "tokens": [50648, 295, 1185, 24550, 4514, 3361, 10826, 764, 293, 680, 77, 3784, 2093, 32152, 321, 362, 281, 483, 3156, 51076], "temperature": 0.0, "avg_logprob": -0.040684793676648824, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.23580431938171387}, {"id": 443, "seek": 262332, "start": 2637.56, "end": 2643.6400000000003, "text": " about all these issues if we're gonna responsibly have all of the impact that we're achieving at this", "tokens": [51076, 466, 439, 613, 2663, 498, 321, 434, 799, 2914, 3545, 362, 439, 295, 264, 2712, 300, 321, 434, 19626, 412, 341, 51380], "temperature": 0.0, "avg_logprob": -0.040684793676648824, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.23580431938171387}, {"id": 444, "seek": 262332, "start": 2643.6400000000003, "end": 2649.7200000000003, "text": " point all of these things are incredibly difficult because the systems we're talking about are these", "tokens": [51380, 935, 439, 295, 613, 721, 366, 6252, 2252, 570, 264, 3652, 321, 434, 1417, 466, 366, 613, 51684], "temperature": 0.0, "avg_logprob": -0.040684793676648824, "compression_ratio": 1.728448275862069, "no_speech_prob": 0.23580431938171387}, {"id": 445, "seek": 264972, "start": 2649.72, "end": 2655.3999999999996, "text": " enormous opaque impossible to understand analytically devices like this that are just", "tokens": [50364, 11322, 42687, 6243, 281, 1223, 10783, 984, 5759, 411, 341, 300, 366, 445, 50648], "temperature": 0.0, "avg_logprob": -0.03825689303247552, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005464321933686733}, {"id": 446, "seek": 264972, "start": 2655.3999999999996, "end": 2661.0, "text": " clouding our understanding of them and so to me that shines a light on the importance of", "tokens": [50648, 4588, 278, 527, 3701, 295, 552, 293, 370, 281, 385, 300, 28056, 257, 1442, 322, 264, 7379, 295, 50928], "temperature": 0.0, "avg_logprob": -0.03825689303247552, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005464321933686733}, {"id": 447, "seek": 264972, "start": 2661.0, "end": 2666.9199999999996, "text": " achieving analytic guarantees about our model behaviors that seems to me to be a prerequisite", "tokens": [50928, 19626, 40358, 32567, 466, 527, 2316, 15501, 300, 2544, 281, 385, 281, 312, 257, 38333, 34152, 51224], "temperature": 0.0, "avg_logprob": -0.03825689303247552, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005464321933686733}, {"id": 448, "seek": 264972, "start": 2666.9199999999996, "end": 2672.52, "text": " for getting serious about any one of these topics and the goal there in our terms is to achieve", "tokens": [51224, 337, 1242, 3156, 466, 604, 472, 295, 613, 8378, 293, 264, 3387, 456, 294, 527, 2115, 307, 281, 4584, 51504], "temperature": 0.0, "avg_logprob": -0.03825689303247552, "compression_ratio": 1.7416267942583732, "no_speech_prob": 0.005464321933686733}, {"id": 449, "seek": 267252, "start": 2673.24, "end": 2679.4, "text": " faithful human interpretable explanations of model behavior we have great coverage of these", "tokens": [50400, 17808, 1952, 7302, 712, 28708, 295, 2316, 5223, 321, 362, 869, 9645, 295, 613, 50708], "temperature": 0.0, "avg_logprob": -0.038284220107614175, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.10363421589136124}, {"id": 450, "seek": 267252, "start": 2679.4, "end": 2685.08, "text": " methods in the course hands-on materials screencasts and other things that will help you", "tokens": [50708, 7150, 294, 264, 1164, 2377, 12, 266, 5319, 2568, 3734, 82, 293, 661, 721, 300, 486, 854, 291, 50992], "temperature": 0.0, "avg_logprob": -0.038284220107614175, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.10363421589136124}, {"id": 451, "seek": 267252, "start": 2685.64, "end": 2691.24, "text": " participate in this research and also as a side effect write absolutely outstanding", "tokens": [51020, 8197, 294, 341, 2132, 293, 611, 382, 257, 1252, 1802, 2464, 3122, 14485, 51300], "temperature": 0.0, "avg_logprob": -0.038284220107614175, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.10363421589136124}, {"id": 452, "seek": 267252, "start": 2691.24, "end": 2698.36, "text": " discussion and analysis sections for your papers and the final thing I wanted to call out is just", "tokens": [51300, 5017, 293, 5215, 10863, 337, 428, 10577, 293, 264, 2572, 551, 286, 1415, 281, 818, 484, 307, 445, 51656], "temperature": 0.0, "avg_logprob": -0.038284220107614175, "compression_ratio": 1.6160714285714286, "no_speech_prob": 0.10363421589136124}, {"id": 453, "seek": 269836, "start": 2698.44, "end": 2705.8, "text": " that last mile problem fundamental advances in AI take us 95 percent of the way there but that last", "tokens": [50368, 300, 1036, 12620, 1154, 8088, 25297, 294, 7318, 747, 505, 13420, 3043, 295, 264, 636, 456, 457, 300, 1036, 50736], "temperature": 0.0, "avg_logprob": -0.07442995480128697, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.045963678508996964}, {"id": 454, "seek": 269836, "start": 2705.8, "end": 2712.28, "text": " five percent is every bit as difficult as the first 95 in my group we've been looking a lot at", "tokens": [50736, 1732, 3043, 307, 633, 857, 382, 2252, 382, 264, 700, 13420, 294, 452, 1594, 321, 600, 668, 1237, 257, 688, 412, 51060], "temperature": 0.0, "avg_logprob": -0.07442995480128697, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.045963678508996964}, {"id": 455, "seek": 269836, "start": 2712.28, "end": 2719.96, "text": " image accessibility this is an incredibly important societal problem because images are so central", "tokens": [51060, 3256, 15002, 341, 307, 364, 6252, 1021, 33472, 1154, 570, 5267, 366, 370, 5777, 51444], "temperature": 0.0, "avg_logprob": -0.07442995480128697, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.045963678508996964}, {"id": 456, "seek": 269836, "start": 2719.96, "end": 2726.44, "text": " to modern life across being on the web and in social media also in the news and in our scientific", "tokens": [51444, 281, 4363, 993, 2108, 885, 322, 264, 3670, 293, 294, 2093, 3021, 611, 294, 264, 2583, 293, 294, 527, 8134, 51768], "temperature": 0.0, "avg_logprob": -0.07442995480128697, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.045963678508996964}, {"id": 457, "seek": 272644, "start": 2726.44, "end": 2732.52, "text": " discourse and it's a sad fact about the current state of the world that almost none of these images", "tokens": [50364, 23938, 293, 309, 311, 257, 4227, 1186, 466, 264, 2190, 1785, 295, 264, 1002, 300, 1920, 6022, 295, 613, 5267, 50668], "temperature": 0.0, "avg_logprob": -0.05672605633735657, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.01687963865697384}, {"id": 458, "seek": 272644, "start": 2732.52, "end": 2738.84, "text": " are made non-visually accessible so blind and low vision users are basically unable to understand", "tokens": [50668, 366, 1027, 2107, 12, 4938, 671, 9515, 370, 6865, 293, 2295, 5201, 5022, 366, 1936, 11299, 281, 1223, 50984], "temperature": 0.0, "avg_logprob": -0.05672605633735657, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.01687963865697384}, {"id": 459, "seek": 272644, "start": 2738.84, "end": 2743.64, "text": " all this context and receive all of this information something has to change that", "tokens": [50984, 439, 341, 4319, 293, 4774, 439, 295, 341, 1589, 746, 575, 281, 1319, 300, 51224], "temperature": 0.0, "avg_logprob": -0.05672605633735657, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.01687963865697384}, {"id": 460, "seek": 272644, "start": 2744.76, "end": 2750.92, "text": " image-based text generation has become incredibly good over the last 10 years that's another story", "tokens": [51280, 3256, 12, 6032, 2487, 5125, 575, 1813, 6252, 665, 670, 264, 1036, 1266, 924, 300, 311, 1071, 1657, 51588], "temperature": 0.0, "avg_logprob": -0.05672605633735657, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.01687963865697384}, {"id": 461, "seek": 275092, "start": 2750.92, "end": 2757.48, "text": " of astounding progress but it has yet to take us to the point where we can actually write useful", "tokens": [50364, 295, 5357, 24625, 4205, 457, 309, 575, 1939, 281, 747, 505, 281, 264, 935, 689, 321, 393, 767, 2464, 4420, 50692], "temperature": 0.0, "avg_logprob": -0.04275597805200621, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.18208085000514984}, {"id": 462, "seek": 275092, "start": 2757.48, "end": 2763.32, "text": " descriptions of these images that would help a BLB user and that last bit is going to require", "tokens": [50692, 24406, 295, 613, 5267, 300, 576, 854, 257, 15132, 33, 4195, 293, 300, 1036, 857, 307, 516, 281, 3651, 50984], "temperature": 0.0, "avg_logprob": -0.04275597805200621, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.18208085000514984}, {"id": 463, "seek": 275092, "start": 2763.32, "end": 2771.0, "text": " HCI research linguistic research and fundamental advances in AI and by the way lots of astounding", "tokens": [50984, 389, 25240, 2132, 43002, 2132, 293, 8088, 25297, 294, 7318, 293, 538, 264, 636, 3195, 295, 5357, 24625, 51368], "temperature": 0.0, "avg_logprob": -0.04275597805200621, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.18208085000514984}, {"id": 464, "seek": 275092, "start": 2771.0, "end": 2777.88, "text": " new data sets and this is just one example of in the innumerable number of applied problems", "tokens": [51368, 777, 1412, 6352, 293, 341, 307, 445, 472, 1365, 295, 294, 264, 7714, 15583, 712, 1230, 295, 6456, 2740, 51712], "temperature": 0.0, "avg_logprob": -0.04275597805200621, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.18208085000514984}, {"id": 465, "seek": 277788, "start": 2777.88, "end": 2783.56, "text": " that fall into this mode and that can be very exciting for people who have domain expertise", "tokens": [50364, 300, 2100, 666, 341, 4391, 293, 300, 393, 312, 588, 4670, 337, 561, 567, 362, 9274, 11769, 50648], "temperature": 0.0, "avg_logprob": -0.054377698350226746, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.005816364660859108}, {"id": 466, "seek": 277788, "start": 2783.56, "end": 2792.6, "text": " that can help us close that final mile so let me wrap up here I don't want to have a standard", "tokens": [50648, 300, 393, 854, 505, 1998, 300, 2572, 12620, 370, 718, 385, 7019, 493, 510, 286, 500, 380, 528, 281, 362, 257, 3832, 51100], "temperature": 0.0, "avg_logprob": -0.054377698350226746, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.005816364660859108}, {"id": 467, "seek": 277788, "start": 2792.6, "end": 2798.6, "text": " conclusion I think it's fun to close with some predictions about the future and I have put this", "tokens": [51100, 10063, 286, 519, 309, 311, 1019, 281, 1998, 365, 512, 21264, 466, 264, 2027, 293, 286, 362, 829, 341, 51400], "temperature": 0.0, "avg_logprob": -0.054377698350226746, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.005816364660859108}, {"id": 468, "seek": 277788, "start": 2798.6, "end": 2803.8, "text": " under the heading of predictions for the text next 10 years or so although I'm about to retract", "tokens": [51400, 833, 264, 9864, 295, 21264, 337, 264, 2487, 958, 1266, 924, 420, 370, 4878, 286, 478, 466, 281, 41107, 51660], "temperature": 0.0, "avg_logprob": -0.054377698350226746, "compression_ratio": 1.6607929515418502, "no_speech_prob": 0.005816364660859108}, {"id": 469, "seek": 280380, "start": 2803.8, "end": 2810.28, "text": " that for reasons I will get to but here are the predictions first laggard industries that are rich", "tokens": [50364, 300, 337, 4112, 286, 486, 483, 281, 457, 510, 366, 264, 21264, 700, 8953, 36260, 13284, 300, 366, 4593, 50688], "temperature": 0.0, "avg_logprob": -0.05303743763973839, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.027133304625749588}, {"id": 470, "seek": 280380, "start": 2810.28, "end": 2816.04, "text": " in text data will be transformed in part by NLP technology and that's likely to happen from", "tokens": [50688, 294, 2487, 1412, 486, 312, 16894, 294, 644, 538, 426, 45196, 2899, 293, 300, 311, 3700, 281, 1051, 490, 50976], "temperature": 0.0, "avg_logprob": -0.05303743763973839, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.027133304625749588}, {"id": 471, "seek": 280380, "start": 2816.04, "end": 2822.36, "text": " some disruptive newcomers coming out of left field second prediction artificial assistants", "tokens": [50976, 512, 37865, 40014, 433, 1348, 484, 295, 1411, 2519, 1150, 17630, 11677, 34949, 51292], "temperature": 0.0, "avg_logprob": -0.05303743763973839, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.027133304625749588}, {"id": 472, "seek": 280380, "start": 2822.36, "end": 2827.2400000000002, "text": " will get dramatically better and become more ubiquitous with the side effect that you'll", "tokens": [51292, 486, 483, 17548, 1101, 293, 1813, 544, 43868, 39831, 365, 264, 1252, 1802, 300, 291, 603, 51536], "temperature": 0.0, "avg_logprob": -0.05303743763973839, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.027133304625749588}, {"id": 473, "seek": 280380, "start": 2827.2400000000002, "end": 2833.6400000000003, "text": " often be unsure in life whether this customer service representative is a person or an AI", "tokens": [51536, 2049, 312, 32486, 294, 993, 1968, 341, 5474, 2643, 12424, 307, 257, 954, 420, 364, 7318, 51856], "temperature": 0.0, "avg_logprob": -0.05303743763973839, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.027133304625749588}, {"id": 474, "seek": 283364, "start": 2833.64, "end": 2841.48, "text": " or some team combining the two many kinds of writing including student papers at universities", "tokens": [50364, 420, 512, 1469, 21928, 264, 732, 867, 3685, 295, 3579, 3009, 3107, 10577, 412, 11779, 50756], "temperature": 0.0, "avg_logprob": -0.04512625694274902, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0005025719874538481}, {"id": 475, "seek": 283364, "start": 2841.48, "end": 2846.44, "text": " will be done with AI writing assistants and this might be transparently true given how", "tokens": [50756, 486, 312, 1096, 365, 7318, 3579, 34949, 293, 341, 1062, 312, 7132, 6420, 2074, 2212, 577, 51004], "temperature": 0.0, "avg_logprob": -0.04512625694274902, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0005025719874538481}, {"id": 476, "seek": 283364, "start": 2846.44, "end": 2852.3599999999997, "text": " sophisticated autocomplete and other tools have gotten at this point and then finally", "tokens": [51004, 16950, 45833, 298, 17220, 293, 661, 3873, 362, 5768, 412, 341, 935, 293, 550, 2721, 51300], "temperature": 0.0, "avg_logprob": -0.04512625694274902, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0005025719874538481}, {"id": 477, "seek": 283364, "start": 2852.3599999999997, "end": 2857.56, "text": " the negative effects of NLP and of AI will be amplified along with the positives I'm thinking", "tokens": [51300, 264, 3671, 5065, 295, 426, 45196, 293, 295, 7318, 486, 312, 49237, 2051, 365, 264, 35127, 286, 478, 1953, 51560], "temperature": 0.0, "avg_logprob": -0.04512625694274902, "compression_ratio": 1.592920353982301, "no_speech_prob": 0.0005025719874538481}, {"id": 478, "seek": 285756, "start": 2857.56, "end": 2864.6, "text": " of things like disinformation spread market disruption systemic bias it's almost sure to", "tokens": [50364, 295, 721, 411, 717, 20941, 3974, 2142, 28751, 23789, 12577, 309, 311, 1920, 988, 281, 50716], "temperature": 0.0, "avg_logprob": -0.04303973186306837, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.2418256551027298}, {"id": 479, "seek": 285756, "start": 2864.6, "end": 2869.16, "text": " be the case if it hasn't already happened already that there will be some calamitous world event", "tokens": [50716, 312, 264, 1389, 498, 309, 6132, 380, 1217, 2011, 1217, 300, 456, 486, 312, 512, 43936, 39831, 1002, 2280, 50944], "temperature": 0.0, "avg_logprob": -0.04303973186306837, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.2418256551027298}, {"id": 480, "seek": 285756, "start": 2869.72, "end": 2876.04, "text": " that traces to the intentional or unintentional misuse of some AI technology that's in our future", "tokens": [50972, 300, 26076, 281, 264, 21935, 420, 45514, 304, 3346, 438, 295, 512, 7318, 2899, 300, 311, 294, 527, 2027, 51288], "temperature": 0.0, "avg_logprob": -0.04303973186306837, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.2418256551027298}, {"id": 481, "seek": 285756, "start": 2876.92, "end": 2880.92, "text": " so I think these are reasonable predictions and I'm curious for yours but I have to tell you", "tokens": [51332, 370, 286, 519, 613, 366, 10585, 21264, 293, 286, 478, 6369, 337, 6342, 457, 286, 362, 281, 980, 291, 51532], "temperature": 0.0, "avg_logprob": -0.04303973186306837, "compression_ratio": 1.6419213973799127, "no_speech_prob": 0.2418256551027298}, {"id": 482, "seek": 288092, "start": 2881.7200000000003, "end": 2889.7200000000003, "text": " that I made these predictions in 2020 two years ago with the expectation that they would be good", "tokens": [50404, 300, 286, 1027, 613, 21264, 294, 4808, 732, 924, 2057, 365, 264, 14334, 300, 436, 576, 312, 665, 50804], "temperature": 0.0, "avg_logprob": -0.0423338862026439, "compression_ratio": 1.7, "no_speech_prob": 0.4914236068725586}, {"id": 483, "seek": 288092, "start": 2889.7200000000003, "end": 2896.52, "text": " for 10 years but more than half of them probably have already come true two and three are definitely", "tokens": [50804, 337, 1266, 924, 457, 544, 813, 1922, 295, 552, 1391, 362, 1217, 808, 2074, 732, 293, 1045, 366, 2138, 51144], "temperature": 0.0, "avg_logprob": -0.0423338862026439, "compression_ratio": 1.7, "no_speech_prob": 0.4914236068725586}, {"id": 484, "seek": 288092, "start": 2896.52, "end": 2901.64, "text": " true about the world we live in and on the flip side I just failed to predict so many important", "tokens": [51144, 2074, 466, 264, 1002, 321, 1621, 294, 293, 322, 264, 7929, 1252, 286, 445, 7612, 281, 6069, 370, 867, 1021, 51400], "temperature": 0.0, "avg_logprob": -0.0423338862026439, "compression_ratio": 1.7, "no_speech_prob": 0.4914236068725586}, {"id": 485, "seek": 288092, "start": 2901.64, "end": 2906.36, "text": " things like the most prominent example is that I just failed to predict the progress we would see", "tokens": [51400, 721, 411, 264, 881, 17034, 1365, 307, 300, 286, 445, 7612, 281, 6069, 264, 4205, 321, 576, 536, 51636], "temperature": 0.0, "avg_logprob": -0.0423338862026439, "compression_ratio": 1.7, "no_speech_prob": 0.4914236068725586}, {"id": 486, "seek": 290636, "start": 2906.44, "end": 2913.4, "text": " in text image models like dolly two and and stable diffusion in fact I'll be honest with you I might", "tokens": [50368, 294, 2487, 3256, 5245, 411, 2722, 88, 732, 293, 293, 8351, 25242, 294, 1186, 286, 603, 312, 3245, 365, 291, 286, 1062, 50716], "temperature": 0.0, "avg_logprob": -0.08905168597617846, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.04595369100570679}, {"id": 487, "seek": 290636, "start": 2913.4, "end": 2918.36, "text": " have bet against them I thought that was an area that was going to languish for a long time and yet", "tokens": [50716, 362, 778, 1970, 552, 286, 1194, 300, 390, 364, 1859, 300, 390, 516, 281, 2510, 742, 337, 257, 938, 565, 293, 1939, 50964], "temperature": 0.0, "avg_logprob": -0.08905168597617846, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.04595369100570679}, {"id": 488, "seek": 290636, "start": 2918.36, "end": 2923.32, "text": " nonetheless seemingly out of nowhere we had this incredible set of advances and there are probably", "tokens": [50964, 26756, 18709, 484, 295, 11159, 321, 632, 341, 4651, 992, 295, 25297, 293, 456, 366, 1391, 51212], "temperature": 0.0, "avg_logprob": -0.08905168597617846, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.04595369100570679}, {"id": 489, "seek": 290636, "start": 2923.32, "end": 2930.6, "text": " lots of other areas where I would make similarly bad predictions so I said 10 years but I think", "tokens": [51212, 3195, 295, 661, 3179, 689, 286, 576, 652, 14138, 1578, 21264, 370, 286, 848, 1266, 924, 457, 286, 519, 51576], "temperature": 0.0, "avg_logprob": -0.08905168597617846, "compression_ratio": 1.6188524590163935, "no_speech_prob": 0.04595369100570679}, {"id": 490, "seek": 293060, "start": 2930.68, "end": 2937.3199999999997, "text": " my new rule is going to be that I'm going to predict only through 2024 at the very outside", "tokens": [50368, 452, 777, 4978, 307, 516, 281, 312, 300, 286, 478, 516, 281, 6069, 787, 807, 45237, 412, 264, 588, 2380, 50700], "temperature": 0.0, "avg_logprob": -0.04033672672578658, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.047320906072854996}, {"id": 491, "seek": 293060, "start": 2937.3199999999997, "end": 2943.96, "text": " because in 10 years the only thing I can say with confidence is that we will be in a radically", "tokens": [50700, 570, 294, 1266, 924, 264, 787, 551, 286, 393, 584, 365, 6687, 307, 300, 321, 486, 312, 294, 257, 35508, 51032], "temperature": 0.0, "avg_logprob": -0.04033672672578658, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.047320906072854996}, {"id": 492, "seek": 293060, "start": 2943.96, "end": 2949.48, "text": " different place from where we are now but what that place will be like is anyone's guess I'm", "tokens": [51032, 819, 1081, 490, 689, 321, 366, 586, 457, 437, 300, 1081, 486, 312, 411, 307, 2878, 311, 2041, 286, 478, 51308], "temperature": 0.0, "avg_logprob": -0.04033672672578658, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.047320906072854996}, {"id": 493, "seek": 293060, "start": 2949.48, "end": 2954.2, "text": " interested in your predictions about it but I think I will stop here thank you very much", "tokens": [51308, 3102, 294, 428, 21264, 466, 309, 457, 286, 519, 286, 486, 1590, 510, 1309, 291, 588, 709, 51544], "temperature": 0.0, "avg_logprob": -0.04033672672578658, "compression_ratio": 1.660633484162896, "no_speech_prob": 0.047320906072854996}, {"id": 494, "seek": 295420, "start": 2954.7599999999998, "end": 2961.72, "text": " thank you so much Chris for the engaging and extremely interesting topic and presentation", "tokens": [50392, 1309, 291, 370, 709, 6688, 337, 264, 11268, 293, 4664, 1880, 4829, 293, 5860, 50740], "temperature": 0.0, "avg_logprob": -0.11590899673162722, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.028658302500844002}, {"id": 495, "seek": 295420, "start": 2961.72, "end": 2967.16, "text": " you have given I'm always amazed by all the new things you're mentioning every single time we", "tokens": [50740, 291, 362, 2212, 286, 478, 1009, 20507, 538, 439, 264, 777, 721, 291, 434, 18315, 633, 2167, 565, 321, 51012], "temperature": 0.0, "avg_logprob": -0.11590899673162722, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.028658302500844002}, {"id": 496, "seek": 295420, "start": 2967.16, "end": 2973.08, "text": " talk I feel it is something new something exciting you know not you not me especially not me like", "tokens": [51012, 751, 286, 841, 309, 307, 746, 777, 746, 4670, 291, 458, 406, 291, 406, 385, 2318, 406, 385, 411, 51308], "temperature": 0.0, "avg_logprob": -0.11590899673162722, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.028658302500844002}, {"id": 497, "seek": 295420, "start": 2973.08, "end": 2979.0, "text": " expected if you'll be talking about it so soon many questions came in I must already see people", "tokens": [51308, 5176, 498, 291, 603, 312, 1417, 466, 309, 370, 2321, 867, 1651, 1361, 294, 286, 1633, 1217, 536, 561, 51604], "temperature": 0.0, "avg_logprob": -0.11590899673162722, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.028658302500844002}, {"id": 498, "seek": 295420, "start": 2979.0, "end": 2983.72, "text": " unfortunately not be able to get to all of them because the time is limited and the audience is", "tokens": [51604, 7015, 406, 312, 1075, 281, 483, 281, 439, 295, 552, 570, 264, 565, 307, 5567, 293, 264, 4034, 307, 51840], "temperature": 0.0, "avg_logprob": -0.11590899673162722, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.028658302500844002}, {"id": 499, "seek": 298372, "start": 2983.7999999999997, "end": 2991.08, "text": " so active and so many people showed up so let me pick a few um Chris so the cost of the training", "tokens": [50368, 370, 4967, 293, 370, 867, 561, 4712, 493, 370, 718, 385, 1888, 257, 1326, 1105, 6688, 370, 264, 2063, 295, 264, 3097, 50732], "temperature": 0.0, "avg_logprob": -0.06442031860351563, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.0036422875709831715}, {"id": 500, "seek": 298372, "start": 2991.08, "end": 2995.9599999999996, "text": " model so it seems it really scales with the size and we are paying a lot of attention and like", "tokens": [50732, 2316, 370, 309, 2544, 309, 534, 17408, 365, 264, 2744, 293, 321, 366, 6229, 257, 688, 295, 3202, 293, 411, 50976], "temperature": 0.0, "avg_logprob": -0.06442031860351563, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.0036422875709831715}, {"id": 501, "seek": 298372, "start": 2995.9599999999996, "end": 3001.3199999999997, "text": " putting a lot of effort into the training uh so what does it mean for the energy requirements", "tokens": [50976, 3372, 257, 688, 295, 4630, 666, 264, 3097, 2232, 370, 437, 775, 309, 914, 337, 264, 2281, 7728, 51244], "temperature": 0.0, "avg_logprob": -0.06442031860351563, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.0036422875709831715}, {"id": 502, "seek": 298372, "start": 3001.3199999999997, "end": 3005.3199999999997, "text": " and I guess we are talking about predictions but like how does it look like now and like", "tokens": [51244, 293, 286, 2041, 321, 366, 1417, 466, 21264, 457, 411, 577, 775, 309, 574, 411, 586, 293, 411, 51444], "temperature": 0.0, "avg_logprob": -0.06442031860351563, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.0036422875709831715}, {"id": 503, "seek": 298372, "start": 3005.3199999999997, "end": 3012.3599999999997, "text": " what do you recommend people to to pay attention to oh it's a wonderful set of questions to be", "tokens": [51444, 437, 360, 291, 2748, 561, 281, 281, 1689, 3202, 281, 1954, 309, 311, 257, 3715, 992, 295, 1651, 281, 312, 51796], "temperature": 0.0, "avg_logprob": -0.06442031860351563, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.0036422875709831715}, {"id": 504, "seek": 301236, "start": 3012.36, "end": 3020.28, "text": " answering and critically important I mean I ask myself you know you know if you think about", "tokens": [50364, 13430, 293, 22797, 1021, 286, 914, 286, 1029, 2059, 291, 458, 291, 458, 498, 291, 519, 466, 50760], "temperature": 0.0, "avg_logprob": -0.06253321965535481, "compression_ratio": 1.7899543378995433, "no_speech_prob": 0.055731844156980515}, {"id": 505, "seek": 301236, "start": 3020.28, "end": 3025.0, "text": " industries in the world some of them are improving in terms of their environmental impacts some are", "tokens": [50760, 13284, 294, 264, 1002, 512, 295, 552, 366, 11470, 294, 2115, 295, 641, 8303, 11606, 512, 366, 50996], "temperature": 0.0, "avg_logprob": -0.06253321965535481, "compression_ratio": 1.7899543378995433, "no_speech_prob": 0.055731844156980515}, {"id": 506, "seek": 301236, "start": 3025.0, "end": 3031.32, "text": " getting much worse where is artificial intelligence in that is it getting better or is it getting worse", "tokens": [50996, 1242, 709, 5324, 689, 307, 11677, 7599, 294, 300, 307, 309, 1242, 1101, 420, 307, 309, 1242, 5324, 51312], "temperature": 0.0, "avg_logprob": -0.06253321965535481, "compression_ratio": 1.7899543378995433, "no_speech_prob": 0.055731844156980515}, {"id": 507, "seek": 301236, "start": 3031.32, "end": 3037.7200000000003, "text": " I don't know the answer because on the one hand the expenditure for training and now serving for", "tokens": [51312, 286, 500, 380, 458, 264, 1867, 570, 322, 264, 472, 1011, 264, 40377, 337, 3097, 293, 586, 8148, 337, 51632], "temperature": 0.0, "avg_logprob": -0.06253321965535481, "compression_ratio": 1.7899543378995433, "no_speech_prob": 0.055731844156980515}, {"id": 508, "seek": 303772, "start": 3037.72, "end": 3044.04, "text": " example GPT-3 to everyone who wants to use it is absolutely enormous and it has real costs", "tokens": [50364, 1365, 26039, 51, 12, 18, 281, 1518, 567, 2738, 281, 764, 309, 307, 3122, 11322, 293, 309, 575, 957, 5497, 50680], "temperature": 0.0, "avg_logprob": -0.06285002619721168, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.17991869151592255}, {"id": 509, "seek": 303772, "start": 3044.8399999999997, "end": 3051.72, "text": " like measured in emissions and things like that on the other hand this is a centralization of all", "tokens": [50720, 411, 12690, 294, 14607, 293, 721, 411, 300, 322, 264, 661, 1011, 341, 307, 257, 5777, 2144, 295, 439, 51064], "temperature": 0.0, "avg_logprob": -0.06285002619721168, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.17991869151592255}, {"id": 510, "seek": 303772, "start": 3051.72, "end": 3057.9599999999996, "text": " of that and that can often bring real benefits and I want to not forget of the previous era", "tokens": [51064, 295, 300, 293, 300, 393, 2049, 1565, 957, 5311, 293, 286, 528, 281, 406, 2870, 295, 264, 3894, 4249, 51376], "temperature": 0.0, "avg_logprob": -0.06285002619721168, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.17991869151592255}, {"id": 511, "seek": 303772, "start": 3057.9599999999996, "end": 3065.08, "text": " where every single person trained every single model from scratch and so now a lot of our research", "tokens": [51376, 689, 633, 2167, 954, 8895, 633, 2167, 2316, 490, 8459, 293, 370, 586, 257, 688, 295, 527, 2132, 51732], "temperature": 0.0, "avg_logprob": -0.06285002619721168, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.17991869151592255}, {"id": 512, "seek": 306508, "start": 3065.16, "end": 3071.96, "text": " is actually just using these frozen components they were expensive but the expenditure of our lab", "tokens": [50368, 307, 767, 445, 1228, 613, 12496, 6677, 436, 645, 5124, 457, 264, 40377, 295, 527, 2715, 50708], "temperature": 0.0, "avg_logprob": -0.04488404393196106, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.01449581142514944}, {"id": 513, "seek": 306508, "start": 3071.96, "end": 3079.24, "text": " is probably going way down because we are not training these big models it kind of reminds", "tokens": [50708, 307, 1391, 516, 636, 760, 570, 321, 366, 406, 3097, 613, 955, 5245, 309, 733, 295, 12025, 51072], "temperature": 0.0, "avg_logprob": -0.04488404393196106, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.01449581142514944}, {"id": 514, "seek": 306508, "start": 3079.24, "end": 3084.04, "text": " me of that last mile problem again in the previous era it was like we were all driving to pick up our", "tokens": [51072, 385, 295, 300, 1036, 12620, 1154, 797, 294, 264, 3894, 4249, 309, 390, 411, 321, 645, 439, 4840, 281, 1888, 493, 527, 51312], "temperature": 0.0, "avg_logprob": -0.04488404393196106, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.01449581142514944}, {"id": 515, "seek": 306508, "start": 3084.04, "end": 3090.04, "text": " groceries everywhere huge expenditure with all those individual trips now it's much more like", "tokens": [51312, 31391, 5315, 2603, 40377, 365, 439, 729, 2609, 16051, 586, 309, 311, 709, 544, 411, 51612], "temperature": 0.0, "avg_logprob": -0.04488404393196106, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.01449581142514944}, {"id": 516, "seek": 309004, "start": 3090.12, "end": 3093.32, "text": " they're all brought to the end of the street and we walk to get them", "tokens": [50368, 436, 434, 439, 3038, 281, 264, 917, 295, 264, 4838, 293, 321, 1792, 281, 483, 552, 50528], "temperature": 0.0, "avg_logprob": -0.06014786616410359, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.011780439876019955}, {"id": 517, "seek": 309004, "start": 3094.2, "end": 3098.44, "text": " but of course that's done in big trucks and those have real consequences as well", "tokens": [50572, 457, 295, 1164, 300, 311, 1096, 294, 955, 16156, 293, 729, 362, 957, 10098, 382, 731, 50784], "temperature": 0.0, "avg_logprob": -0.06014786616410359, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.011780439876019955}, {"id": 518, "seek": 309004, "start": 3098.44, "end": 3104.92, "text": " I don't know but I hope that a lot of smart people work continue to work on this problem", "tokens": [50784, 286, 500, 380, 458, 457, 286, 1454, 300, 257, 688, 295, 4069, 561, 589, 2354, 281, 589, 322, 341, 1154, 51108], "temperature": 0.0, "avg_logprob": -0.06014786616410359, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.011780439876019955}, {"id": 519, "seek": 309004, "start": 3104.92, "end": 3108.84, "text": " and that'll lead to benefits in terms of us doing all these things more efficiently as well", "tokens": [51108, 293, 300, 603, 1477, 281, 5311, 294, 2115, 295, 505, 884, 439, 613, 721, 544, 19621, 382, 731, 51304], "temperature": 0.0, "avg_logprob": -0.06014786616410359, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.011780439876019955}, {"id": 520, "seek": 309004, "start": 3110.84, "end": 3117.0, "text": " thank you so much the next question and you touched on that a few times but it might be", "tokens": [51404, 1309, 291, 370, 709, 264, 958, 1168, 293, 291, 9828, 322, 300, 257, 1326, 1413, 457, 309, 1062, 312, 51712], "temperature": 0.0, "avg_logprob": -0.06014786616410359, "compression_ratio": 1.685483870967742, "no_speech_prob": 0.011780439876019955}, {"id": 521, "seek": 311700, "start": 3117.0, "end": 3122.2, "text": " good to summarize that a little bit because we got a lot of the questions about kind of the", "tokens": [50364, 665, 281, 20858, 300, 257, 707, 857, 570, 321, 658, 257, 688, 295, 264, 1651, 466, 733, 295, 264, 50624], "temperature": 0.0, "avg_logprob": -0.07032328509212879, "compression_ratio": 2.021276595744681, "no_speech_prob": 0.05413242056965828}, {"id": 522, "seek": 311700, "start": 3122.2, "end": 3129.08, "text": " trustworthiness and if the model actually knows that it's wrong or correct and like how do how", "tokens": [50624, 3361, 13136, 1324, 293, 498, 264, 2316, 767, 3255, 300, 309, 311, 2085, 420, 3006, 293, 411, 577, 360, 577, 50968], "temperature": 0.0, "avg_logprob": -0.07032328509212879, "compression_ratio": 2.021276595744681, "no_speech_prob": 0.05413242056965828}, {"id": 523, "seek": 311700, "start": 3129.08, "end": 3133.48, "text": " do we trust the model or like how do we achieve the trustworthiness of the model because right now", "tokens": [50968, 360, 321, 3361, 264, 2316, 420, 411, 577, 360, 321, 4584, 264, 3361, 13136, 1324, 295, 264, 2316, 570, 558, 586, 51188], "temperature": 0.0, "avg_logprob": -0.07032328509212879, "compression_ratio": 2.021276595744681, "no_speech_prob": 0.05413242056965828}, {"id": 524, "seek": 311700, "start": 3133.48, "end": 3139.24, "text": " it's a lot of the generation happening generative models happening so like how do we pass that", "tokens": [51188, 309, 311, 257, 688, 295, 264, 5125, 2737, 1337, 1166, 5245, 2737, 370, 411, 577, 360, 321, 1320, 300, 51476], "temperature": 0.0, "avg_logprob": -0.07032328509212879, "compression_ratio": 2.021276595744681, "no_speech_prob": 0.05413242056965828}, {"id": 525, "seek": 313924, "start": 3139.7999999999997, "end": 3149.16, "text": " it's an incredibly good question and it is the thing I have in mind when we're doing all our work", "tokens": [50392, 309, 311, 364, 6252, 665, 1168, 293, 309, 307, 264, 551, 286, 362, 294, 1575, 562, 321, 434, 884, 439, 527, 589, 50860], "temperature": 0.0, "avg_logprob": -0.08041012854803176, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.017973260954022408}, {"id": 526, "seek": 313924, "start": 3149.16, "end": 3155.72, "text": " on explaining models because I feel like offering faithful human interpretable explanations is the", "tokens": [50860, 322, 13468, 5245, 570, 286, 841, 411, 8745, 17808, 1952, 7302, 712, 28708, 307, 264, 51188], "temperature": 0.0, "avg_logprob": -0.08041012854803176, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.017973260954022408}, {"id": 527, "seek": 313924, "start": 3155.72, "end": 3162.2, "text": " step we can take toward trustworthiness it's a very difficult problem I just want to add that it", "tokens": [51188, 1823, 321, 393, 747, 7361, 3361, 13136, 1324, 309, 311, 257, 588, 2252, 1154, 286, 445, 528, 281, 909, 300, 309, 51512], "temperature": 0.0, "avg_logprob": -0.08041012854803176, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.017973260954022408}, {"id": 528, "seek": 313924, "start": 3162.2, "end": 3167.64, "text": " might be even harder than we've anticipated because people are also pretty untrustworthy", "tokens": [51512, 1062, 312, 754, 6081, 813, 321, 600, 23267, 570, 561, 366, 611, 1238, 1701, 22326, 23727, 51784], "temperature": 0.0, "avg_logprob": -0.08041012854803176, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.017973260954022408}, {"id": 529, "seek": 316764, "start": 3168.6, "end": 3176.12, "text": " it's just that individual people often don't have like a systemic effect right so if you're", "tokens": [50412, 309, 311, 445, 300, 2609, 561, 2049, 500, 380, 362, 411, 257, 23789, 1802, 558, 370, 498, 291, 434, 50788], "temperature": 0.0, "avg_logprob": -0.04875763257344564, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.003943504299968481}, {"id": 530, "seek": 316764, "start": 3176.12, "end": 3181.48, "text": " really doing a poor job at something you probably impact just a handful of people", "tokens": [50788, 534, 884, 257, 4716, 1691, 412, 746, 291, 1391, 2712, 445, 257, 16458, 295, 561, 51056], "temperature": 0.0, "avg_logprob": -0.04875763257344564, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.003943504299968481}, {"id": 531, "seek": 316764, "start": 3181.48, "end": 3187.64, "text": " and other people say at your company do a much better job but these ai's are now it's like they're", "tokens": [51056, 293, 661, 561, 584, 412, 428, 2237, 360, 257, 709, 1101, 1691, 457, 613, 9783, 311, 366, 586, 309, 311, 411, 436, 434, 51364], "temperature": 0.0, "avg_logprob": -0.04875763257344564, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.003943504299968481}, {"id": 532, "seek": 316764, "start": 3187.64, "end": 3195.0, "text": " everyone and so any kind of small problem that they have is amplified across the entire population", "tokens": [51364, 1518, 293, 370, 604, 733, 295, 1359, 1154, 300, 436, 362, 307, 49237, 2108, 264, 2302, 4415, 51732], "temperature": 0.0, "avg_logprob": -0.04875763257344564, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.003943504299968481}, {"id": 533, "seek": 319500, "start": 3195.08, "end": 3199.96, "text": " they interact with and that's going to probably mean that our standards for trustworthiness for", "tokens": [50368, 436, 4648, 365, 293, 300, 311, 516, 281, 1391, 914, 300, 527, 7787, 337, 3361, 13136, 1324, 337, 50612], "temperature": 0.0, "avg_logprob": -0.059010378150052806, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.02670484595000744}, {"id": 534, "seek": 319500, "start": 3199.96, "end": 3205.4, "text": " them need to be higher than they are for humans and that's another sense in which they're going", "tokens": [50612, 552, 643, 281, 312, 2946, 813, 436, 366, 337, 6255, 293, 300, 311, 1071, 2020, 294, 597, 436, 434, 516, 50884], "temperature": 0.0, "avg_logprob": -0.059010378150052806, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.02670484595000744}, {"id": 535, "seek": 319500, "start": 3205.4, "end": 3211.48, "text": " to have to be superhuman to achieve the jobs we're asking of them and the field cannot offer", "tokens": [50884, 281, 362, 281, 312, 1687, 18796, 281, 4584, 264, 4782, 321, 434, 3365, 295, 552, 293, 264, 2519, 2644, 2626, 51188], "temperature": 0.0, "avg_logprob": -0.059010378150052806, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.02670484595000744}, {"id": 536, "seek": 319500, "start": 3211.48, "end": 3219.96, "text": " guarantees right now so come help us fascinating thank you so much and like I saw also some", "tokens": [51188, 32567, 558, 586, 370, 808, 854, 505, 10343, 1309, 291, 370, 709, 293, 411, 286, 1866, 611, 512, 51612], "temperature": 0.0, "avg_logprob": -0.059010378150052806, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.02670484595000744}, {"id": 537, "seek": 321996, "start": 3219.96, "end": 3224.6, "text": " questions or comments about the bias in data and like you mentioned it also right like you", "tokens": [50364, 1651, 420, 3053, 466, 264, 12577, 294, 1412, 293, 411, 291, 2835, 309, 611, 558, 411, 291, 50596], "temperature": 0.0, "avg_logprob": -0.05967693603955782, "compression_ratio": 1.839080459770115, "no_speech_prob": 0.11573492735624313}, {"id": 538, "seek": 321996, "start": 3224.6, "end": 3232.12, "text": " like we are improving like there is a big improvement happening um last question for you um like a", "tokens": [50596, 411, 321, 366, 11470, 411, 456, 307, 257, 955, 10444, 2737, 1105, 1036, 1168, 337, 291, 1105, 411, 257, 50972], "temperature": 0.0, "avg_logprob": -0.05967693603955782, "compression_ratio": 1.839080459770115, "no_speech_prob": 0.11573492735624313}, {"id": 539, "seek": 321996, "start": 3232.12, "end": 3236.68, "text": " little bit of a thought experiment but like do you think that the large language models might be", "tokens": [50972, 707, 857, 295, 257, 1194, 5120, 457, 411, 360, 291, 519, 300, 264, 2416, 2856, 5245, 1062, 312, 51200], "temperature": 0.0, "avg_logprob": -0.05967693603955782, "compression_ratio": 1.839080459770115, "no_speech_prob": 0.11573492735624313}, {"id": 540, "seek": 321996, "start": 3236.68, "end": 3242.52, "text": " able to come up with answers to as yet unanswered important scientific questions like something", "tokens": [51200, 1075, 281, 808, 493, 365, 6338, 281, 382, 1939, 517, 43904, 292, 1021, 8134, 1651, 411, 746, 51492], "temperature": 0.0, "avg_logprob": -0.05967693603955782, "compression_ratio": 1.839080459770115, "no_speech_prob": 0.11573492735624313}, {"id": 541, "seek": 321996, "start": 3242.52, "end": 3248.92, "text": " we are not even sure that it even exists like in our minds right now oh it's a wonderful question", "tokens": [51492, 321, 366, 406, 754, 988, 300, 309, 754, 8198, 411, 294, 527, 9634, 558, 586, 1954, 309, 311, 257, 3715, 1168, 51812], "temperature": 0.0, "avg_logprob": -0.05967693603955782, "compression_ratio": 1.839080459770115, "no_speech_prob": 0.11573492735624313}, {"id": 542, "seek": 324892, "start": 3248.92, "end": 3253.7200000000003, "text": " yeah and people are asking this across multiple domains like they're producing incredible artwork", "tokens": [50364, 1338, 293, 561, 366, 3365, 341, 2108, 3866, 25514, 411, 436, 434, 10501, 4651, 15829, 50604], "temperature": 0.0, "avg_logprob": -0.06276392095229205, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.014941642992198467}, {"id": 543, "seek": 324892, "start": 3253.7200000000003, "end": 3259.32, "text": " but are we now trapped inside a feedback loop that's going to lead to less truly innovative art", "tokens": [50604, 457, 366, 321, 586, 14994, 1854, 257, 5824, 6367, 300, 311, 516, 281, 1477, 281, 1570, 4908, 12999, 1523, 50884], "temperature": 0.0, "avg_logprob": -0.06276392095229205, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.014941642992198467}, {"id": 544, "seek": 324892, "start": 3259.32, "end": 3265.2400000000002, "text": " and and if we ask them to generate text are they going to do either weird irrelevant stuff or just", "tokens": [50884, 293, 293, 498, 321, 1029, 552, 281, 8460, 2487, 366, 436, 516, 281, 360, 2139, 3657, 28682, 1507, 420, 445, 51180], "temperature": 0.0, "avg_logprob": -0.06276392095229205, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.014941642992198467}, {"id": 545, "seek": 324892, "start": 3265.2400000000002, "end": 3272.84, "text": " more of the boring average case stuff um I don't know the answer I will say though that these models", "tokens": [51180, 544, 295, 264, 9989, 4274, 1389, 1507, 1105, 286, 500, 380, 458, 264, 1867, 286, 486, 584, 1673, 300, 613, 5245, 51560], "temperature": 0.0, "avg_logprob": -0.06276392095229205, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.014941642992198467}, {"id": 546, "seek": 327284, "start": 3272.84, "end": 3278.84, "text": " have an incredible capacity to synthesize information across sources and I feel like", "tokens": [50364, 362, 364, 4651, 6042, 281, 26617, 1125, 1589, 2108, 7139, 293, 286, 841, 411, 50664], "temperature": 0.0, "avg_logprob": -0.04935349064108766, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.6885958909988403}, {"id": 547, "seek": 327284, "start": 3279.48, "end": 3285.88, "text": " that is a source of innovation for humans as well simply making those connections and it might be", "tokens": [50696, 300, 307, 257, 4009, 295, 8504, 337, 6255, 382, 731, 2935, 1455, 729, 9271, 293, 309, 1062, 312, 51016], "temperature": 0.0, "avg_logprob": -0.04935349064108766, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.6885958909988403}, {"id": 548, "seek": 327284, "start": 3285.88, "end": 3290.92, "text": " true that there is nothing new under the sun but there are lots of new connections perspectives", "tokens": [51016, 2074, 300, 456, 307, 1825, 777, 833, 264, 3295, 457, 456, 366, 3195, 295, 777, 9271, 16766, 51268], "temperature": 0.0, "avg_logprob": -0.04935349064108766, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.6885958909988403}, {"id": 549, "seek": 327284, "start": 3290.92, "end": 3296.52, "text": " and so forth to be had and I actually do have faith that models are going to be able to at least", "tokens": [51268, 293, 370, 5220, 281, 312, 632, 293, 286, 767, 360, 362, 4522, 300, 5245, 366, 516, 281, 312, 1075, 281, 412, 1935, 51548], "temperature": 0.0, "avg_logprob": -0.04935349064108766, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.6885958909988403}, {"id": 550, "seek": 329652, "start": 3296.52, "end": 3304.28, "text": " simulate some of that and it might look to us like innovation but this is not to say that this", "tokens": [50364, 27817, 512, 295, 300, 293, 309, 1062, 574, 281, 505, 411, 8504, 457, 341, 307, 406, 281, 584, 300, 341, 50752], "temperature": 0.0, "avg_logprob": -0.057190292183010055, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.13272342085838318}, {"id": 551, "seek": 329652, "start": 3304.28, "end": 3309.96, "text": " is uh not a concern for us it should be something we think about especially because we might be", "tokens": [50752, 307, 2232, 406, 257, 3136, 337, 505, 309, 820, 312, 746, 321, 519, 466, 2318, 570, 321, 1062, 312, 51036], "temperature": 0.0, "avg_logprob": -0.057190292183010055, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.13272342085838318}, {"id": 552, "seek": 329652, "start": 3309.96, "end": 3314.36, "text": " heading into an era when whether we want them to or not mostly these models are trained on their own", "tokens": [51036, 9864, 666, 364, 4249, 562, 1968, 321, 528, 552, 281, 420, 406, 5240, 613, 5245, 366, 8895, 322, 641, 1065, 51256], "temperature": 0.0, "avg_logprob": -0.057190292183010055, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.13272342085838318}, {"id": 553, "seek": 329652, "start": 3314.36, "end": 3319.72, "text": " output which is being put on the web and then consumed when people create train sets and so", "tokens": [51256, 5598, 597, 307, 885, 829, 322, 264, 3670, 293, 550, 21226, 562, 561, 1884, 3847, 6352, 293, 370, 51524], "temperature": 0.0, "avg_logprob": -0.057190292183010055, "compression_ratio": 1.7252252252252251, "no_speech_prob": 0.13272342085838318}, {"id": 554, "seek": 331972, "start": 3319.72, "end": 3328.8399999999997, "text": " forth and so on yeah great thank you so much and we are nearing the end so like last point um", "tokens": [50364, 5220, 293, 370, 322, 1338, 869, 1309, 291, 370, 709, 293, 321, 366, 408, 1921, 264, 917, 370, 411, 1036, 935, 1105, 50820], "temperature": 0.0, "avg_logprob": -0.11178899556398392, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.025122325867414474}, {"id": 555, "seek": 331972, "start": 3329.3999999999996, "end": 3335.16, "text": " do you have any like last remarks any anything anything interesting you would suggest others to", "tokens": [50848, 360, 291, 362, 604, 411, 1036, 19151, 604, 1340, 1340, 1880, 291, 576, 3402, 2357, 281, 51136], "temperature": 0.0, "avg_logprob": -0.11178899556398392, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.025122325867414474}, {"id": 556, "seek": 331972, "start": 3335.16, "end": 3342.68, "text": " look at follow read um learn about to kind of get more acquainted with the subject well learn", "tokens": [51136, 574, 412, 1524, 1401, 1105, 1466, 466, 281, 733, 295, 483, 544, 50224, 365, 264, 3983, 731, 1466, 51512], "temperature": 0.0, "avg_logprob": -0.11178899556398392, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.025122325867414474}, {"id": 557, "seek": 334268, "start": 3342.68, "end": 3347.56, "text": " more about the NLU GPT-3 other large language models and their recommendations", "tokens": [50364, 544, 466, 264, 426, 43, 52, 26039, 51, 12, 18, 661, 2416, 2856, 5245, 293, 641, 10434, 50608], "temperature": 0.0, "avg_logprob": -0.12725100579199852, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.02965172380208969}, {"id": 558, "seek": 334268, "start": 3350.68, "end": 3355.72, "text": " the thing that comes to mind based on all the interactions I have with the professional", "tokens": [50764, 264, 551, 300, 1487, 281, 1575, 2361, 322, 439, 264, 13280, 286, 362, 365, 264, 4843, 51016], "temperature": 0.0, "avg_logprob": -0.12725100579199852, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.02965172380208969}, {"id": 559, "seek": 334268, "start": 3355.72, "end": 3361.72, "text": " development students who have taken our course before is that a lot of you I'm guessing have", "tokens": [51016, 3250, 1731, 567, 362, 2726, 527, 1164, 949, 307, 300, 257, 688, 295, 291, 286, 478, 17939, 362, 51316], "temperature": 0.0, "avg_logprob": -0.12725100579199852, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.02965172380208969}, {"id": 560, "seek": 334268, "start": 3361.72, "end": 3368.7599999999998, "text": " incredibly valuable valuable domain expertise you work in an industry in a position that has taught", "tokens": [51316, 6252, 8263, 8263, 9274, 11769, 291, 589, 294, 364, 3518, 294, 257, 2535, 300, 575, 5928, 51668], "temperature": 0.0, "avg_logprob": -0.12725100579199852, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.02965172380208969}, {"id": 561, "seek": 336876, "start": 3368.76, "end": 3375.5600000000004, "text": " you tons of things and given you lots of skills and my last mile problem shows you that that is", "tokens": [50364, 291, 9131, 295, 721, 293, 2212, 291, 3195, 295, 3942, 293, 452, 1036, 12620, 1154, 3110, 291, 300, 300, 307, 50704], "temperature": 0.0, "avg_logprob": -0.04908859873392496, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.046657681465148926}, {"id": 562, "seek": 336876, "start": 3375.5600000000004, "end": 3381.32, "text": " relevant to AI and therefore you could bring it to bear on AI and we might all benefit where you", "tokens": [50704, 7340, 281, 7318, 293, 4412, 291, 727, 1565, 309, 281, 6155, 322, 7318, 293, 321, 1062, 439, 5121, 689, 291, 50992], "temperature": 0.0, "avg_logprob": -0.04908859873392496, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.046657681465148926}, {"id": 563, "seek": 336876, "start": 3381.32, "end": 3385.8, "text": " would be taking all these innovations you can learn about in our course and other courses", "tokens": [50992, 576, 312, 1940, 439, 613, 24283, 291, 393, 1466, 466, 294, 527, 1164, 293, 661, 7712, 51216], "temperature": 0.0, "avg_logprob": -0.04908859873392496, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.046657681465148926}, {"id": 564, "seek": 336876, "start": 3385.8, "end": 3392.6000000000004, "text": " combining that with your domain expertise and maybe actually making progress in a meaningful way on a", "tokens": [51216, 21928, 300, 365, 428, 9274, 11769, 293, 1310, 767, 1455, 4205, 294, 257, 10995, 636, 322, 257, 51556], "temperature": 0.0, "avg_logprob": -0.04908859873392496, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.046657681465148926}, {"id": 565, "seek": 339260, "start": 3392.6, "end": 3399.72, "text": " problem as opposed to merely having demos and things that our scientific community often produces", "tokens": [50364, 1154, 382, 8851, 281, 17003, 1419, 33788, 293, 721, 300, 527, 8134, 1768, 2049, 14725, 50720], "temperature": 0.0, "avg_logprob": -0.16000275553008655, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.12227319180965424}, {"id": 566, "seek": 339260, "start": 3399.72, "end": 3405.16, "text": " real impact so often requires real domain expertise of the sort you all have", "tokens": [50720, 957, 2712, 370, 2049, 7029, 957, 9274, 11769, 295, 264, 1333, 291, 439, 362, 50992], "temperature": 0.0, "avg_logprob": -0.16000275553008655, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.12227319180965424}, {"id": 567, "seek": 339260, "start": 3407.0, "end": 3413.16, "text": " great thank you so much um and yeah at the end thank you so much Chris for taking the time to do", "tokens": [51084, 869, 1309, 291, 370, 709, 1105, 293, 1338, 412, 264, 917, 1309, 291, 370, 709, 6688, 337, 1940, 264, 565, 281, 360, 51392], "temperature": 0.0, "avg_logprob": -0.16000275553008655, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.12227319180965424}, {"id": 568, "seek": 339260, "start": 3413.16, "end": 3419.56, "text": " this I know beginning of the quarter hectic Stanford live and I appreciate you taking the time to do", "tokens": [51392, 341, 286, 458, 2863, 295, 264, 6555, 415, 15518, 20374, 1621, 293, 286, 4449, 291, 1940, 264, 565, 281, 360, 51712], "temperature": 0.0, "avg_logprob": -0.16000275553008655, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.12227319180965424}, {"id": 569, "seek": 341956, "start": 3419.56, "end": 3425.32, "text": " this to run this webinar thank you also everybody who had a chance to join us live or like who's", "tokens": [50364, 341, 281, 1190, 341, 10942, 1309, 291, 611, 2201, 567, 632, 257, 2931, 281, 3917, 505, 1621, 420, 411, 567, 311, 50652], "temperature": 0.0, "avg_logprob": -0.10080937589152475, "compression_ratio": 1.68, "no_speech_prob": 0.05227971076965332}, {"id": 570, "seek": 341956, "start": 3425.32, "end": 3431.24, "text": " watching this recording if you could please let us know what kind of other topics you might be", "tokens": [50652, 1976, 341, 6613, 498, 291, 727, 1767, 718, 505, 458, 437, 733, 295, 661, 8378, 291, 1062, 312, 50948], "temperature": 0.0, "avg_logprob": -0.10080937589152475, "compression_ratio": 1.68, "no_speech_prob": 0.05227971076965332}, {"id": 571, "seek": 341956, "start": 3431.24, "end": 3438.44, "text": " interested in in this sort of a free webinar structure we have a little survey down on the console", "tokens": [50948, 3102, 294, 294, 341, 1333, 295, 257, 1737, 10942, 3877, 321, 362, 257, 707, 8984, 760, 322, 264, 11076, 51308], "temperature": 0.0, "avg_logprob": -0.10080937589152475, "compression_ratio": 1.68, "no_speech_prob": 0.05227971076965332}, {"id": 572, "seek": 341956, "start": 3439.64, "end": 3446.2799999999997, "text": " and yeah I hope you all have a great day a wonderful start of the of or like end of the", "tokens": [51368, 293, 1338, 286, 1454, 291, 439, 362, 257, 869, 786, 257, 3715, 722, 295, 264, 295, 420, 411, 917, 295, 264, 51700], "temperature": 0.0, "avg_logprob": -0.10080937589152475, "compression_ratio": 1.68, "no_speech_prob": 0.05227971076965332}, {"id": 573, "seek": 344628, "start": 3446.28, "end": 3451.0, "text": " winter start of the spring and yeah thank you everybody for joining us yeah Petra this is", "tokens": [50364, 6355, 722, 295, 264, 5587, 293, 1338, 1309, 291, 2201, 337, 5549, 505, 1338, 10472, 424, 341, 307, 50600], "temperature": 0.0, "avg_logprob": -0.06428043877900537, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.02140767127275467}, {"id": 574, "seek": 344628, "start": 3451.0, "end": 3455.4, "text": " wonderful we got an astounding number of really great questions it's too bad we're out of time", "tokens": [50600, 3715, 321, 658, 364, 5357, 24625, 1230, 295, 534, 869, 1651, 309, 311, 886, 1578, 321, 434, 484, 295, 565, 50820], "temperature": 0.0, "avg_logprob": -0.06428043877900537, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.02140767127275467}, {"id": 575, "seek": 344628, "start": 3455.4, "end": 3459.48, "text": " there's a lot to think about here and so that's just another thank you to the audience for all", "tokens": [50820, 456, 311, 257, 688, 281, 519, 466, 510, 293, 370, 300, 311, 445, 1071, 1309, 291, 281, 264, 4034, 337, 439, 51024], "temperature": 0.0, "avg_logprob": -0.06428043877900537, "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.02140767127275467}, {"id": 576, "seek": 345948, "start": 3459.48, "end": 3472.68, "text": " this food for thought thank you", "tokens": [50364, 341, 1755, 337, 1194, 1309, 291, 51024], "temperature": 0.0, "avg_logprob": -0.4012090100182427, "compression_ratio": 0.8611111111111112, "no_speech_prob": 0.1352783888578415}], "language": "en"}