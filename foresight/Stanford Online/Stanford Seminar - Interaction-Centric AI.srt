1
00:00:00,000 --> 00:00:16,560
Being a speaker at this seminar series is, I mean, it means a lot to me personally.

2
00:00:16,560 --> 00:00:22,040
When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but

3
00:00:22,040 --> 00:00:26,420
I didn't really have that many resources, so I had to rely on online resources.

4
00:00:26,940 --> 00:00:31,340
This seminar series recordings have been posted online, and I think I've watched

5
00:00:31,340 --> 00:00:34,860
pretty much everything to really learn about HCI.

6
00:00:34,860 --> 00:00:38,740
And then I came as a master's student here in 2008 and

7
00:00:38,740 --> 00:00:42,940
took 547 pretty much for the entire two years I was here.

8
00:00:42,940 --> 00:00:49,300
And now I feel great that I get a chance to speak as a speaker, so this is great.

9
00:00:50,340 --> 00:00:54,020
Today I want to talk about interaction centric AI.

10
00:00:54,020 --> 00:01:00,340
This is a reprise of the New Europe's keynote talk that I gave two weeks ago

11
00:01:00,340 --> 00:01:02,700
in front of thousands of AI researchers.

12
00:01:02,700 --> 00:01:06,860
I tried to reframe it a little bit so that it's more customized for

13
00:01:06,860 --> 00:01:10,020
an HCI audience rather than an AI audience.

14
00:01:10,020 --> 00:01:15,300
But the idea is that I want to think about using human AI interaction

15
00:01:15,300 --> 00:01:18,060
at the center of developing AI technologies.

16
00:01:18,060 --> 00:01:21,180
And of course, I don't have to preach to the choir that human AI interaction

17
00:01:21,180 --> 00:01:26,340
actually matters, but diving deeper based on my experience of building

18
00:01:26,340 --> 00:01:30,300
these interactive systems in different contexts, like education, discussion,

19
00:01:30,300 --> 00:01:34,740
decision making, I want to dive deeper and report some of the detailed

20
00:01:34,740 --> 00:01:38,900
interactions that we've been observing and learning from.

21
00:01:38,900 --> 00:01:44,540
And think about what it means to design human AI interaction in various contexts

22
00:01:44,540 --> 00:01:48,460
and what are some action items moving forward as a community.

23
00:01:49,460 --> 00:01:53,060
So let's first start with some definitions and terms.

24
00:01:53,060 --> 00:02:00,300
I would say the dominant paradigm for developing AI technologies has been model centric.

25
00:02:00,300 --> 00:02:05,180
The idea is to build a model with high accuracy and we want to evaluate it

26
00:02:05,180 --> 00:02:08,740
against unseen examples for its generalizability.

27
00:02:08,740 --> 00:02:13,820
And benchmarks have been great in that they could help us competitively

28
00:02:14,220 --> 00:02:18,940
compare different models' performances, which could be useful in making

29
00:02:18,940 --> 00:02:22,260
scientific advances possible.

30
00:02:22,260 --> 00:02:25,900
And more recently, people have been talking a lot about data centric AI,

31
00:02:25,900 --> 00:02:30,780
where the idea is using this nicely performing model, what is a good,

32
00:02:30,780 --> 00:02:35,460
sort of robust and efficient data pipeline around it in terms of collection

33
00:02:35,460 --> 00:02:40,820
of the data, processing of it, cleaning of it, so that the model actually performs

34
00:02:40,820 --> 00:02:43,060
really well in different contexts.

35
00:02:43,100 --> 00:02:48,500
And here the focus is acquiring quality data and setting the pipeline in a way

36
00:02:48,500 --> 00:02:51,340
that really helps the machine perform its best.

37
00:02:52,980 --> 00:02:59,660
And these two paradigms are great, but then what is slightly missing is the user

38
00:02:59,660 --> 00:03:04,580
who's using these AI technologies and those who are affected by what the AI

39
00:03:04,580 --> 00:03:06,020
systems give you.

40
00:03:06,020 --> 00:03:12,140
So interaction centric AI is sort of my term in some contrast to model

41
00:03:12,140 --> 00:03:16,300
centric and data centric AI, where the goal would be basically what HCI

42
00:03:16,300 --> 00:03:20,780
researchers do in this context, like improving the user experience by

43
00:03:20,780 --> 00:03:23,980
building usable and useful applications.

44
00:03:23,980 --> 00:03:28,860
And the unit that we often grapple with is a human AI interaction.

45
00:03:28,860 --> 00:03:32,340
And you might be wondering, is this some sort of like marketing term?

46
00:03:32,340 --> 00:03:37,260
Like how is it different from human centric AI we've been talking all about?

47
00:03:37,260 --> 00:03:41,340
It's largely similar, so I'm not trying to say I invented this new term or

48
00:03:41,340 --> 00:03:45,820
anything, but I want to focus our attention to the interaction that is

49
00:03:45,820 --> 00:03:50,540
happening between humans and AI and the complex relationships and the dynamics

50
00:03:50,540 --> 00:03:54,620
that are happening between the two, rather than focusing on just the humans

51
00:03:54,620 --> 00:03:56,140
or machine alone.

52
00:03:56,140 --> 00:04:01,100
So I can say that that's the focus of where my discussion will be today.

53
00:04:03,260 --> 00:04:09,780
So let's say you're this AI researcher and your team has built this amazing model.

54
00:04:09,780 --> 00:04:14,900
So this is actually something that I copied and pasted from one of the

55
00:04:14,900 --> 00:04:16,500
diffusion models papers.

56
00:04:16,500 --> 00:04:20,540
I don't know what they actually mean, some of them I understand.

57
00:04:20,540 --> 00:04:25,300
But basically, this is what you have as an AI researcher.

58
00:04:25,300 --> 00:04:29,700
But what would a person using this kind of AI want to do with it?

59
00:04:29,700 --> 00:04:31,140
Here's an example.

60
00:04:31,140 --> 00:04:36,340
So this is a Twitch streamer in South Korea who was trying to use this

61
00:04:36,340 --> 00:04:41,940
diffusion-based text to image generation model to create this image of an

62
00:04:41,940 --> 00:04:46,420
animated character eating ramen with chopsticks with noodles around the

63
00:04:46,420 --> 00:04:47,300
character.

64
00:04:47,300 --> 00:04:52,500
So this is the roughly sketched out goal that the user company has.

65
00:04:52,500 --> 00:04:59,220
And he ended up spending two hours fiddling with text-based prompts to get at

66
00:04:59,220 --> 00:05:01,460
the final image that he wants.

67
00:05:01,460 --> 00:05:04,820
And this is somewhat similar to what Manish shared a couple of weeks ago at

68
00:05:04,820 --> 00:05:10,580
the HAI conference in terms of what he had to do with the prompt-based interface.

69
00:05:10,580 --> 00:05:14,500
And here, the entire two-hour journey was live streamed.

70
00:05:14,500 --> 00:05:19,060
So I want to kind of share a quick summary of what happened in that stream.

71
00:05:19,060 --> 00:05:22,740
And of course, we need something in the middle to bridge between the technology

72
00:05:23,380 --> 00:05:24,740
and the human user.

73
00:05:24,740 --> 00:05:29,540
And that's what we have, the prompt-based interface and interaction that's

74
00:05:29,540 --> 00:05:30,500
happening between the two.

75
00:05:31,380 --> 00:05:35,140
So the streamer started by something simple and obvious.

76
00:05:35,140 --> 00:05:37,780
The prompt says, eating ramen, and this is what he got.

77
00:05:37,780 --> 00:05:42,740
It's okay, it's kind of there, but the bowl is perhaps too large.

78
00:05:42,740 --> 00:05:44,820
The chopsticks are all to be placed.

79
00:05:44,820 --> 00:05:49,940
And he heard from somewhere that adding a full sentence might make things better.

80
00:05:49,940 --> 00:05:52,500
So he goes, she is eating ramen.

81
00:05:53,780 --> 00:05:58,900
She is eating ramen, for sure, but you can see that something's not quite right.

82
00:05:59,220 --> 00:06:03,220
So he keeps going on by adding more descriptions.

83
00:06:03,220 --> 00:06:06,660
And the prompt is definitely getting longer.

84
00:06:07,620 --> 00:06:12,340
And it seems that the AI is not quite getting how chopsticks should be used

85
00:06:12,340 --> 00:06:14,900
and how many should be used.

86
00:06:14,900 --> 00:06:20,020
So he keeps adding these descriptions to really explain what it means to use chopsticks.

87
00:06:20,340 --> 00:06:30,900
And to be fair, there's hair, there's chopsticks, there's noodles.

88
00:06:30,900 --> 00:06:33,060
So it's in computer graphics.

89
00:06:33,060 --> 00:06:35,860
Dealing with human hair, I heard, is a really tough challenge.

90
00:06:35,860 --> 00:06:41,860
And maybe for AI, it's also kind of struggling to deal with all these similar looking objects.

91
00:06:41,860 --> 00:06:47,620
And it doesn't really seem to get how to differentiate between chopsticks and noodles.

92
00:06:48,580 --> 00:06:52,340
And another interesting aspect was that since it was a live stream,

93
00:06:52,340 --> 00:06:59,060
the viewers were actively participating in recommending new prompts to try out,

94
00:06:59,060 --> 00:07:00,900
sharing their interpretations.

95
00:07:00,900 --> 00:07:05,860
And this is somewhat of a collaborative mental model construction process,

96
00:07:05,860 --> 00:07:08,260
if you will, as a group of people.

97
00:07:08,260 --> 00:07:10,660
They are really trying to figure out what's going on.

98
00:07:10,660 --> 00:07:14,180
And now the prompt is five lines long.

99
00:07:14,740 --> 00:07:19,140
And the service that this streamer was using was supporting variations,

100
00:07:19,140 --> 00:07:22,340
where you could pick an image and say create some variations.

101
00:07:22,340 --> 00:07:26,100
And he was referring to this interaction as variation gacha.

102
00:07:26,100 --> 00:07:30,340
So gacha is a Japanese word for like a random box or blind box.

103
00:07:30,340 --> 00:07:35,140
And this kind of tells us that how unpredictable this sort of interface is.

104
00:07:35,140 --> 00:07:40,740
Once you hit the generate button, the user doesn't really have a good sense of knowing what to expect.

105
00:07:41,220 --> 00:07:45,140
And this is the actual stream, as you can see, like his praying,

106
00:07:45,140 --> 00:07:50,020
and which also tells us about the usability of this sort of system.

107
00:07:50,020 --> 00:07:54,660
He doesn't have a good way of knowing what to expect, so that he actually has to pray.

108
00:07:56,660 --> 00:08:01,140
After two hours of hard work, this is the final image that he landed.

109
00:08:01,140 --> 00:08:04,580
And it looks pretty good, and he claims victory.

110
00:08:04,580 --> 00:08:09,380
But then look at what he had to do at the top, right?

111
00:08:09,700 --> 00:08:14,740
At the top, there are seven lines of prompt that he had to write.

112
00:08:15,620 --> 00:08:19,060
And arguably, this is natural language.

113
00:08:19,780 --> 00:08:22,420
But I would say this is really pseudo-natural language.

114
00:08:23,700 --> 00:08:27,380
And so this is basically the experience that he had to go through.

115
00:08:27,380 --> 00:08:29,220
So is this a good interface?

116
00:08:29,220 --> 00:08:34,180
And I sort of got inspired by Manish's discussion of discussing the usability

117
00:08:34,180 --> 00:08:36,900
of these text prompt-based interfaces.

118
00:08:37,540 --> 00:08:38,740
There are some good elements, right?

119
00:08:38,740 --> 00:08:39,780
It's quite intuitive.

120
00:08:39,780 --> 00:08:43,940
You can use natural language, or you believe natural language could be used.

121
00:08:44,500 --> 00:08:47,300
And the output is presented in a visual manner,

122
00:08:47,300 --> 00:08:51,540
which helps you kind of understand whether you got the image that you like or not,

123
00:08:51,540 --> 00:08:52,980
so that you can sort of debug.

124
00:08:53,780 --> 00:08:56,980
And there are some interactions that are supported,

125
00:08:56,980 --> 00:09:02,580
like variations and seeds and like words that should not be used and things like that.

126
00:09:03,540 --> 00:09:07,140
But there are many ways in which this interface actually fails

127
00:09:07,140 --> 00:09:08,820
to support what the actual user wants.

128
00:09:09,700 --> 00:09:11,860
He had to rely on trial and error.

129
00:09:11,860 --> 00:09:16,420
And just the fact that he had to spend two hours to get that image

130
00:09:16,420 --> 00:09:19,060
seems to suggest that something is really wrong.

131
00:09:19,940 --> 00:09:21,940
And of course, it was not really predictable

132
00:09:21,940 --> 00:09:26,180
and lack of specific feedback on the effect of what specific words

133
00:09:26,180 --> 00:09:29,860
in the prompt had influenced on the final outcome.

134
00:09:29,940 --> 00:09:33,060
These links were often missing, which made it really difficult.

135
00:09:34,340 --> 00:09:38,660
So is this really just a problem for these text-based prompt-based systems?

136
00:09:39,300 --> 00:09:44,340
I would say every AI application faces these interaction challenges.

137
00:09:44,340 --> 00:09:47,540
On the user side, when they first encounter these systems,

138
00:09:47,540 --> 00:09:51,700
they often have to struggle to kind of figure out how to make it work.

139
00:09:51,700 --> 00:09:57,140
Often people resort to misusing it, abusing it, and learning takes a long time.

140
00:09:57,700 --> 00:10:00,100
And part of it is really a design challenge.

141
00:10:01,380 --> 00:10:03,220
And we've seen other examples like this,

142
00:10:03,220 --> 00:10:06,260
where people don't have a good sense of what's happening

143
00:10:06,260 --> 00:10:11,060
in this algorithmically-generated systems and AI-powered systems.

144
00:10:11,060 --> 00:10:14,980
Like in the famous study of Facebook newsfeed users,

145
00:10:14,980 --> 00:10:20,020
more than half of the participants were not aware of the newsfeed curation algorithm's existence

146
00:10:20,020 --> 00:10:22,100
at all, which is far from being true.

147
00:10:23,060 --> 00:10:29,540
And on the right, what you see is in the pathologists' diagnosis scenario,

148
00:10:30,260 --> 00:10:33,140
often they would rely on some notion of similarity.

149
00:10:33,140 --> 00:10:37,380
So there are these algorithms that are designed to help people find similar images,

150
00:10:37,940 --> 00:10:44,900
but then the realization that the researchers had was that people had different notions of similarity.

151
00:10:44,900 --> 00:10:50,100
So a singular notion of similarity that was used in building an algorithm would not really suffice.

152
00:10:50,100 --> 00:10:55,700
So what they ended up doing was to support three different types of similarity interaction,

153
00:10:55,700 --> 00:11:00,900
and the user was able to kind of transition between these different terms in a fluid manner,

154
00:11:00,900 --> 00:11:04,580
which really gives more control and agency on the user side.

155
00:11:07,460 --> 00:11:12,420
And these, you know, put in a more simple sort of diagram manner,

156
00:11:12,420 --> 00:11:16,900
whether you are a creator or Facebook user pathologist,

157
00:11:17,460 --> 00:11:20,740
you seem to have some kind of a mental model of how the system works,

158
00:11:20,740 --> 00:11:25,300
a very sort of a classical sort of gap between what the user wants and the system wants.

159
00:11:25,300 --> 00:11:29,620
And obviously, the system is not behaving in a way that you really want.

160
00:11:29,620 --> 00:11:36,900
And this gap arguably seems to be larger with these more complex black box and deep learning based systems.

161
00:11:38,740 --> 00:11:42,020
And AI community has been tackling this problem as well.

162
00:11:42,100 --> 00:11:47,540
And, you know, some of the folks have been framing this as an alignment problem,

163
00:11:47,540 --> 00:11:52,260
which is about aligning the model's behavior with human intent.

164
00:11:52,900 --> 00:12:00,340
And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open

165
00:12:00,340 --> 00:12:06,100
AI's response to the alignment problem, where their idea is, in addition to the, you know,

166
00:12:06,100 --> 00:12:11,860
basic large language model that they have, they would add this fine tuning layer with human feedback,

167
00:12:11,860 --> 00:12:17,060
which often involves asking people whether, you know, they were happy with the results they got.

168
00:12:17,060 --> 00:12:23,300
And the system kind of uses that feedback to train a reinforcement learning agent to

169
00:12:23,300 --> 00:12:28,660
do the fine tuning so that the resulting text aligns better with what the user wants.

170
00:12:28,660 --> 00:12:34,100
And they were seeing some success from it. And a quote from the paper is that

171
00:12:34,100 --> 00:12:39,460
making language models bigger does not inherently make them better at following a user's intent.

172
00:12:40,420 --> 00:12:46,580
And aligning language models with user intent on a wide range of tasks by fine tuning with human feedback.

173
00:12:46,580 --> 00:12:51,940
And of course, there's been a lot of discussion about whether this is really the most promising way

174
00:12:51,940 --> 00:12:57,940
to, you know, involve humans or alignment problem. But I think this is some progress towards that direction.

175
00:12:58,820 --> 00:13:06,900
But all of these examples, I would say, basically lead us to revisit these classical notions of

176
00:13:06,900 --> 00:13:13,300
Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s, right?

177
00:13:13,300 --> 00:13:19,300
As a user, they want to know what's going on with the system, and they want to have more control and agency.

178
00:13:19,300 --> 00:13:24,900
And on the evaluation side, when AI gives you some kind of result, they want to be able to

179
00:13:24,900 --> 00:13:31,700
understand it, interpret it, and want to get some explanation of it. And as an HCI researcher who's

180
00:13:31,700 --> 00:13:37,140
building these interactive systems, I feel like in often cases, I try to bridge these gaps.

181
00:13:37,140 --> 00:13:43,060
I come up with new ways of designing these social interactions and human AI interactions

182
00:13:43,060 --> 00:13:48,500
in a way that tries to bridge these gaps. And these are just some of the systems that I've been

183
00:13:48,500 --> 00:13:55,140
developing in different application domains. And I think many of them have somewhat succeeded

184
00:13:55,140 --> 00:13:59,220
in bridging these gaps. But other times, to be honest, we haven't done a good job of doing that.

185
00:14:00,180 --> 00:14:05,700
So what I want to do for the remaining time for this talk is to share some of these lessons,

186
00:14:05,700 --> 00:14:12,580
and some of them from positive experiences, but other times, bitter experiences by something

187
00:14:12,580 --> 00:14:18,260
that we haven't really done a good job of. And the main message that I want to send across

188
00:14:18,260 --> 00:14:23,860
is that beyond these point solutions for this system that works in this particular context,

189
00:14:23,860 --> 00:14:28,500
we've seen some success, I think as a field, we really need to start thinking about,

190
00:14:29,140 --> 00:14:34,900
can we do something more systematic and sustainable? Or empower designers and developers in thinking

191
00:14:34,900 --> 00:14:42,500
about can we develop these AI applications that are more usable and useful for more groups of people

192
00:14:42,500 --> 00:14:48,340
rather than having to reinvent the wheel each time someone has to develop these applications.

193
00:14:48,340 --> 00:14:51,540
And I think we're seeing too many of these cases where people are like,

194
00:14:52,260 --> 00:14:56,580
there's this cool model, let's build something around it, and it just gets released in a few

195
00:14:56,580 --> 00:15:01,780
days and realizes that people want it in a completely different way, people abuse it,

196
00:15:01,780 --> 00:15:05,860
a few days later, it goes down. We're seeing too many of these failure cases.

197
00:15:07,460 --> 00:15:12,100
So from the HCI point of view, I think HCI research can really advance this

198
00:15:13,460 --> 00:15:18,580
interaction-centric AI by contributing these generalizable building blocks for designing

199
00:15:18,580 --> 00:15:25,540
these systems and interface affordances. And AI research can also advance by embracing the idea

200
00:15:25,540 --> 00:15:31,540
of interaction-centric AI by rethinking models, architecture design, benchmarks, metrics, and

201
00:15:31,540 --> 00:15:39,380
research process. The part of it has to involve sort of broadening the perspective beyond just

202
00:15:39,380 --> 00:15:46,020
thinking about the model and the output that it generates to think about the users behind those

203
00:15:46,020 --> 00:15:51,220
and their mental models. And often there's not just a single user, but a group of user,

204
00:15:51,220 --> 00:15:56,020
community of user, a society of users. And there's also the temporal dimension,

205
00:15:56,020 --> 00:16:01,780
like before the user comes in and tries to use a system, we should be asking the questions about

206
00:16:01,780 --> 00:16:07,380
like, what's the task and who are these users and why and how. And during the interaction,

207
00:16:07,380 --> 00:16:12,420
we need to be thinking about presentation visualization. And the other way around as

208
00:16:12,420 --> 00:16:16,500
well, like interpretable results are being presented to the user. Do they have a way to

209
00:16:16,500 --> 00:16:22,260
provide feedback to the system? And also, it's never going to be just a single use, right? People

210
00:16:22,260 --> 00:16:27,940
would want to come back and use a system for a sustained amount of time. In those cases, people's

211
00:16:27,940 --> 00:16:35,220
mental model would evolve. And what does it mean for the system? And so I think this is sort of

212
00:16:35,220 --> 00:16:43,540
the ecosystem that I have in mind. And with these, I want to dive into these specific examples

213
00:16:43,540 --> 00:16:50,180
where we designed human-AI interactions. And I identify four major challenges

214
00:16:52,340 --> 00:16:57,940
in terms of human-AI interaction. The first one is about bridging the accuracy gap.

215
00:16:59,620 --> 00:17:04,660
So I'm on my sabbatical now. I'm working with this startup called Ringle, where they are

216
00:17:04,660 --> 00:17:10,260
basically Uber for language learning. They are matching tutors and tuties, and they have this

217
00:17:10,260 --> 00:17:16,500
video-based language tutoring session. So what we try to do here is to build this diagnostic

218
00:17:16,500 --> 00:17:21,940
service based on analyzing the chat-based tutoring session to give people personalized

219
00:17:21,940 --> 00:17:28,580
feedback and suggestions for improvement. But instead of going into the details of the service

220
00:17:28,580 --> 00:17:35,300
itself, I want to touch upon the case that we ran into when we were trying to run this automated

221
00:17:35,300 --> 00:17:42,100
speech recognition AI, which is crucial in sort of turning the video-based chat into text format,

222
00:17:42,660 --> 00:17:46,980
which is really required for us to run all these diagnostic algorithms on top of.

223
00:17:48,420 --> 00:17:56,020
And the standard metric of success in ASR would be word error rate, how correctly it can

224
00:17:56,580 --> 00:18:04,180
recover the original text. And on the tutor side, when we ran ASR on like hundreds and

225
00:18:04,180 --> 00:18:11,060
thousands of sessions, the average word error rate was around 8%. Can you take a guess as to what

226
00:18:11,060 --> 00:18:18,980
the number would have been for students? Obviously there's this white margin that's quite high,

227
00:18:19,060 --> 00:18:28,340
so you can imagine, 30. Yeah, we're seeing 23. So there's quite a bit of a gap. And this is an

228
00:18:28,340 --> 00:18:34,740
example of an accuracy gap where different groups of users are getting disproportionate results from

229
00:18:34,740 --> 00:18:41,620
the same AI. And the gap actually widens if we look at like the best tutor and the worst student

230
00:18:41,620 --> 00:18:47,380
when it comes to the performance of these models. But in terms of thinking about the interaction

231
00:18:47,380 --> 00:18:53,620
that these people are trying to have with this AI, I would argue that the students are the ones

232
00:18:53,620 --> 00:18:59,620
who really need this AI to work. Based on the accuracy of this AI, they want to kind of look

233
00:18:59,620 --> 00:19:06,420
at where they succeeded and failed and they want to learn and reflect. And with this low accuracy,

234
00:19:06,420 --> 00:19:11,940
they would really be struggling to come up with good action items and they might be frustrated,

235
00:19:11,940 --> 00:19:17,220
they might lose trust on the system. But interestingly, a lot of focus when it comes to

236
00:19:17,220 --> 00:19:24,180
model development is that we seem to be focusing on the 6%, like making the 6% better instead of

237
00:19:24,180 --> 00:19:30,420
narrowing the gap between 6 and 36%. And we have to really be asking like, what is the most important

238
00:19:30,420 --> 00:19:34,660
question in this context? And are we really focusing on the most important question here?

239
00:19:35,460 --> 00:19:42,580
And we see these other examples too, where Tyra and others have studied the machine translation

240
00:19:42,580 --> 00:19:48,100
that is being used in emergency rooms when it comes to discharge statements that are presented

241
00:19:48,100 --> 00:19:54,660
to patients and patients' families. And we see a huge disparity between different languages.

242
00:19:54,660 --> 00:20:01,380
And in the natural language processing community, this support for low resource languages has been

243
00:20:02,100 --> 00:20:07,700
a topic for research and there has been great efforts. And on the right is a famous example

244
00:20:07,700 --> 00:20:15,700
of gender shades, where the gender classification algorithm shows, again, an accuracy disparity

245
00:20:15,700 --> 00:20:24,900
between darker skin female versus lighter scale male. And of course, these diversity and inclusion

246
00:20:25,700 --> 00:20:30,980
efforts and low resource language support research in the AI community and in the community

247
00:20:30,980 --> 00:20:36,580
have been tackling these issues of accuracy gap, of course. But then I would argue that they could

248
00:20:37,380 --> 00:20:43,940
advance further by embracing more interaction-centric approach in trying to really see how in the real

249
00:20:43,940 --> 00:20:49,380
world people are interacting with these results and what kind of actual struggles that they have

250
00:20:49,380 --> 00:20:57,460
because of poor or good AI accuracy and what, as a community, how can we define the problem that's

251
00:20:57,460 --> 00:21:05,780
most important. And conceptually speaking, I feel like a good analogy might be the ceiling and floor

252
00:21:05,780 --> 00:21:12,740
analogy. The ceiling would be this primary user group who gets the best part of AI. And floor

253
00:21:12,740 --> 00:21:19,540
would be secondary user group who is disproportionately getting more negative impact of

254
00:21:19,540 --> 00:21:25,780
the same AI. And there's this accuracy gap. And often I feel like taking a model-centric approach

255
00:21:25,780 --> 00:21:32,340
incentivizes people and researchers to work on raising the ceiling. There could be a couple

256
00:21:32,340 --> 00:21:37,460
reasons for this. First of all, that's the sota number you get, which might be what you need to

257
00:21:37,460 --> 00:21:43,060
publish a paper out of it. Or the benchmarks that you're working with do not really have much data

258
00:21:43,940 --> 00:21:48,500
on the floor side. It's maybe more focused on the ceiling side. And that's why the ceiling is there

259
00:21:48,500 --> 00:21:54,500
in the first place. So it might be just incentivizing people to continue to push the boundaries of

260
00:21:54,500 --> 00:22:02,580
ceiling. And as a result, what we see is a lot of a widened accuracy gap. And if we take a more

261
00:22:02,580 --> 00:22:07,380
interaction-centric approach, I would argue that if we identify that narrowing this gap is a more

262
00:22:07,380 --> 00:22:13,620
important problem, we can narrow this accuracy gap. And it's not just a matter of accuracy,

263
00:22:13,620 --> 00:22:18,180
if you think about it. It's about experience, benefit, and value that people get out of

264
00:22:18,180 --> 00:22:26,420
interacting with this AI. So there was a first challenge about the accuracy gap and how thinking

265
00:22:26,420 --> 00:22:31,860
about how people interact with this AI can help us identify what problems are worth tackling.

266
00:22:32,660 --> 00:22:38,420
And second of all, I want to talk about when people actually use AI. And one of the

267
00:22:38,980 --> 00:22:45,460
anti-patterns of human-AI interaction is that people just stop using AI altogether or abandon it,

268
00:22:46,020 --> 00:22:52,500
which is something you might want to avoid as a system designer. And that's why it's important

269
00:22:52,500 --> 00:22:59,380
to think about how do we incentivize people to work with AI? And in most cases, people abandon

270
00:22:59,380 --> 00:23:05,460
using AI because it's not really giving them concrete value that they expect. And we explore

271
00:23:05,460 --> 00:23:11,860
this in the context of online education in this system called XS. So the problem that we wanted

272
00:23:11,860 --> 00:23:17,380
to focus here is that in online, let's say you want to learn some new concept like probability,

273
00:23:17,380 --> 00:23:22,980
there are lots of problems and answers you can find. But finding good explanations is

274
00:23:22,980 --> 00:23:29,140
surprisingly difficult. And generating high-quality explanations is costly and resource-intensive

275
00:23:29,140 --> 00:23:36,020
console. So we wanted to tackle this problem by building this online education platform,

276
00:23:36,020 --> 00:23:41,780
where people are presented with a problem and they solve this problem, they submit an answer,

277
00:23:43,540 --> 00:23:49,700
and they see an example that's presented by the system and they get a chance to rate how helpful

278
00:23:49,700 --> 00:23:57,460
the explanation that they saw was. And then they are getting a chance to sort of self-explain

279
00:23:57,460 --> 00:24:03,540
their own answer. So this is a pedagogically meaningful activity to be able to sort of explain

280
00:24:03,540 --> 00:24:09,940
your thought process, externalize it, and lots of research supports doing self-explanation.

281
00:24:10,740 --> 00:24:16,420
Okay, so fairly simple sort of front end in terms of the learner's experience. So what's

282
00:24:16,420 --> 00:24:23,380
happening behind the scene is that the system is collecting these explanations and ratings

283
00:24:23,380 --> 00:24:29,780
from learners, right? Since it's a live system, new learners keep coming in and provide new ratings

284
00:24:29,780 --> 00:24:36,020
and explanations. And we formulate this in a multi-armed bandit manner, which means that

285
00:24:36,020 --> 00:24:41,860
as a new explanation comes into the system, as a byproduct of humans' learning activity,

286
00:24:41,860 --> 00:24:47,860
a new arm gets added to the system. And what the system is doing is to determine this dynamic

287
00:24:47,860 --> 00:24:54,180
policy for what the most effective explanation would be for the next learner coming into the system.

288
00:24:55,780 --> 00:25:01,380
So if you're familiar with the reinforcement learning of concepts, we are navigating

289
00:25:01,380 --> 00:25:07,220
exploitation and exploration trade-off. Exploitation in the sense that the system wants to present the

290
00:25:07,220 --> 00:25:12,980
best explanations to the next learner coming into the system, but the system doesn't really know what

291
00:25:12,980 --> 00:25:18,980
the best explanations are until it collects some amount of ratings from people. So it has to do some

292
00:25:18,980 --> 00:25:25,220
exploration where it should collect this data. And to solve that, we use a technique called

293
00:25:25,220 --> 00:25:31,780
Thomson sampling. So what happens is the system keeps track of these policies and when a new

294
00:25:31,780 --> 00:25:36,740
explanation comes in and ratings come in, these things get updated and the policy

295
00:25:37,140 --> 00:25:44,260
of probabilistic policy gets updated so that it uses this distribution to determine

296
00:25:44,260 --> 00:25:54,340
what explanation to show to the next learner. So when we ran a study, these access-generated

297
00:25:54,340 --> 00:26:01,780
explanations were helpful in terms of helping people learn better. So when we compared against

298
00:26:01,860 --> 00:26:07,220
presenting no explanation at all and measured differences between pre-test and post-test

299
00:26:07,220 --> 00:26:13,860
results, we were seeing that people were gaining 3% increase in their scores. So just getting a

300
00:26:14,660 --> 00:26:21,060
chance to rethink the problem, I think still gave them some increase in their scores. And when they

301
00:26:21,060 --> 00:26:28,020
were seeing the instructor-generated explanation, which is, I guess, somewhat of an ideal case or

302
00:26:28,020 --> 00:26:34,420
the standard case, we're seeing 9% increase and with access, we're seeing 12% increase. So between

303
00:26:34,420 --> 00:26:39,620
these two conditions, it was not statistically significantly different, but there were certainly

304
00:26:39,620 --> 00:26:47,220
cases where access was picking explanations from learners that were even more powerful than the

305
00:26:47,220 --> 00:26:54,740
instructor-generated ones. So in this system, if we were to take a more model-centric approach,

306
00:26:54,740 --> 00:27:00,900
I think we might have built an AI that automatically generates high quality explanations.

307
00:27:02,340 --> 00:27:08,260
But instead, in taking an interaction-centric approach, I think the system we created is basically

308
00:27:08,260 --> 00:27:15,540
this co-learning system where the user, the learner, and AI are learning at the same time in a single

309
00:27:15,540 --> 00:27:22,420
system. So it's sort of an education-focused system of the game-with-the-purpose kind of setting,

310
00:27:22,420 --> 00:27:27,380
where organic benefits are provided to people who are interacting with the system,

311
00:27:27,380 --> 00:27:33,380
and the system is learning something useful out of it. And this is basically the mechanism that

312
00:27:33,380 --> 00:27:38,820
we have in that both sides are learning and explanation and feedback are establishing this

313
00:27:38,820 --> 00:27:45,460
loop. And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing,

314
00:27:45,460 --> 00:27:51,780
where learners as a crowd coming into the system are basically doing this by getting their individual

315
00:27:51,780 --> 00:27:57,620
benefits while they're providing something useful for the system to learn and do its thing better.

316
00:27:58,580 --> 00:28:05,380
So since then, I've been expanding this idea to a broad array of applications. So for example,

317
00:28:05,380 --> 00:28:11,860
can we use this kind of co-learning ideas to summarize how-to videos in terms of steps and

318
00:28:11,860 --> 00:28:17,780
sub-steps, or building a concept map out of an instructional video that shows relationships

319
00:28:17,780 --> 00:28:22,740
between different concepts, or helping learners come up with the solution plans

320
00:28:23,380 --> 00:28:28,500
in algorithmic problem-solving settings. And other researchers have taken on this idea

321
00:28:28,500 --> 00:28:33,540
in different application contexts as well. So I think we can try to really generalize this kind

322
00:28:33,540 --> 00:28:42,980
of idea of co-learning system design in different contexts. Moving on to the third challenge,

323
00:28:43,940 --> 00:28:50,740
is about beyond a single user. And often we think about a single user, a single AI interacting

324
00:28:50,740 --> 00:28:55,940
with each other. In real life, it would be much more complex and there would be diverse configurations.

325
00:28:56,980 --> 00:29:03,220
So how can we consider these social dynamics? And there could be various types of social dynamics,

326
00:29:03,780 --> 00:29:10,900
but one specific instance that we did in was group-based, chat-based discussion in a group.

327
00:29:11,780 --> 00:29:18,020
So we built this system called Solution Chat, where the idea is what if this AI agent could

328
00:29:18,740 --> 00:29:25,060
recommend real-time moderation messages to a group. So let's say a group is discussing,

329
00:29:25,060 --> 00:29:30,020
you know, what to do for the company retreat next week, and they're having a discussion.

330
00:29:30,020 --> 00:29:35,060
The system, in real time, based on the understanding of the discussion context,

331
00:29:35,060 --> 00:29:38,820
and also knowing what kind of messages would be useful for the group,

332
00:29:38,820 --> 00:29:44,020
based on our sort of literature survey of discussion and discussion-based education,

333
00:29:44,740 --> 00:29:52,340
it presents these recommendation messages, like any more ideas, or can this person share their

334
00:29:52,340 --> 00:29:58,100
opinions, you have been quiet for a while, or should we try to move on to the next stage,

335
00:29:58,100 --> 00:30:03,780
or thank you for your opinion. So these kinds of moderation messages are presented by the system

336
00:30:03,780 --> 00:30:10,020
in real time, just like what you get in smart replies in Gmail, for example. And as a moderator,

337
00:30:10,020 --> 00:30:16,500
you can just choose to accept any of the messages that you like, and discard the ones that you don't

338
00:30:16,500 --> 00:30:28,500
like. So a quick summary of the results of what we saw was that in our lab study with 55 users

339
00:30:28,500 --> 00:30:34,740
in 12 different groups, when we compared how many moderation messages were used in different groups,

340
00:30:34,740 --> 00:30:39,540
when we compared the baseline condition without these real-time recommendations versus

341
00:30:39,540 --> 00:30:45,300
solution chat or system, we're seeing a significant increase in the number of moderation messages

342
00:30:45,300 --> 00:30:50,340
that were present in the chat stream in the solution chat condition. But interestingly,

343
00:30:50,340 --> 00:30:55,700
you can see that the users manually typed moderation messages were actually decreasing

344
00:30:55,700 --> 00:31:04,980
in solution chat, but many of them were replaced by the accepting AI-generated recommendations.

345
00:31:07,380 --> 00:31:13,140
And furthermore, we had this great opportunity to actually release this system to over 2,000

346
00:31:13,140 --> 00:31:18,180
real-world users in a corporate education setting. So during COVID, a lot of these corporate

347
00:31:18,180 --> 00:31:24,580
education programs moved online, and this company that we worked with wanted to use these kinds of

348
00:31:24,580 --> 00:31:31,620
system to moderate hundreds of chat rooms that were doing discussion-based activity.

349
00:31:32,980 --> 00:31:39,860
And not surprisingly, just like the very first live stream prompt example that I mentioned,

350
00:31:39,860 --> 00:31:45,540
here again, people were collaboratively trying to understand the capabilities and limitations of

351
00:31:45,540 --> 00:31:51,540
AI when they were first presented with the system. So they were using the chat to test

352
00:31:51,540 --> 00:31:57,940
different messages, often things that they believe would be not working, and they would be

353
00:31:57,940 --> 00:32:03,620
sharing the results of, oh, this is working, this is not working, I think this does this well,

354
00:32:03,620 --> 00:32:10,420
but not that well. And it seems as a group does this kind of testing in the very first phase of

355
00:32:10,420 --> 00:32:16,660
their usage of the system, people have this shared expectation of the system, and that seems to sort

356
00:32:16,660 --> 00:32:23,540
of determine their further interactions with the system. And it was also notable how different

357
00:32:23,540 --> 00:32:28,500
groups had different expectations based on their limited experimentation that they did in the beginning.

358
00:32:30,340 --> 00:32:33,620
And there were some interesting social dynamics that we observed as well,

359
00:32:34,500 --> 00:32:41,380
like in how people use these AI recommendations to socially interact with each other.

360
00:32:42,340 --> 00:32:46,420
Some people were using AI as proxy. So one of the quotes that we had was,

361
00:32:46,420 --> 00:32:51,220
I didn't want to directly ask the person to stop talking. So the person relied on the AI

362
00:32:51,220 --> 00:32:58,820
recommended message to kind of send it. They still chose to send it, but it was their way of kind of

363
00:32:58,820 --> 00:33:07,540
softening the potential sort of dispute with the person. Other people were using AI as a reference.

364
00:33:07,620 --> 00:33:12,820
So what we were seeing is that it was a fairly simple technical pipeline that we had. So it was

365
00:33:12,820 --> 00:33:20,580
just a canned response. So people were sometimes not really fond of the tone of the message,

366
00:33:20,580 --> 00:33:25,300
style of the message that we showed. So the person said, I found no fun in the recommended

367
00:33:25,300 --> 00:33:30,900
messages because all the messages look the same. So in those cases, what people did was they still

368
00:33:31,460 --> 00:33:39,380
adopted the idea from the recommendation, but then rewrote it so that it feels more personal,

369
00:33:39,380 --> 00:33:46,820
and it feels more like it's coming from them, not AI. In other cases, AI seems to be adding

370
00:33:46,820 --> 00:33:53,220
a social burden. So in this excerpt, so one of the people said, I'm doubtful about the

371
00:33:53,780 --> 00:33:59,460
credibility of AI. And then the moderator picks this AI recommendation. Thanks for your opinion.

372
00:33:59,540 --> 00:34:04,100
Another person says, I also think negatively. Thanks for your opinion. Thanks for sharing a good

373
00:34:04,100 --> 00:34:09,460
opinion. Shall we go to the next topic? And then the moderator realizes he might have clicked,

374
00:34:09,460 --> 00:34:14,820
accept way too many times, and it was a little unnatural. So he stopped to kind of

375
00:34:14,820 --> 00:34:20,420
clarify and apologize for my unnatural words as I'm using AI recommendations.

376
00:34:21,220 --> 00:34:27,380
So while we were seeing how people were saving their time and cognitive effort in moderation

377
00:34:27,380 --> 00:34:32,500
could have decreased, it might have actually introduced other types of burden at the same time.

378
00:34:35,460 --> 00:34:39,700
Again, so if we were to build this kind of system in a more model-centric manner,

379
00:34:39,700 --> 00:34:44,580
I think a good alternative might have been automated discussion moderation, where AI

380
00:34:44,580 --> 00:34:52,500
would actually do all the moderation by itself. But instead, we chose to take a more AI-assisted

381
00:34:52,500 --> 00:34:58,340
moderation for obvious reasons. Users want to have more agency and control, and they wanted to

382
00:34:58,340 --> 00:35:04,340
keep their style of communication. So instead of handing over the entire control to AI,

383
00:35:04,900 --> 00:35:12,420
we still sort of gave that control to the human moderator who could kind of use it as an additional

384
00:35:12,420 --> 00:35:20,900
resource. Okay, so there was a third challenge. And moving on to the final challenge of supporting

385
00:35:20,900 --> 00:35:27,060
sustainable engagement. Here, the concern is that we want to think beyond this single

386
00:35:27,060 --> 00:35:34,020
session usage. And over time, how people react to these systems might change, their mental model

387
00:35:34,020 --> 00:35:38,260
might change, and how AI actually works might change. So we need to really think about this

388
00:35:38,260 --> 00:35:47,060
temporal dimension more carefully. And for this thread, we investigated in the context of

389
00:35:47,860 --> 00:35:54,260
novices making changes to websites that they're seeing. So for example, you might have a case

390
00:35:54,260 --> 00:35:59,220
where you visited this website that colors hurt your eyes, or you couldn't really find this button

391
00:35:59,220 --> 00:36:05,860
or tap it because it's too small, maybe you want to make it larger. But then people without

392
00:36:05,860 --> 00:36:12,580
expertise in HTML and CSS have difficulty doing this. So we thought by leveraging the power of

393
00:36:12,580 --> 00:36:18,420
large language models and so on, maybe we can support more natural language queries. So if a

394
00:36:18,420 --> 00:36:24,660
person says tone down the text, the system can kind of display these style recommendations that they

395
00:36:24,660 --> 00:36:33,300
can explore and select from that are about toning down the text. So the way the system works is

396
00:36:33,300 --> 00:36:40,260
if the user clicks and says make this larger, the system presents a set of design attributes that

397
00:36:40,260 --> 00:36:50,820
are about making something larger. And the user can say emphasize this part. It's somewhat ambiguous.

398
00:36:50,820 --> 00:36:56,180
There isn't a clear single design attribute that is about emphasis. So it presents these

399
00:36:57,220 --> 00:37:05,460
few recommendations that are about emphasizing something. So we built this by establishing

400
00:37:05,460 --> 00:37:12,660
this NLP pipeline and computer vision pipeline. On the NLP side, what it does is analyzing the

401
00:37:12,660 --> 00:37:19,700
user's query and mapping them with the style attributes that seem to be connected to what

402
00:37:19,700 --> 00:37:26,740
the user's intent is about. In terms of computer vision, we collected millions of web design

403
00:37:27,620 --> 00:37:34,020
elements to determine a good set of recommendations to show to the learner. So by combining those,

404
00:37:34,020 --> 00:37:39,940
we built this system. Again, so instead of going deep into the technical details of the system,

405
00:37:39,940 --> 00:37:46,660
I want to focus on the interaction dynamics. So we ran this user study with 40 people where we

406
00:37:46,660 --> 00:37:52,660
presented them with either stylet, which is the name of our system, versus the baseline, the

407
00:37:52,660 --> 00:37:57,220
Chrome developer tool, which is sort of the standard tool for making these style changes.

408
00:37:58,180 --> 00:38:04,340
So we compared these two groups. And we gave people two tasks. One is a well-defined task

409
00:38:04,340 --> 00:38:10,900
where we ask people to turn this before image into an after image. And then secondly, we had this

410
00:38:10,900 --> 00:38:16,100
open-ended task where we gave this blank slate and people were able to make any kind of change

411
00:38:16,100 --> 00:38:24,900
that they want. First, I want to share success stories. People were more successful in completing

412
00:38:24,980 --> 00:38:32,180
these design tasks when using stylet. 80% of the stylet users completed the task as opposed to only

413
00:38:32,180 --> 00:38:38,180
35% in Chrome developer tools. And these were complete novices in web design, no experience at all.

414
00:38:39,060 --> 00:38:44,500
And people completed the task in 35% less times. It was efficient to use stylet.

415
00:38:45,380 --> 00:38:51,300
Another interesting observation was that people were making same similar number of changes in

416
00:38:51,300 --> 00:38:58,740
both conditions. But in stylet condition, people were making more diverse changes, which means that

417
00:38:58,740 --> 00:39:04,740
it probably had to do with how stylet shows these multiple options for people to explore. And there

418
00:39:04,740 --> 00:39:10,100
was a conscious decision to not just show the most obvious one, but show somewhat related ones as

419
00:39:10,100 --> 00:39:16,260
well so that people could explore and tinker around different options. But then an unexpected

420
00:39:16,820 --> 00:39:22,260
finding was when we looked at people's self-confidence. Because we thought this kind of system would be

421
00:39:22,260 --> 00:39:27,860
useful for people's learning of the skills and confidence that they have about the skills,

422
00:39:27,860 --> 00:39:33,540
we asked people's self-confidence after each task. What we noted was that after the first task,

423
00:39:34,100 --> 00:39:38,980
in both conditions, people's self-confidence increased. But then in the second task,

424
00:39:38,980 --> 00:39:45,220
after the second task, users' self-confidence decreased for stylet while in the developer

425
00:39:45,220 --> 00:39:51,220
tool, it kept increasing. Why would that be the case? And we were seeing many cases where

426
00:39:51,220 --> 00:39:56,420
stylet users were frustrated that the only control that they had was natural language.

427
00:39:57,060 --> 00:40:01,220
Now they have some grasp of how it works. They wanted to do more fine-grained control more

428
00:40:01,220 --> 00:40:06,180
directly. And they wanted more specific things. But because they only had natural language,

429
00:40:06,740 --> 00:40:11,780
they sometimes just got frustrated. Whereas in the Chrome Developer Tools condition,

430
00:40:11,780 --> 00:40:16,260
people were just happy that they accomplished something with their own hands.

431
00:40:16,260 --> 00:40:22,020
And I think that is presented as a continued increase in self-confidence.

432
00:40:23,220 --> 00:40:31,860
And we know from HCI and CS147 that people's expertise and learnability really matters. And as

433
00:40:31,860 --> 00:40:39,140
they have more knowledge of the domain and the skill, they might need to get more advanced

434
00:40:39,140 --> 00:40:45,060
controls or being able to more directly manipulate what they are working on. So I think this had

435
00:40:45,060 --> 00:40:50,020
some interesting lessons in terms of thinking about the temporal dimension in that learners are

436
00:40:50,020 --> 00:40:57,060
changing. And other researchers have been reporting that considering these temporal dynamics is

437
00:40:57,060 --> 00:41:02,900
important. On the left, what you see is design researchers who have shown that there are these

438
00:41:02,900 --> 00:41:08,900
different stages of relationship that people have in technologies like self-tracking devices.

439
00:41:09,220 --> 00:41:14,420
First, they would start with initiation and experimentation, followed by intensifying and

440
00:41:14,420 --> 00:41:19,620
integration, and then stagnation and termination. And one of the design lessons might be that

441
00:41:20,180 --> 00:41:26,340
these might be more meta-level factors that really should be considered in design systems,

442
00:41:26,340 --> 00:41:31,780
in that even the same kind of intervention might need to be presented in different

443
00:41:31,780 --> 00:41:36,340
manners depending on what stage you are or what your expectation is with the system.

444
00:41:36,900 --> 00:41:42,260
On the right, what you're seeing is the guidelines for human-AI interaction, really influential

445
00:41:42,260 --> 00:41:48,420
work from Emershi et al. And they organize these guidelines for human-AI interaction

446
00:41:48,420 --> 00:41:53,860
in different categories but are organized in the temporal sort of aspect, like initially

447
00:41:53,860 --> 00:42:00,740
encounter with AI during interaction, when things go wrong, and over time. So taking into account

448
00:42:00,820 --> 00:42:06,180
this temporal dimension can really be powerful in supporting more sustainable engagement.

449
00:42:07,620 --> 00:42:14,100
And the related question might be, as people are relying more on these AI tools, like grammar

450
00:42:14,100 --> 00:42:20,660
fixes or even generating text, it's important to think about how people's mental model would

451
00:42:20,660 --> 00:42:27,540
change over time, and AI also changes over time too. And do we hit a point where people become

452
00:42:27,540 --> 00:42:34,100
maybe overly reliant in that maybe their grammar skills or writing skills do not improve anymore,

453
00:42:34,660 --> 00:42:40,100
but then without the tool, they actually might perform worse? And what is that dynamic? Or maybe

454
00:42:40,100 --> 00:42:44,580
over reliance is perfectly fine because if we believe these tools will be around the user all

455
00:42:44,580 --> 00:42:50,260
the time, maybe it's just the final outcome that matters. And I think we need more studies and

456
00:42:50,340 --> 00:42:55,780
analysis of the long-term engagement of users using these kind of technologies.

457
00:42:59,220 --> 00:43:03,700
And to kind of sum up, if we were to take a more model-centric approach here, I think we might

458
00:43:03,700 --> 00:43:11,220
have built a system that makes automatic design fixes to optimize a web page directly, and the

459
00:43:11,220 --> 00:43:17,380
system makes a fix and user can just use it. But instead, we took a more sort of interaction-centric

460
00:43:17,380 --> 00:43:23,780
route where we asked people to do sort of, you know, style change by themselves as the system

461
00:43:23,780 --> 00:43:29,460
was presenting these recommendations, and they still had to do the fix by themselves. But what

462
00:43:29,460 --> 00:43:35,460
we expected here was that people can then customize by seeing these attributes, they can learn,

463
00:43:35,460 --> 00:43:40,100
they can discover new ways of doing things, they can think around, which can empower them,

464
00:43:40,100 --> 00:43:45,140
especially in the more learning context, although the temporal dimension has to be more carefully

465
00:43:45,140 --> 00:43:51,780
taken into account. So these were the four challenges that I wanted to

466
00:43:51,780 --> 00:43:57,140
share today. And to kind of wrap up, I just wanted to pose two questions moving forward

467
00:43:58,020 --> 00:44:05,300
from the interaction-centric perspective as HCI researchers. So first is, how might we design

468
00:44:05,300 --> 00:44:12,820
these building blocks and interface affordances for new and upcoming AI models? Okay, so I think

469
00:44:12,900 --> 00:44:18,180
part of it is that instead of building these point solutions, I think we need to think about,

470
00:44:18,180 --> 00:44:23,700
are there any sort of generalizable frameworks, libraries, widgets, or interface affordances

471
00:44:24,340 --> 00:44:29,860
that we could come up with as a community that is really good at these kinds of things?

472
00:44:30,580 --> 00:44:36,420
And the second question is, does AI really require us to have these new things? I mean,

473
00:44:36,420 --> 00:44:42,020
can we just use existing design elements and frameworks to build AI applications?

474
00:44:42,820 --> 00:44:50,260
And I tend to think that we might need something new for these new and upcoming AI models,

475
00:44:50,260 --> 00:44:55,460
especially because they have these very different characteristics than the conventional systems

476
00:44:55,460 --> 00:45:01,060
that we have been building. They're more probabilistic, harder to predict, more black box in nature,

477
00:45:01,620 --> 00:45:07,700
yet seemingly more impactful and powerful in terms of what they do, hallucinating. All these

478
00:45:07,700 --> 00:45:13,460
properties packed together, I think we might really need to think about, what are the types

479
00:45:13,460 --> 00:45:20,340
of interaction affordances that are really built for supporting the usability of these

480
00:45:20,340 --> 00:45:28,660
AI-powered applications? So in this, I think as a community, we are making all these great

481
00:45:28,660 --> 00:45:33,300
advances, like making different types of contributions. And I tend to focus on more

482
00:45:33,860 --> 00:45:39,700
interactive systems and techniques, whereas other people focus on introducing new design

483
00:45:39,700 --> 00:45:44,820
processes and understandings. And I think all this work is needed. And some of the interesting

484
00:45:44,820 --> 00:45:51,860
examples of adding an interaction layer to these new types of models is in this example,

485
00:45:51,860 --> 00:45:58,260
Tailbrush, where the user can draw the level of fortune that they won in the character to have

486
00:45:58,260 --> 00:46:08,100
when they use generative models to generate a story. Or this AI chains work, which presents these

487
00:46:08,100 --> 00:46:15,620
primitives and workflows for putting together this workflow that can accomplish more complex tasks

488
00:46:15,620 --> 00:46:23,540
with these LLM prompts that a single prompt cannot really perform. And in my research group,

489
00:46:23,540 --> 00:46:29,220
with my PhD student, Tesu Kim, we have been investigating this idea of what would be more

490
00:46:29,220 --> 00:46:34,820
generalizable design framework. And thinking about input, model, and output, we have been

491
00:46:34,820 --> 00:46:41,860
thinking about the concepts of cells, generators, and lenses, and tried to introduce this standardized

492
00:46:41,860 --> 00:46:47,300
libraries and widgets that people can easily adopt in their AI applications. So for example,

493
00:46:47,300 --> 00:46:54,100
using this kind of framework, people can build a copywriting app, email app, or story writing

494
00:46:54,100 --> 00:46:59,860
app using pretty much the same kind of framework, which can save people's time while supporting

495
00:46:59,860 --> 00:47:05,220
the types of interactions like iterations and comparison and experimenting different outputs.

496
00:47:07,540 --> 00:47:12,980
And the second question, and the final question that I want to ask today is, how might we as an

497
00:47:12,980 --> 00:47:18,980
HCI community collaborate better with the AI community on these various things? And it was also

498
00:47:18,980 --> 00:47:25,140
the discussion that I was having a lot with today's meetings, and also with various AI researchers,

499
00:47:25,140 --> 00:47:30,660
especially in Europe. And in terms of community collaboration, of course, one of the important

500
00:47:30,660 --> 00:47:35,780
things is metrics. And there was also a great discussion at the HCI conference a couple weeks

501
00:47:35,780 --> 00:47:42,900
ago, hosted here at Stanford. And in the AI community, it cares a lot about model performance

502
00:47:42,900 --> 00:47:48,260
and generalization errors, where in HCI, we tend to focus on the human experience. So how do we

503
00:47:48,260 --> 00:47:54,900
really bridge the gap between the metrics? And what it means to do AI research with more human

504
00:47:54,900 --> 00:48:00,260
side metrics incorporated? What's the incentive for people to do that? And how do we encourage

505
00:48:00,260 --> 00:48:06,980
poor AI people to use these metrics, too? In terms of human input design, a lot of the comments

506
00:48:06,980 --> 00:48:12,500
that I was getting in terms of interaction-centric AI from AI researchers is that these ideas are

507
00:48:12,500 --> 00:48:18,740
great, but then I don't really know how to actually take action about it. And part of it is, in their

508
00:48:18,740 --> 00:48:25,540
model-building kind of work, how can I incorporate human feedback? And how do I use it in a meaningful

509
00:48:25,540 --> 00:48:31,220
way to really change the way the model actually works, rather than just getting more high-level

510
00:48:31,220 --> 00:48:40,500
design guidance? So one great direction for this might be, think about more making human feedback,

511
00:48:40,500 --> 00:48:44,980
more computationally feasible, so that this compatibility is actually satisfied.

512
00:48:46,500 --> 00:48:50,820
And lastly, we need to think about the change in design process as well. And in a lot of,

513
00:48:51,540 --> 00:48:57,460
this is Stanford D-School's user-centered design cycle. And I think in a lot of the AI research,

514
00:48:57,460 --> 00:49:02,500
what we're seeing is this prototype test kind of culture. You try something new, test it,

515
00:49:02,500 --> 00:49:08,820
iteratively improve it. But then one of the frustrations is that interaction often comes

516
00:49:08,820 --> 00:49:15,460
too late, right? There's this new cool model, and can you build an UI on top of it, is sort of the

517
00:49:15,540 --> 00:49:20,820
kind of discourse we get a lot. And I think interaction should not just be like an icing

518
00:49:20,820 --> 00:49:26,660
on the cake, but really something that can guide the entire design process or help people determine,

519
00:49:26,660 --> 00:49:32,260
is this the right problem to tackle in the first place? Or what kind of interaction should we try

520
00:49:32,260 --> 00:49:37,620
to support with AI? And based on that, think of what AI should do and should not do and how much

521
00:49:37,620 --> 00:49:44,980
AI should be used in a particular context. So that's all I wanted to share. And here's a summary

522
00:49:44,980 --> 00:49:49,140
of what I mentioned today, and I'd be happy to take any questions. Thank you.

523
00:49:54,660 --> 00:49:59,220
All right, so I'll check my recommendations of facilitating messages. If there are any more ideas.

524
00:50:01,140 --> 00:50:05,460
No? What do you think?

525
00:50:06,100 --> 00:50:09,620
It really sounded like an AI.

526
00:50:11,220 --> 00:50:21,060
I'll just click them all. I want to pull the mic on. I want to pull the thread a little bit on

527
00:50:21,060 --> 00:50:28,900
this notion of how to connect human feedback with the objective functions that you touched on near

528
00:50:28,900 --> 00:50:33,700
the end, because that's been rattling around in my head in much of the talk that you're giving,

529
00:50:33,700 --> 00:50:37,380
that if I think about what should AI researchers be doing differently,

530
00:50:38,980 --> 00:50:44,180
then you're asking, well, what's the proper model of the person in their system?

531
00:50:45,460 --> 00:50:49,300
And traditionally, the problem has been that human interaction is really expensive,

532
00:50:50,260 --> 00:50:54,420
just to collect annotated data. Or once you have it to be able to tune the model,

533
00:50:56,260 --> 00:51:02,500
you don't get that much of it. And so they often fall back on self-supervision, or as you've been

534
00:51:02,500 --> 00:51:08,420
talking about in the value alignment, they train an RL model to mimic a human and then let that go

535
00:51:08,420 --> 00:51:14,020
loose. And it seems like until, I think they're kind of, I want you to take a position on one of

536
00:51:14,020 --> 00:51:19,380
the two positions. One either is to say, look, we need to find strategies like that where we can

537
00:51:19,380 --> 00:51:26,900
create proxy humans, and that's how we hook into the objective functions, the loss functions, etc.

538
00:51:26,900 --> 00:51:32,100
The other alternative would be to say, no, we're going to find some other way to actually

539
00:51:32,100 --> 00:51:40,020
make human feedback at a scale and in a form that they can directly use in the models. I'm

540
00:51:40,020 --> 00:51:44,260
just curious, like, if you want to take a bet, where's your bet on that? Where should we be heading?

541
00:51:45,460 --> 00:51:51,700
Yeah, that's an excellent question. I would say, I mean, you asked me to take a position,

542
00:51:51,700 --> 00:51:58,580
but I would say both will be prevalent. And I like the letter much more. And I think that's more

543
00:51:58,580 --> 00:52:03,540
promising and sustainable. And for example, the reason I'm really interested in this, like,

544
00:52:03,540 --> 00:52:09,620
co-learning feedback loop between the human and the machine is that, you know, even if this super

545
00:52:09,620 --> 00:52:16,020
advanced AI comes along and let's say it presents this, like, super accurate explanations, people's

546
00:52:16,020 --> 00:52:21,780
self-explanation activity is still meaningful, right? Because that's how they could learn.

547
00:52:21,780 --> 00:52:27,620
And so I feel like, you know, we can really try to find these compatible

548
00:52:29,700 --> 00:52:35,940
mechanisms in which the human can get the benefit and get the incentive for doing what

549
00:52:35,940 --> 00:52:40,260
they are really good at and what is helpful for them, not necessarily trying to help the system

550
00:52:40,260 --> 00:52:46,340
or, you know, getting paid to system, paid to support the system per se. And at the same time,

551
00:52:46,340 --> 00:52:50,900
the system can use it for something meaningful. And on the system side, I think in the system,

552
00:52:50,900 --> 00:52:57,860
like, access that I presented, I was really happy when we landed at this technical solution where

553
00:52:57,860 --> 00:53:05,860
people's rating data could be almost directly piped into the feedback for the RL agent to kind

554
00:53:05,860 --> 00:53:11,300
of use as meaningful feedback. So I think that's just one example where this kind of worked out

555
00:53:11,300 --> 00:53:16,900
for this kind of context. And I think we need to really investigate more and think about, are there

556
00:53:17,460 --> 00:53:22,740
any generalizable mechanisms that this kind of approach could work in different contexts?

557
00:53:22,740 --> 00:53:27,380
This assumes that you have a large set of users you can draw on, like there are learners that

558
00:53:27,380 --> 00:53:32,100
are coming through your system. If I'm early on in the pipeline and I just kind of have V0,

559
00:53:32,100 --> 00:53:35,220
I don't have the users yet, are there strategies you would recommend?

560
00:53:36,020 --> 00:53:40,740
Yeah, yeah, excellent. So in that same access system, for instance, what we did was to

561
00:53:41,620 --> 00:53:49,780
insert the instructor-generated explanations as sort of the initial seed. And I was also imagining

562
00:53:49,780 --> 00:53:56,260
maybe using LLMs, for instance, we can plug in AI-generated ones to kind of avoid the cold

563
00:53:56,260 --> 00:54:02,340
start problem. And it would be interesting to see how, you know, in the same system, like AI-generated

564
00:54:02,340 --> 00:54:07,860
ones, instructor-generated ones, and learner-generated ones can kind of compete against each other

565
00:54:07,860 --> 00:54:11,780
until the system ultimately just focuses on what is best for learners.

566
00:54:12,820 --> 00:54:19,060
This is kind of a two-part question, going back to the like third challenge or like project you

567
00:54:19,060 --> 00:54:25,380
talked about, where there was that note about AI as proxy, like people kind of using that as

568
00:54:26,100 --> 00:54:30,740
like an excuse to make points, where maybe they wanted to do something but didn't want it to come

569
00:54:30,740 --> 00:54:38,180
off as them. So the first part of the question is like, in that case, did people want to,

570
00:54:39,140 --> 00:54:43,700
later it says people wanted the message to kind of sound like them, but in the case of the AI

571
00:54:43,700 --> 00:54:49,780
as proxy, did they want that to sound like them? Or were they wanting it to sound more artificial?

572
00:54:49,780 --> 00:54:55,940
And then second part of the question is, do you think there are more situations than just this

573
00:54:56,020 --> 00:55:02,500
where maybe we don't want the AI to feel super personable and maybe want the interaction to

574
00:55:02,500 --> 00:55:07,620
feel slightly more kind of mechanical or unnatural? Yeah, that's an excellent question. And I would

575
00:55:07,620 --> 00:55:14,900
say these were somewhat different use cases, and both I think are valuable and smite. And

576
00:55:16,340 --> 00:55:21,860
that again, I think in a more model-centric approach, we also kind of focus on trying to

577
00:55:22,660 --> 00:55:28,500
create these messages that are more like humans. And that could be effective in certain cases,

578
00:55:28,500 --> 00:55:34,260
but as you said, that might not really be what the users want, because in a proxy kind of setting,

579
00:55:34,260 --> 00:55:40,340
you might not actually want it to sound too personalized, because maybe the more canned

580
00:55:40,340 --> 00:55:48,980
message might actually work better in that context. And vice versa. So I think just being able to

581
00:55:48,980 --> 00:55:54,420
identify all these different needs that people have and expectations that people have and being

582
00:55:54,420 --> 00:56:00,820
able to somewhat fluidly support those, I think was really an interesting kind of observation

583
00:56:00,820 --> 00:56:06,580
that we had. And I think moving forward, one of the lessons was that this more personalizable

584
00:56:07,860 --> 00:56:12,100
message generation could be an interesting technology that could be potentially integrated,

585
00:56:12,100 --> 00:56:17,140
but that's not going to solve everything, because there are these other types of needs that will

586
00:56:17,140 --> 00:56:23,780
not be supported, even with the perfect personalizable style transfer. So yeah.

587
00:56:26,020 --> 00:56:31,780
Explaining stuff, I kept thinking about how what you described and sort of the challenges

588
00:56:31,780 --> 00:56:38,180
that we see with this new deep networks and models and how we interact with them are

589
00:56:38,820 --> 00:56:44,260
similar to how people used to interact with search engines, right? At the beginning, people were

590
00:56:44,340 --> 00:56:51,140
not as good as sort of figuring out how to query the search engine right. And over time, both

591
00:56:52,420 --> 00:56:57,220
we became better at querying the search engines, and then the search engines became better at

592
00:56:57,220 --> 00:57:03,540
sort of understanding how to interpret user queries. Do you see any similarities there? Is

593
00:57:03,540 --> 00:57:10,900
there something that's very unique to the challenges we face with this new models? Or is it just that

594
00:57:10,980 --> 00:57:17,060
we haven't had enough time to sort of adopt to each other in a way? Yeah.

595
00:57:17,060 --> 00:57:24,100
Excellent. Yeah. And I think it's a recurring theme as these new technologies come in. Initially,

596
00:57:24,100 --> 00:57:29,060
people would kind of struggle and they would need to learn how it actually works through trial and

597
00:57:29,060 --> 00:57:34,420
error and lots of like failed attempts. And that's what we're seeing with these like

598
00:57:34,420 --> 00:57:38,980
chat GPT, for instance, a lot of people are trying things out, reporting success and failure cases.

599
00:57:39,940 --> 00:57:46,660
So I do think there are certain similarities. What's more unique about what we're seeing right

600
00:57:46,660 --> 00:57:53,940
now is that due to the nature of like how black box, complex, unpredictable these models are,

601
00:57:53,940 --> 00:58:00,980
I think it just confuses people much more. And there's a question of, you know, is this really

602
00:58:00,980 --> 00:58:08,020
like a human learning problem to begin with, right? So if people take, do it more, and you know,

603
00:58:08,020 --> 00:58:13,220
if they had more time, will people be actually able to really get to a point where they could

604
00:58:13,220 --> 00:58:18,020
really easily create something that they like? Probably not. Right? So that's why I think we

605
00:58:18,020 --> 00:58:24,180
need both on the model side to kind of think about what are more interactable and learnable ways of,

606
00:58:24,180 --> 00:58:29,140
you know, architecting this kind of models in the first place. And also from the HCI point of view,

607
00:58:29,140 --> 00:58:35,060
what are these interaction mechanisms that could be added to these models in a way that

608
00:58:35,060 --> 00:58:40,180
it is actually more understandable and usable on the user side? Yeah. Thanks so much. Yeah.

609
00:58:41,140 --> 00:58:44,340
I think we're at about the time, but Duho will be here for a couple minutes after the talk for

610
00:58:44,340 --> 00:58:48,180
further questions. So let's thank him for speaking. Thank you.

