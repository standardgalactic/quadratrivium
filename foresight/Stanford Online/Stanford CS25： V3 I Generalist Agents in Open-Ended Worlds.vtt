WEBVTT

00:00.000 --> 00:09.640
Hi everyone, thanks for coming to our CS35 lecture today.

00:09.640 --> 00:14.240
So today we're honored to have Jim Van from NVIDIA, who we're talking about generalist

00:14.240 --> 00:21.080
agents in open-ended worlds, and he's a senior AI research scientist at NVIDIA, where his

00:21.080 --> 00:26.800
mission is to build generally capable AI agents with applications to gaming, robotics, and

00:26.800 --> 00:29.000
software automation.

00:29.000 --> 00:33.480
He has research spans, foundation models, multi-modal AI, reinforcement learning, and

00:33.480 --> 00:34.800
open-ended learning.

00:34.800 --> 00:41.360
He obtained his PhD degree in computer science from here, Stanford, advised by Professor

00:41.360 --> 00:42.360
Pepe Lee.

00:42.360 --> 00:49.080
And previously, he did research internships at OpenAI, Google AI, as well as Mila Quebec

00:49.080 --> 00:57.760
AI Institute, so yeah, we'll give it up for Jim.

00:57.760 --> 01:02.440
Yeah, thanks for having me.

01:02.440 --> 01:06.880
So I want to start with a story of two kittens.

01:06.880 --> 01:12.320
It's a story that gave me a lot of inspiration over the career, over my career.

01:12.320 --> 01:14.600
So I want to share this one first.

01:14.600 --> 01:19.520
Back in 1963, there were two scientists from MIT held in Hine.

01:19.520 --> 01:25.720
They did this ingenious experiment, where they put two newborn kittens in this device,

01:25.720 --> 01:28.160
and the kittens have not seen the visual world yet.

01:28.160 --> 01:33.120
So it's kind of like a merry-go-round, where the two kittens are linked by a rigid mechanical

01:33.120 --> 01:37.040
bar, so their movements are exactly mirrored.

01:37.040 --> 01:41.040
And there's an active kitten on the right-hand side, and that's the only one able to move

01:41.040 --> 01:46.880
freely, and then transmit the motion over this link to the passive kitten, which is

01:46.880 --> 01:51.760
confined to the basket, and cannot really control its own movements.

01:51.760 --> 01:57.280
And then after a couple of days, held in Hine, kind of take the kittens out of this merry-go-round,

01:57.280 --> 01:59.440
and then did visual testing on them.

01:59.440 --> 02:03.560
And they found that only the active kitten was able to develop a healthy visual motor

02:03.560 --> 02:09.520
loop, like responding correctly to approaching objects or visual cliffs, but the passive

02:09.520 --> 02:13.200
kitten did not have a healthy visual system.

02:13.200 --> 02:19.960
So I find this experiment fascinating, because it shows the importance of having this embodied

02:19.960 --> 02:26.400
active experience to really ground a system of intelligence.

02:26.400 --> 02:30.440
And let's put this experiment in today's AI context right.

02:30.440 --> 02:35.800
We actually have a very powerful passive kitten, and that is ChargerBT.

02:35.800 --> 02:41.880
It passively observes and rehearses the text on the internet, and it doesn't have any embodiment.

02:41.880 --> 02:46.920
And because of this, its knowledge is kind of abstract and ungrounded, and that partially

02:46.920 --> 02:51.760
contributes to the fact that ChargerBT hallucinates things that are just incompatible with our

02:51.760 --> 02:55.480
common sense and our physical experience.

02:55.480 --> 03:01.840
And I believe the future belongs to active kittens, which translates to generalist agents.

03:01.840 --> 03:06.640
They are the decision makers in a constant feedback loop, and they're embodied in this

03:06.640 --> 03:08.120
fully immersive world.

03:08.120 --> 03:12.280
They're also not mutually exclusive with the passive kitten.

03:12.280 --> 03:18.360
And in fact, I see the active embodiment part as a layer on top of the passive pre-training

03:18.360 --> 03:22.560
from lots and lots of internet data.

03:22.560 --> 03:25.000
So are we there yet?

03:25.000 --> 03:27.640
Have we achieved generalist agent?

03:27.640 --> 03:31.760
Back in 2016, I remember it was like spring of 2016.

03:31.760 --> 03:35.440
I was sitting in an undergraduate class at Columbia University, but I wasn't paying attention

03:35.440 --> 03:36.720
to the lecture.

03:36.720 --> 03:43.720
I was watching a board game tournament on my laptop, and this screenshot was the moment

03:43.720 --> 03:49.560
when AlphaGo versus LisaDoll and AlphaGo won three matches out of five and became the

03:49.560 --> 03:52.800
first ever to be the human champion at the game of Go.

03:52.800 --> 03:54.600
I remember the adrenaline that day, right?

03:54.600 --> 03:56.080
I've seen history unfold.

03:56.520 --> 04:00.960
Oh my God, we're finally getting to AGI, and everyone's so excited.

04:00.960 --> 04:05.440
And I think that was the moment when AI agents entered the mainstream.

04:05.440 --> 04:14.920
But when the excitement fades, I felt that even though AlphaGo was so mighty and so great,

04:14.920 --> 04:18.480
it could only do one thing and one thing alone.

04:18.480 --> 04:25.560
And afterwards, in 2019, there were more impressive achievements, like Open AI 5 beating the human

04:25.560 --> 04:30.960
champions at the game of Dota and AlphaStar from DeepMind beat StarCraft.

04:30.960 --> 04:37.200
But all of these, with AlphaGo, they all have a single kind of theme, and that is to beat

04:37.200 --> 04:38.200
the opponent.

04:38.200 --> 04:45.200
There is this one objective that the agent needs to do, and the models trained on Dota

04:45.200 --> 04:48.760
or Go cannot generalize to any other tasks.

04:48.760 --> 04:52.760
It cannot even play other games like Super Mario or Minecraft.

04:52.760 --> 04:59.880
The world is fixed and has very little room for open-ended creativity and exploration.

04:59.880 --> 05:04.800
So I argued that a journalist agent should have the following essential properties.

05:04.800 --> 05:10.360
First, it should be able to pursue very complex, cementally rich and open-world objectives.

05:10.360 --> 05:14.800
Basically, you explain what you want in natural language, and the agent should perform the

05:14.800 --> 05:17.200
actions for you in a dynamic world.

05:17.200 --> 05:22.120
And second, the agent should have a large amount of pre-trained knowledge instead of

05:22.120 --> 05:27.600
knowing only a few concepts that's extremely specific to the task.

05:27.600 --> 05:29.920
And third, massively multitask.

05:29.920 --> 05:35.680
A journalist agent, as the name implies, needs to do more than just a couple of things.

05:35.680 --> 05:42.240
It should be, in the best case, infinitely multitask, as expressive as human language

05:42.240 --> 05:44.440
can dictate.

05:44.440 --> 05:46.200
So what does it take?

05:46.200 --> 05:49.120
Correspondingly, we need three main ingredients.

05:49.600 --> 05:51.960
First is the environment.

05:51.960 --> 05:58.320
The environment used to be open-ended enough because the agent's capability is upper-bounded

05:58.320 --> 06:01.240
by the environment complexity.

06:01.240 --> 06:05.680
And I'd argue that Earth is actually a perfect example because it's so open-ended, this world

06:05.680 --> 06:10.960
we live in, that it allows an algorithm called natural evolution to produce all the diverse

06:10.960 --> 06:14.400
forms and behaviors of life on this planet.

06:14.400 --> 06:19.760
So can we have a simulator that is essentially a low-fi Earth, but we can still run it on

06:19.760 --> 06:23.360
the lab clusters?

06:23.360 --> 06:27.760
And second, we need to provide the agent with massive pre-training data because exploration

06:27.760 --> 06:31.800
in an open-ended world from scratch is just intractable.

06:31.800 --> 06:34.280
And the data will serve at least two purposes.

06:34.280 --> 06:37.520
One as a reference manual on how to do things.

06:37.520 --> 06:42.680
And second, as a guidance on what are the interesting things worth pursuing.

06:43.080 --> 06:48.800
GPT is only, at least up to GPT-4, it only learns from pure text on the web.

06:48.800 --> 06:55.600
But can we provide the agent with much richer data, such as video walkthrough, or like multimedia,

06:55.600 --> 07:00.600
Wiki documents, and other media forms?

07:00.600 --> 07:06.000
And finally, once we have the environment and the database, we are ready to train foundation

07:06.000 --> 07:08.320
models for the agents.

07:08.320 --> 07:13.320
And it should be flexible enough to pursue the open-ended tasks without any task specific

07:13.320 --> 07:20.960
assumptions, and also scalable enough to compress all of the multi-modal data that I just described.

07:20.960 --> 07:24.880
And here language, I argue, will play at least two key roles.

07:24.880 --> 07:30.960
One is as a simple interface to communicate a task, to communicate the human intentions

07:30.960 --> 07:38.040
to the agent, and second as a bridge to ground all of the multi-modal concepts and signals.

07:38.040 --> 07:45.480
And that train of thought landed us in Minecraft, the best-selling video game of all time.

07:45.480 --> 07:51.640
And for those who are unfamiliar, Minecraft is a procedurally generated 3D voxel world,

07:51.640 --> 07:55.480
and in the game you can basically do whatever your heart desires.

07:55.480 --> 08:01.720
And what's so special about the game is that unlike AlphaGo, StarCraft, or Dota, Minecraft

08:01.720 --> 08:07.880
defines no particular objective to maximize, no particular opponent to beat, and doesn't

08:07.880 --> 08:09.960
even have a fixed storyline.

08:09.960 --> 08:13.760
And that makes it very well-suited as a truly open-ended AM playground.

08:13.760 --> 08:18.160
And here we see people doing extremely impressive things in Minecraft.

08:18.160 --> 08:24.840
Like this is a YouTube video where a gamer built the entire Hogwarts castle block-by-block

08:24.840 --> 08:28.200
by hand in the game.

08:28.200 --> 08:32.120
And here's another example of someone just digging a big hole in the ground and then

08:32.120 --> 08:36.680
making this beautiful underground temple with a river nearby.

08:36.680 --> 08:40.120
It's all crafted by hand.

08:40.120 --> 08:41.120
And one more.

08:41.120 --> 08:46.560
This is someone building a functioning CPU circuit inside a game because there is something

08:46.560 --> 08:53.400
called Redstone in Minecraft that you can build circuits out of it, like logical gates.

08:53.400 --> 08:55.360
And actually the game is too incomplete.

08:55.360 --> 08:58.400
You can, you know, simulate a computer inside a game.

08:58.400 --> 09:00.600
Just think about how crazy that is.

09:00.600 --> 09:06.400
And here I want to highlight a number that is 140 million active players.

09:06.400 --> 09:11.360
And just to quote this numbering perspective, this is more than twice the population of

09:11.360 --> 09:13.240
the UK.

09:13.240 --> 09:17.680
And that is the amount of people playing Minecraft on a daily basis.

09:17.680 --> 09:22.800
And you know, it just so happens that gamers are generally happier than PhDs.

09:22.800 --> 09:26.640
So they love to stream and share what they're doing.

09:26.640 --> 09:30.640
And that produces a huge amount of data every day online.

09:30.640 --> 09:35.720
And there's this treasure trove of learning materials that we can tap into for training

09:35.720 --> 09:36.720
generalizations.

09:36.720 --> 09:41.680
You know, remember the data is the key for foundation models.

09:41.680 --> 09:48.600
So we introduce MindDojo, a new open framework to help the community develop generally capable

09:48.600 --> 09:55.440
agents using Minecraft as a kind of primordial soup.

09:55.440 --> 10:01.360
MindDojo features three major parts, an open-ended environment, an international knowledge base,

10:01.360 --> 10:06.560
and then a generalized agent developed with a simulator and a massive data.

10:06.560 --> 10:09.080
So let's zoom in the first one.

10:09.080 --> 10:15.280
Here's a sample gallery of the interesting things that you can do with MindDojo's API.

10:15.280 --> 10:19.440
We feature a massive benchmark suite of more than 3,000 tasks.

10:19.440 --> 10:25.560
And this is by far the largest open source agent benchmark to our knowledge.

10:25.560 --> 10:29.720
And we implement a very versatile API that unlocks the full potential of the game.

10:29.720 --> 10:36.960
Like for example, MindDojo supports multi-modal observation for action space, like moving

10:36.960 --> 10:39.840
or attack or inventory management.

10:39.840 --> 10:46.640
And that can be customized at every detail, like you can tweak the terrain, the weather,

10:46.640 --> 10:52.680
block placement, monster spawning, and just anything you want to customize in the game.

10:52.680 --> 10:58.920
And given this simulator, we introduce around 1,500 programmatic tasks, which are tasks

10:58.960 --> 11:03.240
that have ground true success conditions defined in Python code.

11:03.240 --> 11:08.160
And you can also explicitly write down spars or the best reward functions using this API.

11:08.160 --> 11:12.840
And some examples are harvesting different resources and unlocking the tech tree

11:12.840 --> 11:16.600
or fighting various monsters and getting reward.

11:16.600 --> 11:21.480
And all these tasks come with language prompts that are templated.

11:21.480 --> 11:26.560
Next, we also introduce 1,500 creative tasks that are freeform and open-ended.

11:26.560 --> 11:30.640
And that is in contrast to the programmatic tasks I just mentioned.

11:30.640 --> 11:35.280
So for example, let's say we want the agent to build a house.

11:35.280 --> 11:37.920
But what makes a house a house, right?

11:37.920 --> 11:40.480
It is L defined and just like image generation.

11:40.480 --> 11:45.200
You don't know if it generates a cat correctly or not.

11:45.200 --> 11:50.760
So it's very difficult to use simple Python programs to give these kind of tasks reward functions.

11:50.760 --> 11:56.360
And the best way is to use foundation models trained on Internet skill knowledge.

11:56.360 --> 12:04.360
So that the model itself understands abstract concepts like, you know, the concept of a house.

12:04.360 --> 12:08.280
And finally, there's one task that holds a very special status called play suit,

12:08.280 --> 12:12.520
which is to beat the final boss of Minecraft, the ender dragon.

12:12.520 --> 12:14.560
So Minecraft doesn't force you to do this task.

12:14.560 --> 12:17.120
As we said, it doesn't have a fixed storyline.

12:17.120 --> 12:23.120
But it's still considered a really big milestone for any kind of beginner human players.

12:23.120 --> 12:28.240
I want to highlight it is an extremely difficult task that requires very complex preparation,

12:28.240 --> 12:30.920
exploration, and also martial skills.

12:30.920 --> 12:36.800
And for an average human, it will take many hours or even days to solve.

12:36.800 --> 12:40.440
Easily over like one million action steps in a single episode.

12:40.440 --> 12:45.600
And that would be the longest benchmarking task for policy learning ever created here.

12:45.600 --> 12:49.280
So I admit, I am personally a below average human.

12:49.280 --> 12:51.960
I was never able to beat the ender dragon.

12:51.960 --> 12:59.160
And my friends laugh at me, and I'm like, OK, one day my AI will avenge my poor skills.

12:59.160 --> 13:03.200
That was one of the motivations for this project.

13:03.200 --> 13:09.280
Now, let's move on to the second ingredient, the Internet skill knowledge base part of my module.

13:09.280 --> 13:13.120
We offer three datasets here, the YouTube, Wiki, and Reddit.

13:13.120 --> 13:20.800
And combined, they are the largest open-ended agent behavior database ever compiled to our knowledge.

13:20.840 --> 13:22.280
The first is YouTube.

13:22.280 --> 13:27.640
And we already said Minecraft is one of the most streamed games on YouTube.

13:27.640 --> 13:30.600
And gamers love to narrate what they are doing.

13:30.600 --> 13:37.640
So we collected more than 700,000 videos with two billion words in the corresponding transfers.

13:37.640 --> 13:43.240
And these transfers will help the agent learn about human strategies and creativities

13:43.240 --> 13:48.120
without us manually labeling things.

13:48.160 --> 13:56.680
And second, the Minecraft player base is so crazy that they have compiled a huge Minecraft-specific Wikipedia

13:56.680 --> 14:02.320
that basically explains everything you ever need to know in every version of the game.

14:02.320 --> 14:03.360
It's crazy.

14:03.360 --> 14:10.600
And we scraped 7,000 Wikipedia pages with interleaving, multi-modal data, like images, tables, and diagrams.

14:10.600 --> 14:12.480
And here are some screenshots.

14:12.480 --> 14:17.520
Like, this is a gallery of all of the monsters and their corresponding behaviors,

14:17.560 --> 14:20.200
like spawn and attack patterns.

14:20.200 --> 14:26.400
And also, the thousands of crafting recipes are all present on the Wiki, and we scraped all of them.

14:26.400 --> 14:29.680
And more complex diagrams and tables and embedded figures.

14:29.680 --> 14:32.400
Now we have something like GPT-4V.

14:32.400 --> 14:36.880
It may be able to understand many of these diagrams.

14:36.880 --> 14:43.960
And finally, the Minecraft subreddit is one of the most active forums across the entire Reddit.

14:43.960 --> 14:47.800
And players showcase their creations and also ask questions for help.

14:47.800 --> 14:52.520
So we scraped more than 300,000 posts from Minecraft Reddit.

14:52.520 --> 14:59.480
And here are some examples of how people use the Reddit as a kind of stack overflow for Minecraft.

14:59.480 --> 15:02.920
And we can see that some of the top-golded answers are actually quite good.

15:02.920 --> 15:06.480
Like someone is asking, oh, why doesn't my wheat farm grow?

15:06.480 --> 15:09.280
And the answer says you need to light up the room with more torches.

15:09.280 --> 15:12.560
You don't have enough lighting.

15:12.560 --> 15:16.040
Now, given the massive task suite and internet data,

15:16.040 --> 15:21.480
we have the essential components to build a journalist's agents.

15:21.480 --> 15:26.120
So in the first mind-dozer paper, we introduce a foundation model called Minecraft.

15:26.120 --> 15:27.840
And the idea is very simple.

15:27.840 --> 15:30.280
I can explain in three slides.

15:30.280 --> 15:35.800
Basically, for our YouTube database, we have time-aligned videos and transfers.

15:35.800 --> 15:39.840
And these are actually the real tutorial videos from our dataset.

15:39.840 --> 15:44.320
You see on the third clip, as I raise my axe in front of this pig,

15:44.320 --> 15:47.600
there's only one thing that you know is going to happen.

15:47.600 --> 15:53.080
It's actually someone said this, a big YouTuber of Minecraft.

15:53.080 --> 15:58.960
And then, given this data, we train Minecraft in the same spirit as Open AI Club.

15:58.960 --> 16:03.560
So for those who are unfamiliar, Open AI Club is a contrastive model

16:03.560 --> 16:06.720
that learns the association between an image and its caption.

16:06.720 --> 16:09.000
And here, it's a very simple idea.

16:09.000 --> 16:12.440
By this time, it is a video text contrastive model.

16:12.440 --> 16:22.720
And we associate the text with a video snippet that runs about 8 to 16 seconds each.

16:22.720 --> 16:27.560
And intuitively, Minecraft learns the association between the video and the transcript

16:27.560 --> 16:30.880
that describes the activity in the video.

16:30.880 --> 16:33.800
And Minecraft outputs a score between 0 and 1,

16:33.800 --> 16:37.120
where 1 means a perfect correlation between the text and the video,

16:37.160 --> 16:41.360
and 0 means the text is irrelevant to the activity.

16:41.360 --> 16:47.240
So you see this is effectively a language-prompted foundation reward model

16:47.240 --> 16:51.560
that knows the nuances of things like forests, animal behaviors,

16:51.560 --> 16:55.000
and architectures in Minecraft.

16:55.000 --> 16:57.440
So how do we use Minecraft in action?

16:57.440 --> 17:01.440
Here's an example of our agent interacting with a simulator.

17:01.440 --> 17:05.200
And here, the task is to share sheep to obtain wool.

17:05.200 --> 17:11.120
And as the agent explores in the simulator, it generates a video snippet

17:11.120 --> 17:15.400
as a moving window, which can be encoded and fed into Minecraft,

17:15.400 --> 17:19.400
along with an encoding of the text prompt here.

17:19.400 --> 17:22.240
And Minecraft computes the association.

17:22.240 --> 17:26.720
The higher the association is, the more the agent's behavior in this video

17:26.720 --> 17:30.920
aligns with the language, which is a task you want it to do.

17:30.960 --> 17:35.640
And that becomes a reward function to any reinforcement learning algorithm.

17:35.640 --> 17:38.280
So this looks very familiar, right?

17:38.280 --> 17:46.240
Because it's essentially RL from human feedback, or ROHF in Minecraft.

17:46.240 --> 17:50.600
And ROHF was the cornerstone algorithm that made chatGBT possible.

17:50.600 --> 17:53.840
And I believe it will play a critical role in Jonas agents as well.

17:56.040 --> 17:58.800
I'll quickly gloss over some quantitative results.

17:58.800 --> 18:02.800
I promise there won't be many tables of numbers here.

18:02.800 --> 18:08.120
For these eight tasks, we show the percentage success rate over 200 test episodes.

18:08.120 --> 18:12.480
And here, in the green circle, is two variants of our Minecraft method.

18:12.480 --> 18:14.840
And in the orange circles are the baselines.

18:15.920 --> 18:21.200
So I highlight one baseline, which is that we construct a dance reward function

18:21.200 --> 18:26.320
manually for each task using the Mindoge API, it's a Python API.

18:26.320 --> 18:31.040
And you can consider this column as a kind of oracle, the upper bound of the performance,

18:31.040 --> 18:34.720
because we put a lot of human efforts into designing these reward functions

18:34.720 --> 18:35.680
just for the tasks.

18:37.280 --> 18:42.160
And we can see that Minecraft is able to match the quality of many of these,

18:42.160 --> 18:46.000
not all of them, but many of these manual engineering rewards.

18:46.000 --> 18:49.640
It is important to highlight that Minecraft is open vocabulary.

18:49.640 --> 18:53.680
So we use a single model for all of these tasks instead of one model for each.

18:53.680 --> 18:58.400
And we simply prompt the reward model with different tasks.

18:58.400 --> 18:59.560
And that's the only variation.

19:03.360 --> 19:07.200
One major feature of Foundation Model is strong generalization out of box.

19:07.200 --> 19:12.600
So can our agent generalize to dramatic changes in the visual appearance?

19:12.600 --> 19:17.840
So we did this experiment where during training, we only train our agents on

19:17.840 --> 19:21.760
a default terrain at noon on a sunny day.

19:21.760 --> 19:25.400
But we tested zero shot in a diverse range of terrains,

19:25.400 --> 19:29.480
whether it's in daylight cycles, and you can customize everything in Mindoge.

19:29.480 --> 19:33.320
And in our paper, we have numbers showing that Minecraft significantly beats

19:33.320 --> 19:37.480
an off-the-shelf visual encoder when facing these kind of distribution shift

19:37.480 --> 19:38.960
out of box.

19:38.960 --> 19:40.200
And this is no surprise, right?

19:40.200 --> 19:44.000
Because Minecraft was trained on hundreds of thousands of clips

19:44.000 --> 19:50.160
from Minecraft videos on YouTube, which have a very good coverage of all the scenarios.

19:51.120 --> 19:56.240
And I think that is just a testament to the big advantage of using

19:56.240 --> 20:00.040
international data because you get robustness out of box.

20:01.440 --> 20:05.840
And here are some demos of our learned agent behaviors on various tasks.

20:05.840 --> 20:12.120
So you may notice that these tasks are relatively short, around 100 to 500 time steps.

20:12.120 --> 20:18.240
And that is because Minecraft is not able to plan over very long time horizons.

20:18.400 --> 20:22.280
And it is an inherent limitation in the training pipeline

20:22.280 --> 20:25.440
because we could only use 8 to 16 seconds of the video,

20:25.440 --> 20:28.560
so it's constrained to short actions.

20:28.560 --> 20:32.880
But our hope is to build an agent that can explore and make new discoveries

20:32.880 --> 20:36.240
autonomously, just all by itself, and it keeps going.

20:36.240 --> 20:39.920
And in 2022, this goal seems quite out of reach for us.

20:39.920 --> 20:43.200
Mindoge was June 2022.

20:43.200 --> 20:46.880
And this year, something happened, and that is G4.

20:46.880 --> 20:51.600
A language model that is so good at coding and long horizon planning,

20:51.600 --> 20:54.080
so we just cannot sit still, right?

20:54.080 --> 21:00.400
We built Voyager, the first large language model powered life on a learning agent.

21:00.400 --> 21:04.800
And when we said Voyager lose in Minecraft, we see that it just keeps going.

21:04.800 --> 21:09.840
And by the way, all these video snippets are from a single episode of Voyager.

21:09.840 --> 21:13.680
It's not from different episodes, it's a single one.

21:13.680 --> 21:18.000
And we see that Voyager is just able to keep exploring the terrains,

21:18.000 --> 21:22.320
mine all kinds of materials, fight monsters, craft hundreds of recipes,

21:22.320 --> 21:26.880
and unlock an ever-expanding tree of diverse skills.

21:26.880 --> 21:30.000
So how do we do this?

21:30.000 --> 21:32.400
If we want to use the full power of G4,

21:32.400 --> 21:35.360
a central question is how to stringify things,

21:35.360 --> 21:40.240
converting this 3D world into a textual representation.

21:40.240 --> 21:42.480
We need a magic box here.

21:42.480 --> 21:47.200
And thankfully, again, the crazy Minecraft community already built one for us,

21:47.200 --> 21:49.520
and it's been around for many years.

21:49.520 --> 21:52.960
It's called Mindflayer, a high-level JavaScript API

21:52.960 --> 21:57.120
that's actively maintained to work with any Minecraft version.

21:57.120 --> 22:01.600
And the beauty of Mindflayer is it has access to the game states

22:01.600 --> 22:05.760
surrounding the agent, like the nearby blocks, animals, and enemies.

22:05.760 --> 22:10.560
So we effectively have a ground truth perception module as textual input.

22:10.640 --> 22:14.480
At the same time, Mindflayer also supports action APIs

22:14.480 --> 22:18.320
that we can compose skills.

22:18.320 --> 22:20.560
And now that we can convert everything to text,

22:20.560 --> 22:24.400
we're ready to construct an agent on top of G4.

22:24.400 --> 22:27.040
So on a high level, there are three components.

22:27.040 --> 22:32.240
One is a coding module that writes JavaScript code to control the game bot,

22:32.240 --> 22:36.160
and it's the main module that generates the executable actions.

22:36.160 --> 22:40.960
And second, we have a code base to store the correctly written code

22:40.960 --> 22:44.560
and look it up in the future if the agent needs to record a skill.

22:44.560 --> 22:46.720
And in this way, we don't duplicate efforts,

22:46.720 --> 22:48.960
and whenever facing similar situations in the future,

22:48.960 --> 22:51.040
the agent knows what to do.

22:51.040 --> 22:56.000
And third, we have a curriculum that proposes what to do next,

22:56.000 --> 23:00.080
given the agent's current capabilities and also situation.

23:00.080 --> 23:04.160
And when you wire these components up together,

23:04.160 --> 23:06.720
you get a loop that drives the agent indefinitely

23:06.720 --> 23:09.920
and achieve something like lifelong learning.

23:09.920 --> 23:13.360
So let's zoom in the center module.

23:13.360 --> 23:16.800
We prompt GD4 with documentations and examples

23:16.800 --> 23:20.000
on how to use a subset of the Mindflayer API

23:20.000 --> 23:24.880
and GD4 writes code to take actions given the current assigned task.

23:24.880 --> 23:27.120
And because JavaScript runs a coding interpreter,

23:27.120 --> 23:32.160
GD4 is able to define functions on a fly and run it interactively.

23:32.160 --> 23:34.560
But the code that GD4 writes isn't always correct, right?

23:34.560 --> 23:35.520
Just like human engineers.

23:35.520 --> 23:37.520
You can't get everything correct on the first try.

23:38.240 --> 23:41.040
So we developed an iterative prompting mechanism

23:41.040 --> 23:42.240
to refine the program.

23:43.040 --> 23:45.520
And there are three types of feedback here.

23:45.520 --> 23:48.640
The environment feedback, like what are the new materials

23:48.640 --> 23:52.320
you got after taking an action or some enemies nearby.

23:52.960 --> 23:55.600
And the execution error from the JavaScript interpreter

23:55.600 --> 23:58.880
if you wrote some buggy code, like undefined variable,

23:58.880 --> 24:00.800
for example, if it hallucinates something.

24:01.600 --> 24:05.520
And another GD4 that provides critique

24:05.520 --> 24:09.120
through self-reflection from the agent state and the world state.

24:09.760 --> 24:12.000
And that also helps refine the program effectively.

24:13.440 --> 24:14.800
So I want to show some quick example

24:14.800 --> 24:17.360
of how the critic provides feedback

24:17.360 --> 24:19.040
on the task completion progress.

24:19.760 --> 24:21.120
So let's say in the first example,

24:21.120 --> 24:23.920
the task is to craft a spike mass

24:23.920 --> 24:26.720
and GD4 looks at the agent's inventory

24:26.720 --> 24:28.720
and decides that it has enough copper

24:28.720 --> 24:31.200
but not enough Amherst as a material.

24:32.400 --> 24:35.120
And the second task is to kill three sheeps to collect food.

24:35.760 --> 24:38.320
And each sheep drops one unit of wool,

24:38.320 --> 24:40.320
but there are only two units in inventory.

24:40.320 --> 24:42.560
So GD4 reasons and says that,

24:42.560 --> 24:44.960
okay, you have one more sheep to go, likewise.

24:46.960 --> 24:48.720
Now, moving on to the second part.

24:49.920 --> 24:52.560
Once Voyager implements a skill correctly,

24:52.560 --> 24:54.800
we save it to our persistent storage.

24:55.440 --> 24:57.360
And you can think of the skill library

24:57.440 --> 25:01.200
as a code repository written entirely by a language model

25:01.200 --> 25:03.280
through interaction with the 3D world.

25:04.400 --> 25:07.120
And the agent can record new skills

25:07.120 --> 25:09.200
and also retrieve skills from the library

25:09.200 --> 25:11.760
facing similar situations in the future.

25:11.760 --> 25:14.240
So it doesn't have to go through this whole program refinement

25:14.240 --> 25:15.440
that we just saw again,

25:15.440 --> 25:16.800
which is quite inefficient,

25:16.800 --> 25:19.360
but you do it once you save it to disk.

25:20.080 --> 25:23.120
And in this way, Voyager kind of bootstraps

25:23.120 --> 25:24.960
its own capabilities recursively

25:25.520 --> 25:28.240
as it explores and experiments in the game.

25:29.680 --> 25:30.880
And let's dive a little bit deeper

25:30.880 --> 25:32.800
into how the skill library is implemented.

25:33.680 --> 25:35.520
So this is how we insert a new skill.

25:36.080 --> 25:40.160
First, we use GPT 3.5 to summarize the program into plain English.

25:40.720 --> 25:43.760
And summarization is very easy and GD4 is expensive.

25:43.760 --> 25:46.240
So we just go for a cheaper tier.

25:47.440 --> 25:51.040
And then we embed this summary as the key

25:51.040 --> 25:53.680
and we save the program, which is a bunch of code,

25:53.760 --> 25:54.800
as the value.

25:54.800 --> 25:57.760
And we find that doing this makes retrieval better

25:57.760 --> 25:59.840
because the summary is more semantic

25:59.840 --> 26:03.200
and the code is a bit more discrete and you insert it.

26:06.720 --> 26:09.120
And now for the retrieval process,

26:09.120 --> 26:10.960
where Voyager is faced with a new task,

26:10.960 --> 26:12.560
let's say craft iron pickaxe.

26:13.120 --> 26:16.320
We again use GP3.5 to generate a hint

26:16.320 --> 26:17.840
on how to solve the task.

26:17.840 --> 26:19.920
And that is something like a natural language paragraph.

26:20.560 --> 26:24.640
And then we embed that and use that as the query

26:24.640 --> 26:26.320
into the vector database.

26:26.320 --> 26:29.840
And we retrieve the skill from the library.

26:30.720 --> 26:34.160
So you can think of it as a kind of in-context replay buffer

26:34.160 --> 26:36.000
in the reinforcement learning literature.

26:37.680 --> 26:39.440
And now moving on to the third part.

26:41.120 --> 26:44.000
We have another GP4 that proposes what task to do,

26:44.000 --> 26:47.040
given its own capabilities at the moment.

26:47.680 --> 26:50.400
And here we give GP4 a very high-level

26:50.400 --> 26:52.320
kind of unsupervised objective

26:52.320 --> 26:55.920
that is to obtain as many unique items as possible.

26:55.920 --> 26:57.280
That is our high-level directive.

26:57.840 --> 27:00.560
And then GP4 takes this directive and implements

27:01.440 --> 27:04.640
a curriculum of progressively harder challenges

27:04.640 --> 27:06.160
and more novel challenges to solve.

27:07.040 --> 27:10.720
So it's kind of like curiosity of exploration,

27:10.720 --> 27:13.760
where it is our novelty search in a prior literature,

27:14.320 --> 27:16.320
but implemented purely in context.

27:16.400 --> 27:19.280
Yeah, if you're listening to Zoom, the next example is fun.

27:21.520 --> 27:23.440
Let's go through this example together.

27:24.720 --> 27:27.120
Just to kind of show you how Voyager works,

27:27.120 --> 27:29.920
the whole complicated data flow that I just showed.

27:30.640 --> 27:32.720
So the agent finds itself hungry.

27:33.440 --> 27:35.520
It only has one out of 20 hunger bars.

27:35.520 --> 27:38.720
So it knows GP4 knows that it needs to find food ASAP.

27:39.280 --> 27:42.640
And then it senses there are four entities nearby.

27:43.280 --> 27:45.840
A cat, a villager, a pig, and some wheat seeds.

27:46.400 --> 27:48.320
And now GP4 starts a self-reflection.

27:49.040 --> 27:51.520
Like, do I kill the cat and the villager to get some meat?

27:52.080 --> 27:52.960
That sounds horrible.

27:54.080 --> 27:54.960
How about the wheat seeds?

27:55.760 --> 27:57.600
I can use the seeds to grow a farm,

27:57.600 --> 27:59.440
but that's going to take a very long time

27:59.440 --> 28:01.360
until I can generate some food.

28:01.360 --> 28:04.080
So sorry, Piggy, you are the one being chosen.

28:05.120 --> 28:08.640
So GP4 looks at the inventory, which is the agent state.

28:09.200 --> 28:11.920
There is a piece of iron in inventory.

28:11.920 --> 28:15.680
So it recalls, Voyager recalls a skill from the library

28:15.680 --> 28:17.760
that is to craft an iron sword

28:17.760 --> 28:20.400
and then use that skill to start pursuing,

28:20.400 --> 28:23.600
to start learning a new skill, and that is Hunt Pig.

28:23.600 --> 28:27.840
And once the Hunt Pig routine is successful,

28:27.840 --> 28:30.240
GP4 saves it to the skill library.

28:30.240 --> 28:31.280
That's roughly how it works.

28:32.960 --> 28:34.880
Yeah, and putting all of these together,

28:34.880 --> 28:36.800
we have this iterative prompting mechanism,

28:36.800 --> 28:39.680
the skill library, and an automatic curriculum.

28:40.320 --> 28:42.560
And all of these combine.

28:42.560 --> 28:45.600
It's Voyager's no-gradient architecture

28:45.600 --> 28:47.360
where we don't train any new models

28:47.360 --> 28:48.800
or fine tune any parameters,

28:49.520 --> 28:54.080
and allows Voyager to self-boostrap on top of GP4,

28:54.080 --> 28:56.800
even though we are treating the underlying language model

28:56.800 --> 28:57.600
as a black box.

29:00.480 --> 29:01.920
It looks like my example work,

29:01.920 --> 29:04.560
and they started to listen.

29:08.560 --> 29:10.160
So yeah, these are the tasks

29:10.160 --> 29:12.080
that Voyager picked up along the way.

29:12.560 --> 29:14.560
And we didn't pre-program any of these.

29:14.560 --> 29:16.000
It's all Voyager's idea.

29:16.560 --> 29:18.400
The agent is kind of forever curious

29:18.400 --> 29:21.680
and also forever pursuing new adventures just by itself.

29:24.000 --> 29:26.640
So to quickly show some quantitative results,

29:27.680 --> 29:29.760
here we have a learning curve,

29:29.760 --> 29:33.280
where the x-axis is a number of prompting iterations,

29:33.280 --> 29:36.080
and the y-axis is the number of unique items

29:36.080 --> 29:39.280
that Voyager discovered as it's exploring an environment.

29:40.240 --> 29:43.440
And these two curves are baselines,

29:43.440 --> 29:45.760
a react and reflexion.

29:47.600 --> 29:49.040
And this is auto-GPT,

29:49.040 --> 29:50.880
which is like a popular software repo.

29:50.880 --> 29:53.920
Basically, you can think of it as combining react

29:53.920 --> 29:55.760
and a task planner that decomposes

29:55.760 --> 29:57.600
an objective into sub-goals.

29:58.240 --> 29:59.200
And this is Voyager.

30:00.000 --> 30:03.200
We're able to obtain three times more novel items

30:03.200 --> 30:04.720
than the prior methods,

30:04.720 --> 30:07.600
and also unlock the entire texture significantly faster.

30:08.320 --> 30:11.600
And if you take away the skill library,

30:11.600 --> 30:13.440
you see that Voyager really suffers.

30:13.440 --> 30:15.120
The performance takes a hit,

30:15.120 --> 30:17.840
because every time it needs to kind of repeat

30:17.840 --> 30:19.840
and relearn every skill from scratch

30:19.840 --> 30:22.000
and starts to make a lot more mistakes,

30:22.000 --> 30:24.400
and that really degrades the exploration.

30:26.640 --> 30:30.720
Here, these two are the bird-eye views of the Minecraft map,

30:30.720 --> 30:34.000
and these circles are what the prior methods

30:34.000 --> 30:35.360
are able to explore,

30:36.240 --> 30:38.160
given the same prompting iteration budget.

30:39.040 --> 30:42.160
And we see that they tend to get stuck in local areas

30:42.160 --> 30:44.160
and kind of fail to explore more,

30:44.800 --> 30:48.320
but Voyager is able to navigate distances at least two times

30:49.280 --> 30:51.360
as much as the prior works.

30:52.480 --> 30:55.280
So it's able to visit a lot more places,

30:55.280 --> 30:58.160
because to satisfy this high-level directive

30:58.160 --> 31:00.640
of obtaining as many unique items as possible,

31:00.640 --> 31:02.320
you've got to travel.

31:02.320 --> 31:03.360
If you stay at one place,

31:03.440 --> 31:05.280
you will quickly exhaust the interesting things to do.

31:06.320 --> 31:07.920
And Voyager travels a lot,

31:07.920 --> 31:09.520
so that's how we came up with the name.

31:11.760 --> 31:14.800
So finally, one limitation is that Voyager

31:14.800 --> 31:17.680
does not currently support visual perception,

31:17.680 --> 31:20.720
because the GV4 that we used back then was text-only,

31:21.440 --> 31:23.120
but there's nothing stopping Voyager

31:23.120 --> 31:27.120
from adopting like multi-modal language models in the future.

31:27.120 --> 31:29.520
So here we have a little proof-of-concept demo,

31:29.520 --> 31:32.080
where we ask a human to basically function

31:32.160 --> 31:33.840
as the image captioner.

31:33.840 --> 31:36.320
And the human will tell Voyager

31:36.320 --> 31:38.160
that as you're building these houses,

31:38.160 --> 31:39.520
what are the things that are missing?

31:39.520 --> 31:41.440
Like you place a door incorrectly,

31:42.160 --> 31:45.040
like the roof is also not done correctly.

31:45.040 --> 31:48.720
So the human is acting as a critic module of the Voyager stack.

31:49.360 --> 31:52.080
And we see that with some of that help,

31:52.080 --> 31:55.520
Voyager is able to build a farmhouse and another portal,

31:55.520 --> 31:59.440
but it has a hard time understanding 3D spatial coordinates

31:59.440 --> 32:01.680
just by itself in a textual domain.

32:02.560 --> 32:11.360
Now, after doing Voyager, we're considering like, where else can we apply this idea

32:11.360 --> 32:14.000
of coding in an embodying environment,

32:14.000 --> 32:17.920
observe the feedback, and iteratively refine the program.

32:18.720 --> 32:22.720
So we came to realize that physics simulations themselves

32:22.720 --> 32:24.560
are also just Python code.

32:24.560 --> 32:27.760
So why not apply some of the principles for Voyager

32:27.760 --> 32:29.920
and do something in another domain?

32:30.160 --> 32:33.280
What if you apply Voyager in the space of this physics simulator API?

32:33.280 --> 32:38.640
And this is Eureka, which my team announced just like three days ago,

32:38.640 --> 32:39.760
fresh out of the oven.

32:40.800 --> 32:44.400
It is an open-ended agent that designs reward functions

32:44.400 --> 32:47.520
for robot dexterity at superhuman level.

32:47.520 --> 32:50.880
And it turns out that GD4-POS reinforcement learning

32:50.880 --> 32:54.160
can spin a pen much better than I do.

32:54.160 --> 32:58.480
I gave up on this task a long time ago from childhood.

32:58.800 --> 33:00.720
It's so hard for me.

33:03.440 --> 33:06.480
So Eureka's idea is very simple and intuitive.

33:07.120 --> 33:11.200
GD4 generates a bunch of possible reward function candidates

33:11.200 --> 33:12.240
implemented in Python.

33:12.800 --> 33:17.200
And then you just do a full reinforcement learning training loop

33:17.200 --> 33:20.320
for each candidate in a GPU accelerated simulator.

33:21.280 --> 33:25.280
And you get a performance metric and you take the best candidates

33:25.280 --> 33:27.120
and feedback to GD4.

33:27.120 --> 33:30.480
And it samples the next proposals of candidates

33:30.480 --> 33:33.600
and keeps improving the whole population on the reward functions.

33:34.320 --> 33:35.120
That's the whole idea.

33:35.680 --> 33:38.080
It's kind of like an in-context evolutionary search.

33:40.160 --> 33:42.320
So here's the initial reward generation,

33:42.320 --> 33:46.000
where Eureka takes as context the environment code

33:46.000 --> 33:50.320
of NVIDIA's ISAC sim and a task description

33:50.320 --> 33:53.280
and samples the initial reward function implementation.

33:54.240 --> 33:56.480
So we found that the simulator code itself

33:56.480 --> 33:58.480
is actually a very good reference manual

33:58.480 --> 34:01.520
because it tells you, Eureka, what are the variables you can use,

34:02.160 --> 34:05.520
like the hand positions here, the fingertip position,

34:05.520 --> 34:07.440
the fingertips have safe, the rotation,

34:07.440 --> 34:08.880
angular velocity, et cetera.

34:09.600 --> 34:13.120
So you know all of these variables from the simulator code

34:13.120 --> 34:16.320
and you know how they interact with each other.

34:16.320 --> 34:20.560
So that serves as a very good in-context instruction.

34:21.440 --> 34:23.120
So Eureka doesn't need to reference

34:23.120 --> 34:24.960
any human return reward functions.

34:26.560 --> 34:28.960
And then once you have the generated reward,

34:28.960 --> 34:31.600
you plug it into any reinforcement learning algorithm

34:31.600 --> 34:33.120
and just train it to completion.

34:33.680 --> 34:37.760
So this step is typically very costly and very slow

34:37.760 --> 34:39.360
because reinforcement learning is always slow.

34:39.920 --> 34:42.480
And we were only able to scale up Eureka

34:42.480 --> 34:44.080
because of NVIDIA's ISAC chain,

34:44.800 --> 34:48.560
which runs a thousand simulated environment copies

34:48.560 --> 34:49.520
on a single GPU.

34:50.160 --> 34:53.840
So basically, you can think of it as speeding up reality

34:53.840 --> 34:54.800
by a thousand lags.

34:57.120 --> 34:58.160
And then after training,

34:58.160 --> 35:00.160
you will get the performance metrics

35:00.160 --> 35:01.680
back on each reward component.

35:02.320 --> 35:03.680
And as we saw from Voyager,

35:04.240 --> 35:07.040
GBT4 is very good at self-reflection.

35:07.040 --> 35:08.960
So we leverage that capability.

35:13.440 --> 35:16.720
There's a software trial reminding you to activate a license.

35:17.520 --> 35:20.320
Yeah, so Voyager reflects on it

35:20.320 --> 35:23.920
and then proposes mutations on the code.

35:25.520 --> 35:28.960
So here, the mutations we found can be very diverse,

35:28.960 --> 35:30.880
ranging from something as simple as just changing

35:30.880 --> 35:33.520
a hyperparameter in the reward function weighting

35:33.520 --> 35:37.440
to all the way to adding completely novel components

35:37.440 --> 35:38.240
to the reward function.

35:40.000 --> 35:41.520
And in our experiments,

35:41.520 --> 35:45.920
Eureka turns out to be a superhuman reward engineer

35:46.160 --> 35:48.480
actually outperforming some of the functions

35:48.480 --> 35:51.280
implemented by the expert human engineers

35:51.280 --> 35:53.280
on NVIDIA's ISAC same team.

35:55.680 --> 35:58.080
So here are some more demos of how Eureka

35:58.080 --> 36:00.240
is able to write very complex rewards

36:00.240 --> 36:04.160
that lead to these extremely dexterous behaviors.

36:04.160 --> 36:07.200
And we can actually train the robot hand

36:07.200 --> 36:09.840
to rotate pens not just in one direction,

36:09.840 --> 36:12.320
but in different directions, along different 3D axes.

36:12.960 --> 36:15.360
I think one major contribution of Eureka,

36:15.360 --> 36:17.920
different from Voyager, is to bridge the gap

36:17.920 --> 36:22.080
between high-level reasoning and low-level model controls.

36:22.080 --> 36:24.400
So Eureka introduces a new paradigm

36:24.400 --> 36:27.760
that I'm calling hybrid gradient architecture.

36:27.760 --> 36:29.920
So recall Voyager is a no-gradient architecture.

36:29.920 --> 36:33.040
We don't touch anything and we don't train anything.

36:33.040 --> 36:34.880
But Eureka is a hybrid gradient,

36:34.880 --> 36:38.640
where a black box inference-only language model

36:38.640 --> 36:41.040
instructs a wide range of functions

36:41.200 --> 36:44.080
instructs a white box, learnable neural network.

36:45.040 --> 36:47.520
So you can think of it as two loops, right?

36:47.520 --> 36:49.040
The outer loop is great and free,

36:49.600 --> 36:52.000
and it was, it's driven by GV4,

36:52.000 --> 36:54.240
kind of selecting the reward functions.

36:54.240 --> 36:56.160
And the inner loop is great and based.

36:56.160 --> 36:59.520
You train like a full reinforcement learning episode from it

36:59.520 --> 37:03.200
to achieve extreme dexterity using a specialized,

37:03.200 --> 37:06.160
like training by training a special neural network controller.

37:07.040 --> 37:09.280
And you must have both loops to succeed

37:09.280 --> 37:10.880
to deliver this kind of dexterity.

37:11.600 --> 37:14.160
And I think it will be a very useful paradigm

37:14.160 --> 37:16.640
for training robot agents in the future.

37:18.880 --> 37:23.120
So these days, when I go on Twitter or X,

37:23.680 --> 37:26.800
I see AI conquering new lands every week.

37:28.000 --> 37:30.160
Chat, image generation, and music,

37:30.160 --> 37:32.000
they're all very well within reach.

37:32.960 --> 37:35.520
But my dojo, Voyager, and Eureka,

37:35.520 --> 37:37.120
these are just scratching the surface

37:37.120 --> 37:39.120
of open-ended journalist agents.

37:40.480 --> 37:41.600
And looking forward,

37:41.600 --> 37:44.720
I want to share two key research directions

37:44.720 --> 37:47.600
that I personally find extremely promising,

37:47.600 --> 37:49.200
and I'm also working on it myself.

37:50.080 --> 37:52.320
The first is a continuation of Minecraft,

37:52.320 --> 37:54.720
basically how to develop methods

37:54.720 --> 37:56.880
that learn from Internet-skilled videos.

37:57.440 --> 38:00.480
And the second is multimodal foundation models.

38:00.480 --> 38:02.480
Now that GV4V is coming,

38:02.480 --> 38:04.720
but it is just the beginning of an era.

38:05.440 --> 38:09.120
And I think it's important to have all of the modalities

38:09.120 --> 38:10.880
in a single foundation model.

38:12.560 --> 38:13.680
So first, about videos.

38:14.880 --> 38:17.200
We all know that videos are abundant, right?

38:17.200 --> 38:21.440
Like so many data on YouTube, way too many

38:21.440 --> 38:23.120
for our limited GPUs to process.

38:24.320 --> 38:27.200
They're extremely useful to train models

38:27.200 --> 38:30.960
that not only have dynamic perception and intuitive physics,

38:30.960 --> 38:34.560
but also capture the complexity of human creativity

38:34.640 --> 38:35.680
and human behaviors.

38:36.320 --> 38:40.880
It's all good, except that when you are using video

38:40.880 --> 38:42.640
to pre-training body nations,

38:42.640 --> 38:44.400
there is huge distribution shift.

38:44.400 --> 38:46.320
You also don't get action labels,

38:46.320 --> 38:47.920
and you don't get any of the groundings

38:47.920 --> 38:49.440
because you are a passive observer.

38:50.960 --> 38:52.240
So I think here is a demonstration

38:52.240 --> 38:53.840
of why learning from video is hard,

38:53.840 --> 38:55.120
even for natural intelligence.

38:57.040 --> 39:00.400
So Little Cat is seeing boxers shaking their head,

39:01.200 --> 39:02.640
and it thinks maybe shaking head

39:02.640 --> 39:04.320
is the best way to do fighting.

39:04.960 --> 39:10.080
All right, this is why learning from video is hard.

39:13.040 --> 39:15.520
You have no idea, like why...

39:18.000 --> 39:18.800
This is too good.

39:18.800 --> 39:20.000
Let's play this again.

39:20.000 --> 39:22.800
You have no idea why Tyson is doing this, right?

39:22.800 --> 39:24.240
Like the cat has no idea,

39:24.240 --> 39:28.240
and then it associates this with just wrong kind of policy.

39:30.640 --> 39:33.360
But for sure, it doesn't help the fighting,

39:33.360 --> 39:35.520
but it definitely boosts the cat's confidence.

39:38.960 --> 39:40.320
That's why learning from video is hard.

39:42.320 --> 39:44.960
Now, I want to point out a few kind of latest research

39:44.960 --> 39:48.000
in how to leverage so much video for journalist agents.

39:49.040 --> 39:50.720
There are a couple of approaches.

39:51.280 --> 39:52.800
The first is the simplest,

39:52.800 --> 39:56.240
just learn kind of a visual feature extractor from the videos.

39:56.800 --> 40:00.000
So this is R3M from Chelsea's group at Stanford,

40:00.960 --> 40:03.520
and this model is still an image-level representation,

40:03.520 --> 40:06.560
just that it uses a video-level loss function to train,

40:07.200 --> 40:09.360
more specifically, time-contrastive learning.

40:10.000 --> 40:13.600
And after that, you can use this as an image backbone

40:13.600 --> 40:14.880
for any agent,

40:14.880 --> 40:16.160
but you still need to kind of find

40:16.160 --> 40:18.960
to using domain-specific data for the agent.

40:20.800 --> 40:24.480
The second path is to learn reward functions from video,

40:24.480 --> 40:28.640
and MineClip is one model under this category.

40:29.200 --> 40:32.880
It uses a contrastive objective between the transfer and video.

40:32.880 --> 40:36.160
And here, this work, VIP, is another way

40:36.160 --> 40:38.640
to learn a similarity-based reward

40:38.640 --> 40:40.960
for goal-conditioned tasks in the image space.

40:41.680 --> 40:45.600
So this work, VIP, is led by also the first author of Eureka,

40:45.600 --> 40:48.400
and Eureka is his internship project with me.

40:50.240 --> 40:51.680
And the third idea is very interesting.

40:52.400 --> 40:55.920
Can we directly do imitation learning from video,

40:56.480 --> 40:59.120
but better than the cat that we just saw?

40:59.840 --> 41:03.440
So we just said, you know, the videos don't have the actions, right?

41:04.400 --> 41:06.880
We need to find some ways to pseudo-level the actions.

41:07.600 --> 41:11.440
And this is video portraying a VPT from OpenAI last year

41:11.440 --> 41:13.520
to solve long-range tasks in Minecraft.

41:14.640 --> 41:17.680
And here, the pipeline works like this.

41:18.240 --> 41:21.360
Basically, you use a keyboard and a mouse action space,

41:22.000 --> 41:25.200
so you can align this action space with the human actions.

41:26.000 --> 41:29.280
And OpenAI hires a bunch of Minecraft players

41:29.840 --> 41:31.760
and actually collect data in-house,

41:31.760 --> 41:34.400
so they record the episodes done by those gamers.

41:35.040 --> 41:39.200
And now you have a data set of video and action pairs, right?

41:39.920 --> 41:42.640
And you train something called an inverse dynamics model,

41:42.640 --> 41:46.880
which is to take the observation and then predict the actions

41:46.880 --> 41:49.040
that cost the observation to change.

41:49.040 --> 41:50.480
So that's the inverse dynamics model.

41:51.040 --> 41:55.840
And that becomes a labeler that you can apply

41:55.840 --> 41:58.640
to in-the-wild YouTube videos that don't have the actions.

41:59.200 --> 42:04.160
So you apply IDM to like 70K hours of in-the-wild YouTube videos,

42:04.160 --> 42:06.080
and you will get these pseudo-actions

42:06.080 --> 42:09.440
that are not always correct, but also way better than random.

42:10.080 --> 42:12.000
And then you're training imitation learning

42:12.000 --> 42:13.760
on top of this augmented data set.

42:14.320 --> 42:18.560
And in this way, OpenAI is able to greatly expand the data

42:18.640 --> 42:22.480
because the original data collected from the humans

42:22.480 --> 42:25.200
are high quality, but they're extremely expensive,

42:25.200 --> 42:27.360
while in-the-wild YouTube videos are very cheap,

42:27.360 --> 42:28.800
but you don't have the actions.

42:28.800 --> 42:31.920
So they kind of solved and got the best of those roles.

42:32.640 --> 42:35.040
But still, it's really expensive to hire these humans.

42:37.680 --> 42:39.600
Now, what's beyond the videos, right?

42:40.320 --> 42:43.520
I'm a firm believer that multimodal models will be the future.

42:44.240 --> 42:46.160
And I see text as a very lossy,

42:46.160 --> 42:48.560
kind of 1D projection of our physical world.

42:49.120 --> 42:52.560
So it's essential to include the other sensory modalities

42:52.560 --> 42:54.320
to provide a full in-body experience.

42:55.200 --> 42:57.440
And in the context of in-body relations,

42:57.440 --> 43:01.520
I think the input will be a mixture of text, images, videos,

43:01.520 --> 43:05.200
and even audio in the future, and the output will be actions.

43:06.560 --> 43:11.280
So here's a very early example of a multimodal language model

43:11.280 --> 43:12.160
for robot learning.

43:12.720 --> 43:14.400
So let's imagine a household robot.

43:15.200 --> 43:19.120
We can ask the robot to bring us a cup of tea from the kitchen,

43:19.120 --> 43:20.560
but if we want to be more specific,

43:21.200 --> 43:23.840
I want this particular cup that is my favorite cup.

43:23.840 --> 43:25.120
So show me this image.

43:26.320 --> 43:31.440
And we also provide a video demo of how we want to mop the floor

43:31.440 --> 43:35.120
and ask the robot to imitate the similar motion in context.

43:36.240 --> 43:38.960
And when a robot sees an unfamiliar object like a sweeper,

43:38.960 --> 43:41.120
we can explain it by providing an image

43:41.120 --> 43:42.400
and showing this is a sweeper.

43:42.400 --> 43:45.040
Now go ahead and do something with the tool.

43:45.600 --> 43:47.840
And finally, to ensure safety, we can say,

43:47.840 --> 43:50.160
take a picture of that room and just do not enter that room.

43:51.520 --> 43:55.120
To achieve this, back last year,

43:55.120 --> 43:57.360
we proposed a model called VIMA,

43:57.360 --> 43:59.040
which stands for Visual Model Attention.

43:59.680 --> 44:02.720
And in this work, we introduce a concept called multimodal prompting,

44:03.360 --> 44:07.440
where the prompt can be a mixture of text, image, and videos.

44:08.160 --> 44:10.640
And this provides a very expressive API

44:10.640 --> 44:14.000
that just unifies a bunch of different robot tasks

44:14.000 --> 44:17.280
that otherwise would require a very different pipeline

44:17.280 --> 44:19.680
or specialized models to solve in prior literature.

44:20.560 --> 44:23.360
And VIMA simply tokenizes everything,

44:25.120 --> 44:28.080
converting image and text into sequences of tokens,

44:28.080 --> 44:29.760
and train a transformer on top

44:29.760 --> 44:32.480
to output the robot arm actions

44:32.480 --> 44:36.080
autoregressively one step at a time during inference time.

44:37.440 --> 44:40.320
So just to look at some of the examples here,

44:40.960 --> 44:43.600
this prompt rearrange objects to match the scene.

44:43.600 --> 44:45.680
It is a classical task called Visual Go Reaching

44:45.680 --> 44:47.760
that has a big body of prior works on it.

44:48.960 --> 44:53.120
And that's how our robot does it, given this prompt.

44:54.160 --> 44:57.600
And we can also give it novel concepts in context.

44:57.600 --> 44:59.600
Like this is a blanket, this is a work,

44:59.600 --> 45:01.520
now put a work into a blanket.

45:01.520 --> 45:05.200
And both words are nonsensical, so it's not in the training data,

45:05.200 --> 45:06.960
but VIMA is able to generalize zero shot.

45:07.840 --> 45:10.640
And follow the motion to manipulate this object.

45:11.440 --> 45:13.520
So the bot understands what we want

45:13.520 --> 45:14.880
and then follow this trajectory.

45:15.440 --> 45:18.080
And finally, we can give it more complex prompt,

45:18.080 --> 45:20.240
like these are the safety constraints,

45:20.240 --> 45:23.680
sweep the box into this, but without exceeding that line.

45:23.680 --> 45:27.360
And we would do this using the interleaving image

45:27.360 --> 45:28.320
and text tokens.

45:30.480 --> 45:34.400
And recently, Google Brain Robotics followed up after VIMA

45:34.400 --> 45:38.000
with RT1 and RT2, robot transformer one and two.

45:38.960 --> 45:42.640
And RT2 is using a similar recipe, as I described,

45:42.640 --> 45:46.000
where they first kind of pre-train on internet scale data

45:46.560 --> 45:49.680
and then fine tune with some human collected demonstrations

45:49.680 --> 45:50.640
on the Google robots.

45:51.440 --> 45:53.920
And RoboCAD from DeepMind is another interesting work.

45:54.640 --> 45:58.880
They train a single unified policy that works not just on

45:59.600 --> 46:03.120
a single robot, but actually across different embodiments,

46:03.120 --> 46:06.320
different robot forms, and even generalize to a new hardware.

46:07.280 --> 46:10.560
So I think this is like a higher form of multimodal agent

46:10.560 --> 46:12.320
with a physical form factor.

46:12.320 --> 46:15.440
The morphology of the agent itself is another modality.

46:17.440 --> 46:20.640
So that concludes our looking forward section.

46:21.840 --> 46:25.280
And lastly, I want to kind of put all the links together

46:25.280 --> 46:27.120
of the works I described.

46:27.120 --> 46:28.960
So this is mindodger.org.

46:28.960 --> 46:31.360
We have open source everything.

46:31.360 --> 46:34.640
Well, for all the projects where big fans are open source,

46:34.640 --> 46:38.480
we open source as much as we can, including like the model code,

46:38.480 --> 46:43.120
checkpoints, simulator code, and training data.

46:44.560 --> 46:46.960
And this is Voyager.mindodger.org.

46:47.600 --> 46:48.560
This is Eureka.

46:49.600 --> 46:50.960
And this is VIMA.

46:53.280 --> 46:54.400
And one more thing, right?

46:54.960 --> 46:58.000
If you just want an excuse to play Minecraft at work,

46:58.720 --> 47:00.240
then mindodger is perfect for you

47:00.240 --> 47:02.800
because you are collecting human demonstration

47:02.800 --> 47:04.000
to train generalization.

47:04.000 --> 47:06.800
And there's one thing that you take away from this talk.

47:06.800 --> 47:07.680
It should be this slide.

47:09.920 --> 47:11.760
And lastly, I just want to remind all of us,

47:12.560 --> 47:15.200
despite all the progress I've shown, what we can do

47:15.760 --> 47:20.000
is still very far from human ingenuity as embodied agents.

47:21.120 --> 47:23.920
These are the videos from our dataset

47:23.920 --> 47:26.960
of people doing like decorating a winter wonderland

47:26.960 --> 47:29.600
or building the functioning CPU circuit within Minecraft.

47:30.480 --> 47:33.120
And we are very far from that as AI research.

47:33.680 --> 47:35.520
So here's a call to the community.

47:35.520 --> 47:38.480
If human can do these mind-blowing tasks,

47:38.480 --> 47:40.400
then why not our AI, right?

47:40.400 --> 47:41.520
Let's find out together.

