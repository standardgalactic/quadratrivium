1
00:00:00,000 --> 00:00:17,960
This is definitely one of the shortest travel trips that I've ever had to give a talk because

2
00:00:17,960 --> 00:00:22,960
my office is just across the street, and this is a familiar space because we ran the Human

3
00:00:22,960 --> 00:00:26,440
Computer Interaction Seminar in here last year.

4
00:00:26,440 --> 00:00:31,720
So it's great to be here today, and as Mark said, I'm one of the faculty members in the

5
00:00:31,720 --> 00:00:36,880
mechanical engineering department where I run the Shape Lab, but I'm also very much involved

6
00:00:36,880 --> 00:00:40,560
with the Stanford Human Computer Interaction Group as well.

7
00:00:40,560 --> 00:00:44,640
And so today I'm going to be talking about our work sort of at the intersection between

8
00:00:44,640 --> 00:00:49,560
yeah, human computer interaction and robotics and mechatronics systems and thinking about

9
00:00:49,560 --> 00:00:53,800
how can we really enable people to interact in much richer ways.

10
00:00:53,800 --> 00:01:00,920
I also have to apologize, I had a hand procedure done earlier today, and basically I thought

11
00:01:00,920 --> 00:01:06,000
it was going to be not as painful as it was, and so I'm a little bit more under the weather

12
00:01:06,000 --> 00:01:12,480
than normal, so you'll have to believe that this would be even more exciting normally.

13
00:01:12,480 --> 00:01:19,520
So speaking of hands, I've long been impressed by the dexterous ability of our hands to allow

14
00:01:19,560 --> 00:01:25,640
us to manipulate the world, and certainly having recently had some injury for my wrist,

15
00:01:25,640 --> 00:01:31,920
that's become even more clear to me how important our hands are for manipulating things in the

16
00:01:31,920 --> 00:01:36,840
world and really just the amazing complexity of our hands and the way in which we're able to

17
00:01:36,840 --> 00:01:43,360
interact so gracefully and dexterously to achieve really stillful manipulation such as here,

18
00:01:43,360 --> 00:01:46,760
sculpting, and clay, but all of our everyday tasks as well.

19
00:01:47,320 --> 00:01:54,040
But to me what's really exciting about the hand and manipulation is the role of manipulation

20
00:01:54,680 --> 00:02:03,320
in terms of our cognition as well. And so physical interaction is very important for not just

21
00:02:03,320 --> 00:02:08,680
physical manipulation for the sake of changing the world, but also for us to express our thoughts

22
00:02:08,680 --> 00:02:14,360
as you see me gesturing like this, but also for us to better understand information. So there's

23
00:02:14,360 --> 00:02:19,880
been a lot of studies that have looked at how children when they learn with abacai, for example,

24
00:02:19,880 --> 00:02:26,920
or abacus in plural, abacai, that they actually learn math concepts differently and better than if

25
00:02:26,920 --> 00:02:32,120
they learn them just on paper and pencil. And so there's something behind this idea of embodied

26
00:02:32,120 --> 00:02:37,720
cognition that's actually quite important. And we see that not just in terms of education,

27
00:02:37,720 --> 00:02:42,440
but really in terms of very specialized domains as well. So for example, this is a photograph

28
00:02:43,080 --> 00:02:47,960
of urban planners and architects. And what we see there is there's this, you know, basically

29
00:02:47,960 --> 00:02:53,080
rich interplay between the physical models that they're building, space, and the ways in which

30
00:02:53,080 --> 00:02:59,080
people want to interact as well. And so physical and spatial form can really help us better understand

31
00:02:59,080 --> 00:03:05,080
and solve problems and very complex ones in the architecture domain, but many other domains as

32
00:03:05,080 --> 00:03:11,320
well. And the big kind of reason behind this is that spatial manipulation, as I kind of highlighted

33
00:03:11,320 --> 00:03:17,080
before, really aids in spatial cognition. These two things are very tightly coupled together.

34
00:03:17,080 --> 00:03:22,680
And one of the great professors that we have had at Stanford, Barbara Tversky, has done a lot of work

35
00:03:22,680 --> 00:03:28,040
as well, looking into the role that spatial manipulation plays in our cognition and kind

36
00:03:28,040 --> 00:03:33,560
of the evolutionary basis for this as well. So she has a great new book out called Mind and

37
00:03:33,560 --> 00:03:38,840
Motion that I highly recommend you take a look at. But in many ways, this is because cognition

38
00:03:38,840 --> 00:03:45,160
and perception are very much coupled together through action. And we've evolved over a long

39
00:03:45,160 --> 00:03:50,440
period of time to really benefit from the way that we can manipulate the world to better

40
00:03:50,440 --> 00:03:55,640
understand it. And these things are tightly coupled together. This is even more important

41
00:03:55,640 --> 00:04:01,800
when we think about access to information. And so for example, these physical representations,

42
00:04:01,800 --> 00:04:06,680
you know, can be really important for people with different abilities. So here's a picture

43
00:04:06,680 --> 00:04:12,760
of a blind student at the Perkins School for the Blind in the Boston area looking or feeling,

44
00:04:12,760 --> 00:04:18,200
rather, a double helix model. And this is something that would be very hard for us to explain through

45
00:04:18,200 --> 00:04:23,160
words, but by being able to touch, feel, and manipulate it, they can, you know, much more

46
00:04:23,160 --> 00:04:30,200
easily understand. And this is a tactile model of Berlin's Museum Island. Again, this type of

47
00:04:30,280 --> 00:04:36,920
spatial information might be very hard for us to convey through text alone, but by allowing

48
00:04:36,920 --> 00:04:43,480
people to directly access it through touch and through our haptic sense, we can much more readily

49
00:04:43,480 --> 00:04:48,840
access it. But if we look at the ways in which we traditionally interact with computing and with

50
00:04:48,840 --> 00:04:56,040
information, not much has changed since 30 years ago in some ways for the ways in which we interact.

51
00:04:56,040 --> 00:05:00,520
And historically, it's been the case that we really haven't been able to leverage our sense of

52
00:05:00,520 --> 00:05:06,440
touch in that. And if we look towards, you know, basically newer interfaces for interaction such

53
00:05:06,440 --> 00:05:11,720
as in virtual reality, a lot of times we really get these benefits of spatial interaction where

54
00:05:11,720 --> 00:05:17,160
here I can move around and interact and collaborate with someone else. But when we reach out to feel

55
00:05:17,160 --> 00:05:22,520
something or touch something, we're not really receiving any meaningful feedback. And so one of

56
00:05:22,520 --> 00:05:27,000
the key questions that I've been interested in as well as many other people in the field of haptic

57
00:05:27,000 --> 00:05:32,760
interfaces is, you know, what if we could reach out and touch the void? And so during my PhD at

58
00:05:32,760 --> 00:05:37,720
the MIT Media Lab, we were really interested in trying to merge between the physical and the digital

59
00:05:37,720 --> 00:05:44,680
world and bring that sense of physical touch into the real world so that if we were, you know, two

60
00:05:44,680 --> 00:05:49,800
people working around a workstation, we could look at each other, see each other, and also directly

61
00:05:49,800 --> 00:05:56,200
interact with a surface and be able to feel it. And so let me show you what that might look like.

62
00:05:56,200 --> 00:06:02,760
So here's a picture of a new type of haptic display. We call this a shape display that we developed at

63
00:06:02,760 --> 00:06:07,880
the MIT Media Lab with my advisor, PhD advisor Hiroshi Ishii and my colleague Daniel Lightinger,

64
00:06:07,880 --> 00:06:11,960
where we could, you know, really reach out and sort of sculpt with digital clay,

65
00:06:11,960 --> 00:06:17,400
or we could reach out and new interface elements could appear that have the correct affordances

66
00:06:17,400 --> 00:06:23,480
and ergonomics for how we might want to interact. Or we could render different types of 3D models in

67
00:06:23,480 --> 00:06:30,600
sort of real time and be able to, again, see them in 3D but also touch them. So this type of display

68
00:06:30,600 --> 00:06:35,880
had been sort of considered before by a number of other researchers, but here we were able to really

69
00:06:35,880 --> 00:06:40,600
look at what are these meaningful interaction techniques and start to look at some example

70
00:06:40,600 --> 00:06:45,640
applications as well. So this was a project that we worked on with Tony Tain, who was a

71
00:06:45,720 --> 00:06:51,160
formerly a practicing urban planner. And as I said before, you know, the spatial models that

72
00:06:51,160 --> 00:06:55,960
urban planners and architects build are often very important for how they think and consider this

73
00:06:55,960 --> 00:07:00,520
space. But they're, you know, basically they would make one set of full models and then not

74
00:07:00,520 --> 00:07:06,280
be able to change anything. And so here we can in real time change the models, but also have other

75
00:07:06,280 --> 00:07:11,640
simulations that we can load on top of that and be able to, you know, edit, manipulate and collaboratively

76
00:07:11,640 --> 00:07:17,880
work together. So this sort of gives a hint for what this new type of interaction with what we

77
00:07:17,880 --> 00:07:23,720
call shape changing displays might be like. And so when I came to Stanford, I guess now seven-ish

78
00:07:23,720 --> 00:07:29,480
years ago or so, I started the Shape Lab to really deeper investigate these types of systems,

79
00:07:29,480 --> 00:07:34,600
but also work on some other areas as well. And so in our group, we've been working on new types of

80
00:07:34,600 --> 00:07:38,840
haptic displays, like the ones that I've shown here. And that's really what I'm going to spend

81
00:07:38,840 --> 00:07:43,960
the most of today talking about. But I quickly wanted to mention two other areas that our group

82
00:07:43,960 --> 00:07:49,320
has also been working on as well, and that are kind of related to the robotics areas. And so

83
00:07:49,320 --> 00:07:54,760
the next area that we've also been thinking about is this notion of ubiquitous robotic interfaces.

84
00:07:54,760 --> 00:08:01,000
And really this idea harkens back to someone who maybe some of you know of, but Mark Weiser,

85
00:08:01,000 --> 00:08:05,880
who sort of started the field of ubiquitous computing that we really live in today, where

86
00:08:05,960 --> 00:08:11,080
there's, you know, in this room around us, many different sensors and actual or sorry, many different

87
00:08:11,080 --> 00:08:18,360
sensors and displays, there's probably, you know, at least 100, if not 1000 computers in this room

88
00:08:18,360 --> 00:08:23,160
right now. And so the question that we had is sort of what would it be like in the future if we

89
00:08:23,160 --> 00:08:30,920
could have actuation and robotics be embedded in our environment as easily as the computing and

90
00:08:30,920 --> 00:08:36,760
displays that we have today. And so with my PhD student, who's now a professor at Simon Frazier

91
00:08:36,760 --> 00:08:42,600
University, Lawrence Kim, we created this platform of small mobile robots that we could then explore.

92
00:08:42,600 --> 00:08:48,200
And again, here we can have them display information like this. But we also looked at ways in which

93
00:08:48,200 --> 00:08:54,440
they could be embedded in our environment, like on our desk, move around to display information when

94
00:08:54,440 --> 00:08:59,480
we need, but also manipulate things in the environment or go about, you know, remotely

95
00:08:59,480 --> 00:09:04,680
sensing different things as well. And we think this kind of opens up some interesting opportunities

96
00:09:04,680 --> 00:09:11,160
to think about, again, where does this line between robot and environment end. And so that's

97
00:09:11,160 --> 00:09:16,360
something that we're also quite interested in and kind of intersects a lot with human robot

98
00:09:16,360 --> 00:09:21,400
interaction. And the last area that we're doing work in is in shape changing robotics, which is

99
00:09:21,400 --> 00:09:26,600
something that Mark highlighted before. And a lot of that has to do with the enabling technologies

100
00:09:26,600 --> 00:09:32,440
that we are looking at to make shape changing displays and also shape changing robots. And so

101
00:09:32,440 --> 00:09:36,840
if you think back to that first example haptic display that I showed that is sort of like a

102
00:09:36,840 --> 00:09:43,240
2.5D surface display, one of the things that we have been thinking about is how might we sort of

103
00:09:43,240 --> 00:09:50,360
make full 3D shape changing displays. This is sort of the holy grail of some of our work in terms of

104
00:09:50,360 --> 00:09:55,480
thinking about, oh, how could we actually, you know, feel a whole entire dolphin and actually have it

105
00:09:55,480 --> 00:10:01,160
be able to change shape between different surfaces. And so as one of my four of our PhD students,

106
00:10:02,280 --> 00:10:07,320
Zach Hammond and a collaborator of ours, Nathan Yusevich, from Allison Otelmore's group,

107
00:10:07,880 --> 00:10:12,600
we're trying to think about how might we make this vision a reality. And so we got inspired by

108
00:10:13,320 --> 00:10:18,520
basically balloon animals to think about new ways that we might approach this. And the interesting

109
00:10:18,520 --> 00:10:23,320
thing about balloon animals is that you have started out with something very simple, like

110
00:10:23,400 --> 00:10:29,000
essentially inflated tube. And then we can pinch it at certain points, knot it together,

111
00:10:29,000 --> 00:10:34,360
and basically create many different shapes out of the same, you know, simple balloon idea.

112
00:10:34,360 --> 00:10:40,520
And so we tried to bring this idea to the field of soft robotics where we could have basically an

113
00:10:40,520 --> 00:10:46,440
inflated beam. And as opposed to relying on pumps to move and actuate different areas,

114
00:10:46,440 --> 00:10:52,520
what if we created a pinch point in that beam? And then what if we could move that pinch point around?

115
00:10:53,320 --> 00:10:57,320
And then basically what that allows us to do is create some type of system that can really

116
00:10:57,320 --> 00:11:04,040
dramatically change shape by putting a robot roller node on that surface and driving it around and

117
00:11:04,040 --> 00:11:10,040
then creating what we call sort of this idea of a isoparametric type of robot where we have a fixed

118
00:11:10,040 --> 00:11:17,160
length of inflated tube, but then we can change kind of the overall basically geometry, even though

119
00:11:17,160 --> 00:11:23,480
it has a fixed topology. And so here is, you know, one element inside that, but we can place

120
00:11:23,480 --> 00:11:29,080
these together through other kinematic connections to create some type of tetrahedral robot that we

121
00:11:29,080 --> 00:11:34,200
can then basically a truss robot that we can then really dramatically change shape. And again,

122
00:11:34,200 --> 00:11:38,920
we get these benefits of having this constant volume of air that we don't need to really pump

123
00:11:38,920 --> 00:11:44,360
around, but instead using motors to move these pinch points or buckle points around. And so

124
00:11:44,360 --> 00:11:51,080
here's an example of what those types of basically truss robots could look like. And so this is,

125
00:11:51,080 --> 00:11:56,840
you know, a fairly large device about, you know, this size and it's able to pretty dramatically

126
00:11:56,840 --> 00:12:01,720
change shape and move around. And this is all shown in real time. And so it can, you know,

127
00:12:01,720 --> 00:12:08,120
locomote by basically doing some type of punctuated rolling, change its shape and move around. But

128
00:12:08,120 --> 00:12:16,440
it can also go ahead and manipulate objects as well. So it can use the geometry of itself to

129
00:12:16,440 --> 00:12:20,920
basically grasp an object and then be able to pick it up and actually do some interesting

130
00:12:20,920 --> 00:12:27,000
in-hand manipulation as well. And so we think that this kind of idea of having these large

131
00:12:27,000 --> 00:12:32,040
shape-changing truss robots has some interesting applications in terms of thinking about new

132
00:12:32,040 --> 00:12:37,480
ways that we could be able to locomote, manipulate, and also use the shape change to afford different

133
00:12:37,480 --> 00:12:43,000
types of interaction in the environment. And so we've also done some work to look at the

134
00:12:43,000 --> 00:12:48,040
modeling and kinematic control and planning of this as well. So Zach and our group did a lot of

135
00:12:48,040 --> 00:12:52,840
work on grasp optimization planning with these types of robot. And we have a collaboration

136
00:12:52,840 --> 00:12:57,240
with Matt Schwager looking at decentralized control of these types of systems as well.

137
00:12:58,360 --> 00:13:04,280
So I won't go into too many details in that area today, but I just wanted to give you kind of an

138
00:13:04,280 --> 00:13:08,200
overview of some of the different things that we're working on in our group. But today I'm really

139
00:13:08,200 --> 00:13:14,040
going to focus on these types of shape-changing or haptic displays. And I'm going to start out by

140
00:13:14,040 --> 00:13:19,560
talking a little bit about the way in which we approach these problems. And really the way in

141
00:13:19,560 --> 00:13:24,200
which we like to think about this in our group is really focusing very deeply on specific

142
00:13:24,200 --> 00:13:30,280
applications and needs, working with domain experts in those areas, and then trying to learn more

143
00:13:30,280 --> 00:13:35,160
about what are the enabling technologies or the requirements for the enabling technologies

144
00:13:35,720 --> 00:13:41,240
to actually make these systems work. And then that kind of feeds back into other applications

145
00:13:41,240 --> 00:13:47,000
and needs. And we think this is kind of a nice paradigm for working on, you know, basically

146
00:13:47,000 --> 00:13:52,520
new technology to make these things possible. So I wanted to start by talking about two vignettes

147
00:13:52,520 --> 00:13:57,400
of specific applications that we've been working on. And the first is in car design. And the second

148
00:13:57,400 --> 00:14:02,200
is in making, making accessible to sort of highlight some of the challenges in making these

149
00:14:02,200 --> 00:14:08,040
systems actually useful. And so many of you might be familiar with, you know, car design in general

150
00:14:08,040 --> 00:14:14,120
and have seen these types of clay models that basically industrial designers and human factors

151
00:14:14,120 --> 00:14:22,200
experts create to prototype and test car designs. And so we had been reached or we have been working

152
00:14:22,200 --> 00:14:29,080
with Volkswagen who's trying to transition from making these clay models, which end up costing,

153
00:14:29,080 --> 00:14:34,600
you know, something on the order of $100,000 per clay model. So it's very expensive and time consuming

154
00:14:34,600 --> 00:14:41,640
to make those and moving instead to using virtual reality to prototype and test at steel, especially

155
00:14:41,640 --> 00:14:46,440
in terms of working with different stakeholders that they care about, as well as the human factors

156
00:14:46,520 --> 00:14:52,120
aspects of like the interior of the car as well. And so what they're trying to do is basically

157
00:14:52,120 --> 00:14:57,000
transition from this, you know, very physical style of doing things to now move into the virtual

158
00:14:57,000 --> 00:15:02,280
reality systems. But what they found is that, for example, in the case of the interior of the car

159
00:15:02,280 --> 00:15:08,120
design, what they call the seating buck and the human machine interaction, basically, they'll be

160
00:15:08,120 --> 00:15:13,480
able to load up different car designs and virtual reality. And then they have these very fancy seating

161
00:15:13,480 --> 00:15:19,080
butts to adjust the position and height and steering wheel position to basically create any

162
00:15:19,080 --> 00:15:24,680
different kind of car. But then you go and reach out and try to touch the HMI, the human machine

163
00:15:24,680 --> 00:15:29,640
interface part of it, and the dashboard. And basically, you, you know, again, reach out and

164
00:15:29,640 --> 00:15:34,920
touch nothing or touch air. And so one thing that they end up doing is creating these, you know,

165
00:15:34,920 --> 00:15:41,320
basically machined or milled foam models that they can then place inside this seating buck

166
00:15:41,320 --> 00:15:45,640
to then be able to go ahead and test. But it turns out it takes a long time for them to actually be

167
00:15:45,640 --> 00:15:51,000
able to create these full models. They can't switch something, change things on, on demand, etc.

168
00:15:51,000 --> 00:15:56,680
And so we had this idea of what if we could create, you know, interactive seating buck,

169
00:15:57,640 --> 00:16:02,200
basically simulator, where we could create this, you know, basically surface that we could change

170
00:16:02,200 --> 00:16:08,120
in real time to allow us to explore basically the different HMI interactions and some of the

171
00:16:08,120 --> 00:16:13,720
ergonomic issues as well. So we started out working with them on this concept and trying to create a

172
00:16:13,720 --> 00:16:19,320
new generation of our tactile displays to be able to do this. So this was led by Alexa Sue.

173
00:16:20,120 --> 00:16:24,600
And so she created this, you know, basically, this is about the smallest that you can do with

174
00:16:24,600 --> 00:16:30,920
kind of like low cost off the shelf DC actuators. So this has a direct drive between them with

175
00:16:30,920 --> 00:16:35,640
the lead screw and, you know, many, many actuators and we made this modular so you can stack them

176
00:16:35,640 --> 00:16:40,920
together. And we also, you know, looked at the integration with this as well as in virtual

177
00:16:40,920 --> 00:16:46,600
reality. And then we brought it back to our collaborators at Volkswagen, particularly like

178
00:16:46,600 --> 00:16:52,280
the industrial designers and the, you know, human factor specialists. And they sort of said, Oh,

179
00:16:52,280 --> 00:16:57,480
hey, this is great. We like the idea. But, you know, wouldn't it be great if these could be a lot

180
00:16:57,480 --> 00:17:03,160
higher resolution and much cheaper? So, you know, in terms of the types of things that they're looking

181
00:17:03,160 --> 00:17:09,240
for, they were not still there with this technology. And again, Oh, wouldn't it be nice if, you know,

182
00:17:09,240 --> 00:17:13,800
we could create many of these, not just, you know, basically, this device here cost something like

183
00:17:13,800 --> 00:17:19,720
$6,000 in parts to create. So wouldn't it be nice if we could, you know, basically have this be

184
00:17:19,720 --> 00:17:25,480
higher resolution and, you know, basically much larger in scale and lower in cost. So those were

185
00:17:25,480 --> 00:17:31,400
some of the feedback that we got, which sort of makes sense in some ways. But then I wanted to

186
00:17:31,400 --> 00:17:36,040
highlight another application area that we've been thinking about that also informs some of these

187
00:17:36,040 --> 00:17:41,720
other challenges. And so this is in the area of still computerated design and this idea of making,

188
00:17:41,720 --> 00:17:46,920
making accessible. And so many of you are probably very familiar with the, the making movement or

189
00:17:46,920 --> 00:17:54,760
maker movement that kind of has gone on over the past 15 or 20 years. And basically, you know,

190
00:17:54,760 --> 00:17:59,480
one of the great things about that is that it's really empowered a lot of people and, and served

191
00:17:59,560 --> 00:18:05,320
as a great way to involve more people in STEM education. But that doesn't mean that all people

192
00:18:05,320 --> 00:18:11,800
are unable to do that. And particularly a lot of the tools that we use for making, especially in

193
00:18:11,800 --> 00:18:16,440
terms of computerated design are not accessible to people that are blind or visually impaired.

194
00:18:16,440 --> 00:18:22,280
And so those people have historically been excluded from those areas. And so while 3D printing can

195
00:18:22,280 --> 00:18:27,960
be very helpful in terms of supporting accessible education through the use of creating tactile

196
00:18:27,960 --> 00:18:33,400
graphics or other types of materials that blind people can touch and feel, there's really this

197
00:18:33,400 --> 00:18:38,840
lack of authoring tools for blind people for them to be able to be the designers and engineers

198
00:18:38,840 --> 00:18:44,440
themselves. And so the big problem with these, essentially with these systems that exist for

199
00:18:44,440 --> 00:18:50,040
computerated design is that they're all based on basically graphical user interfaces, where you

200
00:18:50,040 --> 00:18:55,560
have to directly manipulate with a mouse and a keyboard, as well as the computer screen to be

201
00:18:55,640 --> 00:19:01,000
able to select and control many features. And that's really great for people that are cited, right?

202
00:19:01,000 --> 00:19:06,840
We've moved beyond command line interfaces to this direct, direct manipulation type of interface.

203
00:19:06,840 --> 00:19:10,440
But for someone that's blind and visually impaired, basically the main way that they

204
00:19:10,440 --> 00:19:15,640
interact with computers is through different types of screen readers, which basically provide audio

205
00:19:15,640 --> 00:19:20,600
feedback. And so how might we think about ways in which they could still be able to use this?

206
00:19:20,600 --> 00:19:28,280
So there is some work on using text-based editors for creating geometries. So for example, OpenSCAD

207
00:19:28,280 --> 00:19:34,840
is a computed solid geometry modeler where you write basically in a declarative programming

208
00:19:34,840 --> 00:19:39,800
language to define the geometry. And there's been some great work at the Dimensions Project at the

209
00:19:39,800 --> 00:19:44,760
New York Public Library, Chansey Fleet there, in terms of basically using that for people that are

210
00:19:44,760 --> 00:19:49,240
blind or visually impaired to write code to then be able to 3D print something. But if you can

211
00:19:49,240 --> 00:19:53,800
imagine and you've used a 3D printer, you know they're not particularly fast. And so basically

212
00:19:53,800 --> 00:19:59,480
you write some code. And then while we, you know, people that are cited would be able to, you know,

213
00:19:59,480 --> 00:20:03,240
instantly see the changes that we're making, someone that's blind and visually impaired would

214
00:20:03,240 --> 00:20:09,960
have to basically use a 3D printer, wait somewhere between an hour to, you know, 10 hours to find

215
00:20:09,960 --> 00:20:14,600
out, oh, the change that I made was that exactly what I wanted. And so one of the questions we've

216
00:20:14,600 --> 00:20:20,280
been asking is, how might dynamic tactile feedback, you know, support this type of interaction? And so

217
00:20:20,280 --> 00:20:26,200
again, my former PhD student, Alexa Su, as well as some members from the blind and visually impaired

218
00:20:26,200 --> 00:20:31,160
community in the Bay Area, Sun Kim, who's an Access Technology Specialist at the Vista Center for

219
00:20:31,160 --> 00:20:37,800
the Blind, as well as Josh Miele, who is an amazing blind engineer and researcher who's now at

220
00:20:38,600 --> 00:20:44,440
at Amazon working on accessibility there, we all work together to create a co-design with

221
00:20:44,440 --> 00:20:49,640
other blind makers, a tool that allows people to use these types of tactile shape displays we've

222
00:20:49,640 --> 00:20:54,920
been talking about to in real time have that feedback. So here, basically you're able to write

223
00:20:54,920 --> 00:21:01,160
code in open SCAD, and then in real time be able to touch and feel the geometry and sort of be able

224
00:21:01,160 --> 00:21:07,080
to understand what it is. And so one of the questions that we had, and then 3D print the design.

225
00:21:07,080 --> 00:21:12,040
So one of the questions we had is, you know, what are the types of interactions that we need to port

226
00:21:12,040 --> 00:21:17,560
from these direct manipulation interfaces that are really essential for basically helping blind

227
00:21:17,560 --> 00:21:22,120
and visually impaired people understand that. So we did a lot of different co-design sessions to try

228
00:21:22,120 --> 00:21:26,680
to understand what these challenges are. And one of the things that's interesting, it turns out that

229
00:21:26,680 --> 00:21:31,880
section views are even more important, you know, basically for blind and visually impaired people

230
00:21:31,880 --> 00:21:38,440
than for in our, you know, sighted base CAD systems. Because, you know, essentially, you know,

231
00:21:38,440 --> 00:21:44,280
there's no notion of transparency in these types of displays. You're basically just feeling the top

232
00:21:44,280 --> 00:21:49,000
surface of the convex hole or whatever. So you can't have any way to display, at least with these

233
00:21:49,000 --> 00:21:55,720
displays, this notion of transparency, which we rely on a lot. So we have some, you know, basically

234
00:21:55,720 --> 00:22:00,600
promising interactions that we think are possible with this and compared to like a single point haptic

235
00:22:01,160 --> 00:22:07,480
device. We might have less high resolution in terms of the spatial component. But by allowing

236
00:22:07,480 --> 00:22:12,680
people to touch it with their whole hands, they're actually much more easily able to understand what

237
00:22:12,680 --> 00:22:18,040
it is and the shape. So we think this is a promising direction. But if we look at the types of geometries

238
00:22:18,040 --> 00:22:22,600
that the people we were working with are able to create with this type of tactile display,

239
00:22:22,600 --> 00:22:29,160
we can see they're quite limited. And so this was a big issue that we found. But there is this real

240
00:22:29,240 --> 00:22:34,200
benefit from having this tight coupling of the iteration. And so we think this is a promising

241
00:22:34,200 --> 00:22:40,920
direction and very promising in terms of this notion of real time iterative feedback. And also

242
00:22:40,920 --> 00:22:45,000
that people in the blind and visually impaired community really do want to be designers and

243
00:22:45,000 --> 00:22:50,920
makers of their own, you know, basically technology. And this type of system can be very empowering

244
00:22:50,920 --> 00:22:57,240
for them. But, you know, there's still these big issues in terms of resolution. So if we think

245
00:22:57,240 --> 00:23:02,760
about a computer display that we might use being very high resolution versus this pin display that

246
00:23:02,760 --> 00:23:08,440
I'm showing here, you know, there's a big gap in terms of the resolution and potentially the

247
00:23:08,440 --> 00:23:13,880
under understandability of that geometry. And then also in terms of access. So if each of those

248
00:23:13,880 --> 00:23:19,400
tactile displays that still low resolution costs upwards of $6,000, that means that, you know,

249
00:23:19,400 --> 00:23:25,080
many people can't have access to it, as opposed to, you know, $500 laptop that we could use to

250
00:23:25,080 --> 00:23:30,840
teach cat. So across these different application areas, we really saw these big challenges in terms

251
00:23:30,840 --> 00:23:36,280
of cost, steel and resolution, as well as interaction techniques. But I'm not going to talk as much

252
00:23:36,280 --> 00:23:41,000
about that today. And so a lot of the work that we've done in the past couple of years has been

253
00:23:41,000 --> 00:23:47,240
trying to address or mitigate some of these issues of cost, steel and resolution. And we've kind of

254
00:23:47,240 --> 00:23:53,480
taken two different approaches to try to tackle that on our group. The first is in trying to create

255
00:23:53,560 --> 00:24:00,360
new and novel technical solutions, hardware solutions to solve that. And the second area is on

256
00:24:00,360 --> 00:24:06,360
kind of what we call kind of perceptual illusions or using basically perceptual engineering to think

257
00:24:06,360 --> 00:24:12,280
about how do we basically use the existing hardware that we have, but maybe use some clever

258
00:24:12,280 --> 00:24:17,480
tricks in terms of how we integrate information together to improve the perceived resolution.

259
00:24:17,480 --> 00:24:22,840
And so I'm going to talk about both of these areas. So the first area that I'll talk about

260
00:24:22,840 --> 00:24:28,840
is on creating higher resolution tactile displays. And this has been kind of a big challenge in the

261
00:24:28,840 --> 00:24:33,880
field of haptics for a long time. And there's sort of this big tradeoff or dichotomy between people

262
00:24:33,880 --> 00:24:39,720
that are trying to make really high bandwidth tactile displays versus people that are trying to

263
00:24:39,720 --> 00:24:45,080
make kind of these shape displays or tactile displays that maybe don't need to move as fast

264
00:24:45,080 --> 00:24:51,080
or maybe don't need to move at all. And how do we find this balance between the two? And so one of

265
00:24:51,080 --> 00:24:56,040
my former PhD students, Ty Zane, started to think about ways in which we could really try to push

266
00:24:56,040 --> 00:25:02,040
the resolution if we trade off on that bandwidth side of things. And so if we think about this

267
00:25:02,040 --> 00:25:07,960
design space or list of design requirements for like the ideal or ultimate tactile display,

268
00:25:07,960 --> 00:25:12,440
there's a lot of different things that we might consider, one of which is like, what is the necessary

269
00:25:12,440 --> 00:25:18,040
resolution? And so we can look to the haptics and psychophysics literature for some intuition

270
00:25:18,040 --> 00:25:24,440
around that. Basically, if we think about just statically touching a shape, something like that,

271
00:25:24,440 --> 00:25:31,240
we need to be in that one or 1.25 millimeter range for us to be able to not really be able to feel

272
00:25:31,240 --> 00:25:35,960
individual pins. So you could think about this idea of the retina display that you might be

273
00:25:35,960 --> 00:25:41,960
familiar with from Apple's marketing, where if you look at an iPhone today, you can't see where one

274
00:25:41,960 --> 00:25:47,640
pixel ends and the next begins. Basically, this two point discrimination threshold is kind of

275
00:25:48,040 --> 00:25:53,320
the same concept. And so we want to be in that one to one point two five millimeter range. But

276
00:25:53,320 --> 00:25:58,840
that's I would say a very generous, you know, basically ballpark estimate, because actually

277
00:25:58,840 --> 00:26:04,120
if you start to move your finger, then you can basically feel, you know, down to, you know,

278
00:26:04,120 --> 00:26:09,480
tens of microns or below. So you can feel, you know, a single hair very easily. But that's relying

279
00:26:09,480 --> 00:26:16,200
much more on your, you know, basically cutaneous information from vibration and essentially texture

280
00:26:16,200 --> 00:26:22,760
information. But if we just think about gross shape, this 1.25 millimeters gets us pretty close.

281
00:26:23,560 --> 00:26:29,000
So as I said, people have been trying to work on this problem for a long time. And there's a lot

282
00:26:29,000 --> 00:26:33,480
of different approaches in our own work. We've used like, you know, basically mechanical linear

283
00:26:33,480 --> 00:26:39,080
actuators and also different types of pneumatic actuators, which have again, challenges in terms

284
00:26:39,080 --> 00:26:45,080
of scaling these down, as well as, you know, basically the high cost of the actuators and

285
00:26:45,080 --> 00:26:50,680
being able to steal those together. Other people have created like electromagnetic tactile displays.

286
00:26:50,680 --> 00:26:55,720
But in general, those have some challenges as you scale them down, because the magnetic,

287
00:26:55,720 --> 00:27:00,520
electromagnetic fields start to bleed in with each other. So there was some interesting work from

288
00:27:01,320 --> 00:27:06,760
Juan Zarate and Herbert Shea in terms of thinking about electromagnetic shielding for this, but

289
00:27:06,760 --> 00:27:11,480
they're still kind of on the centimeter steel type of size or other people using

290
00:27:11,480 --> 00:27:16,440
electroactive polymers. And again, all of these are really thinking about this idea of, you know,

291
00:27:16,440 --> 00:27:23,320
how do we make a really fast or high bandwidth tactile display. And so our, the intuition

292
00:27:23,320 --> 00:27:28,760
that Kai had was, okay, let's not focus on the high bandwidth aspect. But instead, what if we had

293
00:27:28,760 --> 00:27:34,280
something that's much more like e-ink, where, oh, we're not changing it very frequently, we're

294
00:27:34,280 --> 00:27:40,520
going to kind of refresh the whole, the whole thing. How do we instead have maybe clutches or

295
00:27:40,520 --> 00:27:45,960
brakes that we could engage or lock when we need them to, but then have one global actuator that

296
00:27:45,960 --> 00:27:51,800
might change everything. And so by trading off on this high bandwidth or the temporal domain,

297
00:27:52,360 --> 00:27:57,240
or frequency domain, and instead focusing on the spatial, what might we be able to do?

298
00:27:57,240 --> 00:28:03,240
And so the technology that we sort of came to in terms of a good, good trade off between

299
00:28:03,240 --> 00:28:09,160
this sort of high force density in terms of braking or clutching, and then also the steel

300
00:28:09,240 --> 00:28:15,080
ability is electrostatic adhesion. And so many of you are probably familiar with the electrostatic

301
00:28:15,080 --> 00:28:18,280
fact where, you know, basically you have a balloon, you rub it on your hair,

302
00:28:18,280 --> 00:28:23,720
and it sticks to your head. And so this has long been used in different industries, for example,

303
00:28:23,720 --> 00:28:31,560
in like wafer chucking in the, in the semiconductor industry, and paper handling and other things.

304
00:28:31,560 --> 00:28:38,280
And then over the past 15 or so years, it's really come into vogue in the robotics community as well.

305
00:28:38,280 --> 00:28:43,320
So probably many of you are familiar with it, but, and also very commonly used in the MEMS

306
00:28:43,880 --> 00:28:49,640
steel devices as well. And so basically we started to think about how do we make these millimeter

307
00:28:49,640 --> 00:28:55,720
steel, you know, basically high force density clutches, and what are the different techniques

308
00:28:55,720 --> 00:29:00,840
that we might need to, to use to do that. And we think that this is a promising technology for

309
00:29:00,840 --> 00:29:06,840
this type of, you know, refreshable display. And so basically here we can see kind of an example

310
00:29:06,840 --> 00:29:12,440
of one of these displays where we have essentially a dielectric thin film that we've then patterned

311
00:29:12,440 --> 00:29:17,240
with these interdigitated electrodes that are on the order of, you know, basically a millimeter

312
00:29:17,240 --> 00:29:24,680
across. And then we can basically turn on an electric field that then basically induces some

313
00:29:24,680 --> 00:29:31,160
charge on these brass or different types of metal pins that, and then locks them into place.

314
00:29:31,720 --> 00:29:41,720
What does this actually look like in action? Okay. Yeah. So basically we can raise up,

315
00:29:41,720 --> 00:29:47,160
so it's a refreshable display. So essentially we, now we unlock all the pins, we raise them up,

316
00:29:47,160 --> 00:29:53,880
and then as we move it down, we lock them into place. And so the electrostatic clutches can turn

317
00:29:53,880 --> 00:29:58,840
on and off very quickly, giving us very high precision in terms of linear positioning,

318
00:29:59,480 --> 00:30:05,080
and are relatively high force compared to their size. So again, the basic operating principle

319
00:30:05,080 --> 00:30:10,520
is that we have this, you know, interdigitated electrode that's serving as this clutch that's

320
00:30:10,520 --> 00:30:15,640
on one side of a high dielectric constant thin film. And then on the other side, there's a pin,

321
00:30:15,640 --> 00:30:22,200
and basically we induce opposite charge on the pin. And that basically creates this electrostatic

322
00:30:22,200 --> 00:30:28,040
force. And so kind of very simple model of how this electrostatic force works is very similar

323
00:30:28,120 --> 00:30:33,400
to the parallel plate capacitor equation, where essentially, you know, the things that matter

324
00:30:33,400 --> 00:30:40,360
are essentially the dielectric constant of the thin film, the contact area between the metal

325
00:30:40,360 --> 00:30:47,800
pin and the dielectric film and the electrodes on the other side, the voltage as well as the film

326
00:30:47,800 --> 00:30:53,240
thickness. And so historically in kind of robotics and other applications, people have really pushed

327
00:30:53,240 --> 00:30:59,480
on the voltage as the thing to kind of improve the the actuation force. Here we have some

328
00:30:59,480 --> 00:31:04,600
challenges in doing that because one, we're dealing with people and, you know, basically some of these

329
00:31:04,600 --> 00:31:10,600
very high, you know, 10 kilovolt types of range, a very small amount of current can actually be

330
00:31:10,600 --> 00:31:16,200
not very good for people. So that's one issue. And then the other issue is we want to have,

331
00:31:16,200 --> 00:31:21,080
you know, not just one of these actuators, we want to have tens of thousands of these actuators.

332
00:31:21,080 --> 00:31:27,080
And so how do we have, you know, very small and low cost transistors that allow us to, you know,

333
00:31:27,080 --> 00:31:32,440
steal this in terms of production? And so we ended up trying to push more on the thin,

334
00:31:32,440 --> 00:31:37,880
thinness and also the high dielectric constant as things that we could push on as opposed to

335
00:31:37,880 --> 00:31:43,480
the voltage because of those two constraints. So this is kind of, you know, basically what the

336
00:31:43,480 --> 00:31:50,040
actual device looks like when we fabricate these and using the UV laser cutter that Mark's lab

337
00:31:50,040 --> 00:31:55,400
or actually Alison Okamura's lab. Well, I forget where it is now, but basically between Mark and

338
00:31:55,400 --> 00:32:04,200
Alison, exactly. Shared facility, which is great. Basically, that allows us to, you know, very easily

339
00:32:04,200 --> 00:32:09,160
fabricate these and test out different patterns. And so we use PVDF, which is a high dielectric

340
00:32:09,160 --> 00:32:14,360
constant in film and a very small sheet of it. And then we have gold that's sputtered directly

341
00:32:14,360 --> 00:32:19,640
on it or some other aluminum, for example. And then we laser a blade off the parts that we don't

342
00:32:19,640 --> 00:32:25,320
want. So the actuation principle is very simple in terms of, you know, we have these clutches

343
00:32:25,320 --> 00:32:30,760
and then we lower down the platform lock when the pins get to the right place. We turn on the

344
00:32:30,760 --> 00:32:35,960
clutch and then we create the whole pattern. And then when we want to erase it or refresh it,

345
00:32:35,960 --> 00:32:41,080
we just turn them off and then move the pin up and down. And so we can get, again, very high

346
00:32:41,080 --> 00:32:46,680
spatial accuracy in terms of the, you know, basically the linear positioning of the height

347
00:32:46,680 --> 00:32:53,000
because we can turn them, turn on and off the clutches and about, you know, on the order of

348
00:32:53,000 --> 00:33:00,840
10 milliseconds or so, which allows us to have pretty high spatial resolution. We've also done a

349
00:33:00,840 --> 00:33:06,760
lot of work on quasi-static loading as well. And unfortunately, the, you know, basically other

350
00:33:06,760 --> 00:33:11,400
people as well. So for example, Steve Collins lab has done a lot of work on these electrostatic

351
00:33:11,480 --> 00:33:16,920
types of clutches and also found that, you know, basically the back of the envelope calculations

352
00:33:16,920 --> 00:33:21,320
don't really end up matching very well with performance because of the effective contact

353
00:33:21,320 --> 00:33:26,760
area. And so we've done a lot of work also on data-driven modeling for this. But basically,

354
00:33:26,760 --> 00:33:32,920
we think these clutches, you know, on the order of, you know, 50 to 100 grams of force for the

355
00:33:32,920 --> 00:33:37,720
areas that we're looking at, which, you know, is not that much force. But if we think about

356
00:33:37,720 --> 00:33:42,120
the contact force that's necessary as you're exploring something, that's on the order of

357
00:33:42,120 --> 00:33:48,680
51 grams of force. And that would likely be spread across multiple pins as well. So we think we're

358
00:33:48,680 --> 00:33:53,560
in the right, right ballpark for this. And my current student, Ahad, who's there now, is trying

359
00:33:53,560 --> 00:33:59,000
to work on improving the performance and thinking about other things. So we've done some user testing

360
00:33:59,000 --> 00:34:05,000
as well to explore how well these tactile displays work. And it seems like it's a promising direction

361
00:34:05,000 --> 00:34:10,120
and seems to be working very well. So we think this, you know, basically these electrostatic

362
00:34:10,760 --> 00:34:17,080
pin displays are kind of a promising approach to really pushing the resolution and low-cost

363
00:34:17,080 --> 00:34:22,280
aspects of these types of refreshable tactile displays. So that's something that we're quite

364
00:34:22,280 --> 00:34:28,760
excited about. And we've been able to achieve sort of this 1.5 or 1.7 millimeter pitch as well.

365
00:34:30,040 --> 00:34:33,720
One of the challenges with these types of displays, though, is that they end up creating

366
00:34:33,720 --> 00:34:38,440
these very discrete types of shapes, right? And so it's possible that, you know, we might want to

367
00:34:38,440 --> 00:34:44,600
have more continuous shapes that could be approximated better with some other method. And also,

368
00:34:44,600 --> 00:34:51,320
maybe there's a way to do that where we trade off, you know, basically, and basically are able to have,

369
00:34:51,320 --> 00:34:57,880
you know, basically more continuous shapes with fewer number of actuators. And so Ahad, who recently

370
00:34:57,880 --> 00:35:04,600
had a paper that was accepted to ICRA, that's on thinking about how might we make these basically

371
00:35:04,600 --> 00:35:09,960
more continuous shape displays and what are techniques we could use to sort of have sort of a

372
00:35:09,960 --> 00:35:15,400
monolithic manufacturing process where we can kind of create these in one go. And so Ahad has been

373
00:35:15,400 --> 00:35:22,360
working on thinking about kind of the combination of electrostatic and electroadhesive actuation

374
00:35:22,440 --> 00:35:28,840
with auxetic materials to be able to create basically the shape-changing continuous displays

375
00:35:28,840 --> 00:35:36,120
where we're able to vary the curvature of them by locking individual cells in an auxetic grid

376
00:35:36,120 --> 00:35:41,800
or an auxetic network. And so here's an example of that shown here. And so basically, the idea is

377
00:35:41,800 --> 00:35:48,040
that we have, you know, work in auxetic skins where we can basically have these different patterns that

378
00:35:48,040 --> 00:35:55,720
can expand basically differently based upon, you know, how much, you know, strain there is in the

379
00:35:55,720 --> 00:36:01,880
system. And what we're doing is then locking some of these cells in this auxetic pattern. And what

380
00:36:01,880 --> 00:36:07,480
that does is it means that there's going to be very different local strain concentrations that end up

381
00:36:07,480 --> 00:36:13,160
as you inflate this surface being able to create different geometries. And so basically, there's

382
00:36:13,160 --> 00:36:18,600
been exciting work in the computer graphics field as well as in other areas on basically being able

383
00:36:18,600 --> 00:36:25,480
to computationally design these auxetic patterns so that you can then create some arbitrary geometry

384
00:36:25,480 --> 00:36:31,880
that's beyond sort of like what a develop surface could be. And our kind of contribution here is

385
00:36:31,880 --> 00:36:38,600
to think about, oh, as opposed to being able to, you know, essentially pre-plan and create a custom

386
00:36:38,600 --> 00:36:44,280
auxetic pattern that could create some given shape, could we essentially create a smart skin

387
00:36:44,280 --> 00:36:51,480
where we can change that local amount that each cell can open and close in real time or at run time.

388
00:36:51,480 --> 00:36:58,200
And so basically, the way that we do that is by having each auxetic cell essentially be electrostatic

389
00:36:58,920 --> 00:37:04,520
break or clutch that can, you know, basically either open or close, depending on how much

390
00:37:04,520 --> 00:37:09,000
voltage we're applying across them. And so what this means is that we want to have essentially

391
00:37:09,000 --> 00:37:15,400
an auxetic pattern with a very large surface area, because as I said before, the electrostatic force

392
00:37:15,400 --> 00:37:20,760
is basically proportional to the amount of surface area that we have. And so essentially, you know,

393
00:37:20,760 --> 00:37:26,200
we've looked at different patterns, but this one here, you know, again, has a very large surface area.

394
00:37:26,200 --> 00:37:32,360
And so what we end up doing is taking two of these sheets and then rotating them off phase

395
00:37:32,440 --> 00:37:38,760
so that basically, essentially, there's a lot of overlap between them. And we can essentially lock

396
00:37:38,760 --> 00:37:44,280
which parts will expand and which parts work. So here you get an idea of what that single cell

397
00:37:44,280 --> 00:37:53,560
might look like. And so we have two. Yeah, so here you can see these, these sets expanding and

398
00:37:53,560 --> 00:37:59,240
contracting. And so essentially, we can as we pull on them or inflate them, they're opening up and

399
00:37:59,240 --> 00:38:05,720
closing. And we can basically turn on the electrostatic adhesion to lock them and allow them to not

400
00:38:05,720 --> 00:38:11,480
open up, which means that basically there's less displacement and less strain in the system.

401
00:38:11,480 --> 00:38:15,880
So we've looked at different types of combinations of layers and different materials as well.

402
00:38:16,760 --> 00:38:21,320
And are looking at, you know, basically, how do we then make this into this monolithic system? So

403
00:38:21,320 --> 00:38:26,280
again, there's these two sheets that are on top of each other. And then basically, we create them

404
00:38:26,280 --> 00:38:30,760
out of this flexible printed circuit board. And so that's really nice, because we can just go

405
00:38:30,760 --> 00:38:35,880
ahead and fabricate that using off the shelf, you know, printed circuit board techniques.

406
00:38:35,880 --> 00:38:41,640
And then we have one sheet that's in this orientation, then a dielectric thin film in between,

407
00:38:41,640 --> 00:38:48,760
and then the other flex PCB that's on the on the next side as well. So then that's what this sort

408
00:38:48,760 --> 00:38:54,920
of 2D service here looks like. And here we're engaging and locking between them. And you can see

409
00:38:54,920 --> 00:38:59,160
some of them start to fail as well. That I guess that was the unlocked one here,

410
00:38:59,160 --> 00:39:04,840
this region below is locked and above is unlocked, you can sort of see. And then here's the locked

411
00:39:04,840 --> 00:39:10,840
region. And at a certain point, it'll start to fail as well. And so basically, we can, again,

412
00:39:10,840 --> 00:39:15,240
computationally control which of those areas we want to be locked and unlocked. And that allows

413
00:39:15,240 --> 00:39:20,200
us to create these different shapes. And then we have a inflated bladder that's underneath it that

414
00:39:20,200 --> 00:39:26,680
we can then basically inflate. And that will then create this global shape change. And so here's

415
00:39:26,680 --> 00:39:33,640
kind of an example of this 100% locked, which sort of creates this, you know, basically a very

416
00:39:33,640 --> 00:39:38,520
uniform shape. And then now we're locking less and less of the display to create different

417
00:39:38,520 --> 00:39:43,720
curvatures, if you can see like that. So we think again, this is kind of a promising approach. We're

418
00:39:43,800 --> 00:39:50,280
still looking for basically, exotic patterns where we can have higher, you know, amount of strain and

419
00:39:50,280 --> 00:39:55,160
more basically curvature that we can create then. But we think this again has some benefits in terms

420
00:39:55,160 --> 00:40:01,320
of being able to really create and manufacture this very quickly. A related project that was kind

421
00:40:01,320 --> 00:40:06,680
of in the early stages is on kind of connecting this idea of these continuous shape displays

422
00:40:06,680 --> 00:40:10,280
with some of the work that I showed at the beginning on these shape changing robots,

423
00:40:11,000 --> 00:40:15,800
where basically we want to have these elastic grid shells. So if you think about in computer

424
00:40:15,800 --> 00:40:20,600
graphics, we often have like NERBs, surfaces, which are kind of combinations of these,

425
00:40:21,960 --> 00:40:26,600
you know, different splines that are connected to each other. What if we do that in the real world?

426
00:40:26,600 --> 00:40:31,880
And so basically, Sophia Weitzner and Wingsum Lawn, our group, are trying to create these basically

427
00:40:32,520 --> 00:40:37,720
robotic elastic grid shells that can then again change their geometry in real time.

428
00:40:37,720 --> 00:40:42,680
This is a small one by one prototype, but here you can see we locked one part of it and then

429
00:40:42,680 --> 00:40:48,120
we're able to inject more material into it and create this curvature there. And so this is what

430
00:40:48,120 --> 00:40:54,200
we're kind of aiming to do on the left is create these, you know, basically interconnected grid

431
00:40:54,200 --> 00:40:59,480
shells that we can in real time change their surface. So these are some of the things we've

432
00:40:59,480 --> 00:41:03,640
been doing in our group to think about, you know, how do we push forward and make these higher

433
00:41:03,640 --> 00:41:09,000
resolution, you know, surface displays. And I think we've had some great promise in looking

434
00:41:09,000 --> 00:41:13,880
at electrostatic adhesion, as well as kind of new approaches to making more continuous

435
00:41:13,880 --> 00:41:18,600
surface displays. But I think it's really clear to us and probably you as well that the hardware

436
00:41:18,600 --> 00:41:24,200
will really never perfectly render the real world, right? The real world is so rich and very

437
00:41:24,200 --> 00:41:30,920
complex. And so I think there's this big gap between that. But the interesting thing is that

438
00:41:31,000 --> 00:41:36,920
our perception is also imperfect as well. And so maybe we don't need to have perfect hardware

439
00:41:36,920 --> 00:41:41,640
when we're considering these types of displays. And so the last part of my talk, I want to talk

440
00:41:41,640 --> 00:41:46,600
about some of the work we're doing in terms of using visual haptic illusions to improve the

441
00:41:46,600 --> 00:41:51,400
perceived performance of these types of tactile displays or other types of shape displays.

442
00:41:52,360 --> 00:41:57,720
So the first is work from my former PhD student, Parastu Abtaiki, who is now starting at Princeton

443
00:41:57,720 --> 00:42:04,120
University next year, that's looking at this same problem of how we might use the fact that our

444
00:42:05,560 --> 00:42:10,200
basically our visual perception is often dominates our haptic perception. And so

445
00:42:10,200 --> 00:42:15,320
one way to illustrate this is basically our proprioceptive system is not very good. And so

446
00:42:15,320 --> 00:42:21,320
if I am able to basically touch my fingers together in front of my face, I'm actually often

447
00:42:21,320 --> 00:42:26,040
using my visual system to really help me with that. But if I try to do it above my head,

448
00:42:26,040 --> 00:42:31,160
I can sometimes get it, but you'll find that it's not as accurate. And so again, that's because of

449
00:42:31,160 --> 00:42:35,960
all the kind of errors along the line in terms of, you know, our different joints and different

450
00:42:35,960 --> 00:42:41,320
mechanical receptors that we have. But our proprioceptive system has more noise and air

451
00:42:41,320 --> 00:42:44,920
than our visual system. And therefore, our, you know, brains, when we're thinking about

452
00:42:44,920 --> 00:42:50,360
integrating this multi sensory integration, sorry, integrating this multi sensory information,

453
00:42:50,360 --> 00:42:55,720
tend to rely on our visual system. And so as I mentioned before, there's many of these limitations

454
00:42:55,800 --> 00:43:01,160
of these shape displays that I talked about in terms of low spatial resolution, limited display

455
00:43:01,160 --> 00:43:07,640
size or low actuation speed. And the intuition or insight that Parastu had was, oh, how do we

456
00:43:07,640 --> 00:43:13,000
leverage this fact that our proprioceptive system isn't very good to sort of increase the perceived

457
00:43:13,000 --> 00:43:20,040
resolution. And this really kind of essentially builds on a technique from the field of virtual

458
00:43:20,040 --> 00:43:24,200
reality that's called redirected touch. Some of you might be familiar with the idea of redirected

459
00:43:24,280 --> 00:43:29,800
walking, where again, we can kind of steer people in virtual reality by having some slight offset

460
00:43:29,800 --> 00:43:35,480
between where you are in the real world and where you see you are, where you see yourself or where

461
00:43:35,480 --> 00:43:41,160
you see your hand in the virtual scene. And so what we can do is essentially apply some small

462
00:43:41,160 --> 00:43:47,080
virtual offset as I'm reaching that basically makes it seem like my hand as I move straight

463
00:43:47,080 --> 00:43:53,560
is moving to the left. And my basically visual motor system will basically compensate for that

464
00:43:53,560 --> 00:43:59,000
and make my hand move to the right in the real world to basically compensate for that

465
00:43:59,000 --> 00:44:04,760
bias or shift to the left. And so we can basically computationally steer where a person's hand is

466
00:44:04,760 --> 00:44:11,080
going by applying these offsets in the virtual world. And by leveraging that redirected touch

467
00:44:11,080 --> 00:44:16,120
effect, we're able to address some of these different aspects of low spatial resolution

468
00:44:16,120 --> 00:44:21,320
and low actuation speed. And so I'll talk about some of the ways in which we're doing that here

469
00:44:22,280 --> 00:44:28,920
using angle redirection, scaling up and vertical redirection as well. So you might be familiar

470
00:44:28,920 --> 00:44:33,320
with this idea of anti aliasing or the aliasing effect that happens when we look at

471
00:44:33,960 --> 00:44:38,920
graphical display. This aliasing effect is actually also very pronounced in these tactile

472
00:44:38,920 --> 00:44:44,120
displays as well. And so one way that we were trying to mitigate this low resolution is by

473
00:44:44,120 --> 00:44:50,280
trying to essentially get rid of this aliasing that happens when you display a vertical or,

474
00:44:50,280 --> 00:44:57,240
sorry, a diagonal line. And so if we see here, as we move along this surface here,

475
00:44:57,960 --> 00:45:02,280
you know, basically we have this kind of, again, aliasing effect that you can feel these bumps

476
00:45:02,280 --> 00:45:09,480
as well. And so what if instead we could redirect you so that you're moving along a straight line,

477
00:45:09,480 --> 00:45:14,200
which is very doesn't have that aliasing problem. But in the virtual world, you think you're moving

478
00:45:14,200 --> 00:45:20,120
along this diagonal line. And so again, by applying this slight offset between where your hand is

479
00:45:20,120 --> 00:45:25,400
in the real world and where it is in the virtual scene, we can make you touch different areas of

480
00:45:25,400 --> 00:45:32,200
the display and not be able to realize it or perceive it. In terms of the overall resolution

481
00:45:32,200 --> 00:45:37,400
of the device, we can think about the number of pixels or taxles that you're in contact with as

482
00:45:37,400 --> 00:45:42,920
you move over a surface. And so one of the challenges is if we have a small object that

483
00:45:42,920 --> 00:45:47,400
we're rendering on a tactile display of a fixed resolution, then you're not going to encounter

484
00:45:47,400 --> 00:45:53,640
that many taxles as you move along the surface. So again, what if we could change this essentially

485
00:45:53,640 --> 00:46:00,120
control the display ratio, or utilize this offset between where my real hand is and my virtual hand

486
00:46:00,120 --> 00:46:07,080
is. And in the virtual scene, render a small object, but in the real world, render a larger

487
00:46:07,080 --> 00:46:12,600
object of the same version, and we can render it in higher resolution. So again, we can change the

488
00:46:12,600 --> 00:46:17,960
ratio between where I'm interacting in the real world, where I'm interacting in the virtual world,

489
00:46:17,960 --> 00:46:23,160
and leverage that to improve the perceived resolution. And we've done this also for things

490
00:46:23,160 --> 00:46:29,000
like extending the height of the display, the workspace limitations of it, and other things

491
00:46:29,000 --> 00:46:34,840
like that as well. So here, you can see as I'm moving up, I have some fixed amount of range

492
00:46:34,840 --> 00:46:40,600
that I can move up. And then here, we're able to, again, offset where you think your hand is

493
00:46:40,680 --> 00:46:46,440
in the virtual scene. And so you can think that basically the tactile display has

494
00:46:47,080 --> 00:46:54,360
larger pins than we can actually create with these displays. We've run a lot of psychophysical

495
00:46:54,360 --> 00:47:02,120
studies to find what these thresholds are, and also find that when we display an offset

496
00:47:02,120 --> 00:47:07,560
underneath that threshold, essentially, people perceive it as being higher resolution without

497
00:47:07,560 --> 00:47:12,360
noticing that it's there. But we've also found some interesting effects in the difference between

498
00:47:12,360 --> 00:47:17,720
active versus passive touch, where in the active touch conditions, that's where you're

499
00:47:17,720 --> 00:47:24,920
moving your hand as well. So for example, in that angle redirection, we can't offset people's hands

500
00:47:24,920 --> 00:47:30,280
as much as we can in the passive touch condition, where for example, the motor is moving your hand,

501
00:47:31,000 --> 00:47:34,280
where we can offset people more. And so again, that has to do with kind of the

502
00:47:35,240 --> 00:47:40,520
you know, forward model that people have in their sensory motor system in terms of, you know,

503
00:47:40,520 --> 00:47:46,040
basically, my predictions of where my hand might be. And so I'm more willing to allow

504
00:47:46,040 --> 00:47:52,360
things to have noise and there to be more air when I'm being moved versus I'm moving myself

505
00:47:52,360 --> 00:47:57,000
as well. So we've looked at ways in which we can leverage this to create different types of

506
00:47:57,000 --> 00:48:02,520
applications such as, you know, improving the resolution or, or, you know, again, increasing

507
00:48:02,600 --> 00:48:08,840
the vertical redirection as well. But as you can see from those types of systems, you know,

508
00:48:08,840 --> 00:48:14,520
these illusions really only work in this small area of the display. And so another set of work

509
00:48:14,520 --> 00:48:19,720
that we've been doing is trying to improve the kind of scalability of how we might apply these

510
00:48:19,720 --> 00:48:24,680
to a much larger area and create essentially what are called encountered type haptic displays that

511
00:48:24,680 --> 00:48:30,200
might operate over a larger region. So we put these types of haptic displays, tactile displays,

512
00:48:30,200 --> 00:48:35,960
on, you know, robotic arm. And then when we reach out, the robotic device can be there in time.

513
00:48:35,960 --> 00:48:40,440
But one of the challenges with this type of approach of encountered type haptic devices is

514
00:48:40,440 --> 00:48:47,480
that oftentimes the device has some limitations as well, right? So the device may arrive late or

515
00:48:47,480 --> 00:48:52,520
it might be out of the workspace of that robotic system. So I want to touch something up here,

516
00:48:52,520 --> 00:48:56,920
but the robot is only down there. And so again, there's a number of challenges with these

517
00:48:56,920 --> 00:49:02,360
encountered type haptic devices in terms of different reachability issues that lead to these

518
00:49:02,360 --> 00:49:07,320
uncertain spatial discrepancies, which really kind of affect people's perceived performance

519
00:49:07,320 --> 00:49:13,560
of these devices. So we've done a few things in this area to improve the essentially the ability

520
00:49:13,560 --> 00:49:19,800
for these devices to work by essentially redirecting your hand to the reachable area of the robotic

521
00:49:19,800 --> 00:49:31,000
system. Great. Okay, great. So anyway, basically, we can do some cool things in terms of this,

522
00:49:31,000 --> 00:49:35,240
you know, redirection to guide people into different people's space. And we've also done

523
00:49:35,240 --> 00:49:42,280
some interesting work on trying to apply basically model predictive control to basically run this

524
00:49:42,280 --> 00:49:47,160
in real time to improve the perceived performance using a model of human reaching and sensory

525
00:49:47,160 --> 00:49:55,560
integration. So I think that I'll conclude there with just one short statement, if I didn't find it,

526
00:49:56,280 --> 00:50:02,920
which is that, you know, we started out this work really thinking about how real does haptics need

527
00:50:02,920 --> 00:50:08,520
to be. And I think where our group is going is really trying to think more about how real

528
00:50:08,520 --> 00:50:14,200
does haptics need to seem and really trying to leverage a sensory motor control perspective

529
00:50:14,200 --> 00:50:19,480
to optimize both the hardware and the software together. So I'd like to thank, you know, my

530
00:50:19,480 --> 00:50:23,960
PhD students and postdocs in my lab that contributed to this work, as well as our funding sources.

531
00:50:23,960 --> 00:50:30,440
And I guess I'd be happy to answer any questions. Yeah, I thought it was till 130. So my apologies

532
00:50:30,440 --> 00:50:36,760
on that mark. Yeah, so thanks so much.

