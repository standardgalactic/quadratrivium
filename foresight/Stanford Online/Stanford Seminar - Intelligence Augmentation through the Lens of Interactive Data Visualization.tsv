start	end	text
0	16440	Just such a treat to be back, I spend many hours on that side of the room, so it's wild
16440	19840	to be on this side of the room and going, whoa, there was actually like monitors up here,
19840	25440	like that's how the speakers kept track of where in their talk they were, so that's good
25440	27280	to know.
27600	31440	This is sort of the first set of talks I've given since the pandemic, and so I thought
31440	36720	it was a really great opportunity to talk about some new ideas that have been on my
36720	42000	mind, and particularly with all of you as my captive audience, I thought that I would
42000	48840	use this talk as an opportunity to think out loud about what the role of HCI should be
48840	55760	in the face of all of this really incredible rapid progress that AI and ML have made, particularly
55760	61920	kind of scoped in the last six months or so.
61920	68480	As I was trying to think about what the role of HCI should be, I was reminded of this figure
68480	75400	from Jonathan Gruden's 2009 article in triple AI about how AI and HCI are two fields that
75400	77880	are divided by a common focus.
77880	83080	As you can see in moments where AI makes a lot of progress, it's almost like the pendulum
83120	91000	swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches
91000	97960	of human intelligence augmentation or amplification, but also I think HCI is in a more established
97960	107200	stronger position than it's ever been in the past, and so I really think it's our responsibility
107280	113120	to think about what that counterbalance to ever-increased automation should be.
113120	118240	So I often, in moments like this, like to sort of turn to history and ground myself,
118240	123520	and so if we cast back to the first AI winter with Sutherland's sketchpad, right around
123520	129320	that time there was this foundational paper written by Lick Leiter at MIT titled Man
129320	134320	Computers in Biosas, and I think the gendering is unfortunate and unfortunately reflective
134400	140400	of the times, but nevertheless in this paper, Lick Leiter put forth this really compelling
140400	147900	vision about the ways in which a computer could interact with us through this intuitive,
147900	154660	guided trial and error procedure, turning up solutions and revealing unexpected turns
154660	159240	in the reasoning, and I was really tempted to put this sort of side-by-side with this
159320	165560	very recent demo that OpenAI released with ChadGPT plus plugins where you can upload
165560	170560	this music.csv data set and then start to have this very natural language interaction
170560	174960	to ask what are the columns in the data set, how many rows in there in the data set, and
174960	180240	then even say, can you give me some basic visualizations of this data set, and it thinks
180240	186400	a little bit, it's working real hard, and there you go, it produces sort of three visualizations
186440	192240	and even starts to give you maybe something that looks like an explanation, and I wonder,
192240	196840	is it time to roll out our mission accomplished banners? Like have we achieved Lick Leiter's
196840	202160	vision to think in interaction with a computer in the same way that we think with a colleague
202160	207760	whose competence supplements our own? Now, I don't think it's time to roll out the mission
207760	211840	accomplished banners, but I'm hopeful that the reason it's not that is not just my sort
211880	217960	of hope that we haven't been put out of jobs, but rather that there is something more to do.
217960	224960	So two years after Lick Leiter's man-computer symbiosis, Douglas Engelbart wrote up this
224960	230240	really incredible framework called augmenting human intellect, and right in the introduction
230240	236440	of this piece, we already start to see how Engelbart is defining a much more expansive
236480	242680	role of human augmentation. So the idea is not just about problem solving, which he does
242680	247640	mention right at the end there, to derive solutions to a problem, but it's also about
247640	254640	using computers to help us think. It's to increase our capacity to approach a complex
255680	261040	problem situation, to gain comprehension really about this thinking and not just the problem
261040	266280	solving pieces, and really what I like is how he thinks we'll get there. Certainly there
266280	270920	will be sophisticated methods, high powered electronic aids, but to me the part that really
270920	276800	resonates in his prescription here is streamlined terminology and notation, and that's going
276800	283440	to be a theme of my talk here, certainly one of the themes that underlies my group's work.
283440	289000	And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work
289000	292880	with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the
292920	298440	top right-hand corner, on this system called B2. So this is a Jupyter notebook, it's a
298440	303720	very commonly used data science environment where people can start to write code in the
303720	309840	style of a Python REPL, but what B2 does is saying, well, in addition to that sort of
309840	315720	linear style of data science analysis and programming, there's a lot of value in a more
315720	321000	visual analysis dashboard style interface like Tableau. And so what B2 tries to do is
321040	326160	bring these two pieces together. So you can see once I've invoked B2 it adds this on the
326160	331960	sidebar, and I can start to issue regular sort of Python, you know, pandas commands
331960	336960	like looking at the data frame, getting a, you know, a sense of how many rows there are,
336960	341600	what the columns are, and now I can start to write some code to do a little bit of data
341600	346120	transformation and visualization. Notice here in all of these steps, you know, when I'm
346160	350800	authoring a visualization, I don't have to specify what that visualization should look
350800	356800	like, right? I'm just calling these .viz methods on the data frame, and B2 behind the scenes
356800	362000	is figuring out what sort of visualization actually makes sense based on the history
362000	367520	of the transformations that were performed on the data frame. So in the case of year,
367520	372400	for instance, if I've grouped by year, the most sensible visualization to produce is
372440	378200	a histogram of the number of counts of data records across years. You might have also noticed
378200	383000	in the video that if I click the fields on the right hand side there, that it automatically
383000	388240	produces an equivalent visualization, but it doesn't stop there. It adds, you know, the code
388240	394840	and tags them with these little, you know, yellow emojis to indicate that there's actually sort of
394840	401080	a common shared representation here, right? Clicking on the sidebar not only produces the
401080	406440	visualization, but produces the equivalent code as well. And what's interesting is that these
406440	411920	visualizations aren't just output mechanisms, but I can start to interact with them to sort of do
411920	418040	this cross filtering interaction. So all the other bars update to reflect the data shown in the
418040	424640	highlighted bars, and B2 is keeping this as an interaction log that is semantically meaningful
424640	429680	to me. So this interaction log doesn't comprise mouse clicks and keystrokes and things like that,
429920	435040	but it's expressing data queries, right? Which states have been selected? And I can use that
435040	441520	data query to perform subsequent sort of analyses based on my interactive results. So I can say,
441520	447840	great, I'm gonna, you know, copy some code to the clipboard, paste it in as a data query to look
447840	453280	at what the interactive selection should be, and then, you know, proceed with some other sort of
453280	459440	visual analysis. And so as we're sort of, you know, looking at these two forms of interaction,
459600	464720	I was trying to figure out, well, some things feel the same, right? I've got that kind of
464720	469680	conversational back and forth. I'm sure on the left-hand side with ChatGPT, it's a more natural
469680	473800	language conversation. On the right-hand side, it's more of a repel conversation. But also,
473800	479480	things feel qualitatively different. And how do I actually kind of characterize what is the same
479480	484000	and what is the difference? And I thought really hard about it. And I realized that actually,
484480	490240	maybe what still matters is direct manipulation, right? And by direct manipulation, I don't mean
491280	495440	just the sort of Ben Schneiderman version of the term, which is, you know, associated with
495440	500960	graphical user interfaces and having a representation on screen that you can manipulate and undo
500960	506240	redo and things like that. But what I mean here is the deeper treatment of direct manipulation
506240	511680	that three cognitive scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about
511680	518640	the mid-1980s. So in particular, in Hutchins et al's treatment of direct manipulation, they sort
518640	524320	of, you know, imagine direct manipulation to be this cognitive process between a user's goals
524320	530880	and the user interface. And, you know, they identify this gulf of execution that exists when a user
530880	536800	has to translate their goals into commands that they execute on the user interface. And similarly,
536880	542320	a return gulf of evaluation when a user has to figure out, well, did the UI do the thing that
542320	546720	I was expecting it to do? Right? I'm seeing a lot of nods in the audience because, you know, if you've
546720	552880	had experience in user interaction design, user experience, you've maybe experienced these terms
552880	558880	gulf of evaluation and execution. But what I find interesting in this 1985 paper is that they went
558880	564960	one level deeper. So in particular, they identified this idea of a semantic distance,
564960	572640	which is basically how users take the fuzzy notions in their head and translate those into
572640	579680	the nouns and verbs of the user interface, right? So going, doing that sort of sense-meaning operation
579680	586320	of transforming your intentions into the particular actions that might exist in the user interface.
587120	591600	And in addition to the semantic distance, they identified what I love. I love this term in
591600	596720	articulatory distance, right? So it's not necessarily the meaning that we care about anymore,
596720	602080	but the way in which we're conveying that meaning through the UI. And this is particularly important
602080	608400	because you might have several user interfaces that all express the same semantics, right? You can
608400	614720	conduct the same set of, you know, operations with them, the same nouns and verbs. But the way you
614720	619600	do that might be different because one interface might be graphical, the other one might be textual,
619600	624720	another one might be conversational, gesture-oriented, etc. And their claim in this paper was that
624720	630800	that articulation, the form of that meaning is really, really important, just as important as
630800	636160	the semantics. And of course, these distances exist on the Gulf of Evaluation as well. So the
636160	642880	articulatory distance is how do I perceive the changes that occurred in the UI and start to bring
642880	650640	meaning to that perceptual operation by interpreting and evaluating the degree to which
650640	656640	they met my goals. So this is actually going to give us the kind of conceptual machinery for
656640	661920	the rest of the talk. And it's a little bit dense. And so I want to return to sort of the prior two
661920	667600	examples and think about how they manifest these two kinds of distances. So in the case of the
667600	672560	chat GPT example, you know, if we start with semantic distance, I would say that, well, the
672560	677680	semantics aren't really well defined, right? They're not really explicit. Because what these models
677680	683840	have done is they've learned over, you know, vast corpuses of text, often just text that is present
683840	688960	on the internet. And so what they've learned is this latent space that is very ambiguous in the
688960	694240	semantics that are encoded in that latent space. So as a user, it's hard for me to know how to
694320	699360	translate my intentions into something that the system can understand because I don't know what
699360	706720	it is the system knows about the world. But as I'm sure many of us are aware, like prompt engineering
706720	712400	is a thing, right? So if I figure out exactly how to craft my, you know, natural language
712400	717760	expression, suddenly I can get the model to very rapidly almost zero shot adopt the semantics that
717760	723120	I want. And that feels like a very powerful, you know, affordance that we've not necessarily had
723120	730720	before. On the other side, you know, the semantic distance in the Jupiter notebook in B2 had explicitly
730720	735600	defined semantics, right? We have the explicit semantics of pandas on the data frame of the
735600	740960	visualization library of being able to click on the fields in a graphical user interface to produce
740960	747040	visualizations. And every time I did that, I had the shared representation of the code. So either I
747040	750480	would offer the code and it would produce a visualization or if the system produced some
750480	756320	code, I could go in and comment and uncomment entries or tweak the code in a particular way
756320	761440	and things like that. And so it gave me the shared representation that allowed me to bridge
761440	769200	between input and output mechanisms really, really easily. With articulatory distance in chat
769200	776000	GPT, right, natural language, it's been enormously powerful because it's reduced the sort of learning
776080	781280	threshold for a lot of things, right? So if I don't know exactly what it is I want or how to
781280	787440	sort of pose it to the question, I can lean into the ambiguity of natural language and chat GPT
787440	793760	catches up to my intentions pretty rapidly, which is great. But conversely, sometimes I know exactly
793760	799760	what it is I want. And it's really frustrating to have to express precise operations through the
799760	806000	ambiguity of natural language. And then as a result, because of the fact that natural language
806000	810320	is the only mechanism so far by which we can interact with many of these models, there's a
810320	815760	disconnect if your output is visual, like the case of visualization. So I can't interact with
815760	821040	the visualizations in any way to do subsequent back and forth interactions with the model. Now,
821040	825280	I don't think the second point is sort of a fundamental limitation, but it's certainly, you
825280	831280	know, the state of where we are today. And on the other hand, with, you know, Jupiter Notebook and
831280	836480	B2, with the articulatory distance, we've got basically the inverse of this, right? We've got a
836480	842160	nice precise programmatic syntax. So if I know that syntax, I can work really, really efficiently,
842160	848560	right? Sort of a common affordance of many sort of command line style interfaces. But I really
848560	854320	need to learn that syntax to be effective. And in some cases with pearly design syntaxes, which
854400	859840	I might maybe argue, Pandas is an example of, right? I constantly have to look up the documentation
859840	864800	for, right? There's a learning curve associated with it that slows people down. Yeah, Michael.
877440	882000	Yeah, so the reason I put it, I think this is a great question, you know, what lies in semantic
882000	887040	and articulatory. And oftentimes it is quite a fuzzy distinction. The reason I put this in
887040	893120	articulatory is my experience with Pandas oftentimes is I know what it is I want to do, right? I know
893120	897920	the sort of operation I want to perform on my, on my data frame. I just don't know the specific
897920	904880	syntax that I need to look up. Exactly, exactly. But certainly, you know, if, if you don't know
904880	909280	what it is you want to do, then the affordances of natural language absolutely help because you
909280	914960	can kind of, you know, pose things in really fuzzy ways and, and kind of iterate towards your outcome.
917600	921840	And, and I think you see some of this ambiguity in, in sort of, you know, the distinction between
921840	927280	semantic and articulatory distance here with this, this last point where because there are consistent
927280	933680	semantics that actually has this knock on effect on the articulation because now there's a shared
933680	938720	representation of input and output and that simplifies that articulatory distance as well.
939280	944960	So there's not quite that disconnect that we see on the chat GPT side. And so, you know, that's a,
944960	951200	that's, you know, I found semantic and articulatory distances to be a really helpful sort of framework
951200	957840	and I wanted to use it to sort of analyze the very last step in the output that that demo produced. So
957840	962960	it, it, you know, it's basically this, this thing that masquerades as an explanation of the
962960	968400	visualizations that chat GPT produced. But if you actually look at what it says, right, here's some
968400	973520	basic visualizations. Number one, a histogram of song durations colon. This shows the distribution
973520	978720	of song durations in seconds. All right, fair enough. Scatter plot of song hotness versus artist
978720	983600	familiarity. This shows the relationship between song hotness and artist familiarity. Well, I would
983600	989120	hope so. And then bar chart of the top 10 most frequent artist names. This shows the top 10 most
989120	995280	frequent artist names in the data set, right. These are not really explanations, but they're
995280	1000720	pretty provocative or evocative in the potential that these models might have in allowing us to
1000720	1006400	produce these textual descriptions of visual artifacts. And certainly, you know, a lot of,
1006400	1011840	of, of people, certainly lots of big tech companies have thought about the ways in which you could use
1012640	1017440	all kinds of machine learning models, not just LLMs to do the sort of rich description of visual
1017440	1023120	content and particularly for this sort of these accessibility use cases, like how do you describe
1023200	1029120	these kinds of artifacts to people who are blind or have low vision. And lots of people have studied
1029120	1033760	the degree to which these models are effective and found maybe unsurprisingly that they're not
1033760	1039120	terribly effective right now, right. So here is a quote from a participant from one of our studies
1039120	1043680	who says, you know, the reader wouldn't get much insight from texts like this, which not only,
1043680	1048640	you know, is problematic because it doesn't effectively convey information, but more troublingly,
1048640	1053120	it actually increases the burden that readers face when they're trying to make sense
1053120	1058320	of this output, right. There's a lot of sort of noise that gets added to that experience.
1059120	1063840	Another participant, you know, says very, very interestingly, the problem with these textual
1063840	1070560	descriptions is also that it robs me of control of consuming the data, right. A participant,
1070560	1075280	another participant said, I want to have the time and space to interpret the numbers for myself
1075280	1081760	before I read any kind of textual description that does the analysis for me. And so to me,
1081760	1087280	these sound very similar to issues associated with a semantic and an articulatory distance,
1087280	1092240	right. That first quote talking about, well, these texts aren't conveying anything interesting.
1092960	1097360	The second set talking about, well, I want to have that time and space, I want to be able to
1097360	1104240	control the form with which that text is conveyed to me. And so I want to dig into how we might
1104240	1109040	address these two distances in the case of accessibility. But before I do that, I want to
1109040	1115440	give us a sense of how people who are blind or have low vision experience, you know, the
1116240	1121360	internet and graphical interfaces today. So I'm going to turn things over to my PhD student,
1121360	1126720	Jonathan Zhang, who will give us a quick demo of an assistive technology called a screen reader
1126720	1129120	that basically narrates on-screen content.
1134240	1151120	So here I can demonstrate what the accessible HTML version of our paper looks like to a screen reader.
1164960	1175840	So as you can see, what a screen reader does is it basically sort of linearizes the operation
1175840	1183680	of, you know, reading, perceiving, understanding graphical content on a user interface. And in
1183680	1187920	particular, you might notice that the narration was actually quite rapid, right. And this is
1187920	1194960	actually a slowed down version of what, you know, proficient screen reader users use, which is often
1194960	1201040	much, much faster. But what is interesting about the screen reader use case is that it forces that
1201040	1207040	linearity, right. And the key challenge in figuring out the articulatory distance in the case of
1207040	1212160	accessibility is how do you take visualizations that probably all of us in the audience have
1212160	1216560	slightly subtly different ways of reading, right. Maybe some of you start by reading the title,
1216560	1221600	then moving to the axes, then looking at, you know, the shapes, while others might start by
1221600	1227280	looking at the most salient trend and then start to, you know, map out to what the axes and legends
1227280	1233280	and stuff like that are. How do we take all of that rich diversity, but linearize it? So the
1233280	1238800	people who use screen readers can nevertheless have that same, you know, choice in meeting a
1238800	1245440	visualization, but under these conditions. And so the way we have chosen to do that is basically
1245440	1252880	by restructuring the content of a visualization into a text-oriented hierarchy. So at the top,
1252880	1257600	at the root of this hierarchy is just a summary of the chart, probably the trends that are shown
1257600	1263760	in the chart. And then the hierarchy branches off into the individual sort of data fields
1263760	1268960	or the encodings in this case, right. The x-axis, the y-axis, the legend and things like that.
1268960	1275120	And then people can start to drill down in ways that maintain some correspondence with the visual
1275120	1281600	artifacts. So one step below, you know, the x-axis is stepping through them by the major ticks,
1281600	1285200	right. One step below the major ticks would be minor ticks and then ultimately you would get
1285200	1289760	to the individual data points. So let me throw things back to Jonathan to give us a demo of how
1289760	1297920	this works. A scatter plot of Penguin data. And to a screen reader, our system represents this
1297920	1304480	scatter plot as a keyboard navigable data structure that contains text descriptions at
1304480	1311200	varying levels of detail. So when a screen reader user first encounters this visualization on a page,
1311200	1316720	they'll be able to read off a high-level alt text description of what the chart is. So
1319840	1324960	and if they're interested in getting more detail about this visualization, they can dive in by
1324960	1331840	pressing the down arrow key to descend one level in the hierarchy and access descriptions about
1331840	1335760	the different encodings of the scatter plot. So I'm going to press the down arrow key.
1339760	1345120	I can press the left and right arrow keys to flip through descriptions of the other axes and
1345120	1360640	legends. Cool, so let's say I am interested in getting more information about the x-axis. I can
1361360	1366800	use the left arrow key to navigate back to the x-axis description and then press down one more
1366800	1381360	time to descend a level of detail into the x-axis. So on this level underneath the x-axis, I'm
1381360	1388240	accessing descriptions of intervals along the x-axis and it's reading out to me how many data
1388240	1392640	values are contained within each interval. So by pressing left and right, I can kind of get
1392640	1406240	a sense of the distribution of data along the x-axis. So let's say I am interested in this
1406240	1412800	range from 190 to 200. I can then press down arrow again to dive into the individual data points
1412800	1428560	that are contained within this interval. So let's say that instead of moving up and down
1428560	1434160	this hierarchical structure, I would rather just move around the x-y grid in the scatter plot,
1434160	1440720	as if I were kind of feeling around a tactile graphic, for example. I can start by navigating
1440720	1451200	over to the grid view of the scatter plot. And once I descend into this part of the hierarchy,
1451200	1457760	I can use the WASD keys to move up and down different squares along the grid.
1457920	1471600	And so similarly to before, it's starting off by giving me the number of data values that are
1471600	1476320	contained in that square so that I can get a sense of the distribution of the data.
1477680	1485920	And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges.
1485920	1492720	And this was the first time he felt like he actually understood and could build a mental
1492720	1499120	model of what it was that a scatter plot was representing. We saw these sorts of comments
1499120	1505360	reflected in user studies that we ran about how the form of this textual output really
1505360	1510720	influenced participants' mental model of what the data was, what the trends were, and things like that.
1510720	1515520	So one participant, for instance, said, I now know how to drill down and up between different
1515600	1521600	layers in the data to get an overall picture. And it gives me a different way of thinking.
1521600	1526400	And another one said, I'm thinking more in spatial terms because this is just a new method
1526400	1532320	for navigating and moving through the grid and drilling down to information and things like that.
1532320	1539120	And so what I find interesting here is that at every step, the semantic content stayed exactly
1539120	1544160	the same. And there wasn't even very rich semantic content. It was a range and then a count of the
1544160	1550640	data values. All we manipulated was that articulation, that form, giving it a hierarchical
1550640	1556800	nature, adding all of these different navigational affordances, and just manipulating the articulation
1556800	1562640	had this huge impact on people's mental models of the data. And I think that we're really just at
1562640	1568080	the tip of the iceberg of these more accessible structures. Currently, in my group, we're thinking
1568080	1574240	about just the impact that token order has on how people using screen readers build up those
1574240	1579680	mental models. If you're constantly prompting them with the range first rather than the actual data
1579680	1585520	values, does that introduce friction to their capacity to build that mental model and things
1585520	1592880	like that? But in all of this, where is semantic distance? How do we actually start to make that
1592880	1598000	textual descriptions more interesting and meaningful? And this is where I think LLMs can
1598000	1604240	really help us. For one reason, it's because there's just a sheer amount of textual content we need
1604240	1610400	to be able to produce that is infeasible to expect a human to sort of manually author.
1610400	1614880	But there are other sort of implications that we'll touch upon really shortly.
1614880	1619360	But before we can get LLMs to actually sort of produce the content we want,
1619360	1625840	what we need to do is shift from that very latent space with implicit semantics to a set of explicit
1625840	1632720	semantics. We need to impose a conceptual model onto our LLMs. Or another way of saying that is
1632720	1637760	we need to get the LLMs to understand what a good textual description of a visualization is.
1638560	1644480	And so that's what my then PhD student, Alan Lungard, set out to do. We ran a crowdsource study
1644480	1651520	where we got sort of 2,000 descriptions of charts. And through qualitative coding,
1651520	1656000	we realized there are basically four kinds of semantic content that textual descriptions should
1656000	1662080	convey. The first most primitive layer is basically just the sort of construction details of the chart.
1662080	1668480	What are the titles, the labels, the scales, the units, etc. And accessibility best practices say
1668480	1673680	that this is some of the most important content to convey because it gives people sort of important
1673760	1679200	milestones and landmarks. One level above that are the sort of statistical properties like minimum,
1679200	1684880	maximum, outliers, and things like that. And then one level above that is probably what is cited
1684880	1690080	people we consider the real value of visualization to be. The perceptual and cognitive characteristics
1690080	1695680	like complex trends and patterns, things that automated statistical methods we typically think
1695680	1701520	of as not being sufficient at. And then finally, the fourth and highest level are what journalists
1701520	1706240	often consider to be the real value of visualization, which is the narration that gets associated
1706240	1710800	with it. What is the data story that you're able to tell through the visualization? Can you explain
1710800	1716640	what you're seeing, the causal mechanisms, etc., etc. Now, another reason I think LLMs are really
1716640	1722880	suited for this sort of semantic bridging task is because when we asked sighted and blind people
1722880	1728560	what their preferences were, when it came to these four layers, four levels, we saw really
1728560	1735600	distinct preferences. In the case of sighted people, because we've got our own visual perception
1736640	1743120	doing that sort of bridging of the Gulf of Evaluation, sighted people tended to want higher
1743120	1748240	and higher levels of content being conveyed through text. Blind readers, on the other hand,
1748240	1754000	were pretty significantly divergent. For many of them, they didn't want those level three and four,
1754000	1758800	particularly the level four captions at all, because they wanted that time and space to do
1758800	1768080	the interpretation for themselves. And so here, this visualization to me conveys that LLMs can
1768080	1773200	help us or machine learning models can help us think about sort of personalizing the semantics
1773200	1779120	of a user interface in a way that maybe we haven't had the opportunity to study so far. There's been
1779120	1784400	a lot of work in personalization, but it's often been at that level of the articulation,
1784400	1789520	changing the sizes of buttons and adapting color palettes and things like that. And there's maybe
1789520	1797760	an opportunity now to use LLMs to actually change what the nouns, the verbs, the concepts of a user
1797760	1803120	interface are much more fundamentally. And so the way we're going about doing this in the case of
1803120	1808960	textual descriptions is we're going to be releasing very soon a data set of about, actually now we're
1808960	1813760	over 12,000 pairs of chart captions. And we've generated some of these captions and we've crowdsourced
1813760	1819200	some of these captions. And we started to train baseline models to do this task. And one of the
1819200	1825920	interesting features here is how do we represent the semantics of a chart to a large language model,
1825920	1830640	right? One way could just be let's treat the chart as an image, right? This is just a set of pixels.
1831520	1836960	And unsurprisingly, the baseline models don't do very well at that because a chart is a much richer
1836960	1841920	kind of artifact than just an image, right? It's got all this rich structure. So then we said,
1841920	1846160	great, let's look at a data table or let's look at a scene graph, which is just a fancy way of
1846160	1851200	saying the SVG associated with the chart. And a priori, we would have thought, well, the scene
1851200	1856400	graph is maybe like a good in-between between the computational affordances of data table and
1856400	1862480	capturing some of those perceptual characteristics. Turns out for the LLMs we trained that were
1862480	1867360	all transformer models, they did equivalently well on those two representations. And so one of
1867360	1872800	the things my group is working on right now is a new way of representing visualizations that more
1872800	1879120	directly encode some of those perceptual operations that are otherwise currently implicit in a scene
1879120	1886400	graph that grammar of graphics libraries like VegaLite or GGplot perform. But what's interesting
1886400	1891680	in all of this to me is that through these generative models, the goal has been how do we impose
1891760	1898160	a conceptual model onto them, right? How do we bring some explicit semantics? And I think we're
1898160	1903680	just scratching the surface here as well because I think the chart example case is a really great
1903680	1908240	one where a lot of these representations of charts that we've got right now, the grammar of graphics,
1908240	1912960	for instance, were designed for people to author, right? So we're really good at figuring out how
1912960	1918080	to design programming languages, domain specific languages, to emphasize the cognitive characteristics
1918080	1922560	that are important for human authors. Things like, you know, the cognitive dimensions of notation
1922560	1926880	that cares about, you know, how viscous is the programming language? How many premature commitments
1926880	1931840	is the programming language enforced? But I don't know what it means to design a representation
1931840	1938240	to be suitable for an LLM to operate over, right? Do we restructure the programming language more
1938240	1944720	fundamentally to make it tractable for an LLM? Maybe. So in addition to generative models,
1944720	1950560	my group has also been working with predictive models. And here I think the bridging task is
1950560	1957040	really not about imposing a conceptual model, but bridging it or aligning it to the ones that we
1957040	1962640	already have. And often the way that a lot of this work happens is through the lens of model
1962640	1970480	interpretability. So here is a very popular set of techniques called saliency maps. The idea behind
1970480	1978000	saliency maps is they're trying to depict the most important input features for a particular
1978000	1983280	outcome. So in this case, you know, this is an image, the label should be toy terrier, and here's
1983280	1988560	what a variety of different kinds of saliency methods believe to be, you know, the most important
1988560	1997840	pixels to produce that outcome. Now, I look at these visualizations and I go, well, you know,
1997840	2004400	is it telling me something? Maybe, right? And maybe the reason I believe it's telling me something
2004400	2010720	is because I'm the one doing the perception and interpretive tasks, right? Like if I look at some
2010720	2015120	of those visualizations on the bottom, I go, oh, like, it looks like the dog snout is really
2015120	2020560	important to the classification of a toy terrier or the spots. But it's not actually the saliency
2020560	2025600	method that is doing that interpretation for me. I'm the one bringing meaning to those lit pixels,
2026160	2032320	right? And so as a result, if we think about that gulf of evaluation, it's not the saliency
2032320	2038080	method that's helping bridge that gulf in any way, which is why saliency maps for now have been
2038080	2043840	these tools that we just use in a very ad hoc way that require a lot of manual effort to make sense
2043840	2050800	of. And so a question that my student, Angie Boggast, has been focused on is how do we scaffold
2050800	2055920	that semantic sense making operation, right? Providing some additional structure to help
2056560	2061680	sort of scale it up to make it more reproducible and things like that. And what she's developed
2061680	2067440	is these set of metrics that are very analogous to ideas of precision and recall, but are operating
2067440	2072960	at the level of input features and interpretability. So in many data sets, right, you've got some set
2072960	2078080	of ground truth human annotated features. And what shared interest is looking at is what is the
2078080	2083200	overlap between what a saliency method considers as being important to the classification and what
2083200	2088240	the humans, the human annotators thought was important. And there's actually three different
2088240	2094240	ways that these overlaps can manifest. The first is a sort of ground truth coverage, which is very
2094240	2099360	analogous to ideas of recall, right? It's how much of the ground truth does the model incorporate
2099360	2104000	in its prediction or what is the proportion of the ground truth region that is covered by the
2104000	2109200	saliency region. And if we look at some examples of low coverage on the top and high coverage at the
2109200	2114640	bottom, we can see then the case of low ground truth coverage is actually very little overlap,
2114640	2120400	right, between the ground truth, the yellow region, and the salient region in orange. But I often find
2120400	2125600	that it's actually the high coverage regions that are more interesting to analyze. So if we compare,
2125600	2131200	you know, cases where the model was correct on the right with the green label and cases where the
2131200	2136640	model was incorrect with the red label, we can see in the case of correct high ground truth coverage,
2136640	2141040	there are instances where the model relies not just on the object, like in this case with the
2141040	2147840	cab, but a lot of contextual information as well to ultimately make that correct prediction. But
2147840	2153920	on the flip side, right, with the laptop, the model is doing the same thing, but here the context
2153920	2158000	is actually throwing it off, right? It's actually confusing the model because it's accounting for
2158000	2163680	too much of that context in its decision making. Another kind of coverage is something we call
2163680	2168480	saliency coverage, and this is more akin to precision, right, which is how strictly is the
2168480	2174800	model relying only on ground truth features to make its sort of prediction. And again, you know,
2174800	2180400	if we look at low and high coverage in the case of low coverage, we can see again pretty disjoint
2181120	2187360	sorts of sets. But in the case of the high coverage regions, we can see that, you know,
2187360	2193920	in the case of high saliency coverage, it basically means that the salient regions are a strict subset
2193920	2198560	of the ground truth coverage. But the difference between a correct and incorrect prediction is
2198560	2205360	whether that subset was sufficient to make the correct classification or not, right? So in the
2205360	2210240	case of the Maltese dog, it did indeed only need to look at the head to make that correct prediction.
2210560	2216960	But in the case of the Dalmatian, it probably should have accounted for more of that dog's head or
2216960	2221280	some of the other characteristics associated with the dog. By focusing only on the snout,
2221280	2227440	it ended up sort of arriving at the incorrect sort of classification. And finally, the last metric
2227440	2231920	is something that is very familiar IOU, the intersection over the union. This is sort of
2231920	2236880	the strictest shared interest metric. It's really measuring how aligned the model's behavior is
2236880	2241760	with human reasoning. So if you look at some examples, again, you know, low coverage at the
2241760	2249520	top, we can see, you know, in incorrect cases, totally distinct disjoint sets again. But in a
2249520	2254720	correct instance, I actually find that pretty interesting, right? Low IOU coverage, but it
2254720	2259920	got a correct classification. Now, one could say maybe it got lucky. But potentially, what
2259920	2266320	the signal there is, is that maybe all the model needs is a tiny bit of a wheel associated with
2266320	2272000	a horse, right, to make the prediction that is actually a horse cart and not just a horse, right?
2272000	2278080	And on the flip side, with high coverage, you know, Newfoundland, great, you know, total,
2278080	2282960	total alignment. But in this case, right, incorrect classification, even though there was high
2282960	2288560	coverage, this might suggest, you know, genuinely difficult to classify images, right, even for
2288560	2293600	people. Because if I look at that, a pickup truck seems a totally reasonable guess to have made
2293600	2297680	about the image. I don't know that I've got enough sort of visual information there to call
2297680	2303920	that a snowplow. So shared interest basically gives us a mechanism to start to scaffold and
2303920	2309760	structure, bridge that semantic distance, right? People no longer necessarily need to manually
2309760	2315840	start to analyze these things. And in fact, you know, we analyzed lots of different models across,
2315840	2320800	you know, both vision and natural language and found that different combinations of these shared
2320800	2325040	interest metrics, along with figuring out whether the prediction was correct or not,
2325040	2330800	actually surfaced eight kinds of repeating patterns in model behavior. So we can see human
2330800	2336240	aligned and some of these others we also looked at earlier, right, context confusion,
2336240	2341760	context dependent and so forth. And all of these give us sort of semantics that we can start to
2341760	2347040	play around with through different articulations. So one articulation of these semantics might be
2347040	2352480	a very traditional visual analytics interface, right, where I've got all the different kinds of
2353200	2357920	images that I care about. This is a system we built to help a board certified dermatologist
2357920	2362560	make sense of this melanoma detection model. And you've got, you know, query widgets on the top
2362560	2368720	to sort and filter. You can use, you know, these histograms of the shared interest metrics to really
2368720	2373520	drill into the data. But what was maybe most interesting was what the dermatologist said
2373520	2381680	when they started to analyze that recurring pattern of context dependent cases. So in particular,
2381680	2387760	when they switched to these context dependent cases, the dermatologist started to wonder if the
2387760	2392640	model is seeing something we are not truly appreciating in the clinical image. Maybe there
2392640	2399200	are subtle changes we don't yet understand that the model does, right at the boundaries of the
2399280	2406240	skin region and things like that. And so, you know, to me, this is alluding to the fact of,
2406240	2412320	well, can we as domain experts learn something about our problem domain based on how it is
2412320	2417760	models are operating? And I think we see this more clearly in another articulation of shared
2417760	2422800	interest semantics. Here, what we're doing is basically using shared interest to interactively
2422800	2429040	probe or query that latent space. So we're brushing and using that brushed region as ground truth
2429360	2435840	and then calculating the IOU coverage to figure out what are all the classes that maximize IOU
2435840	2442720	coverage for that brush ground truth. So we can see if I brush over hand, a lot of the classes
2442720	2447760	that get returned are things that are often associated with hands like laptops and cleavers
2447760	2454320	and interestingly enough, hen. So I guess a lot of the images in the ImageNet, you know, data set
2454400	2461840	have people holding hens, right, which is, I guess, kind of interesting. But more maybe profoundly
2461840	2468000	is we could ask a question like, what is the essence of a dog, right? What is the minimal
2468000	2474080	amount of region that I would need to brush for the model to still be convinced that what it is
2474080	2479280	classifying as a dog? So I could start with the whole dog and then brush just on its head and
2479280	2484160	sure, you know, querying which shared interest still returns, you know, dog classes. But then I
2484160	2491600	could use a smaller brush and brush just on the nose and it still returns, you know, German shepherd
2491600	2496880	and sheepdog and Tibetan terrier and things like that. So it seems like according to the model,
2497440	2505840	all it really needs to know about, you know, an object in the image is the sort of shape of its
2505840	2511360	nose or something associated with its nose to be able to classify whether it is or is not a dog.
2511360	2517040	And this seems like a really sort of toy example, but it reflects some of the things that real
2517040	2523200	world scientists are doing. So in particular, you know, there's a researcher at the University of
2523200	2529360	Washington, Julia Parrish that runs this grand crowdsource data collection project around seabird
2529360	2534880	deaths. And the way they train their participants to figure out how to do bird classification
2534960	2539840	is by asking them to measure, you know, the bird beaks and the bird feet and things like that.
2539840	2543840	And so I think it's really interesting that we're seeing maybe some of those sorts of
2543840	2552480	representations creep up in how a model is making its decisions as well. And so where I want to end
2552480	2558480	is sort of being most speculative and where I think, you know, there's scope for HCI to sort of
2558480	2563680	grow. And so, you know, we looked at generative models and imposing a conceptual model on them.
2563680	2569120	We looked at predictive models where the idea was to align conceptual models. But what I think,
2569120	2573840	you know, we're hearing from that dermatologist we're seeing in that last case study which shared
2573840	2581440	interest is the potential to use machine learning models to basically discover new representations
2581440	2587200	of particular problem domains, right? And, you know, again, at my most speculative, I don't know
2587200	2591440	what I would call these, but I would maybe call them abstraction models, right, where the goal of
2591440	2596880	these models is not to produce some particular outcome that I care about, but to maximize what
2596880	2601440	are the different ways of representing the world, right? What are all the diverse abstractions that
2601440	2606720	we could learn about a problem domain like classifying dogs or classifying seabirds or things
2606720	2612000	like that. And I think this is a really interesting opportunity to use machine learning to essentially
2612000	2617920	advance our understanding, advance our science. But I want to be careful here because we've already
2618000	2622400	seen, you know, through this talk, but also in the broader discourse, how generative and predictive
2622400	2627120	models can sort of muddy that, that gulf of evaluation, right? Lots of people are starting
2627120	2632320	to anthropomorphize these models, you know, some people think these models are representing general
2632320	2637120	intelligence or conscience or things like that. And there's a potential with, you know, these
2637120	2643520	abstraction models to make this problem worse by muddying the question of, well, how do we know
2643600	2650640	what we know, right? Like what counts as evidence? Is it evidence because, you know, the model has
2650640	2656960	learned that representation? And how do we validate what that evidence is? In the case of
2656960	2662240	representations that are designed or interpreted or theorized by people, we know how to consider that
2662240	2667360	to be evidence, right? But I don't know what it means for a learned representation to count as
2667360	2672240	evidence. And as all sorts of problems in machine learning, this is not necessarily a problem that
2672240	2679920	is unique to machine learning. So here are three visualizations that were used to discuss the
2679920	2685680	COVID-19 pandemic right at the, the, the peak of the first wave in the summer of 2020.
2688080	2695280	And I'm curious if anything pops out at you, like any reason, you know, to be curious or
2695360	2705840	suspect of, of these visualizations, right? Like no, right? Probably not. Like these seem pretty
2705840	2711680	legitimate, right? Like our world and data, very legitimate data source, right? And if you look at,
2711680	2715520	look at some of these two other visualizations, you might go, you know what, actually the one on the
2715520	2719680	right, that looks like something in maybe a policy briefing or something, right? It looks very
2719680	2727120	sophisticated, lots of good annotation, you know, a style and aesthetic that looks very sort of
2727120	2732880	sophisticated. But you may be catching what I'm alluding to, which is the fact that all three
2732880	2740800	visualizations were used by people on social media to advance the argument that, you know,
2740800	2747200	our response to COVID was overblown. Not that COVID was a hoax, but that our reaction to it
2747200	2754000	was, was way too extreme. That COVID wasn't as serious an issue as it might initially seem.
2754560	2760080	And I want to be really careful about what I'm, when I'm doing here with these charts, because
2760080	2764720	certainly some of the people that were distributing this were bad actors who were ideologically
2764720	2771920	motivated. But through a very long, laborious ethnography, ethnographic process that we
2771920	2777680	conducted, spending six months on five different Facebook groups, we found that a lot of people
2777680	2782560	who were producing visualizations like that were actually displaying many hallmarks of citizen
2782560	2788000	data science. So they were really many of them filling gaps in, in information sort of collection,
2788000	2792400	because they were situated in rural parts of the country where, you know, there wasn't a lot of good
2792400	2797920	data collection. So many members of these groups were hosting, you know, webcasts, live seminars of
2797920	2802800	how to download data from the government website, how to clean it and excel, how to visualize it and
2802800	2808880	things like that. And, and most surprisingly to us, many of them were engaged in discussion that
2808880	2814560	looked like peer review, right? They were critically assessing data sources, discussing metrics,
2815200	2819920	making arguments for which metrics were, were better or not. But all of this was sort of
2819920	2826160	inflected through a sort of frustration with mainstream institutions and maybe even distrust
2826160	2832640	of those institutions as well, right? But ultimately what these groups cared about was bolstering a
2832640	2838720	sense of social unity and civic engagement, right? So this quote I, I find particularly sort of
2838720	2843600	reflective of that sense of, you know, it's incumbent on all of us to hold our elected officials to
2843600	2848560	account so that they make better decisions through data, right? I'm speaking to you as a neighbor,
2848560	2855120	as a mama bear, right? So this is not some sort of ideologically motivated individual who is,
2855120	2859520	who is, you know, trying to be a bad actor. This is just an engaged member of the citizenry.
2860400	2865520	And similarly, you know, oftentimes they were actually more sophisticated than scientists
2865520	2870720	can be. So many of these members were very reflexive about their own data analysis,
2870720	2875760	data gathering process, right? So someone says, you know, I've never claimed to have no bias,
2875760	2880560	right? I'm human, of course I'm biased, here are my biases. Whereas in science, often we like to
2880560	2886240	portray ourselves as being very objective, you know, arbiters of truth. And so in many ways,
2886240	2892720	you know, what was happening in these groups is, is perhaps more sophisticated than what was happening
2892720	2896880	in science and public health at the time. But the question is, so what does this have to do with
2896880	2902640	sort of bridging semantic distances and abstraction models? Well, I think what was happening in,
2902640	2908400	in those groups was, you know, they, they, they disagreed with the definitions of some of these
2908480	2914240	metrics, right? They were living in rural communities. And so the metrics that public
2914240	2919920	health officials were using to, to, you know, define the, the state and scale of the pandemic
2919920	2923760	was not reflected in their lived experience. They were turning around and well,
2923760	2928480	it didn't seem like COVID was an issue, right? And so our colleagues in the humanities and
2928480	2934720	social sciences often advocate for adopting what they call an interpretive, interpretivist lens,
2934720	2939280	right? The idea that knowledge is subjective, it's socially constructed,
2939280	2943920	and that it's composed of many diff, different diverse perspectives that we have to figure out
2943920	2949680	ways to synthesize together. And while that idea has been adopted in pockets of visualization and
2949680	2955520	HCI and CS, so far, I think it's largely been on the qualitative side, because if we think about
2955520	2962080	how to do computation, we have to, you know, we're forced into making decisions about the world and
2962080	2966960	how to represent that world and computational data structures. And what I think abstraction models
2966960	2972480	allow us to do is start to push, but you know, push that boundary a little bit, right? Rather than
2972480	2978000	being focused on developing a model that produces a single best outcome, we might instead be looking
2978000	2982720	to a world in which we are training sort of ecosystems of abstraction models, where we're
2982720	2988080	forcing them to learn really different representations of the world or of a problem domain,
2988080	2992240	and then leaving it up to people to figure out how to synthesize between those learned
2992240	2997760	representations for, you know, some particular policy goal or, you know,
2999840	3005680	thing that they want to optimize for. So with that, I'm happy to take questions about any of
3005760	3007280	what I talked about. Thank you very much.
3031280	3035120	This image that made me make this decision, so do you think the results would be different
3035120	3042720	if you used, like, iFixations in that comparison? That's an interesting question. We haven't
3042720	3051200	considered iFixations for the salency map work, but certainly I think your intuition is right in
3051200	3057040	the sense that, you know, the current way that we've modeled shared interest is pretty brittle,
3057040	3061680	right? It's operating at the level of abstraction of, like, pixels in an image, and how meaningful
3061680	3067600	are pixels really? And so what Angie is working on right now is a way to raise the
3067600	3072160	level of abstraction that shared interest is working on. So in many of these domains, like,
3072160	3078080	you know, ImageNet, the task that we're asking models to do, the labeling task,
3078080	3084240	actually inherits from a much richer knowledge graph or taxonomy or hierarchy or things like that.
3084240	3088000	But right now, at least, you know, there's a little bit of work in hierarchical learning,
3088000	3093600	but most of the predictive models are just learning at the finest level of detail, right?
3093600	3099840	So we're throwing away all that rich information that might be really relevant to how a person is
3099840	3104880	making a decision. So maybe what I care about is not whether it's a Chihuahua or a golden retriever
3104880	3110320	or a laboratory retriever. I might care, is it a dog or really sometimes is it just an object,
3110320	3115920	right? And so what does it look like to do shared interest in more meaningful abstraction space
3115920	3120240	rather than pixels is something we're working on. Yeah, great question. Thanks. Yeah, Will.
3121680	3127040	Thank you for the great talk, Arvin. So going back to Jupyter notebooks and ChatGPT,
3127920	3134160	you talked about how, right, ChatGPT can shell out to some of these nice plugins like for Excel or
3134160	3139440	whatever to try and help people do natural language data science and that there's this
3139440	3144160	articulatory distance due to the difficulty of learning an API. But conversely, you could say
3144160	3149680	tools like co-pilot are sort of the parallel to overcoming that articulatory distance by
3149680	3153600	almost in some sense, what is the same interface expressing a natural language but just in a
3153600	3158720	code comment and then getting back code, right? But just I guess the only difference is its code
3158720	3162560	you can see as opposed to code that's running in some back end that you don't see. And I'm curious
3162560	3167520	if you think there's sort of a synthesis of these two poles, an interface that can take the best of
3167520	3172480	both worlds and offers conversation but still provides access to the code or encourages people
3172480	3178800	to understand the annoying representations. Yeah, absolutely. I thought really hard about
3179520	3186720	which of those examples I wanted to use as the kind of foil to B2. So I did very seriously
3186720	3193760	consider a co-pilot and I sort of agree with your analysis that it's, I think, a much better
3193760	3200880	example of how to integrate the capacity of these LLMs. And I think there's opportunity
3200880	3207200	to push that even further where what I would often want is really targeted mechanisms to
3207200	3213520	introduce ambiguity, right? Right now, the little that I've used co-pilot, it's almost at the level
3213520	3220720	of, well, it's going to produce the whole function, the whole whatever. And often what I want is it
3220720	3226640	to be the sort of parallel prototyper for me, right? I want to introduce, say, a hole in my program
3226640	3231040	and then go, I don't know that I want that hole to be filled in with just one specific
3232080	3236960	outcome, but I want it to produce the whole space and for me to go, well, I want a little bit of
3236960	3243520	this and a little bit of that and so on and so forth. So yeah, I totally agree with there being
3243520	3248080	some really interesting medium of these things. Cool. Yeah, I like that idea.
3250000	3256400	Yeah. Hey, really exciting talk. I'm wondering towards your kind of vision for these abstraction
3256480	3262400	models, I'm wondering like obviously kind of from a human-interpreter interaction perspective,
3262400	3268320	we know like representation matters so much, right? Like isomorphs of representation very much
3268320	3274080	change how people can approach a problem or understand it. But I guess the ways in which
3274080	3280480	they vary and the benefits of these different representations are tied very much to human
3280480	3285520	cognition and perception. And I'm wondering, you know, in some of the examples you're showing and
3285520	3290640	a lot of work in machine learning, we're sort of training things based upon that output. Yeah.
3290640	3295440	And I'm wondering like, you know, are there ways that we can get at more of how people
3295440	3301200	are thinking versus just how they output and how do we get there? Yeah, I love this question.
3302000	3308160	And the reason I love it is also the reason I love sort of that Hutchins et al description of
3308160	3312480	direct manipulation, right? I find the terms that they use there, particularly these two's
3312480	3318240	distances, really evocative terms. Because to me, a distance is something that I would want to
3318240	3324800	measure, right? But so far at least, as far as I know, those terms have largely been descriptive,
3324800	3330560	right? As you saw in my talk, like I use them to be very analytic, but I'm not able to be generative
3330560	3338080	with them in, you know, a very systematic way. So certainly a lot of the work that my group is
3338080	3343440	trying to do right now is in visualization, you know, there's a lot of work that we've
3344240	3349680	inherited in methods from sort of vision science. So we run these studies of human perception.
3351360	3356560	And increasingly, the field is starting to get to, well, how do we start to measure cognition,
3356560	3362320	right? Can we model sort of a decision making task and start to, you know, operationalize
3362320	3366800	that through experimental design? And so we're starting to push in some of those directions
3366800	3371440	as well, but scope to sort of, you know, interaction in a Jupyter notebook, but then
3371440	3375840	starting to see, you know, the impact that interaction has on sort of the downstream
3375840	3381520	analyses people would do, and then see if that actually maps to, you know, their goals or things
3381520	3394480	like that. Absolutely. Yeah. So I'm curious about the, just continue on this line of
3394480	3400080	perception up through cognition, you know, going back to the sort of like,
3400800	3407840	or 10 Cleveland McGill kinds of stuff, the automatic processing was very key to the design
3407840	3413680	of visualizations, especially early on, that the notion was that my encodings were supposed to
3413680	3420960	map on to almost like system one interpretation, right? Like when I see the scatterplot and
3420960	3424560	you know, encoding distance in the following way, I'm gonna draw the correct conclusion.
3424560	3431520	And it's interesting to me that sort of through the transformations you've started to pursue,
3431520	3439600	we're not trying to like encode those into a similar mapping for audio, but instead
3439600	3445360	directly doing the cognition on behalf of the individual. And those seem like orthogonal
3445360	3450880	directions one could go. I'm curious how we find the right point in the design space.
3451840	3457280	I think this is a fantastic question. So the way my group is starting to think about this
3457920	3461760	of like, how do we find the sort of right balance of who is doing the perception,
3461760	3466720	who is doing the interpretation is starting to consider some of these modalities and concert
3466720	3470560	to better understand what the relative affordances of these modalities are.
3470560	3475280	So in particular, Jonathan, who you saw in the demos is leading some really,
3475280	3483040	really cool work right now around what if I'm sort of specifying the visual, the audio,
3483040	3487840	the sort of sonified audio and the textual audio side by side, and then I'm playing them
3487840	3493680	sort of simultaneously through. Do I want, you know, there to be sort of perceptual redundancy
3493680	3499520	where the sonification is sort of emphasizing what is, you know, described in the texture?
3499520	3506560	Do I want these modalities to be complementary? And, you know, sort of TBD, but I think there's
3506560	3511600	some really exciting sort of questions for us to sort of dig into space.
3511600	3514720	Are there similar pre attentive principles for audio? There must be.
3516000	3520480	As far as I know, so I'm, you know, we're just starting to look in the sort of sonification
3520480	3526720	literature. Yeah, as far as we can tell sound is a very, very different perceptual sense
3527520	3533440	than vision. And so even the sort of, you know, basic sort of visual encoding paradigm where I
3533440	3539600	take a data field, I map it to, you know, position color size that breaks down very rapidly for audio.
3540320	3546400	So oftentimes really all the people are able to sort of, you know, detect differences in our sort
3546400	3553600	of pitch and loudness. And even then our fidelity at that is very, very low. And so there might be
3553600	3559120	some pre attentive characteristics. We're certainly looking at some early work in HCI.
3559120	3567120	I think Stephen Brewster had done around sort of ear cons, you know, discreet sort of representations
3567120	3571040	of icons, but through audio and things like that. So there may be some of that there.
3572000	3575920	But at least so far we're so early in our own work that we don't know.
3575920	3577520	Interesting. Okay, thank you. Yeah.
3579600	3583280	I think we're about at time. So if you have additional questions, please mob him after
3583280	3592800	the talk. Thank you, Arvin for joining us. Thank you very much.
