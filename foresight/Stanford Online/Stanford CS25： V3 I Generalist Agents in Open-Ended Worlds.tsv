start	end	text
0	9640	Hi everyone, thanks for coming to our CS35 lecture today.
9640	14240	So today we're honored to have Jim Van from NVIDIA, who we're talking about generalist
14240	21080	agents in open-ended worlds, and he's a senior AI research scientist at NVIDIA, where his
21080	26800	mission is to build generally capable AI agents with applications to gaming, robotics, and
26800	29000	software automation.
29000	33480	He has research spans, foundation models, multi-modal AI, reinforcement learning, and
33480	34800	open-ended learning.
34800	41360	He obtained his PhD degree in computer science from here, Stanford, advised by Professor
41360	42360	Pepe Lee.
42360	49080	And previously, he did research internships at OpenAI, Google AI, as well as Mila Quebec
49080	57760	AI Institute, so yeah, we'll give it up for Jim.
57760	62440	Yeah, thanks for having me.
62440	66880	So I want to start with a story of two kittens.
66880	72320	It's a story that gave me a lot of inspiration over the career, over my career.
72320	74600	So I want to share this one first.
74600	79520	Back in 1963, there were two scientists from MIT held in Hine.
79520	85720	They did this ingenious experiment, where they put two newborn kittens in this device,
85720	88160	and the kittens have not seen the visual world yet.
88160	93120	So it's kind of like a merry-go-round, where the two kittens are linked by a rigid mechanical
93120	97040	bar, so their movements are exactly mirrored.
97040	101040	And there's an active kitten on the right-hand side, and that's the only one able to move
101040	106880	freely, and then transmit the motion over this link to the passive kitten, which is
106880	111760	confined to the basket, and cannot really control its own movements.
111760	117280	And then after a couple of days, held in Hine, kind of take the kittens out of this merry-go-round,
117280	119440	and then did visual testing on them.
119440	123560	And they found that only the active kitten was able to develop a healthy visual motor
123560	129520	loop, like responding correctly to approaching objects or visual cliffs, but the passive
129520	133200	kitten did not have a healthy visual system.
133200	139960	So I find this experiment fascinating, because it shows the importance of having this embodied
139960	146400	active experience to really ground a system of intelligence.
146400	150440	And let's put this experiment in today's AI context right.
150440	155800	We actually have a very powerful passive kitten, and that is ChargerBT.
155800	161880	It passively observes and rehearses the text on the internet, and it doesn't have any embodiment.
161880	166920	And because of this, its knowledge is kind of abstract and ungrounded, and that partially
166920	171760	contributes to the fact that ChargerBT hallucinates things that are just incompatible with our
171760	175480	common sense and our physical experience.
175480	181840	And I believe the future belongs to active kittens, which translates to generalist agents.
181840	186640	They are the decision makers in a constant feedback loop, and they're embodied in this
186640	188120	fully immersive world.
188120	192280	They're also not mutually exclusive with the passive kitten.
192280	198360	And in fact, I see the active embodiment part as a layer on top of the passive pre-training
198360	202560	from lots and lots of internet data.
202560	205000	So are we there yet?
205000	207640	Have we achieved generalist agent?
207640	211760	Back in 2016, I remember it was like spring of 2016.
211760	215440	I was sitting in an undergraduate class at Columbia University, but I wasn't paying attention
215440	216720	to the lecture.
216720	223720	I was watching a board game tournament on my laptop, and this screenshot was the moment
223720	229560	when AlphaGo versus LisaDoll and AlphaGo won three matches out of five and became the
229560	232800	first ever to be the human champion at the game of Go.
232800	234600	I remember the adrenaline that day, right?
234600	236080	I've seen history unfold.
236520	240960	Oh my God, we're finally getting to AGI, and everyone's so excited.
240960	245440	And I think that was the moment when AI agents entered the mainstream.
245440	254920	But when the excitement fades, I felt that even though AlphaGo was so mighty and so great,
254920	258480	it could only do one thing and one thing alone.
258480	265560	And afterwards, in 2019, there were more impressive achievements, like Open AI 5 beating the human
265560	270960	champions at the game of Dota and AlphaStar from DeepMind beat StarCraft.
270960	277200	But all of these, with AlphaGo, they all have a single kind of theme, and that is to beat
277200	278200	the opponent.
278200	285200	There is this one objective that the agent needs to do, and the models trained on Dota
285200	288760	or Go cannot generalize to any other tasks.
288760	292760	It cannot even play other games like Super Mario or Minecraft.
292760	299880	The world is fixed and has very little room for open-ended creativity and exploration.
299880	304800	So I argued that a journalist agent should have the following essential properties.
304800	310360	First, it should be able to pursue very complex, cementally rich and open-world objectives.
310360	314800	Basically, you explain what you want in natural language, and the agent should perform the
314800	317200	actions for you in a dynamic world.
317200	322120	And second, the agent should have a large amount of pre-trained knowledge instead of
322120	327600	knowing only a few concepts that's extremely specific to the task.
327600	329920	And third, massively multitask.
329920	335680	A journalist agent, as the name implies, needs to do more than just a couple of things.
335680	342240	It should be, in the best case, infinitely multitask, as expressive as human language
342240	344440	can dictate.
344440	346200	So what does it take?
346200	349120	Correspondingly, we need three main ingredients.
349600	351960	First is the environment.
351960	358320	The environment used to be open-ended enough because the agent's capability is upper-bounded
358320	361240	by the environment complexity.
361240	365680	And I'd argue that Earth is actually a perfect example because it's so open-ended, this world
365680	370960	we live in, that it allows an algorithm called natural evolution to produce all the diverse
370960	374400	forms and behaviors of life on this planet.
374400	379760	So can we have a simulator that is essentially a low-fi Earth, but we can still run it on
379760	383360	the lab clusters?
383360	387760	And second, we need to provide the agent with massive pre-training data because exploration
387760	391800	in an open-ended world from scratch is just intractable.
391800	394280	And the data will serve at least two purposes.
394280	397520	One as a reference manual on how to do things.
397520	402680	And second, as a guidance on what are the interesting things worth pursuing.
403080	408800	GPT is only, at least up to GPT-4, it only learns from pure text on the web.
408800	415600	But can we provide the agent with much richer data, such as video walkthrough, or like multimedia,
415600	420600	Wiki documents, and other media forms?
420600	426000	And finally, once we have the environment and the database, we are ready to train foundation
426000	428320	models for the agents.
428320	433320	And it should be flexible enough to pursue the open-ended tasks without any task specific
433320	440960	assumptions, and also scalable enough to compress all of the multi-modal data that I just described.
440960	444880	And here language, I argue, will play at least two key roles.
444880	450960	One is as a simple interface to communicate a task, to communicate the human intentions
450960	458040	to the agent, and second as a bridge to ground all of the multi-modal concepts and signals.
458040	465480	And that train of thought landed us in Minecraft, the best-selling video game of all time.
465480	471640	And for those who are unfamiliar, Minecraft is a procedurally generated 3D voxel world,
471640	475480	and in the game you can basically do whatever your heart desires.
475480	481720	And what's so special about the game is that unlike AlphaGo, StarCraft, or Dota, Minecraft
481720	487880	defines no particular objective to maximize, no particular opponent to beat, and doesn't
487880	489960	even have a fixed storyline.
489960	493760	And that makes it very well-suited as a truly open-ended AM playground.
493760	498160	And here we see people doing extremely impressive things in Minecraft.
498160	504840	Like this is a YouTube video where a gamer built the entire Hogwarts castle block-by-block
504840	508200	by hand in the game.
508200	512120	And here's another example of someone just digging a big hole in the ground and then
512120	516680	making this beautiful underground temple with a river nearby.
516680	520120	It's all crafted by hand.
520120	521120	And one more.
521120	526560	This is someone building a functioning CPU circuit inside a game because there is something
526560	533400	called Redstone in Minecraft that you can build circuits out of it, like logical gates.
533400	535360	And actually the game is too incomplete.
535360	538400	You can, you know, simulate a computer inside a game.
538400	540600	Just think about how crazy that is.
540600	546400	And here I want to highlight a number that is 140 million active players.
546400	551360	And just to quote this numbering perspective, this is more than twice the population of
551360	553240	the UK.
553240	557680	And that is the amount of people playing Minecraft on a daily basis.
557680	562800	And you know, it just so happens that gamers are generally happier than PhDs.
562800	566640	So they love to stream and share what they're doing.
566640	570640	And that produces a huge amount of data every day online.
570640	575720	And there's this treasure trove of learning materials that we can tap into for training
575720	576720	generalizations.
576720	581680	You know, remember the data is the key for foundation models.
581680	588600	So we introduce MindDojo, a new open framework to help the community develop generally capable
588600	595440	agents using Minecraft as a kind of primordial soup.
595440	601360	MindDojo features three major parts, an open-ended environment, an international knowledge base,
601360	606560	and then a generalized agent developed with a simulator and a massive data.
606560	609080	So let's zoom in the first one.
609080	615280	Here's a sample gallery of the interesting things that you can do with MindDojo's API.
615280	619440	We feature a massive benchmark suite of more than 3,000 tasks.
619440	625560	And this is by far the largest open source agent benchmark to our knowledge.
625560	629720	And we implement a very versatile API that unlocks the full potential of the game.
629720	636960	Like for example, MindDojo supports multi-modal observation for action space, like moving
636960	639840	or attack or inventory management.
639840	646640	And that can be customized at every detail, like you can tweak the terrain, the weather,
646640	652680	block placement, monster spawning, and just anything you want to customize in the game.
652680	658920	And given this simulator, we introduce around 1,500 programmatic tasks, which are tasks
658960	663240	that have ground true success conditions defined in Python code.
663240	668160	And you can also explicitly write down spars or the best reward functions using this API.
668160	672840	And some examples are harvesting different resources and unlocking the tech tree
672840	676600	or fighting various monsters and getting reward.
676600	681480	And all these tasks come with language prompts that are templated.
681480	686560	Next, we also introduce 1,500 creative tasks that are freeform and open-ended.
686560	690640	And that is in contrast to the programmatic tasks I just mentioned.
690640	695280	So for example, let's say we want the agent to build a house.
695280	697920	But what makes a house a house, right?
697920	700480	It is L defined and just like image generation.
700480	705200	You don't know if it generates a cat correctly or not.
705200	710760	So it's very difficult to use simple Python programs to give these kind of tasks reward functions.
710760	716360	And the best way is to use foundation models trained on Internet skill knowledge.
716360	724360	So that the model itself understands abstract concepts like, you know, the concept of a house.
724360	728280	And finally, there's one task that holds a very special status called play suit,
728280	732520	which is to beat the final boss of Minecraft, the ender dragon.
732520	734560	So Minecraft doesn't force you to do this task.
734560	737120	As we said, it doesn't have a fixed storyline.
737120	743120	But it's still considered a really big milestone for any kind of beginner human players.
743120	748240	I want to highlight it is an extremely difficult task that requires very complex preparation,
748240	750920	exploration, and also martial skills.
750920	756800	And for an average human, it will take many hours or even days to solve.
756800	760440	Easily over like one million action steps in a single episode.
760440	765600	And that would be the longest benchmarking task for policy learning ever created here.
765600	769280	So I admit, I am personally a below average human.
769280	771960	I was never able to beat the ender dragon.
771960	779160	And my friends laugh at me, and I'm like, OK, one day my AI will avenge my poor skills.
779160	783200	That was one of the motivations for this project.
783200	789280	Now, let's move on to the second ingredient, the Internet skill knowledge base part of my module.
789280	793120	We offer three datasets here, the YouTube, Wiki, and Reddit.
793120	800800	And combined, they are the largest open-ended agent behavior database ever compiled to our knowledge.
800840	802280	The first is YouTube.
802280	807640	And we already said Minecraft is one of the most streamed games on YouTube.
807640	810600	And gamers love to narrate what they are doing.
810600	817640	So we collected more than 700,000 videos with two billion words in the corresponding transfers.
817640	823240	And these transfers will help the agent learn about human strategies and creativities
823240	828120	without us manually labeling things.
828160	836680	And second, the Minecraft player base is so crazy that they have compiled a huge Minecraft-specific Wikipedia
836680	842320	that basically explains everything you ever need to know in every version of the game.
842320	843360	It's crazy.
843360	850600	And we scraped 7,000 Wikipedia pages with interleaving, multi-modal data, like images, tables, and diagrams.
850600	852480	And here are some screenshots.
852480	857520	Like, this is a gallery of all of the monsters and their corresponding behaviors,
857560	860200	like spawn and attack patterns.
860200	866400	And also, the thousands of crafting recipes are all present on the Wiki, and we scraped all of them.
866400	869680	And more complex diagrams and tables and embedded figures.
869680	872400	Now we have something like GPT-4V.
872400	876880	It may be able to understand many of these diagrams.
876880	883960	And finally, the Minecraft subreddit is one of the most active forums across the entire Reddit.
883960	887800	And players showcase their creations and also ask questions for help.
887800	892520	So we scraped more than 300,000 posts from Minecraft Reddit.
892520	899480	And here are some examples of how people use the Reddit as a kind of stack overflow for Minecraft.
899480	902920	And we can see that some of the top-golded answers are actually quite good.
902920	906480	Like someone is asking, oh, why doesn't my wheat farm grow?
906480	909280	And the answer says you need to light up the room with more torches.
909280	912560	You don't have enough lighting.
912560	916040	Now, given the massive task suite and internet data,
916040	921480	we have the essential components to build a journalist's agents.
921480	926120	So in the first mind-dozer paper, we introduce a foundation model called Minecraft.
926120	927840	And the idea is very simple.
927840	930280	I can explain in three slides.
930280	935800	Basically, for our YouTube database, we have time-aligned videos and transfers.
935800	939840	And these are actually the real tutorial videos from our dataset.
939840	944320	You see on the third clip, as I raise my axe in front of this pig,
944320	947600	there's only one thing that you know is going to happen.
947600	953080	It's actually someone said this, a big YouTuber of Minecraft.
953080	958960	And then, given this data, we train Minecraft in the same spirit as Open AI Club.
958960	963560	So for those who are unfamiliar, Open AI Club is a contrastive model
963560	966720	that learns the association between an image and its caption.
966720	969000	And here, it's a very simple idea.
969000	972440	By this time, it is a video text contrastive model.
972440	982720	And we associate the text with a video snippet that runs about 8 to 16 seconds each.
982720	987560	And intuitively, Minecraft learns the association between the video and the transcript
987560	990880	that describes the activity in the video.
990880	993800	And Minecraft outputs a score between 0 and 1,
993800	997120	where 1 means a perfect correlation between the text and the video,
997160	1001360	and 0 means the text is irrelevant to the activity.
1001360	1007240	So you see this is effectively a language-prompted foundation reward model
1007240	1011560	that knows the nuances of things like forests, animal behaviors,
1011560	1015000	and architectures in Minecraft.
1015000	1017440	So how do we use Minecraft in action?
1017440	1021440	Here's an example of our agent interacting with a simulator.
1021440	1025200	And here, the task is to share sheep to obtain wool.
1025200	1031120	And as the agent explores in the simulator, it generates a video snippet
1031120	1035400	as a moving window, which can be encoded and fed into Minecraft,
1035400	1039400	along with an encoding of the text prompt here.
1039400	1042240	And Minecraft computes the association.
1042240	1046720	The higher the association is, the more the agent's behavior in this video
1046720	1050920	aligns with the language, which is a task you want it to do.
1050960	1055640	And that becomes a reward function to any reinforcement learning algorithm.
1055640	1058280	So this looks very familiar, right?
1058280	1066240	Because it's essentially RL from human feedback, or ROHF in Minecraft.
1066240	1070600	And ROHF was the cornerstone algorithm that made chatGBT possible.
1070600	1073840	And I believe it will play a critical role in Jonas agents as well.
1076040	1078800	I'll quickly gloss over some quantitative results.
1078800	1082800	I promise there won't be many tables of numbers here.
1082800	1088120	For these eight tasks, we show the percentage success rate over 200 test episodes.
1088120	1092480	And here, in the green circle, is two variants of our Minecraft method.
1092480	1094840	And in the orange circles are the baselines.
1095920	1101200	So I highlight one baseline, which is that we construct a dance reward function
1101200	1106320	manually for each task using the Mindoge API, it's a Python API.
1106320	1111040	And you can consider this column as a kind of oracle, the upper bound of the performance,
1111040	1114720	because we put a lot of human efforts into designing these reward functions
1114720	1115680	just for the tasks.
1117280	1122160	And we can see that Minecraft is able to match the quality of many of these,
1122160	1126000	not all of them, but many of these manual engineering rewards.
1126000	1129640	It is important to highlight that Minecraft is open vocabulary.
1129640	1133680	So we use a single model for all of these tasks instead of one model for each.
1133680	1138400	And we simply prompt the reward model with different tasks.
1138400	1139560	And that's the only variation.
1143360	1147200	One major feature of Foundation Model is strong generalization out of box.
1147200	1152600	So can our agent generalize to dramatic changes in the visual appearance?
1152600	1157840	So we did this experiment where during training, we only train our agents on
1157840	1161760	a default terrain at noon on a sunny day.
1161760	1165400	But we tested zero shot in a diverse range of terrains,
1165400	1169480	whether it's in daylight cycles, and you can customize everything in Mindoge.
1169480	1173320	And in our paper, we have numbers showing that Minecraft significantly beats
1173320	1177480	an off-the-shelf visual encoder when facing these kind of distribution shift
1177480	1178960	out of box.
1178960	1180200	And this is no surprise, right?
1180200	1184000	Because Minecraft was trained on hundreds of thousands of clips
1184000	1190160	from Minecraft videos on YouTube, which have a very good coverage of all the scenarios.
1191120	1196240	And I think that is just a testament to the big advantage of using
1196240	1200040	international data because you get robustness out of box.
1201440	1205840	And here are some demos of our learned agent behaviors on various tasks.
1205840	1212120	So you may notice that these tasks are relatively short, around 100 to 500 time steps.
1212120	1218240	And that is because Minecraft is not able to plan over very long time horizons.
1218400	1222280	And it is an inherent limitation in the training pipeline
1222280	1225440	because we could only use 8 to 16 seconds of the video,
1225440	1228560	so it's constrained to short actions.
1228560	1232880	But our hope is to build an agent that can explore and make new discoveries
1232880	1236240	autonomously, just all by itself, and it keeps going.
1236240	1239920	And in 2022, this goal seems quite out of reach for us.
1239920	1243200	Mindoge was June 2022.
1243200	1246880	And this year, something happened, and that is G4.
1246880	1251600	A language model that is so good at coding and long horizon planning,
1251600	1254080	so we just cannot sit still, right?
1254080	1260400	We built Voyager, the first large language model powered life on a learning agent.
1260400	1264800	And when we said Voyager lose in Minecraft, we see that it just keeps going.
1264800	1269840	And by the way, all these video snippets are from a single episode of Voyager.
1269840	1273680	It's not from different episodes, it's a single one.
1273680	1278000	And we see that Voyager is just able to keep exploring the terrains,
1278000	1282320	mine all kinds of materials, fight monsters, craft hundreds of recipes,
1282320	1286880	and unlock an ever-expanding tree of diverse skills.
1286880	1290000	So how do we do this?
1290000	1292400	If we want to use the full power of G4,
1292400	1295360	a central question is how to stringify things,
1295360	1300240	converting this 3D world into a textual representation.
1300240	1302480	We need a magic box here.
1302480	1307200	And thankfully, again, the crazy Minecraft community already built one for us,
1307200	1309520	and it's been around for many years.
1309520	1312960	It's called Mindflayer, a high-level JavaScript API
1312960	1317120	that's actively maintained to work with any Minecraft version.
1317120	1321600	And the beauty of Mindflayer is it has access to the game states
1321600	1325760	surrounding the agent, like the nearby blocks, animals, and enemies.
1325760	1330560	So we effectively have a ground truth perception module as textual input.
1330640	1334480	At the same time, Mindflayer also supports action APIs
1334480	1338320	that we can compose skills.
1338320	1340560	And now that we can convert everything to text,
1340560	1344400	we're ready to construct an agent on top of G4.
1344400	1347040	So on a high level, there are three components.
1347040	1352240	One is a coding module that writes JavaScript code to control the game bot,
1352240	1356160	and it's the main module that generates the executable actions.
1356160	1360960	And second, we have a code base to store the correctly written code
1360960	1364560	and look it up in the future if the agent needs to record a skill.
1364560	1366720	And in this way, we don't duplicate efforts,
1366720	1368960	and whenever facing similar situations in the future,
1368960	1371040	the agent knows what to do.
1371040	1376000	And third, we have a curriculum that proposes what to do next,
1376000	1380080	given the agent's current capabilities and also situation.
1380080	1384160	And when you wire these components up together,
1384160	1386720	you get a loop that drives the agent indefinitely
1386720	1389920	and achieve something like lifelong learning.
1389920	1393360	So let's zoom in the center module.
1393360	1396800	We prompt GD4 with documentations and examples
1396800	1400000	on how to use a subset of the Mindflayer API
1400000	1404880	and GD4 writes code to take actions given the current assigned task.
1404880	1407120	And because JavaScript runs a coding interpreter,
1407120	1412160	GD4 is able to define functions on a fly and run it interactively.
1412160	1414560	But the code that GD4 writes isn't always correct, right?
1414560	1415520	Just like human engineers.
1415520	1417520	You can't get everything correct on the first try.
1418240	1421040	So we developed an iterative prompting mechanism
1421040	1422240	to refine the program.
1423040	1425520	And there are three types of feedback here.
1425520	1428640	The environment feedback, like what are the new materials
1428640	1432320	you got after taking an action or some enemies nearby.
1432960	1435600	And the execution error from the JavaScript interpreter
1435600	1438880	if you wrote some buggy code, like undefined variable,
1438880	1440800	for example, if it hallucinates something.
1441600	1445520	And another GD4 that provides critique
1445520	1449120	through self-reflection from the agent state and the world state.
1449760	1452000	And that also helps refine the program effectively.
1453440	1454800	So I want to show some quick example
1454800	1457360	of how the critic provides feedback
1457360	1459040	on the task completion progress.
1459760	1461120	So let's say in the first example,
1461120	1463920	the task is to craft a spike mass
1463920	1466720	and GD4 looks at the agent's inventory
1466720	1468720	and decides that it has enough copper
1468720	1471200	but not enough Amherst as a material.
1472400	1475120	And the second task is to kill three sheeps to collect food.
1475760	1478320	And each sheep drops one unit of wool,
1478320	1480320	but there are only two units in inventory.
1480320	1482560	So GD4 reasons and says that,
1482560	1484960	okay, you have one more sheep to go, likewise.
1486960	1488720	Now, moving on to the second part.
1489920	1492560	Once Voyager implements a skill correctly,
1492560	1494800	we save it to our persistent storage.
1495440	1497360	And you can think of the skill library
1497440	1501200	as a code repository written entirely by a language model
1501200	1503280	through interaction with the 3D world.
1504400	1507120	And the agent can record new skills
1507120	1509200	and also retrieve skills from the library
1509200	1511760	facing similar situations in the future.
1511760	1514240	So it doesn't have to go through this whole program refinement
1514240	1515440	that we just saw again,
1515440	1516800	which is quite inefficient,
1516800	1519360	but you do it once you save it to disk.
1520080	1523120	And in this way, Voyager kind of bootstraps
1523120	1524960	its own capabilities recursively
1525520	1528240	as it explores and experiments in the game.
1529680	1530880	And let's dive a little bit deeper
1530880	1532800	into how the skill library is implemented.
1533680	1535520	So this is how we insert a new skill.
1536080	1540160	First, we use GPT 3.5 to summarize the program into plain English.
1540720	1543760	And summarization is very easy and GD4 is expensive.
1543760	1546240	So we just go for a cheaper tier.
1547440	1551040	And then we embed this summary as the key
1551040	1553680	and we save the program, which is a bunch of code,
1553760	1554800	as the value.
1554800	1557760	And we find that doing this makes retrieval better
1557760	1559840	because the summary is more semantic
1559840	1563200	and the code is a bit more discrete and you insert it.
1566720	1569120	And now for the retrieval process,
1569120	1570960	where Voyager is faced with a new task,
1570960	1572560	let's say craft iron pickaxe.
1573120	1576320	We again use GP3.5 to generate a hint
1576320	1577840	on how to solve the task.
1577840	1579920	And that is something like a natural language paragraph.
1580560	1584640	And then we embed that and use that as the query
1584640	1586320	into the vector database.
1586320	1589840	And we retrieve the skill from the library.
1590720	1594160	So you can think of it as a kind of in-context replay buffer
1594160	1596000	in the reinforcement learning literature.
1597680	1599440	And now moving on to the third part.
1601120	1604000	We have another GP4 that proposes what task to do,
1604000	1607040	given its own capabilities at the moment.
1607680	1610400	And here we give GP4 a very high-level
1610400	1612320	kind of unsupervised objective
1612320	1615920	that is to obtain as many unique items as possible.
1615920	1617280	That is our high-level directive.
1617840	1620560	And then GP4 takes this directive and implements
1621440	1624640	a curriculum of progressively harder challenges
1624640	1626160	and more novel challenges to solve.
1627040	1630720	So it's kind of like curiosity of exploration,
1630720	1633760	where it is our novelty search in a prior literature,
1634320	1636320	but implemented purely in context.
1636400	1639280	Yeah, if you're listening to Zoom, the next example is fun.
1641520	1643440	Let's go through this example together.
1644720	1647120	Just to kind of show you how Voyager works,
1647120	1649920	the whole complicated data flow that I just showed.
1650640	1652720	So the agent finds itself hungry.
1653440	1655520	It only has one out of 20 hunger bars.
1655520	1658720	So it knows GP4 knows that it needs to find food ASAP.
1659280	1662640	And then it senses there are four entities nearby.
1663280	1665840	A cat, a villager, a pig, and some wheat seeds.
1666400	1668320	And now GP4 starts a self-reflection.
1669040	1671520	Like, do I kill the cat and the villager to get some meat?
1672080	1672960	That sounds horrible.
1674080	1674960	How about the wheat seeds?
1675760	1677600	I can use the seeds to grow a farm,
1677600	1679440	but that's going to take a very long time
1679440	1681360	until I can generate some food.
1681360	1684080	So sorry, Piggy, you are the one being chosen.
1685120	1688640	So GP4 looks at the inventory, which is the agent state.
1689200	1691920	There is a piece of iron in inventory.
1691920	1695680	So it recalls, Voyager recalls a skill from the library
1695680	1697760	that is to craft an iron sword
1697760	1700400	and then use that skill to start pursuing,
1700400	1703600	to start learning a new skill, and that is Hunt Pig.
1703600	1707840	And once the Hunt Pig routine is successful,
1707840	1710240	GP4 saves it to the skill library.
1710240	1711280	That's roughly how it works.
1712960	1714880	Yeah, and putting all of these together,
1714880	1716800	we have this iterative prompting mechanism,
1716800	1719680	the skill library, and an automatic curriculum.
1720320	1722560	And all of these combine.
1722560	1725600	It's Voyager's no-gradient architecture
1725600	1727360	where we don't train any new models
1727360	1728800	or fine tune any parameters,
1729520	1734080	and allows Voyager to self-boostrap on top of GP4,
1734080	1736800	even though we are treating the underlying language model
1736800	1737600	as a black box.
1740480	1741920	It looks like my example work,
1741920	1744560	and they started to listen.
1748560	1750160	So yeah, these are the tasks
1750160	1752080	that Voyager picked up along the way.
1752560	1754560	And we didn't pre-program any of these.
1754560	1756000	It's all Voyager's idea.
1756560	1758400	The agent is kind of forever curious
1758400	1761680	and also forever pursuing new adventures just by itself.
1764000	1766640	So to quickly show some quantitative results,
1767680	1769760	here we have a learning curve,
1769760	1773280	where the x-axis is a number of prompting iterations,
1773280	1776080	and the y-axis is the number of unique items
1776080	1779280	that Voyager discovered as it's exploring an environment.
1780240	1783440	And these two curves are baselines,
1783440	1785760	a react and reflexion.
1787600	1789040	And this is auto-GPT,
1789040	1790880	which is like a popular software repo.
1790880	1793920	Basically, you can think of it as combining react
1793920	1795760	and a task planner that decomposes
1795760	1797600	an objective into sub-goals.
1798240	1799200	And this is Voyager.
1800000	1803200	We're able to obtain three times more novel items
1803200	1804720	than the prior methods,
1804720	1807600	and also unlock the entire texture significantly faster.
1808320	1811600	And if you take away the skill library,
1811600	1813440	you see that Voyager really suffers.
1813440	1815120	The performance takes a hit,
1815120	1817840	because every time it needs to kind of repeat
1817840	1819840	and relearn every skill from scratch
1819840	1822000	and starts to make a lot more mistakes,
1822000	1824400	and that really degrades the exploration.
1826640	1830720	Here, these two are the bird-eye views of the Minecraft map,
1830720	1834000	and these circles are what the prior methods
1834000	1835360	are able to explore,
1836240	1838160	given the same prompting iteration budget.
1839040	1842160	And we see that they tend to get stuck in local areas
1842160	1844160	and kind of fail to explore more,
1844800	1848320	but Voyager is able to navigate distances at least two times
1849280	1851360	as much as the prior works.
1852480	1855280	So it's able to visit a lot more places,
1855280	1858160	because to satisfy this high-level directive
1858160	1860640	of obtaining as many unique items as possible,
1860640	1862320	you've got to travel.
1862320	1863360	If you stay at one place,
1863440	1865280	you will quickly exhaust the interesting things to do.
1866320	1867920	And Voyager travels a lot,
1867920	1869520	so that's how we came up with the name.
1871760	1874800	So finally, one limitation is that Voyager
1874800	1877680	does not currently support visual perception,
1877680	1880720	because the GV4 that we used back then was text-only,
1881440	1883120	but there's nothing stopping Voyager
1883120	1887120	from adopting like multi-modal language models in the future.
1887120	1889520	So here we have a little proof-of-concept demo,
1889520	1892080	where we ask a human to basically function
1892160	1893840	as the image captioner.
1893840	1896320	And the human will tell Voyager
1896320	1898160	that as you're building these houses,
1898160	1899520	what are the things that are missing?
1899520	1901440	Like you place a door incorrectly,
1902160	1905040	like the roof is also not done correctly.
1905040	1908720	So the human is acting as a critic module of the Voyager stack.
1909360	1912080	And we see that with some of that help,
1912080	1915520	Voyager is able to build a farmhouse and another portal,
1915520	1919440	but it has a hard time understanding 3D spatial coordinates
1919440	1921680	just by itself in a textual domain.
1922560	1931360	Now, after doing Voyager, we're considering like, where else can we apply this idea
1931360	1934000	of coding in an embodying environment,
1934000	1937920	observe the feedback, and iteratively refine the program.
1938720	1942720	So we came to realize that physics simulations themselves
1942720	1944560	are also just Python code.
1944560	1947760	So why not apply some of the principles for Voyager
1947760	1949920	and do something in another domain?
1950160	1953280	What if you apply Voyager in the space of this physics simulator API?
1953280	1958640	And this is Eureka, which my team announced just like three days ago,
1958640	1959760	fresh out of the oven.
1960800	1964400	It is an open-ended agent that designs reward functions
1964400	1967520	for robot dexterity at superhuman level.
1967520	1970880	And it turns out that GD4-POS reinforcement learning
1970880	1974160	can spin a pen much better than I do.
1974160	1978480	I gave up on this task a long time ago from childhood.
1978800	1980720	It's so hard for me.
1983440	1986480	So Eureka's idea is very simple and intuitive.
1987120	1991200	GD4 generates a bunch of possible reward function candidates
1991200	1992240	implemented in Python.
1992800	1997200	And then you just do a full reinforcement learning training loop
1997200	2000320	for each candidate in a GPU accelerated simulator.
2001280	2005280	And you get a performance metric and you take the best candidates
2005280	2007120	and feedback to GD4.
2007120	2010480	And it samples the next proposals of candidates
2010480	2013600	and keeps improving the whole population on the reward functions.
2014320	2015120	That's the whole idea.
2015680	2018080	It's kind of like an in-context evolutionary search.
2020160	2022320	So here's the initial reward generation,
2022320	2026000	where Eureka takes as context the environment code
2026000	2030320	of NVIDIA's ISAC sim and a task description
2030320	2033280	and samples the initial reward function implementation.
2034240	2036480	So we found that the simulator code itself
2036480	2038480	is actually a very good reference manual
2038480	2041520	because it tells you, Eureka, what are the variables you can use,
2042160	2045520	like the hand positions here, the fingertip position,
2045520	2047440	the fingertips have safe, the rotation,
2047440	2048880	angular velocity, et cetera.
2049600	2053120	So you know all of these variables from the simulator code
2053120	2056320	and you know how they interact with each other.
2056320	2060560	So that serves as a very good in-context instruction.
2061440	2063120	So Eureka doesn't need to reference
2063120	2064960	any human return reward functions.
2066560	2068960	And then once you have the generated reward,
2068960	2071600	you plug it into any reinforcement learning algorithm
2071600	2073120	and just train it to completion.
2073680	2077760	So this step is typically very costly and very slow
2077760	2079360	because reinforcement learning is always slow.
2079920	2082480	And we were only able to scale up Eureka
2082480	2084080	because of NVIDIA's ISAC chain,
2084800	2088560	which runs a thousand simulated environment copies
2088560	2089520	on a single GPU.
2090160	2093840	So basically, you can think of it as speeding up reality
2093840	2094800	by a thousand lags.
2097120	2098160	And then after training,
2098160	2100160	you will get the performance metrics
2100160	2101680	back on each reward component.
2102320	2103680	And as we saw from Voyager,
2104240	2107040	GBT4 is very good at self-reflection.
2107040	2108960	So we leverage that capability.
2113440	2116720	There's a software trial reminding you to activate a license.
2117520	2120320	Yeah, so Voyager reflects on it
2120320	2123920	and then proposes mutations on the code.
2125520	2128960	So here, the mutations we found can be very diverse,
2128960	2130880	ranging from something as simple as just changing
2130880	2133520	a hyperparameter in the reward function weighting
2133520	2137440	to all the way to adding completely novel components
2137440	2138240	to the reward function.
2140000	2141520	And in our experiments,
2141520	2145920	Eureka turns out to be a superhuman reward engineer
2146160	2148480	actually outperforming some of the functions
2148480	2151280	implemented by the expert human engineers
2151280	2153280	on NVIDIA's ISAC same team.
2155680	2158080	So here are some more demos of how Eureka
2158080	2160240	is able to write very complex rewards
2160240	2164160	that lead to these extremely dexterous behaviors.
2164160	2167200	And we can actually train the robot hand
2167200	2169840	to rotate pens not just in one direction,
2169840	2172320	but in different directions, along different 3D axes.
2172960	2175360	I think one major contribution of Eureka,
2175360	2177920	different from Voyager, is to bridge the gap
2177920	2182080	between high-level reasoning and low-level model controls.
2182080	2184400	So Eureka introduces a new paradigm
2184400	2187760	that I'm calling hybrid gradient architecture.
2187760	2189920	So recall Voyager is a no-gradient architecture.
2189920	2193040	We don't touch anything and we don't train anything.
2193040	2194880	But Eureka is a hybrid gradient,
2194880	2198640	where a black box inference-only language model
2198640	2201040	instructs a wide range of functions
2201200	2204080	instructs a white box, learnable neural network.
2205040	2207520	So you can think of it as two loops, right?
2207520	2209040	The outer loop is great and free,
2209600	2212000	and it was, it's driven by GV4,
2212000	2214240	kind of selecting the reward functions.
2214240	2216160	And the inner loop is great and based.
2216160	2219520	You train like a full reinforcement learning episode from it
2219520	2223200	to achieve extreme dexterity using a specialized,
2223200	2226160	like training by training a special neural network controller.
2227040	2229280	And you must have both loops to succeed
2229280	2230880	to deliver this kind of dexterity.
2231600	2234160	And I think it will be a very useful paradigm
2234160	2236640	for training robot agents in the future.
2238880	2243120	So these days, when I go on Twitter or X,
2243680	2246800	I see AI conquering new lands every week.
2248000	2250160	Chat, image generation, and music,
2250160	2252000	they're all very well within reach.
2252960	2255520	But my dojo, Voyager, and Eureka,
2255520	2257120	these are just scratching the surface
2257120	2259120	of open-ended journalist agents.
2260480	2261600	And looking forward,
2261600	2264720	I want to share two key research directions
2264720	2267600	that I personally find extremely promising,
2267600	2269200	and I'm also working on it myself.
2270080	2272320	The first is a continuation of Minecraft,
2272320	2274720	basically how to develop methods
2274720	2276880	that learn from Internet-skilled videos.
2277440	2280480	And the second is multimodal foundation models.
2280480	2282480	Now that GV4V is coming,
2282480	2284720	but it is just the beginning of an era.
2285440	2289120	And I think it's important to have all of the modalities
2289120	2290880	in a single foundation model.
2292560	2293680	So first, about videos.
2294880	2297200	We all know that videos are abundant, right?
2297200	2301440	Like so many data on YouTube, way too many
2301440	2303120	for our limited GPUs to process.
2304320	2307200	They're extremely useful to train models
2307200	2310960	that not only have dynamic perception and intuitive physics,
2310960	2314560	but also capture the complexity of human creativity
2314640	2315680	and human behaviors.
2316320	2320880	It's all good, except that when you are using video
2320880	2322640	to pre-training body nations,
2322640	2324400	there is huge distribution shift.
2324400	2326320	You also don't get action labels,
2326320	2327920	and you don't get any of the groundings
2327920	2329440	because you are a passive observer.
2330960	2332240	So I think here is a demonstration
2332240	2333840	of why learning from video is hard,
2333840	2335120	even for natural intelligence.
2337040	2340400	So Little Cat is seeing boxers shaking their head,
2341200	2342640	and it thinks maybe shaking head
2342640	2344320	is the best way to do fighting.
2344960	2350080	All right, this is why learning from video is hard.
2353040	2355520	You have no idea, like why...
2358000	2358800	This is too good.
2358800	2360000	Let's play this again.
2360000	2362800	You have no idea why Tyson is doing this, right?
2362800	2364240	Like the cat has no idea,
2364240	2368240	and then it associates this with just wrong kind of policy.
2370640	2373360	But for sure, it doesn't help the fighting,
2373360	2375520	but it definitely boosts the cat's confidence.
2378960	2380320	That's why learning from video is hard.
2382320	2384960	Now, I want to point out a few kind of latest research
2384960	2388000	in how to leverage so much video for journalist agents.
2389040	2390720	There are a couple of approaches.
2391280	2392800	The first is the simplest,
2392800	2396240	just learn kind of a visual feature extractor from the videos.
2396800	2400000	So this is R3M from Chelsea's group at Stanford,
2400960	2403520	and this model is still an image-level representation,
2403520	2406560	just that it uses a video-level loss function to train,
2407200	2409360	more specifically, time-contrastive learning.
2410000	2413600	And after that, you can use this as an image backbone
2413600	2414880	for any agent,
2414880	2416160	but you still need to kind of find
2416160	2418960	to using domain-specific data for the agent.
2420800	2424480	The second path is to learn reward functions from video,
2424480	2428640	and MineClip is one model under this category.
2429200	2432880	It uses a contrastive objective between the transfer and video.
2432880	2436160	And here, this work, VIP, is another way
2436160	2438640	to learn a similarity-based reward
2438640	2440960	for goal-conditioned tasks in the image space.
2441680	2445600	So this work, VIP, is led by also the first author of Eureka,
2445600	2448400	and Eureka is his internship project with me.
2450240	2451680	And the third idea is very interesting.
2452400	2455920	Can we directly do imitation learning from video,
2456480	2459120	but better than the cat that we just saw?
2459840	2463440	So we just said, you know, the videos don't have the actions, right?
2464400	2466880	We need to find some ways to pseudo-level the actions.
2467600	2471440	And this is video portraying a VPT from OpenAI last year
2471440	2473520	to solve long-range tasks in Minecraft.
2474640	2477680	And here, the pipeline works like this.
2478240	2481360	Basically, you use a keyboard and a mouse action space,
2482000	2485200	so you can align this action space with the human actions.
2486000	2489280	And OpenAI hires a bunch of Minecraft players
2489840	2491760	and actually collect data in-house,
2491760	2494400	so they record the episodes done by those gamers.
2495040	2499200	And now you have a data set of video and action pairs, right?
2499920	2502640	And you train something called an inverse dynamics model,
2502640	2506880	which is to take the observation and then predict the actions
2506880	2509040	that cost the observation to change.
2509040	2510480	So that's the inverse dynamics model.
2511040	2515840	And that becomes a labeler that you can apply
2515840	2518640	to in-the-wild YouTube videos that don't have the actions.
2519200	2524160	So you apply IDM to like 70K hours of in-the-wild YouTube videos,
2524160	2526080	and you will get these pseudo-actions
2526080	2529440	that are not always correct, but also way better than random.
2530080	2532000	And then you're training imitation learning
2532000	2533760	on top of this augmented data set.
2534320	2538560	And in this way, OpenAI is able to greatly expand the data
2538640	2542480	because the original data collected from the humans
2542480	2545200	are high quality, but they're extremely expensive,
2545200	2547360	while in-the-wild YouTube videos are very cheap,
2547360	2548800	but you don't have the actions.
2548800	2551920	So they kind of solved and got the best of those roles.
2552640	2555040	But still, it's really expensive to hire these humans.
2557680	2559600	Now, what's beyond the videos, right?
2560320	2563520	I'm a firm believer that multimodal models will be the future.
2564240	2566160	And I see text as a very lossy,
2566160	2568560	kind of 1D projection of our physical world.
2569120	2572560	So it's essential to include the other sensory modalities
2572560	2574320	to provide a full in-body experience.
2575200	2577440	And in the context of in-body relations,
2577440	2581520	I think the input will be a mixture of text, images, videos,
2581520	2585200	and even audio in the future, and the output will be actions.
2586560	2591280	So here's a very early example of a multimodal language model
2591280	2592160	for robot learning.
2592720	2594400	So let's imagine a household robot.
2595200	2599120	We can ask the robot to bring us a cup of tea from the kitchen,
2599120	2600560	but if we want to be more specific,
2601200	2603840	I want this particular cup that is my favorite cup.
2603840	2605120	So show me this image.
2606320	2611440	And we also provide a video demo of how we want to mop the floor
2611440	2615120	and ask the robot to imitate the similar motion in context.
2616240	2618960	And when a robot sees an unfamiliar object like a sweeper,
2618960	2621120	we can explain it by providing an image
2621120	2622400	and showing this is a sweeper.
2622400	2625040	Now go ahead and do something with the tool.
2625600	2627840	And finally, to ensure safety, we can say,
2627840	2630160	take a picture of that room and just do not enter that room.
2631520	2635120	To achieve this, back last year,
2635120	2637360	we proposed a model called VIMA,
2637360	2639040	which stands for Visual Model Attention.
2639680	2642720	And in this work, we introduce a concept called multimodal prompting,
2643360	2647440	where the prompt can be a mixture of text, image, and videos.
2648160	2650640	And this provides a very expressive API
2650640	2654000	that just unifies a bunch of different robot tasks
2654000	2657280	that otherwise would require a very different pipeline
2657280	2659680	or specialized models to solve in prior literature.
2660560	2663360	And VIMA simply tokenizes everything,
2665120	2668080	converting image and text into sequences of tokens,
2668080	2669760	and train a transformer on top
2669760	2672480	to output the robot arm actions
2672480	2676080	autoregressively one step at a time during inference time.
2677440	2680320	So just to look at some of the examples here,
2680960	2683600	this prompt rearrange objects to match the scene.
2683600	2685680	It is a classical task called Visual Go Reaching
2685680	2687760	that has a big body of prior works on it.
2688960	2693120	And that's how our robot does it, given this prompt.
2694160	2697600	And we can also give it novel concepts in context.
2697600	2699600	Like this is a blanket, this is a work,
2699600	2701520	now put a work into a blanket.
2701520	2705200	And both words are nonsensical, so it's not in the training data,
2705200	2706960	but VIMA is able to generalize zero shot.
2707840	2710640	And follow the motion to manipulate this object.
2711440	2713520	So the bot understands what we want
2713520	2714880	and then follow this trajectory.
2715440	2718080	And finally, we can give it more complex prompt,
2718080	2720240	like these are the safety constraints,
2720240	2723680	sweep the box into this, but without exceeding that line.
2723680	2727360	And we would do this using the interleaving image
2727360	2728320	and text tokens.
2730480	2734400	And recently, Google Brain Robotics followed up after VIMA
2734400	2738000	with RT1 and RT2, robot transformer one and two.
2738960	2742640	And RT2 is using a similar recipe, as I described,
2742640	2746000	where they first kind of pre-train on internet scale data
2746560	2749680	and then fine tune with some human collected demonstrations
2749680	2750640	on the Google robots.
2751440	2753920	And RoboCAD from DeepMind is another interesting work.
2754640	2758880	They train a single unified policy that works not just on
2759600	2763120	a single robot, but actually across different embodiments,
2763120	2766320	different robot forms, and even generalize to a new hardware.
2767280	2770560	So I think this is like a higher form of multimodal agent
2770560	2772320	with a physical form factor.
2772320	2775440	The morphology of the agent itself is another modality.
2777440	2780640	So that concludes our looking forward section.
2781840	2785280	And lastly, I want to kind of put all the links together
2785280	2787120	of the works I described.
2787120	2788960	So this is mindodger.org.
2788960	2791360	We have open source everything.
2791360	2794640	Well, for all the projects where big fans are open source,
2794640	2798480	we open source as much as we can, including like the model code,
2798480	2803120	checkpoints, simulator code, and training data.
2804560	2806960	And this is Voyager.mindodger.org.
2807600	2808560	This is Eureka.
2809600	2810960	And this is VIMA.
2813280	2814400	And one more thing, right?
2814960	2818000	If you just want an excuse to play Minecraft at work,
2818720	2820240	then mindodger is perfect for you
2820240	2822800	because you are collecting human demonstration
2822800	2824000	to train generalization.
2824000	2826800	And there's one thing that you take away from this talk.
2826800	2827680	It should be this slide.
2829920	2831760	And lastly, I just want to remind all of us,
2832560	2835200	despite all the progress I've shown, what we can do
2835760	2840000	is still very far from human ingenuity as embodied agents.
2841120	2843920	These are the videos from our dataset
2843920	2846960	of people doing like decorating a winter wonderland
2846960	2849600	or building the functioning CPU circuit within Minecraft.
2850480	2853120	And we are very far from that as AI research.
2853680	2855520	So here's a call to the community.
2855520	2858480	If human can do these mind-blowing tasks,
2858480	2860400	then why not our AI, right?
2860400	2861520	Let's find out together.
