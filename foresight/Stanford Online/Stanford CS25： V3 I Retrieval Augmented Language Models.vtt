WEBVTT

00:00.000 --> 00:11.840
Hey guys, welcome to our last lecture of this quarter.

00:11.840 --> 00:15.520
And we're very happy to have Dawa here.

00:15.520 --> 00:22.120
He's the CEO of Contextual AI, the Enterprise LLM company, as well as an adjunct professor

00:22.120 --> 00:25.120
in symbolic systems here at Stanford.

00:25.120 --> 00:29.160
And previously, he was the head of research at Clicking Base, and before that a research

00:29.160 --> 00:32.520
scientist at Facebook AI Research.

00:32.520 --> 00:37.400
He received his PhD in masters from the University of Cambridge, as well as a master's in logic

00:37.400 --> 00:42.320
from the University of Amsterdam, and studied philosophy and incognitive AI in undergrad.

00:42.320 --> 00:48.040
And his work focuses on machine learning as well as NLP, specifically on developing better

00:48.040 --> 00:54.440
models for language understanding and generation, and better tools for evaluation and many times.

00:54.440 --> 00:57.480
Yeah, give it up for Adela.

00:58.480 --> 01:00.200
Thank you.

01:00.200 --> 01:04.120
So I guess I have to sort of stand here in the corner, so people can see me on this

01:04.120 --> 01:05.120
move as well.

01:05.120 --> 01:10.600
Yeah, thanks so much for having me here.

01:10.600 --> 01:12.280
So I asked Steven what I should talk about.

01:12.280 --> 01:17.000
There were a couple of things I could talk about, multi-modality or evaluation.

01:17.000 --> 01:23.520
And this was the preferred topic, I guess, because the others were already covered.

01:23.520 --> 01:28.600
So yeah, I'm very happy to talk to you about everything retrieval augmentation.

01:28.600 --> 01:33.080
I think this is really one of the coolest topics right now in our field.

01:33.080 --> 01:37.800
So I'll just give you an overview of what's been happening and what I think are the interesting

01:37.800 --> 01:40.920
questions to think about.

01:40.920 --> 01:46.520
So first of all, obviously, in case you've missed it, we are in the age of language models.

01:46.520 --> 01:51.200
And I just wanted to do a quick poll here in this not super big audience.

01:51.200 --> 01:57.120
I guess there's more people on the Zoom, but who invented language models?

01:57.120 --> 02:00.920
If you thought OpenAI, then I'm angry with you.

02:00.920 --> 02:04.280
So actually, this is a very, very old idea.

02:04.280 --> 02:10.000
So the idea is just you take a sequence and you factorize out the token probabilities.

02:10.000 --> 02:13.040
And so it wasn't invented by OpenAI.

02:13.040 --> 02:14.840
It's not like a few years old.

02:14.840 --> 02:18.000
It's actually several decades old.

02:18.000 --> 02:21.520
So I'm bringing this up because I was talking to someone and they were like, OpenAI invented

02:21.520 --> 02:22.520
language models.

02:22.520 --> 02:25.640
And I was like, they're kidding me, right?

02:25.640 --> 02:31.480
So I went back to the literature and this is the oldest one I could find actually, 1991

02:31.480 --> 02:33.720
first neural language model.

02:33.720 --> 02:39.760
There's a very nice paper from 2003 from Bengio where they actually have like word embeddings

02:39.760 --> 02:42.120
and everything already in there.

02:42.120 --> 02:46.040
So obviously, these are LLMs, not LLMs.

02:46.040 --> 02:51.000
And as it turns out, if you make them really big and you parameterize them with these massive

02:51.000 --> 02:55.800
neural nets, then you get something really powerful that really shows emergent properties.

02:55.800 --> 02:59.720
And that's why we're also excited in this stuff.

02:59.720 --> 03:04.400
So if we think about this from like a classic CS perspective, there's input output, right?

03:04.400 --> 03:07.360
There's this kind of thing in the middle, it's the generator.

03:07.360 --> 03:12.920
So we take a sequence, the input sequence, and then the task of the model is to predict

03:12.920 --> 03:14.600
the next token.

03:14.600 --> 03:17.360
Very, very simple model.

03:17.360 --> 03:21.480
And so, you know, that's why it was so easy to come up with this in 1991 already because

03:21.480 --> 03:24.080
it's like the idea is very intuitive.

03:24.080 --> 03:29.920
But for a long time, what was really broken with this was the user interface.

03:29.920 --> 03:35.160
And this, I think a lot of people kind of misunderstand what chat GPT was about.

03:35.160 --> 03:37.200
That's really what chat GPT fixed.

03:37.200 --> 03:41.840
So that initially you had to come up with these very weird prompts in order to get your

03:41.840 --> 03:44.880
language model to do what you wanted it to do.

03:44.880 --> 03:46.840
And humans are terrible at this, right?

03:46.840 --> 03:51.200
So we're much better at sort of telling people or things around us what we want, right?

03:51.200 --> 03:56.200
So if we have a dog, we say sit, we don't prompt it in a very weird way so that it sits,

03:56.200 --> 03:57.200
right?

03:57.200 --> 03:58.480
And it's the same with the language model.

03:58.480 --> 04:03.720
If you wanted to generate some red lyrics in the style of a pirate or Shakespeare or

04:03.720 --> 04:08.080
something, then you tell it generate some red lyrics in the style of a pirate, right?

04:08.080 --> 04:13.680
So that kind of instruction data actually turns out to be super, super rare in just web

04:13.680 --> 04:14.680
data.

04:14.680 --> 04:18.280
So what you need to do is you need to fix the user interface to the language model.

04:18.280 --> 04:23.560
And the classic recipe for doing that is the sequence basically that chat GPT used.

04:23.560 --> 04:25.160
So you prompt the model in a specific way.

04:25.160 --> 04:29.840
You will instruction fine tune the model and you can do some alignment or LHF, whatever

04:29.840 --> 04:31.960
you do on top of that.

04:31.960 --> 04:32.960
So that's the first thing.

04:32.960 --> 04:37.920
So now you have a working language model with a working user interface.

04:37.920 --> 04:40.560
So are we done then?

04:40.560 --> 04:41.560
Obviously we're not.

04:41.560 --> 04:45.160
So right now language models are kind of taking the world by storm.

04:45.160 --> 04:49.040
But if you talk to anyone, especially in an enterprise, for example, where they have very

04:49.040 --> 04:54.480
strict accuracy requirements, they will tell you that they can't really productionize this

04:54.480 --> 04:55.720
yet.

04:55.720 --> 04:58.680
And the reason is because there are all these familiar problems, probably a bunch of you

04:58.680 --> 05:03.480
are working on these problems right now around hallucination.

05:03.480 --> 05:07.120
So these models, they kind of make up stuff very often with very high confidence, which

05:07.120 --> 05:11.440
is even more scary in a way attribution.

05:11.440 --> 05:15.520
So we don't really know why these models are saying what they're saying stillness.

05:15.520 --> 05:16.520
They go out of date.

05:16.520 --> 05:20.320
And so this was a big problem with sort of chat GPT, not knowing anything that happened

05:20.320 --> 05:22.160
after a certain cutoff date.

05:22.160 --> 05:24.080
And they keep updating it every once in a while.

05:24.080 --> 05:29.040
But you want to have a system that's always completely up to date that never goes still.

05:29.040 --> 05:31.600
You want to be able to revise the information in the system.

05:31.600 --> 05:37.320
So if you're a European organization, you have to worry about GDPR, which means that

05:37.320 --> 05:42.240
you need to be able to remove information from the language model or maybe revise facts,

05:42.240 --> 05:43.960
which we don't really know how to do.

05:43.960 --> 05:50.040
So again, this is a very interesting area of study for a lot of folks, model editing.

05:50.040 --> 05:53.520
But so this is something that we really want to be able to fix.

05:53.520 --> 05:57.880
And then there's this big question of how do you customize these models?

05:57.880 --> 05:59.880
So different people have different use cases.

05:59.880 --> 06:00.880
You have different data.

06:00.880 --> 06:04.560
If you're a company or if you want to have a language model on your own data, how do

06:04.560 --> 06:06.840
you make it work on your own data?

06:06.840 --> 06:12.160
So one of the solutions that everybody has started using right now is to couple it to

06:12.160 --> 06:13.360
an external memory.

06:13.360 --> 06:17.200
So that's really just rag, right?

06:17.200 --> 06:19.840
This whole lecture is basically about rag.

06:19.840 --> 06:25.880
But the way to understand what is going on here is we have this generator just like before.

06:25.880 --> 06:27.800
We have the input and the prompt just like before.

06:27.800 --> 06:32.720
But now instead of just giving those two things, we give this additional context.

06:32.720 --> 06:37.600
So we contextualize the language model using things we've retrieved.

06:37.600 --> 06:40.680
And the retriever is very often pretty simple.

06:40.680 --> 06:43.600
It's just a query in the document encoder.

06:43.600 --> 06:48.040
And then you get a bunch of documents, you give them as context to the model.

06:48.040 --> 06:51.200
So super simple architecture.

06:51.200 --> 06:56.040
And I think it's useful to think about it from the perspective of these two separate

06:56.080 --> 06:57.840
paradigms.

06:57.840 --> 07:01.280
So if you've ever taken an exam, I'm sure you have, right?

07:01.280 --> 07:04.040
You can have a closed book exam where you have to memorize all of this, so you have

07:04.040 --> 07:08.600
to cram all the knowledge into your parameters, your neurons.

07:08.600 --> 07:12.160
Or you have an open book exam where you have all of this information in the book that you

07:12.160 --> 07:14.960
can access when you do the exam.

07:14.960 --> 07:16.840
So it's a very similar thing with rag, right?

07:16.840 --> 07:20.200
You can just make it an open book setting where you can give it access to this external

07:20.280 --> 07:25.920
information, Wikipedia or something else, or basically the entire internet, and then

07:25.920 --> 07:31.680
have the language model do its job without having to memorize all of it in its parameters.

07:31.680 --> 07:36.800
So Dr. I think useful distinction here is that cramming everything into your parameters,

07:36.800 --> 07:38.640
that's the parametric approach.

07:38.640 --> 07:44.120
So what we're doing with rag is we're adding this non-parametric retrieval component.

07:44.120 --> 07:49.080
So you might call this semi-parametric if you want to give this a name.

07:50.600 --> 07:54.920
All right, so why does that actually solve these issues?

07:54.920 --> 07:59.720
And so the answer is basically that if you have this separate index, right, this separate

07:59.720 --> 08:04.480
retriever, you can swap it in, you can swap it out, you can replace it with a new index

08:04.480 --> 08:06.480
so you can really customize it.

08:06.480 --> 08:12.800
And so you can customize your language model system for what the user really wants to see.

08:12.800 --> 08:17.040
And then obviously you can update this index, so it doesn't really go still and you can

08:17.040 --> 08:21.400
revise it if everything goes wrong, if anything goes wrong.

08:21.400 --> 08:23.560
The other thing you get is grounding.

08:23.560 --> 08:27.480
So that's initially why I became interested in this kind of architecture, because I was

08:27.480 --> 08:31.080
thinking a lot about grounding and multimodality and things like that, and actually one really

08:31.080 --> 08:35.760
nice way to ground things is to find some other information that you can ground your

08:35.760 --> 08:36.920
generation in.

08:36.920 --> 08:41.880
So you really want the language model to only say things that it has evidence for in this

08:41.880 --> 08:46.720
other piece of text, or even multimodal data that it retrieves separately.

08:46.720 --> 08:50.120
So if you do that, then you get less hallucination, because you can always point back to your

08:50.120 --> 08:53.160
source, it's always grounded in your source.

08:53.160 --> 08:56.080
And you get attribution, because you don't know why the model is saying what it's saying

08:56.080 --> 09:00.880
is because it founded this thing here, is that clear?

09:00.880 --> 09:02.800
All right.

09:02.800 --> 09:09.200
So for the rest of this lecture, we're going to talk about this basic architecture.

09:09.200 --> 09:12.920
And so it kind of looks like a pretty simple thing, right?

09:12.920 --> 09:17.360
But there are actually lots and lots of questions you can ask about what the system should really

09:17.360 --> 09:19.000
look like.

09:19.000 --> 09:23.200
And this doesn't even cover half the questions you can ask.

09:23.200 --> 09:28.000
So it really is about how do we optimize this entire system?

09:28.000 --> 09:33.600
So we have the separate components, the retriever, the generator, and then there are things like

09:33.600 --> 09:38.880
this query encoder, how do we encode queries, how do we do the retrieval, do we update the

09:38.880 --> 09:43.440
document encoder, or how do we actually define a document, right?

09:43.440 --> 09:48.720
Is it like a full document or is it a paragraph or a chunk or a sentence or a couple of words?

09:48.720 --> 09:51.280
So there are lots of questions to ask.

09:51.280 --> 09:56.800
And as you'll see, there are lots of possible answers to these questions as well.

09:56.800 --> 10:01.040
So this is what we'll cover.

10:01.040 --> 10:06.320
So there are lots of architectures going into these questions.

10:06.320 --> 10:11.320
And I think as we go through them, it's useful for you to think about what happens during

10:11.320 --> 10:14.440
training time and what happens during test time, right?

10:14.440 --> 10:20.720
So during training time, it's really, okay, we have the language model, we have this retriever.

10:20.720 --> 10:21.920
Which one do we update?

10:21.920 --> 10:23.040
How do we update them?

10:23.040 --> 10:24.880
How do we train this entire system?

10:24.880 --> 10:27.120
Do we maybe not train it at all?

10:27.120 --> 10:28.600
Do we pre-train it from scratch?

10:28.600 --> 10:33.160
Do we initialize it with components that were already separately trained?

10:33.160 --> 10:36.400
These are the kinds of questions that you have to answer if you want to design a system

10:36.400 --> 10:38.000
like this.

10:38.000 --> 10:41.760
And then during test time, you have this entire system, right?

10:41.760 --> 10:47.720
So actually multiple models in a way that are working together.

10:47.720 --> 10:49.800
So there's also different things you can do there, right?

10:49.800 --> 10:54.480
So give it different indices during test time or manipulate kind of how you're sampling

10:54.480 --> 10:56.760
things like that.

10:56.760 --> 11:01.360
So the starting point for all of this stuff, I think if you ask someone now, like, what

11:01.400 --> 11:04.720
is RAG, they will think of this thing.

11:04.720 --> 11:08.400
So this is frozen RAG basically.

11:08.400 --> 11:09.800
There's no training here at all.

11:09.800 --> 11:13.560
So going back to this question of train time, test time, there's only test time here.

11:13.560 --> 11:17.840
Train time happens separately with these kind of black box models that we don't necessarily

11:17.840 --> 11:19.080
have control over, right?

11:19.080 --> 11:24.120
So there's this document embedding model as whatever is currently at the top of some

11:24.160 --> 11:26.760
open source leaderboard.

11:26.760 --> 11:34.160
You use that to get some vectors that you then use to create this vector database.

11:34.160 --> 11:38.400
And then the vector database just does search and it gives the information from the search

11:38.400 --> 11:39.840
to the language model.

11:39.840 --> 11:43.720
And it just passes it as the context, right?

11:43.720 --> 11:47.800
So this only works because of in-context learning.

11:47.880 --> 11:54.240
And I think as a machine learner myself, this feels very inelegant.

11:54.240 --> 12:01.320
So what this lecture is about is, can we do better than this frozen thing?

12:01.320 --> 12:05.160
So let's start from the left side of this.

12:05.160 --> 12:09.760
OK, if we want to outperform this frozen thing itself with just the vector database,

12:09.760 --> 12:14.200
what would that look like from a retrieval perspective?

12:14.200 --> 12:18.080
And the starting point for everything retrieval is TFIDF.

12:18.080 --> 12:21.280
Does everybody know what TFIDF is?

12:21.280 --> 12:21.720
No?

12:21.720 --> 12:23.080
OK.

12:23.080 --> 12:29.400
So TFIDF is basically a sparse retrieval method where you have a score function that

12:29.400 --> 12:33.280
looks at documents and queries, so E and Q.

12:33.280 --> 12:35.280
And then there are basically two terms that matter.

12:35.280 --> 12:40.680
One is the TF, the term frequency, and the other is the IDF, the inverse document frequency.

12:40.720 --> 12:45.160
So this inverse document frequency is actually a really nice idea from Karen Spark-Jones,

12:45.160 --> 12:46.560
really underrated researcher.

12:46.560 --> 12:49.080
She's done some amazing work.

12:49.080 --> 12:53.560
But the basic idea is that you want to look at the words that are very special,

12:53.560 --> 12:55.880
so that don't occur in lots of different documents.

12:55.880 --> 12:59.440
And so the overlap between the word the doesn't really matter, right?

12:59.440 --> 13:01.520
Like the occurs everywhere.

13:01.520 --> 13:04.040
So you want to have sort of the special words.

13:04.040 --> 13:06.440
So that's what TFIDF does in a nutshell.

13:06.440 --> 13:10.040
It gives you a score for document query overlap.

13:10.040 --> 13:13.760
And then you can do all kinds of things here with how you weigh it.

13:13.760 --> 13:17.680
So there's all these weird different parameters like this B and things like that

13:17.680 --> 13:22.360
that allow you to make it better than just having the TFIDF score.

13:22.360 --> 13:24.520
So there's a couple of tweaks you can do there.

13:24.520 --> 13:29.400
So BM25, actually, in case you're wondering, stands for Best Match 25.

13:29.400 --> 13:34.000
So I tried to discover where does the 25 actually come from?

13:34.000 --> 13:39.160
That's because the preceding 24 experiments failed.

13:39.160 --> 13:41.520
So it's literally the 25th one that seemed to work.

13:41.520 --> 13:44.080
And that's why it's called BM25.

13:44.080 --> 13:44.840
Bizarre.

13:44.840 --> 13:48.760
But so this is sparse retrieval.

13:48.760 --> 13:50.000
It's just counting words.

13:50.000 --> 13:53.880
So you have this massive, massive vector of all these word occurrences.

13:53.880 --> 13:56.400
It's sparse because most words never occur.

13:56.400 --> 14:02.200
So it's sort of like a vector of vocabulary size dimensions.

14:02.200 --> 14:04.880
So most of that is obviously zero.

14:04.880 --> 14:07.120
But so that's actually kind of a nice property.

14:07.120 --> 14:12.440
If you want to do fast search on a CPU, because on a CPU sparse product,

14:12.440 --> 14:14.440
it's very easy to compute.

14:14.440 --> 14:21.160
So this is used in the system called Dr. QA, which is really one of the first

14:21.160 --> 14:27.320
neural instances of this open domain, sort of open book question answer in paradigm.

14:27.320 --> 14:31.760
So you have a question like how many of our cells in habitants, blah, blah.

14:31.760 --> 14:35.280
So you want to ask basically Wikipedia what the answer is for this.

14:35.280 --> 14:38.960
So then you have this document retriever based on the sparse.

14:38.960 --> 14:42.760
So BM25, I think in this case, retrieval methods.

14:42.760 --> 14:45.480
You pass that to this.

14:45.480 --> 14:50.640
I think this was still by LSTM at the time, a document reader model.

14:50.640 --> 14:54.280
And then that model gives you the answer.

14:54.280 --> 14:58.080
So this, I think, is really the first instance of having sort of this separation

14:58.080 --> 15:03.000
between a retrieval and a generator system that you use for answering complicated

15:03.000 --> 15:07.200
questions based on sort of open domain knowledge.

15:07.200 --> 15:12.800
So after the sparse stuff, there was a bunch of work on dense retrieval.

15:12.800 --> 15:17.080
And so the advantage of dense retrieval, so this is just like word embeddings,

15:17.080 --> 15:20.240
basically vectors, like they're dense now, no longer sparse.

15:20.240 --> 15:24.080
So they're much smaller in terms of dimensionality.

15:24.080 --> 15:28.960
And a nice advantage of dense retrieval is that it's not really about specific words.

15:28.960 --> 15:35.040
So if they're synonyms, you can still find the relevant document,

15:35.040 --> 15:37.720
which you couldn't really do with a sparse representation.

15:37.720 --> 15:43.400
So that's really the advantage of dense is that you get like semantic similarity.

15:43.400 --> 15:46.040
So you can do this over word embeddings.

15:46.040 --> 15:47.440
That doesn't really work all that well.

15:47.440 --> 15:50.240
But at the time that people started thinking about this,

15:50.240 --> 15:51.400
Bert was already out there.

15:51.400 --> 15:54.360
And Bert is really great for giving you a vector representation

15:54.360 --> 15:56.320
for an entire sequence of words.

15:56.320 --> 15:59.560
So it sent this representation or a passage representation.

15:59.560 --> 16:05.200
So there are all these cool systems like ORCA and DPR, the dense passage retriever,

16:05.200 --> 16:11.800
where they essentially use the retrieval as a kind of latent variable in the system.

16:11.800 --> 16:16.440
And the way to get the latent variable to work, to be good enough essentially

16:16.440 --> 16:21.640
to train the entire system, is to pre-train the retriever on relevant information.

16:21.640 --> 16:25.640
So for ORCA, they do something called inverse close.

16:25.640 --> 16:30.280
So they do kind of a close task where you want to find passages

16:30.280 --> 16:33.480
that are sort of relevant to the preceding passage.

16:33.480 --> 16:36.680
And in DPR, they just train it on the supervised thing.

16:36.680 --> 16:40.840
But really, the core idea here is that, as you can see in this graph here,

16:40.840 --> 16:44.560
you can do better than BM25 if you add lots of documents.

16:44.560 --> 16:47.080
And the way you compute the score function is much simpler.

16:47.080 --> 16:51.000
It's just a dot product, right?

16:51.000 --> 16:53.960
So the nice thing about dot products

16:53.960 --> 16:58.760
is that you can do them very, very efficiently on the GPU as well

16:58.760 --> 17:00.560
if you know what you're doing.

17:00.560 --> 17:05.200
So what you really want to get at is Maximum Inner Product Search MIPS.

17:05.200 --> 17:09.160
So this is one of the kind of core ideas of a lot of this stuff.

17:09.160 --> 17:14.560
And you can do MIPS with ANN, Approximate Neighbor Search.

17:14.560 --> 17:18.960
And so there's this really brilliant piece of work out of there.

17:18.960 --> 17:22.200
For my colleagues at the time, I called FACE,

17:22.200 --> 17:26.160
which really underlies all of these modern vector databases, right?

17:26.160 --> 17:30.440
So all the popular ones are sort of re-implementations of this FACE idea.

17:30.440 --> 17:33.680
One is in Rust, one is in Go, but it's all basically the same idea.

17:33.680 --> 17:36.200
It's just FACE.

17:36.200 --> 17:39.800
And so FACE really powers a lot of this stuff.

17:39.800 --> 17:42.720
And whenever somebody tells you something about a vector database,

17:42.720 --> 17:48.200
just think about FACE, very fast dot product.

17:48.200 --> 17:51.080
So obviously, you can go beyond dot product.

17:51.080 --> 17:51.800
Yes.

17:51.800 --> 17:53.600
What is FACE?

17:53.600 --> 17:55.240
What is FACE?

17:55.240 --> 17:59.000
So it's an open source library, Facebook AI similarity search.

17:59.000 --> 18:00.280
Yeah, it's something.

18:00.280 --> 18:03.080
Yes.

18:03.080 --> 18:06.440
No, so it's just basic off the shelf, ANN algorithms.

18:10.720 --> 18:13.160
Yeah, so there are all kinds of different,

18:13.160 --> 18:16.960
I don't know if you would like product quantization and things like that.

18:16.960 --> 18:20.400
So you have a bunch of vectors.

18:20.400 --> 18:24.920
And you can just compute the full dot product, which is sort of inefficient, right?

18:24.920 --> 18:29.400
So what you can do is try to compress subspaces of the vector

18:29.400 --> 18:33.120
and then just look at the kind of centroids.

18:33.120 --> 18:36.560
So you can quantize sub-vectors of the full vector

18:36.560 --> 18:42.280
and then do much faster search over just the centroids.

18:42.280 --> 18:43.000
It's a good question.

18:43.000 --> 18:43.800
Any other questions?

18:47.560 --> 18:48.480
All right.

18:48.480 --> 18:50.640
So about this dot product idea, right?

18:50.640 --> 18:55.920
So what we have here is some people call this a Siamese network, I guess it is, right?

18:55.920 --> 19:00.720
So you have two different BERT models or whatever your encoder is here.

19:00.720 --> 19:02.480
And then at the end, you get these two vectors

19:02.480 --> 19:05.760
and then you just do dot products where you get one single score.

19:05.760 --> 19:08.080
But you can do all kinds of much fancier things

19:08.080 --> 19:13.360
if you're willing to give up on this buy encoder approach.

19:13.360 --> 19:18.440
So a really nice example from one of your colleagues here at Stanford is Colbert.

19:19.400 --> 19:22.880
So what this does is late interaction.

19:22.880 --> 19:25.640
So instead of just having this dot product here,

19:25.640 --> 19:30.560
you have a kind of more complicated version of computing score

19:30.560 --> 19:35.080
where you aggregate over sort of maximum similarity scores between different words.

19:35.080 --> 19:37.920
So I only recently actually discovered that this is called Colbert

19:37.920 --> 19:40.560
because of the late night show Colbert.

19:40.560 --> 19:46.520
So it's sort of Omar's joke actually, this name, but just so you know, if you run into it.

19:48.920 --> 19:55.240
So, but I think if we look at kind of where the state of the art has been going now,

19:55.240 --> 19:58.920
one of the nice things about inspector databases is that they're super efficient, right?

19:58.920 --> 20:02.120
So dot product is much more efficient than this late interaction stuff,

20:02.120 --> 20:04.760
especially if you do the approximate nearest neighbor search.

20:05.880 --> 20:07.880
But there's been some really cool work.

20:07.880 --> 20:14.440
So things like SPLATE, they basically have sparse meat dents in a way.

20:14.440 --> 20:15.960
So one of the big problems, as I said,

20:16.040 --> 20:19.320
with sparse is that you can't really handle synonyms and things like that.

20:19.320 --> 20:23.320
But what you could do is take a dense model, like a bird model,

20:23.320 --> 20:26.600
look at kind of this one word in your sequence,

20:26.600 --> 20:29.400
try to see which other words fit in the same slot.

20:29.400 --> 20:30.840
So that gives you the synonyms.

20:31.640 --> 20:35.880
So now you can give all these synonyms to a sparse vector,

20:35.880 --> 20:37.960
and then you can just do sparse dot product.

20:37.960 --> 20:40.760
And so have a much, much more efficient way to do search

20:41.720 --> 20:47.080
without sort of giving up on all the cool stuff that you get from a dense representation.

20:47.960 --> 20:49.080
So that's one thing.

20:49.080 --> 20:52.360
And this other idea I really like is called dragon.

20:53.640 --> 20:57.720
So this side, I think, is really the best generalized dense retriever.

20:57.720 --> 20:59.640
So if you want to take something off the shelf right now

20:59.640 --> 21:01.560
and just go to hugging face or something,

21:01.560 --> 21:06.440
then this dragon or dragon plus is probably the thing you want to use for a dense retriever.

21:06.440 --> 21:11.400
And the way they train this is through this progressive data augmentation strategy

21:11.400 --> 21:13.480
to make the model better and better over time

21:13.480 --> 21:15.320
by sampling very difficult negatives.

21:16.360 --> 21:19.480
And that gives you very good representations.

21:20.840 --> 21:22.520
And so the other thing about this,

21:22.520 --> 21:27.080
I think this is the only sort of final point about retrieval in general,

21:27.080 --> 21:29.400
is that what we see happening right now,

21:29.400 --> 21:32.040
if you look at sort of the developer community around drag,

21:32.040 --> 21:34.760
is that they're all doing hybrid search right now.

21:34.840 --> 21:38.600
So you can actually just combine the search results from your sparse,

21:38.600 --> 21:41.160
be in 25 or whatever thing or displayed,

21:41.160 --> 21:43.000
and you can combine them with your dragon.

21:43.960 --> 21:47.080
And then you'll get this ranking that works even better.

21:47.080 --> 21:48.760
So then you kind of get best of both worlds,

21:48.760 --> 21:51.800
but then you get all these questions about how do you combine the results.

21:53.960 --> 21:55.560
Any questions on this part?

21:57.160 --> 21:58.120
Oh, can you hear me?

21:58.920 --> 21:59.160
Yes.

21:59.880 --> 22:00.360
Oh, sorry.

22:00.920 --> 22:05.480
On the earlier slide, has there been any work on benchmark,

22:05.480 --> 22:10.920
how much less hallucination rag incurs over closed book question answering,

22:10.920 --> 22:14.280
for example, directly asking the large language model the question,

22:14.280 --> 22:16.360
has there been any benchmarking studies in this?

22:17.400 --> 22:20.680
Yeah, so there's a great paper, if I can say so myself,

22:20.680 --> 22:23.640
on the fact that retrieval augmentation reduces hallucination.

22:24.360 --> 22:25.880
It's from 2021, I think.

22:26.840 --> 22:30.040
So yeah, you can just find, if you literally look for

22:30.040 --> 22:33.080
retrieval augmentation reduces hallucination, then you'll find the paper.

22:33.960 --> 22:34.520
Oh, thank you.

22:37.880 --> 22:42.040
Well, let's see, is there a picture of your dance approach,

22:42.040 --> 22:43.640
and why do you need swaps?

22:44.200 --> 22:51.320
Yeah, so very often you want to have a very precise word overlap

22:51.320 --> 22:53.720
for things where you don't want to have the synonyms

22:53.720 --> 22:55.240
or the kind of nearest neighbors.

22:55.240 --> 22:59.480
So if there's like a brand name or something like that,

23:00.040 --> 23:02.520
then like let's say your brand is Apple,

23:02.520 --> 23:04.360
you don't want to find stuff about Paris.

23:05.080 --> 23:07.320
So that's what you would do with a dance retriever.

23:08.360 --> 23:12.040
So it really kind of depends on what you want to use it for.

23:12.040 --> 23:13.880
That's why hybrid is probably the way to go.

23:15.720 --> 23:16.440
It's a good question.

23:17.400 --> 23:24.040
But with a dance, it's contextualized in many ways.

23:24.040 --> 23:28.040
Should it realize Apple, the company, would be different from that?

23:28.040 --> 23:28.600
No.

23:28.600 --> 23:31.400
So if they were actually contextualized, then yes,

23:31.400 --> 23:34.200
but very often it's a frozen retrieval system.

23:35.000 --> 23:37.640
That's one of the problems with all the frozen rag stuff.

23:42.040 --> 23:43.640
I might be missing something very, very soon.

23:43.640 --> 24:01.160
So the sort of document and the query, they're the same, right?

24:01.160 --> 24:03.320
So they're either sparse or they're dense.

24:03.320 --> 24:04.840
But so if they're sparse,

24:04.840 --> 24:07.800
the components of the vector are literally the other words.

24:07.800 --> 24:15.880
And you just finalized when you're thinking about the thing that creates the names?

24:17.800 --> 24:19.640
How are you getting expressed here?

24:19.640 --> 24:21.160
So it literally counts, right?

24:22.040 --> 24:26.680
So basically it's one big matrix of documents as rows

24:26.680 --> 24:29.240
and the columns are the words in the documents.

24:29.240 --> 24:32.280
And then you just count how often a word occurs in a document.

24:33.000 --> 24:34.200
So that's a sparse.

24:40.520 --> 24:41.000
Yeah.

24:41.000 --> 24:46.440
And so in the field, we call them sparse embeddings or sparse retrieval

24:46.440 --> 24:48.280
because most of that vector is zero.

24:49.560 --> 24:51.880
Because most words don't occur in that document.

24:54.040 --> 24:55.000
Does that make sense?

24:58.600 --> 24:58.840
Cool.

24:59.800 --> 25:04.920
So let's talk about doing slightly better.

25:04.920 --> 25:08.840
So going back to Steven's question about, okay, we have this kind of retrieval thing,

25:08.840 --> 25:14.360
but how do we actually make this retriever good for the context that is going to be used in?

25:14.360 --> 25:17.560
And so can we contextualize the retriever for the generator?

25:18.280 --> 25:22.120
Even if it's a generator where we might not have access to the weights.

25:22.120 --> 25:26.520
So it could be a GP4 model, we just send it to some API, we get some stuff back.

25:27.480 --> 25:31.240
And so one paper I would like is called Replug.

25:32.360 --> 25:34.920
So just to kind of explain what this looks like.

25:34.920 --> 25:39.720
So you have this context, you have a retriever that we do the standard retrieval step with.

25:39.720 --> 25:40.920
This is a dense retriever.

25:41.960 --> 25:46.840
And now, sorry, and now you compute the likelihood.

25:46.840 --> 25:51.320
So basically just normalize the scores that you get for the top K documents

25:51.320 --> 25:53.000
to get a distribution here.

25:53.000 --> 25:59.080
And then you'll give each one of the retrieved documents separately to this generator,

25:59.080 --> 26:00.600
to your language model.

26:00.600 --> 26:05.320
So you can look at the perplexity of the correct answer for that language model.

26:06.040 --> 26:10.280
So now we have these two probability distributions, or two likelihoods essentially,

26:10.280 --> 26:13.880
and we can minimize the KL divergence to make sure that we can actually

26:14.520 --> 26:20.200
retrieve the documents that lead to the lowest perplexity on the right answer for the language model.

26:21.400 --> 26:24.920
So super simple idea, works really, really well.

26:26.360 --> 26:30.920
And the nice thing about this is completely agnostic of what happens upstream.

26:30.920 --> 26:34.600
So this will work for any sort of encoder decoder for any language model.

26:35.720 --> 26:41.320
What you need is a perplexity score, but for most language models you can get that,

26:41.320 --> 26:42.520
not necessarily all of them.

26:43.080 --> 26:43.880
So that's one thing.

26:43.880 --> 26:46.280
And then there's this other really nice approach.

26:47.000 --> 26:57.400
So in the retriever, you're literally updating the dense representations,

26:58.040 --> 26:58.280
right?

26:58.280 --> 27:00.840
So your encoder basically for your dense representation.

27:00.840 --> 27:02.920
That's a good question, we'll get into that a little bit more.

27:04.600 --> 27:09.800
So there's another paper on in-context retrieval augmented language models,

27:09.800 --> 27:15.400
where the whole paper is basically about just doing BM25 and just giving stuff directly

27:15.400 --> 27:17.960
through the context of the language model and things kind of work.

27:17.960 --> 27:24.280
So it's sort of frozen rag, but even more primitive in a way where the retriever is

27:24.920 --> 27:28.120
this very old sparse algorithm, but it works really, really well.

27:28.920 --> 27:33.160
But then they have this really awesome section where they show that you can just

27:33.160 --> 27:40.200
have this reranker on top of the BM25 results, and you can backdrop into this reranker.

27:40.200 --> 27:43.160
So now you still keep the language model completely fixed.

27:43.160 --> 27:46.680
So that's sort of this part of the loss here.

27:46.680 --> 27:49.320
So you have kind of a stop gradient on the parameters data.

27:49.320 --> 27:50.680
That's just your language model.

27:51.240 --> 27:56.920
But now you have this kind of rank function here that you can backdrop into, right?

27:56.920 --> 27:58.200
So that's your reranker.

27:58.200 --> 28:01.720
It's basically it can be a burden model or anything like that that works on top of the things

28:01.720 --> 28:04.040
you initially retrieved from your BM25.

28:04.040 --> 28:07.240
And now you have this birth reranker that you can backdrop into.

28:08.760 --> 28:11.320
So this also works really, really nice.

28:11.320 --> 28:16.760
So we're slowly progressing towards having a system that is much more optimized for

28:16.760 --> 28:21.960
being properly retrieval-augmented in a way where it's useful and contextualized for what

28:21.960 --> 28:22.920
you want to use it for.

28:24.840 --> 28:28.280
So yeah, just to point out kind of what that looks like with this reranker.

28:28.280 --> 28:30.840
So you just have this extra step essentially, right?

28:30.840 --> 28:34.920
So we have our retriever, then we have our reranker, then we have our generator and our output.

28:36.600 --> 28:38.680
Any grades to the language model?

28:39.800 --> 28:41.000
No, not necessarily.

28:42.280 --> 28:45.400
So for this one, you do, yeah.

28:45.400 --> 28:47.160
But so for a redeplug, you don't, right?

28:48.680 --> 28:49.560
Yeah, for this one.

28:49.560 --> 28:51.960
Yeah, yeah, yeah.

28:51.960 --> 28:53.880
So basically, yeah, you need to get-

28:53.880 --> 28:55.560
Do you guys provide them?

28:55.560 --> 28:56.200
Not all of them.

28:57.160 --> 29:01.080
Some of them do, but yeah, there are all kinds of tricks you can do on top of that.

29:03.960 --> 29:09.240
So basically, the question is how do we get sort of gradients flowing into this, right?

29:09.240 --> 29:14.040
So if you don't actually have access to the full parameters of the model so that you can backprop

29:14.040 --> 29:20.120
all the way through it, then you can do a reinforce style loss on the retrieval,

29:20.120 --> 29:24.360
and then you just pass the kind of log likelihood if you have access to that,

29:24.360 --> 29:26.040
or some other kind of black box function.

29:31.480 --> 29:39.080
All right, so the next thing you can do is to optimize both the retriever and the generator.

29:40.200 --> 29:46.600
And so this really starts getting to the proper kind of contextualization of the entire architecture

29:46.600 --> 29:48.840
where you want everything to work together, right?

29:48.840 --> 29:53.240
So rather than having this frozen thing where everything is basically not aware that the other

29:53.240 --> 29:54.040
part exists, right?

29:54.040 --> 29:55.240
It's like two halves of the brain.

29:55.240 --> 29:56.760
They're not talking to each other.

29:56.760 --> 29:58.920
One is your retriever, the other is your language model.

29:58.920 --> 29:59.720
There's no connection.

29:59.720 --> 30:03.400
They're just like sort of like something is thrown over the fence and then you hope for the best.

30:03.960 --> 30:07.160
So instead of that, we have everything much closer and learning together.

30:08.120 --> 30:16.280
So one of the first ways of doing this with the generator was ragged retrieval augmented

30:16.280 --> 30:19.160
generation, which we did at fair in 2020.

30:20.440 --> 30:23.720
And it's very similar to what we've already seen.

30:23.720 --> 30:27.320
We basically have this retriever here that works over different documents.

30:27.320 --> 30:33.560
You get some score function that gets given to this generator that generates the answer.

30:33.560 --> 30:38.040
And now you want to backdrop all the way and update your generator as well, right?

30:38.040 --> 30:41.880
So in the previous two architectures, we saw you keep the generator fixed,

30:41.880 --> 30:46.040
you backdrop into your retriever, but here we update everything.

30:46.600 --> 30:52.280
Well, not exactly everything as you'll see, but we'll also update the part of the retriever

30:52.280 --> 30:53.080
and the generator.

30:54.440 --> 30:59.080
So in this ragged model, we actually have two different ways of doing this.

31:00.040 --> 31:02.360
It's probably something that when we talk about this,

31:03.480 --> 31:07.880
if you think about this long enough, then you'll think like, okay, but when actually do I need to

31:07.880 --> 31:14.120
retrieve? Do I retrieve every time I generate a new token or do I just retrieve once and then

31:14.120 --> 31:20.600
generate an entire sequence, right? Or maybe I want to retrieve every end tokens, right?

31:20.600 --> 31:23.400
So these are hypergrams, or maybe I want to learn when to retrieve,

31:23.400 --> 31:25.720
as we'll see that's also something people have done.

31:26.360 --> 31:28.840
So these are two different ways to do it.

31:30.040 --> 31:35.000
And what we do in this paper, basically the whole point of the paper is that this frozen thing

31:35.000 --> 31:37.320
doesn't really work all that well, right?

31:37.320 --> 31:42.920
So I think what people call rag now is usually referred to the frozen thing,

31:43.480 --> 31:47.320
but the whole paper basically would never have been accepted anywhere if we had just done the

31:47.320 --> 31:52.280
frozen thing, right? The whole point of the paper is that you want to optimize it.

31:52.280 --> 31:56.920
And so at my company contextual, we call this frozen thing Frankenstein's monster,

31:56.920 --> 32:00.280
because it's really like you cobble together these different pieces, right?

32:00.280 --> 32:04.600
You sort of, yeah, it's really like Frankenstein, you just put it together and then it sort of

32:04.600 --> 32:08.600
walks, you know, but it doesn't really have to solve, it doesn't really actually work,

32:08.600 --> 32:13.640
because not the real thing. So that's great for everyone here, I think,

32:13.640 --> 32:17.960
because there are so many opportunities to do better than what most people are using right now.

32:17.960 --> 32:26.680
So one of the limitations of the original rag architecture is that it only supports a very

32:26.680 --> 32:32.360
small cave. So if you have lots and lots of documents, then the problem is that you have

32:32.360 --> 32:38.040
to fit all of them in the context, but how do you really get that to fit, right?

32:38.040 --> 32:45.080
So one thing you can do is you first encode things so that you get one single representation,

32:45.560 --> 32:49.720
or only diffuse for the top level representations, then you concatenate those,

32:49.720 --> 32:55.880
and then you just feed them to the decoder. So this is FID fusion and decoder. And as you can see,

32:55.880 --> 33:03.960
this scales to a much higher number of passages. And that leads to corresponding improvements in

33:03.960 --> 33:10.520
the scores that you care about. So that's a really cool idea. And so we're slowly moving

33:10.520 --> 33:15.240
towards more decoder-only architectures. So in rag, we have this barred model,

33:15.240 --> 33:19.400
it's sort of an encoder-decoder architecture, but here you just have this decoder that

33:19.400 --> 33:28.520
does some fancy attention over stuff that you retrieved before. And so another pure decoder

33:28.520 --> 33:36.600
language model architecture is this one, KNNLM, which I think is very elegant in its simplicity.

33:36.600 --> 33:42.040
So it's basically, you just have a normal language model, but you interpolate the normal

33:42.040 --> 33:48.360
language model weights with things that you retrieved. So basically, you have some sort

33:48.360 --> 33:54.040
of prompt, right? So like Obama's birthplace is, you go to your big corpus, you find similar things,

33:54.760 --> 34:00.360
you look at the words that come next to the similar things, you rank that thing,

34:00.360 --> 34:06.200
you sample your top K, you renormalize that. So now you have a bunch of scores, and now you

34:06.200 --> 34:11.800
can just interpolate between your retrieved kind of non-parametric memory scores and your

34:11.800 --> 34:16.600
parametric language model scores. So this is very late fusion in a sense, right? At the very

34:16.600 --> 34:22.040
end, you combine these two, and it allows you to reweight the pure language model probabilities

34:22.040 --> 34:27.960
or like this. So this works really well, and it scales especially well if you have a huge

34:28.920 --> 34:33.560
retrieval corpus. And so if you have trillions and trillions of tokens in there, you can have

34:33.560 --> 34:39.160
a much smaller language model that does not that much heavy lifting because you can really rely on

34:39.160 --> 34:45.880
this big source corpus that you're working from. And so that idea was exploited by this paper called

34:45.880 --> 34:52.680
Retro out of DeepMind, where they showed that you can have a 25 times smaller retrieval augmented

34:52.680 --> 34:58.360
language model trained from scratch, so really pre-trained entirely from scratch, that outperforms

34:58.440 --> 35:03.880
this 25 times bigger language model on the same data in terms of complexity, which is pretty

35:03.880 --> 35:09.800
impressive. So this architecture is much more efficient than a parametric model because you

35:09.800 --> 35:15.640
can rely on this external memory. So if your external memory is big enough, you can get pretty

35:15.640 --> 35:22.120
huge gains. So there was a lot of excitement about Retro when it was announced, but this is a DeepMind

35:22.120 --> 35:27.320
paper, so there's really no open source, nothing really to validate that this actually works.

35:28.600 --> 35:33.720
And so very recently, there has been a bit of work from NVIDIA called Retro++,

35:35.320 --> 35:40.760
where they have this hybrid between the Retro architecture and then they do basically RAG,

35:40.760 --> 35:46.440
sort of they put the top one or the top K results in the context of the language model after all.

35:46.440 --> 35:51.880
So it's sort of a crossover between RAG and Retro, and they showed some really nice results here,

35:51.880 --> 35:57.880
but I think it's sort of pointing to this big flaw, I think, is that why is there still no

35:57.880 --> 36:03.880
good open source Retro model? It probably tells you something about whether it actually really

36:03.880 --> 36:09.800
works. I spent a lot of time in my career trying to reproduce DeepMind papers that didn't necessarily

36:09.800 --> 36:17.640
always work. And so I think the same is true for Retro, and that's why we need to do this

36:17.640 --> 36:20.680
in-context RAG on top of Retro to actually get it to work.

36:32.680 --> 36:37.880
So doing retrieval over that big corpus is not that difficult, actually.

36:39.480 --> 36:44.680
So there are even distributed face packages, you can just do everything yourself.

36:45.640 --> 36:51.480
Yeah, so in terms of compute, it's actually not that hard anymore to reproduce something like this,

36:52.280 --> 36:56.280
but I've tried several times and it's not really reproducible.

36:57.240 --> 37:01.400
So the only way to get it to work is if you do this in-context RAG on top of the Retro thing,

37:01.400 --> 37:06.040
and then as you can see here in the results, then it actually gives you a gain over the pure GPT

37:06.040 --> 37:10.920
model. So it starts from a GPT and then they kind of retrofit as they call it the GPT model.

37:11.880 --> 37:16.600
So in short, I think there's still a lot of work to be done in pre-training these systems,

37:16.600 --> 37:21.640
really from scratch. And Retro kind of showed that it might be possible, but we don't necessarily

37:21.640 --> 37:26.440
know exactly how to do it the right way. And this is really one of the interesting open questions.

37:28.360 --> 37:29.560
Any questions on that?

37:33.880 --> 37:34.360
Online?

37:38.920 --> 37:39.400
No? Okay.

37:41.160 --> 37:51.880
Then we'll move on. So let's go all the way with the contextualization now. So with Retro and with

37:51.880 --> 37:59.080
RAG, what we actually did is we only updated the query encoder. So updating the document

37:59.080 --> 38:06.280
encoder is very expensive. So one of the first papers, actually kind of the OG of the non-frozen

38:06.280 --> 38:11.880
dense retrieval augmented methods is this paper called Realm. This is really like visionary

38:11.880 --> 38:19.240
work. This was basically the first kind of version that did this properly, where they updated it

38:19.240 --> 38:25.480
all the way, including the document encoder. So can someone explain to me why it's expensive to

38:25.480 --> 38:33.960
update the document encoder? So let's say we have a trillion tokens in our corpus.

38:34.920 --> 38:41.000
So now we go all the way. So we basically do a forward pass. We get a gradient at the end.

38:41.000 --> 38:45.160
Now we back propagate the gradient through the retriever. We update the query encoder.

38:45.160 --> 38:50.040
Now we have to update the document encoder. So what do we then need to do after we've updated

38:50.040 --> 38:56.760
the document encoder? We need to re-encode the entire internet. So basically every single gradient

38:56.760 --> 39:02.920
update, we have to re-encode whatever our index is. So if this is like trillions of tokens, it's like

39:02.920 --> 39:07.480
re-encoding the internet after every batch update. So that's not very efficient.

39:09.240 --> 39:14.680
Well, I think it does look like we've got a very general international change. So if you learn

39:14.680 --> 39:20.040
digital or other sort of stuff, like if you basically take your old activations and that sounds

39:20.040 --> 39:24.120
like a long, unpredictable change to your entire business. Yeah.

39:26.920 --> 39:31.960
Yeah, that's one way to do it. So there are a bunch of different ways to update the

39:31.960 --> 39:37.880
document encoder. So what they do in Realm is they basically do it for te batches.

39:38.520 --> 39:42.680
Then they stop, they re-encode the entire internet, and then they train again.

39:43.400 --> 39:48.520
So it's sort of asynchronous updates. They have this very fancy sort of sharding mechanisms

39:48.520 --> 39:54.520
where they take down certain parts of their entire index and then update them kind of on the fly.

39:55.720 --> 39:59.880
So you can do it. It's just very expensive. So one of the things that a lot of people have been

39:59.880 --> 40:06.360
thinking about, not exactly the Laura idea, but similar versions of that are around, like,

40:06.360 --> 40:10.760
can you make it more efficient so that you don't have to do this asynchronously?

40:12.920 --> 40:18.600
So one of the downsides of this Realm architecture is that it's really just a BERT model, but then

40:18.600 --> 40:22.360
you do this retrieval augmentation on a BERT model with other BERT models. So it's not really

40:22.360 --> 40:28.200
generative. It's not really gen AI in the modern paradigm. But if you want to read one paper

40:29.160 --> 40:35.320
on this topic, like, this is a very good one to read. The other one that is really, really good

40:35.320 --> 40:44.520
to read is this paper called Atlas. So Atlas is, so this is out of fare with a bunch of folks,

40:44.520 --> 40:51.080
the folks who did, like, RAG, and the folks who did FID, and really a brilliant set of people.

40:51.080 --> 40:57.160
And this is really a comprehensive analysis of everything that's happening in this architecture.

40:57.240 --> 41:01.160
So the first question they really look at is, how do we train this retriever? So we've seen

41:01.160 --> 41:07.080
a couple of versions of this, but which one actually works better? They haven't really

41:07.080 --> 41:12.200
been compared in a head-to-head setting. So one thing is we have this FID style sort of

41:12.200 --> 41:18.040
attention distillation. So that's really too complicated to go into detail here, but the

41:18.040 --> 41:25.320
others are actually very simple. So one is this loss we've basically seen before. So we've seen

41:25.320 --> 41:30.200
this, I think, with the in-context RAG one. So we have a stop gradient on the language model,

41:30.200 --> 41:36.200
and then we update the retriever. The other one is what we've seen with Replug. So this is basically

41:36.200 --> 41:43.480
exactly the Replug loss. So we have the KL divergence of the documents and sort of the

41:43.480 --> 41:48.920
improvement that you see when you give it that document. The other thing they have is basically

41:48.920 --> 41:54.520
the inverse of that one. So if I take this one document out, how does that affect my

41:55.560 --> 42:02.360
perplexity of the language model? And so this one I think is actually quite elegant because

42:02.360 --> 42:08.280
that really gets to like, how valuable is this one single document for me answering this question

42:08.280 --> 42:17.000
correctly? So they compare all of these different versions, and what you can see is that the kind

42:17.080 --> 42:21.640
of Replug style loss and this leave one out loss, they performed a lot better than all of these

42:21.640 --> 42:26.120
others. So this fixed retriever or no joint pre-training, these are really kind of the

42:26.120 --> 42:32.520
baseline sort of frozen RAG models or closed book. And as you can see, you can do really a lot better

42:33.160 --> 42:38.920
if you optimize things. And so this leave one out thing is probably the best I would say.

42:40.040 --> 42:45.000
So then the other question is how do you actually like train that entire system? Like what data or

42:45.000 --> 42:50.600
what tasks do you train this on? So they also experiment with a bunch of different versions.

42:50.600 --> 42:57.960
So one is doing prefix lm, if you're familiar with that. So they basically take a chunk that

42:57.960 --> 43:03.240
occurs somewhere on the internet, and then they predict the next chunk from that chunk. So it's

43:03.240 --> 43:08.280
really like sentence to sentence. So maybe like skip thought back in the day, but now you have

43:08.280 --> 43:14.680
this retrieval step where you predict the next sentence. Then they just do T5 styles or denoising.

43:14.680 --> 43:20.200
So that's mass language modeling if you're familiar with T5. And then they have this title for section

43:20.200 --> 43:26.120
generation piece. So I think the takeaway from this table is basically that whatever you do here,

43:26.120 --> 43:31.160
so they're using T5 models. So whatever you do here needs to be the same that your language

43:31.160 --> 43:40.440
model expects. So for T5, that's T5 style loss. And then the next sort of final question that

43:40.440 --> 43:45.480
they look into going back to what we talked about, how exactly do we update this retriever?

43:46.200 --> 43:51.560
So do we have to update the document encoder? Or do we maybe have to do some sort of re-ranking?

43:52.200 --> 43:58.360
Or do we maybe just update the query? And quite surprisingly, I think they find that just updating

43:58.360 --> 44:04.360
the query. So like in your original RAD paper is actually already basically good enough in many

44:04.360 --> 44:10.040
cases. So that's nice because it's much more efficient if you don't have to update your documents

44:10.040 --> 44:16.520
all the time. I think the real question here though is like, how good is your document representation

44:16.520 --> 44:21.720
to begin with? So you need to have a very, very high quality embedding model for this to work.

44:21.720 --> 44:26.440
If you don't have that, then this will not work. But if you do have that, then you get a very nice

44:26.440 --> 44:36.280
kind of query site fine tuning thing. So the Atlas paper is about trying to do few shop

44:37.240 --> 44:41.720
sort of language modeling tasks. So it's how many examples are given in the context.

44:46.680 --> 44:52.680
Yeah, so the main takeaway here is that if you compare like the close book equivalent model

44:52.680 --> 45:01.000
to the retrieval augmented model, you see very big improvements. That's really the only takeaway

45:01.320 --> 45:09.480
of this entire section. But I think that that's really saying something in terms of what we should

45:09.480 --> 45:20.360
be thinking about. How much time do I have until? Okay. All right. Other questions?

45:20.360 --> 45:37.080
Yeah, so there can be different. So in Atlas, the Atlas basically tries everything. So they also

45:37.080 --> 45:42.520
tried to see what happens if I train this on Wikipedia, but I swap in like a sort of common

45:42.520 --> 45:49.560
crawl index. And I think so in Atlas, but also in retro domain finding is just the more the better.

45:50.760 --> 45:56.040
So it's really just like the bigger your index, the more likely you are to find the exact

45:56.040 --> 45:59.480
right thing and then make the right prediction.

46:05.080 --> 46:11.320
Any other questions on this? Oh, yeah. Sorry. This is a question about the generator in the,

46:11.320 --> 46:20.760
I guess, the RAG system. So recently I saw a paper on Mistral 7B. So it introduces a lot of these

46:21.400 --> 46:26.680
new architectural changes like the sliding window attention to handle longer sequences at a smaller

46:26.680 --> 46:32.840
cost and the group query attention for faster inference. I'd like to like know your thoughts on

46:33.400 --> 46:39.560
designing a generator specifically for RAG, leveraging, for example, where Mistral 7B

46:39.560 --> 46:44.440
currently is. Because for example, like the sliding window attention, I could see how that

46:44.440 --> 46:51.240
could be adapted to the RAG case. Yeah. So maybe you're agreed on sort of what makes Mistral's

46:51.240 --> 46:55.720
special is a bit different from mine. So I don't think that the sliding attention window thing is

46:55.720 --> 46:59.880
actually that interesting. The reason Mistral works so well is because it's trained on a lot of data.

47:00.680 --> 47:04.520
You can do that more efficiently because you have sliding window attention. So you don't need to

47:04.520 --> 47:11.960
attend to everything. But so to answer your question, I guess you're asking sort of about the

47:11.960 --> 47:17.640
architecture of the generator if you know that there's going to be a retriever. So I think

47:18.360 --> 47:25.560
that's basically what Retro tried to do. So Retro actually, some of the people on the Retro paper

47:25.560 --> 47:32.760
are at Mistral now. So they have this chunk cross attention idea here. So you basically

47:32.760 --> 47:37.800
have a language model, but the way it does attention over the things you retrieve in your

47:37.800 --> 47:46.280
Retro architecture, they kind of get integrated into a model, not using the standard attention

47:46.280 --> 47:53.160
mechanism, but using this slightly different chunk cross attention. Okay. So I think the

47:53.160 --> 47:59.480
sliding window attention point I was trying to get at was that it uses a fixed window so that

47:59.480 --> 48:05.720
whenever you're doing the query key computation with the query vectors and the key vectors,

48:05.720 --> 48:13.480
you're using a fixed window attention. So I think my idea was to actually, one, use a dynamic window

48:13.480 --> 48:20.360
because for example, the rag case, if you use a fixed window when you're doing attention, it is

48:20.360 --> 48:27.240
possible that you actually are leaving, you're only looking at a fixed span of information. So if you

48:27.240 --> 48:33.320
could maybe adapt Mistral so that you could make it better for the rag case in, for example,

48:33.320 --> 48:39.400
the making the fixed window size, the dynamic window. Yeah. Yeah, I think it's an interesting

48:39.400 --> 48:46.840
idea. So for me, what Mistral is doing with the sliding window, that's basically like a confnet.

48:47.400 --> 48:52.280
So we had all these convolutional like light confnets where we would have word embeddings,

48:52.280 --> 48:57.080
and you would do confolutions over it and then pull, and then you would still get the information

48:57.080 --> 49:02.120
out. So it's not that the sliding window prohibits you from looking earlier, it's just that that

49:02.120 --> 49:10.760
happens higher up in your transformers. So I think that definitely is an interesting

49:10.760 --> 49:18.440
direction to think in. Yeah, so I think it's like not too crazy to say, are there any architectural

49:18.440 --> 49:23.320
changes that we can introduce into these seven billion parameter models so that they could be

49:23.320 --> 49:32.200
better adapted to the rag case? Yeah, so there might be. Yeah, I think one question is just how

49:32.200 --> 49:38.760
do you do the attention over things you've retrieved, which is what you're doing. Yeah, thanks.

49:40.600 --> 49:47.560
So just to make sure I understand, I mean in this retro model, you're retrieving each block,

49:48.520 --> 49:53.960
and when you struggle about putting a retrieval in the context, are you saying that you'll need to

49:53.960 --> 50:00.520
do it at the beginning and you don't do it at the block? Yeah, so in context, so it's not exactly

50:00.520 --> 50:09.160
every layer, so it's every token, so every step basically, not every block, so it doesn't make

50:09.240 --> 50:18.360
sense. So it's not every layer that you're doing a retrieval. Yeah, so every step. So this is kind

50:18.360 --> 50:24.600
of like what rag token is, so you retrieve every token, so you generate and then you can retrieve

50:24.600 --> 50:29.640
again, or in the case of retro, you can generate like a chunk and then you retrieve chunks again.

50:31.080 --> 50:35.480
If you look at the in-context case, you retrieve once at the beginning and then you give it.

50:36.200 --> 50:51.800
So here you don't actually give it as context at all, like directly to the model, right,

50:51.800 --> 50:56.040
so here you let the decoder kind of attend over it.

50:56.040 --> 51:06.600
Yeah, so I don't think cross-attention really works, yeah.

51:11.800 --> 51:12.600
Other questions?

51:13.800 --> 51:19.560
Yeah, inside the in-context case, the retrieving of the retriever is not necessarily,

51:19.720 --> 51:26.840
because of the large distribution loss, so I'm wondering inside of the cases, like what cases

51:27.640 --> 51:34.120
are really necessary need to evenize updates, or anyways updates for this argument.

51:35.160 --> 51:40.520
Yeah, so you do want to update the retriever, right, but only part of the retriever is necessary to

51:40.520 --> 51:49.400
be updated for a lot of these cases, but so I think it, so these are very specific data sets,

51:49.400 --> 51:54.920
right, natural questions, Wizard of Wikipedia and Fever, so they're really very kind of knowledge

51:54.920 --> 52:01.640
intensive tasks, so in that case, if you already have a very good system like DPR that is specifically

52:01.640 --> 52:07.640
pre-trained for those tasks, then you only need to update the query encoder, but so I would expect

52:07.640 --> 52:13.320
that if you move beyond this to kind of general language modeling things like retro, then you

52:13.320 --> 52:18.200
probably do want to update the document encoder, at least in a way where you can skate it.

52:19.400 --> 52:27.320
So I believe that in this part, it's very knowledge intensive, and actually a couple of

52:28.120 --> 52:37.080
very important topics, as long as we have a good office around knowledge of what

52:37.960 --> 52:46.600
many of the documents by those good models. Yeah, but so you need to learn how to kind of query

52:46.600 --> 52:53.400
into that index, right, so if you don't do that, then yeah, you don't get really good

52:53.400 --> 52:57.880
performance, so that's sort of like your closed book performance, right, if you just have the

52:57.880 --> 53:02.680
language model and you're just like, what does the parametric model on its own without the

53:02.680 --> 53:07.400
retrieval, what does it actually know? As you can see, there are pretty big gaps there, right.

53:07.720 --> 53:15.320
Other questions? Otherwise, I will cover other questions.

53:18.840 --> 53:25.880
No? Hello? Yeah, go for it. Okay, question, like so what about like more hierarchical retrieval,

53:25.880 --> 53:30.120
like I suppose there'll be methods trying to not just retrieve a single chunk, but there's some

53:30.120 --> 53:35.160
kind of like groups of chunks or something, or some rise versions. There's been some

53:35.160 --> 53:40.040
interesting work on doing that where you first tried to find, so you can have multiple indices

53:40.040 --> 53:43.720
and they can kind of cascade, right, so first you want to find the relevant document,

53:43.720 --> 53:47.480
so you have some document representation and then within that document you want to find the

53:47.480 --> 53:53.000
relevant chunk, so you can do it sort of that direction, you can also do it in reverse, I think

53:53.000 --> 53:57.480
I have something on a slide there where you can find the chunk and then sort of expand

53:58.360 --> 54:04.120
the context around it and then give that to the language model. So I think yeah,

54:04.120 --> 54:06.200
there are all kinds of interesting things you can do there.

54:08.360 --> 54:14.520
Cool, thanks, I guess another thing just like can you compare RAG versus like long context

54:14.520 --> 54:20.200
so efforts, so like there are lots of things like around just having a really long context and the

54:20.200 --> 54:28.120
extreme it could replace RAG, but I don't know like if it takes. Yeah, so everybody understands

54:28.120 --> 54:33.160
this question, right, so there's a trend where we want to have very long context language models,

54:33.160 --> 54:37.960
so that basically you can like take Harry Potter or something, just put it in the context and then

54:37.960 --> 54:42.920
ask a question like what is the name of like Harry Potter's owl or something, right, and then it can

54:42.920 --> 54:49.320
just attend over the entire thing. So attending over all of Harry Potter to answer that one question

54:49.320 --> 54:55.800
is super inefficient, right, so most of Harry Potter has nothing to do with the owl, so but you

54:55.800 --> 55:01.080
are still kind of reading it if you do it with the long context window, so that's why I think

55:01.080 --> 55:06.200
the doing it the RAG way where you have this non-parametric component is a much more efficient

55:06.200 --> 55:10.920
way to solve this problem, and if you actually look at the literature on long context windows,

55:11.800 --> 55:17.800
the way they solve the problem of scaling the attention mechanism is by making it very sparse,

55:18.440 --> 55:22.600
so they're basically turning it in, so that's a different kind of sparse, but they're turning

55:22.600 --> 55:28.200
it into a non-parametric retrieval problem kind of behind the scenes, so they're not actually

55:28.200 --> 55:32.440
all that different, if you want to scale long context then you're going to move towards a RAG-style

55:32.440 --> 55:42.600
architecture. Cool, thanks. All right, so let's talk about some other interesting questions,

55:43.240 --> 55:49.560
so one thing and I already alluded to this is when do we actually retrieve, so very if we're

55:49.560 --> 55:55.720
doing like if we want to like retrieve every token that's also very inefficient because I probably

55:55.720 --> 56:01.480
don't have to retrieve to generate the, right, I can probably do that on my own with the language

56:01.480 --> 56:07.160
model, it's sort of a waste to go and retrieve stuff, but if I only retrieve once at the beginning

56:07.160 --> 56:13.000
of the sequence that's probably also not great, so what we ideally want to be able to do is to say,

56:13.000 --> 56:16.680
okay sometimes I want to retrieve, sometimes I don't want to retrieve and I'm going to learn

56:16.680 --> 56:24.600
when I want to kind of expend the compute budget on doing the retrieval, so a nice paper where they

56:24.600 --> 56:29.480
have a stab at, this is called FLARE for active retrieval augmentation where they basically have

56:29.480 --> 56:34.840
the language model decide when it should do a search and what it should do the search for,

56:36.520 --> 56:42.440
so I think this fits in a general trend that you can see in the field around kind of agents,

56:42.440 --> 56:49.160
so we can talk a little bit more about that too. So this other question that I think we've also

56:49.160 --> 56:53.720
kind of covered already here is how do we train this at scale, so we can do these asynchronous

56:53.720 --> 56:59.560
updates, we can do re-rankers, we can do query side only, there's this really nice paper which is

56:59.560 --> 57:07.400
quite close I think to the idea you proposed where you first use BM25 to create a batch basically

57:07.400 --> 57:14.440
where everything is very similar in terms of what you've retrieved and now you have this kind of

57:14.440 --> 57:19.480
in-batch update, so it's sort of like a re-ranker where you encode the information that is just in

57:19.480 --> 57:24.840
your batch using this other model and now you can update this model on the fly, so you don't have

57:24.840 --> 57:30.600
to worry too much about doing the full kind of document side update and again here what really

57:30.600 --> 57:34.840
matters is like how big is your index, if you have an amazing index you can basically solve

57:34.840 --> 57:40.600
any problem just by looking it up, so rather than cramming it into your parameters you can just find

57:40.600 --> 57:50.120
it. This is a really nice paper called SILO, so one of the interesting things I think that's going

57:50.120 --> 57:55.000
to happen in the next year or two around language models is there and you've seen this already,

57:55.000 --> 57:59.880
there's a bunch of lawsuits against OpenAI and other places around where does the data exactly

57:59.880 --> 58:07.560
come from, so one very elegant solution I think is to have a RAG system that you train on data that

58:07.560 --> 58:14.120
you know is safe, so you can train that thing on Wikipedia but now during test time you can give it

58:14.120 --> 58:20.280
a data store that has maybe slightly riskier information in it, so this massive index of

58:20.280 --> 58:26.760
all the stuff on the internet including some things that are maybe higher risk, you can still

58:26.760 --> 58:31.720
have them in your index but your language model, your retrieval augmented language model I should

58:31.720 --> 58:35.640
say, you know that that thing is safe because it was trained on data that is public domain,

58:36.360 --> 58:40.360
so that's what they do in SILO and they show that that works really well, so that's

58:41.240 --> 58:46.520
one possible solution to a lot of the kind of compliance and legal risk around language model

58:46.520 --> 58:56.920
deployments. There's a great paper also from one of your colleagues around context getting lost in

58:56.920 --> 59:01.160
the middle, I think this is also kind of a fascinating phenomenon, this is on a frozen RAG system

59:02.120 --> 59:09.800
but language models are very similar to humans in what things they pay attention to, so if you

59:09.800 --> 59:14.680
give them a bunch of things that you retrieve, what they will look at are the first things you

59:14.680 --> 59:20.040
list and the last things you list and they will sort of ignore the middle, so if it actually

59:20.040 --> 59:25.240
respected the rank function then this curve would get down all the way, but it sort of goes up,

59:25.880 --> 59:32.040
so I think that's a very interesting observation which kind of shows that how brittle

59:32.760 --> 59:38.120
these these systems can be, so if you have a frozen RAG system it can be very very brittle where

59:38.120 --> 59:42.680
like the order of the retrieved context matters a lot in whether you get the right answer or not.

59:44.520 --> 59:49.960
It doesn't work on treating this as a very funny problem in the sense that the colleagues come back

59:49.960 --> 59:55.400
or those of like specifically going towards the interpretation, I'm going to try it out for that

59:55.400 --> 01:00:07.080
period that's going to inter-product with just the RAG. Yeah, so what I just described, somebody

01:00:07.080 --> 01:00:11.720
asked like how do you actually, so I said there are other ways to do this and then the question

01:00:11.720 --> 01:00:18.040
was how do you do that, so the way you do that is using reinforce, so yeah there has been work on

01:00:18.040 --> 01:00:23.480
doing that, so some of the older papers were playing with this, but one of the big problems with,

01:00:25.400 --> 01:00:30.440
so I think the replug solution is sort of more elegant for solving that problem

01:00:31.000 --> 01:00:35.640
because you actually do signal from the language model and if you just do reinforce it's very

01:00:35.640 --> 01:00:41.480
high variance, so it's going to be super finicky if you don't want to destroy your index,

01:00:43.080 --> 01:00:44.040
but people have tried it.

01:00:49.000 --> 01:00:56.920
So there's some really nice work from OpenAI where they basically show and again we're sort of like

01:00:56.920 --> 01:01:02.440
thinking more and more about agents here, where they show something very similar to the flare

01:01:02.440 --> 01:01:06.840
results from earlier with active retrieval that doesn't necessarily have to be some index that

01:01:06.840 --> 01:01:12.520
you only can read just some web search, and obviously in this case you don't really have

01:01:12.520 --> 01:01:16.840
access to the web search necessarily, so Bing or whatever they use here is not going to update

01:01:16.840 --> 01:01:22.200
these parameters, but I just wanted to kind of put this in your mind like this is another thing

01:01:22.200 --> 01:01:28.600
you can do, and if we take this really to the general form then you can think of language

01:01:28.600 --> 01:01:35.000
models as just tool users, so rather than just retrieval augmenting language models we can tool

01:01:35.000 --> 01:01:39.640
augment language models and retrieval is just one of the many tools that language models have access

01:01:39.640 --> 01:01:46.440
to, we can have re-rankers and things on top of the outputs of these tools, and so one of the big

01:01:46.440 --> 01:01:52.920
questions I think is how do you actually get the system to learn stuff, so we're going to need RL if

01:01:52.920 --> 01:02:01.880
we want the system to really learn how to take these actions properly, and so yeah this has been

01:02:01.880 --> 01:02:07.320
taken to the extreme in this sort of self-reg architecture where they have this sort of retrieval

01:02:07.320 --> 01:02:12.280
step and it's active and then you criticize it and then you basically do some natural language

01:02:12.280 --> 01:02:16.840
inference and all of that just with one language model to answer the questions.

01:02:18.760 --> 01:02:23.080
So the other missing piece, so I'm just kind of going through a bunch of open questions

01:02:23.640 --> 01:02:28.280
that people have looked at, but feel free to interrupt me if there's anything you want to know,

01:02:29.400 --> 01:02:33.880
but so instruction tuning we established at the beginning of the lecture that this is pretty

01:02:33.880 --> 01:02:40.600
important for getting things to work, so fixing the user interface, but the instruction tuning

01:02:40.600 --> 01:02:44.920
has almost always only happened on the language model and not on the entire system,

01:02:44.920 --> 01:02:49.880
so I think one of the interesting things that people are looking at now with things like RADT

01:02:49.880 --> 01:02:54.360
and Instruct Retro is how can we instruction fine-tune an entire retrieval augmented system,

01:02:54.360 --> 01:02:59.720
so all the way into the retrieval step can we generate data so that that also follows the

01:02:59.720 --> 01:03:03.720
instructions properly, which currently doesn't happen in any of these model architectures.

01:03:03.720 --> 01:03:10.040
And then finally, I think I would be remiss if I didn't really talk about

01:03:10.760 --> 01:03:14.840
what people call advanced RAG, so like the developer community has been really doing

01:03:14.840 --> 01:03:20.360
some awesome stuff, so like frameworks like Lamaindex and Langchain and there's all these

01:03:20.360 --> 01:03:24.920
open source vector databases like Chroma and Weaviate and they're all sort of about making

01:03:24.920 --> 01:03:30.280
RAG really easy, but this is all frozen RAG, right, but even with frozen RAG you've been

01:03:30.280 --> 01:03:36.120
really doing incredible things, so we mentioned some of these already, so child-parent recursive

01:03:36.120 --> 01:03:41.480
retrievers, so you find small parts and then you give the big parts around it to the language model,

01:03:41.480 --> 01:03:46.200
you can do hybrid search where we use reciprocal rank fusion, so we have like different search

01:03:46.200 --> 01:03:50.360
results that we didn't combine before we give the final thing to the language model.

01:03:51.000 --> 01:03:55.800
There's zero shot like a large language model remanker, so basically the score function is not,

01:03:55.800 --> 01:03:59.000
it doesn't come from your retrieval, it comes directly from the language model,

01:03:59.960 --> 01:04:04.840
and then hypothetical document embeddings which I think is a really cool idea, so you just,

01:04:05.560 --> 01:04:11.400
basically you fix hallucination through hallucination, so you get a question, then you let

01:04:11.400 --> 01:04:15.720
the language model hallucinate a bunch of possible answers, then you go and search for

01:04:15.720 --> 01:04:20.120
nearest neighbors to the possible answers and you give those as context and then it gives the right

01:04:20.120 --> 01:04:26.040
answer based on that, so it was really like hallucinating answers, I think it's a brilliant

01:04:26.120 --> 01:04:32.840
solution, so there's a lot of stuff happening in the kind of frozen rack community too that I think

01:04:32.840 --> 01:04:40.440
is very interesting to look at, so just to wrap up, kind of looking at the future of this stuff,

01:04:41.640 --> 01:04:45.960
there are still lots of very interesting open questions, so if you're a student thinking about

01:04:45.960 --> 01:04:54.200
how to solve any of these, I think you can have quite a lot of impact, so how exactly do we do

01:04:54.200 --> 01:04:59.240
the pretraining of this architecture and do we even need to pretrain, I think even retro kind of

01:04:59.240 --> 01:05:04.840
shows that you don't necessarily have to pretrain, so maybe there's something wrong with how we do

01:05:04.840 --> 01:05:09.800
that, what do skating laws look like, so I think there's a really interesting question here around

01:05:09.800 --> 01:05:15.560
if I have a huge index and a very rich encoder of all the information in that index, maybe I can

01:05:15.560 --> 01:05:20.760
move, so basically decouple all the memorization to this index, so I have a language model that

01:05:20.760 --> 01:05:25.240
doesn't know anything, it just speaks English, it just sort of reasons on top, but it has no

01:05:25.240 --> 01:05:29.480
knowledge because that always comes from this retriever, if you can do something like that then

01:05:29.480 --> 01:05:34.600
you get very interesting scaling trade-offs, so you can have a tiny language model and do your

01:05:34.600 --> 01:05:39.880
retrieval to do a lot of the heavy lifting with your retrieval, which is nice because that's a

01:05:39.880 --> 01:05:45.160
cached computation, so you already have the embeddings, you just need to do the dot product,

01:05:45.960 --> 01:05:49.240
so it's much more efficient than kind of self-attention in the language model.

01:05:50.840 --> 01:05:57.000
Can we move beyond by encoders, so vector databases, I like people who build vector

01:05:57.000 --> 01:06:03.720
databases, but I'm not sure how long we're going to keep vector databases, because I think

01:06:04.280 --> 01:06:09.400
re-rankers probably work just as well and the N25 is much more efficient than a vector database,

01:06:10.920 --> 01:06:17.560
so I don't really see why we need dedicated vector databases, so what we're seeing, but maybe

01:06:17.560 --> 01:06:23.480
this is a bit of a critique of Silicon Valley investment strategies and things like that, but

01:06:23.480 --> 01:06:29.000
a lot of these vector database companies are basically becoming database companies now,

01:06:29.000 --> 01:06:34.680
so they are adding all this sparse stuff because the dense thing is not enough, and as it turns out

01:06:34.680 --> 01:06:40.280
there are a lot of pretty good sparse databases out there already, like Postgres and things like

01:06:40.280 --> 01:06:46.200
that, and there are also all adding vectors to their databases, so I think that's all going to

01:06:46.200 --> 01:06:55.080
kind of coalesce into databases. So I think there are some interesting things to look at

01:06:55.800 --> 01:07:01.960
for kind of the data, so through this instruction problem, can we generate much better data for

01:07:01.960 --> 01:07:07.080
training RAG systems synthetically, and then I think there's this massive open question around

01:07:07.080 --> 01:07:11.560
how we actually measure whether the RAG system is any good, so right now we just look at downstream

01:07:11.560 --> 01:07:17.720
performance, which is sort of okay, but if you mess up the retrieval it's very hard to measure,

01:07:18.920 --> 01:07:23.800
but how to measure whether your retrieval is right is also very difficult, so there are some

01:07:23.800 --> 01:07:28.520
frameworks where they try to take like the harmonic mean of your retrieval accuracy and your language

01:07:28.520 --> 01:07:34.040
model accuracy, but I think those are also very shoddy because we don't really have very good

01:07:34.040 --> 01:07:38.840
data sets to measure that on, so I think that's a very cool problem to work on as well.

01:07:39.720 --> 01:07:45.720
So the other problem that I personally am always very excited about is multimodality,

01:07:47.000 --> 01:07:53.720
and so why would we stop with RAG systems with just text, so you can do the same with images,

01:07:54.280 --> 01:07:59.320
you can augment language models with vision, so we did this work on lens where we have a

01:07:59.320 --> 01:08:05.720
language model enhanced to see, where you can just give kind of a computer vision pipeline,

01:08:05.720 --> 01:08:10.040
just like a retrieval pipeline and give that to a frozen language model and pass it to the

01:08:10.040 --> 01:08:15.320
context and that system actually is an amazing visual question answering system. It's close to

01:08:15.320 --> 01:08:20.760
state-of-the-art sort of flamingo from DeepMind, which is also very hard to reproduce because

01:08:20.760 --> 01:08:28.840
there's no open source version of that, so we've done some early work on this in 2021 where we

01:08:28.840 --> 01:08:34.280
have this cross-modal retrieval and there's some more recent work out of FAIR where they also look

01:08:34.280 --> 01:08:38.920
at this, so I think that's really like if you look at the trend in the field like multimodality

01:08:38.920 --> 01:08:43.560
with GPD4 or V and things like that is really a hot topic, so everything is kind of going in

01:08:43.560 --> 01:08:52.040
that direction, so it's an interesting thing to think about. So overall I think it would be nice

01:08:52.040 --> 01:08:58.280
if everybody sort of moves away from RAG 1.0, the frozen Frankenstein RAG and moves towards this much

01:08:58.280 --> 01:09:03.960
more kind of optimized version RAG 2.0, so it's really about systems over models, right, it's

01:09:03.960 --> 01:09:07.800
not just your language model when you're retriever and they're kind of separate, it's about thinking

01:09:07.800 --> 01:09:12.600
from the from a system's perspective about the entire thing and the problem you're trying to solve

01:09:12.600 --> 01:09:17.640
and so I think that really is the way that in deep learning things have always progressed,

01:09:17.640 --> 01:09:22.360
where if you optimize the system end-to-end that's always going to win out, like back in the day in

01:09:22.360 --> 01:09:26.840
computer vision or NLP, we have like parsers and scene parsers and all this kind of stuff and all

01:09:26.840 --> 01:09:32.600
of that just doesn't exist anymore now because we optimize the system end-to-end and so that's

01:09:32.600 --> 01:09:37.640
what's going to happen here too. So if we take that to the extreme, like there's a chunker thing

01:09:37.640 --> 01:09:42.040
in your documents, right, like cutting it up into pieces, like you could backdrop into debt,

01:09:42.040 --> 01:09:49.560
like why not? Somebody should really do that and so yeah, I think like trading off cost and quality

01:09:50.360 --> 01:09:54.200
and zero-shot domain generalization, that's really like where this stuff is going to come in, right,

01:09:54.200 --> 01:09:59.080
so language models right now, they're amazing but very often they're way too expensive for being

01:09:59.080 --> 01:10:03.800
deployed somewhere where you can actually make money from them if you're in a company. So what

01:10:03.800 --> 01:10:08.760
you want to do is make it much more efficient and have the right cost quality trade-off and the

01:10:08.760 --> 01:10:13.640
easiest way I can think of is to do it through retrieval augmentation but obviously I'm very biased.

01:10:15.720 --> 01:10:22.040
So yeah, that was all I had actually. So if you're interested in this, I'm at Samford so I can

01:10:22.040 --> 01:10:26.760
work with you on research projects on these topics or if you want you can also join contextual

01:10:26.760 --> 01:10:34.040
because we work on this stuff every day. Thank you. Well, sorry, I had a question from earlier.

01:10:35.880 --> 01:10:41.240
Yeah, I think you said something really, really, I think really super helpful earlier about

01:10:41.240 --> 01:10:46.600
Miss Jill 7B. You talked about, you compared the sliding window attention to convolutional neural

01:10:46.600 --> 01:10:50.040
networks and I do see the parallel because with convolutional neural networks you have

01:10:50.680 --> 01:10:54.760
several layers of, several different layers of convolutional layers and the top convolutional

01:10:54.760 --> 01:11:00.120
layers are able to see a larger receptive field than the bottom convolutional layers

01:11:00.120 --> 01:11:07.720
and with convolutional layers you're able to tune the filter sizes and the strides so you're able

01:11:07.720 --> 01:11:12.520
to see a different receptive field and I was wondering if you could see that same innovation

01:11:12.520 --> 01:11:17.960
in Miss Jill 7B by tuning because you have different transformer layers and if each transformer

01:11:17.960 --> 01:11:23.000
layer will have a span over a different set of tokens and you can tune I guess the transformer

01:11:23.000 --> 01:11:27.720
architecture the way you tune those convolutional layers, the filter sizes, the receptive field,

01:11:27.720 --> 01:11:31.720
perhaps we can do some optimization in the transformer realm that we have already done

01:11:31.720 --> 01:11:37.880
in convolution layers. Yeah, I think that's a good idea. There's a great paper on light

01:11:37.880 --> 01:11:44.040
convolutions I think from Michael Auli and David Gange and a bunch of people where it's basically

01:11:44.760 --> 01:11:49.560
this came out at exactly the same time as the transformer and the transformer is slightly

01:11:49.640 --> 01:11:54.840
more optimized for GPU computation but the convolutional model was actually slightly

01:11:54.840 --> 01:12:01.400
better than the transformer so it's definitely worth exploring. Okay, cool, thanks.

01:12:01.400 --> 01:12:18.520
Yeah, so it depends on the problem I think what you probably want to do is

01:12:18.520 --> 01:12:24.280
is sort of cast a white net with VM25 and then just narrow it down with dense search

01:12:25.080 --> 01:12:29.560
so you often see that kind of as a two-stage process where the first one is kind of noisy

01:12:29.560 --> 01:12:34.120
you can add noise actually to your retrieval and then you use the dense one to filter it down.

01:12:36.760 --> 01:12:42.680
Yeah, everyone's trying to maybe adapt their plug-in model to almost

01:12:42.680 --> 01:12:49.240
the only specific area. I think there are many two ways of project one way is to use

01:12:49.240 --> 01:12:55.080
some instrument tuning in some kind of learning way or functionally like tuning that and another way

01:12:55.080 --> 01:13:02.520
is to the main project of this lecture is using the virtual augmented way. So I wonder if I

01:13:03.160 --> 01:13:10.680
know a message of virtual augmented way, do you think the capacity or the quality of virtual

01:13:10.680 --> 01:13:15.640
augmented way can be matched with those tuning methods I think or kind of learning?

01:13:15.640 --> 01:13:21.960
Yeah, so I think actually what's going to happen is that all of this will come together right so

01:13:22.920 --> 01:13:28.440
if you actually train things like end-to-end, rack 2.0 style then you can also fine-tune that system

01:13:29.000 --> 01:13:35.720
on some use case end-to-end. So why would you just take the retrieval augmented system if you can

01:13:35.720 --> 01:13:39.640
also fine-tune it on the thing you care about? So I think in the end everybody's going to do

01:13:39.640 --> 01:13:43.640
all of those things and then there's questions like how do you do that efficiently so that's

01:13:43.640 --> 01:13:50.520
why you would use adapters or things like that. I think there was another question.

01:13:52.920 --> 01:13:57.320
I'm curious about hardware, you say it's going to become database kind of thing,

01:13:57.320 --> 01:14:04.440
in fact it was part of the database but what about retrieval hardware and you know it's mine

01:14:04.440 --> 01:14:11.160
because we've got so much of the learning part but what about because it's huge,

01:14:12.360 --> 01:14:17.000
tree as I said so do you have any ideas it's just a database problem?

01:14:17.000 --> 01:14:24.280
So I don't know if I'm allowed to say this exactly actually but so one of the biggest chip

01:14:24.280 --> 01:14:29.880
manufacturers that recently their stock has done really well they have some dedicated retrieval

01:14:29.880 --> 01:14:40.200
hardware coming out I think sooner it might already be out. So yeah very efficient dense

01:14:40.200 --> 01:14:48.280
retrieval is a very big business. Other questions?

01:14:59.160 --> 01:15:04.920
Yes I think so if you take it to the extreme so one of the big problems right now is that

01:15:04.920 --> 01:15:08.440
if you contextualize an existing language model that already hallucinates

01:15:09.160 --> 01:15:13.000
then then it's going to be kind of hard to get rid of the hallucination right so if you do

01:15:13.000 --> 01:15:19.880
replug on GPT-4 GPT-4 might still hallucinate so you could basically just ignore all the stuff

01:15:19.880 --> 01:15:24.200
you retrieved and just do whatever it wants anyway so that's one of the reasons why you

01:15:24.200 --> 01:15:28.680
want to train the system end to end and if you take that to the extreme where like I said right

01:15:28.680 --> 01:15:34.200
if you can just have the language model only reason and speak so it knows English and reasoning

01:15:34.200 --> 01:15:39.640
but it has no knowledge which all comes from somewhere else then you can't hallucinate and so

01:15:39.640 --> 01:15:51.080
it's really all grounded in whatever is in your index but they're so they're about hallucination

01:15:51.080 --> 01:15:55.320
I'm sort of frustrated that a lot of people in the field misunderstand what hallucination even

01:15:55.320 --> 01:16:00.520
means like so a lot of people are conflating hallucination with a correctness or incorrectness

01:16:00.520 --> 01:16:04.520
so they're like oh the model made a mistake it hallucinated it's like no it made a mistake

01:16:05.320 --> 01:16:10.280
that's different from hallucination hallucination I think is very specific kind of I've retrieved

01:16:10.280 --> 01:16:16.600
something so I have some sort of counterfactual ground truth and what I'm saying does not correspond

01:16:16.600 --> 01:16:24.600
to that ground truth and so yeah I think there's a bunch of folks at Stanford also working on better

01:16:24.600 --> 01:16:27.480
measurements of hallucination and definitions and things like that

01:16:37.560 --> 01:16:43.960
yeah awesome ground truth right so so hallucination is really like there there is something that is

01:16:43.960 --> 01:16:49.000
true right so so if we're talking about like hallucination and yeah so if we're talking about

01:16:49.000 --> 01:16:53.640
just general parametric language models then sort of the ground truth is whatever we consider to be

01:16:53.640 --> 01:17:01.960
true right but we had to work for like language models making mistakes before it was called major

01:17:01.960 --> 01:17:14.120
mistakes yeah ground truth I guess this is solving that helps me to question that path

01:17:14.760 --> 01:17:19.560
are you working on ground truth per se that's around you know if I generate the building

01:17:20.120 --> 01:17:26.680
saying oh well I'm a president I mean everything all the time are you sharing work on that on this

01:17:27.720 --> 01:17:33.960
yeah so so I like the sort of silo uh mansion error as well so I think the whole point is that you

01:17:33.960 --> 01:17:40.120
can you can have different indices and different definitions of ground truth and so um I think

01:17:40.120 --> 01:17:46.600
you could say I only trust uh archive or I only trust like peer review papers and not just archive

01:17:47.480 --> 01:17:51.240
so you can make decisions in your architecture during test time about what you

01:17:51.240 --> 01:17:58.440
define as ground truth and I also think actually that and there's a bunch of work I think happening

01:17:58.440 --> 01:18:03.400
on this right now you can control for how how grounded you want it to be in your ground truth

01:18:03.960 --> 01:18:08.920
so that's another kind of misconception about hallucinations like sometimes hallucinations

01:18:08.920 --> 01:18:13.000
are actually good right if you have a creative writing assistant and you wanted to come up with

01:18:13.000 --> 01:18:18.440
some cool new ideas you want the language model to hallucinate uh so I think what you want to have

01:18:18.440 --> 01:18:22.680
is kind of a tunable knob where you say like oh now you can hallucinate and now maybe you should

01:18:22.680 --> 01:18:26.840
like really tell me the truth only

01:18:31.720 --> 01:18:34.840
anything else

01:18:35.160 --> 01:18:47.960
yeah so but the temperature that's just about how you sample right so how flat your your distribution

01:18:47.960 --> 01:18:57.640
is that you sample from yes but so even if you have a low temperature it can still come up with

01:18:57.640 --> 01:19:02.360
random stuff right so it just says that then you're very likely to do like really sampling

01:19:02.920 --> 01:19:14.200
um so so I think what you want to get at is is something more sophisticated than that

01:19:16.920 --> 01:19:22.200
yeah I like the question

