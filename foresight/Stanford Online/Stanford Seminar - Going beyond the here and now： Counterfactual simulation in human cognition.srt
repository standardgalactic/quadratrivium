1
00:00:00,000 --> 00:00:15,720
Well, thank you. Thank you so much for having me. It's a pleasure to be here. And I hope

2
00:00:15,720 --> 00:00:21,840
maybe some of the things that I talk about may give some inspiration for you, HCI guys.

3
00:00:21,840 --> 00:00:26,040
So I lead the causality and cognition lab in the psychology department. I'm interested

4
00:00:26,040 --> 00:00:30,240
in how people understand causality and basically how the world works and how they understand

5
00:00:30,240 --> 00:00:34,920
each other. And we're interested in how people learn about the causal structure of the world,

6
00:00:34,920 --> 00:00:38,680
how they, once they have it in their mind, how they can use it to reason about the world,

7
00:00:38,680 --> 00:00:42,320
make predictions, make inferences about the past, or think about maybe also how things

8
00:00:42,320 --> 00:00:46,120
could have played out differently from how they actually did. And how those capacities

9
00:00:46,120 --> 00:00:50,520
also allow us to make the kind of judgments we do in our everyday lives, like for example,

10
00:00:50,520 --> 00:00:55,600
assigning responsibility to one another. And that's in fact one of the bigger sort of overarching

11
00:00:55,680 --> 00:00:59,560
goals that my lab is working toward, namely developing a computational framework for

12
00:00:59,560 --> 00:01:04,000
understanding responsibility. And I think to get there, we have to be able to answer at

13
00:01:04,000 --> 00:01:08,040
least two questions, namely one being what causal role somebody's action played in bringing

14
00:01:08,040 --> 00:01:12,360
about the outcome. And the other one being what the action that the person took tells

15
00:01:12,360 --> 00:01:16,640
us about the kind of person that they are. For this first one, we need some intuitive

16
00:01:16,640 --> 00:01:20,600
theory of how the world works. So we can relate the actions that somebody took to the kind

17
00:01:20,600 --> 00:01:24,680
of outcomes that resulted from those actions. And for the second question, we need some

18
00:01:24,760 --> 00:01:28,480
intuitive theory of how people work. So we can go backwards from the actions that we've

19
00:01:28,480 --> 00:01:32,080
observed to the mental states that may have given rise to those actions. So what were

20
00:01:32,080 --> 00:01:35,360
the person's intentions, what did they believe, what were the kinds of things maybe that they

21
00:01:35,360 --> 00:01:41,200
were able to do as well. And so I studied psychology, like in my undergrad, and I was

22
00:01:41,200 --> 00:01:44,680
most excited about social psychology, because I felt sort of most applicable, I guess, to

23
00:01:44,680 --> 00:01:49,240
my everyday life, and somehow also got into responsibility, like back then already. Maybe

24
00:01:49,240 --> 00:01:52,920
it was because I was in some group project where I felt I was doing all the heavy lifting

25
00:01:53,000 --> 00:01:56,720
and maybe I wasn't getting all the credit for it. So that was sort of what interested

26
00:01:56,720 --> 00:02:02,160
me initially. And when I read around in that work in social psychology, a lot of the theories

27
00:02:02,160 --> 00:02:07,160
that I saw took a form sort of like this. So I'll just give you a few examples. So basically

28
00:02:07,160 --> 00:02:11,680
sort of like boxes and arrows theories, where they identified important concepts that were

29
00:02:11,680 --> 00:02:16,040
related to how we assign responsibility, and maybe also roughly how they were related to

30
00:02:16,040 --> 00:02:22,600
one another, but still left a lot in a certain way unspecified. So this is a quote from

31
00:02:22,960 --> 00:02:26,680
Bertrand Molle from a while ago. He says, like, an important limitation of many of these

32
00:02:26,680 --> 00:02:31,200
models of moral judgment or assigning responsibility is they don't really generate any quantitative

33
00:02:31,200 --> 00:02:35,360
predictions. And you might say, like, oh, what do you need quantitative predictions for? Well,

34
00:02:35,360 --> 00:02:39,480
one thing that they're useful for is sort of, you know, laying your cards out and making

35
00:02:39,480 --> 00:02:44,280
it concrete, what your model does also allows it then to be falsified more easily. And I

36
00:02:44,280 --> 00:02:48,440
remember this one instance, it was like me, I think maybe first day of my PhD, I went to

37
00:02:48,440 --> 00:02:52,200
this conference and had dinner, you know, with one of the, one of the people who had made

38
00:02:52,200 --> 00:02:56,720
one of these sort of boxes and arrows and diagrams. And I told them about some experiment

39
00:02:56,720 --> 00:03:01,320
that I thought, that I thought of and thought, like, oh, this would happen. And I think that

40
00:03:01,320 --> 00:03:05,840
would be the result of that experiment. And, and I was very, very smart. I thought, like,

41
00:03:05,840 --> 00:03:09,200
oh, this would totally kind of disprove your theory, right? And he said, no, no, that would

42
00:03:09,200 --> 00:03:13,600
be totally consistent with my theory. And I thought, oh, that's weird. I maybe I really

43
00:03:13,600 --> 00:03:18,240
tried to understand the theory very well. And, and so, so that also was a sort of little

44
00:03:18,240 --> 00:03:22,080
bit of a moment for me that I felt like, okay, maybe it's important to try to make these

45
00:03:22,120 --> 00:03:25,400
theories even more precise. So we know what it is that they're predicting, so we can go

46
00:03:25,400 --> 00:03:29,640
about and, you know, falsify them and sort of improve them. And so that's been very much

47
00:03:29,640 --> 00:03:34,560
kind of an inspiration for me, what I've been trying to do it a little bit. And so one of

48
00:03:34,560 --> 00:03:39,840
the starting points in almost all of these theories of responsibility is there's causality,

49
00:03:39,840 --> 00:03:43,600
always causality comes first. So I thought, okay, let me try, let me try that one. So

50
00:03:43,600 --> 00:03:49,200
can we get more specific about what it means, you know, what people, what it takes for people

51
00:03:49,240 --> 00:03:55,320
to say that one thing caused another thing to happen. And so I think that three key ingredients

52
00:03:55,320 --> 00:03:59,480
that we need, like in order to get a theory for how people think that one thing caused

53
00:03:59,480 --> 00:04:04,720
another thing to happen. And those are starting with a mental model that people have of a

54
00:04:04,720 --> 00:04:09,800
particular domain, a mental model that allows us to conceive of counterfactual interventions.

55
00:04:09,840 --> 00:04:13,480
So, and I'll flush it out a little bit more in a moment. So imagining how things could

56
00:04:13,480 --> 00:04:17,560
have been different from how they actually were. And that allows us then to mentally

57
00:04:17,560 --> 00:04:23,560
simulate what the consequences of this counterfactual intervention would have been. And so the idea

58
00:04:23,560 --> 00:04:27,520
of mental models has been around, you know, for quite some time and has recently gotten

59
00:04:27,520 --> 00:04:32,560
a little bit more attention again, also in AI. And, but, but yeah, some of the credit

60
00:04:32,560 --> 00:04:37,040
at least in modern times that go to the philosopher Kenneth Craig and his book, The Nature of

61
00:04:37,040 --> 00:04:40,600
Explanation, who said something along the lines, well, he said exactly that, but I'm

62
00:04:40,600 --> 00:04:44,760
going to say along the lines, so that we have something like a small scale model. Oh, wouldn't

63
00:04:44,760 --> 00:04:49,520
it be very helpful if we had something like a small scale model of the world in our minds

64
00:04:49,520 --> 00:04:52,920
that we can then use for all sorts of things, like predicting what was going to happen if

65
00:04:52,920 --> 00:04:57,160
I did this, rather than actually having to carry out the action and then, you know, dying

66
00:04:57,160 --> 00:05:02,400
maybe if it was a bad one. And, and yeah, that that would be really helpful for decision

67
00:05:02,400 --> 00:05:06,200
making. And as I will say in a moment also really helpful for explaining kind of why

68
00:05:06,200 --> 00:05:11,400
something happened. So this idea of mental models has been around for a very long time.

69
00:05:11,400 --> 00:05:15,160
And then in somewhat more recent years, at least in cognitive science, has been made

70
00:05:15,160 --> 00:05:18,640
a little bit more concrete, particularly as it pertains to our mental model of the physical

71
00:05:18,640 --> 00:05:23,400
world. And so the idea is was here to say like, well, maybe our mental model of the

72
00:05:23,400 --> 00:05:28,360
physical world is in certain respects, similar to the kinds of physics engines that we use

73
00:05:28,360 --> 00:05:32,640
to make realistic computer games. That's a common move, right? You have some, some tool

74
00:05:32,640 --> 00:05:36,040
and then you think like, okay, maybe the mind is a little bit like that tool. So this was

75
00:05:36,040 --> 00:05:39,760
just, you know, psychologists playing Angry Birds and then thinking like, okay, maybe

76
00:05:39,800 --> 00:05:46,560
the mind is a little bit like Angry Birds. So here, the basic idea, right, is that we

77
00:05:46,560 --> 00:05:50,360
take in the world, you know, through our perceptual senses, and that we then build this internal

78
00:05:50,360 --> 00:05:54,320
representation of the world. That's now the physics engine kind of representation. So

79
00:05:54,320 --> 00:05:58,280
that we pass the world, for example, into objects and the properties of those objects

80
00:05:58,280 --> 00:06:02,880
and then the interactions between those objects. So here, this child maybe passes the world

81
00:06:02,880 --> 00:06:06,720
into the ball and then the eagle on top of the tower and then the tower or the blocks

82
00:06:06,760 --> 00:06:11,840
that make up the tower. And now that you have this internal representation of the world,

83
00:06:11,840 --> 00:06:15,640
you can use it, for example, for planning. So if this child, for example, wants to topple

84
00:06:15,640 --> 00:06:20,040
over that tower, they can think about what's going to happen if they roll the ball like

85
00:06:20,040 --> 00:06:26,000
in different kinds of ways. So I can run simulations using this internal engine in my mind. So

86
00:06:26,000 --> 00:06:29,240
having this would be very useful because I could make predictions about the future. I

87
00:06:29,240 --> 00:06:32,960
could pay sort of Sherlock and infer from the current state of the world what must have

88
00:06:32,960 --> 00:06:38,120
happened in the past. And as I'll show in a moment, this would also be useful for explaining

89
00:06:38,120 --> 00:06:43,960
something that happened in the present. Okay, so what I'll do is in this in the remaining

90
00:06:43,960 --> 00:06:49,200
time, right? I'll basically want to cover these two different aspects of working towards

91
00:06:49,200 --> 00:06:53,320
this computational framework. So in part one, I'm going to focus on the physical domain.

92
00:06:53,320 --> 00:06:58,120
And then in part two, I'm going to expand it to just start to think about people. I should

93
00:06:58,120 --> 00:07:02,360
say, obviously, feel free to ask questions like anytime throughout. Otherwise, I'll try

94
00:07:02,560 --> 00:07:08,760
and end around 12.20 so that we have a little bit of time also for Q&A at the end. Feel

95
00:07:08,760 --> 00:07:13,080
free to ask and throughout if anything's unclear. Okay, so let's start with this part

96
00:07:13,080 --> 00:07:17,160
one. And I should also warn you, there is a little bit of audience participation required.

97
00:07:17,160 --> 00:07:21,960
So get ready for that. So the first, we started really simple, right? I was saying, okay,

98
00:07:21,960 --> 00:07:25,440
I want to understand causality a little bit better. What's the simplest possible setting

99
00:07:25,440 --> 00:07:29,560
maybe in which you could think about causality? Well, it's two billion balls colliding with

100
00:07:29,560 --> 00:07:34,160
one another. And here's the first audience participation part. So there's going to be

101
00:07:34,160 --> 00:07:37,720
these two balls coming in on the right side of the screen. And I'm going to ask you whether

102
00:07:37,720 --> 00:07:41,640
you think that ball A caused ball B to go through the gate. And if you think so, maybe

103
00:07:41,640 --> 00:07:46,640
just raise your arm like at the end of the video clip. So here's what's happening. Okay,

104
00:07:46,640 --> 00:07:53,640
so who thinks that A caused B to go through the gate in this case? Okay, a lot of people

105
00:07:54,320 --> 00:08:00,320
do anyone think that it didn't? No one dares? Okay, cool. So you're in line with what most

106
00:08:00,320 --> 00:08:06,320
people say in this case. And here's what I think was going on in your minds. Not the

107
00:08:13,760 --> 00:08:18,760
motor part of raising your hand, but the kind of judgment, the part of, yeah, was there

108
00:08:18,760 --> 00:08:22,480
causation happening in this case? And the first part is very kind of uncontroversial.

109
00:08:22,480 --> 00:08:25,720
So you looked at what actually happened. You saw that they collided with one another

110
00:08:25,720 --> 00:08:29,440
and then ball B ended up going through the gate. And now the somewhat more controversial

111
00:08:29,440 --> 00:08:34,320
part is to say that, well, that's in itself is not sufficient. That does not contain all

112
00:08:34,320 --> 00:08:37,560
the information you need in order to say that A caused ball B to go through the gate in

113
00:08:37,560 --> 00:08:42,360
this case. But you also need something like this. You need the capacity to simulate in

114
00:08:42,360 --> 00:08:47,640
your mind, in this case, that removing basically ball A from the scene, kind of in your mind.

115
00:08:47,640 --> 00:08:50,880
And then simulating where ball B would have gone if ball A hadn't been present in the

116
00:08:50,920 --> 00:08:55,440
scene. Maybe you all sort of naturally and spontaneously did that. And of course, I already

117
00:08:55,440 --> 00:08:59,520
talked a little bit about counterfactuals and stuff like that in the experiment. Of course,

118
00:08:59,520 --> 00:09:04,720
I don't do that. I just ask people to make causal judgments. So the simple idea here

119
00:09:04,720 --> 00:09:08,920
is then to say, when do you say that A caused me to go through the gate? It's really sort

120
00:09:08,920 --> 00:09:12,920
of like an epistemic notion. So your subjective degree of belief, well, to the extent that

121
00:09:12,920 --> 00:09:18,120
you think that what would have happened in the actual, so that what would have happened

122
00:09:18,160 --> 00:09:21,280
in the counterfactual situation would have been different from the thing that actually

123
00:09:21,280 --> 00:09:26,080
happened, that determines your extent to which you say that A caused B to go through the

124
00:09:26,080 --> 00:09:30,440
gate. And here you're probably pretty sure in this instance that B would have missed

125
00:09:30,440 --> 00:09:35,520
if A hadn't been there. So you say, yes, A caused it to go through. And just a little

126
00:09:35,520 --> 00:09:39,960
bit in terms of sort of background, a lot of the inspiration for this kind of work comes

127
00:09:39,960 --> 00:09:46,840
from Judea Pearl's work on causality. Some of you may have heard of his work. And there

128
00:09:46,880 --> 00:09:50,680
they use different kinds of generative models to capture people's causal knowledge of the

129
00:09:50,680 --> 00:09:55,040
world. So this could be something like causal base nets or structural equations that you

130
00:09:55,040 --> 00:09:59,960
may also remember from your stats class if you had one. And then you define some kind

131
00:09:59,960 --> 00:10:05,400
of operations on these models to support things like counterfactual reasoning. So imagining

132
00:10:05,400 --> 00:10:10,280
that some variable had been replaced with another one, for example. And so I'm doing

133
00:10:10,280 --> 00:10:14,120
something quite similar here, only in that I'm assuming that the generative model that

134
00:10:14,120 --> 00:10:18,160
people have of the world in this case is somewhat richer than what can be represented

135
00:10:18,160 --> 00:10:22,320
with these causal basis or structural equations. So in my case, the generative model that I

136
00:10:22,320 --> 00:10:25,960
assume people have in their mind is something like the physics engine that I actually use

137
00:10:25,960 --> 00:10:30,440
to generate them in the stimuli. And I'll make that noisy. I'll show you in a second

138
00:10:30,440 --> 00:10:35,520
how I'm making it noisy. And then I also have to think about, okay, what are now the counterfactual

139
00:10:35,520 --> 00:10:39,760
intervention operators that you might have over a representation like this one? And in

140
00:10:39,760 --> 00:10:43,680
this case, it could be something like imagining that an object wouldn't have been, would not

141
00:10:43,680 --> 00:10:48,520
have been there, for example. Okay, so you might think now, okay, well, maybe that's

142
00:10:48,520 --> 00:10:52,720
the only game in town. Like what else could you possibly be doing in a setting like this?

143
00:10:52,720 --> 00:10:56,240
And at least luckily for me, there has been a lot of philosophers and psychologists that

144
00:10:56,240 --> 00:11:00,520
have argued for what I called these actualist theories of causation. And they basically

145
00:11:00,520 --> 00:11:04,520
just say you don't need that part, right? All you need, all the information you need

146
00:11:04,520 --> 00:11:09,840
to give causal judgments or causal explanations for what happened is there in the actual situation

147
00:11:09,880 --> 00:11:14,800
in some sense. And so one of the best kind of worked out accounts of that in psychology

148
00:11:14,800 --> 00:11:19,400
comes from a psychologist called Philip Wolff and he calls it the force dynamics model

149
00:11:19,400 --> 00:11:24,080
of causation. And the idea is that all you need to pay attention to is the forces that

150
00:11:24,080 --> 00:11:29,600
are associated with the agent and the patient. That's the sort of lingo they use. And you

151
00:11:29,600 --> 00:11:33,080
then you just look, need to look at how these forces are configured. And that helps you

152
00:11:33,080 --> 00:11:37,680
to say what in this case here, there's different causal expressions is appropriate to use in

153
00:11:37,680 --> 00:11:42,480
a particular situation. And I'll just apply it to this example here. So we have the patient

154
00:11:42,480 --> 00:11:46,520
which is ball B that has a force that is associated with it. Then we have an agent

155
00:11:46,520 --> 00:11:50,520
that applies a force to the patient in this case. As a function of these two forces, we

156
00:11:50,520 --> 00:11:54,360
have some resulting force here. And then in this case, the patient also ended up reaching

157
00:11:54,360 --> 00:12:00,000
the end state. And because this configuration looks like that and that maps onto this force

158
00:12:00,000 --> 00:12:04,440
configuration, Philip Wolff's account would also here say, yes, a cause B to go through

159
00:12:04,440 --> 00:12:08,240
the gate. So this clip would not help us actually tease apart this other model that

160
00:12:08,240 --> 00:12:14,160
I've been kind of promoting. So just to make this distinction sort of clear or clearer.

161
00:12:14,160 --> 00:12:17,480
So in the force dynamics model, you start with some intuitive theory of how the world

162
00:12:17,480 --> 00:12:22,080
works, which in this case are these little force vectors that apply to agents and patients.

163
00:12:22,080 --> 00:12:25,960
And you can then directly go from there to making causal judgments. So there's this direct

164
00:12:25,960 --> 00:12:31,120
route from this kind of intuitive theory to causal judgment. He also says that you can

165
00:12:31,120 --> 00:12:35,720
do counterfactuals too by imagining, for example, if one of the forces hadn't been there, what

166
00:12:35,720 --> 00:12:40,200
would have happened in the situation, but that it's not necessary to figure out whether

167
00:12:40,200 --> 00:12:44,920
something caused something to happen. And sort of what I'm arguing for is sort of a slightly

168
00:12:44,920 --> 00:12:48,360
different picture. Where I'm saying, well, first of all, I start with a slightly different

169
00:12:48,360 --> 00:12:52,520
theory of the domain. In this case, again, using the physics engine rather than using

170
00:12:52,520 --> 00:12:56,880
these force vectors. But then saying that you have to go through this process of counterfactual

171
00:12:56,880 --> 00:13:01,880
simulation to say that something caused something to happen. And what I'm going to try and do

172
00:13:01,880 --> 00:13:07,080
in the next two slides is sort of motivate that account.

173
00:13:07,080 --> 00:13:10,760
One way to motivate at first is that I started off saying like I want to have this model

174
00:13:10,760 --> 00:13:15,160
of responsibility. And that means that I want to have a model of causation that not only

175
00:13:15,160 --> 00:13:19,320
narrowly applies to the physical world, but that can also be applied to, for example, the

176
00:13:19,320 --> 00:13:23,760
kind of causation that happens between people. And here's just some examples of causal statements

177
00:13:23,760 --> 00:13:27,960
that you could hear at the fall of Lehman Brothers, caused the financial crisis. My

178
00:13:27,960 --> 00:13:32,440
housemates failed to water my plants, caused them to die. Realizing that he forgot his

179
00:13:32,440 --> 00:13:36,280
wallet at home, caused him to go back. You probably wouldn't say that exactly in English,

180
00:13:36,280 --> 00:13:40,560
but they all seem to find sort of causal things, like to say. And it's probably a little bit

181
00:13:40,560 --> 00:13:44,960
tricky, or at least I would find it tricky, to think of how would I explain these sorts

182
00:13:44,960 --> 00:13:48,960
of causations with force vectors. And the hope is that the account that I'm developing

183
00:13:48,960 --> 00:13:53,880
is a little bit more flexible so that it can apply to these sorts of situations as well.

184
00:13:53,880 --> 00:13:59,680
But now, and another kind of key advantage, I think, of the model that we've been developing

185
00:13:59,680 --> 00:14:04,640
is that it actually allows us to derive quantitative predictions. And it's hence more easily falsifiable

186
00:14:04,640 --> 00:14:09,120
that some of the prior work. And so you can falsify it if you like, write a paper until

187
00:14:09,120 --> 00:14:15,560
we was wrong. And then I have to go back to the office and improve the account. So here's

188
00:14:15,600 --> 00:14:19,360
a way in which we're getting quantitative predictions out of this model. But I was saying

189
00:14:19,360 --> 00:14:24,400
that how you make causal judgments is by comparing what actually happened with what

190
00:14:24,400 --> 00:14:29,440
would have happened in the relevant counterfactual situation. But now you don't know that. The

191
00:14:29,440 --> 00:14:33,240
thing that I'm showing here on the right-hand side, I guess, that's in some sense the ground

192
00:14:33,240 --> 00:14:37,480
to truth. But you don't get to see that. You only see what actually happens. You don't

193
00:14:37,480 --> 00:14:41,600
get to see what would have happened if ball A hadn't been there. So you have to use your

194
00:14:41,600 --> 00:14:44,880
intuitive understanding, again, of this domain to simulate what would have happened in this

195
00:14:44,880 --> 00:14:50,160
counterfactual. And so one way for us to capture this uncertainty that you may have

196
00:14:50,160 --> 00:14:55,480
about exactly what would have happened if ball A hadn't been there is by generating simulations

197
00:14:55,480 --> 00:14:59,640
from our physics engine, but now injecting a little bit of noise into that engine. So

198
00:14:59,640 --> 00:15:04,120
now it becomes sort of like a probabilistic program because it's now not a deterministic

199
00:15:04,120 --> 00:15:09,240
outcome anymore if ball A hadn't been there. But rather what I'm doing is I'm generating

200
00:15:09,240 --> 00:15:14,800
a simulated sample from my model. And now in this case there's many different ways in

201
00:15:14,800 --> 00:15:18,960
which you could make your model kind of random or uncertain. Here what we did is we just

202
00:15:18,960 --> 00:15:24,600
took the actual ground truth, that ball B, velocity that ball B would have had, and applied

203
00:15:24,600 --> 00:15:28,880
a small perturbation to the velocity vector at each point in time. So now it's sort of

204
00:15:28,880 --> 00:15:33,120
like in your simulation, when you're imagining where ball B would have gone, it sort of jiggles

205
00:15:33,120 --> 00:15:38,640
a little bit along the way. And so this might be now one outcome, like off such a sample.

206
00:15:38,640 --> 00:15:41,520
So if you think like, oh, oh, I think it would have missed. But let me try again. Like,

207
00:15:41,520 --> 00:15:44,440
oh, yeah, I think it would have missed. Yeah, I'm pretty sure it would have missed. So this

208
00:15:44,440 --> 00:15:47,400
is just multiple times sampling in your mind of what would have happened if ball A hadn't

209
00:15:47,400 --> 00:15:50,840
been there. And here, since all of them, you're pretty sure that it would have missed, you

210
00:15:50,840 --> 00:15:55,160
said, yeah, A caused it to go through. But you can probably already anticipate, we can

211
00:15:55,160 --> 00:16:00,320
now do a slightly different case, right, where in the actual situation, again, still A collides

212
00:16:00,320 --> 00:16:03,800
with B and B goes in. But this time it's sort of less clear what would have happened

213
00:16:03,800 --> 00:16:07,600
if ball A hadn't been present in the scene. Because that ball B is headed like right to

214
00:16:07,640 --> 00:16:12,120
the goal post, essentially. And now if you apply the same idea of simulating with noise

215
00:16:12,120 --> 00:16:16,040
what would have happened, you know, in some cases, maybe ball B would have missed. But

216
00:16:16,040 --> 00:16:20,000
it's also possible that ball B would have gone in anyhow, even if A hadn't been there.

217
00:16:20,000 --> 00:16:23,560
And that accordingly, you might say like, yeah, I'm less sure, you know, that A caused

218
00:16:23,560 --> 00:16:28,400
ball B to go through the gate in this case. So that's what we did now in our experiment

219
00:16:28,400 --> 00:16:32,720
where we showed people a bunch of clips like this one. So here's just three different ones,

220
00:16:32,960 --> 00:16:39,120
one clip in which, you know, it's pretty clear here at the top that ball B would have missed

221
00:16:39,120 --> 00:16:43,120
if ball A hadn't been there. The one in the middle is like one, this kind of close call.

222
00:16:43,120 --> 00:16:46,400
And then the one on the right hand side is one in which it was pretty clear that ball

223
00:16:46,400 --> 00:16:50,880
B would have gone in anyhow, even if A hadn't been there. And then between experiments,

224
00:16:50,880 --> 00:16:54,160
we either asked them some counterfactual question. So that's the one here at the bottom,

225
00:16:54,160 --> 00:16:57,760
the blue one. Do you think that ball B would have missed if ball A hadn't been there?

226
00:16:58,400 --> 00:17:02,160
And then we see that in this case, they're pretty sure, yeah, I think it would have missed.

227
00:17:02,160 --> 00:17:06,160
Here, they're right at the midpoint of the scale, not sure, right, whether it would have

228
00:17:06,160 --> 00:17:11,600
missed or not. So we give them some slider where they can just evaluate their degree of belief.

229
00:17:11,600 --> 00:17:17,360
And then in this case, they're pretty sure that it would not have missed, even if ball A hadn't

230
00:17:17,360 --> 00:17:22,480
been there. And then we take a separate group of participants and we ask them a causal question.

231
00:17:22,480 --> 00:17:26,240
So those participants don't hear anything about counterfactuals. We just asked them in a clip

232
00:17:26,240 --> 00:17:30,720
like that, what do you think that ball A caused ball B to go through the gate? And we see that

233
00:17:31,120 --> 00:17:36,480
judgments align very closely with those of the ones in the counterfactual question condition.

234
00:17:37,600 --> 00:17:41,520
And we can also use that model that I described that draws these samples and tries to simulate

235
00:17:41,520 --> 00:17:46,400
what would have happened. And it also yields very similar judgments in this, or makes predictions

236
00:17:46,400 --> 00:17:51,360
in this case. These were just three of the video clips. We had like 18 different clips in that

237
00:17:51,360 --> 00:17:56,080
experiment. And if we just line up here on the x-axis, the average counterfactual judgments

238
00:17:56,160 --> 00:18:00,880
that participants made, and on the y-axis, the average causal ratings that participants gave,

239
00:18:00,880 --> 00:18:04,800
you see that they're very closely aligned with one another, at least suggesting a strong

240
00:18:04,800 --> 00:18:10,720
relationship between these kinds of judgments. But when we published this work as a coxide paper,

241
00:18:10,720 --> 00:18:15,280
so for the cognitive science proceedings, one of the reviewers, they were mostly happy with it,

242
00:18:15,280 --> 00:18:18,880
but one of the reviewers was saying, yeah, but all of the clips that you showed participants,

243
00:18:18,880 --> 00:18:22,880
something slightly different was going on. So maybe you just didn't try hard enough to come

244
00:18:22,880 --> 00:18:26,800
up with an actualistic count, like one that only looks at what actually happened. And if you

245
00:18:26,800 --> 00:18:34,240
tried a little bit harder, then you could have explained it away. So we did try, and we didn't

246
00:18:34,240 --> 00:18:38,160
succeed, but it's also sort of a weird position that you're in when you kind of don't want to

247
00:18:38,160 --> 00:18:43,840
succeed, right? So we thought, okay, maybe the better thing, rather than being crappy at modeling,

248
00:18:44,480 --> 00:18:48,080
you know, just let's come up with an experiment where it feels like if it comes up in the way that

249
00:18:48,080 --> 00:18:53,040
we think it will, there's no way you could possibly explain it with an actualistic count.

250
00:18:53,040 --> 00:18:56,960
And so that's the route we took. So just to really think like, oh, are these counterfactors

251
00:18:56,960 --> 00:19:01,600
really necessary for understanding causal judgments? So second round of audience participation,

252
00:19:01,600 --> 00:19:05,360
get ready. I'm just going to show you a slightly different clip, and this time I'm going to ask

253
00:19:05,360 --> 00:19:08,240
you whether you think that ball A prevented Bobby from going through the gate.

254
00:19:08,240 --> 00:19:20,880
Okay, what do you think? If you think that ball A prevented Bobby from going through

255
00:19:20,880 --> 00:19:28,960
the gate, you can raise your hand. Okay, a few people think so in this case. Okay, I'll show you

256
00:19:28,960 --> 00:19:38,640
another one. Okay, this was not some kind of, you know, glitch. I was having fun, you know,

257
00:19:38,640 --> 00:19:43,600
doing the physics engine and sort of playing portal, right, by turning these things into a

258
00:19:43,600 --> 00:19:46,640
Taylor port, right? And I didn't tell you anything about them, of course, when I showed you the

259
00:19:46,640 --> 00:19:50,800
first clip, but maybe just seeing that one clip, you already have like one shot learning, yeah,

260
00:19:50,800 --> 00:19:55,200
okay, maybe that's a Taylor port. And the Taylor port, it works only for ball B, you know, it

261
00:19:55,280 --> 00:19:59,280
doesn't work for ball A, and the yellow thing is the entry of the Taylor port, and the blue

262
00:19:59,280 --> 00:20:04,640
thing is the exit of the Taylor port. And now that I've shown you that, if I now show you exactly

263
00:20:04,640 --> 00:20:09,680
the same clip again, you're going to say, at least if you're like my participants, yes, it prevented

264
00:20:09,680 --> 00:20:13,760
it from going through, right? Because now what changed is basically your belief about how the

265
00:20:13,760 --> 00:20:17,760
world works, such that your counterfactual looks a little bit more like that now, right? What would

266
00:20:17,760 --> 00:20:21,680
have happened is that it would have gone through the Taylor port and into the goal, right? So the

267
00:20:21,680 --> 00:20:26,880
fact that I can show you exactly the same clip twice, right? And, and all I've changed was your

268
00:20:26,880 --> 00:20:31,040
belief about how the world works. And that makes a big difference to your causal judgment, sort of

269
00:20:31,040 --> 00:20:36,480
shows that it's, it cannot be sufficient to explain causal judgments just in terms of what

270
00:20:36,480 --> 00:20:40,480
actually happened, because actually what actually happened was exactly the same in both of the

271
00:20:40,480 --> 00:20:45,040
times that I showed you the clip. I don't need to do the Taylor port thing. The Taylor port thing

272
00:20:45,040 --> 00:20:49,040
is cute because I can show you exactly the same clip, but I can also move some obstacle in and

273
00:20:49,040 --> 00:20:53,600
out of the way, right here on the left hand side, you're not going to say that A prevented B from

274
00:20:53,600 --> 00:20:58,000
going through the gate. On the right hand side, you are because the block is out of the way, right?

275
00:20:58,000 --> 00:21:02,240
And a similar way for causation. And on the left hand side, you're going to say, yeah, A caused it

276
00:21:02,240 --> 00:21:06,240
because the block would have blocked it. And on the right hand side, you're not really going to say

277
00:21:06,240 --> 00:21:10,800
that it caused it because it would have gone in anyhow, right? Same idea. I'm doing exactly the

278
00:21:10,800 --> 00:21:14,320
same interactions between the balls. I'm just changing something kind of in the background

279
00:21:14,320 --> 00:21:18,400
that affects the counterfactual and, and thereby also affects people's causal judgments.

280
00:21:20,000 --> 00:21:26,080
Okay. So another thing that's sort of neat about this model is that it doesn't only kind of predict

281
00:21:26,080 --> 00:21:30,880
basically the judgment that people should give at the end of it, but also says something about

282
00:21:30,880 --> 00:21:35,680
the cognitive process by which they arrive at the judgment, right? In this case is maybe this

283
00:21:35,680 --> 00:21:39,680
process of mental simulation, that you kind of generating these samples and thinking about what

284
00:21:39,680 --> 00:21:45,520
would have happened, and that those drive the causal judgment. And one way we can do that,

285
00:21:45,600 --> 00:21:49,360
or can sort of get more direct evidence on that is to use eye tracking, right? To see, okay,

286
00:21:49,360 --> 00:21:52,880
where is it that you're looking at when you're asked to make causal judgments in these kinds

287
00:21:52,880 --> 00:21:58,880
of video clips. So we went back to the really simple ones again. And now also between experiments,

288
00:21:58,880 --> 00:22:03,440
just ask participants a different question about the video that, that, that they would see. And

289
00:22:03,440 --> 00:22:07,120
they knew at the beginning what question they would be asked. So we had one condition here that

290
00:22:07,120 --> 00:22:10,880
we call the outcome condition where they'd watch the video and we would just ask them at the end,

291
00:22:10,960 --> 00:22:15,760
in this case, if it ended up missing, did be completely miss the gate. And so I'll show you

292
00:22:15,760 --> 00:22:19,200
the eye movements of one of the participants in this condition. And I'm going to play the

293
00:22:19,200 --> 00:22:24,960
video at half speed and I'll do some sort of life narration as it unfolds. So the blue dot is the

294
00:22:24,960 --> 00:22:28,800
eye movement, right? So the participant here is looking back and forth between ball A and ball B.

295
00:22:29,760 --> 00:22:34,880
So looking, looking at ball B, sort of now trying to extrapolate where ball B will end up hitting

296
00:22:34,880 --> 00:22:43,600
the wall. And then mostly looking at ball B. Not very exciting, but also that's all they need to

297
00:22:43,600 --> 00:22:48,560
know in order to answer this question in this case. So now if you take a different participant

298
00:22:49,120 --> 00:22:54,320
who was asked to make a causal question, or asked to answer a causal question in the video,

299
00:22:54,320 --> 00:22:58,480
but otherwise saw exactly the same video clips as other participants did, you're going to see that

300
00:22:58,480 --> 00:23:01,920
the eye movements look quite different. And they look different in a way that made me very happy

301
00:23:01,920 --> 00:23:07,760
at the time. So you see they're not just looking at ball B, they're trying to anticipate where

302
00:23:07,760 --> 00:23:12,800
ball B would have gone, you know, if ball A wasn't present in the scene. And it's quite likely that

303
00:23:12,800 --> 00:23:17,680
when you guys, when I showed you this first video clip that you did that, right? And may not even

304
00:23:17,680 --> 00:23:22,400
been super, you know, aware to you that you did do that, like I haven't really checked, you know,

305
00:23:23,120 --> 00:23:26,800
yeah, how, well, at some point at the beginning when I ran this on the laptop, I would sometimes

306
00:23:26,800 --> 00:23:29,680
see that people would use their finger, or they would use their, you know, kind of

307
00:23:30,320 --> 00:23:35,520
pen or something. And that's of course pretty aware, I guess, right? But it's possible that with

308
00:23:35,520 --> 00:23:39,520
the eye movements, this sort of comes so natural to us that we don't even realize that we're engaging

309
00:23:39,520 --> 00:23:45,200
kind of in this kind of process. But yeah, I was very happy, you know, when I saw this happening.

310
00:23:46,320 --> 00:23:50,880
And so this is anecdotal in a sense, it's just one video clip, right? But we can also look at

311
00:23:50,880 --> 00:23:55,200
more generally, sort of analyzing the differences in the eye movements that people are producing

312
00:23:55,280 --> 00:23:58,800
between these different experiments. And what I'm showing here is just looking at

313
00:23:58,800 --> 00:24:02,560
the saccades that participants are producing. So those are fast eye movements jumping from

314
00:24:02,560 --> 00:24:08,240
one point to another. And then I look at the endpoints of those saccades. And I look at where

315
00:24:08,240 --> 00:24:13,600
those fall, right? And I took into account only the time between ball A and ball B coming into the

316
00:24:13,600 --> 00:24:21,280
scene. And before basically, when they collide with one another, that time window. And then we

317
00:24:21,280 --> 00:24:27,360
see that on this, for the causal question, a lot of those saccades basically end up along the path

318
00:24:27,360 --> 00:24:31,360
right that ball B would have taken if ball A hadn't been there. Whereas in the other condition,

319
00:24:31,360 --> 00:24:40,160
we see very few of these kinds of eye movements. So nice, I guess, even more direct evidence

320
00:24:40,160 --> 00:24:43,840
that people are engaging in this kind of process and that they're doing it specifically

321
00:24:43,840 --> 00:24:48,160
when asked to answer a causal question about the clip and sort of spontaneously.

322
00:24:48,400 --> 00:24:56,400
There is this other part to it. But I think I will skip, so I have a little bit more time to,

323
00:24:57,520 --> 00:25:02,400
let me see. Well, actually, I'll share it. Sorry about that. So there was another,

324
00:25:02,400 --> 00:25:07,280
after we published this paper, there was another reviewer number two, as there often is.

325
00:25:09,280 --> 00:25:14,480
And they were basically still saying, okay, well, this was for the eye tracking data. And they said,

326
00:25:14,880 --> 00:25:19,920
that's nice. You're showing us these sort of eye movements. But they basically said that, okay,

327
00:25:19,920 --> 00:25:24,320
these eye movements, they're happening before the balls are colliding with one another. And you're

328
00:25:24,320 --> 00:25:28,240
calling it sort of counterfactual simulation. Counterfactual should mean it should be back

329
00:25:28,240 --> 00:25:32,240
in the past. Going back in the past, evaluating that something would have been different,

330
00:25:32,240 --> 00:25:38,000
and then seeing what difference that would have made. And they were saying, oh, what,

331
00:25:38,000 --> 00:25:43,280
you should just call it the hypothetical simulation model instead, and not that. So we were able to

332
00:25:43,440 --> 00:25:51,600
push back. But the reviewer also was right to some extent, I think. So this is a paper that I've

333
00:25:51,600 --> 00:25:56,720
published quite recently, where I was trying to say that, no, you really need the counterfactuals.

334
00:25:56,720 --> 00:26:00,160
So a lot of this has been like, yeah, you really need the counterfactuals. And then you just keep

335
00:26:00,160 --> 00:26:04,720
getting some pushback, and you try to convince people even more so. So this was this reviewer

336
00:26:04,720 --> 00:26:08,800
number two here. You haven't really shown us counterfactual simulation. Those looks are happening

337
00:26:08,800 --> 00:26:12,800
before the balls are colliding. So his idea was, well, maybe what people are doing is they're kind

338
00:26:12,800 --> 00:26:17,200
of simulating some hypothetical future. In this case, the hypothetical future is like,

339
00:26:17,200 --> 00:26:21,760
what would happen if ball A wasn't there? And then they're storing that in their mind,

340
00:26:21,760 --> 00:26:25,840
and comparing that to what actually happened at the end. And that's a slightly different

341
00:26:25,840 --> 00:26:31,120
computation from the one that I think they're carrying out. And this relates to something,

342
00:26:31,120 --> 00:26:38,800
again, here's Judea Pearl, this climbing on this kind of virtual letter here. Because he has argued

343
00:26:38,800 --> 00:26:43,840
that there are these three different ways of thinking about the extent to which people have

344
00:26:43,840 --> 00:26:49,440
causal knowledge of how the world works. On the lowest rung of the letter, and he often accuses

345
00:26:49,440 --> 00:26:53,040
a lot of deep learning and so on to be on that rung, although it's a little unclear,

346
00:26:54,160 --> 00:26:57,760
he calls that rung the level of association. So that's what you learn in the stats classes

347
00:26:57,760 --> 00:27:03,040
correlation. When two things are associated with one another, and you can infer one variable from

348
00:27:03,120 --> 00:27:09,280
the presence of another, so the normal conditional probability, PY given, I would say PY given X.

349
00:27:09,280 --> 00:27:14,160
So what does some symptom tell me about the disease, for example? On the next level,

350
00:27:14,160 --> 00:27:17,840
it's the level of interventional reasoning. That's the kind of when I do a randomized control

351
00:27:17,840 --> 00:27:22,720
trial, for example, or if I'm, again, hypothetically reasoning, oh, what would happen if I were to do

352
00:27:22,720 --> 00:27:28,560
this? What would happen if I were to do that? And that's sort of when your stats teacher tells you

353
00:27:29,120 --> 00:27:33,040
causation and correlation aren't the same thing, that's often the thing that they then think about,

354
00:27:33,040 --> 00:27:36,080
right? That like, oh, on the level of an experiment, now I'm performing an intervention,

355
00:27:36,080 --> 00:27:40,720
randomly assigning people to different groups, and I can draw different kinds of causal inferences

356
00:27:40,720 --> 00:27:46,960
from that information than when I just have observations. But then process ultimately,

357
00:27:46,960 --> 00:27:52,800
the kind of the highest rung on the letter is reserved for counterfactual reasoning,

358
00:27:52,800 --> 00:27:57,360
and that allows you to give specific answers essentially to why questions. So why did this

359
00:27:57,360 --> 00:28:02,000
happen in this particular case? Like, you know, was it the aspirin that stopped my headache,

360
00:28:02,000 --> 00:28:06,240
or would it have stopped anyhow, even if it hadn't taken the aspirin? Or, you know,

361
00:28:06,960 --> 00:28:10,560
was Kennedy shot? Would Kennedy still have been alive if it hadn't been shot by

362
00:28:10,560 --> 00:28:16,080
very heavy-ass world? And so essentially, now the question boils down to, do we need that third

363
00:28:16,080 --> 00:28:23,200
level, like to explain people's causal judgments, or is the second one enough, right? So just to kind

364
00:28:23,200 --> 00:28:26,560
of try and make it a little bit more clear, right? So the hypothetical, luckily in English,

365
00:28:26,640 --> 00:28:30,560
also we have sort of a way of marking the difference between them. So here's an English

366
00:28:30,560 --> 00:28:35,680
hypothetical. Would B go into the goal if A was removed? So what you'd be doing is taking the

367
00:28:35,680 --> 00:28:41,920
time into account until they collide, simulating like a possible future, and then computing the

368
00:28:41,920 --> 00:28:47,840
probability of that. Versus the counterfactual, what I'm doing, slightly different in English,

369
00:28:47,840 --> 00:28:54,400
right, would B have gone into the goal if A had been removed? I sometimes, you know, regret

370
00:28:54,400 --> 00:28:58,080
having gotten into counterfactual so much, so obviously not a native speaker, and the

371
00:28:58,080 --> 00:29:01,760
counterfactuals are sometimes a little complicated, right, that you get the tenses right and so on,

372
00:29:01,760 --> 00:29:06,720
but I think I've mostly gotten it down by now after like 20 years. So would B have gone into

373
00:29:06,720 --> 00:29:10,400
the goal if ball A had been removed? So you're doing slightly different here now, right? You're

374
00:29:10,400 --> 00:29:15,440
taking into account everything until the end, and you're now going back in time to do this

375
00:29:15,440 --> 00:29:19,360
intervention and then think about how the world could have unfolded differently from how it actually

376
00:29:19,360 --> 00:29:25,520
did. So now it turns out in this very simple setting here, that makes no difference. The

377
00:29:25,520 --> 00:29:29,360
hypothetical probability and the counterfactual probability is the same because there's nothing,

378
00:29:29,360 --> 00:29:34,480
there's only this one causal event happening, so it doesn't really come apart. So in a very simple

379
00:29:34,480 --> 00:29:40,640
setting where you have one cause and one effect, essentially, you cannot tease the two apart,

380
00:29:40,640 --> 00:29:44,640
but you don't need to make it much more complicated. It's sufficient if you just have one other

381
00:29:44,640 --> 00:29:49,200
alternative event that you are initially uncertain about, and that will make it such that

382
00:29:49,280 --> 00:29:53,280
now the hypothetical probability and the counterfactual probability will be different from one another.

383
00:29:54,480 --> 00:29:59,200
So here was the genius invention, just putting like a little block again that you've seen earlier,

384
00:29:59,200 --> 00:30:04,960
but this time the block is on rails into the scene, and that will now make it such that we

385
00:30:04,960 --> 00:30:08,720
can tease these two different things apart. So here's an example. I'm not going to ask for

386
00:30:08,720 --> 00:30:12,240
audience petition this time, but let's say that this was happening in the clip,

387
00:30:13,760 --> 00:30:17,120
and now if you were asked to say, oh, did it prevent it from going into the goal?

388
00:30:17,680 --> 00:30:22,240
My participants say in this case, yes, it did. And the idea is, why is it? Well,

389
00:30:22,240 --> 00:30:27,120
because the block moved out of the way in time, such that Balbi would have gone through the goal

390
00:30:27,120 --> 00:30:31,520
if Ball A hadn't been there. But you may have also noticed that the movement of the block

391
00:30:31,520 --> 00:30:36,160
is happening after the balls collided with one another. So not something that you could have

392
00:30:36,160 --> 00:30:40,160
sort of anticipated at this earlier moment in time, or at least had some uncertainty about.

393
00:30:40,960 --> 00:30:46,000
So the basic idea here is to say like, oh, my hypothetical probability at the time

394
00:30:46,000 --> 00:30:49,920
would Ball B go into the goal if Ball A wasn't there? Well, that's unsure. That depends on

395
00:30:49,920 --> 00:30:53,920
whether or not the block's going to move. So I should give it like a 0.5 or something. I told

396
00:30:53,920 --> 00:30:58,400
participants it's just as likely to move as it's not. Whereas for the counterfactual probability,

397
00:30:59,120 --> 00:31:02,880
well, I know that it moved in this case. So I should be pretty certain that it would have

398
00:31:02,880 --> 00:31:07,280
gone in if Ball A had been removed. So now I have a way basically of teasing the two apart

399
00:31:07,280 --> 00:31:11,920
and can see which one better explains the causal judgments. Is it the hypothetical judgments

400
00:31:11,920 --> 00:31:15,680
that I ask participants to do, or is it the counterfactual judgments that I ask another group

401
00:31:15,680 --> 00:31:20,800
to do? And then I ask one group to give causal judgments and then just try to relate them to

402
00:31:20,800 --> 00:31:28,000
one another. And what I find is when I look at the hypothetical, so maybe I should say a little

403
00:31:28,000 --> 00:31:32,400
bit more about that plot here, at the bottom, it basically shows you the initial configuration

404
00:31:32,400 --> 00:31:38,320
of the block. Was it in the way or not? And then did it move yes or no? So in this example here,

405
00:31:38,320 --> 00:31:42,400
it's one where it was initially in the way, but it moved. But in the hypothetical condition,

406
00:31:42,880 --> 00:31:46,800
you don't know that because you only see it until they pause. And then if you look at the

407
00:31:46,800 --> 00:31:50,160
hypothetical judgments, they think when it's initially in the way, they think it's a little

408
00:31:50,160 --> 00:31:53,440
less likely that it's going to go in. And when it's initially out of the way, they think it's a

409
00:31:53,440 --> 00:31:57,840
little bit more likely. So they're sort of a little bit sticky in terms of what actually happened.

410
00:31:59,040 --> 00:32:03,520
For the counterfactual probabilities, pretty much only the final state is what matters.

411
00:32:03,520 --> 00:32:07,920
If it was out of the way at the end, you think, yeah, it would have gone in. If it was in the way

412
00:32:07,920 --> 00:32:12,640
at the end, you think it would have missed. And now if you ask people to make causal judgments

413
00:32:12,640 --> 00:32:16,960
in this case, we see that they align very closely with the counterfactual ones and not with the

414
00:32:16,960 --> 00:32:22,080
hypothetical ones. And this was for the kind of missed cases, but the same story again holds

415
00:32:22,080 --> 00:32:27,360
essentially for the causal cases too. So they think that it caused it when the block would have been

416
00:32:27,360 --> 00:32:31,200
in the way at the end, and they don't think that it caused it when the block was would have been

417
00:32:31,200 --> 00:32:36,000
out of the way at the end. So enough to make this review too happy, but maybe not Michael.

418
00:32:38,160 --> 00:32:43,680
I'm a happy guy. I'm curious. Can you go back one slide? Just to make sure I understand.

419
00:32:45,360 --> 00:32:50,240
There were two things that changed in that intervention. There was the question you asked,

420
00:32:50,240 --> 00:32:55,600
the hypothetical versus the counterfactual. And it also sounds like the changes in how far they

421
00:32:55,600 --> 00:33:01,680
saw into the video. That's right. That's right. And I'm picturing the counterfactual situation

422
00:33:01,680 --> 00:33:06,640
where if you ask me the hypothetical question, but showed me the full video, so I see a whole video

423
00:33:06,720 --> 00:33:12,080
and then you say, would be going into the goal if A was removed? I don't know.

424
00:33:12,960 --> 00:33:16,720
Yeah. Yeah. It's a tricky one. I mean, I guess, you know, you'd have to ask them, like, what did

425
00:33:16,720 --> 00:33:21,280
you think? I guess sort of at the time, right? Like before it happened, did you think, and people

426
00:33:21,280 --> 00:33:24,800
are often bad at that, right? We know that from all the hindsight research and so on, that they

427
00:33:24,800 --> 00:33:29,040
have difficulty putting themselves back into the epistemic state, I guess that they had at an

428
00:33:29,040 --> 00:33:33,680
earlier point in time, right? So I'm not exactly, I haven't tried that one. I haven't tried showing

429
00:33:33,920 --> 00:33:38,000
it until the end, but then asking them the hypothetical question, it's possible, of course,

430
00:33:38,000 --> 00:33:44,880
that they will confuse it like as a counterfactual question, right? And, but for me, it was still

431
00:33:44,880 --> 00:33:48,960
sufficient, I guess, at least to address this reviewer's concern, because his idea was really,

432
00:33:48,960 --> 00:33:53,200
yeah, that computation is happening earlier, right? It's happening before the causal event of

433
00:33:53,200 --> 00:33:57,040
interest, and then you're storing the output of that computation, in this case, the hypothetical

434
00:33:57,040 --> 00:34:02,000
probability, and then just comparing that to what actually happens at the end, right? So it still

435
00:34:02,000 --> 00:34:11,920
felt that it's addressing that, but yeah. Okay, so having these two things helps teasing them apart.

436
00:34:12,720 --> 00:34:16,160
Okay, I'll sum up the first part, and then the second part will be short, but that's okay.

437
00:34:17,200 --> 00:34:20,800
So for this counterfactual simulation model, what I've showed you that there's this sort of nice

438
00:34:20,800 --> 00:34:25,920
correspondence between people's beliefs about the relevant counterfactual and the causal judgments

439
00:34:25,920 --> 00:34:29,920
that they make, that it looks like that these things are necessary, which you can show with the

440
00:34:29,920 --> 00:34:35,040
teleport or with the, with the brick in and out of the way, that people spontaneously engage in

441
00:34:35,040 --> 00:34:39,680
this kind of counterfactual simulation as evidence to the eye movements, and that it's

442
00:34:39,680 --> 00:34:44,240
counterfactuals really and not hypotheticals that seem to be important for expanding causal judgments.

443
00:34:45,440 --> 00:34:49,120
We've played around with this model like a little bit more. Once you have a hammer, right,

444
00:34:49,120 --> 00:34:53,360
you find all the nails. So this one is just like looking at slightly more complex cases.

445
00:34:53,360 --> 00:34:57,120
This one here, philosophers love, because it's a case of, let me show it again, maybe a case of

446
00:34:57,120 --> 00:35:03,040
double prevention, where B prevents ball A from preventing ball E from going through the gate,

447
00:35:03,040 --> 00:35:07,920
right, because knocks it out. It happens in, maybe in football, probably happens often when

448
00:35:07,920 --> 00:35:11,680
one tackles like another person that would have tackled the person running with the ball, right.

449
00:35:11,680 --> 00:35:15,280
And so you might say, oh, to what extent did that cause it? You can also look at omissions when

450
00:35:15,280 --> 00:35:20,240
nothing is happening. So ball A is just chilling here in the corner, and you might still ask,

451
00:35:20,240 --> 00:35:23,840
oh, did it go in because ball A didn't hit it, right? And now you could imagine, well, if it had hit

452
00:35:23,920 --> 00:35:27,600
it, what would have happened in this case? And we can also look at cases where really

453
00:35:27,600 --> 00:35:32,080
nothing is happening at all. So here's just a tower of blocks, right, and you might still wonder,

454
00:35:32,080 --> 00:35:35,600
oh, to what extent is this black one here responsible for the other one staying on the table?

455
00:35:36,160 --> 00:35:39,280
And even though, yeah, there's nothing happening, right, you might still say, well,

456
00:35:39,280 --> 00:35:43,280
how do you answer this question? Maybe by doing something like playing Jenga in your mind, right,

457
00:35:43,280 --> 00:35:47,440
imagining it being removed, and then what would have happened to the scene? So that even just

458
00:35:47,440 --> 00:35:52,800
physical support in some sense is very closely related to ideas of causation, right. What it means

459
00:35:52,800 --> 00:35:59,440
to support is essentially to prevent something from falling. Okay, so that was part one. Now a

460
00:35:59,440 --> 00:36:05,120
sort of short version of part two. And so responsibility attribution was something that I've

461
00:36:05,120 --> 00:36:10,320
been into for quite a while and was also my motivating thing. And then I drifted off into

462
00:36:10,320 --> 00:36:15,040
causality world mostly just because physics engines were around at the time. So it was like, oh,

463
00:36:15,040 --> 00:36:19,680
now I can use those. And with around at the time, I mean, I was a postdoc with Josh Tenenbaum back

464
00:36:19,680 --> 00:36:24,240
then and physics engines were all the rage at the time. And I said, okay, now I'll also use them.

465
00:36:24,240 --> 00:36:29,360
And there aren't really yet, although I guess Michael is working on it, psychology engines,

466
00:36:29,360 --> 00:36:32,480
right, that is easy where you could just have agents and think about what they would have done.

467
00:36:33,760 --> 00:36:38,320
So this work that I had done on responsibility attribution wasn't particularly social, also

468
00:36:38,320 --> 00:36:41,840
didn't really involve simulation, I think. And there was one experiment that got a little bit

469
00:36:41,840 --> 00:36:47,200
closer that I'll briefly share with you here on a paper called Moral Dynamics. And it will look

470
00:36:47,200 --> 00:36:51,760
very billiard ball world like I haven't moved too far away from the billiard balls, but this kind

471
00:36:51,760 --> 00:36:55,360
of that's somewhat agentive, right? So and so we could show people like a video clip like this

472
00:36:55,360 --> 00:36:59,840
and then ask them, what about extent do you think that blue was responsible that the green one got

473
00:36:59,840 --> 00:37:05,360
harmed in this case here? And our inspiration here came from a paper called Moral Kinematics where

474
00:37:05,360 --> 00:37:09,920
they basically argued, again, it's somewhat more actualist view and saying, okay, there's certain

475
00:37:09,920 --> 00:37:13,840
features that people are picking up on in these scenes, like the duration of contact,

476
00:37:13,840 --> 00:37:18,160
how far things moved and things like that. And then they directly mapped from these features

477
00:37:18,160 --> 00:37:24,640
of the scene to the moral judgment in this case. And we liked the general setup, but didn't really

478
00:37:24,640 --> 00:37:30,480
like that model like as much. So we proposed another model that has a slightly different title,

479
00:37:30,480 --> 00:37:34,960
Moral Dynamics instead. And we thought, okay, these features are important, but the features are

480
00:37:34,960 --> 00:37:40,640
important in that they give us evidence for the latent variables and that those are ultimately

481
00:37:40,640 --> 00:37:44,880
the ones that I care about. And in this case, what are the latent ones that we thought one,

482
00:37:44,880 --> 00:37:48,880
not very surprisingly here on the right hand side causality, but did you think that it actually

483
00:37:48,880 --> 00:37:53,840
caused it, you know, to for this negative outcome to happen. And then the left side,

484
00:37:53,840 --> 00:37:57,600
the intuitive psychology part, very kind of minimal in this case here. But it's basically

485
00:37:57,600 --> 00:38:02,160
saying like, well, maybe these features give you some evidence about like how much the agent

486
00:38:02,160 --> 00:38:06,800
actually wanted to bring about this negative outcome. So if you think, for example, if somebody

487
00:38:06,800 --> 00:38:10,480
really wants something to happen, then they're willing to incur a larger cost to make it happen.

488
00:38:11,600 --> 00:38:15,440
Putting a lot of effort, for example. So if somebody puts in a lot of effort into something,

489
00:38:15,440 --> 00:38:19,920
you know that they must have really valued it. And if somebody really valued some negative outcome,

490
00:38:19,920 --> 00:38:24,880
well, that's a bad thing. That was roughly the idea here. And we could then show that if we have

491
00:38:24,880 --> 00:38:30,400
a model that just basically infers the amount of effort that some agent exerted and tried to map

492
00:38:30,400 --> 00:38:35,840
that onto the responsibility that worked kind of, you know, okay-ish. If we only took into account

493
00:38:35,840 --> 00:38:40,160
the causal role that some agent played and tried to use that to explain the extent to which

494
00:38:40,160 --> 00:38:44,960
they're held responsible, that worked okay-ish. But if we now took a model that takes both of

495
00:38:44,960 --> 00:38:49,200
these components into account, that worked pretty well, which was roughly in line with this kind

496
00:38:49,200 --> 00:38:54,400
of unsurprisingly, now this framework that I laid out at the beginning, right, that when we assign

497
00:38:54,400 --> 00:38:58,640
responsibility to others, we don't just care about the causal role that they played, but also what

498
00:38:58,640 --> 00:39:02,720
the action tells me about the kind of person that they are. In this case, the action tells me

499
00:39:02,720 --> 00:39:05,840
something about the desire that they had to bring about this negative outcome.

500
00:39:06,480 --> 00:39:13,040
Okay. But still, we didn't really have a real model of agents in this case. We still sort of

501
00:39:13,040 --> 00:39:18,240
basically just use the physics engine. Also, we weren't able to talk about intentions, and it's

502
00:39:18,240 --> 00:39:23,120
clearly important often when people talk about responsibility. And also still our kind of factual

503
00:39:23,120 --> 00:39:26,880
simulation here was basically purely physical, just seeing how this thing would have moved

504
00:39:26,880 --> 00:39:34,880
without the other one. So I don't have the skills to make it happen with sort of more

505
00:39:34,880 --> 00:39:39,120
agentive agents, but luckily now that I'm here, I get to work out with all these smart people,

506
00:39:39,840 --> 00:39:43,760
and here's my PhD student, Sarah Wu, and our research assistant, Shruti Sreeta,

507
00:39:43,760 --> 00:39:48,000
and they've looked into cases now that are a little bit more agentive. They're still kind of in

508
00:39:48,000 --> 00:39:53,760
in grid world, but at least now planning and intentions and things like that are involved.

509
00:39:54,480 --> 00:39:59,040
And here's the basic setup. So this is inspired by some previous work that has looked into helping

510
00:39:59,040 --> 00:40:05,840
and hindering as a case study. And what they did is essentially they said, well, what it means

511
00:40:05,840 --> 00:40:11,520
for somebody to intend to help someone is that their utility function includes the other person's

512
00:40:11,520 --> 00:40:17,440
utility with a positive sign. Intending to help just means wanting to bring positive utility,

513
00:40:17,440 --> 00:40:21,840
at least in this framework, to the other person. And intending to hinder puts a negative sign,

514
00:40:21,840 --> 00:40:27,760
like now I want it that the other person is a low utility. So it turns out though that

515
00:40:27,760 --> 00:40:32,640
intending to help or hinder versus actually helping or hindering is not necessarily the same

516
00:40:32,640 --> 00:40:38,400
thing. So here's an example. I don't have a child yet, but at some point maybe we'll have a child,

517
00:40:38,400 --> 00:40:42,000
and then if I go grocery shopping with the child, there probably will be a period of time where

518
00:40:42,000 --> 00:40:49,280
they're not actually helping. They're sort of like trying to help, but kind of making it worse,

519
00:40:49,280 --> 00:40:52,880
at least in terms of efficiency and so on. It's going to take longer. Of course, it's useful

520
00:40:52,880 --> 00:40:57,200
because eventually they will be helpful. I have to go through that process just like a PhD student.

521
00:40:58,320 --> 00:41:12,320
So yeah, so you go through that process, and then you might intend to be helpful,

522
00:41:12,320 --> 00:41:18,320
but it might take a little bit of time to actually be helpful. And the claim is to evaluate that,

523
00:41:18,320 --> 00:41:22,000
you need counterfactuals again to tell, oh, is the person actually helpful? Well,

524
00:41:22,000 --> 00:41:26,080
how would it have happened without them, essentially? Or there's different counterfactuals

525
00:41:26,160 --> 00:41:30,080
to consider, but that's one of them. So here's our grid world that we played with,

526
00:41:30,080 --> 00:41:35,040
with the helping and hindering setup. So we have this red guy here who wants to get to the star,

527
00:41:35,040 --> 00:41:39,840
has a pure physical goal in this case, just to get to that location. Then we have this blue one

528
00:41:39,840 --> 00:41:44,400
who has a pure social goal. They either want to help or hinder the red one from getting there.

529
00:41:44,400 --> 00:41:48,560
And then there are these walls here that you can't do anything about, but there's also these blocks,

530
00:41:48,560 --> 00:41:53,920
and only the blue one can interact with these blocks. They can push, pull them out of the way.

531
00:41:53,920 --> 00:41:57,680
So here's our Hollywood clip of what's happening in this situation.

532
00:42:02,160 --> 00:42:08,160
Okay, so in this case, happy end, like a Hollywood movie, red made it,

533
00:42:08,160 --> 00:42:11,200
and then we can show people these clips and we can ask them, oh, how responsible was the

534
00:42:11,200 --> 00:42:15,520
blue player for the red player's success, for example, in this trial? We can also ask them

535
00:42:15,520 --> 00:42:19,600
a counterfactual question, right, would the red player still have succeeded even if the blue player

536
00:42:19,600 --> 00:42:25,040
hadn't been there? And we can ask them to make an inference about the intention of the blue one

537
00:42:25,040 --> 00:42:28,800
in this case. What was the blue player intending to do? Were they trying to help or were they

538
00:42:28,800 --> 00:42:34,480
trying to hinder? Definitely help, definitely hinder. So the idea is now basically the same as earlier,

539
00:42:34,480 --> 00:42:38,480
by just saying, okay, again, we need some kind of generative model of the domain. In this domain,

540
00:42:38,480 --> 00:42:43,840
now it's a model of agents basically planning and recursively reasoning about one another, right?

541
00:42:44,880 --> 00:42:49,120
And that's now our probabilistic program. And we can again compute counterfactuals over that,

542
00:42:49,120 --> 00:42:52,960
maybe in this case thinking, well, what would have happened if the blue one hadn't been there?

543
00:42:52,960 --> 00:42:57,360
And then thinking how the red one would have planned their path differently, but without the

544
00:42:57,360 --> 00:43:02,880
presence of blue, that's a rough idea. So again, we take some actual situation and we can then

545
00:43:02,880 --> 00:43:06,800
simulate what would have happened in the relevant counterfactual situation in this case where blue

546
00:43:06,800 --> 00:43:10,960
hadn't been there. We can talk later if you like about other counterfactuals you might consider,

547
00:43:10,960 --> 00:43:14,960
but we just went with this one here, but what if they hadn't been there? In this case, yeah,

548
00:43:14,960 --> 00:43:19,360
they wouldn't have made it because the block was in the way, right? We also have a model of

549
00:43:19,360 --> 00:43:23,120
intention inference, but I'll sort of skip that. It's basically just saying, okay, if you have a

550
00:43:23,120 --> 00:43:27,760
generative model about what an helping or hindering agent would do, you can then condition on the

551
00:43:27,760 --> 00:43:32,880
observations that you see them acting and see what's more likely that they were helping or

552
00:43:32,880 --> 00:43:37,360
hindering given the actions that they carried out. So I'll just give you a few more examples of the

553
00:43:37,360 --> 00:43:42,160
sort of video clips that we showed to participants. That's a diagram of the one that you've just seen.

554
00:43:42,160 --> 00:43:49,520
Here's another one where kind of, you know, blue is sort of extra mean, you might say. There was

555
00:43:49,520 --> 00:43:54,880
already a block in the way, but they put another block in the way. What the heck? Yeah, really trying

556
00:43:54,880 --> 00:44:03,840
to be helpful through adversarial actions. So here's another one here where blue is sort of

557
00:44:03,840 --> 00:44:08,800
laudably helpful, but like, you know, was not really necessary, but maybe looks nice.

558
00:44:09,760 --> 00:44:12,480
Here's a case in which sort of things go wrong.

559
00:44:15,840 --> 00:44:19,520
Where blue was maybe trying to be helpful, but actually sort of made it worse, you know,

560
00:44:19,520 --> 00:44:24,320
the reactions that they took. And then here's another one. We had a large number, so I'm just

561
00:44:24,320 --> 00:44:29,760
showing like a subset of them. So this is one where blue could have easily hindered if they had

562
00:44:29,760 --> 00:44:36,400
wanted to, but didn't, because they could have just pushed it into the way. And so then we now

563
00:44:36,480 --> 00:44:41,520
have to again, yeah, try to capture whether we can, with our model, capture the counterfactual

564
00:44:41,520 --> 00:44:45,440
judgments that people are making. And we sort of can, there's not as much kind of variance here,

565
00:44:46,000 --> 00:44:50,080
at least in the predictions of the model. So this model is sort of okay-ish. It captures the trends

566
00:44:50,080 --> 00:44:53,600
overall, but there's more variance in people's judgments that is not quite captured by the

567
00:44:53,600 --> 00:44:59,520
model yet. So we're still, this is sort of more ongoing work. In terms of intention inference,

568
00:44:59,520 --> 00:45:03,760
it's fine. So it can also kind of infer whether the person was helping or hindering,

569
00:45:03,760 --> 00:45:07,440
but also here, what you see is stuff are bunched up that the model all gives a hundred to,

570
00:45:08,480 --> 00:45:13,280
where there's still some differentiation that people make, but sort of mostly captures what's

571
00:45:13,280 --> 00:45:18,400
going on. And if we now look at the responsibility judgments, and we try to do the same thing

572
00:45:18,400 --> 00:45:22,240
initially that we did with the billiard balls earlier, that we just take the counterfactuals,

573
00:45:22,240 --> 00:45:29,040
like on the x-axis, and try to predict the responsibility here on the y-axis, it's okay-ish,

574
00:45:29,040 --> 00:45:32,560
but not, you always want, when you do computational modeling, you always want them

575
00:45:32,560 --> 00:45:37,200
nicely line up on the diagonal. And that's not really what was happening in this case,

576
00:45:37,200 --> 00:45:40,800
whereas for the billiard balls, we have this very simple counterfactuals nicely predict

577
00:45:41,440 --> 00:45:47,040
the causal ratings. But if again, if you have a model that incorporates also the intention

578
00:45:47,040 --> 00:45:53,040
inferences, like into the predictions, now they do sort of more nicely line up on the diagonal.

579
00:45:53,680 --> 00:45:57,440
Again, suggesting that when it comes to assigning responsibility for agents,

580
00:45:57,440 --> 00:46:01,840
it's not just the causal role that matters. It also matters what the actions that they took

581
00:46:01,840 --> 00:46:05,440
tell me about the kind of person that they are. In this case, it tells me something about

582
00:46:05,440 --> 00:46:09,760
their intentions, like they try to be helpful, or that they try to be hindering. So the both of

583
00:46:09,760 --> 00:46:15,040
these components. And just to give you a sense of an example where we need this kind of intention

584
00:46:15,040 --> 00:46:19,360
part, like that's back to that mean one where the blue one pushes another one into the way, right?

585
00:46:19,920 --> 00:46:24,240
And so just to help you kind of interpret the bars here, the counterfactual, that's the condition

586
00:46:24,240 --> 00:46:28,400
where we asked them, would red have succeeded if blue hadn't been there? That's basically our

587
00:46:28,400 --> 00:46:35,440
causal model. And they don't think so, right? The pink, pink, purplish one is like very low,

588
00:46:35,440 --> 00:46:39,600
right? But also when we asked them what the intention of the blue one is, they think, yeah,

589
00:46:39,600 --> 00:46:43,440
was really hindering. So here zero means hindering and 100 means helping. So they think, yeah,

590
00:46:43,440 --> 00:46:48,400
they were hindering. So even though they say that, yeah, the blue one didn't really play a causal

591
00:46:48,400 --> 00:46:51,680
role, they still give them quite a bit of responsibility, like in the blue one on the

592
00:46:51,680 --> 00:46:56,720
right hand side. So that's one case, at least, where currently we need this other part. So they

593
00:46:56,720 --> 00:46:59,520
think, yeah, blue blue's actually make no difference, but they were definitely trying to

594
00:46:59,520 --> 00:47:03,120
hinder. And so, yeah, I still give them some responsibility for this outcome.

595
00:47:04,480 --> 00:47:10,640
Okay, so sort of almost last slide. Because we have these agents like recursively thinking about

596
00:47:10,640 --> 00:47:15,120
one another, an interesting setting that also can happen here is that you can actually hinder or

597
00:47:15,120 --> 00:47:20,400
help one another, again, maybe also like in the, in the advisor, advisor setting, not by actually

598
00:47:20,400 --> 00:47:24,880
making any change to the physical world, but changing somebody else's belief. So I just want to

599
00:47:25,360 --> 00:47:28,880
show you that example. And maybe you'll get that intuition from the setting here.

600
00:47:34,960 --> 00:47:40,160
So very, very mean, very, very sad. Because it looked really like blue was going to help,

601
00:47:40,160 --> 00:47:47,520
right? And then they didn't, right? And here's just one participant, what they're saying,

602
00:47:47,520 --> 00:47:50,960
oh, blue tricked red into thinking she was going to move the box to help. But then

603
00:47:50,960 --> 00:47:56,080
once red was stuck on the side of the wall, blue left the box where it was, very sad, you know.

604
00:47:56,080 --> 00:47:59,840
And a lot of people say something along those lines. We also had one condition where we just

605
00:47:59,840 --> 00:48:03,600
have them give explanations of what happened, right? And here the interesting part, right,

606
00:48:03,600 --> 00:48:07,520
is that the hindering is not happening because blue changed anything about the world. They didn't

607
00:48:07,520 --> 00:48:11,280
move a block in the way or something, but they hindered because they made red believe that

608
00:48:11,280 --> 00:48:15,120
they were going to be helpful and then they weren't, right? Here, if blue hadn't been there,

609
00:48:15,120 --> 00:48:18,960
red would have just walked along on the outside and they might have made it, you know, anyhow,

610
00:48:19,040 --> 00:48:23,360
even without blue. And this happens because they're recursively thinking about one another,

611
00:48:23,360 --> 00:48:26,320
right? And red things like, oh, blue is taking actions that are going to help me

612
00:48:26,320 --> 00:48:32,400
so I can take the shortcut. And then it turns out I couldn't in this case. Okay, wrapping up.

613
00:48:33,200 --> 00:48:37,120
So this was the second part where we, I guess, applied this model now to at least a simple

614
00:48:37,120 --> 00:48:41,360
setting where agents are interacting with one another, helping and hindering one another,

615
00:48:42,320 --> 00:48:46,320
that in order to judge whether somebody helped or hindered, I again think that you need this

616
00:48:46,320 --> 00:48:52,480
process of counterfactual simulation and that responsibility judgments are sensitive both

617
00:48:52,480 --> 00:48:56,640
to the cause of the world that somebody played and what the actions tell us about the kind of

618
00:48:56,640 --> 00:49:02,160
mental state that they had. Just to conclude, so together, hopefully, this sort of set of studies

619
00:49:02,160 --> 00:49:06,400
gets some evidence that people seem to be constructing these rich mental models of the

620
00:49:06,400 --> 00:49:10,080
world that we can get evidence for in different kinds of ways, like through eye tracking and other

621
00:49:10,080 --> 00:49:16,400
tools. By imagining interventions like on these mental models, those allow us to compute the

622
00:49:16,400 --> 00:49:20,960
counterfactuals, which I think are important for assigning responsibility, giving explanations

623
00:49:20,960 --> 00:49:26,400
and so on. And that this counterfactual simulation model that I've been kind of developing can then

624
00:49:26,400 --> 00:49:30,640
be relatively flexibly applied to physical and social events, where you think that the main

625
00:49:30,640 --> 00:49:34,400
thing that's happening is that your model of the world changes and maybe the exact

626
00:49:34,400 --> 00:49:38,400
counterfactual cooperation that you're carrying out changes, but otherwise the framework sort of

627
00:49:38,400 --> 00:49:43,520
holds. So with that, I want to thank the main people who helped me do this kind of work,

628
00:49:43,520 --> 00:49:47,680
and then maybe you for your attention. And there's a little bit of time for questions.

629
00:49:59,520 --> 00:50:04,960
So one thing I'm curious is, I assume notions of causality are probably somewhat universal,

630
00:50:05,040 --> 00:50:09,760
but especially issues of moral judgment, intention are likely dependent to some extent on

631
00:50:09,760 --> 00:50:13,680
environmental factors, cultural factors, those kinds of things. And so I'm curious if you've

632
00:50:13,680 --> 00:50:17,040
either observed those in your experiments or if you have some way of controlling for those

633
00:50:17,040 --> 00:50:23,440
factors when you recruit participants. Yeah, so that's an interesting question. And I think

634
00:50:23,440 --> 00:50:30,400
even notions of causality actually, there are cultural effects like who you see there. So

635
00:50:31,280 --> 00:50:36,240
when making causal judgments, there's often, there's basically like in many cases,

636
00:50:37,040 --> 00:50:40,880
what's called the problem of causal selection, how do I even decide what thing to pick out of

637
00:50:40,880 --> 00:50:45,680
as the cause in the first place. In my setting, very often I've kind of made it pretty easy,

638
00:50:45,680 --> 00:50:50,480
and I've sort of constrained it because I already told you like these are the possible causes,

639
00:50:51,520 --> 00:50:55,440
but in the real world it's not like that. And it's sometimes, we may see something,

640
00:50:55,440 --> 00:50:59,680
we may see a person as a cause, or we may see a system as a cause, or we may also

641
00:50:59,680 --> 00:51:04,400
see the kind of counterfactuals that may come to mind to us may also depend on what our background

642
00:51:04,400 --> 00:51:09,680
is. And it often tells us something about, oh, when somebody then gives a certain counterfactual,

643
00:51:09,680 --> 00:51:15,040
it tells us quite a bit about them. So this comes up in the context, for example, also of victim

644
00:51:15,040 --> 00:51:19,840
blaming. Like if that's the counterfactual that came to mind to you, oh, that tells me something

645
00:51:19,840 --> 00:51:26,240
like about you. So I would say that even in that context, there are strong kind of interpersonal

646
00:51:26,320 --> 00:51:32,480
and cultural effects that affect how we attribute causality. Now when it comes to intention inferences,

647
00:51:32,480 --> 00:51:36,800
I'm not sure that that process in and of itself, that at least to me feels relatively

648
00:51:38,800 --> 00:51:43,280
whatever universal, that we kind of, we have to engage in that all the time by trying to

649
00:51:43,280 --> 00:51:48,880
predict what other people are intending in the way that we interact with them. But then again,

650
00:51:48,880 --> 00:51:55,040
how maybe then judgments in this case of responsibility or morality like draw on these

651
00:51:55,040 --> 00:52:00,080
different components, for example, that I've laid out here, no claim that that is in any sense

652
00:52:00,080 --> 00:52:06,640
sort of universal. But it might very well be that in certain cultures like this kind of what I take

653
00:52:06,640 --> 00:52:11,280
here to be more the person component, right, may have a stronger influence on responsibility

654
00:52:11,280 --> 00:52:16,560
judgments and in others, it might mostly be about causality. I certainly in my experiments

655
00:52:17,840 --> 00:52:21,520
for individual participants see a lot of variance along the lines. But there's some people that

656
00:52:21,520 --> 00:52:25,600
don't care about even the intention part at all. They just say like, oh, when it's about responsible,

657
00:52:25,600 --> 00:52:30,400
I just look at what would have happened if they hadn't been there. And then other participants,

658
00:52:30,400 --> 00:52:37,120
judgments are suggesting that they care about the intention part much more. But I have not yet

659
00:52:37,760 --> 00:52:41,840
engaged in the kind of work that then tries to explain why is it, why is it that this person

660
00:52:41,840 --> 00:52:46,480
casts so much about causality and why is it that this person casts so much about the intention

661
00:52:46,480 --> 00:52:55,840
part, for example. Thank you. I'm going to hug the mic actually. I'm interested, like, do you think

662
00:52:55,840 --> 00:53:01,920
this model applies to other settings? Because all of the examples were sort of like physical or

663
00:53:01,920 --> 00:53:07,360
agents taking physical actions. So if you had just like a verbal description of some social

664
00:53:07,360 --> 00:53:11,840
scenario where there's like speech acts that are causing things, do you think it would work the same?

665
00:53:12,320 --> 00:53:18,080
Yeah. Yeah, that's a great question. So would it work the same? So my sense is like, yeah, in a

666
00:53:18,080 --> 00:53:28,080
similar way, so there's a number of things here, I think. So we have applied the model a little bit,

667
00:53:28,080 --> 00:53:32,880
like this kind of counterfactual simulation model in the physical world, also two speech acts.

668
00:53:32,880 --> 00:53:37,280
And there it's in the context of like, we were basically jealous of, you know,

669
00:53:38,240 --> 00:53:41,840
for those of you who remember full wolf, you had these different words, right? And we were like,

670
00:53:41,840 --> 00:53:46,080
oh, our model can only do like cause and prevent. That's kind of sad. But there's other causal

671
00:53:46,080 --> 00:53:52,720
expressions, of course, right? Enabling, affecting, letting, allowing, and so on. And it's going to be

672
00:53:52,720 --> 00:53:57,120
a little bit of a of doing, but I'll get there. So we were trying to see to what extent this

673
00:53:57,120 --> 00:54:01,200
framework that we have could also allow us to explain differences between these different

674
00:54:02,000 --> 00:54:07,680
expressions, right? And, and this also comes up, you know, in philosophy, like even questions,

675
00:54:07,680 --> 00:54:12,080
so the question versus killing versus causing to die, even people like in cases of abortion,

676
00:54:12,080 --> 00:54:15,920
you know, the way that you talk about it, right? Again, reveals something, you know,

677
00:54:15,920 --> 00:54:20,720
how you think about it. And in general, right, like this distinction also, when you have that as

678
00:54:20,720 --> 00:54:25,280
an alternative that you could have said killing, but you chose causing to die, it suggests maybe

679
00:54:25,280 --> 00:54:28,240
a more roundabout way in which something happened, right? Like the person killed it,

680
00:54:28,240 --> 00:54:32,880
caused them to die. You think, yeah, it would be weird to say that someone caused them to die

681
00:54:32,880 --> 00:54:36,720
when they like, you know, directly walked up to them and, you know, shot them. This also came

682
00:54:36,720 --> 00:54:42,000
up recently or still coming up these days, actually, with the case of Alec Baldwin, Rust,

683
00:54:42,000 --> 00:54:46,080
like in the movie, right? The way that people talk about it was it will hold the gun that

684
00:54:46,080 --> 00:54:50,560
discharged or something rather than, you know, shot the person, right? So it matters a lot,

685
00:54:50,560 --> 00:54:55,200
basically, like in this case, the choice of word, right? And the, and the, in some sense,

686
00:54:55,200 --> 00:54:59,520
the counterfactual alternatives you could have had, right, for them, the image that it's creating

687
00:54:59,520 --> 00:55:04,160
in the listener in this case, right? So the fact that, oh, you chose this expression suggests to me

688
00:55:04,160 --> 00:55:08,880
that the scenario must have been such, like rather than such. So that's at least the minimal way,

689
00:55:08,880 --> 00:55:13,040
I think, in which it applies also to, to thinking about speech acts, right? And thinking like,

690
00:55:13,040 --> 00:55:17,200
yeah. And of course, you could think like, oh, you know, again, take the advice example, would the

691
00:55:17,200 --> 00:55:22,160
students still have done that if I had not said that, right? So we are obviously causing each other

692
00:55:22,240 --> 00:55:26,960
a lot in the way that we talk to each other. And sometimes, you know, yeah. Also, of course,

693
00:55:26,960 --> 00:55:31,920
after talking, I might think like, oh, I wish I had, I had answered this question from differently

694
00:55:32,480 --> 00:55:35,680
than what I actually did. And I regret it, right? And things like that.

695
00:55:38,480 --> 00:55:43,360
Yeah. So on a similar note, I'm wondering if you have thoughts on how possible it would be to

696
00:55:44,240 --> 00:55:49,680
use this model on society, large scale societal events, their divisive, such as what cost a

697
00:55:49,680 --> 00:55:56,640
person to be elected, what cost code outbreaks, or what causes climate change, like how possible

698
00:55:56,640 --> 00:56:01,360
would it be to apply this to those events and also what challenges you foresee? Yeah. Yeah,

699
00:56:01,360 --> 00:56:06,800
that's a great question. And, and so, so I had the example of example at the beginning with like,

700
00:56:06,800 --> 00:56:10,080
oh, did the fall of Lehman Brothers caused the financial crisis, right? That's sort of like,

701
00:56:10,080 --> 00:56:14,560
large scale. And I don't know, right? And, and, and partly it might, so, and there's a few options,

702
00:56:14,560 --> 00:56:19,040
right? One is like, okay, just like totally punting, right? And saying like, okay, well, if the system

703
00:56:19,040 --> 00:56:23,600
gets sufficiently complex, such that I cannot carry out the relevant counterfactual computation

704
00:56:23,600 --> 00:56:28,800
anymore, well, I just don't know, right? I cannot give that causal answer. That's, that's one version,

705
00:56:28,800 --> 00:56:33,360
right? And there's another version where you say like, okay, well, to the extent that I can maybe,

706
00:56:33,360 --> 00:56:38,880
you know, abstract away from a lot of the lower level details that say of some, so if I'm, if I'm,

707
00:56:38,880 --> 00:56:44,000
if I have the capacity to build maybe a more abstract model, which, which I can now simulate,

708
00:56:44,000 --> 00:56:48,560
right, then I might be giving you an answer sort of on that level, right? And so, but then it's

709
00:56:48,560 --> 00:56:52,960
also half punting, right? Because now you have to kind of come up with a good model of how people

710
00:56:52,960 --> 00:56:58,080
generate the right kind of causal abstractions for some situation that then allow them to compute

711
00:56:58,080 --> 00:57:03,200
the counterfactual, because now it's not messy anymore, right? And another thing that I should

712
00:57:03,200 --> 00:57:07,280
mention, and that quite a lot of the work on responsibility that I've, that I've looked at

713
00:57:07,280 --> 00:57:12,320
particularly in groups, the sort of situations that you pointed out, like elections and, you know,

714
00:57:12,320 --> 00:57:16,960
global warming, they're, they're characterized by, by large degrees of over determination,

715
00:57:16,960 --> 00:57:23,520
right? Like in election, you hardly ever cast a pivotal vote, right? And, and so those also

716
00:57:23,520 --> 00:57:27,680
traditionally were problems for counterfactual accounts, right? Because everyone can say like,

717
00:57:27,680 --> 00:57:32,320
I made no difference, like if I fly every day, you know, that's not really going to make a difference.

718
00:57:32,320 --> 00:57:37,280
And so, and there you can, and similar with election, why should I go vote, right? Because

719
00:57:37,280 --> 00:57:40,720
if my vote's not going to make any difference, right? And there at least models have been built

720
00:57:40,720 --> 00:57:45,600
that then say like, okay, well, it's not, you're not off the hook, right? It's basically saying,

721
00:57:45,600 --> 00:57:49,280
even if you would not have made a difference in this particular situation,

722
00:57:49,280 --> 00:57:52,800
maybe the degree of responsibility that you have for some election, for example,

723
00:57:52,800 --> 00:57:57,120
maybe related to how close you were to making a difference to the outcome, right? If it's like,

724
00:57:57,120 --> 00:58:02,160
if the outcome is 6-5, you feel very responsible. If it's like 7-4, a little less. If it's like

725
00:58:02,160 --> 00:58:07,840
8-3, a little less, right? But not, but it shouldn't go to zero, right? And then, and then as it,

726
00:58:07,840 --> 00:58:11,520
maybe now relates to kind of, you know, global warming and so on, part of the challenge then

727
00:58:11,520 --> 00:58:16,960
from the more like, you know, what do we do about it? Side might be like, okay, how do we make it

728
00:58:16,960 --> 00:58:23,360
such that people don't perceive a sort of, you know, going to zero sense of responsibility,

729
00:58:23,360 --> 00:58:28,800
right? Such that you feel like actually the actions that you do make a difference to the outcome.

730
00:58:28,800 --> 00:58:36,160
And so, yeah, so that's, so I think a mix of thoughts, I guess, in response to your question.

731
00:58:37,120 --> 00:58:42,160
So we're about it, time. Is there a reminder if you are here? If you're logging attendance,

732
00:58:42,160 --> 00:58:46,400
make sure to grab one of these code words up at the front and give Toby a compliment on his

733
00:58:46,400 --> 00:58:50,800
talk on your way out, maybe make you come up next door. Let's thank our speaker.

