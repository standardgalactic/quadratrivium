1
00:00:00,000 --> 00:00:12,800
Hi. I'm delighted to have with us here today my old friend Professor Fei Fei Li. Fei Fei

2
00:00:12,800 --> 00:00:18,640
is a professor of computer science at Stanford University and also co-director of HAI, the

3
00:00:18,640 --> 00:00:25,880
Human-Centered AI Institute. And previously, she also was responsible for AI at Google

4
00:00:25,880 --> 00:00:29,960
Cloud as a chief scientist for the division. It's great to have you here, Fei Fei.

5
00:00:29,960 --> 00:00:32,240
Thank you, Andrew. Very happy to be here.

6
00:00:32,240 --> 00:00:36,000
So, I guess, actually, how long have you known each other? I've lost track.

7
00:00:36,000 --> 00:00:41,680
Definitely more than a decade. I mean, I've known your work, right, before we even met.

8
00:00:41,680 --> 00:00:50,560
And I came to Stanford 2009, but we started talking 2007, so 15 years.

9
00:00:50,560 --> 00:00:56,640
And I can still have very clear memories of how stressful it was when, collectively,

10
00:00:56,640 --> 00:01:00,720
you know, a bunch of us, me, Chris Manning, a bunch of us, were trying to figure out how

11
00:01:00,720 --> 00:01:02,960
to recruit you to come to Stanford.

12
00:01:02,960 --> 00:01:10,120
It wasn't hard. I just needed to sort out my students and life, but it's hard to resist

13
00:01:10,120 --> 00:01:11,120
Stanford.

14
00:01:11,120 --> 00:01:14,440
It wasn't really great having you as a friend and colleague here.

15
00:01:14,440 --> 00:01:21,480
Yeah, me too. It's been a long time. And we're very lucky to be the generation seeing

16
00:01:21,480 --> 00:01:23,120
AI is great progress.

17
00:01:23,120 --> 00:01:28,720
Okay, so there was something about your background that always found inspiring, which is, you

18
00:01:28,720 --> 00:01:34,560
know, today people are entering AI from all walks of life. And sometimes people still wonder,

19
00:01:34,560 --> 00:01:40,720
oh, I majored in something or other. Is AI a right path for me? So I thought one of the

20
00:01:40,720 --> 00:01:44,120
most interesting parts of your background was that you actually started out not studying

21
00:01:44,120 --> 00:01:49,560
computer science or AI, but you started out studying physics and then had this path to

22
00:01:49,560 --> 00:01:54,880
becoming, you know, one of the most globally recognizable AI scientists. So how did you

23
00:01:54,880 --> 00:01:57,480
make that switch from physics to AI?

24
00:01:57,480 --> 00:02:02,120
Right. Well, that's a great question, Andrew, especially both of us are passionate about

25
00:02:02,120 --> 00:02:10,320
young people's future and their coming to the world of AI. The truth is, if I could

26
00:02:10,320 --> 00:02:16,720
enter AI back then more than 20 years ago, today, anybody can enter AI because AI has

27
00:02:16,720 --> 00:02:27,120
become such a prevalent and globally impactful technology. But myself, maybe I was an accident.

28
00:02:27,120 --> 00:02:35,760
So I have always been a physics kid or STEM kids. I'm sure you were too. But physics was

29
00:02:35,760 --> 00:02:41,240
my passion all the way through, you know, middle school, high school, college. I went

30
00:02:41,240 --> 00:02:48,120
to Princeton and majored in physics. And one thing physics has taught me till today is

31
00:02:48,120 --> 00:02:55,720
really the passion for asking big questions, the passion for seeking North stars. And I

32
00:02:55,720 --> 00:03:01,400
was really having fun as a physics student at Princeton. And one thing I did was reading

33
00:03:01,480 --> 00:03:09,760
up stories and just writings of great physicists of the 20th century and just hear about what

34
00:03:09,760 --> 00:03:16,760
they think about, you know, the world, especially people like Albert Einstein, Roger Penrose,

35
00:03:16,760 --> 00:03:25,720
you know, Erring Schrodinger. And it was really funny to notice that many of the writings

36
00:03:25,720 --> 00:03:32,360
towards the later half of the career of these great physicists were not about just the atomic

37
00:03:32,360 --> 00:03:41,600
world or the physical world, but pondering about equally audacious questions like life,

38
00:03:41,600 --> 00:03:46,440
like intelligence, like human conditions. You know, Schrodinger wrote this book, What

39
00:03:46,440 --> 00:03:56,400
Is Life? And Roger Penrose wrote this book, Emperor's New Mind, right? And that really

40
00:03:56,400 --> 00:04:03,200
got me very curious about the topic of intelligence. So one thing led to another during college

41
00:04:03,200 --> 00:04:11,800
time, I did the intern at a couple of neuroscience labs, and especially vision related. And I

42
00:04:11,800 --> 00:04:19,120
was like, wow, this is just as audacious a question to ask as the beginning of the universe,

43
00:04:19,120 --> 00:04:26,880
or what is matter made of. And that got me to switch from undergraduate degree in physics

44
00:04:26,880 --> 00:04:34,960
to graduate degree in AI, even though I don't know about you, during our time, AI was a

45
00:04:34,960 --> 00:04:41,520
dirty word. It was AI winter, so it was more machine learning and computer vision and computation

46
00:04:41,520 --> 00:04:46,080
on neuroscience. Yeah, I know. Honestly, I think when when I was in undergrad, I was

47
00:04:46,080 --> 00:04:51,840
too busy writing code, I just, you know, managed to blithely ignore the AI winter and just kept

48
00:04:51,840 --> 00:04:56,760
on coding. Yeah, well, I was too busy solving PDE equations.

49
00:04:56,760 --> 00:05:04,960
And so actually, do you do you have an audacious question now? Yes, my audacious question is

50
00:05:04,960 --> 00:05:12,760
still intelligence. I think since Alan Turing, humanity has not fully understand what is

51
00:05:12,760 --> 00:05:21,600
the fundamental computing principles behind intelligence. You know, we, we, we today we

52
00:05:21,600 --> 00:05:28,200
use the words AI, we use the word AGI. But at the end of the day, I still dream of a

53
00:05:28,280 --> 00:05:35,680
set of simple equations or simple principles that can define the process of intelligence,

54
00:05:35,680 --> 00:05:42,520
whether it's animal intelligence or machine intelligence. And this is similar to physics.

55
00:05:42,520 --> 00:05:49,520
For example, many people have joined the analogy of flying, right? Are we replicating birds

56
00:05:49,520 --> 00:05:55,000
flying, or are we building an airplane? And a lot of people ask the question of the relationship

57
00:05:55,040 --> 00:06:03,800
between AI and brain. And to me, whether we're building a bird or replicating a bird or building

58
00:06:03,800 --> 00:06:11,280
an airplane, at the end of the day, aerodynamics and physics that govern the process of flying.

59
00:06:11,280 --> 00:06:16,760
And I do believe one day we'll discover that. I sometimes think about this, you know, one

60
00:06:16,760 --> 00:06:21,480
learning algorithm hypothesis, could a lot of intelligence, maybe not all, but a lot of

61
00:06:21,560 --> 00:06:27,440
it be explained by one or a very simple machine learning principles. And it feels like we're

62
00:06:27,440 --> 00:06:32,840
still so far from cracking that nut. But in the weekends, when I have spare time, when

63
00:06:32,840 --> 00:06:35,960
I think about learning algorithms and where they could go, this is one of the things I

64
00:06:35,960 --> 00:06:38,560
still, you know, I'm excited about, right, just thinking about.

65
00:06:38,560 --> 00:06:46,160
I totally agree. I still feel like we are pre Newtonian. If we're doing physics analogy

66
00:06:46,160 --> 00:06:52,560
before Newton, there has been great physics, great physicists, a lot of phenomenology, a

67
00:06:52,560 --> 00:06:59,200
lot of studies of how the astrobodies move and all that. But it was Newton who start to

68
00:06:59,200 --> 00:07:05,920
write the very simple laws. And I think we are still going to that very exciting coming

69
00:07:05,920 --> 00:07:13,000
of age of AI as a basic science. And we're pre Newton, in my opinion.

70
00:07:13,320 --> 00:07:17,320
It's really nice to hear you talk about how despite machine learning and AI having come so

71
00:07:17,320 --> 00:07:23,000
far, it still feels like there are a lot more unanswered questions, a lot more work to be done

72
00:07:23,000 --> 00:07:27,560
by maybe some of the people joining the field today than work that's already been done.

73
00:07:27,560 --> 00:07:33,800
Absolutely. I mean, let's let's calculate. It's only what 60 years about. It's a very

74
00:07:33,800 --> 00:07:41,720
nascent field modern his physics and chemistry and biology are all hundreds of years. Right. So

75
00:07:41,720 --> 00:07:50,440
I think it is very, it is very exciting to be entering the field of science of intelligence

76
00:07:51,080 --> 00:07:58,120
and studying AI today. Yeah, I think I remember chatting with the late Professor John McCarthy

77
00:07:58,120 --> 00:08:03,800
who had coined the term artificial intelligence. And boy, the field has changed since when,

78
00:08:03,800 --> 00:08:10,600
you know, he conceived of it at the workshop and came up the term AI. But maybe another 10 years

79
00:08:10,600 --> 00:08:15,080
from now, you know, maybe someone watching this will come up with a new set of ideas.

80
00:08:15,080 --> 00:08:19,320
And then we'll be saying, boy, AI sure is different than what you know,

81
00:08:19,320 --> 00:08:22,600
you and I thought it would be. That's an exciting future to build towards.

82
00:08:22,600 --> 00:08:29,720
Yeah, I'm sure Newton would have not dreamed of Einstein. So, you know, our evolution of science

83
00:08:30,520 --> 00:08:37,720
sometimes takes strides, sometimes takes a while. And I think we're absolutely in an exciting phase

84
00:08:37,720 --> 00:08:45,080
of AI right now. You know, it's interesting hearing you paint this grand vision for AI.

85
00:08:46,120 --> 00:08:49,720
Going back a little bit, there was one other piece of your background that I found, you know,

86
00:08:51,080 --> 00:08:56,360
inspiring, which is when you're just getting started, I've heard you speak about how,

87
00:08:57,240 --> 00:09:01,880
you know, you're a physics student, but not only that, you also you're also running a laundromat

88
00:09:02,440 --> 00:09:07,400
to pay for school. And so just tell us more about that.

89
00:09:09,720 --> 00:09:17,160
So I came to this country, to New Jersey, actually, when I was 15. And one thing great

90
00:09:17,160 --> 00:09:23,560
about being in New Jersey is it was close to Princeton. So I often just take a weekend trip

91
00:09:23,560 --> 00:09:30,440
with my parents and to admire the place where Einstein spent most of his career in the latter

92
00:09:30,440 --> 00:09:38,120
half of his life. But, you know, with typical immigrant life, and it was tough. And by the time

93
00:09:38,120 --> 00:09:46,280
I entered Princeton, my parents didn't speak English. And one thing led to another, it turns out

94
00:09:46,280 --> 00:09:52,600
running a dry cleaner might be the best option for my family, especially for me to lead that

95
00:09:52,600 --> 00:09:58,040
business because it's a weekend business. If it's a weekday business, it would be hard for me to be

96
00:09:58,040 --> 00:10:05,480
a student. And it's actually, believe it or not, running a dry cleaning shop is very machine heavy,

97
00:10:05,480 --> 00:10:13,960
which is good for a STEM student like me. So we decided to open a dry cleaner shop in

98
00:10:14,920 --> 00:10:22,040
a small town in New Jersey called Persephone, New Jersey. It turned out we were physically not too

99
00:10:22,040 --> 00:10:29,560
far from Bell Labs and where lots of early convolutional neural network research was

100
00:10:29,560 --> 00:10:35,800
happening, but I had no idea. Actually, a summer intern at the AT&T Bell Labs way back.

101
00:10:35,800 --> 00:10:41,800
That's right, with Rob Shapiro. With Michael Curran was my mentor. And Rob Shapiro invented

102
00:10:41,800 --> 00:10:48,120
boosting great algorithms. So your coding AI, I was trying to cling to. No, that was only much

103
00:10:48,200 --> 00:10:57,240
later in my life. Did I start interrunning? Yeah. And then it was seven years. I did that for

104
00:10:57,880 --> 00:11:02,280
the entire undergrad and most of my grad school and I hired my parents.

105
00:11:04,280 --> 00:11:09,960
Yeah, no, that's really inspiring. I know you've been brilliant at doing exciting work all your

106
00:11:09,960 --> 00:11:16,120
life. And I think the story of running a laundromat to globally prominent computer scientists,

107
00:11:16,120 --> 00:11:21,960
I hope that inspires some people watching this that no matter where you are, there's plenty

108
00:11:21,960 --> 00:11:28,600
of room for young everyone. Don't even notice, my high school job was an office admin.

109
00:11:30,600 --> 00:11:35,720
And so to this day, I remember doing a lot of photocopying. And the exciting part was using

110
00:11:35,720 --> 00:11:40,600
this shredder. That was a glamorous one. But I was doing so much photocopying in high school,

111
00:11:40,600 --> 00:11:45,800
I thought, boy, if only I could build a robot to do this photocopying, maybe I could do something

112
00:11:46,360 --> 00:11:54,920
Did you succeed? I'm still working on it. We'll see. And then when people think about

113
00:11:54,920 --> 00:12:01,880
you and the work you've done, one of the huge successes everyone thinks about is ImageNet,

114
00:12:01,880 --> 00:12:06,840
where Hub established early benchmark for computer vision. It was really completely

115
00:12:06,840 --> 00:12:11,960
instrumental to the modern rise of deep learning in computer vision. One thing I

116
00:12:11,960 --> 00:12:16,120
bet not many people know about is how you actually got started on ImageNet.

117
00:12:16,120 --> 00:12:23,080
So tell us the origin story of ImageNet. Yeah, well, Andrew, that's a good question,

118
00:12:23,080 --> 00:12:29,640
because a lot of people see ImageNet as just labeling a ton of images. But where we began

119
00:12:29,640 --> 00:12:35,640
was really going after a North Star, brings back my physics background. So when I entered

120
00:12:35,640 --> 00:12:41,960
grad school, when did you enter grad school? Which year? 97. Okay, I was three years later

121
00:12:41,960 --> 00:12:50,280
than you, 2000. And that was a very exciting period, because I was in the computer vision

122
00:12:50,280 --> 00:12:56,200
and computational neuroscience lab of Pietro Peronna and Christoph Koch at Caltech. And

123
00:12:56,200 --> 00:13:03,480
leading up to that, there has been, first of all, two things was very exciting. One is that the world

124
00:13:04,120 --> 00:13:09,400
AI at that point wasn't called AI, computer vision or natural language processing,

125
00:13:09,400 --> 00:13:16,920
has found its lingua de franco, its machine learning, statistical modeling as a new tool

126
00:13:16,920 --> 00:13:22,760
has emerged, right? I mean, it's been around. And I remember when the idea of applying machine

127
00:13:22,760 --> 00:13:27,720
learning to computer vision, that was like a controversial thing. Right, and I was the first

128
00:13:27,800 --> 00:13:33,400
generation of graduate students who were embracing all the base net, all the inference

129
00:13:33,400 --> 00:13:41,560
algorithms and all that. And that was one exciting happening. A second exciting happening that

130
00:13:42,520 --> 00:13:48,680
most people don't know and don't appreciate, is that a couple of decades, probably more than

131
00:13:48,680 --> 00:13:55,320
two or three decades of incredible cognitive science and cognitive neuroscience work

132
00:13:55,320 --> 00:14:01,080
in the field of vision, in the world of vision, human vision, that has really established a couple

133
00:14:01,080 --> 00:14:07,000
of really critical north star problems, just understanding human visual processing and human

134
00:14:07,000 --> 00:14:13,320
intelligence. And one of them is the recognition of understanding of natural objects and natural

135
00:14:13,320 --> 00:14:19,880
things, because a lot of the psychology and cognitive science work is pointing to us,

136
00:14:20,520 --> 00:14:29,160
that is an innately optimized, whatever that word is, functionality and ability of human

137
00:14:29,160 --> 00:14:40,600
intelligence. It's more robust, faster and more nuanced than we had thought. We even find neural

138
00:14:40,600 --> 00:14:51,160
correlates, brain areas devoted to faces or places or body parts. So these two things led to my PhD

139
00:14:51,160 --> 00:15:01,000
study of using machine learning methods to work on real-world object recognition. But it became

140
00:15:01,000 --> 00:15:08,680
very painful, very quickly, that we are coming, banging against one of the most,

141
00:15:10,600 --> 00:15:16,840
continue to be the most important challenge in AI and machine learning is the lack of

142
00:15:16,840 --> 00:15:25,960
generalizability. You can design a beautiful model all you want if you're overfitting that model.

143
00:15:26,520 --> 00:15:31,400
I remember when it used to be possible to publish a computer vision paper showing your

144
00:15:31,400 --> 00:15:39,320
works on one image. Exactly. Yeah, it's just the overfitting, the models are not very expressive

145
00:15:39,880 --> 00:15:50,040
and we lack the data. And we also, as a field, was betting on making the variables very rich by

146
00:15:50,600 --> 00:15:56,760
hand-engineered features. Remember, every variable carrying a ton of semantic meaning,

147
00:15:56,760 --> 00:16:05,800
but with hand-engineered features. And then towards the end of my PhD, my advisor,

148
00:16:05,800 --> 00:16:12,600
Pietro and I start to look at each other and say, well, boy, we need more data. If we believe in

149
00:16:12,600 --> 00:16:19,160
this North Star problem of object recognition, and we look back at the tools we have,

150
00:16:20,120 --> 00:16:25,800
mathematically speaking, we're overfitting every model we're encountering. We need to take a fresh

151
00:16:25,800 --> 00:16:32,520
look at this. So one thing led to another. He and I decided we'll just do a, at that point,

152
00:16:32,520 --> 00:16:38,920
we think it was a large-scale data project called Caltech 101. I remember the data set. I wrote

153
00:16:38,920 --> 00:16:44,520
papers using your Caltech 101 data set way back. You did. You and your early graduate student.

154
00:16:44,520 --> 00:16:50,040
It helped benefit a lot of researchers. Yeah. Caltech 101 data set. That was me and my mom

155
00:16:50,040 --> 00:16:57,560
labeling images on a couple of undergrads. But that was, it was the early days of internet.

156
00:16:57,560 --> 00:17:04,280
So suddenly the availability of data was a new thing. You suddenly, I remember Pietro still

157
00:17:04,280 --> 00:17:10,760
have this super expensive digital camera. I think it was Canon or something like $6,000

158
00:17:11,560 --> 00:17:18,680
walking around Caltech taking pictures. But we are the internet generation. I go to Google

159
00:17:18,680 --> 00:17:23,800
Image Search. I start to see these thousands and tens of thousands of images. And I tell Pietro,

160
00:17:23,800 --> 00:17:29,480
let's just download. Of course, it's all that easy to download. So one thing led to another.

161
00:17:29,480 --> 00:17:37,080
We built this Caltech 101 data set of 101 object categories. And about, I would say,

162
00:17:37,080 --> 00:17:43,080
30 to 50, 30,000 pictures. I think it's really interesting that,

163
00:17:44,040 --> 00:17:49,800
you know, even though everyone's heard of ImageNet today, even you kind of took a couple

164
00:17:49,800 --> 00:17:54,440
of iterations where you did Caltech 101. And that was a success. Lots of people used it.

165
00:17:54,440 --> 00:17:59,640
But it's the, even the early learnings from building Caltech 101 that gave you the basis to

166
00:17:59,640 --> 00:18:07,000
build what turned out to be even, an even bigger success. Right. Except that by the time we start,

167
00:18:07,000 --> 00:18:12,920
I became an assistant professor. We started to look at the problem. I realized it's way

168
00:18:12,920 --> 00:18:18,760
bigger than we think. Just mathematically speaking, Caltech 101 was not sufficient to,

169
00:18:18,760 --> 00:18:25,800
to power the, the algorithms. We decided to image, to do ImageNet. And that was the time people

170
00:18:25,800 --> 00:18:32,600
start to think we're, we're doing too much. Right. It's, it's just too crazy. The idea of

171
00:18:32,600 --> 00:18:40,200
downloading the entire Internet of images and mapping out all the English nouns was a little bit,

172
00:18:41,560 --> 00:18:47,240
I start to get a lot of pushback. I remember at one of the CVPR conference when I presented the

173
00:18:47,240 --> 00:18:55,000
early idea of ImageNet, a couple of researchers publicly questioned and said, said, if you cannot

174
00:18:55,000 --> 00:19:02,120
recognize one category of object, let's say the chair you're sitting in, how do you imagine,

175
00:19:02,120 --> 00:19:10,360
or what's the use of a dataset of 22,000 classes of 15 million images? Yeah. But, but in the end,

176
00:19:10,360 --> 00:19:15,640
you know, that giant dataset unlocked a lot of value for, you know, countless number of

177
00:19:15,640 --> 00:19:20,280
researchers around the world. So that, so that works. Well, I, I think it was the combination

178
00:19:20,280 --> 00:19:28,760
of betting on the right North Star problem and the data that drives it. So it was a fun process.

179
00:19:28,760 --> 00:19:35,240
Yeah. And, and, you know, to me, one of the, when I think about that story, it seems like one of

180
00:19:35,240 --> 00:19:41,400
those examples where, you know, sometimes people feel like they should only work on projects without

181
00:19:41,400 --> 00:19:47,160
the huge thing at the first outset, but I feel like for people working in machine learning,

182
00:19:47,160 --> 00:19:52,280
if your first project is a bit smaller, it's totally fine. Have a good win, use the learnings

183
00:19:52,280 --> 00:19:57,080
to build up to even bigger things. And then sometimes you get a, you know, ImageNet size win,

184
00:19:57,080 --> 00:20:04,280
all of it. Yeah. Well, but in the meantime, I think it's also important to be driven by

185
00:20:05,080 --> 00:20:12,040
an audacious goal, though, you know, you can size your problem or size your project as local

186
00:20:12,040 --> 00:20:19,880
milestones and, and, and so on along this journey. But I also look at some of our current students,

187
00:20:19,880 --> 00:20:29,080
they're so peer pressured by this current climate of publishing nonstop. It becomes more incremental

188
00:20:29,080 --> 00:20:37,000
papers to just get into a publication for the sake of it. And I, I personally always push my

189
00:20:37,000 --> 00:20:41,480
students to ask the question, what is the North Star that's driving you? Yeah, that's true. Yeah.

190
00:20:42,200 --> 00:20:45,560
And you're right. You know, for myself, when I do research over the years, I've always

191
00:20:46,280 --> 00:20:52,200
pretty much done what I'm excited about, where I want to, you know, try to push the view forward.

192
00:20:52,200 --> 00:20:56,200
Doesn't it don't listen to people, have to listen to people, let them shape your opinion. But in

193
00:20:56,200 --> 00:21:01,080
the end, I think the best research is, let the world shape their opinion. But in the end,

194
00:21:01,080 --> 00:21:05,400
drive things forward using their own opinion. Totally agree. Yeah. It's your own inner fire,

195
00:21:05,400 --> 00:21:12,840
right? So as your research program developed, you've wound up taking your, let's say, foundations

196
00:21:12,840 --> 00:21:19,240
in computer vision and neuroscience and applying to all sorts of topics, including your very visibly

197
00:21:19,240 --> 00:21:24,680
healthcare, looking at neuroscience applications. Would love to hear a bit more about that.

198
00:21:24,680 --> 00:21:31,320
Yeah, happy to. I think the evolution of my research in computer vision also kind of follows

199
00:21:31,320 --> 00:21:38,360
the evolution of visual intelligence in animals. And there are two topics that truly excites me.

200
00:21:38,360 --> 00:21:46,200
One is, what is a truly impactful application area that would help human lives? And that's my

201
00:21:46,200 --> 00:21:54,280
healthcare work. The other one is, what is vision at the end of the day about? And that brings me to

202
00:21:55,640 --> 00:22:02,840
the, trying to close the loop between perception and robotic learning. So on the healthcare side,

203
00:22:04,600 --> 00:22:11,000
you know, one thing, Andrew, there was a number that shocked me about 10 years ago when I met

204
00:22:11,000 --> 00:22:18,120
my long-term collaborator, Dr. Arnie Milstein at Stanford Medical School. And that number is about

205
00:22:18,120 --> 00:22:28,120
a quarter of a million Americans die of medical errors every year. I had never imagined a number

206
00:22:28,120 --> 00:22:36,360
being that high due to medical errors. There are many, many reasons, but we can rest assured most

207
00:22:36,360 --> 00:22:44,760
of the reasons are not intentional. These are her errors of unintended mistakes and so on.

208
00:22:44,760 --> 00:22:50,520
For example? That's a mind-boggling number. I think it's made about 40,000 deaths a year from

209
00:22:50,520 --> 00:22:54,280
automotive accidents, which is completely tragic. And this is even vastly tragic.

210
00:22:54,280 --> 00:23:00,120
I was going to say that. I'm glad you brought it up. Just one example, one number within that

211
00:23:00,120 --> 00:23:07,080
mind-boggling number is the number of hospital-acquired infection resulted in fatality is

212
00:23:07,880 --> 00:23:19,160
more than 95,000. That's 2.5 times than the death of car accidents. And in this particular case,

213
00:23:19,160 --> 00:23:26,200
hospital-acquired infection is a result of many things, but in large, a lack of good

214
00:23:26,200 --> 00:23:33,560
hand hygiene practice. So if you look at WHO, there has been a lot of protocols about clinicians'

215
00:23:33,560 --> 00:23:42,120
hand hygiene practice. But in real healthcare delivery, when things get busy and when the

216
00:23:42,120 --> 00:23:47,640
process is tedious and when there is a lack of feedback system, you still make a lot of mistakes.

217
00:23:48,280 --> 00:23:56,680
Another tragic medical fact is that more than $70 billion every year are spent in

218
00:23:57,640 --> 00:24:03,880
in fall resulted injuries and fatalities. And most of this happened to elderlies at home,

219
00:24:03,880 --> 00:24:10,040
but also in the hospital rooms. And these are huge issues. And when Arnie and I got together

220
00:24:10,680 --> 00:24:19,720
back in 2012, it was the height of a self-driving car, let's say not hype, but what's the word,

221
00:24:19,960 --> 00:24:27,640
right word, excitement in Silicon Valley. And then we look at the technology of smart sensing

222
00:24:27,640 --> 00:24:34,280
cameras, LiDARs, radars, whatever, smart sensors, machine learning algorithm,

223
00:24:34,280 --> 00:24:41,960
and holistic understanding of a complex environment with high stakes for human lives.

224
00:24:42,920 --> 00:24:48,760
I was looking at all that for self-driving car and realized in healthcare delivery,

225
00:24:49,560 --> 00:24:56,680
we have the same situation. Much of the process, the human behavior process of healthcare is in

226
00:24:56,680 --> 00:25:05,320
the dark. And if we could have smart sensors, be it in patient rooms or senior homes, to help our

227
00:25:05,320 --> 00:25:11,640
clinicians and patients to stay safer, that would be amazing. So Arnie and I embarked on this,

228
00:25:11,720 --> 00:25:18,440
what we call ambient intelligence research agenda. But one thing I learned, which probably will lead

229
00:25:18,440 --> 00:25:28,040
to our other topics, is as soon as you're applying AI to real human conditions, there's a lot of human

230
00:25:28,040 --> 00:25:33,800
issues in addition to machine learning issues. For example, privacy. And I remember reading some

231
00:25:33,800 --> 00:25:38,200
of your papers with Arnie and found it really interesting how you could build and deploy

232
00:25:38,200 --> 00:25:45,560
systems that were relatively privacy preserving. Yeah, well, thank you. Well, the first iteration

233
00:25:45,560 --> 00:25:52,120
of that technology is we use cameras that do not capture RGB information. You've used a lot of that

234
00:25:52,120 --> 00:25:58,840
in self-driving cars, the depth cameras, for example. And there you preserve a lot of privacy

235
00:26:00,360 --> 00:26:05,960
information just by not seeing the faces and the identity of the people. But what's really

236
00:26:06,040 --> 00:26:13,080
interesting over the past decade is the changes of technology is actually giving us a bigger tool set

237
00:26:13,080 --> 00:26:21,320
for privacy preserved computing in this condition. For example, on device inference,

238
00:26:21,960 --> 00:26:27,080
you know, as the chip's getting more and more powerful, if you don't have to transmit any data

239
00:26:27,080 --> 00:26:32,760
through the network and to the central server, you help people better. Federated learning,

240
00:26:32,760 --> 00:26:40,520
we know it's still early stage, but that's another potential tool for privacy preserved computing

241
00:26:40,520 --> 00:26:47,320
and then differential privacy and also encryption technologies. So we're starting to see

242
00:26:47,960 --> 00:26:54,840
that human demand, you know, privacy and other issues is driving actually a new wave of machine

243
00:26:54,840 --> 00:27:00,440
learning technology in ambient intelligence in healthcare. Yeah, that's great. Yeah, I've been

244
00:27:00,440 --> 00:27:06,440
encouraged to see the, you know, real practical applications of differential privacy that are

245
00:27:06,440 --> 00:27:11,320
actually real. Federated learning, as you said, probably the PR is a little bit ahead of the

246
00:27:11,320 --> 00:27:16,120
reality, but I think we'll get there. But it's interesting how consumers in the last several

247
00:27:16,120 --> 00:27:21,160
years have fortunately gotten much more knowledgeable about privacy and are increasingly

248
00:27:21,640 --> 00:27:26,360
so important. I think the public is also making us to be better scientists.

249
00:27:27,080 --> 00:27:32,840
Yeah, yeah. And I think, and I think ultimately, you know, people understanding AI holds everyone,

250
00:27:32,840 --> 00:27:39,080
including us, but holds everyone accountable for really doing the right thing. Yeah, yeah. And,

251
00:27:39,080 --> 00:27:44,920
you know, and on that note, one of the really interesting pieces of work you've been doing has

252
00:27:44,920 --> 00:27:53,320
been leading several efforts to help educate legislators or help governments, especially U.S.

253
00:27:53,320 --> 00:27:58,920
government, work towards better laws and better regulation, especially as it relates to AI.

254
00:28:00,200 --> 00:28:04,520
This sounds like very important. And I suspect some days of the week,

255
00:28:04,520 --> 00:28:07,560
I would get somewhat frustrating work, but we'd love to hear more about that.

256
00:28:08,360 --> 00:28:15,560
Yeah, so I think first of all, I have to credit many, many people. So about four years ago,

257
00:28:15,560 --> 00:28:21,640
and I was actually finishing my sabbatical from Google time, I was very privileged to work with

258
00:28:21,640 --> 00:28:31,880
so many businesses, you know, enterprise developers, just just a large number and variety of vertical

259
00:28:31,880 --> 00:28:40,280
industries and realizing AI's human impact. And that was when many faculty leaders at Stanford

260
00:28:40,280 --> 00:28:46,040
and also just our president provost, former president and former provost all get together

261
00:28:46,200 --> 00:28:54,360
realize there is a role, historical role that Stanford needs to play in the advances of AI.

262
00:28:54,360 --> 00:29:02,600
We were part of the part of the birthplace of AI, you know, a lot of work our previous

263
00:29:03,240 --> 00:29:09,000
generation have done and a lot of work you've done and some of our work I've done led to AI,

264
00:29:09,000 --> 00:29:18,360
today's AI, what is our historical opportunity and responsibility? With that, we believe that the

265
00:29:18,360 --> 00:29:24,440
next generation of AI education and research and policy needs to be human centered. And

266
00:29:25,240 --> 00:29:33,240
having established the Human Center Institute, what we call HAI, one of the work that really took me

267
00:29:33,240 --> 00:29:43,480
outside of my comfort zone or any expertise, is really a deeper engagement with policy thinkers

268
00:29:43,480 --> 00:29:48,920
and makers. Because, you know, we're here in Silicon Valley and there is a culture in Silicon

269
00:29:48,920 --> 00:29:55,720
Valley is we just keep making things and the law will catch up by itself. But AI is impacting human

270
00:29:55,720 --> 00:30:06,440
lives and sometimes negatively so rapidly that it is not good for any of us if we the experts

271
00:30:07,080 --> 00:30:12,760
are not at the table with the policy thinkers and makers to really try to make this technology

272
00:30:12,760 --> 00:30:16,920
better for the people. I mean, we're talking about fairness, we're talking about privacy.

273
00:30:17,640 --> 00:30:27,080
We also are talking about the brain drain of AI to industry and the concentration of data and

274
00:30:27,080 --> 00:30:36,600
compute in a small number of technology companies. All these are really part of the changes of our

275
00:30:36,600 --> 00:30:43,240
time. Some are really exciting changes, some have profound impact that we cannot necessarily

276
00:30:43,880 --> 00:30:51,160
predict yet. So one of the policy work that Stanford HAI has very proudly engaged in is

277
00:30:51,160 --> 00:30:59,000
we were the one of the leading universities that lobbied a bill called the National AI Research

278
00:30:59,000 --> 00:31:05,720
Cloud Task Force Bill. It changed the name from Research Cloud to Research Resource. So now the

279
00:31:05,720 --> 00:31:12,680
bill's acronym is NAIR, National AI Research Resource. And this bill is calling for a task force

280
00:31:13,480 --> 00:31:20,200
to put together a roadmap for America's public sector, especially higher education and research

281
00:31:21,480 --> 00:31:30,280
sector to increase their access to resource for AI compute and AI data. It really is

282
00:31:30,360 --> 00:31:39,800
aimed to rejuvenate America's ecosystem in AI innovation and research. And I'm on the 12-person

283
00:31:39,800 --> 00:31:47,640
task force for under Biden administration for this bill. And we hope that's a piece of policy that

284
00:31:48,520 --> 00:31:56,120
is not a regulatory policy, it's more an incentive policy to build and rejuvenate ecosystems.

285
00:31:56,120 --> 00:32:01,320
I'm glad that you're doing this to help shape U.S. policy and this type of making sure enough

286
00:32:01,320 --> 00:32:07,160
resources allocated to ensure healthy development of AI feels like this is something that every

287
00:32:07,160 --> 00:32:15,000
country needs at this point. So just from the things that you are doing by yourself, not to

288
00:32:15,000 --> 00:32:20,520
speak of the things that the global AI community is doing, there's just so much going on in AI

289
00:32:20,520 --> 00:32:26,520
right now. So many opportunities, so much excitement. I found that for someone getting

290
00:32:26,520 --> 00:32:30,280
started in machine learning for the first time, sometimes there's so much going on,

291
00:32:30,280 --> 00:32:37,240
it can almost be a little bit overwhelming. What advice do you have for someone getting

292
00:32:37,240 --> 00:32:43,640
started in machine learning? Good question, Andrew. I'm sure you have great advice. You're one of the

293
00:32:43,640 --> 00:32:50,600
world-known advocates for AI machine learning education, so I do get this question a lot as

294
00:32:50,600 --> 00:32:56,920
well. And one thing you're totally right is AI really today feels different from our time.

295
00:32:57,960 --> 00:33:04,520
During our time... Just further, I thought, now is still our time. That's true, when we were

296
00:33:05,400 --> 00:33:13,160
starting in AI. I love that, exactly. We're still part of this. When we get started,

297
00:33:13,240 --> 00:33:20,120
entrance to AI and machine learning was relatively narrow. You almost have to start from computer

298
00:33:20,120 --> 00:33:27,560
science and go. As a physics major, I still had to wedge myself into the computer science track

299
00:33:27,560 --> 00:33:34,840
or electrical engineering track to get to AI. But today, I actually think that there is a

300
00:33:34,920 --> 00:33:41,880
remaining aspect of AI that creates entry points for people from all walks of life.

301
00:33:43,000 --> 00:33:51,560
On the technical side, I think it's obvious that there's just an incredible plethora of

302
00:33:51,560 --> 00:33:58,280
resources out there on the internet, from Coursera to YouTube to TikTok to GitHub.

303
00:33:59,240 --> 00:34:06,280
There's just so much that students worldwide can learn about AI and machine learning compared

304
00:34:06,280 --> 00:34:12,360
to the time we began learning machine learning. And also, any campuses, we're not talking about

305
00:34:12,360 --> 00:34:19,080
just college campuses. We're talking about high school campuses. Even sometimes earlier, we're

306
00:34:19,080 --> 00:34:29,640
starting to see more available classes and resources. I do encourage those of the young

307
00:34:29,640 --> 00:34:37,960
people with a technical interest and resource and opportunity to embrace these resources,

308
00:34:37,960 --> 00:34:45,640
because it's a lot of fun. But having said that, for those of you who are not coming from a technical

309
00:34:45,640 --> 00:34:51,960
angle, who still are passionate about AI, whether it's the downstream application or

310
00:34:51,960 --> 00:35:00,520
the creativity it engenders, or the policy and social angle or important social problems,

311
00:35:00,520 --> 00:35:09,480
whether it's digital economics or the governance or history, ethics, political sciences,

312
00:35:10,200 --> 00:35:18,280
there, I do invite you to join us, because there is a lot of work to be done. There's a lot of

313
00:35:20,040 --> 00:35:27,480
unknown questions. For example, my colleague at HAI are questioning, are trying to find answers on,

314
00:35:27,480 --> 00:35:36,040
how do you define our economy in the digital age? What does it mean when robots and software

315
00:35:36,200 --> 00:35:44,040
are participating in the workflow more and more? How do you measure our economy? That's not an AI

316
00:35:44,040 --> 00:35:50,520
coding question. That is an AI impact question. We're looking at the incredible advances of

317
00:35:50,520 --> 00:35:57,960
generative AI, and there will be more. What does that mean for creativity and to the creators,

318
00:35:57,960 --> 00:36:06,760
from music to art to writing? There is a lot of concerns, and I think it's rightfully so,

319
00:36:06,760 --> 00:36:14,040
but in the meantime, it takes people together to figure this out, and also to use this new tool.

320
00:36:16,280 --> 00:36:22,360
In short, I just think it's a very exciting time, and anybody with any walks of life,

321
00:36:23,000 --> 00:36:25,960
as long as you're passionate about this, there's a role to play.

322
00:36:26,680 --> 00:36:30,760
Yeah, and that's really exciting. We're going to talk about economics, think about my conversations

323
00:36:30,760 --> 00:36:37,720
with Professor Eric Brynoson, studying the impact of AI on the economy. From what you're saying,

324
00:36:37,720 --> 00:36:44,840
and I agree, it seems like no matter what your current interests are, AI is such a general purpose

325
00:36:44,840 --> 00:36:52,120
technology that the combination of your current interests and AI is often promising. I find that

326
00:36:52,120 --> 00:36:57,880
even for learners that may not yet have a specific interest, if you find your way into AI,

327
00:36:57,880 --> 00:37:03,720
start learning things, often the interests will evolve, and then you can start to craft your own

328
00:37:03,720 --> 00:37:10,200
path. And given where AI is today, there's still so much room and so much need for a lot more people

329
00:37:10,200 --> 00:37:16,040
to craft their own paths to do this exciting work that I think the world still needs a lot more of.

330
00:37:16,040 --> 00:37:20,760
Totally agree, yeah. So, one piece of work that you did that I thought was very cool was starting

331
00:37:20,760 --> 00:37:26,520
a program, initially called Sailors, and then later AI for all, which was really reaching out

332
00:37:26,520 --> 00:37:32,200
to high school and even younger students to try to give them more opportunities in AI,

333
00:37:32,200 --> 00:37:35,880
including people of all walks of life. We'd love to hear more about that.

334
00:37:35,880 --> 00:37:46,280
Yeah, well, this is in the spirit of this conversation. That was back in 2015. There was

335
00:37:46,360 --> 00:37:52,520
starting to be a lot of excitement of AI, but there was also starting to be this talk about

336
00:37:53,800 --> 00:38:00,120
killer robot coming next door, terminators coming. And I was, at that time, Andrew,

337
00:38:01,080 --> 00:38:07,800
I was the director of Steveria Lab, and I was thinking, you know, we know how far we are from

338
00:38:07,800 --> 00:38:13,400
terminators coming, and that seemed to be a really a little bit of far-fetched concern,

339
00:38:13,480 --> 00:38:19,320
but I was living my work life with a real concern I felt no one was talking about,

340
00:38:19,320 --> 00:38:25,080
which was the lack of representation in AI. At that time, I guess after Daphne has left,

341
00:38:25,080 --> 00:38:35,000
I was the only woman faculty at Steveria Lab, and we're having very small, around 15% of women

342
00:38:35,000 --> 00:38:42,040
graduate students, and we really don't see anybody from the underrepresented minority groups

343
00:38:42,040 --> 00:38:50,280
in Stanford AI program, and this is a national or even worldwide issue, so it wasn't just Stanford.

344
00:38:50,280 --> 00:38:55,800
Frankly, it still needs a lot of work today. Exactly. So how do we do this? Well, I got together

345
00:38:55,800 --> 00:39:05,000
with my former student Olga Rosakowski and also a long-term educator of STEM topics,

346
00:39:05,000 --> 00:39:12,520
Dr. Rick Sommer from a Stanford pre-collegiate study program, and thought about inviting

347
00:39:12,520 --> 00:39:18,920
high schoolers at that time, women, high school young women, to participate in a summer program

348
00:39:18,920 --> 00:39:28,360
to inspire them to learn AI, and that was how it started in 2015 and 2017. We got a lot of

349
00:39:28,360 --> 00:39:34,760
encouragement and support from people like Jensen and Laurie Huang and Melinda Gates,

350
00:39:34,760 --> 00:39:41,960
and we formed the National Nonprofit AI for All, which is really committed to training or

351
00:39:43,480 --> 00:39:51,240
helping tomorrow's leaders, shaping tomorrow's leaders for AI from students of all walks of

352
00:39:51,240 --> 00:39:56,840
life, especially the traditionally underserved and underrepresented communities, and

353
00:39:57,800 --> 00:40:05,480
you know, till today, we've had many, many summer camps and summer programs across the country. More

354
00:40:05,480 --> 00:40:16,120
than 15 universities are involved, and we have online curriculum to encourage students as well as

355
00:40:16,120 --> 00:40:23,720
college pathway programs to continue support these students' career by matching them with

356
00:40:23,720 --> 00:40:30,760
internships and mentors, so it's a it's a continued effort of encouraging students of all walks of

357
00:40:30,760 --> 00:40:35,880
life. And I remember back then, I think your group was printing these really cool t-shirts that

358
00:40:35,880 --> 00:40:42,040
asked the question, AI will change the world, who will change AI, and I thought the answer of

359
00:40:42,040 --> 00:40:47,640
making sure everyone can come in and participate, that was a great answer. Yeah, still an important

360
00:40:47,640 --> 00:40:55,080
question today. So that's a great thought, and I think that takes us to the end of the interview.

361
00:40:56,120 --> 00:41:03,000
Any final thoughts for people watching this? Still, that this is a very nascent field.

362
00:41:03,000 --> 00:41:08,280
As you said, Andrew, we are still in the middle of this. I still feel there's just so many questions

363
00:41:08,280 --> 00:41:14,920
that, you know, I wake up excited to work on with my students in the lab, and I think there's a lot

364
00:41:14,920 --> 00:41:21,080
more opportunities for the young people out there who want to learn and contribute and shape

365
00:41:21,080 --> 00:41:28,040
tomorrow's AI. Well said, that's very inspiring. Really great to chat with you, and thank you.

366
00:41:28,040 --> 00:41:41,160
Thank you, it's fun to have these conversations.

