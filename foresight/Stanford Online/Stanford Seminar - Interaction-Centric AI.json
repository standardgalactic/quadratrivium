{"text": " Being a speaker at this seminar series is, I mean, it means a lot to me personally. When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but I didn't really have that many resources, so I had to rely on online resources. This seminar series recordings have been posted online, and I think I've watched pretty much everything to really learn about HCI. And then I came as a master's student here in 2008 and took 547 pretty much for the entire two years I was here. And now I feel great that I get a chance to speak as a speaker, so this is great. Today I want to talk about interaction centric AI. This is a reprise of the New Europe's keynote talk that I gave two weeks ago in front of thousands of AI researchers. I tried to reframe it a little bit so that it's more customized for an HCI audience rather than an AI audience. But the idea is that I want to think about using human AI interaction at the center of developing AI technologies. And of course, I don't have to preach to the choir that human AI interaction actually matters, but diving deeper based on my experience of building these interactive systems in different contexts, like education, discussion, decision making, I want to dive deeper and report some of the detailed interactions that we've been observing and learning from. And think about what it means to design human AI interaction in various contexts and what are some action items moving forward as a community. So let's first start with some definitions and terms. I would say the dominant paradigm for developing AI technologies has been model centric. The idea is to build a model with high accuracy and we want to evaluate it against unseen examples for its generalizability. And benchmarks have been great in that they could help us competitively compare different models' performances, which could be useful in making scientific advances possible. And more recently, people have been talking a lot about data centric AI, where the idea is using this nicely performing model, what is a good, sort of robust and efficient data pipeline around it in terms of collection of the data, processing of it, cleaning of it, so that the model actually performs really well in different contexts. And here the focus is acquiring quality data and setting the pipeline in a way that really helps the machine perform its best. And these two paradigms are great, but then what is slightly missing is the user who's using these AI technologies and those who are affected by what the AI systems give you. So interaction centric AI is sort of my term in some contrast to model centric and data centric AI, where the goal would be basically what HCI researchers do in this context, like improving the user experience by building usable and useful applications. And the unit that we often grapple with is a human AI interaction. And you might be wondering, is this some sort of like marketing term? Like how is it different from human centric AI we've been talking all about? It's largely similar, so I'm not trying to say I invented this new term or anything, but I want to focus our attention to the interaction that is happening between humans and AI and the complex relationships and the dynamics that are happening between the two, rather than focusing on just the humans or machine alone. So I can say that that's the focus of where my discussion will be today. So let's say you're this AI researcher and your team has built this amazing model. So this is actually something that I copied and pasted from one of the diffusion models papers. I don't know what they actually mean, some of them I understand. But basically, this is what you have as an AI researcher. But what would a person using this kind of AI want to do with it? Here's an example. So this is a Twitch streamer in South Korea who was trying to use this diffusion-based text to image generation model to create this image of an animated character eating ramen with chopsticks with noodles around the character. So this is the roughly sketched out goal that the user company has. And he ended up spending two hours fiddling with text-based prompts to get at the final image that he wants. And this is somewhat similar to what Manish shared a couple of weeks ago at the HAI conference in terms of what he had to do with the prompt-based interface. And here, the entire two-hour journey was live streamed. So I want to kind of share a quick summary of what happened in that stream. And of course, we need something in the middle to bridge between the technology and the human user. And that's what we have, the prompt-based interface and interaction that's happening between the two. So the streamer started by something simple and obvious. The prompt says, eating ramen, and this is what he got. It's okay, it's kind of there, but the bowl is perhaps too large. The chopsticks are all to be placed. And he heard from somewhere that adding a full sentence might make things better. So he goes, she is eating ramen. She is eating ramen, for sure, but you can see that something's not quite right. So he keeps going on by adding more descriptions. And the prompt is definitely getting longer. And it seems that the AI is not quite getting how chopsticks should be used and how many should be used. So he keeps adding these descriptions to really explain what it means to use chopsticks. And to be fair, there's hair, there's chopsticks, there's noodles. So it's in computer graphics. Dealing with human hair, I heard, is a really tough challenge. And maybe for AI, it's also kind of struggling to deal with all these similar looking objects. And it doesn't really seem to get how to differentiate between chopsticks and noodles. And another interesting aspect was that since it was a live stream, the viewers were actively participating in recommending new prompts to try out, sharing their interpretations. And this is somewhat of a collaborative mental model construction process, if you will, as a group of people. They are really trying to figure out what's going on. And now the prompt is five lines long. And the service that this streamer was using was supporting variations, where you could pick an image and say create some variations. And he was referring to this interaction as variation gacha. So gacha is a Japanese word for like a random box or blind box. And this kind of tells us that how unpredictable this sort of interface is. Once you hit the generate button, the user doesn't really have a good sense of knowing what to expect. And this is the actual stream, as you can see, like his praying, and which also tells us about the usability of this sort of system. He doesn't have a good way of knowing what to expect, so that he actually has to pray. After two hours of hard work, this is the final image that he landed. And it looks pretty good, and he claims victory. But then look at what he had to do at the top, right? At the top, there are seven lines of prompt that he had to write. And arguably, this is natural language. But I would say this is really pseudo-natural language. And so this is basically the experience that he had to go through. So is this a good interface? And I sort of got inspired by Manish's discussion of discussing the usability of these text prompt-based interfaces. There are some good elements, right? It's quite intuitive. You can use natural language, or you believe natural language could be used. And the output is presented in a visual manner, which helps you kind of understand whether you got the image that you like or not, so that you can sort of debug. And there are some interactions that are supported, like variations and seeds and like words that should not be used and things like that. But there are many ways in which this interface actually fails to support what the actual user wants. He had to rely on trial and error. And just the fact that he had to spend two hours to get that image seems to suggest that something is really wrong. And of course, it was not really predictable and lack of specific feedback on the effect of what specific words in the prompt had influenced on the final outcome. These links were often missing, which made it really difficult. So is this really just a problem for these text-based prompt-based systems? I would say every AI application faces these interaction challenges. On the user side, when they first encounter these systems, they often have to struggle to kind of figure out how to make it work. Often people resort to misusing it, abusing it, and learning takes a long time. And part of it is really a design challenge. And we've seen other examples like this, where people don't have a good sense of what's happening in this algorithmically-generated systems and AI-powered systems. Like in the famous study of Facebook newsfeed users, more than half of the participants were not aware of the newsfeed curation algorithm's existence at all, which is far from being true. And on the right, what you see is in the pathologists' diagnosis scenario, often they would rely on some notion of similarity. So there are these algorithms that are designed to help people find similar images, but then the realization that the researchers had was that people had different notions of similarity. So a singular notion of similarity that was used in building an algorithm would not really suffice. So what they ended up doing was to support three different types of similarity interaction, and the user was able to kind of transition between these different terms in a fluid manner, which really gives more control and agency on the user side. And these, you know, put in a more simple sort of diagram manner, whether you are a creator or Facebook user pathologist, you seem to have some kind of a mental model of how the system works, a very sort of a classical sort of gap between what the user wants and the system wants. And obviously, the system is not behaving in a way that you really want. And this gap arguably seems to be larger with these more complex black box and deep learning based systems. And AI community has been tackling this problem as well. And, you know, some of the folks have been framing this as an alignment problem, which is about aligning the model's behavior with human intent. And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open AI's response to the alignment problem, where their idea is, in addition to the, you know, basic large language model that they have, they would add this fine tuning layer with human feedback, which often involves asking people whether, you know, they were happy with the results they got. And the system kind of uses that feedback to train a reinforcement learning agent to do the fine tuning so that the resulting text aligns better with what the user wants. And they were seeing some success from it. And a quote from the paper is that making language models bigger does not inherently make them better at following a user's intent. And aligning language models with user intent on a wide range of tasks by fine tuning with human feedback. And of course, there's been a lot of discussion about whether this is really the most promising way to, you know, involve humans or alignment problem. But I think this is some progress towards that direction. But all of these examples, I would say, basically lead us to revisit these classical notions of Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s, right? As a user, they want to know what's going on with the system, and they want to have more control and agency. And on the evaluation side, when AI gives you some kind of result, they want to be able to understand it, interpret it, and want to get some explanation of it. And as an HCI researcher who's building these interactive systems, I feel like in often cases, I try to bridge these gaps. I come up with new ways of designing these social interactions and human AI interactions in a way that tries to bridge these gaps. And these are just some of the systems that I've been developing in different application domains. And I think many of them have somewhat succeeded in bridging these gaps. But other times, to be honest, we haven't done a good job of doing that. So what I want to do for the remaining time for this talk is to share some of these lessons, and some of them from positive experiences, but other times, bitter experiences by something that we haven't really done a good job of. And the main message that I want to send across is that beyond these point solutions for this system that works in this particular context, we've seen some success, I think as a field, we really need to start thinking about, can we do something more systematic and sustainable? Or empower designers and developers in thinking about can we develop these AI applications that are more usable and useful for more groups of people rather than having to reinvent the wheel each time someone has to develop these applications. And I think we're seeing too many of these cases where people are like, there's this cool model, let's build something around it, and it just gets released in a few days and realizes that people want it in a completely different way, people abuse it, a few days later, it goes down. We're seeing too many of these failure cases. So from the HCI point of view, I think HCI research can really advance this interaction-centric AI by contributing these generalizable building blocks for designing these systems and interface affordances. And AI research can also advance by embracing the idea of interaction-centric AI by rethinking models, architecture design, benchmarks, metrics, and research process. The part of it has to involve sort of broadening the perspective beyond just thinking about the model and the output that it generates to think about the users behind those and their mental models. And often there's not just a single user, but a group of user, community of user, a society of users. And there's also the temporal dimension, like before the user comes in and tries to use a system, we should be asking the questions about like, what's the task and who are these users and why and how. And during the interaction, we need to be thinking about presentation visualization. And the other way around as well, like interpretable results are being presented to the user. Do they have a way to provide feedback to the system? And also, it's never going to be just a single use, right? People would want to come back and use a system for a sustained amount of time. In those cases, people's mental model would evolve. And what does it mean for the system? And so I think this is sort of the ecosystem that I have in mind. And with these, I want to dive into these specific examples where we designed human-AI interactions. And I identify four major challenges in terms of human-AI interaction. The first one is about bridging the accuracy gap. So I'm on my sabbatical now. I'm working with this startup called Ringle, where they are basically Uber for language learning. They are matching tutors and tuties, and they have this video-based language tutoring session. So what we try to do here is to build this diagnostic service based on analyzing the chat-based tutoring session to give people personalized feedback and suggestions for improvement. But instead of going into the details of the service itself, I want to touch upon the case that we ran into when we were trying to run this automated speech recognition AI, which is crucial in sort of turning the video-based chat into text format, which is really required for us to run all these diagnostic algorithms on top of. And the standard metric of success in ASR would be word error rate, how correctly it can recover the original text. And on the tutor side, when we ran ASR on like hundreds and thousands of sessions, the average word error rate was around 8%. Can you take a guess as to what the number would have been for students? Obviously there's this white margin that's quite high, so you can imagine, 30. Yeah, we're seeing 23. So there's quite a bit of a gap. And this is an example of an accuracy gap where different groups of users are getting disproportionate results from the same AI. And the gap actually widens if we look at like the best tutor and the worst student when it comes to the performance of these models. But in terms of thinking about the interaction that these people are trying to have with this AI, I would argue that the students are the ones who really need this AI to work. Based on the accuracy of this AI, they want to kind of look at where they succeeded and failed and they want to learn and reflect. And with this low accuracy, they would really be struggling to come up with good action items and they might be frustrated, they might lose trust on the system. But interestingly, a lot of focus when it comes to model development is that we seem to be focusing on the 6%, like making the 6% better instead of narrowing the gap between 6 and 36%. And we have to really be asking like, what is the most important question in this context? And are we really focusing on the most important question here? And we see these other examples too, where Tyra and others have studied the machine translation that is being used in emergency rooms when it comes to discharge statements that are presented to patients and patients' families. And we see a huge disparity between different languages. And in the natural language processing community, this support for low resource languages has been a topic for research and there has been great efforts. And on the right is a famous example of gender shades, where the gender classification algorithm shows, again, an accuracy disparity between darker skin female versus lighter scale male. And of course, these diversity and inclusion efforts and low resource language support research in the AI community and in the community have been tackling these issues of accuracy gap, of course. But then I would argue that they could advance further by embracing more interaction-centric approach in trying to really see how in the real world people are interacting with these results and what kind of actual struggles that they have because of poor or good AI accuracy and what, as a community, how can we define the problem that's most important. And conceptually speaking, I feel like a good analogy might be the ceiling and floor analogy. The ceiling would be this primary user group who gets the best part of AI. And floor would be secondary user group who is disproportionately getting more negative impact of the same AI. And there's this accuracy gap. And often I feel like taking a model-centric approach incentivizes people and researchers to work on raising the ceiling. There could be a couple reasons for this. First of all, that's the sota number you get, which might be what you need to publish a paper out of it. Or the benchmarks that you're working with do not really have much data on the floor side. It's maybe more focused on the ceiling side. And that's why the ceiling is there in the first place. So it might be just incentivizing people to continue to push the boundaries of ceiling. And as a result, what we see is a lot of a widened accuracy gap. And if we take a more interaction-centric approach, I would argue that if we identify that narrowing this gap is a more important problem, we can narrow this accuracy gap. And it's not just a matter of accuracy, if you think about it. It's about experience, benefit, and value that people get out of interacting with this AI. So there was a first challenge about the accuracy gap and how thinking about how people interact with this AI can help us identify what problems are worth tackling. And second of all, I want to talk about when people actually use AI. And one of the anti-patterns of human-AI interaction is that people just stop using AI altogether or abandon it, which is something you might want to avoid as a system designer. And that's why it's important to think about how do we incentivize people to work with AI? And in most cases, people abandon using AI because it's not really giving them concrete value that they expect. And we explore this in the context of online education in this system called XS. So the problem that we wanted to focus here is that in online, let's say you want to learn some new concept like probability, there are lots of problems and answers you can find. But finding good explanations is surprisingly difficult. And generating high-quality explanations is costly and resource-intensive console. So we wanted to tackle this problem by building this online education platform, where people are presented with a problem and they solve this problem, they submit an answer, and they see an example that's presented by the system and they get a chance to rate how helpful the explanation that they saw was. And then they are getting a chance to sort of self-explain their own answer. So this is a pedagogically meaningful activity to be able to sort of explain your thought process, externalize it, and lots of research supports doing self-explanation. Okay, so fairly simple sort of front end in terms of the learner's experience. So what's happening behind the scene is that the system is collecting these explanations and ratings from learners, right? Since it's a live system, new learners keep coming in and provide new ratings and explanations. And we formulate this in a multi-armed bandit manner, which means that as a new explanation comes into the system, as a byproduct of humans' learning activity, a new arm gets added to the system. And what the system is doing is to determine this dynamic policy for what the most effective explanation would be for the next learner coming into the system. So if you're familiar with the reinforcement learning of concepts, we are navigating exploitation and exploration trade-off. Exploitation in the sense that the system wants to present the best explanations to the next learner coming into the system, but the system doesn't really know what the best explanations are until it collects some amount of ratings from people. So it has to do some exploration where it should collect this data. And to solve that, we use a technique called Thomson sampling. So what happens is the system keeps track of these policies and when a new explanation comes in and ratings come in, these things get updated and the policy of probabilistic policy gets updated so that it uses this distribution to determine what explanation to show to the next learner. So when we ran a study, these access-generated explanations were helpful in terms of helping people learn better. So when we compared against presenting no explanation at all and measured differences between pre-test and post-test results, we were seeing that people were gaining 3% increase in their scores. So just getting a chance to rethink the problem, I think still gave them some increase in their scores. And when they were seeing the instructor-generated explanation, which is, I guess, somewhat of an ideal case or the standard case, we're seeing 9% increase and with access, we're seeing 12% increase. So between these two conditions, it was not statistically significantly different, but there were certainly cases where access was picking explanations from learners that were even more powerful than the instructor-generated ones. So in this system, if we were to take a more model-centric approach, I think we might have built an AI that automatically generates high quality explanations. But instead, in taking an interaction-centric approach, I think the system we created is basically this co-learning system where the user, the learner, and AI are learning at the same time in a single system. So it's sort of an education-focused system of the game-with-the-purpose kind of setting, where organic benefits are provided to people who are interacting with the system, and the system is learning something useful out of it. And this is basically the mechanism that we have in that both sides are learning and explanation and feedback are establishing this loop. And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing, where learners as a crowd coming into the system are basically doing this by getting their individual benefits while they're providing something useful for the system to learn and do its thing better. So since then, I've been expanding this idea to a broad array of applications. So for example, can we use this kind of co-learning ideas to summarize how-to videos in terms of steps and sub-steps, or building a concept map out of an instructional video that shows relationships between different concepts, or helping learners come up with the solution plans in algorithmic problem-solving settings. And other researchers have taken on this idea in different application contexts as well. So I think we can try to really generalize this kind of idea of co-learning system design in different contexts. Moving on to the third challenge, is about beyond a single user. And often we think about a single user, a single AI interacting with each other. In real life, it would be much more complex and there would be diverse configurations. So how can we consider these social dynamics? And there could be various types of social dynamics, but one specific instance that we did in was group-based, chat-based discussion in a group. So we built this system called Solution Chat, where the idea is what if this AI agent could recommend real-time moderation messages to a group. So let's say a group is discussing, you know, what to do for the company retreat next week, and they're having a discussion. The system, in real time, based on the understanding of the discussion context, and also knowing what kind of messages would be useful for the group, based on our sort of literature survey of discussion and discussion-based education, it presents these recommendation messages, like any more ideas, or can this person share their opinions, you have been quiet for a while, or should we try to move on to the next stage, or thank you for your opinion. So these kinds of moderation messages are presented by the system in real time, just like what you get in smart replies in Gmail, for example. And as a moderator, you can just choose to accept any of the messages that you like, and discard the ones that you don't like. So a quick summary of the results of what we saw was that in our lab study with 55 users in 12 different groups, when we compared how many moderation messages were used in different groups, when we compared the baseline condition without these real-time recommendations versus solution chat or system, we're seeing a significant increase in the number of moderation messages that were present in the chat stream in the solution chat condition. But interestingly, you can see that the users manually typed moderation messages were actually decreasing in solution chat, but many of them were replaced by the accepting AI-generated recommendations. And furthermore, we had this great opportunity to actually release this system to over 2,000 real-world users in a corporate education setting. So during COVID, a lot of these corporate education programs moved online, and this company that we worked with wanted to use these kinds of system to moderate hundreds of chat rooms that were doing discussion-based activity. And not surprisingly, just like the very first live stream prompt example that I mentioned, here again, people were collaboratively trying to understand the capabilities and limitations of AI when they were first presented with the system. So they were using the chat to test different messages, often things that they believe would be not working, and they would be sharing the results of, oh, this is working, this is not working, I think this does this well, but not that well. And it seems as a group does this kind of testing in the very first phase of their usage of the system, people have this shared expectation of the system, and that seems to sort of determine their further interactions with the system. And it was also notable how different groups had different expectations based on their limited experimentation that they did in the beginning. And there were some interesting social dynamics that we observed as well, like in how people use these AI recommendations to socially interact with each other. Some people were using AI as proxy. So one of the quotes that we had was, I didn't want to directly ask the person to stop talking. So the person relied on the AI recommended message to kind of send it. They still chose to send it, but it was their way of kind of softening the potential sort of dispute with the person. Other people were using AI as a reference. So what we were seeing is that it was a fairly simple technical pipeline that we had. So it was just a canned response. So people were sometimes not really fond of the tone of the message, style of the message that we showed. So the person said, I found no fun in the recommended messages because all the messages look the same. So in those cases, what people did was they still adopted the idea from the recommendation, but then rewrote it so that it feels more personal, and it feels more like it's coming from them, not AI. In other cases, AI seems to be adding a social burden. So in this excerpt, so one of the people said, I'm doubtful about the credibility of AI. And then the moderator picks this AI recommendation. Thanks for your opinion. Another person says, I also think negatively. Thanks for your opinion. Thanks for sharing a good opinion. Shall we go to the next topic? And then the moderator realizes he might have clicked, accept way too many times, and it was a little unnatural. So he stopped to kind of clarify and apologize for my unnatural words as I'm using AI recommendations. So while we were seeing how people were saving their time and cognitive effort in moderation could have decreased, it might have actually introduced other types of burden at the same time. Again, so if we were to build this kind of system in a more model-centric manner, I think a good alternative might have been automated discussion moderation, where AI would actually do all the moderation by itself. But instead, we chose to take a more AI-assisted moderation for obvious reasons. Users want to have more agency and control, and they wanted to keep their style of communication. So instead of handing over the entire control to AI, we still sort of gave that control to the human moderator who could kind of use it as an additional resource. Okay, so there was a third challenge. And moving on to the final challenge of supporting sustainable engagement. Here, the concern is that we want to think beyond this single session usage. And over time, how people react to these systems might change, their mental model might change, and how AI actually works might change. So we need to really think about this temporal dimension more carefully. And for this thread, we investigated in the context of novices making changes to websites that they're seeing. So for example, you might have a case where you visited this website that colors hurt your eyes, or you couldn't really find this button or tap it because it's too small, maybe you want to make it larger. But then people without expertise in HTML and CSS have difficulty doing this. So we thought by leveraging the power of large language models and so on, maybe we can support more natural language queries. So if a person says tone down the text, the system can kind of display these style recommendations that they can explore and select from that are about toning down the text. So the way the system works is if the user clicks and says make this larger, the system presents a set of design attributes that are about making something larger. And the user can say emphasize this part. It's somewhat ambiguous. There isn't a clear single design attribute that is about emphasis. So it presents these few recommendations that are about emphasizing something. So we built this by establishing this NLP pipeline and computer vision pipeline. On the NLP side, what it does is analyzing the user's query and mapping them with the style attributes that seem to be connected to what the user's intent is about. In terms of computer vision, we collected millions of web design elements to determine a good set of recommendations to show to the learner. So by combining those, we built this system. Again, so instead of going deep into the technical details of the system, I want to focus on the interaction dynamics. So we ran this user study with 40 people where we presented them with either stylet, which is the name of our system, versus the baseline, the Chrome developer tool, which is sort of the standard tool for making these style changes. So we compared these two groups. And we gave people two tasks. One is a well-defined task where we ask people to turn this before image into an after image. And then secondly, we had this open-ended task where we gave this blank slate and people were able to make any kind of change that they want. First, I want to share success stories. People were more successful in completing these design tasks when using stylet. 80% of the stylet users completed the task as opposed to only 35% in Chrome developer tools. And these were complete novices in web design, no experience at all. And people completed the task in 35% less times. It was efficient to use stylet. Another interesting observation was that people were making same similar number of changes in both conditions. But in stylet condition, people were making more diverse changes, which means that it probably had to do with how stylet shows these multiple options for people to explore. And there was a conscious decision to not just show the most obvious one, but show somewhat related ones as well so that people could explore and tinker around different options. But then an unexpected finding was when we looked at people's self-confidence. Because we thought this kind of system would be useful for people's learning of the skills and confidence that they have about the skills, we asked people's self-confidence after each task. What we noted was that after the first task, in both conditions, people's self-confidence increased. But then in the second task, after the second task, users' self-confidence decreased for stylet while in the developer tool, it kept increasing. Why would that be the case? And we were seeing many cases where stylet users were frustrated that the only control that they had was natural language. Now they have some grasp of how it works. They wanted to do more fine-grained control more directly. And they wanted more specific things. But because they only had natural language, they sometimes just got frustrated. Whereas in the Chrome Developer Tools condition, people were just happy that they accomplished something with their own hands. And I think that is presented as a continued increase in self-confidence. And we know from HCI and CS147 that people's expertise and learnability really matters. And as they have more knowledge of the domain and the skill, they might need to get more advanced controls or being able to more directly manipulate what they are working on. So I think this had some interesting lessons in terms of thinking about the temporal dimension in that learners are changing. And other researchers have been reporting that considering these temporal dynamics is important. On the left, what you see is design researchers who have shown that there are these different stages of relationship that people have in technologies like self-tracking devices. First, they would start with initiation and experimentation, followed by intensifying and integration, and then stagnation and termination. And one of the design lessons might be that these might be more meta-level factors that really should be considered in design systems, in that even the same kind of intervention might need to be presented in different manners depending on what stage you are or what your expectation is with the system. On the right, what you're seeing is the guidelines for human-AI interaction, really influential work from Emershi et al. And they organize these guidelines for human-AI interaction in different categories but are organized in the temporal sort of aspect, like initially encounter with AI during interaction, when things go wrong, and over time. So taking into account this temporal dimension can really be powerful in supporting more sustainable engagement. And the related question might be, as people are relying more on these AI tools, like grammar fixes or even generating text, it's important to think about how people's mental model would change over time, and AI also changes over time too. And do we hit a point where people become maybe overly reliant in that maybe their grammar skills or writing skills do not improve anymore, but then without the tool, they actually might perform worse? And what is that dynamic? Or maybe over reliance is perfectly fine because if we believe these tools will be around the user all the time, maybe it's just the final outcome that matters. And I think we need more studies and analysis of the long-term engagement of users using these kind of technologies. And to kind of sum up, if we were to take a more model-centric approach here, I think we might have built a system that makes automatic design fixes to optimize a web page directly, and the system makes a fix and user can just use it. But instead, we took a more sort of interaction-centric route where we asked people to do sort of, you know, style change by themselves as the system was presenting these recommendations, and they still had to do the fix by themselves. But what we expected here was that people can then customize by seeing these attributes, they can learn, they can discover new ways of doing things, they can think around, which can empower them, especially in the more learning context, although the temporal dimension has to be more carefully taken into account. So these were the four challenges that I wanted to share today. And to kind of wrap up, I just wanted to pose two questions moving forward from the interaction-centric perspective as HCI researchers. So first is, how might we design these building blocks and interface affordances for new and upcoming AI models? Okay, so I think part of it is that instead of building these point solutions, I think we need to think about, are there any sort of generalizable frameworks, libraries, widgets, or interface affordances that we could come up with as a community that is really good at these kinds of things? And the second question is, does AI really require us to have these new things? I mean, can we just use existing design elements and frameworks to build AI applications? And I tend to think that we might need something new for these new and upcoming AI models, especially because they have these very different characteristics than the conventional systems that we have been building. They're more probabilistic, harder to predict, more black box in nature, yet seemingly more impactful and powerful in terms of what they do, hallucinating. All these properties packed together, I think we might really need to think about, what are the types of interaction affordances that are really built for supporting the usability of these AI-powered applications? So in this, I think as a community, we are making all these great advances, like making different types of contributions. And I tend to focus on more interactive systems and techniques, whereas other people focus on introducing new design processes and understandings. And I think all this work is needed. And some of the interesting examples of adding an interaction layer to these new types of models is in this example, Tailbrush, where the user can draw the level of fortune that they won in the character to have when they use generative models to generate a story. Or this AI chains work, which presents these primitives and workflows for putting together this workflow that can accomplish more complex tasks with these LLM prompts that a single prompt cannot really perform. And in my research group, with my PhD student, Tesu Kim, we have been investigating this idea of what would be more generalizable design framework. And thinking about input, model, and output, we have been thinking about the concepts of cells, generators, and lenses, and tried to introduce this standardized libraries and widgets that people can easily adopt in their AI applications. So for example, using this kind of framework, people can build a copywriting app, email app, or story writing app using pretty much the same kind of framework, which can save people's time while supporting the types of interactions like iterations and comparison and experimenting different outputs. And the second question, and the final question that I want to ask today is, how might we as an HCI community collaborate better with the AI community on these various things? And it was also the discussion that I was having a lot with today's meetings, and also with various AI researchers, especially in Europe. And in terms of community collaboration, of course, one of the important things is metrics. And there was also a great discussion at the HCI conference a couple weeks ago, hosted here at Stanford. And in the AI community, it cares a lot about model performance and generalization errors, where in HCI, we tend to focus on the human experience. So how do we really bridge the gap between the metrics? And what it means to do AI research with more human side metrics incorporated? What's the incentive for people to do that? And how do we encourage poor AI people to use these metrics, too? In terms of human input design, a lot of the comments that I was getting in terms of interaction-centric AI from AI researchers is that these ideas are great, but then I don't really know how to actually take action about it. And part of it is, in their model-building kind of work, how can I incorporate human feedback? And how do I use it in a meaningful way to really change the way the model actually works, rather than just getting more high-level design guidance? So one great direction for this might be, think about more making human feedback, more computationally feasible, so that this compatibility is actually satisfied. And lastly, we need to think about the change in design process as well. And in a lot of, this is Stanford D-School's user-centered design cycle. And I think in a lot of the AI research, what we're seeing is this prototype test kind of culture. You try something new, test it, iteratively improve it. But then one of the frustrations is that interaction often comes too late, right? There's this new cool model, and can you build an UI on top of it, is sort of the kind of discourse we get a lot. And I think interaction should not just be like an icing on the cake, but really something that can guide the entire design process or help people determine, is this the right problem to tackle in the first place? Or what kind of interaction should we try to support with AI? And based on that, think of what AI should do and should not do and how much AI should be used in a particular context. So that's all I wanted to share. And here's a summary of what I mentioned today, and I'd be happy to take any questions. Thank you. All right, so I'll check my recommendations of facilitating messages. If there are any more ideas. No? What do you think? It really sounded like an AI. I'll just click them all. I want to pull the mic on. I want to pull the thread a little bit on this notion of how to connect human feedback with the objective functions that you touched on near the end, because that's been rattling around in my head in much of the talk that you're giving, that if I think about what should AI researchers be doing differently, then you're asking, well, what's the proper model of the person in their system? And traditionally, the problem has been that human interaction is really expensive, just to collect annotated data. Or once you have it to be able to tune the model, you don't get that much of it. And so they often fall back on self-supervision, or as you've been talking about in the value alignment, they train an RL model to mimic a human and then let that go loose. And it seems like until, I think they're kind of, I want you to take a position on one of the two positions. One either is to say, look, we need to find strategies like that where we can create proxy humans, and that's how we hook into the objective functions, the loss functions, etc. The other alternative would be to say, no, we're going to find some other way to actually make human feedback at a scale and in a form that they can directly use in the models. I'm just curious, like, if you want to take a bet, where's your bet on that? Where should we be heading? Yeah, that's an excellent question. I would say, I mean, you asked me to take a position, but I would say both will be prevalent. And I like the letter much more. And I think that's more promising and sustainable. And for example, the reason I'm really interested in this, like, co-learning feedback loop between the human and the machine is that, you know, even if this super advanced AI comes along and let's say it presents this, like, super accurate explanations, people's self-explanation activity is still meaningful, right? Because that's how they could learn. And so I feel like, you know, we can really try to find these compatible mechanisms in which the human can get the benefit and get the incentive for doing what they are really good at and what is helpful for them, not necessarily trying to help the system or, you know, getting paid to system, paid to support the system per se. And at the same time, the system can use it for something meaningful. And on the system side, I think in the system, like, access that I presented, I was really happy when we landed at this technical solution where people's rating data could be almost directly piped into the feedback for the RL agent to kind of use as meaningful feedback. So I think that's just one example where this kind of worked out for this kind of context. And I think we need to really investigate more and think about, are there any generalizable mechanisms that this kind of approach could work in different contexts? This assumes that you have a large set of users you can draw on, like there are learners that are coming through your system. If I'm early on in the pipeline and I just kind of have V0, I don't have the users yet, are there strategies you would recommend? Yeah, yeah, excellent. So in that same access system, for instance, what we did was to insert the instructor-generated explanations as sort of the initial seed. And I was also imagining maybe using LLMs, for instance, we can plug in AI-generated ones to kind of avoid the cold start problem. And it would be interesting to see how, you know, in the same system, like AI-generated ones, instructor-generated ones, and learner-generated ones can kind of compete against each other until the system ultimately just focuses on what is best for learners. This is kind of a two-part question, going back to the like third challenge or like project you talked about, where there was that note about AI as proxy, like people kind of using that as like an excuse to make points, where maybe they wanted to do something but didn't want it to come off as them. So the first part of the question is like, in that case, did people want to, later it says people wanted the message to kind of sound like them, but in the case of the AI as proxy, did they want that to sound like them? Or were they wanting it to sound more artificial? And then second part of the question is, do you think there are more situations than just this where maybe we don't want the AI to feel super personable and maybe want the interaction to feel slightly more kind of mechanical or unnatural? Yeah, that's an excellent question. And I would say these were somewhat different use cases, and both I think are valuable and smite. And that again, I think in a more model-centric approach, we also kind of focus on trying to create these messages that are more like humans. And that could be effective in certain cases, but as you said, that might not really be what the users want, because in a proxy kind of setting, you might not actually want it to sound too personalized, because maybe the more canned message might actually work better in that context. And vice versa. So I think just being able to identify all these different needs that people have and expectations that people have and being able to somewhat fluidly support those, I think was really an interesting kind of observation that we had. And I think moving forward, one of the lessons was that this more personalizable message generation could be an interesting technology that could be potentially integrated, but that's not going to solve everything, because there are these other types of needs that will not be supported, even with the perfect personalizable style transfer. So yeah. Explaining stuff, I kept thinking about how what you described and sort of the challenges that we see with this new deep networks and models and how we interact with them are similar to how people used to interact with search engines, right? At the beginning, people were not as good as sort of figuring out how to query the search engine right. And over time, both we became better at querying the search engines, and then the search engines became better at sort of understanding how to interpret user queries. Do you see any similarities there? Is there something that's very unique to the challenges we face with this new models? Or is it just that we haven't had enough time to sort of adopt to each other in a way? Yeah. Excellent. Yeah. And I think it's a recurring theme as these new technologies come in. Initially, people would kind of struggle and they would need to learn how it actually works through trial and error and lots of like failed attempts. And that's what we're seeing with these like chat GPT, for instance, a lot of people are trying things out, reporting success and failure cases. So I do think there are certain similarities. What's more unique about what we're seeing right now is that due to the nature of like how black box, complex, unpredictable these models are, I think it just confuses people much more. And there's a question of, you know, is this really like a human learning problem to begin with, right? So if people take, do it more, and you know, if they had more time, will people be actually able to really get to a point where they could really easily create something that they like? Probably not. Right? So that's why I think we need both on the model side to kind of think about what are more interactable and learnable ways of, you know, architecting this kind of models in the first place. And also from the HCI point of view, what are these interaction mechanisms that could be added to these models in a way that it is actually more understandable and usable on the user side? Yeah. Thanks so much. Yeah. I think we're at about the time, but Duho will be here for a couple minutes after the talk for further questions. So let's thank him for speaking. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.56, "text": " Being a speaker at this seminar series is, I mean, it means a lot to me personally.", "tokens": [50364, 8891, 257, 8145, 412, 341, 29235, 2638, 307, 11, 286, 914, 11, 309, 1355, 257, 688, 281, 385, 5665, 13, 51192], "temperature": 0.0, "avg_logprob": -0.21659284479477825, "compression_ratio": 1.398876404494382, "no_speech_prob": 0.08569056540727615}, {"id": 1, "seek": 0, "start": 16.56, "end": 22.04, "text": " When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but", "tokens": [51192, 1133, 286, 390, 364, 14295, 646, 294, 14062, 11, 12656, 11, 286, 600, 668, 3620, 281, 1466, 466, 389, 25240, 11, 457, 51466], "temperature": 0.0, "avg_logprob": -0.21659284479477825, "compression_ratio": 1.398876404494382, "no_speech_prob": 0.08569056540727615}, {"id": 2, "seek": 0, "start": 22.04, "end": 26.42, "text": " I didn't really have that many resources, so I had to rely on online resources.", "tokens": [51466, 286, 994, 380, 534, 362, 300, 867, 3593, 11, 370, 286, 632, 281, 10687, 322, 2950, 3593, 13, 51685], "temperature": 0.0, "avg_logprob": -0.21659284479477825, "compression_ratio": 1.398876404494382, "no_speech_prob": 0.08569056540727615}, {"id": 3, "seek": 2642, "start": 26.94, "end": 31.340000000000003, "text": " This seminar series recordings have been posted online, and I think I've watched", "tokens": [50390, 639, 29235, 2638, 25162, 362, 668, 9437, 2950, 11, 293, 286, 519, 286, 600, 6337, 50610], "temperature": 0.0, "avg_logprob": -0.21919011096565091, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.010130491107702255}, {"id": 4, "seek": 2642, "start": 31.340000000000003, "end": 34.86, "text": " pretty much everything to really learn about HCI.", "tokens": [50610, 1238, 709, 1203, 281, 534, 1466, 466, 389, 25240, 13, 50786], "temperature": 0.0, "avg_logprob": -0.21919011096565091, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.010130491107702255}, {"id": 5, "seek": 2642, "start": 34.86, "end": 38.74, "text": " And then I came as a master's student here in 2008 and", "tokens": [50786, 400, 550, 286, 1361, 382, 257, 4505, 311, 3107, 510, 294, 10389, 293, 50980], "temperature": 0.0, "avg_logprob": -0.21919011096565091, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.010130491107702255}, {"id": 6, "seek": 2642, "start": 38.74, "end": 42.94, "text": " took 547 pretty much for the entire two years I was here.", "tokens": [50980, 1890, 1025, 14060, 1238, 709, 337, 264, 2302, 732, 924, 286, 390, 510, 13, 51190], "temperature": 0.0, "avg_logprob": -0.21919011096565091, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.010130491107702255}, {"id": 7, "seek": 2642, "start": 42.94, "end": 49.3, "text": " And now I feel great that I get a chance to speak as a speaker, so this is great.", "tokens": [51190, 400, 586, 286, 841, 869, 300, 286, 483, 257, 2931, 281, 1710, 382, 257, 8145, 11, 370, 341, 307, 869, 13, 51508], "temperature": 0.0, "avg_logprob": -0.21919011096565091, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.010130491107702255}, {"id": 8, "seek": 2642, "start": 50.34, "end": 54.02, "text": " Today I want to talk about interaction centric AI.", "tokens": [51560, 2692, 286, 528, 281, 751, 466, 9285, 1489, 1341, 7318, 13, 51744], "temperature": 0.0, "avg_logprob": -0.21919011096565091, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.010130491107702255}, {"id": 9, "seek": 5402, "start": 54.02, "end": 60.34, "text": " This is a reprise of the New Europe's keynote talk that I gave two weeks ago", "tokens": [50364, 639, 307, 257, 1085, 18619, 295, 264, 1873, 3315, 311, 33896, 751, 300, 286, 2729, 732, 3259, 2057, 50680], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 10, "seek": 5402, "start": 60.34, "end": 62.7, "text": " in front of thousands of AI researchers.", "tokens": [50680, 294, 1868, 295, 5383, 295, 7318, 10309, 13, 50798], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 11, "seek": 5402, "start": 62.7, "end": 66.86, "text": " I tried to reframe it a little bit so that it's more customized for", "tokens": [50798, 286, 3031, 281, 13334, 529, 309, 257, 707, 857, 370, 300, 309, 311, 544, 30581, 337, 51006], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 12, "seek": 5402, "start": 66.86, "end": 70.02000000000001, "text": " an HCI audience rather than an AI audience.", "tokens": [51006, 364, 389, 25240, 4034, 2831, 813, 364, 7318, 4034, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 13, "seek": 5402, "start": 70.02000000000001, "end": 75.30000000000001, "text": " But the idea is that I want to think about using human AI interaction", "tokens": [51164, 583, 264, 1558, 307, 300, 286, 528, 281, 519, 466, 1228, 1952, 7318, 9285, 51428], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 14, "seek": 5402, "start": 75.30000000000001, "end": 78.06, "text": " at the center of developing AI technologies.", "tokens": [51428, 412, 264, 3056, 295, 6416, 7318, 7943, 13, 51566], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 15, "seek": 5402, "start": 78.06, "end": 81.18, "text": " And of course, I don't have to preach to the choir that human AI interaction", "tokens": [51566, 400, 295, 1164, 11, 286, 500, 380, 362, 281, 21552, 281, 264, 31244, 300, 1952, 7318, 9285, 51722], "temperature": 0.0, "avg_logprob": -0.16089655082916546, "compression_ratio": 1.64453125, "no_speech_prob": 0.0011655454291030765}, {"id": 16, "seek": 8118, "start": 81.18, "end": 86.34, "text": " actually matters, but diving deeper based on my experience of building", "tokens": [50364, 767, 7001, 11, 457, 20241, 7731, 2361, 322, 452, 1752, 295, 2390, 50622], "temperature": 0.0, "avg_logprob": -0.15643276951529764, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.004815479274839163}, {"id": 17, "seek": 8118, "start": 86.34, "end": 90.30000000000001, "text": " these interactive systems in different contexts, like education, discussion,", "tokens": [50622, 613, 15141, 3652, 294, 819, 30628, 11, 411, 3309, 11, 5017, 11, 50820], "temperature": 0.0, "avg_logprob": -0.15643276951529764, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.004815479274839163}, {"id": 18, "seek": 8118, "start": 90.30000000000001, "end": 94.74000000000001, "text": " decision making, I want to dive deeper and report some of the detailed", "tokens": [50820, 3537, 1455, 11, 286, 528, 281, 9192, 7731, 293, 2275, 512, 295, 264, 9942, 51042], "temperature": 0.0, "avg_logprob": -0.15643276951529764, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.004815479274839163}, {"id": 19, "seek": 8118, "start": 94.74000000000001, "end": 98.9, "text": " interactions that we've been observing and learning from.", "tokens": [51042, 13280, 300, 321, 600, 668, 22107, 293, 2539, 490, 13, 51250], "temperature": 0.0, "avg_logprob": -0.15643276951529764, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.004815479274839163}, {"id": 20, "seek": 8118, "start": 98.9, "end": 104.54, "text": " And think about what it means to design human AI interaction in various contexts", "tokens": [51250, 400, 519, 466, 437, 309, 1355, 281, 1715, 1952, 7318, 9285, 294, 3683, 30628, 51532], "temperature": 0.0, "avg_logprob": -0.15643276951529764, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.004815479274839163}, {"id": 21, "seek": 8118, "start": 104.54, "end": 108.46000000000001, "text": " and what are some action items moving forward as a community.", "tokens": [51532, 293, 437, 366, 512, 3069, 4754, 2684, 2128, 382, 257, 1768, 13, 51728], "temperature": 0.0, "avg_logprob": -0.15643276951529764, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.004815479274839163}, {"id": 22, "seek": 10846, "start": 109.46, "end": 113.05999999999999, "text": " So let's first start with some definitions and terms.", "tokens": [50414, 407, 718, 311, 700, 722, 365, 512, 21988, 293, 2115, 13, 50594], "temperature": 0.0, "avg_logprob": -0.2240066032905083, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.0004231227212585509}, {"id": 23, "seek": 10846, "start": 113.05999999999999, "end": 120.3, "text": " I would say the dominant paradigm for developing AI technologies has been model centric.", "tokens": [50594, 286, 576, 584, 264, 15657, 24709, 337, 6416, 7318, 7943, 575, 668, 2316, 1489, 1341, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2240066032905083, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.0004231227212585509}, {"id": 24, "seek": 10846, "start": 120.3, "end": 125.17999999999999, "text": " The idea is to build a model with high accuracy and we want to evaluate it", "tokens": [50956, 440, 1558, 307, 281, 1322, 257, 2316, 365, 1090, 14170, 293, 321, 528, 281, 13059, 309, 51200], "temperature": 0.0, "avg_logprob": -0.2240066032905083, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.0004231227212585509}, {"id": 25, "seek": 10846, "start": 125.17999999999999, "end": 128.74, "text": " against unseen examples for its generalizability.", "tokens": [51200, 1970, 40608, 5110, 337, 1080, 2674, 590, 2310, 13, 51378], "temperature": 0.0, "avg_logprob": -0.2240066032905083, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.0004231227212585509}, {"id": 26, "seek": 10846, "start": 128.74, "end": 133.82, "text": " And benchmarks have been great in that they could help us competitively", "tokens": [51378, 400, 43751, 362, 668, 869, 294, 300, 436, 727, 854, 505, 10043, 356, 51632], "temperature": 0.0, "avg_logprob": -0.2240066032905083, "compression_ratio": 1.4933920704845816, "no_speech_prob": 0.0004231227212585509}, {"id": 27, "seek": 13382, "start": 134.22, "end": 138.94, "text": " compare different models' performances, which could be useful in making", "tokens": [50384, 6794, 819, 5245, 6, 16087, 11, 597, 727, 312, 4420, 294, 1455, 50620], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 28, "seek": 13382, "start": 138.94, "end": 142.26, "text": " scientific advances possible.", "tokens": [50620, 8134, 25297, 1944, 13, 50786], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 29, "seek": 13382, "start": 142.26, "end": 145.9, "text": " And more recently, people have been talking a lot about data centric AI,", "tokens": [50786, 400, 544, 3938, 11, 561, 362, 668, 1417, 257, 688, 466, 1412, 1489, 1341, 7318, 11, 50968], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 30, "seek": 13382, "start": 145.9, "end": 150.78, "text": " where the idea is using this nicely performing model, what is a good,", "tokens": [50968, 689, 264, 1558, 307, 1228, 341, 9594, 10205, 2316, 11, 437, 307, 257, 665, 11, 51212], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 31, "seek": 13382, "start": 150.78, "end": 155.45999999999998, "text": " sort of robust and efficient data pipeline around it in terms of collection", "tokens": [51212, 1333, 295, 13956, 293, 7148, 1412, 15517, 926, 309, 294, 2115, 295, 5765, 51446], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 32, "seek": 13382, "start": 155.45999999999998, "end": 160.82, "text": " of the data, processing of it, cleaning of it, so that the model actually performs", "tokens": [51446, 295, 264, 1412, 11, 9007, 295, 309, 11, 8924, 295, 309, 11, 370, 300, 264, 2316, 767, 26213, 51714], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 33, "seek": 13382, "start": 160.82, "end": 163.06, "text": " really well in different contexts.", "tokens": [51714, 534, 731, 294, 819, 30628, 13, 51826], "temperature": 0.0, "avg_logprob": -0.17619277954101562, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0017262211767956614}, {"id": 34, "seek": 16306, "start": 163.1, "end": 168.5, "text": " And here the focus is acquiring quality data and setting the pipeline in a way", "tokens": [50366, 400, 510, 264, 1879, 307, 37374, 3125, 1412, 293, 3287, 264, 15517, 294, 257, 636, 50636], "temperature": 0.0, "avg_logprob": -0.16421164406670463, "compression_ratio": 1.576271186440678, "no_speech_prob": 7.96220701886341e-05}, {"id": 35, "seek": 16306, "start": 168.5, "end": 171.34, "text": " that really helps the machine perform its best.", "tokens": [50636, 300, 534, 3665, 264, 3479, 2042, 1080, 1151, 13, 50778], "temperature": 0.0, "avg_logprob": -0.16421164406670463, "compression_ratio": 1.576271186440678, "no_speech_prob": 7.96220701886341e-05}, {"id": 36, "seek": 16306, "start": 172.98, "end": 179.66, "text": " And these two paradigms are great, but then what is slightly missing is the user", "tokens": [50860, 400, 613, 732, 13480, 328, 2592, 366, 869, 11, 457, 550, 437, 307, 4748, 5361, 307, 264, 4195, 51194], "temperature": 0.0, "avg_logprob": -0.16421164406670463, "compression_ratio": 1.576271186440678, "no_speech_prob": 7.96220701886341e-05}, {"id": 37, "seek": 16306, "start": 179.66, "end": 184.58, "text": " who's using these AI technologies and those who are affected by what the AI", "tokens": [51194, 567, 311, 1228, 613, 7318, 7943, 293, 729, 567, 366, 8028, 538, 437, 264, 7318, 51440], "temperature": 0.0, "avg_logprob": -0.16421164406670463, "compression_ratio": 1.576271186440678, "no_speech_prob": 7.96220701886341e-05}, {"id": 38, "seek": 16306, "start": 184.58, "end": 186.02, "text": " systems give you.", "tokens": [51440, 3652, 976, 291, 13, 51512], "temperature": 0.0, "avg_logprob": -0.16421164406670463, "compression_ratio": 1.576271186440678, "no_speech_prob": 7.96220701886341e-05}, {"id": 39, "seek": 16306, "start": 186.02, "end": 192.14000000000001, "text": " So interaction centric AI is sort of my term in some contrast to model", "tokens": [51512, 407, 9285, 1489, 1341, 7318, 307, 1333, 295, 452, 1433, 294, 512, 8712, 281, 2316, 51818], "temperature": 0.0, "avg_logprob": -0.16421164406670463, "compression_ratio": 1.576271186440678, "no_speech_prob": 7.96220701886341e-05}, {"id": 40, "seek": 19214, "start": 192.14, "end": 196.29999999999998, "text": " centric and data centric AI, where the goal would be basically what HCI", "tokens": [50364, 1489, 1341, 293, 1412, 1489, 1341, 7318, 11, 689, 264, 3387, 576, 312, 1936, 437, 389, 25240, 50572], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 41, "seek": 19214, "start": 196.29999999999998, "end": 200.77999999999997, "text": " researchers do in this context, like improving the user experience by", "tokens": [50572, 10309, 360, 294, 341, 4319, 11, 411, 11470, 264, 4195, 1752, 538, 50796], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 42, "seek": 19214, "start": 200.77999999999997, "end": 203.98, "text": " building usable and useful applications.", "tokens": [50796, 2390, 29975, 293, 4420, 5821, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 43, "seek": 19214, "start": 203.98, "end": 208.85999999999999, "text": " And the unit that we often grapple with is a human AI interaction.", "tokens": [50956, 400, 264, 4985, 300, 321, 2049, 27165, 306, 365, 307, 257, 1952, 7318, 9285, 13, 51200], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 44, "seek": 19214, "start": 208.85999999999999, "end": 212.33999999999997, "text": " And you might be wondering, is this some sort of like marketing term?", "tokens": [51200, 400, 291, 1062, 312, 6359, 11, 307, 341, 512, 1333, 295, 411, 6370, 1433, 30, 51374], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 45, "seek": 19214, "start": 212.33999999999997, "end": 217.26, "text": " Like how is it different from human centric AI we've been talking all about?", "tokens": [51374, 1743, 577, 307, 309, 819, 490, 1952, 1489, 1341, 7318, 321, 600, 668, 1417, 439, 466, 30, 51620], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 46, "seek": 19214, "start": 217.26, "end": 221.33999999999997, "text": " It's largely similar, so I'm not trying to say I invented this new term or", "tokens": [51620, 467, 311, 11611, 2531, 11, 370, 286, 478, 406, 1382, 281, 584, 286, 14479, 341, 777, 1433, 420, 51824], "temperature": 0.0, "avg_logprob": -0.14808738642725452, "compression_ratio": 1.6020408163265305, "no_speech_prob": 0.0005271455738693476}, {"id": 47, "seek": 22134, "start": 221.34, "end": 225.82, "text": " anything, but I want to focus our attention to the interaction that is", "tokens": [50364, 1340, 11, 457, 286, 528, 281, 1879, 527, 3202, 281, 264, 9285, 300, 307, 50588], "temperature": 0.0, "avg_logprob": -0.15350296676799816, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0006257311906665564}, {"id": 48, "seek": 22134, "start": 225.82, "end": 230.54, "text": " happening between humans and AI and the complex relationships and the dynamics", "tokens": [50588, 2737, 1296, 6255, 293, 7318, 293, 264, 3997, 6159, 293, 264, 15679, 50824], "temperature": 0.0, "avg_logprob": -0.15350296676799816, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0006257311906665564}, {"id": 49, "seek": 22134, "start": 230.54, "end": 234.62, "text": " that are happening between the two, rather than focusing on just the humans", "tokens": [50824, 300, 366, 2737, 1296, 264, 732, 11, 2831, 813, 8416, 322, 445, 264, 6255, 51028], "temperature": 0.0, "avg_logprob": -0.15350296676799816, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0006257311906665564}, {"id": 50, "seek": 22134, "start": 234.62, "end": 236.14000000000001, "text": " or machine alone.", "tokens": [51028, 420, 3479, 3312, 13, 51104], "temperature": 0.0, "avg_logprob": -0.15350296676799816, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0006257311906665564}, {"id": 51, "seek": 22134, "start": 236.14000000000001, "end": 241.1, "text": " So I can say that that's the focus of where my discussion will be today.", "tokens": [51104, 407, 286, 393, 584, 300, 300, 311, 264, 1879, 295, 689, 452, 5017, 486, 312, 965, 13, 51352], "temperature": 0.0, "avg_logprob": -0.15350296676799816, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0006257311906665564}, {"id": 52, "seek": 22134, "start": 243.26, "end": 249.78, "text": " So let's say you're this AI researcher and your team has built this amazing model.", "tokens": [51460, 407, 718, 311, 584, 291, 434, 341, 7318, 21751, 293, 428, 1469, 575, 3094, 341, 2243, 2316, 13, 51786], "temperature": 0.0, "avg_logprob": -0.15350296676799816, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0006257311906665564}, {"id": 53, "seek": 24978, "start": 249.78, "end": 254.9, "text": " So this is actually something that I copied and pasted from one of the", "tokens": [50364, 407, 341, 307, 767, 746, 300, 286, 25365, 293, 1791, 292, 490, 472, 295, 264, 50620], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 54, "seek": 24978, "start": 254.9, "end": 256.5, "text": " diffusion models papers.", "tokens": [50620, 25242, 5245, 10577, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 55, "seek": 24978, "start": 256.5, "end": 260.54, "text": " I don't know what they actually mean, some of them I understand.", "tokens": [50700, 286, 500, 380, 458, 437, 436, 767, 914, 11, 512, 295, 552, 286, 1223, 13, 50902], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 56, "seek": 24978, "start": 260.54, "end": 265.3, "text": " But basically, this is what you have as an AI researcher.", "tokens": [50902, 583, 1936, 11, 341, 307, 437, 291, 362, 382, 364, 7318, 21751, 13, 51140], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 57, "seek": 24978, "start": 265.3, "end": 269.7, "text": " But what would a person using this kind of AI want to do with it?", "tokens": [51140, 583, 437, 576, 257, 954, 1228, 341, 733, 295, 7318, 528, 281, 360, 365, 309, 30, 51360], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 58, "seek": 24978, "start": 269.7, "end": 271.14, "text": " Here's an example.", "tokens": [51360, 1692, 311, 364, 1365, 13, 51432], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 59, "seek": 24978, "start": 271.14, "end": 276.34000000000003, "text": " So this is a Twitch streamer in South Korea who was trying to use this", "tokens": [51432, 407, 341, 307, 257, 22222, 4309, 260, 294, 4242, 6307, 567, 390, 1382, 281, 764, 341, 51692], "temperature": 0.0, "avg_logprob": -0.13425980567932128, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000391289999242872}, {"id": 60, "seek": 27634, "start": 276.34, "end": 281.94, "text": " diffusion-based text to image generation model to create this image of an", "tokens": [50364, 25242, 12, 6032, 2487, 281, 3256, 5125, 2316, 281, 1884, 341, 3256, 295, 364, 50644], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 61, "seek": 27634, "start": 281.94, "end": 286.41999999999996, "text": " animated character eating ramen with chopsticks with noodles around the", "tokens": [50644, 18947, 2517, 3936, 20948, 365, 39443, 365, 10480, 926, 264, 50868], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 62, "seek": 27634, "start": 286.41999999999996, "end": 287.29999999999995, "text": " character.", "tokens": [50868, 2517, 13, 50912], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 63, "seek": 27634, "start": 287.29999999999995, "end": 292.5, "text": " So this is the roughly sketched out goal that the user company has.", "tokens": [50912, 407, 341, 307, 264, 9810, 12325, 292, 484, 3387, 300, 264, 4195, 2237, 575, 13, 51172], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 64, "seek": 27634, "start": 292.5, "end": 299.21999999999997, "text": " And he ended up spending two hours fiddling with text-based prompts to get at", "tokens": [51172, 400, 415, 4590, 493, 6434, 732, 2496, 283, 14273, 1688, 365, 2487, 12, 6032, 41095, 281, 483, 412, 51508], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 65, "seek": 27634, "start": 299.21999999999997, "end": 301.46, "text": " the final image that he wants.", "tokens": [51508, 264, 2572, 3256, 300, 415, 2738, 13, 51620], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 66, "seek": 27634, "start": 301.46, "end": 304.82, "text": " And this is somewhat similar to what Manish shared a couple of weeks ago at", "tokens": [51620, 400, 341, 307, 8344, 2531, 281, 437, 2458, 742, 5507, 257, 1916, 295, 3259, 2057, 412, 51788], "temperature": 0.0, "avg_logprob": -0.13567348402373644, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.003855990245938301}, {"id": 67, "seek": 30482, "start": 304.82, "end": 310.58, "text": " the HAI conference in terms of what he had to do with the prompt-based interface.", "tokens": [50364, 264, 11979, 40, 7586, 294, 2115, 295, 437, 415, 632, 281, 360, 365, 264, 12391, 12, 6032, 9226, 13, 50652], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 68, "seek": 30482, "start": 310.58, "end": 314.5, "text": " And here, the entire two-hour journey was live streamed.", "tokens": [50652, 400, 510, 11, 264, 2302, 732, 12, 18048, 4671, 390, 1621, 4309, 292, 13, 50848], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 69, "seek": 30482, "start": 314.5, "end": 319.06, "text": " So I want to kind of share a quick summary of what happened in that stream.", "tokens": [50848, 407, 286, 528, 281, 733, 295, 2073, 257, 1702, 12691, 295, 437, 2011, 294, 300, 4309, 13, 51076], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 70, "seek": 30482, "start": 319.06, "end": 322.74, "text": " And of course, we need something in the middle to bridge between the technology", "tokens": [51076, 400, 295, 1164, 11, 321, 643, 746, 294, 264, 2808, 281, 7283, 1296, 264, 2899, 51260], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 71, "seek": 30482, "start": 323.38, "end": 324.74, "text": " and the human user.", "tokens": [51292, 293, 264, 1952, 4195, 13, 51360], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 72, "seek": 30482, "start": 324.74, "end": 329.53999999999996, "text": " And that's what we have, the prompt-based interface and interaction that's", "tokens": [51360, 400, 300, 311, 437, 321, 362, 11, 264, 12391, 12, 6032, 9226, 293, 9285, 300, 311, 51600], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 73, "seek": 30482, "start": 329.53999999999996, "end": 330.5, "text": " happening between the two.", "tokens": [51600, 2737, 1296, 264, 732, 13, 51648], "temperature": 0.0, "avg_logprob": -0.11308187859080662, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.0010145128471776843}, {"id": 74, "seek": 33050, "start": 331.38, "end": 335.14, "text": " So the streamer started by something simple and obvious.", "tokens": [50408, 407, 264, 4309, 260, 1409, 538, 746, 2199, 293, 6322, 13, 50596], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 75, "seek": 33050, "start": 335.14, "end": 337.78, "text": " The prompt says, eating ramen, and this is what he got.", "tokens": [50596, 440, 12391, 1619, 11, 3936, 20948, 11, 293, 341, 307, 437, 415, 658, 13, 50728], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 76, "seek": 33050, "start": 337.78, "end": 342.74, "text": " It's okay, it's kind of there, but the bowl is perhaps too large.", "tokens": [50728, 467, 311, 1392, 11, 309, 311, 733, 295, 456, 11, 457, 264, 6571, 307, 4317, 886, 2416, 13, 50976], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 77, "seek": 33050, "start": 342.74, "end": 344.82, "text": " The chopsticks are all to be placed.", "tokens": [50976, 440, 39443, 366, 439, 281, 312, 7074, 13, 51080], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 78, "seek": 33050, "start": 344.82, "end": 349.94, "text": " And he heard from somewhere that adding a full sentence might make things better.", "tokens": [51080, 400, 415, 2198, 490, 4079, 300, 5127, 257, 1577, 8174, 1062, 652, 721, 1101, 13, 51336], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 79, "seek": 33050, "start": 349.94, "end": 352.5, "text": " So he goes, she is eating ramen.", "tokens": [51336, 407, 415, 1709, 11, 750, 307, 3936, 20948, 13, 51464], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 80, "seek": 33050, "start": 353.78, "end": 358.9, "text": " She is eating ramen, for sure, but you can see that something's not quite right.", "tokens": [51528, 1240, 307, 3936, 20948, 11, 337, 988, 11, 457, 291, 393, 536, 300, 746, 311, 406, 1596, 558, 13, 51784], "temperature": 0.0, "avg_logprob": -0.28202356858686967, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0031700232066214085}, {"id": 81, "seek": 35890, "start": 359.21999999999997, "end": 363.21999999999997, "text": " So he keeps going on by adding more descriptions.", "tokens": [50380, 407, 415, 5965, 516, 322, 538, 5127, 544, 24406, 13, 50580], "temperature": 0.0, "avg_logprob": -0.15296191327712116, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0036951664369553328}, {"id": 82, "seek": 35890, "start": 363.21999999999997, "end": 366.65999999999997, "text": " And the prompt is definitely getting longer.", "tokens": [50580, 400, 264, 12391, 307, 2138, 1242, 2854, 13, 50752], "temperature": 0.0, "avg_logprob": -0.15296191327712116, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0036951664369553328}, {"id": 83, "seek": 35890, "start": 367.62, "end": 372.34, "text": " And it seems that the AI is not quite getting how chopsticks should be used", "tokens": [50800, 400, 309, 2544, 300, 264, 7318, 307, 406, 1596, 1242, 577, 39443, 820, 312, 1143, 51036], "temperature": 0.0, "avg_logprob": -0.15296191327712116, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0036951664369553328}, {"id": 84, "seek": 35890, "start": 372.34, "end": 374.9, "text": " and how many should be used.", "tokens": [51036, 293, 577, 867, 820, 312, 1143, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15296191327712116, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0036951664369553328}, {"id": 85, "seek": 35890, "start": 374.9, "end": 380.02, "text": " So he keeps adding these descriptions to really explain what it means to use chopsticks.", "tokens": [51164, 407, 415, 5965, 5127, 613, 24406, 281, 534, 2903, 437, 309, 1355, 281, 764, 39443, 13, 51420], "temperature": 0.0, "avg_logprob": -0.15296191327712116, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0036951664369553328}, {"id": 86, "seek": 38002, "start": 380.34, "end": 390.9, "text": " And to be fair, there's hair, there's chopsticks, there's noodles.", "tokens": [50380, 400, 281, 312, 3143, 11, 456, 311, 2578, 11, 456, 311, 39443, 11, 456, 311, 10480, 13, 50908], "temperature": 0.0, "avg_logprob": -0.14633964670115504, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.0027539252769201994}, {"id": 87, "seek": 38002, "start": 390.9, "end": 393.06, "text": " So it's in computer graphics.", "tokens": [50908, 407, 309, 311, 294, 3820, 11837, 13, 51016], "temperature": 0.0, "avg_logprob": -0.14633964670115504, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.0027539252769201994}, {"id": 88, "seek": 38002, "start": 393.06, "end": 395.85999999999996, "text": " Dealing with human hair, I heard, is a really tough challenge.", "tokens": [51016, 1346, 4270, 365, 1952, 2578, 11, 286, 2198, 11, 307, 257, 534, 4930, 3430, 13, 51156], "temperature": 0.0, "avg_logprob": -0.14633964670115504, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.0027539252769201994}, {"id": 89, "seek": 38002, "start": 395.85999999999996, "end": 401.85999999999996, "text": " And maybe for AI, it's also kind of struggling to deal with all these similar looking objects.", "tokens": [51156, 400, 1310, 337, 7318, 11, 309, 311, 611, 733, 295, 9314, 281, 2028, 365, 439, 613, 2531, 1237, 6565, 13, 51456], "temperature": 0.0, "avg_logprob": -0.14633964670115504, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.0027539252769201994}, {"id": 90, "seek": 38002, "start": 401.85999999999996, "end": 407.62, "text": " And it doesn't really seem to get how to differentiate between chopsticks and noodles.", "tokens": [51456, 400, 309, 1177, 380, 534, 1643, 281, 483, 577, 281, 23203, 1296, 39443, 293, 10480, 13, 51744], "temperature": 0.0, "avg_logprob": -0.14633964670115504, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.0027539252769201994}, {"id": 91, "seek": 40762, "start": 408.58, "end": 412.34000000000003, "text": " And another interesting aspect was that since it was a live stream,", "tokens": [50412, 400, 1071, 1880, 4171, 390, 300, 1670, 309, 390, 257, 1621, 4309, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 92, "seek": 40762, "start": 412.34000000000003, "end": 419.06, "text": " the viewers were actively participating in recommending new prompts to try out,", "tokens": [50600, 264, 8499, 645, 13022, 13950, 294, 30559, 777, 41095, 281, 853, 484, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 93, "seek": 40762, "start": 419.06, "end": 420.9, "text": " sharing their interpretations.", "tokens": [50936, 5414, 641, 37547, 13, 51028], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 94, "seek": 40762, "start": 420.9, "end": 425.86, "text": " And this is somewhat of a collaborative mental model construction process,", "tokens": [51028, 400, 341, 307, 8344, 295, 257, 16555, 4973, 2316, 6435, 1399, 11, 51276], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 95, "seek": 40762, "start": 425.86, "end": 428.26, "text": " if you will, as a group of people.", "tokens": [51276, 498, 291, 486, 11, 382, 257, 1594, 295, 561, 13, 51396], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 96, "seek": 40762, "start": 428.26, "end": 430.66, "text": " They are really trying to figure out what's going on.", "tokens": [51396, 814, 366, 534, 1382, 281, 2573, 484, 437, 311, 516, 322, 13, 51516], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 97, "seek": 40762, "start": 430.66, "end": 434.18, "text": " And now the prompt is five lines long.", "tokens": [51516, 400, 586, 264, 12391, 307, 1732, 3876, 938, 13, 51692], "temperature": 0.0, "avg_logprob": -0.09200210785597898, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.00038554627099074423}, {"id": 98, "seek": 43418, "start": 434.74, "end": 439.14, "text": " And the service that this streamer was using was supporting variations,", "tokens": [50392, 400, 264, 2643, 300, 341, 4309, 260, 390, 1228, 390, 7231, 17840, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1849533594571627, "compression_ratio": 1.6305970149253732, "no_speech_prob": 0.002317414851859212}, {"id": 99, "seek": 43418, "start": 439.14, "end": 442.34000000000003, "text": " where you could pick an image and say create some variations.", "tokens": [50612, 689, 291, 727, 1888, 364, 3256, 293, 584, 1884, 512, 17840, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1849533594571627, "compression_ratio": 1.6305970149253732, "no_speech_prob": 0.002317414851859212}, {"id": 100, "seek": 43418, "start": 442.34000000000003, "end": 446.1, "text": " And he was referring to this interaction as variation gacha.", "tokens": [50772, 400, 415, 390, 13761, 281, 341, 9285, 382, 12990, 290, 27442, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1849533594571627, "compression_ratio": 1.6305970149253732, "no_speech_prob": 0.002317414851859212}, {"id": 101, "seek": 43418, "start": 446.1, "end": 450.34000000000003, "text": " So gacha is a Japanese word for like a random box or blind box.", "tokens": [50960, 407, 290, 27442, 307, 257, 5433, 1349, 337, 411, 257, 4974, 2424, 420, 6865, 2424, 13, 51172], "temperature": 0.0, "avg_logprob": -0.1849533594571627, "compression_ratio": 1.6305970149253732, "no_speech_prob": 0.002317414851859212}, {"id": 102, "seek": 43418, "start": 450.34000000000003, "end": 455.14, "text": " And this kind of tells us that how unpredictable this sort of interface is.", "tokens": [51172, 400, 341, 733, 295, 5112, 505, 300, 577, 31160, 341, 1333, 295, 9226, 307, 13, 51412], "temperature": 0.0, "avg_logprob": -0.1849533594571627, "compression_ratio": 1.6305970149253732, "no_speech_prob": 0.002317414851859212}, {"id": 103, "seek": 43418, "start": 455.14, "end": 460.74, "text": " Once you hit the generate button, the user doesn't really have a good sense of knowing what to expect.", "tokens": [51412, 3443, 291, 2045, 264, 8460, 2960, 11, 264, 4195, 1177, 380, 534, 362, 257, 665, 2020, 295, 5276, 437, 281, 2066, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1849533594571627, "compression_ratio": 1.6305970149253732, "no_speech_prob": 0.002317414851859212}, {"id": 104, "seek": 46074, "start": 461.22, "end": 465.14, "text": " And this is the actual stream, as you can see, like his praying,", "tokens": [50388, 400, 341, 307, 264, 3539, 4309, 11, 382, 291, 393, 536, 11, 411, 702, 15611, 11, 50584], "temperature": 0.0, "avg_logprob": -0.24173205366758543, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.004125329200178385}, {"id": 105, "seek": 46074, "start": 465.14, "end": 470.02, "text": " and which also tells us about the usability of this sort of system.", "tokens": [50584, 293, 597, 611, 5112, 505, 466, 264, 46878, 295, 341, 1333, 295, 1185, 13, 50828], "temperature": 0.0, "avg_logprob": -0.24173205366758543, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.004125329200178385}, {"id": 106, "seek": 46074, "start": 470.02, "end": 474.66, "text": " He doesn't have a good way of knowing what to expect, so that he actually has to pray.", "tokens": [50828, 634, 1177, 380, 362, 257, 665, 636, 295, 5276, 437, 281, 2066, 11, 370, 300, 415, 767, 575, 281, 3690, 13, 51060], "temperature": 0.0, "avg_logprob": -0.24173205366758543, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.004125329200178385}, {"id": 107, "seek": 46074, "start": 476.66, "end": 481.14, "text": " After two hours of hard work, this is the final image that he landed.", "tokens": [51160, 2381, 732, 2496, 295, 1152, 589, 11, 341, 307, 264, 2572, 3256, 300, 415, 15336, 13, 51384], "temperature": 0.0, "avg_logprob": -0.24173205366758543, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.004125329200178385}, {"id": 108, "seek": 46074, "start": 481.14, "end": 484.58, "text": " And it looks pretty good, and he claims victory.", "tokens": [51384, 400, 309, 1542, 1238, 665, 11, 293, 415, 9441, 9812, 13, 51556], "temperature": 0.0, "avg_logprob": -0.24173205366758543, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.004125329200178385}, {"id": 109, "seek": 46074, "start": 484.58, "end": 489.38, "text": " But then look at what he had to do at the top, right?", "tokens": [51556, 583, 550, 574, 412, 437, 415, 632, 281, 360, 412, 264, 1192, 11, 558, 30, 51796], "temperature": 0.0, "avg_logprob": -0.24173205366758543, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.004125329200178385}, {"id": 110, "seek": 48938, "start": 489.7, "end": 494.74, "text": " At the top, there are seven lines of prompt that he had to write.", "tokens": [50380, 1711, 264, 1192, 11, 456, 366, 3407, 3876, 295, 12391, 300, 415, 632, 281, 2464, 13, 50632], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 111, "seek": 48938, "start": 495.62, "end": 499.06, "text": " And arguably, this is natural language.", "tokens": [50676, 400, 26771, 11, 341, 307, 3303, 2856, 13, 50848], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 112, "seek": 48938, "start": 499.78, "end": 502.42, "text": " But I would say this is really pseudo-natural language.", "tokens": [50884, 583, 286, 576, 584, 341, 307, 534, 35899, 12, 16296, 2856, 13, 51016], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 113, "seek": 48938, "start": 503.7, "end": 507.38, "text": " And so this is basically the experience that he had to go through.", "tokens": [51080, 400, 370, 341, 307, 1936, 264, 1752, 300, 415, 632, 281, 352, 807, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 114, "seek": 48938, "start": 507.38, "end": 509.21999999999997, "text": " So is this a good interface?", "tokens": [51264, 407, 307, 341, 257, 665, 9226, 30, 51356], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 115, "seek": 48938, "start": 509.21999999999997, "end": 514.18, "text": " And I sort of got inspired by Manish's discussion of discussing the usability", "tokens": [51356, 400, 286, 1333, 295, 658, 7547, 538, 2458, 742, 311, 5017, 295, 10850, 264, 46878, 51604], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 116, "seek": 48938, "start": 514.18, "end": 516.9, "text": " of these text prompt-based interfaces.", "tokens": [51604, 295, 613, 2487, 12391, 12, 6032, 28416, 13, 51740], "temperature": 0.0, "avg_logprob": -0.15477478504180908, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.00046515234862454236}, {"id": 117, "seek": 51690, "start": 517.54, "end": 518.74, "text": " There are some good elements, right?", "tokens": [50396, 821, 366, 512, 665, 4959, 11, 558, 30, 50456], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 118, "seek": 51690, "start": 518.74, "end": 519.78, "text": " It's quite intuitive.", "tokens": [50456, 467, 311, 1596, 21769, 13, 50508], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 119, "seek": 51690, "start": 519.78, "end": 523.9399999999999, "text": " You can use natural language, or you believe natural language could be used.", "tokens": [50508, 509, 393, 764, 3303, 2856, 11, 420, 291, 1697, 3303, 2856, 727, 312, 1143, 13, 50716], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 120, "seek": 51690, "start": 524.5, "end": 527.3, "text": " And the output is presented in a visual manner,", "tokens": [50744, 400, 264, 5598, 307, 8212, 294, 257, 5056, 9060, 11, 50884], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 121, "seek": 51690, "start": 527.3, "end": 531.54, "text": " which helps you kind of understand whether you got the image that you like or not,", "tokens": [50884, 597, 3665, 291, 733, 295, 1223, 1968, 291, 658, 264, 3256, 300, 291, 411, 420, 406, 11, 51096], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 122, "seek": 51690, "start": 531.54, "end": 532.98, "text": " so that you can sort of debug.", "tokens": [51096, 370, 300, 291, 393, 1333, 295, 24083, 13, 51168], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 123, "seek": 51690, "start": 533.78, "end": 536.98, "text": " And there are some interactions that are supported,", "tokens": [51208, 400, 456, 366, 512, 13280, 300, 366, 8104, 11, 51368], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 124, "seek": 51690, "start": 536.98, "end": 542.5799999999999, "text": " like variations and seeds and like words that should not be used and things like that.", "tokens": [51368, 411, 17840, 293, 9203, 293, 411, 2283, 300, 820, 406, 312, 1143, 293, 721, 411, 300, 13, 51648], "temperature": 0.0, "avg_logprob": -0.08004742025214935, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0009692102903500199}, {"id": 125, "seek": 54258, "start": 543.5400000000001, "end": 547.14, "text": " But there are many ways in which this interface actually fails", "tokens": [50412, 583, 456, 366, 867, 2098, 294, 597, 341, 9226, 767, 18199, 50592], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 126, "seek": 54258, "start": 547.14, "end": 548.82, "text": " to support what the actual user wants.", "tokens": [50592, 281, 1406, 437, 264, 3539, 4195, 2738, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 127, "seek": 54258, "start": 549.7, "end": 551.86, "text": " He had to rely on trial and error.", "tokens": [50720, 634, 632, 281, 10687, 322, 7308, 293, 6713, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 128, "seek": 54258, "start": 551.86, "end": 556.4200000000001, "text": " And just the fact that he had to spend two hours to get that image", "tokens": [50828, 400, 445, 264, 1186, 300, 415, 632, 281, 3496, 732, 2496, 281, 483, 300, 3256, 51056], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 129, "seek": 54258, "start": 556.4200000000001, "end": 559.0600000000001, "text": " seems to suggest that something is really wrong.", "tokens": [51056, 2544, 281, 3402, 300, 746, 307, 534, 2085, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 130, "seek": 54258, "start": 559.94, "end": 561.94, "text": " And of course, it was not really predictable", "tokens": [51232, 400, 295, 1164, 11, 309, 390, 406, 534, 27737, 51332], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 131, "seek": 54258, "start": 561.94, "end": 566.1800000000001, "text": " and lack of specific feedback on the effect of what specific words", "tokens": [51332, 293, 5011, 295, 2685, 5824, 322, 264, 1802, 295, 437, 2685, 2283, 51544], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 132, "seek": 54258, "start": 566.1800000000001, "end": 569.86, "text": " in the prompt had influenced on the final outcome.", "tokens": [51544, 294, 264, 12391, 632, 15269, 322, 264, 2572, 9700, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1159257699947546, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0005972387152723968}, {"id": 133, "seek": 56986, "start": 569.94, "end": 573.0600000000001, "text": " These links were often missing, which made it really difficult.", "tokens": [50368, 1981, 6123, 645, 2049, 5361, 11, 597, 1027, 309, 534, 2252, 13, 50524], "temperature": 0.0, "avg_logprob": -0.07260398581476495, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.001187434303574264}, {"id": 134, "seek": 56986, "start": 574.34, "end": 578.66, "text": " So is this really just a problem for these text-based prompt-based systems?", "tokens": [50588, 407, 307, 341, 534, 445, 257, 1154, 337, 613, 2487, 12, 6032, 12391, 12, 6032, 3652, 30, 50804], "temperature": 0.0, "avg_logprob": -0.07260398581476495, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.001187434303574264}, {"id": 135, "seek": 56986, "start": 579.3000000000001, "end": 584.34, "text": " I would say every AI application faces these interaction challenges.", "tokens": [50836, 286, 576, 584, 633, 7318, 3861, 8475, 613, 9285, 4759, 13, 51088], "temperature": 0.0, "avg_logprob": -0.07260398581476495, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.001187434303574264}, {"id": 136, "seek": 56986, "start": 584.34, "end": 587.54, "text": " On the user side, when they first encounter these systems,", "tokens": [51088, 1282, 264, 4195, 1252, 11, 562, 436, 700, 8593, 613, 3652, 11, 51248], "temperature": 0.0, "avg_logprob": -0.07260398581476495, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.001187434303574264}, {"id": 137, "seek": 56986, "start": 587.54, "end": 591.7, "text": " they often have to struggle to kind of figure out how to make it work.", "tokens": [51248, 436, 2049, 362, 281, 7799, 281, 733, 295, 2573, 484, 577, 281, 652, 309, 589, 13, 51456], "temperature": 0.0, "avg_logprob": -0.07260398581476495, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.001187434303574264}, {"id": 138, "seek": 56986, "start": 591.7, "end": 597.14, "text": " Often people resort to misusing it, abusing it, and learning takes a long time.", "tokens": [51456, 20043, 561, 19606, 281, 3346, 7981, 309, 11, 410, 7981, 309, 11, 293, 2539, 2516, 257, 938, 565, 13, 51728], "temperature": 0.0, "avg_logprob": -0.07260398581476495, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.001187434303574264}, {"id": 139, "seek": 59714, "start": 597.6999999999999, "end": 600.1, "text": " And part of it is really a design challenge.", "tokens": [50392, 400, 644, 295, 309, 307, 534, 257, 1715, 3430, 13, 50512], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 140, "seek": 59714, "start": 601.38, "end": 603.22, "text": " And we've seen other examples like this,", "tokens": [50576, 400, 321, 600, 1612, 661, 5110, 411, 341, 11, 50668], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 141, "seek": 59714, "start": 603.22, "end": 606.26, "text": " where people don't have a good sense of what's happening", "tokens": [50668, 689, 561, 500, 380, 362, 257, 665, 2020, 295, 437, 311, 2737, 50820], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 142, "seek": 59714, "start": 606.26, "end": 611.06, "text": " in this algorithmically-generated systems and AI-powered systems.", "tokens": [50820, 294, 341, 9284, 984, 12, 21848, 770, 3652, 293, 7318, 12, 27178, 3652, 13, 51060], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 143, "seek": 59714, "start": 611.06, "end": 614.98, "text": " Like in the famous study of Facebook newsfeed users,", "tokens": [51060, 1743, 294, 264, 4618, 2979, 295, 4384, 2583, 37036, 5022, 11, 51256], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 144, "seek": 59714, "start": 614.98, "end": 620.02, "text": " more than half of the participants were not aware of the newsfeed curation algorithm's existence", "tokens": [51256, 544, 813, 1922, 295, 264, 10503, 645, 406, 3650, 295, 264, 2583, 37036, 1262, 399, 9284, 311, 9123, 51508], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 145, "seek": 59714, "start": 620.02, "end": 622.1, "text": " at all, which is far from being true.", "tokens": [51508, 412, 439, 11, 597, 307, 1400, 490, 885, 2074, 13, 51612], "temperature": 0.0, "avg_logprob": -0.10910372734069824, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015478046843782067}, {"id": 146, "seek": 62210, "start": 623.0600000000001, "end": 629.5400000000001, "text": " And on the right, what you see is in the pathologists' diagnosis scenario,", "tokens": [50412, 400, 322, 264, 558, 11, 437, 291, 536, 307, 294, 264, 3100, 12256, 6, 15217, 9005, 11, 50736], "temperature": 0.0, "avg_logprob": -0.08727684550815158, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.0005699731409549713}, {"id": 147, "seek": 62210, "start": 630.26, "end": 633.14, "text": " often they would rely on some notion of similarity.", "tokens": [50772, 2049, 436, 576, 10687, 322, 512, 10710, 295, 32194, 13, 50916], "temperature": 0.0, "avg_logprob": -0.08727684550815158, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.0005699731409549713}, {"id": 148, "seek": 62210, "start": 633.14, "end": 637.38, "text": " So there are these algorithms that are designed to help people find similar images,", "tokens": [50916, 407, 456, 366, 613, 14642, 300, 366, 4761, 281, 854, 561, 915, 2531, 5267, 11, 51128], "temperature": 0.0, "avg_logprob": -0.08727684550815158, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.0005699731409549713}, {"id": 149, "seek": 62210, "start": 637.94, "end": 644.9, "text": " but then the realization that the researchers had was that people had different notions of similarity.", "tokens": [51156, 457, 550, 264, 25138, 300, 264, 10309, 632, 390, 300, 561, 632, 819, 35799, 295, 32194, 13, 51504], "temperature": 0.0, "avg_logprob": -0.08727684550815158, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.0005699731409549713}, {"id": 150, "seek": 62210, "start": 644.9, "end": 650.1, "text": " So a singular notion of similarity that was used in building an algorithm would not really suffice.", "tokens": [51504, 407, 257, 20010, 10710, 295, 32194, 300, 390, 1143, 294, 2390, 364, 9284, 576, 406, 534, 3889, 573, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08727684550815158, "compression_ratio": 1.8034934497816595, "no_speech_prob": 0.0005699731409549713}, {"id": 151, "seek": 65010, "start": 650.1, "end": 655.7, "text": " So what they ended up doing was to support three different types of similarity interaction,", "tokens": [50364, 407, 437, 436, 4590, 493, 884, 390, 281, 1406, 1045, 819, 3467, 295, 32194, 9285, 11, 50644], "temperature": 0.0, "avg_logprob": -0.10444703943589155, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.00016597428475506604}, {"id": 152, "seek": 65010, "start": 655.7, "end": 660.9, "text": " and the user was able to kind of transition between these different terms in a fluid manner,", "tokens": [50644, 293, 264, 4195, 390, 1075, 281, 733, 295, 6034, 1296, 613, 819, 2115, 294, 257, 9113, 9060, 11, 50904], "temperature": 0.0, "avg_logprob": -0.10444703943589155, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.00016597428475506604}, {"id": 153, "seek": 65010, "start": 660.9, "end": 664.58, "text": " which really gives more control and agency on the user side.", "tokens": [50904, 597, 534, 2709, 544, 1969, 293, 7934, 322, 264, 4195, 1252, 13, 51088], "temperature": 0.0, "avg_logprob": -0.10444703943589155, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.00016597428475506604}, {"id": 154, "seek": 65010, "start": 667.46, "end": 672.4200000000001, "text": " And these, you know, put in a more simple sort of diagram manner,", "tokens": [51232, 400, 613, 11, 291, 458, 11, 829, 294, 257, 544, 2199, 1333, 295, 10686, 9060, 11, 51480], "temperature": 0.0, "avg_logprob": -0.10444703943589155, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.00016597428475506604}, {"id": 155, "seek": 65010, "start": 672.4200000000001, "end": 676.9, "text": " whether you are a creator or Facebook user pathologist,", "tokens": [51480, 1968, 291, 366, 257, 14181, 420, 4384, 4195, 3100, 9201, 11, 51704], "temperature": 0.0, "avg_logprob": -0.10444703943589155, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.00016597428475506604}, {"id": 156, "seek": 67690, "start": 677.4599999999999, "end": 680.74, "text": " you seem to have some kind of a mental model of how the system works,", "tokens": [50392, 291, 1643, 281, 362, 512, 733, 295, 257, 4973, 2316, 295, 577, 264, 1185, 1985, 11, 50556], "temperature": 0.0, "avg_logprob": -0.09912765787002888, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.001000206102617085}, {"id": 157, "seek": 67690, "start": 680.74, "end": 685.3, "text": " a very sort of a classical sort of gap between what the user wants and the system wants.", "tokens": [50556, 257, 588, 1333, 295, 257, 13735, 1333, 295, 7417, 1296, 437, 264, 4195, 2738, 293, 264, 1185, 2738, 13, 50784], "temperature": 0.0, "avg_logprob": -0.09912765787002888, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.001000206102617085}, {"id": 158, "seek": 67690, "start": 685.3, "end": 689.62, "text": " And obviously, the system is not behaving in a way that you really want.", "tokens": [50784, 400, 2745, 11, 264, 1185, 307, 406, 35263, 294, 257, 636, 300, 291, 534, 528, 13, 51000], "temperature": 0.0, "avg_logprob": -0.09912765787002888, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.001000206102617085}, {"id": 159, "seek": 67690, "start": 689.62, "end": 696.9, "text": " And this gap arguably seems to be larger with these more complex black box and deep learning based systems.", "tokens": [51000, 400, 341, 7417, 26771, 2544, 281, 312, 4833, 365, 613, 544, 3997, 2211, 2424, 293, 2452, 2539, 2361, 3652, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09912765787002888, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.001000206102617085}, {"id": 160, "seek": 67690, "start": 698.74, "end": 702.02, "text": " And AI community has been tackling this problem as well.", "tokens": [51456, 400, 7318, 1768, 575, 668, 34415, 341, 1154, 382, 731, 13, 51620], "temperature": 0.0, "avg_logprob": -0.09912765787002888, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.001000206102617085}, {"id": 161, "seek": 70202, "start": 702.1, "end": 707.54, "text": " And, you know, some of the folks have been framing this as an alignment problem,", "tokens": [50368, 400, 11, 291, 458, 11, 512, 295, 264, 4024, 362, 668, 28971, 341, 382, 364, 18515, 1154, 11, 50640], "temperature": 0.0, "avg_logprob": -0.12087005318947208, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.0013225830625742674}, {"id": 162, "seek": 70202, "start": 707.54, "end": 712.26, "text": " which is about aligning the model's behavior with human intent.", "tokens": [50640, 597, 307, 466, 419, 9676, 264, 2316, 311, 5223, 365, 1952, 8446, 13, 50876], "temperature": 0.0, "avg_logprob": -0.12087005318947208, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.0013225830625742674}, {"id": 163, "seek": 70202, "start": 712.9, "end": 720.34, "text": " And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open", "tokens": [50908, 400, 337, 1365, 11, 264, 4618, 5081, 26039, 51, 293, 264, 7232, 26039, 51, 24709, 575, 668, 1333, 295, 1269, 51280], "temperature": 0.0, "avg_logprob": -0.12087005318947208, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.0013225830625742674}, {"id": 164, "seek": 70202, "start": 720.34, "end": 726.1, "text": " AI's response to the alignment problem, where their idea is, in addition to the, you know,", "tokens": [51280, 7318, 311, 4134, 281, 264, 18515, 1154, 11, 689, 641, 1558, 307, 11, 294, 4500, 281, 264, 11, 291, 458, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12087005318947208, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.0013225830625742674}, {"id": 165, "seek": 70202, "start": 726.1, "end": 731.86, "text": " basic large language model that they have, they would add this fine tuning layer with human feedback,", "tokens": [51568, 3875, 2416, 2856, 2316, 300, 436, 362, 11, 436, 576, 909, 341, 2489, 15164, 4583, 365, 1952, 5824, 11, 51856], "temperature": 0.0, "avg_logprob": -0.12087005318947208, "compression_ratio": 1.697211155378486, "no_speech_prob": 0.0013225830625742674}, {"id": 166, "seek": 73186, "start": 731.86, "end": 737.0600000000001, "text": " which often involves asking people whether, you know, they were happy with the results they got.", "tokens": [50364, 597, 2049, 11626, 3365, 561, 1968, 11, 291, 458, 11, 436, 645, 2055, 365, 264, 3542, 436, 658, 13, 50624], "temperature": 0.0, "avg_logprob": -0.08929368427821568, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00020011408196296543}, {"id": 167, "seek": 73186, "start": 737.0600000000001, "end": 743.3000000000001, "text": " And the system kind of uses that feedback to train a reinforcement learning agent to", "tokens": [50624, 400, 264, 1185, 733, 295, 4960, 300, 5824, 281, 3847, 257, 29280, 2539, 9461, 281, 50936], "temperature": 0.0, "avg_logprob": -0.08929368427821568, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00020011408196296543}, {"id": 168, "seek": 73186, "start": 743.3000000000001, "end": 748.66, "text": " do the fine tuning so that the resulting text aligns better with what the user wants.", "tokens": [50936, 360, 264, 2489, 15164, 370, 300, 264, 16505, 2487, 7975, 82, 1101, 365, 437, 264, 4195, 2738, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08929368427821568, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00020011408196296543}, {"id": 169, "seek": 73186, "start": 748.66, "end": 754.1, "text": " And they were seeing some success from it. And a quote from the paper is that", "tokens": [51204, 400, 436, 645, 2577, 512, 2245, 490, 309, 13, 400, 257, 6513, 490, 264, 3035, 307, 300, 51476], "temperature": 0.0, "avg_logprob": -0.08929368427821568, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00020011408196296543}, {"id": 170, "seek": 73186, "start": 754.1, "end": 759.46, "text": " making language models bigger does not inherently make them better at following a user's intent.", "tokens": [51476, 1455, 2856, 5245, 3801, 775, 406, 27993, 652, 552, 1101, 412, 3480, 257, 4195, 311, 8446, 13, 51744], "temperature": 0.0, "avg_logprob": -0.08929368427821568, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00020011408196296543}, {"id": 171, "seek": 75946, "start": 760.4200000000001, "end": 766.58, "text": " And aligning language models with user intent on a wide range of tasks by fine tuning with human feedback.", "tokens": [50412, 400, 419, 9676, 2856, 5245, 365, 4195, 8446, 322, 257, 4874, 3613, 295, 9608, 538, 2489, 15164, 365, 1952, 5824, 13, 50720], "temperature": 0.0, "avg_logprob": -0.13297037456346594, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.00039189562085084617}, {"id": 172, "seek": 75946, "start": 766.58, "end": 771.94, "text": " And of course, there's been a lot of discussion about whether this is really the most promising way", "tokens": [50720, 400, 295, 1164, 11, 456, 311, 668, 257, 688, 295, 5017, 466, 1968, 341, 307, 534, 264, 881, 20257, 636, 50988], "temperature": 0.0, "avg_logprob": -0.13297037456346594, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.00039189562085084617}, {"id": 173, "seek": 75946, "start": 771.94, "end": 777.94, "text": " to, you know, involve humans or alignment problem. But I think this is some progress towards that direction.", "tokens": [50988, 281, 11, 291, 458, 11, 9494, 6255, 420, 18515, 1154, 13, 583, 286, 519, 341, 307, 512, 4205, 3030, 300, 3513, 13, 51288], "temperature": 0.0, "avg_logprob": -0.13297037456346594, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.00039189562085084617}, {"id": 174, "seek": 75946, "start": 778.82, "end": 786.9000000000001, "text": " But all of these examples, I would say, basically lead us to revisit these classical notions of", "tokens": [51332, 583, 439, 295, 613, 5110, 11, 286, 576, 584, 11, 1936, 1477, 505, 281, 32676, 613, 13735, 35799, 295, 51736], "temperature": 0.0, "avg_logprob": -0.13297037456346594, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.00039189562085084617}, {"id": 175, "seek": 78690, "start": 786.9, "end": 793.3, "text": " Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s, right?", "tokens": [50364, 23033, 295, 17662, 1448, 293, 23033, 295, 462, 46504, 10348, 538, 1468, 30475, 646, 294, 264, 13626, 82, 11, 558, 30, 50684], "temperature": 0.0, "avg_logprob": -0.11332239963040494, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0006359876715578139}, {"id": 176, "seek": 78690, "start": 793.3, "end": 799.3, "text": " As a user, they want to know what's going on with the system, and they want to have more control and agency.", "tokens": [50684, 1018, 257, 4195, 11, 436, 528, 281, 458, 437, 311, 516, 322, 365, 264, 1185, 11, 293, 436, 528, 281, 362, 544, 1969, 293, 7934, 13, 50984], "temperature": 0.0, "avg_logprob": -0.11332239963040494, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0006359876715578139}, {"id": 177, "seek": 78690, "start": 799.3, "end": 804.9, "text": " And on the evaluation side, when AI gives you some kind of result, they want to be able to", "tokens": [50984, 400, 322, 264, 13344, 1252, 11, 562, 7318, 2709, 291, 512, 733, 295, 1874, 11, 436, 528, 281, 312, 1075, 281, 51264], "temperature": 0.0, "avg_logprob": -0.11332239963040494, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0006359876715578139}, {"id": 178, "seek": 78690, "start": 804.9, "end": 811.6999999999999, "text": " understand it, interpret it, and want to get some explanation of it. And as an HCI researcher who's", "tokens": [51264, 1223, 309, 11, 7302, 309, 11, 293, 528, 281, 483, 512, 10835, 295, 309, 13, 400, 382, 364, 389, 25240, 21751, 567, 311, 51604], "temperature": 0.0, "avg_logprob": -0.11332239963040494, "compression_ratio": 1.6008230452674896, "no_speech_prob": 0.0006359876715578139}, {"id": 179, "seek": 81170, "start": 811.7, "end": 817.1400000000001, "text": " building these interactive systems, I feel like in often cases, I try to bridge these gaps.", "tokens": [50364, 2390, 613, 15141, 3652, 11, 286, 841, 411, 294, 2049, 3331, 11, 286, 853, 281, 7283, 613, 15031, 13, 50636], "temperature": 0.0, "avg_logprob": -0.0742076591209129, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.002933216281235218}, {"id": 180, "seek": 81170, "start": 817.1400000000001, "end": 823.0600000000001, "text": " I come up with new ways of designing these social interactions and human AI interactions", "tokens": [50636, 286, 808, 493, 365, 777, 2098, 295, 14685, 613, 2093, 13280, 293, 1952, 7318, 13280, 50932], "temperature": 0.0, "avg_logprob": -0.0742076591209129, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.002933216281235218}, {"id": 181, "seek": 81170, "start": 823.0600000000001, "end": 828.5, "text": " in a way that tries to bridge these gaps. And these are just some of the systems that I've been", "tokens": [50932, 294, 257, 636, 300, 9898, 281, 7283, 613, 15031, 13, 400, 613, 366, 445, 512, 295, 264, 3652, 300, 286, 600, 668, 51204], "temperature": 0.0, "avg_logprob": -0.0742076591209129, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.002933216281235218}, {"id": 182, "seek": 81170, "start": 828.5, "end": 835.1400000000001, "text": " developing in different application domains. And I think many of them have somewhat succeeded", "tokens": [51204, 6416, 294, 819, 3861, 25514, 13, 400, 286, 519, 867, 295, 552, 362, 8344, 20263, 51536], "temperature": 0.0, "avg_logprob": -0.0742076591209129, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.002933216281235218}, {"id": 183, "seek": 81170, "start": 835.1400000000001, "end": 839.22, "text": " in bridging these gaps. But other times, to be honest, we haven't done a good job of doing that.", "tokens": [51536, 294, 16362, 3249, 613, 15031, 13, 583, 661, 1413, 11, 281, 312, 3245, 11, 321, 2378, 380, 1096, 257, 665, 1691, 295, 884, 300, 13, 51740], "temperature": 0.0, "avg_logprob": -0.0742076591209129, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.002933216281235218}, {"id": 184, "seek": 83922, "start": 840.1800000000001, "end": 845.7, "text": " So what I want to do for the remaining time for this talk is to share some of these lessons,", "tokens": [50412, 407, 437, 286, 528, 281, 360, 337, 264, 8877, 565, 337, 341, 751, 307, 281, 2073, 512, 295, 613, 8820, 11, 50688], "temperature": 0.0, "avg_logprob": -0.07200086116790771, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0009393381187692285}, {"id": 185, "seek": 83922, "start": 845.7, "end": 852.58, "text": " and some of them from positive experiences, but other times, bitter experiences by something", "tokens": [50688, 293, 512, 295, 552, 490, 3353, 5235, 11, 457, 661, 1413, 11, 13871, 5235, 538, 746, 51032], "temperature": 0.0, "avg_logprob": -0.07200086116790771, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0009393381187692285}, {"id": 186, "seek": 83922, "start": 852.58, "end": 858.26, "text": " that we haven't really done a good job of. And the main message that I want to send across", "tokens": [51032, 300, 321, 2378, 380, 534, 1096, 257, 665, 1691, 295, 13, 400, 264, 2135, 3636, 300, 286, 528, 281, 2845, 2108, 51316], "temperature": 0.0, "avg_logprob": -0.07200086116790771, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0009393381187692285}, {"id": 187, "seek": 83922, "start": 858.26, "end": 863.86, "text": " is that beyond these point solutions for this system that works in this particular context,", "tokens": [51316, 307, 300, 4399, 613, 935, 6547, 337, 341, 1185, 300, 1985, 294, 341, 1729, 4319, 11, 51596], "temperature": 0.0, "avg_logprob": -0.07200086116790771, "compression_ratio": 1.6803652968036529, "no_speech_prob": 0.0009393381187692285}, {"id": 188, "seek": 86386, "start": 863.86, "end": 868.5, "text": " we've seen some success, I think as a field, we really need to start thinking about,", "tokens": [50364, 321, 600, 1612, 512, 2245, 11, 286, 519, 382, 257, 2519, 11, 321, 534, 643, 281, 722, 1953, 466, 11, 50596], "temperature": 0.0, "avg_logprob": -0.08177408880116988, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.014481207355856895}, {"id": 189, "seek": 86386, "start": 869.14, "end": 874.9, "text": " can we do something more systematic and sustainable? Or empower designers and developers in thinking", "tokens": [50628, 393, 321, 360, 746, 544, 27249, 293, 11235, 30, 1610, 11071, 16196, 293, 8849, 294, 1953, 50916], "temperature": 0.0, "avg_logprob": -0.08177408880116988, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.014481207355856895}, {"id": 190, "seek": 86386, "start": 874.9, "end": 882.5, "text": " about can we develop these AI applications that are more usable and useful for more groups of people", "tokens": [50916, 466, 393, 321, 1499, 613, 7318, 5821, 300, 366, 544, 29975, 293, 4420, 337, 544, 3935, 295, 561, 51296], "temperature": 0.0, "avg_logprob": -0.08177408880116988, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.014481207355856895}, {"id": 191, "seek": 86386, "start": 882.5, "end": 888.34, "text": " rather than having to reinvent the wheel each time someone has to develop these applications.", "tokens": [51296, 2831, 813, 1419, 281, 33477, 264, 5589, 1184, 565, 1580, 575, 281, 1499, 613, 5821, 13, 51588], "temperature": 0.0, "avg_logprob": -0.08177408880116988, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.014481207355856895}, {"id": 192, "seek": 86386, "start": 888.34, "end": 891.54, "text": " And I think we're seeing too many of these cases where people are like,", "tokens": [51588, 400, 286, 519, 321, 434, 2577, 886, 867, 295, 613, 3331, 689, 561, 366, 411, 11, 51748], "temperature": 0.0, "avg_logprob": -0.08177408880116988, "compression_ratio": 1.751937984496124, "no_speech_prob": 0.014481207355856895}, {"id": 193, "seek": 89154, "start": 892.26, "end": 896.5799999999999, "text": " there's this cool model, let's build something around it, and it just gets released in a few", "tokens": [50400, 456, 311, 341, 1627, 2316, 11, 718, 311, 1322, 746, 926, 309, 11, 293, 309, 445, 2170, 4736, 294, 257, 1326, 50616], "temperature": 0.0, "avg_logprob": -0.07426640510559082, "compression_ratio": 1.588679245283019, "no_speech_prob": 0.0002692177367862314}, {"id": 194, "seek": 89154, "start": 896.5799999999999, "end": 901.78, "text": " days and realizes that people want it in a completely different way, people abuse it,", "tokens": [50616, 1708, 293, 29316, 300, 561, 528, 309, 294, 257, 2584, 819, 636, 11, 561, 9852, 309, 11, 50876], "temperature": 0.0, "avg_logprob": -0.07426640510559082, "compression_ratio": 1.588679245283019, "no_speech_prob": 0.0002692177367862314}, {"id": 195, "seek": 89154, "start": 901.78, "end": 905.86, "text": " a few days later, it goes down. We're seeing too many of these failure cases.", "tokens": [50876, 257, 1326, 1708, 1780, 11, 309, 1709, 760, 13, 492, 434, 2577, 886, 867, 295, 613, 7763, 3331, 13, 51080], "temperature": 0.0, "avg_logprob": -0.07426640510559082, "compression_ratio": 1.588679245283019, "no_speech_prob": 0.0002692177367862314}, {"id": 196, "seek": 89154, "start": 907.4599999999999, "end": 912.0999999999999, "text": " So from the HCI point of view, I think HCI research can really advance this", "tokens": [51160, 407, 490, 264, 389, 25240, 935, 295, 1910, 11, 286, 519, 389, 25240, 2132, 393, 534, 7295, 341, 51392], "temperature": 0.0, "avg_logprob": -0.07426640510559082, "compression_ratio": 1.588679245283019, "no_speech_prob": 0.0002692177367862314}, {"id": 197, "seek": 89154, "start": 913.4599999999999, "end": 918.5799999999999, "text": " interaction-centric AI by contributing these generalizable building blocks for designing", "tokens": [51460, 9285, 12, 45300, 7318, 538, 19270, 613, 2674, 22395, 2390, 8474, 337, 14685, 51716], "temperature": 0.0, "avg_logprob": -0.07426640510559082, "compression_ratio": 1.588679245283019, "no_speech_prob": 0.0002692177367862314}, {"id": 198, "seek": 91858, "start": 918.58, "end": 925.5400000000001, "text": " these systems and interface affordances. And AI research can also advance by embracing the idea", "tokens": [50364, 613, 3652, 293, 9226, 6157, 2676, 13, 400, 7318, 2132, 393, 611, 7295, 538, 31596, 264, 1558, 50712], "temperature": 0.0, "avg_logprob": -0.07823463678359985, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0004438671167008579}, {"id": 199, "seek": 91858, "start": 925.5400000000001, "end": 931.5400000000001, "text": " of interaction-centric AI by rethinking models, architecture design, benchmarks, metrics, and", "tokens": [50712, 295, 9285, 12, 45300, 7318, 538, 319, 39873, 5245, 11, 9482, 1715, 11, 43751, 11, 16367, 11, 293, 51012], "temperature": 0.0, "avg_logprob": -0.07823463678359985, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0004438671167008579}, {"id": 200, "seek": 91858, "start": 931.5400000000001, "end": 939.38, "text": " research process. The part of it has to involve sort of broadening the perspective beyond just", "tokens": [51012, 2132, 1399, 13, 440, 644, 295, 309, 575, 281, 9494, 1333, 295, 4152, 4559, 264, 4585, 4399, 445, 51404], "temperature": 0.0, "avg_logprob": -0.07823463678359985, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0004438671167008579}, {"id": 201, "seek": 91858, "start": 939.38, "end": 946.0200000000001, "text": " thinking about the model and the output that it generates to think about the users behind those", "tokens": [51404, 1953, 466, 264, 2316, 293, 264, 5598, 300, 309, 23815, 281, 519, 466, 264, 5022, 2261, 729, 51736], "temperature": 0.0, "avg_logprob": -0.07823463678359985, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0004438671167008579}, {"id": 202, "seek": 94602, "start": 946.02, "end": 951.22, "text": " and their mental models. And often there's not just a single user, but a group of user,", "tokens": [50364, 293, 641, 4973, 5245, 13, 400, 2049, 456, 311, 406, 445, 257, 2167, 4195, 11, 457, 257, 1594, 295, 4195, 11, 50624], "temperature": 0.0, "avg_logprob": -0.1080392801536704, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0015002997824922204}, {"id": 203, "seek": 94602, "start": 951.22, "end": 956.02, "text": " community of user, a society of users. And there's also the temporal dimension,", "tokens": [50624, 1768, 295, 4195, 11, 257, 4086, 295, 5022, 13, 400, 456, 311, 611, 264, 30881, 10139, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1080392801536704, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0015002997824922204}, {"id": 204, "seek": 94602, "start": 956.02, "end": 961.78, "text": " like before the user comes in and tries to use a system, we should be asking the questions about", "tokens": [50864, 411, 949, 264, 4195, 1487, 294, 293, 9898, 281, 764, 257, 1185, 11, 321, 820, 312, 3365, 264, 1651, 466, 51152], "temperature": 0.0, "avg_logprob": -0.1080392801536704, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0015002997824922204}, {"id": 205, "seek": 94602, "start": 961.78, "end": 967.38, "text": " like, what's the task and who are these users and why and how. And during the interaction,", "tokens": [51152, 411, 11, 437, 311, 264, 5633, 293, 567, 366, 613, 5022, 293, 983, 293, 577, 13, 400, 1830, 264, 9285, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1080392801536704, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0015002997824922204}, {"id": 206, "seek": 94602, "start": 967.38, "end": 972.42, "text": " we need to be thinking about presentation visualization. And the other way around as", "tokens": [51432, 321, 643, 281, 312, 1953, 466, 5860, 25801, 13, 400, 264, 661, 636, 926, 382, 51684], "temperature": 0.0, "avg_logprob": -0.1080392801536704, "compression_ratio": 1.8032786885245902, "no_speech_prob": 0.0015002997824922204}, {"id": 207, "seek": 97242, "start": 972.42, "end": 976.5, "text": " well, like interpretable results are being presented to the user. Do they have a way to", "tokens": [50364, 731, 11, 411, 7302, 712, 3542, 366, 885, 8212, 281, 264, 4195, 13, 1144, 436, 362, 257, 636, 281, 50568], "temperature": 0.0, "avg_logprob": -0.08376696705818176, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0012633131118491292}, {"id": 208, "seek": 97242, "start": 976.5, "end": 982.26, "text": " provide feedback to the system? And also, it's never going to be just a single use, right? People", "tokens": [50568, 2893, 5824, 281, 264, 1185, 30, 400, 611, 11, 309, 311, 1128, 516, 281, 312, 445, 257, 2167, 764, 11, 558, 30, 3432, 50856], "temperature": 0.0, "avg_logprob": -0.08376696705818176, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0012633131118491292}, {"id": 209, "seek": 97242, "start": 982.26, "end": 987.9399999999999, "text": " would want to come back and use a system for a sustained amount of time. In those cases, people's", "tokens": [50856, 576, 528, 281, 808, 646, 293, 764, 257, 1185, 337, 257, 23389, 2372, 295, 565, 13, 682, 729, 3331, 11, 561, 311, 51140], "temperature": 0.0, "avg_logprob": -0.08376696705818176, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0012633131118491292}, {"id": 210, "seek": 97242, "start": 987.9399999999999, "end": 995.2199999999999, "text": " mental model would evolve. And what does it mean for the system? And so I think this is sort of", "tokens": [51140, 4973, 2316, 576, 16693, 13, 400, 437, 775, 309, 914, 337, 264, 1185, 30, 400, 370, 286, 519, 341, 307, 1333, 295, 51504], "temperature": 0.0, "avg_logprob": -0.08376696705818176, "compression_ratio": 1.59915611814346, "no_speech_prob": 0.0012633131118491292}, {"id": 211, "seek": 99522, "start": 995.22, "end": 1003.5400000000001, "text": " the ecosystem that I have in mind. And with these, I want to dive into these specific examples", "tokens": [50364, 264, 11311, 300, 286, 362, 294, 1575, 13, 400, 365, 613, 11, 286, 528, 281, 9192, 666, 613, 2685, 5110, 50780], "temperature": 0.0, "avg_logprob": -0.09866955113965412, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0009536580182611942}, {"id": 212, "seek": 99522, "start": 1003.5400000000001, "end": 1010.1800000000001, "text": " where we designed human-AI interactions. And I identify four major challenges", "tokens": [50780, 689, 321, 4761, 1952, 12, 48698, 13280, 13, 400, 286, 5876, 1451, 2563, 4759, 51112], "temperature": 0.0, "avg_logprob": -0.09866955113965412, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0009536580182611942}, {"id": 213, "seek": 99522, "start": 1012.34, "end": 1017.94, "text": " in terms of human-AI interaction. The first one is about bridging the accuracy gap.", "tokens": [51220, 294, 2115, 295, 1952, 12, 48698, 9285, 13, 440, 700, 472, 307, 466, 16362, 3249, 264, 14170, 7417, 13, 51500], "temperature": 0.0, "avg_logprob": -0.09866955113965412, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0009536580182611942}, {"id": 214, "seek": 99522, "start": 1019.62, "end": 1024.66, "text": " So I'm on my sabbatical now. I'm working with this startup called Ringle, where they are", "tokens": [51584, 407, 286, 478, 322, 452, 5560, 11980, 804, 586, 13, 286, 478, 1364, 365, 341, 18578, 1219, 19844, 306, 11, 689, 436, 366, 51836], "temperature": 0.0, "avg_logprob": -0.09866955113965412, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.0009536580182611942}, {"id": 215, "seek": 102466, "start": 1024.66, "end": 1030.26, "text": " basically Uber for language learning. They are matching tutors and tuties, and they have this", "tokens": [50364, 1936, 21839, 337, 2856, 2539, 13, 814, 366, 14324, 3672, 830, 293, 3672, 530, 11, 293, 436, 362, 341, 50644], "temperature": 0.0, "avg_logprob": -0.08418052624433468, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0009101160103455186}, {"id": 216, "seek": 102466, "start": 1030.26, "end": 1036.5, "text": " video-based language tutoring session. So what we try to do here is to build this diagnostic", "tokens": [50644, 960, 12, 6032, 2856, 44410, 5481, 13, 407, 437, 321, 853, 281, 360, 510, 307, 281, 1322, 341, 27897, 50956], "temperature": 0.0, "avg_logprob": -0.08418052624433468, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0009101160103455186}, {"id": 217, "seek": 102466, "start": 1036.5, "end": 1041.94, "text": " service based on analyzing the chat-based tutoring session to give people personalized", "tokens": [50956, 2643, 2361, 322, 23663, 264, 5081, 12, 6032, 44410, 5481, 281, 976, 561, 28415, 51228], "temperature": 0.0, "avg_logprob": -0.08418052624433468, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0009101160103455186}, {"id": 218, "seek": 102466, "start": 1041.94, "end": 1048.5800000000002, "text": " feedback and suggestions for improvement. But instead of going into the details of the service", "tokens": [51228, 5824, 293, 13396, 337, 10444, 13, 583, 2602, 295, 516, 666, 264, 4365, 295, 264, 2643, 51560], "temperature": 0.0, "avg_logprob": -0.08418052624433468, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0009101160103455186}, {"id": 219, "seek": 104858, "start": 1048.58, "end": 1055.3, "text": " itself, I want to touch upon the case that we ran into when we were trying to run this automated", "tokens": [50364, 2564, 11, 286, 528, 281, 2557, 3564, 264, 1389, 300, 321, 5872, 666, 562, 321, 645, 1382, 281, 1190, 341, 18473, 50700], "temperature": 0.0, "avg_logprob": -0.0898683182028837, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0027111058589071035}, {"id": 220, "seek": 104858, "start": 1055.3, "end": 1062.1, "text": " speech recognition AI, which is crucial in sort of turning the video-based chat into text format,", "tokens": [50700, 6218, 11150, 7318, 11, 597, 307, 11462, 294, 1333, 295, 6246, 264, 960, 12, 6032, 5081, 666, 2487, 7877, 11, 51040], "temperature": 0.0, "avg_logprob": -0.0898683182028837, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0027111058589071035}, {"id": 221, "seek": 104858, "start": 1062.6599999999999, "end": 1066.98, "text": " which is really required for us to run all these diagnostic algorithms on top of.", "tokens": [51068, 597, 307, 534, 4739, 337, 505, 281, 1190, 439, 613, 27897, 14642, 322, 1192, 295, 13, 51284], "temperature": 0.0, "avg_logprob": -0.0898683182028837, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0027111058589071035}, {"id": 222, "seek": 104858, "start": 1068.4199999999998, "end": 1076.02, "text": " And the standard metric of success in ASR would be word error rate, how correctly it can", "tokens": [51356, 400, 264, 3832, 20678, 295, 2245, 294, 7469, 49, 576, 312, 1349, 6713, 3314, 11, 577, 8944, 309, 393, 51736], "temperature": 0.0, "avg_logprob": -0.0898683182028837, "compression_ratio": 1.5665236051502145, "no_speech_prob": 0.0027111058589071035}, {"id": 223, "seek": 107602, "start": 1076.58, "end": 1084.18, "text": " recover the original text. And on the tutor side, when we ran ASR on like hundreds and", "tokens": [50392, 8114, 264, 3380, 2487, 13, 400, 322, 264, 35613, 1252, 11, 562, 321, 5872, 7469, 49, 322, 411, 6779, 293, 50772], "temperature": 0.0, "avg_logprob": -0.11558947843663833, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.00030508116469718516}, {"id": 224, "seek": 107602, "start": 1084.18, "end": 1091.06, "text": " thousands of sessions, the average word error rate was around 8%. Can you take a guess as to what", "tokens": [50772, 5383, 295, 11081, 11, 264, 4274, 1349, 6713, 3314, 390, 926, 1649, 6856, 1664, 291, 747, 257, 2041, 382, 281, 437, 51116], "temperature": 0.0, "avg_logprob": -0.11558947843663833, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.00030508116469718516}, {"id": 225, "seek": 107602, "start": 1091.06, "end": 1098.98, "text": " the number would have been for students? Obviously there's this white margin that's quite high,", "tokens": [51116, 264, 1230, 576, 362, 668, 337, 1731, 30, 7580, 456, 311, 341, 2418, 10270, 300, 311, 1596, 1090, 11, 51512], "temperature": 0.0, "avg_logprob": -0.11558947843663833, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.00030508116469718516}, {"id": 226, "seek": 109898, "start": 1099.06, "end": 1108.34, "text": " so you can imagine, 30. Yeah, we're seeing 23. So there's quite a bit of a gap. And this is an", "tokens": [50368, 370, 291, 393, 3811, 11, 2217, 13, 865, 11, 321, 434, 2577, 6673, 13, 407, 456, 311, 1596, 257, 857, 295, 257, 7417, 13, 400, 341, 307, 364, 50832], "temperature": 0.0, "avg_logprob": -0.10457042937583112, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0021481511648744345}, {"id": 227, "seek": 109898, "start": 1108.34, "end": 1114.74, "text": " example of an accuracy gap where different groups of users are getting disproportionate results from", "tokens": [50832, 1365, 295, 364, 14170, 7417, 689, 819, 3935, 295, 5022, 366, 1242, 28734, 473, 3542, 490, 51152], "temperature": 0.0, "avg_logprob": -0.10457042937583112, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0021481511648744345}, {"id": 228, "seek": 109898, "start": 1114.74, "end": 1121.6200000000001, "text": " the same AI. And the gap actually widens if we look at like the best tutor and the worst student", "tokens": [51152, 264, 912, 7318, 13, 400, 264, 7417, 767, 5274, 694, 498, 321, 574, 412, 411, 264, 1151, 35613, 293, 264, 5855, 3107, 51496], "temperature": 0.0, "avg_logprob": -0.10457042937583112, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0021481511648744345}, {"id": 229, "seek": 109898, "start": 1121.6200000000001, "end": 1127.38, "text": " when it comes to the performance of these models. But in terms of thinking about the interaction", "tokens": [51496, 562, 309, 1487, 281, 264, 3389, 295, 613, 5245, 13, 583, 294, 2115, 295, 1953, 466, 264, 9285, 51784], "temperature": 0.0, "avg_logprob": -0.10457042937583112, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.0021481511648744345}, {"id": 230, "seek": 112738, "start": 1127.38, "end": 1133.6200000000001, "text": " that these people are trying to have with this AI, I would argue that the students are the ones", "tokens": [50364, 300, 613, 561, 366, 1382, 281, 362, 365, 341, 7318, 11, 286, 576, 9695, 300, 264, 1731, 366, 264, 2306, 50676], "temperature": 0.0, "avg_logprob": -0.06918860011630588, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017536637606099248}, {"id": 231, "seek": 112738, "start": 1133.6200000000001, "end": 1139.6200000000001, "text": " who really need this AI to work. Based on the accuracy of this AI, they want to kind of look", "tokens": [50676, 567, 534, 643, 341, 7318, 281, 589, 13, 18785, 322, 264, 14170, 295, 341, 7318, 11, 436, 528, 281, 733, 295, 574, 50976], "temperature": 0.0, "avg_logprob": -0.06918860011630588, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017536637606099248}, {"id": 232, "seek": 112738, "start": 1139.6200000000001, "end": 1146.42, "text": " at where they succeeded and failed and they want to learn and reflect. And with this low accuracy,", "tokens": [50976, 412, 689, 436, 20263, 293, 7612, 293, 436, 528, 281, 1466, 293, 5031, 13, 400, 365, 341, 2295, 14170, 11, 51316], "temperature": 0.0, "avg_logprob": -0.06918860011630588, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017536637606099248}, {"id": 233, "seek": 112738, "start": 1146.42, "end": 1151.94, "text": " they would really be struggling to come up with good action items and they might be frustrated,", "tokens": [51316, 436, 576, 534, 312, 9314, 281, 808, 493, 365, 665, 3069, 4754, 293, 436, 1062, 312, 15751, 11, 51592], "temperature": 0.0, "avg_logprob": -0.06918860011630588, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.0017536637606099248}, {"id": 234, "seek": 115194, "start": 1151.94, "end": 1157.22, "text": " they might lose trust on the system. But interestingly, a lot of focus when it comes to", "tokens": [50364, 436, 1062, 3624, 3361, 322, 264, 1185, 13, 583, 25873, 11, 257, 688, 295, 1879, 562, 309, 1487, 281, 50628], "temperature": 0.0, "avg_logprob": -0.10607189602322048, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0012642124202102423}, {"id": 235, "seek": 115194, "start": 1157.22, "end": 1164.18, "text": " model development is that we seem to be focusing on the 6%, like making the 6% better instead of", "tokens": [50628, 2316, 3250, 307, 300, 321, 1643, 281, 312, 8416, 322, 264, 1386, 8923, 411, 1455, 264, 1386, 4, 1101, 2602, 295, 50976], "temperature": 0.0, "avg_logprob": -0.10607189602322048, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0012642124202102423}, {"id": 236, "seek": 115194, "start": 1164.18, "end": 1170.42, "text": " narrowing the gap between 6 and 36%. And we have to really be asking like, what is the most important", "tokens": [50976, 9432, 278, 264, 7417, 1296, 1386, 293, 8652, 6856, 400, 321, 362, 281, 534, 312, 3365, 411, 11, 437, 307, 264, 881, 1021, 51288], "temperature": 0.0, "avg_logprob": -0.10607189602322048, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0012642124202102423}, {"id": 237, "seek": 115194, "start": 1170.42, "end": 1174.66, "text": " question in this context? And are we really focusing on the most important question here?", "tokens": [51288, 1168, 294, 341, 4319, 30, 400, 366, 321, 534, 8416, 322, 264, 881, 1021, 1168, 510, 30, 51500], "temperature": 0.0, "avg_logprob": -0.10607189602322048, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.0012642124202102423}, {"id": 238, "seek": 117466, "start": 1175.46, "end": 1182.5800000000002, "text": " And we see these other examples too, where Tyra and others have studied the machine translation", "tokens": [50404, 400, 321, 536, 613, 661, 5110, 886, 11, 689, 5569, 424, 293, 2357, 362, 9454, 264, 3479, 12853, 50760], "temperature": 0.0, "avg_logprob": -0.19062495541262936, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0006769038154743612}, {"id": 239, "seek": 117466, "start": 1182.5800000000002, "end": 1188.1000000000001, "text": " that is being used in emergency rooms when it comes to discharge statements that are presented", "tokens": [50760, 300, 307, 885, 1143, 294, 7473, 9396, 562, 309, 1487, 281, 21718, 12363, 300, 366, 8212, 51036], "temperature": 0.0, "avg_logprob": -0.19062495541262936, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0006769038154743612}, {"id": 240, "seek": 117466, "start": 1188.1000000000001, "end": 1194.66, "text": " to patients and patients' families. And we see a huge disparity between different languages.", "tokens": [51036, 281, 4209, 293, 4209, 6, 4466, 13, 400, 321, 536, 257, 2603, 47415, 1296, 819, 8650, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19062495541262936, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0006769038154743612}, {"id": 241, "seek": 117466, "start": 1194.66, "end": 1201.38, "text": " And in the natural language processing community, this support for low resource languages has been", "tokens": [51364, 400, 294, 264, 3303, 2856, 9007, 1768, 11, 341, 1406, 337, 2295, 7684, 8650, 575, 668, 51700], "temperature": 0.0, "avg_logprob": -0.19062495541262936, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0006769038154743612}, {"id": 242, "seek": 120138, "start": 1202.1000000000001, "end": 1207.7, "text": " a topic for research and there has been great efforts. And on the right is a famous example", "tokens": [50400, 257, 4829, 337, 2132, 293, 456, 575, 668, 869, 6484, 13, 400, 322, 264, 558, 307, 257, 4618, 1365, 50680], "temperature": 0.0, "avg_logprob": -0.18716638882954914, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0011862292885780334}, {"id": 243, "seek": 120138, "start": 1207.7, "end": 1215.7, "text": " of gender shades, where the gender classification algorithm shows, again, an accuracy disparity", "tokens": [50680, 295, 7898, 20639, 11, 689, 264, 7898, 21538, 9284, 3110, 11, 797, 11, 364, 14170, 47415, 51080], "temperature": 0.0, "avg_logprob": -0.18716638882954914, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0011862292885780334}, {"id": 244, "seek": 120138, "start": 1215.7, "end": 1224.9, "text": " between darker skin female versus lighter scale male. And of course, these diversity and inclusion", "tokens": [51080, 1296, 12741, 3178, 6556, 5717, 11546, 4373, 7133, 13, 400, 295, 1164, 11, 613, 8811, 293, 15874, 51540], "temperature": 0.0, "avg_logprob": -0.18716638882954914, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0011862292885780334}, {"id": 245, "seek": 122490, "start": 1225.7, "end": 1230.98, "text": " efforts and low resource language support research in the AI community and in the community", "tokens": [50404, 6484, 293, 2295, 7684, 2856, 1406, 2132, 294, 264, 7318, 1768, 293, 294, 264, 1768, 50668], "temperature": 0.0, "avg_logprob": -0.10454138019416906, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0320451483130455}, {"id": 246, "seek": 122490, "start": 1230.98, "end": 1236.5800000000002, "text": " have been tackling these issues of accuracy gap, of course. But then I would argue that they could", "tokens": [50668, 362, 668, 34415, 613, 2663, 295, 14170, 7417, 11, 295, 1164, 13, 583, 550, 286, 576, 9695, 300, 436, 727, 50948], "temperature": 0.0, "avg_logprob": -0.10454138019416906, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0320451483130455}, {"id": 247, "seek": 122490, "start": 1237.38, "end": 1243.94, "text": " advance further by embracing more interaction-centric approach in trying to really see how in the real", "tokens": [50988, 7295, 3052, 538, 31596, 544, 9285, 12, 45300, 3109, 294, 1382, 281, 534, 536, 577, 294, 264, 957, 51316], "temperature": 0.0, "avg_logprob": -0.10454138019416906, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0320451483130455}, {"id": 248, "seek": 122490, "start": 1243.94, "end": 1249.38, "text": " world people are interacting with these results and what kind of actual struggles that they have", "tokens": [51316, 1002, 561, 366, 18017, 365, 613, 3542, 293, 437, 733, 295, 3539, 17592, 300, 436, 362, 51588], "temperature": 0.0, "avg_logprob": -0.10454138019416906, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0320451483130455}, {"id": 249, "seek": 124938, "start": 1249.38, "end": 1257.46, "text": " because of poor or good AI accuracy and what, as a community, how can we define the problem that's", "tokens": [50364, 570, 295, 4716, 420, 665, 7318, 14170, 293, 437, 11, 382, 257, 1768, 11, 577, 393, 321, 6964, 264, 1154, 300, 311, 50768], "temperature": 0.0, "avg_logprob": -0.1047982828957694, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.0006767677841708064}, {"id": 250, "seek": 124938, "start": 1257.46, "end": 1265.7800000000002, "text": " most important. And conceptually speaking, I feel like a good analogy might be the ceiling and floor", "tokens": [50768, 881, 1021, 13, 400, 3410, 671, 4124, 11, 286, 841, 411, 257, 665, 21663, 1062, 312, 264, 13655, 293, 4123, 51184], "temperature": 0.0, "avg_logprob": -0.1047982828957694, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.0006767677841708064}, {"id": 251, "seek": 124938, "start": 1265.7800000000002, "end": 1272.74, "text": " analogy. The ceiling would be this primary user group who gets the best part of AI. And floor", "tokens": [51184, 21663, 13, 440, 13655, 576, 312, 341, 6194, 4195, 1594, 567, 2170, 264, 1151, 644, 295, 7318, 13, 400, 4123, 51532], "temperature": 0.0, "avg_logprob": -0.1047982828957694, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.0006767677841708064}, {"id": 252, "seek": 127274, "start": 1272.74, "end": 1279.54, "text": " would be secondary user group who is disproportionately getting more negative impact of", "tokens": [50364, 576, 312, 11396, 4195, 1594, 567, 307, 43397, 1242, 544, 3671, 2712, 295, 50704], "temperature": 0.0, "avg_logprob": -0.10161237938459529, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.005906873848289251}, {"id": 253, "seek": 127274, "start": 1279.54, "end": 1285.78, "text": " the same AI. And there's this accuracy gap. And often I feel like taking a model-centric approach", "tokens": [50704, 264, 912, 7318, 13, 400, 456, 311, 341, 14170, 7417, 13, 400, 2049, 286, 841, 411, 1940, 257, 2316, 12, 45300, 3109, 51016], "temperature": 0.0, "avg_logprob": -0.10161237938459529, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.005906873848289251}, {"id": 254, "seek": 127274, "start": 1285.78, "end": 1292.34, "text": " incentivizes people and researchers to work on raising the ceiling. There could be a couple", "tokens": [51016, 35328, 5660, 561, 293, 10309, 281, 589, 322, 11225, 264, 13655, 13, 821, 727, 312, 257, 1916, 51344], "temperature": 0.0, "avg_logprob": -0.10161237938459529, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.005906873848289251}, {"id": 255, "seek": 127274, "start": 1292.34, "end": 1297.46, "text": " reasons for this. First of all, that's the sota number you get, which might be what you need to", "tokens": [51344, 4112, 337, 341, 13, 2386, 295, 439, 11, 300, 311, 264, 262, 5377, 1230, 291, 483, 11, 597, 1062, 312, 437, 291, 643, 281, 51600], "temperature": 0.0, "avg_logprob": -0.10161237938459529, "compression_ratio": 1.5413223140495869, "no_speech_prob": 0.005906873848289251}, {"id": 256, "seek": 129746, "start": 1297.46, "end": 1303.06, "text": " publish a paper out of it. Or the benchmarks that you're working with do not really have much data", "tokens": [50364, 11374, 257, 3035, 484, 295, 309, 13, 1610, 264, 43751, 300, 291, 434, 1364, 365, 360, 406, 534, 362, 709, 1412, 50644], "temperature": 0.0, "avg_logprob": -0.07768333935346759, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0014100221451371908}, {"id": 257, "seek": 129746, "start": 1303.94, "end": 1308.5, "text": " on the floor side. It's maybe more focused on the ceiling side. And that's why the ceiling is there", "tokens": [50688, 322, 264, 4123, 1252, 13, 467, 311, 1310, 544, 5178, 322, 264, 13655, 1252, 13, 400, 300, 311, 983, 264, 13655, 307, 456, 50916], "temperature": 0.0, "avg_logprob": -0.07768333935346759, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0014100221451371908}, {"id": 258, "seek": 129746, "start": 1308.5, "end": 1314.5, "text": " in the first place. So it might be just incentivizing people to continue to push the boundaries of", "tokens": [50916, 294, 264, 700, 1081, 13, 407, 309, 1062, 312, 445, 35328, 3319, 561, 281, 2354, 281, 2944, 264, 13180, 295, 51216], "temperature": 0.0, "avg_logprob": -0.07768333935346759, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0014100221451371908}, {"id": 259, "seek": 129746, "start": 1314.5, "end": 1322.58, "text": " ceiling. And as a result, what we see is a lot of a widened accuracy gap. And if we take a more", "tokens": [51216, 13655, 13, 400, 382, 257, 1874, 11, 437, 321, 536, 307, 257, 688, 295, 257, 32552, 292, 14170, 7417, 13, 400, 498, 321, 747, 257, 544, 51620], "temperature": 0.0, "avg_logprob": -0.07768333935346759, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0014100221451371908}, {"id": 260, "seek": 129746, "start": 1322.58, "end": 1327.38, "text": " interaction-centric approach, I would argue that if we identify that narrowing this gap is a more", "tokens": [51620, 9285, 12, 45300, 3109, 11, 286, 576, 9695, 300, 498, 321, 5876, 300, 9432, 278, 341, 7417, 307, 257, 544, 51860], "temperature": 0.0, "avg_logprob": -0.07768333935346759, "compression_ratio": 1.693103448275862, "no_speech_prob": 0.0014100221451371908}, {"id": 261, "seek": 132738, "start": 1327.38, "end": 1333.6200000000001, "text": " important problem, we can narrow this accuracy gap. And it's not just a matter of accuracy,", "tokens": [50364, 1021, 1154, 11, 321, 393, 9432, 341, 14170, 7417, 13, 400, 309, 311, 406, 445, 257, 1871, 295, 14170, 11, 50676], "temperature": 0.0, "avg_logprob": -0.06120008019840016, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.00010718064004322514}, {"id": 262, "seek": 132738, "start": 1333.6200000000001, "end": 1338.18, "text": " if you think about it. It's about experience, benefit, and value that people get out of", "tokens": [50676, 498, 291, 519, 466, 309, 13, 467, 311, 466, 1752, 11, 5121, 11, 293, 2158, 300, 561, 483, 484, 295, 50904], "temperature": 0.0, "avg_logprob": -0.06120008019840016, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.00010718064004322514}, {"id": 263, "seek": 132738, "start": 1338.18, "end": 1346.42, "text": " interacting with this AI. So there was a first challenge about the accuracy gap and how thinking", "tokens": [50904, 18017, 365, 341, 7318, 13, 407, 456, 390, 257, 700, 3430, 466, 264, 14170, 7417, 293, 577, 1953, 51316], "temperature": 0.0, "avg_logprob": -0.06120008019840016, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.00010718064004322514}, {"id": 264, "seek": 132738, "start": 1346.42, "end": 1351.8600000000001, "text": " about how people interact with this AI can help us identify what problems are worth tackling.", "tokens": [51316, 466, 577, 561, 4648, 365, 341, 7318, 393, 854, 505, 5876, 437, 2740, 366, 3163, 34415, 13, 51588], "temperature": 0.0, "avg_logprob": -0.06120008019840016, "compression_ratio": 1.6742081447963801, "no_speech_prob": 0.00010718064004322514}, {"id": 265, "seek": 135186, "start": 1352.6599999999999, "end": 1358.4199999999998, "text": " And second of all, I want to talk about when people actually use AI. And one of the", "tokens": [50404, 400, 1150, 295, 439, 11, 286, 528, 281, 751, 466, 562, 561, 767, 764, 7318, 13, 400, 472, 295, 264, 50692], "temperature": 0.0, "avg_logprob": -0.12279675596503802, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.0010481522185727954}, {"id": 266, "seek": 135186, "start": 1358.9799999999998, "end": 1365.4599999999998, "text": " anti-patterns of human-AI interaction is that people just stop using AI altogether or abandon it,", "tokens": [50720, 6061, 12, 79, 1161, 3695, 295, 1952, 12, 48698, 9285, 307, 300, 561, 445, 1590, 1228, 7318, 19051, 420, 9072, 309, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12279675596503802, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.0010481522185727954}, {"id": 267, "seek": 135186, "start": 1366.02, "end": 1372.5, "text": " which is something you might want to avoid as a system designer. And that's why it's important", "tokens": [51072, 597, 307, 746, 291, 1062, 528, 281, 5042, 382, 257, 1185, 11795, 13, 400, 300, 311, 983, 309, 311, 1021, 51396], "temperature": 0.0, "avg_logprob": -0.12279675596503802, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.0010481522185727954}, {"id": 268, "seek": 135186, "start": 1372.5, "end": 1379.3799999999999, "text": " to think about how do we incentivize people to work with AI? And in most cases, people abandon", "tokens": [51396, 281, 519, 466, 577, 360, 321, 35328, 1125, 561, 281, 589, 365, 7318, 30, 400, 294, 881, 3331, 11, 561, 9072, 51740], "temperature": 0.0, "avg_logprob": -0.12279675596503802, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.0010481522185727954}, {"id": 269, "seek": 137938, "start": 1379.38, "end": 1385.46, "text": " using AI because it's not really giving them concrete value that they expect. And we explore", "tokens": [50364, 1228, 7318, 570, 309, 311, 406, 534, 2902, 552, 9859, 2158, 300, 436, 2066, 13, 400, 321, 6839, 50668], "temperature": 0.0, "avg_logprob": -0.0729850380166063, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.004749604966491461}, {"id": 270, "seek": 137938, "start": 1385.46, "end": 1391.8600000000001, "text": " this in the context of online education in this system called XS. So the problem that we wanted", "tokens": [50668, 341, 294, 264, 4319, 295, 2950, 3309, 294, 341, 1185, 1219, 1783, 50, 13, 407, 264, 1154, 300, 321, 1415, 50988], "temperature": 0.0, "avg_logprob": -0.0729850380166063, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.004749604966491461}, {"id": 271, "seek": 137938, "start": 1391.8600000000001, "end": 1397.38, "text": " to focus here is that in online, let's say you want to learn some new concept like probability,", "tokens": [50988, 281, 1879, 510, 307, 300, 294, 2950, 11, 718, 311, 584, 291, 528, 281, 1466, 512, 777, 3410, 411, 8482, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0729850380166063, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.004749604966491461}, {"id": 272, "seek": 137938, "start": 1397.38, "end": 1402.98, "text": " there are lots of problems and answers you can find. But finding good explanations is", "tokens": [51264, 456, 366, 3195, 295, 2740, 293, 6338, 291, 393, 915, 13, 583, 5006, 665, 28708, 307, 51544], "temperature": 0.0, "avg_logprob": -0.0729850380166063, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.004749604966491461}, {"id": 273, "seek": 137938, "start": 1402.98, "end": 1409.14, "text": " surprisingly difficult. And generating high-quality explanations is costly and resource-intensive", "tokens": [51544, 17600, 2252, 13, 400, 17746, 1090, 12, 11286, 28708, 307, 28328, 293, 7684, 12, 686, 2953, 51852], "temperature": 0.0, "avg_logprob": -0.0729850380166063, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.004749604966491461}, {"id": 274, "seek": 140914, "start": 1409.14, "end": 1416.0200000000002, "text": " console. So we wanted to tackle this problem by building this online education platform,", "tokens": [50364, 11076, 13, 407, 321, 1415, 281, 14896, 341, 1154, 538, 2390, 341, 2950, 3309, 3663, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1270841822904699, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0002911252959165722}, {"id": 275, "seek": 140914, "start": 1416.0200000000002, "end": 1421.7800000000002, "text": " where people are presented with a problem and they solve this problem, they submit an answer,", "tokens": [50708, 689, 561, 366, 8212, 365, 257, 1154, 293, 436, 5039, 341, 1154, 11, 436, 10315, 364, 1867, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1270841822904699, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0002911252959165722}, {"id": 276, "seek": 140914, "start": 1423.5400000000002, "end": 1429.7, "text": " and they see an example that's presented by the system and they get a chance to rate how helpful", "tokens": [51084, 293, 436, 536, 364, 1365, 300, 311, 8212, 538, 264, 1185, 293, 436, 483, 257, 2931, 281, 3314, 577, 4961, 51392], "temperature": 0.0, "avg_logprob": -0.1270841822904699, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0002911252959165722}, {"id": 277, "seek": 140914, "start": 1429.7, "end": 1437.46, "text": " the explanation that they saw was. And then they are getting a chance to sort of self-explain", "tokens": [51392, 264, 10835, 300, 436, 1866, 390, 13, 400, 550, 436, 366, 1242, 257, 2931, 281, 1333, 295, 2698, 12, 23040, 491, 51780], "temperature": 0.0, "avg_logprob": -0.1270841822904699, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0002911252959165722}, {"id": 278, "seek": 143746, "start": 1437.46, "end": 1443.54, "text": " their own answer. So this is a pedagogically meaningful activity to be able to sort of explain", "tokens": [50364, 641, 1065, 1867, 13, 407, 341, 307, 257, 5670, 31599, 984, 10995, 5191, 281, 312, 1075, 281, 1333, 295, 2903, 50668], "temperature": 0.0, "avg_logprob": -0.10311048371451241, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.00039802162791602314}, {"id": 279, "seek": 143746, "start": 1443.54, "end": 1449.94, "text": " your thought process, externalize it, and lots of research supports doing self-explanation.", "tokens": [50668, 428, 1194, 1399, 11, 8320, 1125, 309, 11, 293, 3195, 295, 2132, 9346, 884, 2698, 12, 3121, 16554, 399, 13, 50988], "temperature": 0.0, "avg_logprob": -0.10311048371451241, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.00039802162791602314}, {"id": 280, "seek": 143746, "start": 1450.74, "end": 1456.42, "text": " Okay, so fairly simple sort of front end in terms of the learner's experience. So what's", "tokens": [51028, 1033, 11, 370, 6457, 2199, 1333, 295, 1868, 917, 294, 2115, 295, 264, 33347, 311, 1752, 13, 407, 437, 311, 51312], "temperature": 0.0, "avg_logprob": -0.10311048371451241, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.00039802162791602314}, {"id": 281, "seek": 143746, "start": 1456.42, "end": 1463.38, "text": " happening behind the scene is that the system is collecting these explanations and ratings", "tokens": [51312, 2737, 2261, 264, 4145, 307, 300, 264, 1185, 307, 12510, 613, 28708, 293, 24603, 51660], "temperature": 0.0, "avg_logprob": -0.10311048371451241, "compression_ratio": 1.5775862068965518, "no_speech_prob": 0.00039802162791602314}, {"id": 282, "seek": 146338, "start": 1463.38, "end": 1469.7800000000002, "text": " from learners, right? Since it's a live system, new learners keep coming in and provide new ratings", "tokens": [50364, 490, 23655, 11, 558, 30, 4162, 309, 311, 257, 1621, 1185, 11, 777, 23655, 1066, 1348, 294, 293, 2893, 777, 24603, 50684], "temperature": 0.0, "avg_logprob": -0.10155003794123617, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.004195706453174353}, {"id": 283, "seek": 146338, "start": 1469.7800000000002, "end": 1476.0200000000002, "text": " and explanations. And we formulate this in a multi-armed bandit manner, which means that", "tokens": [50684, 293, 28708, 13, 400, 321, 47881, 341, 294, 257, 4825, 12, 38375, 4116, 270, 9060, 11, 597, 1355, 300, 50996], "temperature": 0.0, "avg_logprob": -0.10155003794123617, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.004195706453174353}, {"id": 284, "seek": 146338, "start": 1476.0200000000002, "end": 1481.8600000000001, "text": " as a new explanation comes into the system, as a byproduct of humans' learning activity,", "tokens": [50996, 382, 257, 777, 10835, 1487, 666, 264, 1185, 11, 382, 257, 538, 33244, 295, 6255, 6, 2539, 5191, 11, 51288], "temperature": 0.0, "avg_logprob": -0.10155003794123617, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.004195706453174353}, {"id": 285, "seek": 146338, "start": 1481.8600000000001, "end": 1487.8600000000001, "text": " a new arm gets added to the system. And what the system is doing is to determine this dynamic", "tokens": [51288, 257, 777, 3726, 2170, 3869, 281, 264, 1185, 13, 400, 437, 264, 1185, 307, 884, 307, 281, 6997, 341, 8546, 51588], "temperature": 0.0, "avg_logprob": -0.10155003794123617, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.004195706453174353}, {"id": 286, "seek": 148786, "start": 1487.86, "end": 1494.1799999999998, "text": " policy for what the most effective explanation would be for the next learner coming into the system.", "tokens": [50364, 3897, 337, 437, 264, 881, 4942, 10835, 576, 312, 337, 264, 958, 33347, 1348, 666, 264, 1185, 13, 50680], "temperature": 0.0, "avg_logprob": -0.09522689842596287, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.0007433027494698763}, {"id": 287, "seek": 148786, "start": 1495.78, "end": 1501.3799999999999, "text": " So if you're familiar with the reinforcement learning of concepts, we are navigating", "tokens": [50760, 407, 498, 291, 434, 4963, 365, 264, 29280, 2539, 295, 10392, 11, 321, 366, 32054, 51040], "temperature": 0.0, "avg_logprob": -0.09522689842596287, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.0007433027494698763}, {"id": 288, "seek": 148786, "start": 1501.3799999999999, "end": 1507.2199999999998, "text": " exploitation and exploration trade-off. Exploitation in the sense that the system wants to present the", "tokens": [51040, 33122, 293, 16197, 4923, 12, 4506, 13, 12514, 78, 4614, 294, 264, 2020, 300, 264, 1185, 2738, 281, 1974, 264, 51332], "temperature": 0.0, "avg_logprob": -0.09522689842596287, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.0007433027494698763}, {"id": 289, "seek": 148786, "start": 1507.2199999999998, "end": 1512.9799999999998, "text": " best explanations to the next learner coming into the system, but the system doesn't really know what", "tokens": [51332, 1151, 28708, 281, 264, 958, 33347, 1348, 666, 264, 1185, 11, 457, 264, 1185, 1177, 380, 534, 458, 437, 51620], "temperature": 0.0, "avg_logprob": -0.09522689842596287, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.0007433027494698763}, {"id": 290, "seek": 151298, "start": 1512.98, "end": 1518.98, "text": " the best explanations are until it collects some amount of ratings from people. So it has to do some", "tokens": [50364, 264, 1151, 28708, 366, 1826, 309, 39897, 512, 2372, 295, 24603, 490, 561, 13, 407, 309, 575, 281, 360, 512, 50664], "temperature": 0.0, "avg_logprob": -0.09683597378614472, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.00028669959283433855}, {"id": 291, "seek": 151298, "start": 1518.98, "end": 1525.22, "text": " exploration where it should collect this data. And to solve that, we use a technique called", "tokens": [50664, 16197, 689, 309, 820, 2500, 341, 1412, 13, 400, 281, 5039, 300, 11, 321, 764, 257, 6532, 1219, 50976], "temperature": 0.0, "avg_logprob": -0.09683597378614472, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.00028669959283433855}, {"id": 292, "seek": 151298, "start": 1525.22, "end": 1531.78, "text": " Thomson sampling. So what happens is the system keeps track of these policies and when a new", "tokens": [50976, 19409, 3015, 21179, 13, 407, 437, 2314, 307, 264, 1185, 5965, 2837, 295, 613, 7657, 293, 562, 257, 777, 51304], "temperature": 0.0, "avg_logprob": -0.09683597378614472, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.00028669959283433855}, {"id": 293, "seek": 151298, "start": 1531.78, "end": 1536.74, "text": " explanation comes in and ratings come in, these things get updated and the policy", "tokens": [51304, 10835, 1487, 294, 293, 24603, 808, 294, 11, 613, 721, 483, 10588, 293, 264, 3897, 51552], "temperature": 0.0, "avg_logprob": -0.09683597378614472, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.00028669959283433855}, {"id": 294, "seek": 153674, "start": 1537.14, "end": 1544.26, "text": " of probabilistic policy gets updated so that it uses this distribution to determine", "tokens": [50384, 295, 31959, 3142, 3897, 2170, 10588, 370, 300, 309, 4960, 341, 7316, 281, 6997, 50740], "temperature": 0.0, "avg_logprob": -0.1657040321220786, "compression_ratio": 1.5755813953488371, "no_speech_prob": 0.0004102418606635183}, {"id": 295, "seek": 153674, "start": 1544.26, "end": 1554.34, "text": " what explanation to show to the next learner. So when we ran a study, these access-generated", "tokens": [50740, 437, 10835, 281, 855, 281, 264, 958, 33347, 13, 407, 562, 321, 5872, 257, 2979, 11, 613, 2105, 12, 21848, 770, 51244], "temperature": 0.0, "avg_logprob": -0.1657040321220786, "compression_ratio": 1.5755813953488371, "no_speech_prob": 0.0004102418606635183}, {"id": 296, "seek": 153674, "start": 1554.34, "end": 1561.78, "text": " explanations were helpful in terms of helping people learn better. So when we compared against", "tokens": [51244, 28708, 645, 4961, 294, 2115, 295, 4315, 561, 1466, 1101, 13, 407, 562, 321, 5347, 1970, 51616], "temperature": 0.0, "avg_logprob": -0.1657040321220786, "compression_ratio": 1.5755813953488371, "no_speech_prob": 0.0004102418606635183}, {"id": 297, "seek": 156178, "start": 1561.86, "end": 1567.22, "text": " presenting no explanation at all and measured differences between pre-test and post-test", "tokens": [50368, 15578, 572, 10835, 412, 439, 293, 12690, 7300, 1296, 659, 12, 31636, 293, 2183, 12, 31636, 50636], "temperature": 0.0, "avg_logprob": -0.11296617573705213, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.0026694368571043015}, {"id": 298, "seek": 156178, "start": 1567.22, "end": 1573.86, "text": " results, we were seeing that people were gaining 3% increase in their scores. So just getting a", "tokens": [50636, 3542, 11, 321, 645, 2577, 300, 561, 645, 19752, 805, 4, 3488, 294, 641, 13444, 13, 407, 445, 1242, 257, 50968], "temperature": 0.0, "avg_logprob": -0.11296617573705213, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.0026694368571043015}, {"id": 299, "seek": 156178, "start": 1574.66, "end": 1581.06, "text": " chance to rethink the problem, I think still gave them some increase in their scores. And when they", "tokens": [51008, 2931, 281, 34595, 264, 1154, 11, 286, 519, 920, 2729, 552, 512, 3488, 294, 641, 13444, 13, 400, 562, 436, 51328], "temperature": 0.0, "avg_logprob": -0.11296617573705213, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.0026694368571043015}, {"id": 300, "seek": 156178, "start": 1581.06, "end": 1588.02, "text": " were seeing the instructor-generated explanation, which is, I guess, somewhat of an ideal case or", "tokens": [51328, 645, 2577, 264, 18499, 12, 21848, 770, 10835, 11, 597, 307, 11, 286, 2041, 11, 8344, 295, 364, 7157, 1389, 420, 51676], "temperature": 0.0, "avg_logprob": -0.11296617573705213, "compression_ratio": 1.6977777777777778, "no_speech_prob": 0.0026694368571043015}, {"id": 301, "seek": 158802, "start": 1588.02, "end": 1594.42, "text": " the standard case, we're seeing 9% increase and with access, we're seeing 12% increase. So between", "tokens": [50364, 264, 3832, 1389, 11, 321, 434, 2577, 1722, 4, 3488, 293, 365, 2105, 11, 321, 434, 2577, 2272, 4, 3488, 13, 407, 1296, 50684], "temperature": 0.0, "avg_logprob": -0.06901831405107366, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0003249206056352705}, {"id": 302, "seek": 158802, "start": 1594.42, "end": 1599.62, "text": " these two conditions, it was not statistically significantly different, but there were certainly", "tokens": [50684, 613, 732, 4487, 11, 309, 390, 406, 36478, 10591, 819, 11, 457, 456, 645, 3297, 50944], "temperature": 0.0, "avg_logprob": -0.06901831405107366, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0003249206056352705}, {"id": 303, "seek": 158802, "start": 1599.62, "end": 1607.22, "text": " cases where access was picking explanations from learners that were even more powerful than the", "tokens": [50944, 3331, 689, 2105, 390, 8867, 28708, 490, 23655, 300, 645, 754, 544, 4005, 813, 264, 51324], "temperature": 0.0, "avg_logprob": -0.06901831405107366, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0003249206056352705}, {"id": 304, "seek": 158802, "start": 1607.22, "end": 1614.74, "text": " instructor-generated ones. So in this system, if we were to take a more model-centric approach,", "tokens": [51324, 18499, 12, 21848, 770, 2306, 13, 407, 294, 341, 1185, 11, 498, 321, 645, 281, 747, 257, 544, 2316, 12, 45300, 3109, 11, 51700], "temperature": 0.0, "avg_logprob": -0.06901831405107366, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0003249206056352705}, {"id": 305, "seek": 161474, "start": 1614.74, "end": 1620.9, "text": " I think we might have built an AI that automatically generates high quality explanations.", "tokens": [50364, 286, 519, 321, 1062, 362, 3094, 364, 7318, 300, 6772, 23815, 1090, 3125, 28708, 13, 50672], "temperature": 0.0, "avg_logprob": -0.09284569906151813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0003247382992412895}, {"id": 306, "seek": 161474, "start": 1622.34, "end": 1628.26, "text": " But instead, in taking an interaction-centric approach, I think the system we created is basically", "tokens": [50744, 583, 2602, 11, 294, 1940, 364, 9285, 12, 45300, 3109, 11, 286, 519, 264, 1185, 321, 2942, 307, 1936, 51040], "temperature": 0.0, "avg_logprob": -0.09284569906151813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0003247382992412895}, {"id": 307, "seek": 161474, "start": 1628.26, "end": 1635.54, "text": " this co-learning system where the user, the learner, and AI are learning at the same time in a single", "tokens": [51040, 341, 598, 12, 47204, 1185, 689, 264, 4195, 11, 264, 33347, 11, 293, 7318, 366, 2539, 412, 264, 912, 565, 294, 257, 2167, 51404], "temperature": 0.0, "avg_logprob": -0.09284569906151813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0003247382992412895}, {"id": 308, "seek": 161474, "start": 1635.54, "end": 1642.42, "text": " system. So it's sort of an education-focused system of the game-with-the-purpose kind of setting,", "tokens": [51404, 1185, 13, 407, 309, 311, 1333, 295, 364, 3309, 12, 44062, 1185, 295, 264, 1216, 12, 11820, 12, 3322, 12, 42601, 733, 295, 3287, 11, 51748], "temperature": 0.0, "avg_logprob": -0.09284569906151813, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0003247382992412895}, {"id": 309, "seek": 164242, "start": 1642.42, "end": 1647.38, "text": " where organic benefits are provided to people who are interacting with the system,", "tokens": [50364, 689, 10220, 5311, 366, 5649, 281, 561, 567, 366, 18017, 365, 264, 1185, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08241410255432129, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.00028663972625508904}, {"id": 310, "seek": 164242, "start": 1647.38, "end": 1653.38, "text": " and the system is learning something useful out of it. And this is basically the mechanism that", "tokens": [50612, 293, 264, 1185, 307, 2539, 746, 4420, 484, 295, 309, 13, 400, 341, 307, 1936, 264, 7513, 300, 50912], "temperature": 0.0, "avg_logprob": -0.08241410255432129, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.00028663972625508904}, {"id": 311, "seek": 164242, "start": 1653.38, "end": 1658.8200000000002, "text": " we have in that both sides are learning and explanation and feedback are establishing this", "tokens": [50912, 321, 362, 294, 300, 1293, 4881, 366, 2539, 293, 10835, 293, 5824, 366, 22494, 341, 51184], "temperature": 0.0, "avg_logprob": -0.08241410255432129, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.00028663972625508904}, {"id": 312, "seek": 164242, "start": 1658.8200000000002, "end": 1665.46, "text": " loop. And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing,", "tokens": [51184, 6367, 13, 400, 341, 307, 264, 4829, 295, 452, 14476, 22288, 11, 293, 286, 6839, 341, 294, 264, 3410, 295, 33347, 11006, 2175, 11, 51516], "temperature": 0.0, "avg_logprob": -0.08241410255432129, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.00028663972625508904}, {"id": 313, "seek": 164242, "start": 1665.46, "end": 1671.78, "text": " where learners as a crowd coming into the system are basically doing this by getting their individual", "tokens": [51516, 689, 23655, 382, 257, 6919, 1348, 666, 264, 1185, 366, 1936, 884, 341, 538, 1242, 641, 2609, 51832], "temperature": 0.0, "avg_logprob": -0.08241410255432129, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.00028663972625508904}, {"id": 314, "seek": 167178, "start": 1671.78, "end": 1677.62, "text": " benefits while they're providing something useful for the system to learn and do its thing better.", "tokens": [50364, 5311, 1339, 436, 434, 6530, 746, 4420, 337, 264, 1185, 281, 1466, 293, 360, 1080, 551, 1101, 13, 50656], "temperature": 0.0, "avg_logprob": -0.07255473082092986, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003196852339897305}, {"id": 315, "seek": 167178, "start": 1678.58, "end": 1685.3799999999999, "text": " So since then, I've been expanding this idea to a broad array of applications. So for example,", "tokens": [50704, 407, 1670, 550, 11, 286, 600, 668, 14702, 341, 1558, 281, 257, 4152, 10225, 295, 5821, 13, 407, 337, 1365, 11, 51044], "temperature": 0.0, "avg_logprob": -0.07255473082092986, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003196852339897305}, {"id": 316, "seek": 167178, "start": 1685.3799999999999, "end": 1691.86, "text": " can we use this kind of co-learning ideas to summarize how-to videos in terms of steps and", "tokens": [51044, 393, 321, 764, 341, 733, 295, 598, 12, 47204, 3487, 281, 20858, 577, 12, 1353, 2145, 294, 2115, 295, 4439, 293, 51368], "temperature": 0.0, "avg_logprob": -0.07255473082092986, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003196852339897305}, {"id": 317, "seek": 167178, "start": 1691.86, "end": 1697.78, "text": " sub-steps, or building a concept map out of an instructional video that shows relationships", "tokens": [51368, 1422, 12, 20413, 11, 420, 2390, 257, 3410, 4471, 484, 295, 364, 35716, 960, 300, 3110, 6159, 51664], "temperature": 0.0, "avg_logprob": -0.07255473082092986, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003196852339897305}, {"id": 318, "seek": 169778, "start": 1697.78, "end": 1702.74, "text": " between different concepts, or helping learners come up with the solution plans", "tokens": [50364, 1296, 819, 10392, 11, 420, 4315, 23655, 808, 493, 365, 264, 3827, 5482, 50612], "temperature": 0.0, "avg_logprob": -0.08695886685298039, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0004106477426830679}, {"id": 319, "seek": 169778, "start": 1703.3799999999999, "end": 1708.5, "text": " in algorithmic problem-solving settings. And other researchers have taken on this idea", "tokens": [50644, 294, 9284, 299, 1154, 12, 30926, 798, 6257, 13, 400, 661, 10309, 362, 2726, 322, 341, 1558, 50900], "temperature": 0.0, "avg_logprob": -0.08695886685298039, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0004106477426830679}, {"id": 320, "seek": 169778, "start": 1708.5, "end": 1713.54, "text": " in different application contexts as well. So I think we can try to really generalize this kind", "tokens": [50900, 294, 819, 3861, 30628, 382, 731, 13, 407, 286, 519, 321, 393, 853, 281, 534, 2674, 1125, 341, 733, 51152], "temperature": 0.0, "avg_logprob": -0.08695886685298039, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0004106477426830679}, {"id": 321, "seek": 169778, "start": 1713.54, "end": 1722.98, "text": " of idea of co-learning system design in different contexts. Moving on to the third challenge,", "tokens": [51152, 295, 1558, 295, 598, 12, 47204, 1185, 1715, 294, 819, 30628, 13, 14242, 322, 281, 264, 2636, 3430, 11, 51624], "temperature": 0.0, "avg_logprob": -0.08695886685298039, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0004106477426830679}, {"id": 322, "seek": 172298, "start": 1723.94, "end": 1730.74, "text": " is about beyond a single user. And often we think about a single user, a single AI interacting", "tokens": [50412, 307, 466, 4399, 257, 2167, 4195, 13, 400, 2049, 321, 519, 466, 257, 2167, 4195, 11, 257, 2167, 7318, 18017, 50752], "temperature": 0.0, "avg_logprob": -0.16144474784096519, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0013868396636098623}, {"id": 323, "seek": 172298, "start": 1730.74, "end": 1735.94, "text": " with each other. In real life, it would be much more complex and there would be diverse configurations.", "tokens": [50752, 365, 1184, 661, 13, 682, 957, 993, 11, 309, 576, 312, 709, 544, 3997, 293, 456, 576, 312, 9521, 31493, 13, 51012], "temperature": 0.0, "avg_logprob": -0.16144474784096519, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0013868396636098623}, {"id": 324, "seek": 172298, "start": 1736.98, "end": 1743.22, "text": " So how can we consider these social dynamics? And there could be various types of social dynamics,", "tokens": [51064, 407, 577, 393, 321, 1949, 613, 2093, 15679, 30, 400, 456, 727, 312, 3683, 3467, 295, 2093, 15679, 11, 51376], "temperature": 0.0, "avg_logprob": -0.16144474784096519, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0013868396636098623}, {"id": 325, "seek": 172298, "start": 1743.78, "end": 1750.9, "text": " but one specific instance that we did in was group-based, chat-based discussion in a group.", "tokens": [51404, 457, 472, 2685, 5197, 300, 321, 630, 294, 390, 1594, 12, 6032, 11, 5081, 12, 6032, 5017, 294, 257, 1594, 13, 51760], "temperature": 0.0, "avg_logprob": -0.16144474784096519, "compression_ratio": 1.683982683982684, "no_speech_prob": 0.0013868396636098623}, {"id": 326, "seek": 175090, "start": 1751.7800000000002, "end": 1758.02, "text": " So we built this system called Solution Chat, where the idea is what if this AI agent could", "tokens": [50408, 407, 321, 3094, 341, 1185, 1219, 318, 3386, 27503, 11, 689, 264, 1558, 307, 437, 498, 341, 7318, 9461, 727, 50720], "temperature": 0.0, "avg_logprob": -0.10227333673156135, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.0010957333724945784}, {"id": 327, "seek": 175090, "start": 1758.74, "end": 1765.0600000000002, "text": " recommend real-time moderation messages to a group. So let's say a group is discussing,", "tokens": [50756, 2748, 957, 12, 3766, 49471, 7897, 281, 257, 1594, 13, 407, 718, 311, 584, 257, 1594, 307, 10850, 11, 51072], "temperature": 0.0, "avg_logprob": -0.10227333673156135, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.0010957333724945784}, {"id": 328, "seek": 175090, "start": 1765.0600000000002, "end": 1770.02, "text": " you know, what to do for the company retreat next week, and they're having a discussion.", "tokens": [51072, 291, 458, 11, 437, 281, 360, 337, 264, 2237, 15505, 958, 1243, 11, 293, 436, 434, 1419, 257, 5017, 13, 51320], "temperature": 0.0, "avg_logprob": -0.10227333673156135, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.0010957333724945784}, {"id": 329, "seek": 175090, "start": 1770.02, "end": 1775.0600000000002, "text": " The system, in real time, based on the understanding of the discussion context,", "tokens": [51320, 440, 1185, 11, 294, 957, 565, 11, 2361, 322, 264, 3701, 295, 264, 5017, 4319, 11, 51572], "temperature": 0.0, "avg_logprob": -0.10227333673156135, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.0010957333724945784}, {"id": 330, "seek": 175090, "start": 1775.0600000000002, "end": 1778.8200000000002, "text": " and also knowing what kind of messages would be useful for the group,", "tokens": [51572, 293, 611, 5276, 437, 733, 295, 7897, 576, 312, 4420, 337, 264, 1594, 11, 51760], "temperature": 0.0, "avg_logprob": -0.10227333673156135, "compression_ratio": 1.678714859437751, "no_speech_prob": 0.0010957333724945784}, {"id": 331, "seek": 177882, "start": 1778.82, "end": 1784.02, "text": " based on our sort of literature survey of discussion and discussion-based education,", "tokens": [50364, 2361, 322, 527, 1333, 295, 10394, 8984, 295, 5017, 293, 5017, 12, 6032, 3309, 11, 50624], "temperature": 0.0, "avg_logprob": -0.13668431305303808, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.0011319874320179224}, {"id": 332, "seek": 177882, "start": 1784.74, "end": 1792.34, "text": " it presents these recommendation messages, like any more ideas, or can this person share their", "tokens": [50660, 309, 13533, 613, 11879, 7897, 11, 411, 604, 544, 3487, 11, 420, 393, 341, 954, 2073, 641, 51040], "temperature": 0.0, "avg_logprob": -0.13668431305303808, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.0011319874320179224}, {"id": 333, "seek": 177882, "start": 1792.34, "end": 1798.1, "text": " opinions, you have been quiet for a while, or should we try to move on to the next stage,", "tokens": [51040, 11819, 11, 291, 362, 668, 5677, 337, 257, 1339, 11, 420, 820, 321, 853, 281, 1286, 322, 281, 264, 958, 3233, 11, 51328], "temperature": 0.0, "avg_logprob": -0.13668431305303808, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.0011319874320179224}, {"id": 334, "seek": 177882, "start": 1798.1, "end": 1803.78, "text": " or thank you for your opinion. So these kinds of moderation messages are presented by the system", "tokens": [51328, 420, 1309, 291, 337, 428, 4800, 13, 407, 613, 3685, 295, 49471, 7897, 366, 8212, 538, 264, 1185, 51612], "temperature": 0.0, "avg_logprob": -0.13668431305303808, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.0011319874320179224}, {"id": 335, "seek": 180378, "start": 1803.78, "end": 1810.02, "text": " in real time, just like what you get in smart replies in Gmail, for example. And as a moderator,", "tokens": [50364, 294, 957, 565, 11, 445, 411, 437, 291, 483, 294, 4069, 42289, 294, 36732, 11, 337, 1365, 13, 400, 382, 257, 37778, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09493496576944987, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0008556816028431058}, {"id": 336, "seek": 180378, "start": 1810.02, "end": 1816.5, "text": " you can just choose to accept any of the messages that you like, and discard the ones that you don't", "tokens": [50676, 291, 393, 445, 2826, 281, 3241, 604, 295, 264, 7897, 300, 291, 411, 11, 293, 31597, 264, 2306, 300, 291, 500, 380, 51000], "temperature": 0.0, "avg_logprob": -0.09493496576944987, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0008556816028431058}, {"id": 337, "seek": 180378, "start": 1816.5, "end": 1828.5, "text": " like. So a quick summary of the results of what we saw was that in our lab study with 55 users", "tokens": [51000, 411, 13, 407, 257, 1702, 12691, 295, 264, 3542, 295, 437, 321, 1866, 390, 300, 294, 527, 2715, 2979, 365, 12330, 5022, 51600], "temperature": 0.0, "avg_logprob": -0.09493496576944987, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.0008556816028431058}, {"id": 338, "seek": 182850, "start": 1828.5, "end": 1834.74, "text": " in 12 different groups, when we compared how many moderation messages were used in different groups,", "tokens": [50364, 294, 2272, 819, 3935, 11, 562, 321, 5347, 577, 867, 49471, 7897, 645, 1143, 294, 819, 3935, 11, 50676], "temperature": 0.0, "avg_logprob": -0.0812653691581126, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.03355760499835014}, {"id": 339, "seek": 182850, "start": 1834.74, "end": 1839.54, "text": " when we compared the baseline condition without these real-time recommendations versus", "tokens": [50676, 562, 321, 5347, 264, 20518, 4188, 1553, 613, 957, 12, 3766, 10434, 5717, 50916], "temperature": 0.0, "avg_logprob": -0.0812653691581126, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.03355760499835014}, {"id": 340, "seek": 182850, "start": 1839.54, "end": 1845.3, "text": " solution chat or system, we're seeing a significant increase in the number of moderation messages", "tokens": [50916, 3827, 5081, 420, 1185, 11, 321, 434, 2577, 257, 4776, 3488, 294, 264, 1230, 295, 49471, 7897, 51204], "temperature": 0.0, "avg_logprob": -0.0812653691581126, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.03355760499835014}, {"id": 341, "seek": 182850, "start": 1845.3, "end": 1850.34, "text": " that were present in the chat stream in the solution chat condition. But interestingly,", "tokens": [51204, 300, 645, 1974, 294, 264, 5081, 4309, 294, 264, 3827, 5081, 4188, 13, 583, 25873, 11, 51456], "temperature": 0.0, "avg_logprob": -0.0812653691581126, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.03355760499835014}, {"id": 342, "seek": 182850, "start": 1850.34, "end": 1855.7, "text": " you can see that the users manually typed moderation messages were actually decreasing", "tokens": [51456, 291, 393, 536, 300, 264, 5022, 16945, 33941, 49471, 7897, 645, 767, 23223, 51724], "temperature": 0.0, "avg_logprob": -0.0812653691581126, "compression_ratio": 1.9327731092436975, "no_speech_prob": 0.03355760499835014}, {"id": 343, "seek": 185570, "start": 1855.7, "end": 1864.98, "text": " in solution chat, but many of them were replaced by the accepting AI-generated recommendations.", "tokens": [50364, 294, 3827, 5081, 11, 457, 867, 295, 552, 645, 10772, 538, 264, 17391, 7318, 12, 21848, 770, 10434, 13, 50828], "temperature": 0.0, "avg_logprob": -0.07740483564489029, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00038572470657527447}, {"id": 344, "seek": 185570, "start": 1867.38, "end": 1873.14, "text": " And furthermore, we had this great opportunity to actually release this system to over 2,000", "tokens": [50948, 400, 3052, 3138, 11, 321, 632, 341, 869, 2650, 281, 767, 4374, 341, 1185, 281, 670, 568, 11, 1360, 51236], "temperature": 0.0, "avg_logprob": -0.07740483564489029, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00038572470657527447}, {"id": 345, "seek": 185570, "start": 1873.14, "end": 1878.18, "text": " real-world users in a corporate education setting. So during COVID, a lot of these corporate", "tokens": [51236, 957, 12, 13217, 5022, 294, 257, 10896, 3309, 3287, 13, 407, 1830, 4566, 11, 257, 688, 295, 613, 10896, 51488], "temperature": 0.0, "avg_logprob": -0.07740483564489029, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00038572470657527447}, {"id": 346, "seek": 185570, "start": 1878.18, "end": 1884.5800000000002, "text": " education programs moved online, and this company that we worked with wanted to use these kinds of", "tokens": [51488, 3309, 4268, 4259, 2950, 11, 293, 341, 2237, 300, 321, 2732, 365, 1415, 281, 764, 613, 3685, 295, 51808], "temperature": 0.0, "avg_logprob": -0.07740483564489029, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00038572470657527447}, {"id": 347, "seek": 188458, "start": 1884.58, "end": 1891.62, "text": " system to moderate hundreds of chat rooms that were doing discussion-based activity.", "tokens": [50364, 1185, 281, 18174, 6779, 295, 5081, 9396, 300, 645, 884, 5017, 12, 6032, 5191, 13, 50716], "temperature": 0.0, "avg_logprob": -0.057299861907958986, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0004367518413346261}, {"id": 348, "seek": 188458, "start": 1892.98, "end": 1899.86, "text": " And not surprisingly, just like the very first live stream prompt example that I mentioned,", "tokens": [50784, 400, 406, 17600, 11, 445, 411, 264, 588, 700, 1621, 4309, 12391, 1365, 300, 286, 2835, 11, 51128], "temperature": 0.0, "avg_logprob": -0.057299861907958986, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0004367518413346261}, {"id": 349, "seek": 188458, "start": 1899.86, "end": 1905.54, "text": " here again, people were collaboratively trying to understand the capabilities and limitations of", "tokens": [51128, 510, 797, 11, 561, 645, 16555, 356, 1382, 281, 1223, 264, 10862, 293, 15705, 295, 51412], "temperature": 0.0, "avg_logprob": -0.057299861907958986, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0004367518413346261}, {"id": 350, "seek": 188458, "start": 1905.54, "end": 1911.54, "text": " AI when they were first presented with the system. So they were using the chat to test", "tokens": [51412, 7318, 562, 436, 645, 700, 8212, 365, 264, 1185, 13, 407, 436, 645, 1228, 264, 5081, 281, 1500, 51712], "temperature": 0.0, "avg_logprob": -0.057299861907958986, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0004367518413346261}, {"id": 351, "seek": 191154, "start": 1911.54, "end": 1917.94, "text": " different messages, often things that they believe would be not working, and they would be", "tokens": [50364, 819, 7897, 11, 2049, 721, 300, 436, 1697, 576, 312, 406, 1364, 11, 293, 436, 576, 312, 50684], "temperature": 0.0, "avg_logprob": -0.08771221534065578, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0020487233996391296}, {"id": 352, "seek": 191154, "start": 1917.94, "end": 1923.62, "text": " sharing the results of, oh, this is working, this is not working, I think this does this well,", "tokens": [50684, 5414, 264, 3542, 295, 11, 1954, 11, 341, 307, 1364, 11, 341, 307, 406, 1364, 11, 286, 519, 341, 775, 341, 731, 11, 50968], "temperature": 0.0, "avg_logprob": -0.08771221534065578, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0020487233996391296}, {"id": 353, "seek": 191154, "start": 1923.62, "end": 1930.42, "text": " but not that well. And it seems as a group does this kind of testing in the very first phase of", "tokens": [50968, 457, 406, 300, 731, 13, 400, 309, 2544, 382, 257, 1594, 775, 341, 733, 295, 4997, 294, 264, 588, 700, 5574, 295, 51308], "temperature": 0.0, "avg_logprob": -0.08771221534065578, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0020487233996391296}, {"id": 354, "seek": 191154, "start": 1930.42, "end": 1936.6599999999999, "text": " their usage of the system, people have this shared expectation of the system, and that seems to sort", "tokens": [51308, 641, 14924, 295, 264, 1185, 11, 561, 362, 341, 5507, 14334, 295, 264, 1185, 11, 293, 300, 2544, 281, 1333, 51620], "temperature": 0.0, "avg_logprob": -0.08771221534065578, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.0020487233996391296}, {"id": 355, "seek": 193666, "start": 1936.66, "end": 1943.5400000000002, "text": " of determine their further interactions with the system. And it was also notable how different", "tokens": [50364, 295, 6997, 641, 3052, 13280, 365, 264, 1185, 13, 400, 309, 390, 611, 22556, 577, 819, 50708], "temperature": 0.0, "avg_logprob": -0.06405677795410156, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0008964893640950322}, {"id": 356, "seek": 193666, "start": 1943.5400000000002, "end": 1948.5, "text": " groups had different expectations based on their limited experimentation that they did in the beginning.", "tokens": [50708, 3935, 632, 819, 9843, 2361, 322, 641, 5567, 37142, 300, 436, 630, 294, 264, 2863, 13, 50956], "temperature": 0.0, "avg_logprob": -0.06405677795410156, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0008964893640950322}, {"id": 357, "seek": 193666, "start": 1950.3400000000001, "end": 1953.6200000000001, "text": " And there were some interesting social dynamics that we observed as well,", "tokens": [51048, 400, 456, 645, 512, 1880, 2093, 15679, 300, 321, 13095, 382, 731, 11, 51212], "temperature": 0.0, "avg_logprob": -0.06405677795410156, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0008964893640950322}, {"id": 358, "seek": 193666, "start": 1954.5, "end": 1961.38, "text": " like in how people use these AI recommendations to socially interact with each other.", "tokens": [51256, 411, 294, 577, 561, 764, 613, 7318, 10434, 281, 21397, 4648, 365, 1184, 661, 13, 51600], "temperature": 0.0, "avg_logprob": -0.06405677795410156, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.0008964893640950322}, {"id": 359, "seek": 196138, "start": 1962.3400000000001, "end": 1966.42, "text": " Some people were using AI as proxy. So one of the quotes that we had was,", "tokens": [50412, 2188, 561, 645, 1228, 7318, 382, 29690, 13, 407, 472, 295, 264, 19963, 300, 321, 632, 390, 11, 50616], "temperature": 0.0, "avg_logprob": -0.07156073916089403, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.0015241821529343724}, {"id": 360, "seek": 196138, "start": 1966.42, "end": 1971.22, "text": " I didn't want to directly ask the person to stop talking. So the person relied on the AI", "tokens": [50616, 286, 994, 380, 528, 281, 3838, 1029, 264, 954, 281, 1590, 1417, 13, 407, 264, 954, 35463, 322, 264, 7318, 50856], "temperature": 0.0, "avg_logprob": -0.07156073916089403, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.0015241821529343724}, {"id": 361, "seek": 196138, "start": 1971.22, "end": 1978.8200000000002, "text": " recommended message to kind of send it. They still chose to send it, but it was their way of kind of", "tokens": [50856, 9628, 3636, 281, 733, 295, 2845, 309, 13, 814, 920, 5111, 281, 2845, 309, 11, 457, 309, 390, 641, 636, 295, 733, 295, 51236], "temperature": 0.0, "avg_logprob": -0.07156073916089403, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.0015241821529343724}, {"id": 362, "seek": 196138, "start": 1978.8200000000002, "end": 1987.5400000000002, "text": " softening the potential sort of dispute with the person. Other people were using AI as a reference.", "tokens": [51236, 2787, 4559, 264, 3995, 1333, 295, 25379, 365, 264, 954, 13, 5358, 561, 645, 1228, 7318, 382, 257, 6408, 13, 51672], "temperature": 0.0, "avg_logprob": -0.07156073916089403, "compression_ratio": 1.6883720930232557, "no_speech_prob": 0.0015241821529343724}, {"id": 363, "seek": 198754, "start": 1987.62, "end": 1992.82, "text": " So what we were seeing is that it was a fairly simple technical pipeline that we had. So it was", "tokens": [50368, 407, 437, 321, 645, 2577, 307, 300, 309, 390, 257, 6457, 2199, 6191, 15517, 300, 321, 632, 13, 407, 309, 390, 50628], "temperature": 0.0, "avg_logprob": -0.0828291204240587, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.003648149548098445}, {"id": 364, "seek": 198754, "start": 1992.82, "end": 2000.58, "text": " just a canned response. So people were sometimes not really fond of the tone of the message,", "tokens": [50628, 445, 257, 36462, 4134, 13, 407, 561, 645, 2171, 406, 534, 9557, 295, 264, 8027, 295, 264, 3636, 11, 51016], "temperature": 0.0, "avg_logprob": -0.0828291204240587, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.003648149548098445}, {"id": 365, "seek": 198754, "start": 2000.58, "end": 2005.3, "text": " style of the message that we showed. So the person said, I found no fun in the recommended", "tokens": [51016, 3758, 295, 264, 3636, 300, 321, 4712, 13, 407, 264, 954, 848, 11, 286, 1352, 572, 1019, 294, 264, 9628, 51252], "temperature": 0.0, "avg_logprob": -0.0828291204240587, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.003648149548098445}, {"id": 366, "seek": 198754, "start": 2005.3, "end": 2010.8999999999999, "text": " messages because all the messages look the same. So in those cases, what people did was they still", "tokens": [51252, 7897, 570, 439, 264, 7897, 574, 264, 912, 13, 407, 294, 729, 3331, 11, 437, 561, 630, 390, 436, 920, 51532], "temperature": 0.0, "avg_logprob": -0.0828291204240587, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.003648149548098445}, {"id": 367, "seek": 201090, "start": 2011.46, "end": 2019.38, "text": " adopted the idea from the recommendation, but then rewrote it so that it feels more personal,", "tokens": [50392, 12175, 264, 1558, 490, 264, 11879, 11, 457, 550, 319, 7449, 1370, 309, 370, 300, 309, 3417, 544, 2973, 11, 50788], "temperature": 0.0, "avg_logprob": -0.09134115056788668, "compression_ratio": 1.64, "no_speech_prob": 0.0017541873967275023}, {"id": 368, "seek": 201090, "start": 2019.38, "end": 2026.8200000000002, "text": " and it feels more like it's coming from them, not AI. In other cases, AI seems to be adding", "tokens": [50788, 293, 309, 3417, 544, 411, 309, 311, 1348, 490, 552, 11, 406, 7318, 13, 682, 661, 3331, 11, 7318, 2544, 281, 312, 5127, 51160], "temperature": 0.0, "avg_logprob": -0.09134115056788668, "compression_ratio": 1.64, "no_speech_prob": 0.0017541873967275023}, {"id": 369, "seek": 201090, "start": 2026.8200000000002, "end": 2033.22, "text": " a social burden. So in this excerpt, so one of the people said, I'm doubtful about the", "tokens": [51160, 257, 2093, 12578, 13, 407, 294, 341, 42760, 662, 11, 370, 472, 295, 264, 561, 848, 11, 286, 478, 6385, 906, 466, 264, 51480], "temperature": 0.0, "avg_logprob": -0.09134115056788668, "compression_ratio": 1.64, "no_speech_prob": 0.0017541873967275023}, {"id": 370, "seek": 201090, "start": 2033.7800000000002, "end": 2039.46, "text": " credibility of AI. And then the moderator picks this AI recommendation. Thanks for your opinion.", "tokens": [51508, 28852, 295, 7318, 13, 400, 550, 264, 37778, 16137, 341, 7318, 11879, 13, 2561, 337, 428, 4800, 13, 51792], "temperature": 0.0, "avg_logprob": -0.09134115056788668, "compression_ratio": 1.64, "no_speech_prob": 0.0017541873967275023}, {"id": 371, "seek": 203946, "start": 2039.54, "end": 2044.1000000000001, "text": " Another person says, I also think negatively. Thanks for your opinion. Thanks for sharing a good", "tokens": [50368, 3996, 954, 1619, 11, 286, 611, 519, 29519, 13, 2561, 337, 428, 4800, 13, 2561, 337, 5414, 257, 665, 50596], "temperature": 0.0, "avg_logprob": -0.11719656944274902, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.002588402945548296}, {"id": 372, "seek": 203946, "start": 2044.1000000000001, "end": 2049.46, "text": " opinion. Shall we go to the next topic? And then the moderator realizes he might have clicked,", "tokens": [50596, 4800, 13, 12128, 321, 352, 281, 264, 958, 4829, 30, 400, 550, 264, 37778, 29316, 415, 1062, 362, 23370, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11719656944274902, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.002588402945548296}, {"id": 373, "seek": 203946, "start": 2049.46, "end": 2054.82, "text": " accept way too many times, and it was a little unnatural. So he stopped to kind of", "tokens": [50864, 3241, 636, 886, 867, 1413, 11, 293, 309, 390, 257, 707, 43470, 13, 407, 415, 5936, 281, 733, 295, 51132], "temperature": 0.0, "avg_logprob": -0.11719656944274902, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.002588402945548296}, {"id": 374, "seek": 203946, "start": 2054.82, "end": 2060.42, "text": " clarify and apologize for my unnatural words as I'm using AI recommendations.", "tokens": [51132, 17594, 293, 12328, 337, 452, 43470, 2283, 382, 286, 478, 1228, 7318, 10434, 13, 51412], "temperature": 0.0, "avg_logprob": -0.11719656944274902, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.002588402945548296}, {"id": 375, "seek": 203946, "start": 2061.2200000000003, "end": 2067.38, "text": " So while we were seeing how people were saving their time and cognitive effort in moderation", "tokens": [51452, 407, 1339, 321, 645, 2577, 577, 561, 645, 6816, 641, 565, 293, 15605, 4630, 294, 49471, 51760], "temperature": 0.0, "avg_logprob": -0.11719656944274902, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.002588402945548296}, {"id": 376, "seek": 206738, "start": 2067.38, "end": 2072.5, "text": " could have decreased, it might have actually introduced other types of burden at the same time.", "tokens": [50364, 727, 362, 24436, 11, 309, 1062, 362, 767, 7268, 661, 3467, 295, 12578, 412, 264, 912, 565, 13, 50620], "temperature": 0.0, "avg_logprob": -0.05177547818138486, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0008550580823794007}, {"id": 377, "seek": 206738, "start": 2075.46, "end": 2079.7000000000003, "text": " Again, so if we were to build this kind of system in a more model-centric manner,", "tokens": [50768, 3764, 11, 370, 498, 321, 645, 281, 1322, 341, 733, 295, 1185, 294, 257, 544, 2316, 12, 45300, 9060, 11, 50980], "temperature": 0.0, "avg_logprob": -0.05177547818138486, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0008550580823794007}, {"id": 378, "seek": 206738, "start": 2079.7000000000003, "end": 2084.58, "text": " I think a good alternative might have been automated discussion moderation, where AI", "tokens": [50980, 286, 519, 257, 665, 8535, 1062, 362, 668, 18473, 5017, 49471, 11, 689, 7318, 51224], "temperature": 0.0, "avg_logprob": -0.05177547818138486, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0008550580823794007}, {"id": 379, "seek": 206738, "start": 2084.58, "end": 2092.5, "text": " would actually do all the moderation by itself. But instead, we chose to take a more AI-assisted", "tokens": [51224, 576, 767, 360, 439, 264, 49471, 538, 2564, 13, 583, 2602, 11, 321, 5111, 281, 747, 257, 544, 7318, 12, 640, 33250, 51620], "temperature": 0.0, "avg_logprob": -0.05177547818138486, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.0008550580823794007}, {"id": 380, "seek": 209250, "start": 2092.5, "end": 2098.34, "text": " moderation for obvious reasons. Users want to have more agency and control, and they wanted to", "tokens": [50364, 49471, 337, 6322, 4112, 13, 47092, 528, 281, 362, 544, 7934, 293, 1969, 11, 293, 436, 1415, 281, 50656], "temperature": 0.0, "avg_logprob": -0.09839621151194852, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00048765219980850816}, {"id": 381, "seek": 209250, "start": 2098.34, "end": 2104.34, "text": " keep their style of communication. So instead of handing over the entire control to AI,", "tokens": [50656, 1066, 641, 3758, 295, 6101, 13, 407, 2602, 295, 34774, 670, 264, 2302, 1969, 281, 7318, 11, 50956], "temperature": 0.0, "avg_logprob": -0.09839621151194852, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00048765219980850816}, {"id": 382, "seek": 209250, "start": 2104.9, "end": 2112.42, "text": " we still sort of gave that control to the human moderator who could kind of use it as an additional", "tokens": [50984, 321, 920, 1333, 295, 2729, 300, 1969, 281, 264, 1952, 37778, 567, 727, 733, 295, 764, 309, 382, 364, 4497, 51360], "temperature": 0.0, "avg_logprob": -0.09839621151194852, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00048765219980850816}, {"id": 383, "seek": 209250, "start": 2112.42, "end": 2120.9, "text": " resource. Okay, so there was a third challenge. And moving on to the final challenge of supporting", "tokens": [51360, 7684, 13, 1033, 11, 370, 456, 390, 257, 2636, 3430, 13, 400, 2684, 322, 281, 264, 2572, 3430, 295, 7231, 51784], "temperature": 0.0, "avg_logprob": -0.09839621151194852, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.00048765219980850816}, {"id": 384, "seek": 212090, "start": 2120.9, "end": 2127.06, "text": " sustainable engagement. Here, the concern is that we want to think beyond this single", "tokens": [50364, 11235, 8742, 13, 1692, 11, 264, 3136, 307, 300, 321, 528, 281, 519, 4399, 341, 2167, 50672], "temperature": 0.0, "avg_logprob": -0.0808943271636963, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0006766212172806263}, {"id": 385, "seek": 212090, "start": 2127.06, "end": 2134.02, "text": " session usage. And over time, how people react to these systems might change, their mental model", "tokens": [50672, 5481, 14924, 13, 400, 670, 565, 11, 577, 561, 4515, 281, 613, 3652, 1062, 1319, 11, 641, 4973, 2316, 51020], "temperature": 0.0, "avg_logprob": -0.0808943271636963, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0006766212172806263}, {"id": 386, "seek": 212090, "start": 2134.02, "end": 2138.26, "text": " might change, and how AI actually works might change. So we need to really think about this", "tokens": [51020, 1062, 1319, 11, 293, 577, 7318, 767, 1985, 1062, 1319, 13, 407, 321, 643, 281, 534, 519, 466, 341, 51232], "temperature": 0.0, "avg_logprob": -0.0808943271636963, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0006766212172806263}, {"id": 387, "seek": 212090, "start": 2138.26, "end": 2147.06, "text": " temporal dimension more carefully. And for this thread, we investigated in the context of", "tokens": [51232, 30881, 10139, 544, 7500, 13, 400, 337, 341, 7207, 11, 321, 30070, 294, 264, 4319, 295, 51672], "temperature": 0.0, "avg_logprob": -0.0808943271636963, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0006766212172806263}, {"id": 388, "seek": 214706, "start": 2147.86, "end": 2154.2599999999998, "text": " novices making changes to websites that they're seeing. So for example, you might have a case", "tokens": [50404, 23883, 1473, 1455, 2962, 281, 12891, 300, 436, 434, 2577, 13, 407, 337, 1365, 11, 291, 1062, 362, 257, 1389, 50724], "temperature": 0.0, "avg_logprob": -0.07132590900767934, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0032696202397346497}, {"id": 389, "seek": 214706, "start": 2154.2599999999998, "end": 2159.22, "text": " where you visited this website that colors hurt your eyes, or you couldn't really find this button", "tokens": [50724, 689, 291, 11220, 341, 3144, 300, 4577, 4607, 428, 2575, 11, 420, 291, 2809, 380, 534, 915, 341, 2960, 50972], "temperature": 0.0, "avg_logprob": -0.07132590900767934, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0032696202397346497}, {"id": 390, "seek": 214706, "start": 2159.22, "end": 2165.86, "text": " or tap it because it's too small, maybe you want to make it larger. But then people without", "tokens": [50972, 420, 5119, 309, 570, 309, 311, 886, 1359, 11, 1310, 291, 528, 281, 652, 309, 4833, 13, 583, 550, 561, 1553, 51304], "temperature": 0.0, "avg_logprob": -0.07132590900767934, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0032696202397346497}, {"id": 391, "seek": 214706, "start": 2165.86, "end": 2172.58, "text": " expertise in HTML and CSS have difficulty doing this. So we thought by leveraging the power of", "tokens": [51304, 11769, 294, 17995, 293, 24387, 362, 10360, 884, 341, 13, 407, 321, 1194, 538, 32666, 264, 1347, 295, 51640], "temperature": 0.0, "avg_logprob": -0.07132590900767934, "compression_ratio": 1.546938775510204, "no_speech_prob": 0.0032696202397346497}, {"id": 392, "seek": 217258, "start": 2172.58, "end": 2178.42, "text": " large language models and so on, maybe we can support more natural language queries. So if a", "tokens": [50364, 2416, 2856, 5245, 293, 370, 322, 11, 1310, 321, 393, 1406, 544, 3303, 2856, 24109, 13, 407, 498, 257, 50656], "temperature": 0.0, "avg_logprob": -0.056665897369384766, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.0014315915759652853}, {"id": 393, "seek": 217258, "start": 2178.42, "end": 2184.66, "text": " person says tone down the text, the system can kind of display these style recommendations that they", "tokens": [50656, 954, 1619, 8027, 760, 264, 2487, 11, 264, 1185, 393, 733, 295, 4674, 613, 3758, 10434, 300, 436, 50968], "temperature": 0.0, "avg_logprob": -0.056665897369384766, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.0014315915759652853}, {"id": 394, "seek": 217258, "start": 2184.66, "end": 2193.2999999999997, "text": " can explore and select from that are about toning down the text. So the way the system works is", "tokens": [50968, 393, 6839, 293, 3048, 490, 300, 366, 466, 2952, 278, 760, 264, 2487, 13, 407, 264, 636, 264, 1185, 1985, 307, 51400], "temperature": 0.0, "avg_logprob": -0.056665897369384766, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.0014315915759652853}, {"id": 395, "seek": 217258, "start": 2193.2999999999997, "end": 2200.2599999999998, "text": " if the user clicks and says make this larger, the system presents a set of design attributes that", "tokens": [51400, 498, 264, 4195, 18521, 293, 1619, 652, 341, 4833, 11, 264, 1185, 13533, 257, 992, 295, 1715, 17212, 300, 51748], "temperature": 0.0, "avg_logprob": -0.056665897369384766, "compression_ratio": 1.7276785714285714, "no_speech_prob": 0.0014315915759652853}, {"id": 396, "seek": 220026, "start": 2200.26, "end": 2210.82, "text": " are about making something larger. And the user can say emphasize this part. It's somewhat ambiguous.", "tokens": [50364, 366, 466, 1455, 746, 4833, 13, 400, 264, 4195, 393, 584, 16078, 341, 644, 13, 467, 311, 8344, 39465, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11387318271701619, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.00026939273811876774}, {"id": 397, "seek": 220026, "start": 2210.82, "end": 2216.1800000000003, "text": " There isn't a clear single design attribute that is about emphasis. So it presents these", "tokens": [50892, 821, 1943, 380, 257, 1850, 2167, 1715, 19667, 300, 307, 466, 16271, 13, 407, 309, 13533, 613, 51160], "temperature": 0.0, "avg_logprob": -0.11387318271701619, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.00026939273811876774}, {"id": 398, "seek": 220026, "start": 2217.2200000000003, "end": 2225.46, "text": " few recommendations that are about emphasizing something. So we built this by establishing", "tokens": [51212, 1326, 10434, 300, 366, 466, 45550, 746, 13, 407, 321, 3094, 341, 538, 22494, 51624], "temperature": 0.0, "avg_logprob": -0.11387318271701619, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.00026939273811876774}, {"id": 399, "seek": 222546, "start": 2225.46, "end": 2232.66, "text": " this NLP pipeline and computer vision pipeline. On the NLP side, what it does is analyzing the", "tokens": [50364, 341, 426, 45196, 15517, 293, 3820, 5201, 15517, 13, 1282, 264, 426, 45196, 1252, 11, 437, 309, 775, 307, 23663, 264, 50724], "temperature": 0.0, "avg_logprob": -0.06725334566692974, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0007431770209223032}, {"id": 400, "seek": 222546, "start": 2232.66, "end": 2239.7, "text": " user's query and mapping them with the style attributes that seem to be connected to what", "tokens": [50724, 4195, 311, 14581, 293, 18350, 552, 365, 264, 3758, 17212, 300, 1643, 281, 312, 4582, 281, 437, 51076], "temperature": 0.0, "avg_logprob": -0.06725334566692974, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0007431770209223032}, {"id": 401, "seek": 222546, "start": 2239.7, "end": 2246.7400000000002, "text": " the user's intent is about. In terms of computer vision, we collected millions of web design", "tokens": [51076, 264, 4195, 311, 8446, 307, 466, 13, 682, 2115, 295, 3820, 5201, 11, 321, 11087, 6803, 295, 3670, 1715, 51428], "temperature": 0.0, "avg_logprob": -0.06725334566692974, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0007431770209223032}, {"id": 402, "seek": 222546, "start": 2247.62, "end": 2254.02, "text": " elements to determine a good set of recommendations to show to the learner. So by combining those,", "tokens": [51472, 4959, 281, 6997, 257, 665, 992, 295, 10434, 281, 855, 281, 264, 33347, 13, 407, 538, 21928, 729, 11, 51792], "temperature": 0.0, "avg_logprob": -0.06725334566692974, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.0007431770209223032}, {"id": 403, "seek": 225402, "start": 2254.02, "end": 2259.94, "text": " we built this system. Again, so instead of going deep into the technical details of the system,", "tokens": [50364, 321, 3094, 341, 1185, 13, 3764, 11, 370, 2602, 295, 516, 2452, 666, 264, 6191, 4365, 295, 264, 1185, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1204922445889177, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.00042986293556168675}, {"id": 404, "seek": 225402, "start": 2259.94, "end": 2266.66, "text": " I want to focus on the interaction dynamics. So we ran this user study with 40 people where we", "tokens": [50660, 286, 528, 281, 1879, 322, 264, 9285, 15679, 13, 407, 321, 5872, 341, 4195, 2979, 365, 3356, 561, 689, 321, 50996], "temperature": 0.0, "avg_logprob": -0.1204922445889177, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.00042986293556168675}, {"id": 405, "seek": 225402, "start": 2266.66, "end": 2272.66, "text": " presented them with either stylet, which is the name of our system, versus the baseline, the", "tokens": [50996, 8212, 552, 365, 2139, 7952, 2631, 11, 597, 307, 264, 1315, 295, 527, 1185, 11, 5717, 264, 20518, 11, 264, 51296], "temperature": 0.0, "avg_logprob": -0.1204922445889177, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.00042986293556168675}, {"id": 406, "seek": 225402, "start": 2272.66, "end": 2277.22, "text": " Chrome developer tool, which is sort of the standard tool for making these style changes.", "tokens": [51296, 15327, 10754, 2290, 11, 597, 307, 1333, 295, 264, 3832, 2290, 337, 1455, 613, 3758, 2962, 13, 51524], "temperature": 0.0, "avg_logprob": -0.1204922445889177, "compression_ratio": 1.6147186147186148, "no_speech_prob": 0.00042986293556168675}, {"id": 407, "seek": 227722, "start": 2278.18, "end": 2284.3399999999997, "text": " So we compared these two groups. And we gave people two tasks. One is a well-defined task", "tokens": [50412, 407, 321, 5347, 613, 732, 3935, 13, 400, 321, 2729, 561, 732, 9608, 13, 1485, 307, 257, 731, 12, 37716, 5633, 50720], "temperature": 0.0, "avg_logprob": -0.0977251241495321, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.002843113150447607}, {"id": 408, "seek": 227722, "start": 2284.3399999999997, "end": 2290.8999999999996, "text": " where we ask people to turn this before image into an after image. And then secondly, we had this", "tokens": [50720, 689, 321, 1029, 561, 281, 1261, 341, 949, 3256, 666, 364, 934, 3256, 13, 400, 550, 26246, 11, 321, 632, 341, 51048], "temperature": 0.0, "avg_logprob": -0.0977251241495321, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.002843113150447607}, {"id": 409, "seek": 227722, "start": 2290.8999999999996, "end": 2296.1, "text": " open-ended task where we gave this blank slate and people were able to make any kind of change", "tokens": [51048, 1269, 12, 3502, 5633, 689, 321, 2729, 341, 8247, 39118, 293, 561, 645, 1075, 281, 652, 604, 733, 295, 1319, 51308], "temperature": 0.0, "avg_logprob": -0.0977251241495321, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.002843113150447607}, {"id": 410, "seek": 227722, "start": 2296.1, "end": 2304.8999999999996, "text": " that they want. First, I want to share success stories. People were more successful in completing", "tokens": [51308, 300, 436, 528, 13, 2386, 11, 286, 528, 281, 2073, 2245, 3676, 13, 3432, 645, 544, 4406, 294, 19472, 51748], "temperature": 0.0, "avg_logprob": -0.0977251241495321, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.002843113150447607}, {"id": 411, "seek": 230490, "start": 2304.98, "end": 2312.1800000000003, "text": " these design tasks when using stylet. 80% of the stylet users completed the task as opposed to only", "tokens": [50368, 613, 1715, 9608, 562, 1228, 7952, 2631, 13, 4688, 4, 295, 264, 7952, 2631, 5022, 7365, 264, 5633, 382, 8851, 281, 787, 50728], "temperature": 0.0, "avg_logprob": -0.10661786178062702, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.001097952714189887}, {"id": 412, "seek": 230490, "start": 2312.1800000000003, "end": 2318.1800000000003, "text": " 35% in Chrome developer tools. And these were complete novices in web design, no experience at all.", "tokens": [50728, 6976, 4, 294, 15327, 10754, 3873, 13, 400, 613, 645, 3566, 23883, 1473, 294, 3670, 1715, 11, 572, 1752, 412, 439, 13, 51028], "temperature": 0.0, "avg_logprob": -0.10661786178062702, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.001097952714189887}, {"id": 413, "seek": 230490, "start": 2319.06, "end": 2324.5, "text": " And people completed the task in 35% less times. It was efficient to use stylet.", "tokens": [51072, 400, 561, 7365, 264, 5633, 294, 6976, 4, 1570, 1413, 13, 467, 390, 7148, 281, 764, 7952, 2631, 13, 51344], "temperature": 0.0, "avg_logprob": -0.10661786178062702, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.001097952714189887}, {"id": 414, "seek": 230490, "start": 2325.38, "end": 2331.3, "text": " Another interesting observation was that people were making same similar number of changes in", "tokens": [51388, 3996, 1880, 14816, 390, 300, 561, 645, 1455, 912, 2531, 1230, 295, 2962, 294, 51684], "temperature": 0.0, "avg_logprob": -0.10661786178062702, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.001097952714189887}, {"id": 415, "seek": 233130, "start": 2331.3, "end": 2338.7400000000002, "text": " both conditions. But in stylet condition, people were making more diverse changes, which means that", "tokens": [50364, 1293, 4487, 13, 583, 294, 7952, 2631, 4188, 11, 561, 645, 1455, 544, 9521, 2962, 11, 597, 1355, 300, 50736], "temperature": 0.0, "avg_logprob": -0.09720944797291475, "compression_ratio": 1.7, "no_speech_prob": 0.000939305464271456}, {"id": 416, "seek": 233130, "start": 2338.7400000000002, "end": 2344.7400000000002, "text": " it probably had to do with how stylet shows these multiple options for people to explore. And there", "tokens": [50736, 309, 1391, 632, 281, 360, 365, 577, 7952, 2631, 3110, 613, 3866, 3956, 337, 561, 281, 6839, 13, 400, 456, 51036], "temperature": 0.0, "avg_logprob": -0.09720944797291475, "compression_ratio": 1.7, "no_speech_prob": 0.000939305464271456}, {"id": 417, "seek": 233130, "start": 2344.7400000000002, "end": 2350.1000000000004, "text": " was a conscious decision to not just show the most obvious one, but show somewhat related ones as", "tokens": [51036, 390, 257, 6648, 3537, 281, 406, 445, 855, 264, 881, 6322, 472, 11, 457, 855, 8344, 4077, 2306, 382, 51304], "temperature": 0.0, "avg_logprob": -0.09720944797291475, "compression_ratio": 1.7, "no_speech_prob": 0.000939305464271456}, {"id": 418, "seek": 233130, "start": 2350.1000000000004, "end": 2356.26, "text": " well so that people could explore and tinker around different options. But then an unexpected", "tokens": [51304, 731, 370, 300, 561, 727, 6839, 293, 256, 40467, 926, 819, 3956, 13, 583, 550, 364, 13106, 51612], "temperature": 0.0, "avg_logprob": -0.09720944797291475, "compression_ratio": 1.7, "no_speech_prob": 0.000939305464271456}, {"id": 419, "seek": 235626, "start": 2356.82, "end": 2362.26, "text": " finding was when we looked at people's self-confidence. Because we thought this kind of system would be", "tokens": [50392, 5006, 390, 562, 321, 2956, 412, 561, 311, 2698, 12, 47273, 13, 1436, 321, 1194, 341, 733, 295, 1185, 576, 312, 50664], "temperature": 0.0, "avg_logprob": -0.0596745482114988, "compression_ratio": 1.9703389830508475, "no_speech_prob": 0.008438197895884514}, {"id": 420, "seek": 235626, "start": 2362.26, "end": 2367.86, "text": " useful for people's learning of the skills and confidence that they have about the skills,", "tokens": [50664, 4420, 337, 561, 311, 2539, 295, 264, 3942, 293, 6687, 300, 436, 362, 466, 264, 3942, 11, 50944], "temperature": 0.0, "avg_logprob": -0.0596745482114988, "compression_ratio": 1.9703389830508475, "no_speech_prob": 0.008438197895884514}, {"id": 421, "seek": 235626, "start": 2367.86, "end": 2373.5400000000004, "text": " we asked people's self-confidence after each task. What we noted was that after the first task,", "tokens": [50944, 321, 2351, 561, 311, 2698, 12, 47273, 934, 1184, 5633, 13, 708, 321, 12964, 390, 300, 934, 264, 700, 5633, 11, 51228], "temperature": 0.0, "avg_logprob": -0.0596745482114988, "compression_ratio": 1.9703389830508475, "no_speech_prob": 0.008438197895884514}, {"id": 422, "seek": 235626, "start": 2374.1000000000004, "end": 2378.98, "text": " in both conditions, people's self-confidence increased. But then in the second task,", "tokens": [51256, 294, 1293, 4487, 11, 561, 311, 2698, 12, 47273, 6505, 13, 583, 550, 294, 264, 1150, 5633, 11, 51500], "temperature": 0.0, "avg_logprob": -0.0596745482114988, "compression_ratio": 1.9703389830508475, "no_speech_prob": 0.008438197895884514}, {"id": 423, "seek": 235626, "start": 2378.98, "end": 2385.2200000000003, "text": " after the second task, users' self-confidence decreased for stylet while in the developer", "tokens": [51500, 934, 264, 1150, 5633, 11, 5022, 6, 2698, 12, 47273, 24436, 337, 7952, 2631, 1339, 294, 264, 10754, 51812], "temperature": 0.0, "avg_logprob": -0.0596745482114988, "compression_ratio": 1.9703389830508475, "no_speech_prob": 0.008438197895884514}, {"id": 424, "seek": 238522, "start": 2385.22, "end": 2391.22, "text": " tool, it kept increasing. Why would that be the case? And we were seeing many cases where", "tokens": [50364, 2290, 11, 309, 4305, 5662, 13, 1545, 576, 300, 312, 264, 1389, 30, 400, 321, 645, 2577, 867, 3331, 689, 50664], "temperature": 0.0, "avg_logprob": -0.10776512145996094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0025073529686778784}, {"id": 425, "seek": 238522, "start": 2391.22, "end": 2396.4199999999996, "text": " stylet users were frustrated that the only control that they had was natural language.", "tokens": [50664, 7952, 2631, 5022, 645, 15751, 300, 264, 787, 1969, 300, 436, 632, 390, 3303, 2856, 13, 50924], "temperature": 0.0, "avg_logprob": -0.10776512145996094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0025073529686778784}, {"id": 426, "seek": 238522, "start": 2397.06, "end": 2401.22, "text": " Now they have some grasp of how it works. They wanted to do more fine-grained control more", "tokens": [50956, 823, 436, 362, 512, 21743, 295, 577, 309, 1985, 13, 814, 1415, 281, 360, 544, 2489, 12, 20735, 2001, 1969, 544, 51164], "temperature": 0.0, "avg_logprob": -0.10776512145996094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0025073529686778784}, {"id": 427, "seek": 238522, "start": 2401.22, "end": 2406.18, "text": " directly. And they wanted more specific things. But because they only had natural language,", "tokens": [51164, 3838, 13, 400, 436, 1415, 544, 2685, 721, 13, 583, 570, 436, 787, 632, 3303, 2856, 11, 51412], "temperature": 0.0, "avg_logprob": -0.10776512145996094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0025073529686778784}, {"id": 428, "seek": 238522, "start": 2406.74, "end": 2411.7799999999997, "text": " they sometimes just got frustrated. Whereas in the Chrome Developer Tools condition,", "tokens": [51440, 436, 2171, 445, 658, 15751, 13, 13813, 294, 264, 15327, 44915, 30302, 4188, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10776512145996094, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0025073529686778784}, {"id": 429, "seek": 241178, "start": 2411.78, "end": 2416.26, "text": " people were just happy that they accomplished something with their own hands.", "tokens": [50364, 561, 645, 445, 2055, 300, 436, 15419, 746, 365, 641, 1065, 2377, 13, 50588], "temperature": 0.0, "avg_logprob": -0.10949291327060798, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.0007788221118971705}, {"id": 430, "seek": 241178, "start": 2416.26, "end": 2422.02, "text": " And I think that is presented as a continued increase in self-confidence.", "tokens": [50588, 400, 286, 519, 300, 307, 8212, 382, 257, 7014, 3488, 294, 2698, 12, 47273, 13, 50876], "temperature": 0.0, "avg_logprob": -0.10949291327060798, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.0007788221118971705}, {"id": 431, "seek": 241178, "start": 2423.2200000000003, "end": 2431.86, "text": " And we know from HCI and CS147 that people's expertise and learnability really matters. And as", "tokens": [50936, 400, 321, 458, 490, 389, 25240, 293, 9460, 7271, 22, 300, 561, 311, 11769, 293, 1466, 2310, 534, 7001, 13, 400, 382, 51368], "temperature": 0.0, "avg_logprob": -0.10949291327060798, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.0007788221118971705}, {"id": 432, "seek": 241178, "start": 2431.86, "end": 2439.1400000000003, "text": " they have more knowledge of the domain and the skill, they might need to get more advanced", "tokens": [51368, 436, 362, 544, 3601, 295, 264, 9274, 293, 264, 5389, 11, 436, 1062, 643, 281, 483, 544, 7339, 51732], "temperature": 0.0, "avg_logprob": -0.10949291327060798, "compression_ratio": 1.5044642857142858, "no_speech_prob": 0.0007788221118971705}, {"id": 433, "seek": 243914, "start": 2439.14, "end": 2445.06, "text": " controls or being able to more directly manipulate what they are working on. So I think this had", "tokens": [50364, 9003, 420, 885, 1075, 281, 544, 3838, 20459, 437, 436, 366, 1364, 322, 13, 407, 286, 519, 341, 632, 50660], "temperature": 0.0, "avg_logprob": -0.08578313665187105, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0011327615939080715}, {"id": 434, "seek": 243914, "start": 2445.06, "end": 2450.02, "text": " some interesting lessons in terms of thinking about the temporal dimension in that learners are", "tokens": [50660, 512, 1880, 8820, 294, 2115, 295, 1953, 466, 264, 30881, 10139, 294, 300, 23655, 366, 50908], "temperature": 0.0, "avg_logprob": -0.08578313665187105, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0011327615939080715}, {"id": 435, "seek": 243914, "start": 2450.02, "end": 2457.06, "text": " changing. And other researchers have been reporting that considering these temporal dynamics is", "tokens": [50908, 4473, 13, 400, 661, 10309, 362, 668, 10031, 300, 8079, 613, 30881, 15679, 307, 51260], "temperature": 0.0, "avg_logprob": -0.08578313665187105, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0011327615939080715}, {"id": 436, "seek": 243914, "start": 2457.06, "end": 2462.9, "text": " important. On the left, what you see is design researchers who have shown that there are these", "tokens": [51260, 1021, 13, 1282, 264, 1411, 11, 437, 291, 536, 307, 1715, 10309, 567, 362, 4898, 300, 456, 366, 613, 51552], "temperature": 0.0, "avg_logprob": -0.08578313665187105, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0011327615939080715}, {"id": 437, "seek": 243914, "start": 2462.9, "end": 2468.9, "text": " different stages of relationship that people have in technologies like self-tracking devices.", "tokens": [51552, 819, 10232, 295, 2480, 300, 561, 362, 294, 7943, 411, 2698, 12, 6903, 14134, 5759, 13, 51852], "temperature": 0.0, "avg_logprob": -0.08578313665187105, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0011327615939080715}, {"id": 438, "seek": 246914, "start": 2469.22, "end": 2474.42, "text": " First, they would start with initiation and experimentation, followed by intensifying and", "tokens": [50368, 2386, 11, 436, 576, 722, 365, 43569, 293, 37142, 11, 6263, 538, 14056, 5489, 293, 50628], "temperature": 0.0, "avg_logprob": -0.1044019232404993, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.0003735603531822562}, {"id": 439, "seek": 246914, "start": 2474.42, "end": 2479.62, "text": " integration, and then stagnation and termination. And one of the design lessons might be that", "tokens": [50628, 10980, 11, 293, 550, 32853, 399, 293, 1433, 2486, 13, 400, 472, 295, 264, 1715, 8820, 1062, 312, 300, 50888], "temperature": 0.0, "avg_logprob": -0.1044019232404993, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.0003735603531822562}, {"id": 440, "seek": 246914, "start": 2480.18, "end": 2486.3399999999997, "text": " these might be more meta-level factors that really should be considered in design systems,", "tokens": [50916, 613, 1062, 312, 544, 19616, 12, 12418, 6771, 300, 534, 820, 312, 4888, 294, 1715, 3652, 11, 51224], "temperature": 0.0, "avg_logprob": -0.1044019232404993, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.0003735603531822562}, {"id": 441, "seek": 246914, "start": 2486.3399999999997, "end": 2491.7799999999997, "text": " in that even the same kind of intervention might need to be presented in different", "tokens": [51224, 294, 300, 754, 264, 912, 733, 295, 13176, 1062, 643, 281, 312, 8212, 294, 819, 51496], "temperature": 0.0, "avg_logprob": -0.1044019232404993, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.0003735603531822562}, {"id": 442, "seek": 246914, "start": 2491.7799999999997, "end": 2496.3399999999997, "text": " manners depending on what stage you are or what your expectation is with the system.", "tokens": [51496, 34672, 5413, 322, 437, 3233, 291, 366, 420, 437, 428, 14334, 307, 365, 264, 1185, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1044019232404993, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.0003735603531822562}, {"id": 443, "seek": 249634, "start": 2496.9, "end": 2502.26, "text": " On the right, what you're seeing is the guidelines for human-AI interaction, really influential", "tokens": [50392, 1282, 264, 558, 11, 437, 291, 434, 2577, 307, 264, 12470, 337, 1952, 12, 48698, 9285, 11, 534, 22215, 50660], "temperature": 0.0, "avg_logprob": -0.1910840258186246, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0003248611174058169}, {"id": 444, "seek": 249634, "start": 2502.26, "end": 2508.42, "text": " work from Emershi et al. And they organize these guidelines for human-AI interaction", "tokens": [50660, 589, 490, 3968, 433, 4954, 1030, 419, 13, 400, 436, 13859, 613, 12470, 337, 1952, 12, 48698, 9285, 50968], "temperature": 0.0, "avg_logprob": -0.1910840258186246, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0003248611174058169}, {"id": 445, "seek": 249634, "start": 2508.42, "end": 2513.86, "text": " in different categories but are organized in the temporal sort of aspect, like initially", "tokens": [50968, 294, 819, 10479, 457, 366, 9983, 294, 264, 30881, 1333, 295, 4171, 11, 411, 9105, 51240], "temperature": 0.0, "avg_logprob": -0.1910840258186246, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0003248611174058169}, {"id": 446, "seek": 249634, "start": 2513.86, "end": 2520.7400000000002, "text": " encounter with AI during interaction, when things go wrong, and over time. So taking into account", "tokens": [51240, 8593, 365, 7318, 1830, 9285, 11, 562, 721, 352, 2085, 11, 293, 670, 565, 13, 407, 1940, 666, 2696, 51584], "temperature": 0.0, "avg_logprob": -0.1910840258186246, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0003248611174058169}, {"id": 447, "seek": 252074, "start": 2520.8199999999997, "end": 2526.18, "text": " this temporal dimension can really be powerful in supporting more sustainable engagement.", "tokens": [50368, 341, 30881, 10139, 393, 534, 312, 4005, 294, 7231, 544, 11235, 8742, 13, 50636], "temperature": 0.0, "avg_logprob": -0.09822662965750989, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.005997575353831053}, {"id": 448, "seek": 252074, "start": 2527.62, "end": 2534.1, "text": " And the related question might be, as people are relying more on these AI tools, like grammar", "tokens": [50708, 400, 264, 4077, 1168, 1062, 312, 11, 382, 561, 366, 24140, 544, 322, 613, 7318, 3873, 11, 411, 22317, 51032], "temperature": 0.0, "avg_logprob": -0.09822662965750989, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.005997575353831053}, {"id": 449, "seek": 252074, "start": 2534.1, "end": 2540.66, "text": " fixes or even generating text, it's important to think about how people's mental model would", "tokens": [51032, 32539, 420, 754, 17746, 2487, 11, 309, 311, 1021, 281, 519, 466, 577, 561, 311, 4973, 2316, 576, 51360], "temperature": 0.0, "avg_logprob": -0.09822662965750989, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.005997575353831053}, {"id": 450, "seek": 252074, "start": 2540.66, "end": 2547.54, "text": " change over time, and AI also changes over time too. And do we hit a point where people become", "tokens": [51360, 1319, 670, 565, 11, 293, 7318, 611, 2962, 670, 565, 886, 13, 400, 360, 321, 2045, 257, 935, 689, 561, 1813, 51704], "temperature": 0.0, "avg_logprob": -0.09822662965750989, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.005997575353831053}, {"id": 451, "seek": 254754, "start": 2547.54, "end": 2554.1, "text": " maybe overly reliant in that maybe their grammar skills or writing skills do not improve anymore,", "tokens": [50364, 1310, 24324, 1039, 5798, 294, 300, 1310, 641, 22317, 3942, 420, 3579, 3942, 360, 406, 3470, 3602, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1032297090552319, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0009540124447084963}, {"id": 452, "seek": 254754, "start": 2554.66, "end": 2560.1, "text": " but then without the tool, they actually might perform worse? And what is that dynamic? Or maybe", "tokens": [50720, 457, 550, 1553, 264, 2290, 11, 436, 767, 1062, 2042, 5324, 30, 400, 437, 307, 300, 8546, 30, 1610, 1310, 50992], "temperature": 0.0, "avg_logprob": -0.1032297090552319, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0009540124447084963}, {"id": 453, "seek": 254754, "start": 2560.1, "end": 2564.58, "text": " over reliance is perfectly fine because if we believe these tools will be around the user all", "tokens": [50992, 670, 1039, 6276, 307, 6239, 2489, 570, 498, 321, 1697, 613, 3873, 486, 312, 926, 264, 4195, 439, 51216], "temperature": 0.0, "avg_logprob": -0.1032297090552319, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0009540124447084963}, {"id": 454, "seek": 254754, "start": 2564.58, "end": 2570.2599999999998, "text": " the time, maybe it's just the final outcome that matters. And I think we need more studies and", "tokens": [51216, 264, 565, 11, 1310, 309, 311, 445, 264, 2572, 9700, 300, 7001, 13, 400, 286, 519, 321, 643, 544, 5313, 293, 51500], "temperature": 0.0, "avg_logprob": -0.1032297090552319, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0009540124447084963}, {"id": 455, "seek": 257026, "start": 2570.34, "end": 2575.78, "text": " analysis of the long-term engagement of users using these kind of technologies.", "tokens": [50368, 5215, 295, 264, 938, 12, 7039, 8742, 295, 5022, 1228, 613, 733, 295, 7943, 13, 50640], "temperature": 0.0, "avg_logprob": -0.09302534527248807, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0014093936188146472}, {"id": 456, "seek": 257026, "start": 2579.2200000000003, "end": 2583.7000000000003, "text": " And to kind of sum up, if we were to take a more model-centric approach here, I think we might", "tokens": [50812, 400, 281, 733, 295, 2408, 493, 11, 498, 321, 645, 281, 747, 257, 544, 2316, 12, 45300, 3109, 510, 11, 286, 519, 321, 1062, 51036], "temperature": 0.0, "avg_logprob": -0.09302534527248807, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0014093936188146472}, {"id": 457, "seek": 257026, "start": 2583.7000000000003, "end": 2591.2200000000003, "text": " have built a system that makes automatic design fixes to optimize a web page directly, and the", "tokens": [51036, 362, 3094, 257, 1185, 300, 1669, 12509, 1715, 32539, 281, 19719, 257, 3670, 3028, 3838, 11, 293, 264, 51412], "temperature": 0.0, "avg_logprob": -0.09302534527248807, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0014093936188146472}, {"id": 458, "seek": 257026, "start": 2591.2200000000003, "end": 2597.38, "text": " system makes a fix and user can just use it. But instead, we took a more sort of interaction-centric", "tokens": [51412, 1185, 1669, 257, 3191, 293, 4195, 393, 445, 764, 309, 13, 583, 2602, 11, 321, 1890, 257, 544, 1333, 295, 9285, 12, 45300, 51720], "temperature": 0.0, "avg_logprob": -0.09302534527248807, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.0014093936188146472}, {"id": 459, "seek": 259738, "start": 2597.38, "end": 2603.78, "text": " route where we asked people to do sort of, you know, style change by themselves as the system", "tokens": [50364, 7955, 689, 321, 2351, 561, 281, 360, 1333, 295, 11, 291, 458, 11, 3758, 1319, 538, 2969, 382, 264, 1185, 50684], "temperature": 0.0, "avg_logprob": -0.0956006096404733, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004130595829337835}, {"id": 460, "seek": 259738, "start": 2603.78, "end": 2609.46, "text": " was presenting these recommendations, and they still had to do the fix by themselves. But what", "tokens": [50684, 390, 15578, 613, 10434, 11, 293, 436, 920, 632, 281, 360, 264, 3191, 538, 2969, 13, 583, 437, 50968], "temperature": 0.0, "avg_logprob": -0.0956006096404733, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004130595829337835}, {"id": 461, "seek": 259738, "start": 2609.46, "end": 2615.46, "text": " we expected here was that people can then customize by seeing these attributes, they can learn,", "tokens": [50968, 321, 5176, 510, 390, 300, 561, 393, 550, 19734, 538, 2577, 613, 17212, 11, 436, 393, 1466, 11, 51268], "temperature": 0.0, "avg_logprob": -0.0956006096404733, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004130595829337835}, {"id": 462, "seek": 259738, "start": 2615.46, "end": 2620.1, "text": " they can discover new ways of doing things, they can think around, which can empower them,", "tokens": [51268, 436, 393, 4411, 777, 2098, 295, 884, 721, 11, 436, 393, 519, 926, 11, 597, 393, 11071, 552, 11, 51500], "temperature": 0.0, "avg_logprob": -0.0956006096404733, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004130595829337835}, {"id": 463, "seek": 259738, "start": 2620.1, "end": 2625.1400000000003, "text": " especially in the more learning context, although the temporal dimension has to be more carefully", "tokens": [51500, 2318, 294, 264, 544, 2539, 4319, 11, 4878, 264, 30881, 10139, 575, 281, 312, 544, 7500, 51752], "temperature": 0.0, "avg_logprob": -0.0956006096404733, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.004130595829337835}, {"id": 464, "seek": 262514, "start": 2625.14, "end": 2631.7799999999997, "text": " taken into account. So these were the four challenges that I wanted to", "tokens": [50364, 2726, 666, 2696, 13, 407, 613, 645, 264, 1451, 4759, 300, 286, 1415, 281, 50696], "temperature": 0.0, "avg_logprob": -0.08493666589995962, "compression_ratio": 1.4914529914529915, "no_speech_prob": 0.00015595006698276848}, {"id": 465, "seek": 262514, "start": 2631.7799999999997, "end": 2637.14, "text": " share today. And to kind of wrap up, I just wanted to pose two questions moving forward", "tokens": [50696, 2073, 965, 13, 400, 281, 733, 295, 7019, 493, 11, 286, 445, 1415, 281, 10774, 732, 1651, 2684, 2128, 50964], "temperature": 0.0, "avg_logprob": -0.08493666589995962, "compression_ratio": 1.4914529914529915, "no_speech_prob": 0.00015595006698276848}, {"id": 466, "seek": 262514, "start": 2638.02, "end": 2645.2999999999997, "text": " from the interaction-centric perspective as HCI researchers. So first is, how might we design", "tokens": [51008, 490, 264, 9285, 12, 45300, 4585, 382, 389, 25240, 10309, 13, 407, 700, 307, 11, 577, 1062, 321, 1715, 51372], "temperature": 0.0, "avg_logprob": -0.08493666589995962, "compression_ratio": 1.4914529914529915, "no_speech_prob": 0.00015595006698276848}, {"id": 467, "seek": 262514, "start": 2645.2999999999997, "end": 2652.8199999999997, "text": " these building blocks and interface affordances for new and upcoming AI models? Okay, so I think", "tokens": [51372, 613, 2390, 8474, 293, 9226, 6157, 2676, 337, 777, 293, 11500, 7318, 5245, 30, 1033, 11, 370, 286, 519, 51748], "temperature": 0.0, "avg_logprob": -0.08493666589995962, "compression_ratio": 1.4914529914529915, "no_speech_prob": 0.00015595006698276848}, {"id": 468, "seek": 265282, "start": 2652.9, "end": 2658.1800000000003, "text": " part of it is that instead of building these point solutions, I think we need to think about,", "tokens": [50368, 644, 295, 309, 307, 300, 2602, 295, 2390, 613, 935, 6547, 11, 286, 519, 321, 643, 281, 519, 466, 11, 50632], "temperature": 0.0, "avg_logprob": -0.0795508761738622, "compression_ratio": 1.6380090497737556, "no_speech_prob": 0.0005439877859316766}, {"id": 469, "seek": 265282, "start": 2658.1800000000003, "end": 2663.7000000000003, "text": " are there any sort of generalizable frameworks, libraries, widgets, or interface affordances", "tokens": [50632, 366, 456, 604, 1333, 295, 2674, 22395, 29834, 11, 15148, 11, 43355, 11, 420, 9226, 6157, 2676, 50908], "temperature": 0.0, "avg_logprob": -0.0795508761738622, "compression_ratio": 1.6380090497737556, "no_speech_prob": 0.0005439877859316766}, {"id": 470, "seek": 265282, "start": 2664.34, "end": 2669.86, "text": " that we could come up with as a community that is really good at these kinds of things?", "tokens": [50940, 300, 321, 727, 808, 493, 365, 382, 257, 1768, 300, 307, 534, 665, 412, 613, 3685, 295, 721, 30, 51216], "temperature": 0.0, "avg_logprob": -0.0795508761738622, "compression_ratio": 1.6380090497737556, "no_speech_prob": 0.0005439877859316766}, {"id": 471, "seek": 265282, "start": 2670.5800000000004, "end": 2676.42, "text": " And the second question is, does AI really require us to have these new things? I mean,", "tokens": [51252, 400, 264, 1150, 1168, 307, 11, 775, 7318, 534, 3651, 505, 281, 362, 613, 777, 721, 30, 286, 914, 11, 51544], "temperature": 0.0, "avg_logprob": -0.0795508761738622, "compression_ratio": 1.6380090497737556, "no_speech_prob": 0.0005439877859316766}, {"id": 472, "seek": 267642, "start": 2676.42, "end": 2682.02, "text": " can we just use existing design elements and frameworks to build AI applications?", "tokens": [50364, 393, 321, 445, 764, 6741, 1715, 4959, 293, 29834, 281, 1322, 7318, 5821, 30, 50644], "temperature": 0.0, "avg_logprob": -0.054233612952294286, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0011690892279148102}, {"id": 473, "seek": 267642, "start": 2682.82, "end": 2690.26, "text": " And I tend to think that we might need something new for these new and upcoming AI models,", "tokens": [50684, 400, 286, 3928, 281, 519, 300, 321, 1062, 643, 746, 777, 337, 613, 777, 293, 11500, 7318, 5245, 11, 51056], "temperature": 0.0, "avg_logprob": -0.054233612952294286, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0011690892279148102}, {"id": 474, "seek": 267642, "start": 2690.26, "end": 2695.46, "text": " especially because they have these very different characteristics than the conventional systems", "tokens": [51056, 2318, 570, 436, 362, 613, 588, 819, 10891, 813, 264, 16011, 3652, 51316], "temperature": 0.0, "avg_logprob": -0.054233612952294286, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0011690892279148102}, {"id": 475, "seek": 267642, "start": 2695.46, "end": 2701.06, "text": " that we have been building. They're more probabilistic, harder to predict, more black box in nature,", "tokens": [51316, 300, 321, 362, 668, 2390, 13, 814, 434, 544, 31959, 3142, 11, 6081, 281, 6069, 11, 544, 2211, 2424, 294, 3687, 11, 51596], "temperature": 0.0, "avg_logprob": -0.054233612952294286, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.0011690892279148102}, {"id": 476, "seek": 270106, "start": 2701.62, "end": 2707.7, "text": " yet seemingly more impactful and powerful in terms of what they do, hallucinating. All these", "tokens": [50392, 1939, 18709, 544, 30842, 293, 4005, 294, 2115, 295, 437, 436, 360, 11, 35212, 8205, 13, 1057, 613, 50696], "temperature": 0.0, "avg_logprob": -0.080250793033176, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0005612761015072465}, {"id": 477, "seek": 270106, "start": 2707.7, "end": 2713.46, "text": " properties packed together, I think we might really need to think about, what are the types", "tokens": [50696, 7221, 13265, 1214, 11, 286, 519, 321, 1062, 534, 643, 281, 519, 466, 11, 437, 366, 264, 3467, 50984], "temperature": 0.0, "avg_logprob": -0.080250793033176, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0005612761015072465}, {"id": 478, "seek": 270106, "start": 2713.46, "end": 2720.34, "text": " of interaction affordances that are really built for supporting the usability of these", "tokens": [50984, 295, 9285, 6157, 2676, 300, 366, 534, 3094, 337, 7231, 264, 46878, 295, 613, 51328], "temperature": 0.0, "avg_logprob": -0.080250793033176, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0005612761015072465}, {"id": 479, "seek": 270106, "start": 2720.34, "end": 2728.66, "text": " AI-powered applications? So in this, I think as a community, we are making all these great", "tokens": [51328, 7318, 12, 27178, 5821, 30, 407, 294, 341, 11, 286, 519, 382, 257, 1768, 11, 321, 366, 1455, 439, 613, 869, 51744], "temperature": 0.0, "avg_logprob": -0.080250793033176, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.0005612761015072465}, {"id": 480, "seek": 272866, "start": 2728.66, "end": 2733.2999999999997, "text": " advances, like making different types of contributions. And I tend to focus on more", "tokens": [50364, 25297, 11, 411, 1455, 819, 3467, 295, 15725, 13, 400, 286, 3928, 281, 1879, 322, 544, 50596], "temperature": 0.0, "avg_logprob": -0.09081347947268142, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0023209801875054836}, {"id": 481, "seek": 272866, "start": 2733.8599999999997, "end": 2739.7, "text": " interactive systems and techniques, whereas other people focus on introducing new design", "tokens": [50624, 15141, 3652, 293, 7512, 11, 9735, 661, 561, 1879, 322, 15424, 777, 1715, 50916], "temperature": 0.0, "avg_logprob": -0.09081347947268142, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0023209801875054836}, {"id": 482, "seek": 272866, "start": 2739.7, "end": 2744.8199999999997, "text": " processes and understandings. And I think all this work is needed. And some of the interesting", "tokens": [50916, 7555, 293, 1223, 1109, 13, 400, 286, 519, 439, 341, 589, 307, 2978, 13, 400, 512, 295, 264, 1880, 51172], "temperature": 0.0, "avg_logprob": -0.09081347947268142, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0023209801875054836}, {"id": 483, "seek": 272866, "start": 2744.8199999999997, "end": 2751.8599999999997, "text": " examples of adding an interaction layer to these new types of models is in this example,", "tokens": [51172, 5110, 295, 5127, 364, 9285, 4583, 281, 613, 777, 3467, 295, 5245, 307, 294, 341, 1365, 11, 51524], "temperature": 0.0, "avg_logprob": -0.09081347947268142, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0023209801875054836}, {"id": 484, "seek": 272866, "start": 2751.8599999999997, "end": 2758.2599999999998, "text": " Tailbrush, where the user can draw the level of fortune that they won in the character to have", "tokens": [51524, 46074, 21330, 11, 689, 264, 4195, 393, 2642, 264, 1496, 295, 16531, 300, 436, 1582, 294, 264, 2517, 281, 362, 51844], "temperature": 0.0, "avg_logprob": -0.09081347947268142, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.0023209801875054836}, {"id": 485, "seek": 275826, "start": 2758.26, "end": 2768.1000000000004, "text": " when they use generative models to generate a story. Or this AI chains work, which presents these", "tokens": [50364, 562, 436, 764, 1337, 1166, 5245, 281, 8460, 257, 1657, 13, 1610, 341, 7318, 12626, 589, 11, 597, 13533, 613, 50856], "temperature": 0.0, "avg_logprob": -0.10691139433119032, "compression_ratio": 1.553763440860215, "no_speech_prob": 0.00037304367288015783}, {"id": 486, "seek": 275826, "start": 2768.1000000000004, "end": 2775.6200000000003, "text": " primitives and workflows for putting together this workflow that can accomplish more complex tasks", "tokens": [50856, 2886, 38970, 293, 43461, 337, 3372, 1214, 341, 20993, 300, 393, 9021, 544, 3997, 9608, 51232], "temperature": 0.0, "avg_logprob": -0.10691139433119032, "compression_ratio": 1.553763440860215, "no_speech_prob": 0.00037304367288015783}, {"id": 487, "seek": 275826, "start": 2775.6200000000003, "end": 2783.5400000000004, "text": " with these LLM prompts that a single prompt cannot really perform. And in my research group,", "tokens": [51232, 365, 613, 441, 43, 44, 41095, 300, 257, 2167, 12391, 2644, 534, 2042, 13, 400, 294, 452, 2132, 1594, 11, 51628], "temperature": 0.0, "avg_logprob": -0.10691139433119032, "compression_ratio": 1.553763440860215, "no_speech_prob": 0.00037304367288015783}, {"id": 488, "seek": 278354, "start": 2783.54, "end": 2789.22, "text": " with my PhD student, Tesu Kim, we have been investigating this idea of what would be more", "tokens": [50364, 365, 452, 14476, 3107, 11, 12262, 84, 5652, 11, 321, 362, 668, 22858, 341, 1558, 295, 437, 576, 312, 544, 50648], "temperature": 0.0, "avg_logprob": -0.12944322609039674, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.02293580211699009}, {"id": 489, "seek": 278354, "start": 2789.22, "end": 2794.82, "text": " generalizable design framework. And thinking about input, model, and output, we have been", "tokens": [50648, 2674, 22395, 1715, 8388, 13, 400, 1953, 466, 4846, 11, 2316, 11, 293, 5598, 11, 321, 362, 668, 50928], "temperature": 0.0, "avg_logprob": -0.12944322609039674, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.02293580211699009}, {"id": 490, "seek": 278354, "start": 2794.82, "end": 2801.86, "text": " thinking about the concepts of cells, generators, and lenses, and tried to introduce this standardized", "tokens": [50928, 1953, 466, 264, 10392, 295, 5438, 11, 38662, 11, 293, 18059, 11, 293, 3031, 281, 5366, 341, 31677, 51280], "temperature": 0.0, "avg_logprob": -0.12944322609039674, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.02293580211699009}, {"id": 491, "seek": 278354, "start": 2801.86, "end": 2807.3, "text": " libraries and widgets that people can easily adopt in their AI applications. So for example,", "tokens": [51280, 15148, 293, 43355, 300, 561, 393, 3612, 6878, 294, 641, 7318, 5821, 13, 407, 337, 1365, 11, 51552], "temperature": 0.0, "avg_logprob": -0.12944322609039674, "compression_ratio": 1.5690376569037656, "no_speech_prob": 0.02293580211699009}, {"id": 492, "seek": 280730, "start": 2807.3, "end": 2814.1000000000004, "text": " using this kind of framework, people can build a copywriting app, email app, or story writing", "tokens": [50364, 1228, 341, 733, 295, 8388, 11, 561, 393, 1322, 257, 5055, 19868, 724, 11, 3796, 724, 11, 420, 1657, 3579, 50704], "temperature": 0.0, "avg_logprob": -0.09971160199268754, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0010983939282596111}, {"id": 493, "seek": 280730, "start": 2814.1000000000004, "end": 2819.86, "text": " app using pretty much the same kind of framework, which can save people's time while supporting", "tokens": [50704, 724, 1228, 1238, 709, 264, 912, 733, 295, 8388, 11, 597, 393, 3155, 561, 311, 565, 1339, 7231, 50992], "temperature": 0.0, "avg_logprob": -0.09971160199268754, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0010983939282596111}, {"id": 494, "seek": 280730, "start": 2819.86, "end": 2825.2200000000003, "text": " the types of interactions like iterations and comparison and experimenting different outputs.", "tokens": [50992, 264, 3467, 295, 13280, 411, 36540, 293, 9660, 293, 29070, 819, 23930, 13, 51260], "temperature": 0.0, "avg_logprob": -0.09971160199268754, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0010983939282596111}, {"id": 495, "seek": 280730, "start": 2827.54, "end": 2832.98, "text": " And the second question, and the final question that I want to ask today is, how might we as an", "tokens": [51376, 400, 264, 1150, 1168, 11, 293, 264, 2572, 1168, 300, 286, 528, 281, 1029, 965, 307, 11, 577, 1062, 321, 382, 364, 51648], "temperature": 0.0, "avg_logprob": -0.09971160199268754, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0010983939282596111}, {"id": 496, "seek": 283298, "start": 2832.98, "end": 2838.98, "text": " HCI community collaborate better with the AI community on these various things? And it was also", "tokens": [50364, 389, 25240, 1768, 18338, 1101, 365, 264, 7318, 1768, 322, 613, 3683, 721, 30, 400, 309, 390, 611, 50664], "temperature": 0.0, "avg_logprob": -0.08078930237713983, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.002282574540004134}, {"id": 497, "seek": 283298, "start": 2838.98, "end": 2845.14, "text": " the discussion that I was having a lot with today's meetings, and also with various AI researchers,", "tokens": [50664, 264, 5017, 300, 286, 390, 1419, 257, 688, 365, 965, 311, 8410, 11, 293, 611, 365, 3683, 7318, 10309, 11, 50972], "temperature": 0.0, "avg_logprob": -0.08078930237713983, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.002282574540004134}, {"id": 498, "seek": 283298, "start": 2845.14, "end": 2850.66, "text": " especially in Europe. And in terms of community collaboration, of course, one of the important", "tokens": [50972, 2318, 294, 3315, 13, 400, 294, 2115, 295, 1768, 9363, 11, 295, 1164, 11, 472, 295, 264, 1021, 51248], "temperature": 0.0, "avg_logprob": -0.08078930237713983, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.002282574540004134}, {"id": 499, "seek": 283298, "start": 2850.66, "end": 2855.78, "text": " things is metrics. And there was also a great discussion at the HCI conference a couple weeks", "tokens": [51248, 721, 307, 16367, 13, 400, 456, 390, 611, 257, 869, 5017, 412, 264, 389, 25240, 7586, 257, 1916, 3259, 51504], "temperature": 0.0, "avg_logprob": -0.08078930237713983, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.002282574540004134}, {"id": 500, "seek": 285578, "start": 2855.78, "end": 2862.9, "text": " ago, hosted here at Stanford. And in the AI community, it cares a lot about model performance", "tokens": [50364, 2057, 11, 19204, 510, 412, 20374, 13, 400, 294, 264, 7318, 1768, 11, 309, 12310, 257, 688, 466, 2316, 3389, 50720], "temperature": 0.0, "avg_logprob": -0.07363241321437962, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.004063967149704695}, {"id": 501, "seek": 285578, "start": 2862.9, "end": 2868.26, "text": " and generalization errors, where in HCI, we tend to focus on the human experience. So how do we", "tokens": [50720, 293, 2674, 2144, 13603, 11, 689, 294, 389, 25240, 11, 321, 3928, 281, 1879, 322, 264, 1952, 1752, 13, 407, 577, 360, 321, 50988], "temperature": 0.0, "avg_logprob": -0.07363241321437962, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.004063967149704695}, {"id": 502, "seek": 285578, "start": 2868.26, "end": 2874.9, "text": " really bridge the gap between the metrics? And what it means to do AI research with more human", "tokens": [50988, 534, 7283, 264, 7417, 1296, 264, 16367, 30, 400, 437, 309, 1355, 281, 360, 7318, 2132, 365, 544, 1952, 51320], "temperature": 0.0, "avg_logprob": -0.07363241321437962, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.004063967149704695}, {"id": 503, "seek": 285578, "start": 2874.9, "end": 2880.26, "text": " side metrics incorporated? What's the incentive for people to do that? And how do we encourage", "tokens": [51320, 1252, 16367, 21654, 30, 708, 311, 264, 22346, 337, 561, 281, 360, 300, 30, 400, 577, 360, 321, 5373, 51588], "temperature": 0.0, "avg_logprob": -0.07363241321437962, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.004063967149704695}, {"id": 504, "seek": 288026, "start": 2880.26, "end": 2886.98, "text": " poor AI people to use these metrics, too? In terms of human input design, a lot of the comments", "tokens": [50364, 4716, 7318, 561, 281, 764, 613, 16367, 11, 886, 30, 682, 2115, 295, 1952, 4846, 1715, 11, 257, 688, 295, 264, 3053, 50700], "temperature": 0.0, "avg_logprob": -0.08737434040416371, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.002286217175424099}, {"id": 505, "seek": 288026, "start": 2886.98, "end": 2892.5, "text": " that I was getting in terms of interaction-centric AI from AI researchers is that these ideas are", "tokens": [50700, 300, 286, 390, 1242, 294, 2115, 295, 9285, 12, 45300, 7318, 490, 7318, 10309, 307, 300, 613, 3487, 366, 50976], "temperature": 0.0, "avg_logprob": -0.08737434040416371, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.002286217175424099}, {"id": 506, "seek": 288026, "start": 2892.5, "end": 2898.7400000000002, "text": " great, but then I don't really know how to actually take action about it. And part of it is, in their", "tokens": [50976, 869, 11, 457, 550, 286, 500, 380, 534, 458, 577, 281, 767, 747, 3069, 466, 309, 13, 400, 644, 295, 309, 307, 11, 294, 641, 51288], "temperature": 0.0, "avg_logprob": -0.08737434040416371, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.002286217175424099}, {"id": 507, "seek": 288026, "start": 2898.7400000000002, "end": 2905.5400000000004, "text": " model-building kind of work, how can I incorporate human feedback? And how do I use it in a meaningful", "tokens": [51288, 2316, 12, 28126, 733, 295, 589, 11, 577, 393, 286, 16091, 1952, 5824, 30, 400, 577, 360, 286, 764, 309, 294, 257, 10995, 51628], "temperature": 0.0, "avg_logprob": -0.08737434040416371, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.002286217175424099}, {"id": 508, "seek": 290554, "start": 2905.54, "end": 2911.22, "text": " way to really change the way the model actually works, rather than just getting more high-level", "tokens": [50364, 636, 281, 534, 1319, 264, 636, 264, 2316, 767, 1985, 11, 2831, 813, 445, 1242, 544, 1090, 12, 12418, 50648], "temperature": 0.0, "avg_logprob": -0.11704643663153591, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0005030903848819435}, {"id": 509, "seek": 290554, "start": 2911.22, "end": 2920.5, "text": " design guidance? So one great direction for this might be, think about more making human feedback,", "tokens": [50648, 1715, 10056, 30, 407, 472, 869, 3513, 337, 341, 1062, 312, 11, 519, 466, 544, 1455, 1952, 5824, 11, 51112], "temperature": 0.0, "avg_logprob": -0.11704643663153591, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0005030903848819435}, {"id": 510, "seek": 290554, "start": 2920.5, "end": 2924.98, "text": " more computationally feasible, so that this compatibility is actually satisfied.", "tokens": [51112, 544, 24903, 379, 26648, 11, 370, 300, 341, 34237, 307, 767, 11239, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11704643663153591, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0005030903848819435}, {"id": 511, "seek": 290554, "start": 2926.5, "end": 2930.82, "text": " And lastly, we need to think about the change in design process as well. And in a lot of,", "tokens": [51412, 400, 16386, 11, 321, 643, 281, 519, 466, 264, 1319, 294, 1715, 1399, 382, 731, 13, 400, 294, 257, 688, 295, 11, 51628], "temperature": 0.0, "avg_logprob": -0.11704643663153591, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.0005030903848819435}, {"id": 512, "seek": 293082, "start": 2931.54, "end": 2937.46, "text": " this is Stanford D-School's user-centered design cycle. And I think in a lot of the AI research,", "tokens": [50400, 341, 307, 20374, 413, 12, 50, 21856, 311, 4195, 12, 36814, 1715, 6586, 13, 400, 286, 519, 294, 257, 688, 295, 264, 7318, 2132, 11, 50696], "temperature": 0.0, "avg_logprob": -0.1300338783649483, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.005052695982158184}, {"id": 513, "seek": 293082, "start": 2937.46, "end": 2942.5, "text": " what we're seeing is this prototype test kind of culture. You try something new, test it,", "tokens": [50696, 437, 321, 434, 2577, 307, 341, 19475, 1500, 733, 295, 3713, 13, 509, 853, 746, 777, 11, 1500, 309, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1300338783649483, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.005052695982158184}, {"id": 514, "seek": 293082, "start": 2942.5, "end": 2948.82, "text": " iteratively improve it. But then one of the frustrations is that interaction often comes", "tokens": [50948, 17138, 19020, 3470, 309, 13, 583, 550, 472, 295, 264, 7454, 12154, 307, 300, 9285, 2049, 1487, 51264], "temperature": 0.0, "avg_logprob": -0.1300338783649483, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.005052695982158184}, {"id": 515, "seek": 293082, "start": 2948.82, "end": 2955.46, "text": " too late, right? There's this new cool model, and can you build an UI on top of it, is sort of the", "tokens": [51264, 886, 3469, 11, 558, 30, 821, 311, 341, 777, 1627, 2316, 11, 293, 393, 291, 1322, 364, 15682, 322, 1192, 295, 309, 11, 307, 1333, 295, 264, 51596], "temperature": 0.0, "avg_logprob": -0.1300338783649483, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.005052695982158184}, {"id": 516, "seek": 295546, "start": 2955.54, "end": 2960.82, "text": " kind of discourse we get a lot. And I think interaction should not just be like an icing", "tokens": [50368, 733, 295, 23938, 321, 483, 257, 688, 13, 400, 286, 519, 9285, 820, 406, 445, 312, 411, 364, 30086, 50632], "temperature": 0.0, "avg_logprob": -0.06876239694398024, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0027140940073877573}, {"id": 517, "seek": 295546, "start": 2960.82, "end": 2966.66, "text": " on the cake, but really something that can guide the entire design process or help people determine,", "tokens": [50632, 322, 264, 5908, 11, 457, 534, 746, 300, 393, 5934, 264, 2302, 1715, 1399, 420, 854, 561, 6997, 11, 50924], "temperature": 0.0, "avg_logprob": -0.06876239694398024, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0027140940073877573}, {"id": 518, "seek": 295546, "start": 2966.66, "end": 2972.26, "text": " is this the right problem to tackle in the first place? Or what kind of interaction should we try", "tokens": [50924, 307, 341, 264, 558, 1154, 281, 14896, 294, 264, 700, 1081, 30, 1610, 437, 733, 295, 9285, 820, 321, 853, 51204], "temperature": 0.0, "avg_logprob": -0.06876239694398024, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0027140940073877573}, {"id": 519, "seek": 295546, "start": 2972.26, "end": 2977.62, "text": " to support with AI? And based on that, think of what AI should do and should not do and how much", "tokens": [51204, 281, 1406, 365, 7318, 30, 400, 2361, 322, 300, 11, 519, 295, 437, 7318, 820, 360, 293, 820, 406, 360, 293, 577, 709, 51472], "temperature": 0.0, "avg_logprob": -0.06876239694398024, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0027140940073877573}, {"id": 520, "seek": 295546, "start": 2977.62, "end": 2984.98, "text": " AI should be used in a particular context. So that's all I wanted to share. And here's a summary", "tokens": [51472, 7318, 820, 312, 1143, 294, 257, 1729, 4319, 13, 407, 300, 311, 439, 286, 1415, 281, 2073, 13, 400, 510, 311, 257, 12691, 51840], "temperature": 0.0, "avg_logprob": -0.06876239694398024, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.0027140940073877573}, {"id": 521, "seek": 298498, "start": 2984.98, "end": 2989.14, "text": " of what I mentioned today, and I'd be happy to take any questions. Thank you.", "tokens": [50364, 295, 437, 286, 2835, 965, 11, 293, 286, 1116, 312, 2055, 281, 747, 604, 1651, 13, 1044, 291, 13, 50572], "temperature": 0.0, "avg_logprob": -0.20508275208649812, "compression_ratio": 1.3178807947019868, "no_speech_prob": 0.0013440684415400028}, {"id": 522, "seek": 298498, "start": 2994.66, "end": 2999.22, "text": " All right, so I'll check my recommendations of facilitating messages. If there are any more ideas.", "tokens": [50848, 1057, 558, 11, 370, 286, 603, 1520, 452, 10434, 295, 47558, 7897, 13, 759, 456, 366, 604, 544, 3487, 13, 51076], "temperature": 0.0, "avg_logprob": -0.20508275208649812, "compression_ratio": 1.3178807947019868, "no_speech_prob": 0.0013440684415400028}, {"id": 523, "seek": 298498, "start": 3001.14, "end": 3005.46, "text": " No? What do you think?", "tokens": [51172, 883, 30, 708, 360, 291, 519, 30, 51388], "temperature": 0.0, "avg_logprob": -0.20508275208649812, "compression_ratio": 1.3178807947019868, "no_speech_prob": 0.0013440684415400028}, {"id": 524, "seek": 300546, "start": 3006.1, "end": 3009.62, "text": " It really sounded like an AI.", "tokens": [50396, 467, 534, 17714, 411, 364, 7318, 13, 50572], "temperature": 0.0, "avg_logprob": -0.16539472415123457, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.02554725483059883}, {"id": 525, "seek": 300546, "start": 3011.2200000000003, "end": 3021.06, "text": " I'll just click them all. I want to pull the mic on. I want to pull the thread a little bit on", "tokens": [50652, 286, 603, 445, 2052, 552, 439, 13, 286, 528, 281, 2235, 264, 3123, 322, 13, 286, 528, 281, 2235, 264, 7207, 257, 707, 857, 322, 51144], "temperature": 0.0, "avg_logprob": -0.16539472415123457, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.02554725483059883}, {"id": 526, "seek": 300546, "start": 3021.06, "end": 3028.9, "text": " this notion of how to connect human feedback with the objective functions that you touched on near", "tokens": [51144, 341, 10710, 295, 577, 281, 1745, 1952, 5824, 365, 264, 10024, 6828, 300, 291, 9828, 322, 2651, 51536], "temperature": 0.0, "avg_logprob": -0.16539472415123457, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.02554725483059883}, {"id": 527, "seek": 300546, "start": 3028.9, "end": 3033.7, "text": " the end, because that's been rattling around in my head in much of the talk that you're giving,", "tokens": [51536, 264, 917, 11, 570, 300, 311, 668, 48822, 926, 294, 452, 1378, 294, 709, 295, 264, 751, 300, 291, 434, 2902, 11, 51776], "temperature": 0.0, "avg_logprob": -0.16539472415123457, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.02554725483059883}, {"id": 528, "seek": 303370, "start": 3033.7, "end": 3037.3799999999997, "text": " that if I think about what should AI researchers be doing differently,", "tokens": [50364, 300, 498, 286, 519, 466, 437, 820, 7318, 10309, 312, 884, 7614, 11, 50548], "temperature": 0.0, "avg_logprob": -0.12733114682711089, "compression_ratio": 1.5900383141762453, "no_speech_prob": 0.008568796329200268}, {"id": 529, "seek": 303370, "start": 3038.98, "end": 3044.18, "text": " then you're asking, well, what's the proper model of the person in their system?", "tokens": [50628, 550, 291, 434, 3365, 11, 731, 11, 437, 311, 264, 2296, 2316, 295, 264, 954, 294, 641, 1185, 30, 50888], "temperature": 0.0, "avg_logprob": -0.12733114682711089, "compression_ratio": 1.5900383141762453, "no_speech_prob": 0.008568796329200268}, {"id": 530, "seek": 303370, "start": 3045.46, "end": 3049.2999999999997, "text": " And traditionally, the problem has been that human interaction is really expensive,", "tokens": [50952, 400, 19067, 11, 264, 1154, 575, 668, 300, 1952, 9285, 307, 534, 5124, 11, 51144], "temperature": 0.0, "avg_logprob": -0.12733114682711089, "compression_ratio": 1.5900383141762453, "no_speech_prob": 0.008568796329200268}, {"id": 531, "seek": 303370, "start": 3050.2599999999998, "end": 3054.4199999999996, "text": " just to collect annotated data. Or once you have it to be able to tune the model,", "tokens": [51192, 445, 281, 2500, 25339, 770, 1412, 13, 1610, 1564, 291, 362, 309, 281, 312, 1075, 281, 10864, 264, 2316, 11, 51400], "temperature": 0.0, "avg_logprob": -0.12733114682711089, "compression_ratio": 1.5900383141762453, "no_speech_prob": 0.008568796329200268}, {"id": 532, "seek": 303370, "start": 3056.2599999999998, "end": 3062.5, "text": " you don't get that much of it. And so they often fall back on self-supervision, or as you've been", "tokens": [51492, 291, 500, 380, 483, 300, 709, 295, 309, 13, 400, 370, 436, 2049, 2100, 646, 322, 2698, 12, 48172, 6763, 11, 420, 382, 291, 600, 668, 51804], "temperature": 0.0, "avg_logprob": -0.12733114682711089, "compression_ratio": 1.5900383141762453, "no_speech_prob": 0.008568796329200268}, {"id": 533, "seek": 306250, "start": 3062.5, "end": 3068.42, "text": " talking about in the value alignment, they train an RL model to mimic a human and then let that go", "tokens": [50364, 1417, 466, 294, 264, 2158, 18515, 11, 436, 3847, 364, 497, 43, 2316, 281, 31075, 257, 1952, 293, 550, 718, 300, 352, 50660], "temperature": 0.0, "avg_logprob": -0.12621089278674516, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.02440672181546688}, {"id": 534, "seek": 306250, "start": 3068.42, "end": 3074.02, "text": " loose. And it seems like until, I think they're kind of, I want you to take a position on one of", "tokens": [50660, 9612, 13, 400, 309, 2544, 411, 1826, 11, 286, 519, 436, 434, 733, 295, 11, 286, 528, 291, 281, 747, 257, 2535, 322, 472, 295, 50940], "temperature": 0.0, "avg_logprob": -0.12621089278674516, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.02440672181546688}, {"id": 535, "seek": 306250, "start": 3074.02, "end": 3079.38, "text": " the two positions. One either is to say, look, we need to find strategies like that where we can", "tokens": [50940, 264, 732, 8432, 13, 1485, 2139, 307, 281, 584, 11, 574, 11, 321, 643, 281, 915, 9029, 411, 300, 689, 321, 393, 51208], "temperature": 0.0, "avg_logprob": -0.12621089278674516, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.02440672181546688}, {"id": 536, "seek": 306250, "start": 3079.38, "end": 3086.9, "text": " create proxy humans, and that's how we hook into the objective functions, the loss functions, etc.", "tokens": [51208, 1884, 29690, 6255, 11, 293, 300, 311, 577, 321, 6328, 666, 264, 10024, 6828, 11, 264, 4470, 6828, 11, 5183, 13, 51584], "temperature": 0.0, "avg_logprob": -0.12621089278674516, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.02440672181546688}, {"id": 537, "seek": 306250, "start": 3086.9, "end": 3092.1, "text": " The other alternative would be to say, no, we're going to find some other way to actually", "tokens": [51584, 440, 661, 8535, 576, 312, 281, 584, 11, 572, 11, 321, 434, 516, 281, 915, 512, 661, 636, 281, 767, 51844], "temperature": 0.0, "avg_logprob": -0.12621089278674516, "compression_ratio": 1.7117437722419928, "no_speech_prob": 0.02440672181546688}, {"id": 538, "seek": 309210, "start": 3092.1, "end": 3100.02, "text": " make human feedback at a scale and in a form that they can directly use in the models. I'm", "tokens": [50364, 652, 1952, 5824, 412, 257, 4373, 293, 294, 257, 1254, 300, 436, 393, 3838, 764, 294, 264, 5245, 13, 286, 478, 50760], "temperature": 0.0, "avg_logprob": -0.10079647730855108, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048776634503155947}, {"id": 539, "seek": 309210, "start": 3100.02, "end": 3104.2599999999998, "text": " just curious, like, if you want to take a bet, where's your bet on that? Where should we be heading?", "tokens": [50760, 445, 6369, 11, 411, 11, 498, 291, 528, 281, 747, 257, 778, 11, 689, 311, 428, 778, 322, 300, 30, 2305, 820, 321, 312, 9864, 30, 50972], "temperature": 0.0, "avg_logprob": -0.10079647730855108, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048776634503155947}, {"id": 540, "seek": 309210, "start": 3105.46, "end": 3111.7, "text": " Yeah, that's an excellent question. I would say, I mean, you asked me to take a position,", "tokens": [51032, 865, 11, 300, 311, 364, 7103, 1168, 13, 286, 576, 584, 11, 286, 914, 11, 291, 2351, 385, 281, 747, 257, 2535, 11, 51344], "temperature": 0.0, "avg_logprob": -0.10079647730855108, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048776634503155947}, {"id": 541, "seek": 309210, "start": 3111.7, "end": 3118.58, "text": " but I would say both will be prevalent. And I like the letter much more. And I think that's more", "tokens": [51344, 457, 286, 576, 584, 1293, 486, 312, 30652, 13, 400, 286, 411, 264, 5063, 709, 544, 13, 400, 286, 519, 300, 311, 544, 51688], "temperature": 0.0, "avg_logprob": -0.10079647730855108, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.00048776634503155947}, {"id": 542, "seek": 311858, "start": 3118.58, "end": 3123.54, "text": " promising and sustainable. And for example, the reason I'm really interested in this, like,", "tokens": [50364, 20257, 293, 11235, 13, 400, 337, 1365, 11, 264, 1778, 286, 478, 534, 3102, 294, 341, 11, 411, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12217700746324327, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.014698134735226631}, {"id": 543, "seek": 311858, "start": 3123.54, "end": 3129.62, "text": " co-learning feedback loop between the human and the machine is that, you know, even if this super", "tokens": [50612, 598, 12, 47204, 5824, 6367, 1296, 264, 1952, 293, 264, 3479, 307, 300, 11, 291, 458, 11, 754, 498, 341, 1687, 50916], "temperature": 0.0, "avg_logprob": -0.12217700746324327, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.014698134735226631}, {"id": 544, "seek": 311858, "start": 3129.62, "end": 3136.02, "text": " advanced AI comes along and let's say it presents this, like, super accurate explanations, people's", "tokens": [50916, 7339, 7318, 1487, 2051, 293, 718, 311, 584, 309, 13533, 341, 11, 411, 11, 1687, 8559, 28708, 11, 561, 311, 51236], "temperature": 0.0, "avg_logprob": -0.12217700746324327, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.014698134735226631}, {"id": 545, "seek": 311858, "start": 3136.02, "end": 3141.7799999999997, "text": " self-explanation activity is still meaningful, right? Because that's how they could learn.", "tokens": [51236, 2698, 12, 3121, 16554, 399, 5191, 307, 920, 10995, 11, 558, 30, 1436, 300, 311, 577, 436, 727, 1466, 13, 51524], "temperature": 0.0, "avg_logprob": -0.12217700746324327, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.014698134735226631}, {"id": 546, "seek": 314178, "start": 3141.78, "end": 3147.6200000000003, "text": " And so I feel like, you know, we can really try to find these compatible", "tokens": [50364, 400, 370, 286, 841, 411, 11, 291, 458, 11, 321, 393, 534, 853, 281, 915, 613, 18218, 50656], "temperature": 0.0, "avg_logprob": -0.0702365560269137, "compression_ratio": 1.8388429752066116, "no_speech_prob": 0.00364775862544775}, {"id": 547, "seek": 314178, "start": 3149.7000000000003, "end": 3155.94, "text": " mechanisms in which the human can get the benefit and get the incentive for doing what", "tokens": [50760, 15902, 294, 597, 264, 1952, 393, 483, 264, 5121, 293, 483, 264, 22346, 337, 884, 437, 51072], "temperature": 0.0, "avg_logprob": -0.0702365560269137, "compression_ratio": 1.8388429752066116, "no_speech_prob": 0.00364775862544775}, {"id": 548, "seek": 314178, "start": 3155.94, "end": 3160.26, "text": " they are really good at and what is helpful for them, not necessarily trying to help the system", "tokens": [51072, 436, 366, 534, 665, 412, 293, 437, 307, 4961, 337, 552, 11, 406, 4725, 1382, 281, 854, 264, 1185, 51288], "temperature": 0.0, "avg_logprob": -0.0702365560269137, "compression_ratio": 1.8388429752066116, "no_speech_prob": 0.00364775862544775}, {"id": 549, "seek": 314178, "start": 3160.26, "end": 3166.34, "text": " or, you know, getting paid to system, paid to support the system per se. And at the same time,", "tokens": [51288, 420, 11, 291, 458, 11, 1242, 4835, 281, 1185, 11, 4835, 281, 1406, 264, 1185, 680, 369, 13, 400, 412, 264, 912, 565, 11, 51592], "temperature": 0.0, "avg_logprob": -0.0702365560269137, "compression_ratio": 1.8388429752066116, "no_speech_prob": 0.00364775862544775}, {"id": 550, "seek": 314178, "start": 3166.34, "end": 3170.9, "text": " the system can use it for something meaningful. And on the system side, I think in the system,", "tokens": [51592, 264, 1185, 393, 764, 309, 337, 746, 10995, 13, 400, 322, 264, 1185, 1252, 11, 286, 519, 294, 264, 1185, 11, 51820], "temperature": 0.0, "avg_logprob": -0.0702365560269137, "compression_ratio": 1.8388429752066116, "no_speech_prob": 0.00364775862544775}, {"id": 551, "seek": 317090, "start": 3170.9, "end": 3177.86, "text": " like, access that I presented, I was really happy when we landed at this technical solution where", "tokens": [50364, 411, 11, 2105, 300, 286, 8212, 11, 286, 390, 534, 2055, 562, 321, 15336, 412, 341, 6191, 3827, 689, 50712], "temperature": 0.0, "avg_logprob": -0.07852154307895237, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0038784558419138193}, {"id": 552, "seek": 317090, "start": 3177.86, "end": 3185.86, "text": " people's rating data could be almost directly piped into the feedback for the RL agent to kind", "tokens": [50712, 561, 311, 10990, 1412, 727, 312, 1920, 3838, 8489, 292, 666, 264, 5824, 337, 264, 497, 43, 9461, 281, 733, 51112], "temperature": 0.0, "avg_logprob": -0.07852154307895237, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0038784558419138193}, {"id": 553, "seek": 317090, "start": 3185.86, "end": 3191.3, "text": " of use as meaningful feedback. So I think that's just one example where this kind of worked out", "tokens": [51112, 295, 764, 382, 10995, 5824, 13, 407, 286, 519, 300, 311, 445, 472, 1365, 689, 341, 733, 295, 2732, 484, 51384], "temperature": 0.0, "avg_logprob": -0.07852154307895237, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0038784558419138193}, {"id": 554, "seek": 317090, "start": 3191.3, "end": 3196.9, "text": " for this kind of context. And I think we need to really investigate more and think about, are there", "tokens": [51384, 337, 341, 733, 295, 4319, 13, 400, 286, 519, 321, 643, 281, 534, 15013, 544, 293, 519, 466, 11, 366, 456, 51664], "temperature": 0.0, "avg_logprob": -0.07852154307895237, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0038784558419138193}, {"id": 555, "seek": 319690, "start": 3197.46, "end": 3202.7400000000002, "text": " any generalizable mechanisms that this kind of approach could work in different contexts?", "tokens": [50392, 604, 2674, 22395, 15902, 300, 341, 733, 295, 3109, 727, 589, 294, 819, 30628, 30, 50656], "temperature": 0.0, "avg_logprob": -0.12057103750840673, "compression_ratio": 1.5940959409594095, "no_speech_prob": 0.002886531874537468}, {"id": 556, "seek": 319690, "start": 3202.7400000000002, "end": 3207.38, "text": " This assumes that you have a large set of users you can draw on, like there are learners that", "tokens": [50656, 639, 37808, 300, 291, 362, 257, 2416, 992, 295, 5022, 291, 393, 2642, 322, 11, 411, 456, 366, 23655, 300, 50888], "temperature": 0.0, "avg_logprob": -0.12057103750840673, "compression_ratio": 1.5940959409594095, "no_speech_prob": 0.002886531874537468}, {"id": 557, "seek": 319690, "start": 3207.38, "end": 3212.1, "text": " are coming through your system. If I'm early on in the pipeline and I just kind of have V0,", "tokens": [50888, 366, 1348, 807, 428, 1185, 13, 759, 286, 478, 2440, 322, 294, 264, 15517, 293, 286, 445, 733, 295, 362, 691, 15, 11, 51124], "temperature": 0.0, "avg_logprob": -0.12057103750840673, "compression_ratio": 1.5940959409594095, "no_speech_prob": 0.002886531874537468}, {"id": 558, "seek": 319690, "start": 3212.1, "end": 3215.2200000000003, "text": " I don't have the users yet, are there strategies you would recommend?", "tokens": [51124, 286, 500, 380, 362, 264, 5022, 1939, 11, 366, 456, 9029, 291, 576, 2748, 30, 51280], "temperature": 0.0, "avg_logprob": -0.12057103750840673, "compression_ratio": 1.5940959409594095, "no_speech_prob": 0.002886531874537468}, {"id": 559, "seek": 319690, "start": 3216.02, "end": 3220.7400000000002, "text": " Yeah, yeah, excellent. So in that same access system, for instance, what we did was to", "tokens": [51320, 865, 11, 1338, 11, 7103, 13, 407, 294, 300, 912, 2105, 1185, 11, 337, 5197, 11, 437, 321, 630, 390, 281, 51556], "temperature": 0.0, "avg_logprob": -0.12057103750840673, "compression_ratio": 1.5940959409594095, "no_speech_prob": 0.002886531874537468}, {"id": 560, "seek": 322074, "start": 3221.62, "end": 3229.7799999999997, "text": " insert the instructor-generated explanations as sort of the initial seed. And I was also imagining", "tokens": [50408, 8969, 264, 18499, 12, 21848, 770, 28708, 382, 1333, 295, 264, 5883, 8871, 13, 400, 286, 390, 611, 27798, 50816], "temperature": 0.0, "avg_logprob": -0.10162597232394749, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0021801339462399483}, {"id": 561, "seek": 322074, "start": 3229.7799999999997, "end": 3236.2599999999998, "text": " maybe using LLMs, for instance, we can plug in AI-generated ones to kind of avoid the cold", "tokens": [50816, 1310, 1228, 441, 43, 26386, 11, 337, 5197, 11, 321, 393, 5452, 294, 7318, 12, 21848, 770, 2306, 281, 733, 295, 5042, 264, 3554, 51140], "temperature": 0.0, "avg_logprob": -0.10162597232394749, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0021801339462399483}, {"id": 562, "seek": 322074, "start": 3236.2599999999998, "end": 3242.3399999999997, "text": " start problem. And it would be interesting to see how, you know, in the same system, like AI-generated", "tokens": [51140, 722, 1154, 13, 400, 309, 576, 312, 1880, 281, 536, 577, 11, 291, 458, 11, 294, 264, 912, 1185, 11, 411, 7318, 12, 21848, 770, 51444], "temperature": 0.0, "avg_logprob": -0.10162597232394749, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0021801339462399483}, {"id": 563, "seek": 322074, "start": 3242.3399999999997, "end": 3247.8599999999997, "text": " ones, instructor-generated ones, and learner-generated ones can kind of compete against each other", "tokens": [51444, 2306, 11, 18499, 12, 21848, 770, 2306, 11, 293, 33347, 12, 21848, 770, 2306, 393, 733, 295, 11831, 1970, 1184, 661, 51720], "temperature": 0.0, "avg_logprob": -0.10162597232394749, "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0021801339462399483}, {"id": 564, "seek": 324786, "start": 3247.86, "end": 3251.78, "text": " until the system ultimately just focuses on what is best for learners.", "tokens": [50364, 1826, 264, 1185, 6284, 445, 16109, 322, 437, 307, 1151, 337, 23655, 13, 50560], "temperature": 0.0, "avg_logprob": -0.11989705702837776, "compression_ratio": 1.59375, "no_speech_prob": 0.0037005331832915545}, {"id": 565, "seek": 324786, "start": 3252.82, "end": 3259.06, "text": " This is kind of a two-part question, going back to the like third challenge or like project you", "tokens": [50612, 639, 307, 733, 295, 257, 732, 12, 6971, 1168, 11, 516, 646, 281, 264, 411, 2636, 3430, 420, 411, 1716, 291, 50924], "temperature": 0.0, "avg_logprob": -0.11989705702837776, "compression_ratio": 1.59375, "no_speech_prob": 0.0037005331832915545}, {"id": 566, "seek": 324786, "start": 3259.06, "end": 3265.38, "text": " talked about, where there was that note about AI as proxy, like people kind of using that as", "tokens": [50924, 2825, 466, 11, 689, 456, 390, 300, 3637, 466, 7318, 382, 29690, 11, 411, 561, 733, 295, 1228, 300, 382, 51240], "temperature": 0.0, "avg_logprob": -0.11989705702837776, "compression_ratio": 1.59375, "no_speech_prob": 0.0037005331832915545}, {"id": 567, "seek": 324786, "start": 3266.1, "end": 3270.7400000000002, "text": " like an excuse to make points, where maybe they wanted to do something but didn't want it to come", "tokens": [51276, 411, 364, 8960, 281, 652, 2793, 11, 689, 1310, 436, 1415, 281, 360, 746, 457, 994, 380, 528, 309, 281, 808, 51508], "temperature": 0.0, "avg_logprob": -0.11989705702837776, "compression_ratio": 1.59375, "no_speech_prob": 0.0037005331832915545}, {"id": 568, "seek": 327074, "start": 3270.74, "end": 3278.18, "text": " off as them. So the first part of the question is like, in that case, did people want to,", "tokens": [50364, 766, 382, 552, 13, 407, 264, 700, 644, 295, 264, 1168, 307, 411, 11, 294, 300, 1389, 11, 630, 561, 528, 281, 11, 50736], "temperature": 0.0, "avg_logprob": -0.1032192029451069, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.14178559184074402}, {"id": 569, "seek": 327074, "start": 3279.14, "end": 3283.7, "text": " later it says people wanted the message to kind of sound like them, but in the case of the AI", "tokens": [50784, 1780, 309, 1619, 561, 1415, 264, 3636, 281, 733, 295, 1626, 411, 552, 11, 457, 294, 264, 1389, 295, 264, 7318, 51012], "temperature": 0.0, "avg_logprob": -0.1032192029451069, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.14178559184074402}, {"id": 570, "seek": 327074, "start": 3283.7, "end": 3289.7799999999997, "text": " as proxy, did they want that to sound like them? Or were they wanting it to sound more artificial?", "tokens": [51012, 382, 29690, 11, 630, 436, 528, 300, 281, 1626, 411, 552, 30, 1610, 645, 436, 7935, 309, 281, 1626, 544, 11677, 30, 51316], "temperature": 0.0, "avg_logprob": -0.1032192029451069, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.14178559184074402}, {"id": 571, "seek": 327074, "start": 3289.7799999999997, "end": 3295.9399999999996, "text": " And then second part of the question is, do you think there are more situations than just this", "tokens": [51316, 400, 550, 1150, 644, 295, 264, 1168, 307, 11, 360, 291, 519, 456, 366, 544, 6851, 813, 445, 341, 51624], "temperature": 0.0, "avg_logprob": -0.1032192029451069, "compression_ratio": 1.7952380952380953, "no_speech_prob": 0.14178559184074402}, {"id": 572, "seek": 329594, "start": 3296.02, "end": 3302.5, "text": " where maybe we don't want the AI to feel super personable and maybe want the interaction to", "tokens": [50368, 689, 1310, 321, 500, 380, 528, 264, 7318, 281, 841, 1687, 954, 712, 293, 1310, 528, 264, 9285, 281, 50692], "temperature": 0.0, "avg_logprob": -0.11238460326462649, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00446384446695447}, {"id": 573, "seek": 329594, "start": 3302.5, "end": 3307.62, "text": " feel slightly more kind of mechanical or unnatural? Yeah, that's an excellent question. And I would", "tokens": [50692, 841, 4748, 544, 733, 295, 12070, 420, 43470, 30, 865, 11, 300, 311, 364, 7103, 1168, 13, 400, 286, 576, 50948], "temperature": 0.0, "avg_logprob": -0.11238460326462649, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00446384446695447}, {"id": 574, "seek": 329594, "start": 3307.62, "end": 3314.9, "text": " say these were somewhat different use cases, and both I think are valuable and smite. And", "tokens": [50948, 584, 613, 645, 8344, 819, 764, 3331, 11, 293, 1293, 286, 519, 366, 8263, 293, 899, 642, 13, 400, 51312], "temperature": 0.0, "avg_logprob": -0.11238460326462649, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00446384446695447}, {"id": 575, "seek": 329594, "start": 3316.34, "end": 3321.86, "text": " that again, I think in a more model-centric approach, we also kind of focus on trying to", "tokens": [51384, 300, 797, 11, 286, 519, 294, 257, 544, 2316, 12, 45300, 3109, 11, 321, 611, 733, 295, 1879, 322, 1382, 281, 51660], "temperature": 0.0, "avg_logprob": -0.11238460326462649, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.00446384446695447}, {"id": 576, "seek": 332186, "start": 3322.6600000000003, "end": 3328.5, "text": " create these messages that are more like humans. And that could be effective in certain cases,", "tokens": [50404, 1884, 613, 7897, 300, 366, 544, 411, 6255, 13, 400, 300, 727, 312, 4942, 294, 1629, 3331, 11, 50696], "temperature": 0.0, "avg_logprob": -0.09237054846752649, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007005662657320499}, {"id": 577, "seek": 332186, "start": 3328.5, "end": 3334.26, "text": " but as you said, that might not really be what the users want, because in a proxy kind of setting,", "tokens": [50696, 457, 382, 291, 848, 11, 300, 1062, 406, 534, 312, 437, 264, 5022, 528, 11, 570, 294, 257, 29690, 733, 295, 3287, 11, 50984], "temperature": 0.0, "avg_logprob": -0.09237054846752649, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007005662657320499}, {"id": 578, "seek": 332186, "start": 3334.26, "end": 3340.34, "text": " you might not actually want it to sound too personalized, because maybe the more canned", "tokens": [50984, 291, 1062, 406, 767, 528, 309, 281, 1626, 886, 28415, 11, 570, 1310, 264, 544, 36462, 51288], "temperature": 0.0, "avg_logprob": -0.09237054846752649, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007005662657320499}, {"id": 579, "seek": 332186, "start": 3340.34, "end": 3348.98, "text": " message might actually work better in that context. And vice versa. So I think just being able to", "tokens": [51288, 3636, 1062, 767, 589, 1101, 294, 300, 4319, 13, 400, 11964, 25650, 13, 407, 286, 519, 445, 885, 1075, 281, 51720], "temperature": 0.0, "avg_logprob": -0.09237054846752649, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.007005662657320499}, {"id": 580, "seek": 334898, "start": 3348.98, "end": 3354.42, "text": " identify all these different needs that people have and expectations that people have and being", "tokens": [50364, 5876, 439, 613, 819, 2203, 300, 561, 362, 293, 9843, 300, 561, 362, 293, 885, 50636], "temperature": 0.0, "avg_logprob": -0.07743724683920543, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.01031867228448391}, {"id": 581, "seek": 334898, "start": 3354.42, "end": 3360.82, "text": " able to somewhat fluidly support those, I think was really an interesting kind of observation", "tokens": [50636, 1075, 281, 8344, 9113, 356, 1406, 729, 11, 286, 519, 390, 534, 364, 1880, 733, 295, 14816, 50956], "temperature": 0.0, "avg_logprob": -0.07743724683920543, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.01031867228448391}, {"id": 582, "seek": 334898, "start": 3360.82, "end": 3366.58, "text": " that we had. And I think moving forward, one of the lessons was that this more personalizable", "tokens": [50956, 300, 321, 632, 13, 400, 286, 519, 2684, 2128, 11, 472, 295, 264, 8820, 390, 300, 341, 544, 2973, 22395, 51244], "temperature": 0.0, "avg_logprob": -0.07743724683920543, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.01031867228448391}, {"id": 583, "seek": 334898, "start": 3367.86, "end": 3372.1, "text": " message generation could be an interesting technology that could be potentially integrated,", "tokens": [51308, 3636, 5125, 727, 312, 364, 1880, 2899, 300, 727, 312, 7263, 10919, 11, 51520], "temperature": 0.0, "avg_logprob": -0.07743724683920543, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.01031867228448391}, {"id": 584, "seek": 334898, "start": 3372.1, "end": 3377.14, "text": " but that's not going to solve everything, because there are these other types of needs that will", "tokens": [51520, 457, 300, 311, 406, 516, 281, 5039, 1203, 11, 570, 456, 366, 613, 661, 3467, 295, 2203, 300, 486, 51772], "temperature": 0.0, "avg_logprob": -0.07743724683920543, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.01031867228448391}, {"id": 585, "seek": 337714, "start": 3377.14, "end": 3383.7799999999997, "text": " not be supported, even with the perfect personalizable style transfer. So yeah.", "tokens": [50364, 406, 312, 8104, 11, 754, 365, 264, 2176, 2973, 22395, 3758, 5003, 13, 407, 1338, 13, 50696], "temperature": 0.0, "avg_logprob": -0.12714040732081933, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00023267518554348499}, {"id": 586, "seek": 337714, "start": 3386.02, "end": 3391.7799999999997, "text": " Explaining stuff, I kept thinking about how what you described and sort of the challenges", "tokens": [50808, 12514, 3686, 1507, 11, 286, 4305, 1953, 466, 577, 437, 291, 7619, 293, 1333, 295, 264, 4759, 51096], "temperature": 0.0, "avg_logprob": -0.12714040732081933, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00023267518554348499}, {"id": 587, "seek": 337714, "start": 3391.7799999999997, "end": 3398.18, "text": " that we see with this new deep networks and models and how we interact with them are", "tokens": [51096, 300, 321, 536, 365, 341, 777, 2452, 9590, 293, 5245, 293, 577, 321, 4648, 365, 552, 366, 51416], "temperature": 0.0, "avg_logprob": -0.12714040732081933, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00023267518554348499}, {"id": 588, "seek": 337714, "start": 3398.8199999999997, "end": 3404.2599999999998, "text": " similar to how people used to interact with search engines, right? At the beginning, people were", "tokens": [51448, 2531, 281, 577, 561, 1143, 281, 4648, 365, 3164, 12982, 11, 558, 30, 1711, 264, 2863, 11, 561, 645, 51720], "temperature": 0.0, "avg_logprob": -0.12714040732081933, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00023267518554348499}, {"id": 589, "seek": 340426, "start": 3404.34, "end": 3411.1400000000003, "text": " not as good as sort of figuring out how to query the search engine right. And over time, both", "tokens": [50368, 406, 382, 665, 382, 1333, 295, 15213, 484, 577, 281, 14581, 264, 3164, 2848, 558, 13, 400, 670, 565, 11, 1293, 50708], "temperature": 0.0, "avg_logprob": -0.11964973362012842, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008253210689872503}, {"id": 590, "seek": 340426, "start": 3412.42, "end": 3417.2200000000003, "text": " we became better at querying the search engines, and then the search engines became better at", "tokens": [50772, 321, 3062, 1101, 412, 7083, 1840, 264, 3164, 12982, 11, 293, 550, 264, 3164, 12982, 3062, 1101, 412, 51012], "temperature": 0.0, "avg_logprob": -0.11964973362012842, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008253210689872503}, {"id": 591, "seek": 340426, "start": 3417.2200000000003, "end": 3423.5400000000004, "text": " sort of understanding how to interpret user queries. Do you see any similarities there? Is", "tokens": [51012, 1333, 295, 3701, 577, 281, 7302, 4195, 24109, 13, 1144, 291, 536, 604, 24197, 456, 30, 1119, 51328], "temperature": 0.0, "avg_logprob": -0.11964973362012842, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008253210689872503}, {"id": 592, "seek": 340426, "start": 3423.5400000000004, "end": 3430.9, "text": " there something that's very unique to the challenges we face with this new models? Or is it just that", "tokens": [51328, 456, 746, 300, 311, 588, 3845, 281, 264, 4759, 321, 1851, 365, 341, 777, 5245, 30, 1610, 307, 309, 445, 300, 51696], "temperature": 0.0, "avg_logprob": -0.11964973362012842, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0008253210689872503}, {"id": 593, "seek": 343090, "start": 3430.98, "end": 3437.06, "text": " we haven't had enough time to sort of adopt to each other in a way? Yeah.", "tokens": [50368, 321, 2378, 380, 632, 1547, 565, 281, 1333, 295, 6878, 281, 1184, 661, 294, 257, 636, 30, 865, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1570020328868519, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0035877435002475977}, {"id": 594, "seek": 343090, "start": 3437.06, "end": 3444.1, "text": " Excellent. Yeah. And I think it's a recurring theme as these new technologies come in. Initially,", "tokens": [50672, 16723, 13, 865, 13, 400, 286, 519, 309, 311, 257, 32279, 6314, 382, 613, 777, 7943, 808, 294, 13, 29446, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1570020328868519, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0035877435002475977}, {"id": 595, "seek": 343090, "start": 3444.1, "end": 3449.06, "text": " people would kind of struggle and they would need to learn how it actually works through trial and", "tokens": [51024, 561, 576, 733, 295, 7799, 293, 436, 576, 643, 281, 1466, 577, 309, 767, 1985, 807, 7308, 293, 51272], "temperature": 0.0, "avg_logprob": -0.1570020328868519, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0035877435002475977}, {"id": 596, "seek": 343090, "start": 3449.06, "end": 3454.42, "text": " error and lots of like failed attempts. And that's what we're seeing with these like", "tokens": [51272, 6713, 293, 3195, 295, 411, 7612, 15257, 13, 400, 300, 311, 437, 321, 434, 2577, 365, 613, 411, 51540], "temperature": 0.0, "avg_logprob": -0.1570020328868519, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0035877435002475977}, {"id": 597, "seek": 343090, "start": 3454.42, "end": 3458.98, "text": " chat GPT, for instance, a lot of people are trying things out, reporting success and failure cases.", "tokens": [51540, 5081, 26039, 51, 11, 337, 5197, 11, 257, 688, 295, 561, 366, 1382, 721, 484, 11, 10031, 2245, 293, 7763, 3331, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1570020328868519, "compression_ratio": 1.6134751773049645, "no_speech_prob": 0.0035877435002475977}, {"id": 598, "seek": 345898, "start": 3459.94, "end": 3466.66, "text": " So I do think there are certain similarities. What's more unique about what we're seeing right", "tokens": [50412, 407, 286, 360, 519, 456, 366, 1629, 24197, 13, 708, 311, 544, 3845, 466, 437, 321, 434, 2577, 558, 50748], "temperature": 0.0, "avg_logprob": -0.10377444823582967, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0005523907020688057}, {"id": 599, "seek": 345898, "start": 3466.66, "end": 3473.94, "text": " now is that due to the nature of like how black box, complex, unpredictable these models are,", "tokens": [50748, 586, 307, 300, 3462, 281, 264, 3687, 295, 411, 577, 2211, 2424, 11, 3997, 11, 31160, 613, 5245, 366, 11, 51112], "temperature": 0.0, "avg_logprob": -0.10377444823582967, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0005523907020688057}, {"id": 600, "seek": 345898, "start": 3473.94, "end": 3480.98, "text": " I think it just confuses people much more. And there's a question of, you know, is this really", "tokens": [51112, 286, 519, 309, 445, 1497, 8355, 561, 709, 544, 13, 400, 456, 311, 257, 1168, 295, 11, 291, 458, 11, 307, 341, 534, 51464], "temperature": 0.0, "avg_logprob": -0.10377444823582967, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0005523907020688057}, {"id": 601, "seek": 345898, "start": 3480.98, "end": 3488.02, "text": " like a human learning problem to begin with, right? So if people take, do it more, and you know,", "tokens": [51464, 411, 257, 1952, 2539, 1154, 281, 1841, 365, 11, 558, 30, 407, 498, 561, 747, 11, 360, 309, 544, 11, 293, 291, 458, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10377444823582967, "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0005523907020688057}, {"id": 602, "seek": 348802, "start": 3488.02, "end": 3493.22, "text": " if they had more time, will people be actually able to really get to a point where they could", "tokens": [50364, 498, 436, 632, 544, 565, 11, 486, 561, 312, 767, 1075, 281, 534, 483, 281, 257, 935, 689, 436, 727, 50624], "temperature": 0.0, "avg_logprob": -0.08115124284175404, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.000676825235132128}, {"id": 603, "seek": 348802, "start": 3493.22, "end": 3498.02, "text": " really easily create something that they like? Probably not. Right? So that's why I think we", "tokens": [50624, 534, 3612, 1884, 746, 300, 436, 411, 30, 9210, 406, 13, 1779, 30, 407, 300, 311, 983, 286, 519, 321, 50864], "temperature": 0.0, "avg_logprob": -0.08115124284175404, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.000676825235132128}, {"id": 604, "seek": 348802, "start": 3498.02, "end": 3504.18, "text": " need both on the model side to kind of think about what are more interactable and learnable ways of,", "tokens": [50864, 643, 1293, 322, 264, 2316, 1252, 281, 733, 295, 519, 466, 437, 366, 544, 4648, 712, 293, 1466, 712, 2098, 295, 11, 51172], "temperature": 0.0, "avg_logprob": -0.08115124284175404, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.000676825235132128}, {"id": 605, "seek": 348802, "start": 3504.18, "end": 3509.14, "text": " you know, architecting this kind of models in the first place. And also from the HCI point of view,", "tokens": [51172, 291, 458, 11, 6331, 278, 341, 733, 295, 5245, 294, 264, 700, 1081, 13, 400, 611, 490, 264, 389, 25240, 935, 295, 1910, 11, 51420], "temperature": 0.0, "avg_logprob": -0.08115124284175404, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.000676825235132128}, {"id": 606, "seek": 348802, "start": 3509.14, "end": 3515.06, "text": " what are these interaction mechanisms that could be added to these models in a way that", "tokens": [51420, 437, 366, 613, 9285, 15902, 300, 727, 312, 3869, 281, 613, 5245, 294, 257, 636, 300, 51716], "temperature": 0.0, "avg_logprob": -0.08115124284175404, "compression_ratio": 1.7210144927536233, "no_speech_prob": 0.000676825235132128}, {"id": 607, "seek": 351506, "start": 3515.06, "end": 3520.18, "text": " it is actually more understandable and usable on the user side? Yeah. Thanks so much. Yeah.", "tokens": [50364, 309, 307, 767, 544, 25648, 293, 29975, 322, 264, 4195, 1252, 30, 865, 13, 2561, 370, 709, 13, 865, 13, 50620], "temperature": 0.0, "avg_logprob": -0.297206312417984, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.0037878111470490694}, {"id": 608, "seek": 351506, "start": 3521.14, "end": 3524.34, "text": " I think we're at about the time, but Duho will be here for a couple minutes after the talk for", "tokens": [50668, 286, 519, 321, 434, 412, 466, 264, 565, 11, 457, 5153, 1289, 486, 312, 510, 337, 257, 1916, 2077, 934, 264, 751, 337, 50828], "temperature": 0.0, "avg_logprob": -0.297206312417984, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.0037878111470490694}, {"id": 609, "seek": 351506, "start": 3524.34, "end": 3528.18, "text": " further questions. So let's thank him for speaking. Thank you.", "tokens": [50828, 3052, 1651, 13, 407, 718, 311, 1309, 796, 337, 4124, 13, 1044, 291, 13, 51020], "temperature": 0.0, "avg_logprob": -0.297206312417984, "compression_ratio": 1.4310344827586208, "no_speech_prob": 0.0037878111470490694}], "language": "en"}