WEBVTT

00:00.000 --> 00:16.440
Just such a treat to be back, I spend many hours on that side of the room, so it's wild

00:16.440 --> 00:19.840
to be on this side of the room and going, whoa, there was actually like monitors up here,

00:19.840 --> 00:25.440
like that's how the speakers kept track of where in their talk they were, so that's good

00:25.440 --> 00:27.280
to know.

00:27.600 --> 00:31.440
This is sort of the first set of talks I've given since the pandemic, and so I thought

00:31.440 --> 00:36.720
it was a really great opportunity to talk about some new ideas that have been on my

00:36.720 --> 00:42.000
mind, and particularly with all of you as my captive audience, I thought that I would

00:42.000 --> 00:48.840
use this talk as an opportunity to think out loud about what the role of HCI should be

00:48.840 --> 00:55.760
in the face of all of this really incredible rapid progress that AI and ML have made, particularly

00:55.760 --> 01:01.920
kind of scoped in the last six months or so.

01:01.920 --> 01:08.480
As I was trying to think about what the role of HCI should be, I was reminded of this figure

01:08.480 --> 01:15.400
from Jonathan Gruden's 2009 article in triple AI about how AI and HCI are two fields that

01:15.400 --> 01:17.880
are divided by a common focus.

01:17.880 --> 01:23.080
As you can see in moments where AI makes a lot of progress, it's almost like the pendulum

01:23.120 --> 01:31.000
swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches

01:31.000 --> 01:37.960
of human intelligence augmentation or amplification, but also I think HCI is in a more established

01:37.960 --> 01:47.200
stronger position than it's ever been in the past, and so I really think it's our responsibility

01:47.280 --> 01:53.120
to think about what that counterbalance to ever-increased automation should be.

01:53.120 --> 01:58.240
So I often, in moments like this, like to sort of turn to history and ground myself,

01:58.240 --> 02:03.520
and so if we cast back to the first AI winter with Sutherland's sketchpad, right around

02:03.520 --> 02:09.320
that time there was this foundational paper written by Lick Leiter at MIT titled Man

02:09.320 --> 02:14.320
Computers in Biosas, and I think the gendering is unfortunate and unfortunately reflective

02:14.400 --> 02:20.400
of the times, but nevertheless in this paper, Lick Leiter put forth this really compelling

02:20.400 --> 02:27.900
vision about the ways in which a computer could interact with us through this intuitive,

02:27.900 --> 02:34.660
guided trial and error procedure, turning up solutions and revealing unexpected turns

02:34.660 --> 02:39.240
in the reasoning, and I was really tempted to put this sort of side-by-side with this

02:39.320 --> 02:45.560
very recent demo that OpenAI released with ChadGPT plus plugins where you can upload

02:45.560 --> 02:50.560
this music.csv data set and then start to have this very natural language interaction

02:50.560 --> 02:54.960
to ask what are the columns in the data set, how many rows in there in the data set, and

02:54.960 --> 03:00.240
then even say, can you give me some basic visualizations of this data set, and it thinks

03:00.240 --> 03:06.400
a little bit, it's working real hard, and there you go, it produces sort of three visualizations

03:06.440 --> 03:12.240
and even starts to give you maybe something that looks like an explanation, and I wonder,

03:12.240 --> 03:16.840
is it time to roll out our mission accomplished banners? Like have we achieved Lick Leiter's

03:16.840 --> 03:22.160
vision to think in interaction with a computer in the same way that we think with a colleague

03:22.160 --> 03:27.760
whose competence supplements our own? Now, I don't think it's time to roll out the mission

03:27.760 --> 03:31.840
accomplished banners, but I'm hopeful that the reason it's not that is not just my sort

03:31.880 --> 03:37.960
of hope that we haven't been put out of jobs, but rather that there is something more to do.

03:37.960 --> 03:44.960
So two years after Lick Leiter's man-computer symbiosis, Douglas Engelbart wrote up this

03:44.960 --> 03:50.240
really incredible framework called augmenting human intellect, and right in the introduction

03:50.240 --> 03:56.440
of this piece, we already start to see how Engelbart is defining a much more expansive

03:56.480 --> 04:02.680
role of human augmentation. So the idea is not just about problem solving, which he does

04:02.680 --> 04:07.640
mention right at the end there, to derive solutions to a problem, but it's also about

04:07.640 --> 04:14.640
using computers to help us think. It's to increase our capacity to approach a complex

04:15.680 --> 04:21.040
problem situation, to gain comprehension really about this thinking and not just the problem

04:21.040 --> 04:26.280
solving pieces, and really what I like is how he thinks we'll get there. Certainly there

04:26.280 --> 04:30.920
will be sophisticated methods, high powered electronic aids, but to me the part that really

04:30.920 --> 04:36.800
resonates in his prescription here is streamlined terminology and notation, and that's going

04:36.800 --> 04:43.440
to be a theme of my talk here, certainly one of the themes that underlies my group's work.

04:43.440 --> 04:49.000
And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work

04:49.000 --> 04:52.880
with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the

04:52.920 --> 04:58.440
top right-hand corner, on this system called B2. So this is a Jupyter notebook, it's a

04:58.440 --> 05:03.720
very commonly used data science environment where people can start to write code in the

05:03.720 --> 05:09.840
style of a Python REPL, but what B2 does is saying, well, in addition to that sort of

05:09.840 --> 05:15.720
linear style of data science analysis and programming, there's a lot of value in a more

05:15.720 --> 05:21.000
visual analysis dashboard style interface like Tableau. And so what B2 tries to do is

05:21.040 --> 05:26.160
bring these two pieces together. So you can see once I've invoked B2 it adds this on the

05:26.160 --> 05:31.960
sidebar, and I can start to issue regular sort of Python, you know, pandas commands

05:31.960 --> 05:36.960
like looking at the data frame, getting a, you know, a sense of how many rows there are,

05:36.960 --> 05:41.600
what the columns are, and now I can start to write some code to do a little bit of data

05:41.600 --> 05:46.120
transformation and visualization. Notice here in all of these steps, you know, when I'm

05:46.160 --> 05:50.800
authoring a visualization, I don't have to specify what that visualization should look

05:50.800 --> 05:56.800
like, right? I'm just calling these .viz methods on the data frame, and B2 behind the scenes

05:56.800 --> 06:02.000
is figuring out what sort of visualization actually makes sense based on the history

06:02.000 --> 06:07.520
of the transformations that were performed on the data frame. So in the case of year,

06:07.520 --> 06:12.400
for instance, if I've grouped by year, the most sensible visualization to produce is

06:12.440 --> 06:18.200
a histogram of the number of counts of data records across years. You might have also noticed

06:18.200 --> 06:23.000
in the video that if I click the fields on the right hand side there, that it automatically

06:23.000 --> 06:28.240
produces an equivalent visualization, but it doesn't stop there. It adds, you know, the code

06:28.240 --> 06:34.840
and tags them with these little, you know, yellow emojis to indicate that there's actually sort of

06:34.840 --> 06:41.080
a common shared representation here, right? Clicking on the sidebar not only produces the

06:41.080 --> 06:46.440
visualization, but produces the equivalent code as well. And what's interesting is that these

06:46.440 --> 06:51.920
visualizations aren't just output mechanisms, but I can start to interact with them to sort of do

06:51.920 --> 06:58.040
this cross filtering interaction. So all the other bars update to reflect the data shown in the

06:58.040 --> 07:04.640
highlighted bars, and B2 is keeping this as an interaction log that is semantically meaningful

07:04.640 --> 07:09.680
to me. So this interaction log doesn't comprise mouse clicks and keystrokes and things like that,

07:09.920 --> 07:15.040
but it's expressing data queries, right? Which states have been selected? And I can use that

07:15.040 --> 07:21.520
data query to perform subsequent sort of analyses based on my interactive results. So I can say,

07:21.520 --> 07:27.840
great, I'm gonna, you know, copy some code to the clipboard, paste it in as a data query to look

07:27.840 --> 07:33.280
at what the interactive selection should be, and then, you know, proceed with some other sort of

07:33.280 --> 07:39.440
visual analysis. And so as we're sort of, you know, looking at these two forms of interaction,

07:39.600 --> 07:44.720
I was trying to figure out, well, some things feel the same, right? I've got that kind of

07:44.720 --> 07:49.680
conversational back and forth. I'm sure on the left-hand side with ChatGPT, it's a more natural

07:49.680 --> 07:53.800
language conversation. On the right-hand side, it's more of a repel conversation. But also,

07:53.800 --> 07:59.480
things feel qualitatively different. And how do I actually kind of characterize what is the same

07:59.480 --> 08:04.000
and what is the difference? And I thought really hard about it. And I realized that actually,

08:04.480 --> 08:10.240
maybe what still matters is direct manipulation, right? And by direct manipulation, I don't mean

08:11.280 --> 08:15.440
just the sort of Ben Schneiderman version of the term, which is, you know, associated with

08:15.440 --> 08:20.960
graphical user interfaces and having a representation on screen that you can manipulate and undo

08:20.960 --> 08:26.240
redo and things like that. But what I mean here is the deeper treatment of direct manipulation

08:26.240 --> 08:31.680
that three cognitive scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about

08:31.680 --> 08:38.640
the mid-1980s. So in particular, in Hutchins et al's treatment of direct manipulation, they sort

08:38.640 --> 08:44.320
of, you know, imagine direct manipulation to be this cognitive process between a user's goals

08:44.320 --> 08:50.880
and the user interface. And, you know, they identify this gulf of execution that exists when a user

08:50.880 --> 08:56.800
has to translate their goals into commands that they execute on the user interface. And similarly,

08:56.880 --> 09:02.320
a return gulf of evaluation when a user has to figure out, well, did the UI do the thing that

09:02.320 --> 09:06.720
I was expecting it to do? Right? I'm seeing a lot of nods in the audience because, you know, if you've

09:06.720 --> 09:12.880
had experience in user interaction design, user experience, you've maybe experienced these terms

09:12.880 --> 09:18.880
gulf of evaluation and execution. But what I find interesting in this 1985 paper is that they went

09:18.880 --> 09:24.960
one level deeper. So in particular, they identified this idea of a semantic distance,

09:24.960 --> 09:32.640
which is basically how users take the fuzzy notions in their head and translate those into

09:32.640 --> 09:39.680
the nouns and verbs of the user interface, right? So going, doing that sort of sense-meaning operation

09:39.680 --> 09:46.320
of transforming your intentions into the particular actions that might exist in the user interface.

09:47.120 --> 09:51.600
And in addition to the semantic distance, they identified what I love. I love this term in

09:51.600 --> 09:56.720
articulatory distance, right? So it's not necessarily the meaning that we care about anymore,

09:56.720 --> 10:02.080
but the way in which we're conveying that meaning through the UI. And this is particularly important

10:02.080 --> 10:08.400
because you might have several user interfaces that all express the same semantics, right? You can

10:08.400 --> 10:14.720
conduct the same set of, you know, operations with them, the same nouns and verbs. But the way you

10:14.720 --> 10:19.600
do that might be different because one interface might be graphical, the other one might be textual,

10:19.600 --> 10:24.720
another one might be conversational, gesture-oriented, etc. And their claim in this paper was that

10:24.720 --> 10:30.800
that articulation, the form of that meaning is really, really important, just as important as

10:30.800 --> 10:36.160
the semantics. And of course, these distances exist on the Gulf of Evaluation as well. So the

10:36.160 --> 10:42.880
articulatory distance is how do I perceive the changes that occurred in the UI and start to bring

10:42.880 --> 10:50.640
meaning to that perceptual operation by interpreting and evaluating the degree to which

10:50.640 --> 10:56.640
they met my goals. So this is actually going to give us the kind of conceptual machinery for

10:56.640 --> 11:01.920
the rest of the talk. And it's a little bit dense. And so I want to return to sort of the prior two

11:01.920 --> 11:07.600
examples and think about how they manifest these two kinds of distances. So in the case of the

11:07.600 --> 11:12.560
chat GPT example, you know, if we start with semantic distance, I would say that, well, the

11:12.560 --> 11:17.680
semantics aren't really well defined, right? They're not really explicit. Because what these models

11:17.680 --> 11:23.840
have done is they've learned over, you know, vast corpuses of text, often just text that is present

11:23.840 --> 11:28.960
on the internet. And so what they've learned is this latent space that is very ambiguous in the

11:28.960 --> 11:34.240
semantics that are encoded in that latent space. So as a user, it's hard for me to know how to

11:34.320 --> 11:39.360
translate my intentions into something that the system can understand because I don't know what

11:39.360 --> 11:46.720
it is the system knows about the world. But as I'm sure many of us are aware, like prompt engineering

11:46.720 --> 11:52.400
is a thing, right? So if I figure out exactly how to craft my, you know, natural language

11:52.400 --> 11:57.760
expression, suddenly I can get the model to very rapidly almost zero shot adopt the semantics that

11:57.760 --> 12:03.120
I want. And that feels like a very powerful, you know, affordance that we've not necessarily had

12:03.120 --> 12:10.720
before. On the other side, you know, the semantic distance in the Jupiter notebook in B2 had explicitly

12:10.720 --> 12:15.600
defined semantics, right? We have the explicit semantics of pandas on the data frame of the

12:15.600 --> 12:20.960
visualization library of being able to click on the fields in a graphical user interface to produce

12:20.960 --> 12:27.040
visualizations. And every time I did that, I had the shared representation of the code. So either I

12:27.040 --> 12:30.480
would offer the code and it would produce a visualization or if the system produced some

12:30.480 --> 12:36.320
code, I could go in and comment and uncomment entries or tweak the code in a particular way

12:36.320 --> 12:41.440
and things like that. And so it gave me the shared representation that allowed me to bridge

12:41.440 --> 12:49.200
between input and output mechanisms really, really easily. With articulatory distance in chat

12:49.200 --> 12:56.000
GPT, right, natural language, it's been enormously powerful because it's reduced the sort of learning

12:56.080 --> 13:01.280
threshold for a lot of things, right? So if I don't know exactly what it is I want or how to

13:01.280 --> 13:07.440
sort of pose it to the question, I can lean into the ambiguity of natural language and chat GPT

13:07.440 --> 13:13.760
catches up to my intentions pretty rapidly, which is great. But conversely, sometimes I know exactly

13:13.760 --> 13:19.760
what it is I want. And it's really frustrating to have to express precise operations through the

13:19.760 --> 13:26.000
ambiguity of natural language. And then as a result, because of the fact that natural language

13:26.000 --> 13:30.320
is the only mechanism so far by which we can interact with many of these models, there's a

13:30.320 --> 13:35.760
disconnect if your output is visual, like the case of visualization. So I can't interact with

13:35.760 --> 13:41.040
the visualizations in any way to do subsequent back and forth interactions with the model. Now,

13:41.040 --> 13:45.280
I don't think the second point is sort of a fundamental limitation, but it's certainly, you

13:45.280 --> 13:51.280
know, the state of where we are today. And on the other hand, with, you know, Jupiter Notebook and

13:51.280 --> 13:56.480
B2, with the articulatory distance, we've got basically the inverse of this, right? We've got a

13:56.480 --> 14:02.160
nice precise programmatic syntax. So if I know that syntax, I can work really, really efficiently,

14:02.160 --> 14:08.560
right? Sort of a common affordance of many sort of command line style interfaces. But I really

14:08.560 --> 14:14.320
need to learn that syntax to be effective. And in some cases with pearly design syntaxes, which

14:14.400 --> 14:19.840
I might maybe argue, Pandas is an example of, right? I constantly have to look up the documentation

14:19.840 --> 14:24.800
for, right? There's a learning curve associated with it that slows people down. Yeah, Michael.

14:37.440 --> 14:42.000
Yeah, so the reason I put it, I think this is a great question, you know, what lies in semantic

14:42.000 --> 14:47.040
and articulatory. And oftentimes it is quite a fuzzy distinction. The reason I put this in

14:47.040 --> 14:53.120
articulatory is my experience with Pandas oftentimes is I know what it is I want to do, right? I know

14:53.120 --> 14:57.920
the sort of operation I want to perform on my, on my data frame. I just don't know the specific

14:57.920 --> 15:04.880
syntax that I need to look up. Exactly, exactly. But certainly, you know, if, if you don't know

15:04.880 --> 15:09.280
what it is you want to do, then the affordances of natural language absolutely help because you

15:09.280 --> 15:14.960
can kind of, you know, pose things in really fuzzy ways and, and kind of iterate towards your outcome.

15:17.600 --> 15:21.840
And, and I think you see some of this ambiguity in, in sort of, you know, the distinction between

15:21.840 --> 15:27.280
semantic and articulatory distance here with this, this last point where because there are consistent

15:27.280 --> 15:33.680
semantics that actually has this knock on effect on the articulation because now there's a shared

15:33.680 --> 15:38.720
representation of input and output and that simplifies that articulatory distance as well.

15:39.280 --> 15:44.960
So there's not quite that disconnect that we see on the chat GPT side. And so, you know, that's a,

15:44.960 --> 15:51.200
that's, you know, I found semantic and articulatory distances to be a really helpful sort of framework

15:51.200 --> 15:57.840
and I wanted to use it to sort of analyze the very last step in the output that that demo produced. So

15:57.840 --> 16:02.960
it, it, you know, it's basically this, this thing that masquerades as an explanation of the

16:02.960 --> 16:08.400
visualizations that chat GPT produced. But if you actually look at what it says, right, here's some

16:08.400 --> 16:13.520
basic visualizations. Number one, a histogram of song durations colon. This shows the distribution

16:13.520 --> 16:18.720
of song durations in seconds. All right, fair enough. Scatter plot of song hotness versus artist

16:18.720 --> 16:23.600
familiarity. This shows the relationship between song hotness and artist familiarity. Well, I would

16:23.600 --> 16:29.120
hope so. And then bar chart of the top 10 most frequent artist names. This shows the top 10 most

16:29.120 --> 16:35.280
frequent artist names in the data set, right. These are not really explanations, but they're

16:35.280 --> 16:40.720
pretty provocative or evocative in the potential that these models might have in allowing us to

16:40.720 --> 16:46.400
produce these textual descriptions of visual artifacts. And certainly, you know, a lot of,

16:46.400 --> 16:51.840
of, of people, certainly lots of big tech companies have thought about the ways in which you could use

16:52.640 --> 16:57.440
all kinds of machine learning models, not just LLMs to do the sort of rich description of visual

16:57.440 --> 17:03.120
content and particularly for this sort of these accessibility use cases, like how do you describe

17:03.200 --> 17:09.120
these kinds of artifacts to people who are blind or have low vision. And lots of people have studied

17:09.120 --> 17:13.760
the degree to which these models are effective and found maybe unsurprisingly that they're not

17:13.760 --> 17:19.120
terribly effective right now, right. So here is a quote from a participant from one of our studies

17:19.120 --> 17:23.680
who says, you know, the reader wouldn't get much insight from texts like this, which not only,

17:23.680 --> 17:28.640
you know, is problematic because it doesn't effectively convey information, but more troublingly,

17:28.640 --> 17:33.120
it actually increases the burden that readers face when they're trying to make sense

17:33.120 --> 17:38.320
of this output, right. There's a lot of sort of noise that gets added to that experience.

17:39.120 --> 17:43.840
Another participant, you know, says very, very interestingly, the problem with these textual

17:43.840 --> 17:50.560
descriptions is also that it robs me of control of consuming the data, right. A participant,

17:50.560 --> 17:55.280
another participant said, I want to have the time and space to interpret the numbers for myself

17:55.280 --> 18:01.760
before I read any kind of textual description that does the analysis for me. And so to me,

18:01.760 --> 18:07.280
these sound very similar to issues associated with a semantic and an articulatory distance,

18:07.280 --> 18:12.240
right. That first quote talking about, well, these texts aren't conveying anything interesting.

18:12.960 --> 18:17.360
The second set talking about, well, I want to have that time and space, I want to be able to

18:17.360 --> 18:24.240
control the form with which that text is conveyed to me. And so I want to dig into how we might

18:24.240 --> 18:29.040
address these two distances in the case of accessibility. But before I do that, I want to

18:29.040 --> 18:35.440
give us a sense of how people who are blind or have low vision experience, you know, the

18:36.240 --> 18:41.360
internet and graphical interfaces today. So I'm going to turn things over to my PhD student,

18:41.360 --> 18:46.720
Jonathan Zhang, who will give us a quick demo of an assistive technology called a screen reader

18:46.720 --> 18:49.120
that basically narrates on-screen content.

18:54.240 --> 19:11.120
So here I can demonstrate what the accessible HTML version of our paper looks like to a screen reader.

19:24.960 --> 19:35.840
So as you can see, what a screen reader does is it basically sort of linearizes the operation

19:35.840 --> 19:43.680
of, you know, reading, perceiving, understanding graphical content on a user interface. And in

19:43.680 --> 19:47.920
particular, you might notice that the narration was actually quite rapid, right. And this is

19:47.920 --> 19:54.960
actually a slowed down version of what, you know, proficient screen reader users use, which is often

19:54.960 --> 20:01.040
much, much faster. But what is interesting about the screen reader use case is that it forces that

20:01.040 --> 20:07.040
linearity, right. And the key challenge in figuring out the articulatory distance in the case of

20:07.040 --> 20:12.160
accessibility is how do you take visualizations that probably all of us in the audience have

20:12.160 --> 20:16.560
slightly subtly different ways of reading, right. Maybe some of you start by reading the title,

20:16.560 --> 20:21.600
then moving to the axes, then looking at, you know, the shapes, while others might start by

20:21.600 --> 20:27.280
looking at the most salient trend and then start to, you know, map out to what the axes and legends

20:27.280 --> 20:33.280
and stuff like that are. How do we take all of that rich diversity, but linearize it? So the

20:33.280 --> 20:38.800
people who use screen readers can nevertheless have that same, you know, choice in meeting a

20:38.800 --> 20:45.440
visualization, but under these conditions. And so the way we have chosen to do that is basically

20:45.440 --> 20:52.880
by restructuring the content of a visualization into a text-oriented hierarchy. So at the top,

20:52.880 --> 20:57.600
at the root of this hierarchy is just a summary of the chart, probably the trends that are shown

20:57.600 --> 21:03.760
in the chart. And then the hierarchy branches off into the individual sort of data fields

21:03.760 --> 21:08.960
or the encodings in this case, right. The x-axis, the y-axis, the legend and things like that.

21:08.960 --> 21:15.120
And then people can start to drill down in ways that maintain some correspondence with the visual

21:15.120 --> 21:21.600
artifacts. So one step below, you know, the x-axis is stepping through them by the major ticks,

21:21.600 --> 21:25.200
right. One step below the major ticks would be minor ticks and then ultimately you would get

21:25.200 --> 21:29.760
to the individual data points. So let me throw things back to Jonathan to give us a demo of how

21:29.760 --> 21:37.920
this works. A scatter plot of Penguin data. And to a screen reader, our system represents this

21:37.920 --> 21:44.480
scatter plot as a keyboard navigable data structure that contains text descriptions at

21:44.480 --> 21:51.200
varying levels of detail. So when a screen reader user first encounters this visualization on a page,

21:51.200 --> 21:56.720
they'll be able to read off a high-level alt text description of what the chart is. So

21:59.840 --> 22:04.960
and if they're interested in getting more detail about this visualization, they can dive in by

22:04.960 --> 22:11.840
pressing the down arrow key to descend one level in the hierarchy and access descriptions about

22:11.840 --> 22:15.760
the different encodings of the scatter plot. So I'm going to press the down arrow key.

22:19.760 --> 22:25.120
I can press the left and right arrow keys to flip through descriptions of the other axes and

22:25.120 --> 22:40.640
legends. Cool, so let's say I am interested in getting more information about the x-axis. I can

22:41.360 --> 22:46.800
use the left arrow key to navigate back to the x-axis description and then press down one more

22:46.800 --> 23:01.360
time to descend a level of detail into the x-axis. So on this level underneath the x-axis, I'm

23:01.360 --> 23:08.240
accessing descriptions of intervals along the x-axis and it's reading out to me how many data

23:08.240 --> 23:12.640
values are contained within each interval. So by pressing left and right, I can kind of get

23:12.640 --> 23:26.240
a sense of the distribution of data along the x-axis. So let's say I am interested in this

23:26.240 --> 23:32.800
range from 190 to 200. I can then press down arrow again to dive into the individual data points

23:32.800 --> 23:48.560
that are contained within this interval. So let's say that instead of moving up and down

23:48.560 --> 23:54.160
this hierarchical structure, I would rather just move around the x-y grid in the scatter plot,

23:54.160 --> 24:00.720
as if I were kind of feeling around a tactile graphic, for example. I can start by navigating

24:00.720 --> 24:11.200
over to the grid view of the scatter plot. And once I descend into this part of the hierarchy,

24:11.200 --> 24:17.760
I can use the WASD keys to move up and down different squares along the grid.

24:17.920 --> 24:31.600
And so similarly to before, it's starting off by giving me the number of data values that are

24:31.600 --> 24:36.320
contained in that square so that I can get a sense of the distribution of the data.

24:37.680 --> 24:45.920
And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges.

24:45.920 --> 24:52.720
And this was the first time he felt like he actually understood and could build a mental

24:52.720 --> 24:59.120
model of what it was that a scatter plot was representing. We saw these sorts of comments

24:59.120 --> 25:05.360
reflected in user studies that we ran about how the form of this textual output really

25:05.360 --> 25:10.720
influenced participants' mental model of what the data was, what the trends were, and things like that.

25:10.720 --> 25:15.520
So one participant, for instance, said, I now know how to drill down and up between different

25:15.600 --> 25:21.600
layers in the data to get an overall picture. And it gives me a different way of thinking.

25:21.600 --> 25:26.400
And another one said, I'm thinking more in spatial terms because this is just a new method

25:26.400 --> 25:32.320
for navigating and moving through the grid and drilling down to information and things like that.

25:32.320 --> 25:39.120
And so what I find interesting here is that at every step, the semantic content stayed exactly

25:39.120 --> 25:44.160
the same. And there wasn't even very rich semantic content. It was a range and then a count of the

25:44.160 --> 25:50.640
data values. All we manipulated was that articulation, that form, giving it a hierarchical

25:50.640 --> 25:56.800
nature, adding all of these different navigational affordances, and just manipulating the articulation

25:56.800 --> 26:02.640
had this huge impact on people's mental models of the data. And I think that we're really just at

26:02.640 --> 26:08.080
the tip of the iceberg of these more accessible structures. Currently, in my group, we're thinking

26:08.080 --> 26:14.240
about just the impact that token order has on how people using screen readers build up those

26:14.240 --> 26:19.680
mental models. If you're constantly prompting them with the range first rather than the actual data

26:19.680 --> 26:25.520
values, does that introduce friction to their capacity to build that mental model and things

26:25.520 --> 26:32.880
like that? But in all of this, where is semantic distance? How do we actually start to make that

26:32.880 --> 26:38.000
textual descriptions more interesting and meaningful? And this is where I think LLMs can

26:38.000 --> 26:44.240
really help us. For one reason, it's because there's just a sheer amount of textual content we need

26:44.240 --> 26:50.400
to be able to produce that is infeasible to expect a human to sort of manually author.

26:50.400 --> 26:54.880
But there are other sort of implications that we'll touch upon really shortly.

26:54.880 --> 26:59.360
But before we can get LLMs to actually sort of produce the content we want,

26:59.360 --> 27:05.840
what we need to do is shift from that very latent space with implicit semantics to a set of explicit

27:05.840 --> 27:12.720
semantics. We need to impose a conceptual model onto our LLMs. Or another way of saying that is

27:12.720 --> 27:17.760
we need to get the LLMs to understand what a good textual description of a visualization is.

27:18.560 --> 27:24.480
And so that's what my then PhD student, Alan Lungard, set out to do. We ran a crowdsource study

27:24.480 --> 27:31.520
where we got sort of 2,000 descriptions of charts. And through qualitative coding,

27:31.520 --> 27:36.000
we realized there are basically four kinds of semantic content that textual descriptions should

27:36.000 --> 27:42.080
convey. The first most primitive layer is basically just the sort of construction details of the chart.

27:42.080 --> 27:48.480
What are the titles, the labels, the scales, the units, etc. And accessibility best practices say

27:48.480 --> 27:53.680
that this is some of the most important content to convey because it gives people sort of important

27:53.760 --> 27:59.200
milestones and landmarks. One level above that are the sort of statistical properties like minimum,

27:59.200 --> 28:04.880
maximum, outliers, and things like that. And then one level above that is probably what is cited

28:04.880 --> 28:10.080
people we consider the real value of visualization to be. The perceptual and cognitive characteristics

28:10.080 --> 28:15.680
like complex trends and patterns, things that automated statistical methods we typically think

28:15.680 --> 28:21.520
of as not being sufficient at. And then finally, the fourth and highest level are what journalists

28:21.520 --> 28:26.240
often consider to be the real value of visualization, which is the narration that gets associated

28:26.240 --> 28:30.800
with it. What is the data story that you're able to tell through the visualization? Can you explain

28:30.800 --> 28:36.640
what you're seeing, the causal mechanisms, etc., etc. Now, another reason I think LLMs are really

28:36.640 --> 28:42.880
suited for this sort of semantic bridging task is because when we asked sighted and blind people

28:42.880 --> 28:48.560
what their preferences were, when it came to these four layers, four levels, we saw really

28:48.560 --> 28:55.600
distinct preferences. In the case of sighted people, because we've got our own visual perception

28:56.640 --> 29:03.120
doing that sort of bridging of the Gulf of Evaluation, sighted people tended to want higher

29:03.120 --> 29:08.240
and higher levels of content being conveyed through text. Blind readers, on the other hand,

29:08.240 --> 29:14.000
were pretty significantly divergent. For many of them, they didn't want those level three and four,

29:14.000 --> 29:18.800
particularly the level four captions at all, because they wanted that time and space to do

29:18.800 --> 29:28.080
the interpretation for themselves. And so here, this visualization to me conveys that LLMs can

29:28.080 --> 29:33.200
help us or machine learning models can help us think about sort of personalizing the semantics

29:33.200 --> 29:39.120
of a user interface in a way that maybe we haven't had the opportunity to study so far. There's been

29:39.120 --> 29:44.400
a lot of work in personalization, but it's often been at that level of the articulation,

29:44.400 --> 29:49.520
changing the sizes of buttons and adapting color palettes and things like that. And there's maybe

29:49.520 --> 29:57.760
an opportunity now to use LLMs to actually change what the nouns, the verbs, the concepts of a user

29:57.760 --> 30:03.120
interface are much more fundamentally. And so the way we're going about doing this in the case of

30:03.120 --> 30:08.960
textual descriptions is we're going to be releasing very soon a data set of about, actually now we're

30:08.960 --> 30:13.760
over 12,000 pairs of chart captions. And we've generated some of these captions and we've crowdsourced

30:13.760 --> 30:19.200
some of these captions. And we started to train baseline models to do this task. And one of the

30:19.200 --> 30:25.920
interesting features here is how do we represent the semantics of a chart to a large language model,

30:25.920 --> 30:30.640
right? One way could just be let's treat the chart as an image, right? This is just a set of pixels.

30:31.520 --> 30:36.960
And unsurprisingly, the baseline models don't do very well at that because a chart is a much richer

30:36.960 --> 30:41.920
kind of artifact than just an image, right? It's got all this rich structure. So then we said,

30:41.920 --> 30:46.160
great, let's look at a data table or let's look at a scene graph, which is just a fancy way of

30:46.160 --> 30:51.200
saying the SVG associated with the chart. And a priori, we would have thought, well, the scene

30:51.200 --> 30:56.400
graph is maybe like a good in-between between the computational affordances of data table and

30:56.400 --> 31:02.480
capturing some of those perceptual characteristics. Turns out for the LLMs we trained that were

31:02.480 --> 31:07.360
all transformer models, they did equivalently well on those two representations. And so one of

31:07.360 --> 31:12.800
the things my group is working on right now is a new way of representing visualizations that more

31:12.800 --> 31:19.120
directly encode some of those perceptual operations that are otherwise currently implicit in a scene

31:19.120 --> 31:26.400
graph that grammar of graphics libraries like VegaLite or GGplot perform. But what's interesting

31:26.400 --> 31:31.680
in all of this to me is that through these generative models, the goal has been how do we impose

31:31.760 --> 31:38.160
a conceptual model onto them, right? How do we bring some explicit semantics? And I think we're

31:38.160 --> 31:43.680
just scratching the surface here as well because I think the chart example case is a really great

31:43.680 --> 31:48.240
one where a lot of these representations of charts that we've got right now, the grammar of graphics,

31:48.240 --> 31:52.960
for instance, were designed for people to author, right? So we're really good at figuring out how

31:52.960 --> 31:58.080
to design programming languages, domain specific languages, to emphasize the cognitive characteristics

31:58.080 --> 32:02.560
that are important for human authors. Things like, you know, the cognitive dimensions of notation

32:02.560 --> 32:06.880
that cares about, you know, how viscous is the programming language? How many premature commitments

32:06.880 --> 32:11.840
is the programming language enforced? But I don't know what it means to design a representation

32:11.840 --> 32:18.240
to be suitable for an LLM to operate over, right? Do we restructure the programming language more

32:18.240 --> 32:24.720
fundamentally to make it tractable for an LLM? Maybe. So in addition to generative models,

32:24.720 --> 32:30.560
my group has also been working with predictive models. And here I think the bridging task is

32:30.560 --> 32:37.040
really not about imposing a conceptual model, but bridging it or aligning it to the ones that we

32:37.040 --> 32:42.640
already have. And often the way that a lot of this work happens is through the lens of model

32:42.640 --> 32:50.480
interpretability. So here is a very popular set of techniques called saliency maps. The idea behind

32:50.480 --> 32:58.000
saliency maps is they're trying to depict the most important input features for a particular

32:58.000 --> 33:03.280
outcome. So in this case, you know, this is an image, the label should be toy terrier, and here's

33:03.280 --> 33:08.560
what a variety of different kinds of saliency methods believe to be, you know, the most important

33:08.560 --> 33:17.840
pixels to produce that outcome. Now, I look at these visualizations and I go, well, you know,

33:17.840 --> 33:24.400
is it telling me something? Maybe, right? And maybe the reason I believe it's telling me something

33:24.400 --> 33:30.720
is because I'm the one doing the perception and interpretive tasks, right? Like if I look at some

33:30.720 --> 33:35.120
of those visualizations on the bottom, I go, oh, like, it looks like the dog snout is really

33:35.120 --> 33:40.560
important to the classification of a toy terrier or the spots. But it's not actually the saliency

33:40.560 --> 33:45.600
method that is doing that interpretation for me. I'm the one bringing meaning to those lit pixels,

33:46.160 --> 33:52.320
right? And so as a result, if we think about that gulf of evaluation, it's not the saliency

33:52.320 --> 33:58.080
method that's helping bridge that gulf in any way, which is why saliency maps for now have been

33:58.080 --> 34:03.840
these tools that we just use in a very ad hoc way that require a lot of manual effort to make sense

34:03.840 --> 34:10.800
of. And so a question that my student, Angie Boggast, has been focused on is how do we scaffold

34:10.800 --> 34:15.920
that semantic sense making operation, right? Providing some additional structure to help

34:16.560 --> 34:21.680
sort of scale it up to make it more reproducible and things like that. And what she's developed

34:21.680 --> 34:27.440
is these set of metrics that are very analogous to ideas of precision and recall, but are operating

34:27.440 --> 34:32.960
at the level of input features and interpretability. So in many data sets, right, you've got some set

34:32.960 --> 34:38.080
of ground truth human annotated features. And what shared interest is looking at is what is the

34:38.080 --> 34:43.200
overlap between what a saliency method considers as being important to the classification and what

34:43.200 --> 34:48.240
the humans, the human annotators thought was important. And there's actually three different

34:48.240 --> 34:54.240
ways that these overlaps can manifest. The first is a sort of ground truth coverage, which is very

34:54.240 --> 34:59.360
analogous to ideas of recall, right? It's how much of the ground truth does the model incorporate

34:59.360 --> 35:04.000
in its prediction or what is the proportion of the ground truth region that is covered by the

35:04.000 --> 35:09.200
saliency region. And if we look at some examples of low coverage on the top and high coverage at the

35:09.200 --> 35:14.640
bottom, we can see then the case of low ground truth coverage is actually very little overlap,

35:14.640 --> 35:20.400
right, between the ground truth, the yellow region, and the salient region in orange. But I often find

35:20.400 --> 35:25.600
that it's actually the high coverage regions that are more interesting to analyze. So if we compare,

35:25.600 --> 35:31.200
you know, cases where the model was correct on the right with the green label and cases where the

35:31.200 --> 35:36.640
model was incorrect with the red label, we can see in the case of correct high ground truth coverage,

35:36.640 --> 35:41.040
there are instances where the model relies not just on the object, like in this case with the

35:41.040 --> 35:47.840
cab, but a lot of contextual information as well to ultimately make that correct prediction. But

35:47.840 --> 35:53.920
on the flip side, right, with the laptop, the model is doing the same thing, but here the context

35:53.920 --> 35:58.000
is actually throwing it off, right? It's actually confusing the model because it's accounting for

35:58.000 --> 36:03.680
too much of that context in its decision making. Another kind of coverage is something we call

36:03.680 --> 36:08.480
saliency coverage, and this is more akin to precision, right, which is how strictly is the

36:08.480 --> 36:14.800
model relying only on ground truth features to make its sort of prediction. And again, you know,

36:14.800 --> 36:20.400
if we look at low and high coverage in the case of low coverage, we can see again pretty disjoint

36:21.120 --> 36:27.360
sorts of sets. But in the case of the high coverage regions, we can see that, you know,

36:27.360 --> 36:33.920
in the case of high saliency coverage, it basically means that the salient regions are a strict subset

36:33.920 --> 36:38.560
of the ground truth coverage. But the difference between a correct and incorrect prediction is

36:38.560 --> 36:45.360
whether that subset was sufficient to make the correct classification or not, right? So in the

36:45.360 --> 36:50.240
case of the Maltese dog, it did indeed only need to look at the head to make that correct prediction.

36:50.560 --> 36:56.960
But in the case of the Dalmatian, it probably should have accounted for more of that dog's head or

36:56.960 --> 37:01.280
some of the other characteristics associated with the dog. By focusing only on the snout,

37:01.280 --> 37:07.440
it ended up sort of arriving at the incorrect sort of classification. And finally, the last metric

37:07.440 --> 37:11.920
is something that is very familiar IOU, the intersection over the union. This is sort of

37:11.920 --> 37:16.880
the strictest shared interest metric. It's really measuring how aligned the model's behavior is

37:16.880 --> 37:21.760
with human reasoning. So if you look at some examples, again, you know, low coverage at the

37:21.760 --> 37:29.520
top, we can see, you know, in incorrect cases, totally distinct disjoint sets again. But in a

37:29.520 --> 37:34.720
correct instance, I actually find that pretty interesting, right? Low IOU coverage, but it

37:34.720 --> 37:39.920
got a correct classification. Now, one could say maybe it got lucky. But potentially, what

37:39.920 --> 37:46.320
the signal there is, is that maybe all the model needs is a tiny bit of a wheel associated with

37:46.320 --> 37:52.000
a horse, right, to make the prediction that is actually a horse cart and not just a horse, right?

37:52.000 --> 37:58.080
And on the flip side, with high coverage, you know, Newfoundland, great, you know, total,

37:58.080 --> 38:02.960
total alignment. But in this case, right, incorrect classification, even though there was high

38:02.960 --> 38:08.560
coverage, this might suggest, you know, genuinely difficult to classify images, right, even for

38:08.560 --> 38:13.600
people. Because if I look at that, a pickup truck seems a totally reasonable guess to have made

38:13.600 --> 38:17.680
about the image. I don't know that I've got enough sort of visual information there to call

38:17.680 --> 38:23.920
that a snowplow. So shared interest basically gives us a mechanism to start to scaffold and

38:23.920 --> 38:29.760
structure, bridge that semantic distance, right? People no longer necessarily need to manually

38:29.760 --> 38:35.840
start to analyze these things. And in fact, you know, we analyzed lots of different models across,

38:35.840 --> 38:40.800
you know, both vision and natural language and found that different combinations of these shared

38:40.800 --> 38:45.040
interest metrics, along with figuring out whether the prediction was correct or not,

38:45.040 --> 38:50.800
actually surfaced eight kinds of repeating patterns in model behavior. So we can see human

38:50.800 --> 38:56.240
aligned and some of these others we also looked at earlier, right, context confusion,

38:56.240 --> 39:01.760
context dependent and so forth. And all of these give us sort of semantics that we can start to

39:01.760 --> 39:07.040
play around with through different articulations. So one articulation of these semantics might be

39:07.040 --> 39:12.480
a very traditional visual analytics interface, right, where I've got all the different kinds of

39:13.200 --> 39:17.920
images that I care about. This is a system we built to help a board certified dermatologist

39:17.920 --> 39:22.560
make sense of this melanoma detection model. And you've got, you know, query widgets on the top

39:22.560 --> 39:28.720
to sort and filter. You can use, you know, these histograms of the shared interest metrics to really

39:28.720 --> 39:33.520
drill into the data. But what was maybe most interesting was what the dermatologist said

39:33.520 --> 39:41.680
when they started to analyze that recurring pattern of context dependent cases. So in particular,

39:41.680 --> 39:47.760
when they switched to these context dependent cases, the dermatologist started to wonder if the

39:47.760 --> 39:52.640
model is seeing something we are not truly appreciating in the clinical image. Maybe there

39:52.640 --> 39:59.200
are subtle changes we don't yet understand that the model does, right at the boundaries of the

39:59.280 --> 40:06.240
skin region and things like that. And so, you know, to me, this is alluding to the fact of,

40:06.240 --> 40:12.320
well, can we as domain experts learn something about our problem domain based on how it is

40:12.320 --> 40:17.760
models are operating? And I think we see this more clearly in another articulation of shared

40:17.760 --> 40:22.800
interest semantics. Here, what we're doing is basically using shared interest to interactively

40:22.800 --> 40:29.040
probe or query that latent space. So we're brushing and using that brushed region as ground truth

40:29.360 --> 40:35.840
and then calculating the IOU coverage to figure out what are all the classes that maximize IOU

40:35.840 --> 40:42.720
coverage for that brush ground truth. So we can see if I brush over hand, a lot of the classes

40:42.720 --> 40:47.760
that get returned are things that are often associated with hands like laptops and cleavers

40:47.760 --> 40:54.320
and interestingly enough, hen. So I guess a lot of the images in the ImageNet, you know, data set

40:54.400 --> 41:01.840
have people holding hens, right, which is, I guess, kind of interesting. But more maybe profoundly

41:01.840 --> 41:08.000
is we could ask a question like, what is the essence of a dog, right? What is the minimal

41:08.000 --> 41:14.080
amount of region that I would need to brush for the model to still be convinced that what it is

41:14.080 --> 41:19.280
classifying as a dog? So I could start with the whole dog and then brush just on its head and

41:19.280 --> 41:24.160
sure, you know, querying which shared interest still returns, you know, dog classes. But then I

41:24.160 --> 41:31.600
could use a smaller brush and brush just on the nose and it still returns, you know, German shepherd

41:31.600 --> 41:36.880
and sheepdog and Tibetan terrier and things like that. So it seems like according to the model,

41:37.440 --> 41:45.840
all it really needs to know about, you know, an object in the image is the sort of shape of its

41:45.840 --> 41:51.360
nose or something associated with its nose to be able to classify whether it is or is not a dog.

41:51.360 --> 41:57.040
And this seems like a really sort of toy example, but it reflects some of the things that real

41:57.040 --> 42:03.200
world scientists are doing. So in particular, you know, there's a researcher at the University of

42:03.200 --> 42:09.360
Washington, Julia Parrish that runs this grand crowdsource data collection project around seabird

42:09.360 --> 42:14.880
deaths. And the way they train their participants to figure out how to do bird classification

42:14.960 --> 42:19.840
is by asking them to measure, you know, the bird beaks and the bird feet and things like that.

42:19.840 --> 42:23.840
And so I think it's really interesting that we're seeing maybe some of those sorts of

42:23.840 --> 42:32.480
representations creep up in how a model is making its decisions as well. And so where I want to end

42:32.480 --> 42:38.480
is sort of being most speculative and where I think, you know, there's scope for HCI to sort of

42:38.480 --> 42:43.680
grow. And so, you know, we looked at generative models and imposing a conceptual model on them.

42:43.680 --> 42:49.120
We looked at predictive models where the idea was to align conceptual models. But what I think,

42:49.120 --> 42:53.840
you know, we're hearing from that dermatologist we're seeing in that last case study which shared

42:53.840 --> 43:01.440
interest is the potential to use machine learning models to basically discover new representations

43:01.440 --> 43:07.200
of particular problem domains, right? And, you know, again, at my most speculative, I don't know

43:07.200 --> 43:11.440
what I would call these, but I would maybe call them abstraction models, right, where the goal of

43:11.440 --> 43:16.880
these models is not to produce some particular outcome that I care about, but to maximize what

43:16.880 --> 43:21.440
are the different ways of representing the world, right? What are all the diverse abstractions that

43:21.440 --> 43:26.720
we could learn about a problem domain like classifying dogs or classifying seabirds or things

43:26.720 --> 43:32.000
like that. And I think this is a really interesting opportunity to use machine learning to essentially

43:32.000 --> 43:37.920
advance our understanding, advance our science. But I want to be careful here because we've already

43:38.000 --> 43:42.400
seen, you know, through this talk, but also in the broader discourse, how generative and predictive

43:42.400 --> 43:47.120
models can sort of muddy that, that gulf of evaluation, right? Lots of people are starting

43:47.120 --> 43:52.320
to anthropomorphize these models, you know, some people think these models are representing general

43:52.320 --> 43:57.120
intelligence or conscience or things like that. And there's a potential with, you know, these

43:57.120 --> 44:03.520
abstraction models to make this problem worse by muddying the question of, well, how do we know

44:03.600 --> 44:10.640
what we know, right? Like what counts as evidence? Is it evidence because, you know, the model has

44:10.640 --> 44:16.960
learned that representation? And how do we validate what that evidence is? In the case of

44:16.960 --> 44:22.240
representations that are designed or interpreted or theorized by people, we know how to consider that

44:22.240 --> 44:27.360
to be evidence, right? But I don't know what it means for a learned representation to count as

44:27.360 --> 44:32.240
evidence. And as all sorts of problems in machine learning, this is not necessarily a problem that

44:32.240 --> 44:39.920
is unique to machine learning. So here are three visualizations that were used to discuss the

44:39.920 --> 44:45.680
COVID-19 pandemic right at the, the, the peak of the first wave in the summer of 2020.

44:48.080 --> 44:55.280
And I'm curious if anything pops out at you, like any reason, you know, to be curious or

44:55.360 --> 45:05.840
suspect of, of these visualizations, right? Like no, right? Probably not. Like these seem pretty

45:05.840 --> 45:11.680
legitimate, right? Like our world and data, very legitimate data source, right? And if you look at,

45:11.680 --> 45:15.520
look at some of these two other visualizations, you might go, you know what, actually the one on the

45:15.520 --> 45:19.680
right, that looks like something in maybe a policy briefing or something, right? It looks very

45:19.680 --> 45:27.120
sophisticated, lots of good annotation, you know, a style and aesthetic that looks very sort of

45:27.120 --> 45:32.880
sophisticated. But you may be catching what I'm alluding to, which is the fact that all three

45:32.880 --> 45:40.800
visualizations were used by people on social media to advance the argument that, you know,

45:40.800 --> 45:47.200
our response to COVID was overblown. Not that COVID was a hoax, but that our reaction to it

45:47.200 --> 45:54.000
was, was way too extreme. That COVID wasn't as serious an issue as it might initially seem.

45:54.560 --> 46:00.080
And I want to be really careful about what I'm, when I'm doing here with these charts, because

46:00.080 --> 46:04.720
certainly some of the people that were distributing this were bad actors who were ideologically

46:04.720 --> 46:11.920
motivated. But through a very long, laborious ethnography, ethnographic process that we

46:11.920 --> 46:17.680
conducted, spending six months on five different Facebook groups, we found that a lot of people

46:17.680 --> 46:22.560
who were producing visualizations like that were actually displaying many hallmarks of citizen

46:22.560 --> 46:28.000
data science. So they were really many of them filling gaps in, in information sort of collection,

46:28.000 --> 46:32.400
because they were situated in rural parts of the country where, you know, there wasn't a lot of good

46:32.400 --> 46:37.920
data collection. So many members of these groups were hosting, you know, webcasts, live seminars of

46:37.920 --> 46:42.800
how to download data from the government website, how to clean it and excel, how to visualize it and

46:42.800 --> 46:48.880
things like that. And, and most surprisingly to us, many of them were engaged in discussion that

46:48.880 --> 46:54.560
looked like peer review, right? They were critically assessing data sources, discussing metrics,

46:55.200 --> 46:59.920
making arguments for which metrics were, were better or not. But all of this was sort of

46:59.920 --> 47:06.160
inflected through a sort of frustration with mainstream institutions and maybe even distrust

47:06.160 --> 47:12.640
of those institutions as well, right? But ultimately what these groups cared about was bolstering a

47:12.640 --> 47:18.720
sense of social unity and civic engagement, right? So this quote I, I find particularly sort of

47:18.720 --> 47:23.600
reflective of that sense of, you know, it's incumbent on all of us to hold our elected officials to

47:23.600 --> 47:28.560
account so that they make better decisions through data, right? I'm speaking to you as a neighbor,

47:28.560 --> 47:35.120
as a mama bear, right? So this is not some sort of ideologically motivated individual who is,

47:35.120 --> 47:39.520
who is, you know, trying to be a bad actor. This is just an engaged member of the citizenry.

47:40.400 --> 47:45.520
And similarly, you know, oftentimes they were actually more sophisticated than scientists

47:45.520 --> 47:50.720
can be. So many of these members were very reflexive about their own data analysis,

47:50.720 --> 47:55.760
data gathering process, right? So someone says, you know, I've never claimed to have no bias,

47:55.760 --> 48:00.560
right? I'm human, of course I'm biased, here are my biases. Whereas in science, often we like to

48:00.560 --> 48:06.240
portray ourselves as being very objective, you know, arbiters of truth. And so in many ways,

48:06.240 --> 48:12.720
you know, what was happening in these groups is, is perhaps more sophisticated than what was happening

48:12.720 --> 48:16.880
in science and public health at the time. But the question is, so what does this have to do with

48:16.880 --> 48:22.640
sort of bridging semantic distances and abstraction models? Well, I think what was happening in,

48:22.640 --> 48:28.400
in those groups was, you know, they, they, they disagreed with the definitions of some of these

48:28.480 --> 48:34.240
metrics, right? They were living in rural communities. And so the metrics that public

48:34.240 --> 48:39.920
health officials were using to, to, you know, define the, the state and scale of the pandemic

48:39.920 --> 48:43.760
was not reflected in their lived experience. They were turning around and well,

48:43.760 --> 48:48.480
it didn't seem like COVID was an issue, right? And so our colleagues in the humanities and

48:48.480 --> 48:54.720
social sciences often advocate for adopting what they call an interpretive, interpretivist lens,

48:54.720 --> 48:59.280
right? The idea that knowledge is subjective, it's socially constructed,

48:59.280 --> 49:03.920
and that it's composed of many diff, different diverse perspectives that we have to figure out

49:03.920 --> 49:09.680
ways to synthesize together. And while that idea has been adopted in pockets of visualization and

49:09.680 --> 49:15.520
HCI and CS, so far, I think it's largely been on the qualitative side, because if we think about

49:15.520 --> 49:22.080
how to do computation, we have to, you know, we're forced into making decisions about the world and

49:22.080 --> 49:26.960
how to represent that world and computational data structures. And what I think abstraction models

49:26.960 --> 49:32.480
allow us to do is start to push, but you know, push that boundary a little bit, right? Rather than

49:32.480 --> 49:38.000
being focused on developing a model that produces a single best outcome, we might instead be looking

49:38.000 --> 49:42.720
to a world in which we are training sort of ecosystems of abstraction models, where we're

49:42.720 --> 49:48.080
forcing them to learn really different representations of the world or of a problem domain,

49:48.080 --> 49:52.240
and then leaving it up to people to figure out how to synthesize between those learned

49:52.240 --> 49:57.760
representations for, you know, some particular policy goal or, you know,

49:59.840 --> 50:05.680
thing that they want to optimize for. So with that, I'm happy to take questions about any of

50:05.760 --> 50:07.280
what I talked about. Thank you very much.

50:31.280 --> 50:35.120
This image that made me make this decision, so do you think the results would be different

50:35.120 --> 50:42.720
if you used, like, iFixations in that comparison? That's an interesting question. We haven't

50:42.720 --> 50:51.200
considered iFixations for the salency map work, but certainly I think your intuition is right in

50:51.200 --> 50:57.040
the sense that, you know, the current way that we've modeled shared interest is pretty brittle,

50:57.040 --> 51:01.680
right? It's operating at the level of abstraction of, like, pixels in an image, and how meaningful

51:01.680 --> 51:07.600
are pixels really? And so what Angie is working on right now is a way to raise the

51:07.600 --> 51:12.160
level of abstraction that shared interest is working on. So in many of these domains, like,

51:12.160 --> 51:18.080
you know, ImageNet, the task that we're asking models to do, the labeling task,

51:18.080 --> 51:24.240
actually inherits from a much richer knowledge graph or taxonomy or hierarchy or things like that.

51:24.240 --> 51:28.000
But right now, at least, you know, there's a little bit of work in hierarchical learning,

51:28.000 --> 51:33.600
but most of the predictive models are just learning at the finest level of detail, right?

51:33.600 --> 51:39.840
So we're throwing away all that rich information that might be really relevant to how a person is

51:39.840 --> 51:44.880
making a decision. So maybe what I care about is not whether it's a Chihuahua or a golden retriever

51:44.880 --> 51:50.320
or a laboratory retriever. I might care, is it a dog or really sometimes is it just an object,

51:50.320 --> 51:55.920
right? And so what does it look like to do shared interest in more meaningful abstraction space

51:55.920 --> 52:00.240
rather than pixels is something we're working on. Yeah, great question. Thanks. Yeah, Will.

52:01.680 --> 52:07.040
Thank you for the great talk, Arvin. So going back to Jupyter notebooks and ChatGPT,

52:07.920 --> 52:14.160
you talked about how, right, ChatGPT can shell out to some of these nice plugins like for Excel or

52:14.160 --> 52:19.440
whatever to try and help people do natural language data science and that there's this

52:19.440 --> 52:24.160
articulatory distance due to the difficulty of learning an API. But conversely, you could say

52:24.160 --> 52:29.680
tools like co-pilot are sort of the parallel to overcoming that articulatory distance by

52:29.680 --> 52:33.600
almost in some sense, what is the same interface expressing a natural language but just in a

52:33.600 --> 52:38.720
code comment and then getting back code, right? But just I guess the only difference is its code

52:38.720 --> 52:42.560
you can see as opposed to code that's running in some back end that you don't see. And I'm curious

52:42.560 --> 52:47.520
if you think there's sort of a synthesis of these two poles, an interface that can take the best of

52:47.520 --> 52:52.480
both worlds and offers conversation but still provides access to the code or encourages people

52:52.480 --> 52:58.800
to understand the annoying representations. Yeah, absolutely. I thought really hard about

52:59.520 --> 53:06.720
which of those examples I wanted to use as the kind of foil to B2. So I did very seriously

53:06.720 --> 53:13.760
consider a co-pilot and I sort of agree with your analysis that it's, I think, a much better

53:13.760 --> 53:20.880
example of how to integrate the capacity of these LLMs. And I think there's opportunity

53:20.880 --> 53:27.200
to push that even further where what I would often want is really targeted mechanisms to

53:27.200 --> 53:33.520
introduce ambiguity, right? Right now, the little that I've used co-pilot, it's almost at the level

53:33.520 --> 53:40.720
of, well, it's going to produce the whole function, the whole whatever. And often what I want is it

53:40.720 --> 53:46.640
to be the sort of parallel prototyper for me, right? I want to introduce, say, a hole in my program

53:46.640 --> 53:51.040
and then go, I don't know that I want that hole to be filled in with just one specific

53:52.080 --> 53:56.960
outcome, but I want it to produce the whole space and for me to go, well, I want a little bit of

53:56.960 --> 54:03.520
this and a little bit of that and so on and so forth. So yeah, I totally agree with there being

54:03.520 --> 54:08.080
some really interesting medium of these things. Cool. Yeah, I like that idea.

54:10.000 --> 54:16.400
Yeah. Hey, really exciting talk. I'm wondering towards your kind of vision for these abstraction

54:16.480 --> 54:22.400
models, I'm wondering like obviously kind of from a human-interpreter interaction perspective,

54:22.400 --> 54:28.320
we know like representation matters so much, right? Like isomorphs of representation very much

54:28.320 --> 54:34.080
change how people can approach a problem or understand it. But I guess the ways in which

54:34.080 --> 54:40.480
they vary and the benefits of these different representations are tied very much to human

54:40.480 --> 54:45.520
cognition and perception. And I'm wondering, you know, in some of the examples you're showing and

54:45.520 --> 54:50.640
a lot of work in machine learning, we're sort of training things based upon that output. Yeah.

54:50.640 --> 54:55.440
And I'm wondering like, you know, are there ways that we can get at more of how people

54:55.440 --> 55:01.200
are thinking versus just how they output and how do we get there? Yeah, I love this question.

55:02.000 --> 55:08.160
And the reason I love it is also the reason I love sort of that Hutchins et al description of

55:08.160 --> 55:12.480
direct manipulation, right? I find the terms that they use there, particularly these two's

55:12.480 --> 55:18.240
distances, really evocative terms. Because to me, a distance is something that I would want to

55:18.240 --> 55:24.800
measure, right? But so far at least, as far as I know, those terms have largely been descriptive,

55:24.800 --> 55:30.560
right? As you saw in my talk, like I use them to be very analytic, but I'm not able to be generative

55:30.560 --> 55:38.080
with them in, you know, a very systematic way. So certainly a lot of the work that my group is

55:38.080 --> 55:43.440
trying to do right now is in visualization, you know, there's a lot of work that we've

55:44.240 --> 55:49.680
inherited in methods from sort of vision science. So we run these studies of human perception.

55:51.360 --> 55:56.560
And increasingly, the field is starting to get to, well, how do we start to measure cognition,

55:56.560 --> 56:02.320
right? Can we model sort of a decision making task and start to, you know, operationalize

56:02.320 --> 56:06.800
that through experimental design? And so we're starting to push in some of those directions

56:06.800 --> 56:11.440
as well, but scope to sort of, you know, interaction in a Jupyter notebook, but then

56:11.440 --> 56:15.840
starting to see, you know, the impact that interaction has on sort of the downstream

56:15.840 --> 56:21.520
analyses people would do, and then see if that actually maps to, you know, their goals or things

56:21.520 --> 56:34.480
like that. Absolutely. Yeah. So I'm curious about the, just continue on this line of

56:34.480 --> 56:40.080
perception up through cognition, you know, going back to the sort of like,

56:40.800 --> 56:47.840
or 10 Cleveland McGill kinds of stuff, the automatic processing was very key to the design

56:47.840 --> 56:53.680
of visualizations, especially early on, that the notion was that my encodings were supposed to

56:53.680 --> 57:00.960
map on to almost like system one interpretation, right? Like when I see the scatterplot and

57:00.960 --> 57:04.560
you know, encoding distance in the following way, I'm gonna draw the correct conclusion.

57:04.560 --> 57:11.520
And it's interesting to me that sort of through the transformations you've started to pursue,

57:11.520 --> 57:19.600
we're not trying to like encode those into a similar mapping for audio, but instead

57:19.600 --> 57:25.360
directly doing the cognition on behalf of the individual. And those seem like orthogonal

57:25.360 --> 57:30.880
directions one could go. I'm curious how we find the right point in the design space.

57:31.840 --> 57:37.280
I think this is a fantastic question. So the way my group is starting to think about this

57:37.920 --> 57:41.760
of like, how do we find the sort of right balance of who is doing the perception,

57:41.760 --> 57:46.720
who is doing the interpretation is starting to consider some of these modalities and concert

57:46.720 --> 57:50.560
to better understand what the relative affordances of these modalities are.

57:50.560 --> 57:55.280
So in particular, Jonathan, who you saw in the demos is leading some really,

57:55.280 --> 58:03.040
really cool work right now around what if I'm sort of specifying the visual, the audio,

58:03.040 --> 58:07.840
the sort of sonified audio and the textual audio side by side, and then I'm playing them

58:07.840 --> 58:13.680
sort of simultaneously through. Do I want, you know, there to be sort of perceptual redundancy

58:13.680 --> 58:19.520
where the sonification is sort of emphasizing what is, you know, described in the texture?

58:19.520 --> 58:26.560
Do I want these modalities to be complementary? And, you know, sort of TBD, but I think there's

58:26.560 --> 58:31.600
some really exciting sort of questions for us to sort of dig into space.

58:31.600 --> 58:34.720
Are there similar pre attentive principles for audio? There must be.

58:36.000 --> 58:40.480
As far as I know, so I'm, you know, we're just starting to look in the sort of sonification

58:40.480 --> 58:46.720
literature. Yeah, as far as we can tell sound is a very, very different perceptual sense

58:47.520 --> 58:53.440
than vision. And so even the sort of, you know, basic sort of visual encoding paradigm where I

58:53.440 --> 58:59.600
take a data field, I map it to, you know, position color size that breaks down very rapidly for audio.

59:00.320 --> 59:06.400
So oftentimes really all the people are able to sort of, you know, detect differences in our sort

59:06.400 --> 59:13.600
of pitch and loudness. And even then our fidelity at that is very, very low. And so there might be

59:13.600 --> 59:19.120
some pre attentive characteristics. We're certainly looking at some early work in HCI.

59:19.120 --> 59:27.120
I think Stephen Brewster had done around sort of ear cons, you know, discreet sort of representations

59:27.120 --> 59:31.040
of icons, but through audio and things like that. So there may be some of that there.

59:32.000 --> 59:35.920
But at least so far we're so early in our own work that we don't know.

59:35.920 --> 59:37.520
Interesting. Okay, thank you. Yeah.

59:39.600 --> 59:43.280
I think we're about at time. So if you have additional questions, please mob him after

59:43.280 --> 59:52.800
the talk. Thank you, Arvin for joining us. Thank you very much.

