WEBVTT

00:00.000 --> 00:13.020
Hi, I'm delighted to be here with my old friend and collaborator, Professor Chris Manning.

00:13.020 --> 00:17.080
Chris has a very long and impressive bio, but just briefly, he is Professor of Computer

00:17.080 --> 00:22.080
Science at Stanford University and also the Director of the Stanford AI Lab.

00:22.080 --> 00:27.520
And he also has a distinction of being the most highly cited researcher in NLP or natural

00:27.520 --> 00:28.520
language processing.

00:28.920 --> 00:30.880
So really good to be here with you, Chris.

00:30.880 --> 00:32.560
Oh, good to get a chance to chat, Andrew.

00:33.640 --> 00:37.480
So, you know, we've known each other, collaborated for many years.

00:37.480 --> 00:41.880
And one interesting part of your background, I always thought, was that even though today

00:41.880 --> 00:46.360
you're a distinguished researcher in machine learning and NLP, you actually started off

00:46.360 --> 00:48.080
in a very different area.

00:48.080 --> 00:55.120
Your PhD, if I'm not correctly, was in linguistics, and you were studying the syntax of language.

00:55.560 --> 01:00.080
So how did you go from studying syntax to being an NLP researcher?

01:00.680 --> 01:02.480
So I can certainly tell you about that.

01:02.480 --> 01:06.720
But I should also point out that, you know, I'm still actually a professor of linguistics

01:06.720 --> 01:07.120
as well.

01:07.120 --> 01:08.920
I have a joint appointment at Stanford.

01:09.880 --> 01:14.160
And, you know, once in a blue moon, not very often, I do actually still teach some real

01:14.160 --> 01:18.320
linguistics as well as computer-involved natural language processing.

01:19.160 --> 01:27.040
So, you know, so starting out, I was very interested in human languages and how they

01:27.040 --> 01:32.240
work, how people understand them, how they are required.

01:32.240 --> 01:37.960
So I had this sort of appeal, I saw this appeal in human languages.

01:38.560 --> 01:46.680
But that equally led me to think about ideas that we now very much think about as machine

01:46.680 --> 01:48.960
learning or computational ideas.

01:48.960 --> 01:57.080
So two of the central ideas in human language, how do little children acquire human language?

01:57.080 --> 02:01.920
And for adults, well, we're just talking to each other now, and we pretty much understand

02:01.920 --> 02:02.640
each other.

02:02.640 --> 02:06.040
And, you know, that's actually an amazing thing how we managed to do that.

02:06.040 --> 02:08.200
So what kind of processing allows that?

02:08.200 --> 02:13.120
And so that early on got me interested in looking at machine learning.

02:13.120 --> 02:18.560
In fact, even before I'd made it to grad school, I'd started, you know, baby steps

02:18.560 --> 02:21.240
and learning machine learning coming off of those interests.

02:21.680 --> 02:23.960
Yeah, in fact, all human languages learn.

02:23.960 --> 02:26.560
You know, we had learned at some point in our lives to speak English.

02:26.560 --> 02:28.280
We'd grown up in a different place.

02:28.280 --> 02:30.360
We would learn a totally different language.

02:30.360 --> 02:35.880
So it's amazing to think how humans do that and now maybe machines learn language too.

02:36.240 --> 02:40.360
But so, so just, you know, tell us more about your journey.

02:40.360 --> 02:44.920
So you had a PhD in linguistics, and then, and then how did you?

02:45.320 --> 02:47.480
So there's some stuff before that as well.

02:47.640 --> 02:53.680
So, I mean, you know, when I was an undergrad, well, officially, I actually did three majors.

02:53.680 --> 02:58.800
This was in Australia, one in math, one in computer science and one in linguistics.

02:59.000 --> 03:04.480
Now, people get a slightly exaggerated sense of what that means if you're in an American

03:04.480 --> 03:09.000
context, because, you know, it'd be, I think, impossible to complete three majors

03:09.000 --> 03:11.080
or not undergrad at Stanford.

03:11.200 --> 03:15.840
But, you know, actually, where I was as an undergrad doing, I did an arts degree.

03:15.840 --> 03:18.040
So I could do whatever I wanted, like linguistics.

03:18.200 --> 03:21.720
You had to complete two majors to complete the arts degree.

03:21.720 --> 03:25.400
So, you know, it's sort of more like double majoring, maybe in US terms.

03:26.280 --> 03:30.160
You probably don't know this about me, but at Carnegie Mellon, I actually was a triple major.

03:30.760 --> 03:33.560
Math CS was once in the statistics and economics.

03:34.600 --> 03:36.560
That's great, we're both fellow triple majors.

03:37.040 --> 03:43.080
Yeah, so anyway, I did have background in interest in doing things with computer science.

03:44.040 --> 03:47.360
And so my interests were kind of mixed.

03:47.360 --> 03:51.080
And I mean, actually, you know, when I applied to grad schools, I mean, one of the places

03:51.080 --> 03:55.840
I applied to was Carnegie Mellon, because they were strong in computational linguistics,

03:55.840 --> 03:59.760
you know, and if I'd gone there, I would have been enrolled as a CS student.

04:00.200 --> 04:04.720
But I ended up at Stanford as a linguistics student, because at that time there wasn't

04:04.720 --> 04:08.480
any natural language processing in the CS department.

04:09.040 --> 04:14.280
But, you know, I was still interested in pursuing ideas in natural language processing.

04:14.280 --> 04:20.880
But at that point in the early 90s, things were just starting to change.

04:20.880 --> 04:29.600
But the bulk of natural language processing was rule-based, logical, declarative systems.

04:29.840 --> 04:35.480
But it was also in those years, at the beginning of the 90s, when there first started to be

04:35.480 --> 04:40.360
lots of human language material, text and speech available digitally.

04:40.360 --> 04:43.920
So this was really actually just before the World Wide Web exploded.

04:43.920 --> 04:49.640
But there had already started to be things like legal materials and newspaper articles

04:49.640 --> 04:55.600
and parliamentary handsaws, where you could at last get your hands on millions of words

04:55.720 --> 04:57.160
of human language.

04:57.160 --> 05:02.200
And it just seemed really clear that there had to be exciting things that you could do

05:02.200 --> 05:05.240
by working empirically from lots of human language.

05:05.240 --> 05:11.320
And that's what really sort of got me involved in a new kind of natural language processing

05:11.320 --> 05:13.440
that then led into my subsequent career.

05:14.000 --> 05:17.280
It sounds like your career was initially more linguistics.

05:17.280 --> 05:23.000
And if the rise of data and machine learning and empirical methods, it shifted to what NLP

05:23.000 --> 05:24.800
and machine learning and NLP?

05:25.360 --> 05:27.720
Yeah, I mean, it absolutely certainly shifted.

05:27.720 --> 05:33.200
And I've certainly sort of shifted much more to doing both natural language processing

05:33.200 --> 05:35.240
and machine learning models.

05:35.240 --> 05:38.840
But to some extent, the balances varied.

05:38.840 --> 05:41.280
But I've sort of been with that as of while.

05:41.280 --> 05:43.800
You know, actually, there's an undergrad.

05:43.800 --> 05:50.280
For my undergrad on this thesis, it was sort of learning the forms of words.

05:50.280 --> 05:55.520
So how you can, which became a famous problem of sort of learning past tense of English

05:55.520 --> 05:58.560
verbs in the early connectionist literature.

05:58.560 --> 06:02.720
And I was trying to sort of learn paradigms of forms of verbs.

06:02.720 --> 06:09.720
And I was learning rules for the different forms using the C4.5 decision tree learning

06:09.720 --> 06:11.200
algorithm, if you remember that.

06:12.440 --> 06:12.760
Yes.

06:13.800 --> 06:14.240
Right.

06:14.400 --> 06:15.000
Good times.

06:15.000 --> 06:15.200
Yeah.

06:15.200 --> 06:17.520
And it's surprisingly non-intuitive, right?

06:17.520 --> 06:19.880
How going from present tense to past tense.

06:20.960 --> 06:25.320
From, I don't know, run to run and all the other weird special cases can be.

06:25.320 --> 06:25.680
Yeah.

06:27.400 --> 06:31.200
Hey, so we talked a bunch about NLP, natural language processing.

06:31.200 --> 06:38.680
So for some of the learners, pick up machine learning for the first time, can you say, what is NLP?

06:38.680 --> 06:39.960
Sure, absolutely.

06:39.960 --> 06:43.400
Yes, NLP stands for natural language processing.

06:43.400 --> 06:47.920
Another word this term that's sometimes used for that is computational linguistics.

06:47.920 --> 06:49.000
It's the same thing.

06:49.040 --> 06:52.800
I mean, natural language processing is actually a weird term, right?

06:52.800 --> 06:56.160
So it means that we're doing things with human languages.

06:56.160 --> 07:00.760
So you have to have the conception that you're enough of a computer scientist that when you

07:00.760 --> 07:05.360
say language, you think in your brain programming language, and therefore you need to say natural

07:05.360 --> 07:09.320
language to mean that you're talking about the languages that human beings use.

07:10.040 --> 07:15.680
So overall natural language processing is doing anything intelligent with human languages.

07:15.680 --> 07:23.480
So in one sense, that breaks down into understanding human languages, producing human languages,

07:23.480 --> 07:25.600
acquiring human languages.

07:25.600 --> 07:29.560
Though people also often think about it in terms of different applications.

07:29.560 --> 07:35.560
And so then you might think about things like machine translation or doing question answering

07:35.560 --> 07:42.120
or generating advertising copy or summarization.

07:42.120 --> 07:47.880
There are so many different tasks that people work on with particular goals in mind where

07:47.880 --> 07:49.640
you do things with human language.

07:49.640 --> 07:55.280
And there's a lot of natural language processing because so much of what the world works on

07:55.280 --> 08:01.560
our human world is dealt with and transmitted in terms of human language material.

08:02.880 --> 08:09.680
So because of all of these applications or even a web stage, most of us use NLP many, many times.

08:09.680 --> 08:15.600
Yeah, you're right. In some sense, the biggest application of natural language is web search,

08:15.600 --> 08:18.440
right? That's really the big one.

08:18.440 --> 08:22.520
I mean, traditionally, it was a kind of a simple one, right?

08:22.520 --> 08:27.600
But in the good old days, it was, you know, there were various waiting factors and so on,

08:27.600 --> 08:33.440
but it was mainly sort of matching keywords than your search terms and then some factors

08:33.440 --> 08:35.120
about the quality of the page.

08:35.120 --> 08:39.520
It didn't really feel like language understanding, but that's really been changing

08:39.520 --> 08:45.840
over the years. So these days, you'll often, if you ask a question to a search engine,

08:45.840 --> 08:51.760
it'll give you, you know, an answer box where it has extracted a piece of text and puts

08:51.760 --> 08:55.720
what it thinks is the answer in bold or color or something like that, which is then this

08:55.720 --> 08:57.720
task of question answering.

08:57.720 --> 09:00.440
And then it's really a natural language understanding task.

09:00.440 --> 09:05.600
Yeah, yeah. And I feel like in addition to web searches, maybe the big one, you know,

09:05.680 --> 09:11.080
when we're going to a online shopping website or a movie website and typing in what we want

09:11.080 --> 09:16.440
and doing a website search on a much smaller website than, you know, the big search engines,

09:16.440 --> 09:22.240
that also increasingly uses sophisticated NLP algorithms and it's also creating quite

09:22.240 --> 09:27.680
a lot of value. Maybe to you is not, you know, the real NLP, but it still seems very valuable.

09:27.680 --> 09:32.400
I agree. It's very valuable. And there are, you know, lots of interesting problems in

09:32.400 --> 09:37.560
any e-commerce website with search, very difficult problems, actually, when people

09:37.560 --> 09:41.840
describe the kind of goods they want and you need to be trying to match it to products

09:41.840 --> 09:45.640
that are available. That isn't an easy problem at all, it turns out.

09:45.640 --> 09:52.640
Yeah, that's true. Yeah. So over the last couple of decades, NLP has gone through a major

09:52.640 --> 09:58.440
shift from more of the rule-based techniques that you alluded to just now to using really

09:58.480 --> 10:05.480
machine learning much more pervasively. And so you were one of the people at, you know,

10:05.480 --> 10:10.240
leading parts of that charge and seeing every step of the way of creating some of the steps

10:10.240 --> 10:14.880
as it happened. Can you say a bit about that process and what you saw?

10:14.880 --> 10:21.000
Sure, absolutely. Yeah. So when I started off as an undergrad and grad student, really

10:21.040 --> 10:28.040
most of natural language processing was done by hand-built systems, which variously used

10:29.720 --> 10:36.720
rules and inference procedures to sort of try and build up paths and an understanding

10:37.040 --> 10:42.280
of a piece of text. What's an example of a rule or an inference system?

10:42.280 --> 10:49.280
So, you know, a rule could be part of the structure of human language, like English

10:49.680 --> 10:54.400
sentence normally consists of a subject noun phrase followed by a verb and an object noun

10:54.400 --> 11:00.040
phrase, and that gives you some ideas to how to understand the meaning of the sentence.

11:00.040 --> 11:07.040
But it might also be saying something about how to interpret a word, so that a lot of

11:07.160 --> 11:14.160
words in English are very ambiguous. But if you have something like the word star and

11:15.160 --> 11:20.280
it's in the context of a movie, then it's probably referring to a human being on this

11:20.280 --> 11:26.240
astronomical object. And in those days, people tried to deal with things like that using

11:26.240 --> 11:33.240
rules of that sort. That doesn't seem very likely to work to us these days, but, you

11:34.240 --> 11:41.240
know, once upon a time that was pretty standard. And so it was only when lots of digital text

11:42.160 --> 11:46.480
and speech started to become available that it really seemed like there was this different

11:46.480 --> 11:53.240
way that instead we could start calculating statistics over human language material and

11:53.240 --> 12:00.240
building machine learning models. And so that was the first thing that I got into in the

12:02.200 --> 12:09.200
sort of mid to late 1990s. And so, you know, the first area where I started doing lots

12:09.320 --> 12:14.600
of research and publishing papers and getting well known is building what in the early days

12:14.600 --> 12:20.440
we often called statistical natural language processing. But it later merged into in general

12:20.440 --> 12:26.040
probabilistic approaches to artificial intelligence and machine learning. And that sort of took

12:26.040 --> 12:33.040
us through to approximately 2010, let's say. And that's roughly when the new interest in

12:34.040 --> 12:41.040
deep learning using large artificial neural networks started to take off. For my interest

12:42.360 --> 12:47.840
in that, I really have you to thank Andrew, because at this stage, Andrew was still full

12:47.840 --> 12:53.840
time at Stanford, and he was in the office next door to me, and he was really excited

12:53.840 --> 12:59.160
about the new things that were happening in the area of deep learning. I guess anyone

12:59.200 --> 13:03.200
who walked into his office, he'd tell them, oh, it's so exciting what's happening now,

13:03.200 --> 13:08.720
and neural networks should start looking at that. And so, you know, that was really the

13:08.720 --> 13:15.720
impetus that got me pretty early on involved in looking at things in neural networks. I

13:17.520 --> 13:23.000
had actually seen a bit of it before. So while I was a grad student here, actually Dave Ruhmelhardt

13:23.000 --> 13:28.200
was at Stanford and Psych, and I'd taken his neural networks class. And so, you know,

13:28.240 --> 13:33.560
I'd seen some of that, but it hadn't actually really been what I'd gotten into for my own

13:33.560 --> 13:36.880
research. So around.

13:36.880 --> 13:39.880
I didn't know that. Thank you.

13:39.880 --> 13:42.880
And then we wound up, you know, supervising some students together.

13:42.880 --> 13:43.880
Yeah, absolutely.

13:43.880 --> 13:48.880
But I'd love to hear the rise of also deep learning in NLP. What are the GCs?

13:48.880 --> 13:55.880
Yeah. So starting about 2010, yeah, me, students started to do the first papers in

13:58.880 --> 14:03.920
deep learning aimed at NLP conferences. You know, it's always hard when you're trying

14:03.920 --> 14:10.680
to do something new. We had exactly the same experiences that people 15 or so years earlier

14:10.680 --> 14:15.760
had had when they started trying to do statistical NLP of when there's an established way of

14:15.760 --> 14:20.360
doing things. It's really hard to push out new ideas. So really some of our first papers

14:20.360 --> 14:27.160
were rejected from conferences and instead appeared at machine learning conferences or

14:27.160 --> 14:32.240
deep learning workshops. But very quickly that started to change and people got super

14:32.240 --> 14:39.240
interested in neural network ideas. But I sort of feel like the neural network period

14:39.600 --> 14:46.600
which started effectively about 2010 itself divides in two. Because for the first period,

14:47.600 --> 14:54.680
let's basically say it's till 2018, we showed a lot of success at building neural networks

14:54.720 --> 15:00.720
for all sorts of tasks. We built them for syntactic parsing and sentiment analysis and

15:00.720 --> 15:07.720
what else to question answering. But it was sort of like we were doing the same thing

15:08.600 --> 15:14.480
that we used to do with other kinds of machine learning models, except we now had a better

15:14.480 --> 15:19.760
machine learning model. And we were sort of instead of training up a logistic regression

15:19.800 --> 15:24.400
or a support vector machine, we were still doing the same kind of sentiment analysis

15:24.400 --> 15:30.640
task but now we're doing it with a neural network. So I think looking back now in some

15:30.640 --> 15:37.640
sense, the bigger change came around 2018 because that was when the idea of well we

15:39.040 --> 15:46.040
could just start with a large amount of human language material and build large self-supervised

15:47.040 --> 15:54.040
models. So that was models then like BERT and GPT and successor models to that. And

15:54.960 --> 16:00.760
they could just sort of acquire from word prediction over a huge amount of text this

16:00.760 --> 16:07.760
amazing knowledge of human languages. I think really probably that's going to be viewed

16:07.760 --> 16:14.280
in retrospect as the bigger kind of cut point where the way things were done really changed.

16:14.760 --> 16:19.760
Yeah, I think there is that trend for the large language models, learning from math

16:19.760 --> 16:24.640
and the mouse and data. I think even to lead up to that, there was one of your research

16:24.640 --> 16:31.640
papers that really slightly blew my mind, which is a glove paper. So because with word

16:32.160 --> 16:37.720
embeddings where you learn a vector of numbers to represent a word using a neural network,

16:37.720 --> 16:44.120
that was quite mind blowing for me. And then the glove work that you did really

16:44.160 --> 16:48.000
cleaned up the math, made it so much simpler. And then I remember reading I said, oh, that's

16:48.000 --> 16:53.160
all there is to it. And then you can learn these really surprisingly detailed representations

16:53.160 --> 16:57.440
of the computer learns real nuances of what words mean.

16:57.440 --> 17:02.840
Absolutely. Yeah, so I should give a little bit of credit to others. Other people also

17:02.840 --> 17:09.800
worked on some similar ideas, including Renan Colbert and Jace Weston and Tom Ostermeek

17:10.000 --> 17:16.760
and colleagues at Google. But the glove word vectors is one of the very prominent systems

17:16.760 --> 17:21.960
of word vectors. So these word vectors already did, yeah, you're right, illustrate this idea

17:21.960 --> 17:26.560
of using self-supervised learning that we just took massive amounts of text, and then

17:26.560 --> 17:33.160
we could build these models that knew an enormous amount about the meaning of words. I mean,

17:33.160 --> 17:40.160
it's still something I sort of show people every year in the first lecture of my NLP class,

17:41.440 --> 17:48.440
because it's something simple, but it actually just works so surprisingly well. You can do

17:48.440 --> 17:54.600
this sort of simple modeling of trying to predict a word given the words in the context.

17:54.600 --> 17:59.560
And simply by sort of running the math of learning to do those predictions well, you

17:59.600 --> 18:04.200
learn all these things about word meaning, and you can do these really nice patterns

18:04.200 --> 18:11.200
of similar word meaning or analogies of something like, you know, pencil is to drawing as paint

18:12.800 --> 18:19.800
brush is to, and it'll say painting, right? That it's sort of already showing just a lot

18:20.000 --> 18:27.000
of successful learning. So that was the precursor to what then got developed to the next stage

18:28.000 --> 18:34.000
with things like Burton GPT, where it wasn't just meanings of individual words, but meanings

18:34.760 --> 18:36.880
of whole pieces of text and context.

18:36.880 --> 18:41.880
Yeah, so I thought it was amazing that you can, you know, take a small neural network

18:41.880 --> 18:47.720
or some model and then give it lots of English sentences or some of the language and hide

18:47.720 --> 18:52.240
the word, ask it to predict what is the word that I just hit, and that allows it to learn

18:52.280 --> 18:58.780
these analogies and these very deep, what you would think are really deep things behind

18:58.780 --> 19:05.280
the meaning of the words. And then, you know, 2018, maybe there's other infection point.

19:05.280 --> 19:06.800
What happened after that?

19:06.800 --> 19:13.800
Yeah, so I mean, in 2018, that was the point in which, well, sort of really two things happened.

19:14.000 --> 19:21.000
One thing is that people, well, really in 2017, had developed this new neural architecture,

19:22.760 --> 19:29.760
which was much more scalable onto modern parallel GPUs, and so that was the transformer architecture.

19:31.000 --> 19:36.360
The second part of it, though, was, you know, maybe people rediscovered because it was using

19:36.360 --> 19:41.960
the same trick as the glove model, that if you have the task of say, of just predicting

19:41.960 --> 19:48.200
a word given a context, either a context on both sides of it or the preceding words,

19:48.200 --> 19:55.200
that that just turns out to be an amazing learning task. And that surprises a lot of

19:55.200 --> 20:02.040
people, and a lot of the time you see discussions where people say disparaging things of, you

20:02.040 --> 20:06.760
know, this is nothing interesting is happening, all it's doing is statistics to predict which

20:06.840 --> 20:13.840
word is most likely to come after the preceding words. And I think the really interesting

20:13.840 --> 20:20.680
thing is that that's true, but it's not true. I mean, because, yes, what the task is, is

20:20.680 --> 20:27.440
you're predicting the next word given preceding words. But the really interesting thing is,

20:27.440 --> 20:34.440
if you want to do that task really as well as possible, then it actually helps to understand

20:35.200 --> 20:41.200
the whole of the rest of the sentence and know who's doing what to who in what's in the sentence.

20:41.200 --> 20:48.200
But more than that, it also helps to understand the world. Because if your, your text is going

20:48.800 --> 20:55.800
something along the lines of, you know, the currency used in Fiji is that, well, you need

20:57.360 --> 21:02.920
to have some world knowledge to know what the right answer to that is. And so good models

21:02.960 --> 21:09.960
at doing this learn both to follow the structure of sentences and their meaning and to know

21:11.400 --> 21:16.800
facts about the world also that they can predict. And therefore this turns into what

21:16.800 --> 21:22.240
sometimes referred to as an AI complete task, right? That you really need, there's nothing

21:22.240 --> 21:29.080
that can't actually be useful in answering this, what word comes next sense, right? You

21:29.080 --> 21:36.080
know, you can be in the World Cup semifinals, the teams are, and you need to know something

21:36.600 --> 21:39.520
about soccer to be giving the right answer.

21:39.520 --> 21:44.720
AI completes this funny concept, or is this idea that you can solve this one problem,

21:44.720 --> 21:50.400
you can solve, you know, everything in AI or kind of make an analogy to NP complete problems

21:50.400 --> 21:55.640
from the theory of computing. What do you think? Do you think predicting the next word

21:55.680 --> 21:58.640
is AI complete? I have very mixed feelings about that myself.

21:58.640 --> 22:03.640
I shall, I shall go ahead and say, I don't think it's true. So just what do you think?

22:03.640 --> 22:10.640
I think it's not quite true because I think there are other kinds of things that human

22:13.200 --> 22:19.080
beings manage to work out. You know, there are human beings that have clever insights

22:19.080 --> 22:25.600
in mathematics, or there are human beings who are looking at something that's much more

22:25.760 --> 22:32.560
you know, three dimensional, real world puzzle of sort of figuring out how to do something

22:32.560 --> 22:39.560
mechanical or something like that, and that's just not a language problem. But on the other

22:40.960 --> 22:47.960
hand, I mean, I think language gets closer to universality than some people think as

22:48.920 --> 22:55.920
well, because, you know, we live in this 3D world and operate in it with our bodies and

22:58.320 --> 23:05.320
our feelings and other creatures and artifacts around it, and you could think, well, not

23:05.520 --> 23:12.520
much of that is in language at all. But actually, just about all of this stuff, we think about,

23:13.400 --> 23:19.440
we talk about, we write about it in language, we can describe the positions of things relative

23:19.440 --> 23:24.680
to each other in language. So a surprising amount of the other parts of the world are

23:24.680 --> 23:29.640
seen in reflection in language, and therefore you're learning about all of them too when

23:29.640 --> 23:31.600
you learn about language use.

23:31.600 --> 23:37.600
You learn about, you know, one aspect of a lot of things, even if things like, how do

23:38.280 --> 23:45.280
you ride a bicycle? You don't really learn how to ride a bicycle, but you learn some

23:45.480 --> 23:49.880
aspects of what it involves, that you need to balance and you have to have your feet

23:49.880 --> 23:54.680
on the pedals and push them and all of that kind of things, yeah.

23:54.680 --> 24:01.200
And so with this trend in NLP, the large language models has been very exciting for the last

24:01.200 --> 24:06.160
several years. What are your thoughts on where all this will go?

24:06.520 --> 24:13.520
Well, I mean, yeah, so it's just been amazingly successful and exciting, right? So we haven't

24:15.440 --> 24:18.760
really explained all the details, right? So there's a first stage of learning these

24:18.760 --> 24:25.760
large language models where the task is just to predict the next word and you do that billions

24:25.760 --> 24:32.360
of times over a very large piece of text, and behold, you get this large neural network,

24:32.400 --> 24:37.240
which is just a really useful artifact for all sorts of natural language processing tasks.

24:37.240 --> 24:41.520
But then you still actually have to do something with it if you want to do a particular task,

24:41.520 --> 24:48.520
whether that's question answering or summarization or detecting toxic content in social media

24:48.520 --> 24:52.480
or something like that. And at that point, there's a choice of things that you could

24:52.480 --> 24:58.840
do with it. The traditional answer was then you had a particular task, like say, detecting

24:58.840 --> 25:04.480
toxic comments in social media, and you'd take some supervised data for that, and then

25:04.480 --> 25:11.480
you'd fine tune the language model to answer that classification task. But you were enormously

25:11.480 --> 25:17.520
helped by having this base of this large self-supervised model because it meant that the model had

25:17.520 --> 25:23.280
enormous knowledge of language and it could generalize very quickly. So unlike the sort

25:23.320 --> 25:30.320
of the standard old days of supervised learning where it was kind of, well, if you give me

25:31.000 --> 25:36.840
10,000 labeled examples, I might be able to produce a halfway decent model for you, but

25:36.840 --> 25:42.520
if you give me 50,000 labeled examples, it'll be a lot better. It's sort of turned into this

25:42.520 --> 25:49.040
world of, well, if you give me 100 labeled examples and I'm fine tuning a large language

25:49.040 --> 25:53.960
model, I'll be able to do great, better than I would be able to do with the 50,000 examples

25:53.960 --> 25:59.480
in the old world. Some of the more recent, exciting works now even going beyond that,

25:59.480 --> 26:03.360
it's now, well, maybe you don't actually have to fine tune the model at all. So people have

26:03.360 --> 26:09.360
done a lot of work using methods sometimes referred to as prompting or instruction where

26:09.360 --> 26:16.360
you can simply in natural language, perhaps with examples, perhaps with explicit instructions,

26:16.440 --> 26:22.240
just tell the model what you want it to do and it does it. Which, even as someone who's

26:22.240 --> 26:28.560
been working in natural language processing for 30 years, it actually just blows my mind

26:28.560 --> 26:35.560
how well this works. I guess I wasn't a decade ago thinking that in now we'd be able to

26:37.600 --> 26:44.600
just tell the model, I want you to summarize this piece of text here and there will be

26:46.360 --> 26:53.360
then summarize it. I think that's incredible. So we're in this very exciting time where

26:53.920 --> 27:00.600
a lot of new natural language capabilities are unfolding. I think there's just no doubt

27:00.600 --> 27:07.600
at all for the next couple of years the future of that is extremely bright as people work

27:08.080 --> 27:13.400
out different things and different ways to do things and people start to apply in different

27:13.440 --> 27:19.560
application areas, the kind of capabilities that have been unlocked with recent technological

27:19.560 --> 27:25.560
developments. There's always a question in technology is to sort of whether the curve

27:25.560 --> 27:30.400
keeps on heading steeply upwards or whether there's then some new things we have to discover

27:30.400 --> 27:37.240
how to do. It's been going up for quite a while. So hopefully extrapolation is always

27:37.240 --> 27:44.240
dangerous but we'll see. You mentioned writing prompts. It's still the NLP system, the large

27:45.400 --> 27:50.120
language model, what you want and it seems to magically do it. I'm curious, do you think

27:50.120 --> 27:56.320
prompt engineering is the path of the future where actually when I write these prompts

27:56.320 --> 28:01.360
I sometimes find it works miraculously and sometimes it's frustrating. The process of

28:01.360 --> 28:06.160
re-wording my instructions to tweak the wording to get it just right to generate the result

28:06.200 --> 28:10.880
I want. So do you think prompt engineering is the way of the future or do you think it's

28:10.880 --> 28:16.880
an intermediate hack until someone invents a better way to control the outputs of these

28:16.880 --> 28:23.880
systems? I think it's both. I think it will be the way of the future but I also think

28:26.840 --> 28:33.840
at the moment people are doing a lot of hacking around and re-wording to try and get things

28:34.760 --> 28:41.760
to work better. With any luck with a few more years of development that will start to go

28:42.600 --> 28:49.600
away. One way to think about the difference is in comparison to the kind of voice assistance

28:51.800 --> 28:58.800
or virtual assistance that are available on phones and speaker devices like Amazon Alexa

28:59.120 --> 29:06.120
these days. I think all of us have had the experience that present those devices aren't

29:07.200 --> 29:11.720
always great but if you know the right way to word things it will do something but if

29:11.720 --> 29:18.720
you use the wrong wording it won't. The difference with human beings is by and large you don't

29:18.720 --> 29:22.520
have to think about that. You can say what you want and it doesn't matter what word you

29:22.520 --> 29:28.720
choose. The other human being assuming it's someone who knows the same language etc. Well

29:28.920 --> 29:35.920
understand you and do what you want. I think and would hope that we'll start to see the

29:36.200 --> 29:41.040
same kind of progression with these models that at the moment fiddling around with the

29:41.040 --> 29:46.440
particular wording you use can make a very big difference to how well it works but hopefully

29:46.440 --> 29:51.880
in a few years time that just won't be true. You'll be able to use different wordings and

29:51.920 --> 29:58.920
it'll still work but the basic idea that we're moving into this age where actually human

29:59.440 --> 30:06.440
language will be able to be used as an instruction language to tell your computer what to do so

30:06.720 --> 30:13.720
instead of having to use menus and radio buttons and things like that or writing Python code

30:15.680 --> 30:20.240
instead of either of those things that you'll be able to say what you want in the computer

30:20.280 --> 30:26.800
or do it. I think that age is opening up in front of us that will continue to build and

30:26.800 --> 30:33.800
that will be hugely transformative. It feels like come a long ways but only much more to

30:34.720 --> 30:39.920
come and much more to go. Absolutely. In the development of NLP technology there's one

30:39.920 --> 30:43.640
thing I want to ask you and I suspect you and I may have different perspectives on this

30:43.720 --> 30:50.720
but in the last couple of decades the trend has been to rely less on rule based engineering

30:50.720 --> 30:57.240
and more on machine learning on data sometimes lots of data. Looking to the future where

30:57.240 --> 31:04.240
do you think that mix of hand coded constraints or other constraints explicit constraints versus

31:04.240 --> 31:08.040
you know let's get a neural network and throw lots of data at it. Where do you think that

31:08.120 --> 31:15.120
balance will fall? I think that there's no doubt that using learning from data is the

31:18.320 --> 31:25.320
way forward and what we're going to continue to do but I think there's still a space for

31:25.600 --> 31:32.600
models that have more structure, more inductive bias that have some kind of basis of exploiting

31:33.360 --> 31:40.200
the nature of language. So in recent years the model that's been enormously successful

31:40.200 --> 31:47.200
is the transform in your network and the transform in your network is essentially this huge association

31:47.200 --> 31:53.720
machine so it'll just suck associations from anywhere and look at two words and figure

31:53.720 --> 31:57.760
out which words release and which other words for all words. Yes so you use everything to

31:57.760 --> 32:03.520
predict anything and do it over and over again and you'll get anything you want and you know

32:03.520 --> 32:10.520
that's been incredibly, incredibly successful but it's been incredibly successful in the

32:10.560 --> 32:17.560
domain where you have humongous, humongous amounts of data right so that these transformer

32:17.800 --> 32:22.880
models for these large language models are now being trained on tens of billions of words

32:22.960 --> 32:29.400
of text. When I started off in statistical natural language processing and some of the

32:29.400 --> 32:35.640
traditional linguists used to complain about the fact that I was collecting statistics

32:35.640 --> 32:42.560
from 30 million words of newswire and building a predictive model and thought that was just

32:42.560 --> 32:49.560
not what linguistics was about. I felt I had a perfectly good answer which is that a human

32:50.560 --> 32:57.560
kid as their learning language they're exposed to actually well more than 30 million words

32:57.920 --> 33:02.880
of data but you know that kind of amount of data so you know the kind of amount of data

33:02.880 --> 33:09.640
we were using were perfectly reasonable amounts of data to be using to be you know not exactly

33:09.640 --> 33:14.240
trying to model human language acquisition but to be thinking about how we can learn

33:14.240 --> 33:21.240
about language from lots of data. But you know these modern transformers are now you

33:23.480 --> 33:30.240
know using already at least two orders of magnitude more data and you know most people

33:30.240 --> 33:37.000
think the way to get things to the next level is to use more still and make it three orders

33:37.000 --> 33:42.520
of magnitude and you know in one sense that scaling up strategy has been hugely effective

33:42.640 --> 33:47.080
so you know I don't blame anybody for saying let's make another order magnitude bigger and

33:47.080 --> 33:54.080
see what amazing things we can do but it also shows that human learning is just way way

33:57.160 --> 34:02.920
better in being able to extract a lot more information out of a quite limited amount

34:02.920 --> 34:08.840
of data and at that point you can have various hypotheses but I think it's reasonable to

34:08.880 --> 34:15.880
assume that human learning is somewhat structured towards the structure of the world and things

34:17.440 --> 34:22.440
that sees in the world and that allows it to learn more quickly from less data.

34:22.440 --> 34:26.840
Alright I'll move you on that. I think better learning algorithms, our current machine learning

34:26.840 --> 34:31.760
algorithms are much less efficient or makes much less efficient use of data and so there's

34:31.760 --> 34:37.440
way more data than any you know child and then I think whether the improved learning

34:37.480 --> 34:43.520
algorithms will be from linguistic like rules or whether it'll just be engineers engineering

34:43.520 --> 34:50.520
much more efficient versions of the transform or whatever comes after it. That will be traditional.

34:50.880 --> 34:57.880
I don't think it'll be by people explicitly putting traditional linguistic rules into

34:58.200 --> 35:05.200
the system. I don't think that's the way forward. On the other hand I mean you know I think

35:05.320 --> 35:12.320
what we're starting to see is models like these transformer models are actually discovering

35:12.320 --> 35:19.320
the structure of language themselves right so you know the broad facts of you know human

35:19.320 --> 35:24.080
language that you know English has the subject before the verb and the object afterwards

35:24.080 --> 35:29.040
whereas you know in Japanese that the verb at the end of the sentence and the subject

35:29.040 --> 35:33.360
and object are normally in that order before it that could be in the other order you know

35:33.360 --> 35:39.280
actually transformer models are learning these facts you can interrogate them and see

35:39.280 --> 35:43.360
that even though they were never explicitly told about subjects and objects that they

35:43.360 --> 35:49.360
know these notions so I think they you know they're discovering a lot else as well about

35:49.360 --> 35:54.960
language use and context and the meanings and senses of words and what is and isn't you

35:54.960 --> 36:00.840
know unpleasant language but part of what they're learning is the same kind of structure

36:00.840 --> 36:05.680
that linguists have laid out as the sort of structure of different human languages.

36:05.680 --> 36:11.800
So does it over many decades linguists discover certain things and by training on billions

36:11.800 --> 36:15.800
of words transformers are discovering the same things that linguists discovered in human

36:15.800 --> 36:21.680
language that's that's that's cool. So all this is really exciting progress in NLP driven

36:21.680 --> 36:26.880
by machine learning and by other things. To someone entering the field entering machine

36:26.920 --> 36:32.960
learning or AI or NLP there's just a lot going on. What advice would you have for someone

36:32.960 --> 36:41.080
wanting to break into machine learning? Yeah well it's a great time to break in. I think

36:41.080 --> 36:46.840
there's just no doubt at all that we're still in the early stages of seeing the impact of

36:46.840 --> 36:56.000
this new approach where effectively software computer science is being reinvented in on

36:56.000 --> 37:01.000
the basis of much more use of machine learning and the various other things that come away

37:01.000 --> 37:06.360
from that and then more generally across industries there are just lots of opportunities for more

37:06.360 --> 37:13.040
automation making more use of you know interpretation of human language material for me or in other

37:13.040 --> 37:22.120
areas like vision and robotics or the same kinds of things. So lots of possibilities.

37:22.240 --> 37:27.200
So you know at that point there's lots to do obviously and you want to get some kind

37:27.200 --> 37:33.840
of good foundation right so knowing some of the core technical methods of machine learning

37:33.840 --> 37:40.840
understanding ideas of how to build models from data look at losses do training diagnose

37:40.840 --> 37:48.200
errors all of these core things I mean that's definitely useful for natural language processing

37:48.280 --> 37:53.840
in particular some of those skills are completely relevant but then there are particular kinds

37:53.840 --> 37:59.200
of models that are commonly used including the transformer that we've talked about a lot

37:59.200 --> 38:03.400
today you definitely should know about transformers and indeed they're increasingly being used

38:03.400 --> 38:09.320
in every other part of machine learning as well for vision bioinformatics even robotics

38:09.320 --> 38:15.320
is now using transformers but beyond that I think it's also useful to learn something

38:15.360 --> 38:21.440
about human language and the nature of the problems that involves because I mean even

38:21.440 --> 38:27.920
though people aren't directly going to be encoding rules of human language into their

38:27.920 --> 38:35.400
computing system a sensitivity to sort of what kind of things happen in language and what to

38:35.400 --> 38:40.720
look out for and what you might want to model that's still a useful skill to have.

38:41.720 --> 38:47.920
And then in terms of learning the foundations learning about these concepts you had entered

38:47.920 --> 38:55.520
AI from a linguistic background and we now see people from you know all walks of life wanting

38:55.520 --> 39:01.360
to to start doing work in AI what are your thoughts on the preparation one should have

39:01.360 --> 39:06.800
or any thoughts on how to start from something other than computer science or AI so there are

39:06.840 --> 39:15.360
lots of places you can come from and vector across in different ways and we're seeing

39:15.360 --> 39:21.600
tons of people doing that that they're people who started off in different areas whether

39:21.600 --> 39:29.200
you know it was chemistry physics or even much further in field and people you know history

39:29.200 --> 39:34.160
whatever have started to look at machine learning I mean I think there are sort of two levels of

39:34.160 --> 39:41.480
answer there I mean one level of answer is you know one of the amazing transformations is that

39:41.480 --> 39:49.320
there's now these very good software packages for doing things with neural network models I mean

39:49.320 --> 39:55.400
this these software is really easy to use you don't actually need to understand a lot of highly

39:55.400 --> 40:01.360
technical stuff you've got need to have some kind of high-level conception about what is the idea of

40:01.360 --> 40:06.560
machine learning and how do I go about training a model and what should I look at in the numbers

40:06.560 --> 40:11.520
that are being printed out to see if it's working right but you know you don't actually have to

40:11.520 --> 40:16.960
have a higher degree to be able to build these models I mean and indeed what we're seeing is you

40:16.960 --> 40:22.360
know lots of high school students are getting into doing this because it's actually something that if

40:22.360 --> 40:29.640
you have some basic computer skills and a bit of programming you can pick up and do it's just way

40:29.680 --> 40:36.720
more accessible than lots of stuff that preceded a weather in AI or outside of AI and other areas

40:36.720 --> 40:42.280
you know like operating systems or security but you know if you want to get to a deeper level than

40:42.280 --> 40:49.320
that and actually want to understand more of what's going on I think you can't really get there if you

40:49.320 --> 40:57.400
don't have a certain mathematics foundation like at the end of the day that deep learning is based

40:57.520 --> 41:05.840
on calculus and you need to be optimizing functions and if you sort of don't have any background in

41:05.840 --> 41:13.520
that I think that sort of ends up as a wall at some point so you know. The math for machine learning

41:13.520 --> 41:20.960
and data science it does come in handy for some of the work we're going to do. Yeah so I think at

41:20.960 --> 41:28.720
some level if you're at the major in history or you know non-mathematical parts of psychology I

41:28.720 --> 41:34.360
actually have a good friend who yeah he you know learnt calculus in grad school because he was a

41:34.360 --> 41:39.200
psychologist and he'd never done it before and decided he wanted to start learning about these

41:39.200 --> 41:46.040
new kinds of models and decided it wasn't too late to be able to go and take a Cal course and so he

41:46.040 --> 41:53.800
did right so you know you do need to know some of that stuff but for lots of people if they've seen

41:53.800 --> 42:01.760
some of that before even if you're kind of rusty I think you can kind of get back in the zone and

42:01.760 --> 42:08.440
it doesn't really matter that you haven't you know done AI as an undergrad or machine learnings and

42:08.440 --> 42:13.560
things like that that you can really start to learn how to build these models and do things and you

42:13.560 --> 42:19.480
know really that's my own story right that despite the fact that they let me sit in the school of

42:19.480 --> 42:26.400
engineering at Stanford these days you know my background isn't as an engineer you know my PhDs

42:26.400 --> 42:34.880
and linguistics but you know I've sort of largely vectored across from having some knowledge of

42:34.880 --> 42:41.120
mathematics and linguistics and knowing some programming into sort of getting much more into

42:41.160 --> 42:43.120
building AI models.

42:43.120 --> 42:48.160
I was curious about something do you think the improved libraries and abstractions that are now

42:48.160 --> 42:53.680
available like coding frameworks like TensorFlow or PyTorch do you think that reduces the need to

42:53.680 --> 42:59.360
understand calculus because boy it's been it's been a while since I had to actually take a derivative

42:59.360 --> 43:04.720
in order to even implement or create a new neural network architecture because of automatic

43:04.720 --> 43:14.000
differentiation. Yeah I mean absolutely I mean so in the early days when we were doing things sort

43:14.000 --> 43:21.120
of 2010 to 2015 right for every model we built we were working out the derivatives by hand and then

43:21.120 --> 43:26.480
you know writing some code and whatever it was you know sometimes it was Python but sometimes it

43:26.480 --> 43:33.320
might have been Java or C to calculate these derivatives and checking that we got them right

43:33.320 --> 43:41.960
and so on where you know these days you actually don't need to know any of that to build deep

43:41.960 --> 43:47.000
learning models I mean this is actually something I think about been thinking about even with respect

43:47.000 --> 43:53.080
to my own natural language processing with deep learning class that I teach you know at the beginning

43:53.720 --> 44:01.960
we do still go through doing you know matrix calculus and making sure people know about

44:02.040 --> 44:09.640
Jacobians and things like that so that they understand what's being done in back propagation

44:09.640 --> 44:15.640
deep learning but you know there's sort of this sense in which that means that we just give them

44:15.640 --> 44:21.800
hell for two weeks you know sort of like boot camp or something to make them suffer and then we say

44:21.800 --> 44:26.760
oh but you do the rest of the class with PyTorch and they sort of never have to know any of that

44:26.760 --> 44:33.400
again right I you know there's always a question of how deep you want to go in technical foundations

44:33.400 --> 44:40.840
right you can keep on going right like does a computer scientist in the 2020s need to understand

44:40.840 --> 44:49.640
you know electronics and transistors or what happens in you know CPU well you know it's complicated

44:49.640 --> 44:54.840
I mean in various ways it is helpful to know some of that stuff I mean you know I know Andrew you

44:54.840 --> 45:01.080
were one of the pioneers and getting machine learning onto GPUs and well you know that sort of

45:01.080 --> 45:06.600
means you had to have some sense that there's this new hardware out there and it has some attributes

45:06.600 --> 45:13.080
of parallelism that means there's likely to be able to do something exciting so you know it is useful

45:13.080 --> 45:17.880
to have some broader knowledge and understanding and you know sometimes something breaks and if

45:17.880 --> 45:23.080
you have some deep knowledge you can understand why it broke but there's another sense in which you

45:23.080 --> 45:30.040
know most people have to take some things on trust and you can do most of what you want to do in

45:31.160 --> 45:37.160
neural network modeling these days without knowing calculus at all yeah that's a great point I feel

45:37.160 --> 45:42.360
like sometimes the reliability of the abstraction determines how often you need to go in to fix

45:42.360 --> 45:47.880
something that's broken so I actually my understanding of quantum physics is very weak I barely

45:47.880 --> 45:52.440
understand it so you could argue I don't understand how computers work because transistors are built

45:52.440 --> 45:57.800
in quantum physics but fortunately you know if something went wrong with transistors I've never

45:57.800 --> 46:06.040
had to go in to try to fix it so they're a bit hard to fix I think and so I think I think well

46:06.040 --> 46:11.240
another example you know the sort function their libraries are sort things and sometimes they actually

46:11.240 --> 46:15.960
don't work right swap in the memory or whatever and that's when if you really understand how the

46:15.960 --> 46:22.200
sort function works you can go in and fix it but then sometimes if we have abstractions libraries

46:22.280 --> 46:28.600
APIs are reliable enough then that is nice to those abstractions then diminishes them to understand

46:28.600 --> 46:33.720
some of the things that happen so it's an exciting world feels like you know we have giants building

46:33.720 --> 46:38.440
on the shoulders of giants and and all of these things are becoming more complex and more exciting

46:38.440 --> 46:45.960
every every every month yeah absolutely so thanks Chris that was really um interesting and inspiring

46:45.960 --> 46:52.040
and and I hope that to everyone watching this hearing Chris's own journey um to become a

46:52.040 --> 46:57.560
computer scientist and to become a leading maybe the leading NLP computer scientists as well as all

46:57.560 --> 47:04.520
of this exciting work having an NLP right now I hope that inspires you to jump into the sphere and

47:04.520 --> 47:10.360
take a go at it there's just a lot more work to be done collectively by our community than still

47:10.360 --> 47:13.880
so I think the more of us are working on this the better off the world will be

47:13.880 --> 47:25.720
so thanks a lot Chris it was really great having you thanks a lot Andrew it's been fun chatting

