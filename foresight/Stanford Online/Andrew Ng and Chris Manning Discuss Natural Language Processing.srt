1
00:00:00,000 --> 00:00:13,020
Hi, I'm delighted to be here with my old friend and collaborator, Professor Chris Manning.

2
00:00:13,020 --> 00:00:17,080
Chris has a very long and impressive bio, but just briefly, he is Professor of Computer

3
00:00:17,080 --> 00:00:22,080
Science at Stanford University and also the Director of the Stanford AI Lab.

4
00:00:22,080 --> 00:00:27,520
And he also has a distinction of being the most highly cited researcher in NLP or natural

5
00:00:27,520 --> 00:00:28,520
language processing.

6
00:00:28,920 --> 00:00:30,880
So really good to be here with you, Chris.

7
00:00:30,880 --> 00:00:32,560
Oh, good to get a chance to chat, Andrew.

8
00:00:33,640 --> 00:00:37,480
So, you know, we've known each other, collaborated for many years.

9
00:00:37,480 --> 00:00:41,880
And one interesting part of your background, I always thought, was that even though today

10
00:00:41,880 --> 00:00:46,360
you're a distinguished researcher in machine learning and NLP, you actually started off

11
00:00:46,360 --> 00:00:48,080
in a very different area.

12
00:00:48,080 --> 00:00:55,120
Your PhD, if I'm not correctly, was in linguistics, and you were studying the syntax of language.

13
00:00:55,560 --> 00:01:00,080
So how did you go from studying syntax to being an NLP researcher?

14
00:01:00,680 --> 00:01:02,480
So I can certainly tell you about that.

15
00:01:02,480 --> 00:01:06,720
But I should also point out that, you know, I'm still actually a professor of linguistics

16
00:01:06,720 --> 00:01:07,120
as well.

17
00:01:07,120 --> 00:01:08,920
I have a joint appointment at Stanford.

18
00:01:09,880 --> 00:01:14,160
And, you know, once in a blue moon, not very often, I do actually still teach some real

19
00:01:14,160 --> 00:01:18,320
linguistics as well as computer-involved natural language processing.

20
00:01:19,160 --> 00:01:27,040
So, you know, so starting out, I was very interested in human languages and how they

21
00:01:27,040 --> 00:01:32,240
work, how people understand them, how they are required.

22
00:01:32,240 --> 00:01:37,960
So I had this sort of appeal, I saw this appeal in human languages.

23
00:01:38,560 --> 00:01:46,680
But that equally led me to think about ideas that we now very much think about as machine

24
00:01:46,680 --> 00:01:48,960
learning or computational ideas.

25
00:01:48,960 --> 00:01:57,080
So two of the central ideas in human language, how do little children acquire human language?

26
00:01:57,080 --> 00:02:01,920
And for adults, well, we're just talking to each other now, and we pretty much understand

27
00:02:01,920 --> 00:02:02,640
each other.

28
00:02:02,640 --> 00:02:06,040
And, you know, that's actually an amazing thing how we managed to do that.

29
00:02:06,040 --> 00:02:08,200
So what kind of processing allows that?

30
00:02:08,200 --> 00:02:13,120
And so that early on got me interested in looking at machine learning.

31
00:02:13,120 --> 00:02:18,560
In fact, even before I'd made it to grad school, I'd started, you know, baby steps

32
00:02:18,560 --> 00:02:21,240
and learning machine learning coming off of those interests.

33
00:02:21,680 --> 00:02:23,960
Yeah, in fact, all human languages learn.

34
00:02:23,960 --> 00:02:26,560
You know, we had learned at some point in our lives to speak English.

35
00:02:26,560 --> 00:02:28,280
We'd grown up in a different place.

36
00:02:28,280 --> 00:02:30,360
We would learn a totally different language.

37
00:02:30,360 --> 00:02:35,880
So it's amazing to think how humans do that and now maybe machines learn language too.

38
00:02:36,240 --> 00:02:40,360
But so, so just, you know, tell us more about your journey.

39
00:02:40,360 --> 00:02:44,920
So you had a PhD in linguistics, and then, and then how did you?

40
00:02:45,320 --> 00:02:47,480
So there's some stuff before that as well.

41
00:02:47,640 --> 00:02:53,680
So, I mean, you know, when I was an undergrad, well, officially, I actually did three majors.

42
00:02:53,680 --> 00:02:58,800
This was in Australia, one in math, one in computer science and one in linguistics.

43
00:02:59,000 --> 00:03:04,480
Now, people get a slightly exaggerated sense of what that means if you're in an American

44
00:03:04,480 --> 00:03:09,000
context, because, you know, it'd be, I think, impossible to complete three majors

45
00:03:09,000 --> 00:03:11,080
or not undergrad at Stanford.

46
00:03:11,200 --> 00:03:15,840
But, you know, actually, where I was as an undergrad doing, I did an arts degree.

47
00:03:15,840 --> 00:03:18,040
So I could do whatever I wanted, like linguistics.

48
00:03:18,200 --> 00:03:21,720
You had to complete two majors to complete the arts degree.

49
00:03:21,720 --> 00:03:25,400
So, you know, it's sort of more like double majoring, maybe in US terms.

50
00:03:26,280 --> 00:03:30,160
You probably don't know this about me, but at Carnegie Mellon, I actually was a triple major.

51
00:03:30,760 --> 00:03:33,560
Math CS was once in the statistics and economics.

52
00:03:34,600 --> 00:03:36,560
That's great, we're both fellow triple majors.

53
00:03:37,040 --> 00:03:43,080
Yeah, so anyway, I did have background in interest in doing things with computer science.

54
00:03:44,040 --> 00:03:47,360
And so my interests were kind of mixed.

55
00:03:47,360 --> 00:03:51,080
And I mean, actually, you know, when I applied to grad schools, I mean, one of the places

56
00:03:51,080 --> 00:03:55,840
I applied to was Carnegie Mellon, because they were strong in computational linguistics,

57
00:03:55,840 --> 00:03:59,760
you know, and if I'd gone there, I would have been enrolled as a CS student.

58
00:04:00,200 --> 00:04:04,720
But I ended up at Stanford as a linguistics student, because at that time there wasn't

59
00:04:04,720 --> 00:04:08,480
any natural language processing in the CS department.

60
00:04:09,040 --> 00:04:14,280
But, you know, I was still interested in pursuing ideas in natural language processing.

61
00:04:14,280 --> 00:04:20,880
But at that point in the early 90s, things were just starting to change.

62
00:04:20,880 --> 00:04:29,600
But the bulk of natural language processing was rule-based, logical, declarative systems.

63
00:04:29,840 --> 00:04:35,480
But it was also in those years, at the beginning of the 90s, when there first started to be

64
00:04:35,480 --> 00:04:40,360
lots of human language material, text and speech available digitally.

65
00:04:40,360 --> 00:04:43,920
So this was really actually just before the World Wide Web exploded.

66
00:04:43,920 --> 00:04:49,640
But there had already started to be things like legal materials and newspaper articles

67
00:04:49,640 --> 00:04:55,600
and parliamentary handsaws, where you could at last get your hands on millions of words

68
00:04:55,720 --> 00:04:57,160
of human language.

69
00:04:57,160 --> 00:05:02,200
And it just seemed really clear that there had to be exciting things that you could do

70
00:05:02,200 --> 00:05:05,240
by working empirically from lots of human language.

71
00:05:05,240 --> 00:05:11,320
And that's what really sort of got me involved in a new kind of natural language processing

72
00:05:11,320 --> 00:05:13,440
that then led into my subsequent career.

73
00:05:14,000 --> 00:05:17,280
It sounds like your career was initially more linguistics.

74
00:05:17,280 --> 00:05:23,000
And if the rise of data and machine learning and empirical methods, it shifted to what NLP

75
00:05:23,000 --> 00:05:24,800
and machine learning and NLP?

76
00:05:25,360 --> 00:05:27,720
Yeah, I mean, it absolutely certainly shifted.

77
00:05:27,720 --> 00:05:33,200
And I've certainly sort of shifted much more to doing both natural language processing

78
00:05:33,200 --> 00:05:35,240
and machine learning models.

79
00:05:35,240 --> 00:05:38,840
But to some extent, the balances varied.

80
00:05:38,840 --> 00:05:41,280
But I've sort of been with that as of while.

81
00:05:41,280 --> 00:05:43,800
You know, actually, there's an undergrad.

82
00:05:43,800 --> 00:05:50,280
For my undergrad on this thesis, it was sort of learning the forms of words.

83
00:05:50,280 --> 00:05:55,520
So how you can, which became a famous problem of sort of learning past tense of English

84
00:05:55,520 --> 00:05:58,560
verbs in the early connectionist literature.

85
00:05:58,560 --> 00:06:02,720
And I was trying to sort of learn paradigms of forms of verbs.

86
00:06:02,720 --> 00:06:09,720
And I was learning rules for the different forms using the C4.5 decision tree learning

87
00:06:09,720 --> 00:06:11,200
algorithm, if you remember that.

88
00:06:12,440 --> 00:06:12,760
Yes.

89
00:06:13,800 --> 00:06:14,240
Right.

90
00:06:14,400 --> 00:06:15,000
Good times.

91
00:06:15,000 --> 00:06:15,200
Yeah.

92
00:06:15,200 --> 00:06:17,520
And it's surprisingly non-intuitive, right?

93
00:06:17,520 --> 00:06:19,880
How going from present tense to past tense.

94
00:06:20,960 --> 00:06:25,320
From, I don't know, run to run and all the other weird special cases can be.

95
00:06:25,320 --> 00:06:25,680
Yeah.

96
00:06:27,400 --> 00:06:31,200
Hey, so we talked a bunch about NLP, natural language processing.

97
00:06:31,200 --> 00:06:38,680
So for some of the learners, pick up machine learning for the first time, can you say, what is NLP?

98
00:06:38,680 --> 00:06:39,960
Sure, absolutely.

99
00:06:39,960 --> 00:06:43,400
Yes, NLP stands for natural language processing.

100
00:06:43,400 --> 00:06:47,920
Another word this term that's sometimes used for that is computational linguistics.

101
00:06:47,920 --> 00:06:49,000
It's the same thing.

102
00:06:49,040 --> 00:06:52,800
I mean, natural language processing is actually a weird term, right?

103
00:06:52,800 --> 00:06:56,160
So it means that we're doing things with human languages.

104
00:06:56,160 --> 00:07:00,760
So you have to have the conception that you're enough of a computer scientist that when you

105
00:07:00,760 --> 00:07:05,360
say language, you think in your brain programming language, and therefore you need to say natural

106
00:07:05,360 --> 00:07:09,320
language to mean that you're talking about the languages that human beings use.

107
00:07:10,040 --> 00:07:15,680
So overall natural language processing is doing anything intelligent with human languages.

108
00:07:15,680 --> 00:07:23,480
So in one sense, that breaks down into understanding human languages, producing human languages,

109
00:07:23,480 --> 00:07:25,600
acquiring human languages.

110
00:07:25,600 --> 00:07:29,560
Though people also often think about it in terms of different applications.

111
00:07:29,560 --> 00:07:35,560
And so then you might think about things like machine translation or doing question answering

112
00:07:35,560 --> 00:07:42,120
or generating advertising copy or summarization.

113
00:07:42,120 --> 00:07:47,880
There are so many different tasks that people work on with particular goals in mind where

114
00:07:47,880 --> 00:07:49,640
you do things with human language.

115
00:07:49,640 --> 00:07:55,280
And there's a lot of natural language processing because so much of what the world works on

116
00:07:55,280 --> 00:08:01,560
our human world is dealt with and transmitted in terms of human language material.

117
00:08:02,880 --> 00:08:09,680
So because of all of these applications or even a web stage, most of us use NLP many, many times.

118
00:08:09,680 --> 00:08:15,600
Yeah, you're right. In some sense, the biggest application of natural language is web search,

119
00:08:15,600 --> 00:08:18,440
right? That's really the big one.

120
00:08:18,440 --> 00:08:22,520
I mean, traditionally, it was a kind of a simple one, right?

121
00:08:22,520 --> 00:08:27,600
But in the good old days, it was, you know, there were various waiting factors and so on,

122
00:08:27,600 --> 00:08:33,440
but it was mainly sort of matching keywords than your search terms and then some factors

123
00:08:33,440 --> 00:08:35,120
about the quality of the page.

124
00:08:35,120 --> 00:08:39,520
It didn't really feel like language understanding, but that's really been changing

125
00:08:39,520 --> 00:08:45,840
over the years. So these days, you'll often, if you ask a question to a search engine,

126
00:08:45,840 --> 00:08:51,760
it'll give you, you know, an answer box where it has extracted a piece of text and puts

127
00:08:51,760 --> 00:08:55,720
what it thinks is the answer in bold or color or something like that, which is then this

128
00:08:55,720 --> 00:08:57,720
task of question answering.

129
00:08:57,720 --> 00:09:00,440
And then it's really a natural language understanding task.

130
00:09:00,440 --> 00:09:05,600
Yeah, yeah. And I feel like in addition to web searches, maybe the big one, you know,

131
00:09:05,680 --> 00:09:11,080
when we're going to a online shopping website or a movie website and typing in what we want

132
00:09:11,080 --> 00:09:16,440
and doing a website search on a much smaller website than, you know, the big search engines,

133
00:09:16,440 --> 00:09:22,240
that also increasingly uses sophisticated NLP algorithms and it's also creating quite

134
00:09:22,240 --> 00:09:27,680
a lot of value. Maybe to you is not, you know, the real NLP, but it still seems very valuable.

135
00:09:27,680 --> 00:09:32,400
I agree. It's very valuable. And there are, you know, lots of interesting problems in

136
00:09:32,400 --> 00:09:37,560
any e-commerce website with search, very difficult problems, actually, when people

137
00:09:37,560 --> 00:09:41,840
describe the kind of goods they want and you need to be trying to match it to products

138
00:09:41,840 --> 00:09:45,640
that are available. That isn't an easy problem at all, it turns out.

139
00:09:45,640 --> 00:09:52,640
Yeah, that's true. Yeah. So over the last couple of decades, NLP has gone through a major

140
00:09:52,640 --> 00:09:58,440
shift from more of the rule-based techniques that you alluded to just now to using really

141
00:09:58,480 --> 00:10:05,480
machine learning much more pervasively. And so you were one of the people at, you know,

142
00:10:05,480 --> 00:10:10,240
leading parts of that charge and seeing every step of the way of creating some of the steps

143
00:10:10,240 --> 00:10:14,880
as it happened. Can you say a bit about that process and what you saw?

144
00:10:14,880 --> 00:10:21,000
Sure, absolutely. Yeah. So when I started off as an undergrad and grad student, really

145
00:10:21,040 --> 00:10:28,040
most of natural language processing was done by hand-built systems, which variously used

146
00:10:29,720 --> 00:10:36,720
rules and inference procedures to sort of try and build up paths and an understanding

147
00:10:37,040 --> 00:10:42,280
of a piece of text. What's an example of a rule or an inference system?

148
00:10:42,280 --> 00:10:49,280
So, you know, a rule could be part of the structure of human language, like English

149
00:10:49,680 --> 00:10:54,400
sentence normally consists of a subject noun phrase followed by a verb and an object noun

150
00:10:54,400 --> 00:11:00,040
phrase, and that gives you some ideas to how to understand the meaning of the sentence.

151
00:11:00,040 --> 00:11:07,040
But it might also be saying something about how to interpret a word, so that a lot of

152
00:11:07,160 --> 00:11:14,160
words in English are very ambiguous. But if you have something like the word star and

153
00:11:15,160 --> 00:11:20,280
it's in the context of a movie, then it's probably referring to a human being on this

154
00:11:20,280 --> 00:11:26,240
astronomical object. And in those days, people tried to deal with things like that using

155
00:11:26,240 --> 00:11:33,240
rules of that sort. That doesn't seem very likely to work to us these days, but, you

156
00:11:34,240 --> 00:11:41,240
know, once upon a time that was pretty standard. And so it was only when lots of digital text

157
00:11:42,160 --> 00:11:46,480
and speech started to become available that it really seemed like there was this different

158
00:11:46,480 --> 00:11:53,240
way that instead we could start calculating statistics over human language material and

159
00:11:53,240 --> 00:12:00,240
building machine learning models. And so that was the first thing that I got into in the

160
00:12:02,200 --> 00:12:09,200
sort of mid to late 1990s. And so, you know, the first area where I started doing lots

161
00:12:09,320 --> 00:12:14,600
of research and publishing papers and getting well known is building what in the early days

162
00:12:14,600 --> 00:12:20,440
we often called statistical natural language processing. But it later merged into in general

163
00:12:20,440 --> 00:12:26,040
probabilistic approaches to artificial intelligence and machine learning. And that sort of took

164
00:12:26,040 --> 00:12:33,040
us through to approximately 2010, let's say. And that's roughly when the new interest in

165
00:12:34,040 --> 00:12:41,040
deep learning using large artificial neural networks started to take off. For my interest

166
00:12:42,360 --> 00:12:47,840
in that, I really have you to thank Andrew, because at this stage, Andrew was still full

167
00:12:47,840 --> 00:12:53,840
time at Stanford, and he was in the office next door to me, and he was really excited

168
00:12:53,840 --> 00:12:59,160
about the new things that were happening in the area of deep learning. I guess anyone

169
00:12:59,200 --> 00:13:03,200
who walked into his office, he'd tell them, oh, it's so exciting what's happening now,

170
00:13:03,200 --> 00:13:08,720
and neural networks should start looking at that. And so, you know, that was really the

171
00:13:08,720 --> 00:13:15,720
impetus that got me pretty early on involved in looking at things in neural networks. I

172
00:13:17,520 --> 00:13:23,000
had actually seen a bit of it before. So while I was a grad student here, actually Dave Ruhmelhardt

173
00:13:23,000 --> 00:13:28,200
was at Stanford and Psych, and I'd taken his neural networks class. And so, you know,

174
00:13:28,240 --> 00:13:33,560
I'd seen some of that, but it hadn't actually really been what I'd gotten into for my own

175
00:13:33,560 --> 00:13:36,880
research. So around.

176
00:13:36,880 --> 00:13:39,880
I didn't know that. Thank you.

177
00:13:39,880 --> 00:13:42,880
And then we wound up, you know, supervising some students together.

178
00:13:42,880 --> 00:13:43,880
Yeah, absolutely.

179
00:13:43,880 --> 00:13:48,880
But I'd love to hear the rise of also deep learning in NLP. What are the GCs?

180
00:13:48,880 --> 00:13:55,880
Yeah. So starting about 2010, yeah, me, students started to do the first papers in

181
00:13:58,880 --> 00:14:03,920
deep learning aimed at NLP conferences. You know, it's always hard when you're trying

182
00:14:03,920 --> 00:14:10,680
to do something new. We had exactly the same experiences that people 15 or so years earlier

183
00:14:10,680 --> 00:14:15,760
had had when they started trying to do statistical NLP of when there's an established way of

184
00:14:15,760 --> 00:14:20,360
doing things. It's really hard to push out new ideas. So really some of our first papers

185
00:14:20,360 --> 00:14:27,160
were rejected from conferences and instead appeared at machine learning conferences or

186
00:14:27,160 --> 00:14:32,240
deep learning workshops. But very quickly that started to change and people got super

187
00:14:32,240 --> 00:14:39,240
interested in neural network ideas. But I sort of feel like the neural network period

188
00:14:39,600 --> 00:14:46,600
which started effectively about 2010 itself divides in two. Because for the first period,

189
00:14:47,600 --> 00:14:54,680
let's basically say it's till 2018, we showed a lot of success at building neural networks

190
00:14:54,720 --> 00:15:00,720
for all sorts of tasks. We built them for syntactic parsing and sentiment analysis and

191
00:15:00,720 --> 00:15:07,720
what else to question answering. But it was sort of like we were doing the same thing

192
00:15:08,600 --> 00:15:14,480
that we used to do with other kinds of machine learning models, except we now had a better

193
00:15:14,480 --> 00:15:19,760
machine learning model. And we were sort of instead of training up a logistic regression

194
00:15:19,800 --> 00:15:24,400
or a support vector machine, we were still doing the same kind of sentiment analysis

195
00:15:24,400 --> 00:15:30,640
task but now we're doing it with a neural network. So I think looking back now in some

196
00:15:30,640 --> 00:15:37,640
sense, the bigger change came around 2018 because that was when the idea of well we

197
00:15:39,040 --> 00:15:46,040
could just start with a large amount of human language material and build large self-supervised

198
00:15:47,040 --> 00:15:54,040
models. So that was models then like BERT and GPT and successor models to that. And

199
00:15:54,960 --> 00:16:00,760
they could just sort of acquire from word prediction over a huge amount of text this

200
00:16:00,760 --> 00:16:07,760
amazing knowledge of human languages. I think really probably that's going to be viewed

201
00:16:07,760 --> 00:16:14,280
in retrospect as the bigger kind of cut point where the way things were done really changed.

202
00:16:14,760 --> 00:16:19,760
Yeah, I think there is that trend for the large language models, learning from math

203
00:16:19,760 --> 00:16:24,640
and the mouse and data. I think even to lead up to that, there was one of your research

204
00:16:24,640 --> 00:16:31,640
papers that really slightly blew my mind, which is a glove paper. So because with word

205
00:16:32,160 --> 00:16:37,720
embeddings where you learn a vector of numbers to represent a word using a neural network,

206
00:16:37,720 --> 00:16:44,120
that was quite mind blowing for me. And then the glove work that you did really

207
00:16:44,160 --> 00:16:48,000
cleaned up the math, made it so much simpler. And then I remember reading I said, oh, that's

208
00:16:48,000 --> 00:16:53,160
all there is to it. And then you can learn these really surprisingly detailed representations

209
00:16:53,160 --> 00:16:57,440
of the computer learns real nuances of what words mean.

210
00:16:57,440 --> 00:17:02,840
Absolutely. Yeah, so I should give a little bit of credit to others. Other people also

211
00:17:02,840 --> 00:17:09,800
worked on some similar ideas, including Renan Colbert and Jace Weston and Tom Ostermeek

212
00:17:10,000 --> 00:17:16,760
and colleagues at Google. But the glove word vectors is one of the very prominent systems

213
00:17:16,760 --> 00:17:21,960
of word vectors. So these word vectors already did, yeah, you're right, illustrate this idea

214
00:17:21,960 --> 00:17:26,560
of using self-supervised learning that we just took massive amounts of text, and then

215
00:17:26,560 --> 00:17:33,160
we could build these models that knew an enormous amount about the meaning of words. I mean,

216
00:17:33,160 --> 00:17:40,160
it's still something I sort of show people every year in the first lecture of my NLP class,

217
00:17:41,440 --> 00:17:48,440
because it's something simple, but it actually just works so surprisingly well. You can do

218
00:17:48,440 --> 00:17:54,600
this sort of simple modeling of trying to predict a word given the words in the context.

219
00:17:54,600 --> 00:17:59,560
And simply by sort of running the math of learning to do those predictions well, you

220
00:17:59,600 --> 00:18:04,200
learn all these things about word meaning, and you can do these really nice patterns

221
00:18:04,200 --> 00:18:11,200
of similar word meaning or analogies of something like, you know, pencil is to drawing as paint

222
00:18:12,800 --> 00:18:19,800
brush is to, and it'll say painting, right? That it's sort of already showing just a lot

223
00:18:20,000 --> 00:18:27,000
of successful learning. So that was the precursor to what then got developed to the next stage

224
00:18:28,000 --> 00:18:34,000
with things like Burton GPT, where it wasn't just meanings of individual words, but meanings

225
00:18:34,760 --> 00:18:36,880
of whole pieces of text and context.

226
00:18:36,880 --> 00:18:41,880
Yeah, so I thought it was amazing that you can, you know, take a small neural network

227
00:18:41,880 --> 00:18:47,720
or some model and then give it lots of English sentences or some of the language and hide

228
00:18:47,720 --> 00:18:52,240
the word, ask it to predict what is the word that I just hit, and that allows it to learn

229
00:18:52,280 --> 00:18:58,780
these analogies and these very deep, what you would think are really deep things behind

230
00:18:58,780 --> 00:19:05,280
the meaning of the words. And then, you know, 2018, maybe there's other infection point.

231
00:19:05,280 --> 00:19:06,800
What happened after that?

232
00:19:06,800 --> 00:19:13,800
Yeah, so I mean, in 2018, that was the point in which, well, sort of really two things happened.

233
00:19:14,000 --> 00:19:21,000
One thing is that people, well, really in 2017, had developed this new neural architecture,

234
00:19:22,760 --> 00:19:29,760
which was much more scalable onto modern parallel GPUs, and so that was the transformer architecture.

235
00:19:31,000 --> 00:19:36,360
The second part of it, though, was, you know, maybe people rediscovered because it was using

236
00:19:36,360 --> 00:19:41,960
the same trick as the glove model, that if you have the task of say, of just predicting

237
00:19:41,960 --> 00:19:48,200
a word given a context, either a context on both sides of it or the preceding words,

238
00:19:48,200 --> 00:19:55,200
that that just turns out to be an amazing learning task. And that surprises a lot of

239
00:19:55,200 --> 00:20:02,040
people, and a lot of the time you see discussions where people say disparaging things of, you

240
00:20:02,040 --> 00:20:06,760
know, this is nothing interesting is happening, all it's doing is statistics to predict which

241
00:20:06,840 --> 00:20:13,840
word is most likely to come after the preceding words. And I think the really interesting

242
00:20:13,840 --> 00:20:20,680
thing is that that's true, but it's not true. I mean, because, yes, what the task is, is

243
00:20:20,680 --> 00:20:27,440
you're predicting the next word given preceding words. But the really interesting thing is,

244
00:20:27,440 --> 00:20:34,440
if you want to do that task really as well as possible, then it actually helps to understand

245
00:20:35,200 --> 00:20:41,200
the whole of the rest of the sentence and know who's doing what to who in what's in the sentence.

246
00:20:41,200 --> 00:20:48,200
But more than that, it also helps to understand the world. Because if your, your text is going

247
00:20:48,800 --> 00:20:55,800
something along the lines of, you know, the currency used in Fiji is that, well, you need

248
00:20:57,360 --> 00:21:02,920
to have some world knowledge to know what the right answer to that is. And so good models

249
00:21:02,960 --> 00:21:09,960
at doing this learn both to follow the structure of sentences and their meaning and to know

250
00:21:11,400 --> 00:21:16,800
facts about the world also that they can predict. And therefore this turns into what

251
00:21:16,800 --> 00:21:22,240
sometimes referred to as an AI complete task, right? That you really need, there's nothing

252
00:21:22,240 --> 00:21:29,080
that can't actually be useful in answering this, what word comes next sense, right? You

253
00:21:29,080 --> 00:21:36,080
know, you can be in the World Cup semifinals, the teams are, and you need to know something

254
00:21:36,600 --> 00:21:39,520
about soccer to be giving the right answer.

255
00:21:39,520 --> 00:21:44,720
AI completes this funny concept, or is this idea that you can solve this one problem,

256
00:21:44,720 --> 00:21:50,400
you can solve, you know, everything in AI or kind of make an analogy to NP complete problems

257
00:21:50,400 --> 00:21:55,640
from the theory of computing. What do you think? Do you think predicting the next word

258
00:21:55,680 --> 00:21:58,640
is AI complete? I have very mixed feelings about that myself.

259
00:21:58,640 --> 00:22:03,640
I shall, I shall go ahead and say, I don't think it's true. So just what do you think?

260
00:22:03,640 --> 00:22:10,640
I think it's not quite true because I think there are other kinds of things that human

261
00:22:13,200 --> 00:22:19,080
beings manage to work out. You know, there are human beings that have clever insights

262
00:22:19,080 --> 00:22:25,600
in mathematics, or there are human beings who are looking at something that's much more

263
00:22:25,760 --> 00:22:32,560
you know, three dimensional, real world puzzle of sort of figuring out how to do something

264
00:22:32,560 --> 00:22:39,560
mechanical or something like that, and that's just not a language problem. But on the other

265
00:22:40,960 --> 00:22:47,960
hand, I mean, I think language gets closer to universality than some people think as

266
00:22:48,920 --> 00:22:55,920
well, because, you know, we live in this 3D world and operate in it with our bodies and

267
00:22:58,320 --> 00:23:05,320
our feelings and other creatures and artifacts around it, and you could think, well, not

268
00:23:05,520 --> 00:23:12,520
much of that is in language at all. But actually, just about all of this stuff, we think about,

269
00:23:13,400 --> 00:23:19,440
we talk about, we write about it in language, we can describe the positions of things relative

270
00:23:19,440 --> 00:23:24,680
to each other in language. So a surprising amount of the other parts of the world are

271
00:23:24,680 --> 00:23:29,640
seen in reflection in language, and therefore you're learning about all of them too when

272
00:23:29,640 --> 00:23:31,600
you learn about language use.

273
00:23:31,600 --> 00:23:37,600
You learn about, you know, one aspect of a lot of things, even if things like, how do

274
00:23:38,280 --> 00:23:45,280
you ride a bicycle? You don't really learn how to ride a bicycle, but you learn some

275
00:23:45,480 --> 00:23:49,880
aspects of what it involves, that you need to balance and you have to have your feet

276
00:23:49,880 --> 00:23:54,680
on the pedals and push them and all of that kind of things, yeah.

277
00:23:54,680 --> 00:24:01,200
And so with this trend in NLP, the large language models has been very exciting for the last

278
00:24:01,200 --> 00:24:06,160
several years. What are your thoughts on where all this will go?

279
00:24:06,520 --> 00:24:13,520
Well, I mean, yeah, so it's just been amazingly successful and exciting, right? So we haven't

280
00:24:15,440 --> 00:24:18,760
really explained all the details, right? So there's a first stage of learning these

281
00:24:18,760 --> 00:24:25,760
large language models where the task is just to predict the next word and you do that billions

282
00:24:25,760 --> 00:24:32,360
of times over a very large piece of text, and behold, you get this large neural network,

283
00:24:32,400 --> 00:24:37,240
which is just a really useful artifact for all sorts of natural language processing tasks.

284
00:24:37,240 --> 00:24:41,520
But then you still actually have to do something with it if you want to do a particular task,

285
00:24:41,520 --> 00:24:48,520
whether that's question answering or summarization or detecting toxic content in social media

286
00:24:48,520 --> 00:24:52,480
or something like that. And at that point, there's a choice of things that you could

287
00:24:52,480 --> 00:24:58,840
do with it. The traditional answer was then you had a particular task, like say, detecting

288
00:24:58,840 --> 00:25:04,480
toxic comments in social media, and you'd take some supervised data for that, and then

289
00:25:04,480 --> 00:25:11,480
you'd fine tune the language model to answer that classification task. But you were enormously

290
00:25:11,480 --> 00:25:17,520
helped by having this base of this large self-supervised model because it meant that the model had

291
00:25:17,520 --> 00:25:23,280
enormous knowledge of language and it could generalize very quickly. So unlike the sort

292
00:25:23,320 --> 00:25:30,320
of the standard old days of supervised learning where it was kind of, well, if you give me

293
00:25:31,000 --> 00:25:36,840
10,000 labeled examples, I might be able to produce a halfway decent model for you, but

294
00:25:36,840 --> 00:25:42,520
if you give me 50,000 labeled examples, it'll be a lot better. It's sort of turned into this

295
00:25:42,520 --> 00:25:49,040
world of, well, if you give me 100 labeled examples and I'm fine tuning a large language

296
00:25:49,040 --> 00:25:53,960
model, I'll be able to do great, better than I would be able to do with the 50,000 examples

297
00:25:53,960 --> 00:25:59,480
in the old world. Some of the more recent, exciting works now even going beyond that,

298
00:25:59,480 --> 00:26:03,360
it's now, well, maybe you don't actually have to fine tune the model at all. So people have

299
00:26:03,360 --> 00:26:09,360
done a lot of work using methods sometimes referred to as prompting or instruction where

300
00:26:09,360 --> 00:26:16,360
you can simply in natural language, perhaps with examples, perhaps with explicit instructions,

301
00:26:16,440 --> 00:26:22,240
just tell the model what you want it to do and it does it. Which, even as someone who's

302
00:26:22,240 --> 00:26:28,560
been working in natural language processing for 30 years, it actually just blows my mind

303
00:26:28,560 --> 00:26:35,560
how well this works. I guess I wasn't a decade ago thinking that in now we'd be able to

304
00:26:37,600 --> 00:26:44,600
just tell the model, I want you to summarize this piece of text here and there will be

305
00:26:46,360 --> 00:26:53,360
then summarize it. I think that's incredible. So we're in this very exciting time where

306
00:26:53,920 --> 00:27:00,600
a lot of new natural language capabilities are unfolding. I think there's just no doubt

307
00:27:00,600 --> 00:27:07,600
at all for the next couple of years the future of that is extremely bright as people work

308
00:27:08,080 --> 00:27:13,400
out different things and different ways to do things and people start to apply in different

309
00:27:13,440 --> 00:27:19,560
application areas, the kind of capabilities that have been unlocked with recent technological

310
00:27:19,560 --> 00:27:25,560
developments. There's always a question in technology is to sort of whether the curve

311
00:27:25,560 --> 00:27:30,400
keeps on heading steeply upwards or whether there's then some new things we have to discover

312
00:27:30,400 --> 00:27:37,240
how to do. It's been going up for quite a while. So hopefully extrapolation is always

313
00:27:37,240 --> 00:27:44,240
dangerous but we'll see. You mentioned writing prompts. It's still the NLP system, the large

314
00:27:45,400 --> 00:27:50,120
language model, what you want and it seems to magically do it. I'm curious, do you think

315
00:27:50,120 --> 00:27:56,320
prompt engineering is the path of the future where actually when I write these prompts

316
00:27:56,320 --> 00:28:01,360
I sometimes find it works miraculously and sometimes it's frustrating. The process of

317
00:28:01,360 --> 00:28:06,160
re-wording my instructions to tweak the wording to get it just right to generate the result

318
00:28:06,200 --> 00:28:10,880
I want. So do you think prompt engineering is the way of the future or do you think it's

319
00:28:10,880 --> 00:28:16,880
an intermediate hack until someone invents a better way to control the outputs of these

320
00:28:16,880 --> 00:28:23,880
systems? I think it's both. I think it will be the way of the future but I also think

321
00:28:26,840 --> 00:28:33,840
at the moment people are doing a lot of hacking around and re-wording to try and get things

322
00:28:34,760 --> 00:28:41,760
to work better. With any luck with a few more years of development that will start to go

323
00:28:42,600 --> 00:28:49,600
away. One way to think about the difference is in comparison to the kind of voice assistance

324
00:28:51,800 --> 00:28:58,800
or virtual assistance that are available on phones and speaker devices like Amazon Alexa

325
00:28:59,120 --> 00:29:06,120
these days. I think all of us have had the experience that present those devices aren't

326
00:29:07,200 --> 00:29:11,720
always great but if you know the right way to word things it will do something but if

327
00:29:11,720 --> 00:29:18,720
you use the wrong wording it won't. The difference with human beings is by and large you don't

328
00:29:18,720 --> 00:29:22,520
have to think about that. You can say what you want and it doesn't matter what word you

329
00:29:22,520 --> 00:29:28,720
choose. The other human being assuming it's someone who knows the same language etc. Well

330
00:29:28,920 --> 00:29:35,920
understand you and do what you want. I think and would hope that we'll start to see the

331
00:29:36,200 --> 00:29:41,040
same kind of progression with these models that at the moment fiddling around with the

332
00:29:41,040 --> 00:29:46,440
particular wording you use can make a very big difference to how well it works but hopefully

333
00:29:46,440 --> 00:29:51,880
in a few years time that just won't be true. You'll be able to use different wordings and

334
00:29:51,920 --> 00:29:58,920
it'll still work but the basic idea that we're moving into this age where actually human

335
00:29:59,440 --> 00:30:06,440
language will be able to be used as an instruction language to tell your computer what to do so

336
00:30:06,720 --> 00:30:13,720
instead of having to use menus and radio buttons and things like that or writing Python code

337
00:30:15,680 --> 00:30:20,240
instead of either of those things that you'll be able to say what you want in the computer

338
00:30:20,280 --> 00:30:26,800
or do it. I think that age is opening up in front of us that will continue to build and

339
00:30:26,800 --> 00:30:33,800
that will be hugely transformative. It feels like come a long ways but only much more to

340
00:30:34,720 --> 00:30:39,920
come and much more to go. Absolutely. In the development of NLP technology there's one

341
00:30:39,920 --> 00:30:43,640
thing I want to ask you and I suspect you and I may have different perspectives on this

342
00:30:43,720 --> 00:30:50,720
but in the last couple of decades the trend has been to rely less on rule based engineering

343
00:30:50,720 --> 00:30:57,240
and more on machine learning on data sometimes lots of data. Looking to the future where

344
00:30:57,240 --> 00:31:04,240
do you think that mix of hand coded constraints or other constraints explicit constraints versus

345
00:31:04,240 --> 00:31:08,040
you know let's get a neural network and throw lots of data at it. Where do you think that

346
00:31:08,120 --> 00:31:15,120
balance will fall? I think that there's no doubt that using learning from data is the

347
00:31:18,320 --> 00:31:25,320
way forward and what we're going to continue to do but I think there's still a space for

348
00:31:25,600 --> 00:31:32,600
models that have more structure, more inductive bias that have some kind of basis of exploiting

349
00:31:33,360 --> 00:31:40,200
the nature of language. So in recent years the model that's been enormously successful

350
00:31:40,200 --> 00:31:47,200
is the transform in your network and the transform in your network is essentially this huge association

351
00:31:47,200 --> 00:31:53,720
machine so it'll just suck associations from anywhere and look at two words and figure

352
00:31:53,720 --> 00:31:57,760
out which words release and which other words for all words. Yes so you use everything to

353
00:31:57,760 --> 00:32:03,520
predict anything and do it over and over again and you'll get anything you want and you know

354
00:32:03,520 --> 00:32:10,520
that's been incredibly, incredibly successful but it's been incredibly successful in the

355
00:32:10,560 --> 00:32:17,560
domain where you have humongous, humongous amounts of data right so that these transformer

356
00:32:17,800 --> 00:32:22,880
models for these large language models are now being trained on tens of billions of words

357
00:32:22,960 --> 00:32:29,400
of text. When I started off in statistical natural language processing and some of the

358
00:32:29,400 --> 00:32:35,640
traditional linguists used to complain about the fact that I was collecting statistics

359
00:32:35,640 --> 00:32:42,560
from 30 million words of newswire and building a predictive model and thought that was just

360
00:32:42,560 --> 00:32:49,560
not what linguistics was about. I felt I had a perfectly good answer which is that a human

361
00:32:50,560 --> 00:32:57,560
kid as their learning language they're exposed to actually well more than 30 million words

362
00:32:57,920 --> 00:33:02,880
of data but you know that kind of amount of data so you know the kind of amount of data

363
00:33:02,880 --> 00:33:09,640
we were using were perfectly reasonable amounts of data to be using to be you know not exactly

364
00:33:09,640 --> 00:33:14,240
trying to model human language acquisition but to be thinking about how we can learn

365
00:33:14,240 --> 00:33:21,240
about language from lots of data. But you know these modern transformers are now you

366
00:33:23,480 --> 00:33:30,240
know using already at least two orders of magnitude more data and you know most people

367
00:33:30,240 --> 00:33:37,000
think the way to get things to the next level is to use more still and make it three orders

368
00:33:37,000 --> 00:33:42,520
of magnitude and you know in one sense that scaling up strategy has been hugely effective

369
00:33:42,640 --> 00:33:47,080
so you know I don't blame anybody for saying let's make another order magnitude bigger and

370
00:33:47,080 --> 00:33:54,080
see what amazing things we can do but it also shows that human learning is just way way

371
00:33:57,160 --> 00:34:02,920
better in being able to extract a lot more information out of a quite limited amount

372
00:34:02,920 --> 00:34:08,840
of data and at that point you can have various hypotheses but I think it's reasonable to

373
00:34:08,880 --> 00:34:15,880
assume that human learning is somewhat structured towards the structure of the world and things

374
00:34:17,440 --> 00:34:22,440
that sees in the world and that allows it to learn more quickly from less data.

375
00:34:22,440 --> 00:34:26,840
Alright I'll move you on that. I think better learning algorithms, our current machine learning

376
00:34:26,840 --> 00:34:31,760
algorithms are much less efficient or makes much less efficient use of data and so there's

377
00:34:31,760 --> 00:34:37,440
way more data than any you know child and then I think whether the improved learning

378
00:34:37,480 --> 00:34:43,520
algorithms will be from linguistic like rules or whether it'll just be engineers engineering

379
00:34:43,520 --> 00:34:50,520
much more efficient versions of the transform or whatever comes after it. That will be traditional.

380
00:34:50,880 --> 00:34:57,880
I don't think it'll be by people explicitly putting traditional linguistic rules into

381
00:34:58,200 --> 00:35:05,200
the system. I don't think that's the way forward. On the other hand I mean you know I think

382
00:35:05,320 --> 00:35:12,320
what we're starting to see is models like these transformer models are actually discovering

383
00:35:12,320 --> 00:35:19,320
the structure of language themselves right so you know the broad facts of you know human

384
00:35:19,320 --> 00:35:24,080
language that you know English has the subject before the verb and the object afterwards

385
00:35:24,080 --> 00:35:29,040
whereas you know in Japanese that the verb at the end of the sentence and the subject

386
00:35:29,040 --> 00:35:33,360
and object are normally in that order before it that could be in the other order you know

387
00:35:33,360 --> 00:35:39,280
actually transformer models are learning these facts you can interrogate them and see

388
00:35:39,280 --> 00:35:43,360
that even though they were never explicitly told about subjects and objects that they

389
00:35:43,360 --> 00:35:49,360
know these notions so I think they you know they're discovering a lot else as well about

390
00:35:49,360 --> 00:35:54,960
language use and context and the meanings and senses of words and what is and isn't you

391
00:35:54,960 --> 00:36:00,840
know unpleasant language but part of what they're learning is the same kind of structure

392
00:36:00,840 --> 00:36:05,680
that linguists have laid out as the sort of structure of different human languages.

393
00:36:05,680 --> 00:36:11,800
So does it over many decades linguists discover certain things and by training on billions

394
00:36:11,800 --> 00:36:15,800
of words transformers are discovering the same things that linguists discovered in human

395
00:36:15,800 --> 00:36:21,680
language that's that's that's cool. So all this is really exciting progress in NLP driven

396
00:36:21,680 --> 00:36:26,880
by machine learning and by other things. To someone entering the field entering machine

397
00:36:26,920 --> 00:36:32,960
learning or AI or NLP there's just a lot going on. What advice would you have for someone

398
00:36:32,960 --> 00:36:41,080
wanting to break into machine learning? Yeah well it's a great time to break in. I think

399
00:36:41,080 --> 00:36:46,840
there's just no doubt at all that we're still in the early stages of seeing the impact of

400
00:36:46,840 --> 00:36:56,000
this new approach where effectively software computer science is being reinvented in on

401
00:36:56,000 --> 00:37:01,000
the basis of much more use of machine learning and the various other things that come away

402
00:37:01,000 --> 00:37:06,360
from that and then more generally across industries there are just lots of opportunities for more

403
00:37:06,360 --> 00:37:13,040
automation making more use of you know interpretation of human language material for me or in other

404
00:37:13,040 --> 00:37:22,120
areas like vision and robotics or the same kinds of things. So lots of possibilities.

405
00:37:22,240 --> 00:37:27,200
So you know at that point there's lots to do obviously and you want to get some kind

406
00:37:27,200 --> 00:37:33,840
of good foundation right so knowing some of the core technical methods of machine learning

407
00:37:33,840 --> 00:37:40,840
understanding ideas of how to build models from data look at losses do training diagnose

408
00:37:40,840 --> 00:37:48,200
errors all of these core things I mean that's definitely useful for natural language processing

409
00:37:48,280 --> 00:37:53,840
in particular some of those skills are completely relevant but then there are particular kinds

410
00:37:53,840 --> 00:37:59,200
of models that are commonly used including the transformer that we've talked about a lot

411
00:37:59,200 --> 00:38:03,400
today you definitely should know about transformers and indeed they're increasingly being used

412
00:38:03,400 --> 00:38:09,320
in every other part of machine learning as well for vision bioinformatics even robotics

413
00:38:09,320 --> 00:38:15,320
is now using transformers but beyond that I think it's also useful to learn something

414
00:38:15,360 --> 00:38:21,440
about human language and the nature of the problems that involves because I mean even

415
00:38:21,440 --> 00:38:27,920
though people aren't directly going to be encoding rules of human language into their

416
00:38:27,920 --> 00:38:35,400
computing system a sensitivity to sort of what kind of things happen in language and what to

417
00:38:35,400 --> 00:38:40,720
look out for and what you might want to model that's still a useful skill to have.

418
00:38:41,720 --> 00:38:47,920
And then in terms of learning the foundations learning about these concepts you had entered

419
00:38:47,920 --> 00:38:55,520
AI from a linguistic background and we now see people from you know all walks of life wanting

420
00:38:55,520 --> 00:39:01,360
to to start doing work in AI what are your thoughts on the preparation one should have

421
00:39:01,360 --> 00:39:06,800
or any thoughts on how to start from something other than computer science or AI so there are

422
00:39:06,840 --> 00:39:15,360
lots of places you can come from and vector across in different ways and we're seeing

423
00:39:15,360 --> 00:39:21,600
tons of people doing that that they're people who started off in different areas whether

424
00:39:21,600 --> 00:39:29,200
you know it was chemistry physics or even much further in field and people you know history

425
00:39:29,200 --> 00:39:34,160
whatever have started to look at machine learning I mean I think there are sort of two levels of

426
00:39:34,160 --> 00:39:41,480
answer there I mean one level of answer is you know one of the amazing transformations is that

427
00:39:41,480 --> 00:39:49,320
there's now these very good software packages for doing things with neural network models I mean

428
00:39:49,320 --> 00:39:55,400
this these software is really easy to use you don't actually need to understand a lot of highly

429
00:39:55,400 --> 00:40:01,360
technical stuff you've got need to have some kind of high-level conception about what is the idea of

430
00:40:01,360 --> 00:40:06,560
machine learning and how do I go about training a model and what should I look at in the numbers

431
00:40:06,560 --> 00:40:11,520
that are being printed out to see if it's working right but you know you don't actually have to

432
00:40:11,520 --> 00:40:16,960
have a higher degree to be able to build these models I mean and indeed what we're seeing is you

433
00:40:16,960 --> 00:40:22,360
know lots of high school students are getting into doing this because it's actually something that if

434
00:40:22,360 --> 00:40:29,640
you have some basic computer skills and a bit of programming you can pick up and do it's just way

435
00:40:29,680 --> 00:40:36,720
more accessible than lots of stuff that preceded a weather in AI or outside of AI and other areas

436
00:40:36,720 --> 00:40:42,280
you know like operating systems or security but you know if you want to get to a deeper level than

437
00:40:42,280 --> 00:40:49,320
that and actually want to understand more of what's going on I think you can't really get there if you

438
00:40:49,320 --> 00:40:57,400
don't have a certain mathematics foundation like at the end of the day that deep learning is based

439
00:40:57,520 --> 00:41:05,840
on calculus and you need to be optimizing functions and if you sort of don't have any background in

440
00:41:05,840 --> 00:41:13,520
that I think that sort of ends up as a wall at some point so you know. The math for machine learning

441
00:41:13,520 --> 00:41:20,960
and data science it does come in handy for some of the work we're going to do. Yeah so I think at

442
00:41:20,960 --> 00:41:28,720
some level if you're at the major in history or you know non-mathematical parts of psychology I

443
00:41:28,720 --> 00:41:34,360
actually have a good friend who yeah he you know learnt calculus in grad school because he was a

444
00:41:34,360 --> 00:41:39,200
psychologist and he'd never done it before and decided he wanted to start learning about these

445
00:41:39,200 --> 00:41:46,040
new kinds of models and decided it wasn't too late to be able to go and take a Cal course and so he

446
00:41:46,040 --> 00:41:53,800
did right so you know you do need to know some of that stuff but for lots of people if they've seen

447
00:41:53,800 --> 00:42:01,760
some of that before even if you're kind of rusty I think you can kind of get back in the zone and

448
00:42:01,760 --> 00:42:08,440
it doesn't really matter that you haven't you know done AI as an undergrad or machine learnings and

449
00:42:08,440 --> 00:42:13,560
things like that that you can really start to learn how to build these models and do things and you

450
00:42:13,560 --> 00:42:19,480
know really that's my own story right that despite the fact that they let me sit in the school of

451
00:42:19,480 --> 00:42:26,400
engineering at Stanford these days you know my background isn't as an engineer you know my PhDs

452
00:42:26,400 --> 00:42:34,880
and linguistics but you know I've sort of largely vectored across from having some knowledge of

453
00:42:34,880 --> 00:42:41,120
mathematics and linguistics and knowing some programming into sort of getting much more into

454
00:42:41,160 --> 00:42:43,120
building AI models.

455
00:42:43,120 --> 00:42:48,160
I was curious about something do you think the improved libraries and abstractions that are now

456
00:42:48,160 --> 00:42:53,680
available like coding frameworks like TensorFlow or PyTorch do you think that reduces the need to

457
00:42:53,680 --> 00:42:59,360
understand calculus because boy it's been it's been a while since I had to actually take a derivative

458
00:42:59,360 --> 00:43:04,720
in order to even implement or create a new neural network architecture because of automatic

459
00:43:04,720 --> 00:43:14,000
differentiation. Yeah I mean absolutely I mean so in the early days when we were doing things sort

460
00:43:14,000 --> 00:43:21,120
of 2010 to 2015 right for every model we built we were working out the derivatives by hand and then

461
00:43:21,120 --> 00:43:26,480
you know writing some code and whatever it was you know sometimes it was Python but sometimes it

462
00:43:26,480 --> 00:43:33,320
might have been Java or C to calculate these derivatives and checking that we got them right

463
00:43:33,320 --> 00:43:41,960
and so on where you know these days you actually don't need to know any of that to build deep

464
00:43:41,960 --> 00:43:47,000
learning models I mean this is actually something I think about been thinking about even with respect

465
00:43:47,000 --> 00:43:53,080
to my own natural language processing with deep learning class that I teach you know at the beginning

466
00:43:53,720 --> 00:44:01,960
we do still go through doing you know matrix calculus and making sure people know about

467
00:44:02,040 --> 00:44:09,640
Jacobians and things like that so that they understand what's being done in back propagation

468
00:44:09,640 --> 00:44:15,640
deep learning but you know there's sort of this sense in which that means that we just give them

469
00:44:15,640 --> 00:44:21,800
hell for two weeks you know sort of like boot camp or something to make them suffer and then we say

470
00:44:21,800 --> 00:44:26,760
oh but you do the rest of the class with PyTorch and they sort of never have to know any of that

471
00:44:26,760 --> 00:44:33,400
again right I you know there's always a question of how deep you want to go in technical foundations

472
00:44:33,400 --> 00:44:40,840
right you can keep on going right like does a computer scientist in the 2020s need to understand

473
00:44:40,840 --> 00:44:49,640
you know electronics and transistors or what happens in you know CPU well you know it's complicated

474
00:44:49,640 --> 00:44:54,840
I mean in various ways it is helpful to know some of that stuff I mean you know I know Andrew you

475
00:44:54,840 --> 00:45:01,080
were one of the pioneers and getting machine learning onto GPUs and well you know that sort of

476
00:45:01,080 --> 00:45:06,600
means you had to have some sense that there's this new hardware out there and it has some attributes

477
00:45:06,600 --> 00:45:13,080
of parallelism that means there's likely to be able to do something exciting so you know it is useful

478
00:45:13,080 --> 00:45:17,880
to have some broader knowledge and understanding and you know sometimes something breaks and if

479
00:45:17,880 --> 00:45:23,080
you have some deep knowledge you can understand why it broke but there's another sense in which you

480
00:45:23,080 --> 00:45:30,040
know most people have to take some things on trust and you can do most of what you want to do in

481
00:45:31,160 --> 00:45:37,160
neural network modeling these days without knowing calculus at all yeah that's a great point I feel

482
00:45:37,160 --> 00:45:42,360
like sometimes the reliability of the abstraction determines how often you need to go in to fix

483
00:45:42,360 --> 00:45:47,880
something that's broken so I actually my understanding of quantum physics is very weak I barely

484
00:45:47,880 --> 00:45:52,440
understand it so you could argue I don't understand how computers work because transistors are built

485
00:45:52,440 --> 00:45:57,800
in quantum physics but fortunately you know if something went wrong with transistors I've never

486
00:45:57,800 --> 00:46:06,040
had to go in to try to fix it so they're a bit hard to fix I think and so I think I think well

487
00:46:06,040 --> 00:46:11,240
another example you know the sort function their libraries are sort things and sometimes they actually

488
00:46:11,240 --> 00:46:15,960
don't work right swap in the memory or whatever and that's when if you really understand how the

489
00:46:15,960 --> 00:46:22,200
sort function works you can go in and fix it but then sometimes if we have abstractions libraries

490
00:46:22,280 --> 00:46:28,600
APIs are reliable enough then that is nice to those abstractions then diminishes them to understand

491
00:46:28,600 --> 00:46:33,720
some of the things that happen so it's an exciting world feels like you know we have giants building

492
00:46:33,720 --> 00:46:38,440
on the shoulders of giants and and all of these things are becoming more complex and more exciting

493
00:46:38,440 --> 00:46:45,960
every every every month yeah absolutely so thanks Chris that was really um interesting and inspiring

494
00:46:45,960 --> 00:46:52,040
and and I hope that to everyone watching this hearing Chris's own journey um to become a

495
00:46:52,040 --> 00:46:57,560
computer scientist and to become a leading maybe the leading NLP computer scientists as well as all

496
00:46:57,560 --> 00:47:04,520
of this exciting work having an NLP right now I hope that inspires you to jump into the sphere and

497
00:47:04,520 --> 00:47:10,360
take a go at it there's just a lot more work to be done collectively by our community than still

498
00:47:10,360 --> 00:47:13,880
so I think the more of us are working on this the better off the world will be

499
00:47:13,880 --> 00:47:25,720
so thanks a lot Chris it was really great having you thanks a lot Andrew it's been fun chatting

