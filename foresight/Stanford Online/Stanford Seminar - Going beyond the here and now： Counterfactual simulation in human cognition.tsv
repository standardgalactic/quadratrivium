start	end	text
0	15720	Well, thank you. Thank you so much for having me. It's a pleasure to be here. And I hope
15720	21840	maybe some of the things that I talk about may give some inspiration for you, HCI guys.
21840	26040	So I lead the causality and cognition lab in the psychology department. I'm interested
26040	30240	in how people understand causality and basically how the world works and how they understand
30240	34920	each other. And we're interested in how people learn about the causal structure of the world,
34920	38680	how they, once they have it in their mind, how they can use it to reason about the world,
38680	42320	make predictions, make inferences about the past, or think about maybe also how things
42320	46120	could have played out differently from how they actually did. And how those capacities
46120	50520	also allow us to make the kind of judgments we do in our everyday lives, like for example,
50520	55600	assigning responsibility to one another. And that's in fact one of the bigger sort of overarching
55680	59560	goals that my lab is working toward, namely developing a computational framework for
59560	64000	understanding responsibility. And I think to get there, we have to be able to answer at
64000	68040	least two questions, namely one being what causal role somebody's action played in bringing
68040	72360	about the outcome. And the other one being what the action that the person took tells
72360	76640	us about the kind of person that they are. For this first one, we need some intuitive
76640	80600	theory of how the world works. So we can relate the actions that somebody took to the kind
80600	84680	of outcomes that resulted from those actions. And for the second question, we need some
84760	88480	intuitive theory of how people work. So we can go backwards from the actions that we've
88480	92080	observed to the mental states that may have given rise to those actions. So what were
92080	95360	the person's intentions, what did they believe, what were the kinds of things maybe that they
95360	101200	were able to do as well. And so I studied psychology, like in my undergrad, and I was
101200	104680	most excited about social psychology, because I felt sort of most applicable, I guess, to
104680	109240	my everyday life, and somehow also got into responsibility, like back then already. Maybe
109240	112920	it was because I was in some group project where I felt I was doing all the heavy lifting
113000	116720	and maybe I wasn't getting all the credit for it. So that was sort of what interested
116720	122160	me initially. And when I read around in that work in social psychology, a lot of the theories
122160	127160	that I saw took a form sort of like this. So I'll just give you a few examples. So basically
127160	131680	sort of like boxes and arrows theories, where they identified important concepts that were
131680	136040	related to how we assign responsibility, and maybe also roughly how they were related to
136040	142600	one another, but still left a lot in a certain way unspecified. So this is a quote from
142960	146680	Bertrand Molle from a while ago. He says, like, an important limitation of many of these
146680	151200	models of moral judgment or assigning responsibility is they don't really generate any quantitative
151200	155360	predictions. And you might say, like, oh, what do you need quantitative predictions for? Well,
155360	159480	one thing that they're useful for is sort of, you know, laying your cards out and making
159480	164280	it concrete, what your model does also allows it then to be falsified more easily. And I
164280	168440	remember this one instance, it was like me, I think maybe first day of my PhD, I went to
168440	172200	this conference and had dinner, you know, with one of the, one of the people who had made
172200	176720	one of these sort of boxes and arrows and diagrams. And I told them about some experiment
176720	181320	that I thought, that I thought of and thought, like, oh, this would happen. And I think that
181320	185840	would be the result of that experiment. And, and I was very, very smart. I thought, like,
185840	189200	oh, this would totally kind of disprove your theory, right? And he said, no, no, that would
189200	193600	be totally consistent with my theory. And I thought, oh, that's weird. I maybe I really
193600	198240	tried to understand the theory very well. And, and so, so that also was a sort of little
198240	202080	bit of a moment for me that I felt like, okay, maybe it's important to try to make these
202120	205400	theories even more precise. So we know what it is that they're predicting, so we can go
205400	209640	about and, you know, falsify them and sort of improve them. And so that's been very much
209640	214560	kind of an inspiration for me, what I've been trying to do it a little bit. And so one of
214560	219840	the starting points in almost all of these theories of responsibility is there's causality,
219840	223600	always causality comes first. So I thought, okay, let me try, let me try that one. So
223600	229200	can we get more specific about what it means, you know, what people, what it takes for people
229240	235320	to say that one thing caused another thing to happen. And so I think that three key ingredients
235320	239480	that we need, like in order to get a theory for how people think that one thing caused
239480	244720	another thing to happen. And those are starting with a mental model that people have of a
244720	249800	particular domain, a mental model that allows us to conceive of counterfactual interventions.
249840	253480	So, and I'll flush it out a little bit more in a moment. So imagining how things could
253480	257560	have been different from how they actually were. And that allows us then to mentally
257560	263560	simulate what the consequences of this counterfactual intervention would have been. And so the idea
263560	267520	of mental models has been around, you know, for quite some time and has recently gotten
267520	272560	a little bit more attention again, also in AI. And, but, but yeah, some of the credit
272560	277040	at least in modern times that go to the philosopher Kenneth Craig and his book, The Nature of
277040	280600	Explanation, who said something along the lines, well, he said exactly that, but I'm
280600	284760	going to say along the lines, so that we have something like a small scale model. Oh, wouldn't
284760	289520	it be very helpful if we had something like a small scale model of the world in our minds
289520	292920	that we can then use for all sorts of things, like predicting what was going to happen if
292920	297160	I did this, rather than actually having to carry out the action and then, you know, dying
297160	302400	maybe if it was a bad one. And, and yeah, that that would be really helpful for decision
302400	306200	making. And as I will say in a moment also really helpful for explaining kind of why
306200	311400	something happened. So this idea of mental models has been around for a very long time.
311400	315160	And then in somewhat more recent years, at least in cognitive science, has been made
315160	318640	a little bit more concrete, particularly as it pertains to our mental model of the physical
318640	323400	world. And so the idea is was here to say like, well, maybe our mental model of the
323400	328360	physical world is in certain respects, similar to the kinds of physics engines that we use
328360	332640	to make realistic computer games. That's a common move, right? You have some, some tool
332640	336040	and then you think like, okay, maybe the mind is a little bit like that tool. So this was
336040	339760	just, you know, psychologists playing Angry Birds and then thinking like, okay, maybe
339800	346560	the mind is a little bit like Angry Birds. So here, the basic idea, right, is that we
346560	350360	take in the world, you know, through our perceptual senses, and that we then build this internal
350360	354320	representation of the world. That's now the physics engine kind of representation. So
354320	358280	that we pass the world, for example, into objects and the properties of those objects
358280	362880	and then the interactions between those objects. So here, this child maybe passes the world
362880	366720	into the ball and then the eagle on top of the tower and then the tower or the blocks
366760	371840	that make up the tower. And now that you have this internal representation of the world,
371840	375640	you can use it, for example, for planning. So if this child, for example, wants to topple
375640	380040	over that tower, they can think about what's going to happen if they roll the ball like
380040	386000	in different kinds of ways. So I can run simulations using this internal engine in my mind. So
386000	389240	having this would be very useful because I could make predictions about the future. I
389240	392960	could pay sort of Sherlock and infer from the current state of the world what must have
392960	398120	happened in the past. And as I'll show in a moment, this would also be useful for explaining
398120	403960	something that happened in the present. Okay, so what I'll do is in this in the remaining
403960	409200	time, right? I'll basically want to cover these two different aspects of working towards
409200	413320	this computational framework. So in part one, I'm going to focus on the physical domain.
413320	418120	And then in part two, I'm going to expand it to just start to think about people. I should
418120	422360	say, obviously, feel free to ask questions like anytime throughout. Otherwise, I'll try
422560	428760	and end around 12.20 so that we have a little bit of time also for Q&A at the end. Feel
428760	433080	free to ask and throughout if anything's unclear. Okay, so let's start with this part
433080	437160	one. And I should also warn you, there is a little bit of audience participation required.
437160	441960	So get ready for that. So the first, we started really simple, right? I was saying, okay,
441960	445440	I want to understand causality a little bit better. What's the simplest possible setting
445440	449560	maybe in which you could think about causality? Well, it's two billion balls colliding with
449560	454160	one another. And here's the first audience participation part. So there's going to be
454160	457720	these two balls coming in on the right side of the screen. And I'm going to ask you whether
457720	461640	you think that ball A caused ball B to go through the gate. And if you think so, maybe
461640	466640	just raise your arm like at the end of the video clip. So here's what's happening. Okay,
466640	473640	so who thinks that A caused B to go through the gate in this case? Okay, a lot of people
474320	480320	do anyone think that it didn't? No one dares? Okay, cool. So you're in line with what most
480320	486320	people say in this case. And here's what I think was going on in your minds. Not the
493760	498760	motor part of raising your hand, but the kind of judgment, the part of, yeah, was there
498760	502480	causation happening in this case? And the first part is very kind of uncontroversial.
502480	505720	So you looked at what actually happened. You saw that they collided with one another
505720	509440	and then ball B ended up going through the gate. And now the somewhat more controversial
509440	514320	part is to say that, well, that's in itself is not sufficient. That does not contain all
514320	517560	the information you need in order to say that A caused ball B to go through the gate in
517560	522360	this case. But you also need something like this. You need the capacity to simulate in
522360	527640	your mind, in this case, that removing basically ball A from the scene, kind of in your mind.
527640	530880	And then simulating where ball B would have gone if ball A hadn't been present in the
530920	535440	scene. Maybe you all sort of naturally and spontaneously did that. And of course, I already
535440	539520	talked a little bit about counterfactuals and stuff like that in the experiment. Of course,
539520	544720	I don't do that. I just ask people to make causal judgments. So the simple idea here
544720	548920	is then to say, when do you say that A caused me to go through the gate? It's really sort
548920	552920	of like an epistemic notion. So your subjective degree of belief, well, to the extent that
552920	558120	you think that what would have happened in the actual, so that what would have happened
558160	561280	in the counterfactual situation would have been different from the thing that actually
561280	566080	happened, that determines your extent to which you say that A caused B to go through the
566080	570440	gate. And here you're probably pretty sure in this instance that B would have missed
570440	575520	if A hadn't been there. So you say, yes, A caused it to go through. And just a little
575520	579960	bit in terms of sort of background, a lot of the inspiration for this kind of work comes
579960	586840	from Judea Pearl's work on causality. Some of you may have heard of his work. And there
586880	590680	they use different kinds of generative models to capture people's causal knowledge of the
590680	595040	world. So this could be something like causal base nets or structural equations that you
595040	599960	may also remember from your stats class if you had one. And then you define some kind
599960	605400	of operations on these models to support things like counterfactual reasoning. So imagining
605400	610280	that some variable had been replaced with another one, for example. And so I'm doing
610280	614120	something quite similar here, only in that I'm assuming that the generative model that
614120	618160	people have of the world in this case is somewhat richer than what can be represented
618160	622320	with these causal basis or structural equations. So in my case, the generative model that I
622320	625960	assume people have in their mind is something like the physics engine that I actually use
625960	630440	to generate them in the stimuli. And I'll make that noisy. I'll show you in a second
630440	635520	how I'm making it noisy. And then I also have to think about, okay, what are now the counterfactual
635520	639760	intervention operators that you might have over a representation like this one? And in
639760	643680	this case, it could be something like imagining that an object wouldn't have been, would not
643680	648520	have been there, for example. Okay, so you might think now, okay, well, maybe that's
648520	652720	the only game in town. Like what else could you possibly be doing in a setting like this?
652720	656240	And at least luckily for me, there has been a lot of philosophers and psychologists that
656240	660520	have argued for what I called these actualist theories of causation. And they basically
660520	664520	just say you don't need that part, right? All you need, all the information you need
664520	669840	to give causal judgments or causal explanations for what happened is there in the actual situation
669880	674800	in some sense. And so one of the best kind of worked out accounts of that in psychology
674800	679400	comes from a psychologist called Philip Wolff and he calls it the force dynamics model
679400	684080	of causation. And the idea is that all you need to pay attention to is the forces that
684080	689600	are associated with the agent and the patient. That's the sort of lingo they use. And you
689600	693080	then you just look, need to look at how these forces are configured. And that helps you
693080	697680	to say what in this case here, there's different causal expressions is appropriate to use in
697680	702480	a particular situation. And I'll just apply it to this example here. So we have the patient
702480	706520	which is ball B that has a force that is associated with it. Then we have an agent
706520	710520	that applies a force to the patient in this case. As a function of these two forces, we
710520	714360	have some resulting force here. And then in this case, the patient also ended up reaching
714360	720000	the end state. And because this configuration looks like that and that maps onto this force
720000	724440	configuration, Philip Wolff's account would also here say, yes, a cause B to go through
724440	728240	the gate. So this clip would not help us actually tease apart this other model that
728240	734160	I've been kind of promoting. So just to make this distinction sort of clear or clearer.
734160	737480	So in the force dynamics model, you start with some intuitive theory of how the world
737480	742080	works, which in this case are these little force vectors that apply to agents and patients.
742080	745960	And you can then directly go from there to making causal judgments. So there's this direct
745960	751120	route from this kind of intuitive theory to causal judgment. He also says that you can
751120	755720	do counterfactuals too by imagining, for example, if one of the forces hadn't been there, what
755720	760200	would have happened in the situation, but that it's not necessary to figure out whether
760200	764920	something caused something to happen. And sort of what I'm arguing for is sort of a slightly
764920	768360	different picture. Where I'm saying, well, first of all, I start with a slightly different
768360	772520	theory of the domain. In this case, again, using the physics engine rather than using
772520	776880	these force vectors. But then saying that you have to go through this process of counterfactual
776880	781880	simulation to say that something caused something to happen. And what I'm going to try and do
781880	787080	in the next two slides is sort of motivate that account.
787080	790760	One way to motivate at first is that I started off saying like I want to have this model
790760	795160	of responsibility. And that means that I want to have a model of causation that not only
795160	799320	narrowly applies to the physical world, but that can also be applied to, for example, the
799320	803760	kind of causation that happens between people. And here's just some examples of causal statements
803760	807960	that you could hear at the fall of Lehman Brothers, caused the financial crisis. My
807960	812440	housemates failed to water my plants, caused them to die. Realizing that he forgot his
812440	816280	wallet at home, caused him to go back. You probably wouldn't say that exactly in English,
816280	820560	but they all seem to find sort of causal things, like to say. And it's probably a little bit
820560	824960	tricky, or at least I would find it tricky, to think of how would I explain these sorts
824960	828960	of causations with force vectors. And the hope is that the account that I'm developing
828960	833880	is a little bit more flexible so that it can apply to these sorts of situations as well.
833880	839680	But now, and another kind of key advantage, I think, of the model that we've been developing
839680	844640	is that it actually allows us to derive quantitative predictions. And it's hence more easily falsifiable
844640	849120	that some of the prior work. And so you can falsify it if you like, write a paper until
849120	855560	we was wrong. And then I have to go back to the office and improve the account. So here's
855600	859360	a way in which we're getting quantitative predictions out of this model. But I was saying
859360	864400	that how you make causal judgments is by comparing what actually happened with what
864400	869440	would have happened in the relevant counterfactual situation. But now you don't know that. The
869440	873240	thing that I'm showing here on the right-hand side, I guess, that's in some sense the ground
873240	877480	to truth. But you don't get to see that. You only see what actually happens. You don't
877480	881600	get to see what would have happened if ball A hadn't been there. So you have to use your
881600	884880	intuitive understanding, again, of this domain to simulate what would have happened in this
884880	890160	counterfactual. And so one way for us to capture this uncertainty that you may have
890160	895480	about exactly what would have happened if ball A hadn't been there is by generating simulations
895480	899640	from our physics engine, but now injecting a little bit of noise into that engine. So
899640	904120	now it becomes sort of like a probabilistic program because it's now not a deterministic
904120	909240	outcome anymore if ball A hadn't been there. But rather what I'm doing is I'm generating
909240	914800	a simulated sample from my model. And now in this case there's many different ways in
914800	918960	which you could make your model kind of random or uncertain. Here what we did is we just
918960	924600	took the actual ground truth, that ball B, velocity that ball B would have had, and applied
924600	928880	a small perturbation to the velocity vector at each point in time. So now it's sort of
928880	933120	like in your simulation, when you're imagining where ball B would have gone, it sort of jiggles
933120	938640	a little bit along the way. And so this might be now one outcome, like off such a sample.
938640	941520	So if you think like, oh, oh, I think it would have missed. But let me try again. Like,
941520	944440	oh, yeah, I think it would have missed. Yeah, I'm pretty sure it would have missed. So this
944440	947400	is just multiple times sampling in your mind of what would have happened if ball A hadn't
947400	950840	been there. And here, since all of them, you're pretty sure that it would have missed, you
950840	955160	said, yeah, A caused it to go through. But you can probably already anticipate, we can
955160	960320	now do a slightly different case, right, where in the actual situation, again, still A collides
960320	963800	with B and B goes in. But this time it's sort of less clear what would have happened
963800	967600	if ball A hadn't been present in the scene. Because that ball B is headed like right to
967640	972120	the goal post, essentially. And now if you apply the same idea of simulating with noise
972120	976040	what would have happened, you know, in some cases, maybe ball B would have missed. But
976040	980000	it's also possible that ball B would have gone in anyhow, even if A hadn't been there.
980000	983560	And that accordingly, you might say like, yeah, I'm less sure, you know, that A caused
983560	988400	ball B to go through the gate in this case. So that's what we did now in our experiment
988400	992720	where we showed people a bunch of clips like this one. So here's just three different ones,
992960	999120	one clip in which, you know, it's pretty clear here at the top that ball B would have missed
999120	1003120	if ball A hadn't been there. The one in the middle is like one, this kind of close call.
1003120	1006400	And then the one on the right hand side is one in which it was pretty clear that ball
1006400	1010880	B would have gone in anyhow, even if A hadn't been there. And then between experiments,
1010880	1014160	we either asked them some counterfactual question. So that's the one here at the bottom,
1014160	1017760	the blue one. Do you think that ball B would have missed if ball A hadn't been there?
1018400	1022160	And then we see that in this case, they're pretty sure, yeah, I think it would have missed.
1022160	1026160	Here, they're right at the midpoint of the scale, not sure, right, whether it would have
1026160	1031600	missed or not. So we give them some slider where they can just evaluate their degree of belief.
1031600	1037360	And then in this case, they're pretty sure that it would not have missed, even if ball A hadn't
1037360	1042480	been there. And then we take a separate group of participants and we ask them a causal question.
1042480	1046240	So those participants don't hear anything about counterfactuals. We just asked them in a clip
1046240	1050720	like that, what do you think that ball A caused ball B to go through the gate? And we see that
1051120	1056480	judgments align very closely with those of the ones in the counterfactual question condition.
1057600	1061520	And we can also use that model that I described that draws these samples and tries to simulate
1061520	1066400	what would have happened. And it also yields very similar judgments in this, or makes predictions
1066400	1071360	in this case. These were just three of the video clips. We had like 18 different clips in that
1071360	1076080	experiment. And if we just line up here on the x-axis, the average counterfactual judgments
1076160	1080880	that participants made, and on the y-axis, the average causal ratings that participants gave,
1080880	1084800	you see that they're very closely aligned with one another, at least suggesting a strong
1084800	1090720	relationship between these kinds of judgments. But when we published this work as a coxide paper,
1090720	1095280	so for the cognitive science proceedings, one of the reviewers, they were mostly happy with it,
1095280	1098880	but one of the reviewers was saying, yeah, but all of the clips that you showed participants,
1098880	1102880	something slightly different was going on. So maybe you just didn't try hard enough to come
1102880	1106800	up with an actualistic count, like one that only looks at what actually happened. And if you
1106800	1114240	tried a little bit harder, then you could have explained it away. So we did try, and we didn't
1114240	1118160	succeed, but it's also sort of a weird position that you're in when you kind of don't want to
1118160	1123840	succeed, right? So we thought, okay, maybe the better thing, rather than being crappy at modeling,
1124480	1128080	you know, just let's come up with an experiment where it feels like if it comes up in the way that
1128080	1133040	we think it will, there's no way you could possibly explain it with an actualistic count.
1133040	1136960	And so that's the route we took. So just to really think like, oh, are these counterfactors
1136960	1141600	really necessary for understanding causal judgments? So second round of audience participation,
1141600	1145360	get ready. I'm just going to show you a slightly different clip, and this time I'm going to ask
1145360	1148240	you whether you think that ball A prevented Bobby from going through the gate.
1148240	1160880	Okay, what do you think? If you think that ball A prevented Bobby from going through
1160880	1168960	the gate, you can raise your hand. Okay, a few people think so in this case. Okay, I'll show you
1168960	1178640	another one. Okay, this was not some kind of, you know, glitch. I was having fun, you know,
1178640	1183600	doing the physics engine and sort of playing portal, right, by turning these things into a
1183600	1186640	Taylor port, right? And I didn't tell you anything about them, of course, when I showed you the
1186640	1190800	first clip, but maybe just seeing that one clip, you already have like one shot learning, yeah,
1190800	1195200	okay, maybe that's a Taylor port. And the Taylor port, it works only for ball B, you know, it
1195280	1199280	doesn't work for ball A, and the yellow thing is the entry of the Taylor port, and the blue
1199280	1204640	thing is the exit of the Taylor port. And now that I've shown you that, if I now show you exactly
1204640	1209680	the same clip again, you're going to say, at least if you're like my participants, yes, it prevented
1209680	1213760	it from going through, right? Because now what changed is basically your belief about how the
1213760	1217760	world works, such that your counterfactual looks a little bit more like that now, right? What would
1217760	1221680	have happened is that it would have gone through the Taylor port and into the goal, right? So the
1221680	1226880	fact that I can show you exactly the same clip twice, right? And, and all I've changed was your
1226880	1231040	belief about how the world works. And that makes a big difference to your causal judgment, sort of
1231040	1236480	shows that it's, it cannot be sufficient to explain causal judgments just in terms of what
1236480	1240480	actually happened, because actually what actually happened was exactly the same in both of the
1240480	1245040	times that I showed you the clip. I don't need to do the Taylor port thing. The Taylor port thing
1245040	1249040	is cute because I can show you exactly the same clip, but I can also move some obstacle in and
1249040	1253600	out of the way, right here on the left hand side, you're not going to say that A prevented B from
1253600	1258000	going through the gate. On the right hand side, you are because the block is out of the way, right?
1258000	1262240	And a similar way for causation. And on the left hand side, you're going to say, yeah, A caused it
1262240	1266240	because the block would have blocked it. And on the right hand side, you're not really going to say
1266240	1270800	that it caused it because it would have gone in anyhow, right? Same idea. I'm doing exactly the
1270800	1274320	same interactions between the balls. I'm just changing something kind of in the background
1274320	1278400	that affects the counterfactual and, and thereby also affects people's causal judgments.
1280000	1286080	Okay. So another thing that's sort of neat about this model is that it doesn't only kind of predict
1286080	1290880	basically the judgment that people should give at the end of it, but also says something about
1290880	1295680	the cognitive process by which they arrive at the judgment, right? In this case is maybe this
1295680	1299680	process of mental simulation, that you kind of generating these samples and thinking about what
1299680	1305520	would have happened, and that those drive the causal judgment. And one way we can do that,
1305600	1309360	or can sort of get more direct evidence on that is to use eye tracking, right? To see, okay,
1309360	1312880	where is it that you're looking at when you're asked to make causal judgments in these kinds
1312880	1318880	of video clips. So we went back to the really simple ones again. And now also between experiments,
1318880	1323440	just ask participants a different question about the video that, that, that they would see. And
1323440	1327120	they knew at the beginning what question they would be asked. So we had one condition here that
1327120	1330880	we call the outcome condition where they'd watch the video and we would just ask them at the end,
1330960	1335760	in this case, if it ended up missing, did be completely miss the gate. And so I'll show you
1335760	1339200	the eye movements of one of the participants in this condition. And I'm going to play the
1339200	1344960	video at half speed and I'll do some sort of life narration as it unfolds. So the blue dot is the
1344960	1348800	eye movement, right? So the participant here is looking back and forth between ball A and ball B.
1349760	1354880	So looking, looking at ball B, sort of now trying to extrapolate where ball B will end up hitting
1354880	1363600	the wall. And then mostly looking at ball B. Not very exciting, but also that's all they need to
1363600	1368560	know in order to answer this question in this case. So now if you take a different participant
1369120	1374320	who was asked to make a causal question, or asked to answer a causal question in the video,
1374320	1378480	but otherwise saw exactly the same video clips as other participants did, you're going to see that
1378480	1381920	the eye movements look quite different. And they look different in a way that made me very happy
1381920	1387760	at the time. So you see they're not just looking at ball B, they're trying to anticipate where
1387760	1392800	ball B would have gone, you know, if ball A wasn't present in the scene. And it's quite likely that
1392800	1397680	when you guys, when I showed you this first video clip that you did that, right? And may not even
1397680	1402400	been super, you know, aware to you that you did do that, like I haven't really checked, you know,
1403120	1406800	yeah, how, well, at some point at the beginning when I ran this on the laptop, I would sometimes
1406800	1409680	see that people would use their finger, or they would use their, you know, kind of
1410320	1415520	pen or something. And that's of course pretty aware, I guess, right? But it's possible that with
1415520	1419520	the eye movements, this sort of comes so natural to us that we don't even realize that we're engaging
1419520	1425200	kind of in this kind of process. But yeah, I was very happy, you know, when I saw this happening.
1426320	1430880	And so this is anecdotal in a sense, it's just one video clip, right? But we can also look at
1430880	1435200	more generally, sort of analyzing the differences in the eye movements that people are producing
1435280	1438800	between these different experiments. And what I'm showing here is just looking at
1438800	1442560	the saccades that participants are producing. So those are fast eye movements jumping from
1442560	1448240	one point to another. And then I look at the endpoints of those saccades. And I look at where
1448240	1453600	those fall, right? And I took into account only the time between ball A and ball B coming into the
1453600	1461280	scene. And before basically, when they collide with one another, that time window. And then we
1461280	1467360	see that on this, for the causal question, a lot of those saccades basically end up along the path
1467360	1471360	right that ball B would have taken if ball A hadn't been there. Whereas in the other condition,
1471360	1480160	we see very few of these kinds of eye movements. So nice, I guess, even more direct evidence
1480160	1483840	that people are engaging in this kind of process and that they're doing it specifically
1483840	1488160	when asked to answer a causal question about the clip and sort of spontaneously.
1488400	1496400	There is this other part to it. But I think I will skip, so I have a little bit more time to,
1497520	1502400	let me see. Well, actually, I'll share it. Sorry about that. So there was another,
1502400	1507280	after we published this paper, there was another reviewer number two, as there often is.
1509280	1514480	And they were basically still saying, okay, well, this was for the eye tracking data. And they said,
1514880	1519920	that's nice. You're showing us these sort of eye movements. But they basically said that, okay,
1519920	1524320	these eye movements, they're happening before the balls are colliding with one another. And you're
1524320	1528240	calling it sort of counterfactual simulation. Counterfactual should mean it should be back
1528240	1532240	in the past. Going back in the past, evaluating that something would have been different,
1532240	1538000	and then seeing what difference that would have made. And they were saying, oh, what,
1538000	1543280	you should just call it the hypothetical simulation model instead, and not that. So we were able to
1543440	1551600	push back. But the reviewer also was right to some extent, I think. So this is a paper that I've
1551600	1556720	published quite recently, where I was trying to say that, no, you really need the counterfactuals.
1556720	1560160	So a lot of this has been like, yeah, you really need the counterfactuals. And then you just keep
1560160	1564720	getting some pushback, and you try to convince people even more so. So this was this reviewer
1564720	1568800	number two here. You haven't really shown us counterfactual simulation. Those looks are happening
1568800	1572800	before the balls are colliding. So his idea was, well, maybe what people are doing is they're kind
1572800	1577200	of simulating some hypothetical future. In this case, the hypothetical future is like,
1577200	1581760	what would happen if ball A wasn't there? And then they're storing that in their mind,
1581760	1585840	and comparing that to what actually happened at the end. And that's a slightly different
1585840	1591120	computation from the one that I think they're carrying out. And this relates to something,
1591120	1598800	again, here's Judea Pearl, this climbing on this kind of virtual letter here. Because he has argued
1598800	1603840	that there are these three different ways of thinking about the extent to which people have
1603840	1609440	causal knowledge of how the world works. On the lowest rung of the letter, and he often accuses
1609440	1613040	a lot of deep learning and so on to be on that rung, although it's a little unclear,
1614160	1617760	he calls that rung the level of association. So that's what you learn in the stats classes
1617760	1623040	correlation. When two things are associated with one another, and you can infer one variable from
1623120	1629280	the presence of another, so the normal conditional probability, PY given, I would say PY given X.
1629280	1634160	So what does some symptom tell me about the disease, for example? On the next level,
1634160	1637840	it's the level of interventional reasoning. That's the kind of when I do a randomized control
1637840	1642720	trial, for example, or if I'm, again, hypothetically reasoning, oh, what would happen if I were to do
1642720	1648560	this? What would happen if I were to do that? And that's sort of when your stats teacher tells you
1649120	1653040	causation and correlation aren't the same thing, that's often the thing that they then think about,
1653040	1656080	right? That like, oh, on the level of an experiment, now I'm performing an intervention,
1656080	1660720	randomly assigning people to different groups, and I can draw different kinds of causal inferences
1660720	1666960	from that information than when I just have observations. But then process ultimately,
1666960	1672800	the kind of the highest rung on the letter is reserved for counterfactual reasoning,
1672800	1677360	and that allows you to give specific answers essentially to why questions. So why did this
1677360	1682000	happen in this particular case? Like, you know, was it the aspirin that stopped my headache,
1682000	1686240	or would it have stopped anyhow, even if it hadn't taken the aspirin? Or, you know,
1686960	1690560	was Kennedy shot? Would Kennedy still have been alive if it hadn't been shot by
1690560	1696080	very heavy-ass world? And so essentially, now the question boils down to, do we need that third
1696080	1703200	level, like to explain people's causal judgments, or is the second one enough, right? So just to kind
1703200	1706560	of try and make it a little bit more clear, right? So the hypothetical, luckily in English,
1706640	1710560	also we have sort of a way of marking the difference between them. So here's an English
1710560	1715680	hypothetical. Would B go into the goal if A was removed? So what you'd be doing is taking the
1715680	1721920	time into account until they collide, simulating like a possible future, and then computing the
1721920	1727840	probability of that. Versus the counterfactual, what I'm doing, slightly different in English,
1727840	1734400	right, would B have gone into the goal if A had been removed? I sometimes, you know, regret
1734400	1738080	having gotten into counterfactual so much, so obviously not a native speaker, and the
1738080	1741760	counterfactuals are sometimes a little complicated, right, that you get the tenses right and so on,
1741760	1746720	but I think I've mostly gotten it down by now after like 20 years. So would B have gone into
1746720	1750400	the goal if ball A had been removed? So you're doing slightly different here now, right? You're
1750400	1755440	taking into account everything until the end, and you're now going back in time to do this
1755440	1759360	intervention and then think about how the world could have unfolded differently from how it actually
1759360	1765520	did. So now it turns out in this very simple setting here, that makes no difference. The
1765520	1769360	hypothetical probability and the counterfactual probability is the same because there's nothing,
1769360	1774480	there's only this one causal event happening, so it doesn't really come apart. So in a very simple
1774480	1780640	setting where you have one cause and one effect, essentially, you cannot tease the two apart,
1780640	1784640	but you don't need to make it much more complicated. It's sufficient if you just have one other
1784640	1789200	alternative event that you are initially uncertain about, and that will make it such that
1789280	1793280	now the hypothetical probability and the counterfactual probability will be different from one another.
1794480	1799200	So here was the genius invention, just putting like a little block again that you've seen earlier,
1799200	1804960	but this time the block is on rails into the scene, and that will now make it such that we
1804960	1808720	can tease these two different things apart. So here's an example. I'm not going to ask for
1808720	1812240	audience petition this time, but let's say that this was happening in the clip,
1813760	1817120	and now if you were asked to say, oh, did it prevent it from going into the goal?
1817680	1822240	My participants say in this case, yes, it did. And the idea is, why is it? Well,
1822240	1827120	because the block moved out of the way in time, such that Balbi would have gone through the goal
1827120	1831520	if Ball A hadn't been there. But you may have also noticed that the movement of the block
1831520	1836160	is happening after the balls collided with one another. So not something that you could have
1836160	1840160	sort of anticipated at this earlier moment in time, or at least had some uncertainty about.
1840960	1846000	So the basic idea here is to say like, oh, my hypothetical probability at the time
1846000	1849920	would Ball B go into the goal if Ball A wasn't there? Well, that's unsure. That depends on
1849920	1853920	whether or not the block's going to move. So I should give it like a 0.5 or something. I told
1853920	1858400	participants it's just as likely to move as it's not. Whereas for the counterfactual probability,
1859120	1862880	well, I know that it moved in this case. So I should be pretty certain that it would have
1862880	1867280	gone in if Ball A had been removed. So now I have a way basically of teasing the two apart
1867280	1871920	and can see which one better explains the causal judgments. Is it the hypothetical judgments
1871920	1875680	that I ask participants to do, or is it the counterfactual judgments that I ask another group
1875680	1880800	to do? And then I ask one group to give causal judgments and then just try to relate them to
1880800	1888000	one another. And what I find is when I look at the hypothetical, so maybe I should say a little
1888000	1892400	bit more about that plot here, at the bottom, it basically shows you the initial configuration
1892400	1898320	of the block. Was it in the way or not? And then did it move yes or no? So in this example here,
1898320	1902400	it's one where it was initially in the way, but it moved. But in the hypothetical condition,
1902880	1906800	you don't know that because you only see it until they pause. And then if you look at the
1906800	1910160	hypothetical judgments, they think when it's initially in the way, they think it's a little
1910160	1913440	less likely that it's going to go in. And when it's initially out of the way, they think it's a
1913440	1917840	little bit more likely. So they're sort of a little bit sticky in terms of what actually happened.
1919040	1923520	For the counterfactual probabilities, pretty much only the final state is what matters.
1923520	1927920	If it was out of the way at the end, you think, yeah, it would have gone in. If it was in the way
1927920	1932640	at the end, you think it would have missed. And now if you ask people to make causal judgments
1932640	1936960	in this case, we see that they align very closely with the counterfactual ones and not with the
1936960	1942080	hypothetical ones. And this was for the kind of missed cases, but the same story again holds
1942080	1947360	essentially for the causal cases too. So they think that it caused it when the block would have been
1947360	1951200	in the way at the end, and they don't think that it caused it when the block was would have been
1951200	1956000	out of the way at the end. So enough to make this review too happy, but maybe not Michael.
1958160	1963680	I'm a happy guy. I'm curious. Can you go back one slide? Just to make sure I understand.
1965360	1970240	There were two things that changed in that intervention. There was the question you asked,
1970240	1975600	the hypothetical versus the counterfactual. And it also sounds like the changes in how far they
1975600	1981680	saw into the video. That's right. That's right. And I'm picturing the counterfactual situation
1981680	1986640	where if you ask me the hypothetical question, but showed me the full video, so I see a whole video
1986720	1992080	and then you say, would be going into the goal if A was removed? I don't know.
1992960	1996720	Yeah. Yeah. It's a tricky one. I mean, I guess, you know, you'd have to ask them, like, what did
1996720	2001280	you think? I guess sort of at the time, right? Like before it happened, did you think, and people
2001280	2004800	are often bad at that, right? We know that from all the hindsight research and so on, that they
2004800	2009040	have difficulty putting themselves back into the epistemic state, I guess that they had at an
2009040	2013680	earlier point in time, right? So I'm not exactly, I haven't tried that one. I haven't tried showing
2013920	2018000	it until the end, but then asking them the hypothetical question, it's possible, of course,
2018000	2024880	that they will confuse it like as a counterfactual question, right? And, but for me, it was still
2024880	2028960	sufficient, I guess, at least to address this reviewer's concern, because his idea was really,
2028960	2033200	yeah, that computation is happening earlier, right? It's happening before the causal event of
2033200	2037040	interest, and then you're storing the output of that computation, in this case, the hypothetical
2037040	2042000	probability, and then just comparing that to what actually happens at the end, right? So it still
2042000	2051920	felt that it's addressing that, but yeah. Okay, so having these two things helps teasing them apart.
2052720	2056160	Okay, I'll sum up the first part, and then the second part will be short, but that's okay.
2057200	2060800	So for this counterfactual simulation model, what I've showed you that there's this sort of nice
2060800	2065920	correspondence between people's beliefs about the relevant counterfactual and the causal judgments
2065920	2069920	that they make, that it looks like that these things are necessary, which you can show with the
2069920	2075040	teleport or with the, with the brick in and out of the way, that people spontaneously engage in
2075040	2079680	this kind of counterfactual simulation as evidence to the eye movements, and that it's
2079680	2084240	counterfactuals really and not hypotheticals that seem to be important for expanding causal judgments.
2085440	2089120	We've played around with this model like a little bit more. Once you have a hammer, right,
2089120	2093360	you find all the nails. So this one is just like looking at slightly more complex cases.
2093360	2097120	This one here, philosophers love, because it's a case of, let me show it again, maybe a case of
2097120	2103040	double prevention, where B prevents ball A from preventing ball E from going through the gate,
2103040	2107920	right, because knocks it out. It happens in, maybe in football, probably happens often when
2107920	2111680	one tackles like another person that would have tackled the person running with the ball, right.
2111680	2115280	And so you might say, oh, to what extent did that cause it? You can also look at omissions when
2115280	2120240	nothing is happening. So ball A is just chilling here in the corner, and you might still ask,
2120240	2123840	oh, did it go in because ball A didn't hit it, right? And now you could imagine, well, if it had hit
2123920	2127600	it, what would have happened in this case? And we can also look at cases where really
2127600	2132080	nothing is happening at all. So here's just a tower of blocks, right, and you might still wonder,
2132080	2135600	oh, to what extent is this black one here responsible for the other one staying on the table?
2136160	2139280	And even though, yeah, there's nothing happening, right, you might still say, well,
2139280	2143280	how do you answer this question? Maybe by doing something like playing Jenga in your mind, right,
2143280	2147440	imagining it being removed, and then what would have happened to the scene? So that even just
2147440	2152800	physical support in some sense is very closely related to ideas of causation, right. What it means
2152800	2159440	to support is essentially to prevent something from falling. Okay, so that was part one. Now a
2159440	2165120	sort of short version of part two. And so responsibility attribution was something that I've
2165120	2170320	been into for quite a while and was also my motivating thing. And then I drifted off into
2170320	2175040	causality world mostly just because physics engines were around at the time. So it was like, oh,
2175040	2179680	now I can use those. And with around at the time, I mean, I was a postdoc with Josh Tenenbaum back
2179680	2184240	then and physics engines were all the rage at the time. And I said, okay, now I'll also use them.
2184240	2189360	And there aren't really yet, although I guess Michael is working on it, psychology engines,
2189360	2192480	right, that is easy where you could just have agents and think about what they would have done.
2193760	2198320	So this work that I had done on responsibility attribution wasn't particularly social, also
2198320	2201840	didn't really involve simulation, I think. And there was one experiment that got a little bit
2201840	2207200	closer that I'll briefly share with you here on a paper called Moral Dynamics. And it will look
2207200	2211760	very billiard ball world like I haven't moved too far away from the billiard balls, but this kind
2211760	2215360	of that's somewhat agentive, right? So and so we could show people like a video clip like this
2215360	2219840	and then ask them, what about extent do you think that blue was responsible that the green one got
2219840	2225360	harmed in this case here? And our inspiration here came from a paper called Moral Kinematics where
2225360	2229920	they basically argued, again, it's somewhat more actualist view and saying, okay, there's certain
2229920	2233840	features that people are picking up on in these scenes, like the duration of contact,
2233840	2238160	how far things moved and things like that. And then they directly mapped from these features
2238160	2244640	of the scene to the moral judgment in this case. And we liked the general setup, but didn't really
2244640	2250480	like that model like as much. So we proposed another model that has a slightly different title,
2250480	2254960	Moral Dynamics instead. And we thought, okay, these features are important, but the features are
2254960	2260640	important in that they give us evidence for the latent variables and that those are ultimately
2260640	2264880	the ones that I care about. And in this case, what are the latent ones that we thought one,
2264880	2268880	not very surprisingly here on the right hand side causality, but did you think that it actually
2268880	2273840	caused it, you know, to for this negative outcome to happen. And then the left side,
2273840	2277600	the intuitive psychology part, very kind of minimal in this case here. But it's basically
2277600	2282160	saying like, well, maybe these features give you some evidence about like how much the agent
2282160	2286800	actually wanted to bring about this negative outcome. So if you think, for example, if somebody
2286800	2290480	really wants something to happen, then they're willing to incur a larger cost to make it happen.
2291600	2295440	Putting a lot of effort, for example. So if somebody puts in a lot of effort into something,
2295440	2299920	you know that they must have really valued it. And if somebody really valued some negative outcome,
2299920	2304880	well, that's a bad thing. That was roughly the idea here. And we could then show that if we have
2304880	2310400	a model that just basically infers the amount of effort that some agent exerted and tried to map
2310400	2315840	that onto the responsibility that worked kind of, you know, okay-ish. If we only took into account
2315840	2320160	the causal role that some agent played and tried to use that to explain the extent to which
2320160	2324960	they're held responsible, that worked okay-ish. But if we now took a model that takes both of
2324960	2329200	these components into account, that worked pretty well, which was roughly in line with this kind
2329200	2334400	of unsurprisingly, now this framework that I laid out at the beginning, right, that when we assign
2334400	2338640	responsibility to others, we don't just care about the causal role that they played, but also what
2338640	2342720	the action tells me about the kind of person that they are. In this case, the action tells me
2342720	2345840	something about the desire that they had to bring about this negative outcome.
2346480	2353040	Okay. But still, we didn't really have a real model of agents in this case. We still sort of
2353040	2358240	basically just use the physics engine. Also, we weren't able to talk about intentions, and it's
2358240	2363120	clearly important often when people talk about responsibility. And also still our kind of factual
2363120	2366880	simulation here was basically purely physical, just seeing how this thing would have moved
2366880	2374880	without the other one. So I don't have the skills to make it happen with sort of more
2374880	2379120	agentive agents, but luckily now that I'm here, I get to work out with all these smart people,
2379840	2383760	and here's my PhD student, Sarah Wu, and our research assistant, Shruti Sreeta,
2383760	2388000	and they've looked into cases now that are a little bit more agentive. They're still kind of in
2388000	2393760	in grid world, but at least now planning and intentions and things like that are involved.
2394480	2399040	And here's the basic setup. So this is inspired by some previous work that has looked into helping
2399040	2405840	and hindering as a case study. And what they did is essentially they said, well, what it means
2405840	2411520	for somebody to intend to help someone is that their utility function includes the other person's
2411520	2417440	utility with a positive sign. Intending to help just means wanting to bring positive utility,
2417440	2421840	at least in this framework, to the other person. And intending to hinder puts a negative sign,
2421840	2427760	like now I want it that the other person is a low utility. So it turns out though that
2427760	2432640	intending to help or hinder versus actually helping or hindering is not necessarily the same
2432640	2438400	thing. So here's an example. I don't have a child yet, but at some point maybe we'll have a child,
2438400	2442000	and then if I go grocery shopping with the child, there probably will be a period of time where
2442000	2449280	they're not actually helping. They're sort of like trying to help, but kind of making it worse,
2449280	2452880	at least in terms of efficiency and so on. It's going to take longer. Of course, it's useful
2452880	2457200	because eventually they will be helpful. I have to go through that process just like a PhD student.
2458320	2472320	So yeah, so you go through that process, and then you might intend to be helpful,
2472320	2478320	but it might take a little bit of time to actually be helpful. And the claim is to evaluate that,
2478320	2482000	you need counterfactuals again to tell, oh, is the person actually helpful? Well,
2482000	2486080	how would it have happened without them, essentially? Or there's different counterfactuals
2486160	2490080	to consider, but that's one of them. So here's our grid world that we played with,
2490080	2495040	with the helping and hindering setup. So we have this red guy here who wants to get to the star,
2495040	2499840	has a pure physical goal in this case, just to get to that location. Then we have this blue one
2499840	2504400	who has a pure social goal. They either want to help or hinder the red one from getting there.
2504400	2508560	And then there are these walls here that you can't do anything about, but there's also these blocks,
2508560	2513920	and only the blue one can interact with these blocks. They can push, pull them out of the way.
2513920	2517680	So here's our Hollywood clip of what's happening in this situation.
2522160	2528160	Okay, so in this case, happy end, like a Hollywood movie, red made it,
2528160	2531200	and then we can show people these clips and we can ask them, oh, how responsible was the
2531200	2535520	blue player for the red player's success, for example, in this trial? We can also ask them
2535520	2539600	a counterfactual question, right, would the red player still have succeeded even if the blue player
2539600	2545040	hadn't been there? And we can ask them to make an inference about the intention of the blue one
2545040	2548800	in this case. What was the blue player intending to do? Were they trying to help or were they
2548800	2554480	trying to hinder? Definitely help, definitely hinder. So the idea is now basically the same as earlier,
2554480	2558480	by just saying, okay, again, we need some kind of generative model of the domain. In this domain,
2558480	2563840	now it's a model of agents basically planning and recursively reasoning about one another, right?
2564880	2569120	And that's now our probabilistic program. And we can again compute counterfactuals over that,
2569120	2572960	maybe in this case thinking, well, what would have happened if the blue one hadn't been there?
2572960	2577360	And then thinking how the red one would have planned their path differently, but without the
2577360	2582880	presence of blue, that's a rough idea. So again, we take some actual situation and we can then
2582880	2586800	simulate what would have happened in the relevant counterfactual situation in this case where blue
2586800	2590960	hadn't been there. We can talk later if you like about other counterfactuals you might consider,
2590960	2594960	but we just went with this one here, but what if they hadn't been there? In this case, yeah,
2594960	2599360	they wouldn't have made it because the block was in the way, right? We also have a model of
2599360	2603120	intention inference, but I'll sort of skip that. It's basically just saying, okay, if you have a
2603120	2607760	generative model about what an helping or hindering agent would do, you can then condition on the
2607760	2612880	observations that you see them acting and see what's more likely that they were helping or
2612880	2617360	hindering given the actions that they carried out. So I'll just give you a few more examples of the
2617360	2622160	sort of video clips that we showed to participants. That's a diagram of the one that you've just seen.
2622160	2629520	Here's another one where kind of, you know, blue is sort of extra mean, you might say. There was
2629520	2634880	already a block in the way, but they put another block in the way. What the heck? Yeah, really trying
2634880	2643840	to be helpful through adversarial actions. So here's another one here where blue is sort of
2643840	2648800	laudably helpful, but like, you know, was not really necessary, but maybe looks nice.
2649760	2652480	Here's a case in which sort of things go wrong.
2655840	2659520	Where blue was maybe trying to be helpful, but actually sort of made it worse, you know,
2659520	2664320	the reactions that they took. And then here's another one. We had a large number, so I'm just
2664320	2669760	showing like a subset of them. So this is one where blue could have easily hindered if they had
2669760	2676400	wanted to, but didn't, because they could have just pushed it into the way. And so then we now
2676480	2681520	have to again, yeah, try to capture whether we can, with our model, capture the counterfactual
2681520	2685440	judgments that people are making. And we sort of can, there's not as much kind of variance here,
2686000	2690080	at least in the predictions of the model. So this model is sort of okay-ish. It captures the trends
2690080	2693600	overall, but there's more variance in people's judgments that is not quite captured by the
2693600	2699520	model yet. So we're still, this is sort of more ongoing work. In terms of intention inference,
2699520	2703760	it's fine. So it can also kind of infer whether the person was helping or hindering,
2703760	2707440	but also here, what you see is stuff are bunched up that the model all gives a hundred to,
2708480	2713280	where there's still some differentiation that people make, but sort of mostly captures what's
2713280	2718400	going on. And if we now look at the responsibility judgments, and we try to do the same thing
2718400	2722240	initially that we did with the billiard balls earlier, that we just take the counterfactuals,
2722240	2729040	like on the x-axis, and try to predict the responsibility here on the y-axis, it's okay-ish,
2729040	2732560	but not, you always want, when you do computational modeling, you always want them
2732560	2737200	nicely line up on the diagonal. And that's not really what was happening in this case,
2737200	2740800	whereas for the billiard balls, we have this very simple counterfactuals nicely predict
2741440	2747040	the causal ratings. But if again, if you have a model that incorporates also the intention
2747040	2753040	inferences, like into the predictions, now they do sort of more nicely line up on the diagonal.
2753680	2757440	Again, suggesting that when it comes to assigning responsibility for agents,
2757440	2761840	it's not just the causal role that matters. It also matters what the actions that they took
2761840	2765440	tell me about the kind of person that they are. In this case, it tells me something about
2765440	2769760	their intentions, like they try to be helpful, or that they try to be hindering. So the both of
2769760	2775040	these components. And just to give you a sense of an example where we need this kind of intention
2775040	2779360	part, like that's back to that mean one where the blue one pushes another one into the way, right?
2779920	2784240	And so just to help you kind of interpret the bars here, the counterfactual, that's the condition
2784240	2788400	where we asked them, would red have succeeded if blue hadn't been there? That's basically our
2788400	2795440	causal model. And they don't think so, right? The pink, pink, purplish one is like very low,
2795440	2799600	right? But also when we asked them what the intention of the blue one is, they think, yeah,
2799600	2803440	was really hindering. So here zero means hindering and 100 means helping. So they think, yeah,
2803440	2808400	they were hindering. So even though they say that, yeah, the blue one didn't really play a causal
2808400	2811680	role, they still give them quite a bit of responsibility, like in the blue one on the
2811680	2816720	right hand side. So that's one case, at least, where currently we need this other part. So they
2816720	2819520	think, yeah, blue blue's actually make no difference, but they were definitely trying to
2819520	2823120	hinder. And so, yeah, I still give them some responsibility for this outcome.
2824480	2830640	Okay, so sort of almost last slide. Because we have these agents like recursively thinking about
2830640	2835120	one another, an interesting setting that also can happen here is that you can actually hinder or
2835120	2840400	help one another, again, maybe also like in the, in the advisor, advisor setting, not by actually
2840400	2844880	making any change to the physical world, but changing somebody else's belief. So I just want to
2845360	2848880	show you that example. And maybe you'll get that intuition from the setting here.
2854960	2860160	So very, very mean, very, very sad. Because it looked really like blue was going to help,
2860160	2867520	right? And then they didn't, right? And here's just one participant, what they're saying,
2867520	2870960	oh, blue tricked red into thinking she was going to move the box to help. But then
2870960	2876080	once red was stuck on the side of the wall, blue left the box where it was, very sad, you know.
2876080	2879840	And a lot of people say something along those lines. We also had one condition where we just
2879840	2883600	have them give explanations of what happened, right? And here the interesting part, right,
2883600	2887520	is that the hindering is not happening because blue changed anything about the world. They didn't
2887520	2891280	move a block in the way or something, but they hindered because they made red believe that
2891280	2895120	they were going to be helpful and then they weren't, right? Here, if blue hadn't been there,
2895120	2898960	red would have just walked along on the outside and they might have made it, you know, anyhow,
2899040	2903360	even without blue. And this happens because they're recursively thinking about one another,
2903360	2906320	right? And red things like, oh, blue is taking actions that are going to help me
2906320	2912400	so I can take the shortcut. And then it turns out I couldn't in this case. Okay, wrapping up.
2913200	2917120	So this was the second part where we, I guess, applied this model now to at least a simple
2917120	2921360	setting where agents are interacting with one another, helping and hindering one another,
2922320	2926320	that in order to judge whether somebody helped or hindered, I again think that you need this
2926320	2932480	process of counterfactual simulation and that responsibility judgments are sensitive both
2932480	2936640	to the cause of the world that somebody played and what the actions tell us about the kind of
2936640	2942160	mental state that they had. Just to conclude, so together, hopefully, this sort of set of studies
2942160	2946400	gets some evidence that people seem to be constructing these rich mental models of the
2946400	2950080	world that we can get evidence for in different kinds of ways, like through eye tracking and other
2950080	2956400	tools. By imagining interventions like on these mental models, those allow us to compute the
2956400	2960960	counterfactuals, which I think are important for assigning responsibility, giving explanations
2960960	2966400	and so on. And that this counterfactual simulation model that I've been kind of developing can then
2966400	2970640	be relatively flexibly applied to physical and social events, where you think that the main
2970640	2974400	thing that's happening is that your model of the world changes and maybe the exact
2974400	2978400	counterfactual cooperation that you're carrying out changes, but otherwise the framework sort of
2978400	2983520	holds. So with that, I want to thank the main people who helped me do this kind of work,
2983520	2987680	and then maybe you for your attention. And there's a little bit of time for questions.
2999520	3004960	So one thing I'm curious is, I assume notions of causality are probably somewhat universal,
3005040	3009760	but especially issues of moral judgment, intention are likely dependent to some extent on
3009760	3013680	environmental factors, cultural factors, those kinds of things. And so I'm curious if you've
3013680	3017040	either observed those in your experiments or if you have some way of controlling for those
3017040	3023440	factors when you recruit participants. Yeah, so that's an interesting question. And I think
3023440	3030400	even notions of causality actually, there are cultural effects like who you see there. So
3031280	3036240	when making causal judgments, there's often, there's basically like in many cases,
3037040	3040880	what's called the problem of causal selection, how do I even decide what thing to pick out of
3040880	3045680	as the cause in the first place. In my setting, very often I've kind of made it pretty easy,
3045680	3050480	and I've sort of constrained it because I already told you like these are the possible causes,
3051520	3055440	but in the real world it's not like that. And it's sometimes, we may see something,
3055440	3059680	we may see a person as a cause, or we may see a system as a cause, or we may also
3059680	3064400	see the kind of counterfactuals that may come to mind to us may also depend on what our background
3064400	3069680	is. And it often tells us something about, oh, when somebody then gives a certain counterfactual,
3069680	3075040	it tells us quite a bit about them. So this comes up in the context, for example, also of victim
3075040	3079840	blaming. Like if that's the counterfactual that came to mind to you, oh, that tells me something
3079840	3086240	like about you. So I would say that even in that context, there are strong kind of interpersonal
3086320	3092480	and cultural effects that affect how we attribute causality. Now when it comes to intention inferences,
3092480	3096800	I'm not sure that that process in and of itself, that at least to me feels relatively
3098800	3103280	whatever universal, that we kind of, we have to engage in that all the time by trying to
3103280	3108880	predict what other people are intending in the way that we interact with them. But then again,
3108880	3115040	how maybe then judgments in this case of responsibility or morality like draw on these
3115040	3120080	different components, for example, that I've laid out here, no claim that that is in any sense
3120080	3126640	sort of universal. But it might very well be that in certain cultures like this kind of what I take
3126640	3131280	here to be more the person component, right, may have a stronger influence on responsibility
3131280	3136560	judgments and in others, it might mostly be about causality. I certainly in my experiments
3137840	3141520	for individual participants see a lot of variance along the lines. But there's some people that
3141520	3145600	don't care about even the intention part at all. They just say like, oh, when it's about responsible,
3145600	3150400	I just look at what would have happened if they hadn't been there. And then other participants,
3150400	3157120	judgments are suggesting that they care about the intention part much more. But I have not yet
3157760	3161840	engaged in the kind of work that then tries to explain why is it, why is it that this person
3161840	3166480	casts so much about causality and why is it that this person casts so much about the intention
3166480	3175840	part, for example. Thank you. I'm going to hug the mic actually. I'm interested, like, do you think
3175840	3181920	this model applies to other settings? Because all of the examples were sort of like physical or
3181920	3187360	agents taking physical actions. So if you had just like a verbal description of some social
3187360	3191840	scenario where there's like speech acts that are causing things, do you think it would work the same?
3192320	3198080	Yeah. Yeah, that's a great question. So would it work the same? So my sense is like, yeah, in a
3198080	3208080	similar way, so there's a number of things here, I think. So we have applied the model a little bit,
3208080	3212880	like this kind of counterfactual simulation model in the physical world, also two speech acts.
3212880	3217280	And there it's in the context of like, we were basically jealous of, you know,
3218240	3221840	for those of you who remember full wolf, you had these different words, right? And we were like,
3221840	3226080	oh, our model can only do like cause and prevent. That's kind of sad. But there's other causal
3226080	3232720	expressions, of course, right? Enabling, affecting, letting, allowing, and so on. And it's going to be
3232720	3237120	a little bit of a of doing, but I'll get there. So we were trying to see to what extent this
3237120	3241200	framework that we have could also allow us to explain differences between these different
3242000	3247680	expressions, right? And, and this also comes up, you know, in philosophy, like even questions,
3247680	3252080	so the question versus killing versus causing to die, even people like in cases of abortion,
3252080	3255920	you know, the way that you talk about it, right? Again, reveals something, you know,
3255920	3260720	how you think about it. And in general, right, like this distinction also, when you have that as
3260720	3265280	an alternative that you could have said killing, but you chose causing to die, it suggests maybe
3265280	3268240	a more roundabout way in which something happened, right? Like the person killed it,
3268240	3272880	caused them to die. You think, yeah, it would be weird to say that someone caused them to die
3272880	3276720	when they like, you know, directly walked up to them and, you know, shot them. This also came
3276720	3282000	up recently or still coming up these days, actually, with the case of Alec Baldwin, Rust,
3282000	3286080	like in the movie, right? The way that people talk about it was it will hold the gun that
3286080	3290560	discharged or something rather than, you know, shot the person, right? So it matters a lot,
3290560	3295200	basically, like in this case, the choice of word, right? And the, and the, in some sense,
3295200	3299520	the counterfactual alternatives you could have had, right, for them, the image that it's creating
3299520	3304160	in the listener in this case, right? So the fact that, oh, you chose this expression suggests to me
3304160	3308880	that the scenario must have been such, like rather than such. So that's at least the minimal way,
3308880	3313040	I think, in which it applies also to, to thinking about speech acts, right? And thinking like,
3313040	3317200	yeah. And of course, you could think like, oh, you know, again, take the advice example, would the
3317200	3322160	students still have done that if I had not said that, right? So we are obviously causing each other
3322240	3326960	a lot in the way that we talk to each other. And sometimes, you know, yeah. Also, of course,
3326960	3331920	after talking, I might think like, oh, I wish I had, I had answered this question from differently
3332480	3335680	than what I actually did. And I regret it, right? And things like that.
3338480	3343360	Yeah. So on a similar note, I'm wondering if you have thoughts on how possible it would be to
3344240	3349680	use this model on society, large scale societal events, their divisive, such as what cost a
3349680	3356640	person to be elected, what cost code outbreaks, or what causes climate change, like how possible
3356640	3361360	would it be to apply this to those events and also what challenges you foresee? Yeah. Yeah,
3361360	3366800	that's a great question. And, and so, so I had the example of example at the beginning with like,
3366800	3370080	oh, did the fall of Lehman Brothers caused the financial crisis, right? That's sort of like,
3370080	3374560	large scale. And I don't know, right? And, and, and partly it might, so, and there's a few options,
3374560	3379040	right? One is like, okay, just like totally punting, right? And saying like, okay, well, if the system
3379040	3383600	gets sufficiently complex, such that I cannot carry out the relevant counterfactual computation
3383600	3388800	anymore, well, I just don't know, right? I cannot give that causal answer. That's, that's one version,
3388800	3393360	right? And there's another version where you say like, okay, well, to the extent that I can maybe,
3393360	3398880	you know, abstract away from a lot of the lower level details that say of some, so if I'm, if I'm,
3398880	3404000	if I have the capacity to build maybe a more abstract model, which, which I can now simulate,
3404000	3408560	right, then I might be giving you an answer sort of on that level, right? And so, but then it's
3408560	3412960	also half punting, right? Because now you have to kind of come up with a good model of how people
3412960	3418080	generate the right kind of causal abstractions for some situation that then allow them to compute
3418080	3423200	the counterfactual, because now it's not messy anymore, right? And another thing that I should
3423200	3427280	mention, and that quite a lot of the work on responsibility that I've, that I've looked at
3427280	3432320	particularly in groups, the sort of situations that you pointed out, like elections and, you know,
3432320	3436960	global warming, they're, they're characterized by, by large degrees of over determination,
3436960	3443520	right? Like in election, you hardly ever cast a pivotal vote, right? And, and so those also
3443520	3447680	traditionally were problems for counterfactual accounts, right? Because everyone can say like,
3447680	3452320	I made no difference, like if I fly every day, you know, that's not really going to make a difference.
3452320	3457280	And so, and there you can, and similar with election, why should I go vote, right? Because
3457280	3460720	if my vote's not going to make any difference, right? And there at least models have been built
3460720	3465600	that then say like, okay, well, it's not, you're not off the hook, right? It's basically saying,
3465600	3469280	even if you would not have made a difference in this particular situation,
3469280	3472800	maybe the degree of responsibility that you have for some election, for example,
3472800	3477120	maybe related to how close you were to making a difference to the outcome, right? If it's like,
3477120	3482160	if the outcome is 6-5, you feel very responsible. If it's like 7-4, a little less. If it's like
3482160	3487840	8-3, a little less, right? But not, but it shouldn't go to zero, right? And then, and then as it,
3487840	3491520	maybe now relates to kind of, you know, global warming and so on, part of the challenge then
3491520	3496960	from the more like, you know, what do we do about it? Side might be like, okay, how do we make it
3496960	3503360	such that people don't perceive a sort of, you know, going to zero sense of responsibility,
3503360	3508800	right? Such that you feel like actually the actions that you do make a difference to the outcome.
3508800	3516160	And so, yeah, so that's, so I think a mix of thoughts, I guess, in response to your question.
3517120	3522160	So we're about it, time. Is there a reminder if you are here? If you're logging attendance,
3522160	3526400	make sure to grab one of these code words up at the front and give Toby a compliment on his
3526400	3530800	talk on your way out, maybe make you come up next door. Let's thank our speaker.
