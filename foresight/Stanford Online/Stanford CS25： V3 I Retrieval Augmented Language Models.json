{"text": " Hey guys, welcome to our last lecture of this quarter. And we're very happy to have Dawa here. He's the CEO of Contextual AI, the Enterprise LLM company, as well as an adjunct professor in symbolic systems here at Stanford. And previously, he was the head of research at Clicking Base, and before that a research scientist at Facebook AI Research. He received his PhD in masters from the University of Cambridge, as well as a master's in logic from the University of Amsterdam, and studied philosophy and incognitive AI in undergrad. And his work focuses on machine learning as well as NLP, specifically on developing better models for language understanding and generation, and better tools for evaluation and many times. Yeah, give it up for Adela. Thank you. So I guess I have to sort of stand here in the corner, so people can see me on this move as well. Yeah, thanks so much for having me here. So I asked Steven what I should talk about. There were a couple of things I could talk about, multi-modality or evaluation. And this was the preferred topic, I guess, because the others were already covered. So yeah, I'm very happy to talk to you about everything retrieval augmentation. I think this is really one of the coolest topics right now in our field. So I'll just give you an overview of what's been happening and what I think are the interesting questions to think about. So first of all, obviously, in case you've missed it, we are in the age of language models. And I just wanted to do a quick poll here in this not super big audience. I guess there's more people on the Zoom, but who invented language models? If you thought OpenAI, then I'm angry with you. So actually, this is a very, very old idea. So the idea is just you take a sequence and you factorize out the token probabilities. And so it wasn't invented by OpenAI. It's not like a few years old. It's actually several decades old. So I'm bringing this up because I was talking to someone and they were like, OpenAI invented language models. And I was like, they're kidding me, right? So I went back to the literature and this is the oldest one I could find actually, 1991 first neural language model. There's a very nice paper from 2003 from Bengio where they actually have like word embeddings and everything already in there. So obviously, these are LLMs, not LLMs. And as it turns out, if you make them really big and you parameterize them with these massive neural nets, then you get something really powerful that really shows emergent properties. And that's why we're also excited in this stuff. So if we think about this from like a classic CS perspective, there's input output, right? There's this kind of thing in the middle, it's the generator. So we take a sequence, the input sequence, and then the task of the model is to predict the next token. Very, very simple model. And so, you know, that's why it was so easy to come up with this in 1991 already because it's like the idea is very intuitive. But for a long time, what was really broken with this was the user interface. And this, I think a lot of people kind of misunderstand what chat GPT was about. That's really what chat GPT fixed. So that initially you had to come up with these very weird prompts in order to get your language model to do what you wanted it to do. And humans are terrible at this, right? So we're much better at sort of telling people or things around us what we want, right? So if we have a dog, we say sit, we don't prompt it in a very weird way so that it sits, right? And it's the same with the language model. If you wanted to generate some red lyrics in the style of a pirate or Shakespeare or something, then you tell it generate some red lyrics in the style of a pirate, right? So that kind of instruction data actually turns out to be super, super rare in just web data. So what you need to do is you need to fix the user interface to the language model. And the classic recipe for doing that is the sequence basically that chat GPT used. So you prompt the model in a specific way. You will instruction fine tune the model and you can do some alignment or LHF, whatever you do on top of that. So that's the first thing. So now you have a working language model with a working user interface. So are we done then? Obviously we're not. So right now language models are kind of taking the world by storm. But if you talk to anyone, especially in an enterprise, for example, where they have very strict accuracy requirements, they will tell you that they can't really productionize this yet. And the reason is because there are all these familiar problems, probably a bunch of you are working on these problems right now around hallucination. So these models, they kind of make up stuff very often with very high confidence, which is even more scary in a way attribution. So we don't really know why these models are saying what they're saying stillness. They go out of date. And so this was a big problem with sort of chat GPT, not knowing anything that happened after a certain cutoff date. And they keep updating it every once in a while. But you want to have a system that's always completely up to date that never goes still. You want to be able to revise the information in the system. So if you're a European organization, you have to worry about GDPR, which means that you need to be able to remove information from the language model or maybe revise facts, which we don't really know how to do. So again, this is a very interesting area of study for a lot of folks, model editing. But so this is something that we really want to be able to fix. And then there's this big question of how do you customize these models? So different people have different use cases. You have different data. If you're a company or if you want to have a language model on your own data, how do you make it work on your own data? So one of the solutions that everybody has started using right now is to couple it to an external memory. So that's really just rag, right? This whole lecture is basically about rag. But the way to understand what is going on here is we have this generator just like before. We have the input and the prompt just like before. But now instead of just giving those two things, we give this additional context. So we contextualize the language model using things we've retrieved. And the retriever is very often pretty simple. It's just a query in the document encoder. And then you get a bunch of documents, you give them as context to the model. So super simple architecture. And I think it's useful to think about it from the perspective of these two separate paradigms. So if you've ever taken an exam, I'm sure you have, right? You can have a closed book exam where you have to memorize all of this, so you have to cram all the knowledge into your parameters, your neurons. Or you have an open book exam where you have all of this information in the book that you can access when you do the exam. So it's a very similar thing with rag, right? You can just make it an open book setting where you can give it access to this external information, Wikipedia or something else, or basically the entire internet, and then have the language model do its job without having to memorize all of it in its parameters. So Dr. I think useful distinction here is that cramming everything into your parameters, that's the parametric approach. So what we're doing with rag is we're adding this non-parametric retrieval component. So you might call this semi-parametric if you want to give this a name. All right, so why does that actually solve these issues? And so the answer is basically that if you have this separate index, right, this separate retriever, you can swap it in, you can swap it out, you can replace it with a new index so you can really customize it. And so you can customize your language model system for what the user really wants to see. And then obviously you can update this index, so it doesn't really go still and you can revise it if everything goes wrong, if anything goes wrong. The other thing you get is grounding. So that's initially why I became interested in this kind of architecture, because I was thinking a lot about grounding and multimodality and things like that, and actually one really nice way to ground things is to find some other information that you can ground your generation in. So you really want the language model to only say things that it has evidence for in this other piece of text, or even multimodal data that it retrieves separately. So if you do that, then you get less hallucination, because you can always point back to your source, it's always grounded in your source. And you get attribution, because you don't know why the model is saying what it's saying is because it founded this thing here, is that clear? All right. So for the rest of this lecture, we're going to talk about this basic architecture. And so it kind of looks like a pretty simple thing, right? But there are actually lots and lots of questions you can ask about what the system should really look like. And this doesn't even cover half the questions you can ask. So it really is about how do we optimize this entire system? So we have the separate components, the retriever, the generator, and then there are things like this query encoder, how do we encode queries, how do we do the retrieval, do we update the document encoder, or how do we actually define a document, right? Is it like a full document or is it a paragraph or a chunk or a sentence or a couple of words? So there are lots of questions to ask. And as you'll see, there are lots of possible answers to these questions as well. So this is what we'll cover. So there are lots of architectures going into these questions. And I think as we go through them, it's useful for you to think about what happens during training time and what happens during test time, right? So during training time, it's really, okay, we have the language model, we have this retriever. Which one do we update? How do we update them? How do we train this entire system? Do we maybe not train it at all? Do we pre-train it from scratch? Do we initialize it with components that were already separately trained? These are the kinds of questions that you have to answer if you want to design a system like this. And then during test time, you have this entire system, right? So actually multiple models in a way that are working together. So there's also different things you can do there, right? So give it different indices during test time or manipulate kind of how you're sampling things like that. So the starting point for all of this stuff, I think if you ask someone now, like, what is RAG, they will think of this thing. So this is frozen RAG basically. There's no training here at all. So going back to this question of train time, test time, there's only test time here. Train time happens separately with these kind of black box models that we don't necessarily have control over, right? So there's this document embedding model as whatever is currently at the top of some open source leaderboard. You use that to get some vectors that you then use to create this vector database. And then the vector database just does search and it gives the information from the search to the language model. And it just passes it as the context, right? So this only works because of in-context learning. And I think as a machine learner myself, this feels very inelegant. So what this lecture is about is, can we do better than this frozen thing? So let's start from the left side of this. OK, if we want to outperform this frozen thing itself with just the vector database, what would that look like from a retrieval perspective? And the starting point for everything retrieval is TFIDF. Does everybody know what TFIDF is? No? OK. So TFIDF is basically a sparse retrieval method where you have a score function that looks at documents and queries, so E and Q. And then there are basically two terms that matter. One is the TF, the term frequency, and the other is the IDF, the inverse document frequency. So this inverse document frequency is actually a really nice idea from Karen Spark-Jones, really underrated researcher. She's done some amazing work. But the basic idea is that you want to look at the words that are very special, so that don't occur in lots of different documents. And so the overlap between the word the doesn't really matter, right? Like the occurs everywhere. So you want to have sort of the special words. So that's what TFIDF does in a nutshell. It gives you a score for document query overlap. And then you can do all kinds of things here with how you weigh it. So there's all these weird different parameters like this B and things like that that allow you to make it better than just having the TFIDF score. So there's a couple of tweaks you can do there. So BM25, actually, in case you're wondering, stands for Best Match 25. So I tried to discover where does the 25 actually come from? That's because the preceding 24 experiments failed. So it's literally the 25th one that seemed to work. And that's why it's called BM25. Bizarre. But so this is sparse retrieval. It's just counting words. So you have this massive, massive vector of all these word occurrences. It's sparse because most words never occur. So it's sort of like a vector of vocabulary size dimensions. So most of that is obviously zero. But so that's actually kind of a nice property. If you want to do fast search on a CPU, because on a CPU sparse product, it's very easy to compute. So this is used in the system called Dr. QA, which is really one of the first neural instances of this open domain, sort of open book question answer in paradigm. So you have a question like how many of our cells in habitants, blah, blah. So you want to ask basically Wikipedia what the answer is for this. So then you have this document retriever based on the sparse. So BM25, I think in this case, retrieval methods. You pass that to this. I think this was still by LSTM at the time, a document reader model. And then that model gives you the answer. So this, I think, is really the first instance of having sort of this separation between a retrieval and a generator system that you use for answering complicated questions based on sort of open domain knowledge. So after the sparse stuff, there was a bunch of work on dense retrieval. And so the advantage of dense retrieval, so this is just like word embeddings, basically vectors, like they're dense now, no longer sparse. So they're much smaller in terms of dimensionality. And a nice advantage of dense retrieval is that it's not really about specific words. So if they're synonyms, you can still find the relevant document, which you couldn't really do with a sparse representation. So that's really the advantage of dense is that you get like semantic similarity. So you can do this over word embeddings. That doesn't really work all that well. But at the time that people started thinking about this, Bert was already out there. And Bert is really great for giving you a vector representation for an entire sequence of words. So it sent this representation or a passage representation. So there are all these cool systems like ORCA and DPR, the dense passage retriever, where they essentially use the retrieval as a kind of latent variable in the system. And the way to get the latent variable to work, to be good enough essentially to train the entire system, is to pre-train the retriever on relevant information. So for ORCA, they do something called inverse close. So they do kind of a close task where you want to find passages that are sort of relevant to the preceding passage. And in DPR, they just train it on the supervised thing. But really, the core idea here is that, as you can see in this graph here, you can do better than BM25 if you add lots of documents. And the way you compute the score function is much simpler. It's just a dot product, right? So the nice thing about dot products is that you can do them very, very efficiently on the GPU as well if you know what you're doing. So what you really want to get at is Maximum Inner Product Search MIPS. So this is one of the kind of core ideas of a lot of this stuff. And you can do MIPS with ANN, Approximate Neighbor Search. And so there's this really brilliant piece of work out of there. For my colleagues at the time, I called FACE, which really underlies all of these modern vector databases, right? So all the popular ones are sort of re-implementations of this FACE idea. One is in Rust, one is in Go, but it's all basically the same idea. It's just FACE. And so FACE really powers a lot of this stuff. And whenever somebody tells you something about a vector database, just think about FACE, very fast dot product. So obviously, you can go beyond dot product. Yes. What is FACE? What is FACE? So it's an open source library, Facebook AI similarity search. Yeah, it's something. Yes. No, so it's just basic off the shelf, ANN algorithms. Yeah, so there are all kinds of different, I don't know if you would like product quantization and things like that. So you have a bunch of vectors. And you can just compute the full dot product, which is sort of inefficient, right? So what you can do is try to compress subspaces of the vector and then just look at the kind of centroids. So you can quantize sub-vectors of the full vector and then do much faster search over just the centroids. It's a good question. Any other questions? All right. So about this dot product idea, right? So what we have here is some people call this a Siamese network, I guess it is, right? So you have two different BERT models or whatever your encoder is here. And then at the end, you get these two vectors and then you just do dot products where you get one single score. But you can do all kinds of much fancier things if you're willing to give up on this buy encoder approach. So a really nice example from one of your colleagues here at Stanford is Colbert. So what this does is late interaction. So instead of just having this dot product here, you have a kind of more complicated version of computing score where you aggregate over sort of maximum similarity scores between different words. So I only recently actually discovered that this is called Colbert because of the late night show Colbert. So it's sort of Omar's joke actually, this name, but just so you know, if you run into it. So, but I think if we look at kind of where the state of the art has been going now, one of the nice things about inspector databases is that they're super efficient, right? So dot product is much more efficient than this late interaction stuff, especially if you do the approximate nearest neighbor search. But there's been some really cool work. So things like SPLATE, they basically have sparse meat dents in a way. So one of the big problems, as I said, with sparse is that you can't really handle synonyms and things like that. But what you could do is take a dense model, like a bird model, look at kind of this one word in your sequence, try to see which other words fit in the same slot. So that gives you the synonyms. So now you can give all these synonyms to a sparse vector, and then you can just do sparse dot product. And so have a much, much more efficient way to do search without sort of giving up on all the cool stuff that you get from a dense representation. So that's one thing. And this other idea I really like is called dragon. So this side, I think, is really the best generalized dense retriever. So if you want to take something off the shelf right now and just go to hugging face or something, then this dragon or dragon plus is probably the thing you want to use for a dense retriever. And the way they train this is through this progressive data augmentation strategy to make the model better and better over time by sampling very difficult negatives. And that gives you very good representations. And so the other thing about this, I think this is the only sort of final point about retrieval in general, is that what we see happening right now, if you look at sort of the developer community around drag, is that they're all doing hybrid search right now. So you can actually just combine the search results from your sparse, be in 25 or whatever thing or displayed, and you can combine them with your dragon. And then you'll get this ranking that works even better. So then you kind of get best of both worlds, but then you get all these questions about how do you combine the results. Any questions on this part? Oh, can you hear me? Yes. Oh, sorry. On the earlier slide, has there been any work on benchmark, how much less hallucination rag incurs over closed book question answering, for example, directly asking the large language model the question, has there been any benchmarking studies in this? Yeah, so there's a great paper, if I can say so myself, on the fact that retrieval augmentation reduces hallucination. It's from 2021, I think. So yeah, you can just find, if you literally look for retrieval augmentation reduces hallucination, then you'll find the paper. Oh, thank you. Well, let's see, is there a picture of your dance approach, and why do you need swaps? Yeah, so very often you want to have a very precise word overlap for things where you don't want to have the synonyms or the kind of nearest neighbors. So if there's like a brand name or something like that, then like let's say your brand is Apple, you don't want to find stuff about Paris. So that's what you would do with a dance retriever. So it really kind of depends on what you want to use it for. That's why hybrid is probably the way to go. It's a good question. But with a dance, it's contextualized in many ways. Should it realize Apple, the company, would be different from that? No. So if they were actually contextualized, then yes, but very often it's a frozen retrieval system. That's one of the problems with all the frozen rag stuff. I might be missing something very, very soon. So the sort of document and the query, they're the same, right? So they're either sparse or they're dense. But so if they're sparse, the components of the vector are literally the other words. And you just finalized when you're thinking about the thing that creates the names? How are you getting expressed here? So it literally counts, right? So basically it's one big matrix of documents as rows and the columns are the words in the documents. And then you just count how often a word occurs in a document. So that's a sparse. Yeah. And so in the field, we call them sparse embeddings or sparse retrieval because most of that vector is zero. Because most words don't occur in that document. Does that make sense? Cool. So let's talk about doing slightly better. So going back to Steven's question about, okay, we have this kind of retrieval thing, but how do we actually make this retriever good for the context that is going to be used in? And so can we contextualize the retriever for the generator? Even if it's a generator where we might not have access to the weights. So it could be a GP4 model, we just send it to some API, we get some stuff back. And so one paper I would like is called Replug. So just to kind of explain what this looks like. So you have this context, you have a retriever that we do the standard retrieval step with. This is a dense retriever. And now, sorry, and now you compute the likelihood. So basically just normalize the scores that you get for the top K documents to get a distribution here. And then you'll give each one of the retrieved documents separately to this generator, to your language model. So you can look at the perplexity of the correct answer for that language model. So now we have these two probability distributions, or two likelihoods essentially, and we can minimize the KL divergence to make sure that we can actually retrieve the documents that lead to the lowest perplexity on the right answer for the language model. So super simple idea, works really, really well. And the nice thing about this is completely agnostic of what happens upstream. So this will work for any sort of encoder decoder for any language model. What you need is a perplexity score, but for most language models you can get that, not necessarily all of them. So that's one thing. And then there's this other really nice approach. So in the retriever, you're literally updating the dense representations, right? So your encoder basically for your dense representation. That's a good question, we'll get into that a little bit more. So there's another paper on in-context retrieval augmented language models, where the whole paper is basically about just doing BM25 and just giving stuff directly through the context of the language model and things kind of work. So it's sort of frozen rag, but even more primitive in a way where the retriever is this very old sparse algorithm, but it works really, really well. But then they have this really awesome section where they show that you can just have this reranker on top of the BM25 results, and you can backdrop into this reranker. So now you still keep the language model completely fixed. So that's sort of this part of the loss here. So you have kind of a stop gradient on the parameters data. That's just your language model. But now you have this kind of rank function here that you can backdrop into, right? So that's your reranker. It's basically it can be a burden model or anything like that that works on top of the things you initially retrieved from your BM25. And now you have this birth reranker that you can backdrop into. So this also works really, really nice. So we're slowly progressing towards having a system that is much more optimized for being properly retrieval-augmented in a way where it's useful and contextualized for what you want to use it for. So yeah, just to point out kind of what that looks like with this reranker. So you just have this extra step essentially, right? So we have our retriever, then we have our reranker, then we have our generator and our output. Any grades to the language model? No, not necessarily. So for this one, you do, yeah. But so for a redeplug, you don't, right? Yeah, for this one. Yeah, yeah, yeah. So basically, yeah, you need to get- Do you guys provide them? Not all of them. Some of them do, but yeah, there are all kinds of tricks you can do on top of that. So basically, the question is how do we get sort of gradients flowing into this, right? So if you don't actually have access to the full parameters of the model so that you can backprop all the way through it, then you can do a reinforce style loss on the retrieval, and then you just pass the kind of log likelihood if you have access to that, or some other kind of black box function. All right, so the next thing you can do is to optimize both the retriever and the generator. And so this really starts getting to the proper kind of contextualization of the entire architecture where you want everything to work together, right? So rather than having this frozen thing where everything is basically not aware that the other part exists, right? It's like two halves of the brain. They're not talking to each other. One is your retriever, the other is your language model. There's no connection. They're just like sort of like something is thrown over the fence and then you hope for the best. So instead of that, we have everything much closer and learning together. So one of the first ways of doing this with the generator was ragged retrieval augmented generation, which we did at fair in 2020. And it's very similar to what we've already seen. We basically have this retriever here that works over different documents. You get some score function that gets given to this generator that generates the answer. And now you want to backdrop all the way and update your generator as well, right? So in the previous two architectures, we saw you keep the generator fixed, you backdrop into your retriever, but here we update everything. Well, not exactly everything as you'll see, but we'll also update the part of the retriever and the generator. So in this ragged model, we actually have two different ways of doing this. It's probably something that when we talk about this, if you think about this long enough, then you'll think like, okay, but when actually do I need to retrieve? Do I retrieve every time I generate a new token or do I just retrieve once and then generate an entire sequence, right? Or maybe I want to retrieve every end tokens, right? So these are hypergrams, or maybe I want to learn when to retrieve, as we'll see that's also something people have done. So these are two different ways to do it. And what we do in this paper, basically the whole point of the paper is that this frozen thing doesn't really work all that well, right? So I think what people call rag now is usually referred to the frozen thing, but the whole paper basically would never have been accepted anywhere if we had just done the frozen thing, right? The whole point of the paper is that you want to optimize it. And so at my company contextual, we call this frozen thing Frankenstein's monster, because it's really like you cobble together these different pieces, right? You sort of, yeah, it's really like Frankenstein, you just put it together and then it sort of walks, you know, but it doesn't really have to solve, it doesn't really actually work, because not the real thing. So that's great for everyone here, I think, because there are so many opportunities to do better than what most people are using right now. So one of the limitations of the original rag architecture is that it only supports a very small cave. So if you have lots and lots of documents, then the problem is that you have to fit all of them in the context, but how do you really get that to fit, right? So one thing you can do is you first encode things so that you get one single representation, or only diffuse for the top level representations, then you concatenate those, and then you just feed them to the decoder. So this is FID fusion and decoder. And as you can see, this scales to a much higher number of passages. And that leads to corresponding improvements in the scores that you care about. So that's a really cool idea. And so we're slowly moving towards more decoder-only architectures. So in rag, we have this barred model, it's sort of an encoder-decoder architecture, but here you just have this decoder that does some fancy attention over stuff that you retrieved before. And so another pure decoder language model architecture is this one, KNNLM, which I think is very elegant in its simplicity. So it's basically, you just have a normal language model, but you interpolate the normal language model weights with things that you retrieved. So basically, you have some sort of prompt, right? So like Obama's birthplace is, you go to your big corpus, you find similar things, you look at the words that come next to the similar things, you rank that thing, you sample your top K, you renormalize that. So now you have a bunch of scores, and now you can just interpolate between your retrieved kind of non-parametric memory scores and your parametric language model scores. So this is very late fusion in a sense, right? At the very end, you combine these two, and it allows you to reweight the pure language model probabilities or like this. So this works really well, and it scales especially well if you have a huge retrieval corpus. And so if you have trillions and trillions of tokens in there, you can have a much smaller language model that does not that much heavy lifting because you can really rely on this big source corpus that you're working from. And so that idea was exploited by this paper called Retro out of DeepMind, where they showed that you can have a 25 times smaller retrieval augmented language model trained from scratch, so really pre-trained entirely from scratch, that outperforms this 25 times bigger language model on the same data in terms of complexity, which is pretty impressive. So this architecture is much more efficient than a parametric model because you can rely on this external memory. So if your external memory is big enough, you can get pretty huge gains. So there was a lot of excitement about Retro when it was announced, but this is a DeepMind paper, so there's really no open source, nothing really to validate that this actually works. And so very recently, there has been a bit of work from NVIDIA called Retro++, where they have this hybrid between the Retro architecture and then they do basically RAG, sort of they put the top one or the top K results in the context of the language model after all. So it's sort of a crossover between RAG and Retro, and they showed some really nice results here, but I think it's sort of pointing to this big flaw, I think, is that why is there still no good open source Retro model? It probably tells you something about whether it actually really works. I spent a lot of time in my career trying to reproduce DeepMind papers that didn't necessarily always work. And so I think the same is true for Retro, and that's why we need to do this in-context RAG on top of Retro to actually get it to work. So doing retrieval over that big corpus is not that difficult, actually. So there are even distributed face packages, you can just do everything yourself. Yeah, so in terms of compute, it's actually not that hard anymore to reproduce something like this, but I've tried several times and it's not really reproducible. So the only way to get it to work is if you do this in-context RAG on top of the Retro thing, and then as you can see here in the results, then it actually gives you a gain over the pure GPT model. So it starts from a GPT and then they kind of retrofit as they call it the GPT model. So in short, I think there's still a lot of work to be done in pre-training these systems, really from scratch. And Retro kind of showed that it might be possible, but we don't necessarily know exactly how to do it the right way. And this is really one of the interesting open questions. Any questions on that? Online? No? Okay. Then we'll move on. So let's go all the way with the contextualization now. So with Retro and with RAG, what we actually did is we only updated the query encoder. So updating the document encoder is very expensive. So one of the first papers, actually kind of the OG of the non-frozen dense retrieval augmented methods is this paper called Realm. This is really like visionary work. This was basically the first kind of version that did this properly, where they updated it all the way, including the document encoder. So can someone explain to me why it's expensive to update the document encoder? So let's say we have a trillion tokens in our corpus. So now we go all the way. So we basically do a forward pass. We get a gradient at the end. Now we back propagate the gradient through the retriever. We update the query encoder. Now we have to update the document encoder. So what do we then need to do after we've updated the document encoder? We need to re-encode the entire internet. So basically every single gradient update, we have to re-encode whatever our index is. So if this is like trillions of tokens, it's like re-encoding the internet after every batch update. So that's not very efficient. Well, I think it does look like we've got a very general international change. So if you learn digital or other sort of stuff, like if you basically take your old activations and that sounds like a long, unpredictable change to your entire business. Yeah. Yeah, that's one way to do it. So there are a bunch of different ways to update the document encoder. So what they do in Realm is they basically do it for te batches. Then they stop, they re-encode the entire internet, and then they train again. So it's sort of asynchronous updates. They have this very fancy sort of sharding mechanisms where they take down certain parts of their entire index and then update them kind of on the fly. So you can do it. It's just very expensive. So one of the things that a lot of people have been thinking about, not exactly the Laura idea, but similar versions of that are around, like, can you make it more efficient so that you don't have to do this asynchronously? So one of the downsides of this Realm architecture is that it's really just a BERT model, but then you do this retrieval augmentation on a BERT model with other BERT models. So it's not really generative. It's not really gen AI in the modern paradigm. But if you want to read one paper on this topic, like, this is a very good one to read. The other one that is really, really good to read is this paper called Atlas. So Atlas is, so this is out of fare with a bunch of folks, the folks who did, like, RAG, and the folks who did FID, and really a brilliant set of people. And this is really a comprehensive analysis of everything that's happening in this architecture. So the first question they really look at is, how do we train this retriever? So we've seen a couple of versions of this, but which one actually works better? They haven't really been compared in a head-to-head setting. So one thing is we have this FID style sort of attention distillation. So that's really too complicated to go into detail here, but the others are actually very simple. So one is this loss we've basically seen before. So we've seen this, I think, with the in-context RAG one. So we have a stop gradient on the language model, and then we update the retriever. The other one is what we've seen with Replug. So this is basically exactly the Replug loss. So we have the KL divergence of the documents and sort of the improvement that you see when you give it that document. The other thing they have is basically the inverse of that one. So if I take this one document out, how does that affect my perplexity of the language model? And so this one I think is actually quite elegant because that really gets to like, how valuable is this one single document for me answering this question correctly? So they compare all of these different versions, and what you can see is that the kind of Replug style loss and this leave one out loss, they performed a lot better than all of these others. So this fixed retriever or no joint pre-training, these are really kind of the baseline sort of frozen RAG models or closed book. And as you can see, you can do really a lot better if you optimize things. And so this leave one out thing is probably the best I would say. So then the other question is how do you actually like train that entire system? Like what data or what tasks do you train this on? So they also experiment with a bunch of different versions. So one is doing prefix lm, if you're familiar with that. So they basically take a chunk that occurs somewhere on the internet, and then they predict the next chunk from that chunk. So it's really like sentence to sentence. So maybe like skip thought back in the day, but now you have this retrieval step where you predict the next sentence. Then they just do T5 styles or denoising. So that's mass language modeling if you're familiar with T5. And then they have this title for section generation piece. So I think the takeaway from this table is basically that whatever you do here, so they're using T5 models. So whatever you do here needs to be the same that your language model expects. So for T5, that's T5 style loss. And then the next sort of final question that they look into going back to what we talked about, how exactly do we update this retriever? So do we have to update the document encoder? Or do we maybe have to do some sort of re-ranking? Or do we maybe just update the query? And quite surprisingly, I think they find that just updating the query. So like in your original RAD paper is actually already basically good enough in many cases. So that's nice because it's much more efficient if you don't have to update your documents all the time. I think the real question here though is like, how good is your document representation to begin with? So you need to have a very, very high quality embedding model for this to work. If you don't have that, then this will not work. But if you do have that, then you get a very nice kind of query site fine tuning thing. So the Atlas paper is about trying to do few shop sort of language modeling tasks. So it's how many examples are given in the context. Yeah, so the main takeaway here is that if you compare like the close book equivalent model to the retrieval augmented model, you see very big improvements. That's really the only takeaway of this entire section. But I think that that's really saying something in terms of what we should be thinking about. How much time do I have until? Okay. All right. Other questions? Yeah, so there can be different. So in Atlas, the Atlas basically tries everything. So they also tried to see what happens if I train this on Wikipedia, but I swap in like a sort of common crawl index. And I think so in Atlas, but also in retro domain finding is just the more the better. So it's really just like the bigger your index, the more likely you are to find the exact right thing and then make the right prediction. Any other questions on this? Oh, yeah. Sorry. This is a question about the generator in the, I guess, the RAG system. So recently I saw a paper on Mistral 7B. So it introduces a lot of these new architectural changes like the sliding window attention to handle longer sequences at a smaller cost and the group query attention for faster inference. I'd like to like know your thoughts on designing a generator specifically for RAG, leveraging, for example, where Mistral 7B currently is. Because for example, like the sliding window attention, I could see how that could be adapted to the RAG case. Yeah. So maybe you're agreed on sort of what makes Mistral's special is a bit different from mine. So I don't think that the sliding attention window thing is actually that interesting. The reason Mistral works so well is because it's trained on a lot of data. You can do that more efficiently because you have sliding window attention. So you don't need to attend to everything. But so to answer your question, I guess you're asking sort of about the architecture of the generator if you know that there's going to be a retriever. So I think that's basically what Retro tried to do. So Retro actually, some of the people on the Retro paper are at Mistral now. So they have this chunk cross attention idea here. So you basically have a language model, but the way it does attention over the things you retrieve in your Retro architecture, they kind of get integrated into a model, not using the standard attention mechanism, but using this slightly different chunk cross attention. Okay. So I think the sliding window attention point I was trying to get at was that it uses a fixed window so that whenever you're doing the query key computation with the query vectors and the key vectors, you're using a fixed window attention. So I think my idea was to actually, one, use a dynamic window because for example, the rag case, if you use a fixed window when you're doing attention, it is possible that you actually are leaving, you're only looking at a fixed span of information. So if you could maybe adapt Mistral so that you could make it better for the rag case in, for example, the making the fixed window size, the dynamic window. Yeah. Yeah, I think it's an interesting idea. So for me, what Mistral is doing with the sliding window, that's basically like a confnet. So we had all these convolutional like light confnets where we would have word embeddings, and you would do confolutions over it and then pull, and then you would still get the information out. So it's not that the sliding window prohibits you from looking earlier, it's just that that happens higher up in your transformers. So I think that definitely is an interesting direction to think in. Yeah, so I think it's like not too crazy to say, are there any architectural changes that we can introduce into these seven billion parameter models so that they could be better adapted to the rag case? Yeah, so there might be. Yeah, I think one question is just how do you do the attention over things you've retrieved, which is what you're doing. Yeah, thanks. So just to make sure I understand, I mean in this retro model, you're retrieving each block, and when you struggle about putting a retrieval in the context, are you saying that you'll need to do it at the beginning and you don't do it at the block? Yeah, so in context, so it's not exactly every layer, so it's every token, so every step basically, not every block, so it doesn't make sense. So it's not every layer that you're doing a retrieval. Yeah, so every step. So this is kind of like what rag token is, so you retrieve every token, so you generate and then you can retrieve again, or in the case of retro, you can generate like a chunk and then you retrieve chunks again. If you look at the in-context case, you retrieve once at the beginning and then you give it. So here you don't actually give it as context at all, like directly to the model, right, so here you let the decoder kind of attend over it. Yeah, so I don't think cross-attention really works, yeah. Other questions? Yeah, inside the in-context case, the retrieving of the retriever is not necessarily, because of the large distribution loss, so I'm wondering inside of the cases, like what cases are really necessary need to evenize updates, or anyways updates for this argument. Yeah, so you do want to update the retriever, right, but only part of the retriever is necessary to be updated for a lot of these cases, but so I think it, so these are very specific data sets, right, natural questions, Wizard of Wikipedia and Fever, so they're really very kind of knowledge intensive tasks, so in that case, if you already have a very good system like DPR that is specifically pre-trained for those tasks, then you only need to update the query encoder, but so I would expect that if you move beyond this to kind of general language modeling things like retro, then you probably do want to update the document encoder, at least in a way where you can skate it. So I believe that in this part, it's very knowledge intensive, and actually a couple of very important topics, as long as we have a good office around knowledge of what many of the documents by those good models. Yeah, but so you need to learn how to kind of query into that index, right, so if you don't do that, then yeah, you don't get really good performance, so that's sort of like your closed book performance, right, if you just have the language model and you're just like, what does the parametric model on its own without the retrieval, what does it actually know? As you can see, there are pretty big gaps there, right. Other questions? Otherwise, I will cover other questions. No? Hello? Yeah, go for it. Okay, question, like so what about like more hierarchical retrieval, like I suppose there'll be methods trying to not just retrieve a single chunk, but there's some kind of like groups of chunks or something, or some rise versions. There's been some interesting work on doing that where you first tried to find, so you can have multiple indices and they can kind of cascade, right, so first you want to find the relevant document, so you have some document representation and then within that document you want to find the relevant chunk, so you can do it sort of that direction, you can also do it in reverse, I think I have something on a slide there where you can find the chunk and then sort of expand the context around it and then give that to the language model. So I think yeah, there are all kinds of interesting things you can do there. Cool, thanks, I guess another thing just like can you compare RAG versus like long context so efforts, so like there are lots of things like around just having a really long context and the extreme it could replace RAG, but I don't know like if it takes. Yeah, so everybody understands this question, right, so there's a trend where we want to have very long context language models, so that basically you can like take Harry Potter or something, just put it in the context and then ask a question like what is the name of like Harry Potter's owl or something, right, and then it can just attend over the entire thing. So attending over all of Harry Potter to answer that one question is super inefficient, right, so most of Harry Potter has nothing to do with the owl, so but you are still kind of reading it if you do it with the long context window, so that's why I think the doing it the RAG way where you have this non-parametric component is a much more efficient way to solve this problem, and if you actually look at the literature on long context windows, the way they solve the problem of scaling the attention mechanism is by making it very sparse, so they're basically turning it in, so that's a different kind of sparse, but they're turning it into a non-parametric retrieval problem kind of behind the scenes, so they're not actually all that different, if you want to scale long context then you're going to move towards a RAG-style architecture. Cool, thanks. All right, so let's talk about some other interesting questions, so one thing and I already alluded to this is when do we actually retrieve, so very if we're doing like if we want to like retrieve every token that's also very inefficient because I probably don't have to retrieve to generate the, right, I can probably do that on my own with the language model, it's sort of a waste to go and retrieve stuff, but if I only retrieve once at the beginning of the sequence that's probably also not great, so what we ideally want to be able to do is to say, okay sometimes I want to retrieve, sometimes I don't want to retrieve and I'm going to learn when I want to kind of expend the compute budget on doing the retrieval, so a nice paper where they have a stab at, this is called FLARE for active retrieval augmentation where they basically have the language model decide when it should do a search and what it should do the search for, so I think this fits in a general trend that you can see in the field around kind of agents, so we can talk a little bit more about that too. So this other question that I think we've also kind of covered already here is how do we train this at scale, so we can do these asynchronous updates, we can do re-rankers, we can do query side only, there's this really nice paper which is quite close I think to the idea you proposed where you first use BM25 to create a batch basically where everything is very similar in terms of what you've retrieved and now you have this kind of in-batch update, so it's sort of like a re-ranker where you encode the information that is just in your batch using this other model and now you can update this model on the fly, so you don't have to worry too much about doing the full kind of document side update and again here what really matters is like how big is your index, if you have an amazing index you can basically solve any problem just by looking it up, so rather than cramming it into your parameters you can just find it. This is a really nice paper called SILO, so one of the interesting things I think that's going to happen in the next year or two around language models is there and you've seen this already, there's a bunch of lawsuits against OpenAI and other places around where does the data exactly come from, so one very elegant solution I think is to have a RAG system that you train on data that you know is safe, so you can train that thing on Wikipedia but now during test time you can give it a data store that has maybe slightly riskier information in it, so this massive index of all the stuff on the internet including some things that are maybe higher risk, you can still have them in your index but your language model, your retrieval augmented language model I should say, you know that that thing is safe because it was trained on data that is public domain, so that's what they do in SILO and they show that that works really well, so that's one possible solution to a lot of the kind of compliance and legal risk around language model deployments. There's a great paper also from one of your colleagues around context getting lost in the middle, I think this is also kind of a fascinating phenomenon, this is on a frozen RAG system but language models are very similar to humans in what things they pay attention to, so if you give them a bunch of things that you retrieve, what they will look at are the first things you list and the last things you list and they will sort of ignore the middle, so if it actually respected the rank function then this curve would get down all the way, but it sort of goes up, so I think that's a very interesting observation which kind of shows that how brittle these these systems can be, so if you have a frozen RAG system it can be very very brittle where like the order of the retrieved context matters a lot in whether you get the right answer or not. It doesn't work on treating this as a very funny problem in the sense that the colleagues come back or those of like specifically going towards the interpretation, I'm going to try it out for that period that's going to inter-product with just the RAG. Yeah, so what I just described, somebody asked like how do you actually, so I said there are other ways to do this and then the question was how do you do that, so the way you do that is using reinforce, so yeah there has been work on doing that, so some of the older papers were playing with this, but one of the big problems with, so I think the replug solution is sort of more elegant for solving that problem because you actually do signal from the language model and if you just do reinforce it's very high variance, so it's going to be super finicky if you don't want to destroy your index, but people have tried it. So there's some really nice work from OpenAI where they basically show and again we're sort of like thinking more and more about agents here, where they show something very similar to the flare results from earlier with active retrieval that doesn't necessarily have to be some index that you only can read just some web search, and obviously in this case you don't really have access to the web search necessarily, so Bing or whatever they use here is not going to update these parameters, but I just wanted to kind of put this in your mind like this is another thing you can do, and if we take this really to the general form then you can think of language models as just tool users, so rather than just retrieval augmenting language models we can tool augment language models and retrieval is just one of the many tools that language models have access to, we can have re-rankers and things on top of the outputs of these tools, and so one of the big questions I think is how do you actually get the system to learn stuff, so we're going to need RL if we want the system to really learn how to take these actions properly, and so yeah this has been taken to the extreme in this sort of self-reg architecture where they have this sort of retrieval step and it's active and then you criticize it and then you basically do some natural language inference and all of that just with one language model to answer the questions. So the other missing piece, so I'm just kind of going through a bunch of open questions that people have looked at, but feel free to interrupt me if there's anything you want to know, but so instruction tuning we established at the beginning of the lecture that this is pretty important for getting things to work, so fixing the user interface, but the instruction tuning has almost always only happened on the language model and not on the entire system, so I think one of the interesting things that people are looking at now with things like RADT and Instruct Retro is how can we instruction fine-tune an entire retrieval augmented system, so all the way into the retrieval step can we generate data so that that also follows the instructions properly, which currently doesn't happen in any of these model architectures. And then finally, I think I would be remiss if I didn't really talk about what people call advanced RAG, so like the developer community has been really doing some awesome stuff, so like frameworks like Lamaindex and Langchain and there's all these open source vector databases like Chroma and Weaviate and they're all sort of about making RAG really easy, but this is all frozen RAG, right, but even with frozen RAG you've been really doing incredible things, so we mentioned some of these already, so child-parent recursive retrievers, so you find small parts and then you give the big parts around it to the language model, you can do hybrid search where we use reciprocal rank fusion, so we have like different search results that we didn't combine before we give the final thing to the language model. There's zero shot like a large language model remanker, so basically the score function is not, it doesn't come from your retrieval, it comes directly from the language model, and then hypothetical document embeddings which I think is a really cool idea, so you just, basically you fix hallucination through hallucination, so you get a question, then you let the language model hallucinate a bunch of possible answers, then you go and search for nearest neighbors to the possible answers and you give those as context and then it gives the right answer based on that, so it was really like hallucinating answers, I think it's a brilliant solution, so there's a lot of stuff happening in the kind of frozen rack community too that I think is very interesting to look at, so just to wrap up, kind of looking at the future of this stuff, there are still lots of very interesting open questions, so if you're a student thinking about how to solve any of these, I think you can have quite a lot of impact, so how exactly do we do the pretraining of this architecture and do we even need to pretrain, I think even retro kind of shows that you don't necessarily have to pretrain, so maybe there's something wrong with how we do that, what do skating laws look like, so I think there's a really interesting question here around if I have a huge index and a very rich encoder of all the information in that index, maybe I can move, so basically decouple all the memorization to this index, so I have a language model that doesn't know anything, it just speaks English, it just sort of reasons on top, but it has no knowledge because that always comes from this retriever, if you can do something like that then you get very interesting scaling trade-offs, so you can have a tiny language model and do your retrieval to do a lot of the heavy lifting with your retrieval, which is nice because that's a cached computation, so you already have the embeddings, you just need to do the dot product, so it's much more efficient than kind of self-attention in the language model. Can we move beyond by encoders, so vector databases, I like people who build vector databases, but I'm not sure how long we're going to keep vector databases, because I think re-rankers probably work just as well and the N25 is much more efficient than a vector database, so I don't really see why we need dedicated vector databases, so what we're seeing, but maybe this is a bit of a critique of Silicon Valley investment strategies and things like that, but a lot of these vector database companies are basically becoming database companies now, so they are adding all this sparse stuff because the dense thing is not enough, and as it turns out there are a lot of pretty good sparse databases out there already, like Postgres and things like that, and there are also all adding vectors to their databases, so I think that's all going to kind of coalesce into databases. So I think there are some interesting things to look at for kind of the data, so through this instruction problem, can we generate much better data for training RAG systems synthetically, and then I think there's this massive open question around how we actually measure whether the RAG system is any good, so right now we just look at downstream performance, which is sort of okay, but if you mess up the retrieval it's very hard to measure, but how to measure whether your retrieval is right is also very difficult, so there are some frameworks where they try to take like the harmonic mean of your retrieval accuracy and your language model accuracy, but I think those are also very shoddy because we don't really have very good data sets to measure that on, so I think that's a very cool problem to work on as well. So the other problem that I personally am always very excited about is multimodality, and so why would we stop with RAG systems with just text, so you can do the same with images, you can augment language models with vision, so we did this work on lens where we have a language model enhanced to see, where you can just give kind of a computer vision pipeline, just like a retrieval pipeline and give that to a frozen language model and pass it to the context and that system actually is an amazing visual question answering system. It's close to state-of-the-art sort of flamingo from DeepMind, which is also very hard to reproduce because there's no open source version of that, so we've done some early work on this in 2021 where we have this cross-modal retrieval and there's some more recent work out of FAIR where they also look at this, so I think that's really like if you look at the trend in the field like multimodality with GPD4 or V and things like that is really a hot topic, so everything is kind of going in that direction, so it's an interesting thing to think about. So overall I think it would be nice if everybody sort of moves away from RAG 1.0, the frozen Frankenstein RAG and moves towards this much more kind of optimized version RAG 2.0, so it's really about systems over models, right, it's not just your language model when you're retriever and they're kind of separate, it's about thinking from the from a system's perspective about the entire thing and the problem you're trying to solve and so I think that really is the way that in deep learning things have always progressed, where if you optimize the system end-to-end that's always going to win out, like back in the day in computer vision or NLP, we have like parsers and scene parsers and all this kind of stuff and all of that just doesn't exist anymore now because we optimize the system end-to-end and so that's what's going to happen here too. So if we take that to the extreme, like there's a chunker thing in your documents, right, like cutting it up into pieces, like you could backdrop into debt, like why not? Somebody should really do that and so yeah, I think like trading off cost and quality and zero-shot domain generalization, that's really like where this stuff is going to come in, right, so language models right now, they're amazing but very often they're way too expensive for being deployed somewhere where you can actually make money from them if you're in a company. So what you want to do is make it much more efficient and have the right cost quality trade-off and the easiest way I can think of is to do it through retrieval augmentation but obviously I'm very biased. So yeah, that was all I had actually. So if you're interested in this, I'm at Samford so I can work with you on research projects on these topics or if you want you can also join contextual because we work on this stuff every day. Thank you. Well, sorry, I had a question from earlier. Yeah, I think you said something really, really, I think really super helpful earlier about Miss Jill 7B. You talked about, you compared the sliding window attention to convolutional neural networks and I do see the parallel because with convolutional neural networks you have several layers of, several different layers of convolutional layers and the top convolutional layers are able to see a larger receptive field than the bottom convolutional layers and with convolutional layers you're able to tune the filter sizes and the strides so you're able to see a different receptive field and I was wondering if you could see that same innovation in Miss Jill 7B by tuning because you have different transformer layers and if each transformer layer will have a span over a different set of tokens and you can tune I guess the transformer architecture the way you tune those convolutional layers, the filter sizes, the receptive field, perhaps we can do some optimization in the transformer realm that we have already done in convolution layers. Yeah, I think that's a good idea. There's a great paper on light convolutions I think from Michael Auli and David Gange and a bunch of people where it's basically this came out at exactly the same time as the transformer and the transformer is slightly more optimized for GPU computation but the convolutional model was actually slightly better than the transformer so it's definitely worth exploring. Okay, cool, thanks. Yeah, so it depends on the problem I think what you probably want to do is is sort of cast a white net with VM25 and then just narrow it down with dense search so you often see that kind of as a two-stage process where the first one is kind of noisy you can add noise actually to your retrieval and then you use the dense one to filter it down. Yeah, everyone's trying to maybe adapt their plug-in model to almost the only specific area. I think there are many two ways of project one way is to use some instrument tuning in some kind of learning way or functionally like tuning that and another way is to the main project of this lecture is using the virtual augmented way. So I wonder if I know a message of virtual augmented way, do you think the capacity or the quality of virtual augmented way can be matched with those tuning methods I think or kind of learning? Yeah, so I think actually what's going to happen is that all of this will come together right so if you actually train things like end-to-end, rack 2.0 style then you can also fine-tune that system on some use case end-to-end. So why would you just take the retrieval augmented system if you can also fine-tune it on the thing you care about? So I think in the end everybody's going to do all of those things and then there's questions like how do you do that efficiently so that's why you would use adapters or things like that. I think there was another question. I'm curious about hardware, you say it's going to become database kind of thing, in fact it was part of the database but what about retrieval hardware and you know it's mine because we've got so much of the learning part but what about because it's huge, tree as I said so do you have any ideas it's just a database problem? So I don't know if I'm allowed to say this exactly actually but so one of the biggest chip manufacturers that recently their stock has done really well they have some dedicated retrieval hardware coming out I think sooner it might already be out. So yeah very efficient dense retrieval is a very big business. Other questions? Yes I think so if you take it to the extreme so one of the big problems right now is that if you contextualize an existing language model that already hallucinates then then it's going to be kind of hard to get rid of the hallucination right so if you do replug on GPT-4 GPT-4 might still hallucinate so you could basically just ignore all the stuff you retrieved and just do whatever it wants anyway so that's one of the reasons why you want to train the system end to end and if you take that to the extreme where like I said right if you can just have the language model only reason and speak so it knows English and reasoning but it has no knowledge which all comes from somewhere else then you can't hallucinate and so it's really all grounded in whatever is in your index but they're so they're about hallucination I'm sort of frustrated that a lot of people in the field misunderstand what hallucination even means like so a lot of people are conflating hallucination with a correctness or incorrectness so they're like oh the model made a mistake it hallucinated it's like no it made a mistake that's different from hallucination hallucination I think is very specific kind of I've retrieved something so I have some sort of counterfactual ground truth and what I'm saying does not correspond to that ground truth and so yeah I think there's a bunch of folks at Stanford also working on better measurements of hallucination and definitions and things like that yeah awesome ground truth right so so hallucination is really like there there is something that is true right so so if we're talking about like hallucination and yeah so if we're talking about just general parametric language models then sort of the ground truth is whatever we consider to be true right but we had to work for like language models making mistakes before it was called major mistakes yeah ground truth I guess this is solving that helps me to question that path are you working on ground truth per se that's around you know if I generate the building saying oh well I'm a president I mean everything all the time are you sharing work on that on this yeah so so I like the sort of silo uh mansion error as well so I think the whole point is that you can you can have different indices and different definitions of ground truth and so um I think you could say I only trust uh archive or I only trust like peer review papers and not just archive so you can make decisions in your architecture during test time about what you define as ground truth and I also think actually that and there's a bunch of work I think happening on this right now you can control for how how grounded you want it to be in your ground truth so that's another kind of misconception about hallucinations like sometimes hallucinations are actually good right if you have a creative writing assistant and you wanted to come up with some cool new ideas you want the language model to hallucinate uh so I think what you want to have is kind of a tunable knob where you say like oh now you can hallucinate and now maybe you should like really tell me the truth only anything else yeah so but the temperature that's just about how you sample right so how flat your your distribution is that you sample from yes but so even if you have a low temperature it can still come up with random stuff right so it just says that then you're very likely to do like really sampling um so so I think what you want to get at is is something more sophisticated than that yeah I like the question", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.84, "text": " Hey guys, welcome to our last lecture of this quarter.", "tokens": [50364, 1911, 1074, 11, 2928, 281, 527, 1036, 7991, 295, 341, 6555, 13, 50956], "temperature": 0.0, "avg_logprob": -0.3179072003031886, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.04930606484413147}, {"id": 1, "seek": 0, "start": 11.84, "end": 15.52, "text": " And we're very happy to have Dawa here.", "tokens": [50956, 400, 321, 434, 588, 2055, 281, 362, 413, 10449, 510, 13, 51140], "temperature": 0.0, "avg_logprob": -0.3179072003031886, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.04930606484413147}, {"id": 2, "seek": 0, "start": 15.52, "end": 22.12, "text": " He's the CEO of Contextual AI, the Enterprise LLM company, as well as an adjunct professor", "tokens": [51140, 634, 311, 264, 9282, 295, 4839, 3828, 901, 7318, 11, 264, 26696, 441, 43, 44, 2237, 11, 382, 731, 382, 364, 614, 10010, 349, 8304, 51470], "temperature": 0.0, "avg_logprob": -0.3179072003031886, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.04930606484413147}, {"id": 3, "seek": 0, "start": 22.12, "end": 25.12, "text": " in symbolic systems here at Stanford.", "tokens": [51470, 294, 25755, 3652, 510, 412, 20374, 13, 51620], "temperature": 0.0, "avg_logprob": -0.3179072003031886, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.04930606484413147}, {"id": 4, "seek": 0, "start": 25.12, "end": 29.16, "text": " And previously, he was the head of research at Clicking Base, and before that a research", "tokens": [51620, 400, 8046, 11, 415, 390, 264, 1378, 295, 2132, 412, 8230, 278, 21054, 11, 293, 949, 300, 257, 2132, 51822], "temperature": 0.0, "avg_logprob": -0.3179072003031886, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.04930606484413147}, {"id": 5, "seek": 2916, "start": 29.16, "end": 32.52, "text": " scientist at Facebook AI Research.", "tokens": [50364, 12662, 412, 4384, 7318, 10303, 13, 50532], "temperature": 0.0, "avg_logprob": -0.3060812901968908, "compression_ratio": 1.6245353159851301, "no_speech_prob": 0.006251942832022905}, {"id": 6, "seek": 2916, "start": 32.52, "end": 37.4, "text": " He received his PhD in masters from the University of Cambridge, as well as a master's in logic", "tokens": [50532, 634, 4613, 702, 14476, 294, 19294, 490, 264, 3535, 295, 24876, 11, 382, 731, 382, 257, 4505, 311, 294, 9952, 50776], "temperature": 0.0, "avg_logprob": -0.3060812901968908, "compression_ratio": 1.6245353159851301, "no_speech_prob": 0.006251942832022905}, {"id": 7, "seek": 2916, "start": 37.4, "end": 42.32, "text": " from the University of Amsterdam, and studied philosophy and incognitive AI in undergrad.", "tokens": [50776, 490, 264, 3535, 295, 28291, 11, 293, 9454, 10675, 293, 834, 2912, 2187, 7318, 294, 14295, 13, 51022], "temperature": 0.0, "avg_logprob": -0.3060812901968908, "compression_ratio": 1.6245353159851301, "no_speech_prob": 0.006251942832022905}, {"id": 8, "seek": 2916, "start": 42.32, "end": 48.04, "text": " And his work focuses on machine learning as well as NLP, specifically on developing better", "tokens": [51022, 400, 702, 589, 16109, 322, 3479, 2539, 382, 731, 382, 426, 45196, 11, 4682, 322, 6416, 1101, 51308], "temperature": 0.0, "avg_logprob": -0.3060812901968908, "compression_ratio": 1.6245353159851301, "no_speech_prob": 0.006251942832022905}, {"id": 9, "seek": 2916, "start": 48.04, "end": 54.44, "text": " models for language understanding and generation, and better tools for evaluation and many times.", "tokens": [51308, 5245, 337, 2856, 3701, 293, 5125, 11, 293, 1101, 3873, 337, 13344, 293, 867, 1413, 13, 51628], "temperature": 0.0, "avg_logprob": -0.3060812901968908, "compression_ratio": 1.6245353159851301, "no_speech_prob": 0.006251942832022905}, {"id": 10, "seek": 2916, "start": 54.44, "end": 57.480000000000004, "text": " Yeah, give it up for Adela.", "tokens": [51628, 865, 11, 976, 309, 493, 337, 1999, 4053, 13, 51780], "temperature": 0.0, "avg_logprob": -0.3060812901968908, "compression_ratio": 1.6245353159851301, "no_speech_prob": 0.006251942832022905}, {"id": 11, "seek": 5748, "start": 58.48, "end": 60.199999999999996, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50500], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 12, "seek": 5748, "start": 60.199999999999996, "end": 64.12, "text": " So I guess I have to sort of stand here in the corner, so people can see me on this", "tokens": [50500, 407, 286, 2041, 286, 362, 281, 1333, 295, 1463, 510, 294, 264, 4538, 11, 370, 561, 393, 536, 385, 322, 341, 50696], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 13, "seek": 5748, "start": 64.12, "end": 65.12, "text": " move as well.", "tokens": [50696, 1286, 382, 731, 13, 50746], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 14, "seek": 5748, "start": 65.12, "end": 70.6, "text": " Yeah, thanks so much for having me here.", "tokens": [50746, 865, 11, 3231, 370, 709, 337, 1419, 385, 510, 13, 51020], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 15, "seek": 5748, "start": 70.6, "end": 72.28, "text": " So I asked Steven what I should talk about.", "tokens": [51020, 407, 286, 2351, 12754, 437, 286, 820, 751, 466, 13, 51104], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 16, "seek": 5748, "start": 72.28, "end": 77.0, "text": " There were a couple of things I could talk about, multi-modality or evaluation.", "tokens": [51104, 821, 645, 257, 1916, 295, 721, 286, 727, 751, 466, 11, 4825, 12, 8014, 1860, 420, 13344, 13, 51340], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 17, "seek": 5748, "start": 77.0, "end": 83.52, "text": " And this was the preferred topic, I guess, because the others were already covered.", "tokens": [51340, 400, 341, 390, 264, 16494, 4829, 11, 286, 2041, 11, 570, 264, 2357, 645, 1217, 5343, 13, 51666], "temperature": 0.0, "avg_logprob": -0.25221109871912484, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.026257630437612534}, {"id": 18, "seek": 8352, "start": 83.52, "end": 88.6, "text": " So yeah, I'm very happy to talk to you about everything retrieval augmentation.", "tokens": [50364, 407, 1338, 11, 286, 478, 588, 2055, 281, 751, 281, 291, 466, 1203, 19817, 3337, 14501, 19631, 13, 50618], "temperature": 0.0, "avg_logprob": -0.08972041374814194, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.014690708369016647}, {"id": 19, "seek": 8352, "start": 88.6, "end": 93.08, "text": " I think this is really one of the coolest topics right now in our field.", "tokens": [50618, 286, 519, 341, 307, 534, 472, 295, 264, 22013, 8378, 558, 586, 294, 527, 2519, 13, 50842], "temperature": 0.0, "avg_logprob": -0.08972041374814194, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.014690708369016647}, {"id": 20, "seek": 8352, "start": 93.08, "end": 97.8, "text": " So I'll just give you an overview of what's been happening and what I think are the interesting", "tokens": [50842, 407, 286, 603, 445, 976, 291, 364, 12492, 295, 437, 311, 668, 2737, 293, 437, 286, 519, 366, 264, 1880, 51078], "temperature": 0.0, "avg_logprob": -0.08972041374814194, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.014690708369016647}, {"id": 21, "seek": 8352, "start": 97.8, "end": 100.92, "text": " questions to think about.", "tokens": [51078, 1651, 281, 519, 466, 13, 51234], "temperature": 0.0, "avg_logprob": -0.08972041374814194, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.014690708369016647}, {"id": 22, "seek": 8352, "start": 100.92, "end": 106.52, "text": " So first of all, obviously, in case you've missed it, we are in the age of language models.", "tokens": [51234, 407, 700, 295, 439, 11, 2745, 11, 294, 1389, 291, 600, 6721, 309, 11, 321, 366, 294, 264, 3205, 295, 2856, 5245, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08972041374814194, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.014690708369016647}, {"id": 23, "seek": 8352, "start": 106.52, "end": 111.19999999999999, "text": " And I just wanted to do a quick poll here in this not super big audience.", "tokens": [51514, 400, 286, 445, 1415, 281, 360, 257, 1702, 6418, 510, 294, 341, 406, 1687, 955, 4034, 13, 51748], "temperature": 0.0, "avg_logprob": -0.08972041374814194, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.014690708369016647}, {"id": 24, "seek": 11120, "start": 111.2, "end": 117.12, "text": " I guess there's more people on the Zoom, but who invented language models?", "tokens": [50364, 286, 2041, 456, 311, 544, 561, 322, 264, 13453, 11, 457, 567, 14479, 2856, 5245, 30, 50660], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 25, "seek": 11120, "start": 117.12, "end": 120.92, "text": " If you thought OpenAI, then I'm angry with you.", "tokens": [50660, 759, 291, 1194, 7238, 48698, 11, 550, 286, 478, 6884, 365, 291, 13, 50850], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 26, "seek": 11120, "start": 120.92, "end": 124.28, "text": " So actually, this is a very, very old idea.", "tokens": [50850, 407, 767, 11, 341, 307, 257, 588, 11, 588, 1331, 1558, 13, 51018], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 27, "seek": 11120, "start": 124.28, "end": 130.0, "text": " So the idea is just you take a sequence and you factorize out the token probabilities.", "tokens": [51018, 407, 264, 1558, 307, 445, 291, 747, 257, 8310, 293, 291, 5952, 1125, 484, 264, 14862, 33783, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 28, "seek": 11120, "start": 130.0, "end": 133.04, "text": " And so it wasn't invented by OpenAI.", "tokens": [51304, 400, 370, 309, 2067, 380, 14479, 538, 7238, 48698, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 29, "seek": 11120, "start": 133.04, "end": 134.84, "text": " It's not like a few years old.", "tokens": [51456, 467, 311, 406, 411, 257, 1326, 924, 1331, 13, 51546], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 30, "seek": 11120, "start": 134.84, "end": 138.0, "text": " It's actually several decades old.", "tokens": [51546, 467, 311, 767, 2940, 7878, 1331, 13, 51704], "temperature": 0.0, "avg_logprob": -0.1813359213347482, "compression_ratio": 1.5411255411255411, "no_speech_prob": 0.0015477800043299794}, {"id": 31, "seek": 13800, "start": 138.0, "end": 141.52, "text": " So I'm bringing this up because I was talking to someone and they were like, OpenAI invented", "tokens": [50364, 407, 286, 478, 5062, 341, 493, 570, 286, 390, 1417, 281, 1580, 293, 436, 645, 411, 11, 7238, 48698, 14479, 50540], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 32, "seek": 13800, "start": 141.52, "end": 142.52, "text": " language models.", "tokens": [50540, 2856, 5245, 13, 50590], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 33, "seek": 13800, "start": 142.52, "end": 145.64, "text": " And I was like, they're kidding me, right?", "tokens": [50590, 400, 286, 390, 411, 11, 436, 434, 9287, 385, 11, 558, 30, 50746], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 34, "seek": 13800, "start": 145.64, "end": 151.48, "text": " So I went back to the literature and this is the oldest one I could find actually, 1991", "tokens": [50746, 407, 286, 1437, 646, 281, 264, 10394, 293, 341, 307, 264, 14026, 472, 286, 727, 915, 767, 11, 24097, 51038], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 35, "seek": 13800, "start": 151.48, "end": 153.72, "text": " first neural language model.", "tokens": [51038, 700, 18161, 2856, 2316, 13, 51150], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 36, "seek": 13800, "start": 153.72, "end": 159.76, "text": " There's a very nice paper from 2003 from Bengio where they actually have like word embeddings", "tokens": [51150, 821, 311, 257, 588, 1481, 3035, 490, 16416, 490, 3964, 17862, 689, 436, 767, 362, 411, 1349, 12240, 29432, 51452], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 37, "seek": 13800, "start": 159.76, "end": 162.12, "text": " and everything already in there.", "tokens": [51452, 293, 1203, 1217, 294, 456, 13, 51570], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 38, "seek": 13800, "start": 162.12, "end": 166.04, "text": " So obviously, these are LLMs, not LLMs.", "tokens": [51570, 407, 2745, 11, 613, 366, 441, 43, 26386, 11, 406, 441, 43, 26386, 13, 51766], "temperature": 0.0, "avg_logprob": -0.20415182771353885, "compression_ratio": 1.5740072202166064, "no_speech_prob": 0.002432603156194091}, {"id": 39, "seek": 16604, "start": 166.04, "end": 171.0, "text": " And as it turns out, if you make them really big and you parameterize them with these massive", "tokens": [50364, 400, 382, 309, 4523, 484, 11, 498, 291, 652, 552, 534, 955, 293, 291, 13075, 1125, 552, 365, 613, 5994, 50612], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 40, "seek": 16604, "start": 171.0, "end": 175.79999999999998, "text": " neural nets, then you get something really powerful that really shows emergent properties.", "tokens": [50612, 18161, 36170, 11, 550, 291, 483, 746, 534, 4005, 300, 534, 3110, 4345, 6930, 7221, 13, 50852], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 41, "seek": 16604, "start": 175.79999999999998, "end": 179.72, "text": " And that's why we're also excited in this stuff.", "tokens": [50852, 400, 300, 311, 983, 321, 434, 611, 2919, 294, 341, 1507, 13, 51048], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 42, "seek": 16604, "start": 179.72, "end": 184.39999999999998, "text": " So if we think about this from like a classic CS perspective, there's input output, right?", "tokens": [51048, 407, 498, 321, 519, 466, 341, 490, 411, 257, 7230, 9460, 4585, 11, 456, 311, 4846, 5598, 11, 558, 30, 51282], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 43, "seek": 16604, "start": 184.39999999999998, "end": 187.35999999999999, "text": " There's this kind of thing in the middle, it's the generator.", "tokens": [51282, 821, 311, 341, 733, 295, 551, 294, 264, 2808, 11, 309, 311, 264, 19265, 13, 51430], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 44, "seek": 16604, "start": 187.35999999999999, "end": 192.92, "text": " So we take a sequence, the input sequence, and then the task of the model is to predict", "tokens": [51430, 407, 321, 747, 257, 8310, 11, 264, 4846, 8310, 11, 293, 550, 264, 5633, 295, 264, 2316, 307, 281, 6069, 51708], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 45, "seek": 16604, "start": 192.92, "end": 194.6, "text": " the next token.", "tokens": [51708, 264, 958, 14862, 13, 51792], "temperature": 0.0, "avg_logprob": -0.14970543714073614, "compression_ratio": 1.7132867132867133, "no_speech_prob": 0.0006069000228308141}, {"id": 46, "seek": 19460, "start": 194.6, "end": 197.35999999999999, "text": " Very, very simple model.", "tokens": [50364, 4372, 11, 588, 2199, 2316, 13, 50502], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 47, "seek": 19460, "start": 197.35999999999999, "end": 201.48, "text": " And so, you know, that's why it was so easy to come up with this in 1991 already because", "tokens": [50502, 400, 370, 11, 291, 458, 11, 300, 311, 983, 309, 390, 370, 1858, 281, 808, 493, 365, 341, 294, 24097, 1217, 570, 50708], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 48, "seek": 19460, "start": 201.48, "end": 204.07999999999998, "text": " it's like the idea is very intuitive.", "tokens": [50708, 309, 311, 411, 264, 1558, 307, 588, 21769, 13, 50838], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 49, "seek": 19460, "start": 204.07999999999998, "end": 209.92, "text": " But for a long time, what was really broken with this was the user interface.", "tokens": [50838, 583, 337, 257, 938, 565, 11, 437, 390, 534, 5463, 365, 341, 390, 264, 4195, 9226, 13, 51130], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 50, "seek": 19460, "start": 209.92, "end": 215.16, "text": " And this, I think a lot of people kind of misunderstand what chat GPT was about.", "tokens": [51130, 400, 341, 11, 286, 519, 257, 688, 295, 561, 733, 295, 35736, 437, 5081, 26039, 51, 390, 466, 13, 51392], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 51, "seek": 19460, "start": 215.16, "end": 217.2, "text": " That's really what chat GPT fixed.", "tokens": [51392, 663, 311, 534, 437, 5081, 26039, 51, 6806, 13, 51494], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 52, "seek": 19460, "start": 217.2, "end": 221.84, "text": " So that initially you had to come up with these very weird prompts in order to get your", "tokens": [51494, 407, 300, 9105, 291, 632, 281, 808, 493, 365, 613, 588, 3657, 41095, 294, 1668, 281, 483, 428, 51726], "temperature": 0.0, "avg_logprob": -0.17542765058320145, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0118555948138237}, {"id": 53, "seek": 22184, "start": 221.84, "end": 224.88, "text": " language model to do what you wanted it to do.", "tokens": [50364, 2856, 2316, 281, 360, 437, 291, 1415, 309, 281, 360, 13, 50516], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 54, "seek": 22184, "start": 224.88, "end": 226.84, "text": " And humans are terrible at this, right?", "tokens": [50516, 400, 6255, 366, 6237, 412, 341, 11, 558, 30, 50614], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 55, "seek": 22184, "start": 226.84, "end": 231.2, "text": " So we're much better at sort of telling people or things around us what we want, right?", "tokens": [50614, 407, 321, 434, 709, 1101, 412, 1333, 295, 3585, 561, 420, 721, 926, 505, 437, 321, 528, 11, 558, 30, 50832], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 56, "seek": 22184, "start": 231.2, "end": 236.2, "text": " So if we have a dog, we say sit, we don't prompt it in a very weird way so that it sits,", "tokens": [50832, 407, 498, 321, 362, 257, 3000, 11, 321, 584, 1394, 11, 321, 500, 380, 12391, 309, 294, 257, 588, 3657, 636, 370, 300, 309, 12696, 11, 51082], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 57, "seek": 22184, "start": 236.2, "end": 237.2, "text": " right?", "tokens": [51082, 558, 30, 51132], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 58, "seek": 22184, "start": 237.2, "end": 238.48000000000002, "text": " And it's the same with the language model.", "tokens": [51132, 400, 309, 311, 264, 912, 365, 264, 2856, 2316, 13, 51196], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 59, "seek": 22184, "start": 238.48000000000002, "end": 243.72, "text": " If you wanted to generate some red lyrics in the style of a pirate or Shakespeare or", "tokens": [51196, 759, 291, 1415, 281, 8460, 512, 2182, 12189, 294, 264, 3758, 295, 257, 27424, 420, 22825, 420, 51458], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 60, "seek": 22184, "start": 243.72, "end": 248.08, "text": " something, then you tell it generate some red lyrics in the style of a pirate, right?", "tokens": [51458, 746, 11, 550, 291, 980, 309, 8460, 512, 2182, 12189, 294, 264, 3758, 295, 257, 27424, 11, 558, 30, 51676], "temperature": 0.0, "avg_logprob": -0.16862324512366092, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.017968228086829185}, {"id": 61, "seek": 24808, "start": 248.08, "end": 253.68, "text": " So that kind of instruction data actually turns out to be super, super rare in just web", "tokens": [50364, 407, 300, 733, 295, 10951, 1412, 767, 4523, 484, 281, 312, 1687, 11, 1687, 5892, 294, 445, 3670, 50644], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 62, "seek": 24808, "start": 253.68, "end": 254.68, "text": " data.", "tokens": [50644, 1412, 13, 50694], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 63, "seek": 24808, "start": 254.68, "end": 258.28000000000003, "text": " So what you need to do is you need to fix the user interface to the language model.", "tokens": [50694, 407, 437, 291, 643, 281, 360, 307, 291, 643, 281, 3191, 264, 4195, 9226, 281, 264, 2856, 2316, 13, 50874], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 64, "seek": 24808, "start": 258.28000000000003, "end": 263.56, "text": " And the classic recipe for doing that is the sequence basically that chat GPT used.", "tokens": [50874, 400, 264, 7230, 6782, 337, 884, 300, 307, 264, 8310, 1936, 300, 5081, 26039, 51, 1143, 13, 51138], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 65, "seek": 24808, "start": 263.56, "end": 265.16, "text": " So you prompt the model in a specific way.", "tokens": [51138, 407, 291, 12391, 264, 2316, 294, 257, 2685, 636, 13, 51218], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 66, "seek": 24808, "start": 265.16, "end": 269.84000000000003, "text": " You will instruction fine tune the model and you can do some alignment or LHF, whatever", "tokens": [51218, 509, 486, 10951, 2489, 10864, 264, 2316, 293, 291, 393, 360, 512, 18515, 420, 441, 39, 37, 11, 2035, 51452], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 67, "seek": 24808, "start": 269.84000000000003, "end": 271.96000000000004, "text": " you do on top of that.", "tokens": [51452, 291, 360, 322, 1192, 295, 300, 13, 51558], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 68, "seek": 24808, "start": 271.96000000000004, "end": 272.96000000000004, "text": " So that's the first thing.", "tokens": [51558, 407, 300, 311, 264, 700, 551, 13, 51608], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 69, "seek": 24808, "start": 272.96000000000004, "end": 277.92, "text": " So now you have a working language model with a working user interface.", "tokens": [51608, 407, 586, 291, 362, 257, 1364, 2856, 2316, 365, 257, 1364, 4195, 9226, 13, 51856], "temperature": 0.0, "avg_logprob": -0.16513550550417794, "compression_ratio": 1.7663230240549828, "no_speech_prob": 0.010162213817238808}, {"id": 70, "seek": 27792, "start": 277.92, "end": 280.56, "text": " So are we done then?", "tokens": [50364, 407, 366, 321, 1096, 550, 30, 50496], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 71, "seek": 27792, "start": 280.56, "end": 281.56, "text": " Obviously we're not.", "tokens": [50496, 7580, 321, 434, 406, 13, 50546], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 72, "seek": 27792, "start": 281.56, "end": 285.16, "text": " So right now language models are kind of taking the world by storm.", "tokens": [50546, 407, 558, 586, 2856, 5245, 366, 733, 295, 1940, 264, 1002, 538, 7679, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 73, "seek": 27792, "start": 285.16, "end": 289.04, "text": " But if you talk to anyone, especially in an enterprise, for example, where they have very", "tokens": [50726, 583, 498, 291, 751, 281, 2878, 11, 2318, 294, 364, 14132, 11, 337, 1365, 11, 689, 436, 362, 588, 50920], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 74, "seek": 27792, "start": 289.04, "end": 294.48, "text": " strict accuracy requirements, they will tell you that they can't really productionize this", "tokens": [50920, 10910, 14170, 7728, 11, 436, 486, 980, 291, 300, 436, 393, 380, 534, 4265, 1125, 341, 51192], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 75, "seek": 27792, "start": 294.48, "end": 295.72, "text": " yet.", "tokens": [51192, 1939, 13, 51254], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 76, "seek": 27792, "start": 295.72, "end": 298.68, "text": " And the reason is because there are all these familiar problems, probably a bunch of you", "tokens": [51254, 400, 264, 1778, 307, 570, 456, 366, 439, 613, 4963, 2740, 11, 1391, 257, 3840, 295, 291, 51402], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 77, "seek": 27792, "start": 298.68, "end": 303.48, "text": " are working on these problems right now around hallucination.", "tokens": [51402, 366, 1364, 322, 613, 2740, 558, 586, 926, 35212, 2486, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1602354226288972, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.004462737590074539}, {"id": 78, "seek": 30348, "start": 303.48, "end": 307.12, "text": " So these models, they kind of make up stuff very often with very high confidence, which", "tokens": [50364, 407, 613, 5245, 11, 436, 733, 295, 652, 493, 1507, 588, 2049, 365, 588, 1090, 6687, 11, 597, 50546], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 79, "seek": 30348, "start": 307.12, "end": 311.44, "text": " is even more scary in a way attribution.", "tokens": [50546, 307, 754, 544, 6958, 294, 257, 636, 9080, 1448, 13, 50762], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 80, "seek": 30348, "start": 311.44, "end": 315.52000000000004, "text": " So we don't really know why these models are saying what they're saying stillness.", "tokens": [50762, 407, 321, 500, 380, 534, 458, 983, 613, 5245, 366, 1566, 437, 436, 434, 1566, 920, 1287, 13, 50966], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 81, "seek": 30348, "start": 315.52000000000004, "end": 316.52000000000004, "text": " They go out of date.", "tokens": [50966, 814, 352, 484, 295, 4002, 13, 51016], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 82, "seek": 30348, "start": 316.52000000000004, "end": 320.32, "text": " And so this was a big problem with sort of chat GPT, not knowing anything that happened", "tokens": [51016, 400, 370, 341, 390, 257, 955, 1154, 365, 1333, 295, 5081, 26039, 51, 11, 406, 5276, 1340, 300, 2011, 51206], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 83, "seek": 30348, "start": 320.32, "end": 322.16, "text": " after a certain cutoff date.", "tokens": [51206, 934, 257, 1629, 1723, 4506, 4002, 13, 51298], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 84, "seek": 30348, "start": 322.16, "end": 324.08000000000004, "text": " And they keep updating it every once in a while.", "tokens": [51298, 400, 436, 1066, 25113, 309, 633, 1564, 294, 257, 1339, 13, 51394], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 85, "seek": 30348, "start": 324.08000000000004, "end": 329.04, "text": " But you want to have a system that's always completely up to date that never goes still.", "tokens": [51394, 583, 291, 528, 281, 362, 257, 1185, 300, 311, 1009, 2584, 493, 281, 4002, 300, 1128, 1709, 920, 13, 51642], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 86, "seek": 30348, "start": 329.04, "end": 331.6, "text": " You want to be able to revise the information in the system.", "tokens": [51642, 509, 528, 281, 312, 1075, 281, 44252, 264, 1589, 294, 264, 1185, 13, 51770], "temperature": 0.0, "avg_logprob": -0.17467167360562805, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.155878946185112}, {"id": 87, "seek": 33160, "start": 331.6, "end": 337.32000000000005, "text": " So if you're a European organization, you have to worry about GDPR, which means that", "tokens": [50364, 407, 498, 291, 434, 257, 6473, 4475, 11, 291, 362, 281, 3292, 466, 19599, 49, 11, 597, 1355, 300, 50650], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 88, "seek": 33160, "start": 337.32000000000005, "end": 342.24, "text": " you need to be able to remove information from the language model or maybe revise facts,", "tokens": [50650, 291, 643, 281, 312, 1075, 281, 4159, 1589, 490, 264, 2856, 2316, 420, 1310, 44252, 9130, 11, 50896], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 89, "seek": 33160, "start": 342.24, "end": 343.96000000000004, "text": " which we don't really know how to do.", "tokens": [50896, 597, 321, 500, 380, 534, 458, 577, 281, 360, 13, 50982], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 90, "seek": 33160, "start": 343.96000000000004, "end": 350.04, "text": " So again, this is a very interesting area of study for a lot of folks, model editing.", "tokens": [50982, 407, 797, 11, 341, 307, 257, 588, 1880, 1859, 295, 2979, 337, 257, 688, 295, 4024, 11, 2316, 10000, 13, 51286], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 91, "seek": 33160, "start": 350.04, "end": 353.52000000000004, "text": " But so this is something that we really want to be able to fix.", "tokens": [51286, 583, 370, 341, 307, 746, 300, 321, 534, 528, 281, 312, 1075, 281, 3191, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 92, "seek": 33160, "start": 353.52000000000004, "end": 357.88, "text": " And then there's this big question of how do you customize these models?", "tokens": [51460, 400, 550, 456, 311, 341, 955, 1168, 295, 577, 360, 291, 19734, 613, 5245, 30, 51678], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 93, "seek": 33160, "start": 357.88, "end": 359.88, "text": " So different people have different use cases.", "tokens": [51678, 407, 819, 561, 362, 819, 764, 3331, 13, 51778], "temperature": 0.0, "avg_logprob": -0.1229678233464559, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0007550009177066386}, {"id": 94, "seek": 35988, "start": 359.88, "end": 360.88, "text": " You have different data.", "tokens": [50364, 509, 362, 819, 1412, 13, 50414], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 95, "seek": 35988, "start": 360.88, "end": 364.56, "text": " If you're a company or if you want to have a language model on your own data, how do", "tokens": [50414, 759, 291, 434, 257, 2237, 420, 498, 291, 528, 281, 362, 257, 2856, 2316, 322, 428, 1065, 1412, 11, 577, 360, 50598], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 96, "seek": 35988, "start": 364.56, "end": 366.84, "text": " you make it work on your own data?", "tokens": [50598, 291, 652, 309, 589, 322, 428, 1065, 1412, 30, 50712], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 97, "seek": 35988, "start": 366.84, "end": 372.15999999999997, "text": " So one of the solutions that everybody has started using right now is to couple it to", "tokens": [50712, 407, 472, 295, 264, 6547, 300, 2201, 575, 1409, 1228, 558, 586, 307, 281, 1916, 309, 281, 50978], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 98, "seek": 35988, "start": 372.15999999999997, "end": 373.36, "text": " an external memory.", "tokens": [50978, 364, 8320, 4675, 13, 51038], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 99, "seek": 35988, "start": 373.36, "end": 377.2, "text": " So that's really just rag, right?", "tokens": [51038, 407, 300, 311, 534, 445, 17539, 11, 558, 30, 51230], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 100, "seek": 35988, "start": 377.2, "end": 379.84, "text": " This whole lecture is basically about rag.", "tokens": [51230, 639, 1379, 7991, 307, 1936, 466, 17539, 13, 51362], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 101, "seek": 35988, "start": 379.84, "end": 385.88, "text": " But the way to understand what is going on here is we have this generator just like before.", "tokens": [51362, 583, 264, 636, 281, 1223, 437, 307, 516, 322, 510, 307, 321, 362, 341, 19265, 445, 411, 949, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 102, "seek": 35988, "start": 385.88, "end": 387.8, "text": " We have the input and the prompt just like before.", "tokens": [51664, 492, 362, 264, 4846, 293, 264, 12391, 445, 411, 949, 13, 51760], "temperature": 0.0, "avg_logprob": -0.16039411033072123, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.017423268407583237}, {"id": 103, "seek": 38780, "start": 387.8, "end": 392.72, "text": " But now instead of just giving those two things, we give this additional context.", "tokens": [50364, 583, 586, 2602, 295, 445, 2902, 729, 732, 721, 11, 321, 976, 341, 4497, 4319, 13, 50610], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 104, "seek": 38780, "start": 392.72, "end": 397.6, "text": " So we contextualize the language model using things we've retrieved.", "tokens": [50610, 407, 321, 35526, 1125, 264, 2856, 2316, 1228, 721, 321, 600, 19817, 937, 13, 50854], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 105, "seek": 38780, "start": 397.6, "end": 400.68, "text": " And the retriever is very often pretty simple.", "tokens": [50854, 400, 264, 19817, 331, 307, 588, 2049, 1238, 2199, 13, 51008], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 106, "seek": 38780, "start": 400.68, "end": 403.6, "text": " It's just a query in the document encoder.", "tokens": [51008, 467, 311, 445, 257, 14581, 294, 264, 4166, 2058, 19866, 13, 51154], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 107, "seek": 38780, "start": 403.6, "end": 408.04, "text": " And then you get a bunch of documents, you give them as context to the model.", "tokens": [51154, 400, 550, 291, 483, 257, 3840, 295, 8512, 11, 291, 976, 552, 382, 4319, 281, 264, 2316, 13, 51376], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 108, "seek": 38780, "start": 408.04, "end": 411.2, "text": " So super simple architecture.", "tokens": [51376, 407, 1687, 2199, 9482, 13, 51534], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 109, "seek": 38780, "start": 411.2, "end": 416.04, "text": " And I think it's useful to think about it from the perspective of these two separate", "tokens": [51534, 400, 286, 519, 309, 311, 4420, 281, 519, 466, 309, 490, 264, 4585, 295, 613, 732, 4994, 51776], "temperature": 0.0, "avg_logprob": -0.15066186067099882, "compression_ratio": 1.7389558232931728, "no_speech_prob": 0.001323921955190599}, {"id": 110, "seek": 41604, "start": 416.08000000000004, "end": 417.84000000000003, "text": " paradigms.", "tokens": [50366, 13480, 328, 2592, 13, 50454], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 111, "seek": 41604, "start": 417.84000000000003, "end": 421.28000000000003, "text": " So if you've ever taken an exam, I'm sure you have, right?", "tokens": [50454, 407, 498, 291, 600, 1562, 2726, 364, 1139, 11, 286, 478, 988, 291, 362, 11, 558, 30, 50626], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 112, "seek": 41604, "start": 421.28000000000003, "end": 424.04, "text": " You can have a closed book exam where you have to memorize all of this, so you have", "tokens": [50626, 509, 393, 362, 257, 5395, 1446, 1139, 689, 291, 362, 281, 27478, 439, 295, 341, 11, 370, 291, 362, 50764], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 113, "seek": 41604, "start": 424.04, "end": 428.6, "text": " to cram all the knowledge into your parameters, your neurons.", "tokens": [50764, 281, 941, 335, 439, 264, 3601, 666, 428, 9834, 11, 428, 22027, 13, 50992], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 114, "seek": 41604, "start": 428.6, "end": 432.16, "text": " Or you have an open book exam where you have all of this information in the book that you", "tokens": [50992, 1610, 291, 362, 364, 1269, 1446, 1139, 689, 291, 362, 439, 295, 341, 1589, 294, 264, 1446, 300, 291, 51170], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 115, "seek": 41604, "start": 432.16, "end": 434.96000000000004, "text": " can access when you do the exam.", "tokens": [51170, 393, 2105, 562, 291, 360, 264, 1139, 13, 51310], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 116, "seek": 41604, "start": 434.96000000000004, "end": 436.84000000000003, "text": " So it's a very similar thing with rag, right?", "tokens": [51310, 407, 309, 311, 257, 588, 2531, 551, 365, 17539, 11, 558, 30, 51404], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 117, "seek": 41604, "start": 436.84000000000003, "end": 440.20000000000005, "text": " You can just make it an open book setting where you can give it access to this external", "tokens": [51404, 509, 393, 445, 652, 309, 364, 1269, 1446, 3287, 689, 291, 393, 976, 309, 2105, 281, 341, 8320, 51572], "temperature": 0.0, "avg_logprob": -0.14710459113121033, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.02227327786386013}, {"id": 118, "seek": 44020, "start": 440.28, "end": 445.92, "text": " information, Wikipedia or something else, or basically the entire internet, and then", "tokens": [50368, 1589, 11, 28999, 420, 746, 1646, 11, 420, 1936, 264, 2302, 4705, 11, 293, 550, 50650], "temperature": 0.0, "avg_logprob": -0.19267743772214596, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.007228471338748932}, {"id": 119, "seek": 44020, "start": 445.92, "end": 451.68, "text": " have the language model do its job without having to memorize all of it in its parameters.", "tokens": [50650, 362, 264, 2856, 2316, 360, 1080, 1691, 1553, 1419, 281, 27478, 439, 295, 309, 294, 1080, 9834, 13, 50938], "temperature": 0.0, "avg_logprob": -0.19267743772214596, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.007228471338748932}, {"id": 120, "seek": 44020, "start": 451.68, "end": 456.8, "text": " So Dr. I think useful distinction here is that cramming everything into your parameters,", "tokens": [50938, 407, 2491, 13, 286, 519, 4420, 16844, 510, 307, 300, 941, 335, 2810, 1203, 666, 428, 9834, 11, 51194], "temperature": 0.0, "avg_logprob": -0.19267743772214596, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.007228471338748932}, {"id": 121, "seek": 44020, "start": 456.8, "end": 458.64, "text": " that's the parametric approach.", "tokens": [51194, 300, 311, 264, 6220, 17475, 3109, 13, 51286], "temperature": 0.0, "avg_logprob": -0.19267743772214596, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.007228471338748932}, {"id": 122, "seek": 44020, "start": 458.64, "end": 464.12, "text": " So what we're doing with rag is we're adding this non-parametric retrieval component.", "tokens": [51286, 407, 437, 321, 434, 884, 365, 17539, 307, 321, 434, 5127, 341, 2107, 12, 2181, 335, 17475, 19817, 3337, 6542, 13, 51560], "temperature": 0.0, "avg_logprob": -0.19267743772214596, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.007228471338748932}, {"id": 123, "seek": 44020, "start": 464.12, "end": 469.08, "text": " So you might call this semi-parametric if you want to give this a name.", "tokens": [51560, 407, 291, 1062, 818, 341, 12909, 12, 2181, 335, 17475, 498, 291, 528, 281, 976, 341, 257, 1315, 13, 51808], "temperature": 0.0, "avg_logprob": -0.19267743772214596, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.007228471338748932}, {"id": 124, "seek": 47020, "start": 470.59999999999997, "end": 474.92, "text": " All right, so why does that actually solve these issues?", "tokens": [50384, 1057, 558, 11, 370, 983, 775, 300, 767, 5039, 613, 2663, 30, 50600], "temperature": 0.0, "avg_logprob": -0.17394132871885556, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.003425631206482649}, {"id": 125, "seek": 47020, "start": 474.92, "end": 479.71999999999997, "text": " And so the answer is basically that if you have this separate index, right, this separate", "tokens": [50600, 400, 370, 264, 1867, 307, 1936, 300, 498, 291, 362, 341, 4994, 8186, 11, 558, 11, 341, 4994, 50840], "temperature": 0.0, "avg_logprob": -0.17394132871885556, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.003425631206482649}, {"id": 126, "seek": 47020, "start": 479.71999999999997, "end": 484.48, "text": " retriever, you can swap it in, you can swap it out, you can replace it with a new index", "tokens": [50840, 19817, 331, 11, 291, 393, 18135, 309, 294, 11, 291, 393, 18135, 309, 484, 11, 291, 393, 7406, 309, 365, 257, 777, 8186, 51078], "temperature": 0.0, "avg_logprob": -0.17394132871885556, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.003425631206482649}, {"id": 127, "seek": 47020, "start": 484.48, "end": 486.48, "text": " so you can really customize it.", "tokens": [51078, 370, 291, 393, 534, 19734, 309, 13, 51178], "temperature": 0.0, "avg_logprob": -0.17394132871885556, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.003425631206482649}, {"id": 128, "seek": 47020, "start": 486.48, "end": 492.8, "text": " And so you can customize your language model system for what the user really wants to see.", "tokens": [51178, 400, 370, 291, 393, 19734, 428, 2856, 2316, 1185, 337, 437, 264, 4195, 534, 2738, 281, 536, 13, 51494], "temperature": 0.0, "avg_logprob": -0.17394132871885556, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.003425631206482649}, {"id": 129, "seek": 47020, "start": 492.8, "end": 497.03999999999996, "text": " And then obviously you can update this index, so it doesn't really go still and you can", "tokens": [51494, 400, 550, 2745, 291, 393, 5623, 341, 8186, 11, 370, 309, 1177, 380, 534, 352, 920, 293, 291, 393, 51706], "temperature": 0.0, "avg_logprob": -0.17394132871885556, "compression_ratio": 1.8855932203389831, "no_speech_prob": 0.003425631206482649}, {"id": 130, "seek": 49704, "start": 497.04, "end": 501.40000000000003, "text": " revise it if everything goes wrong, if anything goes wrong.", "tokens": [50364, 44252, 309, 498, 1203, 1709, 2085, 11, 498, 1340, 1709, 2085, 13, 50582], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 131, "seek": 49704, "start": 501.40000000000003, "end": 503.56, "text": " The other thing you get is grounding.", "tokens": [50582, 440, 661, 551, 291, 483, 307, 46727, 13, 50690], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 132, "seek": 49704, "start": 503.56, "end": 507.48, "text": " So that's initially why I became interested in this kind of architecture, because I was", "tokens": [50690, 407, 300, 311, 9105, 983, 286, 3062, 3102, 294, 341, 733, 295, 9482, 11, 570, 286, 390, 50886], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 133, "seek": 49704, "start": 507.48, "end": 511.08000000000004, "text": " thinking a lot about grounding and multimodality and things like that, and actually one really", "tokens": [50886, 1953, 257, 688, 466, 46727, 293, 32972, 378, 1860, 293, 721, 411, 300, 11, 293, 767, 472, 534, 51066], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 134, "seek": 49704, "start": 511.08000000000004, "end": 515.76, "text": " nice way to ground things is to find some other information that you can ground your", "tokens": [51066, 1481, 636, 281, 2727, 721, 307, 281, 915, 512, 661, 1589, 300, 291, 393, 2727, 428, 51300], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 135, "seek": 49704, "start": 515.76, "end": 516.9200000000001, "text": " generation in.", "tokens": [51300, 5125, 294, 13, 51358], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 136, "seek": 49704, "start": 516.9200000000001, "end": 521.88, "text": " So you really want the language model to only say things that it has evidence for in this", "tokens": [51358, 407, 291, 534, 528, 264, 2856, 2316, 281, 787, 584, 721, 300, 309, 575, 4467, 337, 294, 341, 51606], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 137, "seek": 49704, "start": 521.88, "end": 526.72, "text": " other piece of text, or even multimodal data that it retrieves separately.", "tokens": [51606, 661, 2522, 295, 2487, 11, 420, 754, 32972, 378, 304, 1412, 300, 309, 19817, 977, 14759, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1391580836979423, "compression_ratio": 1.847457627118644, "no_speech_prob": 0.001432031742297113}, {"id": 138, "seek": 52672, "start": 526.72, "end": 530.12, "text": " So if you do that, then you get less hallucination, because you can always point back to your", "tokens": [50364, 407, 498, 291, 360, 300, 11, 550, 291, 483, 1570, 35212, 2486, 11, 570, 291, 393, 1009, 935, 646, 281, 428, 50534], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 139, "seek": 52672, "start": 530.12, "end": 533.1600000000001, "text": " source, it's always grounded in your source.", "tokens": [50534, 4009, 11, 309, 311, 1009, 23535, 294, 428, 4009, 13, 50686], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 140, "seek": 52672, "start": 533.1600000000001, "end": 536.08, "text": " And you get attribution, because you don't know why the model is saying what it's saying", "tokens": [50686, 400, 291, 483, 9080, 1448, 11, 570, 291, 500, 380, 458, 983, 264, 2316, 307, 1566, 437, 309, 311, 1566, 50832], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 141, "seek": 52672, "start": 536.08, "end": 540.88, "text": " is because it founded this thing here, is that clear?", "tokens": [50832, 307, 570, 309, 13234, 341, 551, 510, 11, 307, 300, 1850, 30, 51072], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 142, "seek": 52672, "start": 540.88, "end": 542.8000000000001, "text": " All right.", "tokens": [51072, 1057, 558, 13, 51168], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 143, "seek": 52672, "start": 542.8000000000001, "end": 549.2, "text": " So for the rest of this lecture, we're going to talk about this basic architecture.", "tokens": [51168, 407, 337, 264, 1472, 295, 341, 7991, 11, 321, 434, 516, 281, 751, 466, 341, 3875, 9482, 13, 51488], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 144, "seek": 52672, "start": 549.2, "end": 552.9200000000001, "text": " And so it kind of looks like a pretty simple thing, right?", "tokens": [51488, 400, 370, 309, 733, 295, 1542, 411, 257, 1238, 2199, 551, 11, 558, 30, 51674], "temperature": 0.0, "avg_logprob": -0.19656106045371607, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0011871957685798407}, {"id": 145, "seek": 55292, "start": 552.92, "end": 557.36, "text": " But there are actually lots and lots of questions you can ask about what the system should really", "tokens": [50364, 583, 456, 366, 767, 3195, 293, 3195, 295, 1651, 291, 393, 1029, 466, 437, 264, 1185, 820, 534, 50586], "temperature": 0.0, "avg_logprob": -0.14345722198486327, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.011858345940709114}, {"id": 146, "seek": 55292, "start": 557.36, "end": 559.0, "text": " look like.", "tokens": [50586, 574, 411, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14345722198486327, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.011858345940709114}, {"id": 147, "seek": 55292, "start": 559.0, "end": 563.1999999999999, "text": " And this doesn't even cover half the questions you can ask.", "tokens": [50668, 400, 341, 1177, 380, 754, 2060, 1922, 264, 1651, 291, 393, 1029, 13, 50878], "temperature": 0.0, "avg_logprob": -0.14345722198486327, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.011858345940709114}, {"id": 148, "seek": 55292, "start": 563.1999999999999, "end": 568.0, "text": " So it really is about how do we optimize this entire system?", "tokens": [50878, 407, 309, 534, 307, 466, 577, 360, 321, 19719, 341, 2302, 1185, 30, 51118], "temperature": 0.0, "avg_logprob": -0.14345722198486327, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.011858345940709114}, {"id": 149, "seek": 55292, "start": 568.0, "end": 573.5999999999999, "text": " So we have the separate components, the retriever, the generator, and then there are things like", "tokens": [51118, 407, 321, 362, 264, 4994, 6677, 11, 264, 19817, 331, 11, 264, 19265, 11, 293, 550, 456, 366, 721, 411, 51398], "temperature": 0.0, "avg_logprob": -0.14345722198486327, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.011858345940709114}, {"id": 150, "seek": 55292, "start": 573.5999999999999, "end": 578.88, "text": " this query encoder, how do we encode queries, how do we do the retrieval, do we update the", "tokens": [51398, 341, 14581, 2058, 19866, 11, 577, 360, 321, 2058, 1429, 24109, 11, 577, 360, 321, 360, 264, 19817, 3337, 11, 360, 321, 5623, 264, 51662], "temperature": 0.0, "avg_logprob": -0.14345722198486327, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.011858345940709114}, {"id": 151, "seek": 57888, "start": 578.88, "end": 583.4399999999999, "text": " document encoder, or how do we actually define a document, right?", "tokens": [50364, 4166, 2058, 19866, 11, 420, 577, 360, 321, 767, 6964, 257, 4166, 11, 558, 30, 50592], "temperature": 0.0, "avg_logprob": -0.15539445780744457, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.002509218640625477}, {"id": 152, "seek": 57888, "start": 583.4399999999999, "end": 588.72, "text": " Is it like a full document or is it a paragraph or a chunk or a sentence or a couple of words?", "tokens": [50592, 1119, 309, 411, 257, 1577, 4166, 420, 307, 309, 257, 18865, 420, 257, 16635, 420, 257, 8174, 420, 257, 1916, 295, 2283, 30, 50856], "temperature": 0.0, "avg_logprob": -0.15539445780744457, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.002509218640625477}, {"id": 153, "seek": 57888, "start": 588.72, "end": 591.28, "text": " So there are lots of questions to ask.", "tokens": [50856, 407, 456, 366, 3195, 295, 1651, 281, 1029, 13, 50984], "temperature": 0.0, "avg_logprob": -0.15539445780744457, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.002509218640625477}, {"id": 154, "seek": 57888, "start": 591.28, "end": 596.8, "text": " And as you'll see, there are lots of possible answers to these questions as well.", "tokens": [50984, 400, 382, 291, 603, 536, 11, 456, 366, 3195, 295, 1944, 6338, 281, 613, 1651, 382, 731, 13, 51260], "temperature": 0.0, "avg_logprob": -0.15539445780744457, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.002509218640625477}, {"id": 155, "seek": 57888, "start": 596.8, "end": 601.04, "text": " So this is what we'll cover.", "tokens": [51260, 407, 341, 307, 437, 321, 603, 2060, 13, 51472], "temperature": 0.0, "avg_logprob": -0.15539445780744457, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.002509218640625477}, {"id": 156, "seek": 57888, "start": 601.04, "end": 606.32, "text": " So there are lots of architectures going into these questions.", "tokens": [51472, 407, 456, 366, 3195, 295, 6331, 1303, 516, 666, 613, 1651, 13, 51736], "temperature": 0.0, "avg_logprob": -0.15539445780744457, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.002509218640625477}, {"id": 157, "seek": 60632, "start": 606.32, "end": 611.32, "text": " And I think as we go through them, it's useful for you to think about what happens during", "tokens": [50364, 400, 286, 519, 382, 321, 352, 807, 552, 11, 309, 311, 4420, 337, 291, 281, 519, 466, 437, 2314, 1830, 50614], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 158, "seek": 60632, "start": 611.32, "end": 614.44, "text": " training time and what happens during test time, right?", "tokens": [50614, 3097, 565, 293, 437, 2314, 1830, 1500, 565, 11, 558, 30, 50770], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 159, "seek": 60632, "start": 614.44, "end": 620.72, "text": " So during training time, it's really, okay, we have the language model, we have this retriever.", "tokens": [50770, 407, 1830, 3097, 565, 11, 309, 311, 534, 11, 1392, 11, 321, 362, 264, 2856, 2316, 11, 321, 362, 341, 19817, 331, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 160, "seek": 60632, "start": 620.72, "end": 621.9200000000001, "text": " Which one do we update?", "tokens": [51084, 3013, 472, 360, 321, 5623, 30, 51144], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 161, "seek": 60632, "start": 621.9200000000001, "end": 623.0400000000001, "text": " How do we update them?", "tokens": [51144, 1012, 360, 321, 5623, 552, 30, 51200], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 162, "seek": 60632, "start": 623.0400000000001, "end": 624.88, "text": " How do we train this entire system?", "tokens": [51200, 1012, 360, 321, 3847, 341, 2302, 1185, 30, 51292], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 163, "seek": 60632, "start": 624.88, "end": 627.12, "text": " Do we maybe not train it at all?", "tokens": [51292, 1144, 321, 1310, 406, 3847, 309, 412, 439, 30, 51404], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 164, "seek": 60632, "start": 627.12, "end": 628.6, "text": " Do we pre-train it from scratch?", "tokens": [51404, 1144, 321, 659, 12, 83, 7146, 309, 490, 8459, 30, 51478], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 165, "seek": 60632, "start": 628.6, "end": 633.1600000000001, "text": " Do we initialize it with components that were already separately trained?", "tokens": [51478, 1144, 321, 5883, 1125, 309, 365, 6677, 300, 645, 1217, 14759, 8895, 30, 51706], "temperature": 0.0, "avg_logprob": -0.1215373296586294, "compression_ratio": 1.8125, "no_speech_prob": 0.010976317338645458}, {"id": 166, "seek": 63316, "start": 633.16, "end": 636.4, "text": " These are the kinds of questions that you have to answer if you want to design a system", "tokens": [50364, 1981, 366, 264, 3685, 295, 1651, 300, 291, 362, 281, 1867, 498, 291, 528, 281, 1715, 257, 1185, 50526], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 167, "seek": 63316, "start": 636.4, "end": 638.0, "text": " like this.", "tokens": [50526, 411, 341, 13, 50606], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 168, "seek": 63316, "start": 638.0, "end": 641.76, "text": " And then during test time, you have this entire system, right?", "tokens": [50606, 400, 550, 1830, 1500, 565, 11, 291, 362, 341, 2302, 1185, 11, 558, 30, 50794], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 169, "seek": 63316, "start": 641.76, "end": 647.7199999999999, "text": " So actually multiple models in a way that are working together.", "tokens": [50794, 407, 767, 3866, 5245, 294, 257, 636, 300, 366, 1364, 1214, 13, 51092], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 170, "seek": 63316, "start": 647.7199999999999, "end": 649.8, "text": " So there's also different things you can do there, right?", "tokens": [51092, 407, 456, 311, 611, 819, 721, 291, 393, 360, 456, 11, 558, 30, 51196], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 171, "seek": 63316, "start": 649.8, "end": 654.48, "text": " So give it different indices during test time or manipulate kind of how you're sampling", "tokens": [51196, 407, 976, 309, 819, 43840, 1830, 1500, 565, 420, 20459, 733, 295, 577, 291, 434, 21179, 51430], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 172, "seek": 63316, "start": 654.48, "end": 656.76, "text": " things like that.", "tokens": [51430, 721, 411, 300, 13, 51544], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 173, "seek": 63316, "start": 656.76, "end": 661.36, "text": " So the starting point for all of this stuff, I think if you ask someone now, like, what", "tokens": [51544, 407, 264, 2891, 935, 337, 439, 295, 341, 1507, 11, 286, 519, 498, 291, 1029, 1580, 586, 11, 411, 11, 437, 51774], "temperature": 0.0, "avg_logprob": -0.12380620010760653, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.007691705133765936}, {"id": 174, "seek": 66136, "start": 661.4, "end": 664.72, "text": " is RAG, they will think of this thing.", "tokens": [50366, 307, 14626, 38, 11, 436, 486, 519, 295, 341, 551, 13, 50532], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 175, "seek": 66136, "start": 664.72, "end": 668.4, "text": " So this is frozen RAG basically.", "tokens": [50532, 407, 341, 307, 12496, 14626, 38, 1936, 13, 50716], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 176, "seek": 66136, "start": 668.4, "end": 669.8000000000001, "text": " There's no training here at all.", "tokens": [50716, 821, 311, 572, 3097, 510, 412, 439, 13, 50786], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 177, "seek": 66136, "start": 669.8000000000001, "end": 673.5600000000001, "text": " So going back to this question of train time, test time, there's only test time here.", "tokens": [50786, 407, 516, 646, 281, 341, 1168, 295, 3847, 565, 11, 1500, 565, 11, 456, 311, 787, 1500, 565, 510, 13, 50974], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 178, "seek": 66136, "start": 673.5600000000001, "end": 677.84, "text": " Train time happens separately with these kind of black box models that we don't necessarily", "tokens": [50974, 28029, 565, 2314, 14759, 365, 613, 733, 295, 2211, 2424, 5245, 300, 321, 500, 380, 4725, 51188], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 179, "seek": 66136, "start": 677.84, "end": 679.08, "text": " have control over, right?", "tokens": [51188, 362, 1969, 670, 11, 558, 30, 51250], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 180, "seek": 66136, "start": 679.08, "end": 684.12, "text": " So there's this document embedding model as whatever is currently at the top of some", "tokens": [51250, 407, 456, 311, 341, 4166, 12240, 3584, 2316, 382, 2035, 307, 4362, 412, 264, 1192, 295, 512, 51502], "temperature": 0.0, "avg_logprob": -0.14596897948021983, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.04020120948553085}, {"id": 181, "seek": 68412, "start": 684.16, "end": 686.76, "text": " open source leaderboard.", "tokens": [50366, 1269, 4009, 5263, 3787, 13, 50496], "temperature": 0.0, "avg_logprob": -0.18784424066543579, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.00711642112582922}, {"id": 182, "seek": 68412, "start": 686.76, "end": 694.16, "text": " You use that to get some vectors that you then use to create this vector database.", "tokens": [50496, 509, 764, 300, 281, 483, 512, 18875, 300, 291, 550, 764, 281, 1884, 341, 8062, 8149, 13, 50866], "temperature": 0.0, "avg_logprob": -0.18784424066543579, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.00711642112582922}, {"id": 183, "seek": 68412, "start": 694.16, "end": 698.4, "text": " And then the vector database just does search and it gives the information from the search", "tokens": [50866, 400, 550, 264, 8062, 8149, 445, 775, 3164, 293, 309, 2709, 264, 1589, 490, 264, 3164, 51078], "temperature": 0.0, "avg_logprob": -0.18784424066543579, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.00711642112582922}, {"id": 184, "seek": 68412, "start": 698.4, "end": 699.84, "text": " to the language model.", "tokens": [51078, 281, 264, 2856, 2316, 13, 51150], "temperature": 0.0, "avg_logprob": -0.18784424066543579, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.00711642112582922}, {"id": 185, "seek": 68412, "start": 699.84, "end": 703.72, "text": " And it just passes it as the context, right?", "tokens": [51150, 400, 309, 445, 11335, 309, 382, 264, 4319, 11, 558, 30, 51344], "temperature": 0.0, "avg_logprob": -0.18784424066543579, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.00711642112582922}, {"id": 186, "seek": 68412, "start": 703.72, "end": 707.8, "text": " So this only works because of in-context learning.", "tokens": [51344, 407, 341, 787, 1985, 570, 295, 294, 12, 9000, 3828, 2539, 13, 51548], "temperature": 0.0, "avg_logprob": -0.18784424066543579, "compression_ratio": 1.6256410256410256, "no_speech_prob": 0.00711642112582922}, {"id": 187, "seek": 70780, "start": 707.88, "end": 714.24, "text": " And I think as a machine learner myself, this feels very inelegant.", "tokens": [50368, 400, 286, 519, 382, 257, 3479, 33347, 2059, 11, 341, 3417, 588, 7167, 6363, 394, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1940585641299977, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0011154116364195943}, {"id": 188, "seek": 70780, "start": 714.24, "end": 721.3199999999999, "text": " So what this lecture is about is, can we do better than this frozen thing?", "tokens": [50686, 407, 437, 341, 7991, 307, 466, 307, 11, 393, 321, 360, 1101, 813, 341, 12496, 551, 30, 51040], "temperature": 0.0, "avg_logprob": -0.1940585641299977, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0011154116364195943}, {"id": 189, "seek": 70780, "start": 721.3199999999999, "end": 725.16, "text": " So let's start from the left side of this.", "tokens": [51040, 407, 718, 311, 722, 490, 264, 1411, 1252, 295, 341, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1940585641299977, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0011154116364195943}, {"id": 190, "seek": 70780, "start": 725.16, "end": 729.76, "text": " OK, if we want to outperform this frozen thing itself with just the vector database,", "tokens": [51232, 2264, 11, 498, 321, 528, 281, 484, 26765, 341, 12496, 551, 2564, 365, 445, 264, 8062, 8149, 11, 51462], "temperature": 0.0, "avg_logprob": -0.1940585641299977, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0011154116364195943}, {"id": 191, "seek": 70780, "start": 729.76, "end": 734.1999999999999, "text": " what would that look like from a retrieval perspective?", "tokens": [51462, 437, 576, 300, 574, 411, 490, 257, 19817, 3337, 4585, 30, 51684], "temperature": 0.0, "avg_logprob": -0.1940585641299977, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0011154116364195943}, {"id": 192, "seek": 73420, "start": 734.2, "end": 738.08, "text": " And the starting point for everything retrieval is TFIDF.", "tokens": [50364, 400, 264, 2891, 935, 337, 1203, 19817, 3337, 307, 40964, 2777, 37, 13, 50558], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 193, "seek": 73420, "start": 738.08, "end": 741.2800000000001, "text": " Does everybody know what TFIDF is?", "tokens": [50558, 4402, 2201, 458, 437, 40964, 2777, 37, 307, 30, 50718], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 194, "seek": 73420, "start": 741.2800000000001, "end": 741.72, "text": " No?", "tokens": [50718, 883, 30, 50740], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 195, "seek": 73420, "start": 741.72, "end": 743.08, "text": " OK.", "tokens": [50740, 2264, 13, 50808], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 196, "seek": 73420, "start": 743.08, "end": 749.4000000000001, "text": " So TFIDF is basically a sparse retrieval method where you have a score function that", "tokens": [50808, 407, 40964, 2777, 37, 307, 1936, 257, 637, 11668, 19817, 3337, 3170, 689, 291, 362, 257, 6175, 2445, 300, 51124], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 197, "seek": 73420, "start": 749.4000000000001, "end": 753.2800000000001, "text": " looks at documents and queries, so E and Q.", "tokens": [51124, 1542, 412, 8512, 293, 24109, 11, 370, 462, 293, 1249, 13, 51318], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 198, "seek": 73420, "start": 753.2800000000001, "end": 755.2800000000001, "text": " And then there are basically two terms that matter.", "tokens": [51318, 400, 550, 456, 366, 1936, 732, 2115, 300, 1871, 13, 51418], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 199, "seek": 73420, "start": 755.2800000000001, "end": 760.6800000000001, "text": " One is the TF, the term frequency, and the other is the IDF, the inverse document frequency.", "tokens": [51418, 1485, 307, 264, 40964, 11, 264, 1433, 7893, 11, 293, 264, 661, 307, 264, 7348, 37, 11, 264, 17340, 4166, 7893, 13, 51688], "temperature": 0.0, "avg_logprob": -0.15384967372102556, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0018377237720414996}, {"id": 200, "seek": 76068, "start": 760.7199999999999, "end": 765.16, "text": " So this inverse document frequency is actually a really nice idea from Karen Spark-Jones,", "tokens": [50366, 407, 341, 17340, 4166, 7893, 307, 767, 257, 534, 1481, 1558, 490, 14834, 23424, 12, 41, 2213, 11, 50588], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 201, "seek": 76068, "start": 765.16, "end": 766.56, "text": " really underrated researcher.", "tokens": [50588, 534, 833, 5468, 21751, 13, 50658], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 202, "seek": 76068, "start": 766.56, "end": 769.0799999999999, "text": " She's done some amazing work.", "tokens": [50658, 1240, 311, 1096, 512, 2243, 589, 13, 50784], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 203, "seek": 76068, "start": 769.0799999999999, "end": 773.56, "text": " But the basic idea is that you want to look at the words that are very special,", "tokens": [50784, 583, 264, 3875, 1558, 307, 300, 291, 528, 281, 574, 412, 264, 2283, 300, 366, 588, 2121, 11, 51008], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 204, "seek": 76068, "start": 773.56, "end": 775.88, "text": " so that don't occur in lots of different documents.", "tokens": [51008, 370, 300, 500, 380, 5160, 294, 3195, 295, 819, 8512, 13, 51124], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 205, "seek": 76068, "start": 775.88, "end": 779.4399999999999, "text": " And so the overlap between the word the doesn't really matter, right?", "tokens": [51124, 400, 370, 264, 19959, 1296, 264, 1349, 264, 1177, 380, 534, 1871, 11, 558, 30, 51302], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 206, "seek": 76068, "start": 779.4399999999999, "end": 781.52, "text": " Like the occurs everywhere.", "tokens": [51302, 1743, 264, 11843, 5315, 13, 51406], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 207, "seek": 76068, "start": 781.52, "end": 784.04, "text": " So you want to have sort of the special words.", "tokens": [51406, 407, 291, 528, 281, 362, 1333, 295, 264, 2121, 2283, 13, 51532], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 208, "seek": 76068, "start": 784.04, "end": 786.4399999999999, "text": " So that's what TFIDF does in a nutshell.", "tokens": [51532, 407, 300, 311, 437, 40964, 2777, 37, 775, 294, 257, 37711, 13, 51652], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 209, "seek": 76068, "start": 786.4399999999999, "end": 790.04, "text": " It gives you a score for document query overlap.", "tokens": [51652, 467, 2709, 291, 257, 6175, 337, 4166, 14581, 19959, 13, 51832], "temperature": 0.0, "avg_logprob": -0.144466827164835, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.0006984478677622974}, {"id": 210, "seek": 79004, "start": 790.04, "end": 793.76, "text": " And then you can do all kinds of things here with how you weigh it.", "tokens": [50364, 400, 550, 291, 393, 360, 439, 3685, 295, 721, 510, 365, 577, 291, 13843, 309, 13, 50550], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 211, "seek": 79004, "start": 793.76, "end": 797.68, "text": " So there's all these weird different parameters like this B and things like that", "tokens": [50550, 407, 456, 311, 439, 613, 3657, 819, 9834, 411, 341, 363, 293, 721, 411, 300, 50746], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 212, "seek": 79004, "start": 797.68, "end": 802.36, "text": " that allow you to make it better than just having the TFIDF score.", "tokens": [50746, 300, 2089, 291, 281, 652, 309, 1101, 813, 445, 1419, 264, 40964, 2777, 37, 6175, 13, 50980], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 213, "seek": 79004, "start": 802.36, "end": 804.52, "text": " So there's a couple of tweaks you can do there.", "tokens": [50980, 407, 456, 311, 257, 1916, 295, 46664, 291, 393, 360, 456, 13, 51088], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 214, "seek": 79004, "start": 804.52, "end": 809.4, "text": " So BM25, actually, in case you're wondering, stands for Best Match 25.", "tokens": [51088, 407, 15901, 6074, 11, 767, 11, 294, 1389, 291, 434, 6359, 11, 7382, 337, 9752, 26178, 3552, 13, 51332], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 215, "seek": 79004, "start": 809.4, "end": 814.0, "text": " So I tried to discover where does the 25 actually come from?", "tokens": [51332, 407, 286, 3031, 281, 4411, 689, 775, 264, 3552, 767, 808, 490, 30, 51562], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 216, "seek": 79004, "start": 814.0, "end": 819.16, "text": " That's because the preceding 24 experiments failed.", "tokens": [51562, 663, 311, 570, 264, 16969, 278, 4022, 12050, 7612, 13, 51820], "temperature": 0.0, "avg_logprob": -0.14878169421491952, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.0012249520514160395}, {"id": 217, "seek": 81916, "start": 819.16, "end": 821.52, "text": " So it's literally the 25th one that seemed to work.", "tokens": [50364, 407, 309, 311, 3736, 264, 3552, 392, 472, 300, 6576, 281, 589, 13, 50482], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 218, "seek": 81916, "start": 821.52, "end": 824.0799999999999, "text": " And that's why it's called BM25.", "tokens": [50482, 400, 300, 311, 983, 309, 311, 1219, 15901, 6074, 13, 50610], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 219, "seek": 81916, "start": 824.0799999999999, "end": 824.8399999999999, "text": " Bizarre.", "tokens": [50610, 363, 9736, 265, 13, 50648], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 220, "seek": 81916, "start": 824.8399999999999, "end": 828.76, "text": " But so this is sparse retrieval.", "tokens": [50648, 583, 370, 341, 307, 637, 11668, 19817, 3337, 13, 50844], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 221, "seek": 81916, "start": 828.76, "end": 830.0, "text": " It's just counting words.", "tokens": [50844, 467, 311, 445, 13251, 2283, 13, 50906], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 222, "seek": 81916, "start": 830.0, "end": 833.88, "text": " So you have this massive, massive vector of all these word occurrences.", "tokens": [50906, 407, 291, 362, 341, 5994, 11, 5994, 8062, 295, 439, 613, 1349, 5160, 38983, 13, 51100], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 223, "seek": 81916, "start": 833.88, "end": 836.4, "text": " It's sparse because most words never occur.", "tokens": [51100, 467, 311, 637, 11668, 570, 881, 2283, 1128, 5160, 13, 51226], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 224, "seek": 81916, "start": 836.4, "end": 842.1999999999999, "text": " So it's sort of like a vector of vocabulary size dimensions.", "tokens": [51226, 407, 309, 311, 1333, 295, 411, 257, 8062, 295, 19864, 2744, 12819, 13, 51516], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 225, "seek": 81916, "start": 842.1999999999999, "end": 844.88, "text": " So most of that is obviously zero.", "tokens": [51516, 407, 881, 295, 300, 307, 2745, 4018, 13, 51650], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 226, "seek": 81916, "start": 844.88, "end": 847.12, "text": " But so that's actually kind of a nice property.", "tokens": [51650, 583, 370, 300, 311, 767, 733, 295, 257, 1481, 4707, 13, 51762], "temperature": 0.0, "avg_logprob": -0.17856637308420228, "compression_ratio": 1.6284584980237153, "no_speech_prob": 0.00028667590231634676}, {"id": 227, "seek": 84712, "start": 847.12, "end": 852.44, "text": " If you want to do fast search on a CPU, because on a CPU sparse product,", "tokens": [50364, 759, 291, 528, 281, 360, 2370, 3164, 322, 257, 13199, 11, 570, 322, 257, 13199, 637, 11668, 1674, 11, 50630], "temperature": 0.0, "avg_logprob": -0.21948075950692553, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0006459591677412391}, {"id": 228, "seek": 84712, "start": 852.44, "end": 854.44, "text": " it's very easy to compute.", "tokens": [50630, 309, 311, 588, 1858, 281, 14722, 13, 50730], "temperature": 0.0, "avg_logprob": -0.21948075950692553, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0006459591677412391}, {"id": 229, "seek": 84712, "start": 854.44, "end": 861.16, "text": " So this is used in the system called Dr. QA, which is really one of the first", "tokens": [50730, 407, 341, 307, 1143, 294, 264, 1185, 1219, 2491, 13, 1249, 32, 11, 597, 307, 534, 472, 295, 264, 700, 51066], "temperature": 0.0, "avg_logprob": -0.21948075950692553, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0006459591677412391}, {"id": 230, "seek": 84712, "start": 861.16, "end": 867.32, "text": " neural instances of this open domain, sort of open book question answer in paradigm.", "tokens": [51066, 18161, 14519, 295, 341, 1269, 9274, 11, 1333, 295, 1269, 1446, 1168, 1867, 294, 24709, 13, 51374], "temperature": 0.0, "avg_logprob": -0.21948075950692553, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0006459591677412391}, {"id": 231, "seek": 84712, "start": 867.32, "end": 871.76, "text": " So you have a question like how many of our cells in habitants, blah, blah.", "tokens": [51374, 407, 291, 362, 257, 1168, 411, 577, 867, 295, 527, 5438, 294, 7164, 1719, 11, 12288, 11, 12288, 13, 51596], "temperature": 0.0, "avg_logprob": -0.21948075950692553, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0006459591677412391}, {"id": 232, "seek": 84712, "start": 871.76, "end": 875.28, "text": " So you want to ask basically Wikipedia what the answer is for this.", "tokens": [51596, 407, 291, 528, 281, 1029, 1936, 28999, 437, 264, 1867, 307, 337, 341, 13, 51772], "temperature": 0.0, "avg_logprob": -0.21948075950692553, "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0006459591677412391}, {"id": 233, "seek": 87528, "start": 875.28, "end": 878.9599999999999, "text": " So then you have this document retriever based on the sparse.", "tokens": [50364, 407, 550, 291, 362, 341, 4166, 19817, 331, 2361, 322, 264, 637, 11668, 13, 50548], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 234, "seek": 87528, "start": 878.9599999999999, "end": 882.76, "text": " So BM25, I think in this case, retrieval methods.", "tokens": [50548, 407, 15901, 6074, 11, 286, 519, 294, 341, 1389, 11, 19817, 3337, 7150, 13, 50738], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 235, "seek": 87528, "start": 882.76, "end": 885.48, "text": " You pass that to this.", "tokens": [50738, 509, 1320, 300, 281, 341, 13, 50874], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 236, "seek": 87528, "start": 885.48, "end": 890.64, "text": " I think this was still by LSTM at the time, a document reader model.", "tokens": [50874, 286, 519, 341, 390, 920, 538, 441, 6840, 44, 412, 264, 565, 11, 257, 4166, 15149, 2316, 13, 51132], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 237, "seek": 87528, "start": 890.64, "end": 894.28, "text": " And then that model gives you the answer.", "tokens": [51132, 400, 550, 300, 2316, 2709, 291, 264, 1867, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 238, "seek": 87528, "start": 894.28, "end": 898.0799999999999, "text": " So this, I think, is really the first instance of having sort of this separation", "tokens": [51314, 407, 341, 11, 286, 519, 11, 307, 534, 264, 700, 5197, 295, 1419, 1333, 295, 341, 14634, 51504], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 239, "seek": 87528, "start": 898.0799999999999, "end": 903.0, "text": " between a retrieval and a generator system that you use for answering complicated", "tokens": [51504, 1296, 257, 19817, 3337, 293, 257, 19265, 1185, 300, 291, 764, 337, 13430, 6179, 51750], "temperature": 0.0, "avg_logprob": -0.18450597480491357, "compression_ratio": 1.6721311475409837, "no_speech_prob": 0.0002164938487112522}, {"id": 240, "seek": 90300, "start": 903.0, "end": 907.2, "text": " questions based on sort of open domain knowledge.", "tokens": [50364, 1651, 2361, 322, 1333, 295, 1269, 9274, 3601, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14963490355248546, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0005031829350627959}, {"id": 241, "seek": 90300, "start": 907.2, "end": 912.8, "text": " So after the sparse stuff, there was a bunch of work on dense retrieval.", "tokens": [50574, 407, 934, 264, 637, 11668, 1507, 11, 456, 390, 257, 3840, 295, 589, 322, 18011, 19817, 3337, 13, 50854], "temperature": 0.0, "avg_logprob": -0.14963490355248546, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0005031829350627959}, {"id": 242, "seek": 90300, "start": 912.8, "end": 917.08, "text": " And so the advantage of dense retrieval, so this is just like word embeddings,", "tokens": [50854, 400, 370, 264, 5002, 295, 18011, 19817, 3337, 11, 370, 341, 307, 445, 411, 1349, 12240, 29432, 11, 51068], "temperature": 0.0, "avg_logprob": -0.14963490355248546, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0005031829350627959}, {"id": 243, "seek": 90300, "start": 917.08, "end": 920.24, "text": " basically vectors, like they're dense now, no longer sparse.", "tokens": [51068, 1936, 18875, 11, 411, 436, 434, 18011, 586, 11, 572, 2854, 637, 11668, 13, 51226], "temperature": 0.0, "avg_logprob": -0.14963490355248546, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0005031829350627959}, {"id": 244, "seek": 90300, "start": 920.24, "end": 924.08, "text": " So they're much smaller in terms of dimensionality.", "tokens": [51226, 407, 436, 434, 709, 4356, 294, 2115, 295, 10139, 1860, 13, 51418], "temperature": 0.0, "avg_logprob": -0.14963490355248546, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0005031829350627959}, {"id": 245, "seek": 90300, "start": 924.08, "end": 928.96, "text": " And a nice advantage of dense retrieval is that it's not really about specific words.", "tokens": [51418, 400, 257, 1481, 5002, 295, 18011, 19817, 3337, 307, 300, 309, 311, 406, 534, 466, 2685, 2283, 13, 51662], "temperature": 0.0, "avg_logprob": -0.14963490355248546, "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.0005031829350627959}, {"id": 246, "seek": 92896, "start": 928.96, "end": 935.0400000000001, "text": " So if they're synonyms, you can still find the relevant document,", "tokens": [50364, 407, 498, 436, 434, 5451, 2526, 2592, 11, 291, 393, 920, 915, 264, 7340, 4166, 11, 50668], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 247, "seek": 92896, "start": 935.0400000000001, "end": 937.72, "text": " which you couldn't really do with a sparse representation.", "tokens": [50668, 597, 291, 2809, 380, 534, 360, 365, 257, 637, 11668, 10290, 13, 50802], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 248, "seek": 92896, "start": 937.72, "end": 943.4000000000001, "text": " So that's really the advantage of dense is that you get like semantic similarity.", "tokens": [50802, 407, 300, 311, 534, 264, 5002, 295, 18011, 307, 300, 291, 483, 411, 47982, 32194, 13, 51086], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 249, "seek": 92896, "start": 943.4000000000001, "end": 946.0400000000001, "text": " So you can do this over word embeddings.", "tokens": [51086, 407, 291, 393, 360, 341, 670, 1349, 12240, 29432, 13, 51218], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 250, "seek": 92896, "start": 946.0400000000001, "end": 947.44, "text": " That doesn't really work all that well.", "tokens": [51218, 663, 1177, 380, 534, 589, 439, 300, 731, 13, 51288], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 251, "seek": 92896, "start": 947.44, "end": 950.24, "text": " But at the time that people started thinking about this,", "tokens": [51288, 583, 412, 264, 565, 300, 561, 1409, 1953, 466, 341, 11, 51428], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 252, "seek": 92896, "start": 950.24, "end": 951.4000000000001, "text": " Bert was already out there.", "tokens": [51428, 29594, 390, 1217, 484, 456, 13, 51486], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 253, "seek": 92896, "start": 951.4000000000001, "end": 954.36, "text": " And Bert is really great for giving you a vector representation", "tokens": [51486, 400, 29594, 307, 534, 869, 337, 2902, 291, 257, 8062, 10290, 51634], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 254, "seek": 92896, "start": 954.36, "end": 956.32, "text": " for an entire sequence of words.", "tokens": [51634, 337, 364, 2302, 8310, 295, 2283, 13, 51732], "temperature": 0.0, "avg_logprob": -0.16543541924428132, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0001313243992626667}, {"id": 255, "seek": 95632, "start": 956.32, "end": 959.5600000000001, "text": " So it sent this representation or a passage representation.", "tokens": [50364, 407, 309, 2279, 341, 10290, 420, 257, 11497, 10290, 13, 50526], "temperature": 0.0, "avg_logprob": -0.1657528347439236, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.0009393239160999656}, {"id": 256, "seek": 95632, "start": 959.5600000000001, "end": 965.2, "text": " So there are all these cool systems like ORCA and DPR, the dense passage retriever,", "tokens": [50526, 407, 456, 366, 439, 613, 1627, 3652, 411, 19654, 15515, 293, 413, 15958, 11, 264, 18011, 11497, 19817, 331, 11, 50808], "temperature": 0.0, "avg_logprob": -0.1657528347439236, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.0009393239160999656}, {"id": 257, "seek": 95632, "start": 965.2, "end": 971.8000000000001, "text": " where they essentially use the retrieval as a kind of latent variable in the system.", "tokens": [50808, 689, 436, 4476, 764, 264, 19817, 3337, 382, 257, 733, 295, 48994, 7006, 294, 264, 1185, 13, 51138], "temperature": 0.0, "avg_logprob": -0.1657528347439236, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.0009393239160999656}, {"id": 258, "seek": 95632, "start": 971.8000000000001, "end": 976.44, "text": " And the way to get the latent variable to work, to be good enough essentially", "tokens": [51138, 400, 264, 636, 281, 483, 264, 48994, 7006, 281, 589, 11, 281, 312, 665, 1547, 4476, 51370], "temperature": 0.0, "avg_logprob": -0.1657528347439236, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.0009393239160999656}, {"id": 259, "seek": 95632, "start": 976.44, "end": 981.6400000000001, "text": " to train the entire system, is to pre-train the retriever on relevant information.", "tokens": [51370, 281, 3847, 264, 2302, 1185, 11, 307, 281, 659, 12, 83, 7146, 264, 19817, 331, 322, 7340, 1589, 13, 51630], "temperature": 0.0, "avg_logprob": -0.1657528347439236, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.0009393239160999656}, {"id": 260, "seek": 95632, "start": 981.6400000000001, "end": 985.6400000000001, "text": " So for ORCA, they do something called inverse close.", "tokens": [51630, 407, 337, 19654, 15515, 11, 436, 360, 746, 1219, 17340, 1998, 13, 51830], "temperature": 0.0, "avg_logprob": -0.1657528347439236, "compression_ratio": 1.8264462809917354, "no_speech_prob": 0.0009393239160999656}, {"id": 261, "seek": 98564, "start": 985.64, "end": 990.28, "text": " So they do kind of a close task where you want to find passages", "tokens": [50364, 407, 436, 360, 733, 295, 257, 1998, 5633, 689, 291, 528, 281, 915, 31589, 50596], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 262, "seek": 98564, "start": 990.28, "end": 993.48, "text": " that are sort of relevant to the preceding passage.", "tokens": [50596, 300, 366, 1333, 295, 7340, 281, 264, 16969, 278, 11497, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 263, "seek": 98564, "start": 993.48, "end": 996.68, "text": " And in DPR, they just train it on the supervised thing.", "tokens": [50756, 400, 294, 413, 15958, 11, 436, 445, 3847, 309, 322, 264, 46533, 551, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 264, "seek": 98564, "start": 996.68, "end": 1000.84, "text": " But really, the core idea here is that, as you can see in this graph here,", "tokens": [50916, 583, 534, 11, 264, 4965, 1558, 510, 307, 300, 11, 382, 291, 393, 536, 294, 341, 4295, 510, 11, 51124], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 265, "seek": 98564, "start": 1000.84, "end": 1004.56, "text": " you can do better than BM25 if you add lots of documents.", "tokens": [51124, 291, 393, 360, 1101, 813, 15901, 6074, 498, 291, 909, 3195, 295, 8512, 13, 51310], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 266, "seek": 98564, "start": 1004.56, "end": 1007.08, "text": " And the way you compute the score function is much simpler.", "tokens": [51310, 400, 264, 636, 291, 14722, 264, 6175, 2445, 307, 709, 18587, 13, 51436], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 267, "seek": 98564, "start": 1007.08, "end": 1011.0, "text": " It's just a dot product, right?", "tokens": [51436, 467, 311, 445, 257, 5893, 1674, 11, 558, 30, 51632], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 268, "seek": 98564, "start": 1011.0, "end": 1013.96, "text": " So the nice thing about dot products", "tokens": [51632, 407, 264, 1481, 551, 466, 5893, 3383, 51780], "temperature": 0.0, "avg_logprob": -0.1585661031432071, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0003005375911016017}, {"id": 269, "seek": 101396, "start": 1013.96, "end": 1018.76, "text": " is that you can do them very, very efficiently on the GPU as well", "tokens": [50364, 307, 300, 291, 393, 360, 552, 588, 11, 588, 19621, 322, 264, 18407, 382, 731, 50604], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 270, "seek": 101396, "start": 1018.76, "end": 1020.5600000000001, "text": " if you know what you're doing.", "tokens": [50604, 498, 291, 458, 437, 291, 434, 884, 13, 50694], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 271, "seek": 101396, "start": 1020.5600000000001, "end": 1025.2, "text": " So what you really want to get at is Maximum Inner Product Search MIPS.", "tokens": [50694, 407, 437, 291, 534, 528, 281, 483, 412, 307, 29076, 449, 36705, 22005, 17180, 13696, 6273, 13, 50926], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 272, "seek": 101396, "start": 1025.2, "end": 1029.16, "text": " So this is one of the kind of core ideas of a lot of this stuff.", "tokens": [50926, 407, 341, 307, 472, 295, 264, 733, 295, 4965, 3487, 295, 257, 688, 295, 341, 1507, 13, 51124], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 273, "seek": 101396, "start": 1029.16, "end": 1034.56, "text": " And you can do MIPS with ANN, Approximate Neighbor Search.", "tokens": [51124, 400, 291, 393, 360, 13696, 6273, 365, 5252, 45, 11, 29551, 87, 2905, 47729, 17180, 13, 51394], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 274, "seek": 101396, "start": 1034.56, "end": 1038.96, "text": " And so there's this really brilliant piece of work out of there.", "tokens": [51394, 400, 370, 456, 311, 341, 534, 10248, 2522, 295, 589, 484, 295, 456, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 275, "seek": 101396, "start": 1038.96, "end": 1042.2, "text": " For my colleagues at the time, I called FACE,", "tokens": [51614, 1171, 452, 7734, 412, 264, 565, 11, 286, 1219, 479, 23866, 11, 51776], "temperature": 0.0, "avg_logprob": -0.16466114210045857, "compression_ratio": 1.57421875, "no_speech_prob": 0.0017810994759202003}, {"id": 276, "seek": 104220, "start": 1042.2, "end": 1046.16, "text": " which really underlies all of these modern vector databases, right?", "tokens": [50364, 597, 534, 833, 24119, 439, 295, 613, 4363, 8062, 22380, 11, 558, 30, 50562], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 277, "seek": 104220, "start": 1046.16, "end": 1050.44, "text": " So all the popular ones are sort of re-implementations of this FACE idea.", "tokens": [50562, 407, 439, 264, 3743, 2306, 366, 1333, 295, 319, 12, 332, 43704, 763, 295, 341, 479, 23866, 1558, 13, 50776], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 278, "seek": 104220, "start": 1050.44, "end": 1053.68, "text": " One is in Rust, one is in Go, but it's all basically the same idea.", "tokens": [50776, 1485, 307, 294, 34952, 11, 472, 307, 294, 1037, 11, 457, 309, 311, 439, 1936, 264, 912, 1558, 13, 50938], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 279, "seek": 104220, "start": 1053.68, "end": 1056.2, "text": " It's just FACE.", "tokens": [50938, 467, 311, 445, 479, 23866, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 280, "seek": 104220, "start": 1056.2, "end": 1059.8, "text": " And so FACE really powers a lot of this stuff.", "tokens": [51064, 400, 370, 479, 23866, 534, 8674, 257, 688, 295, 341, 1507, 13, 51244], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 281, "seek": 104220, "start": 1059.8, "end": 1062.72, "text": " And whenever somebody tells you something about a vector database,", "tokens": [51244, 400, 5699, 2618, 5112, 291, 746, 466, 257, 8062, 8149, 11, 51390], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 282, "seek": 104220, "start": 1062.72, "end": 1068.2, "text": " just think about FACE, very fast dot product.", "tokens": [51390, 445, 519, 466, 479, 23866, 11, 588, 2370, 5893, 1674, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 283, "seek": 104220, "start": 1068.2, "end": 1071.0800000000002, "text": " So obviously, you can go beyond dot product.", "tokens": [51664, 407, 2745, 11, 291, 393, 352, 4399, 5893, 1674, 13, 51808], "temperature": 0.0, "avg_logprob": -0.16356771132525275, "compression_ratio": 1.6796875, "no_speech_prob": 0.0002097912220051512}, {"id": 284, "seek": 107108, "start": 1071.08, "end": 1071.8, "text": " Yes.", "tokens": [50364, 1079, 13, 50400], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 285, "seek": 107108, "start": 1071.8, "end": 1073.6, "text": " What is FACE?", "tokens": [50400, 708, 307, 479, 23866, 30, 50490], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 286, "seek": 107108, "start": 1073.6, "end": 1075.24, "text": " What is FACE?", "tokens": [50490, 708, 307, 479, 23866, 30, 50572], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 287, "seek": 107108, "start": 1075.24, "end": 1079.0, "text": " So it's an open source library, Facebook AI similarity search.", "tokens": [50572, 407, 309, 311, 364, 1269, 4009, 6405, 11, 4384, 7318, 32194, 3164, 13, 50760], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 288, "seek": 107108, "start": 1079.0, "end": 1080.28, "text": " Yeah, it's something.", "tokens": [50760, 865, 11, 309, 311, 746, 13, 50824], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 289, "seek": 107108, "start": 1080.28, "end": 1083.08, "text": " Yes.", "tokens": [50824, 1079, 13, 50964], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 290, "seek": 107108, "start": 1083.08, "end": 1086.4399999999998, "text": " No, so it's just basic off the shelf, ANN algorithms.", "tokens": [50964, 883, 11, 370, 309, 311, 445, 3875, 766, 264, 15222, 11, 5252, 45, 14642, 13, 51132], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 291, "seek": 107108, "start": 1090.72, "end": 1093.1599999999999, "text": " Yeah, so there are all kinds of different,", "tokens": [51346, 865, 11, 370, 456, 366, 439, 3685, 295, 819, 11, 51468], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 292, "seek": 107108, "start": 1093.1599999999999, "end": 1096.96, "text": " I don't know if you would like product quantization and things like that.", "tokens": [51468, 286, 500, 380, 458, 498, 291, 576, 411, 1674, 4426, 2144, 293, 721, 411, 300, 13, 51658], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 293, "seek": 107108, "start": 1096.96, "end": 1100.3999999999999, "text": " So you have a bunch of vectors.", "tokens": [51658, 407, 291, 362, 257, 3840, 295, 18875, 13, 51830], "temperature": 0.0, "avg_logprob": -0.34805829708392805, "compression_ratio": 1.4508928571428572, "no_speech_prob": 0.0003567796666175127}, {"id": 294, "seek": 110040, "start": 1100.4, "end": 1104.92, "text": " And you can just compute the full dot product, which is sort of inefficient, right?", "tokens": [50364, 400, 291, 393, 445, 14722, 264, 1577, 5893, 1674, 11, 597, 307, 1333, 295, 43495, 11, 558, 30, 50590], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 295, "seek": 110040, "start": 1104.92, "end": 1109.4, "text": " So what you can do is try to compress subspaces of the vector", "tokens": [50590, 407, 437, 291, 393, 360, 307, 853, 281, 14778, 2090, 79, 2116, 295, 264, 8062, 50814], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 296, "seek": 110040, "start": 1109.4, "end": 1113.1200000000001, "text": " and then just look at the kind of centroids.", "tokens": [50814, 293, 550, 445, 574, 412, 264, 733, 295, 24607, 3742, 13, 51000], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 297, "seek": 110040, "start": 1113.1200000000001, "end": 1116.5600000000002, "text": " So you can quantize sub-vectors of the full vector", "tokens": [51000, 407, 291, 393, 4426, 1125, 1422, 12, 303, 5547, 295, 264, 1577, 8062, 51172], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 298, "seek": 110040, "start": 1116.5600000000002, "end": 1122.2800000000002, "text": " and then do much faster search over just the centroids.", "tokens": [51172, 293, 550, 360, 709, 4663, 3164, 670, 445, 264, 24607, 3742, 13, 51458], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 299, "seek": 110040, "start": 1122.2800000000002, "end": 1123.0, "text": " It's a good question.", "tokens": [51458, 467, 311, 257, 665, 1168, 13, 51494], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 300, "seek": 110040, "start": 1123.0, "end": 1123.8000000000002, "text": " Any other questions?", "tokens": [51494, 2639, 661, 1651, 30, 51534], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 301, "seek": 110040, "start": 1127.5600000000002, "end": 1128.48, "text": " All right.", "tokens": [51722, 1057, 558, 13, 51768], "temperature": 0.0, "avg_logprob": -0.21033132553100586, "compression_ratio": 1.6714285714285715, "no_speech_prob": 0.00021988601656630635}, {"id": 302, "seek": 112848, "start": 1128.48, "end": 1130.64, "text": " So about this dot product idea, right?", "tokens": [50364, 407, 466, 341, 5893, 1674, 1558, 11, 558, 30, 50472], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 303, "seek": 112848, "start": 1130.64, "end": 1135.92, "text": " So what we have here is some people call this a Siamese network, I guess it is, right?", "tokens": [50472, 407, 437, 321, 362, 510, 307, 512, 561, 818, 341, 257, 318, 2918, 1130, 3209, 11, 286, 2041, 309, 307, 11, 558, 30, 50736], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 304, "seek": 112848, "start": 1135.92, "end": 1140.72, "text": " So you have two different BERT models or whatever your encoder is here.", "tokens": [50736, 407, 291, 362, 732, 819, 363, 31479, 5245, 420, 2035, 428, 2058, 19866, 307, 510, 13, 50976], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 305, "seek": 112848, "start": 1140.72, "end": 1142.48, "text": " And then at the end, you get these two vectors", "tokens": [50976, 400, 550, 412, 264, 917, 11, 291, 483, 613, 732, 18875, 51064], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 306, "seek": 112848, "start": 1142.48, "end": 1145.76, "text": " and then you just do dot products where you get one single score.", "tokens": [51064, 293, 550, 291, 445, 360, 5893, 3383, 689, 291, 483, 472, 2167, 6175, 13, 51228], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 307, "seek": 112848, "start": 1145.76, "end": 1148.08, "text": " But you can do all kinds of much fancier things", "tokens": [51228, 583, 291, 393, 360, 439, 3685, 295, 709, 3429, 27674, 721, 51344], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 308, "seek": 112848, "start": 1148.08, "end": 1153.3600000000001, "text": " if you're willing to give up on this buy encoder approach.", "tokens": [51344, 498, 291, 434, 4950, 281, 976, 493, 322, 341, 2256, 2058, 19866, 3109, 13, 51608], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 309, "seek": 112848, "start": 1153.3600000000001, "end": 1158.44, "text": " So a really nice example from one of your colleagues here at Stanford is Colbert.", "tokens": [51608, 407, 257, 534, 1481, 1365, 490, 472, 295, 428, 7734, 510, 412, 20374, 307, 4004, 4290, 13, 51862], "temperature": 0.0, "avg_logprob": -0.19347433219278665, "compression_ratio": 1.6688963210702341, "no_speech_prob": 0.00039184477645903826}, {"id": 310, "seek": 115844, "start": 1159.4, "end": 1162.88, "text": " So what this does is late interaction.", "tokens": [50412, 407, 437, 341, 775, 307, 3469, 9285, 13, 50586], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 311, "seek": 115844, "start": 1162.88, "end": 1165.64, "text": " So instead of just having this dot product here,", "tokens": [50586, 407, 2602, 295, 445, 1419, 341, 5893, 1674, 510, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 312, "seek": 115844, "start": 1165.64, "end": 1170.56, "text": " you have a kind of more complicated version of computing score", "tokens": [50724, 291, 362, 257, 733, 295, 544, 6179, 3037, 295, 15866, 6175, 50970], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 313, "seek": 115844, "start": 1170.56, "end": 1175.0800000000002, "text": " where you aggregate over sort of maximum similarity scores between different words.", "tokens": [50970, 689, 291, 26118, 670, 1333, 295, 6674, 32194, 13444, 1296, 819, 2283, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 314, "seek": 115844, "start": 1175.0800000000002, "end": 1177.92, "text": " So I only recently actually discovered that this is called Colbert", "tokens": [51196, 407, 286, 787, 3938, 767, 6941, 300, 341, 307, 1219, 4004, 4290, 51338], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 315, "seek": 115844, "start": 1177.92, "end": 1180.56, "text": " because of the late night show Colbert.", "tokens": [51338, 570, 295, 264, 3469, 1818, 855, 4004, 4290, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 316, "seek": 115844, "start": 1180.56, "end": 1186.52, "text": " So it's sort of Omar's joke actually, this name, but just so you know, if you run into it.", "tokens": [51470, 407, 309, 311, 1333, 295, 33784, 311, 7647, 767, 11, 341, 1315, 11, 457, 445, 370, 291, 458, 11, 498, 291, 1190, 666, 309, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1768990303706197, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0002131016372004524}, {"id": 317, "seek": 118844, "start": 1188.92, "end": 1195.24, "text": " So, but I think if we look at kind of where the state of the art has been going now,", "tokens": [50388, 407, 11, 457, 286, 519, 498, 321, 574, 412, 733, 295, 689, 264, 1785, 295, 264, 1523, 575, 668, 516, 586, 11, 50704], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 318, "seek": 118844, "start": 1195.24, "end": 1198.92, "text": " one of the nice things about inspector databases is that they're super efficient, right?", "tokens": [50704, 472, 295, 264, 1481, 721, 466, 34564, 22380, 307, 300, 436, 434, 1687, 7148, 11, 558, 30, 50888], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 319, "seek": 118844, "start": 1198.92, "end": 1202.1200000000001, "text": " So dot product is much more efficient than this late interaction stuff,", "tokens": [50888, 407, 5893, 1674, 307, 709, 544, 7148, 813, 341, 3469, 9285, 1507, 11, 51048], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 320, "seek": 118844, "start": 1202.1200000000001, "end": 1204.76, "text": " especially if you do the approximate nearest neighbor search.", "tokens": [51048, 2318, 498, 291, 360, 264, 30874, 23831, 5987, 3164, 13, 51180], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 321, "seek": 118844, "start": 1205.88, "end": 1207.88, "text": " But there's been some really cool work.", "tokens": [51236, 583, 456, 311, 668, 512, 534, 1627, 589, 13, 51336], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 322, "seek": 118844, "start": 1207.88, "end": 1214.44, "text": " So things like SPLATE, they basically have sparse meat dents in a way.", "tokens": [51336, 407, 721, 411, 8420, 43, 20047, 11, 436, 1936, 362, 637, 11668, 4615, 274, 791, 294, 257, 636, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 323, "seek": 118844, "start": 1214.44, "end": 1215.96, "text": " So one of the big problems, as I said,", "tokens": [51664, 407, 472, 295, 264, 955, 2740, 11, 382, 286, 848, 11, 51740], "temperature": 0.0, "avg_logprob": -0.15773161863669372, "compression_ratio": 1.6035087719298247, "no_speech_prob": 0.00035673376987688243}, {"id": 324, "seek": 121596, "start": 1216.04, "end": 1219.32, "text": " with sparse is that you can't really handle synonyms and things like that.", "tokens": [50368, 365, 637, 11668, 307, 300, 291, 393, 380, 534, 4813, 5451, 2526, 2592, 293, 721, 411, 300, 13, 50532], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 325, "seek": 121596, "start": 1219.32, "end": 1223.32, "text": " But what you could do is take a dense model, like a bird model,", "tokens": [50532, 583, 437, 291, 727, 360, 307, 747, 257, 18011, 2316, 11, 411, 257, 5255, 2316, 11, 50732], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 326, "seek": 121596, "start": 1223.32, "end": 1226.6000000000001, "text": " look at kind of this one word in your sequence,", "tokens": [50732, 574, 412, 733, 295, 341, 472, 1349, 294, 428, 8310, 11, 50896], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 327, "seek": 121596, "start": 1226.6000000000001, "end": 1229.4, "text": " try to see which other words fit in the same slot.", "tokens": [50896, 853, 281, 536, 597, 661, 2283, 3318, 294, 264, 912, 14747, 13, 51036], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 328, "seek": 121596, "start": 1229.4, "end": 1230.8400000000001, "text": " So that gives you the synonyms.", "tokens": [51036, 407, 300, 2709, 291, 264, 5451, 2526, 2592, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 329, "seek": 121596, "start": 1231.64, "end": 1235.88, "text": " So now you can give all these synonyms to a sparse vector,", "tokens": [51148, 407, 586, 291, 393, 976, 439, 613, 5451, 2526, 2592, 281, 257, 637, 11668, 8062, 11, 51360], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 330, "seek": 121596, "start": 1235.88, "end": 1237.96, "text": " and then you can just do sparse dot product.", "tokens": [51360, 293, 550, 291, 393, 445, 360, 637, 11668, 5893, 1674, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 331, "seek": 121596, "start": 1237.96, "end": 1240.76, "text": " And so have a much, much more efficient way to do search", "tokens": [51464, 400, 370, 362, 257, 709, 11, 709, 544, 7148, 636, 281, 360, 3164, 51604], "temperature": 0.0, "avg_logprob": -0.12311321689236548, "compression_ratio": 1.7479674796747968, "no_speech_prob": 0.005727983079850674}, {"id": 332, "seek": 124076, "start": 1241.72, "end": 1247.08, "text": " without sort of giving up on all the cool stuff that you get from a dense representation.", "tokens": [50412, 1553, 1333, 295, 2902, 493, 322, 439, 264, 1627, 1507, 300, 291, 483, 490, 257, 18011, 10290, 13, 50680], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 333, "seek": 124076, "start": 1247.96, "end": 1249.08, "text": " So that's one thing.", "tokens": [50724, 407, 300, 311, 472, 551, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 334, "seek": 124076, "start": 1249.08, "end": 1252.36, "text": " And this other idea I really like is called dragon.", "tokens": [50780, 400, 341, 661, 1558, 286, 534, 411, 307, 1219, 12165, 13, 50944], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 335, "seek": 124076, "start": 1253.64, "end": 1257.72, "text": " So this side, I think, is really the best generalized dense retriever.", "tokens": [51008, 407, 341, 1252, 11, 286, 519, 11, 307, 534, 264, 1151, 44498, 18011, 19817, 331, 13, 51212], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 336, "seek": 124076, "start": 1257.72, "end": 1259.64, "text": " So if you want to take something off the shelf right now", "tokens": [51212, 407, 498, 291, 528, 281, 747, 746, 766, 264, 15222, 558, 586, 51308], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 337, "seek": 124076, "start": 1259.64, "end": 1261.56, "text": " and just go to hugging face or something,", "tokens": [51308, 293, 445, 352, 281, 41706, 1851, 420, 746, 11, 51404], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 338, "seek": 124076, "start": 1261.56, "end": 1266.44, "text": " then this dragon or dragon plus is probably the thing you want to use for a dense retriever.", "tokens": [51404, 550, 341, 12165, 420, 12165, 1804, 307, 1391, 264, 551, 291, 528, 281, 764, 337, 257, 18011, 19817, 331, 13, 51648], "temperature": 0.0, "avg_logprob": -0.12453458044264051, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.0012841678690165281}, {"id": 339, "seek": 126644, "start": 1266.44, "end": 1271.4, "text": " And the way they train this is through this progressive data augmentation strategy", "tokens": [50364, 400, 264, 636, 436, 3847, 341, 307, 807, 341, 16131, 1412, 14501, 19631, 5206, 50612], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 340, "seek": 126644, "start": 1271.4, "end": 1273.48, "text": " to make the model better and better over time", "tokens": [50612, 281, 652, 264, 2316, 1101, 293, 1101, 670, 565, 50716], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 341, "seek": 126644, "start": 1273.48, "end": 1275.3200000000002, "text": " by sampling very difficult negatives.", "tokens": [50716, 538, 21179, 588, 2252, 40019, 13, 50808], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 342, "seek": 126644, "start": 1276.3600000000001, "end": 1279.48, "text": " And that gives you very good representations.", "tokens": [50860, 400, 300, 2709, 291, 588, 665, 33358, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 343, "seek": 126644, "start": 1280.8400000000001, "end": 1282.52, "text": " And so the other thing about this,", "tokens": [51084, 400, 370, 264, 661, 551, 466, 341, 11, 51168], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 344, "seek": 126644, "start": 1282.52, "end": 1287.0800000000002, "text": " I think this is the only sort of final point about retrieval in general,", "tokens": [51168, 286, 519, 341, 307, 264, 787, 1333, 295, 2572, 935, 466, 19817, 3337, 294, 2674, 11, 51396], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 345, "seek": 126644, "start": 1287.0800000000002, "end": 1289.4, "text": " is that what we see happening right now,", "tokens": [51396, 307, 300, 437, 321, 536, 2737, 558, 586, 11, 51512], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 346, "seek": 126644, "start": 1289.4, "end": 1292.04, "text": " if you look at sort of the developer community around drag,", "tokens": [51512, 498, 291, 574, 412, 1333, 295, 264, 10754, 1768, 926, 5286, 11, 51644], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 347, "seek": 126644, "start": 1292.04, "end": 1294.76, "text": " is that they're all doing hybrid search right now.", "tokens": [51644, 307, 300, 436, 434, 439, 884, 13051, 3164, 558, 586, 13, 51780], "temperature": 0.0, "avg_logprob": -0.10186777705639864, "compression_ratio": 1.7416974169741697, "no_speech_prob": 0.0006665705586783588}, {"id": 348, "seek": 129476, "start": 1294.84, "end": 1298.6, "text": " So you can actually just combine the search results from your sparse,", "tokens": [50368, 407, 291, 393, 767, 445, 10432, 264, 3164, 3542, 490, 428, 637, 11668, 11, 50556], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 349, "seek": 129476, "start": 1298.6, "end": 1301.16, "text": " be in 25 or whatever thing or displayed,", "tokens": [50556, 312, 294, 3552, 420, 2035, 551, 420, 16372, 11, 50684], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 350, "seek": 129476, "start": 1301.16, "end": 1303.0, "text": " and you can combine them with your dragon.", "tokens": [50684, 293, 291, 393, 10432, 552, 365, 428, 12165, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 351, "seek": 129476, "start": 1303.96, "end": 1307.08, "text": " And then you'll get this ranking that works even better.", "tokens": [50824, 400, 550, 291, 603, 483, 341, 17833, 300, 1985, 754, 1101, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 352, "seek": 129476, "start": 1307.08, "end": 1308.76, "text": " So then you kind of get best of both worlds,", "tokens": [50980, 407, 550, 291, 733, 295, 483, 1151, 295, 1293, 13401, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 353, "seek": 129476, "start": 1308.76, "end": 1311.8, "text": " but then you get all these questions about how do you combine the results.", "tokens": [51064, 457, 550, 291, 483, 439, 613, 1651, 466, 577, 360, 291, 10432, 264, 3542, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 354, "seek": 129476, "start": 1313.96, "end": 1315.56, "text": " Any questions on this part?", "tokens": [51324, 2639, 1651, 322, 341, 644, 30, 51404], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 355, "seek": 129476, "start": 1317.16, "end": 1318.12, "text": " Oh, can you hear me?", "tokens": [51484, 876, 11, 393, 291, 1568, 385, 30, 51532], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 356, "seek": 129476, "start": 1318.92, "end": 1319.16, "text": " Yes.", "tokens": [51572, 1079, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 357, "seek": 129476, "start": 1319.8799999999999, "end": 1320.36, "text": " Oh, sorry.", "tokens": [51620, 876, 11, 2597, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1511285111710832, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.0006357048405334353}, {"id": 358, "seek": 132036, "start": 1320.9199999999998, "end": 1325.4799999999998, "text": " On the earlier slide, has there been any work on benchmark,", "tokens": [50392, 1282, 264, 3071, 4137, 11, 575, 456, 668, 604, 589, 322, 18927, 11, 50620], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 359, "seek": 132036, "start": 1325.4799999999998, "end": 1330.9199999999998, "text": " how much less hallucination rag incurs over closed book question answering,", "tokens": [50620, 577, 709, 1570, 35212, 2486, 17539, 834, 2156, 670, 5395, 1446, 1168, 13430, 11, 50892], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 360, "seek": 132036, "start": 1330.9199999999998, "end": 1334.28, "text": " for example, directly asking the large language model the question,", "tokens": [50892, 337, 1365, 11, 3838, 3365, 264, 2416, 2856, 2316, 264, 1168, 11, 51060], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 361, "seek": 132036, "start": 1334.28, "end": 1336.36, "text": " has there been any benchmarking studies in this?", "tokens": [51060, 575, 456, 668, 604, 18927, 278, 5313, 294, 341, 30, 51164], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 362, "seek": 132036, "start": 1337.3999999999999, "end": 1340.6799999999998, "text": " Yeah, so there's a great paper, if I can say so myself,", "tokens": [51216, 865, 11, 370, 456, 311, 257, 869, 3035, 11, 498, 286, 393, 584, 370, 2059, 11, 51380], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 363, "seek": 132036, "start": 1340.6799999999998, "end": 1343.6399999999999, "text": " on the fact that retrieval augmentation reduces hallucination.", "tokens": [51380, 322, 264, 1186, 300, 19817, 3337, 14501, 19631, 18081, 35212, 2486, 13, 51528], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 364, "seek": 132036, "start": 1344.36, "end": 1345.8799999999999, "text": " It's from 2021, I think.", "tokens": [51564, 467, 311, 490, 7201, 11, 286, 519, 13, 51640], "temperature": 0.0, "avg_logprob": -0.15799543173006264, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.000791190832387656}, {"id": 365, "seek": 134588, "start": 1346.8400000000001, "end": 1350.0400000000002, "text": " So yeah, you can just find, if you literally look for", "tokens": [50412, 407, 1338, 11, 291, 393, 445, 915, 11, 498, 291, 3736, 574, 337, 50572], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 366, "seek": 134588, "start": 1350.0400000000002, "end": 1353.0800000000002, "text": " retrieval augmentation reduces hallucination, then you'll find the paper.", "tokens": [50572, 19817, 3337, 14501, 19631, 18081, 35212, 2486, 11, 550, 291, 603, 915, 264, 3035, 13, 50724], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 367, "seek": 134588, "start": 1353.96, "end": 1354.5200000000002, "text": " Oh, thank you.", "tokens": [50768, 876, 11, 1309, 291, 13, 50796], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 368, "seek": 134588, "start": 1357.88, "end": 1362.0400000000002, "text": " Well, let's see, is there a picture of your dance approach,", "tokens": [50964, 1042, 11, 718, 311, 536, 11, 307, 456, 257, 3036, 295, 428, 4489, 3109, 11, 51172], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 369, "seek": 134588, "start": 1362.0400000000002, "end": 1363.64, "text": " and why do you need swaps?", "tokens": [51172, 293, 983, 360, 291, 643, 1693, 2382, 30, 51252], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 370, "seek": 134588, "start": 1364.2, "end": 1371.3200000000002, "text": " Yeah, so very often you want to have a very precise word overlap", "tokens": [51280, 865, 11, 370, 588, 2049, 291, 528, 281, 362, 257, 588, 13600, 1349, 19959, 51636], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 371, "seek": 134588, "start": 1371.3200000000002, "end": 1373.72, "text": " for things where you don't want to have the synonyms", "tokens": [51636, 337, 721, 689, 291, 500, 380, 528, 281, 362, 264, 5451, 2526, 2592, 51756], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 372, "seek": 134588, "start": 1373.72, "end": 1375.24, "text": " or the kind of nearest neighbors.", "tokens": [51756, 420, 264, 733, 295, 23831, 12512, 13, 51832], "temperature": 0.0, "avg_logprob": -0.26590278413560653, "compression_ratio": 1.5875, "no_speech_prob": 0.0014315518783405423}, {"id": 373, "seek": 137524, "start": 1375.24, "end": 1379.48, "text": " So if there's like a brand name or something like that,", "tokens": [50364, 407, 498, 456, 311, 411, 257, 3360, 1315, 420, 746, 411, 300, 11, 50576], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 374, "seek": 137524, "start": 1380.04, "end": 1382.52, "text": " then like let's say your brand is Apple,", "tokens": [50604, 550, 411, 718, 311, 584, 428, 3360, 307, 6373, 11, 50728], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 375, "seek": 137524, "start": 1382.52, "end": 1384.36, "text": " you don't want to find stuff about Paris.", "tokens": [50728, 291, 500, 380, 528, 281, 915, 1507, 466, 8380, 13, 50820], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 376, "seek": 137524, "start": 1385.08, "end": 1387.32, "text": " So that's what you would do with a dance retriever.", "tokens": [50856, 407, 300, 311, 437, 291, 576, 360, 365, 257, 4489, 19817, 331, 13, 50968], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 377, "seek": 137524, "start": 1388.36, "end": 1392.04, "text": " So it really kind of depends on what you want to use it for.", "tokens": [51020, 407, 309, 534, 733, 295, 5946, 322, 437, 291, 528, 281, 764, 309, 337, 13, 51204], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 378, "seek": 137524, "start": 1392.04, "end": 1393.88, "text": " That's why hybrid is probably the way to go.", "tokens": [51204, 663, 311, 983, 13051, 307, 1391, 264, 636, 281, 352, 13, 51296], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 379, "seek": 137524, "start": 1395.72, "end": 1396.44, "text": " It's a good question.", "tokens": [51388, 467, 311, 257, 665, 1168, 13, 51424], "temperature": 0.0, "avg_logprob": -0.16659702138697846, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.000368128705304116}, {"id": 380, "seek": 139644, "start": 1397.4, "end": 1404.04, "text": " But with a dance, it's contextualized in many ways.", "tokens": [50412, 583, 365, 257, 4489, 11, 309, 311, 35526, 1602, 294, 867, 2098, 13, 50744], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 381, "seek": 139644, "start": 1404.04, "end": 1408.04, "text": " Should it realize Apple, the company, would be different from that?", "tokens": [50744, 6454, 309, 4325, 6373, 11, 264, 2237, 11, 576, 312, 819, 490, 300, 30, 50944], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 382, "seek": 139644, "start": 1408.04, "end": 1408.6000000000001, "text": " No.", "tokens": [50944, 883, 13, 50972], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 383, "seek": 139644, "start": 1408.6000000000001, "end": 1411.4, "text": " So if they were actually contextualized, then yes,", "tokens": [50972, 407, 498, 436, 645, 767, 35526, 1602, 11, 550, 2086, 11, 51112], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 384, "seek": 139644, "start": 1411.4, "end": 1414.2, "text": " but very often it's a frozen retrieval system.", "tokens": [51112, 457, 588, 2049, 309, 311, 257, 12496, 19817, 3337, 1185, 13, 51252], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 385, "seek": 139644, "start": 1415.0, "end": 1417.64, "text": " That's one of the problems with all the frozen rag stuff.", "tokens": [51292, 663, 311, 472, 295, 264, 2740, 365, 439, 264, 12496, 17539, 1507, 13, 51424], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 386, "seek": 139644, "start": 1422.04, "end": 1423.64, "text": " I might be missing something very, very soon.", "tokens": [51644, 286, 1062, 312, 5361, 746, 588, 11, 588, 2321, 13, 51724], "temperature": 0.0, "avg_logprob": -0.39567027621799045, "compression_ratio": 1.5476190476190477, "no_speech_prob": 0.010645323432981968}, {"id": 387, "seek": 142364, "start": 1423.64, "end": 1441.16, "text": " So the sort of document and the query, they're the same, right?", "tokens": [50364, 407, 264, 1333, 295, 4166, 293, 264, 14581, 11, 436, 434, 264, 912, 11, 558, 30, 51240], "temperature": 0.0, "avg_logprob": -0.2656292063849313, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0038223795127123594}, {"id": 388, "seek": 142364, "start": 1441.16, "end": 1443.3200000000002, "text": " So they're either sparse or they're dense.", "tokens": [51240, 407, 436, 434, 2139, 637, 11668, 420, 436, 434, 18011, 13, 51348], "temperature": 0.0, "avg_logprob": -0.2656292063849313, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0038223795127123594}, {"id": 389, "seek": 142364, "start": 1443.3200000000002, "end": 1444.8400000000001, "text": " But so if they're sparse,", "tokens": [51348, 583, 370, 498, 436, 434, 637, 11668, 11, 51424], "temperature": 0.0, "avg_logprob": -0.2656292063849313, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0038223795127123594}, {"id": 390, "seek": 142364, "start": 1444.8400000000001, "end": 1447.8000000000002, "text": " the components of the vector are literally the other words.", "tokens": [51424, 264, 6677, 295, 264, 8062, 366, 3736, 264, 661, 2283, 13, 51572], "temperature": 0.0, "avg_logprob": -0.2656292063849313, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0038223795127123594}, {"id": 391, "seek": 144780, "start": 1447.8, "end": 1455.8799999999999, "text": " And you just finalized when you're thinking about the thing that creates the names?", "tokens": [50364, 400, 291, 445, 2572, 1602, 562, 291, 434, 1953, 466, 264, 551, 300, 7829, 264, 5288, 30, 50768], "temperature": 0.0, "avg_logprob": -0.31163949966430665, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.01852107234299183}, {"id": 392, "seek": 144780, "start": 1457.8, "end": 1459.6399999999999, "text": " How are you getting expressed here?", "tokens": [50864, 1012, 366, 291, 1242, 12675, 510, 30, 50956], "temperature": 0.0, "avg_logprob": -0.31163949966430665, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.01852107234299183}, {"id": 393, "seek": 144780, "start": 1459.6399999999999, "end": 1461.1599999999999, "text": " So it literally counts, right?", "tokens": [50956, 407, 309, 3736, 14893, 11, 558, 30, 51032], "temperature": 0.0, "avg_logprob": -0.31163949966430665, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.01852107234299183}, {"id": 394, "seek": 144780, "start": 1462.04, "end": 1466.68, "text": " So basically it's one big matrix of documents as rows", "tokens": [51076, 407, 1936, 309, 311, 472, 955, 8141, 295, 8512, 382, 13241, 51308], "temperature": 0.0, "avg_logprob": -0.31163949966430665, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.01852107234299183}, {"id": 395, "seek": 144780, "start": 1466.68, "end": 1469.24, "text": " and the columns are the words in the documents.", "tokens": [51308, 293, 264, 13766, 366, 264, 2283, 294, 264, 8512, 13, 51436], "temperature": 0.0, "avg_logprob": -0.31163949966430665, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.01852107234299183}, {"id": 396, "seek": 144780, "start": 1469.24, "end": 1472.28, "text": " And then you just count how often a word occurs in a document.", "tokens": [51436, 400, 550, 291, 445, 1207, 577, 2049, 257, 1349, 11843, 294, 257, 4166, 13, 51588], "temperature": 0.0, "avg_logprob": -0.31163949966430665, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.01852107234299183}, {"id": 397, "seek": 147228, "start": 1473.0, "end": 1474.2, "text": " So that's a sparse.", "tokens": [50400, 407, 300, 311, 257, 637, 11668, 13, 50460], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 398, "seek": 147228, "start": 1480.52, "end": 1481.0, "text": " Yeah.", "tokens": [50776, 865, 13, 50800], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 399, "seek": 147228, "start": 1481.0, "end": 1486.44, "text": " And so in the field, we call them sparse embeddings or sparse retrieval", "tokens": [50800, 400, 370, 294, 264, 2519, 11, 321, 818, 552, 637, 11668, 12240, 29432, 420, 637, 11668, 19817, 3337, 51072], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 400, "seek": 147228, "start": 1486.44, "end": 1488.28, "text": " because most of that vector is zero.", "tokens": [51072, 570, 881, 295, 300, 8062, 307, 4018, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 401, "seek": 147228, "start": 1489.56, "end": 1491.8799999999999, "text": " Because most words don't occur in that document.", "tokens": [51228, 1436, 881, 2283, 500, 380, 5160, 294, 300, 4166, 13, 51344], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 402, "seek": 147228, "start": 1494.04, "end": 1495.0, "text": " Does that make sense?", "tokens": [51452, 4402, 300, 652, 2020, 30, 51500], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 403, "seek": 147228, "start": 1498.6, "end": 1498.84, "text": " Cool.", "tokens": [51680, 8561, 13, 51692], "temperature": 0.0, "avg_logprob": -0.2282551176407758, "compression_ratio": 1.4256756756756757, "no_speech_prob": 0.0017002468230202794}, {"id": 404, "seek": 149884, "start": 1499.8, "end": 1504.9199999999998, "text": " So let's talk about doing slightly better.", "tokens": [50412, 407, 718, 311, 751, 466, 884, 4748, 1101, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14554143774098363, "compression_ratio": 1.6539923954372624, "no_speech_prob": 0.0007205638103187084}, {"id": 405, "seek": 149884, "start": 1504.9199999999998, "end": 1508.84, "text": " So going back to Steven's question about, okay, we have this kind of retrieval thing,", "tokens": [50668, 407, 516, 646, 281, 12754, 311, 1168, 466, 11, 1392, 11, 321, 362, 341, 733, 295, 19817, 3337, 551, 11, 50864], "temperature": 0.0, "avg_logprob": -0.14554143774098363, "compression_ratio": 1.6539923954372624, "no_speech_prob": 0.0007205638103187084}, {"id": 406, "seek": 149884, "start": 1508.84, "end": 1514.36, "text": " but how do we actually make this retriever good for the context that is going to be used in?", "tokens": [50864, 457, 577, 360, 321, 767, 652, 341, 19817, 331, 665, 337, 264, 4319, 300, 307, 516, 281, 312, 1143, 294, 30, 51140], "temperature": 0.0, "avg_logprob": -0.14554143774098363, "compression_ratio": 1.6539923954372624, "no_speech_prob": 0.0007205638103187084}, {"id": 407, "seek": 149884, "start": 1514.36, "end": 1517.56, "text": " And so can we contextualize the retriever for the generator?", "tokens": [51140, 400, 370, 393, 321, 35526, 1125, 264, 19817, 331, 337, 264, 19265, 30, 51300], "temperature": 0.0, "avg_logprob": -0.14554143774098363, "compression_ratio": 1.6539923954372624, "no_speech_prob": 0.0007205638103187084}, {"id": 408, "seek": 149884, "start": 1518.28, "end": 1522.12, "text": " Even if it's a generator where we might not have access to the weights.", "tokens": [51336, 2754, 498, 309, 311, 257, 19265, 689, 321, 1062, 406, 362, 2105, 281, 264, 17443, 13, 51528], "temperature": 0.0, "avg_logprob": -0.14554143774098363, "compression_ratio": 1.6539923954372624, "no_speech_prob": 0.0007205638103187084}, {"id": 409, "seek": 149884, "start": 1522.12, "end": 1526.52, "text": " So it could be a GP4 model, we just send it to some API, we get some stuff back.", "tokens": [51528, 407, 309, 727, 312, 257, 26039, 19, 2316, 11, 321, 445, 2845, 309, 281, 512, 9362, 11, 321, 483, 512, 1507, 646, 13, 51748], "temperature": 0.0, "avg_logprob": -0.14554143774098363, "compression_ratio": 1.6539923954372624, "no_speech_prob": 0.0007205638103187084}, {"id": 410, "seek": 152652, "start": 1527.48, "end": 1531.24, "text": " And so one paper I would like is called Replug.", "tokens": [50412, 400, 370, 472, 3035, 286, 576, 411, 307, 1219, 1300, 564, 697, 13, 50600], "temperature": 0.0, "avg_logprob": -0.21070765423518356, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.0005791287403553724}, {"id": 411, "seek": 152652, "start": 1532.36, "end": 1534.92, "text": " So just to kind of explain what this looks like.", "tokens": [50656, 407, 445, 281, 733, 295, 2903, 437, 341, 1542, 411, 13, 50784], "temperature": 0.0, "avg_logprob": -0.21070765423518356, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.0005791287403553724}, {"id": 412, "seek": 152652, "start": 1534.92, "end": 1539.72, "text": " So you have this context, you have a retriever that we do the standard retrieval step with.", "tokens": [50784, 407, 291, 362, 341, 4319, 11, 291, 362, 257, 19817, 331, 300, 321, 360, 264, 3832, 19817, 3337, 1823, 365, 13, 51024], "temperature": 0.0, "avg_logprob": -0.21070765423518356, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.0005791287403553724}, {"id": 413, "seek": 152652, "start": 1539.72, "end": 1540.92, "text": " This is a dense retriever.", "tokens": [51024, 639, 307, 257, 18011, 19817, 331, 13, 51084], "temperature": 0.0, "avg_logprob": -0.21070765423518356, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.0005791287403553724}, {"id": 414, "seek": 152652, "start": 1541.96, "end": 1546.84, "text": " And now, sorry, and now you compute the likelihood.", "tokens": [51136, 400, 586, 11, 2597, 11, 293, 586, 291, 14722, 264, 22119, 13, 51380], "temperature": 0.0, "avg_logprob": -0.21070765423518356, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.0005791287403553724}, {"id": 415, "seek": 152652, "start": 1546.84, "end": 1551.32, "text": " So basically just normalize the scores that you get for the top K documents", "tokens": [51380, 407, 1936, 445, 2710, 1125, 264, 13444, 300, 291, 483, 337, 264, 1192, 591, 8512, 51604], "temperature": 0.0, "avg_logprob": -0.21070765423518356, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.0005791287403553724}, {"id": 416, "seek": 155132, "start": 1551.32, "end": 1553.0, "text": " to get a distribution here.", "tokens": [50364, 281, 483, 257, 7316, 510, 13, 50448], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 417, "seek": 155132, "start": 1553.0, "end": 1559.08, "text": " And then you'll give each one of the retrieved documents separately to this generator,", "tokens": [50448, 400, 550, 291, 603, 976, 1184, 472, 295, 264, 19817, 937, 8512, 14759, 281, 341, 19265, 11, 50752], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 418, "seek": 155132, "start": 1559.08, "end": 1560.6, "text": " to your language model.", "tokens": [50752, 281, 428, 2856, 2316, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 419, "seek": 155132, "start": 1560.6, "end": 1565.32, "text": " So you can look at the perplexity of the correct answer for that language model.", "tokens": [50828, 407, 291, 393, 574, 412, 264, 680, 18945, 507, 295, 264, 3006, 1867, 337, 300, 2856, 2316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 420, "seek": 155132, "start": 1566.04, "end": 1570.28, "text": " So now we have these two probability distributions, or two likelihoods essentially,", "tokens": [51100, 407, 586, 321, 362, 613, 732, 8482, 37870, 11, 420, 732, 22119, 82, 4476, 11, 51312], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 421, "seek": 155132, "start": 1570.28, "end": 1573.8799999999999, "text": " and we can minimize the KL divergence to make sure that we can actually", "tokens": [51312, 293, 321, 393, 17522, 264, 47991, 47387, 281, 652, 988, 300, 321, 393, 767, 51492], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 422, "seek": 155132, "start": 1574.52, "end": 1580.2, "text": " retrieve the documents that lead to the lowest perplexity on the right answer for the language model.", "tokens": [51524, 30254, 264, 8512, 300, 1477, 281, 264, 12437, 680, 18945, 507, 322, 264, 558, 1867, 337, 264, 2856, 2316, 13, 51808], "temperature": 0.0, "avg_logprob": -0.10001049041748047, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.002843681490048766}, {"id": 423, "seek": 158132, "start": 1581.3999999999999, "end": 1584.9199999999998, "text": " So super simple idea, works really, really well.", "tokens": [50368, 407, 1687, 2199, 1558, 11, 1985, 534, 11, 534, 731, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 424, "seek": 158132, "start": 1586.36, "end": 1590.9199999999998, "text": " And the nice thing about this is completely agnostic of what happens upstream.", "tokens": [50616, 400, 264, 1481, 551, 466, 341, 307, 2584, 623, 77, 19634, 295, 437, 2314, 33915, 13, 50844], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 425, "seek": 158132, "start": 1590.9199999999998, "end": 1594.6, "text": " So this will work for any sort of encoder decoder for any language model.", "tokens": [50844, 407, 341, 486, 589, 337, 604, 1333, 295, 2058, 19866, 979, 19866, 337, 604, 2856, 2316, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 426, "seek": 158132, "start": 1595.72, "end": 1601.32, "text": " What you need is a perplexity score, but for most language models you can get that,", "tokens": [51084, 708, 291, 643, 307, 257, 680, 18945, 507, 6175, 11, 457, 337, 881, 2856, 5245, 291, 393, 483, 300, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 427, "seek": 158132, "start": 1601.32, "end": 1602.52, "text": " not necessarily all of them.", "tokens": [51364, 406, 4725, 439, 295, 552, 13, 51424], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 428, "seek": 158132, "start": 1603.08, "end": 1603.8799999999999, "text": " So that's one thing.", "tokens": [51452, 407, 300, 311, 472, 551, 13, 51492], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 429, "seek": 158132, "start": 1603.8799999999999, "end": 1606.28, "text": " And then there's this other really nice approach.", "tokens": [51492, 400, 550, 456, 311, 341, 661, 534, 1481, 3109, 13, 51612], "temperature": 0.0, "avg_logprob": -0.11905902974745806, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0008825349505059421}, {"id": 430, "seek": 160628, "start": 1607.0, "end": 1617.3999999999999, "text": " So in the retriever, you're literally updating the dense representations,", "tokens": [50400, 407, 294, 264, 19817, 331, 11, 291, 434, 3736, 25113, 264, 18011, 33358, 11, 50920], "temperature": 0.0, "avg_logprob": -0.23543381690979004, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0017262337496504188}, {"id": 431, "seek": 160628, "start": 1618.04, "end": 1618.28, "text": " right?", "tokens": [50952, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.23543381690979004, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0017262337496504188}, {"id": 432, "seek": 160628, "start": 1618.28, "end": 1620.84, "text": " So your encoder basically for your dense representation.", "tokens": [50964, 407, 428, 2058, 19866, 1936, 337, 428, 18011, 10290, 13, 51092], "temperature": 0.0, "avg_logprob": -0.23543381690979004, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0017262337496504188}, {"id": 433, "seek": 160628, "start": 1620.84, "end": 1622.92, "text": " That's a good question, we'll get into that a little bit more.", "tokens": [51092, 663, 311, 257, 665, 1168, 11, 321, 603, 483, 666, 300, 257, 707, 857, 544, 13, 51196], "temperature": 0.0, "avg_logprob": -0.23543381690979004, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0017262337496504188}, {"id": 434, "seek": 160628, "start": 1624.6, "end": 1629.8, "text": " So there's another paper on in-context retrieval augmented language models,", "tokens": [51280, 407, 456, 311, 1071, 3035, 322, 294, 12, 9000, 3828, 19817, 3337, 36155, 2856, 5245, 11, 51540], "temperature": 0.0, "avg_logprob": -0.23543381690979004, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0017262337496504188}, {"id": 435, "seek": 160628, "start": 1629.8, "end": 1635.3999999999999, "text": " where the whole paper is basically about just doing BM25 and just giving stuff directly", "tokens": [51540, 689, 264, 1379, 3035, 307, 1936, 466, 445, 884, 15901, 6074, 293, 445, 2902, 1507, 3838, 51820], "temperature": 0.0, "avg_logprob": -0.23543381690979004, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0017262337496504188}, {"id": 436, "seek": 163540, "start": 1635.4, "end": 1637.96, "text": " through the context of the language model and things kind of work.", "tokens": [50364, 807, 264, 4319, 295, 264, 2856, 2316, 293, 721, 733, 295, 589, 13, 50492], "temperature": 0.0, "avg_logprob": -0.12256558736165364, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0030252141878008842}, {"id": 437, "seek": 163540, "start": 1637.96, "end": 1644.2800000000002, "text": " So it's sort of frozen rag, but even more primitive in a way where the retriever is", "tokens": [50492, 407, 309, 311, 1333, 295, 12496, 17539, 11, 457, 754, 544, 28540, 294, 257, 636, 689, 264, 19817, 331, 307, 50808], "temperature": 0.0, "avg_logprob": -0.12256558736165364, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0030252141878008842}, {"id": 438, "seek": 163540, "start": 1644.92, "end": 1648.1200000000001, "text": " this very old sparse algorithm, but it works really, really well.", "tokens": [50840, 341, 588, 1331, 637, 11668, 9284, 11, 457, 309, 1985, 534, 11, 534, 731, 13, 51000], "temperature": 0.0, "avg_logprob": -0.12256558736165364, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0030252141878008842}, {"id": 439, "seek": 163540, "start": 1648.92, "end": 1653.16, "text": " But then they have this really awesome section where they show that you can just", "tokens": [51040, 583, 550, 436, 362, 341, 534, 3476, 3541, 689, 436, 855, 300, 291, 393, 445, 51252], "temperature": 0.0, "avg_logprob": -0.12256558736165364, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0030252141878008842}, {"id": 440, "seek": 163540, "start": 1653.16, "end": 1660.2, "text": " have this reranker on top of the BM25 results, and you can backdrop into this reranker.", "tokens": [51252, 362, 341, 43819, 657, 260, 322, 1192, 295, 264, 15901, 6074, 3542, 11, 293, 291, 393, 32697, 666, 341, 43819, 657, 260, 13, 51604], "temperature": 0.0, "avg_logprob": -0.12256558736165364, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0030252141878008842}, {"id": 441, "seek": 163540, "start": 1660.2, "end": 1663.16, "text": " So now you still keep the language model completely fixed.", "tokens": [51604, 407, 586, 291, 920, 1066, 264, 2856, 2316, 2584, 6806, 13, 51752], "temperature": 0.0, "avg_logprob": -0.12256558736165364, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0030252141878008842}, {"id": 442, "seek": 166316, "start": 1663.16, "end": 1666.68, "text": " So that's sort of this part of the loss here.", "tokens": [50364, 407, 300, 311, 1333, 295, 341, 644, 295, 264, 4470, 510, 13, 50540], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 443, "seek": 166316, "start": 1666.68, "end": 1669.3200000000002, "text": " So you have kind of a stop gradient on the parameters data.", "tokens": [50540, 407, 291, 362, 733, 295, 257, 1590, 16235, 322, 264, 9834, 1412, 13, 50672], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 444, "seek": 166316, "start": 1669.3200000000002, "end": 1670.68, "text": " That's just your language model.", "tokens": [50672, 663, 311, 445, 428, 2856, 2316, 13, 50740], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 445, "seek": 166316, "start": 1671.24, "end": 1676.92, "text": " But now you have this kind of rank function here that you can backdrop into, right?", "tokens": [50768, 583, 586, 291, 362, 341, 733, 295, 6181, 2445, 510, 300, 291, 393, 32697, 666, 11, 558, 30, 51052], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 446, "seek": 166316, "start": 1676.92, "end": 1678.2, "text": " So that's your reranker.", "tokens": [51052, 407, 300, 311, 428, 43819, 657, 260, 13, 51116], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 447, "seek": 166316, "start": 1678.2, "end": 1681.72, "text": " It's basically it can be a burden model or anything like that that works on top of the things", "tokens": [51116, 467, 311, 1936, 309, 393, 312, 257, 12578, 2316, 420, 1340, 411, 300, 300, 1985, 322, 1192, 295, 264, 721, 51292], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 448, "seek": 166316, "start": 1681.72, "end": 1684.0400000000002, "text": " you initially retrieved from your BM25.", "tokens": [51292, 291, 9105, 19817, 937, 490, 428, 15901, 6074, 13, 51408], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 449, "seek": 166316, "start": 1684.0400000000002, "end": 1687.24, "text": " And now you have this birth reranker that you can backdrop into.", "tokens": [51408, 400, 586, 291, 362, 341, 3965, 43819, 657, 260, 300, 291, 393, 32697, 666, 13, 51568], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 450, "seek": 166316, "start": 1688.76, "end": 1691.3200000000002, "text": " So this also works really, really nice.", "tokens": [51644, 407, 341, 611, 1985, 534, 11, 534, 1481, 13, 51772], "temperature": 0.0, "avg_logprob": -0.15982643884556894, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0005791872972622514}, {"id": 451, "seek": 169132, "start": 1691.32, "end": 1696.76, "text": " So we're slowly progressing towards having a system that is much more optimized for", "tokens": [50364, 407, 321, 434, 5692, 36305, 3030, 1419, 257, 1185, 300, 307, 709, 544, 26941, 337, 50636], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 452, "seek": 169132, "start": 1696.76, "end": 1701.96, "text": " being properly retrieval-augmented in a way where it's useful and contextualized for what", "tokens": [50636, 885, 6108, 19817, 3337, 12, 20056, 14684, 294, 257, 636, 689, 309, 311, 4420, 293, 35526, 1602, 337, 437, 50896], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 453, "seek": 169132, "start": 1701.96, "end": 1702.9199999999998, "text": " you want to use it for.", "tokens": [50896, 291, 528, 281, 764, 309, 337, 13, 50944], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 454, "seek": 169132, "start": 1704.84, "end": 1708.28, "text": " So yeah, just to point out kind of what that looks like with this reranker.", "tokens": [51040, 407, 1338, 11, 445, 281, 935, 484, 733, 295, 437, 300, 1542, 411, 365, 341, 43819, 657, 260, 13, 51212], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 455, "seek": 169132, "start": 1708.28, "end": 1710.84, "text": " So you just have this extra step essentially, right?", "tokens": [51212, 407, 291, 445, 362, 341, 2857, 1823, 4476, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 456, "seek": 169132, "start": 1710.84, "end": 1714.9199999999998, "text": " So we have our retriever, then we have our reranker, then we have our generator and our output.", "tokens": [51340, 407, 321, 362, 527, 19817, 331, 11, 550, 321, 362, 527, 43819, 657, 260, 11, 550, 321, 362, 527, 19265, 293, 527, 5598, 13, 51544], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 457, "seek": 169132, "start": 1716.6, "end": 1718.6799999999998, "text": " Any grades to the language model?", "tokens": [51628, 2639, 18041, 281, 264, 2856, 2316, 30, 51732], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 458, "seek": 169132, "start": 1719.8, "end": 1721.0, "text": " No, not necessarily.", "tokens": [51788, 883, 11, 406, 4725, 13, 51848], "temperature": 0.0, "avg_logprob": -0.18624076535624842, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.0003798202669713646}, {"id": 459, "seek": 172132, "start": 1722.28, "end": 1725.3999999999999, "text": " So for this one, you do, yeah.", "tokens": [50412, 407, 337, 341, 472, 11, 291, 360, 11, 1338, 13, 50568], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 460, "seek": 172132, "start": 1725.3999999999999, "end": 1727.1599999999999, "text": " But so for a redeplug, you don't, right?", "tokens": [50568, 583, 370, 337, 257, 14328, 564, 697, 11, 291, 500, 380, 11, 558, 30, 50656], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 461, "seek": 172132, "start": 1728.6799999999998, "end": 1729.56, "text": " Yeah, for this one.", "tokens": [50732, 865, 11, 337, 341, 472, 13, 50776], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 462, "seek": 172132, "start": 1729.56, "end": 1731.96, "text": " Yeah, yeah, yeah.", "tokens": [50776, 865, 11, 1338, 11, 1338, 13, 50896], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 463, "seek": 172132, "start": 1731.96, "end": 1733.8799999999999, "text": " So basically, yeah, you need to get-", "tokens": [50896, 407, 1936, 11, 1338, 11, 291, 643, 281, 483, 12, 50992], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 464, "seek": 172132, "start": 1733.8799999999999, "end": 1735.56, "text": " Do you guys provide them?", "tokens": [50992, 1144, 291, 1074, 2893, 552, 30, 51076], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 465, "seek": 172132, "start": 1735.56, "end": 1736.2, "text": " Not all of them.", "tokens": [51076, 1726, 439, 295, 552, 13, 51108], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 466, "seek": 172132, "start": 1737.1599999999999, "end": 1741.08, "text": " Some of them do, but yeah, there are all kinds of tricks you can do on top of that.", "tokens": [51156, 2188, 295, 552, 360, 11, 457, 1338, 11, 456, 366, 439, 3685, 295, 11733, 291, 393, 360, 322, 1192, 295, 300, 13, 51352], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 467, "seek": 172132, "start": 1743.96, "end": 1749.24, "text": " So basically, the question is how do we get sort of gradients flowing into this, right?", "tokens": [51496, 407, 1936, 11, 264, 1168, 307, 577, 360, 321, 483, 1333, 295, 2771, 2448, 13974, 666, 341, 11, 558, 30, 51760], "temperature": 0.0, "avg_logprob": -0.282006624366055, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.00041706874617375433}, {"id": 468, "seek": 174924, "start": 1749.24, "end": 1754.04, "text": " So if you don't actually have access to the full parameters of the model so that you can backprop", "tokens": [50364, 407, 498, 291, 500, 380, 767, 362, 2105, 281, 264, 1577, 9834, 295, 264, 2316, 370, 300, 291, 393, 646, 79, 1513, 50604], "temperature": 0.0, "avg_logprob": -0.1639375590314769, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.00026526019792072475}, {"id": 469, "seek": 174924, "start": 1754.04, "end": 1760.1200000000001, "text": " all the way through it, then you can do a reinforce style loss on the retrieval,", "tokens": [50604, 439, 264, 636, 807, 309, 11, 550, 291, 393, 360, 257, 22634, 3758, 4470, 322, 264, 19817, 3337, 11, 50908], "temperature": 0.0, "avg_logprob": -0.1639375590314769, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.00026526019792072475}, {"id": 470, "seek": 174924, "start": 1760.1200000000001, "end": 1764.36, "text": " and then you just pass the kind of log likelihood if you have access to that,", "tokens": [50908, 293, 550, 291, 445, 1320, 264, 733, 295, 3565, 22119, 498, 291, 362, 2105, 281, 300, 11, 51120], "temperature": 0.0, "avg_logprob": -0.1639375590314769, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.00026526019792072475}, {"id": 471, "seek": 174924, "start": 1764.36, "end": 1766.04, "text": " or some other kind of black box function.", "tokens": [51120, 420, 512, 661, 733, 295, 2211, 2424, 2445, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1639375590314769, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.00026526019792072475}, {"id": 472, "seek": 174924, "start": 1771.48, "end": 1779.08, "text": " All right, so the next thing you can do is to optimize both the retriever and the generator.", "tokens": [51476, 1057, 558, 11, 370, 264, 958, 551, 291, 393, 360, 307, 281, 19719, 1293, 264, 19817, 331, 293, 264, 19265, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1639375590314769, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.00026526019792072475}, {"id": 473, "seek": 177924, "start": 1780.2, "end": 1786.6, "text": " And so this really starts getting to the proper kind of contextualization of the entire architecture", "tokens": [50412, 400, 370, 341, 534, 3719, 1242, 281, 264, 2296, 733, 295, 35526, 2144, 295, 264, 2302, 9482, 50732], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 474, "seek": 177924, "start": 1786.6, "end": 1788.84, "text": " where you want everything to work together, right?", "tokens": [50732, 689, 291, 528, 1203, 281, 589, 1214, 11, 558, 30, 50844], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 475, "seek": 177924, "start": 1788.84, "end": 1793.24, "text": " So rather than having this frozen thing where everything is basically not aware that the other", "tokens": [50844, 407, 2831, 813, 1419, 341, 12496, 551, 689, 1203, 307, 1936, 406, 3650, 300, 264, 661, 51064], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 476, "seek": 177924, "start": 1793.24, "end": 1794.04, "text": " part exists, right?", "tokens": [51064, 644, 8198, 11, 558, 30, 51104], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 477, "seek": 177924, "start": 1794.04, "end": 1795.24, "text": " It's like two halves of the brain.", "tokens": [51104, 467, 311, 411, 732, 38490, 295, 264, 3567, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 478, "seek": 177924, "start": 1795.24, "end": 1796.76, "text": " They're not talking to each other.", "tokens": [51164, 814, 434, 406, 1417, 281, 1184, 661, 13, 51240], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 479, "seek": 177924, "start": 1796.76, "end": 1798.92, "text": " One is your retriever, the other is your language model.", "tokens": [51240, 1485, 307, 428, 19817, 331, 11, 264, 661, 307, 428, 2856, 2316, 13, 51348], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 480, "seek": 177924, "start": 1798.92, "end": 1799.72, "text": " There's no connection.", "tokens": [51348, 821, 311, 572, 4984, 13, 51388], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 481, "seek": 177924, "start": 1799.72, "end": 1803.4, "text": " They're just like sort of like something is thrown over the fence and then you hope for the best.", "tokens": [51388, 814, 434, 445, 411, 1333, 295, 411, 746, 307, 11732, 670, 264, 15422, 293, 550, 291, 1454, 337, 264, 1151, 13, 51572], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 482, "seek": 177924, "start": 1803.96, "end": 1807.16, "text": " So instead of that, we have everything much closer and learning together.", "tokens": [51600, 407, 2602, 295, 300, 11, 321, 362, 1203, 709, 4966, 293, 2539, 1214, 13, 51760], "temperature": 0.0, "avg_logprob": -0.10678116934640067, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.00035686427145265043}, {"id": 483, "seek": 180716, "start": 1808.1200000000001, "end": 1816.28, "text": " So one of the first ways of doing this with the generator was ragged retrieval augmented", "tokens": [50412, 407, 472, 295, 264, 700, 2098, 295, 884, 341, 365, 264, 19265, 390, 17539, 3004, 19817, 3337, 36155, 50820], "temperature": 0.0, "avg_logprob": -0.13292513953314888, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0003740174288395792}, {"id": 484, "seek": 180716, "start": 1816.28, "end": 1819.16, "text": " generation, which we did at fair in 2020.", "tokens": [50820, 5125, 11, 597, 321, 630, 412, 3143, 294, 4808, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13292513953314888, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0003740174288395792}, {"id": 485, "seek": 180716, "start": 1820.44, "end": 1823.72, "text": " And it's very similar to what we've already seen.", "tokens": [51028, 400, 309, 311, 588, 2531, 281, 437, 321, 600, 1217, 1612, 13, 51192], "temperature": 0.0, "avg_logprob": -0.13292513953314888, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0003740174288395792}, {"id": 486, "seek": 180716, "start": 1823.72, "end": 1827.3200000000002, "text": " We basically have this retriever here that works over different documents.", "tokens": [51192, 492, 1936, 362, 341, 19817, 331, 510, 300, 1985, 670, 819, 8512, 13, 51372], "temperature": 0.0, "avg_logprob": -0.13292513953314888, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0003740174288395792}, {"id": 487, "seek": 180716, "start": 1827.3200000000002, "end": 1833.5600000000002, "text": " You get some score function that gets given to this generator that generates the answer.", "tokens": [51372, 509, 483, 512, 6175, 2445, 300, 2170, 2212, 281, 341, 19265, 300, 23815, 264, 1867, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13292513953314888, "compression_ratio": 1.5779816513761469, "no_speech_prob": 0.0003740174288395792}, {"id": 488, "seek": 183356, "start": 1833.56, "end": 1838.04, "text": " And now you want to backdrop all the way and update your generator as well, right?", "tokens": [50364, 400, 586, 291, 528, 281, 32697, 439, 264, 636, 293, 5623, 428, 19265, 382, 731, 11, 558, 30, 50588], "temperature": 0.0, "avg_logprob": -0.09694419786768052, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.0007095145410858095}, {"id": 489, "seek": 183356, "start": 1838.04, "end": 1841.8799999999999, "text": " So in the previous two architectures, we saw you keep the generator fixed,", "tokens": [50588, 407, 294, 264, 3894, 732, 6331, 1303, 11, 321, 1866, 291, 1066, 264, 19265, 6806, 11, 50780], "temperature": 0.0, "avg_logprob": -0.09694419786768052, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.0007095145410858095}, {"id": 490, "seek": 183356, "start": 1841.8799999999999, "end": 1846.04, "text": " you backdrop into your retriever, but here we update everything.", "tokens": [50780, 291, 32697, 666, 428, 19817, 331, 11, 457, 510, 321, 5623, 1203, 13, 50988], "temperature": 0.0, "avg_logprob": -0.09694419786768052, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.0007095145410858095}, {"id": 491, "seek": 183356, "start": 1846.6, "end": 1852.28, "text": " Well, not exactly everything as you'll see, but we'll also update the part of the retriever", "tokens": [51016, 1042, 11, 406, 2293, 1203, 382, 291, 603, 536, 11, 457, 321, 603, 611, 5623, 264, 644, 295, 264, 19817, 331, 51300], "temperature": 0.0, "avg_logprob": -0.09694419786768052, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.0007095145410858095}, {"id": 492, "seek": 183356, "start": 1852.28, "end": 1853.08, "text": " and the generator.", "tokens": [51300, 293, 264, 19265, 13, 51340], "temperature": 0.0, "avg_logprob": -0.09694419786768052, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.0007095145410858095}, {"id": 493, "seek": 183356, "start": 1854.44, "end": 1859.08, "text": " So in this ragged model, we actually have two different ways of doing this.", "tokens": [51408, 407, 294, 341, 17539, 3004, 2316, 11, 321, 767, 362, 732, 819, 2098, 295, 884, 341, 13, 51640], "temperature": 0.0, "avg_logprob": -0.09694419786768052, "compression_ratio": 1.7404255319148936, "no_speech_prob": 0.0007095145410858095}, {"id": 494, "seek": 185908, "start": 1860.04, "end": 1862.36, "text": " It's probably something that when we talk about this,", "tokens": [50412, 467, 311, 1391, 746, 300, 562, 321, 751, 466, 341, 11, 50528], "temperature": 0.0, "avg_logprob": -0.1489421745826458, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.0005883554113097489}, {"id": 495, "seek": 185908, "start": 1863.48, "end": 1867.8799999999999, "text": " if you think about this long enough, then you'll think like, okay, but when actually do I need to", "tokens": [50584, 498, 291, 519, 466, 341, 938, 1547, 11, 550, 291, 603, 519, 411, 11, 1392, 11, 457, 562, 767, 360, 286, 643, 281, 50804], "temperature": 0.0, "avg_logprob": -0.1489421745826458, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.0005883554113097489}, {"id": 496, "seek": 185908, "start": 1867.8799999999999, "end": 1874.12, "text": " retrieve? Do I retrieve every time I generate a new token or do I just retrieve once and then", "tokens": [50804, 30254, 30, 1144, 286, 30254, 633, 565, 286, 8460, 257, 777, 14862, 420, 360, 286, 445, 30254, 1564, 293, 550, 51116], "temperature": 0.0, "avg_logprob": -0.1489421745826458, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.0005883554113097489}, {"id": 497, "seek": 185908, "start": 1874.12, "end": 1880.6, "text": " generate an entire sequence, right? Or maybe I want to retrieve every end tokens, right?", "tokens": [51116, 8460, 364, 2302, 8310, 11, 558, 30, 1610, 1310, 286, 528, 281, 30254, 633, 917, 22667, 11, 558, 30, 51440], "temperature": 0.0, "avg_logprob": -0.1489421745826458, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.0005883554113097489}, {"id": 498, "seek": 185908, "start": 1880.6, "end": 1883.3999999999999, "text": " So these are hypergrams, or maybe I want to learn when to retrieve,", "tokens": [51440, 407, 613, 366, 9848, 1342, 82, 11, 420, 1310, 286, 528, 281, 1466, 562, 281, 30254, 11, 51580], "temperature": 0.0, "avg_logprob": -0.1489421745826458, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.0005883554113097489}, {"id": 499, "seek": 185908, "start": 1883.3999999999999, "end": 1885.72, "text": " as we'll see that's also something people have done.", "tokens": [51580, 382, 321, 603, 536, 300, 311, 611, 746, 561, 362, 1096, 13, 51696], "temperature": 0.0, "avg_logprob": -0.1489421745826458, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.0005883554113097489}, {"id": 500, "seek": 188572, "start": 1886.3600000000001, "end": 1888.84, "text": " So these are two different ways to do it.", "tokens": [50396, 407, 613, 366, 732, 819, 2098, 281, 360, 309, 13, 50520], "temperature": 0.0, "avg_logprob": -0.12673430172902234, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.0004305032780393958}, {"id": 501, "seek": 188572, "start": 1890.04, "end": 1895.0, "text": " And what we do in this paper, basically the whole point of the paper is that this frozen thing", "tokens": [50580, 400, 437, 321, 360, 294, 341, 3035, 11, 1936, 264, 1379, 935, 295, 264, 3035, 307, 300, 341, 12496, 551, 50828], "temperature": 0.0, "avg_logprob": -0.12673430172902234, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.0004305032780393958}, {"id": 502, "seek": 188572, "start": 1895.0, "end": 1897.32, "text": " doesn't really work all that well, right?", "tokens": [50828, 1177, 380, 534, 589, 439, 300, 731, 11, 558, 30, 50944], "temperature": 0.0, "avg_logprob": -0.12673430172902234, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.0004305032780393958}, {"id": 503, "seek": 188572, "start": 1897.32, "end": 1902.92, "text": " So I think what people call rag now is usually referred to the frozen thing,", "tokens": [50944, 407, 286, 519, 437, 561, 818, 17539, 586, 307, 2673, 10839, 281, 264, 12496, 551, 11, 51224], "temperature": 0.0, "avg_logprob": -0.12673430172902234, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.0004305032780393958}, {"id": 504, "seek": 188572, "start": 1903.48, "end": 1907.32, "text": " but the whole paper basically would never have been accepted anywhere if we had just done the", "tokens": [51252, 457, 264, 1379, 3035, 1936, 576, 1128, 362, 668, 9035, 4992, 498, 321, 632, 445, 1096, 264, 51444], "temperature": 0.0, "avg_logprob": -0.12673430172902234, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.0004305032780393958}, {"id": 505, "seek": 188572, "start": 1907.32, "end": 1912.28, "text": " frozen thing, right? The whole point of the paper is that you want to optimize it.", "tokens": [51444, 12496, 551, 11, 558, 30, 440, 1379, 935, 295, 264, 3035, 307, 300, 291, 528, 281, 19719, 309, 13, 51692], "temperature": 0.0, "avg_logprob": -0.12673430172902234, "compression_ratio": 1.8382978723404255, "no_speech_prob": 0.0004305032780393958}, {"id": 506, "seek": 191228, "start": 1912.28, "end": 1916.92, "text": " And so at my company contextual, we call this frozen thing Frankenstein's monster,", "tokens": [50364, 400, 370, 412, 452, 2237, 35526, 11, 321, 818, 341, 12496, 551, 39678, 9089, 311, 10090, 11, 50596], "temperature": 0.0, "avg_logprob": -0.12970710754394532, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.0005701166810467839}, {"id": 507, "seek": 191228, "start": 1916.92, "end": 1920.28, "text": " because it's really like you cobble together these different pieces, right?", "tokens": [50596, 570, 309, 311, 534, 411, 291, 598, 10387, 1214, 613, 819, 3755, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12970710754394532, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.0005701166810467839}, {"id": 508, "seek": 191228, "start": 1920.28, "end": 1924.6, "text": " You sort of, yeah, it's really like Frankenstein, you just put it together and then it sort of", "tokens": [50764, 509, 1333, 295, 11, 1338, 11, 309, 311, 534, 411, 39678, 9089, 11, 291, 445, 829, 309, 1214, 293, 550, 309, 1333, 295, 50980], "temperature": 0.0, "avg_logprob": -0.12970710754394532, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.0005701166810467839}, {"id": 509, "seek": 191228, "start": 1924.6, "end": 1928.6, "text": " walks, you know, but it doesn't really have to solve, it doesn't really actually work,", "tokens": [50980, 12896, 11, 291, 458, 11, 457, 309, 1177, 380, 534, 362, 281, 5039, 11, 309, 1177, 380, 534, 767, 589, 11, 51180], "temperature": 0.0, "avg_logprob": -0.12970710754394532, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.0005701166810467839}, {"id": 510, "seek": 191228, "start": 1928.6, "end": 1933.6399999999999, "text": " because not the real thing. So that's great for everyone here, I think,", "tokens": [51180, 570, 406, 264, 957, 551, 13, 407, 300, 311, 869, 337, 1518, 510, 11, 286, 519, 11, 51432], "temperature": 0.0, "avg_logprob": -0.12970710754394532, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.0005701166810467839}, {"id": 511, "seek": 191228, "start": 1933.6399999999999, "end": 1937.96, "text": " because there are so many opportunities to do better than what most people are using right now.", "tokens": [51432, 570, 456, 366, 370, 867, 4786, 281, 360, 1101, 813, 437, 881, 561, 366, 1228, 558, 586, 13, 51648], "temperature": 0.0, "avg_logprob": -0.12970710754394532, "compression_ratio": 1.7824561403508772, "no_speech_prob": 0.0005701166810467839}, {"id": 512, "seek": 193796, "start": 1937.96, "end": 1946.68, "text": " So one of the limitations of the original rag architecture is that it only supports a very", "tokens": [50364, 407, 472, 295, 264, 15705, 295, 264, 3380, 17539, 9482, 307, 300, 309, 787, 9346, 257, 588, 50800], "temperature": 0.0, "avg_logprob": -0.10967636108398438, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.0005525941378436983}, {"id": 513, "seek": 193796, "start": 1946.68, "end": 1952.3600000000001, "text": " small cave. So if you have lots and lots of documents, then the problem is that you have", "tokens": [50800, 1359, 11730, 13, 407, 498, 291, 362, 3195, 293, 3195, 295, 8512, 11, 550, 264, 1154, 307, 300, 291, 362, 51084], "temperature": 0.0, "avg_logprob": -0.10967636108398438, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.0005525941378436983}, {"id": 514, "seek": 193796, "start": 1952.3600000000001, "end": 1958.04, "text": " to fit all of them in the context, but how do you really get that to fit, right?", "tokens": [51084, 281, 3318, 439, 295, 552, 294, 264, 4319, 11, 457, 577, 360, 291, 534, 483, 300, 281, 3318, 11, 558, 30, 51368], "temperature": 0.0, "avg_logprob": -0.10967636108398438, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.0005525941378436983}, {"id": 515, "seek": 193796, "start": 1958.04, "end": 1965.08, "text": " So one thing you can do is you first encode things so that you get one single representation,", "tokens": [51368, 407, 472, 551, 291, 393, 360, 307, 291, 700, 2058, 1429, 721, 370, 300, 291, 483, 472, 2167, 10290, 11, 51720], "temperature": 0.0, "avg_logprob": -0.10967636108398438, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.0005525941378436983}, {"id": 516, "seek": 196508, "start": 1965.56, "end": 1969.72, "text": " or only diffuse for the top level representations, then you concatenate those,", "tokens": [50388, 420, 787, 42165, 337, 264, 1192, 1496, 33358, 11, 550, 291, 1588, 7186, 473, 729, 11, 50596], "temperature": 0.0, "avg_logprob": -0.12776700742952116, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0026720110327005386}, {"id": 517, "seek": 196508, "start": 1969.72, "end": 1975.8799999999999, "text": " and then you just feed them to the decoder. So this is FID fusion and decoder. And as you can see,", "tokens": [50596, 293, 550, 291, 445, 3154, 552, 281, 264, 979, 19866, 13, 407, 341, 307, 479, 2777, 23100, 293, 979, 19866, 13, 400, 382, 291, 393, 536, 11, 50904], "temperature": 0.0, "avg_logprob": -0.12776700742952116, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0026720110327005386}, {"id": 518, "seek": 196508, "start": 1975.8799999999999, "end": 1983.96, "text": " this scales to a much higher number of passages. And that leads to corresponding improvements in", "tokens": [50904, 341, 17408, 281, 257, 709, 2946, 1230, 295, 31589, 13, 400, 300, 6689, 281, 11760, 13797, 294, 51308], "temperature": 0.0, "avg_logprob": -0.12776700742952116, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0026720110327005386}, {"id": 519, "seek": 196508, "start": 1983.96, "end": 1990.52, "text": " the scores that you care about. So that's a really cool idea. And so we're slowly moving", "tokens": [51308, 264, 13444, 300, 291, 1127, 466, 13, 407, 300, 311, 257, 534, 1627, 1558, 13, 400, 370, 321, 434, 5692, 2684, 51636], "temperature": 0.0, "avg_logprob": -0.12776700742952116, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.0026720110327005386}, {"id": 520, "seek": 199052, "start": 1990.52, "end": 1995.24, "text": " towards more decoder-only architectures. So in rag, we have this barred model,", "tokens": [50364, 3030, 544, 979, 19866, 12, 25202, 6331, 1303, 13, 407, 294, 17539, 11, 321, 362, 341, 2159, 986, 2316, 11, 50600], "temperature": 0.0, "avg_logprob": -0.14437066591702974, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.007572852540761232}, {"id": 521, "seek": 199052, "start": 1995.24, "end": 1999.4, "text": " it's sort of an encoder-decoder architecture, but here you just have this decoder that", "tokens": [50600, 309, 311, 1333, 295, 364, 2058, 19866, 12, 42821, 19866, 9482, 11, 457, 510, 291, 445, 362, 341, 979, 19866, 300, 50808], "temperature": 0.0, "avg_logprob": -0.14437066591702974, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.007572852540761232}, {"id": 522, "seek": 199052, "start": 1999.4, "end": 2008.52, "text": " does some fancy attention over stuff that you retrieved before. And so another pure decoder", "tokens": [50808, 775, 512, 10247, 3202, 670, 1507, 300, 291, 19817, 937, 949, 13, 400, 370, 1071, 6075, 979, 19866, 51264], "temperature": 0.0, "avg_logprob": -0.14437066591702974, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.007572852540761232}, {"id": 523, "seek": 199052, "start": 2008.52, "end": 2016.6, "text": " language model architecture is this one, KNNLM, which I think is very elegant in its simplicity.", "tokens": [51264, 2856, 2316, 9482, 307, 341, 472, 11, 26967, 45, 43, 44, 11, 597, 286, 519, 307, 588, 21117, 294, 1080, 25632, 13, 51668], "temperature": 0.0, "avg_logprob": -0.14437066591702974, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.007572852540761232}, {"id": 524, "seek": 201660, "start": 2016.6, "end": 2022.04, "text": " So it's basically, you just have a normal language model, but you interpolate the normal", "tokens": [50364, 407, 309, 311, 1936, 11, 291, 445, 362, 257, 2710, 2856, 2316, 11, 457, 291, 44902, 473, 264, 2710, 50636], "temperature": 0.0, "avg_logprob": -0.10831302609936945, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.0025104437954723835}, {"id": 525, "seek": 201660, "start": 2022.04, "end": 2028.36, "text": " language model weights with things that you retrieved. So basically, you have some sort", "tokens": [50636, 2856, 2316, 17443, 365, 721, 300, 291, 19817, 937, 13, 407, 1936, 11, 291, 362, 512, 1333, 50952], "temperature": 0.0, "avg_logprob": -0.10831302609936945, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.0025104437954723835}, {"id": 526, "seek": 201660, "start": 2028.36, "end": 2034.04, "text": " of prompt, right? So like Obama's birthplace is, you go to your big corpus, you find similar things,", "tokens": [50952, 295, 12391, 11, 558, 30, 407, 411, 9560, 311, 3965, 6742, 307, 11, 291, 352, 281, 428, 955, 1181, 31624, 11, 291, 915, 2531, 721, 11, 51236], "temperature": 0.0, "avg_logprob": -0.10831302609936945, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.0025104437954723835}, {"id": 527, "seek": 201660, "start": 2034.76, "end": 2040.36, "text": " you look at the words that come next to the similar things, you rank that thing,", "tokens": [51272, 291, 574, 412, 264, 2283, 300, 808, 958, 281, 264, 2531, 721, 11, 291, 6181, 300, 551, 11, 51552], "temperature": 0.0, "avg_logprob": -0.10831302609936945, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.0025104437954723835}, {"id": 528, "seek": 201660, "start": 2040.36, "end": 2046.1999999999998, "text": " you sample your top K, you renormalize that. So now you have a bunch of scores, and now you", "tokens": [51552, 291, 6889, 428, 1192, 591, 11, 291, 8124, 24440, 1125, 300, 13, 407, 586, 291, 362, 257, 3840, 295, 13444, 11, 293, 586, 291, 51844], "temperature": 0.0, "avg_logprob": -0.10831302609936945, "compression_ratio": 1.8442622950819672, "no_speech_prob": 0.0025104437954723835}, {"id": 529, "seek": 204620, "start": 2046.2, "end": 2051.8, "text": " can just interpolate between your retrieved kind of non-parametric memory scores and your", "tokens": [50364, 393, 445, 44902, 473, 1296, 428, 19817, 937, 733, 295, 2107, 12, 2181, 335, 17475, 4675, 13444, 293, 428, 50644], "temperature": 0.0, "avg_logprob": -0.1228355608488384, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.00033008132595568895}, {"id": 530, "seek": 204620, "start": 2051.8, "end": 2056.6, "text": " parametric language model scores. So this is very late fusion in a sense, right? At the very", "tokens": [50644, 6220, 17475, 2856, 2316, 13444, 13, 407, 341, 307, 588, 3469, 23100, 294, 257, 2020, 11, 558, 30, 1711, 264, 588, 50884], "temperature": 0.0, "avg_logprob": -0.1228355608488384, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.00033008132595568895}, {"id": 531, "seek": 204620, "start": 2056.6, "end": 2062.04, "text": " end, you combine these two, and it allows you to reweight the pure language model probabilities", "tokens": [50884, 917, 11, 291, 10432, 613, 732, 11, 293, 309, 4045, 291, 281, 319, 12329, 264, 6075, 2856, 2316, 33783, 51156], "temperature": 0.0, "avg_logprob": -0.1228355608488384, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.00033008132595568895}, {"id": 532, "seek": 204620, "start": 2062.04, "end": 2067.96, "text": " or like this. So this works really well, and it scales especially well if you have a huge", "tokens": [51156, 420, 411, 341, 13, 407, 341, 1985, 534, 731, 11, 293, 309, 17408, 2318, 731, 498, 291, 362, 257, 2603, 51452], "temperature": 0.0, "avg_logprob": -0.1228355608488384, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.00033008132595568895}, {"id": 533, "seek": 204620, "start": 2068.92, "end": 2073.56, "text": " retrieval corpus. And so if you have trillions and trillions of tokens in there, you can have", "tokens": [51500, 19817, 3337, 1181, 31624, 13, 400, 370, 498, 291, 362, 504, 46279, 293, 504, 46279, 295, 22667, 294, 456, 11, 291, 393, 362, 51732], "temperature": 0.0, "avg_logprob": -0.1228355608488384, "compression_ratio": 1.7976653696498055, "no_speech_prob": 0.00033008132595568895}, {"id": 534, "seek": 207356, "start": 2073.56, "end": 2079.16, "text": " a much smaller language model that does not that much heavy lifting because you can really rely on", "tokens": [50364, 257, 709, 4356, 2856, 2316, 300, 775, 406, 300, 709, 4676, 15798, 570, 291, 393, 534, 10687, 322, 50644], "temperature": 0.0, "avg_logprob": -0.08632146395169772, "compression_ratio": 1.65, "no_speech_prob": 0.0006262222887016833}, {"id": 535, "seek": 207356, "start": 2079.16, "end": 2085.88, "text": " this big source corpus that you're working from. And so that idea was exploited by this paper called", "tokens": [50644, 341, 955, 4009, 1181, 31624, 300, 291, 434, 1364, 490, 13, 400, 370, 300, 1558, 390, 40918, 538, 341, 3035, 1219, 50980], "temperature": 0.0, "avg_logprob": -0.08632146395169772, "compression_ratio": 1.65, "no_speech_prob": 0.0006262222887016833}, {"id": 536, "seek": 207356, "start": 2085.88, "end": 2092.68, "text": " Retro out of DeepMind, where they showed that you can have a 25 times smaller retrieval augmented", "tokens": [50980, 11495, 340, 484, 295, 14895, 44, 471, 11, 689, 436, 4712, 300, 291, 393, 362, 257, 3552, 1413, 4356, 19817, 3337, 36155, 51320], "temperature": 0.0, "avg_logprob": -0.08632146395169772, "compression_ratio": 1.65, "no_speech_prob": 0.0006262222887016833}, {"id": 537, "seek": 207356, "start": 2092.68, "end": 2098.36, "text": " language model trained from scratch, so really pre-trained entirely from scratch, that outperforms", "tokens": [51320, 2856, 2316, 8895, 490, 8459, 11, 370, 534, 659, 12, 17227, 2001, 7696, 490, 8459, 11, 300, 484, 26765, 82, 51604], "temperature": 0.0, "avg_logprob": -0.08632146395169772, "compression_ratio": 1.65, "no_speech_prob": 0.0006262222887016833}, {"id": 538, "seek": 209836, "start": 2098.44, "end": 2103.88, "text": " this 25 times bigger language model on the same data in terms of complexity, which is pretty", "tokens": [50368, 341, 3552, 1413, 3801, 2856, 2316, 322, 264, 912, 1412, 294, 2115, 295, 14024, 11, 597, 307, 1238, 50640], "temperature": 0.0, "avg_logprob": -0.07838257080918058, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0006262820097617805}, {"id": 539, "seek": 209836, "start": 2103.88, "end": 2109.8, "text": " impressive. So this architecture is much more efficient than a parametric model because you", "tokens": [50640, 8992, 13, 407, 341, 9482, 307, 709, 544, 7148, 813, 257, 6220, 17475, 2316, 570, 291, 50936], "temperature": 0.0, "avg_logprob": -0.07838257080918058, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0006262820097617805}, {"id": 540, "seek": 209836, "start": 2109.8, "end": 2115.6400000000003, "text": " can rely on this external memory. So if your external memory is big enough, you can get pretty", "tokens": [50936, 393, 10687, 322, 341, 8320, 4675, 13, 407, 498, 428, 8320, 4675, 307, 955, 1547, 11, 291, 393, 483, 1238, 51228], "temperature": 0.0, "avg_logprob": -0.07838257080918058, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0006262820097617805}, {"id": 541, "seek": 209836, "start": 2115.6400000000003, "end": 2122.1200000000003, "text": " huge gains. So there was a lot of excitement about Retro when it was announced, but this is a DeepMind", "tokens": [51228, 2603, 16823, 13, 407, 456, 390, 257, 688, 295, 14755, 466, 11495, 340, 562, 309, 390, 7548, 11, 457, 341, 307, 257, 14895, 44, 471, 51552], "temperature": 0.0, "avg_logprob": -0.07838257080918058, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0006262820097617805}, {"id": 542, "seek": 209836, "start": 2122.1200000000003, "end": 2127.32, "text": " paper, so there's really no open source, nothing really to validate that this actually works.", "tokens": [51552, 3035, 11, 370, 456, 311, 534, 572, 1269, 4009, 11, 1825, 534, 281, 29562, 300, 341, 767, 1985, 13, 51812], "temperature": 0.0, "avg_logprob": -0.07838257080918058, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0006262820097617805}, {"id": 543, "seek": 212836, "start": 2128.6, "end": 2133.7200000000003, "text": " And so very recently, there has been a bit of work from NVIDIA called Retro++,", "tokens": [50376, 400, 370, 588, 3938, 11, 456, 575, 668, 257, 857, 295, 589, 490, 426, 3958, 6914, 1219, 11495, 340, 25472, 11, 50632], "temperature": 0.0, "avg_logprob": -0.13682608908795296, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0013037450844421983}, {"id": 544, "seek": 212836, "start": 2135.32, "end": 2140.76, "text": " where they have this hybrid between the Retro architecture and then they do basically RAG,", "tokens": [50712, 689, 436, 362, 341, 13051, 1296, 264, 11495, 340, 9482, 293, 550, 436, 360, 1936, 14626, 38, 11, 50984], "temperature": 0.0, "avg_logprob": -0.13682608908795296, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0013037450844421983}, {"id": 545, "seek": 212836, "start": 2140.76, "end": 2146.44, "text": " sort of they put the top one or the top K results in the context of the language model after all.", "tokens": [50984, 1333, 295, 436, 829, 264, 1192, 472, 420, 264, 1192, 591, 3542, 294, 264, 4319, 295, 264, 2856, 2316, 934, 439, 13, 51268], "temperature": 0.0, "avg_logprob": -0.13682608908795296, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0013037450844421983}, {"id": 546, "seek": 212836, "start": 2146.44, "end": 2151.88, "text": " So it's sort of a crossover between RAG and Retro, and they showed some really nice results here,", "tokens": [51268, 407, 309, 311, 1333, 295, 257, 33837, 1296, 14626, 38, 293, 11495, 340, 11, 293, 436, 4712, 512, 534, 1481, 3542, 510, 11, 51540], "temperature": 0.0, "avg_logprob": -0.13682608908795296, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0013037450844421983}, {"id": 547, "seek": 215188, "start": 2151.88, "end": 2157.88, "text": " but I think it's sort of pointing to this big flaw, I think, is that why is there still no", "tokens": [50364, 457, 286, 519, 309, 311, 1333, 295, 12166, 281, 341, 955, 13717, 11, 286, 519, 11, 307, 300, 983, 307, 456, 920, 572, 50664], "temperature": 0.0, "avg_logprob": -0.08728155493736267, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0006874619866721332}, {"id": 548, "seek": 215188, "start": 2157.88, "end": 2163.88, "text": " good open source Retro model? It probably tells you something about whether it actually really", "tokens": [50664, 665, 1269, 4009, 11495, 340, 2316, 30, 467, 1391, 5112, 291, 746, 466, 1968, 309, 767, 534, 50964], "temperature": 0.0, "avg_logprob": -0.08728155493736267, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0006874619866721332}, {"id": 549, "seek": 215188, "start": 2163.88, "end": 2169.8, "text": " works. I spent a lot of time in my career trying to reproduce DeepMind papers that didn't necessarily", "tokens": [50964, 1985, 13, 286, 4418, 257, 688, 295, 565, 294, 452, 3988, 1382, 281, 29501, 14895, 44, 471, 10577, 300, 994, 380, 4725, 51260], "temperature": 0.0, "avg_logprob": -0.08728155493736267, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0006874619866721332}, {"id": 550, "seek": 215188, "start": 2169.8, "end": 2177.6400000000003, "text": " always work. And so I think the same is true for Retro, and that's why we need to do this", "tokens": [51260, 1009, 589, 13, 400, 370, 286, 519, 264, 912, 307, 2074, 337, 11495, 340, 11, 293, 300, 311, 983, 321, 643, 281, 360, 341, 51652], "temperature": 0.0, "avg_logprob": -0.08728155493736267, "compression_ratio": 1.584033613445378, "no_speech_prob": 0.0006874619866721332}, {"id": 551, "seek": 217764, "start": 2177.64, "end": 2180.68, "text": " in-context RAG on top of Retro to actually get it to work.", "tokens": [50364, 294, 12, 9000, 3828, 14626, 38, 322, 1192, 295, 11495, 340, 281, 767, 483, 309, 281, 589, 13, 50516], "temperature": 0.0, "avg_logprob": -0.27056745897259626, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.005467490293085575}, {"id": 552, "seek": 217764, "start": 2192.68, "end": 2197.8799999999997, "text": " So doing retrieval over that big corpus is not that difficult, actually.", "tokens": [51116, 407, 884, 19817, 3337, 670, 300, 955, 1181, 31624, 307, 406, 300, 2252, 11, 767, 13, 51376], "temperature": 0.0, "avg_logprob": -0.27056745897259626, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.005467490293085575}, {"id": 553, "seek": 217764, "start": 2199.48, "end": 2204.68, "text": " So there are even distributed face packages, you can just do everything yourself.", "tokens": [51456, 407, 456, 366, 754, 12631, 1851, 17401, 11, 291, 393, 445, 360, 1203, 1803, 13, 51716], "temperature": 0.0, "avg_logprob": -0.27056745897259626, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.005467490293085575}, {"id": 554, "seek": 220468, "start": 2205.64, "end": 2211.48, "text": " Yeah, so in terms of compute, it's actually not that hard anymore to reproduce something like this,", "tokens": [50412, 865, 11, 370, 294, 2115, 295, 14722, 11, 309, 311, 767, 406, 300, 1152, 3602, 281, 29501, 746, 411, 341, 11, 50704], "temperature": 0.0, "avg_logprob": -0.13546394720310118, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0009692126186564565}, {"id": 555, "seek": 220468, "start": 2212.2799999999997, "end": 2216.2799999999997, "text": " but I've tried several times and it's not really reproducible.", "tokens": [50744, 457, 286, 600, 3031, 2940, 1413, 293, 309, 311, 406, 534, 11408, 32128, 13, 50944], "temperature": 0.0, "avg_logprob": -0.13546394720310118, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0009692126186564565}, {"id": 556, "seek": 220468, "start": 2217.24, "end": 2221.3999999999996, "text": " So the only way to get it to work is if you do this in-context RAG on top of the Retro thing,", "tokens": [50992, 407, 264, 787, 636, 281, 483, 309, 281, 589, 307, 498, 291, 360, 341, 294, 12, 9000, 3828, 14626, 38, 322, 1192, 295, 264, 11495, 340, 551, 11, 51200], "temperature": 0.0, "avg_logprob": -0.13546394720310118, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0009692126186564565}, {"id": 557, "seek": 220468, "start": 2221.3999999999996, "end": 2226.04, "text": " and then as you can see here in the results, then it actually gives you a gain over the pure GPT", "tokens": [51200, 293, 550, 382, 291, 393, 536, 510, 294, 264, 3542, 11, 550, 309, 767, 2709, 291, 257, 6052, 670, 264, 6075, 26039, 51, 51432], "temperature": 0.0, "avg_logprob": -0.13546394720310118, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0009692126186564565}, {"id": 558, "seek": 220468, "start": 2226.04, "end": 2230.9199999999996, "text": " model. So it starts from a GPT and then they kind of retrofit as they call it the GPT model.", "tokens": [51432, 2316, 13, 407, 309, 3719, 490, 257, 26039, 51, 293, 550, 436, 733, 295, 18820, 6845, 382, 436, 818, 309, 264, 26039, 51, 2316, 13, 51676], "temperature": 0.0, "avg_logprob": -0.13546394720310118, "compression_ratio": 1.657992565055762, "no_speech_prob": 0.0009692126186564565}, {"id": 559, "seek": 223092, "start": 2231.88, "end": 2236.6, "text": " So in short, I think there's still a lot of work to be done in pre-training these systems,", "tokens": [50412, 407, 294, 2099, 11, 286, 519, 456, 311, 920, 257, 688, 295, 589, 281, 312, 1096, 294, 659, 12, 17227, 1760, 613, 3652, 11, 50648], "temperature": 0.0, "avg_logprob": -0.2936428493923611, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00039193700649775565}, {"id": 560, "seek": 223092, "start": 2236.6, "end": 2241.64, "text": " really from scratch. And Retro kind of showed that it might be possible, but we don't necessarily", "tokens": [50648, 534, 490, 8459, 13, 400, 11495, 340, 733, 295, 4712, 300, 309, 1062, 312, 1944, 11, 457, 321, 500, 380, 4725, 50900], "temperature": 0.0, "avg_logprob": -0.2936428493923611, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00039193700649775565}, {"id": 561, "seek": 223092, "start": 2241.64, "end": 2246.44, "text": " know exactly how to do it the right way. And this is really one of the interesting open questions.", "tokens": [50900, 458, 2293, 577, 281, 360, 309, 264, 558, 636, 13, 400, 341, 307, 534, 472, 295, 264, 1880, 1269, 1651, 13, 51140], "temperature": 0.0, "avg_logprob": -0.2936428493923611, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00039193700649775565}, {"id": 562, "seek": 223092, "start": 2248.36, "end": 2249.56, "text": " Any questions on that?", "tokens": [51236, 2639, 1651, 322, 300, 30, 51296], "temperature": 0.0, "avg_logprob": -0.2936428493923611, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00039193700649775565}, {"id": 563, "seek": 223092, "start": 2253.88, "end": 2254.36, "text": " Online?", "tokens": [51512, 16930, 30, 51536], "temperature": 0.0, "avg_logprob": -0.2936428493923611, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00039193700649775565}, {"id": 564, "seek": 223092, "start": 2258.92, "end": 2259.4, "text": " No? Okay.", "tokens": [51764, 883, 30, 1033, 13, 51788], "temperature": 0.0, "avg_logprob": -0.2936428493923611, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.00039193700649775565}, {"id": 565, "seek": 226092, "start": 2261.16, "end": 2271.88, "text": " Then we'll move on. So let's go all the way with the contextualization now. So with Retro and with", "tokens": [50376, 1396, 321, 603, 1286, 322, 13, 407, 718, 311, 352, 439, 264, 636, 365, 264, 35526, 2144, 586, 13, 407, 365, 11495, 340, 293, 365, 50912], "temperature": 0.0, "avg_logprob": -0.10824882067166842, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.001345081371255219}, {"id": 566, "seek": 226092, "start": 2271.88, "end": 2279.08, "text": " RAG, what we actually did is we only updated the query encoder. So updating the document", "tokens": [50912, 14626, 38, 11, 437, 321, 767, 630, 307, 321, 787, 10588, 264, 14581, 2058, 19866, 13, 407, 25113, 264, 4166, 51272], "temperature": 0.0, "avg_logprob": -0.10824882067166842, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.001345081371255219}, {"id": 567, "seek": 226092, "start": 2279.08, "end": 2286.28, "text": " encoder is very expensive. So one of the first papers, actually kind of the OG of the non-frozen", "tokens": [51272, 2058, 19866, 307, 588, 5124, 13, 407, 472, 295, 264, 700, 10577, 11, 767, 733, 295, 264, 32477, 295, 264, 2107, 12, 69, 340, 2904, 51632], "temperature": 0.0, "avg_logprob": -0.10824882067166842, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.001345081371255219}, {"id": 568, "seek": 228628, "start": 2286.28, "end": 2291.88, "text": " dense retrieval augmented methods is this paper called Realm. This is really like visionary", "tokens": [50364, 18011, 19817, 3337, 36155, 7150, 307, 341, 3035, 1219, 44723, 13, 639, 307, 534, 411, 49442, 50644], "temperature": 0.0, "avg_logprob": -0.10822450282961824, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002550316508859396}, {"id": 569, "seek": 228628, "start": 2291.88, "end": 2299.2400000000002, "text": " work. This was basically the first kind of version that did this properly, where they updated it", "tokens": [50644, 589, 13, 639, 390, 1936, 264, 700, 733, 295, 3037, 300, 630, 341, 6108, 11, 689, 436, 10588, 309, 51012], "temperature": 0.0, "avg_logprob": -0.10822450282961824, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002550316508859396}, {"id": 570, "seek": 228628, "start": 2299.2400000000002, "end": 2305.48, "text": " all the way, including the document encoder. So can someone explain to me why it's expensive to", "tokens": [51012, 439, 264, 636, 11, 3009, 264, 4166, 2058, 19866, 13, 407, 393, 1580, 2903, 281, 385, 983, 309, 311, 5124, 281, 51324], "temperature": 0.0, "avg_logprob": -0.10822450282961824, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002550316508859396}, {"id": 571, "seek": 228628, "start": 2305.48, "end": 2313.96, "text": " update the document encoder? So let's say we have a trillion tokens in our corpus.", "tokens": [51324, 5623, 264, 4166, 2058, 19866, 30, 407, 718, 311, 584, 321, 362, 257, 18723, 22667, 294, 527, 1181, 31624, 13, 51748], "temperature": 0.0, "avg_logprob": -0.10822450282961824, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.002550316508859396}, {"id": 572, "seek": 231396, "start": 2314.92, "end": 2321.0, "text": " So now we go all the way. So we basically do a forward pass. We get a gradient at the end.", "tokens": [50412, 407, 586, 321, 352, 439, 264, 636, 13, 407, 321, 1936, 360, 257, 2128, 1320, 13, 492, 483, 257, 16235, 412, 264, 917, 13, 50716], "temperature": 0.0, "avg_logprob": -0.10842687083828834, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0012841332936659455}, {"id": 573, "seek": 231396, "start": 2321.0, "end": 2325.16, "text": " Now we back propagate the gradient through the retriever. We update the query encoder.", "tokens": [50716, 823, 321, 646, 48256, 264, 16235, 807, 264, 19817, 331, 13, 492, 5623, 264, 14581, 2058, 19866, 13, 50924], "temperature": 0.0, "avg_logprob": -0.10842687083828834, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0012841332936659455}, {"id": 574, "seek": 231396, "start": 2325.16, "end": 2330.04, "text": " Now we have to update the document encoder. So what do we then need to do after we've updated", "tokens": [50924, 823, 321, 362, 281, 5623, 264, 4166, 2058, 19866, 13, 407, 437, 360, 321, 550, 643, 281, 360, 934, 321, 600, 10588, 51168], "temperature": 0.0, "avg_logprob": -0.10842687083828834, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0012841332936659455}, {"id": 575, "seek": 231396, "start": 2330.04, "end": 2336.76, "text": " the document encoder? We need to re-encode the entire internet. So basically every single gradient", "tokens": [51168, 264, 4166, 2058, 19866, 30, 492, 643, 281, 319, 12, 22660, 1429, 264, 2302, 4705, 13, 407, 1936, 633, 2167, 16235, 51504], "temperature": 0.0, "avg_logprob": -0.10842687083828834, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0012841332936659455}, {"id": 576, "seek": 231396, "start": 2336.76, "end": 2342.92, "text": " update, we have to re-encode whatever our index is. So if this is like trillions of tokens, it's like", "tokens": [51504, 5623, 11, 321, 362, 281, 319, 12, 22660, 1429, 2035, 527, 8186, 307, 13, 407, 498, 341, 307, 411, 504, 46279, 295, 22667, 11, 309, 311, 411, 51812], "temperature": 0.0, "avg_logprob": -0.10842687083828834, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0012841332936659455}, {"id": 577, "seek": 234292, "start": 2342.92, "end": 2347.48, "text": " re-encoding the internet after every batch update. So that's not very efficient.", "tokens": [50364, 319, 12, 22660, 8616, 264, 4705, 934, 633, 15245, 5623, 13, 407, 300, 311, 406, 588, 7148, 13, 50592], "temperature": 0.0, "avg_logprob": -0.4248906649076022, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013663788558915257}, {"id": 578, "seek": 234292, "start": 2349.2400000000002, "end": 2354.6800000000003, "text": " Well, I think it does look like we've got a very general international change. So if you learn", "tokens": [50680, 1042, 11, 286, 519, 309, 775, 574, 411, 321, 600, 658, 257, 588, 2674, 5058, 1319, 13, 407, 498, 291, 1466, 50952], "temperature": 0.0, "avg_logprob": -0.4248906649076022, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013663788558915257}, {"id": 579, "seek": 234292, "start": 2354.6800000000003, "end": 2360.04, "text": " digital or other sort of stuff, like if you basically take your old activations and that sounds", "tokens": [50952, 4562, 420, 661, 1333, 295, 1507, 11, 411, 498, 291, 1936, 747, 428, 1331, 2430, 763, 293, 300, 3263, 51220], "temperature": 0.0, "avg_logprob": -0.4248906649076022, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013663788558915257}, {"id": 580, "seek": 234292, "start": 2360.04, "end": 2364.12, "text": " like a long, unpredictable change to your entire business. Yeah.", "tokens": [51220, 411, 257, 938, 11, 31160, 1319, 281, 428, 2302, 1606, 13, 865, 13, 51424], "temperature": 0.0, "avg_logprob": -0.4248906649076022, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013663788558915257}, {"id": 581, "seek": 234292, "start": 2366.92, "end": 2371.96, "text": " Yeah, that's one way to do it. So there are a bunch of different ways to update the", "tokens": [51564, 865, 11, 300, 311, 472, 636, 281, 360, 309, 13, 407, 456, 366, 257, 3840, 295, 819, 2098, 281, 5623, 264, 51816], "temperature": 0.0, "avg_logprob": -0.4248906649076022, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0013663788558915257}, {"id": 582, "seek": 237196, "start": 2371.96, "end": 2377.88, "text": " document encoder. So what they do in Realm is they basically do it for te batches.", "tokens": [50364, 4166, 2058, 19866, 13, 407, 437, 436, 360, 294, 44723, 307, 436, 1936, 360, 309, 337, 535, 15245, 279, 13, 50660], "temperature": 0.0, "avg_logprob": -0.10811645942821838, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.000709463085513562}, {"id": 583, "seek": 237196, "start": 2378.52, "end": 2382.68, "text": " Then they stop, they re-encode the entire internet, and then they train again.", "tokens": [50692, 1396, 436, 1590, 11, 436, 319, 12, 22660, 1429, 264, 2302, 4705, 11, 293, 550, 436, 3847, 797, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10811645942821838, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.000709463085513562}, {"id": 584, "seek": 237196, "start": 2383.4, "end": 2388.52, "text": " So it's sort of asynchronous updates. They have this very fancy sort of sharding mechanisms", "tokens": [50936, 407, 309, 311, 1333, 295, 49174, 9205, 13, 814, 362, 341, 588, 10247, 1333, 295, 402, 515, 278, 15902, 51192], "temperature": 0.0, "avg_logprob": -0.10811645942821838, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.000709463085513562}, {"id": 585, "seek": 237196, "start": 2388.52, "end": 2394.52, "text": " where they take down certain parts of their entire index and then update them kind of on the fly.", "tokens": [51192, 689, 436, 747, 760, 1629, 3166, 295, 641, 2302, 8186, 293, 550, 5623, 552, 733, 295, 322, 264, 3603, 13, 51492], "temperature": 0.0, "avg_logprob": -0.10811645942821838, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.000709463085513562}, {"id": 586, "seek": 237196, "start": 2395.7200000000003, "end": 2399.88, "text": " So you can do it. It's just very expensive. So one of the things that a lot of people have been", "tokens": [51552, 407, 291, 393, 360, 309, 13, 467, 311, 445, 588, 5124, 13, 407, 472, 295, 264, 721, 300, 257, 688, 295, 561, 362, 668, 51760], "temperature": 0.0, "avg_logprob": -0.10811645942821838, "compression_ratio": 1.7529411764705882, "no_speech_prob": 0.000709463085513562}, {"id": 587, "seek": 239988, "start": 2399.88, "end": 2406.36, "text": " thinking about, not exactly the Laura idea, but similar versions of that are around, like,", "tokens": [50364, 1953, 466, 11, 406, 2293, 264, 13220, 1558, 11, 457, 2531, 9606, 295, 300, 366, 926, 11, 411, 11, 50688], "temperature": 0.0, "avg_logprob": -0.13260388985658303, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0005879467935301363}, {"id": 588, "seek": 239988, "start": 2406.36, "end": 2410.76, "text": " can you make it more efficient so that you don't have to do this asynchronously?", "tokens": [50688, 393, 291, 652, 309, 544, 7148, 370, 300, 291, 500, 380, 362, 281, 360, 341, 42642, 5098, 30, 50908], "temperature": 0.0, "avg_logprob": -0.13260388985658303, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0005879467935301363}, {"id": 589, "seek": 239988, "start": 2412.92, "end": 2418.6, "text": " So one of the downsides of this Realm architecture is that it's really just a BERT model, but then", "tokens": [51016, 407, 472, 295, 264, 21554, 1875, 295, 341, 44723, 9482, 307, 300, 309, 311, 534, 445, 257, 363, 31479, 2316, 11, 457, 550, 51300], "temperature": 0.0, "avg_logprob": -0.13260388985658303, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0005879467935301363}, {"id": 590, "seek": 239988, "start": 2418.6, "end": 2422.36, "text": " you do this retrieval augmentation on a BERT model with other BERT models. So it's not really", "tokens": [51300, 291, 360, 341, 19817, 3337, 14501, 19631, 322, 257, 363, 31479, 2316, 365, 661, 363, 31479, 5245, 13, 407, 309, 311, 406, 534, 51488], "temperature": 0.0, "avg_logprob": -0.13260388985658303, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0005879467935301363}, {"id": 591, "seek": 239988, "start": 2422.36, "end": 2428.2000000000003, "text": " generative. It's not really gen AI in the modern paradigm. But if you want to read one paper", "tokens": [51488, 1337, 1166, 13, 467, 311, 406, 534, 1049, 7318, 294, 264, 4363, 24709, 13, 583, 498, 291, 528, 281, 1401, 472, 3035, 51780], "temperature": 0.0, "avg_logprob": -0.13260388985658303, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0005879467935301363}, {"id": 592, "seek": 242820, "start": 2429.16, "end": 2435.3199999999997, "text": " on this topic, like, this is a very good one to read. The other one that is really, really good", "tokens": [50412, 322, 341, 4829, 11, 411, 11, 341, 307, 257, 588, 665, 472, 281, 1401, 13, 440, 661, 472, 300, 307, 534, 11, 534, 665, 50720], "temperature": 0.0, "avg_logprob": -0.13153064727783204, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001168727409094572}, {"id": 593, "seek": 242820, "start": 2435.3199999999997, "end": 2444.52, "text": " to read is this paper called Atlas. So Atlas is, so this is out of fare with a bunch of folks,", "tokens": [50720, 281, 1401, 307, 341, 3035, 1219, 32485, 13, 407, 32485, 307, 11, 370, 341, 307, 484, 295, 11994, 365, 257, 3840, 295, 4024, 11, 51180], "temperature": 0.0, "avg_logprob": -0.13153064727783204, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001168727409094572}, {"id": 594, "seek": 242820, "start": 2444.52, "end": 2451.08, "text": " the folks who did, like, RAG, and the folks who did FID, and really a brilliant set of people.", "tokens": [51180, 264, 4024, 567, 630, 11, 411, 11, 14626, 38, 11, 293, 264, 4024, 567, 630, 479, 2777, 11, 293, 534, 257, 10248, 992, 295, 561, 13, 51508], "temperature": 0.0, "avg_logprob": -0.13153064727783204, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001168727409094572}, {"id": 595, "seek": 242820, "start": 2451.08, "end": 2457.16, "text": " And this is really a comprehensive analysis of everything that's happening in this architecture.", "tokens": [51508, 400, 341, 307, 534, 257, 13914, 5215, 295, 1203, 300, 311, 2737, 294, 341, 9482, 13, 51812], "temperature": 0.0, "avg_logprob": -0.13153064727783204, "compression_ratio": 1.7207207207207207, "no_speech_prob": 0.001168727409094572}, {"id": 596, "seek": 245716, "start": 2457.24, "end": 2461.16, "text": " So the first question they really look at is, how do we train this retriever? So we've seen", "tokens": [50368, 407, 264, 700, 1168, 436, 534, 574, 412, 307, 11, 577, 360, 321, 3847, 341, 19817, 331, 30, 407, 321, 600, 1612, 50564], "temperature": 0.0, "avg_logprob": -0.09152804759510777, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0007667539757676423}, {"id": 597, "seek": 245716, "start": 2461.16, "end": 2467.08, "text": " a couple of versions of this, but which one actually works better? They haven't really", "tokens": [50564, 257, 1916, 295, 9606, 295, 341, 11, 457, 597, 472, 767, 1985, 1101, 30, 814, 2378, 380, 534, 50860], "temperature": 0.0, "avg_logprob": -0.09152804759510777, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0007667539757676423}, {"id": 598, "seek": 245716, "start": 2467.08, "end": 2472.2, "text": " been compared in a head-to-head setting. So one thing is we have this FID style sort of", "tokens": [50860, 668, 5347, 294, 257, 1378, 12, 1353, 12, 1934, 3287, 13, 407, 472, 551, 307, 321, 362, 341, 479, 2777, 3758, 1333, 295, 51116], "temperature": 0.0, "avg_logprob": -0.09152804759510777, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0007667539757676423}, {"id": 599, "seek": 245716, "start": 2472.2, "end": 2478.04, "text": " attention distillation. So that's really too complicated to go into detail here, but the", "tokens": [51116, 3202, 42923, 399, 13, 407, 300, 311, 534, 886, 6179, 281, 352, 666, 2607, 510, 11, 457, 264, 51408], "temperature": 0.0, "avg_logprob": -0.09152804759510777, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0007667539757676423}, {"id": 600, "seek": 245716, "start": 2478.04, "end": 2485.3199999999997, "text": " others are actually very simple. So one is this loss we've basically seen before. So we've seen", "tokens": [51408, 2357, 366, 767, 588, 2199, 13, 407, 472, 307, 341, 4470, 321, 600, 1936, 1612, 949, 13, 407, 321, 600, 1612, 51772], "temperature": 0.0, "avg_logprob": -0.09152804759510777, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0007667539757676423}, {"id": 601, "seek": 248532, "start": 2485.32, "end": 2490.2000000000003, "text": " this, I think, with the in-context RAG one. So we have a stop gradient on the language model,", "tokens": [50364, 341, 11, 286, 519, 11, 365, 264, 294, 12, 9000, 3828, 14626, 38, 472, 13, 407, 321, 362, 257, 1590, 16235, 322, 264, 2856, 2316, 11, 50608], "temperature": 0.0, "avg_logprob": -0.11033951152454723, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.0012247832491993904}, {"id": 602, "seek": 248532, "start": 2490.2000000000003, "end": 2496.2000000000003, "text": " and then we update the retriever. The other one is what we've seen with Replug. So this is basically", "tokens": [50608, 293, 550, 321, 5623, 264, 19817, 331, 13, 440, 661, 472, 307, 437, 321, 600, 1612, 365, 1300, 564, 697, 13, 407, 341, 307, 1936, 50908], "temperature": 0.0, "avg_logprob": -0.11033951152454723, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.0012247832491993904}, {"id": 603, "seek": 248532, "start": 2496.2000000000003, "end": 2503.48, "text": " exactly the Replug loss. So we have the KL divergence of the documents and sort of the", "tokens": [50908, 2293, 264, 1300, 564, 697, 4470, 13, 407, 321, 362, 264, 47991, 47387, 295, 264, 8512, 293, 1333, 295, 264, 51272], "temperature": 0.0, "avg_logprob": -0.11033951152454723, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.0012247832491993904}, {"id": 604, "seek": 248532, "start": 2503.48, "end": 2508.92, "text": " improvement that you see when you give it that document. The other thing they have is basically", "tokens": [51272, 10444, 300, 291, 536, 562, 291, 976, 309, 300, 4166, 13, 440, 661, 551, 436, 362, 307, 1936, 51544], "temperature": 0.0, "avg_logprob": -0.11033951152454723, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.0012247832491993904}, {"id": 605, "seek": 250892, "start": 2508.92, "end": 2514.52, "text": " the inverse of that one. So if I take this one document out, how does that affect my", "tokens": [50364, 264, 17340, 295, 300, 472, 13, 407, 498, 286, 747, 341, 472, 4166, 484, 11, 577, 775, 300, 3345, 452, 50644], "temperature": 0.0, "avg_logprob": -0.09469959653657058, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00870574451982975}, {"id": 606, "seek": 250892, "start": 2515.56, "end": 2522.36, "text": " perplexity of the language model? And so this one I think is actually quite elegant because", "tokens": [50696, 680, 18945, 507, 295, 264, 2856, 2316, 30, 400, 370, 341, 472, 286, 519, 307, 767, 1596, 21117, 570, 51036], "temperature": 0.0, "avg_logprob": -0.09469959653657058, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00870574451982975}, {"id": 607, "seek": 250892, "start": 2522.36, "end": 2528.28, "text": " that really gets to like, how valuable is this one single document for me answering this question", "tokens": [51036, 300, 534, 2170, 281, 411, 11, 577, 8263, 307, 341, 472, 2167, 4166, 337, 385, 13430, 341, 1168, 51332], "temperature": 0.0, "avg_logprob": -0.09469959653657058, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00870574451982975}, {"id": 608, "seek": 250892, "start": 2528.28, "end": 2537.0, "text": " correctly? So they compare all of these different versions, and what you can see is that the kind", "tokens": [51332, 8944, 30, 407, 436, 6794, 439, 295, 613, 819, 9606, 11, 293, 437, 291, 393, 536, 307, 300, 264, 733, 51768], "temperature": 0.0, "avg_logprob": -0.09469959653657058, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00870574451982975}, {"id": 609, "seek": 253700, "start": 2537.08, "end": 2541.64, "text": " of Replug style loss and this leave one out loss, they performed a lot better than all of these", "tokens": [50368, 295, 1300, 564, 697, 3758, 4470, 293, 341, 1856, 472, 484, 4470, 11, 436, 10332, 257, 688, 1101, 813, 439, 295, 613, 50596], "temperature": 0.0, "avg_logprob": -0.10932653090533088, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0005192524986341596}, {"id": 610, "seek": 253700, "start": 2541.64, "end": 2546.12, "text": " others. So this fixed retriever or no joint pre-training, these are really kind of the", "tokens": [50596, 2357, 13, 407, 341, 6806, 19817, 331, 420, 572, 7225, 659, 12, 17227, 1760, 11, 613, 366, 534, 733, 295, 264, 50820], "temperature": 0.0, "avg_logprob": -0.10932653090533088, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0005192524986341596}, {"id": 611, "seek": 253700, "start": 2546.12, "end": 2552.52, "text": " baseline sort of frozen RAG models or closed book. And as you can see, you can do really a lot better", "tokens": [50820, 20518, 1333, 295, 12496, 14626, 38, 5245, 420, 5395, 1446, 13, 400, 382, 291, 393, 536, 11, 291, 393, 360, 534, 257, 688, 1101, 51140], "temperature": 0.0, "avg_logprob": -0.10932653090533088, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0005192524986341596}, {"id": 612, "seek": 253700, "start": 2553.16, "end": 2558.92, "text": " if you optimize things. And so this leave one out thing is probably the best I would say.", "tokens": [51172, 498, 291, 19719, 721, 13, 400, 370, 341, 1856, 472, 484, 551, 307, 1391, 264, 1151, 286, 576, 584, 13, 51460], "temperature": 0.0, "avg_logprob": -0.10932653090533088, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0005192524986341596}, {"id": 613, "seek": 253700, "start": 2560.04, "end": 2565.0, "text": " So then the other question is how do you actually like train that entire system? Like what data or", "tokens": [51516, 407, 550, 264, 661, 1168, 307, 577, 360, 291, 767, 411, 3847, 300, 2302, 1185, 30, 1743, 437, 1412, 420, 51764], "temperature": 0.0, "avg_logprob": -0.10932653090533088, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0005192524986341596}, {"id": 614, "seek": 256500, "start": 2565.0, "end": 2570.6, "text": " what tasks do you train this on? So they also experiment with a bunch of different versions.", "tokens": [50364, 437, 9608, 360, 291, 3847, 341, 322, 30, 407, 436, 611, 5120, 365, 257, 3840, 295, 819, 9606, 13, 50644], "temperature": 0.0, "avg_logprob": -0.11344630964871111, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.0007671291823498905}, {"id": 615, "seek": 256500, "start": 2570.6, "end": 2577.96, "text": " So one is doing prefix lm, if you're familiar with that. So they basically take a chunk that", "tokens": [50644, 407, 472, 307, 884, 46969, 287, 76, 11, 498, 291, 434, 4963, 365, 300, 13, 407, 436, 1936, 747, 257, 16635, 300, 51012], "temperature": 0.0, "avg_logprob": -0.11344630964871111, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.0007671291823498905}, {"id": 616, "seek": 256500, "start": 2577.96, "end": 2583.24, "text": " occurs somewhere on the internet, and then they predict the next chunk from that chunk. So it's", "tokens": [51012, 11843, 4079, 322, 264, 4705, 11, 293, 550, 436, 6069, 264, 958, 16635, 490, 300, 16635, 13, 407, 309, 311, 51276], "temperature": 0.0, "avg_logprob": -0.11344630964871111, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.0007671291823498905}, {"id": 617, "seek": 256500, "start": 2583.24, "end": 2588.28, "text": " really like sentence to sentence. So maybe like skip thought back in the day, but now you have", "tokens": [51276, 534, 411, 8174, 281, 8174, 13, 407, 1310, 411, 10023, 1194, 646, 294, 264, 786, 11, 457, 586, 291, 362, 51528], "temperature": 0.0, "avg_logprob": -0.11344630964871111, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.0007671291823498905}, {"id": 618, "seek": 256500, "start": 2588.28, "end": 2594.68, "text": " this retrieval step where you predict the next sentence. Then they just do T5 styles or denoising.", "tokens": [51528, 341, 19817, 3337, 1823, 689, 291, 6069, 264, 958, 8174, 13, 1396, 436, 445, 360, 314, 20, 13273, 420, 1441, 78, 3436, 13, 51848], "temperature": 0.0, "avg_logprob": -0.11344630964871111, "compression_ratio": 1.7335766423357664, "no_speech_prob": 0.0007671291823498905}, {"id": 619, "seek": 259468, "start": 2594.68, "end": 2600.2, "text": " So that's mass language modeling if you're familiar with T5. And then they have this title for section", "tokens": [50364, 407, 300, 311, 2758, 2856, 15983, 498, 291, 434, 4963, 365, 314, 20, 13, 400, 550, 436, 362, 341, 4876, 337, 3541, 50640], "temperature": 0.0, "avg_logprob": -0.09739363193511963, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00043719011591747403}, {"id": 620, "seek": 259468, "start": 2600.2, "end": 2606.12, "text": " generation piece. So I think the takeaway from this table is basically that whatever you do here,", "tokens": [50640, 5125, 2522, 13, 407, 286, 519, 264, 30681, 490, 341, 3199, 307, 1936, 300, 2035, 291, 360, 510, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09739363193511963, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00043719011591747403}, {"id": 621, "seek": 259468, "start": 2606.12, "end": 2611.16, "text": " so they're using T5 models. So whatever you do here needs to be the same that your language", "tokens": [50936, 370, 436, 434, 1228, 314, 20, 5245, 13, 407, 2035, 291, 360, 510, 2203, 281, 312, 264, 912, 300, 428, 2856, 51188], "temperature": 0.0, "avg_logprob": -0.09739363193511963, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00043719011591747403}, {"id": 622, "seek": 259468, "start": 2611.16, "end": 2620.44, "text": " model expects. So for T5, that's T5 style loss. And then the next sort of final question that", "tokens": [51188, 2316, 33280, 13, 407, 337, 314, 20, 11, 300, 311, 314, 20, 3758, 4470, 13, 400, 550, 264, 958, 1333, 295, 2572, 1168, 300, 51652], "temperature": 0.0, "avg_logprob": -0.09739363193511963, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.00043719011591747403}, {"id": 623, "seek": 262044, "start": 2620.44, "end": 2625.48, "text": " they look into going back to what we talked about, how exactly do we update this retriever?", "tokens": [50364, 436, 574, 666, 516, 646, 281, 437, 321, 2825, 466, 11, 577, 2293, 360, 321, 5623, 341, 19817, 331, 30, 50616], "temperature": 0.0, "avg_logprob": -0.10123460575685662, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0052998741157352924}, {"id": 624, "seek": 262044, "start": 2626.2000000000003, "end": 2631.56, "text": " So do we have to update the document encoder? Or do we maybe have to do some sort of re-ranking?", "tokens": [50652, 407, 360, 321, 362, 281, 5623, 264, 4166, 2058, 19866, 30, 1610, 360, 321, 1310, 362, 281, 360, 512, 1333, 295, 319, 12, 20479, 278, 30, 50920], "temperature": 0.0, "avg_logprob": -0.10123460575685662, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0052998741157352924}, {"id": 625, "seek": 262044, "start": 2632.2000000000003, "end": 2638.36, "text": " Or do we maybe just update the query? And quite surprisingly, I think they find that just updating", "tokens": [50952, 1610, 360, 321, 1310, 445, 5623, 264, 14581, 30, 400, 1596, 17600, 11, 286, 519, 436, 915, 300, 445, 25113, 51260], "temperature": 0.0, "avg_logprob": -0.10123460575685662, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0052998741157352924}, {"id": 626, "seek": 262044, "start": 2638.36, "end": 2644.36, "text": " the query. So like in your original RAD paper is actually already basically good enough in many", "tokens": [51260, 264, 14581, 13, 407, 411, 294, 428, 3380, 497, 6112, 3035, 307, 767, 1217, 1936, 665, 1547, 294, 867, 51560], "temperature": 0.0, "avg_logprob": -0.10123460575685662, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0052998741157352924}, {"id": 627, "seek": 262044, "start": 2644.36, "end": 2650.04, "text": " cases. So that's nice because it's much more efficient if you don't have to update your documents", "tokens": [51560, 3331, 13, 407, 300, 311, 1481, 570, 309, 311, 709, 544, 7148, 498, 291, 500, 380, 362, 281, 5623, 428, 8512, 51844], "temperature": 0.0, "avg_logprob": -0.10123460575685662, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.0052998741157352924}, {"id": 628, "seek": 265004, "start": 2650.04, "end": 2656.52, "text": " all the time. I think the real question here though is like, how good is your document representation", "tokens": [50364, 439, 264, 565, 13, 286, 519, 264, 957, 1168, 510, 1673, 307, 411, 11, 577, 665, 307, 428, 4166, 10290, 50688], "temperature": 0.0, "avg_logprob": -0.10164226804460798, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0007204911089502275}, {"id": 629, "seek": 265004, "start": 2656.52, "end": 2661.72, "text": " to begin with? So you need to have a very, very high quality embedding model for this to work.", "tokens": [50688, 281, 1841, 365, 30, 407, 291, 643, 281, 362, 257, 588, 11, 588, 1090, 3125, 12240, 3584, 2316, 337, 341, 281, 589, 13, 50948], "temperature": 0.0, "avg_logprob": -0.10164226804460798, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0007204911089502275}, {"id": 630, "seek": 265004, "start": 2661.72, "end": 2666.44, "text": " If you don't have that, then this will not work. But if you do have that, then you get a very nice", "tokens": [50948, 759, 291, 500, 380, 362, 300, 11, 550, 341, 486, 406, 589, 13, 583, 498, 291, 360, 362, 300, 11, 550, 291, 483, 257, 588, 1481, 51184], "temperature": 0.0, "avg_logprob": -0.10164226804460798, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0007204911089502275}, {"id": 631, "seek": 265004, "start": 2666.44, "end": 2676.2799999999997, "text": " kind of query site fine tuning thing. So the Atlas paper is about trying to do few shop", "tokens": [51184, 733, 295, 14581, 3621, 2489, 15164, 551, 13, 407, 264, 32485, 3035, 307, 466, 1382, 281, 360, 1326, 3945, 51676], "temperature": 0.0, "avg_logprob": -0.10164226804460798, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.0007204911089502275}, {"id": 632, "seek": 267628, "start": 2677.2400000000002, "end": 2681.7200000000003, "text": " sort of language modeling tasks. So it's how many examples are given in the context.", "tokens": [50412, 1333, 295, 2856, 15983, 9608, 13, 407, 309, 311, 577, 867, 5110, 366, 2212, 294, 264, 4319, 13, 50636], "temperature": 0.0, "avg_logprob": -0.16379371521964906, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.0006562285125255585}, {"id": 633, "seek": 267628, "start": 2686.6800000000003, "end": 2692.6800000000003, "text": " Yeah, so the main takeaway here is that if you compare like the close book equivalent model", "tokens": [50884, 865, 11, 370, 264, 2135, 30681, 510, 307, 300, 498, 291, 6794, 411, 264, 1998, 1446, 10344, 2316, 51184], "temperature": 0.0, "avg_logprob": -0.16379371521964906, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.0006562285125255585}, {"id": 634, "seek": 267628, "start": 2692.6800000000003, "end": 2701.0, "text": " to the retrieval augmented model, you see very big improvements. That's really the only takeaway", "tokens": [51184, 281, 264, 19817, 3337, 36155, 2316, 11, 291, 536, 588, 955, 13797, 13, 663, 311, 534, 264, 787, 30681, 51600], "temperature": 0.0, "avg_logprob": -0.16379371521964906, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.0006562285125255585}, {"id": 635, "seek": 270100, "start": 2701.32, "end": 2709.48, "text": " of this entire section. But I think that that's really saying something in terms of what we should", "tokens": [50380, 295, 341, 2302, 3541, 13, 583, 286, 519, 300, 300, 311, 534, 1566, 746, 294, 2115, 295, 437, 321, 820, 50788], "temperature": 0.0, "avg_logprob": -0.22793284706447436, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.004534841515123844}, {"id": 636, "seek": 270100, "start": 2709.48, "end": 2720.36, "text": " be thinking about. How much time do I have until? Okay. All right. Other questions?", "tokens": [50788, 312, 1953, 466, 13, 1012, 709, 565, 360, 286, 362, 1826, 30, 1033, 13, 1057, 558, 13, 5358, 1651, 30, 51332], "temperature": 0.0, "avg_logprob": -0.22793284706447436, "compression_ratio": 1.3093525179856116, "no_speech_prob": 0.004534841515123844}, {"id": 637, "seek": 272036, "start": 2720.36, "end": 2737.08, "text": " Yeah, so there can be different. So in Atlas, the Atlas basically tries everything. So they also", "tokens": [50364, 865, 11, 370, 456, 393, 312, 819, 13, 407, 294, 32485, 11, 264, 32485, 1936, 9898, 1203, 13, 407, 436, 611, 51200], "temperature": 0.0, "avg_logprob": -0.2081845071580675, "compression_ratio": 1.5, "no_speech_prob": 0.002979863900691271}, {"id": 638, "seek": 272036, "start": 2737.08, "end": 2742.52, "text": " tried to see what happens if I train this on Wikipedia, but I swap in like a sort of common", "tokens": [51200, 3031, 281, 536, 437, 2314, 498, 286, 3847, 341, 322, 28999, 11, 457, 286, 18135, 294, 411, 257, 1333, 295, 2689, 51472], "temperature": 0.0, "avg_logprob": -0.2081845071580675, "compression_ratio": 1.5, "no_speech_prob": 0.002979863900691271}, {"id": 639, "seek": 272036, "start": 2742.52, "end": 2749.56, "text": " crawl index. And I think so in Atlas, but also in retro domain finding is just the more the better.", "tokens": [51472, 24767, 8186, 13, 400, 286, 519, 370, 294, 32485, 11, 457, 611, 294, 18820, 9274, 5006, 307, 445, 264, 544, 264, 1101, 13, 51824], "temperature": 0.0, "avg_logprob": -0.2081845071580675, "compression_ratio": 1.5, "no_speech_prob": 0.002979863900691271}, {"id": 640, "seek": 275036, "start": 2750.76, "end": 2756.04, "text": " So it's really just like the bigger your index, the more likely you are to find the exact", "tokens": [50384, 407, 309, 311, 534, 445, 411, 264, 3801, 428, 8186, 11, 264, 544, 3700, 291, 366, 281, 915, 264, 1900, 50648], "temperature": 0.0, "avg_logprob": -0.11198055138022213, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.0002652556577231735}, {"id": 641, "seek": 275036, "start": 2756.04, "end": 2759.48, "text": " right thing and then make the right prediction.", "tokens": [50648, 558, 551, 293, 550, 652, 264, 558, 17630, 13, 50820], "temperature": 0.0, "avg_logprob": -0.11198055138022213, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.0002652556577231735}, {"id": 642, "seek": 275036, "start": 2765.08, "end": 2771.32, "text": " Any other questions on this? Oh, yeah. Sorry. This is a question about the generator in the,", "tokens": [51100, 2639, 661, 1651, 322, 341, 30, 876, 11, 1338, 13, 4919, 13, 639, 307, 257, 1168, 466, 264, 19265, 294, 264, 11, 51412], "temperature": 0.0, "avg_logprob": -0.11198055138022213, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.0002652556577231735}, {"id": 643, "seek": 277132, "start": 2771.32, "end": 2780.76, "text": " I guess, the RAG system. So recently I saw a paper on Mistral 7B. So it introduces a lot of these", "tokens": [50364, 286, 2041, 11, 264, 14626, 38, 1185, 13, 407, 3938, 286, 1866, 257, 3035, 322, 20166, 2155, 1614, 33, 13, 407, 309, 31472, 257, 688, 295, 613, 50836], "temperature": 0.0, "avg_logprob": -0.13828689060854107, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.02757221832871437}, {"id": 644, "seek": 277132, "start": 2781.4, "end": 2786.6800000000003, "text": " new architectural changes like the sliding window attention to handle longer sequences at a smaller", "tokens": [50868, 777, 26621, 2962, 411, 264, 21169, 4910, 3202, 281, 4813, 2854, 22978, 412, 257, 4356, 51132], "temperature": 0.0, "avg_logprob": -0.13828689060854107, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.02757221832871437}, {"id": 645, "seek": 277132, "start": 2786.6800000000003, "end": 2792.84, "text": " cost and the group query attention for faster inference. I'd like to like know your thoughts on", "tokens": [51132, 2063, 293, 264, 1594, 14581, 3202, 337, 4663, 38253, 13, 286, 1116, 411, 281, 411, 458, 428, 4598, 322, 51440], "temperature": 0.0, "avg_logprob": -0.13828689060854107, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.02757221832871437}, {"id": 646, "seek": 277132, "start": 2793.4, "end": 2799.56, "text": " designing a generator specifically for RAG, leveraging, for example, where Mistral 7B", "tokens": [51468, 14685, 257, 19265, 4682, 337, 14626, 38, 11, 32666, 11, 337, 1365, 11, 689, 20166, 2155, 1614, 33, 51776], "temperature": 0.0, "avg_logprob": -0.13828689060854107, "compression_ratio": 1.559670781893004, "no_speech_prob": 0.02757221832871437}, {"id": 647, "seek": 279956, "start": 2799.56, "end": 2804.44, "text": " currently is. Because for example, like the sliding window attention, I could see how that", "tokens": [50364, 4362, 307, 13, 1436, 337, 1365, 11, 411, 264, 21169, 4910, 3202, 11, 286, 727, 536, 577, 300, 50608], "temperature": 0.0, "avg_logprob": -0.10534181180207626, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0009847769979387522}, {"id": 648, "seek": 279956, "start": 2804.44, "end": 2811.24, "text": " could be adapted to the RAG case. Yeah. So maybe you're agreed on sort of what makes Mistral's", "tokens": [50608, 727, 312, 20871, 281, 264, 14626, 38, 1389, 13, 865, 13, 407, 1310, 291, 434, 9166, 322, 1333, 295, 437, 1669, 20166, 2155, 311, 50948], "temperature": 0.0, "avg_logprob": -0.10534181180207626, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0009847769979387522}, {"id": 649, "seek": 279956, "start": 2811.24, "end": 2815.72, "text": " special is a bit different from mine. So I don't think that the sliding attention window thing is", "tokens": [50948, 2121, 307, 257, 857, 819, 490, 3892, 13, 407, 286, 500, 380, 519, 300, 264, 21169, 3202, 4910, 551, 307, 51172], "temperature": 0.0, "avg_logprob": -0.10534181180207626, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0009847769979387522}, {"id": 650, "seek": 279956, "start": 2815.72, "end": 2819.88, "text": " actually that interesting. The reason Mistral works so well is because it's trained on a lot of data.", "tokens": [51172, 767, 300, 1880, 13, 440, 1778, 20166, 2155, 1985, 370, 731, 307, 570, 309, 311, 8895, 322, 257, 688, 295, 1412, 13, 51380], "temperature": 0.0, "avg_logprob": -0.10534181180207626, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0009847769979387522}, {"id": 651, "seek": 279956, "start": 2820.68, "end": 2824.52, "text": " You can do that more efficiently because you have sliding window attention. So you don't need to", "tokens": [51420, 509, 393, 360, 300, 544, 19621, 570, 291, 362, 21169, 4910, 3202, 13, 407, 291, 500, 380, 643, 281, 51612], "temperature": 0.0, "avg_logprob": -0.10534181180207626, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.0009847769979387522}, {"id": 652, "seek": 282452, "start": 2824.52, "end": 2831.96, "text": " attend to everything. But so to answer your question, I guess you're asking sort of about the", "tokens": [50364, 6888, 281, 1203, 13, 583, 370, 281, 1867, 428, 1168, 11, 286, 2041, 291, 434, 3365, 1333, 295, 466, 264, 50736], "temperature": 0.0, "avg_logprob": -0.10411157915669103, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0038822449278086424}, {"id": 653, "seek": 282452, "start": 2831.96, "end": 2837.64, "text": " architecture of the generator if you know that there's going to be a retriever. So I think", "tokens": [50736, 9482, 295, 264, 19265, 498, 291, 458, 300, 456, 311, 516, 281, 312, 257, 19817, 331, 13, 407, 286, 519, 51020], "temperature": 0.0, "avg_logprob": -0.10411157915669103, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0038822449278086424}, {"id": 654, "seek": 282452, "start": 2838.36, "end": 2845.56, "text": " that's basically what Retro tried to do. So Retro actually, some of the people on the Retro paper", "tokens": [51056, 300, 311, 1936, 437, 11495, 340, 3031, 281, 360, 13, 407, 11495, 340, 767, 11, 512, 295, 264, 561, 322, 264, 11495, 340, 3035, 51416], "temperature": 0.0, "avg_logprob": -0.10411157915669103, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0038822449278086424}, {"id": 655, "seek": 282452, "start": 2845.56, "end": 2852.7599999999998, "text": " are at Mistral now. So they have this chunk cross attention idea here. So you basically", "tokens": [51416, 366, 412, 20166, 2155, 586, 13, 407, 436, 362, 341, 16635, 3278, 3202, 1558, 510, 13, 407, 291, 1936, 51776], "temperature": 0.0, "avg_logprob": -0.10411157915669103, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0038822449278086424}, {"id": 656, "seek": 285276, "start": 2852.76, "end": 2857.8, "text": " have a language model, but the way it does attention over the things you retrieve in your", "tokens": [50364, 362, 257, 2856, 2316, 11, 457, 264, 636, 309, 775, 3202, 670, 264, 721, 291, 30254, 294, 428, 50616], "temperature": 0.0, "avg_logprob": -0.10870339230793279, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.00027798957307823}, {"id": 657, "seek": 285276, "start": 2857.8, "end": 2866.28, "text": " Retro architecture, they kind of get integrated into a model, not using the standard attention", "tokens": [50616, 11495, 340, 9482, 11, 436, 733, 295, 483, 10919, 666, 257, 2316, 11, 406, 1228, 264, 3832, 3202, 51040], "temperature": 0.0, "avg_logprob": -0.10870339230793279, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.00027798957307823}, {"id": 658, "seek": 285276, "start": 2866.28, "end": 2873.1600000000003, "text": " mechanism, but using this slightly different chunk cross attention. Okay. So I think the", "tokens": [51040, 7513, 11, 457, 1228, 341, 4748, 819, 16635, 3278, 3202, 13, 1033, 13, 407, 286, 519, 264, 51384], "temperature": 0.0, "avg_logprob": -0.10870339230793279, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.00027798957307823}, {"id": 659, "seek": 285276, "start": 2873.1600000000003, "end": 2879.48, "text": " sliding window attention point I was trying to get at was that it uses a fixed window so that", "tokens": [51384, 21169, 4910, 3202, 935, 286, 390, 1382, 281, 483, 412, 390, 300, 309, 4960, 257, 6806, 4910, 370, 300, 51700], "temperature": 0.0, "avg_logprob": -0.10870339230793279, "compression_ratio": 1.68348623853211, "no_speech_prob": 0.00027798957307823}, {"id": 660, "seek": 287948, "start": 2879.48, "end": 2885.72, "text": " whenever you're doing the query key computation with the query vectors and the key vectors,", "tokens": [50364, 5699, 291, 434, 884, 264, 14581, 2141, 24903, 365, 264, 14581, 18875, 293, 264, 2141, 18875, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09439705264183783, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0012062187306582928}, {"id": 661, "seek": 287948, "start": 2885.72, "end": 2893.48, "text": " you're using a fixed window attention. So I think my idea was to actually, one, use a dynamic window", "tokens": [50676, 291, 434, 1228, 257, 6806, 4910, 3202, 13, 407, 286, 519, 452, 1558, 390, 281, 767, 11, 472, 11, 764, 257, 8546, 4910, 51064], "temperature": 0.0, "avg_logprob": -0.09439705264183783, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0012062187306582928}, {"id": 662, "seek": 287948, "start": 2893.48, "end": 2900.36, "text": " because for example, the rag case, if you use a fixed window when you're doing attention, it is", "tokens": [51064, 570, 337, 1365, 11, 264, 17539, 1389, 11, 498, 291, 764, 257, 6806, 4910, 562, 291, 434, 884, 3202, 11, 309, 307, 51408], "temperature": 0.0, "avg_logprob": -0.09439705264183783, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0012062187306582928}, {"id": 663, "seek": 287948, "start": 2900.36, "end": 2907.2400000000002, "text": " possible that you actually are leaving, you're only looking at a fixed span of information. So if you", "tokens": [51408, 1944, 300, 291, 767, 366, 5012, 11, 291, 434, 787, 1237, 412, 257, 6806, 16174, 295, 1589, 13, 407, 498, 291, 51752], "temperature": 0.0, "avg_logprob": -0.09439705264183783, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0012062187306582928}, {"id": 664, "seek": 290724, "start": 2907.24, "end": 2913.3199999999997, "text": " could maybe adapt Mistral so that you could make it better for the rag case in, for example,", "tokens": [50364, 727, 1310, 6231, 20166, 2155, 370, 300, 291, 727, 652, 309, 1101, 337, 264, 17539, 1389, 294, 11, 337, 1365, 11, 50668], "temperature": 0.0, "avg_logprob": -0.19504623089806508, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0004304682952351868}, {"id": 665, "seek": 290724, "start": 2913.3199999999997, "end": 2919.3999999999996, "text": " the making the fixed window size, the dynamic window. Yeah. Yeah, I think it's an interesting", "tokens": [50668, 264, 1455, 264, 6806, 4910, 2744, 11, 264, 8546, 4910, 13, 865, 13, 865, 11, 286, 519, 309, 311, 364, 1880, 50972], "temperature": 0.0, "avg_logprob": -0.19504623089806508, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0004304682952351868}, {"id": 666, "seek": 290724, "start": 2919.3999999999996, "end": 2926.8399999999997, "text": " idea. So for me, what Mistral is doing with the sliding window, that's basically like a confnet.", "tokens": [50972, 1558, 13, 407, 337, 385, 11, 437, 20166, 2155, 307, 884, 365, 264, 21169, 4910, 11, 300, 311, 1936, 411, 257, 1497, 7129, 13, 51344], "temperature": 0.0, "avg_logprob": -0.19504623089806508, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0004304682952351868}, {"id": 667, "seek": 290724, "start": 2927.3999999999996, "end": 2932.2799999999997, "text": " So we had all these convolutional like light confnets where we would have word embeddings,", "tokens": [51372, 407, 321, 632, 439, 613, 45216, 304, 411, 1442, 1497, 77, 1385, 689, 321, 576, 362, 1349, 12240, 29432, 11, 51616], "temperature": 0.0, "avg_logprob": -0.19504623089806508, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0004304682952351868}, {"id": 668, "seek": 290724, "start": 2932.2799999999997, "end": 2937.08, "text": " and you would do confolutions over it and then pull, and then you would still get the information", "tokens": [51616, 293, 291, 576, 360, 1497, 15892, 670, 309, 293, 550, 2235, 11, 293, 550, 291, 576, 920, 483, 264, 1589, 51856], "temperature": 0.0, "avg_logprob": -0.19504623089806508, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0004304682952351868}, {"id": 669, "seek": 293708, "start": 2937.08, "end": 2942.12, "text": " out. So it's not that the sliding window prohibits you from looking earlier, it's just that that", "tokens": [50364, 484, 13, 407, 309, 311, 406, 300, 264, 21169, 4910, 16015, 1208, 291, 490, 1237, 3071, 11, 309, 311, 445, 300, 300, 50616], "temperature": 0.0, "avg_logprob": -0.1259064071479885, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0005031073815189302}, {"id": 670, "seek": 293708, "start": 2942.12, "end": 2950.7599999999998, "text": " happens higher up in your transformers. So I think that definitely is an interesting", "tokens": [50616, 2314, 2946, 493, 294, 428, 4088, 433, 13, 407, 286, 519, 300, 2138, 307, 364, 1880, 51048], "temperature": 0.0, "avg_logprob": -0.1259064071479885, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0005031073815189302}, {"id": 671, "seek": 293708, "start": 2950.7599999999998, "end": 2958.44, "text": " direction to think in. Yeah, so I think it's like not too crazy to say, are there any architectural", "tokens": [51048, 3513, 281, 519, 294, 13, 865, 11, 370, 286, 519, 309, 311, 411, 406, 886, 3219, 281, 584, 11, 366, 456, 604, 26621, 51432], "temperature": 0.0, "avg_logprob": -0.1259064071479885, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0005031073815189302}, {"id": 672, "seek": 293708, "start": 2958.44, "end": 2963.3199999999997, "text": " changes that we can introduce into these seven billion parameter models so that they could be", "tokens": [51432, 2962, 300, 321, 393, 5366, 666, 613, 3407, 5218, 13075, 5245, 370, 300, 436, 727, 312, 51676], "temperature": 0.0, "avg_logprob": -0.1259064071479885, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.0005031073815189302}, {"id": 673, "seek": 296332, "start": 2963.32, "end": 2972.2000000000003, "text": " better adapted to the rag case? Yeah, so there might be. Yeah, I think one question is just how", "tokens": [50364, 1101, 20871, 281, 264, 17539, 1389, 30, 865, 11, 370, 456, 1062, 312, 13, 865, 11, 286, 519, 472, 1168, 307, 445, 577, 50808], "temperature": 0.0, "avg_logprob": -0.24563094189292506, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.002017911756411195}, {"id": 674, "seek": 296332, "start": 2972.2000000000003, "end": 2978.76, "text": " do you do the attention over things you've retrieved, which is what you're doing. Yeah, thanks.", "tokens": [50808, 360, 291, 360, 264, 3202, 670, 721, 291, 600, 19817, 937, 11, 597, 307, 437, 291, 434, 884, 13, 865, 11, 3231, 13, 51136], "temperature": 0.0, "avg_logprob": -0.24563094189292506, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.002017911756411195}, {"id": 675, "seek": 296332, "start": 2980.6000000000004, "end": 2987.56, "text": " So just to make sure I understand, I mean in this retro model, you're retrieving each block,", "tokens": [51228, 407, 445, 281, 652, 988, 286, 1223, 11, 286, 914, 294, 341, 18820, 2316, 11, 291, 434, 19817, 798, 1184, 3461, 11, 51576], "temperature": 0.0, "avg_logprob": -0.24563094189292506, "compression_ratio": 1.5519125683060109, "no_speech_prob": 0.002017911756411195}, {"id": 676, "seek": 298756, "start": 2988.52, "end": 2993.96, "text": " and when you struggle about putting a retrieval in the context, are you saying that you'll need to", "tokens": [50412, 293, 562, 291, 7799, 466, 3372, 257, 19817, 3337, 294, 264, 4319, 11, 366, 291, 1566, 300, 291, 603, 643, 281, 50684], "temperature": 0.0, "avg_logprob": -0.2821500973823743, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.009116237051784992}, {"id": 677, "seek": 298756, "start": 2993.96, "end": 3000.52, "text": " do it at the beginning and you don't do it at the block? Yeah, so in context, so it's not exactly", "tokens": [50684, 360, 309, 412, 264, 2863, 293, 291, 500, 380, 360, 309, 412, 264, 3461, 30, 865, 11, 370, 294, 4319, 11, 370, 309, 311, 406, 2293, 51012], "temperature": 0.0, "avg_logprob": -0.2821500973823743, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.009116237051784992}, {"id": 678, "seek": 298756, "start": 3000.52, "end": 3009.16, "text": " every layer, so it's every token, so every step basically, not every block, so it doesn't make", "tokens": [51012, 633, 4583, 11, 370, 309, 311, 633, 14862, 11, 370, 633, 1823, 1936, 11, 406, 633, 3461, 11, 370, 309, 1177, 380, 652, 51444], "temperature": 0.0, "avg_logprob": -0.2821500973823743, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.009116237051784992}, {"id": 679, "seek": 300916, "start": 3009.24, "end": 3018.3599999999997, "text": " sense. So it's not every layer that you're doing a retrieval. Yeah, so every step. So this is kind", "tokens": [50368, 2020, 13, 407, 309, 311, 406, 633, 4583, 300, 291, 434, 884, 257, 19817, 3337, 13, 865, 11, 370, 633, 1823, 13, 407, 341, 307, 733, 50824], "temperature": 0.0, "avg_logprob": -0.1494754903456744, "compression_ratio": 1.9158415841584158, "no_speech_prob": 0.0271466001868248}, {"id": 680, "seek": 300916, "start": 3018.3599999999997, "end": 3024.6, "text": " of like what rag token is, so you retrieve every token, so you generate and then you can retrieve", "tokens": [50824, 295, 411, 437, 17539, 14862, 307, 11, 370, 291, 30254, 633, 14862, 11, 370, 291, 8460, 293, 550, 291, 393, 30254, 51136], "temperature": 0.0, "avg_logprob": -0.1494754903456744, "compression_ratio": 1.9158415841584158, "no_speech_prob": 0.0271466001868248}, {"id": 681, "seek": 300916, "start": 3024.6, "end": 3029.64, "text": " again, or in the case of retro, you can generate like a chunk and then you retrieve chunks again.", "tokens": [51136, 797, 11, 420, 294, 264, 1389, 295, 18820, 11, 291, 393, 8460, 411, 257, 16635, 293, 550, 291, 30254, 24004, 797, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1494754903456744, "compression_ratio": 1.9158415841584158, "no_speech_prob": 0.0271466001868248}, {"id": 682, "seek": 300916, "start": 3031.08, "end": 3035.48, "text": " If you look at the in-context case, you retrieve once at the beginning and then you give it.", "tokens": [51460, 759, 291, 574, 412, 264, 294, 12, 9000, 3828, 1389, 11, 291, 30254, 1564, 412, 264, 2863, 293, 550, 291, 976, 309, 13, 51680], "temperature": 0.0, "avg_logprob": -0.1494754903456744, "compression_ratio": 1.9158415841584158, "no_speech_prob": 0.0271466001868248}, {"id": 683, "seek": 303548, "start": 3036.2, "end": 3051.8, "text": " So here you don't actually give it as context at all, like directly to the model, right,", "tokens": [50400, 407, 510, 291, 500, 380, 767, 976, 309, 382, 4319, 412, 439, 11, 411, 3838, 281, 264, 2316, 11, 558, 11, 51180], "temperature": 0.0, "avg_logprob": -0.2708141803741455, "compression_ratio": 1.2962962962962963, "no_speech_prob": 0.002285885624587536}, {"id": 684, "seek": 303548, "start": 3051.8, "end": 3056.04, "text": " so here you let the decoder kind of attend over it.", "tokens": [51180, 370, 510, 291, 718, 264, 979, 19866, 733, 295, 6888, 670, 309, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2708141803741455, "compression_ratio": 1.2962962962962963, "no_speech_prob": 0.002285885624587536}, {"id": 685, "seek": 305604, "start": 3056.04, "end": 3066.6, "text": " Yeah, so I don't think cross-attention really works, yeah.", "tokens": [50364, 865, 11, 370, 286, 500, 380, 519, 3278, 12, 1591, 1251, 534, 1985, 11, 1338, 13, 50892], "temperature": 0.0, "avg_logprob": -0.4276617765426636, "compression_ratio": 1.3089430894308942, "no_speech_prob": 0.006793972570449114}, {"id": 686, "seek": 305604, "start": 3071.8, "end": 3072.6, "text": " Other questions?", "tokens": [51152, 5358, 1651, 30, 51192], "temperature": 0.0, "avg_logprob": -0.4276617765426636, "compression_ratio": 1.3089430894308942, "no_speech_prob": 0.006793972570449114}, {"id": 687, "seek": 305604, "start": 3073.8, "end": 3079.56, "text": " Yeah, inside the in-context case, the retrieving of the retriever is not necessarily,", "tokens": [51252, 865, 11, 1854, 264, 294, 12, 9000, 3828, 1389, 11, 264, 19817, 798, 295, 264, 19817, 331, 307, 406, 4725, 11, 51540], "temperature": 0.0, "avg_logprob": -0.4276617765426636, "compression_ratio": 1.3089430894308942, "no_speech_prob": 0.006793972570449114}, {"id": 688, "seek": 307956, "start": 3079.72, "end": 3086.84, "text": " because of the large distribution loss, so I'm wondering inside of the cases, like what cases", "tokens": [50372, 570, 295, 264, 2416, 7316, 4470, 11, 370, 286, 478, 6359, 1854, 295, 264, 3331, 11, 411, 437, 3331, 50728], "temperature": 0.0, "avg_logprob": -0.3556905829388162, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0021821700502187014}, {"id": 689, "seek": 307956, "start": 3087.64, "end": 3094.12, "text": " are really necessary need to evenize updates, or anyways updates for this argument.", "tokens": [50768, 366, 534, 4818, 643, 281, 754, 1125, 9205, 11, 420, 13448, 9205, 337, 341, 6770, 13, 51092], "temperature": 0.0, "avg_logprob": -0.3556905829388162, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0021821700502187014}, {"id": 690, "seek": 307956, "start": 3095.16, "end": 3100.52, "text": " Yeah, so you do want to update the retriever, right, but only part of the retriever is necessary to", "tokens": [51144, 865, 11, 370, 291, 360, 528, 281, 5623, 264, 19817, 331, 11, 558, 11, 457, 787, 644, 295, 264, 19817, 331, 307, 4818, 281, 51412], "temperature": 0.0, "avg_logprob": -0.3556905829388162, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0021821700502187014}, {"id": 691, "seek": 307956, "start": 3100.52, "end": 3109.4, "text": " be updated for a lot of these cases, but so I think it, so these are very specific data sets,", "tokens": [51412, 312, 10588, 337, 257, 688, 295, 613, 3331, 11, 457, 370, 286, 519, 309, 11, 370, 613, 366, 588, 2685, 1412, 6352, 11, 51856], "temperature": 0.0, "avg_logprob": -0.3556905829388162, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.0021821700502187014}, {"id": 692, "seek": 310940, "start": 3109.4, "end": 3114.92, "text": " right, natural questions, Wizard of Wikipedia and Fever, so they're really very kind of knowledge", "tokens": [50364, 558, 11, 3303, 1651, 11, 37449, 295, 28999, 293, 479, 1054, 11, 370, 436, 434, 534, 588, 733, 295, 3601, 50640], "temperature": 0.0, "avg_logprob": -0.10361457693165746, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0006459593423642218}, {"id": 693, "seek": 310940, "start": 3114.92, "end": 3121.64, "text": " intensive tasks, so in that case, if you already have a very good system like DPR that is specifically", "tokens": [50640, 18957, 9608, 11, 370, 294, 300, 1389, 11, 498, 291, 1217, 362, 257, 588, 665, 1185, 411, 413, 15958, 300, 307, 4682, 50976], "temperature": 0.0, "avg_logprob": -0.10361457693165746, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0006459593423642218}, {"id": 694, "seek": 310940, "start": 3121.64, "end": 3127.64, "text": " pre-trained for those tasks, then you only need to update the query encoder, but so I would expect", "tokens": [50976, 659, 12, 17227, 2001, 337, 729, 9608, 11, 550, 291, 787, 643, 281, 5623, 264, 14581, 2058, 19866, 11, 457, 370, 286, 576, 2066, 51276], "temperature": 0.0, "avg_logprob": -0.10361457693165746, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0006459593423642218}, {"id": 695, "seek": 310940, "start": 3127.64, "end": 3133.32, "text": " that if you move beyond this to kind of general language modeling things like retro, then you", "tokens": [51276, 300, 498, 291, 1286, 4399, 341, 281, 733, 295, 2674, 2856, 15983, 721, 411, 18820, 11, 550, 291, 51560], "temperature": 0.0, "avg_logprob": -0.10361457693165746, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0006459593423642218}, {"id": 696, "seek": 310940, "start": 3133.32, "end": 3138.2000000000003, "text": " probably do want to update the document encoder, at least in a way where you can skate it.", "tokens": [51560, 1391, 360, 528, 281, 5623, 264, 4166, 2058, 19866, 11, 412, 1935, 294, 257, 636, 689, 291, 393, 18237, 309, 13, 51804], "temperature": 0.0, "avg_logprob": -0.10361457693165746, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.0006459593423642218}, {"id": 697, "seek": 313940, "start": 3139.4, "end": 3147.32, "text": " So I believe that in this part, it's very knowledge intensive, and actually a couple of", "tokens": [50364, 407, 286, 1697, 300, 294, 341, 644, 11, 309, 311, 588, 3601, 18957, 11, 293, 767, 257, 1916, 295, 50760], "temperature": 0.0, "avg_logprob": -0.506846912090595, "compression_ratio": 1.4748603351955307, "no_speech_prob": 0.004068111069500446}, {"id": 698, "seek": 313940, "start": 3148.12, "end": 3157.08, "text": " very important topics, as long as we have a good office around knowledge of what", "tokens": [50800, 588, 1021, 8378, 11, 382, 938, 382, 321, 362, 257, 665, 3398, 926, 3601, 295, 437, 51248], "temperature": 0.0, "avg_logprob": -0.506846912090595, "compression_ratio": 1.4748603351955307, "no_speech_prob": 0.004068111069500446}, {"id": 699, "seek": 313940, "start": 3157.96, "end": 3166.6, "text": " many of the documents by those good models. Yeah, but so you need to learn how to kind of query", "tokens": [51292, 867, 295, 264, 8512, 538, 729, 665, 5245, 13, 865, 11, 457, 370, 291, 643, 281, 1466, 577, 281, 733, 295, 14581, 51724], "temperature": 0.0, "avg_logprob": -0.506846912090595, "compression_ratio": 1.4748603351955307, "no_speech_prob": 0.004068111069500446}, {"id": 700, "seek": 316660, "start": 3166.6, "end": 3173.4, "text": " into that index, right, so if you don't do that, then yeah, you don't get really good", "tokens": [50364, 666, 300, 8186, 11, 558, 11, 370, 498, 291, 500, 380, 360, 300, 11, 550, 1338, 11, 291, 500, 380, 483, 534, 665, 50704], "temperature": 0.0, "avg_logprob": -0.14589100887900905, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.005999862216413021}, {"id": 701, "seek": 316660, "start": 3173.4, "end": 3177.88, "text": " performance, so that's sort of like your closed book performance, right, if you just have the", "tokens": [50704, 3389, 11, 370, 300, 311, 1333, 295, 411, 428, 5395, 1446, 3389, 11, 558, 11, 498, 291, 445, 362, 264, 50928], "temperature": 0.0, "avg_logprob": -0.14589100887900905, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.005999862216413021}, {"id": 702, "seek": 316660, "start": 3177.88, "end": 3182.68, "text": " language model and you're just like, what does the parametric model on its own without the", "tokens": [50928, 2856, 2316, 293, 291, 434, 445, 411, 11, 437, 775, 264, 6220, 17475, 2316, 322, 1080, 1065, 1553, 264, 51168], "temperature": 0.0, "avg_logprob": -0.14589100887900905, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.005999862216413021}, {"id": 703, "seek": 316660, "start": 3182.68, "end": 3187.4, "text": " retrieval, what does it actually know? As you can see, there are pretty big gaps there, right.", "tokens": [51168, 19817, 3337, 11, 437, 775, 309, 767, 458, 30, 1018, 291, 393, 536, 11, 456, 366, 1238, 955, 15031, 456, 11, 558, 13, 51404], "temperature": 0.0, "avg_logprob": -0.14589100887900905, "compression_ratio": 1.721698113207547, "no_speech_prob": 0.005999862216413021}, {"id": 704, "seek": 318740, "start": 3187.7200000000003, "end": 3195.32, "text": " Other questions? Otherwise, I will cover other questions.", "tokens": [50380, 5358, 1651, 30, 10328, 11, 286, 486, 2060, 661, 1651, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2616312617347354, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.005908007267862558}, {"id": 705, "seek": 318740, "start": 3198.84, "end": 3205.88, "text": " No? Hello? Yeah, go for it. Okay, question, like so what about like more hierarchical retrieval,", "tokens": [50936, 883, 30, 2425, 30, 865, 11, 352, 337, 309, 13, 1033, 11, 1168, 11, 411, 370, 437, 466, 411, 544, 35250, 804, 19817, 3337, 11, 51288], "temperature": 0.0, "avg_logprob": -0.2616312617347354, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.005908007267862558}, {"id": 706, "seek": 318740, "start": 3205.88, "end": 3210.12, "text": " like I suppose there'll be methods trying to not just retrieve a single chunk, but there's some", "tokens": [51288, 411, 286, 7297, 456, 603, 312, 7150, 1382, 281, 406, 445, 30254, 257, 2167, 16635, 11, 457, 456, 311, 512, 51500], "temperature": 0.0, "avg_logprob": -0.2616312617347354, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.005908007267862558}, {"id": 707, "seek": 318740, "start": 3210.12, "end": 3215.1600000000003, "text": " kind of like groups of chunks or something, or some rise versions. There's been some", "tokens": [51500, 733, 295, 411, 3935, 295, 24004, 420, 746, 11, 420, 512, 6272, 9606, 13, 821, 311, 668, 512, 51752], "temperature": 0.0, "avg_logprob": -0.2616312617347354, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.005908007267862558}, {"id": 708, "seek": 321516, "start": 3215.16, "end": 3220.04, "text": " interesting work on doing that where you first tried to find, so you can have multiple indices", "tokens": [50364, 1880, 589, 322, 884, 300, 689, 291, 700, 3031, 281, 915, 11, 370, 291, 393, 362, 3866, 43840, 50608], "temperature": 0.0, "avg_logprob": -0.11166836723448738, "compression_ratio": 2.045801526717557, "no_speech_prob": 0.001225228887051344}, {"id": 709, "seek": 321516, "start": 3220.04, "end": 3223.72, "text": " and they can kind of cascade, right, so first you want to find the relevant document,", "tokens": [50608, 293, 436, 393, 733, 295, 50080, 11, 558, 11, 370, 700, 291, 528, 281, 915, 264, 7340, 4166, 11, 50792], "temperature": 0.0, "avg_logprob": -0.11166836723448738, "compression_ratio": 2.045801526717557, "no_speech_prob": 0.001225228887051344}, {"id": 710, "seek": 321516, "start": 3223.72, "end": 3227.48, "text": " so you have some document representation and then within that document you want to find the", "tokens": [50792, 370, 291, 362, 512, 4166, 10290, 293, 550, 1951, 300, 4166, 291, 528, 281, 915, 264, 50980], "temperature": 0.0, "avg_logprob": -0.11166836723448738, "compression_ratio": 2.045801526717557, "no_speech_prob": 0.001225228887051344}, {"id": 711, "seek": 321516, "start": 3227.48, "end": 3233.0, "text": " relevant chunk, so you can do it sort of that direction, you can also do it in reverse, I think", "tokens": [50980, 7340, 16635, 11, 370, 291, 393, 360, 309, 1333, 295, 300, 3513, 11, 291, 393, 611, 360, 309, 294, 9943, 11, 286, 519, 51256], "temperature": 0.0, "avg_logprob": -0.11166836723448738, "compression_ratio": 2.045801526717557, "no_speech_prob": 0.001225228887051344}, {"id": 712, "seek": 321516, "start": 3233.0, "end": 3237.48, "text": " I have something on a slide there where you can find the chunk and then sort of expand", "tokens": [51256, 286, 362, 746, 322, 257, 4137, 456, 689, 291, 393, 915, 264, 16635, 293, 550, 1333, 295, 5268, 51480], "temperature": 0.0, "avg_logprob": -0.11166836723448738, "compression_ratio": 2.045801526717557, "no_speech_prob": 0.001225228887051344}, {"id": 713, "seek": 321516, "start": 3238.3599999999997, "end": 3244.12, "text": " the context around it and then give that to the language model. So I think yeah,", "tokens": [51524, 264, 4319, 926, 309, 293, 550, 976, 300, 281, 264, 2856, 2316, 13, 407, 286, 519, 1338, 11, 51812], "temperature": 0.0, "avg_logprob": -0.11166836723448738, "compression_ratio": 2.045801526717557, "no_speech_prob": 0.001225228887051344}, {"id": 714, "seek": 324412, "start": 3244.12, "end": 3246.2, "text": " there are all kinds of interesting things you can do there.", "tokens": [50364, 456, 366, 439, 3685, 295, 1880, 721, 291, 393, 360, 456, 13, 50468], "temperature": 0.0, "avg_logprob": -0.21603845200448665, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.0007914169109426439}, {"id": 715, "seek": 324412, "start": 3248.3599999999997, "end": 3254.52, "text": " Cool, thanks, I guess another thing just like can you compare RAG versus like long context", "tokens": [50576, 8561, 11, 3231, 11, 286, 2041, 1071, 551, 445, 411, 393, 291, 6794, 14626, 38, 5717, 411, 938, 4319, 50884], "temperature": 0.0, "avg_logprob": -0.21603845200448665, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.0007914169109426439}, {"id": 716, "seek": 324412, "start": 3254.52, "end": 3260.2, "text": " so efforts, so like there are lots of things like around just having a really long context and the", "tokens": [50884, 370, 6484, 11, 370, 411, 456, 366, 3195, 295, 721, 411, 926, 445, 1419, 257, 534, 938, 4319, 293, 264, 51168], "temperature": 0.0, "avg_logprob": -0.21603845200448665, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.0007914169109426439}, {"id": 717, "seek": 324412, "start": 3260.2, "end": 3268.12, "text": " extreme it could replace RAG, but I don't know like if it takes. Yeah, so everybody understands", "tokens": [51168, 8084, 309, 727, 7406, 14626, 38, 11, 457, 286, 500, 380, 458, 411, 498, 309, 2516, 13, 865, 11, 370, 2201, 15146, 51564], "temperature": 0.0, "avg_logprob": -0.21603845200448665, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.0007914169109426439}, {"id": 718, "seek": 324412, "start": 3268.12, "end": 3273.16, "text": " this question, right, so there's a trend where we want to have very long context language models,", "tokens": [51564, 341, 1168, 11, 558, 11, 370, 456, 311, 257, 6028, 689, 321, 528, 281, 362, 588, 938, 4319, 2856, 5245, 11, 51816], "temperature": 0.0, "avg_logprob": -0.21603845200448665, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.0007914169109426439}, {"id": 719, "seek": 327316, "start": 3273.16, "end": 3277.96, "text": " so that basically you can like take Harry Potter or something, just put it in the context and then", "tokens": [50364, 370, 300, 1936, 291, 393, 411, 747, 9378, 18115, 420, 746, 11, 445, 829, 309, 294, 264, 4319, 293, 550, 50604], "temperature": 0.0, "avg_logprob": -0.10818624496459961, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0006260277004912496}, {"id": 720, "seek": 327316, "start": 3277.96, "end": 3282.92, "text": " ask a question like what is the name of like Harry Potter's owl or something, right, and then it can", "tokens": [50604, 1029, 257, 1168, 411, 437, 307, 264, 1315, 295, 411, 9378, 18115, 311, 34488, 420, 746, 11, 558, 11, 293, 550, 309, 393, 50852], "temperature": 0.0, "avg_logprob": -0.10818624496459961, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0006260277004912496}, {"id": 721, "seek": 327316, "start": 3282.92, "end": 3289.3199999999997, "text": " just attend over the entire thing. So attending over all of Harry Potter to answer that one question", "tokens": [50852, 445, 6888, 670, 264, 2302, 551, 13, 407, 15862, 670, 439, 295, 9378, 18115, 281, 1867, 300, 472, 1168, 51172], "temperature": 0.0, "avg_logprob": -0.10818624496459961, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0006260277004912496}, {"id": 722, "seek": 327316, "start": 3289.3199999999997, "end": 3295.7999999999997, "text": " is super inefficient, right, so most of Harry Potter has nothing to do with the owl, so but you", "tokens": [51172, 307, 1687, 43495, 11, 558, 11, 370, 881, 295, 9378, 18115, 575, 1825, 281, 360, 365, 264, 34488, 11, 370, 457, 291, 51496], "temperature": 0.0, "avg_logprob": -0.10818624496459961, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0006260277004912496}, {"id": 723, "seek": 327316, "start": 3295.7999999999997, "end": 3301.08, "text": " are still kind of reading it if you do it with the long context window, so that's why I think", "tokens": [51496, 366, 920, 733, 295, 3760, 309, 498, 291, 360, 309, 365, 264, 938, 4319, 4910, 11, 370, 300, 311, 983, 286, 519, 51760], "temperature": 0.0, "avg_logprob": -0.10818624496459961, "compression_ratio": 1.8918918918918919, "no_speech_prob": 0.0006260277004912496}, {"id": 724, "seek": 330108, "start": 3301.08, "end": 3306.2, "text": " the doing it the RAG way where you have this non-parametric component is a much more efficient", "tokens": [50364, 264, 884, 309, 264, 14626, 38, 636, 689, 291, 362, 341, 2107, 12, 2181, 335, 17475, 6542, 307, 257, 709, 544, 7148, 50620], "temperature": 0.0, "avg_logprob": -0.09489997502030997, "compression_ratio": 1.865612648221344, "no_speech_prob": 0.0047532059252262115}, {"id": 725, "seek": 330108, "start": 3306.2, "end": 3310.92, "text": " way to solve this problem, and if you actually look at the literature on long context windows,", "tokens": [50620, 636, 281, 5039, 341, 1154, 11, 293, 498, 291, 767, 574, 412, 264, 10394, 322, 938, 4319, 9309, 11, 50856], "temperature": 0.0, "avg_logprob": -0.09489997502030997, "compression_ratio": 1.865612648221344, "no_speech_prob": 0.0047532059252262115}, {"id": 726, "seek": 330108, "start": 3311.7999999999997, "end": 3317.7999999999997, "text": " the way they solve the problem of scaling the attention mechanism is by making it very sparse,", "tokens": [50900, 264, 636, 436, 5039, 264, 1154, 295, 21589, 264, 3202, 7513, 307, 538, 1455, 309, 588, 637, 11668, 11, 51200], "temperature": 0.0, "avg_logprob": -0.09489997502030997, "compression_ratio": 1.865612648221344, "no_speech_prob": 0.0047532059252262115}, {"id": 727, "seek": 330108, "start": 3318.44, "end": 3322.6, "text": " so they're basically turning it in, so that's a different kind of sparse, but they're turning", "tokens": [51232, 370, 436, 434, 1936, 6246, 309, 294, 11, 370, 300, 311, 257, 819, 733, 295, 637, 11668, 11, 457, 436, 434, 6246, 51440], "temperature": 0.0, "avg_logprob": -0.09489997502030997, "compression_ratio": 1.865612648221344, "no_speech_prob": 0.0047532059252262115}, {"id": 728, "seek": 330108, "start": 3322.6, "end": 3328.2, "text": " it into a non-parametric retrieval problem kind of behind the scenes, so they're not actually", "tokens": [51440, 309, 666, 257, 2107, 12, 2181, 335, 17475, 19817, 3337, 1154, 733, 295, 2261, 264, 8026, 11, 370, 436, 434, 406, 767, 51720], "temperature": 0.0, "avg_logprob": -0.09489997502030997, "compression_ratio": 1.865612648221344, "no_speech_prob": 0.0047532059252262115}, {"id": 729, "seek": 332820, "start": 3328.2, "end": 3332.4399999999996, "text": " all that different, if you want to scale long context then you're going to move towards a RAG-style", "tokens": [50364, 439, 300, 819, 11, 498, 291, 528, 281, 4373, 938, 4319, 550, 291, 434, 516, 281, 1286, 3030, 257, 14626, 38, 12, 15014, 50576], "temperature": 0.0, "avg_logprob": -0.12685013865376568, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.0017261627363041043}, {"id": 730, "seek": 332820, "start": 3332.4399999999996, "end": 3342.6, "text": " architecture. Cool, thanks. All right, so let's talk about some other interesting questions,", "tokens": [50576, 9482, 13, 8561, 11, 3231, 13, 1057, 558, 11, 370, 718, 311, 751, 466, 512, 661, 1880, 1651, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12685013865376568, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.0017261627363041043}, {"id": 731, "seek": 332820, "start": 3343.24, "end": 3349.56, "text": " so one thing and I already alluded to this is when do we actually retrieve, so very if we're", "tokens": [51116, 370, 472, 551, 293, 286, 1217, 33919, 281, 341, 307, 562, 360, 321, 767, 30254, 11, 370, 588, 498, 321, 434, 51432], "temperature": 0.0, "avg_logprob": -0.12685013865376568, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.0017261627363041043}, {"id": 732, "seek": 332820, "start": 3349.56, "end": 3355.72, "text": " doing like if we want to like retrieve every token that's also very inefficient because I probably", "tokens": [51432, 884, 411, 498, 321, 528, 281, 411, 30254, 633, 14862, 300, 311, 611, 588, 43495, 570, 286, 1391, 51740], "temperature": 0.0, "avg_logprob": -0.12685013865376568, "compression_ratio": 1.5802469135802468, "no_speech_prob": 0.0017261627363041043}, {"id": 733, "seek": 335572, "start": 3355.72, "end": 3361.48, "text": " don't have to retrieve to generate the, right, I can probably do that on my own with the language", "tokens": [50364, 500, 380, 362, 281, 30254, 281, 8460, 264, 11, 558, 11, 286, 393, 1391, 360, 300, 322, 452, 1065, 365, 264, 2856, 50652], "temperature": 0.0, "avg_logprob": -0.08628757600861836, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.0042618876323103905}, {"id": 734, "seek": 335572, "start": 3361.48, "end": 3367.16, "text": " model, it's sort of a waste to go and retrieve stuff, but if I only retrieve once at the beginning", "tokens": [50652, 2316, 11, 309, 311, 1333, 295, 257, 5964, 281, 352, 293, 30254, 1507, 11, 457, 498, 286, 787, 30254, 1564, 412, 264, 2863, 50936], "temperature": 0.0, "avg_logprob": -0.08628757600861836, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.0042618876323103905}, {"id": 735, "seek": 335572, "start": 3367.16, "end": 3373.0, "text": " of the sequence that's probably also not great, so what we ideally want to be able to do is to say,", "tokens": [50936, 295, 264, 8310, 300, 311, 1391, 611, 406, 869, 11, 370, 437, 321, 22915, 528, 281, 312, 1075, 281, 360, 307, 281, 584, 11, 51228], "temperature": 0.0, "avg_logprob": -0.08628757600861836, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.0042618876323103905}, {"id": 736, "seek": 335572, "start": 3373.0, "end": 3376.68, "text": " okay sometimes I want to retrieve, sometimes I don't want to retrieve and I'm going to learn", "tokens": [51228, 1392, 2171, 286, 528, 281, 30254, 11, 2171, 286, 500, 380, 528, 281, 30254, 293, 286, 478, 516, 281, 1466, 51412], "temperature": 0.0, "avg_logprob": -0.08628757600861836, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.0042618876323103905}, {"id": 737, "seek": 335572, "start": 3376.68, "end": 3384.6, "text": " when I want to kind of expend the compute budget on doing the retrieval, so a nice paper where they", "tokens": [51412, 562, 286, 528, 281, 733, 295, 24439, 264, 14722, 4706, 322, 884, 264, 19817, 3337, 11, 370, 257, 1481, 3035, 689, 436, 51808], "temperature": 0.0, "avg_logprob": -0.08628757600861836, "compression_ratio": 1.8314606741573034, "no_speech_prob": 0.0042618876323103905}, {"id": 738, "seek": 338460, "start": 3384.6, "end": 3389.48, "text": " have a stab at, this is called FLARE for active retrieval augmentation where they basically have", "tokens": [50364, 362, 257, 16343, 412, 11, 341, 307, 1219, 24720, 28186, 337, 4967, 19817, 3337, 14501, 19631, 689, 436, 1936, 362, 50608], "temperature": 0.0, "avg_logprob": -0.09762510500456158, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0006459861760959029}, {"id": 739, "seek": 338460, "start": 3389.48, "end": 3394.8399999999997, "text": " the language model decide when it should do a search and what it should do the search for,", "tokens": [50608, 264, 2856, 2316, 4536, 562, 309, 820, 360, 257, 3164, 293, 437, 309, 820, 360, 264, 3164, 337, 11, 50876], "temperature": 0.0, "avg_logprob": -0.09762510500456158, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0006459861760959029}, {"id": 740, "seek": 338460, "start": 3396.52, "end": 3402.44, "text": " so I think this fits in a general trend that you can see in the field around kind of agents,", "tokens": [50960, 370, 286, 519, 341, 9001, 294, 257, 2674, 6028, 300, 291, 393, 536, 294, 264, 2519, 926, 733, 295, 12554, 11, 51256], "temperature": 0.0, "avg_logprob": -0.09762510500456158, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0006459861760959029}, {"id": 741, "seek": 338460, "start": 3402.44, "end": 3409.16, "text": " so we can talk a little bit more about that too. So this other question that I think we've also", "tokens": [51256, 370, 321, 393, 751, 257, 707, 857, 544, 466, 300, 886, 13, 407, 341, 661, 1168, 300, 286, 519, 321, 600, 611, 51592], "temperature": 0.0, "avg_logprob": -0.09762510500456158, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0006459861760959029}, {"id": 742, "seek": 338460, "start": 3409.16, "end": 3413.72, "text": " kind of covered already here is how do we train this at scale, so we can do these asynchronous", "tokens": [51592, 733, 295, 5343, 1217, 510, 307, 577, 360, 321, 3847, 341, 412, 4373, 11, 370, 321, 393, 360, 613, 49174, 51820], "temperature": 0.0, "avg_logprob": -0.09762510500456158, "compression_ratio": 1.7003610108303249, "no_speech_prob": 0.0006459861760959029}, {"id": 743, "seek": 341372, "start": 3413.72, "end": 3419.56, "text": " updates, we can do re-rankers, we can do query side only, there's this really nice paper which is", "tokens": [50364, 9205, 11, 321, 393, 360, 319, 12, 20479, 433, 11, 321, 393, 360, 14581, 1252, 787, 11, 456, 311, 341, 534, 1481, 3035, 597, 307, 50656], "temperature": 0.0, "avg_logprob": -0.085601806640625, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0011332783615216613}, {"id": 744, "seek": 341372, "start": 3419.56, "end": 3427.3999999999996, "text": " quite close I think to the idea you proposed where you first use BM25 to create a batch basically", "tokens": [50656, 1596, 1998, 286, 519, 281, 264, 1558, 291, 10348, 689, 291, 700, 764, 15901, 6074, 281, 1884, 257, 15245, 1936, 51048], "temperature": 0.0, "avg_logprob": -0.085601806640625, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0011332783615216613}, {"id": 745, "seek": 341372, "start": 3427.3999999999996, "end": 3434.4399999999996, "text": " where everything is very similar in terms of what you've retrieved and now you have this kind of", "tokens": [51048, 689, 1203, 307, 588, 2531, 294, 2115, 295, 437, 291, 600, 19817, 937, 293, 586, 291, 362, 341, 733, 295, 51400], "temperature": 0.0, "avg_logprob": -0.085601806640625, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0011332783615216613}, {"id": 746, "seek": 341372, "start": 3434.4399999999996, "end": 3439.48, "text": " in-batch update, so it's sort of like a re-ranker where you encode the information that is just in", "tokens": [51400, 294, 12, 65, 852, 5623, 11, 370, 309, 311, 1333, 295, 411, 257, 319, 12, 20479, 260, 689, 291, 2058, 1429, 264, 1589, 300, 307, 445, 294, 51652], "temperature": 0.0, "avg_logprob": -0.085601806640625, "compression_ratio": 1.6291666666666667, "no_speech_prob": 0.0011332783615216613}, {"id": 747, "seek": 343948, "start": 3439.48, "end": 3444.84, "text": " your batch using this other model and now you can update this model on the fly, so you don't have", "tokens": [50364, 428, 15245, 1228, 341, 661, 2316, 293, 586, 291, 393, 5623, 341, 2316, 322, 264, 3603, 11, 370, 291, 500, 380, 362, 50632], "temperature": 0.0, "avg_logprob": -0.07740819847190773, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.001454692566767335}, {"id": 748, "seek": 343948, "start": 3444.84, "end": 3450.6, "text": " to worry too much about doing the full kind of document side update and again here what really", "tokens": [50632, 281, 3292, 886, 709, 466, 884, 264, 1577, 733, 295, 4166, 1252, 5623, 293, 797, 510, 437, 534, 50920], "temperature": 0.0, "avg_logprob": -0.07740819847190773, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.001454692566767335}, {"id": 749, "seek": 343948, "start": 3450.6, "end": 3454.84, "text": " matters is like how big is your index, if you have an amazing index you can basically solve", "tokens": [50920, 7001, 307, 411, 577, 955, 307, 428, 8186, 11, 498, 291, 362, 364, 2243, 8186, 291, 393, 1936, 5039, 51132], "temperature": 0.0, "avg_logprob": -0.07740819847190773, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.001454692566767335}, {"id": 750, "seek": 343948, "start": 3454.84, "end": 3460.6, "text": " any problem just by looking it up, so rather than cramming it into your parameters you can just find", "tokens": [51132, 604, 1154, 445, 538, 1237, 309, 493, 11, 370, 2831, 813, 941, 335, 2810, 309, 666, 428, 9834, 291, 393, 445, 915, 51420], "temperature": 0.0, "avg_logprob": -0.07740819847190773, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.001454692566767335}, {"id": 751, "seek": 346060, "start": 3460.6, "end": 3470.12, "text": " it. This is a really nice paper called SILO, so one of the interesting things I think that's going", "tokens": [50364, 309, 13, 639, 307, 257, 534, 1481, 3035, 1219, 318, 4620, 46, 11, 370, 472, 295, 264, 1880, 721, 286, 519, 300, 311, 516, 50840], "temperature": 0.0, "avg_logprob": -0.13597746397319593, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0025500631891191006}, {"id": 752, "seek": 346060, "start": 3470.12, "end": 3475.0, "text": " to happen in the next year or two around language models is there and you've seen this already,", "tokens": [50840, 281, 1051, 294, 264, 958, 1064, 420, 732, 926, 2856, 5245, 307, 456, 293, 291, 600, 1612, 341, 1217, 11, 51084], "temperature": 0.0, "avg_logprob": -0.13597746397319593, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0025500631891191006}, {"id": 753, "seek": 346060, "start": 3475.0, "end": 3479.88, "text": " there's a bunch of lawsuits against OpenAI and other places around where does the data exactly", "tokens": [51084, 456, 311, 257, 3840, 295, 39493, 1970, 7238, 48698, 293, 661, 3190, 926, 689, 775, 264, 1412, 2293, 51328], "temperature": 0.0, "avg_logprob": -0.13597746397319593, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0025500631891191006}, {"id": 754, "seek": 346060, "start": 3479.88, "end": 3487.56, "text": " come from, so one very elegant solution I think is to have a RAG system that you train on data that", "tokens": [51328, 808, 490, 11, 370, 472, 588, 21117, 3827, 286, 519, 307, 281, 362, 257, 14626, 38, 1185, 300, 291, 3847, 322, 1412, 300, 51712], "temperature": 0.0, "avg_logprob": -0.13597746397319593, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0025500631891191006}, {"id": 755, "seek": 348756, "start": 3487.56, "end": 3494.12, "text": " you know is safe, so you can train that thing on Wikipedia but now during test time you can give it", "tokens": [50364, 291, 458, 307, 3273, 11, 370, 291, 393, 3847, 300, 551, 322, 28999, 457, 586, 1830, 1500, 565, 291, 393, 976, 309, 50692], "temperature": 0.0, "avg_logprob": -0.08471211680659542, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0027142639737576246}, {"id": 756, "seek": 348756, "start": 3494.12, "end": 3500.2799999999997, "text": " a data store that has maybe slightly riskier information in it, so this massive index of", "tokens": [50692, 257, 1412, 3531, 300, 575, 1310, 4748, 3148, 811, 1589, 294, 309, 11, 370, 341, 5994, 8186, 295, 51000], "temperature": 0.0, "avg_logprob": -0.08471211680659542, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0027142639737576246}, {"id": 757, "seek": 348756, "start": 3500.2799999999997, "end": 3506.7599999999998, "text": " all the stuff on the internet including some things that are maybe higher risk, you can still", "tokens": [51000, 439, 264, 1507, 322, 264, 4705, 3009, 512, 721, 300, 366, 1310, 2946, 3148, 11, 291, 393, 920, 51324], "temperature": 0.0, "avg_logprob": -0.08471211680659542, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0027142639737576246}, {"id": 758, "seek": 348756, "start": 3506.7599999999998, "end": 3511.72, "text": " have them in your index but your language model, your retrieval augmented language model I should", "tokens": [51324, 362, 552, 294, 428, 8186, 457, 428, 2856, 2316, 11, 428, 19817, 3337, 36155, 2856, 2316, 286, 820, 51572], "temperature": 0.0, "avg_logprob": -0.08471211680659542, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0027142639737576246}, {"id": 759, "seek": 348756, "start": 3511.72, "end": 3515.64, "text": " say, you know that that thing is safe because it was trained on data that is public domain,", "tokens": [51572, 584, 11, 291, 458, 300, 300, 551, 307, 3273, 570, 309, 390, 8895, 322, 1412, 300, 307, 1908, 9274, 11, 51768], "temperature": 0.0, "avg_logprob": -0.08471211680659542, "compression_ratio": 1.8015267175572518, "no_speech_prob": 0.0027142639737576246}, {"id": 760, "seek": 351564, "start": 3516.3599999999997, "end": 3520.3599999999997, "text": " so that's what they do in SILO and they show that that works really well, so that's", "tokens": [50400, 370, 300, 311, 437, 436, 360, 294, 318, 4620, 46, 293, 436, 855, 300, 300, 1985, 534, 731, 11, 370, 300, 311, 50600], "temperature": 0.0, "avg_logprob": -0.095329040951199, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0007901827339082956}, {"id": 761, "seek": 351564, "start": 3521.24, "end": 3526.52, "text": " one possible solution to a lot of the kind of compliance and legal risk around language model", "tokens": [50644, 472, 1944, 3827, 281, 257, 688, 295, 264, 733, 295, 15882, 293, 5089, 3148, 926, 2856, 2316, 50908], "temperature": 0.0, "avg_logprob": -0.095329040951199, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0007901827339082956}, {"id": 762, "seek": 351564, "start": 3526.52, "end": 3536.92, "text": " deployments. There's a great paper also from one of your colleagues around context getting lost in", "tokens": [50908, 7274, 1117, 13, 821, 311, 257, 869, 3035, 611, 490, 472, 295, 428, 7734, 926, 4319, 1242, 2731, 294, 51428], "temperature": 0.0, "avg_logprob": -0.095329040951199, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0007901827339082956}, {"id": 763, "seek": 351564, "start": 3536.92, "end": 3541.16, "text": " the middle, I think this is also kind of a fascinating phenomenon, this is on a frozen RAG system", "tokens": [51428, 264, 2808, 11, 286, 519, 341, 307, 611, 733, 295, 257, 10343, 14029, 11, 341, 307, 322, 257, 12496, 14626, 38, 1185, 51640], "temperature": 0.0, "avg_logprob": -0.095329040951199, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0007901827339082956}, {"id": 764, "seek": 354116, "start": 3542.12, "end": 3549.7999999999997, "text": " but language models are very similar to humans in what things they pay attention to, so if you", "tokens": [50412, 457, 2856, 5245, 366, 588, 2531, 281, 6255, 294, 437, 721, 436, 1689, 3202, 281, 11, 370, 498, 291, 50796], "temperature": 0.0, "avg_logprob": -0.1498191409640842, "compression_ratio": 1.8, "no_speech_prob": 0.0009690669248811901}, {"id": 765, "seek": 354116, "start": 3549.7999999999997, "end": 3554.68, "text": " give them a bunch of things that you retrieve, what they will look at are the first things you", "tokens": [50796, 976, 552, 257, 3840, 295, 721, 300, 291, 30254, 11, 437, 436, 486, 574, 412, 366, 264, 700, 721, 291, 51040], "temperature": 0.0, "avg_logprob": -0.1498191409640842, "compression_ratio": 1.8, "no_speech_prob": 0.0009690669248811901}, {"id": 766, "seek": 354116, "start": 3554.68, "end": 3560.04, "text": " list and the last things you list and they will sort of ignore the middle, so if it actually", "tokens": [51040, 1329, 293, 264, 1036, 721, 291, 1329, 293, 436, 486, 1333, 295, 11200, 264, 2808, 11, 370, 498, 309, 767, 51308], "temperature": 0.0, "avg_logprob": -0.1498191409640842, "compression_ratio": 1.8, "no_speech_prob": 0.0009690669248811901}, {"id": 767, "seek": 354116, "start": 3560.04, "end": 3565.24, "text": " respected the rank function then this curve would get down all the way, but it sort of goes up,", "tokens": [51308, 20020, 264, 6181, 2445, 550, 341, 7605, 576, 483, 760, 439, 264, 636, 11, 457, 309, 1333, 295, 1709, 493, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1498191409640842, "compression_ratio": 1.8, "no_speech_prob": 0.0009690669248811901}, {"id": 768, "seek": 356524, "start": 3565.8799999999997, "end": 3572.04, "text": " so I think that's a very interesting observation which kind of shows that how brittle", "tokens": [50396, 370, 286, 519, 300, 311, 257, 588, 1880, 14816, 597, 733, 295, 3110, 300, 577, 49325, 50704], "temperature": 0.0, "avg_logprob": -0.261082295621379, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.004751908592879772}, {"id": 769, "seek": 356524, "start": 3572.7599999999998, "end": 3578.12, "text": " these these systems can be, so if you have a frozen RAG system it can be very very brittle where", "tokens": [50740, 613, 613, 3652, 393, 312, 11, 370, 498, 291, 362, 257, 12496, 14626, 38, 1185, 309, 393, 312, 588, 588, 49325, 689, 51008], "temperature": 0.0, "avg_logprob": -0.261082295621379, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.004751908592879772}, {"id": 770, "seek": 356524, "start": 3578.12, "end": 3582.68, "text": " like the order of the retrieved context matters a lot in whether you get the right answer or not.", "tokens": [51008, 411, 264, 1668, 295, 264, 19817, 937, 4319, 7001, 257, 688, 294, 1968, 291, 483, 264, 558, 1867, 420, 406, 13, 51236], "temperature": 0.0, "avg_logprob": -0.261082295621379, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.004751908592879772}, {"id": 771, "seek": 356524, "start": 3584.52, "end": 3589.9599999999996, "text": " It doesn't work on treating this as a very funny problem in the sense that the colleagues come back", "tokens": [51328, 467, 1177, 380, 589, 322, 15083, 341, 382, 257, 588, 4074, 1154, 294, 264, 2020, 300, 264, 7734, 808, 646, 51600], "temperature": 0.0, "avg_logprob": -0.261082295621379, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.004751908592879772}, {"id": 772, "seek": 358996, "start": 3589.96, "end": 3595.4, "text": " or those of like specifically going towards the interpretation, I'm going to try it out for that", "tokens": [50364, 420, 729, 295, 411, 4682, 516, 3030, 264, 14174, 11, 286, 478, 516, 281, 853, 309, 484, 337, 300, 50636], "temperature": 0.0, "avg_logprob": -0.3442306911822447, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.007806898560374975}, {"id": 773, "seek": 358996, "start": 3595.4, "end": 3607.08, "text": " period that's going to inter-product with just the RAG. Yeah, so what I just described, somebody", "tokens": [50636, 2896, 300, 311, 516, 281, 728, 12, 33244, 365, 445, 264, 14626, 38, 13, 865, 11, 370, 437, 286, 445, 7619, 11, 2618, 51220], "temperature": 0.0, "avg_logprob": -0.3442306911822447, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.007806898560374975}, {"id": 774, "seek": 358996, "start": 3607.08, "end": 3611.7200000000003, "text": " asked like how do you actually, so I said there are other ways to do this and then the question", "tokens": [51220, 2351, 411, 577, 360, 291, 767, 11, 370, 286, 848, 456, 366, 661, 2098, 281, 360, 341, 293, 550, 264, 1168, 51452], "temperature": 0.0, "avg_logprob": -0.3442306911822447, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.007806898560374975}, {"id": 775, "seek": 358996, "start": 3611.7200000000003, "end": 3618.04, "text": " was how do you do that, so the way you do that is using reinforce, so yeah there has been work on", "tokens": [51452, 390, 577, 360, 291, 360, 300, 11, 370, 264, 636, 291, 360, 300, 307, 1228, 22634, 11, 370, 1338, 456, 575, 668, 589, 322, 51768], "temperature": 0.0, "avg_logprob": -0.3442306911822447, "compression_ratio": 1.6899563318777293, "no_speech_prob": 0.007806898560374975}, {"id": 776, "seek": 361804, "start": 3618.04, "end": 3623.48, "text": " doing that, so some of the older papers were playing with this, but one of the big problems with,", "tokens": [50364, 884, 300, 11, 370, 512, 295, 264, 4906, 10577, 645, 2433, 365, 341, 11, 457, 472, 295, 264, 955, 2740, 365, 11, 50636], "temperature": 0.0, "avg_logprob": -0.12972719292891652, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00047272778465412557}, {"id": 777, "seek": 361804, "start": 3625.4, "end": 3630.44, "text": " so I think the replug solution is sort of more elegant for solving that problem", "tokens": [50732, 370, 286, 519, 264, 3248, 697, 3827, 307, 1333, 295, 544, 21117, 337, 12606, 300, 1154, 50984], "temperature": 0.0, "avg_logprob": -0.12972719292891652, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00047272778465412557}, {"id": 778, "seek": 361804, "start": 3631.0, "end": 3635.64, "text": " because you actually do signal from the language model and if you just do reinforce it's very", "tokens": [51012, 570, 291, 767, 360, 6358, 490, 264, 2856, 2316, 293, 498, 291, 445, 360, 22634, 309, 311, 588, 51244], "temperature": 0.0, "avg_logprob": -0.12972719292891652, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00047272778465412557}, {"id": 779, "seek": 361804, "start": 3635.64, "end": 3641.48, "text": " high variance, so it's going to be super finicky if you don't want to destroy your index,", "tokens": [51244, 1090, 21977, 11, 370, 309, 311, 516, 281, 312, 1687, 962, 20539, 498, 291, 500, 380, 528, 281, 5293, 428, 8186, 11, 51536], "temperature": 0.0, "avg_logprob": -0.12972719292891652, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00047272778465412557}, {"id": 780, "seek": 361804, "start": 3643.08, "end": 3644.04, "text": " but people have tried it.", "tokens": [51616, 457, 561, 362, 3031, 309, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12972719292891652, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00047272778465412557}, {"id": 781, "seek": 364804, "start": 3649.0, "end": 3656.92, "text": " So there's some really nice work from OpenAI where they basically show and again we're sort of like", "tokens": [50412, 407, 456, 311, 512, 534, 1481, 589, 490, 7238, 48698, 689, 436, 1936, 855, 293, 797, 321, 434, 1333, 295, 411, 50808], "temperature": 0.0, "avg_logprob": -0.12239982060023717, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0008555281674489379}, {"id": 782, "seek": 364804, "start": 3656.92, "end": 3662.44, "text": " thinking more and more about agents here, where they show something very similar to the flare", "tokens": [50808, 1953, 544, 293, 544, 466, 12554, 510, 11, 689, 436, 855, 746, 588, 2531, 281, 264, 32446, 51084], "temperature": 0.0, "avg_logprob": -0.12239982060023717, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0008555281674489379}, {"id": 783, "seek": 364804, "start": 3662.44, "end": 3666.84, "text": " results from earlier with active retrieval that doesn't necessarily have to be some index that", "tokens": [51084, 3542, 490, 3071, 365, 4967, 19817, 3337, 300, 1177, 380, 4725, 362, 281, 312, 512, 8186, 300, 51304], "temperature": 0.0, "avg_logprob": -0.12239982060023717, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0008555281674489379}, {"id": 784, "seek": 364804, "start": 3666.84, "end": 3672.52, "text": " you only can read just some web search, and obviously in this case you don't really have", "tokens": [51304, 291, 787, 393, 1401, 445, 512, 3670, 3164, 11, 293, 2745, 294, 341, 1389, 291, 500, 380, 534, 362, 51588], "temperature": 0.0, "avg_logprob": -0.12239982060023717, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0008555281674489379}, {"id": 785, "seek": 364804, "start": 3672.52, "end": 3676.84, "text": " access to the web search necessarily, so Bing or whatever they use here is not going to update", "tokens": [51588, 2105, 281, 264, 3670, 3164, 4725, 11, 370, 30755, 420, 2035, 436, 764, 510, 307, 406, 516, 281, 5623, 51804], "temperature": 0.0, "avg_logprob": -0.12239982060023717, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0008555281674489379}, {"id": 786, "seek": 367684, "start": 3676.84, "end": 3682.2000000000003, "text": " these parameters, but I just wanted to kind of put this in your mind like this is another thing", "tokens": [50364, 613, 9834, 11, 457, 286, 445, 1415, 281, 733, 295, 829, 341, 294, 428, 1575, 411, 341, 307, 1071, 551, 50632], "temperature": 0.0, "avg_logprob": -0.08773345117983611, "compression_ratio": 2.0, "no_speech_prob": 0.0027995656710118055}, {"id": 787, "seek": 367684, "start": 3682.2000000000003, "end": 3688.6000000000004, "text": " you can do, and if we take this really to the general form then you can think of language", "tokens": [50632, 291, 393, 360, 11, 293, 498, 321, 747, 341, 534, 281, 264, 2674, 1254, 550, 291, 393, 519, 295, 2856, 50952], "temperature": 0.0, "avg_logprob": -0.08773345117983611, "compression_ratio": 2.0, "no_speech_prob": 0.0027995656710118055}, {"id": 788, "seek": 367684, "start": 3688.6000000000004, "end": 3695.0, "text": " models as just tool users, so rather than just retrieval augmenting language models we can tool", "tokens": [50952, 5245, 382, 445, 2290, 5022, 11, 370, 2831, 813, 445, 19817, 3337, 29919, 278, 2856, 5245, 321, 393, 2290, 51272], "temperature": 0.0, "avg_logprob": -0.08773345117983611, "compression_ratio": 2.0, "no_speech_prob": 0.0027995656710118055}, {"id": 789, "seek": 367684, "start": 3695.0, "end": 3699.6400000000003, "text": " augment language models and retrieval is just one of the many tools that language models have access", "tokens": [51272, 29919, 2856, 5245, 293, 19817, 3337, 307, 445, 472, 295, 264, 867, 3873, 300, 2856, 5245, 362, 2105, 51504], "temperature": 0.0, "avg_logprob": -0.08773345117983611, "compression_ratio": 2.0, "no_speech_prob": 0.0027995656710118055}, {"id": 790, "seek": 367684, "start": 3699.6400000000003, "end": 3706.44, "text": " to, we can have re-rankers and things on top of the outputs of these tools, and so one of the big", "tokens": [51504, 281, 11, 321, 393, 362, 319, 12, 20479, 433, 293, 721, 322, 1192, 295, 264, 23930, 295, 613, 3873, 11, 293, 370, 472, 295, 264, 955, 51844], "temperature": 0.0, "avg_logprob": -0.08773345117983611, "compression_ratio": 2.0, "no_speech_prob": 0.0027995656710118055}, {"id": 791, "seek": 370644, "start": 3706.44, "end": 3712.92, "text": " questions I think is how do you actually get the system to learn stuff, so we're going to need RL if", "tokens": [50364, 1651, 286, 519, 307, 577, 360, 291, 767, 483, 264, 1185, 281, 1466, 1507, 11, 370, 321, 434, 516, 281, 643, 497, 43, 498, 50688], "temperature": 0.0, "avg_logprob": -0.1080894781195599, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.0008965811575762928}, {"id": 792, "seek": 370644, "start": 3712.92, "end": 3721.88, "text": " we want the system to really learn how to take these actions properly, and so yeah this has been", "tokens": [50688, 321, 528, 264, 1185, 281, 534, 1466, 577, 281, 747, 613, 5909, 6108, 11, 293, 370, 1338, 341, 575, 668, 51136], "temperature": 0.0, "avg_logprob": -0.1080894781195599, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.0008965811575762928}, {"id": 793, "seek": 370644, "start": 3721.88, "end": 3727.32, "text": " taken to the extreme in this sort of self-reg architecture where they have this sort of retrieval", "tokens": [51136, 2726, 281, 264, 8084, 294, 341, 1333, 295, 2698, 12, 3375, 9482, 689, 436, 362, 341, 1333, 295, 19817, 3337, 51408], "temperature": 0.0, "avg_logprob": -0.1080894781195599, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.0008965811575762928}, {"id": 794, "seek": 370644, "start": 3727.32, "end": 3732.28, "text": " step and it's active and then you criticize it and then you basically do some natural language", "tokens": [51408, 1823, 293, 309, 311, 4967, 293, 550, 291, 31010, 309, 293, 550, 291, 1936, 360, 512, 3303, 2856, 51656], "temperature": 0.0, "avg_logprob": -0.1080894781195599, "compression_ratio": 1.7105263157894737, "no_speech_prob": 0.0008965811575762928}, {"id": 795, "seek": 373228, "start": 3732.28, "end": 3736.84, "text": " inference and all of that just with one language model to answer the questions.", "tokens": [50364, 38253, 293, 439, 295, 300, 445, 365, 472, 2856, 2316, 281, 1867, 264, 1651, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10160428047180176, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0033211717382073402}, {"id": 796, "seek": 373228, "start": 3738.76, "end": 3743.0800000000004, "text": " So the other missing piece, so I'm just kind of going through a bunch of open questions", "tokens": [50688, 407, 264, 661, 5361, 2522, 11, 370, 286, 478, 445, 733, 295, 516, 807, 257, 3840, 295, 1269, 1651, 50904], "temperature": 0.0, "avg_logprob": -0.10160428047180176, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0033211717382073402}, {"id": 797, "seek": 373228, "start": 3743.6400000000003, "end": 3748.28, "text": " that people have looked at, but feel free to interrupt me if there's anything you want to know,", "tokens": [50932, 300, 561, 362, 2956, 412, 11, 457, 841, 1737, 281, 12729, 385, 498, 456, 311, 1340, 291, 528, 281, 458, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10160428047180176, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0033211717382073402}, {"id": 798, "seek": 373228, "start": 3749.4, "end": 3753.88, "text": " but so instruction tuning we established at the beginning of the lecture that this is pretty", "tokens": [51220, 457, 370, 10951, 15164, 321, 7545, 412, 264, 2863, 295, 264, 7991, 300, 341, 307, 1238, 51444], "temperature": 0.0, "avg_logprob": -0.10160428047180176, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0033211717382073402}, {"id": 799, "seek": 373228, "start": 3753.88, "end": 3760.6000000000004, "text": " important for getting things to work, so fixing the user interface, but the instruction tuning", "tokens": [51444, 1021, 337, 1242, 721, 281, 589, 11, 370, 19442, 264, 4195, 9226, 11, 457, 264, 10951, 15164, 51780], "temperature": 0.0, "avg_logprob": -0.10160428047180176, "compression_ratio": 1.714828897338403, "no_speech_prob": 0.0033211717382073402}, {"id": 800, "seek": 376060, "start": 3760.6, "end": 3764.92, "text": " has almost always only happened on the language model and not on the entire system,", "tokens": [50364, 575, 1920, 1009, 787, 2011, 322, 264, 2856, 2316, 293, 406, 322, 264, 2302, 1185, 11, 50580], "temperature": 0.0, "avg_logprob": -0.13067932312305158, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0006983479834161699}, {"id": 801, "seek": 376060, "start": 3764.92, "end": 3769.88, "text": " so I think one of the interesting things that people are looking at now with things like RADT", "tokens": [50580, 370, 286, 519, 472, 295, 264, 1880, 721, 300, 561, 366, 1237, 412, 586, 365, 721, 411, 497, 6112, 51, 50828], "temperature": 0.0, "avg_logprob": -0.13067932312305158, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0006983479834161699}, {"id": 802, "seek": 376060, "start": 3769.88, "end": 3774.36, "text": " and Instruct Retro is how can we instruction fine-tune an entire retrieval augmented system,", "tokens": [50828, 293, 2730, 1757, 11495, 340, 307, 577, 393, 321, 10951, 2489, 12, 83, 2613, 364, 2302, 19817, 3337, 36155, 1185, 11, 51052], "temperature": 0.0, "avg_logprob": -0.13067932312305158, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0006983479834161699}, {"id": 803, "seek": 376060, "start": 3774.36, "end": 3779.72, "text": " so all the way into the retrieval step can we generate data so that that also follows the", "tokens": [51052, 370, 439, 264, 636, 666, 264, 19817, 3337, 1823, 393, 321, 8460, 1412, 370, 300, 300, 611, 10002, 264, 51320], "temperature": 0.0, "avg_logprob": -0.13067932312305158, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0006983479834161699}, {"id": 804, "seek": 376060, "start": 3779.72, "end": 3783.72, "text": " instructions properly, which currently doesn't happen in any of these model architectures.", "tokens": [51320, 9415, 6108, 11, 597, 4362, 1177, 380, 1051, 294, 604, 295, 613, 2316, 6331, 1303, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13067932312305158, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.0006983479834161699}, {"id": 805, "seek": 378372, "start": 3783.72, "end": 3790.04, "text": " And then finally, I think I would be remiss if I didn't really talk about", "tokens": [50364, 400, 550, 2721, 11, 286, 519, 286, 576, 312, 890, 891, 498, 286, 994, 380, 534, 751, 466, 50680], "temperature": 0.0, "avg_logprob": -0.1716620768975774, "compression_ratio": 1.6398467432950192, "no_speech_prob": 0.005818679463118315}, {"id": 806, "seek": 378372, "start": 3790.7599999999998, "end": 3794.8399999999997, "text": " what people call advanced RAG, so like the developer community has been really doing", "tokens": [50716, 437, 561, 818, 7339, 14626, 38, 11, 370, 411, 264, 10754, 1768, 575, 668, 534, 884, 50920], "temperature": 0.0, "avg_logprob": -0.1716620768975774, "compression_ratio": 1.6398467432950192, "no_speech_prob": 0.005818679463118315}, {"id": 807, "seek": 378372, "start": 3794.8399999999997, "end": 3800.3599999999997, "text": " some awesome stuff, so like frameworks like Lamaindex and Langchain and there's all these", "tokens": [50920, 512, 3476, 1507, 11, 370, 411, 29834, 411, 441, 2404, 471, 3121, 293, 13313, 11509, 293, 456, 311, 439, 613, 51196], "temperature": 0.0, "avg_logprob": -0.1716620768975774, "compression_ratio": 1.6398467432950192, "no_speech_prob": 0.005818679463118315}, {"id": 808, "seek": 378372, "start": 3800.3599999999997, "end": 3804.9199999999996, "text": " open source vector databases like Chroma and Weaviate and they're all sort of about making", "tokens": [51196, 1269, 4009, 8062, 22380, 411, 1721, 6440, 293, 492, 706, 13024, 293, 436, 434, 439, 1333, 295, 466, 1455, 51424], "temperature": 0.0, "avg_logprob": -0.1716620768975774, "compression_ratio": 1.6398467432950192, "no_speech_prob": 0.005818679463118315}, {"id": 809, "seek": 378372, "start": 3804.9199999999996, "end": 3810.2799999999997, "text": " RAG really easy, but this is all frozen RAG, right, but even with frozen RAG you've been", "tokens": [51424, 14626, 38, 534, 1858, 11, 457, 341, 307, 439, 12496, 14626, 38, 11, 558, 11, 457, 754, 365, 12496, 14626, 38, 291, 600, 668, 51692], "temperature": 0.0, "avg_logprob": -0.1716620768975774, "compression_ratio": 1.6398467432950192, "no_speech_prob": 0.005818679463118315}, {"id": 810, "seek": 381028, "start": 3810.28, "end": 3816.1200000000003, "text": " really doing incredible things, so we mentioned some of these already, so child-parent recursive", "tokens": [50364, 534, 884, 4651, 721, 11, 370, 321, 2835, 512, 295, 613, 1217, 11, 370, 1440, 12, 38321, 20560, 488, 50656], "temperature": 0.0, "avg_logprob": -0.15238627791404724, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.002358183963224292}, {"id": 811, "seek": 381028, "start": 3816.1200000000003, "end": 3821.48, "text": " retrievers, so you find small parts and then you give the big parts around it to the language model,", "tokens": [50656, 19817, 840, 11, 370, 291, 915, 1359, 3166, 293, 550, 291, 976, 264, 955, 3166, 926, 309, 281, 264, 2856, 2316, 11, 50924], "temperature": 0.0, "avg_logprob": -0.15238627791404724, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.002358183963224292}, {"id": 812, "seek": 381028, "start": 3821.48, "end": 3826.2000000000003, "text": " you can do hybrid search where we use reciprocal rank fusion, so we have like different search", "tokens": [50924, 291, 393, 360, 13051, 3164, 689, 321, 764, 46948, 6181, 23100, 11, 370, 321, 362, 411, 819, 3164, 51160], "temperature": 0.0, "avg_logprob": -0.15238627791404724, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.002358183963224292}, {"id": 813, "seek": 381028, "start": 3826.2000000000003, "end": 3830.36, "text": " results that we didn't combine before we give the final thing to the language model.", "tokens": [51160, 3542, 300, 321, 994, 380, 10432, 949, 321, 976, 264, 2572, 551, 281, 264, 2856, 2316, 13, 51368], "temperature": 0.0, "avg_logprob": -0.15238627791404724, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.002358183963224292}, {"id": 814, "seek": 381028, "start": 3831.0, "end": 3835.8, "text": " There's zero shot like a large language model remanker, so basically the score function is not,", "tokens": [51400, 821, 311, 4018, 3347, 411, 257, 2416, 2856, 2316, 890, 282, 5767, 11, 370, 1936, 264, 6175, 2445, 307, 406, 11, 51640], "temperature": 0.0, "avg_logprob": -0.15238627791404724, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.002358183963224292}, {"id": 815, "seek": 381028, "start": 3835.8, "end": 3839.0, "text": " it doesn't come from your retrieval, it comes directly from the language model,", "tokens": [51640, 309, 1177, 380, 808, 490, 428, 19817, 3337, 11, 309, 1487, 3838, 490, 264, 2856, 2316, 11, 51800], "temperature": 0.0, "avg_logprob": -0.15238627791404724, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.002358183963224292}, {"id": 816, "seek": 383900, "start": 3839.96, "end": 3844.84, "text": " and then hypothetical document embeddings which I think is a really cool idea, so you just,", "tokens": [50412, 293, 550, 33053, 4166, 12240, 29432, 597, 286, 519, 307, 257, 534, 1627, 1558, 11, 370, 291, 445, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10416018045865573, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.0006982765044085681}, {"id": 817, "seek": 383900, "start": 3845.56, "end": 3851.4, "text": " basically you fix hallucination through hallucination, so you get a question, then you let", "tokens": [50692, 1936, 291, 3191, 35212, 2486, 807, 35212, 2486, 11, 370, 291, 483, 257, 1168, 11, 550, 291, 718, 50984], "temperature": 0.0, "avg_logprob": -0.10416018045865573, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.0006982765044085681}, {"id": 818, "seek": 383900, "start": 3851.4, "end": 3855.72, "text": " the language model hallucinate a bunch of possible answers, then you go and search for", "tokens": [50984, 264, 2856, 2316, 35212, 13923, 257, 3840, 295, 1944, 6338, 11, 550, 291, 352, 293, 3164, 337, 51200], "temperature": 0.0, "avg_logprob": -0.10416018045865573, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.0006982765044085681}, {"id": 819, "seek": 383900, "start": 3855.72, "end": 3860.12, "text": " nearest neighbors to the possible answers and you give those as context and then it gives the right", "tokens": [51200, 23831, 12512, 281, 264, 1944, 6338, 293, 291, 976, 729, 382, 4319, 293, 550, 309, 2709, 264, 558, 51420], "temperature": 0.0, "avg_logprob": -0.10416018045865573, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.0006982765044085681}, {"id": 820, "seek": 383900, "start": 3860.12, "end": 3866.04, "text": " answer based on that, so it was really like hallucinating answers, I think it's a brilliant", "tokens": [51420, 1867, 2361, 322, 300, 11, 370, 309, 390, 534, 411, 35212, 8205, 6338, 11, 286, 519, 309, 311, 257, 10248, 51716], "temperature": 0.0, "avg_logprob": -0.10416018045865573, "compression_ratio": 1.8739837398373984, "no_speech_prob": 0.0006982765044085681}, {"id": 821, "seek": 386604, "start": 3866.12, "end": 3872.84, "text": " solution, so there's a lot of stuff happening in the kind of frozen rack community too that I think", "tokens": [50368, 3827, 11, 370, 456, 311, 257, 688, 295, 1507, 2737, 294, 264, 733, 295, 12496, 14788, 1768, 886, 300, 286, 519, 50704], "temperature": 0.0, "avg_logprob": -0.09972891211509705, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002357887802645564}, {"id": 822, "seek": 386604, "start": 3872.84, "end": 3880.44, "text": " is very interesting to look at, so just to wrap up, kind of looking at the future of this stuff,", "tokens": [50704, 307, 588, 1880, 281, 574, 412, 11, 370, 445, 281, 7019, 493, 11, 733, 295, 1237, 412, 264, 2027, 295, 341, 1507, 11, 51084], "temperature": 0.0, "avg_logprob": -0.09972891211509705, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002357887802645564}, {"id": 823, "seek": 386604, "start": 3881.64, "end": 3885.96, "text": " there are still lots of very interesting open questions, so if you're a student thinking about", "tokens": [51144, 456, 366, 920, 3195, 295, 588, 1880, 1269, 1651, 11, 370, 498, 291, 434, 257, 3107, 1953, 466, 51360], "temperature": 0.0, "avg_logprob": -0.09972891211509705, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002357887802645564}, {"id": 824, "seek": 386604, "start": 3885.96, "end": 3894.2, "text": " how to solve any of these, I think you can have quite a lot of impact, so how exactly do we do", "tokens": [51360, 577, 281, 5039, 604, 295, 613, 11, 286, 519, 291, 393, 362, 1596, 257, 688, 295, 2712, 11, 370, 577, 2293, 360, 321, 360, 51772], "temperature": 0.0, "avg_logprob": -0.09972891211509705, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.002357887802645564}, {"id": 825, "seek": 389420, "start": 3894.2, "end": 3899.24, "text": " the pretraining of this architecture and do we even need to pretrain, I think even retro kind of", "tokens": [50364, 264, 1162, 424, 1760, 295, 341, 9482, 293, 360, 321, 754, 643, 281, 1162, 7146, 11, 286, 519, 754, 18820, 733, 295, 50616], "temperature": 0.0, "avg_logprob": -0.08355757395426432, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.01047958992421627}, {"id": 826, "seek": 389420, "start": 3899.24, "end": 3904.8399999999997, "text": " shows that you don't necessarily have to pretrain, so maybe there's something wrong with how we do", "tokens": [50616, 3110, 300, 291, 500, 380, 4725, 362, 281, 1162, 7146, 11, 370, 1310, 456, 311, 746, 2085, 365, 577, 321, 360, 50896], "temperature": 0.0, "avg_logprob": -0.08355757395426432, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.01047958992421627}, {"id": 827, "seek": 389420, "start": 3904.8399999999997, "end": 3909.7999999999997, "text": " that, what do skating laws look like, so I think there's a really interesting question here around", "tokens": [50896, 300, 11, 437, 360, 29103, 6064, 574, 411, 11, 370, 286, 519, 456, 311, 257, 534, 1880, 1168, 510, 926, 51144], "temperature": 0.0, "avg_logprob": -0.08355757395426432, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.01047958992421627}, {"id": 828, "seek": 389420, "start": 3909.7999999999997, "end": 3915.56, "text": " if I have a huge index and a very rich encoder of all the information in that index, maybe I can", "tokens": [51144, 498, 286, 362, 257, 2603, 8186, 293, 257, 588, 4593, 2058, 19866, 295, 439, 264, 1589, 294, 300, 8186, 11, 1310, 286, 393, 51432], "temperature": 0.0, "avg_logprob": -0.08355757395426432, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.01047958992421627}, {"id": 829, "seek": 389420, "start": 3915.56, "end": 3920.7599999999998, "text": " move, so basically decouple all the memorization to this index, so I have a language model that", "tokens": [51432, 1286, 11, 370, 1936, 979, 263, 781, 439, 264, 10560, 2144, 281, 341, 8186, 11, 370, 286, 362, 257, 2856, 2316, 300, 51692], "temperature": 0.0, "avg_logprob": -0.08355757395426432, "compression_ratio": 1.7970479704797049, "no_speech_prob": 0.01047958992421627}, {"id": 830, "seek": 392076, "start": 3920.76, "end": 3925.2400000000002, "text": " doesn't know anything, it just speaks English, it just sort of reasons on top, but it has no", "tokens": [50364, 1177, 380, 458, 1340, 11, 309, 445, 10789, 3669, 11, 309, 445, 1333, 295, 4112, 322, 1192, 11, 457, 309, 575, 572, 50588], "temperature": 0.0, "avg_logprob": -0.09089632888338459, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.006189857609570026}, {"id": 831, "seek": 392076, "start": 3925.2400000000002, "end": 3929.48, "text": " knowledge because that always comes from this retriever, if you can do something like that then", "tokens": [50588, 3601, 570, 300, 1009, 1487, 490, 341, 19817, 331, 11, 498, 291, 393, 360, 746, 411, 300, 550, 50800], "temperature": 0.0, "avg_logprob": -0.09089632888338459, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.006189857609570026}, {"id": 832, "seek": 392076, "start": 3929.48, "end": 3934.6000000000004, "text": " you get very interesting scaling trade-offs, so you can have a tiny language model and do your", "tokens": [50800, 291, 483, 588, 1880, 21589, 4923, 12, 19231, 11, 370, 291, 393, 362, 257, 5870, 2856, 2316, 293, 360, 428, 51056], "temperature": 0.0, "avg_logprob": -0.09089632888338459, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.006189857609570026}, {"id": 833, "seek": 392076, "start": 3934.6000000000004, "end": 3939.88, "text": " retrieval to do a lot of the heavy lifting with your retrieval, which is nice because that's a", "tokens": [51056, 19817, 3337, 281, 360, 257, 688, 295, 264, 4676, 15798, 365, 428, 19817, 3337, 11, 597, 307, 1481, 570, 300, 311, 257, 51320], "temperature": 0.0, "avg_logprob": -0.09089632888338459, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.006189857609570026}, {"id": 834, "seek": 392076, "start": 3939.88, "end": 3945.1600000000003, "text": " cached computation, so you already have the embeddings, you just need to do the dot product,", "tokens": [51320, 269, 15095, 24903, 11, 370, 291, 1217, 362, 264, 12240, 29432, 11, 291, 445, 643, 281, 360, 264, 5893, 1674, 11, 51584], "temperature": 0.0, "avg_logprob": -0.09089632888338459, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.006189857609570026}, {"id": 835, "seek": 392076, "start": 3945.96, "end": 3949.2400000000002, "text": " so it's much more efficient than kind of self-attention in the language model.", "tokens": [51624, 370, 309, 311, 709, 544, 7148, 813, 733, 295, 2698, 12, 1591, 1251, 294, 264, 2856, 2316, 13, 51788], "temperature": 0.0, "avg_logprob": -0.09089632888338459, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.006189857609570026}, {"id": 836, "seek": 395076, "start": 3950.84, "end": 3957.0, "text": " Can we move beyond by encoders, so vector databases, I like people who build vector", "tokens": [50368, 1664, 321, 1286, 4399, 538, 2058, 378, 433, 11, 370, 8062, 22380, 11, 286, 411, 561, 567, 1322, 8062, 50676], "temperature": 0.0, "avg_logprob": -0.1458300714907439, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0018092090031132102}, {"id": 837, "seek": 395076, "start": 3957.0, "end": 3963.7200000000003, "text": " databases, but I'm not sure how long we're going to keep vector databases, because I think", "tokens": [50676, 22380, 11, 457, 286, 478, 406, 988, 577, 938, 321, 434, 516, 281, 1066, 8062, 22380, 11, 570, 286, 519, 51012], "temperature": 0.0, "avg_logprob": -0.1458300714907439, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0018092090031132102}, {"id": 838, "seek": 395076, "start": 3964.28, "end": 3969.4, "text": " re-rankers probably work just as well and the N25 is much more efficient than a vector database,", "tokens": [51040, 319, 12, 20479, 433, 1391, 589, 445, 382, 731, 293, 264, 426, 6074, 307, 709, 544, 7148, 813, 257, 8062, 8149, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1458300714907439, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0018092090031132102}, {"id": 839, "seek": 395076, "start": 3970.92, "end": 3977.5600000000004, "text": " so I don't really see why we need dedicated vector databases, so what we're seeing, but maybe", "tokens": [51372, 370, 286, 500, 380, 534, 536, 983, 321, 643, 8374, 8062, 22380, 11, 370, 437, 321, 434, 2577, 11, 457, 1310, 51704], "temperature": 0.0, "avg_logprob": -0.1458300714907439, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0018092090031132102}, {"id": 840, "seek": 397756, "start": 3977.56, "end": 3983.48, "text": " this is a bit of a critique of Silicon Valley investment strategies and things like that, but", "tokens": [50364, 341, 307, 257, 857, 295, 257, 25673, 295, 25351, 10666, 6078, 9029, 293, 721, 411, 300, 11, 457, 50660], "temperature": 0.0, "avg_logprob": -0.09210753218035832, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.0028880988247692585}, {"id": 841, "seek": 397756, "start": 3983.48, "end": 3989.0, "text": " a lot of these vector database companies are basically becoming database companies now,", "tokens": [50660, 257, 688, 295, 613, 8062, 8149, 3431, 366, 1936, 5617, 8149, 3431, 586, 11, 50936], "temperature": 0.0, "avg_logprob": -0.09210753218035832, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.0028880988247692585}, {"id": 842, "seek": 397756, "start": 3989.0, "end": 3994.68, "text": " so they are adding all this sparse stuff because the dense thing is not enough, and as it turns out", "tokens": [50936, 370, 436, 366, 5127, 439, 341, 637, 11668, 1507, 570, 264, 18011, 551, 307, 406, 1547, 11, 293, 382, 309, 4523, 484, 51220], "temperature": 0.0, "avg_logprob": -0.09210753218035832, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.0028880988247692585}, {"id": 843, "seek": 397756, "start": 3994.68, "end": 4000.2799999999997, "text": " there are a lot of pretty good sparse databases out there already, like Postgres and things like", "tokens": [51220, 456, 366, 257, 688, 295, 1238, 665, 637, 11668, 22380, 484, 456, 1217, 11, 411, 10223, 45189, 293, 721, 411, 51500], "temperature": 0.0, "avg_logprob": -0.09210753218035832, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.0028880988247692585}, {"id": 844, "seek": 397756, "start": 4000.2799999999997, "end": 4006.2, "text": " that, and there are also all adding vectors to their databases, so I think that's all going to", "tokens": [51500, 300, 11, 293, 456, 366, 611, 439, 5127, 18875, 281, 641, 22380, 11, 370, 286, 519, 300, 311, 439, 516, 281, 51796], "temperature": 0.0, "avg_logprob": -0.09210753218035832, "compression_ratio": 1.907258064516129, "no_speech_prob": 0.0028880988247692585}, {"id": 845, "seek": 400620, "start": 4006.2, "end": 4015.08, "text": " kind of coalesce into databases. So I think there are some interesting things to look at", "tokens": [50364, 733, 295, 598, 4229, 384, 666, 22380, 13, 407, 286, 519, 456, 366, 512, 1880, 721, 281, 574, 412, 50808], "temperature": 0.0, "avg_logprob": -0.10140667838611822, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006663501262664795}, {"id": 846, "seek": 400620, "start": 4015.7999999999997, "end": 4021.96, "text": " for kind of the data, so through this instruction problem, can we generate much better data for", "tokens": [50844, 337, 733, 295, 264, 1412, 11, 370, 807, 341, 10951, 1154, 11, 393, 321, 8460, 709, 1101, 1412, 337, 51152], "temperature": 0.0, "avg_logprob": -0.10140667838611822, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006663501262664795}, {"id": 847, "seek": 400620, "start": 4021.96, "end": 4027.08, "text": " training RAG systems synthetically, and then I think there's this massive open question around", "tokens": [51152, 3097, 14626, 38, 3652, 10657, 22652, 11, 293, 550, 286, 519, 456, 311, 341, 5994, 1269, 1168, 926, 51408], "temperature": 0.0, "avg_logprob": -0.10140667838611822, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006663501262664795}, {"id": 848, "seek": 400620, "start": 4027.08, "end": 4031.56, "text": " how we actually measure whether the RAG system is any good, so right now we just look at downstream", "tokens": [51408, 577, 321, 767, 3481, 1968, 264, 14626, 38, 1185, 307, 604, 665, 11, 370, 558, 586, 321, 445, 574, 412, 30621, 51632], "temperature": 0.0, "avg_logprob": -0.10140667838611822, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0006663501262664795}, {"id": 849, "seek": 403156, "start": 4031.56, "end": 4037.72, "text": " performance, which is sort of okay, but if you mess up the retrieval it's very hard to measure,", "tokens": [50364, 3389, 11, 597, 307, 1333, 295, 1392, 11, 457, 498, 291, 2082, 493, 264, 19817, 3337, 309, 311, 588, 1152, 281, 3481, 11, 50672], "temperature": 0.0, "avg_logprob": -0.08225757764733356, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.003705877810716629}, {"id": 850, "seek": 403156, "start": 4038.92, "end": 4043.7999999999997, "text": " but how to measure whether your retrieval is right is also very difficult, so there are some", "tokens": [50732, 457, 577, 281, 3481, 1968, 428, 19817, 3337, 307, 558, 307, 611, 588, 2252, 11, 370, 456, 366, 512, 50976], "temperature": 0.0, "avg_logprob": -0.08225757764733356, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.003705877810716629}, {"id": 851, "seek": 403156, "start": 4043.7999999999997, "end": 4048.52, "text": " frameworks where they try to take like the harmonic mean of your retrieval accuracy and your language", "tokens": [50976, 29834, 689, 436, 853, 281, 747, 411, 264, 32270, 914, 295, 428, 19817, 3337, 14170, 293, 428, 2856, 51212], "temperature": 0.0, "avg_logprob": -0.08225757764733356, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.003705877810716629}, {"id": 852, "seek": 403156, "start": 4048.52, "end": 4054.04, "text": " model accuracy, but I think those are also very shoddy because we don't really have very good", "tokens": [51212, 2316, 14170, 11, 457, 286, 519, 729, 366, 611, 588, 402, 378, 3173, 570, 321, 500, 380, 534, 362, 588, 665, 51488], "temperature": 0.0, "avg_logprob": -0.08225757764733356, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.003705877810716629}, {"id": 853, "seek": 403156, "start": 4054.04, "end": 4058.84, "text": " data sets to measure that on, so I think that's a very cool problem to work on as well.", "tokens": [51488, 1412, 6352, 281, 3481, 300, 322, 11, 370, 286, 519, 300, 311, 257, 588, 1627, 1154, 281, 589, 322, 382, 731, 13, 51728], "temperature": 0.0, "avg_logprob": -0.08225757764733356, "compression_ratio": 1.8294573643410852, "no_speech_prob": 0.003705877810716629}, {"id": 854, "seek": 405884, "start": 4059.7200000000003, "end": 4065.7200000000003, "text": " So the other problem that I personally am always very excited about is multimodality,", "tokens": [50408, 407, 264, 661, 1154, 300, 286, 5665, 669, 1009, 588, 2919, 466, 307, 32972, 378, 1860, 11, 50708], "temperature": 0.0, "avg_logprob": -0.14048866841984892, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002530939818825573}, {"id": 855, "seek": 405884, "start": 4067.0, "end": 4073.7200000000003, "text": " and so why would we stop with RAG systems with just text, so you can do the same with images,", "tokens": [50772, 293, 370, 983, 576, 321, 1590, 365, 14626, 38, 3652, 365, 445, 2487, 11, 370, 291, 393, 360, 264, 912, 365, 5267, 11, 51108], "temperature": 0.0, "avg_logprob": -0.14048866841984892, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002530939818825573}, {"id": 856, "seek": 405884, "start": 4074.28, "end": 4079.32, "text": " you can augment language models with vision, so we did this work on lens where we have a", "tokens": [51136, 291, 393, 29919, 2856, 5245, 365, 5201, 11, 370, 321, 630, 341, 589, 322, 6765, 689, 321, 362, 257, 51388], "temperature": 0.0, "avg_logprob": -0.14048866841984892, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002530939818825573}, {"id": 857, "seek": 405884, "start": 4079.32, "end": 4085.7200000000003, "text": " language model enhanced to see, where you can just give kind of a computer vision pipeline,", "tokens": [51388, 2856, 2316, 21191, 281, 536, 11, 689, 291, 393, 445, 976, 733, 295, 257, 3820, 5201, 15517, 11, 51708], "temperature": 0.0, "avg_logprob": -0.14048866841984892, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0002530939818825573}, {"id": 858, "seek": 408572, "start": 4085.72, "end": 4090.04, "text": " just like a retrieval pipeline and give that to a frozen language model and pass it to the", "tokens": [50364, 445, 411, 257, 19817, 3337, 15517, 293, 976, 300, 281, 257, 12496, 2856, 2316, 293, 1320, 309, 281, 264, 50580], "temperature": 0.0, "avg_logprob": -0.11143786275488698, "compression_ratio": 1.597972972972973, "no_speech_prob": 0.00039189172093756497}, {"id": 859, "seek": 408572, "start": 4090.04, "end": 4095.3199999999997, "text": " context and that system actually is an amazing visual question answering system. It's close to", "tokens": [50580, 4319, 293, 300, 1185, 767, 307, 364, 2243, 5056, 1168, 13430, 1185, 13, 467, 311, 1998, 281, 50844], "temperature": 0.0, "avg_logprob": -0.11143786275488698, "compression_ratio": 1.597972972972973, "no_speech_prob": 0.00039189172093756497}, {"id": 860, "seek": 408572, "start": 4095.3199999999997, "end": 4100.76, "text": " state-of-the-art sort of flamingo from DeepMind, which is also very hard to reproduce because", "tokens": [50844, 1785, 12, 2670, 12, 3322, 12, 446, 1333, 295, 45718, 78, 490, 14895, 44, 471, 11, 597, 307, 611, 588, 1152, 281, 29501, 570, 51116], "temperature": 0.0, "avg_logprob": -0.11143786275488698, "compression_ratio": 1.597972972972973, "no_speech_prob": 0.00039189172093756497}, {"id": 861, "seek": 408572, "start": 4100.76, "end": 4108.84, "text": " there's no open source version of that, so we've done some early work on this in 2021 where we", "tokens": [51116, 456, 311, 572, 1269, 4009, 3037, 295, 300, 11, 370, 321, 600, 1096, 512, 2440, 589, 322, 341, 294, 7201, 689, 321, 51520], "temperature": 0.0, "avg_logprob": -0.11143786275488698, "compression_ratio": 1.597972972972973, "no_speech_prob": 0.00039189172093756497}, {"id": 862, "seek": 408572, "start": 4108.84, "end": 4114.28, "text": " have this cross-modal retrieval and there's some more recent work out of FAIR where they also look", "tokens": [51520, 362, 341, 3278, 12, 8014, 304, 19817, 3337, 293, 456, 311, 512, 544, 5162, 589, 484, 295, 19894, 7740, 689, 436, 611, 574, 51792], "temperature": 0.0, "avg_logprob": -0.11143786275488698, "compression_ratio": 1.597972972972973, "no_speech_prob": 0.00039189172093756497}, {"id": 863, "seek": 411428, "start": 4114.28, "end": 4118.92, "text": " at this, so I think that's really like if you look at the trend in the field like multimodality", "tokens": [50364, 412, 341, 11, 370, 286, 519, 300, 311, 534, 411, 498, 291, 574, 412, 264, 6028, 294, 264, 2519, 411, 32972, 378, 1860, 50596], "temperature": 0.0, "avg_logprob": -0.14539963429368388, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.001809041597880423}, {"id": 864, "seek": 411428, "start": 4118.92, "end": 4123.5599999999995, "text": " with GPD4 or V and things like that is really a hot topic, so everything is kind of going in", "tokens": [50596, 365, 460, 17349, 19, 420, 691, 293, 721, 411, 300, 307, 534, 257, 2368, 4829, 11, 370, 1203, 307, 733, 295, 516, 294, 50828], "temperature": 0.0, "avg_logprob": -0.14539963429368388, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.001809041597880423}, {"id": 865, "seek": 411428, "start": 4123.5599999999995, "end": 4132.04, "text": " that direction, so it's an interesting thing to think about. So overall I think it would be nice", "tokens": [50828, 300, 3513, 11, 370, 309, 311, 364, 1880, 551, 281, 519, 466, 13, 407, 4787, 286, 519, 309, 576, 312, 1481, 51252], "temperature": 0.0, "avg_logprob": -0.14539963429368388, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.001809041597880423}, {"id": 866, "seek": 411428, "start": 4132.04, "end": 4138.28, "text": " if everybody sort of moves away from RAG 1.0, the frozen Frankenstein RAG and moves towards this much", "tokens": [51252, 498, 2201, 1333, 295, 6067, 1314, 490, 14626, 38, 502, 13, 15, 11, 264, 12496, 39678, 9089, 14626, 38, 293, 6067, 3030, 341, 709, 51564], "temperature": 0.0, "avg_logprob": -0.14539963429368388, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.001809041597880423}, {"id": 867, "seek": 411428, "start": 4138.28, "end": 4143.96, "text": " more kind of optimized version RAG 2.0, so it's really about systems over models, right, it's", "tokens": [51564, 544, 733, 295, 26941, 3037, 14626, 38, 568, 13, 15, 11, 370, 309, 311, 534, 466, 3652, 670, 5245, 11, 558, 11, 309, 311, 51848], "temperature": 0.0, "avg_logprob": -0.14539963429368388, "compression_ratio": 1.6996466431095407, "no_speech_prob": 0.001809041597880423}, {"id": 868, "seek": 414396, "start": 4143.96, "end": 4147.8, "text": " not just your language model when you're retriever and they're kind of separate, it's about thinking", "tokens": [50364, 406, 445, 428, 2856, 2316, 562, 291, 434, 19817, 331, 293, 436, 434, 733, 295, 4994, 11, 309, 311, 466, 1953, 50556], "temperature": 0.0, "avg_logprob": -0.10951002550796723, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0010154922492802143}, {"id": 869, "seek": 414396, "start": 4147.8, "end": 4152.6, "text": " from the from a system's perspective about the entire thing and the problem you're trying to solve", "tokens": [50556, 490, 264, 490, 257, 1185, 311, 4585, 466, 264, 2302, 551, 293, 264, 1154, 291, 434, 1382, 281, 5039, 50796], "temperature": 0.0, "avg_logprob": -0.10951002550796723, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0010154922492802143}, {"id": 870, "seek": 414396, "start": 4152.6, "end": 4157.64, "text": " and so I think that really is the way that in deep learning things have always progressed,", "tokens": [50796, 293, 370, 286, 519, 300, 534, 307, 264, 636, 300, 294, 2452, 2539, 721, 362, 1009, 36789, 11, 51048], "temperature": 0.0, "avg_logprob": -0.10951002550796723, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0010154922492802143}, {"id": 871, "seek": 414396, "start": 4157.64, "end": 4162.36, "text": " where if you optimize the system end-to-end that's always going to win out, like back in the day in", "tokens": [51048, 689, 498, 291, 19719, 264, 1185, 917, 12, 1353, 12, 521, 300, 311, 1009, 516, 281, 1942, 484, 11, 411, 646, 294, 264, 786, 294, 51284], "temperature": 0.0, "avg_logprob": -0.10951002550796723, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0010154922492802143}, {"id": 872, "seek": 414396, "start": 4162.36, "end": 4166.84, "text": " computer vision or NLP, we have like parsers and scene parsers and all this kind of stuff and all", "tokens": [51284, 3820, 5201, 420, 426, 45196, 11, 321, 362, 411, 21156, 433, 293, 4145, 21156, 433, 293, 439, 341, 733, 295, 1507, 293, 439, 51508], "temperature": 0.0, "avg_logprob": -0.10951002550796723, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0010154922492802143}, {"id": 873, "seek": 414396, "start": 4166.84, "end": 4172.6, "text": " of that just doesn't exist anymore now because we optimize the system end-to-end and so that's", "tokens": [51508, 295, 300, 445, 1177, 380, 2514, 3602, 586, 570, 321, 19719, 264, 1185, 917, 12, 1353, 12, 521, 293, 370, 300, 311, 51796], "temperature": 0.0, "avg_logprob": -0.10951002550796723, "compression_ratio": 1.8745980707395498, "no_speech_prob": 0.0010154922492802143}, {"id": 874, "seek": 417260, "start": 4172.6, "end": 4177.64, "text": " what's going to happen here too. So if we take that to the extreme, like there's a chunker thing", "tokens": [50364, 437, 311, 516, 281, 1051, 510, 886, 13, 407, 498, 321, 747, 300, 281, 264, 8084, 11, 411, 456, 311, 257, 16635, 260, 551, 50616], "temperature": 0.0, "avg_logprob": -0.17863831399869518, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.001168734859675169}, {"id": 875, "seek": 417260, "start": 4177.64, "end": 4182.04, "text": " in your documents, right, like cutting it up into pieces, like you could backdrop into debt,", "tokens": [50616, 294, 428, 8512, 11, 558, 11, 411, 6492, 309, 493, 666, 3755, 11, 411, 291, 727, 32697, 666, 7831, 11, 50836], "temperature": 0.0, "avg_logprob": -0.17863831399869518, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.001168734859675169}, {"id": 876, "seek": 417260, "start": 4182.04, "end": 4189.56, "text": " like why not? Somebody should really do that and so yeah, I think like trading off cost and quality", "tokens": [50836, 411, 983, 406, 30, 13463, 820, 534, 360, 300, 293, 370, 1338, 11, 286, 519, 411, 9529, 766, 2063, 293, 3125, 51212], "temperature": 0.0, "avg_logprob": -0.17863831399869518, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.001168734859675169}, {"id": 877, "seek": 417260, "start": 4190.360000000001, "end": 4194.200000000001, "text": " and zero-shot domain generalization, that's really like where this stuff is going to come in, right,", "tokens": [51252, 293, 4018, 12, 18402, 9274, 2674, 2144, 11, 300, 311, 534, 411, 689, 341, 1507, 307, 516, 281, 808, 294, 11, 558, 11, 51444], "temperature": 0.0, "avg_logprob": -0.17863831399869518, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.001168734859675169}, {"id": 878, "seek": 417260, "start": 4194.200000000001, "end": 4199.08, "text": " so language models right now, they're amazing but very often they're way too expensive for being", "tokens": [51444, 370, 2856, 5245, 558, 586, 11, 436, 434, 2243, 457, 588, 2049, 436, 434, 636, 886, 5124, 337, 885, 51688], "temperature": 0.0, "avg_logprob": -0.17863831399869518, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.001168734859675169}, {"id": 879, "seek": 419908, "start": 4199.08, "end": 4203.8, "text": " deployed somewhere where you can actually make money from them if you're in a company. So what", "tokens": [50364, 17826, 4079, 689, 291, 393, 767, 652, 1460, 490, 552, 498, 291, 434, 294, 257, 2237, 13, 407, 437, 50600], "temperature": 0.0, "avg_logprob": -0.12107794971789344, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0006358048995025456}, {"id": 880, "seek": 419908, "start": 4203.8, "end": 4208.76, "text": " you want to do is make it much more efficient and have the right cost quality trade-off and the", "tokens": [50600, 291, 528, 281, 360, 307, 652, 309, 709, 544, 7148, 293, 362, 264, 558, 2063, 3125, 4923, 12, 4506, 293, 264, 50848], "temperature": 0.0, "avg_logprob": -0.12107794971789344, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0006358048995025456}, {"id": 881, "seek": 419908, "start": 4208.76, "end": 4213.64, "text": " easiest way I can think of is to do it through retrieval augmentation but obviously I'm very biased.", "tokens": [50848, 12889, 636, 286, 393, 519, 295, 307, 281, 360, 309, 807, 19817, 3337, 14501, 19631, 457, 2745, 286, 478, 588, 28035, 13, 51092], "temperature": 0.0, "avg_logprob": -0.12107794971789344, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0006358048995025456}, {"id": 882, "seek": 419908, "start": 4215.72, "end": 4222.04, "text": " So yeah, that was all I had actually. So if you're interested in this, I'm at Samford so I can", "tokens": [51196, 407, 1338, 11, 300, 390, 439, 286, 632, 767, 13, 407, 498, 291, 434, 3102, 294, 341, 11, 286, 478, 412, 4832, 7404, 370, 286, 393, 51512], "temperature": 0.0, "avg_logprob": -0.12107794971789344, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0006358048995025456}, {"id": 883, "seek": 419908, "start": 4222.04, "end": 4226.76, "text": " work with you on research projects on these topics or if you want you can also join contextual", "tokens": [51512, 589, 365, 291, 322, 2132, 4455, 322, 613, 8378, 420, 498, 291, 528, 291, 393, 611, 3917, 35526, 51748], "temperature": 0.0, "avg_logprob": -0.12107794971789344, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.0006358048995025456}, {"id": 884, "seek": 422676, "start": 4226.76, "end": 4234.04, "text": " because we work on this stuff every day. Thank you. Well, sorry, I had a question from earlier.", "tokens": [50364, 570, 321, 589, 322, 341, 1507, 633, 786, 13, 1044, 291, 13, 1042, 11, 2597, 11, 286, 632, 257, 1168, 490, 3071, 13, 50728], "temperature": 0.0, "avg_logprob": -0.14883365445924038, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0007205638685263693}, {"id": 885, "seek": 422676, "start": 4235.88, "end": 4241.24, "text": " Yeah, I think you said something really, really, I think really super helpful earlier about", "tokens": [50820, 865, 11, 286, 519, 291, 848, 746, 534, 11, 534, 11, 286, 519, 534, 1687, 4961, 3071, 466, 51088], "temperature": 0.0, "avg_logprob": -0.14883365445924038, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0007205638685263693}, {"id": 886, "seek": 422676, "start": 4241.24, "end": 4246.6, "text": " Miss Jill 7B. You talked about, you compared the sliding window attention to convolutional neural", "tokens": [51088, 5275, 24690, 1614, 33, 13, 509, 2825, 466, 11, 291, 5347, 264, 21169, 4910, 3202, 281, 45216, 304, 18161, 51356], "temperature": 0.0, "avg_logprob": -0.14883365445924038, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0007205638685263693}, {"id": 887, "seek": 422676, "start": 4246.6, "end": 4250.04, "text": " networks and I do see the parallel because with convolutional neural networks you have", "tokens": [51356, 9590, 293, 286, 360, 536, 264, 8952, 570, 365, 45216, 304, 18161, 9590, 291, 362, 51528], "temperature": 0.0, "avg_logprob": -0.14883365445924038, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0007205638685263693}, {"id": 888, "seek": 422676, "start": 4250.68, "end": 4254.76, "text": " several layers of, several different layers of convolutional layers and the top convolutional", "tokens": [51560, 2940, 7914, 295, 11, 2940, 819, 7914, 295, 45216, 304, 7914, 293, 264, 1192, 45216, 304, 51764], "temperature": 0.0, "avg_logprob": -0.14883365445924038, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.0007205638685263693}, {"id": 889, "seek": 425476, "start": 4254.76, "end": 4260.12, "text": " layers are able to see a larger receptive field than the bottom convolutional layers", "tokens": [50364, 7914, 366, 1075, 281, 536, 257, 4833, 45838, 2519, 813, 264, 2767, 45216, 304, 7914, 50632], "temperature": 0.0, "avg_logprob": -0.09319632124192644, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.00045821984531357884}, {"id": 890, "seek": 425476, "start": 4260.12, "end": 4267.72, "text": " and with convolutional layers you're able to tune the filter sizes and the strides so you're able", "tokens": [50632, 293, 365, 45216, 304, 7914, 291, 434, 1075, 281, 10864, 264, 6608, 11602, 293, 264, 1056, 1875, 370, 291, 434, 1075, 51012], "temperature": 0.0, "avg_logprob": -0.09319632124192644, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.00045821984531357884}, {"id": 891, "seek": 425476, "start": 4267.72, "end": 4272.52, "text": " to see a different receptive field and I was wondering if you could see that same innovation", "tokens": [51012, 281, 536, 257, 819, 45838, 2519, 293, 286, 390, 6359, 498, 291, 727, 536, 300, 912, 8504, 51252], "temperature": 0.0, "avg_logprob": -0.09319632124192644, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.00045821984531357884}, {"id": 892, "seek": 425476, "start": 4272.52, "end": 4277.96, "text": " in Miss Jill 7B by tuning because you have different transformer layers and if each transformer", "tokens": [51252, 294, 5275, 24690, 1614, 33, 538, 15164, 570, 291, 362, 819, 31782, 7914, 293, 498, 1184, 31782, 51524], "temperature": 0.0, "avg_logprob": -0.09319632124192644, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.00045821984531357884}, {"id": 893, "seek": 425476, "start": 4277.96, "end": 4283.0, "text": " layer will have a span over a different set of tokens and you can tune I guess the transformer", "tokens": [51524, 4583, 486, 362, 257, 16174, 670, 257, 819, 992, 295, 22667, 293, 291, 393, 10864, 286, 2041, 264, 31782, 51776], "temperature": 0.0, "avg_logprob": -0.09319632124192644, "compression_ratio": 1.933609958506224, "no_speech_prob": 0.00045821984531357884}, {"id": 894, "seek": 428300, "start": 4283.0, "end": 4287.72, "text": " architecture the way you tune those convolutional layers, the filter sizes, the receptive field,", "tokens": [50364, 9482, 264, 636, 291, 10864, 729, 45216, 304, 7914, 11, 264, 6608, 11602, 11, 264, 45838, 2519, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1311245057189349, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000335141783580184}, {"id": 895, "seek": 428300, "start": 4287.72, "end": 4291.72, "text": " perhaps we can do some optimization in the transformer realm that we have already done", "tokens": [50600, 4317, 321, 393, 360, 512, 19618, 294, 264, 31782, 15355, 300, 321, 362, 1217, 1096, 50800], "temperature": 0.0, "avg_logprob": -0.1311245057189349, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000335141783580184}, {"id": 896, "seek": 428300, "start": 4291.72, "end": 4297.88, "text": " in convolution layers. Yeah, I think that's a good idea. There's a great paper on light", "tokens": [50800, 294, 45216, 7914, 13, 865, 11, 286, 519, 300, 311, 257, 665, 1558, 13, 821, 311, 257, 869, 3035, 322, 1442, 51108], "temperature": 0.0, "avg_logprob": -0.1311245057189349, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000335141783580184}, {"id": 897, "seek": 428300, "start": 4297.88, "end": 4304.04, "text": " convolutions I think from Michael Auli and David Gange and a bunch of people where it's basically", "tokens": [51108, 3754, 15892, 286, 519, 490, 5116, 316, 25484, 293, 4389, 460, 933, 293, 257, 3840, 295, 561, 689, 309, 311, 1936, 51416], "temperature": 0.0, "avg_logprob": -0.1311245057189349, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000335141783580184}, {"id": 898, "seek": 428300, "start": 4304.76, "end": 4309.56, "text": " this came out at exactly the same time as the transformer and the transformer is slightly", "tokens": [51452, 341, 1361, 484, 412, 2293, 264, 912, 565, 382, 264, 31782, 293, 264, 31782, 307, 4748, 51692], "temperature": 0.0, "avg_logprob": -0.1311245057189349, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000335141783580184}, {"id": 899, "seek": 430956, "start": 4309.64, "end": 4314.84, "text": " more optimized for GPU computation but the convolutional model was actually slightly", "tokens": [50368, 544, 26941, 337, 18407, 24903, 457, 264, 45216, 304, 2316, 390, 767, 4748, 50628], "temperature": 0.0, "avg_logprob": -0.21360671520233154, "compression_ratio": 1.2537313432835822, "no_speech_prob": 0.0036394833587110043}, {"id": 900, "seek": 430956, "start": 4314.84, "end": 4321.400000000001, "text": " better than the transformer so it's definitely worth exploring. Okay, cool, thanks.", "tokens": [50628, 1101, 813, 264, 31782, 370, 309, 311, 2138, 3163, 12736, 13, 1033, 11, 1627, 11, 3231, 13, 50956], "temperature": 0.0, "avg_logprob": -0.21360671520233154, "compression_ratio": 1.2537313432835822, "no_speech_prob": 0.0036394833587110043}, {"id": 901, "seek": 432140, "start": 4321.4, "end": 4338.5199999999995, "text": " Yeah, so it depends on the problem I think what you probably want to do is", "tokens": [50364, 865, 11, 370, 309, 5946, 322, 264, 1154, 286, 519, 437, 291, 1391, 528, 281, 360, 307, 51220], "temperature": 0.0, "avg_logprob": -0.1455517548781175, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.029691072180867195}, {"id": 902, "seek": 432140, "start": 4338.5199999999995, "end": 4344.28, "text": " is sort of cast a white net with VM25 and then just narrow it down with dense search", "tokens": [51220, 307, 1333, 295, 4193, 257, 2418, 2533, 365, 18038, 6074, 293, 550, 445, 9432, 309, 760, 365, 18011, 3164, 51508], "temperature": 0.0, "avg_logprob": -0.1455517548781175, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.029691072180867195}, {"id": 903, "seek": 432140, "start": 4345.08, "end": 4349.5599999999995, "text": " so you often see that kind of as a two-stage process where the first one is kind of noisy", "tokens": [51548, 370, 291, 2049, 536, 300, 733, 295, 382, 257, 732, 12, 17882, 1399, 689, 264, 700, 472, 307, 733, 295, 24518, 51772], "temperature": 0.0, "avg_logprob": -0.1455517548781175, "compression_ratio": 1.464705882352941, "no_speech_prob": 0.029691072180867195}, {"id": 904, "seek": 434956, "start": 4349.56, "end": 4354.120000000001, "text": " you can add noise actually to your retrieval and then you use the dense one to filter it down.", "tokens": [50364, 291, 393, 909, 5658, 767, 281, 428, 19817, 3337, 293, 550, 291, 764, 264, 18011, 472, 281, 6608, 309, 760, 13, 50592], "temperature": 0.0, "avg_logprob": -0.40237628982727786, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0038813711144030094}, {"id": 905, "seek": 434956, "start": 4356.76, "end": 4362.68, "text": " Yeah, everyone's trying to maybe adapt their plug-in model to almost", "tokens": [50724, 865, 11, 1518, 311, 1382, 281, 1310, 6231, 641, 5452, 12, 259, 2316, 281, 1920, 51020], "temperature": 0.0, "avg_logprob": -0.40237628982727786, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0038813711144030094}, {"id": 906, "seek": 434956, "start": 4362.68, "end": 4369.240000000001, "text": " the only specific area. I think there are many two ways of project one way is to use", "tokens": [51020, 264, 787, 2685, 1859, 13, 286, 519, 456, 366, 867, 732, 2098, 295, 1716, 472, 636, 307, 281, 764, 51348], "temperature": 0.0, "avg_logprob": -0.40237628982727786, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0038813711144030094}, {"id": 907, "seek": 434956, "start": 4369.240000000001, "end": 4375.080000000001, "text": " some instrument tuning in some kind of learning way or functionally like tuning that and another way", "tokens": [51348, 512, 7198, 15164, 294, 512, 733, 295, 2539, 636, 420, 2445, 379, 411, 15164, 300, 293, 1071, 636, 51640], "temperature": 0.0, "avg_logprob": -0.40237628982727786, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0038813711144030094}, {"id": 908, "seek": 437508, "start": 4375.08, "end": 4382.5199999999995, "text": " is to the main project of this lecture is using the virtual augmented way. So I wonder if I", "tokens": [50364, 307, 281, 264, 2135, 1716, 295, 341, 7991, 307, 1228, 264, 6374, 36155, 636, 13, 407, 286, 2441, 498, 286, 50736], "temperature": 0.0, "avg_logprob": -0.40587571088005514, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.005998768378049135}, {"id": 909, "seek": 437508, "start": 4383.16, "end": 4390.68, "text": " know a message of virtual augmented way, do you think the capacity or the quality of virtual", "tokens": [50768, 458, 257, 3636, 295, 6374, 36155, 636, 11, 360, 291, 519, 264, 6042, 420, 264, 3125, 295, 6374, 51144], "temperature": 0.0, "avg_logprob": -0.40587571088005514, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.005998768378049135}, {"id": 910, "seek": 437508, "start": 4390.68, "end": 4395.64, "text": " augmented way can be matched with those tuning methods I think or kind of learning?", "tokens": [51144, 36155, 636, 393, 312, 21447, 365, 729, 15164, 7150, 286, 519, 420, 733, 295, 2539, 30, 51392], "temperature": 0.0, "avg_logprob": -0.40587571088005514, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.005998768378049135}, {"id": 911, "seek": 437508, "start": 4395.64, "end": 4401.96, "text": " Yeah, so I think actually what's going to happen is that all of this will come together right so", "tokens": [51392, 865, 11, 370, 286, 519, 767, 437, 311, 516, 281, 1051, 307, 300, 439, 295, 341, 486, 808, 1214, 558, 370, 51708], "temperature": 0.0, "avg_logprob": -0.40587571088005514, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.005998768378049135}, {"id": 912, "seek": 440196, "start": 4402.92, "end": 4408.44, "text": " if you actually train things like end-to-end, rack 2.0 style then you can also fine-tune that system", "tokens": [50412, 498, 291, 767, 3847, 721, 411, 917, 12, 1353, 12, 521, 11, 14788, 568, 13, 15, 3758, 550, 291, 393, 611, 2489, 12, 83, 2613, 300, 1185, 50688], "temperature": 0.0, "avg_logprob": -0.1249544252225054, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.0002033379569184035}, {"id": 913, "seek": 440196, "start": 4409.0, "end": 4415.72, "text": " on some use case end-to-end. So why would you just take the retrieval augmented system if you can", "tokens": [50716, 322, 512, 764, 1389, 917, 12, 1353, 12, 521, 13, 407, 983, 576, 291, 445, 747, 264, 19817, 3337, 36155, 1185, 498, 291, 393, 51052], "temperature": 0.0, "avg_logprob": -0.1249544252225054, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.0002033379569184035}, {"id": 914, "seek": 440196, "start": 4415.72, "end": 4419.64, "text": " also fine-tune it on the thing you care about? So I think in the end everybody's going to do", "tokens": [51052, 611, 2489, 12, 83, 2613, 309, 322, 264, 551, 291, 1127, 466, 30, 407, 286, 519, 294, 264, 917, 2201, 311, 516, 281, 360, 51248], "temperature": 0.0, "avg_logprob": -0.1249544252225054, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.0002033379569184035}, {"id": 915, "seek": 440196, "start": 4419.64, "end": 4423.64, "text": " all of those things and then there's questions like how do you do that efficiently so that's", "tokens": [51248, 439, 295, 729, 721, 293, 550, 456, 311, 1651, 411, 577, 360, 291, 360, 300, 19621, 370, 300, 311, 51448], "temperature": 0.0, "avg_logprob": -0.1249544252225054, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.0002033379569184035}, {"id": 916, "seek": 440196, "start": 4423.64, "end": 4430.52, "text": " why you would use adapters or things like that. I think there was another question.", "tokens": [51448, 983, 291, 576, 764, 23169, 1559, 420, 721, 411, 300, 13, 286, 519, 456, 390, 1071, 1168, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1249544252225054, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.0002033379569184035}, {"id": 917, "seek": 443196, "start": 4432.92, "end": 4437.32, "text": " I'm curious about hardware, you say it's going to become database kind of thing,", "tokens": [50412, 286, 478, 6369, 466, 8837, 11, 291, 584, 309, 311, 516, 281, 1813, 8149, 733, 295, 551, 11, 50632], "temperature": 0.0, "avg_logprob": -0.4035747482115964, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0070721921510994434}, {"id": 918, "seek": 443196, "start": 4437.32, "end": 4444.44, "text": " in fact it was part of the database but what about retrieval hardware and you know it's mine", "tokens": [50632, 294, 1186, 309, 390, 644, 295, 264, 8149, 457, 437, 466, 19817, 3337, 8837, 293, 291, 458, 309, 311, 3892, 50988], "temperature": 0.0, "avg_logprob": -0.4035747482115964, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0070721921510994434}, {"id": 919, "seek": 443196, "start": 4444.44, "end": 4451.16, "text": " because we've got so much of the learning part but what about because it's huge,", "tokens": [50988, 570, 321, 600, 658, 370, 709, 295, 264, 2539, 644, 457, 437, 466, 570, 309, 311, 2603, 11, 51324], "temperature": 0.0, "avg_logprob": -0.4035747482115964, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0070721921510994434}, {"id": 920, "seek": 443196, "start": 4452.36, "end": 4457.0, "text": " tree as I said so do you have any ideas it's just a database problem?", "tokens": [51384, 4230, 382, 286, 848, 370, 360, 291, 362, 604, 3487, 309, 311, 445, 257, 8149, 1154, 30, 51616], "temperature": 0.0, "avg_logprob": -0.4035747482115964, "compression_ratio": 1.7052631578947368, "no_speech_prob": 0.0070721921510994434}, {"id": 921, "seek": 445700, "start": 4457.0, "end": 4464.28, "text": " So I don't know if I'm allowed to say this exactly actually but so one of the biggest chip", "tokens": [50364, 407, 286, 500, 380, 458, 498, 286, 478, 4350, 281, 584, 341, 2293, 767, 457, 370, 472, 295, 264, 3880, 11409, 50728], "temperature": 0.0, "avg_logprob": -0.1635569275402632, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0005349486018531024}, {"id": 922, "seek": 445700, "start": 4464.28, "end": 4469.88, "text": " manufacturers that recently their stock has done really well they have some dedicated retrieval", "tokens": [50728, 18455, 300, 3938, 641, 4127, 575, 1096, 534, 731, 436, 362, 512, 8374, 19817, 3337, 51008], "temperature": 0.0, "avg_logprob": -0.1635569275402632, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0005349486018531024}, {"id": 923, "seek": 445700, "start": 4469.88, "end": 4480.2, "text": " hardware coming out I think sooner it might already be out. So yeah very efficient dense", "tokens": [51008, 8837, 1348, 484, 286, 519, 15324, 309, 1062, 1217, 312, 484, 13, 407, 1338, 588, 7148, 18011, 51524], "temperature": 0.0, "avg_logprob": -0.1635569275402632, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0005349486018531024}, {"id": 924, "seek": 448020, "start": 4480.2, "end": 4488.28, "text": " retrieval is a very big business. Other questions?", "tokens": [50364, 19817, 3337, 307, 257, 588, 955, 1606, 13, 5358, 1651, 30, 50768], "temperature": 0.0, "avg_logprob": -0.1825123553006154, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0010476130992174149}, {"id": 925, "seek": 448020, "start": 4499.16, "end": 4504.92, "text": " Yes I think so if you take it to the extreme so one of the big problems right now is that", "tokens": [51312, 1079, 286, 519, 370, 498, 291, 747, 309, 281, 264, 8084, 370, 472, 295, 264, 955, 2740, 558, 586, 307, 300, 51600], "temperature": 0.0, "avg_logprob": -0.1825123553006154, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0010476130992174149}, {"id": 926, "seek": 448020, "start": 4504.92, "end": 4508.44, "text": " if you contextualize an existing language model that already hallucinates", "tokens": [51600, 498, 291, 35526, 1125, 364, 6741, 2856, 2316, 300, 1217, 35212, 259, 1024, 51776], "temperature": 0.0, "avg_logprob": -0.1825123553006154, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.0010476130992174149}, {"id": 927, "seek": 450844, "start": 4509.16, "end": 4513.0, "text": " then then it's going to be kind of hard to get rid of the hallucination right so if you do", "tokens": [50400, 550, 550, 309, 311, 516, 281, 312, 733, 295, 1152, 281, 483, 3973, 295, 264, 35212, 2486, 558, 370, 498, 291, 360, 50592], "temperature": 0.0, "avg_logprob": -0.0830321558590593, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0035922867245972157}, {"id": 928, "seek": 450844, "start": 4513.0, "end": 4519.879999999999, "text": " replug on GPT-4 GPT-4 might still hallucinate so you could basically just ignore all the stuff", "tokens": [50592, 3248, 697, 322, 26039, 51, 12, 19, 26039, 51, 12, 19, 1062, 920, 35212, 13923, 370, 291, 727, 1936, 445, 11200, 439, 264, 1507, 50936], "temperature": 0.0, "avg_logprob": -0.0830321558590593, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0035922867245972157}, {"id": 929, "seek": 450844, "start": 4519.879999999999, "end": 4524.2, "text": " you retrieved and just do whatever it wants anyway so that's one of the reasons why you", "tokens": [50936, 291, 19817, 937, 293, 445, 360, 2035, 309, 2738, 4033, 370, 300, 311, 472, 295, 264, 4112, 983, 291, 51152], "temperature": 0.0, "avg_logprob": -0.0830321558590593, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0035922867245972157}, {"id": 930, "seek": 450844, "start": 4524.2, "end": 4528.679999999999, "text": " want to train the system end to end and if you take that to the extreme where like I said right", "tokens": [51152, 528, 281, 3847, 264, 1185, 917, 281, 917, 293, 498, 291, 747, 300, 281, 264, 8084, 689, 411, 286, 848, 558, 51376], "temperature": 0.0, "avg_logprob": -0.0830321558590593, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0035922867245972157}, {"id": 931, "seek": 450844, "start": 4528.679999999999, "end": 4534.2, "text": " if you can just have the language model only reason and speak so it knows English and reasoning", "tokens": [51376, 498, 291, 393, 445, 362, 264, 2856, 2316, 787, 1778, 293, 1710, 370, 309, 3255, 3669, 293, 21577, 51652], "temperature": 0.0, "avg_logprob": -0.0830321558590593, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0035922867245972157}, {"id": 932, "seek": 453420, "start": 4534.2, "end": 4539.639999999999, "text": " but it has no knowledge which all comes from somewhere else then you can't hallucinate and so", "tokens": [50364, 457, 309, 575, 572, 3601, 597, 439, 1487, 490, 4079, 1646, 550, 291, 393, 380, 35212, 13923, 293, 370, 50636], "temperature": 0.0, "avg_logprob": -0.08700414635669226, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004261902999132872}, {"id": 933, "seek": 453420, "start": 4539.639999999999, "end": 4551.08, "text": " it's really all grounded in whatever is in your index but they're so they're about hallucination", "tokens": [50636, 309, 311, 534, 439, 23535, 294, 2035, 307, 294, 428, 8186, 457, 436, 434, 370, 436, 434, 466, 35212, 2486, 51208], "temperature": 0.0, "avg_logprob": -0.08700414635669226, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004261902999132872}, {"id": 934, "seek": 453420, "start": 4551.08, "end": 4555.32, "text": " I'm sort of frustrated that a lot of people in the field misunderstand what hallucination even", "tokens": [51208, 286, 478, 1333, 295, 15751, 300, 257, 688, 295, 561, 294, 264, 2519, 35736, 437, 35212, 2486, 754, 51420], "temperature": 0.0, "avg_logprob": -0.08700414635669226, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004261902999132872}, {"id": 935, "seek": 453420, "start": 4555.32, "end": 4560.5199999999995, "text": " means like so a lot of people are conflating hallucination with a correctness or incorrectness", "tokens": [51420, 1355, 411, 370, 257, 688, 295, 561, 366, 1497, 75, 990, 35212, 2486, 365, 257, 3006, 1287, 420, 18424, 1287, 51680], "temperature": 0.0, "avg_logprob": -0.08700414635669226, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004261902999132872}, {"id": 936, "seek": 456052, "start": 4560.52, "end": 4564.52, "text": " so they're like oh the model made a mistake it hallucinated it's like no it made a mistake", "tokens": [50364, 370, 436, 434, 411, 1954, 264, 2316, 1027, 257, 6146, 309, 35212, 5410, 309, 311, 411, 572, 309, 1027, 257, 6146, 50564], "temperature": 0.0, "avg_logprob": -0.1043562522301307, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001098046312108636}, {"id": 937, "seek": 456052, "start": 4565.320000000001, "end": 4570.280000000001, "text": " that's different from hallucination hallucination I think is very specific kind of I've retrieved", "tokens": [50604, 300, 311, 819, 490, 35212, 2486, 35212, 2486, 286, 519, 307, 588, 2685, 733, 295, 286, 600, 19817, 937, 50852], "temperature": 0.0, "avg_logprob": -0.1043562522301307, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001098046312108636}, {"id": 938, "seek": 456052, "start": 4570.280000000001, "end": 4576.6, "text": " something so I have some sort of counterfactual ground truth and what I'm saying does not correspond", "tokens": [50852, 746, 370, 286, 362, 512, 1333, 295, 5682, 44919, 901, 2727, 3494, 293, 437, 286, 478, 1566, 775, 406, 6805, 51168], "temperature": 0.0, "avg_logprob": -0.1043562522301307, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001098046312108636}, {"id": 939, "seek": 456052, "start": 4576.6, "end": 4584.6, "text": " to that ground truth and so yeah I think there's a bunch of folks at Stanford also working on better", "tokens": [51168, 281, 300, 2727, 3494, 293, 370, 1338, 286, 519, 456, 311, 257, 3840, 295, 4024, 412, 20374, 611, 1364, 322, 1101, 51568], "temperature": 0.0, "avg_logprob": -0.1043562522301307, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.001098046312108636}, {"id": 940, "seek": 458460, "start": 4584.6, "end": 4587.4800000000005, "text": " measurements of hallucination and definitions and things like that", "tokens": [50364, 15383, 295, 35212, 2486, 293, 21988, 293, 721, 411, 300, 50508], "temperature": 0.0, "avg_logprob": -0.12071279475563451, "compression_ratio": 1.9251336898395721, "no_speech_prob": 0.013013126328587532}, {"id": 941, "seek": 458460, "start": 4597.56, "end": 4603.96, "text": " yeah awesome ground truth right so so hallucination is really like there there is something that is", "tokens": [51012, 1338, 3476, 2727, 3494, 558, 370, 370, 35212, 2486, 307, 534, 411, 456, 456, 307, 746, 300, 307, 51332], "temperature": 0.0, "avg_logprob": -0.12071279475563451, "compression_ratio": 1.9251336898395721, "no_speech_prob": 0.013013126328587532}, {"id": 942, "seek": 458460, "start": 4603.96, "end": 4609.0, "text": " true right so so if we're talking about like hallucination and yeah so if we're talking about", "tokens": [51332, 2074, 558, 370, 370, 498, 321, 434, 1417, 466, 411, 35212, 2486, 293, 1338, 370, 498, 321, 434, 1417, 466, 51584], "temperature": 0.0, "avg_logprob": -0.12071279475563451, "compression_ratio": 1.9251336898395721, "no_speech_prob": 0.013013126328587532}, {"id": 943, "seek": 458460, "start": 4609.0, "end": 4613.64, "text": " just general parametric language models then sort of the ground truth is whatever we consider to be", "tokens": [51584, 445, 2674, 6220, 17475, 2856, 5245, 550, 1333, 295, 264, 2727, 3494, 307, 2035, 321, 1949, 281, 312, 51816], "temperature": 0.0, "avg_logprob": -0.12071279475563451, "compression_ratio": 1.9251336898395721, "no_speech_prob": 0.013013126328587532}, {"id": 944, "seek": 461364, "start": 4613.64, "end": 4621.96, "text": " true right but we had to work for like language models making mistakes before it was called major", "tokens": [50364, 2074, 558, 457, 321, 632, 281, 589, 337, 411, 2856, 5245, 1455, 8038, 949, 309, 390, 1219, 2563, 50780], "temperature": 0.0, "avg_logprob": -0.41449476877848307, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.002589270705357194}, {"id": 945, "seek": 461364, "start": 4621.96, "end": 4634.12, "text": " mistakes yeah ground truth I guess this is solving that helps me to question that path", "tokens": [50780, 8038, 1338, 2727, 3494, 286, 2041, 341, 307, 12606, 300, 3665, 385, 281, 1168, 300, 3100, 51388], "temperature": 0.0, "avg_logprob": -0.41449476877848307, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.002589270705357194}, {"id": 946, "seek": 461364, "start": 4634.76, "end": 4639.56, "text": " are you working on ground truth per se that's around you know if I generate the building", "tokens": [51420, 366, 291, 1364, 322, 2727, 3494, 680, 369, 300, 311, 926, 291, 458, 498, 286, 8460, 264, 2390, 51660], "temperature": 0.0, "avg_logprob": -0.41449476877848307, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.002589270705357194}, {"id": 947, "seek": 463956, "start": 4640.120000000001, "end": 4646.68, "text": " saying oh well I'm a president I mean everything all the time are you sharing work on that on this", "tokens": [50392, 1566, 1954, 731, 286, 478, 257, 3868, 286, 914, 1203, 439, 264, 565, 366, 291, 5414, 589, 322, 300, 322, 341, 50720], "temperature": 0.0, "avg_logprob": -0.26332788882048236, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.002146713202819228}, {"id": 948, "seek": 463956, "start": 4647.72, "end": 4653.96, "text": " yeah so so I like the sort of silo uh mansion error as well so I think the whole point is that you", "tokens": [50772, 1338, 370, 370, 286, 411, 264, 1333, 295, 3425, 78, 2232, 25599, 6713, 382, 731, 370, 286, 519, 264, 1379, 935, 307, 300, 291, 51084], "temperature": 0.0, "avg_logprob": -0.26332788882048236, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.002146713202819228}, {"id": 949, "seek": 463956, "start": 4653.96, "end": 4660.120000000001, "text": " can you can have different indices and different definitions of ground truth and so um I think", "tokens": [51084, 393, 291, 393, 362, 819, 43840, 293, 819, 21988, 295, 2727, 3494, 293, 370, 1105, 286, 519, 51392], "temperature": 0.0, "avg_logprob": -0.26332788882048236, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.002146713202819228}, {"id": 950, "seek": 463956, "start": 4660.120000000001, "end": 4666.6, "text": " you could say I only trust uh archive or I only trust like peer review papers and not just archive", "tokens": [51392, 291, 727, 584, 286, 787, 3361, 2232, 23507, 420, 286, 787, 3361, 411, 15108, 3131, 10577, 293, 406, 445, 23507, 51716], "temperature": 0.0, "avg_logprob": -0.26332788882048236, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.002146713202819228}, {"id": 951, "seek": 466660, "start": 4667.4800000000005, "end": 4671.240000000001, "text": " so you can make decisions in your architecture during test time about what you", "tokens": [50408, 370, 291, 393, 652, 5327, 294, 428, 9482, 1830, 1500, 565, 466, 437, 291, 50596], "temperature": 0.0, "avg_logprob": -0.06570507555591817, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0026723407208919525}, {"id": 952, "seek": 466660, "start": 4671.240000000001, "end": 4678.4400000000005, "text": " define as ground truth and I also think actually that and there's a bunch of work I think happening", "tokens": [50596, 6964, 382, 2727, 3494, 293, 286, 611, 519, 767, 300, 293, 456, 311, 257, 3840, 295, 589, 286, 519, 2737, 50956], "temperature": 0.0, "avg_logprob": -0.06570507555591817, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0026723407208919525}, {"id": 953, "seek": 466660, "start": 4678.4400000000005, "end": 4683.400000000001, "text": " on this right now you can control for how how grounded you want it to be in your ground truth", "tokens": [50956, 322, 341, 558, 586, 291, 393, 1969, 337, 577, 577, 23535, 291, 528, 309, 281, 312, 294, 428, 2727, 3494, 51204], "temperature": 0.0, "avg_logprob": -0.06570507555591817, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0026723407208919525}, {"id": 954, "seek": 466660, "start": 4683.96, "end": 4688.92, "text": " so that's another kind of misconception about hallucinations like sometimes hallucinations", "tokens": [51232, 370, 300, 311, 1071, 733, 295, 41350, 466, 35212, 10325, 411, 2171, 35212, 10325, 51480], "temperature": 0.0, "avg_logprob": -0.06570507555591817, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0026723407208919525}, {"id": 955, "seek": 466660, "start": 4688.92, "end": 4693.0, "text": " are actually good right if you have a creative writing assistant and you wanted to come up with", "tokens": [51480, 366, 767, 665, 558, 498, 291, 362, 257, 5880, 3579, 10994, 293, 291, 1415, 281, 808, 493, 365, 51684], "temperature": 0.0, "avg_logprob": -0.06570507555591817, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0026723407208919525}, {"id": 956, "seek": 469300, "start": 4693.0, "end": 4698.44, "text": " some cool new ideas you want the language model to hallucinate uh so I think what you want to have", "tokens": [50364, 512, 1627, 777, 3487, 291, 528, 264, 2856, 2316, 281, 35212, 13923, 2232, 370, 286, 519, 437, 291, 528, 281, 362, 50636], "temperature": 0.0, "avg_logprob": -0.20688967550954512, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.002215790096670389}, {"id": 957, "seek": 469300, "start": 4698.44, "end": 4702.68, "text": " is kind of a tunable knob where you say like oh now you can hallucinate and now maybe you should", "tokens": [50636, 307, 733, 295, 257, 4267, 712, 26759, 689, 291, 584, 411, 1954, 586, 291, 393, 35212, 13923, 293, 586, 1310, 291, 820, 50848], "temperature": 0.0, "avg_logprob": -0.20688967550954512, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.002215790096670389}, {"id": 958, "seek": 469300, "start": 4702.68, "end": 4706.84, "text": " like really tell me the truth only", "tokens": [50848, 411, 534, 980, 385, 264, 3494, 787, 51056], "temperature": 0.0, "avg_logprob": -0.20688967550954512, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.002215790096670389}, {"id": 959, "seek": 469300, "start": 4711.72, "end": 4714.84, "text": " anything else", "tokens": [51300, 1340, 1646, 51456], "temperature": 0.0, "avg_logprob": -0.20688967550954512, "compression_ratio": 1.6158940397350994, "no_speech_prob": 0.002215790096670389}, {"id": 960, "seek": 471484, "start": 4715.16, "end": 4727.96, "text": " yeah so but the temperature that's just about how you sample right so how flat your your distribution", "tokens": [50380, 1338, 370, 457, 264, 4292, 300, 311, 445, 466, 577, 291, 6889, 558, 370, 577, 4962, 428, 428, 7316, 51020], "temperature": 0.0, "avg_logprob": -0.20689644958033707, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.002250977326184511}, {"id": 961, "seek": 471484, "start": 4727.96, "end": 4737.64, "text": " is that you sample from yes but so even if you have a low temperature it can still come up with", "tokens": [51020, 307, 300, 291, 6889, 490, 2086, 457, 370, 754, 498, 291, 362, 257, 2295, 4292, 309, 393, 920, 808, 493, 365, 51504], "temperature": 0.0, "avg_logprob": -0.20689644958033707, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.002250977326184511}, {"id": 962, "seek": 471484, "start": 4737.64, "end": 4742.360000000001, "text": " random stuff right so it just says that then you're very likely to do like really sampling", "tokens": [51504, 4974, 1507, 558, 370, 309, 445, 1619, 300, 550, 291, 434, 588, 3700, 281, 360, 411, 534, 21179, 51740], "temperature": 0.0, "avg_logprob": -0.20689644958033707, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.002250977326184511}, {"id": 963, "seek": 474236, "start": 4742.92, "end": 4754.2, "text": " um so so I think what you want to get at is is something more sophisticated than that", "tokens": [50392, 1105, 370, 370, 286, 519, 437, 291, 528, 281, 483, 412, 307, 307, 746, 544, 16950, 813, 300, 50956], "temperature": 0.0, "avg_logprob": -0.33737993240356445, "compression_ratio": 1.264367816091954, "no_speech_prob": 0.0034727042075246572}, {"id": 964, "seek": 474236, "start": 4756.92, "end": 4762.2, "text": " yeah I like the question", "tokens": [51092, 1338, 286, 411, 264, 1168, 51356], "temperature": 0.0, "avg_logprob": -0.33737993240356445, "compression_ratio": 1.264367816091954, "no_speech_prob": 0.0034727042075246572}], "language": "en"}