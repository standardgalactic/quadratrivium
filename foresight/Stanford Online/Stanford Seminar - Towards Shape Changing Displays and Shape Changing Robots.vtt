WEBVTT

00:00.000 --> 00:17.960
This is definitely one of the shortest travel trips that I've ever had to give a talk because

00:17.960 --> 00:22.960
my office is just across the street, and this is a familiar space because we ran the Human

00:22.960 --> 00:26.440
Computer Interaction Seminar in here last year.

00:26.440 --> 00:31.720
So it's great to be here today, and as Mark said, I'm one of the faculty members in the

00:31.720 --> 00:36.880
mechanical engineering department where I run the Shape Lab, but I'm also very much involved

00:36.880 --> 00:40.560
with the Stanford Human Computer Interaction Group as well.

00:40.560 --> 00:44.640
And so today I'm going to be talking about our work sort of at the intersection between

00:44.640 --> 00:49.560
yeah, human computer interaction and robotics and mechatronics systems and thinking about

00:49.560 --> 00:53.800
how can we really enable people to interact in much richer ways.

00:53.800 --> 01:00.920
I also have to apologize, I had a hand procedure done earlier today, and basically I thought

01:00.920 --> 01:06.000
it was going to be not as painful as it was, and so I'm a little bit more under the weather

01:06.000 --> 01:12.480
than normal, so you'll have to believe that this would be even more exciting normally.

01:12.480 --> 01:19.520
So speaking of hands, I've long been impressed by the dexterous ability of our hands to allow

01:19.560 --> 01:25.640
us to manipulate the world, and certainly having recently had some injury for my wrist,

01:25.640 --> 01:31.920
that's become even more clear to me how important our hands are for manipulating things in the

01:31.920 --> 01:36.840
world and really just the amazing complexity of our hands and the way in which we're able to

01:36.840 --> 01:43.360
interact so gracefully and dexterously to achieve really stillful manipulation such as here,

01:43.360 --> 01:46.760
sculpting, and clay, but all of our everyday tasks as well.

01:47.320 --> 01:54.040
But to me what's really exciting about the hand and manipulation is the role of manipulation

01:54.680 --> 02:03.320
in terms of our cognition as well. And so physical interaction is very important for not just

02:03.320 --> 02:08.680
physical manipulation for the sake of changing the world, but also for us to express our thoughts

02:08.680 --> 02:14.360
as you see me gesturing like this, but also for us to better understand information. So there's

02:14.360 --> 02:19.880
been a lot of studies that have looked at how children when they learn with abacai, for example,

02:19.880 --> 02:26.920
or abacus in plural, abacai, that they actually learn math concepts differently and better than if

02:26.920 --> 02:32.120
they learn them just on paper and pencil. And so there's something behind this idea of embodied

02:32.120 --> 02:37.720
cognition that's actually quite important. And we see that not just in terms of education,

02:37.720 --> 02:42.440
but really in terms of very specialized domains as well. So for example, this is a photograph

02:43.080 --> 02:47.960
of urban planners and architects. And what we see there is there's this, you know, basically

02:47.960 --> 02:53.080
rich interplay between the physical models that they're building, space, and the ways in which

02:53.080 --> 02:59.080
people want to interact as well. And so physical and spatial form can really help us better understand

02:59.080 --> 03:05.080
and solve problems and very complex ones in the architecture domain, but many other domains as

03:05.080 --> 03:11.320
well. And the big kind of reason behind this is that spatial manipulation, as I kind of highlighted

03:11.320 --> 03:17.080
before, really aids in spatial cognition. These two things are very tightly coupled together.

03:17.080 --> 03:22.680
And one of the great professors that we have had at Stanford, Barbara Tversky, has done a lot of work

03:22.680 --> 03:28.040
as well, looking into the role that spatial manipulation plays in our cognition and kind

03:28.040 --> 03:33.560
of the evolutionary basis for this as well. So she has a great new book out called Mind and

03:33.560 --> 03:38.840
Motion that I highly recommend you take a look at. But in many ways, this is because cognition

03:38.840 --> 03:45.160
and perception are very much coupled together through action. And we've evolved over a long

03:45.160 --> 03:50.440
period of time to really benefit from the way that we can manipulate the world to better

03:50.440 --> 03:55.640
understand it. And these things are tightly coupled together. This is even more important

03:55.640 --> 04:01.800
when we think about access to information. And so for example, these physical representations,

04:01.800 --> 04:06.680
you know, can be really important for people with different abilities. So here's a picture

04:06.680 --> 04:12.760
of a blind student at the Perkins School for the Blind in the Boston area looking or feeling,

04:12.760 --> 04:18.200
rather, a double helix model. And this is something that would be very hard for us to explain through

04:18.200 --> 04:23.160
words, but by being able to touch, feel, and manipulate it, they can, you know, much more

04:23.160 --> 04:30.200
easily understand. And this is a tactile model of Berlin's Museum Island. Again, this type of

04:30.280 --> 04:36.920
spatial information might be very hard for us to convey through text alone, but by allowing

04:36.920 --> 04:43.480
people to directly access it through touch and through our haptic sense, we can much more readily

04:43.480 --> 04:48.840
access it. But if we look at the ways in which we traditionally interact with computing and with

04:48.840 --> 04:56.040
information, not much has changed since 30 years ago in some ways for the ways in which we interact.

04:56.040 --> 05:00.520
And historically, it's been the case that we really haven't been able to leverage our sense of

05:00.520 --> 05:06.440
touch in that. And if we look towards, you know, basically newer interfaces for interaction such

05:06.440 --> 05:11.720
as in virtual reality, a lot of times we really get these benefits of spatial interaction where

05:11.720 --> 05:17.160
here I can move around and interact and collaborate with someone else. But when we reach out to feel

05:17.160 --> 05:22.520
something or touch something, we're not really receiving any meaningful feedback. And so one of

05:22.520 --> 05:27.000
the key questions that I've been interested in as well as many other people in the field of haptic

05:27.000 --> 05:32.760
interfaces is, you know, what if we could reach out and touch the void? And so during my PhD at

05:32.760 --> 05:37.720
the MIT Media Lab, we were really interested in trying to merge between the physical and the digital

05:37.720 --> 05:44.680
world and bring that sense of physical touch into the real world so that if we were, you know, two

05:44.680 --> 05:49.800
people working around a workstation, we could look at each other, see each other, and also directly

05:49.800 --> 05:56.200
interact with a surface and be able to feel it. And so let me show you what that might look like.

05:56.200 --> 06:02.760
So here's a picture of a new type of haptic display. We call this a shape display that we developed at

06:02.760 --> 06:07.880
the MIT Media Lab with my advisor, PhD advisor Hiroshi Ishii and my colleague Daniel Lightinger,

06:07.880 --> 06:11.960
where we could, you know, really reach out and sort of sculpt with digital clay,

06:11.960 --> 06:17.400
or we could reach out and new interface elements could appear that have the correct affordances

06:17.400 --> 06:23.480
and ergonomics for how we might want to interact. Or we could render different types of 3D models in

06:23.480 --> 06:30.600
sort of real time and be able to, again, see them in 3D but also touch them. So this type of display

06:30.600 --> 06:35.880
had been sort of considered before by a number of other researchers, but here we were able to really

06:35.880 --> 06:40.600
look at what are these meaningful interaction techniques and start to look at some example

06:40.600 --> 06:45.640
applications as well. So this was a project that we worked on with Tony Tain, who was a

06:45.720 --> 06:51.160
formerly a practicing urban planner. And as I said before, you know, the spatial models that

06:51.160 --> 06:55.960
urban planners and architects build are often very important for how they think and consider this

06:55.960 --> 07:00.520
space. But they're, you know, basically they would make one set of full models and then not

07:00.520 --> 07:06.280
be able to change anything. And so here we can in real time change the models, but also have other

07:06.280 --> 07:11.640
simulations that we can load on top of that and be able to, you know, edit, manipulate and collaboratively

07:11.640 --> 07:17.880
work together. So this sort of gives a hint for what this new type of interaction with what we

07:17.880 --> 07:23.720
call shape changing displays might be like. And so when I came to Stanford, I guess now seven-ish

07:23.720 --> 07:29.480
years ago or so, I started the Shape Lab to really deeper investigate these types of systems,

07:29.480 --> 07:34.600
but also work on some other areas as well. And so in our group, we've been working on new types of

07:34.600 --> 07:38.840
haptic displays, like the ones that I've shown here. And that's really what I'm going to spend

07:38.840 --> 07:43.960
the most of today talking about. But I quickly wanted to mention two other areas that our group

07:43.960 --> 07:49.320
has also been working on as well, and that are kind of related to the robotics areas. And so

07:49.320 --> 07:54.760
the next area that we've also been thinking about is this notion of ubiquitous robotic interfaces.

07:54.760 --> 08:01.000
And really this idea harkens back to someone who maybe some of you know of, but Mark Weiser,

08:01.000 --> 08:05.880
who sort of started the field of ubiquitous computing that we really live in today, where

08:05.960 --> 08:11.080
there's, you know, in this room around us, many different sensors and actual or sorry, many different

08:11.080 --> 08:18.360
sensors and displays, there's probably, you know, at least 100, if not 1000 computers in this room

08:18.360 --> 08:23.160
right now. And so the question that we had is sort of what would it be like in the future if we

08:23.160 --> 08:30.920
could have actuation and robotics be embedded in our environment as easily as the computing and

08:30.920 --> 08:36.760
displays that we have today. And so with my PhD student, who's now a professor at Simon Frazier

08:36.760 --> 08:42.600
University, Lawrence Kim, we created this platform of small mobile robots that we could then explore.

08:42.600 --> 08:48.200
And again, here we can have them display information like this. But we also looked at ways in which

08:48.200 --> 08:54.440
they could be embedded in our environment, like on our desk, move around to display information when

08:54.440 --> 08:59.480
we need, but also manipulate things in the environment or go about, you know, remotely

08:59.480 --> 09:04.680
sensing different things as well. And we think this kind of opens up some interesting opportunities

09:04.680 --> 09:11.160
to think about, again, where does this line between robot and environment end. And so that's

09:11.160 --> 09:16.360
something that we're also quite interested in and kind of intersects a lot with human robot

09:16.360 --> 09:21.400
interaction. And the last area that we're doing work in is in shape changing robotics, which is

09:21.400 --> 09:26.600
something that Mark highlighted before. And a lot of that has to do with the enabling technologies

09:26.600 --> 09:32.440
that we are looking at to make shape changing displays and also shape changing robots. And so

09:32.440 --> 09:36.840
if you think back to that first example haptic display that I showed that is sort of like a

09:36.840 --> 09:43.240
2.5D surface display, one of the things that we have been thinking about is how might we sort of

09:43.240 --> 09:50.360
make full 3D shape changing displays. This is sort of the holy grail of some of our work in terms of

09:50.360 --> 09:55.480
thinking about, oh, how could we actually, you know, feel a whole entire dolphin and actually have it

09:55.480 --> 10:01.160
be able to change shape between different surfaces. And so as one of my four of our PhD students,

10:02.280 --> 10:07.320
Zach Hammond and a collaborator of ours, Nathan Yusevich, from Allison Otelmore's group,

10:07.880 --> 10:12.600
we're trying to think about how might we make this vision a reality. And so we got inspired by

10:13.320 --> 10:18.520
basically balloon animals to think about new ways that we might approach this. And the interesting

10:18.520 --> 10:23.320
thing about balloon animals is that you have started out with something very simple, like

10:23.400 --> 10:29.000
essentially inflated tube. And then we can pinch it at certain points, knot it together,

10:29.000 --> 10:34.360
and basically create many different shapes out of the same, you know, simple balloon idea.

10:34.360 --> 10:40.520
And so we tried to bring this idea to the field of soft robotics where we could have basically an

10:40.520 --> 10:46.440
inflated beam. And as opposed to relying on pumps to move and actuate different areas,

10:46.440 --> 10:52.520
what if we created a pinch point in that beam? And then what if we could move that pinch point around?

10:53.320 --> 10:57.320
And then basically what that allows us to do is create some type of system that can really

10:57.320 --> 11:04.040
dramatically change shape by putting a robot roller node on that surface and driving it around and

11:04.040 --> 11:10.040
then creating what we call sort of this idea of a isoparametric type of robot where we have a fixed

11:10.040 --> 11:17.160
length of inflated tube, but then we can change kind of the overall basically geometry, even though

11:17.160 --> 11:23.480
it has a fixed topology. And so here is, you know, one element inside that, but we can place

11:23.480 --> 11:29.080
these together through other kinematic connections to create some type of tetrahedral robot that we

11:29.080 --> 11:34.200
can then basically a truss robot that we can then really dramatically change shape. And again,

11:34.200 --> 11:38.920
we get these benefits of having this constant volume of air that we don't need to really pump

11:38.920 --> 11:44.360
around, but instead using motors to move these pinch points or buckle points around. And so

11:44.360 --> 11:51.080
here's an example of what those types of basically truss robots could look like. And so this is,

11:51.080 --> 11:56.840
you know, a fairly large device about, you know, this size and it's able to pretty dramatically

11:56.840 --> 12:01.720
change shape and move around. And this is all shown in real time. And so it can, you know,

12:01.720 --> 12:08.120
locomote by basically doing some type of punctuated rolling, change its shape and move around. But

12:08.120 --> 12:16.440
it can also go ahead and manipulate objects as well. So it can use the geometry of itself to

12:16.440 --> 12:20.920
basically grasp an object and then be able to pick it up and actually do some interesting

12:20.920 --> 12:27.000
in-hand manipulation as well. And so we think that this kind of idea of having these large

12:27.000 --> 12:32.040
shape-changing truss robots has some interesting applications in terms of thinking about new

12:32.040 --> 12:37.480
ways that we could be able to locomote, manipulate, and also use the shape change to afford different

12:37.480 --> 12:43.000
types of interaction in the environment. And so we've also done some work to look at the

12:43.000 --> 12:48.040
modeling and kinematic control and planning of this as well. So Zach and our group did a lot of

12:48.040 --> 12:52.840
work on grasp optimization planning with these types of robot. And we have a collaboration

12:52.840 --> 12:57.240
with Matt Schwager looking at decentralized control of these types of systems as well.

12:58.360 --> 13:04.280
So I won't go into too many details in that area today, but I just wanted to give you kind of an

13:04.280 --> 13:08.200
overview of some of the different things that we're working on in our group. But today I'm really

13:08.200 --> 13:14.040
going to focus on these types of shape-changing or haptic displays. And I'm going to start out by

13:14.040 --> 13:19.560
talking a little bit about the way in which we approach these problems. And really the way in

13:19.560 --> 13:24.200
which we like to think about this in our group is really focusing very deeply on specific

13:24.200 --> 13:30.280
applications and needs, working with domain experts in those areas, and then trying to learn more

13:30.280 --> 13:35.160
about what are the enabling technologies or the requirements for the enabling technologies

13:35.720 --> 13:41.240
to actually make these systems work. And then that kind of feeds back into other applications

13:41.240 --> 13:47.000
and needs. And we think this is kind of a nice paradigm for working on, you know, basically

13:47.000 --> 13:52.520
new technology to make these things possible. So I wanted to start by talking about two vignettes

13:52.520 --> 13:57.400
of specific applications that we've been working on. And the first is in car design. And the second

13:57.400 --> 14:02.200
is in making, making accessible to sort of highlight some of the challenges in making these

14:02.200 --> 14:08.040
systems actually useful. And so many of you might be familiar with, you know, car design in general

14:08.040 --> 14:14.120
and have seen these types of clay models that basically industrial designers and human factors

14:14.120 --> 14:22.200
experts create to prototype and test car designs. And so we had been reached or we have been working

14:22.200 --> 14:29.080
with Volkswagen who's trying to transition from making these clay models, which end up costing,

14:29.080 --> 14:34.600
you know, something on the order of $100,000 per clay model. So it's very expensive and time consuming

14:34.600 --> 14:41.640
to make those and moving instead to using virtual reality to prototype and test at steel, especially

14:41.640 --> 14:46.440
in terms of working with different stakeholders that they care about, as well as the human factors

14:46.520 --> 14:52.120
aspects of like the interior of the car as well. And so what they're trying to do is basically

14:52.120 --> 14:57.000
transition from this, you know, very physical style of doing things to now move into the virtual

14:57.000 --> 15:02.280
reality systems. But what they found is that, for example, in the case of the interior of the car

15:02.280 --> 15:08.120
design, what they call the seating buck and the human machine interaction, basically, they'll be

15:08.120 --> 15:13.480
able to load up different car designs and virtual reality. And then they have these very fancy seating

15:13.480 --> 15:19.080
butts to adjust the position and height and steering wheel position to basically create any

15:19.080 --> 15:24.680
different kind of car. But then you go and reach out and try to touch the HMI, the human machine

15:24.680 --> 15:29.640
interface part of it, and the dashboard. And basically, you, you know, again, reach out and

15:29.640 --> 15:34.920
touch nothing or touch air. And so one thing that they end up doing is creating these, you know,

15:34.920 --> 15:41.320
basically machined or milled foam models that they can then place inside this seating buck

15:41.320 --> 15:45.640
to then be able to go ahead and test. But it turns out it takes a long time for them to actually be

15:45.640 --> 15:51.000
able to create these full models. They can't switch something, change things on, on demand, etc.

15:51.000 --> 15:56.680
And so we had this idea of what if we could create, you know, interactive seating buck,

15:57.640 --> 16:02.200
basically simulator, where we could create this, you know, basically surface that we could change

16:02.200 --> 16:08.120
in real time to allow us to explore basically the different HMI interactions and some of the

16:08.120 --> 16:13.720
ergonomic issues as well. So we started out working with them on this concept and trying to create a

16:13.720 --> 16:19.320
new generation of our tactile displays to be able to do this. So this was led by Alexa Sue.

16:20.120 --> 16:24.600
And so she created this, you know, basically, this is about the smallest that you can do with

16:24.600 --> 16:30.920
kind of like low cost off the shelf DC actuators. So this has a direct drive between them with

16:30.920 --> 16:35.640
the lead screw and, you know, many, many actuators and we made this modular so you can stack them

16:35.640 --> 16:40.920
together. And we also, you know, looked at the integration with this as well as in virtual

16:40.920 --> 16:46.600
reality. And then we brought it back to our collaborators at Volkswagen, particularly like

16:46.600 --> 16:52.280
the industrial designers and the, you know, human factor specialists. And they sort of said, Oh,

16:52.280 --> 16:57.480
hey, this is great. We like the idea. But, you know, wouldn't it be great if these could be a lot

16:57.480 --> 17:03.160
higher resolution and much cheaper? So, you know, in terms of the types of things that they're looking

17:03.160 --> 17:09.240
for, they were not still there with this technology. And again, Oh, wouldn't it be nice if, you know,

17:09.240 --> 17:13.800
we could create many of these, not just, you know, basically, this device here cost something like

17:13.800 --> 17:19.720
$6,000 in parts to create. So wouldn't it be nice if we could, you know, basically have this be

17:19.720 --> 17:25.480
higher resolution and, you know, basically much larger in scale and lower in cost. So those were

17:25.480 --> 17:31.400
some of the feedback that we got, which sort of makes sense in some ways. But then I wanted to

17:31.400 --> 17:36.040
highlight another application area that we've been thinking about that also informs some of these

17:36.040 --> 17:41.720
other challenges. And so this is in the area of still computerated design and this idea of making,

17:41.720 --> 17:46.920
making accessible. And so many of you are probably very familiar with the, the making movement or

17:46.920 --> 17:54.760
maker movement that kind of has gone on over the past 15 or 20 years. And basically, you know,

17:54.760 --> 17:59.480
one of the great things about that is that it's really empowered a lot of people and, and served

17:59.560 --> 18:05.320
as a great way to involve more people in STEM education. But that doesn't mean that all people

18:05.320 --> 18:11.800
are unable to do that. And particularly a lot of the tools that we use for making, especially in

18:11.800 --> 18:16.440
terms of computerated design are not accessible to people that are blind or visually impaired.

18:16.440 --> 18:22.280
And so those people have historically been excluded from those areas. And so while 3D printing can

18:22.280 --> 18:27.960
be very helpful in terms of supporting accessible education through the use of creating tactile

18:27.960 --> 18:33.400
graphics or other types of materials that blind people can touch and feel, there's really this

18:33.400 --> 18:38.840
lack of authoring tools for blind people for them to be able to be the designers and engineers

18:38.840 --> 18:44.440
themselves. And so the big problem with these, essentially with these systems that exist for

18:44.440 --> 18:50.040
computerated design is that they're all based on basically graphical user interfaces, where you

18:50.040 --> 18:55.560
have to directly manipulate with a mouse and a keyboard, as well as the computer screen to be

18:55.640 --> 19:01.000
able to select and control many features. And that's really great for people that are cited, right?

19:01.000 --> 19:06.840
We've moved beyond command line interfaces to this direct, direct manipulation type of interface.

19:06.840 --> 19:10.440
But for someone that's blind and visually impaired, basically the main way that they

19:10.440 --> 19:15.640
interact with computers is through different types of screen readers, which basically provide audio

19:15.640 --> 19:20.600
feedback. And so how might we think about ways in which they could still be able to use this?

19:20.600 --> 19:28.280
So there is some work on using text-based editors for creating geometries. So for example, OpenSCAD

19:28.280 --> 19:34.840
is a computed solid geometry modeler where you write basically in a declarative programming

19:34.840 --> 19:39.800
language to define the geometry. And there's been some great work at the Dimensions Project at the

19:39.800 --> 19:44.760
New York Public Library, Chansey Fleet there, in terms of basically using that for people that are

19:44.760 --> 19:49.240
blind or visually impaired to write code to then be able to 3D print something. But if you can

19:49.240 --> 19:53.800
imagine and you've used a 3D printer, you know they're not particularly fast. And so basically

19:53.800 --> 19:59.480
you write some code. And then while we, you know, people that are cited would be able to, you know,

19:59.480 --> 20:03.240
instantly see the changes that we're making, someone that's blind and visually impaired would

20:03.240 --> 20:09.960
have to basically use a 3D printer, wait somewhere between an hour to, you know, 10 hours to find

20:09.960 --> 20:14.600
out, oh, the change that I made was that exactly what I wanted. And so one of the questions we've

20:14.600 --> 20:20.280
been asking is, how might dynamic tactile feedback, you know, support this type of interaction? And so

20:20.280 --> 20:26.200
again, my former PhD student, Alexa Su, as well as some members from the blind and visually impaired

20:26.200 --> 20:31.160
community in the Bay Area, Sun Kim, who's an Access Technology Specialist at the Vista Center for

20:31.160 --> 20:37.800
the Blind, as well as Josh Miele, who is an amazing blind engineer and researcher who's now at

20:38.600 --> 20:44.440
at Amazon working on accessibility there, we all work together to create a co-design with

20:44.440 --> 20:49.640
other blind makers, a tool that allows people to use these types of tactile shape displays we've

20:49.640 --> 20:54.920
been talking about to in real time have that feedback. So here, basically you're able to write

20:54.920 --> 21:01.160
code in open SCAD, and then in real time be able to touch and feel the geometry and sort of be able

21:01.160 --> 21:07.080
to understand what it is. And so one of the questions that we had, and then 3D print the design.

21:07.080 --> 21:12.040
So one of the questions we had is, you know, what are the types of interactions that we need to port

21:12.040 --> 21:17.560
from these direct manipulation interfaces that are really essential for basically helping blind

21:17.560 --> 21:22.120
and visually impaired people understand that. So we did a lot of different co-design sessions to try

21:22.120 --> 21:26.680
to understand what these challenges are. And one of the things that's interesting, it turns out that

21:26.680 --> 21:31.880
section views are even more important, you know, basically for blind and visually impaired people

21:31.880 --> 21:38.440
than for in our, you know, sighted base CAD systems. Because, you know, essentially, you know,

21:38.440 --> 21:44.280
there's no notion of transparency in these types of displays. You're basically just feeling the top

21:44.280 --> 21:49.000
surface of the convex hole or whatever. So you can't have any way to display, at least with these

21:49.000 --> 21:55.720
displays, this notion of transparency, which we rely on a lot. So we have some, you know, basically

21:55.720 --> 22:00.600
promising interactions that we think are possible with this and compared to like a single point haptic

22:01.160 --> 22:07.480
device. We might have less high resolution in terms of the spatial component. But by allowing

22:07.480 --> 22:12.680
people to touch it with their whole hands, they're actually much more easily able to understand what

22:12.680 --> 22:18.040
it is and the shape. So we think this is a promising direction. But if we look at the types of geometries

22:18.040 --> 22:22.600
that the people we were working with are able to create with this type of tactile display,

22:22.600 --> 22:29.160
we can see they're quite limited. And so this was a big issue that we found. But there is this real

22:29.240 --> 22:34.200
benefit from having this tight coupling of the iteration. And so we think this is a promising

22:34.200 --> 22:40.920
direction and very promising in terms of this notion of real time iterative feedback. And also

22:40.920 --> 22:45.000
that people in the blind and visually impaired community really do want to be designers and

22:45.000 --> 22:50.920
makers of their own, you know, basically technology. And this type of system can be very empowering

22:50.920 --> 22:57.240
for them. But, you know, there's still these big issues in terms of resolution. So if we think

22:57.240 --> 23:02.760
about a computer display that we might use being very high resolution versus this pin display that

23:02.760 --> 23:08.440
I'm showing here, you know, there's a big gap in terms of the resolution and potentially the

23:08.440 --> 23:13.880
under understandability of that geometry. And then also in terms of access. So if each of those

23:13.880 --> 23:19.400
tactile displays that still low resolution costs upwards of $6,000, that means that, you know,

23:19.400 --> 23:25.080
many people can't have access to it, as opposed to, you know, $500 laptop that we could use to

23:25.080 --> 23:30.840
teach cat. So across these different application areas, we really saw these big challenges in terms

23:30.840 --> 23:36.280
of cost, steel and resolution, as well as interaction techniques. But I'm not going to talk as much

23:36.280 --> 23:41.000
about that today. And so a lot of the work that we've done in the past couple of years has been

23:41.000 --> 23:47.240
trying to address or mitigate some of these issues of cost, steel and resolution. And we've kind of

23:47.240 --> 23:53.480
taken two different approaches to try to tackle that on our group. The first is in trying to create

23:53.560 --> 24:00.360
new and novel technical solutions, hardware solutions to solve that. And the second area is on

24:00.360 --> 24:06.360
kind of what we call kind of perceptual illusions or using basically perceptual engineering to think

24:06.360 --> 24:12.280
about how do we basically use the existing hardware that we have, but maybe use some clever

24:12.280 --> 24:17.480
tricks in terms of how we integrate information together to improve the perceived resolution.

24:17.480 --> 24:22.840
And so I'm going to talk about both of these areas. So the first area that I'll talk about

24:22.840 --> 24:28.840
is on creating higher resolution tactile displays. And this has been kind of a big challenge in the

24:28.840 --> 24:33.880
field of haptics for a long time. And there's sort of this big tradeoff or dichotomy between people

24:33.880 --> 24:39.720
that are trying to make really high bandwidth tactile displays versus people that are trying to

24:39.720 --> 24:45.080
make kind of these shape displays or tactile displays that maybe don't need to move as fast

24:45.080 --> 24:51.080
or maybe don't need to move at all. And how do we find this balance between the two? And so one of

24:51.080 --> 24:56.040
my former PhD students, Ty Zane, started to think about ways in which we could really try to push

24:56.040 --> 25:02.040
the resolution if we trade off on that bandwidth side of things. And so if we think about this

25:02.040 --> 25:07.960
design space or list of design requirements for like the ideal or ultimate tactile display,

25:07.960 --> 25:12.440
there's a lot of different things that we might consider, one of which is like, what is the necessary

25:12.440 --> 25:18.040
resolution? And so we can look to the haptics and psychophysics literature for some intuition

25:18.040 --> 25:24.440
around that. Basically, if we think about just statically touching a shape, something like that,

25:24.440 --> 25:31.240
we need to be in that one or 1.25 millimeter range for us to be able to not really be able to feel

25:31.240 --> 25:35.960
individual pins. So you could think about this idea of the retina display that you might be

25:35.960 --> 25:41.960
familiar with from Apple's marketing, where if you look at an iPhone today, you can't see where one

25:41.960 --> 25:47.640
pixel ends and the next begins. Basically, this two point discrimination threshold is kind of

25:48.040 --> 25:53.320
the same concept. And so we want to be in that one to one point two five millimeter range. But

25:53.320 --> 25:58.840
that's I would say a very generous, you know, basically ballpark estimate, because actually

25:58.840 --> 26:04.120
if you start to move your finger, then you can basically feel, you know, down to, you know,

26:04.120 --> 26:09.480
tens of microns or below. So you can feel, you know, a single hair very easily. But that's relying

26:09.480 --> 26:16.200
much more on your, you know, basically cutaneous information from vibration and essentially texture

26:16.200 --> 26:22.760
information. But if we just think about gross shape, this 1.25 millimeters gets us pretty close.

26:23.560 --> 26:29.000
So as I said, people have been trying to work on this problem for a long time. And there's a lot

26:29.000 --> 26:33.480
of different approaches in our own work. We've used like, you know, basically mechanical linear

26:33.480 --> 26:39.080
actuators and also different types of pneumatic actuators, which have again, challenges in terms

26:39.080 --> 26:45.080
of scaling these down, as well as, you know, basically the high cost of the actuators and

26:45.080 --> 26:50.680
being able to steal those together. Other people have created like electromagnetic tactile displays.

26:50.680 --> 26:55.720
But in general, those have some challenges as you scale them down, because the magnetic,

26:55.720 --> 27:00.520
electromagnetic fields start to bleed in with each other. So there was some interesting work from

27:01.320 --> 27:06.760
Juan Zarate and Herbert Shea in terms of thinking about electromagnetic shielding for this, but

27:06.760 --> 27:11.480
they're still kind of on the centimeter steel type of size or other people using

27:11.480 --> 27:16.440
electroactive polymers. And again, all of these are really thinking about this idea of, you know,

27:16.440 --> 27:23.320
how do we make a really fast or high bandwidth tactile display. And so our, the intuition

27:23.320 --> 27:28.760
that Kai had was, okay, let's not focus on the high bandwidth aspect. But instead, what if we had

27:28.760 --> 27:34.280
something that's much more like e-ink, where, oh, we're not changing it very frequently, we're

27:34.280 --> 27:40.520
going to kind of refresh the whole, the whole thing. How do we instead have maybe clutches or

27:40.520 --> 27:45.960
brakes that we could engage or lock when we need them to, but then have one global actuator that

27:45.960 --> 27:51.800
might change everything. And so by trading off on this high bandwidth or the temporal domain,

27:52.360 --> 27:57.240
or frequency domain, and instead focusing on the spatial, what might we be able to do?

27:57.240 --> 28:03.240
And so the technology that we sort of came to in terms of a good, good trade off between

28:03.240 --> 28:09.160
this sort of high force density in terms of braking or clutching, and then also the steel

28:09.240 --> 28:15.080
ability is electrostatic adhesion. And so many of you are probably familiar with the electrostatic

28:15.080 --> 28:18.280
fact where, you know, basically you have a balloon, you rub it on your hair,

28:18.280 --> 28:23.720
and it sticks to your head. And so this has long been used in different industries, for example,

28:23.720 --> 28:31.560
in like wafer chucking in the, in the semiconductor industry, and paper handling and other things.

28:31.560 --> 28:38.280
And then over the past 15 or so years, it's really come into vogue in the robotics community as well.

28:38.280 --> 28:43.320
So probably many of you are familiar with it, but, and also very commonly used in the MEMS

28:43.880 --> 28:49.640
steel devices as well. And so basically we started to think about how do we make these millimeter

28:49.640 --> 28:55.720
steel, you know, basically high force density clutches, and what are the different techniques

28:55.720 --> 29:00.840
that we might need to, to use to do that. And we think that this is a promising technology for

29:00.840 --> 29:06.840
this type of, you know, refreshable display. And so basically here we can see kind of an example

29:06.840 --> 29:12.440
of one of these displays where we have essentially a dielectric thin film that we've then patterned

29:12.440 --> 29:17.240
with these interdigitated electrodes that are on the order of, you know, basically a millimeter

29:17.240 --> 29:24.680
across. And then we can basically turn on an electric field that then basically induces some

29:24.680 --> 29:31.160
charge on these brass or different types of metal pins that, and then locks them into place.

29:31.720 --> 29:41.720
What does this actually look like in action? Okay. Yeah. So basically we can raise up,

29:41.720 --> 29:47.160
so it's a refreshable display. So essentially we, now we unlock all the pins, we raise them up,

29:47.160 --> 29:53.880
and then as we move it down, we lock them into place. And so the electrostatic clutches can turn

29:53.880 --> 29:58.840
on and off very quickly, giving us very high precision in terms of linear positioning,

29:59.480 --> 30:05.080
and are relatively high force compared to their size. So again, the basic operating principle

30:05.080 --> 30:10.520
is that we have this, you know, interdigitated electrode that's serving as this clutch that's

30:10.520 --> 30:15.640
on one side of a high dielectric constant thin film. And then on the other side, there's a pin,

30:15.640 --> 30:22.200
and basically we induce opposite charge on the pin. And that basically creates this electrostatic

30:22.200 --> 30:28.040
force. And so kind of very simple model of how this electrostatic force works is very similar

30:28.120 --> 30:33.400
to the parallel plate capacitor equation, where essentially, you know, the things that matter

30:33.400 --> 30:40.360
are essentially the dielectric constant of the thin film, the contact area between the metal

30:40.360 --> 30:47.800
pin and the dielectric film and the electrodes on the other side, the voltage as well as the film

30:47.800 --> 30:53.240
thickness. And so historically in kind of robotics and other applications, people have really pushed

30:53.240 --> 30:59.480
on the voltage as the thing to kind of improve the the actuation force. Here we have some

30:59.480 --> 31:04.600
challenges in doing that because one, we're dealing with people and, you know, basically some of these

31:04.600 --> 31:10.600
very high, you know, 10 kilovolt types of range, a very small amount of current can actually be

31:10.600 --> 31:16.200
not very good for people. So that's one issue. And then the other issue is we want to have,

31:16.200 --> 31:21.080
you know, not just one of these actuators, we want to have tens of thousands of these actuators.

31:21.080 --> 31:27.080
And so how do we have, you know, very small and low cost transistors that allow us to, you know,

31:27.080 --> 31:32.440
steal this in terms of production? And so we ended up trying to push more on the thin,

31:32.440 --> 31:37.880
thinness and also the high dielectric constant as things that we could push on as opposed to

31:37.880 --> 31:43.480
the voltage because of those two constraints. So this is kind of, you know, basically what the

31:43.480 --> 31:50.040
actual device looks like when we fabricate these and using the UV laser cutter that Mark's lab

31:50.040 --> 31:55.400
or actually Alison Okamura's lab. Well, I forget where it is now, but basically between Mark and

31:55.400 --> 32:04.200
Alison, exactly. Shared facility, which is great. Basically, that allows us to, you know, very easily

32:04.200 --> 32:09.160
fabricate these and test out different patterns. And so we use PVDF, which is a high dielectric

32:09.160 --> 32:14.360
constant in film and a very small sheet of it. And then we have gold that's sputtered directly

32:14.360 --> 32:19.640
on it or some other aluminum, for example. And then we laser a blade off the parts that we don't

32:19.640 --> 32:25.320
want. So the actuation principle is very simple in terms of, you know, we have these clutches

32:25.320 --> 32:30.760
and then we lower down the platform lock when the pins get to the right place. We turn on the

32:30.760 --> 32:35.960
clutch and then we create the whole pattern. And then when we want to erase it or refresh it,

32:35.960 --> 32:41.080
we just turn them off and then move the pin up and down. And so we can get, again, very high

32:41.080 --> 32:46.680
spatial accuracy in terms of the, you know, basically the linear positioning of the height

32:46.680 --> 32:53.000
because we can turn them, turn on and off the clutches and about, you know, on the order of

32:53.000 --> 33:00.840
10 milliseconds or so, which allows us to have pretty high spatial resolution. We've also done a

33:00.840 --> 33:06.760
lot of work on quasi-static loading as well. And unfortunately, the, you know, basically other

33:06.760 --> 33:11.400
people as well. So for example, Steve Collins lab has done a lot of work on these electrostatic

33:11.480 --> 33:16.920
types of clutches and also found that, you know, basically the back of the envelope calculations

33:16.920 --> 33:21.320
don't really end up matching very well with performance because of the effective contact

33:21.320 --> 33:26.760
area. And so we've done a lot of work also on data-driven modeling for this. But basically,

33:26.760 --> 33:32.920
we think these clutches, you know, on the order of, you know, 50 to 100 grams of force for the

33:32.920 --> 33:37.720
areas that we're looking at, which, you know, is not that much force. But if we think about

33:37.720 --> 33:42.120
the contact force that's necessary as you're exploring something, that's on the order of

33:42.120 --> 33:48.680
51 grams of force. And that would likely be spread across multiple pins as well. So we think we're

33:48.680 --> 33:53.560
in the right, right ballpark for this. And my current student, Ahad, who's there now, is trying

33:53.560 --> 33:59.000
to work on improving the performance and thinking about other things. So we've done some user testing

33:59.000 --> 34:05.000
as well to explore how well these tactile displays work. And it seems like it's a promising direction

34:05.000 --> 34:10.120
and seems to be working very well. So we think this, you know, basically these electrostatic

34:10.760 --> 34:17.080
pin displays are kind of a promising approach to really pushing the resolution and low-cost

34:17.080 --> 34:22.280
aspects of these types of refreshable tactile displays. So that's something that we're quite

34:22.280 --> 34:28.760
excited about. And we've been able to achieve sort of this 1.5 or 1.7 millimeter pitch as well.

34:30.040 --> 34:33.720
One of the challenges with these types of displays, though, is that they end up creating

34:33.720 --> 34:38.440
these very discrete types of shapes, right? And so it's possible that, you know, we might want to

34:38.440 --> 34:44.600
have more continuous shapes that could be approximated better with some other method. And also,

34:44.600 --> 34:51.320
maybe there's a way to do that where we trade off, you know, basically, and basically are able to have,

34:51.320 --> 34:57.880
you know, basically more continuous shapes with fewer number of actuators. And so Ahad, who recently

34:57.880 --> 35:04.600
had a paper that was accepted to ICRA, that's on thinking about how might we make these basically

35:04.600 --> 35:09.960
more continuous shape displays and what are techniques we could use to sort of have sort of a

35:09.960 --> 35:15.400
monolithic manufacturing process where we can kind of create these in one go. And so Ahad has been

35:15.400 --> 35:22.360
working on thinking about kind of the combination of electrostatic and electroadhesive actuation

35:22.440 --> 35:28.840
with auxetic materials to be able to create basically the shape-changing continuous displays

35:28.840 --> 35:36.120
where we're able to vary the curvature of them by locking individual cells in an auxetic grid

35:36.120 --> 35:41.800
or an auxetic network. And so here's an example of that shown here. And so basically, the idea is

35:41.800 --> 35:48.040
that we have, you know, work in auxetic skins where we can basically have these different patterns that

35:48.040 --> 35:55.720
can expand basically differently based upon, you know, how much, you know, strain there is in the

35:55.720 --> 36:01.880
system. And what we're doing is then locking some of these cells in this auxetic pattern. And what

36:01.880 --> 36:07.480
that does is it means that there's going to be very different local strain concentrations that end up

36:07.480 --> 36:13.160
as you inflate this surface being able to create different geometries. And so basically, there's

36:13.160 --> 36:18.600
been exciting work in the computer graphics field as well as in other areas on basically being able

36:18.600 --> 36:25.480
to computationally design these auxetic patterns so that you can then create some arbitrary geometry

36:25.480 --> 36:31.880
that's beyond sort of like what a develop surface could be. And our kind of contribution here is

36:31.880 --> 36:38.600
to think about, oh, as opposed to being able to, you know, essentially pre-plan and create a custom

36:38.600 --> 36:44.280
auxetic pattern that could create some given shape, could we essentially create a smart skin

36:44.280 --> 36:51.480
where we can change that local amount that each cell can open and close in real time or at run time.

36:51.480 --> 36:58.200
And so basically, the way that we do that is by having each auxetic cell essentially be electrostatic

36:58.920 --> 37:04.520
break or clutch that can, you know, basically either open or close, depending on how much

37:04.520 --> 37:09.000
voltage we're applying across them. And so what this means is that we want to have essentially

37:09.000 --> 37:15.400
an auxetic pattern with a very large surface area, because as I said before, the electrostatic force

37:15.400 --> 37:20.760
is basically proportional to the amount of surface area that we have. And so essentially, you know,

37:20.760 --> 37:26.200
we've looked at different patterns, but this one here, you know, again, has a very large surface area.

37:26.200 --> 37:32.360
And so what we end up doing is taking two of these sheets and then rotating them off phase

37:32.440 --> 37:38.760
so that basically, essentially, there's a lot of overlap between them. And we can essentially lock

37:38.760 --> 37:44.280
which parts will expand and which parts work. So here you get an idea of what that single cell

37:44.280 --> 37:53.560
might look like. And so we have two. Yeah, so here you can see these, these sets expanding and

37:53.560 --> 37:59.240
contracting. And so essentially, we can as we pull on them or inflate them, they're opening up and

37:59.240 --> 38:05.720
closing. And we can basically turn on the electrostatic adhesion to lock them and allow them to not

38:05.720 --> 38:11.480
open up, which means that basically there's less displacement and less strain in the system.

38:11.480 --> 38:15.880
So we've looked at different types of combinations of layers and different materials as well.

38:16.760 --> 38:21.320
And are looking at, you know, basically, how do we then make this into this monolithic system? So

38:21.320 --> 38:26.280
again, there's these two sheets that are on top of each other. And then basically, we create them

38:26.280 --> 38:30.760
out of this flexible printed circuit board. And so that's really nice, because we can just go

38:30.760 --> 38:35.880
ahead and fabricate that using off the shelf, you know, printed circuit board techniques.

38:35.880 --> 38:41.640
And then we have one sheet that's in this orientation, then a dielectric thin film in between,

38:41.640 --> 38:48.760
and then the other flex PCB that's on the on the next side as well. So then that's what this sort

38:48.760 --> 38:54.920
of 2D service here looks like. And here we're engaging and locking between them. And you can see

38:54.920 --> 38:59.160
some of them start to fail as well. That I guess that was the unlocked one here,

38:59.160 --> 39:04.840
this region below is locked and above is unlocked, you can sort of see. And then here's the locked

39:04.840 --> 39:10.840
region. And at a certain point, it'll start to fail as well. And so basically, we can, again,

39:10.840 --> 39:15.240
computationally control which of those areas we want to be locked and unlocked. And that allows

39:15.240 --> 39:20.200
us to create these different shapes. And then we have a inflated bladder that's underneath it that

39:20.200 --> 39:26.680
we can then basically inflate. And that will then create this global shape change. And so here's

39:26.680 --> 39:33.640
kind of an example of this 100% locked, which sort of creates this, you know, basically a very

39:33.640 --> 39:38.520
uniform shape. And then now we're locking less and less of the display to create different

39:38.520 --> 39:43.720
curvatures, if you can see like that. So we think again, this is kind of a promising approach. We're

39:43.800 --> 39:50.280
still looking for basically, exotic patterns where we can have higher, you know, amount of strain and

39:50.280 --> 39:55.160
more basically curvature that we can create then. But we think this again has some benefits in terms

39:55.160 --> 40:01.320
of being able to really create and manufacture this very quickly. A related project that was kind

40:01.320 --> 40:06.680
of in the early stages is on kind of connecting this idea of these continuous shape displays

40:06.680 --> 40:10.280
with some of the work that I showed at the beginning on these shape changing robots,

40:11.000 --> 40:15.800
where basically we want to have these elastic grid shells. So if you think about in computer

40:15.800 --> 40:20.600
graphics, we often have like NERBs, surfaces, which are kind of combinations of these,

40:21.960 --> 40:26.600
you know, different splines that are connected to each other. What if we do that in the real world?

40:26.600 --> 40:31.880
And so basically, Sophia Weitzner and Wingsum Lawn, our group, are trying to create these basically

40:32.520 --> 40:37.720
robotic elastic grid shells that can then again change their geometry in real time.

40:37.720 --> 40:42.680
This is a small one by one prototype, but here you can see we locked one part of it and then

40:42.680 --> 40:48.120
we're able to inject more material into it and create this curvature there. And so this is what

40:48.120 --> 40:54.200
we're kind of aiming to do on the left is create these, you know, basically interconnected grid

40:54.200 --> 40:59.480
shells that we can in real time change their surface. So these are some of the things we've

40:59.480 --> 41:03.640
been doing in our group to think about, you know, how do we push forward and make these higher

41:03.640 --> 41:09.000
resolution, you know, surface displays. And I think we've had some great promise in looking

41:09.000 --> 41:13.880
at electrostatic adhesion, as well as kind of new approaches to making more continuous

41:13.880 --> 41:18.600
surface displays. But I think it's really clear to us and probably you as well that the hardware

41:18.600 --> 41:24.200
will really never perfectly render the real world, right? The real world is so rich and very

41:24.200 --> 41:30.920
complex. And so I think there's this big gap between that. But the interesting thing is that

41:31.000 --> 41:36.920
our perception is also imperfect as well. And so maybe we don't need to have perfect hardware

41:36.920 --> 41:41.640
when we're considering these types of displays. And so the last part of my talk, I want to talk

41:41.640 --> 41:46.600
about some of the work we're doing in terms of using visual haptic illusions to improve the

41:46.600 --> 41:51.400
perceived performance of these types of tactile displays or other types of shape displays.

41:52.360 --> 41:57.720
So the first is work from my former PhD student, Parastu Abtaiki, who is now starting at Princeton

41:57.720 --> 42:04.120
University next year, that's looking at this same problem of how we might use the fact that our

42:05.560 --> 42:10.200
basically our visual perception is often dominates our haptic perception. And so

42:10.200 --> 42:15.320
one way to illustrate this is basically our proprioceptive system is not very good. And so

42:15.320 --> 42:21.320
if I am able to basically touch my fingers together in front of my face, I'm actually often

42:21.320 --> 42:26.040
using my visual system to really help me with that. But if I try to do it above my head,

42:26.040 --> 42:31.160
I can sometimes get it, but you'll find that it's not as accurate. And so again, that's because of

42:31.160 --> 42:35.960
all the kind of errors along the line in terms of, you know, our different joints and different

42:35.960 --> 42:41.320
mechanical receptors that we have. But our proprioceptive system has more noise and air

42:41.320 --> 42:44.920
than our visual system. And therefore, our, you know, brains, when we're thinking about

42:44.920 --> 42:50.360
integrating this multi sensory integration, sorry, integrating this multi sensory information,

42:50.360 --> 42:55.720
tend to rely on our visual system. And so as I mentioned before, there's many of these limitations

42:55.800 --> 43:01.160
of these shape displays that I talked about in terms of low spatial resolution, limited display

43:01.160 --> 43:07.640
size or low actuation speed. And the intuition or insight that Parastu had was, oh, how do we

43:07.640 --> 43:13.000
leverage this fact that our proprioceptive system isn't very good to sort of increase the perceived

43:13.000 --> 43:20.040
resolution. And this really kind of essentially builds on a technique from the field of virtual

43:20.040 --> 43:24.200
reality that's called redirected touch. Some of you might be familiar with the idea of redirected

43:24.280 --> 43:29.800
walking, where again, we can kind of steer people in virtual reality by having some slight offset

43:29.800 --> 43:35.480
between where you are in the real world and where you see you are, where you see yourself or where

43:35.480 --> 43:41.160
you see your hand in the virtual scene. And so what we can do is essentially apply some small

43:41.160 --> 43:47.080
virtual offset as I'm reaching that basically makes it seem like my hand as I move straight

43:47.080 --> 43:53.560
is moving to the left. And my basically visual motor system will basically compensate for that

43:53.560 --> 43:59.000
and make my hand move to the right in the real world to basically compensate for that

43:59.000 --> 44:04.760
bias or shift to the left. And so we can basically computationally steer where a person's hand is

44:04.760 --> 44:11.080
going by applying these offsets in the virtual world. And by leveraging that redirected touch

44:11.080 --> 44:16.120
effect, we're able to address some of these different aspects of low spatial resolution

44:16.120 --> 44:21.320
and low actuation speed. And so I'll talk about some of the ways in which we're doing that here

44:22.280 --> 44:28.920
using angle redirection, scaling up and vertical redirection as well. So you might be familiar

44:28.920 --> 44:33.320
with this idea of anti aliasing or the aliasing effect that happens when we look at

44:33.960 --> 44:38.920
graphical display. This aliasing effect is actually also very pronounced in these tactile

44:38.920 --> 44:44.120
displays as well. And so one way that we were trying to mitigate this low resolution is by

44:44.120 --> 44:50.280
trying to essentially get rid of this aliasing that happens when you display a vertical or,

44:50.280 --> 44:57.240
sorry, a diagonal line. And so if we see here, as we move along this surface here,

44:57.960 --> 45:02.280
you know, basically we have this kind of, again, aliasing effect that you can feel these bumps

45:02.280 --> 45:09.480
as well. And so what if instead we could redirect you so that you're moving along a straight line,

45:09.480 --> 45:14.200
which is very doesn't have that aliasing problem. But in the virtual world, you think you're moving

45:14.200 --> 45:20.120
along this diagonal line. And so again, by applying this slight offset between where your hand is

45:20.120 --> 45:25.400
in the real world and where it is in the virtual scene, we can make you touch different areas of

45:25.400 --> 45:32.200
the display and not be able to realize it or perceive it. In terms of the overall resolution

45:32.200 --> 45:37.400
of the device, we can think about the number of pixels or taxles that you're in contact with as

45:37.400 --> 45:42.920
you move over a surface. And so one of the challenges is if we have a small object that

45:42.920 --> 45:47.400
we're rendering on a tactile display of a fixed resolution, then you're not going to encounter

45:47.400 --> 45:53.640
that many taxles as you move along the surface. So again, what if we could change this essentially

45:53.640 --> 46:00.120
control the display ratio, or utilize this offset between where my real hand is and my virtual hand

46:00.120 --> 46:07.080
is. And in the virtual scene, render a small object, but in the real world, render a larger

46:07.080 --> 46:12.600
object of the same version, and we can render it in higher resolution. So again, we can change the

46:12.600 --> 46:17.960
ratio between where I'm interacting in the real world, where I'm interacting in the virtual world,

46:17.960 --> 46:23.160
and leverage that to improve the perceived resolution. And we've done this also for things

46:23.160 --> 46:29.000
like extending the height of the display, the workspace limitations of it, and other things

46:29.000 --> 46:34.840
like that as well. So here, you can see as I'm moving up, I have some fixed amount of range

46:34.840 --> 46:40.600
that I can move up. And then here, we're able to, again, offset where you think your hand is

46:40.680 --> 46:46.440
in the virtual scene. And so you can think that basically the tactile display has

46:47.080 --> 46:54.360
larger pins than we can actually create with these displays. We've run a lot of psychophysical

46:54.360 --> 47:02.120
studies to find what these thresholds are, and also find that when we display an offset

47:02.120 --> 47:07.560
underneath that threshold, essentially, people perceive it as being higher resolution without

47:07.560 --> 47:12.360
noticing that it's there. But we've also found some interesting effects in the difference between

47:12.360 --> 47:17.720
active versus passive touch, where in the active touch conditions, that's where you're

47:17.720 --> 47:24.920
moving your hand as well. So for example, in that angle redirection, we can't offset people's hands

47:24.920 --> 47:30.280
as much as we can in the passive touch condition, where for example, the motor is moving your hand,

47:31.000 --> 47:34.280
where we can offset people more. And so again, that has to do with kind of the

47:35.240 --> 47:40.520
you know, forward model that people have in their sensory motor system in terms of, you know,

47:40.520 --> 47:46.040
basically, my predictions of where my hand might be. And so I'm more willing to allow

47:46.040 --> 47:52.360
things to have noise and there to be more air when I'm being moved versus I'm moving myself

47:52.360 --> 47:57.000
as well. So we've looked at ways in which we can leverage this to create different types of

47:57.000 --> 48:02.520
applications such as, you know, improving the resolution or, or, you know, again, increasing

48:02.600 --> 48:08.840
the vertical redirection as well. But as you can see from those types of systems, you know,

48:08.840 --> 48:14.520
these illusions really only work in this small area of the display. And so another set of work

48:14.520 --> 48:19.720
that we've been doing is trying to improve the kind of scalability of how we might apply these

48:19.720 --> 48:24.680
to a much larger area and create essentially what are called encountered type haptic displays that

48:24.680 --> 48:30.200
might operate over a larger region. So we put these types of haptic displays, tactile displays,

48:30.200 --> 48:35.960
on, you know, robotic arm. And then when we reach out, the robotic device can be there in time.

48:35.960 --> 48:40.440
But one of the challenges with this type of approach of encountered type haptic devices is

48:40.440 --> 48:47.480
that oftentimes the device has some limitations as well, right? So the device may arrive late or

48:47.480 --> 48:52.520
it might be out of the workspace of that robotic system. So I want to touch something up here,

48:52.520 --> 48:56.920
but the robot is only down there. And so again, there's a number of challenges with these

48:56.920 --> 49:02.360
encountered type haptic devices in terms of different reachability issues that lead to these

49:02.360 --> 49:07.320
uncertain spatial discrepancies, which really kind of affect people's perceived performance

49:07.320 --> 49:13.560
of these devices. So we've done a few things in this area to improve the essentially the ability

49:13.560 --> 49:19.800
for these devices to work by essentially redirecting your hand to the reachable area of the robotic

49:19.800 --> 49:31.000
system. Great. Okay, great. So anyway, basically, we can do some cool things in terms of this,

49:31.000 --> 49:35.240
you know, redirection to guide people into different people's space. And we've also done

49:35.240 --> 49:42.280
some interesting work on trying to apply basically model predictive control to basically run this

49:42.280 --> 49:47.160
in real time to improve the perceived performance using a model of human reaching and sensory

49:47.160 --> 49:55.560
integration. So I think that I'll conclude there with just one short statement, if I didn't find it,

49:56.280 --> 50:02.920
which is that, you know, we started out this work really thinking about how real does haptics need

50:02.920 --> 50:08.520
to be. And I think where our group is going is really trying to think more about how real

50:08.520 --> 50:14.200
does haptics need to seem and really trying to leverage a sensory motor control perspective

50:14.200 --> 50:19.480
to optimize both the hardware and the software together. So I'd like to thank, you know, my

50:19.480 --> 50:23.960
PhD students and postdocs in my lab that contributed to this work, as well as our funding sources.

50:23.960 --> 50:30.440
And I guess I'd be happy to answer any questions. Yeah, I thought it was till 130. So my apologies

50:30.440 --> 50:36.760
on that mark. Yeah, so thanks so much.

