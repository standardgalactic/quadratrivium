start	end	text
0	7120	Welcome everyone, this is natural language understanding.
7120	11440	It is a weird and wonderful and maybe worrying moment
11440	13920	to be doing natural language understanding.
13920	18240	My goal for today is just to kind of immerse us in this moment
18240	23120	and think about how we got here and what it's like to be doing research now.
23120	27360	And I think that'll set us up well to think about what we're going to do in the course,
27360	32960	and how that's going to set you up to participate in this moment in AI
32960	35840	in many ways, in whichever ways you choose.
35840	38480	And it's an especially impactful moment to be doing that.
38480	40640	And this is a project oriented course.
40640	43920	And I feel like we can get you all to the point where you are doing
43920	47600	meaningful things that contribute to this ongoing moment
47600	50560	in ways that are going to be exciting and impactful.
50560	53360	That is the fundamental goal of the course.
53360	56560	Let's now think about the current model of the course.
56560	57920	Let's now think about the current moment.
57920	60640	This is always a moment of reflection for me.
60640	66880	I started teaching this course in 2012, which I guess is ages ago now.
66880	68800	It feels recent in my lived experience,
68800	71840	but it does feel like ages ago in terms of the content.
71840	75600	In 2012, on the first day, I had a slide that looked like this.
75600	80800	I said it was an exciting time to be doing natural language understanding research.
80800	84000	I noted that there was a resurgence of interest in the area
84000	88960	after a long period of people mainly focused on syntax and things like that.
88960	93840	But there was a widespread perception that NLE was poised for a breakthrough
93840	97360	and to have huge impact that was relating to business things
97360	100400	and that there was a white-hot job market for Stanford grads.
100400	103760	A lot of this language is coming from the fact that we were in this moment
103760	108720	when Siri had just launched, Watson had just won on Jeopardy,
108720	111760	and we had all of these in-home devices and all the tech giants
111760	116320	of competing on what was emerging as the field of natural language understanding.
117200	119120	Let's fast forward to 2022.
119120	121920	I did feel like I should update that in 2022
121920	124160	by saying this is the most exciting moment ever,
124160	126320	as opposed to just being an exciting time.
126880	129200	But I emphasize the same things.
129200	133600	We were in this feeling that we had experienced a resurgence of interest
133600	137280	in the area, although now it was hyper-intensified.
137280	138480	Same thing with industry.
138480	144000	The industry interest at this point makes the stuff from 2012 look like small potatoes.
144960	149600	Systems were getting very impressive, but, and I maintain this here,
149600	152080	they show their weaknesses very quickly,
152080	155520	and the core things about NLE remain far from solved.
155520	158000	So the big breakthroughs lie in the future.
158000	160320	I will say that even since 2022,
160320	163040	it has felt like there has been an acceleration
163040	168400	and some problems that we used to focus on feel kind of like they're less pressing.
168400	171920	I won't say solved, but they feel like we've made a lot of progress on them
171920	174480	as a result of models getting better.
174480	179120	But all that means for me is that there are more exciting things in the future
179120	181600	that we can tackle even more ambitious things.
181600	184160	And you'll see that I've tried to overhaul the course
184160	188240	to be ever more ambitious about the kind of problems that we might take on.
189200	192400	But we do kind of live in a golden age for all of this stuff.
192400	197200	And even in 2022, I'm not sure what I would have predicted to say nothing of 2012,
197200	200400	that we would have these incredible models like Dali2,
200400	203680	which can take you from text into these incredible images,
203680	208000	language models, which will more or less be the star of the quarter for us,
208000	211920	but also models that can take you from natural language to code.
211920	215680	And of course, we are all seeing right now as we speak
216480	223200	that the entire industry related to web search is being reshaped around NLU technologies.
223760	228560	So whereas this felt like a kind of niche area of NLP,
228560	231040	when we started this course in 2012,
231040	234080	now it feels like the entire field of NLP,
234080	237200	certainly in some aspects all of AI,
237200	240240	is focused on these questions of natural language understanding,
240240	242000	which is exciting for us.
242960	244800	One more moment of reflection here.
244800	247600	You know, in this course throughout the years,
247600	252560	we have used simple examples to kind of highlight the weaknesses of current models.
252560	256320	And so a classic one for us was simply this question,
256320	259280	which US states border no US states?
259280	262320	And the idea here is that it's a simple question,
262320	264880	but it can be hard for our language technologies
264880	267120	because of that negation, the no there.
268080	271840	In 1980, there was a famous system called Chat 80.
271840	277520	It was a symbolic system representing the first major phase of research in NLP.
277520	280080	You can see the fragment of the system here.
280080	284240	And Chat 80 was an incredible system in that it could answer questions like,
284240	288640	which country bordering the Mediterranean borders a country that is bordered by a country
288640	291440	whose population exceeds the population of India?
291440	294320	I've given you the answer here, Turkey,
295120	298480	at least according to 1980s geography.
299040	301840	But if you asked Chat 80 a simple question like,
301840	303920	which US states border no US states,
303920	306640	it would just say, I don't understand.
306640	310800	It was an incredibly expressive system, but rigid.
310800	313120	It could do some things very deeply,
313120	314880	as you see from the first question,
314880	317360	but things that fell outside of its capacity,
317360	318880	it would just fall down flat.
320240	321440	That was the 1980s.
321440	322400	Let's fast forward.
322400	325040	2009, around the time this course launched,
325040	326960	Wolfram Alphra hit the scene.
326960	330800	And this was meant to be a kind of revolutionary language technology.
330800	332320	The website is still up.
332320	336560	And to my amazement, it still gives the following behavior.
336560	340080	If you search for which US states border no US states,
340080	342800	it kind of just gives you a list of the US states.
343440	348720	Revealing, I would say, that it has no capacity to understand the question posed.
349680	350800	That was 2009.
350800	353440	So we've gone from 1980 to 2009.
354000	355440	Okay, let's go to 2020.
355440	358800	This is the first of the open AI models, ADA,
359360	361520	which US states border no US states?
362080	363200	The answer is no.
363840	365600	And then it sort of starts to babble.
365600	367600	The US border is not a state border.
367600	369280	It did that for a very long time.
370320	372960	But about Babbage, this is still 2020.
373840	376000	The US states border no US states.
376000	377520	What is the name of the US state?
377520	379920	And then it really went off the deep end from there.
379920	382240	Again, for a very long time, that was Babbage.
382240	385600	If you had seen this output, well, at least for me,
385600	389360	it might have shaken my faith that this was a viable approach.
390480	392000	But the team persisted, I guess.
392000	394240	2021, this is the Curie model.
394240	396400	Which US states border no US states?
397120	399440	It had a problem that it started listing things,
399440	402720	but it did say Alaska, Hawaii, and Puerto Rico,
402720	406800	which is an interestingly more impressive answer than the first answer.
407600	410560	It still has some problem understanding what it means to respond,
410560	412960	but it's looking like we're seeing some signal.
414160	415600	Da Vinci instruct beta.
415600	416800	This is 2022.
416800	419520	It's important, I think, that this is the first of the models
419520	421040	that have been struck in the name.
421040	422800	We'll talk about that in a minute.
422800	424720	Which US states border no US states?
424720	426320	Alaska and Hawaii.
426320	431440	We've from 2020 to 2022, we have seen this astounding leap forward,
431440	434560	making everything before then sort of pale in comparison.
434560	439280	And then finally, text Da Vinci won one of the new best in class models
439280	441040	at least until two months ago.
441040	443120	Which US states border no US states?
443120	446880	Alaska and Hawaii are the only US states that border no other US states.
446880	449520	A very impressive answer indeed.
449520	452000	And if you just think about the little history I've given,
452720	456080	a kind of microcosm of what is happening in the field,
456960	462080	a lot of time without much progress with some hype attached.
462160	465920	And now in the last few years, this kind of rapid progress forward.
467360	471280	And you know, that's just one example, but these examples multiply
471280	472320	and we can quantify this.
472320	474080	Here's another impressive case.
474080	479040	I asked the Da Vinci 2 model in which year was Stanford University founded?
479040	480960	When did it enroll its first students?
480960	483680	Who is its current president and what is its mascot?
483680	485760	A complicated question indeed.
485760	490400	And it gave a fluent and factually correct answer on all counts.
491360	495600	This is the Da Vinci 3 model, which was best in class until a few weeks ago.
496160	499200	And it gave exactly the same answer, very impressive.
500560	503600	Now in this course, and you'll see at the website,
503600	506800	one of the readings we've suggested for the start of the course
506800	511200	is this classic paper by Hector Levec called On Our Best Behavior.
511200	514960	And the thrust of this article essentially channeling Terry Winograd
514960	517200	and Terry Winograd's schema.
518000	520880	The idea is that we should come up with examples
520880	524000	that will test whether models deeply understand.
524000	527600	And in particular, get past the kind of simple memorization
527600	530560	of statistics and other things about the data they're trained on
530560	534160	and really probe to see whether they understand what the world is like.
534160	537680	And Levec and Winograd's technique for doing this
537680	541120	is to pose very unlikely questions
541120	543440	where humans have very natural answers.
543440	545600	Like one of the ones Levec poses is,
545600	547840	could a crocodile run the steeple chase?
548960	551280	Maybe it's a question you've never thought about before,
551280	554960	but you probably have a pretty consistent answer across this group.
554960	556880	Could a crocodile run the steeple chase?
557520	559600	Here I asked another one of Levec's questions.
559600	564320	Are professional baseball players allowed to glue small wings onto their caps?
564320	566160	You could think about that for a second.
566160	570880	The Da Vinci 2 model said there is no rule against it, but it is not common.
570880	575040	And that seemed like a very good answer to me at the time.
575040	579200	When the Da Vinci 3 engine came out, though, this started to worry me.
579200	583680	No, professional baseball players are not allowed to glue small wings onto their caps.
583680	585520	Major League Baseball has strict rules
585520	588080	about the appearance of players' uniforms and caps.
588880	591600	And any modifications to the caps are not allowed.
592480	595120	Okay, I thought I was feeling good about this,
595120	598160	but now I don't even myself know what the answer is.
598160	601440	Are professional baseball players allowed to glue small wings onto their caps?
601440	604800	We have two confident answers that are contradictory
605600	608880	across two models that are very closely related.
608880	611680	It's starting to worry us a little bit, I hope.
611680	614080	But still, it's impressive.
614080	614560	What's that?
614560	615840	You want me to ask a part?
615840	616720	Yes.
616720	617520	You could check.
617520	621920	Yes, I have a few cases, and this is an interesting experiment for us to run for sure.
621920	624160	Let me show you the responses I got a bit later.
624160	629040	The point, though, I guess, if you've seen the movie Blade Runner,
629040	633840	this is starting to feel like to figure out whether an agent we were interacting with
633840	636000	was human or AI.
636000	641120	We would need to get very sophisticated interview techniques indeed.
641120	643920	The Turing test long forgotten here.
643920	649520	Now we're into the mode of trying to figure out exactly what kind of agents we're interacting with
649520	654160	by having to be extremely clever about the kinds of things that we do with them.
655920	658560	Now, that's kind of anecdotal evidence,
658560	663840	but I think that the picture of progress is also supported by what's happening in the field.
663840	666800	Let me start this story with our benchmarks.
666800	670000	And the headline here is that our benchmarks, the tasks,
670000	675200	the data sets we use to probe our models are saturating faster than ever before.
675200	677760	And I'll articulate what I mean by saturate.
677760	679760	So we here have a little framework.
679760	684640	Along the x-axis, I have time stretching back into, like, the 1990s.
684640	691280	And along the y-axis, I have a normalized measure of distance from what we call human performance.
691280	693840	That's the red line set at zero.
693840	697040	Each one of these benchmarks has, in its own particular way,
697040	699680	set a so-called estimate of human performance.
699680	701760	I think we should be cynical about that.
701760	705360	But nonetheless, this will be a kind of marker of progress for us.
705760	707760	First data set, MNIST.
707760	710800	This is like digit recognition, famous task in AI.
710800	716000	It was launched in the 1990s, and it took about 20 years for us to see a system
716000	720000	that surpassed human performance in this very loose sense.
720880	724400	The switchboard corpus, this is going from speech to text.
724400	728800	It's a very similar story, launched in the 90s, and it took about 20 years
728800	731120	for us to see a superhuman system.
731120	734880	ImageNet, this was launched, I believe, in 2009,
734880	740400	and it took less than 10 years for us to see a system that surpassed that red line.
740400	743280	And now progress is going to pick up really fast.
743280	748320	Squad 1.1, the Stanford question answering data set, was launched in 2016,
748320	752560	and it took about three years for it to be saturated in this sense.
752560	757760	Squad 2.0 was the team's attempt to pose an even harder problem,
757840	761840	one where there were unanswerable questions, but it took even less time
761840	764640	for systems to get past that red line.
765200	767200	Then we get the glue benchmark.
767200	772480	This is a famous benchmark in natural language understanding, a multi-task benchmark.
772480	777680	When this was launched, a lot of us thought that glue would be too difficult
777680	778800	for present-day systems.
778800	782720	It looked like this might be a challenge that would stand for a very long time,
782720	785200	but it took like less than a year.
785280	790560	But it took like less than a year for systems to pass human performance.
790560	795920	The response was superglue, but it was saturated, if anything, even more quickly.
796960	800800	Now we can be as cynical as we want about this notion of human performance,
800800	804320	and I think we should dwell on whether or not it's fair to call it that.
804320	810960	But even setting that aside, this looks like undeniably a story of progress.
811040	817120	The systems that we had in 2012 would not even have been able to enter the glue benchmark
817120	819760	to say nothing of achieving scores like this.
819760	821920	So something meaningful has happened.
821920	826320	Now you might think by the standards of AI, these data sets are kind of old.
826320	831600	Here's a post from Jason Wei, where he evaluated our latest and greatest large language models
831600	835360	on a bunch of mostly new tasks that were actually designed
835360	839680	to stress test this new class of very large language models.
839680	843280	And Jason's observation is that we see emergent abilities
843280	848240	across more than 100 tasks for these models, especially for our largest models.
848240	853120	The point, though, is that we again thought these tasks would stand for a very long time,
853120	857840	and what we're seeing instead is that one by one systems are certainly getting traction
857840	862080	and in some cases performing at the standard we had set for humans.
862800	866160	Again, an incredible story of progress there.
866320	869920	So I hope that is energizing, maybe a little intimidating,
869920	872480	but I hope fundamentally energizing for you all.
873840	878320	The next question that I want to ask for you is just what is going on?
878320	881440	What is driving all of this sudden progress?
881440	886320	Let's get a feel for that, and that'll kind of serve as the foundation for the course itself.
886880	892000	Before I do that, though, are there questions or comments, things I could resolve,
892720	896080	or things I left out about the current moment?
900240	901760	We're running on bar to think very well.
902800	908160	We should reflect, though, maybe as a group about what it means to do very well.
908160	911200	My question for you, when you say it did well,
911200	915280	what is the Major League Baseball rule about players gluing things onto their caps?
917200	918480	You found the actual rule.
918480	921680	No, this is what, well, I don't...
922480	923280	I didn't find the rule.
923280	924880	Bard found that rule and gave me that number.
924880	925280	Okay.
926400	928640	Yes, that is going to be the question for us.
932640	936960	I'm going to show you the open AI models will offer me links, but the links go nowhere.
940160	943760	What you're pointing out, I think, is an increasing societal problem.
943760	947120	These models are offering us what looks like evidence,
947120	949760	but a lot of the evidence is just fabricated.
949760	952560	And this is worse than offering no evidence at all.
952560	956160	What I really need is someone who knows Major League Baseball to tell me,
956160	959200	what is the rule about players and their caps?
959760	963360	I want it from an expert human, not an expert language model.
965840	966240	What's that?
966240	967040	Can we Google?
968080	969440	Be careful how you Google, though.
969440	971440	I guess that's the lesson of 2023.
974960	976480	All right, what's going on?
976480	978480	Let's start to make some progress on this.
978560	981920	Again, first, a little bit of historical context.
981920	986080	I've got a timeline going back to the 1960s along the x-axis.
986080	988480	This is more or less the start of the field itself.
989040	995920	And in that early era, essentially all of the approaches were based in symbolic algorithms,
995920	997760	like the chat 81 that I showed you.
997760	1001200	In fact, that was kind of pioneered here at Stanford by people
1001200	1003920	who were pioneering the very field of AI.
1003920	1007680	And that paradigm of essentially programming these systems
1007680	1009920	lasted well into the 1980s.
1012000	1016960	In the 90s, early 2000s, we get the statistical revolution
1016960	1021440	throughout artificial intelligence and then in turn in natural language processing.
1021440	1026080	And the big change there is that instead of programming systems with all these rules,
1026080	1030320	we're going to design machine learning systems that are going to try to learn from data.
1030320	1033040	Under the hood, there was still a lot of programming involved
1033040	1035600	because we would write a lot of feature functions
1035600	1039200	that were little programs that would help us detect things about data.
1039200	1042240	And we would hope that our machine learning systems could learn
1042240	1044560	from the output of those feature functions.
1044560	1049520	But in the end, this was the rise of the fully data-driven learning systems.
1049520	1054560	And we just hope that some process of optimization leads us to new capabilities.
1055920	1059200	The next big phase of this was the deep learning revolution.
1059280	1062400	This happened starting around 2009, 2010.
1062400	1065440	Again, Stanford was at the forefront of this, to be sure.
1066240	1069520	It felt like a big change at the time, but in retrospect,
1069520	1072160	this is kind of not so different from this mode here.
1072160	1078160	It's just that we now replace that simple model with really big models,
1078160	1082640	really deep models that have a tremendous capacity to learn things from data.
1083280	1088480	We started also to see a shift even further away from those feature functions,
1088480	1090160	from writing little programs.
1090160	1094160	And more toward a more mode where we would just hope that the data
1094160	1097520	and the optimization process could do all the work for us.
1099520	1105120	Then the next big thing that happened, which I could take us I suppose until about 2018,
1105120	1108240	would be this mode where we have a lot of pre-trained parameters.
1108240	1112800	These are pictures of maybe big language models or computer vision models or something.
1112800	1116800	And when we build systems, we build on those pre-trained components
1116880	1120960	and stitch them together with these task-specific parameters.
1120960	1126240	And we hope that when they're all combined and we do some learning on some task-specific data,
1126240	1129520	we have something that's benefiting from all these pre-trained components.
1131120	1135920	And then the mode that we seem to be in now that I want us to reflect critically on
1136480	1140000	is this mode where we're going to replace everything with maybe one
1140560	1145040	ginormous language model of some kind and hope that that thing,
1145040	1148640	that enormous black box will do all the work for us.
1148640	1151920	We should think critically about whether that's really the path forward,
1151920	1155120	but it certainly feels like the zeitgeist to be sure.
1156160	1156800	Question, yeah?
1157760	1160000	If you think it's worth it, could you go back to the last slide
1160960	1165360	and maybe explain a little bit, a more browner example of what that all means?
1165360	1166320	I couldn't quite follow.
1167040	1168480	Let's do that later.
1168480	1174400	The point for now though is really this shift from here where we're mostly learning from
1174480	1179680	scratch for our task, here we've got things like BERT in the mix.
1179680	1185840	We've got pre-trained components, models that we hope begin in a state that gives us a leg up
1185840	1187680	on the problem we're trying to solve.
1187680	1189040	That's the big thing that happened.
1189040	1193760	And you get this emphasis on people releasing model parameters.
1193760	1198800	In this earlier phase like here, there was no talk of releasing model parameters
1198800	1203520	because mostly the models people trained were just good for the task that they had set.
1204160	1209440	As we move into this era and then certainly this one, these things are meant to be like
1209440	1214800	general purpose language capabilities or maybe general purpose computer vision capabilities
1214800	1220240	that we stitch together into a system that can do more than any previous system could do.
1223920	1225680	Right, so then we have this big thing here.
1226640	1232880	So that's the feeling now behind all of this certainly beginning in this final phase here
1232880	1235040	is the transformer architecture.
1235040	1236720	Just may take the temperature of the room.
1236720	1239120	How many people have encountered the transformer before?
1240560	1243600	Right, yeah, it's sort of unavoidable if you're doing this research.
1243600	1248240	Here's a diagram of it, but I'm not going to go through this diagram now because
1248240	1253840	starting on Wednesday, we are going to have an entire lecture essentially devoted to unpacking
1253840	1256000	this thing and understanding it.
1256000	1261840	All I can say for you now is that I expect you to go on the following journey, which all of us go on.
1262480	1264880	How on earth does the transformer work?
1264880	1266880	It looks very, very complicated.
1267440	1272800	I hope can get you to the point where you feel, oh, this is actually pretty simple components
1272800	1275920	that have been combined in a pretty straightforward way.
1275920	1277520	That's your second step on the journey.
1277520	1282480	The one, the true enlightenment comes from, wait a second, why does this work at all?
1283360	1288000	And then you're with the entire field trying to understand why these simple things
1288000	1290720	were brought together in this way have proved so powerful.
1292800	1298400	The other major thing that happened, which is kind of latent going all the way back to the
1298400	1304320	start of AI, especially as it relates to linguistics, is this notion of self-supervision,
1304320	1309680	of distributional learning, because this is going to unlock the door to us just learning
1309680	1312080	from the world in the most general sense.
1312960	1319840	In self-supervision, your model's only goal is to learn from co-occurrence patterns in the
1319840	1321440	sequences that it's trained on.
1321440	1324960	And the sequences can be language, but they could be language plus
1324960	1330080	sensor readings, computer code, maybe even images that you embed in this space,
1330080	1331200	just symbols.
1331200	1335520	And the model's only goal is to learn from the distributional patterns that they contain.
1336640	1340000	Or for many of these models to assign high probability
1340000	1343440	to the attested sequences in whatever data that you pour in.
1344160	1347120	For this kind of learning, we don't need to do any labeling.
1347840	1352160	All we need to do is have lots and lots of symbol streams.
1353600	1356800	And then when we generate from these models, we're sampling from them.
1356800	1360080	And that's what we all think of when we think of prompting and getting a response back.
1360080	1365200	But the underlying mechanism is, at least in part, this notion of self-supervision.
1365200	1369040	And I'll emphasize again, because I think this is really important for why these models are so
1369040	1372640	powerful, the symbols do not need to be just language.
1372640	1377840	They can include lots of other things that might help a model piece together,
1377840	1382640	a full picture of the world we live in, and also the connections between language and
1382640	1386160	those pieces of the world, just from this distributional learning.
1387680	1393520	The result of this proving so powerful is the advent of large-scale pre-training.
1393520	1398240	Because now we're not held back anymore by the need for labeled data.
1398240	1401600	All we need is lots of data in unstructured format.
1402240	1407840	This really begins in the era of static word representations like word-to-vec and glove.
1408640	1411840	And in fact, those teams, and I would say especially the glove team,
1411840	1418800	they were really visionary in the sense that they not only released a paper and code,
1419360	1421600	but pre-trained parameters.
1421600	1425760	This was really brand new for the field, this idea that you would empower people
1426320	1428640	with model artifacts.
1428640	1434480	And people started using them as the inputs to recurrent neural networks and other things.
1434480	1441040	And you started to see pre-training as an important component to doing really well at hard things.
1443200	1446160	There were some predecessors that I'll talk about next time.
1446160	1451200	But the really big moment for contextual representations is the ELMO model.
1451200	1454240	This is the paper, Deep Contextualized Word Representations.
1454320	1460480	I can remember being at the North American ACL meeting in New Orleans in 2018
1460480	1462320	at the best paper session.
1462320	1467200	They had not announced which of the best papers was going to win the Outstanding Paper Award.
1467200	1473360	But we all knew it was going to be the ELMO paper because the gains that they had reported
1473360	1478640	from fine-tuning their ELMO parameters on hard tasks or the field were just mind-blowing.
1478640	1483520	The sort of thing that you really only see once in a kind of generation of this research.
1483520	1484800	Or so we thought.
1484800	1491760	Because the next year, Burt came out, same thing, I think same best paper award thing.
1491760	1496000	The paper already had had huge impact by the time it was even published.
1497120	1500160	And they too released their model parameters.
1500160	1502240	ELMO is not transformer-based.
1502240	1505680	Burt is the first of the sequence of things that's based in the transformer.
1505680	1509760	And again, lifting all boats even above where ELMO had brought us.
1510640	1512240	Then we get GPT.
1512240	1514160	This is the first GPT paper.
1514160	1517120	And then fast forward a little bit, we get GPT-3.
1517120	1524240	And that was pre-training at a scale that was previously kind of unimaginable.
1524240	1528720	Because now we're talking about, you know, for the Burt model, 100 million parameters
1528720	1531920	and for GPT-3, well north of 100 billion.
1532880	1534800	Different order of magnitude.
1534800	1537840	And what we started to see is emergent capabilities.
1539440	1541120	That model size thing is important.
1541760	1545360	Again, this is a sort of feeling of progress and maybe also despair.
1545360	1550160	I think I can lift your spirits a little bit, but we should think about model size.
1550160	1552640	So I have years along the x-axis again.
1553280	1558800	And I have model size going from 100 million to 1 trillion here on a logarithmic scale.
1559520	1563600	So 2018 GPT, that's like 100 million Burt.
1563600	1566240	I think it's 300 million for the large one.
1566240	1568160	Okay, GPT-2 even larger.
1568960	1570640	Megatron 8.3 billion.
1570640	1573920	I remember when this came out, I probably laughed.
1573920	1575200	Maybe I thought it was a joke.
1575200	1579760	I certainly thought it was some kind of typo because I couldn't imagine that it was actually
1579760	1581440	billion like with a B there.
1583120	1586160	But now that's, you know, we take that for granted.
1586160	1590240	Megatron 11 billion, this is 20, 21 or so.
1590240	1594880	Then we get GPT-3 reportedly at 175 billion parameters.
1594880	1598240	And then we get this thing where it seems like we're doing typos again.
1598240	1605040	Megatron Turing NLG was like 500, and then Palm is 540 billion parameters.
1605680	1610640	And I guess there are rumors that we have gone upward all the way to a trillion, right?
1611440	1613680	There's an undeniable trend here.
1614240	1619120	I think there is something to this trend, but we should reflect on it a little bit.
1619120	1623040	One thing I want to say is there's a noteworthy pattern.
1623040	1629440	A very few entities have participated in this very large, in this race for very large models.
1629440	1633520	We've got like Google, NVIDIA, Meta and OpenAI, right?
1634400	1636640	And that was actually a real cause for concern.
1636640	1642160	I remember being at a workshop between Stanford and OpenAI where the number one source of
1642160	1648960	consternation was really that only OpenAI at that point had trained these really large models.
1648960	1652880	And after that, predictably, these other large tech companies kind of caught up.
1653600	1658320	But it was still for a while looking like a story of real centralization of power.
1659680	1662400	That might still be happening, but I think there's a reason to be optimistic.
1662400	1667200	So here at Stanford, the Helm Group, which is part of the Center for Research on Foundation
1667200	1672560	Models, led this incredibly ambitious project of evaluating lots of language models.
1672560	1677360	And one thing that emerges from that is that we have a more healthy ecosystem now.
1677360	1681440	So we have these like loose collectives, big science and a Luther are both kind of fully
1681440	1687600	open source groups of researchers. We've got, well, one academic institution represented.
1687600	1690640	This could be a little bit embarrassing for Stanford, maybe we'll correct that.
1691280	1694880	And then maybe the more important thing is that we have lots of startups represented.
1694880	1700800	So these are well-funded but relatively small outfits that are producing outstanding language
1700800	1704720	models. And so the result, I think we're going to see much more of this.
1704800	1708400	And then we'll worry less about centralization of power.
1708400	1711840	There's plenty of other things to worry about, so we shouldn't get sanguine about this.
1711840	1716160	But this particular point, I think, is being alleviated by current trends.
1716720	1722080	And there's another aspect of this too, which is you have the scary rise in model size.
1722080	1728080	But what is happening right now as we speak in a very quick way is we're seeing a push
1728080	1733360	towards smaller models. And in particular, we're seeing that models that are in the range of like
1733360	1738240	10 billion parameters can be highly performant, right? So we have the Flan models.
1739280	1743360	We have Lama. And then here at Stanford, they released the Alpaca thing.
1743360	1747840	And then Databricks released Hello Dolly model. These are all models that are like
1747840	1752960	8 to 10 billion parameters, which I know this sounds funny because I laughed a few years ago
1752960	1758000	when the Megatron model had 8.3 billion. And now what I'm saying to you is that this is relatively
1758080	1764640	small, but so it goes. And the point is that a 10 billion parameter model is one that could be run
1764640	1770640	on regular old commercial hardware. Whereas these monsters up here, really, you have lots of
1770640	1775600	pressures towards centralization of power there because almost no one can work with them.
1775600	1780000	But anyone essentially can work with Alpaca. And it won't be long before we've got
1780000	1783840	the ability to kind of work with it on small devices and things like that.
1783840	1788880	And that, too, is really going to open the door to lots of innovation. I think that will
1788880	1793680	bring some good. And I think it will bring some bad, but it is certainly a meaningful change
1793680	1797760	from this scary trend that we were seeing until four months ago.
1802080	1808160	As a result of these models being so powerful, people started to realize that you can get a
1808160	1814080	lot of mileage out of them simply by prompting them. When you prompt one of these very large
1814080	1819120	models, you put it in a temporary state by inputting some text, and then you generate a
1819120	1823200	sample from the model using some technique and you see what comes out, right? So if you type
1823200	1829920	into one of these models, better late than, it's going to probably spit out never. If you put in
1829920	1835600	every day, I eat breakfast, lunch, and it will probably say dinner. And you might have an intuition
1835600	1840320	that the reasons, the causes for that are kind of different. The first one is a sort of idiom,
1840320	1846080	so that it could just learn from co-occurrence patterns in text transparently. For the second one,
1846080	1851680	we kind of interpreted as humans as reflecting something about routines, but you should remind
1851680	1857760	yourself that the mechanism is the same as in the first case. This was just a bunch of co-occurrence
1857760	1863840	patterns. A lot of people described their routines in text and the model picked up on that. And carry
1863920	1868720	that thought forward as you think about things like the president of the U.S. is. When it fills
1868720	1875040	that in with Biden or whoever, it might look like it is offering us factual knowledge and maybe in
1875040	1880960	some sense it is, but it's the same mechanism as for those first two examples. It is just learning
1880960	1886160	from the fact that a lot of people have expressed a lot of text that look like the president of the
1886160	1892640	U.S. is Joe Biden, and it is repeating that back to us. And so definitely, if you ask a model,
1892640	1898720	something like the key to happiness is you should remember that this is just the aggregate of a lot
1898720	1904800	of data that it was trained on. It has no particular wisdom to offer you necessarily beyond what was
1904800	1914800	encoded latently in that giant sea of mostly unaudited unstructured text. Yeah, question.
1916000	1920400	I guess, you know, it would be kind of hard to get something like this, but if we had like a corpus
1920400	1925040	of just like, you know, although language is right, but literally all of the facts were wrong.
1925040	1930800	Like, we just imagined like a very factually incorrect corpus. Like, I guess I'm getting
1930800	1938480	how, like, how do we inject like truth into like these corpuses? It's a question that bears
1938480	1944880	repeating. How do we inject truth? It's a question you all could think about. What is truth, of course.
1944960	1951040	But also, what would that mean? And how would we achieve it? And even if we did back off to something
1951040	1956560	like, how would we ensure self-consistency for a model? Or, you know, at the level of a worldview
1956560	1962560	or a set of facts, even those questions which seem easier to pose are incredibly difficult
1962560	1967600	questions in the current moment, where our only mechanisms are basically that self-supervision
1967600	1973200	thing that I described, and then a little bit of what I'll talk about next. But none of the
1973200	1977520	structure that we used to have where we would have a database of knowledge and things like that,
1978240	1979600	that is posing problems.
1985200	1990160	The prompting thing, we take this a step forward, right? So the GPT-3 paper, remember that's that
1990160	1997120	175 billion parameter monster. The eye-opening thing about that is what we now call in-context
1997120	2002400	learning, which was just the notion that for these very large, very capable models, you could
2002400	2008320	input a bunch of text, like here's a passage, and maybe an example of the kind of behavior that
2008320	2014080	you wanted, and then your actual question, and the model would do a pretty good job at answering
2014080	2019360	the question. And what you're doing here is, with your context passage and your demonstration,
2019360	2025360	pushing the model to be extractive, to find an answer to its question in the context passage,
2026080	2031280	and then the observation of this paper is that they do a pretty good job at following that same
2031280	2037360	behavior for the actual target question at the bottom here. Remember, this is all just prompting,
2037360	2042320	putting the model in a temporary state, and seeing what comes out. You don't change the model,
2042320	2049040	you just prompt it. This, in 2012, if you had asked me whether this was a viable path forward
2049040	2055120	for a class project, I want to prompt a RNN or something. I would have advised you as best I
2055120	2060240	could to choose some other topic, because I never would have guessed that this would work. So the
2060240	2066720	mind-blowing thing about this paper and everything that's followed is that we might be nearing the
2066720	2073760	point where we can design entire AI systems on the basis of this simple in-context learning mechanism,
2073760	2080160	transformatively different from anything that we saw before. In fact, let me just emphasize this
2080160	2086160	a little bit. It is worth dwelling on how strange this is. For those of you who have been in the
2086160	2092240	field a little while, just contrast what I described in-context learning with the standard
2092240	2099360	mode of supervision. Let's imagine for a case here that we want to train a model to detect
2099360	2105280	nervous anticipation, and I have picked this because this is a very particular human emotion.
2105280	2110240	And in the old mode, we would need an entire dedicated model to this, right? We would collect
2110240	2116480	a little data set of positive and negative instances of nervous anticipation, and we would
2116480	2122320	train a supervised classifier on feature representations of these examples over here,
2122320	2128320	learning from this binary distinction. We would need custom data and a custom model
2128320	2135440	for this particular task in all likelihood. In this new mode, few shot in-context learning,
2135440	2140720	we essentially just prompt the model, hey, model, here's an example of nervous anticipation. My
2140720	2145680	palms started to sweat as the Lotto numbers were read off. Hey, model, here's an example without
2145680	2152880	nervous anticipation and so forth. And it learns from all those symbols that you put in and their
2152880	2160320	co-occurrences something about nervous anticipation. On the left for this model here, I've written
2160320	2165520	out nervous anticipation, but remember that has no special status. I've structured the model
2165520	2170800	around the binary distinction, the one and the zero. And everything about the model is geared
2170800	2177120	toward my learning goal. On the right, nervous anticipation is just more of the symbols that
2177120	2183200	I've put into the model. And the eye-opening thing, again, about the GPT-3 paper and what's
2183200	2189840	followed is that models can learn, be put in a temporary state and do well at tasks like this.
2191280	2198720	Now, I talked about self-supervision before, and I think that is a major component to the success
2198720	2204240	of these models. But it is increasingly clear that it is not the only thing that is driving
2204240	2210960	learning in the best models in this class. The other thing that we should think about
2210960	2216240	is what's called reinforcement learning with human feedback. This is a diagram from the
2216240	2222000	chat GPT blog post. There are a lot of details here, but really two of them are important for us
2222000	2230720	for right now. The first is that in a phase of training these models, people are given inputs
2230720	2236960	and ask themselves to produce good outputs for those inputs. So you might be asked to do a little
2236960	2242240	Python program, and you yourself as an annotator might write that Python program, for example.
2242240	2246080	So that's highly skilled work that depends on a lot of human intelligence,
2246640	2252560	and those examples, those pairs are part of how the model is trained. And that is so important
2252560	2257840	because that takes us way beyond just learning from co-occurrence patterns of symbols and text.
2258400	2265440	It is now back to a very familiar story from all of AI, which is that it's not magic. What is
2265440	2270880	happening is that a lot of human intelligence is driving the behavior of these systems.
2271840	2276640	And that happens again at step two here. So now the model produces different outputs,
2276640	2282560	and humans come in and rank those outputs, again expressing direct human preferences
2282560	2288240	that take us well beyond self-supervision. So we should remember, we had that brief moment where
2288240	2293200	it looked like it was all unstructured unlabeled data, and that was important to unlocking these
2293200	2299680	capacities, but now we are back at a very labor-intensive human capacity here, driving
2300480	2303600	what looked like the really important behaviors for these models.
2307120	2311520	Final step, which I think actually intimately relates to that instruct tuning that I just
2311520	2315520	described. That's a kind of way of summarizing this reinforcement learning with human feedback.
2316240	2320480	And this is what's called step by step or chain of thought reasoning. Now we're thinking about
2320480	2325440	the prompts that we use for these models. So suppose we asked ourselves a question like,
2325440	2331280	can models reason about negation? To give an example, does the model know that if the customer
2331280	2335920	doesn't have any auto loan, sorry, doesn't have any loans, then the customer doesn't have any
2335920	2340400	auto loans? It's a simple example. It's the sort of reasoning that you might have to do if you're
2340400	2345760	thinking about a contract or something like that, whether a rule has been followed. And it just
2345760	2352160	involves negation, our old friend from the start of the lecture. Now in the old school
2352160	2358960	prompting style, all the way back in 2021, we would kind of naively just input, is it true that
2358960	2363280	if the customer doesn't have any loans, then the customer doesn't have any auto loans into one of
2363280	2368240	these models, and we would see what came back. And here it says, no, this is not necessarily
2368240	2372800	true. A customer can have auto loans without having any other loans, which is the reverse
2372800	2378240	of the question that I asked. Again, kind of showing it doesn't deeply understand what we've
2378240	2384880	put in here. It just kind of does an act that looks like it did. And that's worrisome. But
2384880	2388720	we're learning how to communicate with these very alien creatures. Now we do what's called
2388720	2393200	step-by-step prompting. This is the cutting edge thing. You would just tell the model that it was
2393200	2398800	in some kind of logical or common sense reasoning exam that matters to the model. Then you could
2398800	2403840	give some instructions, and then you could give an example in your prompts of the kind of thing
2403840	2409120	it was going to see. And then finally you could prompt it with your premise and then your question.
2409680	2414240	And the model would spit out something that looked really good. Here I won't bother going
2414240	2420880	through the details, but with that kind of prompt, the model now not only answers and reasons
2420880	2427360	correctly, but also offers a really nice explanation of its own reasoning. The capacity was there,
2427360	2432400	it was latent, and we didn't see it in the simple prompting mode, but the more sophisticated
2432400	2439280	prompting mode elicited it. And I think this is in large part the result of the fact that this
2439280	2444720	model was instruct tuned. And so people actually taught it about how that markup is supposed to
2444720	2449360	work and how it's supposed to think about prompts like this. So the combination of all that human
2449360	2454720	intelligence and the capacity of the model led to this really interesting and much better behavior.
2455040	2465200	That is a glimpse of the foundations of all of this, I would say. Of course, we're going to unpack
2465200	2469840	all of that stuff as we go through the quarter, but I hope you're getting a sense for it.
2469840	2473520	Are there questions I can answer about it, things I could circle back on? Yes?
2474560	2481360	The human brain has about 100 billion neurons, is my understanding. And I'm not sure how many
2481440	2485120	parameters that might be, maybe like 10 trillion parameters or something like that.
2485920	2490720	Are we approaching a point where these machines can start emulating the human brain or is there
2490720	2495760	something to the language instinct or, you know, instincts of all kinds that maybe take
2495760	2503200	into the human brain? Oh, it's nothing but big questions today. Right, so the question is kind
2503200	2507600	of like, what is the relationship between the models we're talking about and the human brain?
2507600	2512640	And you raise that in terms of the size, and I guess the upshot of your description was that
2512640	2519040	these models remain smaller than the human brain. I think that's reasonable. It's tricky,
2519040	2524160	though. On the one hand, they obviously have superhuman capabilities. On the other hand,
2524160	2530080	they fall down in ways that humans don't. It's very interesting to ask why that difference exists.
2531040	2537360	And maybe that would tell us something about the limitations of learning from scratch versus
2537360	2543200	being initialized by evolution the way all of us were. I don't know, but I would say that
2543920	2549600	underlying your whole line of questioning is the question, can we use these models to eliminate
2549600	2554640	questions of neuroscience and cognitive science? And I think we should be careful,
2554640	2561520	but that the answer is absolutely yes. And in fact, the increased ability of these models to learn
2561520	2567520	from data has been really illuminating about certain kind of recalcitrant questions from
2567520	2572080	cognitive science in particular. You have to be careful because they're so different from us,
2572080	2578080	these models. On the other hand, I think they are helping us understand how to differentiate
2578080	2582640	different theories of cognition. And ultimately, I think they will help us understand cognition
2582640	2589600	itself. And I would, of course, welcome projects that were focused on those cognitive questions
2589600	2595200	in here. This is a wonderful space in which to explore this kind of more speculative angle,
2596080	2598960	connecting AI to the cognitive sciences.
2603520	2605680	Other questions, comments? Yes, in the back.
2606240	2611200	I would be curious to understand whether, I mean, partially following up on the brain thing,
2611200	2616640	just to use a metaphor of our brain not being just one huge lump of neurons, but being separated
2616720	2622160	into different areas. And then also thinking about the previous phase that you talked about,
2622160	2627440	about breaking up the models and potentially having a model in the front that decides which
2627440	2633200	domain our question falls into, and then having different sub-models. And I'm wondering whether
2633200	2638480	that's arising, whether we're going to touch on an architecture like that. Because it just seems
2638480	2645600	natural to me because prompting a huge model is just very expensive computationally. It feels like
2646240	2650720	combining big models and logic trees could be a cool approach.
2650720	2655280	I love it. Yeah, like one quick summary of what you said would relate directly to your question.
2655280	2662160	The modularity of mind is an important old question about human cognition, to what extent
2662160	2670320	are our abilities modularized in the mind-brain? With these current models, which have a capacity
2670320	2674080	to do lots of different things if they have the right pre-training and the right structure,
2674080	2680800	we could ask, does modularity emerge naturally? Or do they learn non-modular solutions? Both of
2680800	2686080	those seem like they could be indirect evidence for how people work. Again, we have to be careful
2686080	2690160	because these models are so different from us. But as a kind of existence proof, for example,
2690160	2695200	that modularity was emergent from otherwise unstructured learning, that would be certainly
2695200	2700560	eye-opening, right? I have no idea. Yeah, I don't know whether there are results for that.
2700640	2708080	Are there results? No, just kind of a follow-up question on that as well. So, given how closed
2708080	2714400	all these big models are, how could we interact with the models in such a way that helps us learn
2714400	2720240	if there is modularity? Because we literally can only interact with the, how do we go about
2720240	2725920	starting that? Right, so the question is, you know, the closed-off nature of a lot of these
2726000	2730960	models has been a problem. We can access the open AI models, but only through an API. We don't get
2730960	2737440	to look at their internal representations, and that has been a blocker. But I mentioned the rise of
2737440	2743120	these 10 billion parameter models as being performant and interesting, and those are models that,
2743120	2747520	with the right hardware, you can dissect a little bit. And I think that's just going to get better
2747520	2752240	and better, and so we'll be able to, you know, peer inside them in ways that we haven't been able to
2752240	2759200	until recently. Yeah. And in fact, like, we're going to talk a lot about explainability. That's
2759200	2764160	a major unit of this course, and I think it's an increasingly important area of the whole field
2764160	2769200	that we have techniques for understanding these models so that we know how they're going to behave
2769200	2773920	when we deploy them. And it would be wonderfully exciting if you all wanted to try to scale the
2773920	2778640	methods we talk about to a model that was as big as eight or 10 billion parameters.
2778720	2783680	Ambitious just to do that, but then maybe a meaningful step forward. Yeah.
2784640	2789440	I have a question back to, like, this baseball cap prompt that we were discussing. So I suppose,
2789440	2794400	like, a part of the way that we discuss rules is, like, there is a little bit of ambiguity
2794400	2798720	for, like, human interpretation, like, for example, in the honor code and the fundamental standard,
2798720	2804240	like, it's intentionally ambiguous so that it's context dependent. And so, like, the idea is that
2804240	2809280	there's, like, this inherent underlying value system that, like, affords whatever the rules
2809280	2815040	that are written out are. And so that's, like, the primary form of evaluation. And so, I guess,
2815040	2819280	like, how does that play into, then, how these language models are understanding? Like, is there
2819280	2824240	some form of encoded or understanding, understood deeper value system that's encoded into them?
2826240	2830800	You could certainly ask. I mean, the essence of your question is, could we, with analysis techniques,
2830880	2836400	say, find out that a model had a particular belief system that was guiding its behavior?
2837040	2841760	I think we can ask that question now. It sounds fantastically difficult, but maybe piecemeal
2841760	2846560	we could get, make some progress on it for sure. Yeah, I want to return to the MLB one, though,
2846560	2851600	because, well, as you'll see, and as I think we already saw, these models purport to offer
2851600	2854960	evidence from a rule book. And that's where I feel stuck.
2854960	2863760	You're keeping score at home. I posted the answer and some other stuff in the class discussion.
2863760	2869680	Wonderful. Thank you. Yes?
2869680	2876080	Can we just hook up these models to a large database of actually required information
2876080	2879600	that's been encyclopedia and allow it to, you know, work stuff up?
2880560	2886720	Oh, well, kind of yes. Actually, this is the sort of solution that I want to advocate for.
2886720	2888240	I'm going to do this in a minute. Yeah.
2891120	2895200	Here, let's, so we'll do this overview. I want to give you a feel for how the course will work
2895200	2900480	and then dive into some of our major themes. So high-level overview. We've got these topics,
2900480	2905200	contextual representations, transformers and stuff, multi-domain sentiment analysis.
2905200	2909760	That will be the topic of the first homework and it's going to build on the first unit there.
2910560	2914800	Retrieval augmented in context learning. This is where we might hook up to a database and get
2914800	2920560	some guarantees about how these models will behave. Compositional generalization. In case
2920560	2924800	you were worried that all the tasks were solved, I'm going to confront you with a task, a seemingly
2924800	2930080	simple task about semantic interpretation that you will, well, I think it will not be solved.
2930080	2933520	I mean, those could be famous last words because who knows what you all are capable of,
2934160	2938960	but it's a very hard task that we will pose. We'll talk about benchmarking and adversarial
2938960	2943360	training and testing. Increasingly important topics as we move into this mode where everyone
2943360	2948400	is interacting with these large language models and feeling impressed by their behavior. We need
2948400	2953680	to take a step back and rigorously assess whether they actually are behaving in good ways or whether
2953680	2957280	we're just biased toward remembering the good things and forgetting the bad ones.
2958400	2962160	We'll do model introspection. That's the explainability stuff that I mentioned and finally
2962160	2967440	methods and metrics. And as you can see for the like five, six, and seven, that's going to be in
2967440	2972560	the phase of the course where you're focused on final projects. And I'm hoping that that gives you
2972560	2978640	tools to write really rich final papers that have great analysis in them and really excellent
2978640	2983360	assessments. And then for the work that you'll do, we're going to have three assignments.
2984320	2988480	And each one of the assignments is paired with what we call a bake-off, which is an informal
2988560	2994000	competition around data and modeling. Essentially, the homework problems ask you to set up some
2994000	2999360	baseline systems and get a feel for a problem. And then you write your own original system
2999920	3004800	and you enter that into the bake-off. And we have a leaderboard on Gradescope and the team
3004800	3009840	is going to look at all your submissions and give out some prizes for top-performing systems,
3009840	3015280	but also systems that are really creative or interesting or ambitious or something like that.
3015280	3020960	And that has always been a lot of fun and also really illuminating because it's like crowdsourcing
3020960	3026720	a whole lot of different approaches to a problem. And then as a group, we can reflect on what worked
3026720	3031680	and what didn't and look at the really ambitious things that you all try. So that's my favorite part.
3031680	3038720	We have three offline quizzes and this is just as a way to make sure you have incentives to really
3038800	3044880	immerse yourself in the course material. And those are done on Canvas. There's actually a
3044880	3049680	fourth quiz, which I'll talk a little bit about probably next time, that is just making sure
3049680	3055200	you understand the course policies. That's quiz zero. You can take it as many times as you want,
3055200	3060880	but the idea is that you will have some incentive to learn about policies like due dates and so
3060880	3066480	forth. And then the real action is in the final project and that will have a lit review phase,
3066560	3072240	an experiment protocol, and a final paper. Those three components, you'll probably do those in teams
3072240	3076720	and throughout all of that work, you'll be mentored by someone from the teaching team.
3076720	3083040	And as I said before, we have this incredibly expert teaching team, lots of varied expertise,
3083040	3087600	a lot of experience in the field. And so we hope to align you with the person,
3088240	3093280	with someone who's really aligned with your project goals. And then I think you can go really,
3093280	3098800	really far. Yeah? It looks like we've got one quarter already looking forward to Bake Offs.
3098800	3104560	I don't understand what kids get obsessed about this stuff. On the final project,
3104560	3111280	is this more of an academic paper or a rather about building working code and
3112480	3116800	showing the state of the art? Great question. For the first one, the Bake Offs. Yes,
3116800	3122160	it is easy to get obsessed with your Bake Off entry. I would say that if you get obsessed
3122160	3129200	and you do really well, just make that into your final project. All three of them are really
3129200	3133440	important problems. They are not idle work. I mean, one of them is on retrieval augmented
3133440	3137600	in-context learning, which is one of my core research focuses right now. So is compositional
3137600	3141680	generalization. If you do something really interesting for a Bake Off, make it your final
3141680	3146960	paper and then go on to publish it. For the second part of your question, I would say that the core
3146960	3151760	goal is to get you to produce something that could be a research contribution in the field.
3152320	3157680	And we have lots of success stories. I've got links at the website to people who have gone on to
3157680	3163840	publish their final paper as an NLP paper. I'm careful the way I say that. They didn't literally
3163840	3168640	publish the final paper because in 10 weeks, almost no one can produce a publishable paper.
3168640	3173760	It's just not enough time. But you could form the basis for then working a little bit more
3173760	3178080	or a lot more and then getting a really outstanding publication out of it. And I would
3178080	3182400	say that that's the default goal. The nature of the contribution though is highly varied.
3182400	3186960	We have one requirement, which is that the final paper have some quantitative evaluation in it.
3187760	3191840	But there are a lot of ways to satisfy that requirement and then you could be serving
3191840	3197360	many different questions in the field for some expansive notion of the field as well.
3198320	3213120	Background materials. So I should say that officially, we are presupposing CS224N or CS224S,
3213120	3218080	as prerequisites for the course. And what that means is that I'm going to skip a lot of the
3218080	3224800	fundamentals that we have covered in past years. If you need a refresher, check out the background
3224800	3231440	page of the course site. It covers fundamentals of scientific computing, static vector representations
3231440	3237680	like Word2Vec and Glove, and supervised learning. And I'm hoping that that's enough of a refresher.
3237680	3243600	If you look at that material and find that it too is kind of beyond where you're at right now,
3243600	3248000	then contact us on the teaching team and we can think about how to manage that.
3248960	3253520	But officially, this is a course that presupposes CS224N.
3256080	3260080	And then the core goals, this kind of relates to that previous question. Hands-on experience with
3260080	3266080	a wide range of problems. Mentorship from the teaching team to guide you through projects and
3266080	3271840	assignments. And then really the central goal here is to make you the best, that is most insightful,
3271840	3278640	most responsible, most flexible NLU researcher and practitioner that you can be for whatever
3278640	3283600	you decide to do next. And we're assuming that you have lots of diverse goals that somehow connect
3283600	3295760	with NLU. All right, let's do some course themes unless there are questions. I have a whole final
3295760	3302000	section of this slideshow that's about the course materials and requirements and stuff
3302000	3305680	might save that for next time. And you can check it out at the website and you'll be
3305680	3311600	forced to engage with it for quiz zero. I thought instead I would dive back into the
3311600	3314080	content part of this unless there are questions or comments.
3314400	3325920	All right, first course theme, transformer-based pre-training. So starting with the transformer,
3325920	3330560	we want to talk about core concepts and goals, give you a sense for what these models are like,
3331200	3336240	why they work, what they're supposed to do, all of that stuff. We'll talk about a bunch of different
3336240	3341840	architectures. There are dozens and dozens of them, but I hope that I have picked enough of them
3341840	3347120	with the right selection of them to give you a feel for how people are thinking about these models
3347120	3352000	and the kind of innovations they've brought in that have led to real meaningful advancement,
3352000	3356480	just at the level of architectures. We'll also talk about positional encoding, which I think
3356480	3361680	maybe a lot of us have been surprised to see just how important that is as a differentiator for
3361680	3368000	different approaches in this space. We'll talk about distillation, taking really large models
3368000	3373920	and making them smaller. It's an important goal for lots of reasons and an exciting area of research.
3373920	3378880	And then as I mentioned, it's going to do a little lecture for us on diffusion objectives for these
3378880	3385360	models, and then it's going to talk about practical pre-training and fine-tuning. I'm going to enlist
3385360	3390080	the entire teaching team to do guest lectures, and these are the two that I've lined up so far.
3391760	3396320	And that will kind of culminate or be aligned with this first homework in Bakeoff, which has a
3396320	3400720	multi-domain sentiment. I'm going to give you a bunch of different sentiment data sets,
3400720	3404480	and you're going to have to design one system that can kind of succeed on all of them.
3405040	3410000	And then for the Bakeoff, we have an unlabeled data set for you. We have the labels, but you won't.
3410640	3416480	And that has data that's like what you developed on, and then some mystery examples that you will
3416480	3421520	not really be able to anticipate. And we're going to see how well you do at handling all of these
3421520	3429360	different domains with one system. And this is by way of kind of, again, a refresher on core
3429360	3434160	concepts in supervised learning and really getting you to think about transformers, although we're
3434160	3438240	not going to constrain the kind of solution that you offer for your original system.
3442480	3448240	Our second major theme will be retrieval augmented in-context learning, a topic
3449120	3456080	that I would not even have dreamt of five years ago and seemed kind of infeasible three years ago,
3456080	3461680	and that we first did two years one year ago. Oh goodness. I think this is only the second time,
3461680	3465440	but I had to redo it entirely because things have changed so much.
3467360	3473040	Here's the idea. We have two characters so far in our kind of emerging narrative for NLU.
3473680	3477840	On the one hand, we have this approach that I'm going to call LLMs for everything,
3477840	3483520	large language models for everything. You input some kind of question. Here I've chosen a very
3483520	3489360	complicated question. Which MVP of a game Red Flaherty umpired was elected to the Baseball
3489360	3493920	Hall of Fame? And hats off to you if you know that the answer is Sandy Kofax.
3496000	3502000	The LLMs for everything approach is that you just type that question in and the model gives you an
3502000	3508480	answer. And hopefully you're happy with the answer. The other character that I'm going to
3508480	3513600	introduce here is what I'm going to call retrieval augmented. So I have the same question at the top
3513600	3517360	here, except now this is going to proceed differently. The first thing that we will do
3517920	3524320	is take some large language model and encode that query into some numerical representation.
3525280	3530080	That's sort of familiar. The new piece is that we're going to also have a knowledge store
3530640	3538240	which you could think of as an old fashioned web index, right? Just a knowledge store of documents
3538240	3544160	with the modern twist that now all of the documents are also represented by large language models.
3544160	3548720	But fundamentally, this is an index of a sort that drives all web search right now.
3549680	3555120	We can score documents with respect to queries on the basis of these numerical representations.
3555120	3561280	And if we want to, we can reproduce the classic search experience. Here I've got a ranked list
3561280	3567680	of documents that came back from my query, just like when you do Google as of the last time I googled.
3568880	3573440	But in this mode, we can continue, right? We could have another language model slurp up those
3573440	3579040	retrieved documents and synthesize them into an answer. And so here at the bottom I've got,
3579040	3583760	it's kind of small, but it's the same answer over here, although notably this answer is now
3583760	3589520	decorated with links that would allow you the user to track back to what documents actually
3590560	3596480	provided that evidence. Whereas on the left, who knows where that information came from.
3596480	3598560	And that's kind of what we were already grappling with.
3600640	3605360	This is an important societal need because this is taking over web search. What are our goals
3605360	3610400	for this kind of model here? So first, we want synthesis, fluency, right? We want to be able to
3610400	3616160	take information from multiple documents and synthesize it down into a single answer. And I
3616160	3621040	think both of the approaches that I just showed you are going to do really well on that. We also
3621040	3626880	need these models to be efficient, to be updatable, because the world is changing all the time.
3627680	3633040	We need it to track provenance and maybe invoke something like factuality, but certainly
3633040	3637520	provenance. We need to know where the information came from. And we need some safety and security.
3637520	3642240	We need to know that the model won't produce private information, and we might need to restrict
3642240	3646880	access to parts of the model's knowledge to different groups, like different customers or
3646880	3651120	different people with different privileges and so forth. That's what we're going to need if we're
3651120	3657200	really going to deploy these models out into the world. As I said, I think both of the approaches
3657200	3661200	that I sketched do well on the synthesis part, because they both use a language model and those
3661200	3666960	are really good. They all have the gift of gab, so to speak. What about efficiency, right? On the
3666960	3674720	LLM for everything approach, we had this undeniable rise in model size, and I pointed out models like
3674720	3681600	an alpaca that are smaller. But I strongly suspect that if we are going to continue to ask these
3681600	3688240	models to be both a knowledge store and a language capability, we're going to be dealing with these
3688240	3695520	really large models. The hope of the retrieval augmented approach is that we could get by with
3695520	3700800	the smaller models, and the reason we could do that is that we're going to factor out the knowledge
3700800	3706480	store into that index and the language capability, which is going to be the language model. The only
3706480	3711920	thing we're going to be asking the language model is to be good at that kind of in-context learning.
3711920	3717600	It doesn't need to also store a full model of the world, and I think that means that these models
3717600	3722960	could be smaller. So overall, a big gain in efficiency if we go retrieval augmented.
3723440	3726160	People will make progress, but I think it's going to be tense.
3728000	3731760	What about updateability? Again, this is a problem that people are working on very
3731760	3738320	concertedly for the LLMs for everything approach, but these models persist in giving outdated answers
3738320	3742880	to questions. And one pattern you see is that there's a lot of progress where you could like
3742880	3747280	edit a model so that it gives the correct answer to who is the president of the US,
3747280	3753760	but then you ask it about something related to the family of the president, and it reveals that it
3753760	3759120	has outdated information stored in its parameters. And that's because all of this information is
3759120	3765600	interconnected, and we don't at the present moment know how to reliably do that kind of systematic
3765600	3773840	editing. Okay, on the retrieval augmented approach, we just re-index our data. If the world changes,
3773840	3779040	we assume that the knowledge store changed like somebody updated a Wikipedia page. So we
3779040	3783840	represent all the documents again, or at least just the ones that changed. And now we have a lot
3783840	3788720	of guarantees that as that propagates forward into the retrieved results, which are consumed by the
3788720	3794880	language model, it will reflect the changes we made to the underlying database in exactly the same
3794880	3802960	way that a web search index is updated now. One forward pass of the large language model
3802960	3809120	compared to maybe training from scratch over here on new data to get an absolute guarantee
3809120	3815200	that the change will propagate. What about provenance? Okay, we have seen this already,
3815200	3822880	this problem here. LLMs for everything. I asked GPT3, the DaVinci 3 model, my question,
3822880	3827760	are professional baseball players allowed to glue small wings onto their caps? But I kind of cut it
3827760	3834400	off, but at the top there I said provide me some links to the evidence. And it dutifully
3834400	3840240	provided the links, but none of the links are real. If you copy them out and follow them,
3840240	3846880	they all go to 404 pages. And I think that this is worse than providing no links at all,
3846880	3852320	because I'm attuned as a human in the current moment to see links and think they're probably
3852400	3857680	evidence. And I don't follow all the links. And here you might look and say, oh yeah,
3857680	3866080	I see it found the relevant MLB pages and that's it, right? Over here, the kind of the point of this
3866080	3870560	is that we are first doing a search phase where we're actually linked back to documents,
3870560	3875040	and then we just need to solve the interesting, non-trivial question of how to link those
3875040	3880480	documents into the synthesized answer. But all of the information we need is right there on the
3880480	3885920	screen for us, and so this feels like a relatively tractable problem compared to what we are faced
3885920	3894240	with on the left. I will say, I've been just amazed at the rollout, especially of the Bing
3894240	3899840	search engine, which now incorporates open AI models at some level, because it is clear that it
3899840	3904800	is doing web search, right? Because it's got information that comes from documents that only
3904800	3911200	appeared on the web days before your query. But what it's doing with that information seems
3911200	3916640	completely chaotic to me, so that it's kind of just getting mushed in with whatever else the
3916640	3923440	model is doing, and you get this unpredictable combination of things that are grounded in
3923440	3928560	documents and things that are completely fabricated. And again, I maintain this is worse than just
3928560	3936160	giving an answer with no evidence attached to it. I don't know why these companies are not simply
3936160	3940480	doing the retrieval augmented thing, but I'm sure they are going to wise up, and maybe your research
3940480	3946560	could help them wise up a little bit about this. Finally, safety and security. This is relatively
3946560	3951280	straightforward. On the LLMs for everything approach, we have a pressing problem, privacy
3951280	3955760	challenges. We know that those models can memorize long strings in their training data, and that
3955840	3960560	could include some very particular information about one of us, and that should be worrying us.
3961280	3966720	We have no known way with a language model to compartmentalize LLM capabilities and say, like,
3966720	3972720	you can see this kind of result, and you cannot. And similarly, we have no known way to restrict
3972720	3978320	access to part of an LLMs capabilities. They just produce things based on their prompts,
3978320	3982640	and you could try to have some prompt tuning that would tell them for this kind of person or setting
3982640	3989120	do this and not that, but nobody could guarantee that that would succeed. Whereas for the retrieval
3989120	3995440	augmented approach, again, we're thinking about accessing information from an index and access
3995440	4001840	restrictions on an index is an old problem by now. Again, I don't want to say solved, but something
4001840	4007760	that a lot of people have tackled for decades now, and so we can offer something like guarantees
4008320	4011440	just from the fact that we have a separated knowledge store.
4014640	4019600	Again, my smiley face. You can see where my feelings are. For the LLMs for everything approach,
4020160	4024080	you know, people are working on these problems, and it's very exciting. And if you want a challenge,
4024640	4029680	take, you know, take up one of these challenges here, but over here on the retrieval augmented side,
4029680	4034000	I think we have lots of reasons to think it's not that they're completely solved. It's just
4034000	4040160	that we can see the path to solving them. And this feels very urgent to me because of how suddenly
4040800	4045440	this kind of technology is being deployed in a very user facing way for one of the core things
4045440	4051120	we do in society, which is web search. So it's an urgent thing that we get good at this.
4053680	4059040	Final things I want to say about this. So until recently, the way you would do even the
4059040	4065040	retrieval augmented thing would be that you would have your index, and then you might train
4065040	4070000	a custom purpose model to do the question answering part, and it could extract things from the text
4070000	4073680	that you produced or maybe even generate some new things from the text that you produced.
4074720	4078800	And that's kind of the mode that I mentioned before where you'd have like some language models,
4078800	4083360	maybe a few of them, and you'd have an index, and you would stitch them together into a question
4083360	4088400	answering system that you would probably train on question answering data. And you would hope
4088400	4093920	that this whole big monster, maybe fine tuned on squad or natural questions or one of those data
4093920	4102000	sets, gave you a general purpose question answering capability. That's the present, but I think it
4102000	4107920	might actually be the recent past. And in fact, the way that you all will probably work when we do
4107920	4114160	this unit, and certainly for the homework, is that we will just have frozen components. And this
4114160	4120560	starts from the observation that the retriever model is really just a model that takes in text
4120560	4127760	and produces text with scores. And a language model is also a device for taking in text and
4127760	4132560	producing text with scores. And these are when these are frozen components, you can think of them
4132560	4137680	as just black box devices that do this input output thing. And then you get into the intriguing
4137680	4143360	mode of asking, well, what if we had them just talk to each other? And that is what you will do
4143360	4148640	for the homework and bake off, you will have frozen retriever and a frozen large language model,
4148640	4155040	and you will get them to work together to solve a very difficult open domain question answering
4155040	4161120	problem. And that's kind of pushing us into a new mode for even thinking about how we design AI
4161120	4166400	systems where it's not so much about fine tuning, it's much more about getting them to communicate
4166400	4173600	with each other effectively to design a system from frozen components. Again, unanticipated,
4173600	4178720	at least by me, as of a few years ago, and now an exciting new direction.
4180800	4184960	So just to wrap out, I think what I'll do since we're near the end of the class here,
4184960	4189200	I'll just finish up this one unit, and then we'll use some of our time next time to introduce
4189200	4195600	a few other of these course themes, and that'll set us up well for diving into transformers.
4195680	4201360	Final piece here just to inspire you, few shot open QA is kind of the task that you will tackle
4201360	4206000	for homework too. And here's how you could think about this. Imagine that the question has come
4206000	4211680	in, what is the course to take? The most standard thing we could do is just prompt the language
4211680	4216720	model with that question, what is the course to take down here and see what answer it gave back,
4216720	4222880	right? But the retrieval augmented insight is that we might also retrieve some kind of passage
4222880	4227120	from a knowledge store. Here I have a very short passage, the course to take is natural language
4227120	4233680	understanding, and that could be done with a retrieval mechanism. But why stop there? It might
4233680	4240080	help the model as we saw going back to the GPT3 paper to have some examples of the kind of behavior
4240080	4244960	that I'm hoping to get from the model. And so here I have retrieved from some data set,
4244960	4248880	question answer pairs that will kind of give it a sense for what I want it to do in the end.
4249680	4256400	But again, why stop there? We could also pick questions that were based very closely on the
4256400	4261760	question that we posed. That would be like K nearest neighbor's approach where we use our retrieval
4261760	4268560	mechanism to find similar questions to the one that we care about. I could also add in some context
4268560	4274640	passages and I could do that by retrieval. So now we've used the retrieval model twice potentially,
4274640	4278880	wants to get good demonstrations and wants to provide context for each one of them.
4279840	4284640	But I could also use my retrieval mechanism with the questions and answers from the demonstration
4284640	4290320	to get even richer connections between my demonstrations and the passages. I could even
4290320	4295600	use a language model to rewrite aspects of those demonstrations to put them in a format
4295600	4301920	that might help me with the final question that I want to pose. So now I have an interwoven use
4301920	4307360	of the retrieval mechanism and the large language model to build up this prompt, right?
4308240	4312640	Down at the retrieval thing, I could do the same thing. And then when you think about the model
4312640	4318240	generation, again, we could just take the top response from the model, but we can do very
4318240	4325040	sophisticated things on up to this full retrieval augmented generation model, which essentially
4325040	4330800	marginalizes out the evidence passage and gives us a really powerful look at a good answer,
4330880	4337440	conditional on that very complicated prompt that we constructed. I think what you're seeing on the
4337440	4343520	left here is that we are going to move from an era where we just type in prompts into these models
4343520	4350640	and hope for the best into an era where prompt construction is a kind of new programming mode
4351280	4357120	where you're writing down computer code, could be Python code, that is doing traditional computing
4357120	4364080	things, but also drawing on very powerful pre-trained components to assemble this kind
4364080	4369920	of instruction kit for your large language model to do whatever task you have set for it.
4370480	4375680	And so instead of designing these AI systems with all that fine tuning I described before,
4375680	4381120	we might actually be moving back into a mode that's like that symbolic mode from the 80s
4381120	4386560	where you type in a computer program. It's just that now the program that you type in
4386640	4393360	is connected to these very powerful modern AI components and we're seeing right now
4394000	4398560	that that is opening doors to all kinds of new capabilities for these systems
4398560	4402640	and this first homework in Bake Off is going to give you a glimpse of that.
4403440	4408080	And you're going to use a programming model we've developed called demonstrate search predict
4408080	4411200	that I hope will give you a glimpse of just how powerful this can be.
4412080	4420480	All right, we are out of time, right, 420? So next time I'll show you a few more units from the
4420480	4423600	course and then we'll dive into transformers.
