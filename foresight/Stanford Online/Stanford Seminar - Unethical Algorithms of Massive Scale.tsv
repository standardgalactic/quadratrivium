start	end	text
0	14880	Who was here last time I was here?
14880	17000	I know Hal was.
17000	20040	Oh, I know that guy, Rick.
20040	21040	And oh, I remember you two.
21040	22040	What's your first name?
22040	23040	Eugene.
23040	24040	Eugene, of course.
24040	25040	How are you?
25040	26040	Okay.
26040	27040	Good.
27080	31400	When I recall, when I was here last time, I came with a lot of slides and I decided
31400	33480	not to use them.
33480	38680	And I'm going to do the same thing again, except I think I'm going to show you one slide
38680	40680	because it just blows my mind.
40680	43520	So when we get to that one point, I'm going to turn this on and I'm going to show you
43520	45440	one slide.
45440	47320	And by then, I hope you'll be prepared.
47320	51040	Oh, dear.
51040	53720	Well, that shouldn't be.
53720	54720	How do we get rid of that?
54720	56960	We're just going to go like that.
56960	59320	So well, that's only half of it anyway.
59320	61560	That's not the important half of the slide.
61560	66680	So I want to take us on a journey.
66680	74080	And the journey is going to take us from the hypothetical to the somewhat real, to the
74080	79280	real, to the surreal.
79280	81920	So we're going to move in that direction.
81920	84680	So let's start with the hypothetical.
84680	88560	And in fact, I liked Andy's introduction, short, sweet.
88560	96960	He mentioned something about influence, perhaps on the internet, perhaps using new technologies.
96960	101400	Let's consider a few hypothetical situations.
101400	102400	Here we go.
102400	104760	We'll start with Facebook.
104760	117440	Let's say that last spring or summer, Facebook sent out, this is hypothetical now, sent out
117440	121280	reminders to go register to vote.
121280	128320	Now at the moment, they have nearly two billion members.
128400	135640	Among those members, there are 72% of the adult population of the US.
135640	143760	So if Facebook chose to send out, go out and register to vote reminders to their people,
143760	145280	they could reach a lot of people.
145280	151000	They could reach a lot of people because in fact, a lot of Americans who are eligible
151000	153760	to vote are not registered.
153760	160800	In fact, I think it's roughly 70 million Americans who are eligible to vote out of the 220 or
160800	164760	so who could vote are not even registered.
164760	171120	So if you take roughly 0.7 times 0.7, if you see where I got those numbers from, then
171120	179680	that means Facebook could reach 50 million Americans who are not registered to vote and
179680	184480	could send out reminders to them.
184480	187560	Now here's the hypothetical part.
187560	193200	What if they sent out those reminders only to people in certain demographics?
193200	199000	What if they chose to send out reminders just to Democrats, let's say, or just to Republicans,
199000	203920	or just to supporters of Donald Trump, or just to supporters of Hillary Clinton?
203920	206840	Would anyone know that?
206840	213600	I don't think anyone would know that because what Facebook sends out is always targeted.
213600	221920	No one keeps track of which groups are receiving those messages or those items on the newsfeed.
221920	224000	No one keeps track.
224000	230000	So in fact, hypothetically, Facebook could have sent out those reminders to register
230000	238000	to vote, reaching 50 million unregistered adults who are eligible to vote, and no one
238000	244080	would have known they could have done so selectively just to supporters of one candidate.
244080	248640	Now how many additional people would then have registered to vote?
248640	250920	We'll get to that.
250920	258600	Okay, now we're still on the hypothetical, but let me just mention on this one issue
258600	266160	that in fact, Facebook did send out reminders to go register to vote.
266160	274780	They sent them out en masse, and the New York Times concluded that because, very clever
274780	279280	and simple what they came up with, the New York Times concluded that because Facebook,
279280	285160	generally speaking, reaches a younger audience, and because younger people tend to be more
285160	291480	Democrat than Republican, then in fact, Facebook, even if it did broadcast these messages to
291480	297520	everyone, that what they did was advantageous to Democrats.
297520	300840	That was the New York Times conclusion, and they came up with some numbers, they came
300840	308400	up with some estimates, but in fact, we don't really know to whom those reminders were sent.
308400	309400	Okay, there's example one.
309400	312360	Now let's go to example two.
312480	319760	Election day, if on election day, Facebook chose to send out go out and vote reminders
319760	326240	just to people of one political party or just to supporters of one candidate, how many additional
326240	336400	people might they stimulate to get off of their sofas and go vote?
337400	344200	Well, as it happens, there's actually an answer to this question, because in 2012, Facebook,
344200	350320	with some people I know from the University of California, San Diego, published an article
350320	355600	about a manipulation that they did in 2010 during the 2010 election.
355600	360920	They in fact sent out go out and vote reminders to 60 million of their members on election
360920	368240	day in 2010, and they had a control group, and they did some surveying to try to figure
368240	376560	out who went out and voted in the experimental group, when the group that was getting the
376560	381560	reminders and in the control group, and they concluded that their reminders on election
381560	390520	day in 2010 caused an additional 340,000 people to go out and vote.
390520	397400	So if you extrapolate from that published experiment, if you extrapolate from that to
397400	409920	election day in 2016, that would tell you conservatively that if on election day Facebook
409920	418280	had sent out go out and vote reminders selectively to supporters of one party or another, conservatively
418280	425240	speaking, they could easily have caused an additional six or 700,000 people to go out
425240	431680	and vote who otherwise would have stayed home, and no one would have known.
431680	436560	So did they actually do that on election day?
436560	443080	I don't know, but as you'll see, we're going to move from the hypothetical to the real
443080	447240	and a little bit in the direction of the surreal, so we'll get back to this.
447240	452640	Now there are other things hypothetically that Facebook could have done to manipulate
452640	454000	people last year.
454000	459880	You may recall that a whistleblower turned up, a former Facebook employee who had been
459880	467040	one of the news curators, and in fact I met another of the news curators for Facebook
467040	472280	sometime later in New York, and who told me that no, they weren't sitting in a basement
472280	478640	as the press had reported, no, that was false, but yes, they really were a bunch of recent
478640	486560	college graduates with very liberal leanings, and yes, they were selectively removing conservative
486560	493040	news stories from the news feed that Facebook shows to people, and Facebook now seems to
493040	497800	be, according to some reports, the main place people are getting their news, or at least
497800	501560	getting links to their news stories.
501560	511920	So the curators, human curators, were indeed messing with the news feed in a way that favored
511920	513520	the Democrats.
513520	515720	So this is an actual whistleblower who came forth.
515720	521260	So this is, but let's keep it hypothetical, because first of all, we don't know that the
521260	525840	management of Facebook had anything to do that, it could have just been that they happened
525840	530280	to hire these particular people with these particular political leanings, right?
530280	536720	So just an error, really a kind of, you know, an oversight, you could consider, however,
536720	542160	let's look at this hypothetically, what if they deliberately wanted to do that?
542160	546680	What if it wasn't just an accident of hiring, what if they deliberately wanted to alter
546680	551200	our opinions on things by altering news feeds?
551200	552200	Could they do that?
552200	554560	Yes, are they doing that?
554560	558960	Well, there was that whistleblower, but you know, all those people got fired, and they
558960	564280	said they were moving to an algorithm, and so are they doing this now?
564280	566400	I don't know.
566400	567960	So it's still hypothetical.
567960	571600	So you've got the news feeds, you've got the trending stories, same thing could be
571600	573120	done there, right?
573120	581580	So Facebook has a bunch of different ways to alter opinions without people knowing.
581580	583840	That's what's important.
583840	586180	That's what's important.
586180	592260	They even have a fifth way, because are any of you here Facebook users and willing to
592260	593260	admit it?
593260	596860	Okay, well, that's most of you, wow.
596860	606820	You know they have a search bar, but what you may or may not know is that although the
606820	612620	search bar is usually used to find your friends and family, it can be used for other purposes
612620	621660	too, and before the election, Facebook posted a video urging people to search for election
621660	630820	2016 in the search bar, and then that would give you a list, again, a feed of information
630820	633500	which of course they had complete control over.
633500	636580	So they've got the search bar, they've got the trending stories, they've got the news
636580	644940	feed, they've got the possibility of sending out selective reminders, lots of things, none
644940	646940	of which would be visible to people.
646940	653460	Let's shift over to the big guy, Google.
653460	662900	So Google controls lists also, news feeds are lists, and Google controls lists.
662900	668980	Two very, very important lists, among others, but two extremely important lists, both of
668980	675820	which are generated on the fly when you get onto Google search and start to conduct a
675820	677060	search.
677060	681020	The first list is the little list up at the top.
681020	685420	Those are the search suggestions in what is sometimes called autocomplete.
685420	692140	Google invented autocomplete, 2004 I believe, and when they invented it, it had I believe
692260	698860	10 search suggestions, and it was an opt-in feature.
698860	706580	But a couple of years later, it was no longer an opt-in, in fact, you couldn't opt out.
706580	712580	And then sometime later, the list began to change, initially it appeared that the list
712580	716700	was just showing you what other people were searching for.
716700	721700	And if you, right now, for example, if you go and conduct a search on Yahoo, don't waste
721740	727380	your time because their search results are terrible, by the way, and they're pulling
727380	733660	almost all of their search results from Google under an arrangement that was signed between
733660	738260	the two companies in late 2015, however, let's put that aside.
738260	744860	The point is, when you try to do a search on Yahoo, you get 10 search results, and they
744860	748140	appear to be what people are searching for.
748140	753340	If you use Bing, which I also wouldn't bother with, but you get eight search results, which
753340	755700	also appear to be what people are searching for.
755700	761700	You can confirm that, in fact, the suggestions you're getting on Bing and Yahoo are really,
761700	764060	generally speaking, just what people are searching for.
764060	765660	How do you confirm that?
765660	767020	It's very simple.
767020	772220	You go over to Google, Trends, and you can see what people are searching for.
772300	779540	So, meanwhile, Google's list of suggestions over the years, for some reason, got smaller
779540	786460	and smaller and smaller, so that, at least on most devices, they only show you four items.
786460	791540	Occasionally, you'll get three, two, or one.
791540	796500	There are some circumstances under which you can get five, six, seven, but in fact, we actually
796500	801140	did an extensive survey to figure out how likely those possibilities are.
801220	802620	Very unlikely.
802620	805980	You generally speaking, on most devices, get four.
805980	809820	On some mobile devices, you get five, but the point is, it's a short list.
809820	816900	And the items on that list no longer have any obvious correspondence to what people
816900	820260	are searching for, so what are they showing you?
820260	827620	Well, the point is, here's a list that Google controls that we all see, perhaps, every day
827620	832300	or perhaps many times a day, and it's the search suggestions list.
832300	834700	And generally speaking, it has four items.
834700	839100	And generally speaking, those four items don't have any obvious correspondence anymore to
839100	840940	what people are searching for.
840940	842140	Very easy to show that.
842140	844900	You just look on Google Trends.
844900	851660	More and more, the items that appear on that list also are customized for the individual.
851660	856020	So that's one reason why, of course, they're not going to have much correspondence to what
856020	859820	people in general are searching for, because they're going to have to do with you and your
859820	864020	history and what Google's algorithm perceives as your needs.
864020	865420	So there's a list.
865420	868660	And then they've got this other list, because once you click on something, sometimes even
868660	876460	if you don't click on something, in the suggestions, the search results populate.
876460	879300	Sometimes you don't even have to click, and the results populate below.
879300	881300	So now you've got a second list.
881300	884620	So I've got a short list, and I've got a long list.
884620	889380	Now the long list, as you know, goes on forever.
889380	894740	But most people don't go beyond the first page of results, which shows you 10 results.
894740	899900	In fact, 50% of all clicks go to the top two results.
899900	905780	And more than 90% of all clicks are on the first page.
905780	914820	So what's on that first page is very, very important, extremely important, two lists.
914820	918660	Now let's get hypothetical here.
918660	928780	What if Google were using search suggestions not to help you do your search, although that's
928780	931540	what they would claim, of course, but they're actually a business.
931540	936060	They're not really the public library, like they pretend to be, they're actually a business.
936060	942100	And so what if Google were actually using search suggestions in a way that makes them
942100	944380	more money?
944380	946140	How could they use search suggestions?
946140	951460	It's hypothetical, I'm not claiming anything.
951460	954540	But how would they do that?
954540	960340	Well, as a matter of fact, we know a lot about this, that's when I get to the real part of
960340	963040	my talk, I'll explain to you how we know.
963040	967180	We know a lot about this, but just staying hypothetical for the moment, how could they
967180	968180	do this?
968180	972220	Well, first of all, if they know about you and your search history and your interests
972220	977420	and so on, in fact, they know a lot about all of us, well, they could put items there
977420	981420	that they think you're likely to click on.
981420	986700	And that they think you want to see, maybe, but more importantly, they could put items
986700	994500	there on that list that you're very likely to click on rather than doing what.
994500	996580	What do they not want you to do?
996580	1004980	Yes, they don't want you to type your search term, your full search term.
1004980	1008940	This is hypothetical, of course.
1008940	1015220	So hypothetically, if I were running Google and I wanted to make a lot of money, I would
1015220	1019860	figure out how to show people search terms that made it very likely that people are going
1019860	1023980	to click on one of the search terms.
1023980	1028860	And I would also, since I'm now controlling what people are clicking on, I would make sure
1028860	1034260	that the results that populate as a result of a click or even without a click, I would
1034260	1039300	make darn sure that those results are results I want to show people.
1039300	1040300	See?
1040300	1041700	Yes, what is your name?
1041700	1042700	Hi, John.
1042700	1050300	Why don't you think they want you to type out what it is you really want?
1050300	1054620	You know, I don't really want to speculate about the motives, particular motives.
1054620	1062560	I guess what I'm saying is if I were in charge, then I would lose control over what people
1062560	1065140	were searching for.
1065140	1066140	Think about this.
1066140	1073620	If I let you go ahead and type your whole search term, and then I let you hit Enter,
1073620	1079260	I'm in a tough position now because my credibility always depends on giving you exactly what
1079260	1080260	you want.
1080260	1084260	So you always have to have that feeling that they answered your question, right?
1084260	1088480	So I'm really kind of stuck because depending on what you typed, I've actually got to give
1088480	1094260	you more or less what it is you wanted or what it is you thought you wanted.
1094260	1095260	See what I'm saying?
1095260	1099500	If I lose some control, whereas if I can get you to click on one of the suggestions that
1099500	1102740	I make, I'm in complete control.
1102740	1106460	That's just better from a business perspective.
1106460	1113660	So we've got this list of here, and again, hypothetically, Google could show people lists
1113660	1117540	that give them more control over what people search for, and another way to put that is
1117540	1123620	Google could show people search terms that would nudge people's searches.
1123620	1128340	You know that great book from 2008 by Thaler and the other guy called Nudge?
1128340	1130500	It's a great, great book.
1130500	1136620	And search suggestions hypothetically could be used to nudge people's searches in directions
1136620	1138580	that are advantageous for the company.
1138580	1141260	It's hypothetical, right?
1141260	1143940	Now let's get to the search results.
1143940	1148820	Search results, 10, those are the key ones.
1148820	1154260	Google has total control over the order in which these items are presented.
1154260	1162940	Hypothetically, they could put items near the top that they want you to click on, and
1162940	1167940	again, where doing so would be advantageous to the company.
1167940	1173180	And that could have to do with making money, that could be one goal, possibly, but could
1173180	1175260	have to do with other things too.
1175260	1180540	People agendas have to do with anything, really.
1180540	1186300	I mean, again, I don't know people's specific motives, I'm just saying one could hypothetically
1186300	1195900	use search results in a way to manipulate people's thinking, beliefs, purchases, certainly,
1195900	1197900	and possibly even their votes.
1197900	1202380	Okay, so we've got a few different hypotheticals here for Facebook.
1202380	1207780	We've got a couple for Google, and I could go on all day about other options Google has.
1207780	1212100	If you want to look at a cool article, it's a piece I wrote for US News and World Report
1212100	1217580	called The News Censorship, and that goes through lots of different crazy lists that
1217580	1223740	Google controls, and if you can control lists, wow, you can control what people think.
1223740	1237220	No, the Great Firewall is trivial by comparison.
1237220	1243300	The Great Firewall cuts off access to a lot of information, although all of the Chinese
1243300	1251500	students who work with me say that in China it's very common for people to use proxies
1251500	1254500	to get around the firewall.
1254500	1260540	But all it does is restrict access, where I'm talking about a much finer degree of control.
1260540	1263340	Also, the Great Firewall is visible.
1263340	1265780	Do you see what I'm saying?
1265780	1269420	It's completely different than the kinds of hypothetical situations I'm discussing so
1269420	1275860	far because I'm talking about methods of influence that are completely invisible.
1275860	1281580	If someone is showing you a list of search suggestions that have been carefully prepared
1281580	1285780	to make sure, make it likely anyway, that you're going to click on one of them and when you
1285780	1290300	do, that serves my corporate needs, you can't see that.
1290300	1291300	It's impossible.
1291300	1293300	And the same is true, of course, with the search results.
1293300	1299180	If I'm putting the results in an order that suits my corporate needs, serves my corporate
1299180	1300340	needs, you can't see that.
1300340	1302100	It's impossible.
1303100	1306060	Okay, so we got a few possibilities for Facebook.
1306060	1307980	We got a couple for Google.
1307980	1312820	Now let's go into crazy land here because remember, we're moving gradually toward the
1312820	1330660	surreal, but still, speaking hypothetically, Tinder, Tinder, Tinder, Tinder is a kind of
1331660	1336540	it's a matching service mainly for people who want to have sex with each other.
1336540	1340900	Very popular sex, I mean, and Tinder too.
1340900	1348860	So Tinder normally just shows you pictures of people and then you swipe left or swipe
1348860	1352780	right indicating whether you're hot or not, right?
1352780	1357620	This brings us right back to Bill Gates and his original app at Harvard that got him into
1357620	1358700	trouble.
1358700	1360580	So that's really what Tinder is.
1360580	1365500	It's just a hot or not, you swipe and so on, and if someone else who you said was hot
1365500	1370340	swipes you as hot, then you get connected with that person and that's basically what
1370340	1371340	Tinder is.
1371340	1377540	Oh, I'm sorry, Zuckerberg, absolutely, sorry.
1377540	1378540	Thank you.
1378540	1379540	I appreciate that.
1379540	1384900	Okay, so, but before the election, a couple months before the election, Tinder announced
1384900	1387180	a very odd application.
1387180	1393220	It was called Swipe the Vote and Tinder offered to help you figure out which candidate you're
1393220	1398220	better suited for, which candidate in other words better serves your values and your needs
1398220	1401460	as a voter, Tinder.
1401460	1407140	So sure enough, when people got onto Tinder, one of the options they had was to swipe the
1407140	1411580	vote, you know, let us help you figure out who to vote for.
1411580	1416540	So of course the people who are most likely to do this are going to be undecided voters.
1417140	1421580	That's gorgeous, that's beautiful because we know a lot about undecided voters and those
1421580	1424740	are the people who are easiest to influence.
1424740	1431740	So over time, I don't know how many people use Swipe the Vote because how would I know,
1431740	1432940	right?
1432940	1434580	But here's the way it worked.
1434580	1439020	It asks a question about immigration and it says, you know, you agree or disagree and
1439020	1443020	you swipe one way or the other and then it asks another question, you know, about taxes
1443020	1445260	and you swipe one way or the other and so on.
1445260	1450660	And it's just pretty much five questions and then it says, you're a perfect match for Donald
1450660	1451660	Trump.
1451660	1457460	So we're still in hypothetical land.
1457460	1467740	What if hypothetically the company had some bias in favor of one candidate or the other?
1467740	1472900	Couldn't they make it, couldn't they tell every single person who swipes the vote?
1473380	1476940	That they should vote for Donald Trump or Hillary Clinton?
1476940	1477940	Couldn't they do that?
1477940	1478940	Would anyone know?
1478940	1479940	Would anyone even notice?
1479940	1486180	You know, if they wanted to mask the effect, as we've done a lot of experiments on masking,
1486180	1489780	you know, and they didn't want it to be that obvious, believe me, that's trivially easy
1489780	1491580	to do.
1491580	1499980	But the point is, here's a matching service at a website used by tens of millions of Americans
1500060	1503660	which is advising people on how to vote.
1503660	1507300	But we don't know what that algorithm is doing.
1507300	1509660	We don't know whether they favor one candidate or the other.
1509660	1516660	Well, it turns out that Tinder is not the only matching service, vote matching service on
1516660	1517660	the internet.
1517660	1520620	They're popping up more and more.
1520620	1525380	Some look really, really credible, you know, they're nonprofits and this and that.
1525380	1527300	But you don't really know who's running them.
1527300	1531060	You don't know what their motives are and you don't know what the algorithm is doing.
1531060	1536900	So hypothetically, you could put matching services on the internet, including on big
1536900	1545500	websites like Tinder, to help people make up their minds about voters or about abortion
1545500	1549140	or about taxes or about homosexuality or about anything.
1549140	1554580	You could put matching services up on the internet and algorithms could be shifting
1554580	1558380	opinions literally by the millions because we know the numbers.
1558380	1559380	This is what we do.
1559380	1560380	This is all we do.
1560380	1562500	We just quantify these effects.
1562500	1569180	And no one would know because it's invisible.
1569180	1574060	Okay, I could go on with hypotheticals but you get the idea.
1574060	1581020	Oh, no, no, was Tinder's algorithm biased toward one candidate or the other?
1581020	1583540	I don't know.
1583540	1588820	Remember all those, I don't know, that's very important.
1588820	1592740	Okay.
1592740	1601020	To put this in perspective, what kinds of manipulations are making the news every day?
1601020	1604140	They're not what I just told you.
1604140	1612220	What are the manipulations that are in the news constantly, especially lately?
1612220	1618380	Fake news, that's number one, absolutely, positively number one, fake news.
1618380	1622460	And number two is Cambridge Analytica.
1622460	1628460	And Cambridge Analytica using massive amounts of data, some of which they kind of, you could
1628460	1633860	say stole or obtained unethically anyway, and a lot of which they bought.
1633860	1642940	Cambridge Analytica funded by that Mercer billionaire fellow who is a staunch Trump supporter.
1642940	1652980	And Cambridge Analytica supposedly helped shift the vote toward Brexit in the Brexit referendum.
1652980	1657260	And Cambridge Analytica supposedly also helped to put Trump in office.
1657260	1659660	That makes the news not as much as fake news does though.
1659660	1665820	Fake news in the news all the time.
1665820	1669420	We don't study, we could easily study the impact.
1669420	1673460	We could quantify the impact of fake news stories on people.
1673460	1675820	We don't, we don't bother.
1675820	1680220	We could quantify the impact of the kinds of manipulations Cambridge Analytica was using,
1680220	1687500	which by the way is just plain old marketing stuff, because all they were doing was customizing
1687740	1692700	basically images and language and ads to get people to click.
1692700	1695820	That's exactly what marketers do, right?
1695820	1699100	You use multivariate analysis, you keep changing things.
1699100	1704940	Now they, they had access to lots of data about people, about supposedly all the voting,
1704940	1706860	all the voters in America.
1706860	1711500	They claim to have more than 5,000 data points for every single voter in the United States.
1712220	1720220	So what I'm saying is that I'm not, we're not studying that.
1720220	1725100	Why are we not studying these incredibly, because they're trivial.
1725100	1731020	By comparison to the hypotheticals that I just described, they're completely trivial.
1731020	1733740	Why is fake news trivial by comparison?
1733740	1736300	First of all, you can see fake news.
1737180	1741500	You know there's fake news in front of your eyeballs, or you know there's news anyway.
1741500	1745340	You're not sure whether it's fake enough, but you sure as heck know that there's a human element
1745340	1749420	there, because it looks like a newspaper, and so a human must have written it.
1749420	1753020	And some, and usually there's an aim of a human who wrote the article.
1753020	1755020	You can see it.
1756220	1760220	That's very different than kinds of influence, which are invisible to people.
1760220	1763180	There's extensive research on this and social psychology.
1763260	1768140	If you influence people using methods, subtle methods that they can't see,
1770380	1774060	people end up believing that they made up their own minds.
1774060	1778300	You can still shift people's opinions and actions and so on,
1778300	1782620	but if they can't see the source of influence, they end up believing that they made up their own minds.
1782620	1785260	They have no idea that they've even been influenced.
1786540	1792060	So what's the earliest, obvious example of fake news?
1792060	1800940	What's the earliest example of that kind of influence in the United States that made big news back in the late 50s, I think, a long time ago?
1806140	1809100	Exactly, subliminal stimulation, that's right.
1809100	1816140	And this made big news in the U.S. a long time ago, because supposedly a movie theater in New Jersey was,
1816460	1821900	you know, had cut in these little frames into their film saying,
1823260	1827020	go buy a soda, you know, you're thirsty, go buy a soda, go buy our popcorn.
1827580	1830860	And that got into the news and there was a big uproar.
1830860	1839100	And as a matter of fact, the association that controlled television standards at the time, they made it supposedly,
1839100	1848060	they prohibited the use of subliminal stimuli, at least on television.
1848060	1854700	Some countries ended up passing very strict laws prohibiting it in all kinds of public situations.
1854700	1857980	The UK subliminal stimulation is unlawful, period.
1858540	1860140	We never made it unlawful here.
1860140	1865500	It's still probably used, but, you know, it doesn't really have that much of an impact.
1866140	1867500	It's just scary, though.
1867500	1872620	The idea that there's some stimulus that's affecting you and you can't really see it and
1872620	1878060	it's caused you to buy a drink, but it turns out subliminal stimuli do have an impact on people,
1878060	1880060	but it's very, very, very, very small.
1881500	1886860	So, you know, if you're building a business, that's probably not where you want to put your marketing money.
1888140	1894460	But the point is, invisible stimuli that affect people, you know, they've been around for a while.
1894860	1896300	There are lots of examples.
1896300	1898060	There's a body of research on this.
1899020	1903820	But what I'm trying to tell you is that looking at those hypotheticals,
1903820	1906140	we have now moved into a very different world,
1908300	1915980	where there are, hypothetically, means of influencing people by the billions online,
1917180	1921340	invisibly, without any awareness on people's part that they're being manipulated.
1921340	1927820	And, you know, the word ethics is in the title of my talk today.
1929580	1933820	Although the use of these methods is currently perfectly legal,
1935980	1938380	not because I think anyone would say they should be legal,
1938380	1941660	but simply because the law hasn't caught up with the technologies.
1942780	1946620	But I think most of us would agree that these are unethical, at least,
1947500	1948860	even if they're legal at the moment.
1949420	1952780	I think most of us would agree that they are unethical.
1953820	1961260	So, more and more, I think we need to be thinking about the ethics of what new technology is bringing to people.
1964140	1967100	All right, let's move now, more in the realm of real.
1969420	1972460	I don't want to spend too much time on this because I could go on forever.
1972460	1974300	And by the way, what time do I need to stop?
1975260	1980940	I know it's five something, but okay, good, we're running schedule.
1980940	1985100	So, let's move a little bit toward the real.
1987500	1995580	In early 2012, in fact, it happened to be New Year's Day, come to think of it, January for 2012,
1996220	2001420	I got a bunch of emails all from Google saying that my website had been hacked
2001740	2007260	and that they were blocking access through their search engine.
2008140	2013740	So, until that day, I had never given any thought to Google at all.
2013740	2015180	I just thought it was a great search engine.
2015980	2021340	And I started to learn some things about Google that made me more concerned about the company.
2021340	2024380	For one thing, I learned that they had no customer service department,
2024380	2025900	which I thought was odd, and they still don't.
2026780	2037980	No, actually, they don't, but we can talk about it more later if you like.
2038540	2044460	But it's not like lots of other companies where you just call an 800 number and someone answers
2044460	2046220	the phone and they help you solve your problem.
2046220	2047580	They don't have anything like that.
2047580	2051580	In fact, at one point when I did get someone on the phone from Google,
2051580	2054060	she basically said, I'm really sorry I can't answer that question.
2054060	2055580	I'm really sorry I can't answer that question.
2055580	2057020	I said, can you help me at all?
2057020	2058780	She goes, no, I'm really not allowed to help you.
2059500	2065660	So, that's the closest I got to a human being who was not a bot and it was an actual person.
2065660	2066940	I don't think she was a bot anyway.
2067580	2072620	And the point is I started to learn some things about the company, which bothered me.
2072620	2077180	It only took me five or six days to get my website taken care of, cleaned up, and so on.
2077180	2080460	And it took much longer to get it through Google sensors,
2081420	2087340	in other words, to get their algorithm to okay my website again.
2087340	2089820	But all right, it was just a hack, no big deal.
2091500	2096060	However, there were a couple things about this because I've been coding since I was 13 years
2096060	2100220	old and there were a couple things about what happened that bothered me and made me want to
2100220	2101820	look more closely at this company.
2102620	2110220	One was that not only did the search engine block people, warn people away from going to my website.
2110220	2111820	I understand that, right?
2111820	2116460	Google's crawlers found malware, I get that, right?
2116460	2119980	So, their search engine should warn people away, makes perfect sense.
2121500	2128860	But also, if you tried to get to my website or any of the 20 psychological tests that are
2128860	2132540	actually based there, so through other URLs you tried to get into those tests,
2133340	2137340	using Firefox, you couldn't get there.
2137340	2138940	And that doesn't make sense.
2138940	2144940	Firefox is a product of Mozilla, which is a non-profit corporation, and I don't get that.
2144940	2148940	I don't see how Google's crawler would have anything to do with Firefox.
2149900	2153740	And then I found the same was true with Safari, which is owned by Apple.
2154780	2157740	So, there were things like this that were bugging me.
2157740	2159020	I don't want to go into details.
2159020	2163740	I just want to point out that I started to think a little bit more critically about Google as a
2163740	2172300	company. Later that year, chatting with some people about search results and search rankings,
2173660	2178380	I got interested in that, not just on Google, but on search engines in general.
2179820	2185500	We're all constantly wondering about the search algorithm that they use and how they do this
2185500	2189420	ordering and how every once in a while they change the ordering, which puts another
2190220	2194780	thousand businesses out of business, and everyone's always wondering about those things.
2196060	2203500	It turns out that by late 2012, there was a growing scientific literature looking at
2204140	2210380	the impact of search rankings, search position, in other words, on people's behavior.
2211340	2220700	This was being done primarily in the field of marketing because where you are in the search
2220700	2226620	results depends a lot on whether your business is going to succeed or fail.
2226620	2230780	If you can get up one more notch, depending on your industry, that might be worth another
2230780	2235980	million dollars in revenue. So, in fact, there was a growing literature looking at
2236060	2239660	those little notches and how they impacted people. Among other things,
2240620	2246140	eye tracking studies showed that people's eyes would go up to the top of the list,
2246140	2251580	even when you deliberately constructed lists in which superior results were down at the bottom.
2252380	2257900	In other words, people were just hung up on the top stuff. It's as if people generally believed
2258700	2261900	that what's at the top is better and what's at the top is truer.
2262380	2270140	Well, Eugene, I was there. The two books I've seen about the history of Google,
2271660	2274300	all of them and all the other articles I've seen,
2275820	2280300	missed the fact that Larry and Sergey had a classmate named Luis Gravano, who is now
2280860	2285660	a computer science professor at Columbia University. And upstairs in Route 104,
2286460	2293820	Luis assembled the most amazing quarter or two seminars. Everybody who wrote a search engine
2293820	2299500	up through the 1990s was invited to come, the guys who wrote Alta Vista, Steve Kersh, who wrote
2299500	2306700	InfoSeq, and so forth. You go down the list and Larry and Sergey sat on the side of the room
2306700	2312940	and they started giggling because Larry had developed page rank with Terry and Hector and
2313260	2320140	other people. And I mean, relevance ranking, and this information retrieval is not my area,
2320140	2325820	but I certainly used dialogue back in 1976-76. I don't want us to get too much off track because
2325820	2332460	my time is limited. The thing is this, relevance ranking has been around for decades. And you
2332460	2337900	should talk to contact Luis Gravano and find out. I can go beyond that, Eugene, because as a matter
2337900	2343980	of fact, list effects have been around for centuries. And they have been studied in detail
2343980	2351340	for at least 100 years. So it has long been known that items at the top of a list have more impact
2351340	2356540	than items in the middle. Under some conditions, items at the bottom of a list also have more impact,
2356540	2361420	their names for all these things. And all this stuff has been well studied. But when I look at
2361420	2370380	this literature, when I looked at this literature, I was finding numbers that were just off the scale.
2371580	2377580	And it wasn't until quite some, I mean, sometime much, much later that we actually figured out
2378460	2386220	why items near the top of these search results are so impactful. They're incredibly impactful.
2386220	2395580	In other words, search results produced list effects that are orders of magnitude greater
2395580	2403900	than any other list effects ever studied. Okay? Well, with luck, I'll be able to tell you why
2403900	2408940	shortly. But let me just explain what happened. I got interested in this because I saw these big
2408940	2415260	numbers and I thought, well, if people have this trust for what's at the top, could you use search
2415260	2421260	results deliberately? I asked to alter people's opinions about things, not just alter their
2421260	2427980	purchases. In other words, obviously, purchasing was the main issue in these studies. Clicks,
2428780	2434940	click throughs, conversion rates, that kind of thing. But I was asking a different question. I
2434940	2441260	was saying, if people have enormous trust for these items near the top, could we use search
2441260	2446620	results to alter people's opinions? And I thought, what kinds of opinions could we alter? Could we
2446620	2454060	alter people's voting preferences, for example? That was a question that I raised. So early 2013,
2454060	2459980	working with a former student of mine, he was working for me at the time, Ronald Robertson,
2459980	2465660	who's now getting his PhD in a network science program at Northeastern University.
2466300	2475020	We decided to test this idea by randomly assigning eligible voters to one of three groups. In one
2475020	2481500	group, they saw search results which were ordered in such a way that favored one political candidate.
2481500	2486300	In other words, if you clicked on an item near the top of that list, you'd get to a web page,
2486300	2490300	which said awesome things about that candidate or terrible things about the opposing candidate.
2490620	2498140	Some people are randomly assigned to a second group in which the opposite is the case. They're
2498140	2502220	seeing search results that favor the opposing candidate. And the third group is the control
2502220	2509740	group. They're seeing the search results mixed up. Now, I thought that using this kind of research
2509740	2516220	design that we could shift, I predicted, two to three percent of the people in these, we call them
2516220	2523340	bias groups. I figured we could shift two to three percent of them using this technique. And I thought,
2523340	2528140	okay, that's not a big number, but still a lot of elections are very close. So if you could shift
2528140	2534300	two to three percent of your undecided voters reliably using, you know, search rankings, I thought,
2534300	2540700	well, that could have an impact on very close elections. First experiment we ran, the shift was
2541420	2548380	over forty-eight percent. Second one we ran, the shift was sixty-three percent. Third one we ran,
2548380	2552860	I think it was thirty-nine percent. These were all pretty small studies. Then we did a national
2552860	2557340	study in the U.S. with more than two thousand people. Shift we got was, again, about thirty-nine
2557340	2563900	percent. We also discovered very quickly that we could mask what we were doing. We could hide it.
2564860	2569340	Even in the first experiment we ran, where people were seeing highly, highly, highly
2569340	2576620	biased search results, a quarter of the people in the study seemed to have no awareness. I'm sorry,
2576620	2580780	a quarter of the people in the study, only a quarter of the people in the study, seemed to be
2580780	2586220	aware of the bias. Three quarters seemed to have no awareness. We found that just by mixing things
2586220	2592620	up a little bit, okay, so you, so I've got, you know, Trump, Trump, Trump, Clinton, Trump,
2592620	2597820	Trump, Trump, Trump, just by mixing things up a little bit, adding in a mask, we could easily
2597820	2603420	boost the number of people who were unaware that they were seeing bias search rankings to one hundred
2603420	2610380	percent. There was very, very simple manipulation to do and a very simple manipulation to hide.
2611740	2618060	Producing outrageous shifts in voting preferences as high as eighty percent
2619340	2624940	in one of the demographic groups that we looked at. We were, we're running experience that were
2624940	2629420	all speaking of hypothetical and we're moving toward the real, though we're not there yet,
2629420	2635980	because this is still all kind of science-y stuff, right? But then we did a big experiment in India
2635980	2641580	during their 2014 election there with more than 2,000 voters from throughout India,
2641580	2646460	right smack in the middle of the campaign. In fact, even after the voting process started,
2646460	2650540	we were still bringing in people who hadn't voted yet, because in India they have so many voters
2650540	2654140	that in fact they, they stretch out the voting process over a period of several weeks and we
2654140	2660940	were still conducting our study. And there in India where we got real voters and they're being
2660940	2665260	bombarded with information and they have high familiarity with the candidates, I was saying,
2665260	2670860	I think we'll still get an effect, but I think it'll be really small, one to two percent or zero.
2671500	2676060	I thought, I thought maybe the, the reality of the campaigning and the pressure and all that
2676060	2684140	would just overwhelm what search results could do. What we learned was that search,
2684140	2689740	bias search results could easily shift voting preferences by more than 20 percent with real
2689740	2696620	voters and over 60 percent in some demographic groups. In other words, here was a kind of
2696620	2703820	manipulation, oh by the way, 99.5 percent of the people in that study showed no awareness whatsoever
2703820	2711340	that they were seeing bias search rankings, 99.5 percent. So this is very different than fake news
2711340	2715020	and it's very different than even what Cambridge Analytica was doing, you know, where they're,
2715020	2720300	they're just coming up with good clickbait for people, because no one can see this occurring,
2720300	2725340	no one's aware that they're being manipulated at all, and yet we're manipulating them. We're
2725340	2733100	manipulating them sufficiently to, we figured out looking at, in fact, election statistics from
2733100	2741020	around the world, we were manipulating people sufficiently so that biased search rankings,
2741020	2748220	could, we calculated, be currently determining the outcomes of upwards of 25 percent of the
2748220	2752700	national elections in the world. That's mainly because a lot of elections are very close.
2755820	2760380	And depending on the country and depending on how, what the internet penetration is in that
2760380	2765020	country, what the percentages of undecided voters and some other things, you know, you can
2765020	2772300	actually calculate fairly precisely whether or not search rankings can be used to flip an election.
2774140	2777820	In some of our experiments, we were using web pages and search results from the
2778460	2784860	2010 election for Prime Minister of Australia. I mentioned this because the winner of that
2784860	2791500	election won by a margin of 0.24 percent. There are a lot of very close elections.
2792460	2798780	And even in our last election, Hillary won the popular vote by approximately 2.9 million votes,
2799580	2806460	but what percentage is that of the total vote of close to 140 million people?
2808940	2813900	Can you do that in your head? The point is, that's a pretty close election. A lot of elections are
2813900	2820700	close. Search rankings are very powerful. And so that's what we began to learn. So we since have
2820700	2828620	done many, many experiments on search results and their power to influence people. We've done
2828620	2835340	experiments showing that you can influence people's attitudes about things like abortion, fracking,
2836300	2846620	homosexuality. We've learned that if you let people do multiple searches and which they're seeing
2847500	2852620	mainly different sets of web pages, but in each case they're seeing a biased set of web pages,
2853420	2860780	that with each additional search, there's an increase in that shift that we call manipulation
2860780	2867340	power. So people who conduct multiple searches on the same topic, if they're seeing search results
2867340	2872380	that are biased in a particular way, that has more and more of an impact on them over time.
2874460	2881500	We've also done some cool work on the role that operant conditioning seems to play
2882060	2885740	in this effect, which we eventually called SEEM, the search engine manipulation effect.
2886220	2890700	I won't go into the details of the experiment, but basically what we figured out, what we confirmed,
2891820	2900060	is that the reason why search rankings have such an enormous effect, again much larger than most
2900060	2906460	list effects, is because there's something very peculiar about the way we use search engines.
2908460	2914860	The vast majority of searches that we conduct are of a routine sort. We're looking for facts.
2916700	2924140	Right? I go, tell me about where Rick Lozanski went to school.
2926540	2933180	Or I say, what is the capital of Massachusetts? And sure enough, over and over and over again,
2933180	2939580	the correct answer appears where? Right at the top. And of course, these days, it even gets up into
2939580	2944380	the so-called featured snippet or the Google box. So you don't even have to look at the search results.
2944380	2948620	We're actually doing experiments on the impact of the featured snippet. Right now,
2948620	2953580	they're running literally right now. The point is, in the experiment we did in operant conditioning,
2953580	2960780	what we figured out was that the reason why people believe that what's at the top is truer and better
2960780	2966940	is because there's this daily regimen of operant conditioning. We're like rats in a skinner box
2966940	2970540	in which we're learning over and over and over again what's at the top is better,
2970540	2975340	what's at the top is truer. So when the day comes, when we want to put in something a little
2975340	2980220	different, like something we're really unsure about, like what's the best vacation spot in the
2980220	2986140	United States? So there's no clear answer, right? It doesn't matter. We're going to trust because
2986140	2990940	of all that conditioning that never stops. We're going to trust what's at the top more than we
2990940	2995980	trust things that are down lower. It's really that simple. In the experiment we did, we actually
2996060	3002300	manipulated people's trust level to show that if you interfere with that operant conditioning,
3002300	3006860	in fact, people don't trust what's at the top so much and they start looking lower and they
3006860	3014700	start going to subsequent pages of search results. So we also have done a series of experiments that
3014700	3021980	will be published soon and looking at the way SIEM can be suppressed with various kinds of alerts
3022620	3027020	and there are people interested in this because, for example, you may be familiar with the project
3027020	3035180	called FindX. Anyone know FindX? It's a new project based in Europe. In fact, the man who
3035180	3039980	started this is now working with me and some other people and something I'll mention at the end of
3039980	3051660	the talk. FindX is meant to be a search engine which is transparent and fair in which the
3051660	3062620	rules for ordering are made public and in which even users have a say actually in determining
3062620	3067660	what the algorithm is doing and how it's computing search results. Another thing they are thinking
3067660	3077660	about is adding alerts. If you add alerts saying this set of results appears to favor Hillary
3077660	3084700	Clinton, that has an impact on the way people treat what's in the search results. You can also
3084700	3090540	add alerts to particular items in the list. So we have work coming out on that.
3092060	3098540	So we're moving gradually toward the real here. We also started last year studying
3098540	3102700	search suggestions. Now, again, we're studying things that are invisible. We're not interested in
3102700	3112860	fake news. Yeah. Yes, I have a question. Yes. Is there a group of people who are somewhat immune
3112860	3120140	to these effects you describe? In other words, everybody is affected by those?
3120940	3125980	Well, in the studies we've done in the United States, we've never found any, well, I mean,
3125980	3130060	there's going to be individuals who are immune, of course, but we have never found a demographic
3130060	3135340	group that's immune. I'm talking individualism. Is there an individual biologically possible
3135340	3139980	who would not be affected by it? Well, of course, anyone who is very, by nature, very skeptical
3139980	3145740	or anyone who's had a bad experience with Google or something, of course, individuals.
3145740	3151340	So there is a group of people who are immune. Because I don't think so. We are all the same.
3151340	3158940	Well, as I say, in the United States, we have never found a demographic group that was immune.
3159660	3165660	Never. Right. So it's just a question of how much people are swayed. Is it this much or is it this
3165660	3170380	much? But I mean, it's crazy. Everybody's a little bit. And if everybody's swayed a little bit,
3170380	3177500	how can we talk about facts and fake news? If the content producers and content consumers
3177500	3184060	also are influenced by other people's opinion, then everything is relative. Well, that's Abraham Lincoln.
3184140	3187740	You can fool all the people some of the time. Some of the people all the time.
3187740	3193260	No, no, I'm not talking about this. Let me just point out that we know how to suppress
3194060	3198780	these effects. So in the paper that we have coming out, we actually show how you can suppress the
3198780	3203420	in our control groups, we suppress the effect completely, 100%. So we know how to suppress
3204860	3212060	this kind of effect. If you mix things up, then people don't shift one way or the other.
3212380	3215980	And there are other ways to suppress the effect in varying degrees. So we're learning about that.
3215980	3220540	Let me just shift over to search suggestions quickly, because we now have learned a lot.
3220540	3226540	Okay, room, nice to meet you. We've now learned a lot about search suggestions and why we're
3226540	3230460	seeing the search suggestions we're seeing, why generally speaking, we're only seeing four.
3231340	3238540	We actually, I mean, really have learned so much about this that again, it's a whole scary area.
3238540	3245100	So we've named a new effect called the search suggestion effect, because search suggestions
3245100	3251100	can in fact be used easily to manipulate people's opinions, attitudes, beliefs, behavior, voting
3251100	3257740	preferences. We've even figured out where the number four comes from. And the key there to put
3257740	3264300	it just briefly, the key to that number four has to do with what happens with negative search
3264300	3269020	suggestions. You've heard of negativity bias, I'm sure, because people study it in a half a
3269020	3274220	dozen different fields. Sometimes it's called the cockroach and the salad phenomenon. When
3274220	3277500	there's something negative, and then when a stimulus is negative, like a cockroach in your
3277500	3282700	salad, your attention is drawn to it and it ruins the whole salad, and you send the salad back.
3284620	3290300	Now, if I put a piece of chocolate into a plate of sewage, that does not upgrade the plate of
3290300	3296380	sewage at all. So something particular about negatives, well, it turns out our new experiments
3296380	3301340	show that there's something very special about those negatives in this list of search suggestions too.
3301340	3309180	And guess who knows that? Google. So in June, July and August of last year, we documented the fact,
3309180	3315260	this was partly based on a video that had gone viral in June, then in fact, Google was systematically
3315260	3321100	suppressing negative search suggestions for Hillary Clinton. Now, when we went public with that,
3321100	3325420	and others went public with that, so it wasn't just us, but when we went public with our findings,
3325420	3330540	Google flipped the switch. They literally just turned off the manipulation, just like that.
3331420	3335980	And from that day on, you could start to see negatives when you did searches for anything
3335980	3340460	related to Hillary Clinton. But June, July and August, it was virtually impossible to get any
3340540	3345660	negative search suggestions. We've learned that when there's a negative in that list of four,
3346300	3354540	it can draw 10 to 15 times as many clicks. And the more undecided someone is on an issue,
3354540	3361100	the more clicks the negative draws. So one of the simplest ways to manipulate people's opinions
3361100	3366140	invisibly is through the differential suppression of negative search suggestions. That is to say,
3366540	3370700	suppress the negative search suggestions for the position I'm supporting,
3370700	3374940	and I allow negative search suggestions to appear for the other position, the one I'm not supporting.
3376300	3382380	And what we're now doing is quantifying what that does to people's searches and what that does
3382380	3386620	to people's opinions and voting preferences. We're doing that right now. But we even figured
3386620	3393980	out that number four, because it turns out that if there is a negative in the list,
3394940	3399340	and I, as I add more and more alternatives to that negative, because I want people clicking
3399340	3402700	on that negative, believe me, I'm allowing that negative to be there because I want people to
3402700	3407900	click on it and I know it attracts attention and I know it attracts clicks. But the more alternatives
3407900	3415420	I add, the fewer people will click on the negative. It dilutes the impact of the negativity bias.
3416060	3422860	Right? Now, by the same token, I want to keep adding more items to my list. Why?
3424780	3430700	Because I don't want people finishing their own search term. I don't want them doing that.
3430700	3436380	So on the one hand, I want my list to be long. On the other hand, I want it to be short. Well,
3436380	3442780	it turns out those two distributions overlap perfectly with one optimal value.
3445420	3453100	Guess what it is? Four. Four is the magic number. We didn't know if this was going to come out of
3453100	3460700	our data, but it popped right out. Okay. So we're learning more and more about how these things
3460700	3471180	work. Now we get to the cereal. You know, all this stuff, even the experiments, even the experiment
3471260	3477580	in India, in some sense, it's all hypothetical, isn't it? Because you don't know what people
3477580	3482460	are really seeing. I mean, to see what people are really seeing, I'd have to creep up behind
3482460	3487900	Eugene like this and I'd have to look over his shoulder and go, are there any European nations
3487900	3494780	that we're practically unaffected by the two world wars? Now I know because I crept up on him
3494780	3500060	and I looked over his shoulder and wouldn't we have to do that to really see what people are seeing?
3501900	3509260	Okay. So I tell the story, the full story. It's coming out in a couple months in a piece called
3509260	3514780	Haming Big Tech and I recommend it to you highly because I'm told by friends who read it that it
3514780	3521100	reads like a spy novel. And I tell the story of what this crazy thing we did. So now we're in the
3521100	3531980	surreal realm here. Starting in late 2015, early 2016, we set up a Nielsen-type network of field
3531980	3542220	agents around the country. All these people were recruited in a clandestine manner. We took incredible
3542220	3548460	steps to make sure that they could not be identified, which Nielsen does too. Nielsen does the same
3548460	3553020	with the families they used to rate television shows. They've been doing that since the 1950s.
3554380	3561660	So we recruited these people. We developed a custom add-on for both Firefox and Chrome
3562620	3567980	that all of these are field agents installed on their computers. That gave us control over
3568700	3573020	information that we would be collecting automatically when they conducted searches.
3573580	3579580	In particular, searches using any one of 500 different election-related search terms
3579580	3583580	that we control. We control that list. Sometimes we could collect whatever we wanted.
3584140	3587900	But as it happens, we were only collecting information about election-related searches.
3588860	3595500	And we got our first data starting to come in on May 19, 2016, and we kept going up to the
3595500	3603100	election. And as we worked out the kinks in our system, the rate of data flow increased. And
3603100	3615260	ultimately, we preserved 13,207 searches on Google Bing and Yahoo and the 98,044 web pages to which
3615260	3624140	the search results linked. And of course, we knew what search positions the links were appearing in.
3625500	3631180	So in other words, we had the ability to determine. We were not looking over the shoulders of our field
3631180	3636300	agents as they were conducting searches and preserving their actual search results and
3636300	3640700	preserving the web pages to which all 10 search results on the first page linked.
3641980	3647500	So this had never been done before, apparently. And it was tremendously exciting. And it was very
3647500	3653580	nerve-wracking. We then used crowdsourcing to determine whether the web pages were
3654380	3660940	favored Hillary Clinton or Donald Trump. And we concluded that, in fact, for roughly,
3660940	3670140	for merely six months before the election, Google's search rankings were biased in favor of Hillary
3670140	3678140	Clinton. We also determined that the bias in Google's search results was larger than the bias in
3678140	3683740	Yahoo's search results, which was much, much smaller. And of course, the fact that they
3683740	3687500	have a bias shouldn't be too surprising since they're pulling almost all of their
3687500	3694460	search results from Google. And then what about Bing? Well, it turns out we couldn't use our Bing
3694460	3700380	data. There were a bunch of data we couldn't use. Why? Because some of our field agents were commuting,
3701100	3707900	we're communicating with us using Gmail. We deliberately recruited a few field agents
3707900	3713500	deliberately who used Gmail because we knew that if Google took an interest in what we were doing,
3714300	3718780	it would be very easy for them to identify those people. So that was kind of our control group.
3719740	3722300	So that brings me finally to the one slide I'm going to show you.
3722300	3727340	Now, is this real or surreal?
3738620	3745900	This is showing you over a 25-day period before November 8 and including November 8.
3745900	3751820	This is showing you the bias, if any of those points above the line is showing bias
3752620	3757180	or favoritism for Hillary Clinton, this pro-Clinton, below the line that would be
3758380	3764540	web pages favored Donald Trump. So this is showing you 25 days before the election
3765340	3772220	and you see there are pretty clear favoritism for Hillary Clinton in search results. And by the
3772220	3776140	way, it said nothing to do with the search terms because if you look at the search terms people
3776140	3783180	were using, the search terms actually slightly favored Donald Trump. So this was not an effective
3783180	3793260	search terms. This is an algorithmic effect. And now this is what's cool. These are all
3794140	3801500	non-Gmail Google users. What about that control group we had? What about the Google users who
3801500	3805020	were also communicating with us during all these months using Gmail?
3814860	3824700	Now, to my eye, those graphs look different. Statistically, those numbers are dramatically
3824780	3832060	different. At the point 001 level, they're dramatically different. You can draw whatever
3832060	3838220	conclusions you like regarding why we got this difference, but what this tells me is when you're
3838220	3843340	going to conduct a study like this, you should be very cautious about how you conduct the study.
3844620	3850540	What we realized at the end of all this was not so much that our numbers really were very
3850540	3858620	important. What we realized is that we have the ability now to look over people's shoulders as
3858620	3866460	they're looking at Tinder and they're swiping as they're using Facebook, as they're looking at
3866460	3871660	Facebook feeds, as they're looking at not just search rankings, as they're looking at search
3871660	3878860	suggestions, you can use the same add-on technology that we successfully developed here in this project
3879580	3884700	to look over people's shoulders around the world. You can scale up what we did
3885660	3892620	and set it up in country after country after country. When we realized that this was possible,
3892620	3898380	what we really had here, a way of keeping tabs on these big tech companies and what they're
3898380	3904620	showing people, then I called up some people I knew, including one of these guys over here
3905420	3910860	and a guy some of you know named Dennis Allison and some other nice folks, Jake Shapiro from
3910860	3917900	Princeton University and Martin Moore from King's College London and on and on and on. The list now
3917900	3923260	is growing and growing and growing of people at major institutions who've become part of a group
3924220	3928860	that is working to set up a new organization. It's called the Sunlight Society.
3929820	3940780	And the Sunlight Society will serve as a kind of monitor of technologies that are being developed
3941340	3947740	which could in fact influence people's behaviors, people's opinions, people's votes,
3947740	3954860	people's purchases, perhaps without them even knowing. This kind of system, whether we do it
3954860	3961180	successfully or not, it needs to exist. There's definitely a need for this at this point because
3961740	3966700	everything that I've been saying up until just a few minutes ago was all hypothetical, wasn't it?
3967820	3973900	This is not so hypothetical anymore. This is much closer to real or even surreal. This is weird.
3977100	3984300	You can use this technology to look at demographic effects, to look at what these
3984300	3994380	companies are showing people, how individualized these stimuli are that people are being subjected
3994380	4001820	to. You could look at anything that people are seeing on their screens, have it instantly transmitted
4002940	4009340	to servers which is what we did, have your servers do an analysis and we're now working on
4009340	4014860	automating the analysis of bias ratings, for example. And all of that stuff can be automated.
4014860	4022380	You can train algorithms to evaluate text in much the same way that humans evaluate text
4022380	4026780	and those algorithms are getting better and better and better. And in fact, both Google and Facebook
4026780	4032140	right now are using algorithms like that to try to identify fake news stories. The point is you
4032140	4041980	could be collecting data in real time on many different platforms, analyzing the data in real
4041980	4050460	time and finding the problems maybe before they get out of hand. You could share these findings
4050460	4056060	as appropriate with the media. You could share them as appropriate with law enforcement agencies.
4057020	4065260	Courts, I had a conference call a couple of weeks ago with the two top investigators in the three
4065260	4072940	antitrust actions that the EU has brought against Google. They're very interested in this kind of
4072940	4079500	technology because the evidence they have to support some of the claims that they have made
4079500	4088620	against Google is actually pretty weak compared to what we have. So I think we're there. We've
4088620	4095900	gone from hypothetical to a little bit more real, but still somewhat hypothetical, to a lot more real
4095900	4104220	and then the possibility of really finally being able to make companies like this accountable to
4104220	4111100	the public. If this interests you and you'd like to help, you know, join this effort,
4111900	4116700	again, whether we do it successfully or not, we know it's going to happen and we know it needs
4116700	4131740	to happen. And that's my story. Thank you. So, given the two examples that gave of Facebook and
4131740	4140940	Google and given the powerful effect, I'm sorry, and Tinder and given that this effect is so powerful
4142060	4150620	as you state, why did you do it from when? Well, he won because of the peculiarities of the electoral
4150620	4158300	college and the American people didn't choose him. I mean, Hillary Clinton won by almost 2.9
4158300	4164620	million votes. She won the popular vote. Maybe in California with nothing about it. That's irrelevant.
4164620	4171020	I mean, if we had a direct vote kind of system, which they have in many countries, then she would
4171020	4180300	have won. You know, the analysis of that election, and people are going to be analyzing that election
4180300	4186860	for a hundred years, and we all know there's a long list of reasons why Donald Trump won.
4188300	4192620	But I will tell you, and I guess it's on the record because I'm being recorded, I will tell you
4192620	4197660	that then I'm a friend of some people in the Trump family, and then I was in touch with them on
4197660	4204540	election eve, and that there came a certain moment in time, I won't tell you the exact time,
4205580	4213660	where I got a text, and the text said, we are all shocked here. And when this person said we,
4214460	4226700	this person meant we. See what I'm saying? Not only did they not believe they were going to win,
4227580	4234940	I personally don't think based on, again, my personal knowledge of some of the people involved,
4234940	4240540	I personally do not believe he had any intention of becoming president, which is why he is still
4240540	4250700	looking very much like a deer in the headlights. He was trying to increase his celebrity status,
4250700	4256940	he was trying to lay the foundations for setting up a huge media network, and he was putting all
4256940	4263260	those pieces in place. And this is not what these people had in mind, which is why they're kind of
4263260	4270940	all scarring around, and there's just complete chaos, and you know. So he won because of what
4270940	4275580	historians will tell us 50 years from now, that's why he won. See what I'm saying?
4278220	4283740	Well, you know, the question is, again, hypotheticals, right? The question is,
4283740	4289180	was Google using, and was Facebook using these manipulations to the full extent that they had,
4289180	4294060	that was possible? And we have reason to believe that they were not. I mean, we know, for example,
4294060	4299340	that Google turned off that negative search suggestion manipulation in early September.
4299340	4305340	We know they just turned it right off, like that. So I think that these companies that,
4305340	4312300	you know, behind the scenes, or in some cases very openly were, you know, wanted Hillary Clinton
4312300	4318380	to win and were supporting her in all kinds of ways. I mean, Dustin Moskowitz, am I pronouncing
4318940	4324540	that correct? One of the co-founders of Facebook, he donated just a couple of months before the
4324540	4331100	election. He donated $25 million to the Democrats. So, you know, these companies, and the people who
4331100	4334780	worked for these companies, they were very strong supporters of the Democrats. But I think, number
4334780	4342700	one, I think that they held back a little bit on the manipulations. And number two, I think that
4342780	4350140	they were just overconfident. I think they were overconfident. The polls said consistently that
4350140	4354300	she had it in the bag. And I think they just got overconfident. And maybe we're being a little
4354300	4358220	cautious. And I don't think that they used all the tools that they had available to them.
4360380	4367740	But that's why you have to have a monitoring system in place. Because the historians are
4367740	4373580	just going to be speculating. We don't need to speculate. We can monitor. We can track. They
4373580	4383260	track us. We can track them. It's that simple. And then this won't be speculation. Why is this
4383260	4387820	all speculation? There was an article that came out in The Guardian, which has done a very, very
4387820	4392860	good series on high tech and very skeptical about, you know, what high tech is serving up to the
4392860	4399740	world. It was a very good piece in early December talking about the Brexit issue. And this was by
4399740	4406540	Carol Cadwalader. I've spoken with a number of times. Very, very, very, very smart journalists
4406540	4413020	and good investigative journalists. And in this article, she laments the fact that what
4414380	4419740	people in the UK were seeing on their computer screens, you know, back in June when the Brexit
4420060	4426860	book occurred, that it was all lost, that we can't know what Cambridge Analytica was showing people.
4426860	4431180	Because it's all lost. It's all ephemeral, right? Most of what we see on screens is ephemeral,
4431820	4436460	especially when we're generating search results. That's ephemeral. It exists for a couple seconds,
4436460	4441660	and it has an impact on us, and it goes away, and it's gone. So what she was saying was,
4441660	4445980	if only, if only we could go back in time and see what people were seeing on their screens.
4446780	4454940	Okay? We could do that. I have an appointment on Friday with folks from the Internet Archive,
4454940	4461660	which is not far from here, which is, you know, Brewster Kale's project. And, you know, they've
4461660	4465340	been following what we've been doing since almost the beginning, because at some point the Internet
4465340	4471660	Archive is going to post our database for everyone to, you know, pour through. But I mean, that's
4471740	4476620	what we need. We need organizations like the Internet Archive working with people who develop
4476620	4482620	monitoring systems, an organization like the Sunlight Society that not only develops and scales
4482620	4487500	up these systems, but that looks around the world for other people developing similar systems,
4487500	4492940	and kind of coordinates, coordinates the efforts. And this, that we don't have to speculate. We'll
4492940	4499420	know what's happening. And this could result in something wonderful. It could actually get
4500060	4505900	some of these manipulations to disappear. It's possible if a good monitoring system were in
4505900	4510460	place that some of these companies who are doing some of these crazy things, think about swipe the
4510460	4517740	boat, will stop because they'll realize this is being recorded. This is being recorded.
4521900	4527020	Yes, what is your name? Brad. Now, are you one of the famous or infamous actual students?
4527740	4532300	I guess I am. Wow. Well, I will say I have not been here for that many times.
4533420	4538300	Well, thank you for coming, person. Thank you. Yeah, this is definitely quite interesting. I'm
4538300	4541900	like curious to see more of this data and the need for it is like, it's definitely very clear.
4542540	4546540	It's less clear to me like how we can actually be interpreting these results
4547660	4552300	right now anyway, without having more of it. Just because like for one difference right off the
4552300	4557020	bat with like gmail users, you're always logged into your Google search results. Like that's
4557020	4561180	a completely different set of personalization and development going on at Google, right to be
4561180	4565820	showing you that you know which results those are. Right. So that's just like already a difference
4565820	4569500	there. And then just thinking about how do you measure I think the hardest part there more
4569500	4572940	fundamentally is like how do you measure what is a bias set of search results. Let's see, Brad,
4572940	4577020	I can see this in your head. I can see the gears because you're not just asking these questions,
4577020	4579900	you're answering them in your head at the same time that you're asking the questions.
4580620	4588220	Deny that. The first. Deny it. Yes or no. I think I have two different questions. One I have like
4588220	4593260	one imagine to answer for which is the personalization components. Yeah. Which is,
4593260	4597900	but I think I could be dissuaded at that one. Right. But the second one. It's easy, right?
4599500	4604780	You know how to do that, right? Oh, how to change the data. Oh, I'm saying though that you know how
4604860	4610140	to track the personalization. It's very easy. Yeah. I'm just saying it's harder to get. I can
4610140	4613980	also imagine coming up with algorithms that seem very neutral and are tempted to be very
4613980	4620460	neutral that would give identical effects. And that's why I think it's very interesting to study
4620460	4625340	more and see if you can like tease out and but B, I'm not ready to like leave to the conclusion
4625340	4630460	that Google is manipulating its results to influencer. I don't know what they're doing,
4630460	4637100	but I know how to track what they're doing. And I'm learning more and more about how to
4637100	4642060	automate the analysis of the data that we're collecting. And I know how to set up systems
4642060	4647900	like this. So I think one thing I am curious about is how do you figure out what's a bias
4647900	4654380	set of search results? Well, we just thought we that's a very good question. This question
4654380	4660300	comes up all the time. And I sometimes I regret using the term bias because it's a loaded term.
4660300	4665820	And I'm not using it in a loaded way, believe it or not. I realized bias sounds like prejudice
4665820	4671820	and things like that. And I don't I'm using it in the way psychology researchers use the term,
4671820	4679100	which is that that favors one perspective over another. And so when we have when we do use
4679100	4685740	crowdsourcing to rate, you know, whether a page is pro one candidate or another, we just we give
4685740	4691180	them an 11 point scale goes from five to zero to five, right? And here's candidate a and here's
4691180	4696460	candidate b. And we say, read the webpage and just tell us whether it favored and on this 11
4696460	4701260	point scale, whether it favors candidate a or candidate b. So bias is an unfortunate term.
4702460	4707020	The point is you can you can take terms like that operationalize them until you're satisfied.
4708060	4712940	So I don't think that's what the problem is. I don't even think that bias per se is the problem.
4712940	4719340	I think we're talking about a whole new world that is emerging. And this is I'm actually working
4719340	4724700	on a book on this subject trying to think ahead 10 or 20 years. Wow, is that impossible these days?
4725340	4731420	But I think a whole new world is emerging in which effects of the sort that I've been telling you
4731420	4736700	about are simply going to multiply. And so, you know, it's going to be a game of catch up. And
4736700	4741180	that's one of the reasons why you have to have monitoring systems in place, because even if you
4741180	4747420	don't understand how those apparently neutral kinds of stimuli, right, but that you mentioned,
4747420	4750620	even if you don't understand how that's being used to manipulate people,
4750620	4754380	well, if you at least if you capture the information, you can go back and analyze the
4754380	4759180	crap out of it. And maybe you can figure it out. And I think that's the world that we're headed
4759180	4765420	toward one in which new technologies, I mean, imagine how Google Home could be used to manipulate
4765900	4771820	or the new product that Apple just announced. But Apple has never had this motive, by the way,
4772460	4778780	because Apple has a different business model. Apple actually sells products. Microsoft actually
4778780	4782700	sells products. I realize more and more companies are moving in the direction of Google's business
4782700	4788540	model. But Google doesn't sell any products. Not really. More than 90% of their revenue is
4788540	4796780	still advertising revenue. They're they use cool looking data collection platforms to collect data,
4797340	4801820	and then they leverage that data to make this this year, they're going to make over they'll
4801820	4809740	have revenues of over $100 billion. So Google is still the place you have to watch. And
4810620	4818620	secondarily, Facebook, Google currently controls five out of the six billion platform
4820300	4824220	applications in the world. And there's only one left. And that's Facebook controls the other one,
4824220	4829180	which is social media. But what I'm saying is Google, you have to keep an eye on Google,
4829180	4832460	but they're going to be other companies. It's not just Google. They're going to be other companies
4832460	4837900	doing other things that have never been done before. You know, these these effects, we're
4837900	4843660	now studying four effects that have never existed before in human history, completely
4843660	4851580	unprecedented, almost entirely invisible, with which produce enormous shifts in people's thinking.
4853020	4856220	Our if we're if we have identified and are studying four,
4859020	4860460	what do you think, could there be five?
4860780	4869340	How many are there? I don't know. You know, we've we've found and are studying four.
4870060	4873900	So that does that mean they're actually 10? Does that mean they're actually 100?
4873900	4879180	I don't know. But I do know this. Next year, there'll be more of those kinds of effects than
4879180	4889180	there are this year. Hal? So let me let me shift to the advertising world. A product is to try and
4889260	4894060	give us some of the emotions associated with politics. There's an incredible battle between
4896940	4901980	search engines trying to give people what they want, and advertisers trying to
4903100	4908060	bump their stuff up there, whether they deserve it or not. Yes, that's right. So how do you decide
4908060	4915740	what's fair in this war? And in terms of if I translate this back into politics, how am I going
4915740	4923100	to decide what's fair in that world? You know, is your crowdsource evaluation? I can tell you
4923100	4929340	what I am, and I can tell you what I'm not. Okay, I'll start with the knots. I am not a lawyer.
4929340	4937180	I'm not a public policymaker. I am not a thought leader. I am not. You know what I am? I'm a
4937180	4941980	researcher. I'm a really, really good researcher. The more I've done research over the years,
4941980	4948300	the more I realized I'm good. I know how to figure these things out, and I love doing it. And that's
4949340	4953980	about as far as I could go. No, I think that's cool though. I think that's pretty far. I think
4953980	4962540	you're on a wonderful project. Okay. The one thing you haven't mentioned yet is keeping track of the
4962540	4969980	stuff that wasn't displayed so you know when you try and go back a year and evaluate something
4969980	4976940	as to whether, you know, the stuff that you got was the right sample. Well, that's why we want to
4976940	4983420	scale up the kind of thing that we did. If you scale it up large enough, you can keep track of
4983420	4988220	all kinds of stuff. Look what Brewster Cale's organization does. You've heard of the Wayback
4988220	4996620	Machine? I mean, the Internet Archive takes snapshots of the entire Internet pretty much.
4997340	5000860	I don't know if they do the dark net, but I mean, at least, you know, the Internet we most of us
5000860	5006860	can see. So, I mean, if you have the resources, you could capture lots of different, and what is it
5006860	5012540	we want to capture ephemeral information? That's what we want to capture. That's what is normally
5012540	5018780	completely lost. Fantastic. Yeah. And what I'm saying is we can capture ephemeral information.
5018780	5024940	Hold on to it. Analyze it now or analyze it later.
