{"text": " Hi, I'm delighted to be here with my old friend and collaborator, Professor Chris Manning. Chris has a very long and impressive bio, but just briefly, he is Professor of Computer Science at Stanford University and also the Director of the Stanford AI Lab. And he also has a distinction of being the most highly cited researcher in NLP or natural language processing. So really good to be here with you, Chris. Oh, good to get a chance to chat, Andrew. So, you know, we've known each other, collaborated for many years. And one interesting part of your background, I always thought, was that even though today you're a distinguished researcher in machine learning and NLP, you actually started off in a very different area. Your PhD, if I'm not correctly, was in linguistics, and you were studying the syntax of language. So how did you go from studying syntax to being an NLP researcher? So I can certainly tell you about that. But I should also point out that, you know, I'm still actually a professor of linguistics as well. I have a joint appointment at Stanford. And, you know, once in a blue moon, not very often, I do actually still teach some real linguistics as well as computer-involved natural language processing. So, you know, so starting out, I was very interested in human languages and how they work, how people understand them, how they are required. So I had this sort of appeal, I saw this appeal in human languages. But that equally led me to think about ideas that we now very much think about as machine learning or computational ideas. So two of the central ideas in human language, how do little children acquire human language? And for adults, well, we're just talking to each other now, and we pretty much understand each other. And, you know, that's actually an amazing thing how we managed to do that. So what kind of processing allows that? And so that early on got me interested in looking at machine learning. In fact, even before I'd made it to grad school, I'd started, you know, baby steps and learning machine learning coming off of those interests. Yeah, in fact, all human languages learn. You know, we had learned at some point in our lives to speak English. We'd grown up in a different place. We would learn a totally different language. So it's amazing to think how humans do that and now maybe machines learn language too. But so, so just, you know, tell us more about your journey. So you had a PhD in linguistics, and then, and then how did you? So there's some stuff before that as well. So, I mean, you know, when I was an undergrad, well, officially, I actually did three majors. This was in Australia, one in math, one in computer science and one in linguistics. Now, people get a slightly exaggerated sense of what that means if you're in an American context, because, you know, it'd be, I think, impossible to complete three majors or not undergrad at Stanford. But, you know, actually, where I was as an undergrad doing, I did an arts degree. So I could do whatever I wanted, like linguistics. You had to complete two majors to complete the arts degree. So, you know, it's sort of more like double majoring, maybe in US terms. You probably don't know this about me, but at Carnegie Mellon, I actually was a triple major. Math CS was once in the statistics and economics. That's great, we're both fellow triple majors. Yeah, so anyway, I did have background in interest in doing things with computer science. And so my interests were kind of mixed. And I mean, actually, you know, when I applied to grad schools, I mean, one of the places I applied to was Carnegie Mellon, because they were strong in computational linguistics, you know, and if I'd gone there, I would have been enrolled as a CS student. But I ended up at Stanford as a linguistics student, because at that time there wasn't any natural language processing in the CS department. But, you know, I was still interested in pursuing ideas in natural language processing. But at that point in the early 90s, things were just starting to change. But the bulk of natural language processing was rule-based, logical, declarative systems. But it was also in those years, at the beginning of the 90s, when there first started to be lots of human language material, text and speech available digitally. So this was really actually just before the World Wide Web exploded. But there had already started to be things like legal materials and newspaper articles and parliamentary handsaws, where you could at last get your hands on millions of words of human language. And it just seemed really clear that there had to be exciting things that you could do by working empirically from lots of human language. And that's what really sort of got me involved in a new kind of natural language processing that then led into my subsequent career. It sounds like your career was initially more linguistics. And if the rise of data and machine learning and empirical methods, it shifted to what NLP and machine learning and NLP? Yeah, I mean, it absolutely certainly shifted. And I've certainly sort of shifted much more to doing both natural language processing and machine learning models. But to some extent, the balances varied. But I've sort of been with that as of while. You know, actually, there's an undergrad. For my undergrad on this thesis, it was sort of learning the forms of words. So how you can, which became a famous problem of sort of learning past tense of English verbs in the early connectionist literature. And I was trying to sort of learn paradigms of forms of verbs. And I was learning rules for the different forms using the C4.5 decision tree learning algorithm, if you remember that. Yes. Right. Good times. Yeah. And it's surprisingly non-intuitive, right? How going from present tense to past tense. From, I don't know, run to run and all the other weird special cases can be. Yeah. Hey, so we talked a bunch about NLP, natural language processing. So for some of the learners, pick up machine learning for the first time, can you say, what is NLP? Sure, absolutely. Yes, NLP stands for natural language processing. Another word this term that's sometimes used for that is computational linguistics. It's the same thing. I mean, natural language processing is actually a weird term, right? So it means that we're doing things with human languages. So you have to have the conception that you're enough of a computer scientist that when you say language, you think in your brain programming language, and therefore you need to say natural language to mean that you're talking about the languages that human beings use. So overall natural language processing is doing anything intelligent with human languages. So in one sense, that breaks down into understanding human languages, producing human languages, acquiring human languages. Though people also often think about it in terms of different applications. And so then you might think about things like machine translation or doing question answering or generating advertising copy or summarization. There are so many different tasks that people work on with particular goals in mind where you do things with human language. And there's a lot of natural language processing because so much of what the world works on our human world is dealt with and transmitted in terms of human language material. So because of all of these applications or even a web stage, most of us use NLP many, many times. Yeah, you're right. In some sense, the biggest application of natural language is web search, right? That's really the big one. I mean, traditionally, it was a kind of a simple one, right? But in the good old days, it was, you know, there were various waiting factors and so on, but it was mainly sort of matching keywords than your search terms and then some factors about the quality of the page. It didn't really feel like language understanding, but that's really been changing over the years. So these days, you'll often, if you ask a question to a search engine, it'll give you, you know, an answer box where it has extracted a piece of text and puts what it thinks is the answer in bold or color or something like that, which is then this task of question answering. And then it's really a natural language understanding task. Yeah, yeah. And I feel like in addition to web searches, maybe the big one, you know, when we're going to a online shopping website or a movie website and typing in what we want and doing a website search on a much smaller website than, you know, the big search engines, that also increasingly uses sophisticated NLP algorithms and it's also creating quite a lot of value. Maybe to you is not, you know, the real NLP, but it still seems very valuable. I agree. It's very valuable. And there are, you know, lots of interesting problems in any e-commerce website with search, very difficult problems, actually, when people describe the kind of goods they want and you need to be trying to match it to products that are available. That isn't an easy problem at all, it turns out. Yeah, that's true. Yeah. So over the last couple of decades, NLP has gone through a major shift from more of the rule-based techniques that you alluded to just now to using really machine learning much more pervasively. And so you were one of the people at, you know, leading parts of that charge and seeing every step of the way of creating some of the steps as it happened. Can you say a bit about that process and what you saw? Sure, absolutely. Yeah. So when I started off as an undergrad and grad student, really most of natural language processing was done by hand-built systems, which variously used rules and inference procedures to sort of try and build up paths and an understanding of a piece of text. What's an example of a rule or an inference system? So, you know, a rule could be part of the structure of human language, like English sentence normally consists of a subject noun phrase followed by a verb and an object noun phrase, and that gives you some ideas to how to understand the meaning of the sentence. But it might also be saying something about how to interpret a word, so that a lot of words in English are very ambiguous. But if you have something like the word star and it's in the context of a movie, then it's probably referring to a human being on this astronomical object. And in those days, people tried to deal with things like that using rules of that sort. That doesn't seem very likely to work to us these days, but, you know, once upon a time that was pretty standard. And so it was only when lots of digital text and speech started to become available that it really seemed like there was this different way that instead we could start calculating statistics over human language material and building machine learning models. And so that was the first thing that I got into in the sort of mid to late 1990s. And so, you know, the first area where I started doing lots of research and publishing papers and getting well known is building what in the early days we often called statistical natural language processing. But it later merged into in general probabilistic approaches to artificial intelligence and machine learning. And that sort of took us through to approximately 2010, let's say. And that's roughly when the new interest in deep learning using large artificial neural networks started to take off. For my interest in that, I really have you to thank Andrew, because at this stage, Andrew was still full time at Stanford, and he was in the office next door to me, and he was really excited about the new things that were happening in the area of deep learning. I guess anyone who walked into his office, he'd tell them, oh, it's so exciting what's happening now, and neural networks should start looking at that. And so, you know, that was really the impetus that got me pretty early on involved in looking at things in neural networks. I had actually seen a bit of it before. So while I was a grad student here, actually Dave Ruhmelhardt was at Stanford and Psych, and I'd taken his neural networks class. And so, you know, I'd seen some of that, but it hadn't actually really been what I'd gotten into for my own research. So around. I didn't know that. Thank you. And then we wound up, you know, supervising some students together. Yeah, absolutely. But I'd love to hear the rise of also deep learning in NLP. What are the GCs? Yeah. So starting about 2010, yeah, me, students started to do the first papers in deep learning aimed at NLP conferences. You know, it's always hard when you're trying to do something new. We had exactly the same experiences that people 15 or so years earlier had had when they started trying to do statistical NLP of when there's an established way of doing things. It's really hard to push out new ideas. So really some of our first papers were rejected from conferences and instead appeared at machine learning conferences or deep learning workshops. But very quickly that started to change and people got super interested in neural network ideas. But I sort of feel like the neural network period which started effectively about 2010 itself divides in two. Because for the first period, let's basically say it's till 2018, we showed a lot of success at building neural networks for all sorts of tasks. We built them for syntactic parsing and sentiment analysis and what else to question answering. But it was sort of like we were doing the same thing that we used to do with other kinds of machine learning models, except we now had a better machine learning model. And we were sort of instead of training up a logistic regression or a support vector machine, we were still doing the same kind of sentiment analysis task but now we're doing it with a neural network. So I think looking back now in some sense, the bigger change came around 2018 because that was when the idea of well we could just start with a large amount of human language material and build large self-supervised models. So that was models then like BERT and GPT and successor models to that. And they could just sort of acquire from word prediction over a huge amount of text this amazing knowledge of human languages. I think really probably that's going to be viewed in retrospect as the bigger kind of cut point where the way things were done really changed. Yeah, I think there is that trend for the large language models, learning from math and the mouse and data. I think even to lead up to that, there was one of your research papers that really slightly blew my mind, which is a glove paper. So because with word embeddings where you learn a vector of numbers to represent a word using a neural network, that was quite mind blowing for me. And then the glove work that you did really cleaned up the math, made it so much simpler. And then I remember reading I said, oh, that's all there is to it. And then you can learn these really surprisingly detailed representations of the computer learns real nuances of what words mean. Absolutely. Yeah, so I should give a little bit of credit to others. Other people also worked on some similar ideas, including Renan Colbert and Jace Weston and Tom Ostermeek and colleagues at Google. But the glove word vectors is one of the very prominent systems of word vectors. So these word vectors already did, yeah, you're right, illustrate this idea of using self-supervised learning that we just took massive amounts of text, and then we could build these models that knew an enormous amount about the meaning of words. I mean, it's still something I sort of show people every year in the first lecture of my NLP class, because it's something simple, but it actually just works so surprisingly well. You can do this sort of simple modeling of trying to predict a word given the words in the context. And simply by sort of running the math of learning to do those predictions well, you learn all these things about word meaning, and you can do these really nice patterns of similar word meaning or analogies of something like, you know, pencil is to drawing as paint brush is to, and it'll say painting, right? That it's sort of already showing just a lot of successful learning. So that was the precursor to what then got developed to the next stage with things like Burton GPT, where it wasn't just meanings of individual words, but meanings of whole pieces of text and context. Yeah, so I thought it was amazing that you can, you know, take a small neural network or some model and then give it lots of English sentences or some of the language and hide the word, ask it to predict what is the word that I just hit, and that allows it to learn these analogies and these very deep, what you would think are really deep things behind the meaning of the words. And then, you know, 2018, maybe there's other infection point. What happened after that? Yeah, so I mean, in 2018, that was the point in which, well, sort of really two things happened. One thing is that people, well, really in 2017, had developed this new neural architecture, which was much more scalable onto modern parallel GPUs, and so that was the transformer architecture. The second part of it, though, was, you know, maybe people rediscovered because it was using the same trick as the glove model, that if you have the task of say, of just predicting a word given a context, either a context on both sides of it or the preceding words, that that just turns out to be an amazing learning task. And that surprises a lot of people, and a lot of the time you see discussions where people say disparaging things of, you know, this is nothing interesting is happening, all it's doing is statistics to predict which word is most likely to come after the preceding words. And I think the really interesting thing is that that's true, but it's not true. I mean, because, yes, what the task is, is you're predicting the next word given preceding words. But the really interesting thing is, if you want to do that task really as well as possible, then it actually helps to understand the whole of the rest of the sentence and know who's doing what to who in what's in the sentence. But more than that, it also helps to understand the world. Because if your, your text is going something along the lines of, you know, the currency used in Fiji is that, well, you need to have some world knowledge to know what the right answer to that is. And so good models at doing this learn both to follow the structure of sentences and their meaning and to know facts about the world also that they can predict. And therefore this turns into what sometimes referred to as an AI complete task, right? That you really need, there's nothing that can't actually be useful in answering this, what word comes next sense, right? You know, you can be in the World Cup semifinals, the teams are, and you need to know something about soccer to be giving the right answer. AI completes this funny concept, or is this idea that you can solve this one problem, you can solve, you know, everything in AI or kind of make an analogy to NP complete problems from the theory of computing. What do you think? Do you think predicting the next word is AI complete? I have very mixed feelings about that myself. I shall, I shall go ahead and say, I don't think it's true. So just what do you think? I think it's not quite true because I think there are other kinds of things that human beings manage to work out. You know, there are human beings that have clever insights in mathematics, or there are human beings who are looking at something that's much more you know, three dimensional, real world puzzle of sort of figuring out how to do something mechanical or something like that, and that's just not a language problem. But on the other hand, I mean, I think language gets closer to universality than some people think as well, because, you know, we live in this 3D world and operate in it with our bodies and our feelings and other creatures and artifacts around it, and you could think, well, not much of that is in language at all. But actually, just about all of this stuff, we think about, we talk about, we write about it in language, we can describe the positions of things relative to each other in language. So a surprising amount of the other parts of the world are seen in reflection in language, and therefore you're learning about all of them too when you learn about language use. You learn about, you know, one aspect of a lot of things, even if things like, how do you ride a bicycle? You don't really learn how to ride a bicycle, but you learn some aspects of what it involves, that you need to balance and you have to have your feet on the pedals and push them and all of that kind of things, yeah. And so with this trend in NLP, the large language models has been very exciting for the last several years. What are your thoughts on where all this will go? Well, I mean, yeah, so it's just been amazingly successful and exciting, right? So we haven't really explained all the details, right? So there's a first stage of learning these large language models where the task is just to predict the next word and you do that billions of times over a very large piece of text, and behold, you get this large neural network, which is just a really useful artifact for all sorts of natural language processing tasks. But then you still actually have to do something with it if you want to do a particular task, whether that's question answering or summarization or detecting toxic content in social media or something like that. And at that point, there's a choice of things that you could do with it. The traditional answer was then you had a particular task, like say, detecting toxic comments in social media, and you'd take some supervised data for that, and then you'd fine tune the language model to answer that classification task. But you were enormously helped by having this base of this large self-supervised model because it meant that the model had enormous knowledge of language and it could generalize very quickly. So unlike the sort of the standard old days of supervised learning where it was kind of, well, if you give me 10,000 labeled examples, I might be able to produce a halfway decent model for you, but if you give me 50,000 labeled examples, it'll be a lot better. It's sort of turned into this world of, well, if you give me 100 labeled examples and I'm fine tuning a large language model, I'll be able to do great, better than I would be able to do with the 50,000 examples in the old world. Some of the more recent, exciting works now even going beyond that, it's now, well, maybe you don't actually have to fine tune the model at all. So people have done a lot of work using methods sometimes referred to as prompting or instruction where you can simply in natural language, perhaps with examples, perhaps with explicit instructions, just tell the model what you want it to do and it does it. Which, even as someone who's been working in natural language processing for 30 years, it actually just blows my mind how well this works. I guess I wasn't a decade ago thinking that in now we'd be able to just tell the model, I want you to summarize this piece of text here and there will be then summarize it. I think that's incredible. So we're in this very exciting time where a lot of new natural language capabilities are unfolding. I think there's just no doubt at all for the next couple of years the future of that is extremely bright as people work out different things and different ways to do things and people start to apply in different application areas, the kind of capabilities that have been unlocked with recent technological developments. There's always a question in technology is to sort of whether the curve keeps on heading steeply upwards or whether there's then some new things we have to discover how to do. It's been going up for quite a while. So hopefully extrapolation is always dangerous but we'll see. You mentioned writing prompts. It's still the NLP system, the large language model, what you want and it seems to magically do it. I'm curious, do you think prompt engineering is the path of the future where actually when I write these prompts I sometimes find it works miraculously and sometimes it's frustrating. The process of re-wording my instructions to tweak the wording to get it just right to generate the result I want. So do you think prompt engineering is the way of the future or do you think it's an intermediate hack until someone invents a better way to control the outputs of these systems? I think it's both. I think it will be the way of the future but I also think at the moment people are doing a lot of hacking around and re-wording to try and get things to work better. With any luck with a few more years of development that will start to go away. One way to think about the difference is in comparison to the kind of voice assistance or virtual assistance that are available on phones and speaker devices like Amazon Alexa these days. I think all of us have had the experience that present those devices aren't always great but if you know the right way to word things it will do something but if you use the wrong wording it won't. The difference with human beings is by and large you don't have to think about that. You can say what you want and it doesn't matter what word you choose. The other human being assuming it's someone who knows the same language etc. Well understand you and do what you want. I think and would hope that we'll start to see the same kind of progression with these models that at the moment fiddling around with the particular wording you use can make a very big difference to how well it works but hopefully in a few years time that just won't be true. You'll be able to use different wordings and it'll still work but the basic idea that we're moving into this age where actually human language will be able to be used as an instruction language to tell your computer what to do so instead of having to use menus and radio buttons and things like that or writing Python code instead of either of those things that you'll be able to say what you want in the computer or do it. I think that age is opening up in front of us that will continue to build and that will be hugely transformative. It feels like come a long ways but only much more to come and much more to go. Absolutely. In the development of NLP technology there's one thing I want to ask you and I suspect you and I may have different perspectives on this but in the last couple of decades the trend has been to rely less on rule based engineering and more on machine learning on data sometimes lots of data. Looking to the future where do you think that mix of hand coded constraints or other constraints explicit constraints versus you know let's get a neural network and throw lots of data at it. Where do you think that balance will fall? I think that there's no doubt that using learning from data is the way forward and what we're going to continue to do but I think there's still a space for models that have more structure, more inductive bias that have some kind of basis of exploiting the nature of language. So in recent years the model that's been enormously successful is the transform in your network and the transform in your network is essentially this huge association machine so it'll just suck associations from anywhere and look at two words and figure out which words release and which other words for all words. Yes so you use everything to predict anything and do it over and over again and you'll get anything you want and you know that's been incredibly, incredibly successful but it's been incredibly successful in the domain where you have humongous, humongous amounts of data right so that these transformer models for these large language models are now being trained on tens of billions of words of text. When I started off in statistical natural language processing and some of the traditional linguists used to complain about the fact that I was collecting statistics from 30 million words of newswire and building a predictive model and thought that was just not what linguistics was about. I felt I had a perfectly good answer which is that a human kid as their learning language they're exposed to actually well more than 30 million words of data but you know that kind of amount of data so you know the kind of amount of data we were using were perfectly reasonable amounts of data to be using to be you know not exactly trying to model human language acquisition but to be thinking about how we can learn about language from lots of data. But you know these modern transformers are now you know using already at least two orders of magnitude more data and you know most people think the way to get things to the next level is to use more still and make it three orders of magnitude and you know in one sense that scaling up strategy has been hugely effective so you know I don't blame anybody for saying let's make another order magnitude bigger and see what amazing things we can do but it also shows that human learning is just way way better in being able to extract a lot more information out of a quite limited amount of data and at that point you can have various hypotheses but I think it's reasonable to assume that human learning is somewhat structured towards the structure of the world and things that sees in the world and that allows it to learn more quickly from less data. Alright I'll move you on that. I think better learning algorithms, our current machine learning algorithms are much less efficient or makes much less efficient use of data and so there's way more data than any you know child and then I think whether the improved learning algorithms will be from linguistic like rules or whether it'll just be engineers engineering much more efficient versions of the transform or whatever comes after it. That will be traditional. I don't think it'll be by people explicitly putting traditional linguistic rules into the system. I don't think that's the way forward. On the other hand I mean you know I think what we're starting to see is models like these transformer models are actually discovering the structure of language themselves right so you know the broad facts of you know human language that you know English has the subject before the verb and the object afterwards whereas you know in Japanese that the verb at the end of the sentence and the subject and object are normally in that order before it that could be in the other order you know actually transformer models are learning these facts you can interrogate them and see that even though they were never explicitly told about subjects and objects that they know these notions so I think they you know they're discovering a lot else as well about language use and context and the meanings and senses of words and what is and isn't you know unpleasant language but part of what they're learning is the same kind of structure that linguists have laid out as the sort of structure of different human languages. So does it over many decades linguists discover certain things and by training on billions of words transformers are discovering the same things that linguists discovered in human language that's that's that's cool. So all this is really exciting progress in NLP driven by machine learning and by other things. To someone entering the field entering machine learning or AI or NLP there's just a lot going on. What advice would you have for someone wanting to break into machine learning? Yeah well it's a great time to break in. I think there's just no doubt at all that we're still in the early stages of seeing the impact of this new approach where effectively software computer science is being reinvented in on the basis of much more use of machine learning and the various other things that come away from that and then more generally across industries there are just lots of opportunities for more automation making more use of you know interpretation of human language material for me or in other areas like vision and robotics or the same kinds of things. So lots of possibilities. So you know at that point there's lots to do obviously and you want to get some kind of good foundation right so knowing some of the core technical methods of machine learning understanding ideas of how to build models from data look at losses do training diagnose errors all of these core things I mean that's definitely useful for natural language processing in particular some of those skills are completely relevant but then there are particular kinds of models that are commonly used including the transformer that we've talked about a lot today you definitely should know about transformers and indeed they're increasingly being used in every other part of machine learning as well for vision bioinformatics even robotics is now using transformers but beyond that I think it's also useful to learn something about human language and the nature of the problems that involves because I mean even though people aren't directly going to be encoding rules of human language into their computing system a sensitivity to sort of what kind of things happen in language and what to look out for and what you might want to model that's still a useful skill to have. And then in terms of learning the foundations learning about these concepts you had entered AI from a linguistic background and we now see people from you know all walks of life wanting to to start doing work in AI what are your thoughts on the preparation one should have or any thoughts on how to start from something other than computer science or AI so there are lots of places you can come from and vector across in different ways and we're seeing tons of people doing that that they're people who started off in different areas whether you know it was chemistry physics or even much further in field and people you know history whatever have started to look at machine learning I mean I think there are sort of two levels of answer there I mean one level of answer is you know one of the amazing transformations is that there's now these very good software packages for doing things with neural network models I mean this these software is really easy to use you don't actually need to understand a lot of highly technical stuff you've got need to have some kind of high-level conception about what is the idea of machine learning and how do I go about training a model and what should I look at in the numbers that are being printed out to see if it's working right but you know you don't actually have to have a higher degree to be able to build these models I mean and indeed what we're seeing is you know lots of high school students are getting into doing this because it's actually something that if you have some basic computer skills and a bit of programming you can pick up and do it's just way more accessible than lots of stuff that preceded a weather in AI or outside of AI and other areas you know like operating systems or security but you know if you want to get to a deeper level than that and actually want to understand more of what's going on I think you can't really get there if you don't have a certain mathematics foundation like at the end of the day that deep learning is based on calculus and you need to be optimizing functions and if you sort of don't have any background in that I think that sort of ends up as a wall at some point so you know. The math for machine learning and data science it does come in handy for some of the work we're going to do. Yeah so I think at some level if you're at the major in history or you know non-mathematical parts of psychology I actually have a good friend who yeah he you know learnt calculus in grad school because he was a psychologist and he'd never done it before and decided he wanted to start learning about these new kinds of models and decided it wasn't too late to be able to go and take a Cal course and so he did right so you know you do need to know some of that stuff but for lots of people if they've seen some of that before even if you're kind of rusty I think you can kind of get back in the zone and it doesn't really matter that you haven't you know done AI as an undergrad or machine learnings and things like that that you can really start to learn how to build these models and do things and you know really that's my own story right that despite the fact that they let me sit in the school of engineering at Stanford these days you know my background isn't as an engineer you know my PhDs and linguistics but you know I've sort of largely vectored across from having some knowledge of mathematics and linguistics and knowing some programming into sort of getting much more into building AI models. I was curious about something do you think the improved libraries and abstractions that are now available like coding frameworks like TensorFlow or PyTorch do you think that reduces the need to understand calculus because boy it's been it's been a while since I had to actually take a derivative in order to even implement or create a new neural network architecture because of automatic differentiation. Yeah I mean absolutely I mean so in the early days when we were doing things sort of 2010 to 2015 right for every model we built we were working out the derivatives by hand and then you know writing some code and whatever it was you know sometimes it was Python but sometimes it might have been Java or C to calculate these derivatives and checking that we got them right and so on where you know these days you actually don't need to know any of that to build deep learning models I mean this is actually something I think about been thinking about even with respect to my own natural language processing with deep learning class that I teach you know at the beginning we do still go through doing you know matrix calculus and making sure people know about Jacobians and things like that so that they understand what's being done in back propagation deep learning but you know there's sort of this sense in which that means that we just give them hell for two weeks you know sort of like boot camp or something to make them suffer and then we say oh but you do the rest of the class with PyTorch and they sort of never have to know any of that again right I you know there's always a question of how deep you want to go in technical foundations right you can keep on going right like does a computer scientist in the 2020s need to understand you know electronics and transistors or what happens in you know CPU well you know it's complicated I mean in various ways it is helpful to know some of that stuff I mean you know I know Andrew you were one of the pioneers and getting machine learning onto GPUs and well you know that sort of means you had to have some sense that there's this new hardware out there and it has some attributes of parallelism that means there's likely to be able to do something exciting so you know it is useful to have some broader knowledge and understanding and you know sometimes something breaks and if you have some deep knowledge you can understand why it broke but there's another sense in which you know most people have to take some things on trust and you can do most of what you want to do in neural network modeling these days without knowing calculus at all yeah that's a great point I feel like sometimes the reliability of the abstraction determines how often you need to go in to fix something that's broken so I actually my understanding of quantum physics is very weak I barely understand it so you could argue I don't understand how computers work because transistors are built in quantum physics but fortunately you know if something went wrong with transistors I've never had to go in to try to fix it so they're a bit hard to fix I think and so I think I think well another example you know the sort function their libraries are sort things and sometimes they actually don't work right swap in the memory or whatever and that's when if you really understand how the sort function works you can go in and fix it but then sometimes if we have abstractions libraries APIs are reliable enough then that is nice to those abstractions then diminishes them to understand some of the things that happen so it's an exciting world feels like you know we have giants building on the shoulders of giants and and all of these things are becoming more complex and more exciting every every every month yeah absolutely so thanks Chris that was really um interesting and inspiring and and I hope that to everyone watching this hearing Chris's own journey um to become a computer scientist and to become a leading maybe the leading NLP computer scientists as well as all of this exciting work having an NLP right now I hope that inspires you to jump into the sphere and take a go at it there's just a lot more work to be done collectively by our community than still so I think the more of us are working on this the better off the world will be so thanks a lot Chris it was really great having you thanks a lot Andrew it's been fun chatting", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.02, "text": " Hi, I'm delighted to be here with my old friend and collaborator, Professor Chris Manning.", "tokens": [50364, 2421, 11, 286, 478, 18783, 281, 312, 510, 365, 452, 1331, 1277, 293, 5091, 1639, 11, 8419, 6688, 2458, 773, 13, 51015], "temperature": 0.0, "avg_logprob": -0.20769884419995685, "compression_ratio": 1.5, "no_speech_prob": 0.0046037170104682446}, {"id": 1, "seek": 0, "start": 13.02, "end": 17.080000000000002, "text": " Chris has a very long and impressive bio, but just briefly, he is Professor of Computer", "tokens": [51015, 6688, 575, 257, 588, 938, 293, 8992, 12198, 11, 457, 445, 10515, 11, 415, 307, 8419, 295, 22289, 51218], "temperature": 0.0, "avg_logprob": -0.20769884419995685, "compression_ratio": 1.5, "no_speech_prob": 0.0046037170104682446}, {"id": 2, "seek": 0, "start": 17.080000000000002, "end": 22.080000000000002, "text": " Science at Stanford University and also the Director of the Stanford AI Lab.", "tokens": [51218, 8976, 412, 20374, 3535, 293, 611, 264, 7680, 295, 264, 20374, 7318, 10137, 13, 51468], "temperature": 0.0, "avg_logprob": -0.20769884419995685, "compression_ratio": 1.5, "no_speech_prob": 0.0046037170104682446}, {"id": 3, "seek": 0, "start": 22.080000000000002, "end": 27.52, "text": " And he also has a distinction of being the most highly cited researcher in NLP or natural", "tokens": [51468, 400, 415, 611, 575, 257, 16844, 295, 885, 264, 881, 5405, 30134, 21751, 294, 426, 45196, 420, 3303, 51740], "temperature": 0.0, "avg_logprob": -0.20769884419995685, "compression_ratio": 1.5, "no_speech_prob": 0.0046037170104682446}, {"id": 4, "seek": 0, "start": 27.52, "end": 28.52, "text": " language processing.", "tokens": [51740, 2856, 9007, 13, 51790], "temperature": 0.0, "avg_logprob": -0.20769884419995685, "compression_ratio": 1.5, "no_speech_prob": 0.0046037170104682446}, {"id": 5, "seek": 2852, "start": 28.919999999999998, "end": 30.88, "text": " So really good to be here with you, Chris.", "tokens": [50384, 407, 534, 665, 281, 312, 510, 365, 291, 11, 6688, 13, 50482], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 6, "seek": 2852, "start": 30.88, "end": 32.56, "text": " Oh, good to get a chance to chat, Andrew.", "tokens": [50482, 876, 11, 665, 281, 483, 257, 2931, 281, 5081, 11, 10110, 13, 50566], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 7, "seek": 2852, "start": 33.64, "end": 37.480000000000004, "text": " So, you know, we've known each other, collaborated for many years.", "tokens": [50620, 407, 11, 291, 458, 11, 321, 600, 2570, 1184, 661, 11, 42463, 337, 867, 924, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 8, "seek": 2852, "start": 37.480000000000004, "end": 41.879999999999995, "text": " And one interesting part of your background, I always thought, was that even though today", "tokens": [50812, 400, 472, 1880, 644, 295, 428, 3678, 11, 286, 1009, 1194, 11, 390, 300, 754, 1673, 965, 51032], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 9, "seek": 2852, "start": 41.879999999999995, "end": 46.36, "text": " you're a distinguished researcher in machine learning and NLP, you actually started off", "tokens": [51032, 291, 434, 257, 21702, 21751, 294, 3479, 2539, 293, 426, 45196, 11, 291, 767, 1409, 766, 51256], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 10, "seek": 2852, "start": 46.36, "end": 48.08, "text": " in a very different area.", "tokens": [51256, 294, 257, 588, 819, 1859, 13, 51342], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 11, "seek": 2852, "start": 48.08, "end": 55.120000000000005, "text": " Your PhD, if I'm not correctly, was in linguistics, and you were studying the syntax of language.", "tokens": [51342, 2260, 14476, 11, 498, 286, 478, 406, 8944, 11, 390, 294, 21766, 6006, 11, 293, 291, 645, 7601, 264, 28431, 295, 2856, 13, 51694], "temperature": 0.0, "avg_logprob": -0.2768592834472656, "compression_ratio": 1.5674740484429066, "no_speech_prob": 0.0010808395454660058}, {"id": 12, "seek": 5512, "start": 55.559999999999995, "end": 60.08, "text": " So how did you go from studying syntax to being an NLP researcher?", "tokens": [50386, 407, 577, 630, 291, 352, 490, 7601, 28431, 281, 885, 364, 426, 45196, 21751, 30, 50612], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 13, "seek": 5512, "start": 60.68, "end": 62.48, "text": " So I can certainly tell you about that.", "tokens": [50642, 407, 286, 393, 3297, 980, 291, 466, 300, 13, 50732], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 14, "seek": 5512, "start": 62.48, "end": 66.72, "text": " But I should also point out that, you know, I'm still actually a professor of linguistics", "tokens": [50732, 583, 286, 820, 611, 935, 484, 300, 11, 291, 458, 11, 286, 478, 920, 767, 257, 8304, 295, 21766, 6006, 50944], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 15, "seek": 5512, "start": 66.72, "end": 67.12, "text": " as well.", "tokens": [50944, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 16, "seek": 5512, "start": 67.12, "end": 68.92, "text": " I have a joint appointment at Stanford.", "tokens": [50964, 286, 362, 257, 7225, 13653, 412, 20374, 13, 51054], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 17, "seek": 5512, "start": 69.88, "end": 74.16, "text": " And, you know, once in a blue moon, not very often, I do actually still teach some real", "tokens": [51102, 400, 11, 291, 458, 11, 1564, 294, 257, 3344, 7135, 11, 406, 588, 2049, 11, 286, 360, 767, 920, 2924, 512, 957, 51316], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 18, "seek": 5512, "start": 74.16, "end": 78.32, "text": " linguistics as well as computer-involved natural language processing.", "tokens": [51316, 21766, 6006, 382, 731, 382, 3820, 12, 259, 9646, 937, 3303, 2856, 9007, 13, 51524], "temperature": 0.0, "avg_logprob": -0.17033366622211776, "compression_ratio": 1.612, "no_speech_prob": 0.0029784294310957193}, {"id": 19, "seek": 7832, "start": 79.16, "end": 87.03999999999999, "text": " So, you know, so starting out, I was very interested in human languages and how they", "tokens": [50406, 407, 11, 291, 458, 11, 370, 2891, 484, 11, 286, 390, 588, 3102, 294, 1952, 8650, 293, 577, 436, 50800], "temperature": 0.0, "avg_logprob": -0.14245174407958985, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0018381025874987245}, {"id": 20, "seek": 7832, "start": 87.03999999999999, "end": 92.24, "text": " work, how people understand them, how they are required.", "tokens": [50800, 589, 11, 577, 561, 1223, 552, 11, 577, 436, 366, 4739, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14245174407958985, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0018381025874987245}, {"id": 21, "seek": 7832, "start": 92.24, "end": 97.96, "text": " So I had this sort of appeal, I saw this appeal in human languages.", "tokens": [51060, 407, 286, 632, 341, 1333, 295, 13668, 11, 286, 1866, 341, 13668, 294, 1952, 8650, 13, 51346], "temperature": 0.0, "avg_logprob": -0.14245174407958985, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0018381025874987245}, {"id": 22, "seek": 7832, "start": 98.56, "end": 106.67999999999999, "text": " But that equally led me to think about ideas that we now very much think about as machine", "tokens": [51376, 583, 300, 12309, 4684, 385, 281, 519, 466, 3487, 300, 321, 586, 588, 709, 519, 466, 382, 3479, 51782], "temperature": 0.0, "avg_logprob": -0.14245174407958985, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0018381025874987245}, {"id": 23, "seek": 10668, "start": 106.68, "end": 108.96000000000001, "text": " learning or computational ideas.", "tokens": [50364, 2539, 420, 28270, 3487, 13, 50478], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 24, "seek": 10668, "start": 108.96000000000001, "end": 117.08000000000001, "text": " So two of the central ideas in human language, how do little children acquire human language?", "tokens": [50478, 407, 732, 295, 264, 5777, 3487, 294, 1952, 2856, 11, 577, 360, 707, 2227, 20001, 1952, 2856, 30, 50884], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 25, "seek": 10668, "start": 117.08000000000001, "end": 121.92, "text": " And for adults, well, we're just talking to each other now, and we pretty much understand", "tokens": [50884, 400, 337, 8865, 11, 731, 11, 321, 434, 445, 1417, 281, 1184, 661, 586, 11, 293, 321, 1238, 709, 1223, 51126], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 26, "seek": 10668, "start": 121.92, "end": 122.64000000000001, "text": " each other.", "tokens": [51126, 1184, 661, 13, 51162], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 27, "seek": 10668, "start": 122.64000000000001, "end": 126.04, "text": " And, you know, that's actually an amazing thing how we managed to do that.", "tokens": [51162, 400, 11, 291, 458, 11, 300, 311, 767, 364, 2243, 551, 577, 321, 6453, 281, 360, 300, 13, 51332], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 28, "seek": 10668, "start": 126.04, "end": 128.20000000000002, "text": " So what kind of processing allows that?", "tokens": [51332, 407, 437, 733, 295, 9007, 4045, 300, 30, 51440], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 29, "seek": 10668, "start": 128.20000000000002, "end": 133.12, "text": " And so that early on got me interested in looking at machine learning.", "tokens": [51440, 400, 370, 300, 2440, 322, 658, 385, 3102, 294, 1237, 412, 3479, 2539, 13, 51686], "temperature": 0.0, "avg_logprob": -0.1444702989914838, "compression_ratio": 1.656, "no_speech_prob": 0.0012437055120244622}, {"id": 30, "seek": 13312, "start": 133.12, "end": 138.56, "text": " In fact, even before I'd made it to grad school, I'd started, you know, baby steps", "tokens": [50364, 682, 1186, 11, 754, 949, 286, 1116, 1027, 309, 281, 2771, 1395, 11, 286, 1116, 1409, 11, 291, 458, 11, 3186, 4439, 50636], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 31, "seek": 13312, "start": 138.56, "end": 141.24, "text": " and learning machine learning coming off of those interests.", "tokens": [50636, 293, 2539, 3479, 2539, 1348, 766, 295, 729, 8847, 13, 50770], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 32, "seek": 13312, "start": 141.68, "end": 143.96, "text": " Yeah, in fact, all human languages learn.", "tokens": [50792, 865, 11, 294, 1186, 11, 439, 1952, 8650, 1466, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 33, "seek": 13312, "start": 143.96, "end": 146.56, "text": " You know, we had learned at some point in our lives to speak English.", "tokens": [50906, 509, 458, 11, 321, 632, 3264, 412, 512, 935, 294, 527, 2909, 281, 1710, 3669, 13, 51036], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 34, "seek": 13312, "start": 146.56, "end": 148.28, "text": " We'd grown up in a different place.", "tokens": [51036, 492, 1116, 7709, 493, 294, 257, 819, 1081, 13, 51122], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 35, "seek": 13312, "start": 148.28, "end": 150.36, "text": " We would learn a totally different language.", "tokens": [51122, 492, 576, 1466, 257, 3879, 819, 2856, 13, 51226], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 36, "seek": 13312, "start": 150.36, "end": 155.88, "text": " So it's amazing to think how humans do that and now maybe machines learn language too.", "tokens": [51226, 407, 309, 311, 2243, 281, 519, 577, 6255, 360, 300, 293, 586, 1310, 8379, 1466, 2856, 886, 13, 51502], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 37, "seek": 13312, "start": 156.24, "end": 160.36, "text": " But so, so just, you know, tell us more about your journey.", "tokens": [51520, 583, 370, 11, 370, 445, 11, 291, 458, 11, 980, 505, 544, 466, 428, 4671, 13, 51726], "temperature": 0.0, "avg_logprob": -0.2025918434924028, "compression_ratio": 1.7007042253521127, "no_speech_prob": 0.003266014624387026}, {"id": 38, "seek": 16036, "start": 160.36, "end": 164.92000000000002, "text": " So you had a PhD in linguistics, and then, and then how did you?", "tokens": [50364, 407, 291, 632, 257, 14476, 294, 21766, 6006, 11, 293, 550, 11, 293, 550, 577, 630, 291, 30, 50592], "temperature": 0.0, "avg_logprob": -0.16278574896640466, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006974600837565958}, {"id": 39, "seek": 16036, "start": 165.32000000000002, "end": 167.48000000000002, "text": " So there's some stuff before that as well.", "tokens": [50612, 407, 456, 311, 512, 1507, 949, 300, 382, 731, 13, 50720], "temperature": 0.0, "avg_logprob": -0.16278574896640466, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006974600837565958}, {"id": 40, "seek": 16036, "start": 167.64000000000001, "end": 173.68, "text": " So, I mean, you know, when I was an undergrad, well, officially, I actually did three majors.", "tokens": [50728, 407, 11, 286, 914, 11, 291, 458, 11, 562, 286, 390, 364, 14295, 11, 731, 11, 12053, 11, 286, 767, 630, 1045, 31770, 13, 51030], "temperature": 0.0, "avg_logprob": -0.16278574896640466, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006974600837565958}, {"id": 41, "seek": 16036, "start": 173.68, "end": 178.8, "text": " This was in Australia, one in math, one in computer science and one in linguistics.", "tokens": [51030, 639, 390, 294, 7060, 11, 472, 294, 5221, 11, 472, 294, 3820, 3497, 293, 472, 294, 21766, 6006, 13, 51286], "temperature": 0.0, "avg_logprob": -0.16278574896640466, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006974600837565958}, {"id": 42, "seek": 16036, "start": 179.0, "end": 184.48000000000002, "text": " Now, people get a slightly exaggerated sense of what that means if you're in an American", "tokens": [51296, 823, 11, 561, 483, 257, 4748, 36196, 2020, 295, 437, 300, 1355, 498, 291, 434, 294, 364, 2665, 51570], "temperature": 0.0, "avg_logprob": -0.16278574896640466, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006974600837565958}, {"id": 43, "seek": 16036, "start": 184.48000000000002, "end": 189.0, "text": " context, because, you know, it'd be, I think, impossible to complete three majors", "tokens": [51570, 4319, 11, 570, 11, 291, 458, 11, 309, 1116, 312, 11, 286, 519, 11, 6243, 281, 3566, 1045, 31770, 51796], "temperature": 0.0, "avg_logprob": -0.16278574896640466, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0006974600837565958}, {"id": 44, "seek": 18900, "start": 189.0, "end": 191.08, "text": " or not undergrad at Stanford.", "tokens": [50364, 420, 406, 14295, 412, 20374, 13, 50468], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 45, "seek": 18900, "start": 191.2, "end": 195.84, "text": " But, you know, actually, where I was as an undergrad doing, I did an arts degree.", "tokens": [50474, 583, 11, 291, 458, 11, 767, 11, 689, 286, 390, 382, 364, 14295, 884, 11, 286, 630, 364, 8609, 4314, 13, 50706], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 46, "seek": 18900, "start": 195.84, "end": 198.04, "text": " So I could do whatever I wanted, like linguistics.", "tokens": [50706, 407, 286, 727, 360, 2035, 286, 1415, 11, 411, 21766, 6006, 13, 50816], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 47, "seek": 18900, "start": 198.2, "end": 201.72, "text": " You had to complete two majors to complete the arts degree.", "tokens": [50824, 509, 632, 281, 3566, 732, 31770, 281, 3566, 264, 8609, 4314, 13, 51000], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 48, "seek": 18900, "start": 201.72, "end": 205.4, "text": " So, you know, it's sort of more like double majoring, maybe in US terms.", "tokens": [51000, 407, 11, 291, 458, 11, 309, 311, 1333, 295, 544, 411, 3834, 2563, 278, 11, 1310, 294, 2546, 2115, 13, 51184], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 49, "seek": 18900, "start": 206.28, "end": 210.16, "text": " You probably don't know this about me, but at Carnegie Mellon, I actually was a triple major.", "tokens": [51228, 509, 1391, 500, 380, 458, 341, 466, 385, 11, 457, 412, 47301, 376, 898, 266, 11, 286, 767, 390, 257, 15508, 2563, 13, 51422], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 50, "seek": 18900, "start": 210.76, "end": 213.56, "text": " Math CS was once in the statistics and economics.", "tokens": [51452, 15776, 9460, 390, 1564, 294, 264, 12523, 293, 14564, 13, 51592], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 51, "seek": 18900, "start": 214.6, "end": 216.56, "text": " That's great, we're both fellow triple majors.", "tokens": [51644, 663, 311, 869, 11, 321, 434, 1293, 7177, 15508, 31770, 13, 51742], "temperature": 0.0, "avg_logprob": -0.24833509975806214, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0013440354960039258}, {"id": 52, "seek": 21656, "start": 217.04, "end": 223.08, "text": " Yeah, so anyway, I did have background in interest in doing things with computer science.", "tokens": [50388, 865, 11, 370, 4033, 11, 286, 630, 362, 3678, 294, 1179, 294, 884, 721, 365, 3820, 3497, 13, 50690], "temperature": 0.0, "avg_logprob": -0.13808595641585422, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.3537970264442265e-05}, {"id": 53, "seek": 21656, "start": 224.04, "end": 227.36, "text": " And so my interests were kind of mixed.", "tokens": [50738, 400, 370, 452, 8847, 645, 733, 295, 7467, 13, 50904], "temperature": 0.0, "avg_logprob": -0.13808595641585422, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.3537970264442265e-05}, {"id": 54, "seek": 21656, "start": 227.36, "end": 231.08, "text": " And I mean, actually, you know, when I applied to grad schools, I mean, one of the places", "tokens": [50904, 400, 286, 914, 11, 767, 11, 291, 458, 11, 562, 286, 6456, 281, 2771, 4656, 11, 286, 914, 11, 472, 295, 264, 3190, 51090], "temperature": 0.0, "avg_logprob": -0.13808595641585422, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.3537970264442265e-05}, {"id": 55, "seek": 21656, "start": 231.08, "end": 235.84, "text": " I applied to was Carnegie Mellon, because they were strong in computational linguistics,", "tokens": [51090, 286, 6456, 281, 390, 47301, 376, 898, 266, 11, 570, 436, 645, 2068, 294, 28270, 21766, 6006, 11, 51328], "temperature": 0.0, "avg_logprob": -0.13808595641585422, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.3537970264442265e-05}, {"id": 56, "seek": 21656, "start": 235.84, "end": 239.76, "text": " you know, and if I'd gone there, I would have been enrolled as a CS student.", "tokens": [51328, 291, 458, 11, 293, 498, 286, 1116, 2780, 456, 11, 286, 576, 362, 668, 25896, 382, 257, 9460, 3107, 13, 51524], "temperature": 0.0, "avg_logprob": -0.13808595641585422, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.3537970264442265e-05}, {"id": 57, "seek": 21656, "start": 240.2, "end": 244.72, "text": " But I ended up at Stanford as a linguistics student, because at that time there wasn't", "tokens": [51546, 583, 286, 4590, 493, 412, 20374, 382, 257, 21766, 6006, 3107, 11, 570, 412, 300, 565, 456, 2067, 380, 51772], "temperature": 0.0, "avg_logprob": -0.13808595641585422, "compression_ratio": 1.710144927536232, "no_speech_prob": 2.3537970264442265e-05}, {"id": 58, "seek": 24472, "start": 244.72, "end": 248.48, "text": " any natural language processing in the CS department.", "tokens": [50364, 604, 3303, 2856, 9007, 294, 264, 9460, 5882, 13, 50552], "temperature": 0.0, "avg_logprob": -0.16607378570126816, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0005031368928030133}, {"id": 59, "seek": 24472, "start": 249.04, "end": 254.28, "text": " But, you know, I was still interested in pursuing ideas in natural language processing.", "tokens": [50580, 583, 11, 291, 458, 11, 286, 390, 920, 3102, 294, 20222, 3487, 294, 3303, 2856, 9007, 13, 50842], "temperature": 0.0, "avg_logprob": -0.16607378570126816, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0005031368928030133}, {"id": 60, "seek": 24472, "start": 254.28, "end": 260.88, "text": " But at that point in the early 90s, things were just starting to change.", "tokens": [50842, 583, 412, 300, 935, 294, 264, 2440, 4289, 82, 11, 721, 645, 445, 2891, 281, 1319, 13, 51172], "temperature": 0.0, "avg_logprob": -0.16607378570126816, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0005031368928030133}, {"id": 61, "seek": 24472, "start": 260.88, "end": 269.6, "text": " But the bulk of natural language processing was rule-based, logical, declarative systems.", "tokens": [51172, 583, 264, 16139, 295, 3303, 2856, 9007, 390, 4978, 12, 6032, 11, 14978, 11, 16694, 1166, 3652, 13, 51608], "temperature": 0.0, "avg_logprob": -0.16607378570126816, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0005031368928030133}, {"id": 62, "seek": 26960, "start": 269.84000000000003, "end": 275.48, "text": " But it was also in those years, at the beginning of the 90s, when there first started to be", "tokens": [50376, 583, 309, 390, 611, 294, 729, 924, 11, 412, 264, 2863, 295, 264, 4289, 82, 11, 562, 456, 700, 1409, 281, 312, 50658], "temperature": 0.0, "avg_logprob": -0.17055491341484919, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.001571538276039064}, {"id": 63, "seek": 26960, "start": 275.48, "end": 280.36, "text": " lots of human language material, text and speech available digitally.", "tokens": [50658, 3195, 295, 1952, 2856, 2527, 11, 2487, 293, 6218, 2435, 36938, 13, 50902], "temperature": 0.0, "avg_logprob": -0.17055491341484919, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.001571538276039064}, {"id": 64, "seek": 26960, "start": 280.36, "end": 283.92, "text": " So this was really actually just before the World Wide Web exploded.", "tokens": [50902, 407, 341, 390, 534, 767, 445, 949, 264, 3937, 42543, 9573, 27049, 13, 51080], "temperature": 0.0, "avg_logprob": -0.17055491341484919, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.001571538276039064}, {"id": 65, "seek": 26960, "start": 283.92, "end": 289.64000000000004, "text": " But there had already started to be things like legal materials and newspaper articles", "tokens": [51080, 583, 456, 632, 1217, 1409, 281, 312, 721, 411, 5089, 5319, 293, 13669, 11290, 51366], "temperature": 0.0, "avg_logprob": -0.17055491341484919, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.001571538276039064}, {"id": 66, "seek": 26960, "start": 289.64000000000004, "end": 295.6, "text": " and parliamentary handsaws, where you could at last get your hands on millions of words", "tokens": [51366, 293, 43067, 2377, 12282, 11, 689, 291, 727, 412, 1036, 483, 428, 2377, 322, 6803, 295, 2283, 51664], "temperature": 0.0, "avg_logprob": -0.17055491341484919, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.001571538276039064}, {"id": 67, "seek": 29560, "start": 295.72, "end": 297.16, "text": " of human language.", "tokens": [50370, 295, 1952, 2856, 13, 50442], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 68, "seek": 29560, "start": 297.16, "end": 302.20000000000005, "text": " And it just seemed really clear that there had to be exciting things that you could do", "tokens": [50442, 400, 309, 445, 6576, 534, 1850, 300, 456, 632, 281, 312, 4670, 721, 300, 291, 727, 360, 50694], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 69, "seek": 29560, "start": 302.20000000000005, "end": 305.24, "text": " by working empirically from lots of human language.", "tokens": [50694, 538, 1364, 25790, 984, 490, 3195, 295, 1952, 2856, 13, 50846], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 70, "seek": 29560, "start": 305.24, "end": 311.32000000000005, "text": " And that's what really sort of got me involved in a new kind of natural language processing", "tokens": [50846, 400, 300, 311, 437, 534, 1333, 295, 658, 385, 3288, 294, 257, 777, 733, 295, 3303, 2856, 9007, 51150], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 71, "seek": 29560, "start": 311.32000000000005, "end": 313.44, "text": " that then led into my subsequent career.", "tokens": [51150, 300, 550, 4684, 666, 452, 19962, 3988, 13, 51256], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 72, "seek": 29560, "start": 314.0, "end": 317.28000000000003, "text": " It sounds like your career was initially more linguistics.", "tokens": [51284, 467, 3263, 411, 428, 3988, 390, 9105, 544, 21766, 6006, 13, 51448], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 73, "seek": 29560, "start": 317.28000000000003, "end": 323.0, "text": " And if the rise of data and machine learning and empirical methods, it shifted to what NLP", "tokens": [51448, 400, 498, 264, 6272, 295, 1412, 293, 3479, 2539, 293, 31886, 7150, 11, 309, 18892, 281, 437, 426, 45196, 51734], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 74, "seek": 29560, "start": 323.0, "end": 324.8, "text": " and machine learning and NLP?", "tokens": [51734, 293, 3479, 2539, 293, 426, 45196, 30, 51824], "temperature": 0.0, "avg_logprob": -0.20818776743752615, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.010000264272093773}, {"id": 75, "seek": 32480, "start": 325.36, "end": 327.72, "text": " Yeah, I mean, it absolutely certainly shifted.", "tokens": [50392, 865, 11, 286, 914, 11, 309, 3122, 3297, 18892, 13, 50510], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 76, "seek": 32480, "start": 327.72, "end": 333.2, "text": " And I've certainly sort of shifted much more to doing both natural language processing", "tokens": [50510, 400, 286, 600, 3297, 1333, 295, 18892, 709, 544, 281, 884, 1293, 3303, 2856, 9007, 50784], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 77, "seek": 32480, "start": 333.2, "end": 335.24, "text": " and machine learning models.", "tokens": [50784, 293, 3479, 2539, 5245, 13, 50886], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 78, "seek": 32480, "start": 335.24, "end": 338.84000000000003, "text": " But to some extent, the balances varied.", "tokens": [50886, 583, 281, 512, 8396, 11, 264, 33993, 22877, 13, 51066], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 79, "seek": 32480, "start": 338.84000000000003, "end": 341.28000000000003, "text": " But I've sort of been with that as of while.", "tokens": [51066, 583, 286, 600, 1333, 295, 668, 365, 300, 382, 295, 1339, 13, 51188], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 80, "seek": 32480, "start": 341.28000000000003, "end": 343.8, "text": " You know, actually, there's an undergrad.", "tokens": [51188, 509, 458, 11, 767, 11, 456, 311, 364, 14295, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 81, "seek": 32480, "start": 343.8, "end": 350.28000000000003, "text": " For my undergrad on this thesis, it was sort of learning the forms of words.", "tokens": [51314, 1171, 452, 14295, 322, 341, 22288, 11, 309, 390, 1333, 295, 2539, 264, 6422, 295, 2283, 13, 51638], "temperature": 0.0, "avg_logprob": -0.18476735784652384, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.00015238534251693636}, {"id": 82, "seek": 35028, "start": 350.28, "end": 355.52, "text": " So how you can, which became a famous problem of sort of learning past tense of English", "tokens": [50364, 407, 577, 291, 393, 11, 597, 3062, 257, 4618, 1154, 295, 1333, 295, 2539, 1791, 18760, 295, 3669, 50626], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 83, "seek": 35028, "start": 355.52, "end": 358.55999999999995, "text": " verbs in the early connectionist literature.", "tokens": [50626, 30051, 294, 264, 2440, 4984, 468, 10394, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 84, "seek": 35028, "start": 358.55999999999995, "end": 362.71999999999997, "text": " And I was trying to sort of learn paradigms of forms of verbs.", "tokens": [50778, 400, 286, 390, 1382, 281, 1333, 295, 1466, 13480, 328, 2592, 295, 6422, 295, 30051, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 85, "seek": 35028, "start": 362.71999999999997, "end": 369.71999999999997, "text": " And I was learning rules for the different forms using the C4.5 decision tree learning", "tokens": [50986, 400, 286, 390, 2539, 4474, 337, 264, 819, 6422, 1228, 264, 383, 19, 13, 20, 3537, 4230, 2539, 51336], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 86, "seek": 35028, "start": 369.71999999999997, "end": 371.2, "text": " algorithm, if you remember that.", "tokens": [51336, 9284, 11, 498, 291, 1604, 300, 13, 51410], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 87, "seek": 35028, "start": 372.44, "end": 372.76, "text": " Yes.", "tokens": [51472, 1079, 13, 51488], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 88, "seek": 35028, "start": 373.79999999999995, "end": 374.23999999999995, "text": " Right.", "tokens": [51540, 1779, 13, 51562], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 89, "seek": 35028, "start": 374.4, "end": 375.0, "text": " Good times.", "tokens": [51570, 2205, 1413, 13, 51600], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 90, "seek": 35028, "start": 375.0, "end": 375.2, "text": " Yeah.", "tokens": [51600, 865, 13, 51610], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 91, "seek": 35028, "start": 375.2, "end": 377.52, "text": " And it's surprisingly non-intuitive, right?", "tokens": [51610, 400, 309, 311, 17600, 2107, 12, 686, 48314, 11, 558, 30, 51726], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 92, "seek": 35028, "start": 377.52, "end": 379.88, "text": " How going from present tense to past tense.", "tokens": [51726, 1012, 516, 490, 1974, 18760, 281, 1791, 18760, 13, 51844], "temperature": 0.0, "avg_logprob": -0.2628294039173287, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.002249745652079582}, {"id": 93, "seek": 38028, "start": 380.96, "end": 385.32, "text": " From, I don't know, run to run and all the other weird special cases can be.", "tokens": [50398, 3358, 11, 286, 500, 380, 458, 11, 1190, 281, 1190, 293, 439, 264, 661, 3657, 2121, 3331, 393, 312, 13, 50616], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 94, "seek": 38028, "start": 385.32, "end": 385.67999999999995, "text": " Yeah.", "tokens": [50616, 865, 13, 50634], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 95, "seek": 38028, "start": 387.4, "end": 391.2, "text": " Hey, so we talked a bunch about NLP, natural language processing.", "tokens": [50720, 1911, 11, 370, 321, 2825, 257, 3840, 466, 426, 45196, 11, 3303, 2856, 9007, 13, 50910], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 96, "seek": 38028, "start": 391.2, "end": 398.67999999999995, "text": " So for some of the learners, pick up machine learning for the first time, can you say, what is NLP?", "tokens": [50910, 407, 337, 512, 295, 264, 23655, 11, 1888, 493, 3479, 2539, 337, 264, 700, 565, 11, 393, 291, 584, 11, 437, 307, 426, 45196, 30, 51284], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 97, "seek": 38028, "start": 398.67999999999995, "end": 399.96, "text": " Sure, absolutely.", "tokens": [51284, 4894, 11, 3122, 13, 51348], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 98, "seek": 38028, "start": 399.96, "end": 403.4, "text": " Yes, NLP stands for natural language processing.", "tokens": [51348, 1079, 11, 426, 45196, 7382, 337, 3303, 2856, 9007, 13, 51520], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 99, "seek": 38028, "start": 403.4, "end": 407.91999999999996, "text": " Another word this term that's sometimes used for that is computational linguistics.", "tokens": [51520, 3996, 1349, 341, 1433, 300, 311, 2171, 1143, 337, 300, 307, 28270, 21766, 6006, 13, 51746], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 100, "seek": 38028, "start": 407.91999999999996, "end": 409.0, "text": " It's the same thing.", "tokens": [51746, 467, 311, 264, 912, 551, 13, 51800], "temperature": 0.0, "avg_logprob": -0.26177122696586275, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0003292374894954264}, {"id": 101, "seek": 40900, "start": 409.04, "end": 412.8, "text": " I mean, natural language processing is actually a weird term, right?", "tokens": [50366, 286, 914, 11, 3303, 2856, 9007, 307, 767, 257, 3657, 1433, 11, 558, 30, 50554], "temperature": 0.0, "avg_logprob": -0.12808861182286188, "compression_ratio": 1.9637096774193548, "no_speech_prob": 4.1327130020363256e-05}, {"id": 102, "seek": 40900, "start": 412.8, "end": 416.16, "text": " So it means that we're doing things with human languages.", "tokens": [50554, 407, 309, 1355, 300, 321, 434, 884, 721, 365, 1952, 8650, 13, 50722], "temperature": 0.0, "avg_logprob": -0.12808861182286188, "compression_ratio": 1.9637096774193548, "no_speech_prob": 4.1327130020363256e-05}, {"id": 103, "seek": 40900, "start": 416.16, "end": 420.76, "text": " So you have to have the conception that you're enough of a computer scientist that when you", "tokens": [50722, 407, 291, 362, 281, 362, 264, 30698, 300, 291, 434, 1547, 295, 257, 3820, 12662, 300, 562, 291, 50952], "temperature": 0.0, "avg_logprob": -0.12808861182286188, "compression_ratio": 1.9637096774193548, "no_speech_prob": 4.1327130020363256e-05}, {"id": 104, "seek": 40900, "start": 420.76, "end": 425.36, "text": " say language, you think in your brain programming language, and therefore you need to say natural", "tokens": [50952, 584, 2856, 11, 291, 519, 294, 428, 3567, 9410, 2856, 11, 293, 4412, 291, 643, 281, 584, 3303, 51182], "temperature": 0.0, "avg_logprob": -0.12808861182286188, "compression_ratio": 1.9637096774193548, "no_speech_prob": 4.1327130020363256e-05}, {"id": 105, "seek": 40900, "start": 425.36, "end": 429.32, "text": " language to mean that you're talking about the languages that human beings use.", "tokens": [51182, 2856, 281, 914, 300, 291, 434, 1417, 466, 264, 8650, 300, 1952, 8958, 764, 13, 51380], "temperature": 0.0, "avg_logprob": -0.12808861182286188, "compression_ratio": 1.9637096774193548, "no_speech_prob": 4.1327130020363256e-05}, {"id": 106, "seek": 40900, "start": 430.04, "end": 435.68, "text": " So overall natural language processing is doing anything intelligent with human languages.", "tokens": [51416, 407, 4787, 3303, 2856, 9007, 307, 884, 1340, 13232, 365, 1952, 8650, 13, 51698], "temperature": 0.0, "avg_logprob": -0.12808861182286188, "compression_ratio": 1.9637096774193548, "no_speech_prob": 4.1327130020363256e-05}, {"id": 107, "seek": 43568, "start": 435.68, "end": 443.48, "text": " So in one sense, that breaks down into understanding human languages, producing human languages,", "tokens": [50364, 407, 294, 472, 2020, 11, 300, 9857, 760, 666, 3701, 1952, 8650, 11, 10501, 1952, 8650, 11, 50754], "temperature": 0.0, "avg_logprob": -0.15855349665102753, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.240189734147862e-05}, {"id": 108, "seek": 43568, "start": 443.48, "end": 445.6, "text": " acquiring human languages.", "tokens": [50754, 37374, 1952, 8650, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15855349665102753, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.240189734147862e-05}, {"id": 109, "seek": 43568, "start": 445.6, "end": 449.56, "text": " Though people also often think about it in terms of different applications.", "tokens": [50860, 10404, 561, 611, 2049, 519, 466, 309, 294, 2115, 295, 819, 5821, 13, 51058], "temperature": 0.0, "avg_logprob": -0.15855349665102753, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.240189734147862e-05}, {"id": 110, "seek": 43568, "start": 449.56, "end": 455.56, "text": " And so then you might think about things like machine translation or doing question answering", "tokens": [51058, 400, 370, 550, 291, 1062, 519, 466, 721, 411, 3479, 12853, 420, 884, 1168, 13430, 51358], "temperature": 0.0, "avg_logprob": -0.15855349665102753, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.240189734147862e-05}, {"id": 111, "seek": 43568, "start": 455.56, "end": 462.12, "text": " or generating advertising copy or summarization.", "tokens": [51358, 420, 17746, 13097, 5055, 420, 14611, 2144, 13, 51686], "temperature": 0.0, "avg_logprob": -0.15855349665102753, "compression_ratio": 1.6764705882352942, "no_speech_prob": 7.240189734147862e-05}, {"id": 112, "seek": 46212, "start": 462.12, "end": 467.88, "text": " There are so many different tasks that people work on with particular goals in mind where", "tokens": [50364, 821, 366, 370, 867, 819, 9608, 300, 561, 589, 322, 365, 1729, 5493, 294, 1575, 689, 50652], "temperature": 0.0, "avg_logprob": -0.1949839384659477, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.0002233270206488669}, {"id": 113, "seek": 46212, "start": 467.88, "end": 469.64, "text": " you do things with human language.", "tokens": [50652, 291, 360, 721, 365, 1952, 2856, 13, 50740], "temperature": 0.0, "avg_logprob": -0.1949839384659477, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.0002233270206488669}, {"id": 114, "seek": 46212, "start": 469.64, "end": 475.28000000000003, "text": " And there's a lot of natural language processing because so much of what the world works on", "tokens": [50740, 400, 456, 311, 257, 688, 295, 3303, 2856, 9007, 570, 370, 709, 295, 437, 264, 1002, 1985, 322, 51022], "temperature": 0.0, "avg_logprob": -0.1949839384659477, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.0002233270206488669}, {"id": 115, "seek": 46212, "start": 475.28000000000003, "end": 481.56, "text": " our human world is dealt with and transmitted in terms of human language material.", "tokens": [51022, 527, 1952, 1002, 307, 15991, 365, 293, 25355, 294, 2115, 295, 1952, 2856, 2527, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1949839384659477, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.0002233270206488669}, {"id": 116, "seek": 46212, "start": 482.88, "end": 489.68, "text": " So because of all of these applications or even a web stage, most of us use NLP many, many times.", "tokens": [51402, 407, 570, 295, 439, 295, 613, 5821, 420, 754, 257, 3670, 3233, 11, 881, 295, 505, 764, 426, 45196, 867, 11, 867, 1413, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1949839384659477, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.0002233270206488669}, {"id": 117, "seek": 48968, "start": 489.68, "end": 495.6, "text": " Yeah, you're right. In some sense, the biggest application of natural language is web search,", "tokens": [50364, 865, 11, 291, 434, 558, 13, 682, 512, 2020, 11, 264, 3880, 3861, 295, 3303, 2856, 307, 3670, 3164, 11, 50660], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 118, "seek": 48968, "start": 495.6, "end": 498.44, "text": " right? That's really the big one.", "tokens": [50660, 558, 30, 663, 311, 534, 264, 955, 472, 13, 50802], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 119, "seek": 48968, "start": 498.44, "end": 502.52, "text": " I mean, traditionally, it was a kind of a simple one, right?", "tokens": [50802, 286, 914, 11, 19067, 11, 309, 390, 257, 733, 295, 257, 2199, 472, 11, 558, 30, 51006], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 120, "seek": 48968, "start": 502.52, "end": 507.6, "text": " But in the good old days, it was, you know, there were various waiting factors and so on,", "tokens": [51006, 583, 294, 264, 665, 1331, 1708, 11, 309, 390, 11, 291, 458, 11, 456, 645, 3683, 3806, 6771, 293, 370, 322, 11, 51260], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 121, "seek": 48968, "start": 507.6, "end": 513.44, "text": " but it was mainly sort of matching keywords than your search terms and then some factors", "tokens": [51260, 457, 309, 390, 8704, 1333, 295, 14324, 21009, 813, 428, 3164, 2115, 293, 550, 512, 6771, 51552], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 122, "seek": 48968, "start": 513.44, "end": 515.12, "text": " about the quality of the page.", "tokens": [51552, 466, 264, 3125, 295, 264, 3028, 13, 51636], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 123, "seek": 48968, "start": 515.12, "end": 519.52, "text": " It didn't really feel like language understanding, but that's really been changing", "tokens": [51636, 467, 994, 380, 534, 841, 411, 2856, 3701, 11, 457, 300, 311, 534, 668, 4473, 51856], "temperature": 0.0, "avg_logprob": -0.21123717442031734, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.0011331841815263033}, {"id": 124, "seek": 51952, "start": 519.52, "end": 525.84, "text": " over the years. So these days, you'll often, if you ask a question to a search engine,", "tokens": [50364, 670, 264, 924, 13, 407, 613, 1708, 11, 291, 603, 2049, 11, 498, 291, 1029, 257, 1168, 281, 257, 3164, 2848, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1419228670889871, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.00046534615103155375}, {"id": 125, "seek": 51952, "start": 525.84, "end": 531.76, "text": " it'll give you, you know, an answer box where it has extracted a piece of text and puts", "tokens": [50680, 309, 603, 976, 291, 11, 291, 458, 11, 364, 1867, 2424, 689, 309, 575, 34086, 257, 2522, 295, 2487, 293, 8137, 50976], "temperature": 0.0, "avg_logprob": -0.1419228670889871, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.00046534615103155375}, {"id": 126, "seek": 51952, "start": 531.76, "end": 535.72, "text": " what it thinks is the answer in bold or color or something like that, which is then this", "tokens": [50976, 437, 309, 7309, 307, 264, 1867, 294, 11928, 420, 2017, 420, 746, 411, 300, 11, 597, 307, 550, 341, 51174], "temperature": 0.0, "avg_logprob": -0.1419228670889871, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.00046534615103155375}, {"id": 127, "seek": 51952, "start": 535.72, "end": 537.72, "text": " task of question answering.", "tokens": [51174, 5633, 295, 1168, 13430, 13, 51274], "temperature": 0.0, "avg_logprob": -0.1419228670889871, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.00046534615103155375}, {"id": 128, "seek": 51952, "start": 537.72, "end": 540.4399999999999, "text": " And then it's really a natural language understanding task.", "tokens": [51274, 400, 550, 309, 311, 534, 257, 3303, 2856, 3701, 5633, 13, 51410], "temperature": 0.0, "avg_logprob": -0.1419228670889871, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.00046534615103155375}, {"id": 129, "seek": 51952, "start": 540.4399999999999, "end": 545.6, "text": " Yeah, yeah. And I feel like in addition to web searches, maybe the big one, you know,", "tokens": [51410, 865, 11, 1338, 13, 400, 286, 841, 411, 294, 4500, 281, 3670, 26701, 11, 1310, 264, 955, 472, 11, 291, 458, 11, 51668], "temperature": 0.0, "avg_logprob": -0.1419228670889871, "compression_ratio": 1.6743295019157087, "no_speech_prob": 0.00046534615103155375}, {"id": 130, "seek": 54560, "start": 545.6800000000001, "end": 551.08, "text": " when we're going to a online shopping website or a movie website and typing in what we want", "tokens": [50368, 562, 321, 434, 516, 281, 257, 2950, 8688, 3144, 420, 257, 3169, 3144, 293, 18444, 294, 437, 321, 528, 50638], "temperature": 0.0, "avg_logprob": -0.176961967536995, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.08255739510059357}, {"id": 131, "seek": 54560, "start": 551.08, "end": 556.44, "text": " and doing a website search on a much smaller website than, you know, the big search engines,", "tokens": [50638, 293, 884, 257, 3144, 3164, 322, 257, 709, 4356, 3144, 813, 11, 291, 458, 11, 264, 955, 3164, 12982, 11, 50906], "temperature": 0.0, "avg_logprob": -0.176961967536995, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.08255739510059357}, {"id": 132, "seek": 54560, "start": 556.44, "end": 562.24, "text": " that also increasingly uses sophisticated NLP algorithms and it's also creating quite", "tokens": [50906, 300, 611, 12980, 4960, 16950, 426, 45196, 14642, 293, 309, 311, 611, 4084, 1596, 51196], "temperature": 0.0, "avg_logprob": -0.176961967536995, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.08255739510059357}, {"id": 133, "seek": 54560, "start": 562.24, "end": 567.6800000000001, "text": " a lot of value. Maybe to you is not, you know, the real NLP, but it still seems very valuable.", "tokens": [51196, 257, 688, 295, 2158, 13, 2704, 281, 291, 307, 406, 11, 291, 458, 11, 264, 957, 426, 45196, 11, 457, 309, 920, 2544, 588, 8263, 13, 51468], "temperature": 0.0, "avg_logprob": -0.176961967536995, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.08255739510059357}, {"id": 134, "seek": 54560, "start": 567.6800000000001, "end": 572.4, "text": " I agree. It's very valuable. And there are, you know, lots of interesting problems in", "tokens": [51468, 286, 3986, 13, 467, 311, 588, 8263, 13, 400, 456, 366, 11, 291, 458, 11, 3195, 295, 1880, 2740, 294, 51704], "temperature": 0.0, "avg_logprob": -0.176961967536995, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.08255739510059357}, {"id": 135, "seek": 57240, "start": 572.4, "end": 577.56, "text": " any e-commerce website with search, very difficult problems, actually, when people", "tokens": [50364, 604, 308, 12, 26926, 3144, 365, 3164, 11, 588, 2252, 2740, 11, 767, 11, 562, 561, 50622], "temperature": 0.0, "avg_logprob": -0.1897862691145677, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.0007093194872140884}, {"id": 136, "seek": 57240, "start": 577.56, "end": 581.84, "text": " describe the kind of goods they want and you need to be trying to match it to products", "tokens": [50622, 6786, 264, 733, 295, 10179, 436, 528, 293, 291, 643, 281, 312, 1382, 281, 2995, 309, 281, 3383, 50836], "temperature": 0.0, "avg_logprob": -0.1897862691145677, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.0007093194872140884}, {"id": 137, "seek": 57240, "start": 581.84, "end": 585.64, "text": " that are available. That isn't an easy problem at all, it turns out.", "tokens": [50836, 300, 366, 2435, 13, 663, 1943, 380, 364, 1858, 1154, 412, 439, 11, 309, 4523, 484, 13, 51026], "temperature": 0.0, "avg_logprob": -0.1897862691145677, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.0007093194872140884}, {"id": 138, "seek": 57240, "start": 585.64, "end": 592.64, "text": " Yeah, that's true. Yeah. So over the last couple of decades, NLP has gone through a major", "tokens": [51026, 865, 11, 300, 311, 2074, 13, 865, 13, 407, 670, 264, 1036, 1916, 295, 7878, 11, 426, 45196, 575, 2780, 807, 257, 2563, 51376], "temperature": 0.0, "avg_logprob": -0.1897862691145677, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.0007093194872140884}, {"id": 139, "seek": 57240, "start": 592.64, "end": 598.4399999999999, "text": " shift from more of the rule-based techniques that you alluded to just now to using really", "tokens": [51376, 5513, 490, 544, 295, 264, 4978, 12, 6032, 7512, 300, 291, 33919, 281, 445, 586, 281, 1228, 534, 51666], "temperature": 0.0, "avg_logprob": -0.1897862691145677, "compression_ratio": 1.5773584905660378, "no_speech_prob": 0.0007093194872140884}, {"id": 140, "seek": 59844, "start": 598.48, "end": 605.48, "text": " machine learning much more pervasively. And so you were one of the people at, you know,", "tokens": [50366, 3479, 2539, 709, 544, 680, 7967, 3413, 13, 400, 370, 291, 645, 472, 295, 264, 561, 412, 11, 291, 458, 11, 50716], "temperature": 0.0, "avg_logprob": -0.153032886280733, "compression_ratio": 1.5896226415094339, "no_speech_prob": 0.0011331656714901328}, {"id": 141, "seek": 59844, "start": 605.48, "end": 610.24, "text": " leading parts of that charge and seeing every step of the way of creating some of the steps", "tokens": [50716, 5775, 3166, 295, 300, 4602, 293, 2577, 633, 1823, 295, 264, 636, 295, 4084, 512, 295, 264, 4439, 50954], "temperature": 0.0, "avg_logprob": -0.153032886280733, "compression_ratio": 1.5896226415094339, "no_speech_prob": 0.0011331656714901328}, {"id": 142, "seek": 59844, "start": 610.24, "end": 614.8800000000001, "text": " as it happened. Can you say a bit about that process and what you saw?", "tokens": [50954, 382, 309, 2011, 13, 1664, 291, 584, 257, 857, 466, 300, 1399, 293, 437, 291, 1866, 30, 51186], "temperature": 0.0, "avg_logprob": -0.153032886280733, "compression_ratio": 1.5896226415094339, "no_speech_prob": 0.0011331656714901328}, {"id": 143, "seek": 59844, "start": 614.8800000000001, "end": 621.0, "text": " Sure, absolutely. Yeah. So when I started off as an undergrad and grad student, really", "tokens": [51186, 4894, 11, 3122, 13, 865, 13, 407, 562, 286, 1409, 766, 382, 364, 14295, 293, 2771, 3107, 11, 534, 51492], "temperature": 0.0, "avg_logprob": -0.153032886280733, "compression_ratio": 1.5896226415094339, "no_speech_prob": 0.0011331656714901328}, {"id": 144, "seek": 62100, "start": 621.04, "end": 628.04, "text": " most of natural language processing was done by hand-built systems, which variously used", "tokens": [50366, 881, 295, 3303, 2856, 9007, 390, 1096, 538, 1011, 12, 23018, 3652, 11, 597, 3683, 356, 1143, 50716], "temperature": 0.0, "avg_logprob": -0.20409079443050337, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010479139164090157}, {"id": 145, "seek": 62100, "start": 629.72, "end": 636.72, "text": " rules and inference procedures to sort of try and build up paths and an understanding", "tokens": [50800, 4474, 293, 38253, 13846, 281, 1333, 295, 853, 293, 1322, 493, 14518, 293, 364, 3701, 51150], "temperature": 0.0, "avg_logprob": -0.20409079443050337, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010479139164090157}, {"id": 146, "seek": 62100, "start": 637.04, "end": 642.28, "text": " of a piece of text. What's an example of a rule or an inference system?", "tokens": [51166, 295, 257, 2522, 295, 2487, 13, 708, 311, 364, 1365, 295, 257, 4978, 420, 364, 38253, 1185, 30, 51428], "temperature": 0.0, "avg_logprob": -0.20409079443050337, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010479139164090157}, {"id": 147, "seek": 62100, "start": 642.28, "end": 649.28, "text": " So, you know, a rule could be part of the structure of human language, like English", "tokens": [51428, 407, 11, 291, 458, 11, 257, 4978, 727, 312, 644, 295, 264, 3877, 295, 1952, 2856, 11, 411, 3669, 51778], "temperature": 0.0, "avg_logprob": -0.20409079443050337, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0010479139164090157}, {"id": 148, "seek": 64928, "start": 649.68, "end": 654.4, "text": " sentence normally consists of a subject noun phrase followed by a verb and an object noun", "tokens": [50384, 8174, 5646, 14689, 295, 257, 3983, 23307, 9535, 6263, 538, 257, 9595, 293, 364, 2657, 23307, 50620], "temperature": 0.0, "avg_logprob": -0.1764459490776062, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0014544150326400995}, {"id": 149, "seek": 64928, "start": 654.4, "end": 660.04, "text": " phrase, and that gives you some ideas to how to understand the meaning of the sentence.", "tokens": [50620, 9535, 11, 293, 300, 2709, 291, 512, 3487, 281, 577, 281, 1223, 264, 3620, 295, 264, 8174, 13, 50902], "temperature": 0.0, "avg_logprob": -0.1764459490776062, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0014544150326400995}, {"id": 150, "seek": 64928, "start": 660.04, "end": 667.04, "text": " But it might also be saying something about how to interpret a word, so that a lot of", "tokens": [50902, 583, 309, 1062, 611, 312, 1566, 746, 466, 577, 281, 7302, 257, 1349, 11, 370, 300, 257, 688, 295, 51252], "temperature": 0.0, "avg_logprob": -0.1764459490776062, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0014544150326400995}, {"id": 151, "seek": 64928, "start": 667.16, "end": 674.16, "text": " words in English are very ambiguous. But if you have something like the word star and", "tokens": [51258, 2283, 294, 3669, 366, 588, 39465, 13, 583, 498, 291, 362, 746, 411, 264, 1349, 3543, 293, 51608], "temperature": 0.0, "avg_logprob": -0.1764459490776062, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0014544150326400995}, {"id": 152, "seek": 67416, "start": 675.16, "end": 680.28, "text": " it's in the context of a movie, then it's probably referring to a human being on this", "tokens": [50414, 309, 311, 294, 264, 4319, 295, 257, 3169, 11, 550, 309, 311, 1391, 13761, 281, 257, 1952, 885, 322, 341, 50670], "temperature": 0.0, "avg_logprob": -0.14097896318757133, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.011498242616653442}, {"id": 153, "seek": 67416, "start": 680.28, "end": 686.24, "text": " astronomical object. And in those days, people tried to deal with things like that using", "tokens": [50670, 49035, 2657, 13, 400, 294, 729, 1708, 11, 561, 3031, 281, 2028, 365, 721, 411, 300, 1228, 50968], "temperature": 0.0, "avg_logprob": -0.14097896318757133, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.011498242616653442}, {"id": 154, "seek": 67416, "start": 686.24, "end": 693.24, "text": " rules of that sort. That doesn't seem very likely to work to us these days, but, you", "tokens": [50968, 4474, 295, 300, 1333, 13, 663, 1177, 380, 1643, 588, 3700, 281, 589, 281, 505, 613, 1708, 11, 457, 11, 291, 51318], "temperature": 0.0, "avg_logprob": -0.14097896318757133, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.011498242616653442}, {"id": 155, "seek": 67416, "start": 694.24, "end": 701.24, "text": " know, once upon a time that was pretty standard. And so it was only when lots of digital text", "tokens": [51368, 458, 11, 1564, 3564, 257, 565, 300, 390, 1238, 3832, 13, 400, 370, 309, 390, 787, 562, 3195, 295, 4562, 2487, 51718], "temperature": 0.0, "avg_logprob": -0.14097896318757133, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.011498242616653442}, {"id": 156, "seek": 70124, "start": 702.16, "end": 706.48, "text": " and speech started to become available that it really seemed like there was this different", "tokens": [50410, 293, 6218, 1409, 281, 1813, 2435, 300, 309, 534, 6576, 411, 456, 390, 341, 819, 50626], "temperature": 0.0, "avg_logprob": -0.14254230108016577, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00043685175478458405}, {"id": 157, "seek": 70124, "start": 706.48, "end": 713.24, "text": " way that instead we could start calculating statistics over human language material and", "tokens": [50626, 636, 300, 2602, 321, 727, 722, 28258, 12523, 670, 1952, 2856, 2527, 293, 50964], "temperature": 0.0, "avg_logprob": -0.14254230108016577, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00043685175478458405}, {"id": 158, "seek": 70124, "start": 713.24, "end": 720.24, "text": " building machine learning models. And so that was the first thing that I got into in the", "tokens": [50964, 2390, 3479, 2539, 5245, 13, 400, 370, 300, 390, 264, 700, 551, 300, 286, 658, 666, 294, 264, 51314], "temperature": 0.0, "avg_logprob": -0.14254230108016577, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00043685175478458405}, {"id": 159, "seek": 70124, "start": 722.2, "end": 729.2, "text": " sort of mid to late 1990s. And so, you know, the first area where I started doing lots", "tokens": [51412, 1333, 295, 2062, 281, 3469, 13384, 82, 13, 400, 370, 11, 291, 458, 11, 264, 700, 1859, 689, 286, 1409, 884, 3195, 51762], "temperature": 0.0, "avg_logprob": -0.14254230108016577, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.00043685175478458405}, {"id": 160, "seek": 72920, "start": 729.32, "end": 734.6, "text": " of research and publishing papers and getting well known is building what in the early days", "tokens": [50370, 295, 2132, 293, 17832, 10577, 293, 1242, 731, 2570, 307, 2390, 437, 294, 264, 2440, 1708, 50634], "temperature": 0.0, "avg_logprob": -0.1396293258666992, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.002794452477246523}, {"id": 161, "seek": 72920, "start": 734.6, "end": 740.44, "text": " we often called statistical natural language processing. But it later merged into in general", "tokens": [50634, 321, 2049, 1219, 22820, 3303, 2856, 9007, 13, 583, 309, 1780, 36427, 666, 294, 2674, 50926], "temperature": 0.0, "avg_logprob": -0.1396293258666992, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.002794452477246523}, {"id": 162, "seek": 72920, "start": 740.44, "end": 746.0400000000001, "text": " probabilistic approaches to artificial intelligence and machine learning. And that sort of took", "tokens": [50926, 31959, 3142, 11587, 281, 11677, 7599, 293, 3479, 2539, 13, 400, 300, 1333, 295, 1890, 51206], "temperature": 0.0, "avg_logprob": -0.1396293258666992, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.002794452477246523}, {"id": 163, "seek": 72920, "start": 746.0400000000001, "end": 753.0400000000001, "text": " us through to approximately 2010, let's say. And that's roughly when the new interest in", "tokens": [51206, 505, 807, 281, 10447, 9657, 11, 718, 311, 584, 13, 400, 300, 311, 9810, 562, 264, 777, 1179, 294, 51556], "temperature": 0.0, "avg_logprob": -0.1396293258666992, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.002794452477246523}, {"id": 164, "seek": 75304, "start": 754.04, "end": 761.04, "text": " deep learning using large artificial neural networks started to take off. For my interest", "tokens": [50414, 2452, 2539, 1228, 2416, 11677, 18161, 9590, 1409, 281, 747, 766, 13, 1171, 452, 1179, 50764], "temperature": 0.0, "avg_logprob": -0.17494416818386171, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.01791958138346672}, {"id": 165, "seek": 75304, "start": 762.36, "end": 767.8399999999999, "text": " in that, I really have you to thank Andrew, because at this stage, Andrew was still full", "tokens": [50830, 294, 300, 11, 286, 534, 362, 291, 281, 1309, 10110, 11, 570, 412, 341, 3233, 11, 10110, 390, 920, 1577, 51104], "temperature": 0.0, "avg_logprob": -0.17494416818386171, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.01791958138346672}, {"id": 166, "seek": 75304, "start": 767.8399999999999, "end": 773.8399999999999, "text": " time at Stanford, and he was in the office next door to me, and he was really excited", "tokens": [51104, 565, 412, 20374, 11, 293, 415, 390, 294, 264, 3398, 958, 2853, 281, 385, 11, 293, 415, 390, 534, 2919, 51404], "temperature": 0.0, "avg_logprob": -0.17494416818386171, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.01791958138346672}, {"id": 167, "seek": 75304, "start": 773.8399999999999, "end": 779.16, "text": " about the new things that were happening in the area of deep learning. I guess anyone", "tokens": [51404, 466, 264, 777, 721, 300, 645, 2737, 294, 264, 1859, 295, 2452, 2539, 13, 286, 2041, 2878, 51670], "temperature": 0.0, "avg_logprob": -0.17494416818386171, "compression_ratio": 1.6355140186915889, "no_speech_prob": 0.01791958138346672}, {"id": 168, "seek": 77916, "start": 779.1999999999999, "end": 783.1999999999999, "text": " who walked into his office, he'd tell them, oh, it's so exciting what's happening now,", "tokens": [50366, 567, 7628, 666, 702, 3398, 11, 415, 1116, 980, 552, 11, 1954, 11, 309, 311, 370, 4670, 437, 311, 2737, 586, 11, 50566], "temperature": 0.0, "avg_logprob": -0.2474801968305539, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.023657795041799545}, {"id": 169, "seek": 77916, "start": 783.1999999999999, "end": 788.7199999999999, "text": " and neural networks should start looking at that. And so, you know, that was really the", "tokens": [50566, 293, 18161, 9590, 820, 722, 1237, 412, 300, 13, 400, 370, 11, 291, 458, 11, 300, 390, 534, 264, 50842], "temperature": 0.0, "avg_logprob": -0.2474801968305539, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.023657795041799545}, {"id": 170, "seek": 77916, "start": 788.7199999999999, "end": 795.7199999999999, "text": " impetus that got me pretty early on involved in looking at things in neural networks. I", "tokens": [50842, 704, 40506, 300, 658, 385, 1238, 2440, 322, 3288, 294, 1237, 412, 721, 294, 18161, 9590, 13, 286, 51192], "temperature": 0.0, "avg_logprob": -0.2474801968305539, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.023657795041799545}, {"id": 171, "seek": 77916, "start": 797.52, "end": 803.0, "text": " had actually seen a bit of it before. So while I was a grad student here, actually Dave Ruhmelhardt", "tokens": [51282, 632, 767, 1612, 257, 857, 295, 309, 949, 13, 407, 1339, 286, 390, 257, 2771, 3107, 510, 11, 767, 11017, 497, 3232, 10909, 21491, 83, 51556], "temperature": 0.0, "avg_logprob": -0.2474801968305539, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.023657795041799545}, {"id": 172, "seek": 77916, "start": 803.0, "end": 808.1999999999999, "text": " was at Stanford and Psych, and I'd taken his neural networks class. And so, you know,", "tokens": [51556, 390, 412, 20374, 293, 17303, 11, 293, 286, 1116, 2726, 702, 18161, 9590, 1508, 13, 400, 370, 11, 291, 458, 11, 51816], "temperature": 0.0, "avg_logprob": -0.2474801968305539, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.023657795041799545}, {"id": 173, "seek": 80820, "start": 808.24, "end": 813.5600000000001, "text": " I'd seen some of that, but it hadn't actually really been what I'd gotten into for my own", "tokens": [50366, 286, 1116, 1612, 512, 295, 300, 11, 457, 309, 8782, 380, 767, 534, 668, 437, 286, 1116, 5768, 666, 337, 452, 1065, 50632], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 174, "seek": 80820, "start": 813.5600000000001, "end": 816.88, "text": " research. So around.", "tokens": [50632, 2132, 13, 407, 926, 13, 50798], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 175, "seek": 80820, "start": 816.88, "end": 819.88, "text": " I didn't know that. Thank you.", "tokens": [50798, 286, 994, 380, 458, 300, 13, 1044, 291, 13, 50948], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 176, "seek": 80820, "start": 819.88, "end": 822.88, "text": " And then we wound up, you know, supervising some students together.", "tokens": [50948, 400, 550, 321, 10999, 493, 11, 291, 458, 11, 37971, 3436, 512, 1731, 1214, 13, 51098], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 177, "seek": 80820, "start": 822.88, "end": 823.88, "text": " Yeah, absolutely.", "tokens": [51098, 865, 11, 3122, 13, 51148], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 178, "seek": 80820, "start": 823.88, "end": 828.88, "text": " But I'd love to hear the rise of also deep learning in NLP. What are the GCs?", "tokens": [51148, 583, 286, 1116, 959, 281, 1568, 264, 6272, 295, 611, 2452, 2539, 294, 426, 45196, 13, 708, 366, 264, 29435, 82, 30, 51398], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 179, "seek": 80820, "start": 828.88, "end": 835.88, "text": " Yeah. So starting about 2010, yeah, me, students started to do the first papers in", "tokens": [51398, 865, 13, 407, 2891, 466, 9657, 11, 1338, 11, 385, 11, 1731, 1409, 281, 360, 264, 700, 10577, 294, 51748], "temperature": 0.0, "avg_logprob": -0.29306583576374223, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.002212722087278962}, {"id": 180, "seek": 83820, "start": 838.88, "end": 843.9200000000001, "text": " deep learning aimed at NLP conferences. You know, it's always hard when you're trying", "tokens": [50398, 2452, 2539, 20540, 412, 426, 45196, 22032, 13, 509, 458, 11, 309, 311, 1009, 1152, 562, 291, 434, 1382, 50650], "temperature": 0.0, "avg_logprob": -0.12888275146484374, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.0015959946904331446}, {"id": 181, "seek": 83820, "start": 843.9200000000001, "end": 850.6800000000001, "text": " to do something new. We had exactly the same experiences that people 15 or so years earlier", "tokens": [50650, 281, 360, 746, 777, 13, 492, 632, 2293, 264, 912, 5235, 300, 561, 2119, 420, 370, 924, 3071, 50988], "temperature": 0.0, "avg_logprob": -0.12888275146484374, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.0015959946904331446}, {"id": 182, "seek": 83820, "start": 850.6800000000001, "end": 855.76, "text": " had had when they started trying to do statistical NLP of when there's an established way of", "tokens": [50988, 632, 632, 562, 436, 1409, 1382, 281, 360, 22820, 426, 45196, 295, 562, 456, 311, 364, 7545, 636, 295, 51242], "temperature": 0.0, "avg_logprob": -0.12888275146484374, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.0015959946904331446}, {"id": 183, "seek": 83820, "start": 855.76, "end": 860.36, "text": " doing things. It's really hard to push out new ideas. So really some of our first papers", "tokens": [51242, 884, 721, 13, 467, 311, 534, 1152, 281, 2944, 484, 777, 3487, 13, 407, 534, 512, 295, 527, 700, 10577, 51472], "temperature": 0.0, "avg_logprob": -0.12888275146484374, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.0015959946904331446}, {"id": 184, "seek": 83820, "start": 860.36, "end": 867.1600000000001, "text": " were rejected from conferences and instead appeared at machine learning conferences or", "tokens": [51472, 645, 15749, 490, 22032, 293, 2602, 8516, 412, 3479, 2539, 22032, 420, 51812], "temperature": 0.0, "avg_logprob": -0.12888275146484374, "compression_ratio": 1.6830188679245284, "no_speech_prob": 0.0015959946904331446}, {"id": 185, "seek": 86716, "start": 867.16, "end": 872.24, "text": " deep learning workshops. But very quickly that started to change and people got super", "tokens": [50364, 2452, 2539, 19162, 13, 583, 588, 2661, 300, 1409, 281, 1319, 293, 561, 658, 1687, 50618], "temperature": 0.0, "avg_logprob": -0.21544539301018967, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0005098592955619097}, {"id": 186, "seek": 86716, "start": 872.24, "end": 879.24, "text": " interested in neural network ideas. But I sort of feel like the neural network period", "tokens": [50618, 3102, 294, 18161, 3209, 3487, 13, 583, 286, 1333, 295, 841, 411, 264, 18161, 3209, 2896, 50968], "temperature": 0.0, "avg_logprob": -0.21544539301018967, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0005098592955619097}, {"id": 187, "seek": 86716, "start": 879.6, "end": 886.6, "text": " which started effectively about 2010 itself divides in two. Because for the first period,", "tokens": [50986, 597, 1409, 8659, 466, 9657, 2564, 41347, 294, 732, 13, 1436, 337, 264, 700, 2896, 11, 51336], "temperature": 0.0, "avg_logprob": -0.21544539301018967, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0005098592955619097}, {"id": 188, "seek": 86716, "start": 887.6, "end": 894.68, "text": " let's basically say it's till 2018, we showed a lot of success at building neural networks", "tokens": [51386, 718, 311, 1936, 584, 309, 311, 4288, 6096, 11, 321, 4712, 257, 688, 295, 2245, 412, 2390, 18161, 9590, 51740], "temperature": 0.0, "avg_logprob": -0.21544539301018967, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0005098592955619097}, {"id": 189, "seek": 89468, "start": 894.7199999999999, "end": 900.7199999999999, "text": " for all sorts of tasks. We built them for syntactic parsing and sentiment analysis and", "tokens": [50366, 337, 439, 7527, 295, 9608, 13, 492, 3094, 552, 337, 23980, 19892, 21156, 278, 293, 16149, 5215, 293, 50666], "temperature": 0.0, "avg_logprob": -0.1734004136992664, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.0004040948406327516}, {"id": 190, "seek": 89468, "start": 900.7199999999999, "end": 907.7199999999999, "text": " what else to question answering. But it was sort of like we were doing the same thing", "tokens": [50666, 437, 1646, 281, 1168, 13430, 13, 583, 309, 390, 1333, 295, 411, 321, 645, 884, 264, 912, 551, 51016], "temperature": 0.0, "avg_logprob": -0.1734004136992664, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.0004040948406327516}, {"id": 191, "seek": 89468, "start": 908.5999999999999, "end": 914.4799999999999, "text": " that we used to do with other kinds of machine learning models, except we now had a better", "tokens": [51060, 300, 321, 1143, 281, 360, 365, 661, 3685, 295, 3479, 2539, 5245, 11, 3993, 321, 586, 632, 257, 1101, 51354], "temperature": 0.0, "avg_logprob": -0.1734004136992664, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.0004040948406327516}, {"id": 192, "seek": 89468, "start": 914.4799999999999, "end": 919.76, "text": " machine learning model. And we were sort of instead of training up a logistic regression", "tokens": [51354, 3479, 2539, 2316, 13, 400, 321, 645, 1333, 295, 2602, 295, 3097, 493, 257, 3565, 3142, 24590, 51618], "temperature": 0.0, "avg_logprob": -0.1734004136992664, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.0004040948406327516}, {"id": 193, "seek": 91976, "start": 919.8, "end": 924.4, "text": " or a support vector machine, we were still doing the same kind of sentiment analysis", "tokens": [50366, 420, 257, 1406, 8062, 3479, 11, 321, 645, 920, 884, 264, 912, 733, 295, 16149, 5215, 50596], "temperature": 0.0, "avg_logprob": -0.20553826697078753, "compression_ratio": 1.5, "no_speech_prob": 0.0010313609382137656}, {"id": 194, "seek": 91976, "start": 924.4, "end": 930.64, "text": " task but now we're doing it with a neural network. So I think looking back now in some", "tokens": [50596, 5633, 457, 586, 321, 434, 884, 309, 365, 257, 18161, 3209, 13, 407, 286, 519, 1237, 646, 586, 294, 512, 50908], "temperature": 0.0, "avg_logprob": -0.20553826697078753, "compression_ratio": 1.5, "no_speech_prob": 0.0010313609382137656}, {"id": 195, "seek": 91976, "start": 930.64, "end": 937.64, "text": " sense, the bigger change came around 2018 because that was when the idea of well we", "tokens": [50908, 2020, 11, 264, 3801, 1319, 1361, 926, 6096, 570, 300, 390, 562, 264, 1558, 295, 731, 321, 51258], "temperature": 0.0, "avg_logprob": -0.20553826697078753, "compression_ratio": 1.5, "no_speech_prob": 0.0010313609382137656}, {"id": 196, "seek": 91976, "start": 939.04, "end": 946.04, "text": " could just start with a large amount of human language material and build large self-supervised", "tokens": [51328, 727, 445, 722, 365, 257, 2416, 2372, 295, 1952, 2856, 2527, 293, 1322, 2416, 2698, 12, 48172, 24420, 51678], "temperature": 0.0, "avg_logprob": -0.20553826697078753, "compression_ratio": 1.5, "no_speech_prob": 0.0010313609382137656}, {"id": 197, "seek": 94604, "start": 947.04, "end": 954.04, "text": " models. So that was models then like BERT and GPT and successor models to that. And", "tokens": [50414, 5245, 13, 407, 300, 390, 5245, 550, 411, 363, 31479, 293, 26039, 51, 293, 31864, 5245, 281, 300, 13, 400, 50764], "temperature": 0.0, "avg_logprob": -0.19982200860977173, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0008822958916425705}, {"id": 198, "seek": 94604, "start": 954.9599999999999, "end": 960.76, "text": " they could just sort of acquire from word prediction over a huge amount of text this", "tokens": [50810, 436, 727, 445, 1333, 295, 20001, 490, 1349, 17630, 670, 257, 2603, 2372, 295, 2487, 341, 51100], "temperature": 0.0, "avg_logprob": -0.19982200860977173, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0008822958916425705}, {"id": 199, "seek": 94604, "start": 960.76, "end": 967.76, "text": " amazing knowledge of human languages. I think really probably that's going to be viewed", "tokens": [51100, 2243, 3601, 295, 1952, 8650, 13, 286, 519, 534, 1391, 300, 311, 516, 281, 312, 19174, 51450], "temperature": 0.0, "avg_logprob": -0.19982200860977173, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0008822958916425705}, {"id": 200, "seek": 94604, "start": 967.76, "end": 974.28, "text": " in retrospect as the bigger kind of cut point where the way things were done really changed.", "tokens": [51450, 294, 34997, 382, 264, 3801, 733, 295, 1723, 935, 689, 264, 636, 721, 645, 1096, 534, 3105, 13, 51776], "temperature": 0.0, "avg_logprob": -0.19982200860977173, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.0008822958916425705}, {"id": 201, "seek": 97428, "start": 974.76, "end": 979.76, "text": " Yeah, I think there is that trend for the large language models, learning from math", "tokens": [50388, 865, 11, 286, 519, 456, 307, 300, 6028, 337, 264, 2416, 2856, 5245, 11, 2539, 490, 5221, 50638], "temperature": 0.0, "avg_logprob": -0.28509654813599816, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0008557203109376132}, {"id": 202, "seek": 97428, "start": 979.76, "end": 984.64, "text": " and the mouse and data. I think even to lead up to that, there was one of your research", "tokens": [50638, 293, 264, 9719, 293, 1412, 13, 286, 519, 754, 281, 1477, 493, 281, 300, 11, 456, 390, 472, 295, 428, 2132, 50882], "temperature": 0.0, "avg_logprob": -0.28509654813599816, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0008557203109376132}, {"id": 203, "seek": 97428, "start": 984.64, "end": 991.64, "text": " papers that really slightly blew my mind, which is a glove paper. So because with word", "tokens": [50882, 10577, 300, 534, 4748, 19075, 452, 1575, 11, 597, 307, 257, 26928, 3035, 13, 407, 570, 365, 1349, 51232], "temperature": 0.0, "avg_logprob": -0.28509654813599816, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0008557203109376132}, {"id": 204, "seek": 97428, "start": 992.16, "end": 997.72, "text": " embeddings where you learn a vector of numbers to represent a word using a neural network,", "tokens": [51258, 12240, 29432, 689, 291, 1466, 257, 8062, 295, 3547, 281, 2906, 257, 1349, 1228, 257, 18161, 3209, 11, 51536], "temperature": 0.0, "avg_logprob": -0.28509654813599816, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0008557203109376132}, {"id": 205, "seek": 97428, "start": 997.72, "end": 1004.12, "text": " that was quite mind blowing for me. And then the glove work that you did really", "tokens": [51536, 300, 390, 1596, 1575, 15068, 337, 385, 13, 400, 550, 264, 26928, 589, 300, 291, 630, 534, 51856], "temperature": 0.0, "avg_logprob": -0.28509654813599816, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0008557203109376132}, {"id": 206, "seek": 100412, "start": 1004.16, "end": 1008.0, "text": " cleaned up the math, made it so much simpler. And then I remember reading I said, oh, that's", "tokens": [50366, 16146, 493, 264, 5221, 11, 1027, 309, 370, 709, 18587, 13, 400, 550, 286, 1604, 3760, 286, 848, 11, 1954, 11, 300, 311, 50558], "temperature": 0.0, "avg_logprob": -0.3316757054004854, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0016732207732275128}, {"id": 207, "seek": 100412, "start": 1008.0, "end": 1013.16, "text": " all there is to it. And then you can learn these really surprisingly detailed representations", "tokens": [50558, 439, 456, 307, 281, 309, 13, 400, 550, 291, 393, 1466, 613, 534, 17600, 9942, 33358, 50816], "temperature": 0.0, "avg_logprob": -0.3316757054004854, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0016732207732275128}, {"id": 208, "seek": 100412, "start": 1013.16, "end": 1017.44, "text": " of the computer learns real nuances of what words mean.", "tokens": [50816, 295, 264, 3820, 27152, 957, 38775, 295, 437, 2283, 914, 13, 51030], "temperature": 0.0, "avg_logprob": -0.3316757054004854, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0016732207732275128}, {"id": 209, "seek": 100412, "start": 1017.44, "end": 1022.84, "text": " Absolutely. Yeah, so I should give a little bit of credit to others. Other people also", "tokens": [51030, 7021, 13, 865, 11, 370, 286, 820, 976, 257, 707, 857, 295, 5397, 281, 2357, 13, 5358, 561, 611, 51300], "temperature": 0.0, "avg_logprob": -0.3316757054004854, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0016732207732275128}, {"id": 210, "seek": 100412, "start": 1022.84, "end": 1029.8, "text": " worked on some similar ideas, including Renan Colbert and Jace Weston and Tom Ostermeek", "tokens": [51300, 2732, 322, 512, 2531, 3487, 11, 3009, 12883, 282, 4004, 4290, 293, 508, 617, 4055, 266, 293, 5041, 422, 3120, 1398, 916, 51648], "temperature": 0.0, "avg_logprob": -0.3316757054004854, "compression_ratio": 1.5559701492537314, "no_speech_prob": 0.0016732207732275128}, {"id": 211, "seek": 102980, "start": 1030.0, "end": 1036.76, "text": " and colleagues at Google. But the glove word vectors is one of the very prominent systems", "tokens": [50374, 293, 7734, 412, 3329, 13, 583, 264, 26928, 1349, 18875, 307, 472, 295, 264, 588, 17034, 3652, 50712], "temperature": 0.0, "avg_logprob": -0.181533382052467, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.014938201755285263}, {"id": 212, "seek": 102980, "start": 1036.76, "end": 1041.96, "text": " of word vectors. So these word vectors already did, yeah, you're right, illustrate this idea", "tokens": [50712, 295, 1349, 18875, 13, 407, 613, 1349, 18875, 1217, 630, 11, 1338, 11, 291, 434, 558, 11, 23221, 341, 1558, 50972], "temperature": 0.0, "avg_logprob": -0.181533382052467, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.014938201755285263}, {"id": 213, "seek": 102980, "start": 1041.96, "end": 1046.56, "text": " of using self-supervised learning that we just took massive amounts of text, and then", "tokens": [50972, 295, 1228, 2698, 12, 48172, 24420, 2539, 300, 321, 445, 1890, 5994, 11663, 295, 2487, 11, 293, 550, 51202], "temperature": 0.0, "avg_logprob": -0.181533382052467, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.014938201755285263}, {"id": 214, "seek": 102980, "start": 1046.56, "end": 1053.1599999999999, "text": " we could build these models that knew an enormous amount about the meaning of words. I mean,", "tokens": [51202, 321, 727, 1322, 613, 5245, 300, 2586, 364, 11322, 2372, 466, 264, 3620, 295, 2283, 13, 286, 914, 11, 51532], "temperature": 0.0, "avg_logprob": -0.181533382052467, "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.014938201755285263}, {"id": 215, "seek": 105316, "start": 1053.16, "end": 1060.16, "text": " it's still something I sort of show people every year in the first lecture of my NLP class,", "tokens": [50364, 309, 311, 920, 746, 286, 1333, 295, 855, 561, 633, 1064, 294, 264, 700, 7991, 295, 452, 426, 45196, 1508, 11, 50714], "temperature": 0.0, "avg_logprob": -0.1715789863041469, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.03456716611981392}, {"id": 216, "seek": 105316, "start": 1061.44, "end": 1068.44, "text": " because it's something simple, but it actually just works so surprisingly well. You can do", "tokens": [50778, 570, 309, 311, 746, 2199, 11, 457, 309, 767, 445, 1985, 370, 17600, 731, 13, 509, 393, 360, 51128], "temperature": 0.0, "avg_logprob": -0.1715789863041469, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.03456716611981392}, {"id": 217, "seek": 105316, "start": 1068.44, "end": 1074.6000000000001, "text": " this sort of simple modeling of trying to predict a word given the words in the context.", "tokens": [51128, 341, 1333, 295, 2199, 15983, 295, 1382, 281, 6069, 257, 1349, 2212, 264, 2283, 294, 264, 4319, 13, 51436], "temperature": 0.0, "avg_logprob": -0.1715789863041469, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.03456716611981392}, {"id": 218, "seek": 105316, "start": 1074.6000000000001, "end": 1079.5600000000002, "text": " And simply by sort of running the math of learning to do those predictions well, you", "tokens": [51436, 400, 2935, 538, 1333, 295, 2614, 264, 5221, 295, 2539, 281, 360, 729, 21264, 731, 11, 291, 51684], "temperature": 0.0, "avg_logprob": -0.1715789863041469, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.03456716611981392}, {"id": 219, "seek": 107956, "start": 1079.6, "end": 1084.2, "text": " learn all these things about word meaning, and you can do these really nice patterns", "tokens": [50366, 1466, 439, 613, 721, 466, 1349, 3620, 11, 293, 291, 393, 360, 613, 534, 1481, 8294, 50596], "temperature": 0.0, "avg_logprob": -0.19299158556707974, "compression_ratio": 1.625, "no_speech_prob": 0.0078100948594510555}, {"id": 220, "seek": 107956, "start": 1084.2, "end": 1091.2, "text": " of similar word meaning or analogies of something like, you know, pencil is to drawing as paint", "tokens": [50596, 295, 2531, 1349, 3620, 420, 16660, 530, 295, 746, 411, 11, 291, 458, 11, 10985, 307, 281, 6316, 382, 4225, 50946], "temperature": 0.0, "avg_logprob": -0.19299158556707974, "compression_ratio": 1.625, "no_speech_prob": 0.0078100948594510555}, {"id": 221, "seek": 107956, "start": 1092.8, "end": 1099.8, "text": " brush is to, and it'll say painting, right? That it's sort of already showing just a lot", "tokens": [51026, 5287, 307, 281, 11, 293, 309, 603, 584, 5370, 11, 558, 30, 663, 309, 311, 1333, 295, 1217, 4099, 445, 257, 688, 51376], "temperature": 0.0, "avg_logprob": -0.19299158556707974, "compression_ratio": 1.625, "no_speech_prob": 0.0078100948594510555}, {"id": 222, "seek": 107956, "start": 1100.0, "end": 1107.0, "text": " of successful learning. So that was the precursor to what then got developed to the next stage", "tokens": [51386, 295, 4406, 2539, 13, 407, 300, 390, 264, 41736, 284, 281, 437, 550, 658, 4743, 281, 264, 958, 3233, 51736], "temperature": 0.0, "avg_logprob": -0.19299158556707974, "compression_ratio": 1.625, "no_speech_prob": 0.0078100948594510555}, {"id": 223, "seek": 110700, "start": 1108.0, "end": 1114.0, "text": " with things like Burton GPT, where it wasn't just meanings of individual words, but meanings", "tokens": [50414, 365, 721, 411, 46011, 26039, 51, 11, 689, 309, 2067, 380, 445, 28138, 295, 2609, 2283, 11, 457, 28138, 50714], "temperature": 0.0, "avg_logprob": -0.2511163769346295, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0007094909669831395}, {"id": 224, "seek": 110700, "start": 1114.76, "end": 1116.88, "text": " of whole pieces of text and context.", "tokens": [50752, 295, 1379, 3755, 295, 2487, 293, 4319, 13, 50858], "temperature": 0.0, "avg_logprob": -0.2511163769346295, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0007094909669831395}, {"id": 225, "seek": 110700, "start": 1116.88, "end": 1121.88, "text": " Yeah, so I thought it was amazing that you can, you know, take a small neural network", "tokens": [50858, 865, 11, 370, 286, 1194, 309, 390, 2243, 300, 291, 393, 11, 291, 458, 11, 747, 257, 1359, 18161, 3209, 51108], "temperature": 0.0, "avg_logprob": -0.2511163769346295, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0007094909669831395}, {"id": 226, "seek": 110700, "start": 1121.88, "end": 1127.72, "text": " or some model and then give it lots of English sentences or some of the language and hide", "tokens": [51108, 420, 512, 2316, 293, 550, 976, 309, 3195, 295, 3669, 16579, 420, 512, 295, 264, 2856, 293, 6479, 51400], "temperature": 0.0, "avg_logprob": -0.2511163769346295, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0007094909669831395}, {"id": 227, "seek": 110700, "start": 1127.72, "end": 1132.24, "text": " the word, ask it to predict what is the word that I just hit, and that allows it to learn", "tokens": [51400, 264, 1349, 11, 1029, 309, 281, 6069, 437, 307, 264, 1349, 300, 286, 445, 2045, 11, 293, 300, 4045, 309, 281, 1466, 51626], "temperature": 0.0, "avg_logprob": -0.2511163769346295, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.0007094909669831395}, {"id": 228, "seek": 113224, "start": 1132.28, "end": 1138.78, "text": " these analogies and these very deep, what you would think are really deep things behind", "tokens": [50366, 613, 16660, 530, 293, 613, 588, 2452, 11, 437, 291, 576, 519, 366, 534, 2452, 721, 2261, 50691], "temperature": 0.0, "avg_logprob": -0.2786863033588116, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.007116412743926048}, {"id": 229, "seek": 113224, "start": 1138.78, "end": 1145.28, "text": " the meaning of the words. And then, you know, 2018, maybe there's other infection point.", "tokens": [50691, 264, 3620, 295, 264, 2283, 13, 400, 550, 11, 291, 458, 11, 6096, 11, 1310, 456, 311, 661, 11764, 935, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2786863033588116, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.007116412743926048}, {"id": 230, "seek": 113224, "start": 1145.28, "end": 1146.8, "text": " What happened after that?", "tokens": [51016, 708, 2011, 934, 300, 30, 51092], "temperature": 0.0, "avg_logprob": -0.2786863033588116, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.007116412743926048}, {"id": 231, "seek": 113224, "start": 1146.8, "end": 1153.8, "text": " Yeah, so I mean, in 2018, that was the point in which, well, sort of really two things happened.", "tokens": [51092, 865, 11, 370, 286, 914, 11, 294, 6096, 11, 300, 390, 264, 935, 294, 597, 11, 731, 11, 1333, 295, 534, 732, 721, 2011, 13, 51442], "temperature": 0.0, "avg_logprob": -0.2786863033588116, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.007116412743926048}, {"id": 232, "seek": 115380, "start": 1154.0, "end": 1161.0, "text": " One thing is that people, well, really in 2017, had developed this new neural architecture,", "tokens": [50374, 1485, 551, 307, 300, 561, 11, 731, 11, 534, 294, 6591, 11, 632, 4743, 341, 777, 18161, 9482, 11, 50724], "temperature": 0.0, "avg_logprob": -0.19416389465332032, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.0003568544634617865}, {"id": 233, "seek": 115380, "start": 1162.76, "end": 1169.76, "text": " which was much more scalable onto modern parallel GPUs, and so that was the transformer architecture.", "tokens": [50812, 597, 390, 709, 544, 38481, 3911, 4363, 8952, 18407, 82, 11, 293, 370, 300, 390, 264, 31782, 9482, 13, 51162], "temperature": 0.0, "avg_logprob": -0.19416389465332032, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.0003568544634617865}, {"id": 234, "seek": 115380, "start": 1171.0, "end": 1176.36, "text": " The second part of it, though, was, you know, maybe people rediscovered because it was using", "tokens": [51224, 440, 1150, 644, 295, 309, 11, 1673, 11, 390, 11, 291, 458, 11, 1310, 561, 2182, 40080, 292, 570, 309, 390, 1228, 51492], "temperature": 0.0, "avg_logprob": -0.19416389465332032, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.0003568544634617865}, {"id": 235, "seek": 115380, "start": 1176.36, "end": 1181.96, "text": " the same trick as the glove model, that if you have the task of say, of just predicting", "tokens": [51492, 264, 912, 4282, 382, 264, 26928, 2316, 11, 300, 498, 291, 362, 264, 5633, 295, 584, 11, 295, 445, 32884, 51772], "temperature": 0.0, "avg_logprob": -0.19416389465332032, "compression_ratio": 1.5583333333333333, "no_speech_prob": 0.0003568544634617865}, {"id": 236, "seek": 118196, "start": 1181.96, "end": 1188.2, "text": " a word given a context, either a context on both sides of it or the preceding words,", "tokens": [50364, 257, 1349, 2212, 257, 4319, 11, 2139, 257, 4319, 322, 1293, 4881, 295, 309, 420, 264, 16969, 278, 2283, 11, 50676], "temperature": 0.0, "avg_logprob": -0.17048223628554232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.0004305079928599298}, {"id": 237, "seek": 118196, "start": 1188.2, "end": 1195.2, "text": " that that just turns out to be an amazing learning task. And that surprises a lot of", "tokens": [50676, 300, 300, 445, 4523, 484, 281, 312, 364, 2243, 2539, 5633, 13, 400, 300, 22655, 257, 688, 295, 51026], "temperature": 0.0, "avg_logprob": -0.17048223628554232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.0004305079928599298}, {"id": 238, "seek": 118196, "start": 1195.2, "end": 1202.04, "text": " people, and a lot of the time you see discussions where people say disparaging things of, you", "tokens": [51026, 561, 11, 293, 257, 688, 295, 264, 565, 291, 536, 11088, 689, 561, 584, 14548, 3568, 721, 295, 11, 291, 51368], "temperature": 0.0, "avg_logprob": -0.17048223628554232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.0004305079928599298}, {"id": 239, "seek": 118196, "start": 1202.04, "end": 1206.76, "text": " know, this is nothing interesting is happening, all it's doing is statistics to predict which", "tokens": [51368, 458, 11, 341, 307, 1825, 1880, 307, 2737, 11, 439, 309, 311, 884, 307, 12523, 281, 6069, 597, 51604], "temperature": 0.0, "avg_logprob": -0.17048223628554232, "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.0004305079928599298}, {"id": 240, "seek": 120676, "start": 1206.84, "end": 1213.84, "text": " word is most likely to come after the preceding words. And I think the really interesting", "tokens": [50368, 1349, 307, 881, 3700, 281, 808, 934, 264, 16969, 278, 2283, 13, 400, 286, 519, 264, 534, 1880, 50718], "temperature": 0.0, "avg_logprob": -0.12423337160885989, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0154188834130764}, {"id": 241, "seek": 120676, "start": 1213.84, "end": 1220.68, "text": " thing is that that's true, but it's not true. I mean, because, yes, what the task is, is", "tokens": [50718, 551, 307, 300, 300, 311, 2074, 11, 457, 309, 311, 406, 2074, 13, 286, 914, 11, 570, 11, 2086, 11, 437, 264, 5633, 307, 11, 307, 51060], "temperature": 0.0, "avg_logprob": -0.12423337160885989, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0154188834130764}, {"id": 242, "seek": 120676, "start": 1220.68, "end": 1227.44, "text": " you're predicting the next word given preceding words. But the really interesting thing is,", "tokens": [51060, 291, 434, 32884, 264, 958, 1349, 2212, 16969, 278, 2283, 13, 583, 264, 534, 1880, 551, 307, 11, 51398], "temperature": 0.0, "avg_logprob": -0.12423337160885989, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0154188834130764}, {"id": 243, "seek": 120676, "start": 1227.44, "end": 1234.44, "text": " if you want to do that task really as well as possible, then it actually helps to understand", "tokens": [51398, 498, 291, 528, 281, 360, 300, 5633, 534, 382, 731, 382, 1944, 11, 550, 309, 767, 3665, 281, 1223, 51748], "temperature": 0.0, "avg_logprob": -0.12423337160885989, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0154188834130764}, {"id": 244, "seek": 123444, "start": 1235.2, "end": 1241.2, "text": " the whole of the rest of the sentence and know who's doing what to who in what's in the sentence.", "tokens": [50402, 264, 1379, 295, 264, 1472, 295, 264, 8174, 293, 458, 567, 311, 884, 437, 281, 567, 294, 437, 311, 294, 264, 8174, 13, 50702], "temperature": 0.0, "avg_logprob": -0.17758315236944902, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0008035710197873414}, {"id": 245, "seek": 123444, "start": 1241.2, "end": 1248.2, "text": " But more than that, it also helps to understand the world. Because if your, your text is going", "tokens": [50702, 583, 544, 813, 300, 11, 309, 611, 3665, 281, 1223, 264, 1002, 13, 1436, 498, 428, 11, 428, 2487, 307, 516, 51052], "temperature": 0.0, "avg_logprob": -0.17758315236944902, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0008035710197873414}, {"id": 246, "seek": 123444, "start": 1248.8, "end": 1255.8, "text": " something along the lines of, you know, the currency used in Fiji is that, well, you need", "tokens": [51082, 746, 2051, 264, 3876, 295, 11, 291, 458, 11, 264, 13346, 1143, 294, 479, 26539, 307, 300, 11, 731, 11, 291, 643, 51432], "temperature": 0.0, "avg_logprob": -0.17758315236944902, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0008035710197873414}, {"id": 247, "seek": 123444, "start": 1257.3600000000001, "end": 1262.92, "text": " to have some world knowledge to know what the right answer to that is. And so good models", "tokens": [51510, 281, 362, 512, 1002, 3601, 281, 458, 437, 264, 558, 1867, 281, 300, 307, 13, 400, 370, 665, 5245, 51788], "temperature": 0.0, "avg_logprob": -0.17758315236944902, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0008035710197873414}, {"id": 248, "seek": 126292, "start": 1262.96, "end": 1269.96, "text": " at doing this learn both to follow the structure of sentences and their meaning and to know", "tokens": [50366, 412, 884, 341, 1466, 1293, 281, 1524, 264, 3877, 295, 16579, 293, 641, 3620, 293, 281, 458, 50716], "temperature": 0.0, "avg_logprob": -0.1908908420138889, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0010810592211782932}, {"id": 249, "seek": 126292, "start": 1271.4, "end": 1276.8000000000002, "text": " facts about the world also that they can predict. And therefore this turns into what", "tokens": [50788, 9130, 466, 264, 1002, 611, 300, 436, 393, 6069, 13, 400, 4412, 341, 4523, 666, 437, 51058], "temperature": 0.0, "avg_logprob": -0.1908908420138889, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0010810592211782932}, {"id": 250, "seek": 126292, "start": 1276.8000000000002, "end": 1282.24, "text": " sometimes referred to as an AI complete task, right? That you really need, there's nothing", "tokens": [51058, 2171, 10839, 281, 382, 364, 7318, 3566, 5633, 11, 558, 30, 663, 291, 534, 643, 11, 456, 311, 1825, 51330], "temperature": 0.0, "avg_logprob": -0.1908908420138889, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0010810592211782932}, {"id": 251, "seek": 126292, "start": 1282.24, "end": 1289.0800000000002, "text": " that can't actually be useful in answering this, what word comes next sense, right? You", "tokens": [51330, 300, 393, 380, 767, 312, 4420, 294, 13430, 341, 11, 437, 1349, 1487, 958, 2020, 11, 558, 30, 509, 51672], "temperature": 0.0, "avg_logprob": -0.1908908420138889, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0010810592211782932}, {"id": 252, "seek": 128908, "start": 1289.08, "end": 1296.08, "text": " know, you can be in the World Cup semifinals, the teams are, and you need to know something", "tokens": [50364, 458, 11, 291, 393, 312, 294, 264, 3937, 13751, 4361, 351, 33961, 11, 264, 5491, 366, 11, 293, 291, 643, 281, 458, 746, 50714], "temperature": 0.0, "avg_logprob": -0.23635679244995117, "compression_ratio": 1.6639004149377594, "no_speech_prob": 0.0035353866405785084}, {"id": 253, "seek": 128908, "start": 1296.6, "end": 1299.52, "text": " about soccer to be giving the right answer.", "tokens": [50740, 466, 15469, 281, 312, 2902, 264, 558, 1867, 13, 50886], "temperature": 0.0, "avg_logprob": -0.23635679244995117, "compression_ratio": 1.6639004149377594, "no_speech_prob": 0.0035353866405785084}, {"id": 254, "seek": 128908, "start": 1299.52, "end": 1304.72, "text": " AI completes this funny concept, or is this idea that you can solve this one problem,", "tokens": [50886, 7318, 36362, 341, 4074, 3410, 11, 420, 307, 341, 1558, 300, 291, 393, 5039, 341, 472, 1154, 11, 51146], "temperature": 0.0, "avg_logprob": -0.23635679244995117, "compression_ratio": 1.6639004149377594, "no_speech_prob": 0.0035353866405785084}, {"id": 255, "seek": 128908, "start": 1304.72, "end": 1310.3999999999999, "text": " you can solve, you know, everything in AI or kind of make an analogy to NP complete problems", "tokens": [51146, 291, 393, 5039, 11, 291, 458, 11, 1203, 294, 7318, 420, 733, 295, 652, 364, 21663, 281, 38611, 3566, 2740, 51430], "temperature": 0.0, "avg_logprob": -0.23635679244995117, "compression_ratio": 1.6639004149377594, "no_speech_prob": 0.0035353866405785084}, {"id": 256, "seek": 128908, "start": 1310.3999999999999, "end": 1315.6399999999999, "text": " from the theory of computing. What do you think? Do you think predicting the next word", "tokens": [51430, 490, 264, 5261, 295, 15866, 13, 708, 360, 291, 519, 30, 1144, 291, 519, 32884, 264, 958, 1349, 51692], "temperature": 0.0, "avg_logprob": -0.23635679244995117, "compression_ratio": 1.6639004149377594, "no_speech_prob": 0.0035353866405785084}, {"id": 257, "seek": 131564, "start": 1315.68, "end": 1318.64, "text": " is AI complete? I have very mixed feelings about that myself.", "tokens": [50366, 307, 7318, 3566, 30, 286, 362, 588, 7467, 6640, 466, 300, 2059, 13, 50514], "temperature": 0.0, "avg_logprob": -0.25812070509966684, "compression_ratio": 1.7330508474576272, "no_speech_prob": 0.025551404803991318}, {"id": 258, "seek": 131564, "start": 1318.64, "end": 1323.64, "text": " I shall, I shall go ahead and say, I don't think it's true. So just what do you think?", "tokens": [50514, 286, 4393, 11, 286, 4393, 352, 2286, 293, 584, 11, 286, 500, 380, 519, 309, 311, 2074, 13, 407, 445, 437, 360, 291, 519, 30, 50764], "temperature": 0.0, "avg_logprob": -0.25812070509966684, "compression_ratio": 1.7330508474576272, "no_speech_prob": 0.025551404803991318}, {"id": 259, "seek": 131564, "start": 1323.64, "end": 1330.64, "text": " I think it's not quite true because I think there are other kinds of things that human", "tokens": [50764, 286, 519, 309, 311, 406, 1596, 2074, 570, 286, 519, 456, 366, 661, 3685, 295, 721, 300, 1952, 51114], "temperature": 0.0, "avg_logprob": -0.25812070509966684, "compression_ratio": 1.7330508474576272, "no_speech_prob": 0.025551404803991318}, {"id": 260, "seek": 131564, "start": 1333.2, "end": 1339.0800000000002, "text": " beings manage to work out. You know, there are human beings that have clever insights", "tokens": [51242, 8958, 3067, 281, 589, 484, 13, 509, 458, 11, 456, 366, 1952, 8958, 300, 362, 13494, 14310, 51536], "temperature": 0.0, "avg_logprob": -0.25812070509966684, "compression_ratio": 1.7330508474576272, "no_speech_prob": 0.025551404803991318}, {"id": 261, "seek": 131564, "start": 1339.0800000000002, "end": 1345.6000000000001, "text": " in mathematics, or there are human beings who are looking at something that's much more", "tokens": [51536, 294, 18666, 11, 420, 456, 366, 1952, 8958, 567, 366, 1237, 412, 746, 300, 311, 709, 544, 51862], "temperature": 0.0, "avg_logprob": -0.25812070509966684, "compression_ratio": 1.7330508474576272, "no_speech_prob": 0.025551404803991318}, {"id": 262, "seek": 134560, "start": 1345.76, "end": 1352.56, "text": " you know, three dimensional, real world puzzle of sort of figuring out how to do something", "tokens": [50372, 291, 458, 11, 1045, 18795, 11, 957, 1002, 12805, 295, 1333, 295, 15213, 484, 577, 281, 360, 746, 50712], "temperature": 0.0, "avg_logprob": -0.21060011121961805, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0010479151969775558}, {"id": 263, "seek": 134560, "start": 1352.56, "end": 1359.56, "text": " mechanical or something like that, and that's just not a language problem. But on the other", "tokens": [50712, 12070, 420, 746, 411, 300, 11, 293, 300, 311, 445, 406, 257, 2856, 1154, 13, 583, 322, 264, 661, 51062], "temperature": 0.0, "avg_logprob": -0.21060011121961805, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0010479151969775558}, {"id": 264, "seek": 134560, "start": 1360.9599999999998, "end": 1367.9599999999998, "text": " hand, I mean, I think language gets closer to universality than some people think as", "tokens": [51132, 1011, 11, 286, 914, 11, 286, 519, 2856, 2170, 4966, 281, 5950, 1860, 813, 512, 561, 519, 382, 51482], "temperature": 0.0, "avg_logprob": -0.21060011121961805, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0010479151969775558}, {"id": 265, "seek": 136796, "start": 1368.92, "end": 1375.92, "text": " well, because, you know, we live in this 3D world and operate in it with our bodies and", "tokens": [50412, 731, 11, 570, 11, 291, 458, 11, 321, 1621, 294, 341, 805, 35, 1002, 293, 9651, 294, 309, 365, 527, 7510, 293, 50762], "temperature": 0.0, "avg_logprob": -0.16620423760212644, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.013407600112259388}, {"id": 266, "seek": 136796, "start": 1378.32, "end": 1385.32, "text": " our feelings and other creatures and artifacts around it, and you could think, well, not", "tokens": [50882, 527, 6640, 293, 661, 12281, 293, 24617, 926, 309, 11, 293, 291, 727, 519, 11, 731, 11, 406, 51232], "temperature": 0.0, "avg_logprob": -0.16620423760212644, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.013407600112259388}, {"id": 267, "seek": 136796, "start": 1385.52, "end": 1392.52, "text": " much of that is in language at all. But actually, just about all of this stuff, we think about,", "tokens": [51242, 709, 295, 300, 307, 294, 2856, 412, 439, 13, 583, 767, 11, 445, 466, 439, 295, 341, 1507, 11, 321, 519, 466, 11, 51592], "temperature": 0.0, "avg_logprob": -0.16620423760212644, "compression_ratio": 1.5542857142857143, "no_speech_prob": 0.013407600112259388}, {"id": 268, "seek": 139252, "start": 1393.4, "end": 1399.44, "text": " we talk about, we write about it in language, we can describe the positions of things relative", "tokens": [50408, 321, 751, 466, 11, 321, 2464, 466, 309, 294, 2856, 11, 321, 393, 6786, 264, 8432, 295, 721, 4972, 50710], "temperature": 0.0, "avg_logprob": -0.1772656745099007, "compression_ratio": 1.7990654205607477, "no_speech_prob": 0.005381787195801735}, {"id": 269, "seek": 139252, "start": 1399.44, "end": 1404.68, "text": " to each other in language. So a surprising amount of the other parts of the world are", "tokens": [50710, 281, 1184, 661, 294, 2856, 13, 407, 257, 8830, 2372, 295, 264, 661, 3166, 295, 264, 1002, 366, 50972], "temperature": 0.0, "avg_logprob": -0.1772656745099007, "compression_ratio": 1.7990654205607477, "no_speech_prob": 0.005381787195801735}, {"id": 270, "seek": 139252, "start": 1404.68, "end": 1409.6399999999999, "text": " seen in reflection in language, and therefore you're learning about all of them too when", "tokens": [50972, 1612, 294, 12914, 294, 2856, 11, 293, 4412, 291, 434, 2539, 466, 439, 295, 552, 886, 562, 51220], "temperature": 0.0, "avg_logprob": -0.1772656745099007, "compression_ratio": 1.7990654205607477, "no_speech_prob": 0.005381787195801735}, {"id": 271, "seek": 139252, "start": 1409.6399999999999, "end": 1411.6, "text": " you learn about language use.", "tokens": [51220, 291, 1466, 466, 2856, 764, 13, 51318], "temperature": 0.0, "avg_logprob": -0.1772656745099007, "compression_ratio": 1.7990654205607477, "no_speech_prob": 0.005381787195801735}, {"id": 272, "seek": 139252, "start": 1411.6, "end": 1417.6, "text": " You learn about, you know, one aspect of a lot of things, even if things like, how do", "tokens": [51318, 509, 1466, 466, 11, 291, 458, 11, 472, 4171, 295, 257, 688, 295, 721, 11, 754, 498, 721, 411, 11, 577, 360, 51618], "temperature": 0.0, "avg_logprob": -0.1772656745099007, "compression_ratio": 1.7990654205607477, "no_speech_prob": 0.005381787195801735}, {"id": 273, "seek": 141760, "start": 1418.28, "end": 1425.28, "text": " you ride a bicycle? You don't really learn how to ride a bicycle, but you learn some", "tokens": [50398, 291, 5077, 257, 20888, 30, 509, 500, 380, 534, 1466, 577, 281, 5077, 257, 20888, 11, 457, 291, 1466, 512, 50748], "temperature": 0.0, "avg_logprob": -0.18082365036010742, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.007116854656487703}, {"id": 274, "seek": 141760, "start": 1425.48, "end": 1429.8799999999999, "text": " aspects of what it involves, that you need to balance and you have to have your feet", "tokens": [50758, 7270, 295, 437, 309, 11626, 11, 300, 291, 643, 281, 4772, 293, 291, 362, 281, 362, 428, 3521, 50978], "temperature": 0.0, "avg_logprob": -0.18082365036010742, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.007116854656487703}, {"id": 275, "seek": 141760, "start": 1429.8799999999999, "end": 1434.6799999999998, "text": " on the pedals and push them and all of that kind of things, yeah.", "tokens": [50978, 322, 264, 35217, 293, 2944, 552, 293, 439, 295, 300, 733, 295, 721, 11, 1338, 13, 51218], "temperature": 0.0, "avg_logprob": -0.18082365036010742, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.007116854656487703}, {"id": 276, "seek": 141760, "start": 1434.6799999999998, "end": 1441.1999999999998, "text": " And so with this trend in NLP, the large language models has been very exciting for the last", "tokens": [51218, 400, 370, 365, 341, 6028, 294, 426, 45196, 11, 264, 2416, 2856, 5245, 575, 668, 588, 4670, 337, 264, 1036, 51544], "temperature": 0.0, "avg_logprob": -0.18082365036010742, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.007116854656487703}, {"id": 277, "seek": 141760, "start": 1441.1999999999998, "end": 1446.1599999999999, "text": " several years. What are your thoughts on where all this will go?", "tokens": [51544, 2940, 924, 13, 708, 366, 428, 4598, 322, 689, 439, 341, 486, 352, 30, 51792], "temperature": 0.0, "avg_logprob": -0.18082365036010742, "compression_ratio": 1.6307053941908713, "no_speech_prob": 0.007116854656487703}, {"id": 278, "seek": 144616, "start": 1446.52, "end": 1453.52, "text": " Well, I mean, yeah, so it's just been amazingly successful and exciting, right? So we haven't", "tokens": [50382, 1042, 11, 286, 914, 11, 1338, 11, 370, 309, 311, 445, 668, 31762, 4406, 293, 4670, 11, 558, 30, 407, 321, 2378, 380, 50732], "temperature": 0.0, "avg_logprob": -0.18143127181313254, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0028399256989359856}, {"id": 279, "seek": 144616, "start": 1455.44, "end": 1458.76, "text": " really explained all the details, right? So there's a first stage of learning these", "tokens": [50828, 534, 8825, 439, 264, 4365, 11, 558, 30, 407, 456, 311, 257, 700, 3233, 295, 2539, 613, 50994], "temperature": 0.0, "avg_logprob": -0.18143127181313254, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0028399256989359856}, {"id": 280, "seek": 144616, "start": 1458.76, "end": 1465.76, "text": " large language models where the task is just to predict the next word and you do that billions", "tokens": [50994, 2416, 2856, 5245, 689, 264, 5633, 307, 445, 281, 6069, 264, 958, 1349, 293, 291, 360, 300, 17375, 51344], "temperature": 0.0, "avg_logprob": -0.18143127181313254, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0028399256989359856}, {"id": 281, "seek": 144616, "start": 1465.76, "end": 1472.3600000000001, "text": " of times over a very large piece of text, and behold, you get this large neural network,", "tokens": [51344, 295, 1413, 670, 257, 588, 2416, 2522, 295, 2487, 11, 293, 27234, 11, 291, 483, 341, 2416, 18161, 3209, 11, 51674], "temperature": 0.0, "avg_logprob": -0.18143127181313254, "compression_ratio": 1.5903083700440528, "no_speech_prob": 0.0028399256989359856}, {"id": 282, "seek": 147236, "start": 1472.3999999999999, "end": 1477.24, "text": " which is just a really useful artifact for all sorts of natural language processing tasks.", "tokens": [50366, 597, 307, 445, 257, 534, 4420, 34806, 337, 439, 7527, 295, 3303, 2856, 9007, 9608, 13, 50608], "temperature": 0.0, "avg_logprob": -0.19532849529001972, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.09129776060581207}, {"id": 283, "seek": 147236, "start": 1477.24, "end": 1481.52, "text": " But then you still actually have to do something with it if you want to do a particular task,", "tokens": [50608, 583, 550, 291, 920, 767, 362, 281, 360, 746, 365, 309, 498, 291, 528, 281, 360, 257, 1729, 5633, 11, 50822], "temperature": 0.0, "avg_logprob": -0.19532849529001972, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.09129776060581207}, {"id": 284, "seek": 147236, "start": 1481.52, "end": 1488.52, "text": " whether that's question answering or summarization or detecting toxic content in social media", "tokens": [50822, 1968, 300, 311, 1168, 13430, 420, 14611, 2144, 420, 40237, 12786, 2701, 294, 2093, 3021, 51172], "temperature": 0.0, "avg_logprob": -0.19532849529001972, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.09129776060581207}, {"id": 285, "seek": 147236, "start": 1488.52, "end": 1492.4799999999998, "text": " or something like that. And at that point, there's a choice of things that you could", "tokens": [51172, 420, 746, 411, 300, 13, 400, 412, 300, 935, 11, 456, 311, 257, 3922, 295, 721, 300, 291, 727, 51370], "temperature": 0.0, "avg_logprob": -0.19532849529001972, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.09129776060581207}, {"id": 286, "seek": 147236, "start": 1492.4799999999998, "end": 1498.84, "text": " do with it. The traditional answer was then you had a particular task, like say, detecting", "tokens": [51370, 360, 365, 309, 13, 440, 5164, 1867, 390, 550, 291, 632, 257, 1729, 5633, 11, 411, 584, 11, 40237, 51688], "temperature": 0.0, "avg_logprob": -0.19532849529001972, "compression_ratio": 1.7461538461538462, "no_speech_prob": 0.09129776060581207}, {"id": 287, "seek": 149884, "start": 1498.84, "end": 1504.48, "text": " toxic comments in social media, and you'd take some supervised data for that, and then", "tokens": [50364, 12786, 3053, 294, 2093, 3021, 11, 293, 291, 1116, 747, 512, 46533, 1412, 337, 300, 11, 293, 550, 50646], "temperature": 0.0, "avg_logprob": -0.1470547546575099, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0006872045341879129}, {"id": 288, "seek": 149884, "start": 1504.48, "end": 1511.48, "text": " you'd fine tune the language model to answer that classification task. But you were enormously", "tokens": [50646, 291, 1116, 2489, 10864, 264, 2856, 2316, 281, 1867, 300, 21538, 5633, 13, 583, 291, 645, 39669, 50996], "temperature": 0.0, "avg_logprob": -0.1470547546575099, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0006872045341879129}, {"id": 289, "seek": 149884, "start": 1511.48, "end": 1517.52, "text": " helped by having this base of this large self-supervised model because it meant that the model had", "tokens": [50996, 4254, 538, 1419, 341, 3096, 295, 341, 2416, 2698, 12, 48172, 24420, 2316, 570, 309, 4140, 300, 264, 2316, 632, 51298], "temperature": 0.0, "avg_logprob": -0.1470547546575099, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0006872045341879129}, {"id": 290, "seek": 149884, "start": 1517.52, "end": 1523.28, "text": " enormous knowledge of language and it could generalize very quickly. So unlike the sort", "tokens": [51298, 11322, 3601, 295, 2856, 293, 309, 727, 2674, 1125, 588, 2661, 13, 407, 8343, 264, 1333, 51586], "temperature": 0.0, "avg_logprob": -0.1470547546575099, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0006872045341879129}, {"id": 291, "seek": 152328, "start": 1523.32, "end": 1530.32, "text": " of the standard old days of supervised learning where it was kind of, well, if you give me", "tokens": [50366, 295, 264, 3832, 1331, 1708, 295, 46533, 2539, 689, 309, 390, 733, 295, 11, 731, 11, 498, 291, 976, 385, 50716], "temperature": 0.0, "avg_logprob": -0.16928662645055892, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.033011212944984436}, {"id": 292, "seek": 152328, "start": 1531.0, "end": 1536.84, "text": " 10,000 labeled examples, I might be able to produce a halfway decent model for you, but", "tokens": [50750, 1266, 11, 1360, 21335, 5110, 11, 286, 1062, 312, 1075, 281, 5258, 257, 15461, 8681, 2316, 337, 291, 11, 457, 51042], "temperature": 0.0, "avg_logprob": -0.16928662645055892, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.033011212944984436}, {"id": 293, "seek": 152328, "start": 1536.84, "end": 1542.52, "text": " if you give me 50,000 labeled examples, it'll be a lot better. It's sort of turned into this", "tokens": [51042, 498, 291, 976, 385, 2625, 11, 1360, 21335, 5110, 11, 309, 603, 312, 257, 688, 1101, 13, 467, 311, 1333, 295, 3574, 666, 341, 51326], "temperature": 0.0, "avg_logprob": -0.16928662645055892, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.033011212944984436}, {"id": 294, "seek": 152328, "start": 1542.52, "end": 1549.04, "text": " world of, well, if you give me 100 labeled examples and I'm fine tuning a large language", "tokens": [51326, 1002, 295, 11, 731, 11, 498, 291, 976, 385, 2319, 21335, 5110, 293, 286, 478, 2489, 15164, 257, 2416, 2856, 51652], "temperature": 0.0, "avg_logprob": -0.16928662645055892, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.033011212944984436}, {"id": 295, "seek": 154904, "start": 1549.04, "end": 1553.96, "text": " model, I'll be able to do great, better than I would be able to do with the 50,000 examples", "tokens": [50364, 2316, 11, 286, 603, 312, 1075, 281, 360, 869, 11, 1101, 813, 286, 576, 312, 1075, 281, 360, 365, 264, 2625, 11, 1360, 5110, 50610], "temperature": 0.0, "avg_logprob": -0.14222071387551047, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.007693124935030937}, {"id": 296, "seek": 154904, "start": 1553.96, "end": 1559.48, "text": " in the old world. Some of the more recent, exciting works now even going beyond that,", "tokens": [50610, 294, 264, 1331, 1002, 13, 2188, 295, 264, 544, 5162, 11, 4670, 1985, 586, 754, 516, 4399, 300, 11, 50886], "temperature": 0.0, "avg_logprob": -0.14222071387551047, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.007693124935030937}, {"id": 297, "seek": 154904, "start": 1559.48, "end": 1563.36, "text": " it's now, well, maybe you don't actually have to fine tune the model at all. So people have", "tokens": [50886, 309, 311, 586, 11, 731, 11, 1310, 291, 500, 380, 767, 362, 281, 2489, 10864, 264, 2316, 412, 439, 13, 407, 561, 362, 51080], "temperature": 0.0, "avg_logprob": -0.14222071387551047, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.007693124935030937}, {"id": 298, "seek": 154904, "start": 1563.36, "end": 1569.36, "text": " done a lot of work using methods sometimes referred to as prompting or instruction where", "tokens": [51080, 1096, 257, 688, 295, 589, 1228, 7150, 2171, 10839, 281, 382, 12391, 278, 420, 10951, 689, 51380], "temperature": 0.0, "avg_logprob": -0.14222071387551047, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.007693124935030937}, {"id": 299, "seek": 154904, "start": 1569.36, "end": 1576.36, "text": " you can simply in natural language, perhaps with examples, perhaps with explicit instructions,", "tokens": [51380, 291, 393, 2935, 294, 3303, 2856, 11, 4317, 365, 5110, 11, 4317, 365, 13691, 9415, 11, 51730], "temperature": 0.0, "avg_logprob": -0.14222071387551047, "compression_ratio": 1.6840148698884758, "no_speech_prob": 0.007693124935030937}, {"id": 300, "seek": 157636, "start": 1576.4399999999998, "end": 1582.24, "text": " just tell the model what you want it to do and it does it. Which, even as someone who's", "tokens": [50368, 445, 980, 264, 2316, 437, 291, 528, 309, 281, 360, 293, 309, 775, 309, 13, 3013, 11, 754, 382, 1580, 567, 311, 50658], "temperature": 0.0, "avg_logprob": -0.1910353357141668, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0022498529870063066}, {"id": 301, "seek": 157636, "start": 1582.24, "end": 1588.56, "text": " been working in natural language processing for 30 years, it actually just blows my mind", "tokens": [50658, 668, 1364, 294, 3303, 2856, 9007, 337, 2217, 924, 11, 309, 767, 445, 18458, 452, 1575, 50974], "temperature": 0.0, "avg_logprob": -0.1910353357141668, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0022498529870063066}, {"id": 302, "seek": 157636, "start": 1588.56, "end": 1595.56, "text": " how well this works. I guess I wasn't a decade ago thinking that in now we'd be able to", "tokens": [50974, 577, 731, 341, 1985, 13, 286, 2041, 286, 2067, 380, 257, 10378, 2057, 1953, 300, 294, 586, 321, 1116, 312, 1075, 281, 51324], "temperature": 0.0, "avg_logprob": -0.1910353357141668, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0022498529870063066}, {"id": 303, "seek": 157636, "start": 1597.6, "end": 1604.6, "text": " just tell the model, I want you to summarize this piece of text here and there will be", "tokens": [51426, 445, 980, 264, 2316, 11, 286, 528, 291, 281, 20858, 341, 2522, 295, 2487, 510, 293, 456, 486, 312, 51776], "temperature": 0.0, "avg_logprob": -0.1910353357141668, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.0022498529870063066}, {"id": 304, "seek": 160636, "start": 1606.36, "end": 1613.36, "text": " then summarize it. I think that's incredible. So we're in this very exciting time where", "tokens": [50364, 550, 20858, 309, 13, 286, 519, 300, 311, 4651, 13, 407, 321, 434, 294, 341, 588, 4670, 565, 689, 50714], "temperature": 0.0, "avg_logprob": -0.156744122505188, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007528528803959489}, {"id": 305, "seek": 160636, "start": 1613.9199999999998, "end": 1620.6, "text": " a lot of new natural language capabilities are unfolding. I think there's just no doubt", "tokens": [50742, 257, 688, 295, 777, 3303, 2856, 10862, 366, 44586, 13, 286, 519, 456, 311, 445, 572, 6385, 51076], "temperature": 0.0, "avg_logprob": -0.156744122505188, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007528528803959489}, {"id": 306, "seek": 160636, "start": 1620.6, "end": 1627.6, "text": " at all for the next couple of years the future of that is extremely bright as people work", "tokens": [51076, 412, 439, 337, 264, 958, 1916, 295, 924, 264, 2027, 295, 300, 307, 4664, 4730, 382, 561, 589, 51426], "temperature": 0.0, "avg_logprob": -0.156744122505188, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007528528803959489}, {"id": 307, "seek": 160636, "start": 1628.08, "end": 1633.3999999999999, "text": " out different things and different ways to do things and people start to apply in different", "tokens": [51450, 484, 819, 721, 293, 819, 2098, 281, 360, 721, 293, 561, 722, 281, 3079, 294, 819, 51716], "temperature": 0.0, "avg_logprob": -0.156744122505188, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0007528528803959489}, {"id": 308, "seek": 163340, "start": 1633.44, "end": 1639.5600000000002, "text": " application areas, the kind of capabilities that have been unlocked with recent technological", "tokens": [50366, 3861, 3179, 11, 264, 733, 295, 10862, 300, 362, 668, 30180, 365, 5162, 18439, 50672], "temperature": 0.0, "avg_logprob": -0.1919234226911496, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0014797509647905827}, {"id": 309, "seek": 163340, "start": 1639.5600000000002, "end": 1645.5600000000002, "text": " developments. There's always a question in technology is to sort of whether the curve", "tokens": [50672, 20862, 13, 821, 311, 1009, 257, 1168, 294, 2899, 307, 281, 1333, 295, 1968, 264, 7605, 50972], "temperature": 0.0, "avg_logprob": -0.1919234226911496, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0014797509647905827}, {"id": 310, "seek": 163340, "start": 1645.5600000000002, "end": 1650.4, "text": " keeps on heading steeply upwards or whether there's then some new things we have to discover", "tokens": [50972, 5965, 322, 9864, 16841, 356, 22167, 420, 1968, 456, 311, 550, 512, 777, 721, 321, 362, 281, 4411, 51214], "temperature": 0.0, "avg_logprob": -0.1919234226911496, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0014797509647905827}, {"id": 311, "seek": 163340, "start": 1650.4, "end": 1657.24, "text": " how to do. It's been going up for quite a while. So hopefully extrapolation is always", "tokens": [51214, 577, 281, 360, 13, 467, 311, 668, 516, 493, 337, 1596, 257, 1339, 13, 407, 4696, 48224, 399, 307, 1009, 51556], "temperature": 0.0, "avg_logprob": -0.1919234226911496, "compression_ratio": 1.5911111111111111, "no_speech_prob": 0.0014797509647905827}, {"id": 312, "seek": 165724, "start": 1657.24, "end": 1664.24, "text": " dangerous but we'll see. You mentioned writing prompts. It's still the NLP system, the large", "tokens": [50364, 5795, 457, 321, 603, 536, 13, 509, 2835, 3579, 41095, 13, 467, 311, 920, 264, 426, 45196, 1185, 11, 264, 2416, 50714], "temperature": 0.0, "avg_logprob": -0.25220485833974987, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.009110653772950172}, {"id": 313, "seek": 165724, "start": 1665.4, "end": 1670.1200000000001, "text": " language model, what you want and it seems to magically do it. I'm curious, do you think", "tokens": [50772, 2856, 2316, 11, 437, 291, 528, 293, 309, 2544, 281, 39763, 360, 309, 13, 286, 478, 6369, 11, 360, 291, 519, 51008], "temperature": 0.0, "avg_logprob": -0.25220485833974987, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.009110653772950172}, {"id": 314, "seek": 165724, "start": 1670.1200000000001, "end": 1676.32, "text": " prompt engineering is the path of the future where actually when I write these prompts", "tokens": [51008, 12391, 7043, 307, 264, 3100, 295, 264, 2027, 689, 767, 562, 286, 2464, 613, 41095, 51318], "temperature": 0.0, "avg_logprob": -0.25220485833974987, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.009110653772950172}, {"id": 315, "seek": 165724, "start": 1676.32, "end": 1681.36, "text": " I sometimes find it works miraculously and sometimes it's frustrating. The process of", "tokens": [51318, 286, 2171, 915, 309, 1985, 30686, 25038, 293, 2171, 309, 311, 16522, 13, 440, 1399, 295, 51570], "temperature": 0.0, "avg_logprob": -0.25220485833974987, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.009110653772950172}, {"id": 316, "seek": 165724, "start": 1681.36, "end": 1686.16, "text": " re-wording my instructions to tweak the wording to get it just right to generate the result", "tokens": [51570, 319, 12, 7462, 278, 452, 9415, 281, 29879, 264, 47602, 281, 483, 309, 445, 558, 281, 8460, 264, 1874, 51810], "temperature": 0.0, "avg_logprob": -0.25220485833974987, "compression_ratio": 1.6457564575645756, "no_speech_prob": 0.009110653772950172}, {"id": 317, "seek": 168616, "start": 1686.2, "end": 1690.88, "text": " I want. So do you think prompt engineering is the way of the future or do you think it's", "tokens": [50366, 286, 528, 13, 407, 360, 291, 519, 12391, 7043, 307, 264, 636, 295, 264, 2027, 420, 360, 291, 519, 309, 311, 50600], "temperature": 0.0, "avg_logprob": -0.15925628874037, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.001408028183504939}, {"id": 318, "seek": 168616, "start": 1690.88, "end": 1696.88, "text": " an intermediate hack until someone invents a better way to control the outputs of these", "tokens": [50600, 364, 19376, 10339, 1826, 1580, 1048, 791, 257, 1101, 636, 281, 1969, 264, 23930, 295, 613, 50900], "temperature": 0.0, "avg_logprob": -0.15925628874037, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.001408028183504939}, {"id": 319, "seek": 168616, "start": 1696.88, "end": 1703.88, "text": " systems? I think it's both. I think it will be the way of the future but I also think", "tokens": [50900, 3652, 30, 286, 519, 309, 311, 1293, 13, 286, 519, 309, 486, 312, 264, 636, 295, 264, 2027, 457, 286, 611, 519, 51250], "temperature": 0.0, "avg_logprob": -0.15925628874037, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.001408028183504939}, {"id": 320, "seek": 168616, "start": 1706.8400000000001, "end": 1713.8400000000001, "text": " at the moment people are doing a lot of hacking around and re-wording to try and get things", "tokens": [51398, 412, 264, 1623, 561, 366, 884, 257, 688, 295, 31422, 926, 293, 319, 12, 7462, 278, 281, 853, 293, 483, 721, 51748], "temperature": 0.0, "avg_logprob": -0.15925628874037, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.001408028183504939}, {"id": 321, "seek": 171384, "start": 1714.76, "end": 1721.76, "text": " to work better. With any luck with a few more years of development that will start to go", "tokens": [50410, 281, 589, 1101, 13, 2022, 604, 3668, 365, 257, 1326, 544, 924, 295, 3250, 300, 486, 722, 281, 352, 50760], "temperature": 0.0, "avg_logprob": -0.1882565223564536, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010621814290061593}, {"id": 322, "seek": 171384, "start": 1722.6, "end": 1729.6, "text": " away. One way to think about the difference is in comparison to the kind of voice assistance", "tokens": [50802, 1314, 13, 1485, 636, 281, 519, 466, 264, 2649, 307, 294, 9660, 281, 264, 733, 295, 3177, 9683, 51152], "temperature": 0.0, "avg_logprob": -0.1882565223564536, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010621814290061593}, {"id": 323, "seek": 171384, "start": 1731.8, "end": 1738.8, "text": " or virtual assistance that are available on phones and speaker devices like Amazon Alexa", "tokens": [51262, 420, 6374, 9683, 300, 366, 2435, 322, 10216, 293, 8145, 5759, 411, 6795, 22595, 51612], "temperature": 0.0, "avg_logprob": -0.1882565223564536, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010621814290061593}, {"id": 324, "seek": 173880, "start": 1739.12, "end": 1746.12, "text": " these days. I think all of us have had the experience that present those devices aren't", "tokens": [50380, 613, 1708, 13, 286, 519, 439, 295, 505, 362, 632, 264, 1752, 300, 1974, 729, 5759, 3212, 380, 50730], "temperature": 0.0, "avg_logprob": -0.1713498374012029, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.028791697695851326}, {"id": 325, "seek": 173880, "start": 1747.2, "end": 1751.72, "text": " always great but if you know the right way to word things it will do something but if", "tokens": [50784, 1009, 869, 457, 498, 291, 458, 264, 558, 636, 281, 1349, 721, 309, 486, 360, 746, 457, 498, 51010], "temperature": 0.0, "avg_logprob": -0.1713498374012029, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.028791697695851326}, {"id": 326, "seek": 173880, "start": 1751.72, "end": 1758.72, "text": " you use the wrong wording it won't. The difference with human beings is by and large you don't", "tokens": [51010, 291, 764, 264, 2085, 47602, 309, 1582, 380, 13, 440, 2649, 365, 1952, 8958, 307, 538, 293, 2416, 291, 500, 380, 51360], "temperature": 0.0, "avg_logprob": -0.1713498374012029, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.028791697695851326}, {"id": 327, "seek": 173880, "start": 1758.72, "end": 1762.52, "text": " have to think about that. You can say what you want and it doesn't matter what word you", "tokens": [51360, 362, 281, 519, 466, 300, 13, 509, 393, 584, 437, 291, 528, 293, 309, 1177, 380, 1871, 437, 1349, 291, 51550], "temperature": 0.0, "avg_logprob": -0.1713498374012029, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.028791697695851326}, {"id": 328, "seek": 173880, "start": 1762.52, "end": 1768.72, "text": " choose. The other human being assuming it's someone who knows the same language etc. Well", "tokens": [51550, 2826, 13, 440, 661, 1952, 885, 11926, 309, 311, 1580, 567, 3255, 264, 912, 2856, 5183, 13, 1042, 51860], "temperature": 0.0, "avg_logprob": -0.1713498374012029, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.028791697695851326}, {"id": 329, "seek": 176872, "start": 1768.92, "end": 1775.92, "text": " understand you and do what you want. I think and would hope that we'll start to see the", "tokens": [50374, 1223, 291, 293, 360, 437, 291, 528, 13, 286, 519, 293, 576, 1454, 300, 321, 603, 722, 281, 536, 264, 50724], "temperature": 0.0, "avg_logprob": -0.16310992351798123, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0005970114143565297}, {"id": 330, "seek": 176872, "start": 1776.2, "end": 1781.04, "text": " same kind of progression with these models that at the moment fiddling around with the", "tokens": [50738, 912, 733, 295, 18733, 365, 613, 5245, 300, 412, 264, 1623, 283, 14273, 1688, 926, 365, 264, 50980], "temperature": 0.0, "avg_logprob": -0.16310992351798123, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0005970114143565297}, {"id": 331, "seek": 176872, "start": 1781.04, "end": 1786.44, "text": " particular wording you use can make a very big difference to how well it works but hopefully", "tokens": [50980, 1729, 47602, 291, 764, 393, 652, 257, 588, 955, 2649, 281, 577, 731, 309, 1985, 457, 4696, 51250], "temperature": 0.0, "avg_logprob": -0.16310992351798123, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0005970114143565297}, {"id": 332, "seek": 176872, "start": 1786.44, "end": 1791.88, "text": " in a few years time that just won't be true. You'll be able to use different wordings and", "tokens": [51250, 294, 257, 1326, 924, 565, 300, 445, 1582, 380, 312, 2074, 13, 509, 603, 312, 1075, 281, 764, 819, 1349, 1109, 293, 51522], "temperature": 0.0, "avg_logprob": -0.16310992351798123, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0005970114143565297}, {"id": 333, "seek": 179188, "start": 1791.92, "end": 1798.92, "text": " it'll still work but the basic idea that we're moving into this age where actually human", "tokens": [50366, 309, 603, 920, 589, 457, 264, 3875, 1558, 300, 321, 434, 2684, 666, 341, 3205, 689, 767, 1952, 50716], "temperature": 0.0, "avg_logprob": -0.1649987094373588, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.5101561546325684}, {"id": 334, "seek": 179188, "start": 1799.44, "end": 1806.44, "text": " language will be able to be used as an instruction language to tell your computer what to do so", "tokens": [50742, 2856, 486, 312, 1075, 281, 312, 1143, 382, 364, 10951, 2856, 281, 980, 428, 3820, 437, 281, 360, 370, 51092], "temperature": 0.0, "avg_logprob": -0.1649987094373588, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.5101561546325684}, {"id": 335, "seek": 179188, "start": 1806.72, "end": 1813.72, "text": " instead of having to use menus and radio buttons and things like that or writing Python code", "tokens": [51106, 2602, 295, 1419, 281, 764, 30347, 293, 6477, 9905, 293, 721, 411, 300, 420, 3579, 15329, 3089, 51456], "temperature": 0.0, "avg_logprob": -0.1649987094373588, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.5101561546325684}, {"id": 336, "seek": 179188, "start": 1815.68, "end": 1820.24, "text": " instead of either of those things that you'll be able to say what you want in the computer", "tokens": [51554, 2602, 295, 2139, 295, 729, 721, 300, 291, 603, 312, 1075, 281, 584, 437, 291, 528, 294, 264, 3820, 51782], "temperature": 0.0, "avg_logprob": -0.1649987094373588, "compression_ratio": 1.7607655502392345, "no_speech_prob": 0.5101561546325684}, {"id": 337, "seek": 182024, "start": 1820.28, "end": 1826.8, "text": " or do it. I think that age is opening up in front of us that will continue to build and", "tokens": [50366, 420, 360, 309, 13, 286, 519, 300, 3205, 307, 5193, 493, 294, 1868, 295, 505, 300, 486, 2354, 281, 1322, 293, 50692], "temperature": 0.0, "avg_logprob": -0.19172615228697312, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011825629509985447}, {"id": 338, "seek": 182024, "start": 1826.8, "end": 1833.8, "text": " that will be hugely transformative. It feels like come a long ways but only much more to", "tokens": [50692, 300, 486, 312, 27417, 36070, 13, 467, 3417, 411, 808, 257, 938, 2098, 457, 787, 709, 544, 281, 51042], "temperature": 0.0, "avg_logprob": -0.19172615228697312, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011825629509985447}, {"id": 339, "seek": 182024, "start": 1834.72, "end": 1839.92, "text": " come and much more to go. Absolutely. In the development of NLP technology there's one", "tokens": [51088, 808, 293, 709, 544, 281, 352, 13, 7021, 13, 682, 264, 3250, 295, 426, 45196, 2899, 456, 311, 472, 51348], "temperature": 0.0, "avg_logprob": -0.19172615228697312, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011825629509985447}, {"id": 340, "seek": 182024, "start": 1839.92, "end": 1843.64, "text": " thing I want to ask you and I suspect you and I may have different perspectives on this", "tokens": [51348, 551, 286, 528, 281, 1029, 291, 293, 286, 9091, 291, 293, 286, 815, 362, 819, 16766, 322, 341, 51534], "temperature": 0.0, "avg_logprob": -0.19172615228697312, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.011825629509985447}, {"id": 341, "seek": 184364, "start": 1843.72, "end": 1850.72, "text": " but in the last couple of decades the trend has been to rely less on rule based engineering", "tokens": [50368, 457, 294, 264, 1036, 1916, 295, 7878, 264, 6028, 575, 668, 281, 10687, 1570, 322, 4978, 2361, 7043, 50718], "temperature": 0.0, "avg_logprob": -0.17725081502655407, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0007785062189213932}, {"id": 342, "seek": 184364, "start": 1850.72, "end": 1857.24, "text": " and more on machine learning on data sometimes lots of data. Looking to the future where", "tokens": [50718, 293, 544, 322, 3479, 2539, 322, 1412, 2171, 3195, 295, 1412, 13, 11053, 281, 264, 2027, 689, 51044], "temperature": 0.0, "avg_logprob": -0.17725081502655407, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0007785062189213932}, {"id": 343, "seek": 184364, "start": 1857.24, "end": 1864.24, "text": " do you think that mix of hand coded constraints or other constraints explicit constraints versus", "tokens": [51044, 360, 291, 519, 300, 2890, 295, 1011, 34874, 18491, 420, 661, 18491, 13691, 18491, 5717, 51394], "temperature": 0.0, "avg_logprob": -0.17725081502655407, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0007785062189213932}, {"id": 344, "seek": 184364, "start": 1864.24, "end": 1868.0400000000002, "text": " you know let's get a neural network and throw lots of data at it. Where do you think that", "tokens": [51394, 291, 458, 718, 311, 483, 257, 18161, 3209, 293, 3507, 3195, 295, 1412, 412, 309, 13, 2305, 360, 291, 519, 300, 51584], "temperature": 0.0, "avg_logprob": -0.17725081502655407, "compression_ratio": 1.7729468599033817, "no_speech_prob": 0.0007785062189213932}, {"id": 345, "seek": 186804, "start": 1868.12, "end": 1875.12, "text": " balance will fall? I think that there's no doubt that using learning from data is the", "tokens": [50368, 4772, 486, 2100, 30, 286, 519, 300, 456, 311, 572, 6385, 300, 1228, 2539, 490, 1412, 307, 264, 50718], "temperature": 0.0, "avg_logprob": -0.14463669703556942, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.004888928961008787}, {"id": 346, "seek": 186804, "start": 1878.32, "end": 1885.32, "text": " way forward and what we're going to continue to do but I think there's still a space for", "tokens": [50878, 636, 2128, 293, 437, 321, 434, 516, 281, 2354, 281, 360, 457, 286, 519, 456, 311, 920, 257, 1901, 337, 51228], "temperature": 0.0, "avg_logprob": -0.14463669703556942, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.004888928961008787}, {"id": 347, "seek": 186804, "start": 1885.6, "end": 1892.6, "text": " models that have more structure, more inductive bias that have some kind of basis of exploiting", "tokens": [51242, 5245, 300, 362, 544, 3877, 11, 544, 31612, 488, 12577, 300, 362, 512, 733, 295, 5143, 295, 12382, 1748, 51592], "temperature": 0.0, "avg_logprob": -0.14463669703556942, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.004888928961008787}, {"id": 348, "seek": 189260, "start": 1893.36, "end": 1900.1999999999998, "text": " the nature of language. So in recent years the model that's been enormously successful", "tokens": [50402, 264, 3687, 295, 2856, 13, 407, 294, 5162, 924, 264, 2316, 300, 311, 668, 39669, 4406, 50744], "temperature": 0.0, "avg_logprob": -0.31449672154017855, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.007906720042228699}, {"id": 349, "seek": 189260, "start": 1900.1999999999998, "end": 1907.1999999999998, "text": " is the transform in your network and the transform in your network is essentially this huge association", "tokens": [50744, 307, 264, 4088, 294, 428, 3209, 293, 264, 4088, 294, 428, 3209, 307, 4476, 341, 2603, 14598, 51094], "temperature": 0.0, "avg_logprob": -0.31449672154017855, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.007906720042228699}, {"id": 350, "seek": 189260, "start": 1907.1999999999998, "end": 1913.7199999999998, "text": " machine so it'll just suck associations from anywhere and look at two words and figure", "tokens": [51094, 3479, 370, 309, 603, 445, 9967, 26597, 490, 4992, 293, 574, 412, 732, 2283, 293, 2573, 51420], "temperature": 0.0, "avg_logprob": -0.31449672154017855, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.007906720042228699}, {"id": 351, "seek": 189260, "start": 1913.7199999999998, "end": 1917.76, "text": " out which words release and which other words for all words. Yes so you use everything to", "tokens": [51420, 484, 597, 2283, 4374, 293, 597, 661, 2283, 337, 439, 2283, 13, 1079, 370, 291, 764, 1203, 281, 51622], "temperature": 0.0, "avg_logprob": -0.31449672154017855, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.007906720042228699}, {"id": 352, "seek": 191776, "start": 1917.76, "end": 1923.52, "text": " predict anything and do it over and over again and you'll get anything you want and you know", "tokens": [50364, 6069, 1340, 293, 360, 309, 670, 293, 670, 797, 293, 291, 603, 483, 1340, 291, 528, 293, 291, 458, 50652], "temperature": 0.0, "avg_logprob": -0.16412667383121538, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0056375483982264996}, {"id": 353, "seek": 191776, "start": 1923.52, "end": 1930.52, "text": " that's been incredibly, incredibly successful but it's been incredibly successful in the", "tokens": [50652, 300, 311, 668, 6252, 11, 6252, 4406, 457, 309, 311, 668, 6252, 4406, 294, 264, 51002], "temperature": 0.0, "avg_logprob": -0.16412667383121538, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0056375483982264996}, {"id": 354, "seek": 191776, "start": 1930.56, "end": 1937.56, "text": " domain where you have humongous, humongous amounts of data right so that these transformer", "tokens": [51004, 9274, 689, 291, 362, 1484, 556, 563, 11, 1484, 556, 563, 11663, 295, 1412, 558, 370, 300, 613, 31782, 51354], "temperature": 0.0, "avg_logprob": -0.16412667383121538, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0056375483982264996}, {"id": 355, "seek": 191776, "start": 1937.8, "end": 1942.8799999999999, "text": " models for these large language models are now being trained on tens of billions of words", "tokens": [51366, 5245, 337, 613, 2416, 2856, 5245, 366, 586, 885, 8895, 322, 10688, 295, 17375, 295, 2283, 51620], "temperature": 0.0, "avg_logprob": -0.16412667383121538, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0056375483982264996}, {"id": 356, "seek": 194288, "start": 1942.96, "end": 1949.4, "text": " of text. When I started off in statistical natural language processing and some of the", "tokens": [50368, 295, 2487, 13, 1133, 286, 1409, 766, 294, 22820, 3303, 2856, 9007, 293, 512, 295, 264, 50690], "temperature": 0.0, "avg_logprob": -0.14111925100351308, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.004707525949925184}, {"id": 357, "seek": 194288, "start": 1949.4, "end": 1955.64, "text": " traditional linguists used to complain about the fact that I was collecting statistics", "tokens": [50690, 5164, 21766, 1751, 1143, 281, 11024, 466, 264, 1186, 300, 286, 390, 12510, 12523, 51002], "temperature": 0.0, "avg_logprob": -0.14111925100351308, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.004707525949925184}, {"id": 358, "seek": 194288, "start": 1955.64, "end": 1962.5600000000002, "text": " from 30 million words of newswire and building a predictive model and thought that was just", "tokens": [51002, 490, 2217, 2459, 2283, 295, 2583, 42689, 293, 2390, 257, 35521, 2316, 293, 1194, 300, 390, 445, 51348], "temperature": 0.0, "avg_logprob": -0.14111925100351308, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.004707525949925184}, {"id": 359, "seek": 194288, "start": 1962.5600000000002, "end": 1969.5600000000002, "text": " not what linguistics was about. I felt I had a perfectly good answer which is that a human", "tokens": [51348, 406, 437, 21766, 6006, 390, 466, 13, 286, 2762, 286, 632, 257, 6239, 665, 1867, 597, 307, 300, 257, 1952, 51698], "temperature": 0.0, "avg_logprob": -0.14111925100351308, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.004707525949925184}, {"id": 360, "seek": 196956, "start": 1970.56, "end": 1977.56, "text": " kid as their learning language they're exposed to actually well more than 30 million words", "tokens": [50414, 1636, 382, 641, 2539, 2856, 436, 434, 9495, 281, 767, 731, 544, 813, 2217, 2459, 2283, 50764], "temperature": 0.0, "avg_logprob": -0.1852452966231334, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0022492376156151295}, {"id": 361, "seek": 196956, "start": 1977.9199999999998, "end": 1982.8799999999999, "text": " of data but you know that kind of amount of data so you know the kind of amount of data", "tokens": [50782, 295, 1412, 457, 291, 458, 300, 733, 295, 2372, 295, 1412, 370, 291, 458, 264, 733, 295, 2372, 295, 1412, 51030], "temperature": 0.0, "avg_logprob": -0.1852452966231334, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0022492376156151295}, {"id": 362, "seek": 196956, "start": 1982.8799999999999, "end": 1989.6399999999999, "text": " we were using were perfectly reasonable amounts of data to be using to be you know not exactly", "tokens": [51030, 321, 645, 1228, 645, 6239, 10585, 11663, 295, 1412, 281, 312, 1228, 281, 312, 291, 458, 406, 2293, 51368], "temperature": 0.0, "avg_logprob": -0.1852452966231334, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0022492376156151295}, {"id": 363, "seek": 196956, "start": 1989.6399999999999, "end": 1994.24, "text": " trying to model human language acquisition but to be thinking about how we can learn", "tokens": [51368, 1382, 281, 2316, 1952, 2856, 21668, 457, 281, 312, 1953, 466, 577, 321, 393, 1466, 51598], "temperature": 0.0, "avg_logprob": -0.1852452966231334, "compression_ratio": 1.7989949748743719, "no_speech_prob": 0.0022492376156151295}, {"id": 364, "seek": 199424, "start": 1994.24, "end": 2001.24, "text": " about language from lots of data. But you know these modern transformers are now you", "tokens": [50364, 466, 2856, 490, 3195, 295, 1412, 13, 583, 291, 458, 613, 4363, 4088, 433, 366, 586, 291, 50714], "temperature": 0.0, "avg_logprob": -0.14068504526645323, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.01851918362081051}, {"id": 365, "seek": 199424, "start": 2003.48, "end": 2010.24, "text": " know using already at least two orders of magnitude more data and you know most people", "tokens": [50826, 458, 1228, 1217, 412, 1935, 732, 9470, 295, 15668, 544, 1412, 293, 291, 458, 881, 561, 51164], "temperature": 0.0, "avg_logprob": -0.14068504526645323, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.01851918362081051}, {"id": 366, "seek": 199424, "start": 2010.24, "end": 2017.0, "text": " think the way to get things to the next level is to use more still and make it three orders", "tokens": [51164, 519, 264, 636, 281, 483, 721, 281, 264, 958, 1496, 307, 281, 764, 544, 920, 293, 652, 309, 1045, 9470, 51502], "temperature": 0.0, "avg_logprob": -0.14068504526645323, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.01851918362081051}, {"id": 367, "seek": 199424, "start": 2017.0, "end": 2022.52, "text": " of magnitude and you know in one sense that scaling up strategy has been hugely effective", "tokens": [51502, 295, 15668, 293, 291, 458, 294, 472, 2020, 300, 21589, 493, 5206, 575, 668, 27417, 4942, 51778], "temperature": 0.0, "avg_logprob": -0.14068504526645323, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.01851918362081051}, {"id": 368, "seek": 202252, "start": 2022.6399999999999, "end": 2027.08, "text": " so you know I don't blame anybody for saying let's make another order magnitude bigger and", "tokens": [50370, 370, 291, 458, 286, 500, 380, 10127, 4472, 337, 1566, 718, 311, 652, 1071, 1668, 15668, 3801, 293, 50592], "temperature": 0.0, "avg_logprob": -0.12807154655456543, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0008425273117609322}, {"id": 369, "seek": 202252, "start": 2027.08, "end": 2034.08, "text": " see what amazing things we can do but it also shows that human learning is just way way", "tokens": [50592, 536, 437, 2243, 721, 321, 393, 360, 457, 309, 611, 3110, 300, 1952, 2539, 307, 445, 636, 636, 50942], "temperature": 0.0, "avg_logprob": -0.12807154655456543, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0008425273117609322}, {"id": 370, "seek": 202252, "start": 2037.16, "end": 2042.92, "text": " better in being able to extract a lot more information out of a quite limited amount", "tokens": [51096, 1101, 294, 885, 1075, 281, 8947, 257, 688, 544, 1589, 484, 295, 257, 1596, 5567, 2372, 51384], "temperature": 0.0, "avg_logprob": -0.12807154655456543, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0008425273117609322}, {"id": 371, "seek": 202252, "start": 2042.92, "end": 2048.84, "text": " of data and at that point you can have various hypotheses but I think it's reasonable to", "tokens": [51384, 295, 1412, 293, 412, 300, 935, 291, 393, 362, 3683, 49969, 457, 286, 519, 309, 311, 10585, 281, 51680], "temperature": 0.0, "avg_logprob": -0.12807154655456543, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.0008425273117609322}, {"id": 372, "seek": 204884, "start": 2048.88, "end": 2055.88, "text": " assume that human learning is somewhat structured towards the structure of the world and things", "tokens": [50366, 6552, 300, 1952, 2539, 307, 8344, 18519, 3030, 264, 3877, 295, 264, 1002, 293, 721, 50716], "temperature": 0.0, "avg_logprob": -0.22036821465743214, "compression_ratio": 1.8860759493670887, "no_speech_prob": 0.0016709158662706614}, {"id": 373, "seek": 204884, "start": 2057.44, "end": 2062.44, "text": " that sees in the world and that allows it to learn more quickly from less data.", "tokens": [50794, 300, 8194, 294, 264, 1002, 293, 300, 4045, 309, 281, 1466, 544, 2661, 490, 1570, 1412, 13, 51044], "temperature": 0.0, "avg_logprob": -0.22036821465743214, "compression_ratio": 1.8860759493670887, "no_speech_prob": 0.0016709158662706614}, {"id": 374, "seek": 204884, "start": 2062.44, "end": 2066.84, "text": " Alright I'll move you on that. I think better learning algorithms, our current machine learning", "tokens": [51044, 2798, 286, 603, 1286, 291, 322, 300, 13, 286, 519, 1101, 2539, 14642, 11, 527, 2190, 3479, 2539, 51264], "temperature": 0.0, "avg_logprob": -0.22036821465743214, "compression_ratio": 1.8860759493670887, "no_speech_prob": 0.0016709158662706614}, {"id": 375, "seek": 204884, "start": 2066.84, "end": 2071.76, "text": " algorithms are much less efficient or makes much less efficient use of data and so there's", "tokens": [51264, 14642, 366, 709, 1570, 7148, 420, 1669, 709, 1570, 7148, 764, 295, 1412, 293, 370, 456, 311, 51510], "temperature": 0.0, "avg_logprob": -0.22036821465743214, "compression_ratio": 1.8860759493670887, "no_speech_prob": 0.0016709158662706614}, {"id": 376, "seek": 204884, "start": 2071.76, "end": 2077.44, "text": " way more data than any you know child and then I think whether the improved learning", "tokens": [51510, 636, 544, 1412, 813, 604, 291, 458, 1440, 293, 550, 286, 519, 1968, 264, 9689, 2539, 51794], "temperature": 0.0, "avg_logprob": -0.22036821465743214, "compression_ratio": 1.8860759493670887, "no_speech_prob": 0.0016709158662706614}, {"id": 377, "seek": 207744, "start": 2077.48, "end": 2083.52, "text": " algorithms will be from linguistic like rules or whether it'll just be engineers engineering", "tokens": [50366, 14642, 486, 312, 490, 43002, 411, 4474, 420, 1968, 309, 603, 445, 312, 11955, 7043, 50668], "temperature": 0.0, "avg_logprob": -0.22697554694281685, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.003477273741737008}, {"id": 378, "seek": 207744, "start": 2083.52, "end": 2090.52, "text": " much more efficient versions of the transform or whatever comes after it. That will be traditional.", "tokens": [50668, 709, 544, 7148, 9606, 295, 264, 4088, 420, 2035, 1487, 934, 309, 13, 663, 486, 312, 5164, 13, 51018], "temperature": 0.0, "avg_logprob": -0.22697554694281685, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.003477273741737008}, {"id": 379, "seek": 207744, "start": 2090.88, "end": 2097.88, "text": " I don't think it'll be by people explicitly putting traditional linguistic rules into", "tokens": [51036, 286, 500, 380, 519, 309, 603, 312, 538, 561, 20803, 3372, 5164, 43002, 4474, 666, 51386], "temperature": 0.0, "avg_logprob": -0.22697554694281685, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.003477273741737008}, {"id": 380, "seek": 207744, "start": 2098.2000000000003, "end": 2105.2000000000003, "text": " the system. I don't think that's the way forward. On the other hand I mean you know I think", "tokens": [51402, 264, 1185, 13, 286, 500, 380, 519, 300, 311, 264, 636, 2128, 13, 1282, 264, 661, 1011, 286, 914, 291, 458, 286, 519, 51752], "temperature": 0.0, "avg_logprob": -0.22697554694281685, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.003477273741737008}, {"id": 381, "seek": 210520, "start": 2105.3199999999997, "end": 2112.3199999999997, "text": " what we're starting to see is models like these transformer models are actually discovering", "tokens": [50370, 437, 321, 434, 2891, 281, 536, 307, 5245, 411, 613, 31782, 5245, 366, 767, 24773, 50720], "temperature": 0.0, "avg_logprob": -0.15966903522450437, "compression_ratio": 1.9017094017094016, "no_speech_prob": 0.002506466582417488}, {"id": 382, "seek": 210520, "start": 2112.3199999999997, "end": 2119.3199999999997, "text": " the structure of language themselves right so you know the broad facts of you know human", "tokens": [50720, 264, 3877, 295, 2856, 2969, 558, 370, 291, 458, 264, 4152, 9130, 295, 291, 458, 1952, 51070], "temperature": 0.0, "avg_logprob": -0.15966903522450437, "compression_ratio": 1.9017094017094016, "no_speech_prob": 0.002506466582417488}, {"id": 383, "seek": 210520, "start": 2119.3199999999997, "end": 2124.08, "text": " language that you know English has the subject before the verb and the object afterwards", "tokens": [51070, 2856, 300, 291, 458, 3669, 575, 264, 3983, 949, 264, 9595, 293, 264, 2657, 10543, 51308], "temperature": 0.0, "avg_logprob": -0.15966903522450437, "compression_ratio": 1.9017094017094016, "no_speech_prob": 0.002506466582417488}, {"id": 384, "seek": 210520, "start": 2124.08, "end": 2129.04, "text": " whereas you know in Japanese that the verb at the end of the sentence and the subject", "tokens": [51308, 9735, 291, 458, 294, 5433, 300, 264, 9595, 412, 264, 917, 295, 264, 8174, 293, 264, 3983, 51556], "temperature": 0.0, "avg_logprob": -0.15966903522450437, "compression_ratio": 1.9017094017094016, "no_speech_prob": 0.002506466582417488}, {"id": 385, "seek": 210520, "start": 2129.04, "end": 2133.3599999999997, "text": " and object are normally in that order before it that could be in the other order you know", "tokens": [51556, 293, 2657, 366, 5646, 294, 300, 1668, 949, 309, 300, 727, 312, 294, 264, 661, 1668, 291, 458, 51772], "temperature": 0.0, "avg_logprob": -0.15966903522450437, "compression_ratio": 1.9017094017094016, "no_speech_prob": 0.002506466582417488}, {"id": 386, "seek": 213336, "start": 2133.36, "end": 2139.28, "text": " actually transformer models are learning these facts you can interrogate them and see", "tokens": [50364, 767, 31782, 5245, 366, 2539, 613, 9130, 291, 393, 24871, 473, 552, 293, 536, 50660], "temperature": 0.0, "avg_logprob": -0.14149726992068085, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00023776234593242407}, {"id": 387, "seek": 213336, "start": 2139.28, "end": 2143.36, "text": " that even though they were never explicitly told about subjects and objects that they", "tokens": [50660, 300, 754, 1673, 436, 645, 1128, 20803, 1907, 466, 13066, 293, 6565, 300, 436, 50864], "temperature": 0.0, "avg_logprob": -0.14149726992068085, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00023776234593242407}, {"id": 388, "seek": 213336, "start": 2143.36, "end": 2149.36, "text": " know these notions so I think they you know they're discovering a lot else as well about", "tokens": [50864, 458, 613, 35799, 370, 286, 519, 436, 291, 458, 436, 434, 24773, 257, 688, 1646, 382, 731, 466, 51164], "temperature": 0.0, "avg_logprob": -0.14149726992068085, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00023776234593242407}, {"id": 389, "seek": 213336, "start": 2149.36, "end": 2154.96, "text": " language use and context and the meanings and senses of words and what is and isn't you", "tokens": [51164, 2856, 764, 293, 4319, 293, 264, 28138, 293, 17057, 295, 2283, 293, 437, 307, 293, 1943, 380, 291, 51444], "temperature": 0.0, "avg_logprob": -0.14149726992068085, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00023776234593242407}, {"id": 390, "seek": 213336, "start": 2154.96, "end": 2160.84, "text": " know unpleasant language but part of what they're learning is the same kind of structure", "tokens": [51444, 458, 29128, 2856, 457, 644, 295, 437, 436, 434, 2539, 307, 264, 912, 733, 295, 3877, 51738], "temperature": 0.0, "avg_logprob": -0.14149726992068085, "compression_ratio": 1.8057851239669422, "no_speech_prob": 0.00023776234593242407}, {"id": 391, "seek": 216084, "start": 2160.84, "end": 2165.6800000000003, "text": " that linguists have laid out as the sort of structure of different human languages.", "tokens": [50364, 300, 21766, 1751, 362, 9897, 484, 382, 264, 1333, 295, 3877, 295, 819, 1952, 8650, 13, 50606], "temperature": 0.0, "avg_logprob": -0.28676688417475277, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.004259197972714901}, {"id": 392, "seek": 216084, "start": 2165.6800000000003, "end": 2171.8, "text": " So does it over many decades linguists discover certain things and by training on billions", "tokens": [50606, 407, 775, 309, 670, 867, 7878, 21766, 1751, 4411, 1629, 721, 293, 538, 3097, 322, 17375, 50912], "temperature": 0.0, "avg_logprob": -0.28676688417475277, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.004259197972714901}, {"id": 393, "seek": 216084, "start": 2171.8, "end": 2175.8, "text": " of words transformers are discovering the same things that linguists discovered in human", "tokens": [50912, 295, 2283, 4088, 433, 366, 24773, 264, 912, 721, 300, 21766, 1751, 6941, 294, 1952, 51112], "temperature": 0.0, "avg_logprob": -0.28676688417475277, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.004259197972714901}, {"id": 394, "seek": 216084, "start": 2175.8, "end": 2181.6800000000003, "text": " language that's that's that's cool. So all this is really exciting progress in NLP driven", "tokens": [51112, 2856, 300, 311, 300, 311, 300, 311, 1627, 13, 407, 439, 341, 307, 534, 4670, 4205, 294, 426, 45196, 9555, 51406], "temperature": 0.0, "avg_logprob": -0.28676688417475277, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.004259197972714901}, {"id": 395, "seek": 216084, "start": 2181.6800000000003, "end": 2186.88, "text": " by machine learning and by other things. To someone entering the field entering machine", "tokens": [51406, 538, 3479, 2539, 293, 538, 661, 721, 13, 1407, 1580, 11104, 264, 2519, 11104, 3479, 51666], "temperature": 0.0, "avg_logprob": -0.28676688417475277, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.004259197972714901}, {"id": 396, "seek": 218688, "start": 2186.92, "end": 2192.96, "text": " learning or AI or NLP there's just a lot going on. What advice would you have for someone", "tokens": [50366, 2539, 420, 7318, 420, 426, 45196, 456, 311, 445, 257, 688, 516, 322, 13, 708, 5192, 576, 291, 362, 337, 1580, 50668], "temperature": 0.0, "avg_logprob": -0.13780782362994026, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.014047346077859402}, {"id": 397, "seek": 218688, "start": 2192.96, "end": 2201.08, "text": " wanting to break into machine learning? Yeah well it's a great time to break in. I think", "tokens": [50668, 7935, 281, 1821, 666, 3479, 2539, 30, 865, 731, 309, 311, 257, 869, 565, 281, 1821, 294, 13, 286, 519, 51074], "temperature": 0.0, "avg_logprob": -0.13780782362994026, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.014047346077859402}, {"id": 398, "seek": 218688, "start": 2201.08, "end": 2206.84, "text": " there's just no doubt at all that we're still in the early stages of seeing the impact of", "tokens": [51074, 456, 311, 445, 572, 6385, 412, 439, 300, 321, 434, 920, 294, 264, 2440, 10232, 295, 2577, 264, 2712, 295, 51362], "temperature": 0.0, "avg_logprob": -0.13780782362994026, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.014047346077859402}, {"id": 399, "seek": 218688, "start": 2206.84, "end": 2216.0, "text": " this new approach where effectively software computer science is being reinvented in on", "tokens": [51362, 341, 777, 3109, 689, 8659, 4722, 3820, 3497, 307, 885, 33477, 292, 294, 322, 51820], "temperature": 0.0, "avg_logprob": -0.13780782362994026, "compression_ratio": 1.5682819383259912, "no_speech_prob": 0.014047346077859402}, {"id": 400, "seek": 221600, "start": 2216.0, "end": 2221.0, "text": " the basis of much more use of machine learning and the various other things that come away", "tokens": [50364, 264, 5143, 295, 709, 544, 764, 295, 3479, 2539, 293, 264, 3683, 661, 721, 300, 808, 1314, 50614], "temperature": 0.0, "avg_logprob": -0.18063748347294795, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.013367045670747757}, {"id": 401, "seek": 221600, "start": 2221.0, "end": 2226.36, "text": " from that and then more generally across industries there are just lots of opportunities for more", "tokens": [50614, 490, 300, 293, 550, 544, 5101, 2108, 13284, 456, 366, 445, 3195, 295, 4786, 337, 544, 50882], "temperature": 0.0, "avg_logprob": -0.18063748347294795, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.013367045670747757}, {"id": 402, "seek": 221600, "start": 2226.36, "end": 2233.04, "text": " automation making more use of you know interpretation of human language material for me or in other", "tokens": [50882, 17769, 1455, 544, 764, 295, 291, 458, 14174, 295, 1952, 2856, 2527, 337, 385, 420, 294, 661, 51216], "temperature": 0.0, "avg_logprob": -0.18063748347294795, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.013367045670747757}, {"id": 403, "seek": 221600, "start": 2233.04, "end": 2242.12, "text": " areas like vision and robotics or the same kinds of things. So lots of possibilities.", "tokens": [51216, 3179, 411, 5201, 293, 34145, 420, 264, 912, 3685, 295, 721, 13, 407, 3195, 295, 12178, 13, 51670], "temperature": 0.0, "avg_logprob": -0.18063748347294795, "compression_ratio": 1.7155963302752293, "no_speech_prob": 0.013367045670747757}, {"id": 404, "seek": 224212, "start": 2242.24, "end": 2247.2, "text": " So you know at that point there's lots to do obviously and you want to get some kind", "tokens": [50370, 407, 291, 458, 412, 300, 935, 456, 311, 3195, 281, 360, 2745, 293, 291, 528, 281, 483, 512, 733, 50618], "temperature": 0.0, "avg_logprob": -0.19048834482828775, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.009216059930622578}, {"id": 405, "seek": 224212, "start": 2247.2, "end": 2253.8399999999997, "text": " of good foundation right so knowing some of the core technical methods of machine learning", "tokens": [50618, 295, 665, 7030, 558, 370, 5276, 512, 295, 264, 4965, 6191, 7150, 295, 3479, 2539, 50950], "temperature": 0.0, "avg_logprob": -0.19048834482828775, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.009216059930622578}, {"id": 406, "seek": 224212, "start": 2253.8399999999997, "end": 2260.8399999999997, "text": " understanding ideas of how to build models from data look at losses do training diagnose", "tokens": [50950, 3701, 3487, 295, 577, 281, 1322, 5245, 490, 1412, 574, 412, 15352, 360, 3097, 36238, 51300], "temperature": 0.0, "avg_logprob": -0.19048834482828775, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.009216059930622578}, {"id": 407, "seek": 224212, "start": 2260.8399999999997, "end": 2268.2, "text": " errors all of these core things I mean that's definitely useful for natural language processing", "tokens": [51300, 13603, 439, 295, 613, 4965, 721, 286, 914, 300, 311, 2138, 4420, 337, 3303, 2856, 9007, 51668], "temperature": 0.0, "avg_logprob": -0.19048834482828775, "compression_ratio": 1.6143497757847534, "no_speech_prob": 0.009216059930622578}, {"id": 408, "seek": 226820, "start": 2268.2799999999997, "end": 2273.8399999999997, "text": " in particular some of those skills are completely relevant but then there are particular kinds", "tokens": [50368, 294, 1729, 512, 295, 729, 3942, 366, 2584, 7340, 457, 550, 456, 366, 1729, 3685, 50646], "temperature": 0.0, "avg_logprob": -0.10198187566065527, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.020556006580591202}, {"id": 409, "seek": 226820, "start": 2273.8399999999997, "end": 2279.2, "text": " of models that are commonly used including the transformer that we've talked about a lot", "tokens": [50646, 295, 5245, 300, 366, 12719, 1143, 3009, 264, 31782, 300, 321, 600, 2825, 466, 257, 688, 50914], "temperature": 0.0, "avg_logprob": -0.10198187566065527, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.020556006580591202}, {"id": 410, "seek": 226820, "start": 2279.2, "end": 2283.3999999999996, "text": " today you definitely should know about transformers and indeed they're increasingly being used", "tokens": [50914, 965, 291, 2138, 820, 458, 466, 4088, 433, 293, 6451, 436, 434, 12980, 885, 1143, 51124], "temperature": 0.0, "avg_logprob": -0.10198187566065527, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.020556006580591202}, {"id": 411, "seek": 226820, "start": 2283.3999999999996, "end": 2289.3199999999997, "text": " in every other part of machine learning as well for vision bioinformatics even robotics", "tokens": [51124, 294, 633, 661, 644, 295, 3479, 2539, 382, 731, 337, 5201, 12198, 37811, 30292, 754, 34145, 51420], "temperature": 0.0, "avg_logprob": -0.10198187566065527, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.020556006580591202}, {"id": 412, "seek": 226820, "start": 2289.3199999999997, "end": 2295.3199999999997, "text": " is now using transformers but beyond that I think it's also useful to learn something", "tokens": [51420, 307, 586, 1228, 4088, 433, 457, 4399, 300, 286, 519, 309, 311, 611, 4420, 281, 1466, 746, 51720], "temperature": 0.0, "avg_logprob": -0.10198187566065527, "compression_ratio": 1.779527559055118, "no_speech_prob": 0.020556006580591202}, {"id": 413, "seek": 229532, "start": 2295.36, "end": 2301.44, "text": " about human language and the nature of the problems that involves because I mean even", "tokens": [50366, 466, 1952, 2856, 293, 264, 3687, 295, 264, 2740, 300, 11626, 570, 286, 914, 754, 50670], "temperature": 0.0, "avg_logprob": -0.1680077126151637, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.002753892680630088}, {"id": 414, "seek": 229532, "start": 2301.44, "end": 2307.92, "text": " though people aren't directly going to be encoding rules of human language into their", "tokens": [50670, 1673, 561, 3212, 380, 3838, 516, 281, 312, 43430, 4474, 295, 1952, 2856, 666, 641, 50994], "temperature": 0.0, "avg_logprob": -0.1680077126151637, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.002753892680630088}, {"id": 415, "seek": 229532, "start": 2307.92, "end": 2315.4, "text": " computing system a sensitivity to sort of what kind of things happen in language and what to", "tokens": [50994, 15866, 1185, 257, 19392, 281, 1333, 295, 437, 733, 295, 721, 1051, 294, 2856, 293, 437, 281, 51368], "temperature": 0.0, "avg_logprob": -0.1680077126151637, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.002753892680630088}, {"id": 416, "seek": 229532, "start": 2315.4, "end": 2320.7200000000003, "text": " look out for and what you might want to model that's still a useful skill to have.", "tokens": [51368, 574, 484, 337, 293, 437, 291, 1062, 528, 281, 2316, 300, 311, 920, 257, 4420, 5389, 281, 362, 13, 51634], "temperature": 0.0, "avg_logprob": -0.1680077126151637, "compression_ratio": 1.6602870813397128, "no_speech_prob": 0.002753892680630088}, {"id": 417, "seek": 232072, "start": 2321.72, "end": 2327.9199999999996, "text": " And then in terms of learning the foundations learning about these concepts you had entered", "tokens": [50414, 400, 550, 294, 2115, 295, 2539, 264, 22467, 2539, 466, 613, 10392, 291, 632, 9065, 50724], "temperature": 0.0, "avg_logprob": -0.18212457803579477, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.007794317789375782}, {"id": 418, "seek": 232072, "start": 2327.9199999999996, "end": 2335.52, "text": " AI from a linguistic background and we now see people from you know all walks of life wanting", "tokens": [50724, 7318, 490, 257, 43002, 3678, 293, 321, 586, 536, 561, 490, 291, 458, 439, 12896, 295, 993, 7935, 51104], "temperature": 0.0, "avg_logprob": -0.18212457803579477, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.007794317789375782}, {"id": 419, "seek": 232072, "start": 2335.52, "end": 2341.3599999999997, "text": " to to start doing work in AI what are your thoughts on the preparation one should have", "tokens": [51104, 281, 281, 722, 884, 589, 294, 7318, 437, 366, 428, 4598, 322, 264, 13081, 472, 820, 362, 51396], "temperature": 0.0, "avg_logprob": -0.18212457803579477, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.007794317789375782}, {"id": 420, "seek": 232072, "start": 2341.3599999999997, "end": 2346.7999999999997, "text": " or any thoughts on how to start from something other than computer science or AI so there are", "tokens": [51396, 420, 604, 4598, 322, 577, 281, 722, 490, 746, 661, 813, 3820, 3497, 420, 7318, 370, 456, 366, 51668], "temperature": 0.0, "avg_logprob": -0.18212457803579477, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.007794317789375782}, {"id": 421, "seek": 234680, "start": 2346.84, "end": 2355.36, "text": " lots of places you can come from and vector across in different ways and we're seeing", "tokens": [50366, 3195, 295, 3190, 291, 393, 808, 490, 293, 8062, 2108, 294, 819, 2098, 293, 321, 434, 2577, 50792], "temperature": 0.0, "avg_logprob": -0.17508564719670935, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.0010280903661623597}, {"id": 422, "seek": 234680, "start": 2355.36, "end": 2361.6000000000004, "text": " tons of people doing that that they're people who started off in different areas whether", "tokens": [50792, 9131, 295, 561, 884, 300, 300, 436, 434, 561, 567, 1409, 766, 294, 819, 3179, 1968, 51104], "temperature": 0.0, "avg_logprob": -0.17508564719670935, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.0010280903661623597}, {"id": 423, "seek": 234680, "start": 2361.6000000000004, "end": 2369.2000000000003, "text": " you know it was chemistry physics or even much further in field and people you know history", "tokens": [51104, 291, 458, 309, 390, 12558, 10649, 420, 754, 709, 3052, 294, 2519, 293, 561, 291, 458, 2503, 51484], "temperature": 0.0, "avg_logprob": -0.17508564719670935, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.0010280903661623597}, {"id": 424, "seek": 234680, "start": 2369.2000000000003, "end": 2374.1600000000003, "text": " whatever have started to look at machine learning I mean I think there are sort of two levels of", "tokens": [51484, 2035, 362, 1409, 281, 574, 412, 3479, 2539, 286, 914, 286, 519, 456, 366, 1333, 295, 732, 4358, 295, 51732], "temperature": 0.0, "avg_logprob": -0.17508564719670935, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.0010280903661623597}, {"id": 425, "seek": 237416, "start": 2374.16, "end": 2381.48, "text": " answer there I mean one level of answer is you know one of the amazing transformations is that", "tokens": [50364, 1867, 456, 286, 914, 472, 1496, 295, 1867, 307, 291, 458, 472, 295, 264, 2243, 34852, 307, 300, 50730], "temperature": 0.0, "avg_logprob": -0.13641343958237592, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.01941005140542984}, {"id": 426, "seek": 237416, "start": 2381.48, "end": 2389.3199999999997, "text": " there's now these very good software packages for doing things with neural network models I mean", "tokens": [50730, 456, 311, 586, 613, 588, 665, 4722, 17401, 337, 884, 721, 365, 18161, 3209, 5245, 286, 914, 51122], "temperature": 0.0, "avg_logprob": -0.13641343958237592, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.01941005140542984}, {"id": 427, "seek": 237416, "start": 2389.3199999999997, "end": 2395.3999999999996, "text": " this these software is really easy to use you don't actually need to understand a lot of highly", "tokens": [51122, 341, 613, 4722, 307, 534, 1858, 281, 764, 291, 500, 380, 767, 643, 281, 1223, 257, 688, 295, 5405, 51426], "temperature": 0.0, "avg_logprob": -0.13641343958237592, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.01941005140542984}, {"id": 428, "seek": 237416, "start": 2395.3999999999996, "end": 2401.3599999999997, "text": " technical stuff you've got need to have some kind of high-level conception about what is the idea of", "tokens": [51426, 6191, 1507, 291, 600, 658, 643, 281, 362, 512, 733, 295, 1090, 12, 12418, 30698, 466, 437, 307, 264, 1558, 295, 51724], "temperature": 0.0, "avg_logprob": -0.13641343958237592, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.01941005140542984}, {"id": 429, "seek": 240136, "start": 2401.36, "end": 2406.56, "text": " machine learning and how do I go about training a model and what should I look at in the numbers", "tokens": [50364, 3479, 2539, 293, 577, 360, 286, 352, 466, 3097, 257, 2316, 293, 437, 820, 286, 574, 412, 294, 264, 3547, 50624], "temperature": 0.0, "avg_logprob": -0.08383693526276445, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0030002903658896685}, {"id": 430, "seek": 240136, "start": 2406.56, "end": 2411.52, "text": " that are being printed out to see if it's working right but you know you don't actually have to", "tokens": [50624, 300, 366, 885, 13567, 484, 281, 536, 498, 309, 311, 1364, 558, 457, 291, 458, 291, 500, 380, 767, 362, 281, 50872], "temperature": 0.0, "avg_logprob": -0.08383693526276445, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0030002903658896685}, {"id": 431, "seek": 240136, "start": 2411.52, "end": 2416.96, "text": " have a higher degree to be able to build these models I mean and indeed what we're seeing is you", "tokens": [50872, 362, 257, 2946, 4314, 281, 312, 1075, 281, 1322, 613, 5245, 286, 914, 293, 6451, 437, 321, 434, 2577, 307, 291, 51144], "temperature": 0.0, "avg_logprob": -0.08383693526276445, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0030002903658896685}, {"id": 432, "seek": 240136, "start": 2416.96, "end": 2422.36, "text": " know lots of high school students are getting into doing this because it's actually something that if", "tokens": [51144, 458, 3195, 295, 1090, 1395, 1731, 366, 1242, 666, 884, 341, 570, 309, 311, 767, 746, 300, 498, 51414], "temperature": 0.0, "avg_logprob": -0.08383693526276445, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0030002903658896685}, {"id": 433, "seek": 240136, "start": 2422.36, "end": 2429.6400000000003, "text": " you have some basic computer skills and a bit of programming you can pick up and do it's just way", "tokens": [51414, 291, 362, 512, 3875, 3820, 3942, 293, 257, 857, 295, 9410, 291, 393, 1888, 493, 293, 360, 309, 311, 445, 636, 51778], "temperature": 0.0, "avg_logprob": -0.08383693526276445, "compression_ratio": 1.7589928057553956, "no_speech_prob": 0.0030002903658896685}, {"id": 434, "seek": 242964, "start": 2429.68, "end": 2436.72, "text": " more accessible than lots of stuff that preceded a weather in AI or outside of AI and other areas", "tokens": [50366, 544, 9515, 813, 3195, 295, 1507, 300, 16969, 292, 257, 5503, 294, 7318, 420, 2380, 295, 7318, 293, 661, 3179, 50718], "temperature": 0.0, "avg_logprob": -0.09921568566626245, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.0015947712818160653}, {"id": 435, "seek": 242964, "start": 2436.72, "end": 2442.2799999999997, "text": " you know like operating systems or security but you know if you want to get to a deeper level than", "tokens": [50718, 291, 458, 411, 7447, 3652, 420, 3825, 457, 291, 458, 498, 291, 528, 281, 483, 281, 257, 7731, 1496, 813, 50996], "temperature": 0.0, "avg_logprob": -0.09921568566626245, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.0015947712818160653}, {"id": 436, "seek": 242964, "start": 2442.2799999999997, "end": 2449.3199999999997, "text": " that and actually want to understand more of what's going on I think you can't really get there if you", "tokens": [50996, 300, 293, 767, 528, 281, 1223, 544, 295, 437, 311, 516, 322, 286, 519, 291, 393, 380, 534, 483, 456, 498, 291, 51348], "temperature": 0.0, "avg_logprob": -0.09921568566626245, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.0015947712818160653}, {"id": 437, "seek": 242964, "start": 2449.3199999999997, "end": 2457.4, "text": " don't have a certain mathematics foundation like at the end of the day that deep learning is based", "tokens": [51348, 500, 380, 362, 257, 1629, 18666, 7030, 411, 412, 264, 917, 295, 264, 786, 300, 2452, 2539, 307, 2361, 51752], "temperature": 0.0, "avg_logprob": -0.09921568566626245, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.0015947712818160653}, {"id": 438, "seek": 245740, "start": 2457.52, "end": 2465.84, "text": " on calculus and you need to be optimizing functions and if you sort of don't have any background in", "tokens": [50370, 322, 33400, 293, 291, 643, 281, 312, 40425, 6828, 293, 498, 291, 1333, 295, 500, 380, 362, 604, 3678, 294, 50786], "temperature": 0.0, "avg_logprob": -0.2044372304280599, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0008157444535754621}, {"id": 439, "seek": 245740, "start": 2465.84, "end": 2473.52, "text": " that I think that sort of ends up as a wall at some point so you know. The math for machine learning", "tokens": [50786, 300, 286, 519, 300, 1333, 295, 5314, 493, 382, 257, 2929, 412, 512, 935, 370, 291, 458, 13, 440, 5221, 337, 3479, 2539, 51170], "temperature": 0.0, "avg_logprob": -0.2044372304280599, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0008157444535754621}, {"id": 440, "seek": 245740, "start": 2473.52, "end": 2480.96, "text": " and data science it does come in handy for some of the work we're going to do. Yeah so I think at", "tokens": [51170, 293, 1412, 3497, 309, 775, 808, 294, 13239, 337, 512, 295, 264, 589, 321, 434, 516, 281, 360, 13, 865, 370, 286, 519, 412, 51542], "temperature": 0.0, "avg_logprob": -0.2044372304280599, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.0008157444535754621}, {"id": 441, "seek": 248096, "start": 2480.96, "end": 2488.7200000000003, "text": " some level if you're at the major in history or you know non-mathematical parts of psychology I", "tokens": [50364, 512, 1496, 498, 291, 434, 412, 264, 2563, 294, 2503, 420, 291, 458, 2107, 12, 24761, 8615, 804, 3166, 295, 15105, 286, 50752], "temperature": 0.0, "avg_logprob": -0.13182523976201596, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.022483892738819122}, {"id": 442, "seek": 248096, "start": 2488.7200000000003, "end": 2494.36, "text": " actually have a good friend who yeah he you know learnt calculus in grad school because he was a", "tokens": [50752, 767, 362, 257, 665, 1277, 567, 1338, 415, 291, 458, 18991, 33400, 294, 2771, 1395, 570, 415, 390, 257, 51034], "temperature": 0.0, "avg_logprob": -0.13182523976201596, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.022483892738819122}, {"id": 443, "seek": 248096, "start": 2494.36, "end": 2499.2, "text": " psychologist and he'd never done it before and decided he wanted to start learning about these", "tokens": [51034, 29514, 293, 415, 1116, 1128, 1096, 309, 949, 293, 3047, 415, 1415, 281, 722, 2539, 466, 613, 51276], "temperature": 0.0, "avg_logprob": -0.13182523976201596, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.022483892738819122}, {"id": 444, "seek": 248096, "start": 2499.2, "end": 2506.04, "text": " new kinds of models and decided it wasn't too late to be able to go and take a Cal course and so he", "tokens": [51276, 777, 3685, 295, 5245, 293, 3047, 309, 2067, 380, 886, 3469, 281, 312, 1075, 281, 352, 293, 747, 257, 3511, 1164, 293, 370, 415, 51618], "temperature": 0.0, "avg_logprob": -0.13182523976201596, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.022483892738819122}, {"id": 445, "seek": 250604, "start": 2506.04, "end": 2513.8, "text": " did right so you know you do need to know some of that stuff but for lots of people if they've seen", "tokens": [50364, 630, 558, 370, 291, 458, 291, 360, 643, 281, 458, 512, 295, 300, 1507, 457, 337, 3195, 295, 561, 498, 436, 600, 1612, 50752], "temperature": 0.0, "avg_logprob": -0.08455251910022854, "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.018515530973672867}, {"id": 446, "seek": 250604, "start": 2513.8, "end": 2521.7599999999998, "text": " some of that before even if you're kind of rusty I think you can kind of get back in the zone and", "tokens": [50752, 512, 295, 300, 949, 754, 498, 291, 434, 733, 295, 45394, 286, 519, 291, 393, 733, 295, 483, 646, 294, 264, 6668, 293, 51150], "temperature": 0.0, "avg_logprob": -0.08455251910022854, "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.018515530973672867}, {"id": 447, "seek": 250604, "start": 2521.7599999999998, "end": 2528.44, "text": " it doesn't really matter that you haven't you know done AI as an undergrad or machine learnings and", "tokens": [51150, 309, 1177, 380, 534, 1871, 300, 291, 2378, 380, 291, 458, 1096, 7318, 382, 364, 14295, 420, 3479, 2539, 82, 293, 51484], "temperature": 0.0, "avg_logprob": -0.08455251910022854, "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.018515530973672867}, {"id": 448, "seek": 250604, "start": 2528.44, "end": 2533.56, "text": " things like that that you can really start to learn how to build these models and do things and you", "tokens": [51484, 721, 411, 300, 300, 291, 393, 534, 722, 281, 1466, 577, 281, 1322, 613, 5245, 293, 360, 721, 293, 291, 51740], "temperature": 0.0, "avg_logprob": -0.08455251910022854, "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.018515530973672867}, {"id": 449, "seek": 253356, "start": 2533.56, "end": 2539.48, "text": " know really that's my own story right that despite the fact that they let me sit in the school of", "tokens": [50364, 458, 534, 300, 311, 452, 1065, 1657, 558, 300, 7228, 264, 1186, 300, 436, 718, 385, 1394, 294, 264, 1395, 295, 50660], "temperature": 0.0, "avg_logprob": -0.12737241632798138, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0016392249381169677}, {"id": 450, "seek": 253356, "start": 2539.48, "end": 2546.4, "text": " engineering at Stanford these days you know my background isn't as an engineer you know my PhDs", "tokens": [50660, 7043, 412, 20374, 613, 1708, 291, 458, 452, 3678, 1943, 380, 382, 364, 11403, 291, 458, 452, 14476, 82, 51006], "temperature": 0.0, "avg_logprob": -0.12737241632798138, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0016392249381169677}, {"id": 451, "seek": 253356, "start": 2546.4, "end": 2554.88, "text": " and linguistics but you know I've sort of largely vectored across from having some knowledge of", "tokens": [51006, 293, 21766, 6006, 457, 291, 458, 286, 600, 1333, 295, 11611, 1241, 349, 2769, 2108, 490, 1419, 512, 3601, 295, 51430], "temperature": 0.0, "avg_logprob": -0.12737241632798138, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0016392249381169677}, {"id": 452, "seek": 253356, "start": 2554.88, "end": 2561.12, "text": " mathematics and linguistics and knowing some programming into sort of getting much more into", "tokens": [51430, 18666, 293, 21766, 6006, 293, 5276, 512, 9410, 666, 1333, 295, 1242, 709, 544, 666, 51742], "temperature": 0.0, "avg_logprob": -0.12737241632798138, "compression_ratio": 1.7285067873303168, "no_speech_prob": 0.0016392249381169677}, {"id": 453, "seek": 256112, "start": 2561.16, "end": 2563.12, "text": " building AI models.", "tokens": [50366, 2390, 7318, 5245, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2530723001765109, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.003931102342903614}, {"id": 454, "seek": 256112, "start": 2563.12, "end": 2568.16, "text": " I was curious about something do you think the improved libraries and abstractions that are now", "tokens": [50464, 286, 390, 6369, 466, 746, 360, 291, 519, 264, 9689, 15148, 293, 12649, 626, 300, 366, 586, 50716], "temperature": 0.0, "avg_logprob": -0.2530723001765109, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.003931102342903614}, {"id": 455, "seek": 256112, "start": 2568.16, "end": 2573.68, "text": " available like coding frameworks like TensorFlow or PyTorch do you think that reduces the need to", "tokens": [50716, 2435, 411, 17720, 29834, 411, 37624, 420, 9953, 51, 284, 339, 360, 291, 519, 300, 18081, 264, 643, 281, 50992], "temperature": 0.0, "avg_logprob": -0.2530723001765109, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.003931102342903614}, {"id": 456, "seek": 256112, "start": 2573.68, "end": 2579.3599999999997, "text": " understand calculus because boy it's been it's been a while since I had to actually take a derivative", "tokens": [50992, 1223, 33400, 570, 3237, 309, 311, 668, 309, 311, 668, 257, 1339, 1670, 286, 632, 281, 767, 747, 257, 13760, 51276], "temperature": 0.0, "avg_logprob": -0.2530723001765109, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.003931102342903614}, {"id": 457, "seek": 256112, "start": 2579.3599999999997, "end": 2584.72, "text": " in order to even implement or create a new neural network architecture because of automatic", "tokens": [51276, 294, 1668, 281, 754, 4445, 420, 1884, 257, 777, 18161, 3209, 9482, 570, 295, 12509, 51544], "temperature": 0.0, "avg_logprob": -0.2530723001765109, "compression_ratio": 1.6023622047244095, "no_speech_prob": 0.003931102342903614}, {"id": 458, "seek": 258472, "start": 2584.72, "end": 2594.0, "text": " differentiation. Yeah I mean absolutely I mean so in the early days when we were doing things sort", "tokens": [50364, 38902, 13, 865, 286, 914, 3122, 286, 914, 370, 294, 264, 2440, 1708, 562, 321, 645, 884, 721, 1333, 50828], "temperature": 0.0, "avg_logprob": -0.143201021921067, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.027065489441156387}, {"id": 459, "seek": 258472, "start": 2594.0, "end": 2601.12, "text": " of 2010 to 2015 right for every model we built we were working out the derivatives by hand and then", "tokens": [50828, 295, 9657, 281, 7546, 558, 337, 633, 2316, 321, 3094, 321, 645, 1364, 484, 264, 33733, 538, 1011, 293, 550, 51184], "temperature": 0.0, "avg_logprob": -0.143201021921067, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.027065489441156387}, {"id": 460, "seek": 258472, "start": 2601.12, "end": 2606.48, "text": " you know writing some code and whatever it was you know sometimes it was Python but sometimes it", "tokens": [51184, 291, 458, 3579, 512, 3089, 293, 2035, 309, 390, 291, 458, 2171, 309, 390, 15329, 457, 2171, 309, 51452], "temperature": 0.0, "avg_logprob": -0.143201021921067, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.027065489441156387}, {"id": 461, "seek": 258472, "start": 2606.48, "end": 2613.3199999999997, "text": " might have been Java or C to calculate these derivatives and checking that we got them right", "tokens": [51452, 1062, 362, 668, 10745, 420, 383, 281, 8873, 613, 33733, 293, 8568, 300, 321, 658, 552, 558, 51794], "temperature": 0.0, "avg_logprob": -0.143201021921067, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.027065489441156387}, {"id": 462, "seek": 261332, "start": 2613.32, "end": 2621.96, "text": " and so on where you know these days you actually don't need to know any of that to build deep", "tokens": [50364, 293, 370, 322, 689, 291, 458, 613, 1708, 291, 767, 500, 380, 643, 281, 458, 604, 295, 300, 281, 1322, 2452, 50796], "temperature": 0.0, "avg_logprob": -0.13216383864239947, "compression_ratio": 1.75, "no_speech_prob": 0.0006763892015442252}, {"id": 463, "seek": 261332, "start": 2621.96, "end": 2627.0, "text": " learning models I mean this is actually something I think about been thinking about even with respect", "tokens": [50796, 2539, 5245, 286, 914, 341, 307, 767, 746, 286, 519, 466, 668, 1953, 466, 754, 365, 3104, 51048], "temperature": 0.0, "avg_logprob": -0.13216383864239947, "compression_ratio": 1.75, "no_speech_prob": 0.0006763892015442252}, {"id": 464, "seek": 261332, "start": 2627.0, "end": 2633.0800000000004, "text": " to my own natural language processing with deep learning class that I teach you know at the beginning", "tokens": [51048, 281, 452, 1065, 3303, 2856, 9007, 365, 2452, 2539, 1508, 300, 286, 2924, 291, 458, 412, 264, 2863, 51352], "temperature": 0.0, "avg_logprob": -0.13216383864239947, "compression_ratio": 1.75, "no_speech_prob": 0.0006763892015442252}, {"id": 465, "seek": 261332, "start": 2633.7200000000003, "end": 2641.96, "text": " we do still go through doing you know matrix calculus and making sure people know about", "tokens": [51384, 321, 360, 920, 352, 807, 884, 291, 458, 8141, 33400, 293, 1455, 988, 561, 458, 466, 51796], "temperature": 0.0, "avg_logprob": -0.13216383864239947, "compression_ratio": 1.75, "no_speech_prob": 0.0006763892015442252}, {"id": 466, "seek": 264196, "start": 2642.04, "end": 2649.64, "text": " Jacobians and things like that so that they understand what's being done in back propagation", "tokens": [50368, 14117, 2567, 293, 721, 411, 300, 370, 300, 436, 1223, 437, 311, 885, 1096, 294, 646, 38377, 50748], "temperature": 0.0, "avg_logprob": -0.05758301929761005, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003473904449492693}, {"id": 467, "seek": 264196, "start": 2649.64, "end": 2655.64, "text": " deep learning but you know there's sort of this sense in which that means that we just give them", "tokens": [50748, 2452, 2539, 457, 291, 458, 456, 311, 1333, 295, 341, 2020, 294, 597, 300, 1355, 300, 321, 445, 976, 552, 51048], "temperature": 0.0, "avg_logprob": -0.05758301929761005, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003473904449492693}, {"id": 468, "seek": 264196, "start": 2655.64, "end": 2661.8, "text": " hell for two weeks you know sort of like boot camp or something to make them suffer and then we say", "tokens": [51048, 4921, 337, 732, 3259, 291, 458, 1333, 295, 411, 11450, 2255, 420, 746, 281, 652, 552, 9753, 293, 550, 321, 584, 51356], "temperature": 0.0, "avg_logprob": -0.05758301929761005, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003473904449492693}, {"id": 469, "seek": 264196, "start": 2661.8, "end": 2666.76, "text": " oh but you do the rest of the class with PyTorch and they sort of never have to know any of that", "tokens": [51356, 1954, 457, 291, 360, 264, 1472, 295, 264, 1508, 365, 9953, 51, 284, 339, 293, 436, 1333, 295, 1128, 362, 281, 458, 604, 295, 300, 51604], "temperature": 0.0, "avg_logprob": -0.05758301929761005, "compression_ratio": 1.7387387387387387, "no_speech_prob": 0.003473904449492693}, {"id": 470, "seek": 266676, "start": 2666.76, "end": 2673.4, "text": " again right I you know there's always a question of how deep you want to go in technical foundations", "tokens": [50364, 797, 558, 286, 291, 458, 456, 311, 1009, 257, 1168, 295, 577, 2452, 291, 528, 281, 352, 294, 6191, 22467, 50696], "temperature": 0.0, "avg_logprob": -0.07431442472669814, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0003430508077144623}, {"id": 471, "seek": 266676, "start": 2673.4, "end": 2680.84, "text": " right you can keep on going right like does a computer scientist in the 2020s need to understand", "tokens": [50696, 558, 291, 393, 1066, 322, 516, 558, 411, 775, 257, 3820, 12662, 294, 264, 4808, 82, 643, 281, 1223, 51068], "temperature": 0.0, "avg_logprob": -0.07431442472669814, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0003430508077144623}, {"id": 472, "seek": 266676, "start": 2680.84, "end": 2689.6400000000003, "text": " you know electronics and transistors or what happens in you know CPU well you know it's complicated", "tokens": [51068, 291, 458, 20611, 293, 1145, 46976, 420, 437, 2314, 294, 291, 458, 13199, 731, 291, 458, 309, 311, 6179, 51508], "temperature": 0.0, "avg_logprob": -0.07431442472669814, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0003430508077144623}, {"id": 473, "seek": 266676, "start": 2689.6400000000003, "end": 2694.84, "text": " I mean in various ways it is helpful to know some of that stuff I mean you know I know Andrew you", "tokens": [51508, 286, 914, 294, 3683, 2098, 309, 307, 4961, 281, 458, 512, 295, 300, 1507, 286, 914, 291, 458, 286, 458, 10110, 291, 51768], "temperature": 0.0, "avg_logprob": -0.07431442472669814, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.0003430508077144623}, {"id": 474, "seek": 269484, "start": 2694.84, "end": 2701.08, "text": " were one of the pioneers and getting machine learning onto GPUs and well you know that sort of", "tokens": [50364, 645, 472, 295, 264, 47381, 293, 1242, 3479, 2539, 3911, 18407, 82, 293, 731, 291, 458, 300, 1333, 295, 50676], "temperature": 0.0, "avg_logprob": -0.06418328915002211, "compression_ratio": 1.9409448818897639, "no_speech_prob": 0.003313641296699643}, {"id": 475, "seek": 269484, "start": 2701.08, "end": 2706.6000000000004, "text": " means you had to have some sense that there's this new hardware out there and it has some attributes", "tokens": [50676, 1355, 291, 632, 281, 362, 512, 2020, 300, 456, 311, 341, 777, 8837, 484, 456, 293, 309, 575, 512, 17212, 50952], "temperature": 0.0, "avg_logprob": -0.06418328915002211, "compression_ratio": 1.9409448818897639, "no_speech_prob": 0.003313641296699643}, {"id": 476, "seek": 269484, "start": 2706.6000000000004, "end": 2713.08, "text": " of parallelism that means there's likely to be able to do something exciting so you know it is useful", "tokens": [50952, 295, 8952, 1434, 300, 1355, 456, 311, 3700, 281, 312, 1075, 281, 360, 746, 4670, 370, 291, 458, 309, 307, 4420, 51276], "temperature": 0.0, "avg_logprob": -0.06418328915002211, "compression_ratio": 1.9409448818897639, "no_speech_prob": 0.003313641296699643}, {"id": 477, "seek": 269484, "start": 2713.08, "end": 2717.88, "text": " to have some broader knowledge and understanding and you know sometimes something breaks and if", "tokens": [51276, 281, 362, 512, 13227, 3601, 293, 3701, 293, 291, 458, 2171, 746, 9857, 293, 498, 51516], "temperature": 0.0, "avg_logprob": -0.06418328915002211, "compression_ratio": 1.9409448818897639, "no_speech_prob": 0.003313641296699643}, {"id": 478, "seek": 269484, "start": 2717.88, "end": 2723.08, "text": " you have some deep knowledge you can understand why it broke but there's another sense in which you", "tokens": [51516, 291, 362, 512, 2452, 3601, 291, 393, 1223, 983, 309, 6902, 457, 456, 311, 1071, 2020, 294, 597, 291, 51776], "temperature": 0.0, "avg_logprob": -0.06418328915002211, "compression_ratio": 1.9409448818897639, "no_speech_prob": 0.003313641296699643}, {"id": 479, "seek": 272308, "start": 2723.08, "end": 2730.04, "text": " know most people have to take some things on trust and you can do most of what you want to do in", "tokens": [50364, 458, 881, 561, 362, 281, 747, 512, 721, 322, 3361, 293, 291, 393, 360, 881, 295, 437, 291, 528, 281, 360, 294, 50712], "temperature": 0.0, "avg_logprob": -0.09318187603583702, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.00032988889142870903}, {"id": 480, "seek": 272308, "start": 2731.16, "end": 2737.16, "text": " neural network modeling these days without knowing calculus at all yeah that's a great point I feel", "tokens": [50768, 18161, 3209, 15983, 613, 1708, 1553, 5276, 33400, 412, 439, 1338, 300, 311, 257, 869, 935, 286, 841, 51068], "temperature": 0.0, "avg_logprob": -0.09318187603583702, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.00032988889142870903}, {"id": 481, "seek": 272308, "start": 2737.16, "end": 2742.36, "text": " like sometimes the reliability of the abstraction determines how often you need to go in to fix", "tokens": [51068, 411, 2171, 264, 24550, 295, 264, 37765, 24799, 577, 2049, 291, 643, 281, 352, 294, 281, 3191, 51328], "temperature": 0.0, "avg_logprob": -0.09318187603583702, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.00032988889142870903}, {"id": 482, "seek": 272308, "start": 2742.36, "end": 2747.88, "text": " something that's broken so I actually my understanding of quantum physics is very weak I barely", "tokens": [51328, 746, 300, 311, 5463, 370, 286, 767, 452, 3701, 295, 13018, 10649, 307, 588, 5336, 286, 10268, 51604], "temperature": 0.0, "avg_logprob": -0.09318187603583702, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.00032988889142870903}, {"id": 483, "seek": 272308, "start": 2747.88, "end": 2752.44, "text": " understand it so you could argue I don't understand how computers work because transistors are built", "tokens": [51604, 1223, 309, 370, 291, 727, 9695, 286, 500, 380, 1223, 577, 10807, 589, 570, 1145, 46976, 366, 3094, 51832], "temperature": 0.0, "avg_logprob": -0.09318187603583702, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.00032988889142870903}, {"id": 484, "seek": 275244, "start": 2752.44, "end": 2757.8, "text": " in quantum physics but fortunately you know if something went wrong with transistors I've never", "tokens": [50364, 294, 13018, 10649, 457, 25511, 291, 458, 498, 746, 1437, 2085, 365, 1145, 46976, 286, 600, 1128, 50632], "temperature": 0.0, "avg_logprob": -0.15326350818980825, "compression_ratio": 1.855513307984791, "no_speech_prob": 0.0013243830762803555}, {"id": 485, "seek": 275244, "start": 2757.8, "end": 2766.04, "text": " had to go in to try to fix it so they're a bit hard to fix I think and so I think I think well", "tokens": [50632, 632, 281, 352, 294, 281, 853, 281, 3191, 309, 370, 436, 434, 257, 857, 1152, 281, 3191, 286, 519, 293, 370, 286, 519, 286, 519, 731, 51044], "temperature": 0.0, "avg_logprob": -0.15326350818980825, "compression_ratio": 1.855513307984791, "no_speech_prob": 0.0013243830762803555}, {"id": 486, "seek": 275244, "start": 2766.04, "end": 2771.2400000000002, "text": " another example you know the sort function their libraries are sort things and sometimes they actually", "tokens": [51044, 1071, 1365, 291, 458, 264, 1333, 2445, 641, 15148, 366, 1333, 721, 293, 2171, 436, 767, 51304], "temperature": 0.0, "avg_logprob": -0.15326350818980825, "compression_ratio": 1.855513307984791, "no_speech_prob": 0.0013243830762803555}, {"id": 487, "seek": 275244, "start": 2771.2400000000002, "end": 2775.96, "text": " don't work right swap in the memory or whatever and that's when if you really understand how the", "tokens": [51304, 500, 380, 589, 558, 18135, 294, 264, 4675, 420, 2035, 293, 300, 311, 562, 498, 291, 534, 1223, 577, 264, 51540], "temperature": 0.0, "avg_logprob": -0.15326350818980825, "compression_ratio": 1.855513307984791, "no_speech_prob": 0.0013243830762803555}, {"id": 488, "seek": 275244, "start": 2775.96, "end": 2782.2000000000003, "text": " sort function works you can go in and fix it but then sometimes if we have abstractions libraries", "tokens": [51540, 1333, 2445, 1985, 291, 393, 352, 294, 293, 3191, 309, 457, 550, 2171, 498, 321, 362, 12649, 626, 15148, 51852], "temperature": 0.0, "avg_logprob": -0.15326350818980825, "compression_ratio": 1.855513307984791, "no_speech_prob": 0.0013243830762803555}, {"id": 489, "seek": 278220, "start": 2782.2799999999997, "end": 2788.6, "text": " APIs are reliable enough then that is nice to those abstractions then diminishes them to understand", "tokens": [50368, 21445, 366, 12924, 1547, 550, 300, 307, 1481, 281, 729, 12649, 626, 550, 15739, 16423, 552, 281, 1223, 50684], "temperature": 0.0, "avg_logprob": -0.15388722513236253, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0003352333151269704}, {"id": 490, "seek": 278220, "start": 2788.6, "end": 2793.72, "text": " some of the things that happen so it's an exciting world feels like you know we have giants building", "tokens": [50684, 512, 295, 264, 721, 300, 1051, 370, 309, 311, 364, 4670, 1002, 3417, 411, 291, 458, 321, 362, 31894, 2390, 50940], "temperature": 0.0, "avg_logprob": -0.15388722513236253, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0003352333151269704}, {"id": 491, "seek": 278220, "start": 2793.72, "end": 2798.4399999999996, "text": " on the shoulders of giants and and all of these things are becoming more complex and more exciting", "tokens": [50940, 322, 264, 10245, 295, 31894, 293, 293, 439, 295, 613, 721, 366, 5617, 544, 3997, 293, 544, 4670, 51176], "temperature": 0.0, "avg_logprob": -0.15388722513236253, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0003352333151269704}, {"id": 492, "seek": 278220, "start": 2798.4399999999996, "end": 2805.96, "text": " every every every month yeah absolutely so thanks Chris that was really um interesting and inspiring", "tokens": [51176, 633, 633, 633, 1618, 1338, 3122, 370, 3231, 6688, 300, 390, 534, 1105, 1880, 293, 15883, 51552], "temperature": 0.0, "avg_logprob": -0.15388722513236253, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0003352333151269704}, {"id": 493, "seek": 278220, "start": 2805.96, "end": 2812.04, "text": " and and I hope that to everyone watching this hearing Chris's own journey um to become a", "tokens": [51552, 293, 293, 286, 1454, 300, 281, 1518, 1976, 341, 4763, 6688, 311, 1065, 4671, 1105, 281, 1813, 257, 51856], "temperature": 0.0, "avg_logprob": -0.15388722513236253, "compression_ratio": 1.7912087912087913, "no_speech_prob": 0.0003352333151269704}, {"id": 494, "seek": 281204, "start": 2812.04, "end": 2817.56, "text": " computer scientist and to become a leading maybe the leading NLP computer scientists as well as all", "tokens": [50364, 3820, 12662, 293, 281, 1813, 257, 5775, 1310, 264, 5775, 426, 45196, 3820, 7708, 382, 731, 382, 439, 50640], "temperature": 0.0, "avg_logprob": -0.13588666915893555, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0013981690863147378}, {"id": 495, "seek": 281204, "start": 2817.56, "end": 2824.52, "text": " of this exciting work having an NLP right now I hope that inspires you to jump into the sphere and", "tokens": [50640, 295, 341, 4670, 589, 1419, 364, 426, 45196, 558, 586, 286, 1454, 300, 32566, 291, 281, 3012, 666, 264, 16687, 293, 50988], "temperature": 0.0, "avg_logprob": -0.13588666915893555, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0013981690863147378}, {"id": 496, "seek": 281204, "start": 2824.52, "end": 2830.36, "text": " take a go at it there's just a lot more work to be done collectively by our community than still", "tokens": [50988, 747, 257, 352, 412, 309, 456, 311, 445, 257, 688, 544, 589, 281, 312, 1096, 24341, 538, 527, 1768, 813, 920, 51280], "temperature": 0.0, "avg_logprob": -0.13588666915893555, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0013981690863147378}, {"id": 497, "seek": 281204, "start": 2830.36, "end": 2833.88, "text": " so I think the more of us are working on this the better off the world will be", "tokens": [51280, 370, 286, 519, 264, 544, 295, 505, 366, 1364, 322, 341, 264, 1101, 766, 264, 1002, 486, 312, 51456], "temperature": 0.0, "avg_logprob": -0.13588666915893555, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0013981690863147378}, {"id": 498, "seek": 283388, "start": 2833.88, "end": 2845.7200000000003, "text": " so thanks a lot Chris it was really great having you thanks a lot Andrew it's been fun chatting", "tokens": [50364, 370, 3231, 257, 688, 6688, 309, 390, 534, 869, 1419, 291, 3231, 257, 688, 10110, 309, 311, 668, 1019, 24654, 50956], "temperature": 0.0, "avg_logprob": -0.2723066703132961, "compression_ratio": 1.1728395061728396, "no_speech_prob": 0.012455005198717117}], "language": "en"}