WEBVTT

00:00.000 --> 00:07.120
Welcome everyone, this is natural language understanding.

00:07.120 --> 00:11.440
It is a weird and wonderful and maybe worrying moment

00:11.440 --> 00:13.920
to be doing natural language understanding.

00:13.920 --> 00:18.240
My goal for today is just to kind of immerse us in this moment

00:18.240 --> 00:23.120
and think about how we got here and what it's like to be doing research now.

00:23.120 --> 00:27.360
And I think that'll set us up well to think about what we're going to do in the course,

00:27.360 --> 00:32.960
and how that's going to set you up to participate in this moment in AI

00:32.960 --> 00:35.840
in many ways, in whichever ways you choose.

00:35.840 --> 00:38.480
And it's an especially impactful moment to be doing that.

00:38.480 --> 00:40.640
And this is a project oriented course.

00:40.640 --> 00:43.920
And I feel like we can get you all to the point where you are doing

00:43.920 --> 00:47.600
meaningful things that contribute to this ongoing moment

00:47.600 --> 00:50.560
in ways that are going to be exciting and impactful.

00:50.560 --> 00:53.360
That is the fundamental goal of the course.

00:53.360 --> 00:56.560
Let's now think about the current model of the course.

00:56.560 --> 00:57.920
Let's now think about the current moment.

00:57.920 --> 01:00.640
This is always a moment of reflection for me.

01:00.640 --> 01:06.880
I started teaching this course in 2012, which I guess is ages ago now.

01:06.880 --> 01:08.800
It feels recent in my lived experience,

01:08.800 --> 01:11.840
but it does feel like ages ago in terms of the content.

01:11.840 --> 01:15.600
In 2012, on the first day, I had a slide that looked like this.

01:15.600 --> 01:20.800
I said it was an exciting time to be doing natural language understanding research.

01:20.800 --> 01:24.000
I noted that there was a resurgence of interest in the area

01:24.000 --> 01:28.960
after a long period of people mainly focused on syntax and things like that.

01:28.960 --> 01:33.840
But there was a widespread perception that NLE was poised for a breakthrough

01:33.840 --> 01:37.360
and to have huge impact that was relating to business things

01:37.360 --> 01:40.400
and that there was a white-hot job market for Stanford grads.

01:40.400 --> 01:43.760
A lot of this language is coming from the fact that we were in this moment

01:43.760 --> 01:48.720
when Siri had just launched, Watson had just won on Jeopardy,

01:48.720 --> 01:51.760
and we had all of these in-home devices and all the tech giants

01:51.760 --> 01:56.320
of competing on what was emerging as the field of natural language understanding.

01:57.200 --> 01:59.120
Let's fast forward to 2022.

01:59.120 --> 02:01.920
I did feel like I should update that in 2022

02:01.920 --> 02:04.160
by saying this is the most exciting moment ever,

02:04.160 --> 02:06.320
as opposed to just being an exciting time.

02:06.880 --> 02:09.200
But I emphasize the same things.

02:09.200 --> 02:13.600
We were in this feeling that we had experienced a resurgence of interest

02:13.600 --> 02:17.280
in the area, although now it was hyper-intensified.

02:17.280 --> 02:18.480
Same thing with industry.

02:18.480 --> 02:24.000
The industry interest at this point makes the stuff from 2012 look like small potatoes.

02:24.960 --> 02:29.600
Systems were getting very impressive, but, and I maintain this here,

02:29.600 --> 02:32.080
they show their weaknesses very quickly,

02:32.080 --> 02:35.520
and the core things about NLE remain far from solved.

02:35.520 --> 02:38.000
So the big breakthroughs lie in the future.

02:38.000 --> 02:40.320
I will say that even since 2022,

02:40.320 --> 02:43.040
it has felt like there has been an acceleration

02:43.040 --> 02:48.400
and some problems that we used to focus on feel kind of like they're less pressing.

02:48.400 --> 02:51.920
I won't say solved, but they feel like we've made a lot of progress on them

02:51.920 --> 02:54.480
as a result of models getting better.

02:54.480 --> 02:59.120
But all that means for me is that there are more exciting things in the future

02:59.120 --> 03:01.600
that we can tackle even more ambitious things.

03:01.600 --> 03:04.160
And you'll see that I've tried to overhaul the course

03:04.160 --> 03:08.240
to be ever more ambitious about the kind of problems that we might take on.

03:09.200 --> 03:12.400
But we do kind of live in a golden age for all of this stuff.

03:12.400 --> 03:17.200
And even in 2022, I'm not sure what I would have predicted to say nothing of 2012,

03:17.200 --> 03:20.400
that we would have these incredible models like Dali2,

03:20.400 --> 03:23.680
which can take you from text into these incredible images,

03:23.680 --> 03:28.000
language models, which will more or less be the star of the quarter for us,

03:28.000 --> 03:31.920
but also models that can take you from natural language to code.

03:31.920 --> 03:35.680
And of course, we are all seeing right now as we speak

03:36.480 --> 03:43.200
that the entire industry related to web search is being reshaped around NLU technologies.

03:43.760 --> 03:48.560
So whereas this felt like a kind of niche area of NLP,

03:48.560 --> 03:51.040
when we started this course in 2012,

03:51.040 --> 03:54.080
now it feels like the entire field of NLP,

03:54.080 --> 03:57.200
certainly in some aspects all of AI,

03:57.200 --> 04:00.240
is focused on these questions of natural language understanding,

04:00.240 --> 04:02.000
which is exciting for us.

04:02.960 --> 04:04.800
One more moment of reflection here.

04:04.800 --> 04:07.600
You know, in this course throughout the years,

04:07.600 --> 04:12.560
we have used simple examples to kind of highlight the weaknesses of current models.

04:12.560 --> 04:16.320
And so a classic one for us was simply this question,

04:16.320 --> 04:19.280
which US states border no US states?

04:19.280 --> 04:22.320
And the idea here is that it's a simple question,

04:22.320 --> 04:24.880
but it can be hard for our language technologies

04:24.880 --> 04:27.120
because of that negation, the no there.

04:28.080 --> 04:31.840
In 1980, there was a famous system called Chat 80.

04:31.840 --> 04:37.520
It was a symbolic system representing the first major phase of research in NLP.

04:37.520 --> 04:40.080
You can see the fragment of the system here.

04:40.080 --> 04:44.240
And Chat 80 was an incredible system in that it could answer questions like,

04:44.240 --> 04:48.640
which country bordering the Mediterranean borders a country that is bordered by a country

04:48.640 --> 04:51.440
whose population exceeds the population of India?

04:51.440 --> 04:54.320
I've given you the answer here, Turkey,

04:55.120 --> 04:58.480
at least according to 1980s geography.

04:59.040 --> 05:01.840
But if you asked Chat 80 a simple question like,

05:01.840 --> 05:03.920
which US states border no US states,

05:03.920 --> 05:06.640
it would just say, I don't understand.

05:06.640 --> 05:10.800
It was an incredibly expressive system, but rigid.

05:10.800 --> 05:13.120
It could do some things very deeply,

05:13.120 --> 05:14.880
as you see from the first question,

05:14.880 --> 05:17.360
but things that fell outside of its capacity,

05:17.360 --> 05:18.880
it would just fall down flat.

05:20.240 --> 05:21.440
That was the 1980s.

05:21.440 --> 05:22.400
Let's fast forward.

05:22.400 --> 05:25.040
2009, around the time this course launched,

05:25.040 --> 05:26.960
Wolfram Alphra hit the scene.

05:26.960 --> 05:30.800
And this was meant to be a kind of revolutionary language technology.

05:30.800 --> 05:32.320
The website is still up.

05:32.320 --> 05:36.560
And to my amazement, it still gives the following behavior.

05:36.560 --> 05:40.080
If you search for which US states border no US states,

05:40.080 --> 05:42.800
it kind of just gives you a list of the US states.

05:43.440 --> 05:48.720
Revealing, I would say, that it has no capacity to understand the question posed.

05:49.680 --> 05:50.800
That was 2009.

05:50.800 --> 05:53.440
So we've gone from 1980 to 2009.

05:54.000 --> 05:55.440
Okay, let's go to 2020.

05:55.440 --> 05:58.800
This is the first of the open AI models, ADA,

05:59.360 --> 06:01.520
which US states border no US states?

06:02.080 --> 06:03.200
The answer is no.

06:03.840 --> 06:05.600
And then it sort of starts to babble.

06:05.600 --> 06:07.600
The US border is not a state border.

06:07.600 --> 06:09.280
It did that for a very long time.

06:10.320 --> 06:12.960
But about Babbage, this is still 2020.

06:13.840 --> 06:16.000
The US states border no US states.

06:16.000 --> 06:17.520
What is the name of the US state?

06:17.520 --> 06:19.920
And then it really went off the deep end from there.

06:19.920 --> 06:22.240
Again, for a very long time, that was Babbage.

06:22.240 --> 06:25.600
If you had seen this output, well, at least for me,

06:25.600 --> 06:29.360
it might have shaken my faith that this was a viable approach.

06:30.480 --> 06:32.000
But the team persisted, I guess.

06:32.000 --> 06:34.240
2021, this is the Curie model.

06:34.240 --> 06:36.400
Which US states border no US states?

06:37.120 --> 06:39.440
It had a problem that it started listing things,

06:39.440 --> 06:42.720
but it did say Alaska, Hawaii, and Puerto Rico,

06:42.720 --> 06:46.800
which is an interestingly more impressive answer than the first answer.

06:47.600 --> 06:50.560
It still has some problem understanding what it means to respond,

06:50.560 --> 06:52.960
but it's looking like we're seeing some signal.

06:54.160 --> 06:55.600
Da Vinci instruct beta.

06:55.600 --> 06:56.800
This is 2022.

06:56.800 --> 06:59.520
It's important, I think, that this is the first of the models

06:59.520 --> 07:01.040
that have been struck in the name.

07:01.040 --> 07:02.800
We'll talk about that in a minute.

07:02.800 --> 07:04.720
Which US states border no US states?

07:04.720 --> 07:06.320
Alaska and Hawaii.

07:06.320 --> 07:11.440
We've from 2020 to 2022, we have seen this astounding leap forward,

07:11.440 --> 07:14.560
making everything before then sort of pale in comparison.

07:14.560 --> 07:19.280
And then finally, text Da Vinci won one of the new best in class models

07:19.280 --> 07:21.040
at least until two months ago.

07:21.040 --> 07:23.120
Which US states border no US states?

07:23.120 --> 07:26.880
Alaska and Hawaii are the only US states that border no other US states.

07:26.880 --> 07:29.520
A very impressive answer indeed.

07:29.520 --> 07:32.000
And if you just think about the little history I've given,

07:32.720 --> 07:36.080
a kind of microcosm of what is happening in the field,

07:36.960 --> 07:42.080
a lot of time without much progress with some hype attached.

07:42.160 --> 07:45.920
And now in the last few years, this kind of rapid progress forward.

07:47.360 --> 07:51.280
And you know, that's just one example, but these examples multiply

07:51.280 --> 07:52.320
and we can quantify this.

07:52.320 --> 07:54.080
Here's another impressive case.

07:54.080 --> 07:59.040
I asked the Da Vinci 2 model in which year was Stanford University founded?

07:59.040 --> 08:00.960
When did it enroll its first students?

08:00.960 --> 08:03.680
Who is its current president and what is its mascot?

08:03.680 --> 08:05.760
A complicated question indeed.

08:05.760 --> 08:10.400
And it gave a fluent and factually correct answer on all counts.

08:11.360 --> 08:15.600
This is the Da Vinci 3 model, which was best in class until a few weeks ago.

08:16.160 --> 08:19.200
And it gave exactly the same answer, very impressive.

08:20.560 --> 08:23.600
Now in this course, and you'll see at the website,

08:23.600 --> 08:26.800
one of the readings we've suggested for the start of the course

08:26.800 --> 08:31.200
is this classic paper by Hector Levec called On Our Best Behavior.

08:31.200 --> 08:34.960
And the thrust of this article essentially channeling Terry Winograd

08:34.960 --> 08:37.200
and Terry Winograd's schema.

08:38.000 --> 08:40.880
The idea is that we should come up with examples

08:40.880 --> 08:44.000
that will test whether models deeply understand.

08:44.000 --> 08:47.600
And in particular, get past the kind of simple memorization

08:47.600 --> 08:50.560
of statistics and other things about the data they're trained on

08:50.560 --> 08:54.160
and really probe to see whether they understand what the world is like.

08:54.160 --> 08:57.680
And Levec and Winograd's technique for doing this

08:57.680 --> 09:01.120
is to pose very unlikely questions

09:01.120 --> 09:03.440
where humans have very natural answers.

09:03.440 --> 09:05.600
Like one of the ones Levec poses is,

09:05.600 --> 09:07.840
could a crocodile run the steeple chase?

09:08.960 --> 09:11.280
Maybe it's a question you've never thought about before,

09:11.280 --> 09:14.960
but you probably have a pretty consistent answer across this group.

09:14.960 --> 09:16.880
Could a crocodile run the steeple chase?

09:17.520 --> 09:19.600
Here I asked another one of Levec's questions.

09:19.600 --> 09:24.320
Are professional baseball players allowed to glue small wings onto their caps?

09:24.320 --> 09:26.160
You could think about that for a second.

09:26.160 --> 09:30.880
The Da Vinci 2 model said there is no rule against it, but it is not common.

09:30.880 --> 09:35.040
And that seemed like a very good answer to me at the time.

09:35.040 --> 09:39.200
When the Da Vinci 3 engine came out, though, this started to worry me.

09:39.200 --> 09:43.680
No, professional baseball players are not allowed to glue small wings onto their caps.

09:43.680 --> 09:45.520
Major League Baseball has strict rules

09:45.520 --> 09:48.080
about the appearance of players' uniforms and caps.

09:48.880 --> 09:51.600
And any modifications to the caps are not allowed.

09:52.480 --> 09:55.120
Okay, I thought I was feeling good about this,

09:55.120 --> 09:58.160
but now I don't even myself know what the answer is.

09:58.160 --> 10:01.440
Are professional baseball players allowed to glue small wings onto their caps?

10:01.440 --> 10:04.800
We have two confident answers that are contradictory

10:05.600 --> 10:08.880
across two models that are very closely related.

10:08.880 --> 10:11.680
It's starting to worry us a little bit, I hope.

10:11.680 --> 10:14.080
But still, it's impressive.

10:14.080 --> 10:14.560
What's that?

10:14.560 --> 10:15.840
You want me to ask a part?

10:15.840 --> 10:16.720
Yes.

10:16.720 --> 10:17.520
You could check.

10:17.520 --> 10:21.920
Yes, I have a few cases, and this is an interesting experiment for us to run for sure.

10:21.920 --> 10:24.160
Let me show you the responses I got a bit later.

10:24.160 --> 10:29.040
The point, though, I guess, if you've seen the movie Blade Runner,

10:29.040 --> 10:33.840
this is starting to feel like to figure out whether an agent we were interacting with

10:33.840 --> 10:36.000
was human or AI.

10:36.000 --> 10:41.120
We would need to get very sophisticated interview techniques indeed.

10:41.120 --> 10:43.920
The Turing test long forgotten here.

10:43.920 --> 10:49.520
Now we're into the mode of trying to figure out exactly what kind of agents we're interacting with

10:49.520 --> 10:54.160
by having to be extremely clever about the kinds of things that we do with them.

10:55.920 --> 10:58.560
Now, that's kind of anecdotal evidence,

10:58.560 --> 11:03.840
but I think that the picture of progress is also supported by what's happening in the field.

11:03.840 --> 11:06.800
Let me start this story with our benchmarks.

11:06.800 --> 11:10.000
And the headline here is that our benchmarks, the tasks,

11:10.000 --> 11:15.200
the data sets we use to probe our models are saturating faster than ever before.

11:15.200 --> 11:17.760
And I'll articulate what I mean by saturate.

11:17.760 --> 11:19.760
So we here have a little framework.

11:19.760 --> 11:24.640
Along the x-axis, I have time stretching back into, like, the 1990s.

11:24.640 --> 11:31.280
And along the y-axis, I have a normalized measure of distance from what we call human performance.

11:31.280 --> 11:33.840
That's the red line set at zero.

11:33.840 --> 11:37.040
Each one of these benchmarks has, in its own particular way,

11:37.040 --> 11:39.680
set a so-called estimate of human performance.

11:39.680 --> 11:41.760
I think we should be cynical about that.

11:41.760 --> 11:45.360
But nonetheless, this will be a kind of marker of progress for us.

11:45.760 --> 11:47.760
First data set, MNIST.

11:47.760 --> 11:50.800
This is like digit recognition, famous task in AI.

11:50.800 --> 11:56.000
It was launched in the 1990s, and it took about 20 years for us to see a system

11:56.000 --> 12:00.000
that surpassed human performance in this very loose sense.

12:00.880 --> 12:04.400
The switchboard corpus, this is going from speech to text.

12:04.400 --> 12:08.800
It's a very similar story, launched in the 90s, and it took about 20 years

12:08.800 --> 12:11.120
for us to see a superhuman system.

12:11.120 --> 12:14.880
ImageNet, this was launched, I believe, in 2009,

12:14.880 --> 12:20.400
and it took less than 10 years for us to see a system that surpassed that red line.

12:20.400 --> 12:23.280
And now progress is going to pick up really fast.

12:23.280 --> 12:28.320
Squad 1.1, the Stanford question answering data set, was launched in 2016,

12:28.320 --> 12:32.560
and it took about three years for it to be saturated in this sense.

12:32.560 --> 12:37.760
Squad 2.0 was the team's attempt to pose an even harder problem,

12:37.840 --> 12:41.840
one where there were unanswerable questions, but it took even less time

12:41.840 --> 12:44.640
for systems to get past that red line.

12:45.200 --> 12:47.200
Then we get the glue benchmark.

12:47.200 --> 12:52.480
This is a famous benchmark in natural language understanding, a multi-task benchmark.

12:52.480 --> 12:57.680
When this was launched, a lot of us thought that glue would be too difficult

12:57.680 --> 12:58.800
for present-day systems.

12:58.800 --> 13:02.720
It looked like this might be a challenge that would stand for a very long time,

13:02.720 --> 13:05.200
but it took like less than a year.

13:05.280 --> 13:10.560
But it took like less than a year for systems to pass human performance.

13:10.560 --> 13:15.920
The response was superglue, but it was saturated, if anything, even more quickly.

13:16.960 --> 13:20.800
Now we can be as cynical as we want about this notion of human performance,

13:20.800 --> 13:24.320
and I think we should dwell on whether or not it's fair to call it that.

13:24.320 --> 13:30.960
But even setting that aside, this looks like undeniably a story of progress.

13:31.040 --> 13:37.120
The systems that we had in 2012 would not even have been able to enter the glue benchmark

13:37.120 --> 13:39.760
to say nothing of achieving scores like this.

13:39.760 --> 13:41.920
So something meaningful has happened.

13:41.920 --> 13:46.320
Now you might think by the standards of AI, these data sets are kind of old.

13:46.320 --> 13:51.600
Here's a post from Jason Wei, where he evaluated our latest and greatest large language models

13:51.600 --> 13:55.360
on a bunch of mostly new tasks that were actually designed

13:55.360 --> 13:59.680
to stress test this new class of very large language models.

13:59.680 --> 14:03.280
And Jason's observation is that we see emergent abilities

14:03.280 --> 14:08.240
across more than 100 tasks for these models, especially for our largest models.

14:08.240 --> 14:13.120
The point, though, is that we again thought these tasks would stand for a very long time,

14:13.120 --> 14:17.840
and what we're seeing instead is that one by one systems are certainly getting traction

14:17.840 --> 14:22.080
and in some cases performing at the standard we had set for humans.

14:22.800 --> 14:26.160
Again, an incredible story of progress there.

14:26.320 --> 14:29.920
So I hope that is energizing, maybe a little intimidating,

14:29.920 --> 14:32.480
but I hope fundamentally energizing for you all.

14:33.840 --> 14:38.320
The next question that I want to ask for you is just what is going on?

14:38.320 --> 14:41.440
What is driving all of this sudden progress?

14:41.440 --> 14:46.320
Let's get a feel for that, and that'll kind of serve as the foundation for the course itself.

14:46.880 --> 14:52.000
Before I do that, though, are there questions or comments, things I could resolve,

14:52.720 --> 14:56.080
or things I left out about the current moment?

15:00.240 --> 15:01.760
We're running on bar to think very well.

15:02.800 --> 15:08.160
We should reflect, though, maybe as a group about what it means to do very well.

15:08.160 --> 15:11.200
My question for you, when you say it did well,

15:11.200 --> 15:15.280
what is the Major League Baseball rule about players gluing things onto their caps?

15:17.200 --> 15:18.480
You found the actual rule.

15:18.480 --> 15:21.680
No, this is what, well, I don't...

15:22.480 --> 15:23.280
I didn't find the rule.

15:23.280 --> 15:24.880
Bard found that rule and gave me that number.

15:24.880 --> 15:25.280
Okay.

15:26.400 --> 15:28.640
Yes, that is going to be the question for us.

15:32.640 --> 15:36.960
I'm going to show you the open AI models will offer me links, but the links go nowhere.

15:40.160 --> 15:43.760
What you're pointing out, I think, is an increasing societal problem.

15:43.760 --> 15:47.120
These models are offering us what looks like evidence,

15:47.120 --> 15:49.760
but a lot of the evidence is just fabricated.

15:49.760 --> 15:52.560
And this is worse than offering no evidence at all.

15:52.560 --> 15:56.160
What I really need is someone who knows Major League Baseball to tell me,

15:56.160 --> 15:59.200
what is the rule about players and their caps?

15:59.760 --> 16:03.360
I want it from an expert human, not an expert language model.

16:05.840 --> 16:06.240
What's that?

16:06.240 --> 16:07.040
Can we Google?

16:08.080 --> 16:09.440
Be careful how you Google, though.

16:09.440 --> 16:11.440
I guess that's the lesson of 2023.

16:14.960 --> 16:16.480
All right, what's going on?

16:16.480 --> 16:18.480
Let's start to make some progress on this.

16:18.560 --> 16:21.920
Again, first, a little bit of historical context.

16:21.920 --> 16:26.080
I've got a timeline going back to the 1960s along the x-axis.

16:26.080 --> 16:28.480
This is more or less the start of the field itself.

16:29.040 --> 16:35.920
And in that early era, essentially all of the approaches were based in symbolic algorithms,

16:35.920 --> 16:37.760
like the chat 81 that I showed you.

16:37.760 --> 16:41.200
In fact, that was kind of pioneered here at Stanford by people

16:41.200 --> 16:43.920
who were pioneering the very field of AI.

16:43.920 --> 16:47.680
And that paradigm of essentially programming these systems

16:47.680 --> 16:49.920
lasted well into the 1980s.

16:52.000 --> 16:56.960
In the 90s, early 2000s, we get the statistical revolution

16:56.960 --> 17:01.440
throughout artificial intelligence and then in turn in natural language processing.

17:01.440 --> 17:06.080
And the big change there is that instead of programming systems with all these rules,

17:06.080 --> 17:10.320
we're going to design machine learning systems that are going to try to learn from data.

17:10.320 --> 17:13.040
Under the hood, there was still a lot of programming involved

17:13.040 --> 17:15.600
because we would write a lot of feature functions

17:15.600 --> 17:19.200
that were little programs that would help us detect things about data.

17:19.200 --> 17:22.240
And we would hope that our machine learning systems could learn

17:22.240 --> 17:24.560
from the output of those feature functions.

17:24.560 --> 17:29.520
But in the end, this was the rise of the fully data-driven learning systems.

17:29.520 --> 17:34.560
And we just hope that some process of optimization leads us to new capabilities.

17:35.920 --> 17:39.200
The next big phase of this was the deep learning revolution.

17:39.280 --> 17:42.400
This happened starting around 2009, 2010.

17:42.400 --> 17:45.440
Again, Stanford was at the forefront of this, to be sure.

17:46.240 --> 17:49.520
It felt like a big change at the time, but in retrospect,

17:49.520 --> 17:52.160
this is kind of not so different from this mode here.

17:52.160 --> 17:58.160
It's just that we now replace that simple model with really big models,

17:58.160 --> 18:02.640
really deep models that have a tremendous capacity to learn things from data.

18:03.280 --> 18:08.480
We started also to see a shift even further away from those feature functions,

18:08.480 --> 18:10.160
from writing little programs.

18:10.160 --> 18:14.160
And more toward a more mode where we would just hope that the data

18:14.160 --> 18:17.520
and the optimization process could do all the work for us.

18:19.520 --> 18:25.120
Then the next big thing that happened, which I could take us I suppose until about 2018,

18:25.120 --> 18:28.240
would be this mode where we have a lot of pre-trained parameters.

18:28.240 --> 18:32.800
These are pictures of maybe big language models or computer vision models or something.

18:32.800 --> 18:36.800
And when we build systems, we build on those pre-trained components

18:36.880 --> 18:40.960
and stitch them together with these task-specific parameters.

18:40.960 --> 18:46.240
And we hope that when they're all combined and we do some learning on some task-specific data,

18:46.240 --> 18:49.520
we have something that's benefiting from all these pre-trained components.

18:51.120 --> 18:55.920
And then the mode that we seem to be in now that I want us to reflect critically on

18:56.480 --> 19:00.000
is this mode where we're going to replace everything with maybe one

19:00.560 --> 19:05.040
ginormous language model of some kind and hope that that thing,

19:05.040 --> 19:08.640
that enormous black box will do all the work for us.

19:08.640 --> 19:11.920
We should think critically about whether that's really the path forward,

19:11.920 --> 19:15.120
but it certainly feels like the zeitgeist to be sure.

19:16.160 --> 19:16.800
Question, yeah?

19:17.760 --> 19:20.000
If you think it's worth it, could you go back to the last slide

19:20.960 --> 19:25.360
and maybe explain a little bit, a more browner example of what that all means?

19:25.360 --> 19:26.320
I couldn't quite follow.

19:27.040 --> 19:28.480
Let's do that later.

19:28.480 --> 19:34.400
The point for now though is really this shift from here where we're mostly learning from

19:34.480 --> 19:39.680
scratch for our task, here we've got things like BERT in the mix.

19:39.680 --> 19:45.840
We've got pre-trained components, models that we hope begin in a state that gives us a leg up

19:45.840 --> 19:47.680
on the problem we're trying to solve.

19:47.680 --> 19:49.040
That's the big thing that happened.

19:49.040 --> 19:53.760
And you get this emphasis on people releasing model parameters.

19:53.760 --> 19:58.800
In this earlier phase like here, there was no talk of releasing model parameters

19:58.800 --> 20:03.520
because mostly the models people trained were just good for the task that they had set.

20:04.160 --> 20:09.440
As we move into this era and then certainly this one, these things are meant to be like

20:09.440 --> 20:14.800
general purpose language capabilities or maybe general purpose computer vision capabilities

20:14.800 --> 20:20.240
that we stitch together into a system that can do more than any previous system could do.

20:23.920 --> 20:25.680
Right, so then we have this big thing here.

20:26.640 --> 20:32.880
So that's the feeling now behind all of this certainly beginning in this final phase here

20:32.880 --> 20:35.040
is the transformer architecture.

20:35.040 --> 20:36.720
Just may take the temperature of the room.

20:36.720 --> 20:39.120
How many people have encountered the transformer before?

20:40.560 --> 20:43.600
Right, yeah, it's sort of unavoidable if you're doing this research.

20:43.600 --> 20:48.240
Here's a diagram of it, but I'm not going to go through this diagram now because

20:48.240 --> 20:53.840
starting on Wednesday, we are going to have an entire lecture essentially devoted to unpacking

20:53.840 --> 20:56.000
this thing and understanding it.

20:56.000 --> 21:01.840
All I can say for you now is that I expect you to go on the following journey, which all of us go on.

21:02.480 --> 21:04.880
How on earth does the transformer work?

21:04.880 --> 21:06.880
It looks very, very complicated.

21:07.440 --> 21:12.800
I hope can get you to the point where you feel, oh, this is actually pretty simple components

21:12.800 --> 21:15.920
that have been combined in a pretty straightforward way.

21:15.920 --> 21:17.520
That's your second step on the journey.

21:17.520 --> 21:22.480
The one, the true enlightenment comes from, wait a second, why does this work at all?

21:23.360 --> 21:28.000
And then you're with the entire field trying to understand why these simple things

21:28.000 --> 21:30.720
were brought together in this way have proved so powerful.

21:32.800 --> 21:38.400
The other major thing that happened, which is kind of latent going all the way back to the

21:38.400 --> 21:44.320
start of AI, especially as it relates to linguistics, is this notion of self-supervision,

21:44.320 --> 21:49.680
of distributional learning, because this is going to unlock the door to us just learning

21:49.680 --> 21:52.080
from the world in the most general sense.

21:52.960 --> 21:59.840
In self-supervision, your model's only goal is to learn from co-occurrence patterns in the

21:59.840 --> 22:01.440
sequences that it's trained on.

22:01.440 --> 22:04.960
And the sequences can be language, but they could be language plus

22:04.960 --> 22:10.080
sensor readings, computer code, maybe even images that you embed in this space,

22:10.080 --> 22:11.200
just symbols.

22:11.200 --> 22:15.520
And the model's only goal is to learn from the distributional patterns that they contain.

22:16.640 --> 22:20.000
Or for many of these models to assign high probability

22:20.000 --> 22:23.440
to the attested sequences in whatever data that you pour in.

22:24.160 --> 22:27.120
For this kind of learning, we don't need to do any labeling.

22:27.840 --> 22:32.160
All we need to do is have lots and lots of symbol streams.

22:33.600 --> 22:36.800
And then when we generate from these models, we're sampling from them.

22:36.800 --> 22:40.080
And that's what we all think of when we think of prompting and getting a response back.

22:40.080 --> 22:45.200
But the underlying mechanism is, at least in part, this notion of self-supervision.

22:45.200 --> 22:49.040
And I'll emphasize again, because I think this is really important for why these models are so

22:49.040 --> 22:52.640
powerful, the symbols do not need to be just language.

22:52.640 --> 22:57.840
They can include lots of other things that might help a model piece together,

22:57.840 --> 23:02.640
a full picture of the world we live in, and also the connections between language and

23:02.640 --> 23:06.160
those pieces of the world, just from this distributional learning.

23:07.680 --> 23:13.520
The result of this proving so powerful is the advent of large-scale pre-training.

23:13.520 --> 23:18.240
Because now we're not held back anymore by the need for labeled data.

23:18.240 --> 23:21.600
All we need is lots of data in unstructured format.

23:22.240 --> 23:27.840
This really begins in the era of static word representations like word-to-vec and glove.

23:28.640 --> 23:31.840
And in fact, those teams, and I would say especially the glove team,

23:31.840 --> 23:38.800
they were really visionary in the sense that they not only released a paper and code,

23:39.360 --> 23:41.600
but pre-trained parameters.

23:41.600 --> 23:45.760
This was really brand new for the field, this idea that you would empower people

23:46.320 --> 23:48.640
with model artifacts.

23:48.640 --> 23:54.480
And people started using them as the inputs to recurrent neural networks and other things.

23:54.480 --> 24:01.040
And you started to see pre-training as an important component to doing really well at hard things.

24:03.200 --> 24:06.160
There were some predecessors that I'll talk about next time.

24:06.160 --> 24:11.200
But the really big moment for contextual representations is the ELMO model.

24:11.200 --> 24:14.240
This is the paper, Deep Contextualized Word Representations.

24:14.320 --> 24:20.480
I can remember being at the North American ACL meeting in New Orleans in 2018

24:20.480 --> 24:22.320
at the best paper session.

24:22.320 --> 24:27.200
They had not announced which of the best papers was going to win the Outstanding Paper Award.

24:27.200 --> 24:33.360
But we all knew it was going to be the ELMO paper because the gains that they had reported

24:33.360 --> 24:38.640
from fine-tuning their ELMO parameters on hard tasks or the field were just mind-blowing.

24:38.640 --> 24:43.520
The sort of thing that you really only see once in a kind of generation of this research.

24:43.520 --> 24:44.800
Or so we thought.

24:44.800 --> 24:51.760
Because the next year, Burt came out, same thing, I think same best paper award thing.

24:51.760 --> 24:56.000
The paper already had had huge impact by the time it was even published.

24:57.120 --> 25:00.160
And they too released their model parameters.

25:00.160 --> 25:02.240
ELMO is not transformer-based.

25:02.240 --> 25:05.680
Burt is the first of the sequence of things that's based in the transformer.

25:05.680 --> 25:09.760
And again, lifting all boats even above where ELMO had brought us.

25:10.640 --> 25:12.240
Then we get GPT.

25:12.240 --> 25:14.160
This is the first GPT paper.

25:14.160 --> 25:17.120
And then fast forward a little bit, we get GPT-3.

25:17.120 --> 25:24.240
And that was pre-training at a scale that was previously kind of unimaginable.

25:24.240 --> 25:28.720
Because now we're talking about, you know, for the Burt model, 100 million parameters

25:28.720 --> 25:31.920
and for GPT-3, well north of 100 billion.

25:32.880 --> 25:34.800
Different order of magnitude.

25:34.800 --> 25:37.840
And what we started to see is emergent capabilities.

25:39.440 --> 25:41.120
That model size thing is important.

25:41.760 --> 25:45.360
Again, this is a sort of feeling of progress and maybe also despair.

25:45.360 --> 25:50.160
I think I can lift your spirits a little bit, but we should think about model size.

25:50.160 --> 25:52.640
So I have years along the x-axis again.

25:53.280 --> 25:58.800
And I have model size going from 100 million to 1 trillion here on a logarithmic scale.

25:59.520 --> 26:03.600
So 2018 GPT, that's like 100 million Burt.

26:03.600 --> 26:06.240
I think it's 300 million for the large one.

26:06.240 --> 26:08.160
Okay, GPT-2 even larger.

26:08.960 --> 26:10.640
Megatron 8.3 billion.

26:10.640 --> 26:13.920
I remember when this came out, I probably laughed.

26:13.920 --> 26:15.200
Maybe I thought it was a joke.

26:15.200 --> 26:19.760
I certainly thought it was some kind of typo because I couldn't imagine that it was actually

26:19.760 --> 26:21.440
billion like with a B there.

26:23.120 --> 26:26.160
But now that's, you know, we take that for granted.

26:26.160 --> 26:30.240
Megatron 11 billion, this is 20, 21 or so.

26:30.240 --> 26:34.880
Then we get GPT-3 reportedly at 175 billion parameters.

26:34.880 --> 26:38.240
And then we get this thing where it seems like we're doing typos again.

26:38.240 --> 26:45.040
Megatron Turing NLG was like 500, and then Palm is 540 billion parameters.

26:45.680 --> 26:50.640
And I guess there are rumors that we have gone upward all the way to a trillion, right?

26:51.440 --> 26:53.680
There's an undeniable trend here.

26:54.240 --> 26:59.120
I think there is something to this trend, but we should reflect on it a little bit.

26:59.120 --> 27:03.040
One thing I want to say is there's a noteworthy pattern.

27:03.040 --> 27:09.440
A very few entities have participated in this very large, in this race for very large models.

27:09.440 --> 27:13.520
We've got like Google, NVIDIA, Meta and OpenAI, right?

27:14.400 --> 27:16.640
And that was actually a real cause for concern.

27:16.640 --> 27:22.160
I remember being at a workshop between Stanford and OpenAI where the number one source of

27:22.160 --> 27:28.960
consternation was really that only OpenAI at that point had trained these really large models.

27:28.960 --> 27:32.880
And after that, predictably, these other large tech companies kind of caught up.

27:33.600 --> 27:38.320
But it was still for a while looking like a story of real centralization of power.

27:39.680 --> 27:42.400
That might still be happening, but I think there's a reason to be optimistic.

27:42.400 --> 27:47.200
So here at Stanford, the Helm Group, which is part of the Center for Research on Foundation

27:47.200 --> 27:52.560
Models, led this incredibly ambitious project of evaluating lots of language models.

27:52.560 --> 27:57.360
And one thing that emerges from that is that we have a more healthy ecosystem now.

27:57.360 --> 28:01.440
So we have these like loose collectives, big science and a Luther are both kind of fully

28:01.440 --> 28:07.600
open source groups of researchers. We've got, well, one academic institution represented.

28:07.600 --> 28:10.640
This could be a little bit embarrassing for Stanford, maybe we'll correct that.

28:11.280 --> 28:14.880
And then maybe the more important thing is that we have lots of startups represented.

28:14.880 --> 28:20.800
So these are well-funded but relatively small outfits that are producing outstanding language

28:20.800 --> 28:24.720
models. And so the result, I think we're going to see much more of this.

28:24.800 --> 28:28.400
And then we'll worry less about centralization of power.

28:28.400 --> 28:31.840
There's plenty of other things to worry about, so we shouldn't get sanguine about this.

28:31.840 --> 28:36.160
But this particular point, I think, is being alleviated by current trends.

28:36.720 --> 28:42.080
And there's another aspect of this too, which is you have the scary rise in model size.

28:42.080 --> 28:48.080
But what is happening right now as we speak in a very quick way is we're seeing a push

28:48.080 --> 28:53.360
towards smaller models. And in particular, we're seeing that models that are in the range of like

28:53.360 --> 28:58.240
10 billion parameters can be highly performant, right? So we have the Flan models.

28:59.280 --> 29:03.360
We have Lama. And then here at Stanford, they released the Alpaca thing.

29:03.360 --> 29:07.840
And then Databricks released Hello Dolly model. These are all models that are like

29:07.840 --> 29:12.960
8 to 10 billion parameters, which I know this sounds funny because I laughed a few years ago

29:12.960 --> 29:18.000
when the Megatron model had 8.3 billion. And now what I'm saying to you is that this is relatively

29:18.080 --> 29:24.640
small, but so it goes. And the point is that a 10 billion parameter model is one that could be run

29:24.640 --> 29:30.640
on regular old commercial hardware. Whereas these monsters up here, really, you have lots of

29:30.640 --> 29:35.600
pressures towards centralization of power there because almost no one can work with them.

29:35.600 --> 29:40.000
But anyone essentially can work with Alpaca. And it won't be long before we've got

29:40.000 --> 29:43.840
the ability to kind of work with it on small devices and things like that.

29:43.840 --> 29:48.880
And that, too, is really going to open the door to lots of innovation. I think that will

29:48.880 --> 29:53.680
bring some good. And I think it will bring some bad, but it is certainly a meaningful change

29:53.680 --> 29:57.760
from this scary trend that we were seeing until four months ago.

30:02.080 --> 30:08.160
As a result of these models being so powerful, people started to realize that you can get a

30:08.160 --> 30:14.080
lot of mileage out of them simply by prompting them. When you prompt one of these very large

30:14.080 --> 30:19.120
models, you put it in a temporary state by inputting some text, and then you generate a

30:19.120 --> 30:23.200
sample from the model using some technique and you see what comes out, right? So if you type

30:23.200 --> 30:29.920
into one of these models, better late than, it's going to probably spit out never. If you put in

30:29.920 --> 30:35.600
every day, I eat breakfast, lunch, and it will probably say dinner. And you might have an intuition

30:35.600 --> 30:40.320
that the reasons, the causes for that are kind of different. The first one is a sort of idiom,

30:40.320 --> 30:46.080
so that it could just learn from co-occurrence patterns in text transparently. For the second one,

30:46.080 --> 30:51.680
we kind of interpreted as humans as reflecting something about routines, but you should remind

30:51.680 --> 30:57.760
yourself that the mechanism is the same as in the first case. This was just a bunch of co-occurrence

30:57.760 --> 31:03.840
patterns. A lot of people described their routines in text and the model picked up on that. And carry

31:03.920 --> 31:08.720
that thought forward as you think about things like the president of the U.S. is. When it fills

31:08.720 --> 31:15.040
that in with Biden or whoever, it might look like it is offering us factual knowledge and maybe in

31:15.040 --> 31:20.960
some sense it is, but it's the same mechanism as for those first two examples. It is just learning

31:20.960 --> 31:26.160
from the fact that a lot of people have expressed a lot of text that look like the president of the

31:26.160 --> 31:32.640
U.S. is Joe Biden, and it is repeating that back to us. And so definitely, if you ask a model,

31:32.640 --> 31:38.720
something like the key to happiness is you should remember that this is just the aggregate of a lot

31:38.720 --> 31:44.800
of data that it was trained on. It has no particular wisdom to offer you necessarily beyond what was

31:44.800 --> 31:54.800
encoded latently in that giant sea of mostly unaudited unstructured text. Yeah, question.

31:56.000 --> 32:00.400
I guess, you know, it would be kind of hard to get something like this, but if we had like a corpus

32:00.400 --> 32:05.040
of just like, you know, although language is right, but literally all of the facts were wrong.

32:05.040 --> 32:10.800
Like, we just imagined like a very factually incorrect corpus. Like, I guess I'm getting

32:10.800 --> 32:18.480
how, like, how do we inject like truth into like these corpuses? It's a question that bears

32:18.480 --> 32:24.880
repeating. How do we inject truth? It's a question you all could think about. What is truth, of course.

32:24.960 --> 32:31.040
But also, what would that mean? And how would we achieve it? And even if we did back off to something

32:31.040 --> 32:36.560
like, how would we ensure self-consistency for a model? Or, you know, at the level of a worldview

32:36.560 --> 32:42.560
or a set of facts, even those questions which seem easier to pose are incredibly difficult

32:42.560 --> 32:47.600
questions in the current moment, where our only mechanisms are basically that self-supervision

32:47.600 --> 32:53.200
thing that I described, and then a little bit of what I'll talk about next. But none of the

32:53.200 --> 32:57.520
structure that we used to have where we would have a database of knowledge and things like that,

32:58.240 --> 32:59.600
that is posing problems.

33:05.200 --> 33:10.160
The prompting thing, we take this a step forward, right? So the GPT-3 paper, remember that's that

33:10.160 --> 33:17.120
175 billion parameter monster. The eye-opening thing about that is what we now call in-context

33:17.120 --> 33:22.400
learning, which was just the notion that for these very large, very capable models, you could

33:22.400 --> 33:28.320
input a bunch of text, like here's a passage, and maybe an example of the kind of behavior that

33:28.320 --> 33:34.080
you wanted, and then your actual question, and the model would do a pretty good job at answering

33:34.080 --> 33:39.360
the question. And what you're doing here is, with your context passage and your demonstration,

33:39.360 --> 33:45.360
pushing the model to be extractive, to find an answer to its question in the context passage,

33:46.080 --> 33:51.280
and then the observation of this paper is that they do a pretty good job at following that same

33:51.280 --> 33:57.360
behavior for the actual target question at the bottom here. Remember, this is all just prompting,

33:57.360 --> 34:02.320
putting the model in a temporary state, and seeing what comes out. You don't change the model,

34:02.320 --> 34:09.040
you just prompt it. This, in 2012, if you had asked me whether this was a viable path forward

34:09.040 --> 34:15.120
for a class project, I want to prompt a RNN or something. I would have advised you as best I

34:15.120 --> 34:20.240
could to choose some other topic, because I never would have guessed that this would work. So the

34:20.240 --> 34:26.720
mind-blowing thing about this paper and everything that's followed is that we might be nearing the

34:26.720 --> 34:33.760
point where we can design entire AI systems on the basis of this simple in-context learning mechanism,

34:33.760 --> 34:40.160
transformatively different from anything that we saw before. In fact, let me just emphasize this

34:40.160 --> 34:46.160
a little bit. It is worth dwelling on how strange this is. For those of you who have been in the

34:46.160 --> 34:52.240
field a little while, just contrast what I described in-context learning with the standard

34:52.240 --> 34:59.360
mode of supervision. Let's imagine for a case here that we want to train a model to detect

34:59.360 --> 35:05.280
nervous anticipation, and I have picked this because this is a very particular human emotion.

35:05.280 --> 35:10.240
And in the old mode, we would need an entire dedicated model to this, right? We would collect

35:10.240 --> 35:16.480
a little data set of positive and negative instances of nervous anticipation, and we would

35:16.480 --> 35:22.320
train a supervised classifier on feature representations of these examples over here,

35:22.320 --> 35:28.320
learning from this binary distinction. We would need custom data and a custom model

35:28.320 --> 35:35.440
for this particular task in all likelihood. In this new mode, few shot in-context learning,

35:35.440 --> 35:40.720
we essentially just prompt the model, hey, model, here's an example of nervous anticipation. My

35:40.720 --> 35:45.680
palms started to sweat as the Lotto numbers were read off. Hey, model, here's an example without

35:45.680 --> 35:52.880
nervous anticipation and so forth. And it learns from all those symbols that you put in and their

35:52.880 --> 36:00.320
co-occurrences something about nervous anticipation. On the left for this model here, I've written

36:00.320 --> 36:05.520
out nervous anticipation, but remember that has no special status. I've structured the model

36:05.520 --> 36:10.800
around the binary distinction, the one and the zero. And everything about the model is geared

36:10.800 --> 36:17.120
toward my learning goal. On the right, nervous anticipation is just more of the symbols that

36:17.120 --> 36:23.200
I've put into the model. And the eye-opening thing, again, about the GPT-3 paper and what's

36:23.200 --> 36:29.840
followed is that models can learn, be put in a temporary state and do well at tasks like this.

36:31.280 --> 36:38.720
Now, I talked about self-supervision before, and I think that is a major component to the success

36:38.720 --> 36:44.240
of these models. But it is increasingly clear that it is not the only thing that is driving

36:44.240 --> 36:50.960
learning in the best models in this class. The other thing that we should think about

36:50.960 --> 36:56.240
is what's called reinforcement learning with human feedback. This is a diagram from the

36:56.240 --> 37:02.000
chat GPT blog post. There are a lot of details here, but really two of them are important for us

37:02.000 --> 37:10.720
for right now. The first is that in a phase of training these models, people are given inputs

37:10.720 --> 37:16.960
and ask themselves to produce good outputs for those inputs. So you might be asked to do a little

37:16.960 --> 37:22.240
Python program, and you yourself as an annotator might write that Python program, for example.

37:22.240 --> 37:26.080
So that's highly skilled work that depends on a lot of human intelligence,

37:26.640 --> 37:32.560
and those examples, those pairs are part of how the model is trained. And that is so important

37:32.560 --> 37:37.840
because that takes us way beyond just learning from co-occurrence patterns of symbols and text.

37:38.400 --> 37:45.440
It is now back to a very familiar story from all of AI, which is that it's not magic. What is

37:45.440 --> 37:50.880
happening is that a lot of human intelligence is driving the behavior of these systems.

37:51.840 --> 37:56.640
And that happens again at step two here. So now the model produces different outputs,

37:56.640 --> 38:02.560
and humans come in and rank those outputs, again expressing direct human preferences

38:02.560 --> 38:08.240
that take us well beyond self-supervision. So we should remember, we had that brief moment where

38:08.240 --> 38:13.200
it looked like it was all unstructured unlabeled data, and that was important to unlocking these

38:13.200 --> 38:19.680
capacities, but now we are back at a very labor-intensive human capacity here, driving

38:20.480 --> 38:23.600
what looked like the really important behaviors for these models.

38:27.120 --> 38:31.520
Final step, which I think actually intimately relates to that instruct tuning that I just

38:31.520 --> 38:35.520
described. That's a kind of way of summarizing this reinforcement learning with human feedback.

38:36.240 --> 38:40.480
And this is what's called step by step or chain of thought reasoning. Now we're thinking about

38:40.480 --> 38:45.440
the prompts that we use for these models. So suppose we asked ourselves a question like,

38:45.440 --> 38:51.280
can models reason about negation? To give an example, does the model know that if the customer

38:51.280 --> 38:55.920
doesn't have any auto loan, sorry, doesn't have any loans, then the customer doesn't have any

38:55.920 --> 39:00.400
auto loans? It's a simple example. It's the sort of reasoning that you might have to do if you're

39:00.400 --> 39:05.760
thinking about a contract or something like that, whether a rule has been followed. And it just

39:05.760 --> 39:12.160
involves negation, our old friend from the start of the lecture. Now in the old school

39:12.160 --> 39:18.960
prompting style, all the way back in 2021, we would kind of naively just input, is it true that

39:18.960 --> 39:23.280
if the customer doesn't have any loans, then the customer doesn't have any auto loans into one of

39:23.280 --> 39:28.240
these models, and we would see what came back. And here it says, no, this is not necessarily

39:28.240 --> 39:32.800
true. A customer can have auto loans without having any other loans, which is the reverse

39:32.800 --> 39:38.240
of the question that I asked. Again, kind of showing it doesn't deeply understand what we've

39:38.240 --> 39:44.880
put in here. It just kind of does an act that looks like it did. And that's worrisome. But

39:44.880 --> 39:48.720
we're learning how to communicate with these very alien creatures. Now we do what's called

39:48.720 --> 39:53.200
step-by-step prompting. This is the cutting edge thing. You would just tell the model that it was

39:53.200 --> 39:58.800
in some kind of logical or common sense reasoning exam that matters to the model. Then you could

39:58.800 --> 40:03.840
give some instructions, and then you could give an example in your prompts of the kind of thing

40:03.840 --> 40:09.120
it was going to see. And then finally you could prompt it with your premise and then your question.

40:09.680 --> 40:14.240
And the model would spit out something that looked really good. Here I won't bother going

40:14.240 --> 40:20.880
through the details, but with that kind of prompt, the model now not only answers and reasons

40:20.880 --> 40:27.360
correctly, but also offers a really nice explanation of its own reasoning. The capacity was there,

40:27.360 --> 40:32.400
it was latent, and we didn't see it in the simple prompting mode, but the more sophisticated

40:32.400 --> 40:39.280
prompting mode elicited it. And I think this is in large part the result of the fact that this

40:39.280 --> 40:44.720
model was instruct tuned. And so people actually taught it about how that markup is supposed to

40:44.720 --> 40:49.360
work and how it's supposed to think about prompts like this. So the combination of all that human

40:49.360 --> 40:54.720
intelligence and the capacity of the model led to this really interesting and much better behavior.

40:55.040 --> 41:05.200
That is a glimpse of the foundations of all of this, I would say. Of course, we're going to unpack

41:05.200 --> 41:09.840
all of that stuff as we go through the quarter, but I hope you're getting a sense for it.

41:09.840 --> 41:13.520
Are there questions I can answer about it, things I could circle back on? Yes?

41:14.560 --> 41:21.360
The human brain has about 100 billion neurons, is my understanding. And I'm not sure how many

41:21.440 --> 41:25.120
parameters that might be, maybe like 10 trillion parameters or something like that.

41:25.920 --> 41:30.720
Are we approaching a point where these machines can start emulating the human brain or is there

41:30.720 --> 41:35.760
something to the language instinct or, you know, instincts of all kinds that maybe take

41:35.760 --> 41:43.200
into the human brain? Oh, it's nothing but big questions today. Right, so the question is kind

41:43.200 --> 41:47.600
of like, what is the relationship between the models we're talking about and the human brain?

41:47.600 --> 41:52.640
And you raise that in terms of the size, and I guess the upshot of your description was that

41:52.640 --> 41:59.040
these models remain smaller than the human brain. I think that's reasonable. It's tricky,

41:59.040 --> 42:04.160
though. On the one hand, they obviously have superhuman capabilities. On the other hand,

42:04.160 --> 42:10.080
they fall down in ways that humans don't. It's very interesting to ask why that difference exists.

42:11.040 --> 42:17.360
And maybe that would tell us something about the limitations of learning from scratch versus

42:17.360 --> 42:23.200
being initialized by evolution the way all of us were. I don't know, but I would say that

42:23.920 --> 42:29.600
underlying your whole line of questioning is the question, can we use these models to eliminate

42:29.600 --> 42:34.640
questions of neuroscience and cognitive science? And I think we should be careful,

42:34.640 --> 42:41.520
but that the answer is absolutely yes. And in fact, the increased ability of these models to learn

42:41.520 --> 42:47.520
from data has been really illuminating about certain kind of recalcitrant questions from

42:47.520 --> 42:52.080
cognitive science in particular. You have to be careful because they're so different from us,

42:52.080 --> 42:58.080
these models. On the other hand, I think they are helping us understand how to differentiate

42:58.080 --> 43:02.640
different theories of cognition. And ultimately, I think they will help us understand cognition

43:02.640 --> 43:09.600
itself. And I would, of course, welcome projects that were focused on those cognitive questions

43:09.600 --> 43:15.200
in here. This is a wonderful space in which to explore this kind of more speculative angle,

43:16.080 --> 43:18.960
connecting AI to the cognitive sciences.

43:23.520 --> 43:25.680
Other questions, comments? Yes, in the back.

43:26.240 --> 43:31.200
I would be curious to understand whether, I mean, partially following up on the brain thing,

43:31.200 --> 43:36.640
just to use a metaphor of our brain not being just one huge lump of neurons, but being separated

43:36.720 --> 43:42.160
into different areas. And then also thinking about the previous phase that you talked about,

43:42.160 --> 43:47.440
about breaking up the models and potentially having a model in the front that decides which

43:47.440 --> 43:53.200
domain our question falls into, and then having different sub-models. And I'm wondering whether

43:53.200 --> 43:58.480
that's arising, whether we're going to touch on an architecture like that. Because it just seems

43:58.480 --> 44:05.600
natural to me because prompting a huge model is just very expensive computationally. It feels like

44:06.240 --> 44:10.720
combining big models and logic trees could be a cool approach.

44:10.720 --> 44:15.280
I love it. Yeah, like one quick summary of what you said would relate directly to your question.

44:15.280 --> 44:22.160
The modularity of mind is an important old question about human cognition, to what extent

44:22.160 --> 44:30.320
are our abilities modularized in the mind-brain? With these current models, which have a capacity

44:30.320 --> 44:34.080
to do lots of different things if they have the right pre-training and the right structure,

44:34.080 --> 44:40.800
we could ask, does modularity emerge naturally? Or do they learn non-modular solutions? Both of

44:40.800 --> 44:46.080
those seem like they could be indirect evidence for how people work. Again, we have to be careful

44:46.080 --> 44:50.160
because these models are so different from us. But as a kind of existence proof, for example,

44:50.160 --> 44:55.200
that modularity was emergent from otherwise unstructured learning, that would be certainly

44:55.200 --> 45:00.560
eye-opening, right? I have no idea. Yeah, I don't know whether there are results for that.

45:00.640 --> 45:08.080
Are there results? No, just kind of a follow-up question on that as well. So, given how closed

45:08.080 --> 45:14.400
all these big models are, how could we interact with the models in such a way that helps us learn

45:14.400 --> 45:20.240
if there is modularity? Because we literally can only interact with the, how do we go about

45:20.240 --> 45:25.920
starting that? Right, so the question is, you know, the closed-off nature of a lot of these

45:26.000 --> 45:30.960
models has been a problem. We can access the open AI models, but only through an API. We don't get

45:30.960 --> 45:37.440
to look at their internal representations, and that has been a blocker. But I mentioned the rise of

45:37.440 --> 45:43.120
these 10 billion parameter models as being performant and interesting, and those are models that,

45:43.120 --> 45:47.520
with the right hardware, you can dissect a little bit. And I think that's just going to get better

45:47.520 --> 45:52.240
and better, and so we'll be able to, you know, peer inside them in ways that we haven't been able to

45:52.240 --> 45:59.200
until recently. Yeah. And in fact, like, we're going to talk a lot about explainability. That's

45:59.200 --> 46:04.160
a major unit of this course, and I think it's an increasingly important area of the whole field

46:04.160 --> 46:09.200
that we have techniques for understanding these models so that we know how they're going to behave

46:09.200 --> 46:13.920
when we deploy them. And it would be wonderfully exciting if you all wanted to try to scale the

46:13.920 --> 46:18.640
methods we talk about to a model that was as big as eight or 10 billion parameters.

46:18.720 --> 46:23.680
Ambitious just to do that, but then maybe a meaningful step forward. Yeah.

46:24.640 --> 46:29.440
I have a question back to, like, this baseball cap prompt that we were discussing. So I suppose,

46:29.440 --> 46:34.400
like, a part of the way that we discuss rules is, like, there is a little bit of ambiguity

46:34.400 --> 46:38.720
for, like, human interpretation, like, for example, in the honor code and the fundamental standard,

46:38.720 --> 46:44.240
like, it's intentionally ambiguous so that it's context dependent. And so, like, the idea is that

46:44.240 --> 46:49.280
there's, like, this inherent underlying value system that, like, affords whatever the rules

46:49.280 --> 46:55.040
that are written out are. And so that's, like, the primary form of evaluation. And so, I guess,

46:55.040 --> 46:59.280
like, how does that play into, then, how these language models are understanding? Like, is there

46:59.280 --> 47:04.240
some form of encoded or understanding, understood deeper value system that's encoded into them?

47:06.240 --> 47:10.800
You could certainly ask. I mean, the essence of your question is, could we, with analysis techniques,

47:10.880 --> 47:16.400
say, find out that a model had a particular belief system that was guiding its behavior?

47:17.040 --> 47:21.760
I think we can ask that question now. It sounds fantastically difficult, but maybe piecemeal

47:21.760 --> 47:26.560
we could get, make some progress on it for sure. Yeah, I want to return to the MLB one, though,

47:26.560 --> 47:31.600
because, well, as you'll see, and as I think we already saw, these models purport to offer

47:31.600 --> 47:34.960
evidence from a rule book. And that's where I feel stuck.

47:34.960 --> 47:43.760
You're keeping score at home. I posted the answer and some other stuff in the class discussion.

47:43.760 --> 47:49.680
Wonderful. Thank you. Yes?

47:49.680 --> 47:56.080
Can we just hook up these models to a large database of actually required information

47:56.080 --> 47:59.600
that's been encyclopedia and allow it to, you know, work stuff up?

48:00.560 --> 48:06.720
Oh, well, kind of yes. Actually, this is the sort of solution that I want to advocate for.

48:06.720 --> 48:08.240
I'm going to do this in a minute. Yeah.

48:11.120 --> 48:15.200
Here, let's, so we'll do this overview. I want to give you a feel for how the course will work

48:15.200 --> 48:20.480
and then dive into some of our major themes. So high-level overview. We've got these topics,

48:20.480 --> 48:25.200
contextual representations, transformers and stuff, multi-domain sentiment analysis.

48:25.200 --> 48:29.760
That will be the topic of the first homework and it's going to build on the first unit there.

48:30.560 --> 48:34.800
Retrieval augmented in context learning. This is where we might hook up to a database and get

48:34.800 --> 48:40.560
some guarantees about how these models will behave. Compositional generalization. In case

48:40.560 --> 48:44.800
you were worried that all the tasks were solved, I'm going to confront you with a task, a seemingly

48:44.800 --> 48:50.080
simple task about semantic interpretation that you will, well, I think it will not be solved.

48:50.080 --> 48:53.520
I mean, those could be famous last words because who knows what you all are capable of,

48:54.160 --> 48:58.960
but it's a very hard task that we will pose. We'll talk about benchmarking and adversarial

48:58.960 --> 49:03.360
training and testing. Increasingly important topics as we move into this mode where everyone

49:03.360 --> 49:08.400
is interacting with these large language models and feeling impressed by their behavior. We need

49:08.400 --> 49:13.680
to take a step back and rigorously assess whether they actually are behaving in good ways or whether

49:13.680 --> 49:17.280
we're just biased toward remembering the good things and forgetting the bad ones.

49:18.400 --> 49:22.160
We'll do model introspection. That's the explainability stuff that I mentioned and finally

49:22.160 --> 49:27.440
methods and metrics. And as you can see for the like five, six, and seven, that's going to be in

49:27.440 --> 49:32.560
the phase of the course where you're focused on final projects. And I'm hoping that that gives you

49:32.560 --> 49:38.640
tools to write really rich final papers that have great analysis in them and really excellent

49:38.640 --> 49:43.360
assessments. And then for the work that you'll do, we're going to have three assignments.

49:44.320 --> 49:48.480
And each one of the assignments is paired with what we call a bake-off, which is an informal

49:48.560 --> 49:54.000
competition around data and modeling. Essentially, the homework problems ask you to set up some

49:54.000 --> 49:59.360
baseline systems and get a feel for a problem. And then you write your own original system

49:59.920 --> 50:04.800
and you enter that into the bake-off. And we have a leaderboard on Gradescope and the team

50:04.800 --> 50:09.840
is going to look at all your submissions and give out some prizes for top-performing systems,

50:09.840 --> 50:15.280
but also systems that are really creative or interesting or ambitious or something like that.

50:15.280 --> 50:20.960
And that has always been a lot of fun and also really illuminating because it's like crowdsourcing

50:20.960 --> 50:26.720
a whole lot of different approaches to a problem. And then as a group, we can reflect on what worked

50:26.720 --> 50:31.680
and what didn't and look at the really ambitious things that you all try. So that's my favorite part.

50:31.680 --> 50:38.720
We have three offline quizzes and this is just as a way to make sure you have incentives to really

50:38.800 --> 50:44.880
immerse yourself in the course material. And those are done on Canvas. There's actually a

50:44.880 --> 50:49.680
fourth quiz, which I'll talk a little bit about probably next time, that is just making sure

50:49.680 --> 50:55.200
you understand the course policies. That's quiz zero. You can take it as many times as you want,

50:55.200 --> 51:00.880
but the idea is that you will have some incentive to learn about policies like due dates and so

51:00.880 --> 51:06.480
forth. And then the real action is in the final project and that will have a lit review phase,

51:06.560 --> 51:12.240
an experiment protocol, and a final paper. Those three components, you'll probably do those in teams

51:12.240 --> 51:16.720
and throughout all of that work, you'll be mentored by someone from the teaching team.

51:16.720 --> 51:23.040
And as I said before, we have this incredibly expert teaching team, lots of varied expertise,

51:23.040 --> 51:27.600
a lot of experience in the field. And so we hope to align you with the person,

51:28.240 --> 51:33.280
with someone who's really aligned with your project goals. And then I think you can go really,

51:33.280 --> 51:38.800
really far. Yeah? It looks like we've got one quarter already looking forward to Bake Offs.

51:38.800 --> 51:44.560
I don't understand what kids get obsessed about this stuff. On the final project,

51:44.560 --> 51:51.280
is this more of an academic paper or a rather about building working code and

51:52.480 --> 51:56.800
showing the state of the art? Great question. For the first one, the Bake Offs. Yes,

51:56.800 --> 52:02.160
it is easy to get obsessed with your Bake Off entry. I would say that if you get obsessed

52:02.160 --> 52:09.200
and you do really well, just make that into your final project. All three of them are really

52:09.200 --> 52:13.440
important problems. They are not idle work. I mean, one of them is on retrieval augmented

52:13.440 --> 52:17.600
in-context learning, which is one of my core research focuses right now. So is compositional

52:17.600 --> 52:21.680
generalization. If you do something really interesting for a Bake Off, make it your final

52:21.680 --> 52:26.960
paper and then go on to publish it. For the second part of your question, I would say that the core

52:26.960 --> 52:31.760
goal is to get you to produce something that could be a research contribution in the field.

52:32.320 --> 52:37.680
And we have lots of success stories. I've got links at the website to people who have gone on to

52:37.680 --> 52:43.840
publish their final paper as an NLP paper. I'm careful the way I say that. They didn't literally

52:43.840 --> 52:48.640
publish the final paper because in 10 weeks, almost no one can produce a publishable paper.

52:48.640 --> 52:53.760
It's just not enough time. But you could form the basis for then working a little bit more

52:53.760 --> 52:58.080
or a lot more and then getting a really outstanding publication out of it. And I would

52:58.080 --> 53:02.400
say that that's the default goal. The nature of the contribution though is highly varied.

53:02.400 --> 53:06.960
We have one requirement, which is that the final paper have some quantitative evaluation in it.

53:07.760 --> 53:11.840
But there are a lot of ways to satisfy that requirement and then you could be serving

53:11.840 --> 53:17.360
many different questions in the field for some expansive notion of the field as well.

53:18.320 --> 53:33.120
Background materials. So I should say that officially, we are presupposing CS224N or CS224S,

53:33.120 --> 53:38.080
as prerequisites for the course. And what that means is that I'm going to skip a lot of the

53:38.080 --> 53:44.800
fundamentals that we have covered in past years. If you need a refresher, check out the background

53:44.800 --> 53:51.440
page of the course site. It covers fundamentals of scientific computing, static vector representations

53:51.440 --> 53:57.680
like Word2Vec and Glove, and supervised learning. And I'm hoping that that's enough of a refresher.

53:57.680 --> 54:03.600
If you look at that material and find that it too is kind of beyond where you're at right now,

54:03.600 --> 54:08.000
then contact us on the teaching team and we can think about how to manage that.

54:08.960 --> 54:13.520
But officially, this is a course that presupposes CS224N.

54:16.080 --> 54:20.080
And then the core goals, this kind of relates to that previous question. Hands-on experience with

54:20.080 --> 54:26.080
a wide range of problems. Mentorship from the teaching team to guide you through projects and

54:26.080 --> 54:31.840
assignments. And then really the central goal here is to make you the best, that is most insightful,

54:31.840 --> 54:38.640
most responsible, most flexible NLU researcher and practitioner that you can be for whatever

54:38.640 --> 54:43.600
you decide to do next. And we're assuming that you have lots of diverse goals that somehow connect

54:43.600 --> 54:55.760
with NLU. All right, let's do some course themes unless there are questions. I have a whole final

54:55.760 --> 55:02.000
section of this slideshow that's about the course materials and requirements and stuff

55:02.000 --> 55:05.680
might save that for next time. And you can check it out at the website and you'll be

55:05.680 --> 55:11.600
forced to engage with it for quiz zero. I thought instead I would dive back into the

55:11.600 --> 55:14.080
content part of this unless there are questions or comments.

55:14.400 --> 55:25.920
All right, first course theme, transformer-based pre-training. So starting with the transformer,

55:25.920 --> 55:30.560
we want to talk about core concepts and goals, give you a sense for what these models are like,

55:31.200 --> 55:36.240
why they work, what they're supposed to do, all of that stuff. We'll talk about a bunch of different

55:36.240 --> 55:41.840
architectures. There are dozens and dozens of them, but I hope that I have picked enough of them

55:41.840 --> 55:47.120
with the right selection of them to give you a feel for how people are thinking about these models

55:47.120 --> 55:52.000
and the kind of innovations they've brought in that have led to real meaningful advancement,

55:52.000 --> 55:56.480
just at the level of architectures. We'll also talk about positional encoding, which I think

55:56.480 --> 56:01.680
maybe a lot of us have been surprised to see just how important that is as a differentiator for

56:01.680 --> 56:08.000
different approaches in this space. We'll talk about distillation, taking really large models

56:08.000 --> 56:13.920
and making them smaller. It's an important goal for lots of reasons and an exciting area of research.

56:13.920 --> 56:18.880
And then as I mentioned, it's going to do a little lecture for us on diffusion objectives for these

56:18.880 --> 56:25.360
models, and then it's going to talk about practical pre-training and fine-tuning. I'm going to enlist

56:25.360 --> 56:30.080
the entire teaching team to do guest lectures, and these are the two that I've lined up so far.

56:31.760 --> 56:36.320
And that will kind of culminate or be aligned with this first homework in Bakeoff, which has a

56:36.320 --> 56:40.720
multi-domain sentiment. I'm going to give you a bunch of different sentiment data sets,

56:40.720 --> 56:44.480
and you're going to have to design one system that can kind of succeed on all of them.

56:45.040 --> 56:50.000
And then for the Bakeoff, we have an unlabeled data set for you. We have the labels, but you won't.

56:50.640 --> 56:56.480
And that has data that's like what you developed on, and then some mystery examples that you will

56:56.480 --> 57:01.520
not really be able to anticipate. And we're going to see how well you do at handling all of these

57:01.520 --> 57:09.360
different domains with one system. And this is by way of kind of, again, a refresher on core

57:09.360 --> 57:14.160
concepts in supervised learning and really getting you to think about transformers, although we're

57:14.160 --> 57:18.240
not going to constrain the kind of solution that you offer for your original system.

57:22.480 --> 57:28.240
Our second major theme will be retrieval augmented in-context learning, a topic

57:29.120 --> 57:36.080
that I would not even have dreamt of five years ago and seemed kind of infeasible three years ago,

57:36.080 --> 57:41.680
and that we first did two years one year ago. Oh goodness. I think this is only the second time,

57:41.680 --> 57:45.440
but I had to redo it entirely because things have changed so much.

57:47.360 --> 57:53.040
Here's the idea. We have two characters so far in our kind of emerging narrative for NLU.

57:53.680 --> 57:57.840
On the one hand, we have this approach that I'm going to call LLMs for everything,

57:57.840 --> 58:03.520
large language models for everything. You input some kind of question. Here I've chosen a very

58:03.520 --> 58:09.360
complicated question. Which MVP of a game Red Flaherty umpired was elected to the Baseball

58:09.360 --> 58:13.920
Hall of Fame? And hats off to you if you know that the answer is Sandy Kofax.

58:16.000 --> 58:22.000
The LLMs for everything approach is that you just type that question in and the model gives you an

58:22.000 --> 58:28.480
answer. And hopefully you're happy with the answer. The other character that I'm going to

58:28.480 --> 58:33.600
introduce here is what I'm going to call retrieval augmented. So I have the same question at the top

58:33.600 --> 58:37.360
here, except now this is going to proceed differently. The first thing that we will do

58:37.920 --> 58:44.320
is take some large language model and encode that query into some numerical representation.

58:45.280 --> 58:50.080
That's sort of familiar. The new piece is that we're going to also have a knowledge store

58:50.640 --> 58:58.240
which you could think of as an old fashioned web index, right? Just a knowledge store of documents

58:58.240 --> 59:04.160
with the modern twist that now all of the documents are also represented by large language models.

59:04.160 --> 59:08.720
But fundamentally, this is an index of a sort that drives all web search right now.

59:09.680 --> 59:15.120
We can score documents with respect to queries on the basis of these numerical representations.

59:15.120 --> 59:21.280
And if we want to, we can reproduce the classic search experience. Here I've got a ranked list

59:21.280 --> 59:27.680
of documents that came back from my query, just like when you do Google as of the last time I googled.

59:28.880 --> 59:33.440
But in this mode, we can continue, right? We could have another language model slurp up those

59:33.440 --> 59:39.040
retrieved documents and synthesize them into an answer. And so here at the bottom I've got,

59:39.040 --> 59:43.760
it's kind of small, but it's the same answer over here, although notably this answer is now

59:43.760 --> 59:49.520
decorated with links that would allow you the user to track back to what documents actually

59:50.560 --> 59:56.480
provided that evidence. Whereas on the left, who knows where that information came from.

59:56.480 --> 59:58.560
And that's kind of what we were already grappling with.

01:00:00.640 --> 01:00:05.360
This is an important societal need because this is taking over web search. What are our goals

01:00:05.360 --> 01:00:10.400
for this kind of model here? So first, we want synthesis, fluency, right? We want to be able to

01:00:10.400 --> 01:00:16.160
take information from multiple documents and synthesize it down into a single answer. And I

01:00:16.160 --> 01:00:21.040
think both of the approaches that I just showed you are going to do really well on that. We also

01:00:21.040 --> 01:00:26.880
need these models to be efficient, to be updatable, because the world is changing all the time.

01:00:27.680 --> 01:00:33.040
We need it to track provenance and maybe invoke something like factuality, but certainly

01:00:33.040 --> 01:00:37.520
provenance. We need to know where the information came from. And we need some safety and security.

01:00:37.520 --> 01:00:42.240
We need to know that the model won't produce private information, and we might need to restrict

01:00:42.240 --> 01:00:46.880
access to parts of the model's knowledge to different groups, like different customers or

01:00:46.880 --> 01:00:51.120
different people with different privileges and so forth. That's what we're going to need if we're

01:00:51.120 --> 01:00:57.200
really going to deploy these models out into the world. As I said, I think both of the approaches

01:00:57.200 --> 01:01:01.200
that I sketched do well on the synthesis part, because they both use a language model and those

01:01:01.200 --> 01:01:06.960
are really good. They all have the gift of gab, so to speak. What about efficiency, right? On the

01:01:06.960 --> 01:01:14.720
LLM for everything approach, we had this undeniable rise in model size, and I pointed out models like

01:01:14.720 --> 01:01:21.600
an alpaca that are smaller. But I strongly suspect that if we are going to continue to ask these

01:01:21.600 --> 01:01:28.240
models to be both a knowledge store and a language capability, we're going to be dealing with these

01:01:28.240 --> 01:01:35.520
really large models. The hope of the retrieval augmented approach is that we could get by with

01:01:35.520 --> 01:01:40.800
the smaller models, and the reason we could do that is that we're going to factor out the knowledge

01:01:40.800 --> 01:01:46.480
store into that index and the language capability, which is going to be the language model. The only

01:01:46.480 --> 01:01:51.920
thing we're going to be asking the language model is to be good at that kind of in-context learning.

01:01:51.920 --> 01:01:57.600
It doesn't need to also store a full model of the world, and I think that means that these models

01:01:57.600 --> 01:02:02.960
could be smaller. So overall, a big gain in efficiency if we go retrieval augmented.

01:02:03.440 --> 01:02:06.160
People will make progress, but I think it's going to be tense.

01:02:08.000 --> 01:02:11.760
What about updateability? Again, this is a problem that people are working on very

01:02:11.760 --> 01:02:18.320
concertedly for the LLMs for everything approach, but these models persist in giving outdated answers

01:02:18.320 --> 01:02:22.880
to questions. And one pattern you see is that there's a lot of progress where you could like

01:02:22.880 --> 01:02:27.280
edit a model so that it gives the correct answer to who is the president of the US,

01:02:27.280 --> 01:02:33.760
but then you ask it about something related to the family of the president, and it reveals that it

01:02:33.760 --> 01:02:39.120
has outdated information stored in its parameters. And that's because all of this information is

01:02:39.120 --> 01:02:45.600
interconnected, and we don't at the present moment know how to reliably do that kind of systematic

01:02:45.600 --> 01:02:53.840
editing. Okay, on the retrieval augmented approach, we just re-index our data. If the world changes,

01:02:53.840 --> 01:02:59.040
we assume that the knowledge store changed like somebody updated a Wikipedia page. So we

01:02:59.040 --> 01:03:03.840
represent all the documents again, or at least just the ones that changed. And now we have a lot

01:03:03.840 --> 01:03:08.720
of guarantees that as that propagates forward into the retrieved results, which are consumed by the

01:03:08.720 --> 01:03:14.880
language model, it will reflect the changes we made to the underlying database in exactly the same

01:03:14.880 --> 01:03:22.960
way that a web search index is updated now. One forward pass of the large language model

01:03:22.960 --> 01:03:29.120
compared to maybe training from scratch over here on new data to get an absolute guarantee

01:03:29.120 --> 01:03:35.200
that the change will propagate. What about provenance? Okay, we have seen this already,

01:03:35.200 --> 01:03:42.880
this problem here. LLMs for everything. I asked GPT3, the DaVinci 3 model, my question,

01:03:42.880 --> 01:03:47.760
are professional baseball players allowed to glue small wings onto their caps? But I kind of cut it

01:03:47.760 --> 01:03:54.400
off, but at the top there I said provide me some links to the evidence. And it dutifully

01:03:54.400 --> 01:04:00.240
provided the links, but none of the links are real. If you copy them out and follow them,

01:04:00.240 --> 01:04:06.880
they all go to 404 pages. And I think that this is worse than providing no links at all,

01:04:06.880 --> 01:04:12.320
because I'm attuned as a human in the current moment to see links and think they're probably

01:04:12.400 --> 01:04:17.680
evidence. And I don't follow all the links. And here you might look and say, oh yeah,

01:04:17.680 --> 01:04:26.080
I see it found the relevant MLB pages and that's it, right? Over here, the kind of the point of this

01:04:26.080 --> 01:04:30.560
is that we are first doing a search phase where we're actually linked back to documents,

01:04:30.560 --> 01:04:35.040
and then we just need to solve the interesting, non-trivial question of how to link those

01:04:35.040 --> 01:04:40.480
documents into the synthesized answer. But all of the information we need is right there on the

01:04:40.480 --> 01:04:45.920
screen for us, and so this feels like a relatively tractable problem compared to what we are faced

01:04:45.920 --> 01:04:54.240
with on the left. I will say, I've been just amazed at the rollout, especially of the Bing

01:04:54.240 --> 01:04:59.840
search engine, which now incorporates open AI models at some level, because it is clear that it

01:04:59.840 --> 01:05:04.800
is doing web search, right? Because it's got information that comes from documents that only

01:05:04.800 --> 01:05:11.200
appeared on the web days before your query. But what it's doing with that information seems

01:05:11.200 --> 01:05:16.640
completely chaotic to me, so that it's kind of just getting mushed in with whatever else the

01:05:16.640 --> 01:05:23.440
model is doing, and you get this unpredictable combination of things that are grounded in

01:05:23.440 --> 01:05:28.560
documents and things that are completely fabricated. And again, I maintain this is worse than just

01:05:28.560 --> 01:05:36.160
giving an answer with no evidence attached to it. I don't know why these companies are not simply

01:05:36.160 --> 01:05:40.480
doing the retrieval augmented thing, but I'm sure they are going to wise up, and maybe your research

01:05:40.480 --> 01:05:46.560
could help them wise up a little bit about this. Finally, safety and security. This is relatively

01:05:46.560 --> 01:05:51.280
straightforward. On the LLMs for everything approach, we have a pressing problem, privacy

01:05:51.280 --> 01:05:55.760
challenges. We know that those models can memorize long strings in their training data, and that

01:05:55.840 --> 01:06:00.560
could include some very particular information about one of us, and that should be worrying us.

01:06:01.280 --> 01:06:06.720
We have no known way with a language model to compartmentalize LLM capabilities and say, like,

01:06:06.720 --> 01:06:12.720
you can see this kind of result, and you cannot. And similarly, we have no known way to restrict

01:06:12.720 --> 01:06:18.320
access to part of an LLMs capabilities. They just produce things based on their prompts,

01:06:18.320 --> 01:06:22.640
and you could try to have some prompt tuning that would tell them for this kind of person or setting

01:06:22.640 --> 01:06:29.120
do this and not that, but nobody could guarantee that that would succeed. Whereas for the retrieval

01:06:29.120 --> 01:06:35.440
augmented approach, again, we're thinking about accessing information from an index and access

01:06:35.440 --> 01:06:41.840
restrictions on an index is an old problem by now. Again, I don't want to say solved, but something

01:06:41.840 --> 01:06:47.760
that a lot of people have tackled for decades now, and so we can offer something like guarantees

01:06:48.320 --> 01:06:51.440
just from the fact that we have a separated knowledge store.

01:06:54.640 --> 01:06:59.600
Again, my smiley face. You can see where my feelings are. For the LLMs for everything approach,

01:07:00.160 --> 01:07:04.080
you know, people are working on these problems, and it's very exciting. And if you want a challenge,

01:07:04.640 --> 01:07:09.680
take, you know, take up one of these challenges here, but over here on the retrieval augmented side,

01:07:09.680 --> 01:07:14.000
I think we have lots of reasons to think it's not that they're completely solved. It's just

01:07:14.000 --> 01:07:20.160
that we can see the path to solving them. And this feels very urgent to me because of how suddenly

01:07:20.800 --> 01:07:25.440
this kind of technology is being deployed in a very user facing way for one of the core things

01:07:25.440 --> 01:07:31.120
we do in society, which is web search. So it's an urgent thing that we get good at this.

01:07:33.680 --> 01:07:39.040
Final things I want to say about this. So until recently, the way you would do even the

01:07:39.040 --> 01:07:45.040
retrieval augmented thing would be that you would have your index, and then you might train

01:07:45.040 --> 01:07:50.000
a custom purpose model to do the question answering part, and it could extract things from the text

01:07:50.000 --> 01:07:53.680
that you produced or maybe even generate some new things from the text that you produced.

01:07:54.720 --> 01:07:58.800
And that's kind of the mode that I mentioned before where you'd have like some language models,

01:07:58.800 --> 01:08:03.360
maybe a few of them, and you'd have an index, and you would stitch them together into a question

01:08:03.360 --> 01:08:08.400
answering system that you would probably train on question answering data. And you would hope

01:08:08.400 --> 01:08:13.920
that this whole big monster, maybe fine tuned on squad or natural questions or one of those data

01:08:13.920 --> 01:08:22.000
sets, gave you a general purpose question answering capability. That's the present, but I think it

01:08:22.000 --> 01:08:27.920
might actually be the recent past. And in fact, the way that you all will probably work when we do

01:08:27.920 --> 01:08:34.160
this unit, and certainly for the homework, is that we will just have frozen components. And this

01:08:34.160 --> 01:08:40.560
starts from the observation that the retriever model is really just a model that takes in text

01:08:40.560 --> 01:08:47.760
and produces text with scores. And a language model is also a device for taking in text and

01:08:47.760 --> 01:08:52.560
producing text with scores. And these are when these are frozen components, you can think of them

01:08:52.560 --> 01:08:57.680
as just black box devices that do this input output thing. And then you get into the intriguing

01:08:57.680 --> 01:09:03.360
mode of asking, well, what if we had them just talk to each other? And that is what you will do

01:09:03.360 --> 01:09:08.640
for the homework and bake off, you will have frozen retriever and a frozen large language model,

01:09:08.640 --> 01:09:15.040
and you will get them to work together to solve a very difficult open domain question answering

01:09:15.040 --> 01:09:21.120
problem. And that's kind of pushing us into a new mode for even thinking about how we design AI

01:09:21.120 --> 01:09:26.400
systems where it's not so much about fine tuning, it's much more about getting them to communicate

01:09:26.400 --> 01:09:33.600
with each other effectively to design a system from frozen components. Again, unanticipated,

01:09:33.600 --> 01:09:38.720
at least by me, as of a few years ago, and now an exciting new direction.

01:09:40.800 --> 01:09:44.960
So just to wrap out, I think what I'll do since we're near the end of the class here,

01:09:44.960 --> 01:09:49.200
I'll just finish up this one unit, and then we'll use some of our time next time to introduce

01:09:49.200 --> 01:09:55.600
a few other of these course themes, and that'll set us up well for diving into transformers.

01:09:55.680 --> 01:10:01.360
Final piece here just to inspire you, few shot open QA is kind of the task that you will tackle

01:10:01.360 --> 01:10:06.000
for homework too. And here's how you could think about this. Imagine that the question has come

01:10:06.000 --> 01:10:11.680
in, what is the course to take? The most standard thing we could do is just prompt the language

01:10:11.680 --> 01:10:16.720
model with that question, what is the course to take down here and see what answer it gave back,

01:10:16.720 --> 01:10:22.880
right? But the retrieval augmented insight is that we might also retrieve some kind of passage

01:10:22.880 --> 01:10:27.120
from a knowledge store. Here I have a very short passage, the course to take is natural language

01:10:27.120 --> 01:10:33.680
understanding, and that could be done with a retrieval mechanism. But why stop there? It might

01:10:33.680 --> 01:10:40.080
help the model as we saw going back to the GPT3 paper to have some examples of the kind of behavior

01:10:40.080 --> 01:10:44.960
that I'm hoping to get from the model. And so here I have retrieved from some data set,

01:10:44.960 --> 01:10:48.880
question answer pairs that will kind of give it a sense for what I want it to do in the end.

01:10:49.680 --> 01:10:56.400
But again, why stop there? We could also pick questions that were based very closely on the

01:10:56.400 --> 01:11:01.760
question that we posed. That would be like K nearest neighbor's approach where we use our retrieval

01:11:01.760 --> 01:11:08.560
mechanism to find similar questions to the one that we care about. I could also add in some context

01:11:08.560 --> 01:11:14.640
passages and I could do that by retrieval. So now we've used the retrieval model twice potentially,

01:11:14.640 --> 01:11:18.880
wants to get good demonstrations and wants to provide context for each one of them.

01:11:19.840 --> 01:11:24.640
But I could also use my retrieval mechanism with the questions and answers from the demonstration

01:11:24.640 --> 01:11:30.320
to get even richer connections between my demonstrations and the passages. I could even

01:11:30.320 --> 01:11:35.600
use a language model to rewrite aspects of those demonstrations to put them in a format

01:11:35.600 --> 01:11:41.920
that might help me with the final question that I want to pose. So now I have an interwoven use

01:11:41.920 --> 01:11:47.360
of the retrieval mechanism and the large language model to build up this prompt, right?

01:11:48.240 --> 01:11:52.640
Down at the retrieval thing, I could do the same thing. And then when you think about the model

01:11:52.640 --> 01:11:58.240
generation, again, we could just take the top response from the model, but we can do very

01:11:58.240 --> 01:12:05.040
sophisticated things on up to this full retrieval augmented generation model, which essentially

01:12:05.040 --> 01:12:10.800
marginalizes out the evidence passage and gives us a really powerful look at a good answer,

01:12:10.880 --> 01:12:17.440
conditional on that very complicated prompt that we constructed. I think what you're seeing on the

01:12:17.440 --> 01:12:23.520
left here is that we are going to move from an era where we just type in prompts into these models

01:12:23.520 --> 01:12:30.640
and hope for the best into an era where prompt construction is a kind of new programming mode

01:12:31.280 --> 01:12:37.120
where you're writing down computer code, could be Python code, that is doing traditional computing

01:12:37.120 --> 01:12:44.080
things, but also drawing on very powerful pre-trained components to assemble this kind

01:12:44.080 --> 01:12:49.920
of instruction kit for your large language model to do whatever task you have set for it.

01:12:50.480 --> 01:12:55.680
And so instead of designing these AI systems with all that fine tuning I described before,

01:12:55.680 --> 01:13:01.120
we might actually be moving back into a mode that's like that symbolic mode from the 80s

01:13:01.120 --> 01:13:06.560
where you type in a computer program. It's just that now the program that you type in

01:13:06.640 --> 01:13:13.360
is connected to these very powerful modern AI components and we're seeing right now

01:13:14.000 --> 01:13:18.560
that that is opening doors to all kinds of new capabilities for these systems

01:13:18.560 --> 01:13:22.640
and this first homework in Bake Off is going to give you a glimpse of that.

01:13:23.440 --> 01:13:28.080
And you're going to use a programming model we've developed called demonstrate search predict

01:13:28.080 --> 01:13:31.200
that I hope will give you a glimpse of just how powerful this can be.

01:13:32.080 --> 01:13:40.480
All right, we are out of time, right, 420? So next time I'll show you a few more units from the

01:13:40.480 --> 01:13:43.600
course and then we'll dive into transformers.

