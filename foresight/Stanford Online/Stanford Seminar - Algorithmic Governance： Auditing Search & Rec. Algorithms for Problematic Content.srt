1
00:00:00,000 --> 00:00:15,240
So this work is part of a larger thread of work centered around algorithmic governance.

2
00:00:15,240 --> 00:00:22,400
That is, how do we audit and measure problematic content like mis-disinformation, online extremism,

3
00:00:22,400 --> 00:00:26,920
and how do we do that on online platforms, and then specifically the search and recommendation

4
00:00:26,920 --> 00:00:29,720
algorithms driving these platforms.

5
00:00:29,720 --> 00:00:34,080
So as I was preparing for this talk, I thought of opening it up in a slightly unconventional

6
00:00:34,080 --> 00:00:39,440
way with this particular quote, which I'm going to read a little bit, which says, mankind

7
00:00:39,440 --> 00:00:44,720
barely noticed when the concept of massively organized information quietly emerged to become

8
00:00:44,720 --> 00:00:50,480
a means of social control, a weapon of war, and a roadmap for group destruction.

9
00:00:50,480 --> 00:00:54,720
Any guesses as to which year of the world this quote describes?

10
00:00:54,720 --> 00:01:03,640
Is it the world, is it a current world, 2020s, 2000s, 1900s?

11
00:01:03,640 --> 00:01:07,960
Some people could guess just by the word mankind in previous talk.

12
00:01:07,960 --> 00:01:13,160
Yeah, that's absolutely accurate.

13
00:01:13,160 --> 00:01:18,440
So this is actually Edwin Black's description of the world of the 1930s, 1940s, during the

14
00:01:18,440 --> 00:01:23,160
World War II, and the invention of the punch card, which was at that time the new wave

15
00:01:23,160 --> 00:01:26,360
of automation and data collection.

16
00:01:26,360 --> 00:01:31,040
And what is more surprising is that this quote is still true in today's 21st century world,

17
00:01:31,040 --> 00:01:36,640
and we have seen many new scholarships, and I'm sure folks here have read and are familiar

18
00:01:36,640 --> 00:01:41,640
with these books, many of these outline the harms of automation and data collection.

19
00:01:41,640 --> 00:01:46,320
And more recently, this discussion has shifted to the harms posed by generative AI and large

20
00:01:46,320 --> 00:01:47,320
language models.

21
00:01:47,320 --> 00:01:51,360
So I'm going to read this quote a little bit more of what Edwin Black had to say in his

22
00:01:51,360 --> 00:01:54,680
book, which is a lot of parallels to a present-day world.

23
00:01:54,680 --> 00:01:59,080
So he goes on to say, the unique igniting event was the most fateful day of the last

24
00:01:59,080 --> 00:02:04,760
century when Adolf Hitler came to power, but for the first time in history, an anti-Semayet

25
00:02:04,760 --> 00:02:08,960
had automation on his side, so that was what was different, and the automation was in the

26
00:02:08,960 --> 00:02:11,720
form of IBM punch cards.

27
00:02:11,720 --> 00:02:16,960
So he goes on to say that IBM was self-criped by a special immoral corporate mantra, if

28
00:02:16,960 --> 00:02:19,680
it can be done, it should be done.

29
00:02:19,680 --> 00:02:23,720
So we should take a pause and kind of re-read this phrase, if it can be done, it should

30
00:02:23,720 --> 00:02:30,200
be done, which is parallels very re-resemblance to the motives and culture practiced in modern-day

31
00:02:30,200 --> 00:02:35,560
technology companies, and I'm sure being in the Silicon Valley, you all probably have

32
00:02:35,560 --> 00:02:40,000
seen some of this while interning at some of these companies, move fast and break things,

33
00:02:40,000 --> 00:02:44,480
done is better than perfect, what would you do if you weren't afraid.

34
00:02:44,480 --> 00:02:48,800
And I argue that the sort of rushed culture, the sort of culture of disruption and speed

35
00:02:48,800 --> 00:02:53,760
often poses a great challenges in governing these technologies and conducting thoughtful

36
00:02:53,760 --> 00:02:59,480
medicalist audits, and in turn this is what makes algorithmic governance a difficult problem.

37
00:02:59,480 --> 00:03:03,400
And I'll come back to these ideas later on towards the end of the talk, but let me first

38
00:03:03,400 --> 00:03:09,360
dive into the meat of this presentation, a couple of studies which we did in this realm

39
00:03:09,360 --> 00:03:11,040
of algorithmic governance.

40
00:03:11,040 --> 00:03:16,560
So here I want to focus on two types of algorithms, search and recommendations, and two specific

41
00:03:16,920 --> 00:03:20,080
platforms, YouTube and Amazon.

42
00:03:20,080 --> 00:03:26,040
And our focus was only one type of problem, misinformation particularly.

43
00:03:26,040 --> 00:03:30,520
Now you might wonder why search and recommendation algorithms, so now the world is moving towards

44
00:03:30,520 --> 00:03:36,040
large language models, why we should focus on these old technologies, search and recommendations.

45
00:03:36,040 --> 00:03:40,680
So users generally have an unwavering trust in search engines, so several scholarly work,

46
00:03:40,680 --> 00:03:45,560
some of these cited here have actually shown that these ranking of search results have

47
00:03:46,040 --> 00:03:50,440
really dramatic effect on users' attitudes, their preferences, their behaviors.

48
00:03:50,440 --> 00:03:55,160
In fact, bias search rankings are so powerful they can even shift voting preferences of

49
00:03:55,160 --> 00:03:59,080
undecided waters by so much so as 20%.

50
00:03:59,080 --> 00:04:03,440
And users don't even show awareness of that kind of manipulation happening in their search

51
00:04:03,440 --> 00:04:04,440
results.

52
00:04:04,440 --> 00:04:09,120
So this should tell you how powerful search algorithms are, and then that is why we should

53
00:04:09,120 --> 00:04:13,640
keep studying them despite the other new shiny technologies coming our way.

54
00:04:13,720 --> 00:04:17,560
For recommendation algorithms, I hope I don't have to make a case here because you see those

55
00:04:17,560 --> 00:04:22,960
almost everywhere starting from what recommendations, what movies you should watch, what products

56
00:04:22,960 --> 00:04:27,080
one should buy, which campaign you should donate to, they are almost everywhere.

57
00:04:27,080 --> 00:04:29,000
And they also have these reinforcing effect, right?

58
00:04:29,000 --> 00:04:34,240
So the more you like, watch certain content, the more you share, the more you get those

59
00:04:34,240 --> 00:04:36,440
types of content.

60
00:04:36,440 --> 00:04:41,160
Now this might be harmless say when you're going out this weekend and trying to buy Christmas

61
00:04:41,200 --> 00:04:46,480
decorations, but these things can get ugly quite quickly when say you are browsing for

62
00:04:46,480 --> 00:04:52,040
vaccine information, health information or climate information.

63
00:04:52,040 --> 00:04:54,720
Now why audit for misinformation in particular, right?

64
00:04:54,720 --> 00:05:00,440
So currently there is this disproportionate focus on AI bias and fairness and tech journalist

65
00:05:00,440 --> 00:05:05,560
Karen Howe captures this notion very well in her article where she mentions how often

66
00:05:05,560 --> 00:05:10,760
responsible AI teams and companies are pigeonholed into targeting AI bias.

67
00:05:10,760 --> 00:05:14,360
Now don't get me wrong, so bias and fairness are indeed important topics that one should

68
00:05:14,360 --> 00:05:20,080
pursue, but tackling just AI bias draws away attention from fixing much bigger problems

69
00:05:20,080 --> 00:05:25,400
of other types of harmful information such as misinformation, extremism, conspiratorial

70
00:05:25,400 --> 00:05:26,480
content.

71
00:05:26,480 --> 00:05:31,000
So this is what is underlying motivation behind the set of studies that we did.

72
00:05:31,000 --> 00:05:36,840
And with that, let's dive into this first study, the YouTube audit work.

73
00:05:37,120 --> 00:05:41,680
A major motivation for this YouTube study was coming from these frequent headlines that

74
00:05:41,680 --> 00:05:46,440
I was noticing a few years ago, how YouTube is driving people in the internet's darkest

75
00:05:46,440 --> 00:05:51,240
corners, and then there were these opinion pieces talking about YouTube being the great

76
00:05:51,240 --> 00:05:53,640
radicalizer.

77
00:05:53,640 --> 00:05:58,240
But YouTube was also responding with articles saying that they will be reducing conspiracy

78
00:05:58,240 --> 00:06:03,400
theory recommendations or making it much harder to find those on the platform.

79
00:06:03,400 --> 00:06:08,640
And all these questions and all these reports are really anecdotal, like empirically how

80
00:06:08,640 --> 00:06:09,920
bad is this, right?

81
00:06:09,920 --> 00:06:12,200
Do we really know that?

82
00:06:12,200 --> 00:06:17,360
And this is what I studied to try to do, verify these anecdotal claims that does YouTube really

83
00:06:17,360 --> 00:06:20,160
surface these problematic content.

84
00:06:20,160 --> 00:06:24,840
And in order to do this, we conducted these systematic audits on YouTube search and recommendation

85
00:06:24,840 --> 00:06:29,480
algorithms and we picked one type of problematic content, conspiracy theories.

86
00:06:29,480 --> 00:06:32,600
Now what really is an audit, right?

87
00:06:32,600 --> 00:06:37,720
I have talked about mentioned audit a couple of times so far, but how do you audit algorithms?

88
00:06:37,720 --> 00:06:42,480
I'm sure some of you in the audience might be familiar with the concept of audit.

89
00:06:42,480 --> 00:06:46,040
For those of you who don't know, I'm going to give you a quick definition through an

90
00:06:46,040 --> 00:06:50,920
example and also say that this is a thriving field of research, lots of work has been done

91
00:06:50,920 --> 00:06:52,800
in this space.

92
00:06:52,800 --> 00:06:56,960
And this was one of the earliest example of audits coming from the social science world.

93
00:06:56,960 --> 00:07:02,000
And it's also one of my favorite ones from this 2004 paper, where researchers conducted

94
00:07:02,000 --> 00:07:07,120
this very clever field experiment to investigate employment discrimination.

95
00:07:07,120 --> 00:07:11,720
That is, they audited the labor market for racial discrimination.

96
00:07:11,720 --> 00:07:16,720
So what they did was they responded with fictitious resumes to help wanted ads in the Boston

97
00:07:16,720 --> 00:07:18,960
and Chicago newspapers.

98
00:07:18,960 --> 00:07:23,280
To manipulate the perception of race, what they did was they kept everything else in

99
00:07:23,280 --> 00:07:26,800
each of these resumes constant, only they changed the names.

100
00:07:26,800 --> 00:07:31,960
And the names were either very African American sounding names such as Laquisha or Jamal or

101
00:07:31,960 --> 00:07:37,080
very white sounding names such as Emily or Greg, and hence the name of this paper.

102
00:07:37,080 --> 00:07:41,160
The results showed that there was significant discrimination against African American names

103
00:07:41,160 --> 00:07:48,040
while white names received 50% more callbacks for interviews compared to the African American

104
00:07:48,040 --> 00:07:51,800
names despite everything else being constant in those resumes.

105
00:07:51,800 --> 00:07:55,640
So this is a core idea behind audits, that is, you would keep everything else constant

106
00:07:55,640 --> 00:07:59,880
and then you manipulate a single variable to determine how that change would affect

107
00:07:59,880 --> 00:08:01,520
the algorithm.

108
00:08:01,520 --> 00:08:06,560
So if you translate this into the context of YouTube, you would manipulate a variable

109
00:08:06,560 --> 00:08:11,520
to determine whether the search and recommendation algorithm returns different results, say when

110
00:08:11,520 --> 00:08:17,680
someone's age, their other demographic attributes, gender, their watch history, where they are

111
00:08:17,680 --> 00:08:24,520
searching from, the geolocation, if that differs, what happens with the research results.

112
00:08:24,520 --> 00:08:28,000
So to answer this, we set up this elaborate audit framework, which broadly looked like

113
00:08:28,000 --> 00:08:31,800
this, where we had programmed bots, or in other words, we were conducting these sock

114
00:08:31,800 --> 00:08:32,800
puppet audits.

115
00:08:32,800 --> 00:08:37,800
So these were bots or sock puppets, which behave like normal users logging into YouTube,

116
00:08:37,800 --> 00:08:42,200
running queries on the search platform while at the same time, a script at the back end

117
00:08:42,200 --> 00:08:48,200
were collecting whatever search and recommendation results the platform was returning.

118
00:08:48,200 --> 00:08:52,080
So we started with selecting search topics, and a goal here was to make sure that the

119
00:08:52,080 --> 00:08:56,280
topics are indeed high impact, that is, lots of people are searching for these topics,

120
00:08:56,280 --> 00:09:00,320
so they should be popular, and they should be topics which also have the potential to

121
00:09:00,320 --> 00:09:02,880
return false conspiracies.

122
00:09:02,880 --> 00:09:07,600
So we did some more background work, referring to Wikipedia, comparing with Google Trends,

123
00:09:07,600 --> 00:09:13,920
and we came up with this list of five different topics, 9-11 conspiracies, vaccine controversies,

124
00:09:13,920 --> 00:09:18,400
moon landing conspiracies, chemtrail, and then flat earth.

125
00:09:18,400 --> 00:09:23,240
And then we audited three components of YouTube, the up next video, the top five recommendations,

126
00:09:23,240 --> 00:09:27,520
and also YouTube's search results.

127
00:09:27,520 --> 00:09:32,000
For demographics, we checked for four different age groups and two different types of gender,

128
00:09:32,000 --> 00:09:37,840
male and female, and to emulate this, we had to create eight different sock puppet combination

129
00:09:37,840 --> 00:09:39,080
accounts.

130
00:09:39,080 --> 00:09:44,960
And then for geolocation, we found this hot and cold regions, that is, we call these

131
00:09:44,960 --> 00:09:48,480
hot and cold regions because these are the regions which have the highest or the lowest

132
00:09:48,480 --> 00:09:50,720
interest for that particular topic.

133
00:09:50,720 --> 00:09:56,680
And we found these hot and cold regions comparing with Google Trends' interest over time graph.

134
00:09:56,680 --> 00:09:58,840
So this is how it looked like for all the topics.

135
00:09:58,840 --> 00:10:04,480
So for example, if you pick the flatter theories, Montana was a hot region or a high interest

136
00:10:04,480 --> 00:10:09,320
showing region, while New Jersey is a low interest or cold region.

137
00:10:09,320 --> 00:10:14,480
So once we had all these parameters, the demographic geolocation and all these parameters, we essentially

138
00:10:14,480 --> 00:10:20,520
created bot accounts or sock puppets and programmed these accounts to keep firing queries on YouTube.

139
00:10:20,520 --> 00:10:26,240
For geolocation, these bots fired queries from IP addresses of these locations.

140
00:10:26,240 --> 00:10:30,520
Now one very important thing that we had to do for running these audits is, throughout

141
00:10:30,520 --> 00:10:33,840
these audit experiments, you would have to control for noise to ensure that the effect

142
00:10:33,840 --> 00:10:39,320
that you're observing is actually from the algorithm is not because of the noise that

143
00:10:39,320 --> 00:10:42,760
might have been introduced while running the experiment.

144
00:10:42,760 --> 00:10:49,480
So we controlled for browser noise by selecting one single version of Firefox browser.

145
00:10:49,480 --> 00:10:54,240
We made sure that the YouTube searches are happening simultaneously to control for temporal

146
00:10:54,240 --> 00:10:57,320
effects and so on.

147
00:10:57,320 --> 00:11:05,400
So all of that audit run resulted in about 56,000, more than 56,000 videos capturing about

148
00:11:05,400 --> 00:11:08,080
3,000 unique videos.

149
00:11:08,080 --> 00:11:13,400
And then was the hard part, right, the manual annotations for this data set.

150
00:11:13,400 --> 00:11:17,480
And I can go into detail of why we went with manual annotations and how we annotated if

151
00:11:17,480 --> 00:11:22,120
anyone is interested in the Q&A round, but essentially this resulted in kind of three

152
00:11:22,120 --> 00:11:25,520
sets of annotations promoting neutral and debunking.

153
00:11:25,520 --> 00:11:29,800
And there was a lot of thought process that went into these annotation scheme, why these

154
00:11:29,800 --> 00:11:34,240
three class made sense for this purpose and so on.

155
00:11:34,240 --> 00:11:39,240
And then we performed statistical comparison tests to essentially find out what's the result

156
00:11:39,240 --> 00:11:40,800
of these audits.

157
00:11:40,800 --> 00:11:43,000
So let's look at some of these.

158
00:11:43,000 --> 00:11:44,000
So what did we find?

159
00:11:44,160 --> 00:11:49,680
We found that for brand new accounts, demography and geolocation do not really have an effect

160
00:11:49,680 --> 00:11:54,640
on the amount of misinformation or the type of conspiracy theories that these platforms

161
00:11:54,640 --> 00:11:59,440
are or YouTube is returning.

162
00:11:59,440 --> 00:12:00,440
This is encouraging.

163
00:12:00,440 --> 00:12:02,160
This is what we want the platform to do, right?

164
00:12:02,160 --> 00:12:07,640
So it tells us that unlike those reports which were blaming YouTube for returning conspiracy

165
00:12:07,640 --> 00:12:12,480
theories when it's a brand new account, turns out the demography and geolocation do not

166
00:12:12,480 --> 00:12:14,480
really have an effect.

167
00:12:14,480 --> 00:12:21,200
But once accounts builds a history by watching both demography and geolocation starts exerting

168
00:12:21,200 --> 00:12:29,120
an effect on the recommendation for certain combination of topics, stances and component.

169
00:12:29,120 --> 00:12:31,000
You might be thinking, okay, this is expected, right?

170
00:12:31,000 --> 00:12:35,720
This is what we, how we think the platform would behave.

171
00:12:35,720 --> 00:12:40,080
But turns out there are a little bit more nuanced results when we dig deeper into these,

172
00:12:40,080 --> 00:12:41,920
the actual audit outcomes.

173
00:12:41,920 --> 00:12:47,680
So for example, for the 9-11 topic, if the sock puppets watched YouTube videos promoting

174
00:12:47,680 --> 00:12:53,200
line of conspiracy, you would end up getting more of these promoting videos in the recommendations.

175
00:12:53,200 --> 00:12:57,560
But if the topic is something different, so for surprisingly for vaccine topic, the effect

176
00:12:57,560 --> 00:12:59,040
was completely opposite.

177
00:12:59,040 --> 00:13:03,400
If you watch anti-vaccine videos, YouTube ended up recommending you debunking videos

178
00:13:03,400 --> 00:13:07,040
in the up next and top five recommendations.

179
00:13:07,040 --> 00:13:12,840
So this, at least from these observations, it tells us that YouTube in some way is handling

180
00:13:12,840 --> 00:13:15,440
misinformation in a much more reactive way.

181
00:13:15,440 --> 00:13:19,800
It's modifying its search and recommendation algorithm selectively based on what reactions

182
00:13:19,800 --> 00:13:22,320
is getting from the media and technology critics.

183
00:13:22,320 --> 00:13:26,880
So we know that there was a lot of pushback for vaccine-related misinformation, and it

184
00:13:26,880 --> 00:13:30,800
appears that they have gone and fixed that, but they have not done that universally for

185
00:13:30,800 --> 00:13:34,320
other problematic topics.

186
00:13:34,320 --> 00:13:38,440
We also found that certain demographics were prone to conspiracy video recommendations.

187
00:13:38,440 --> 00:13:44,400
So for example, among eight of those demographic cases in all but one case, men accounts, that

188
00:13:44,400 --> 00:13:49,040
is, bought accounts who had gender set as male were recommended more misinformation

189
00:13:49,040 --> 00:13:50,640
videos.

190
00:13:50,640 --> 00:13:54,920
And perhaps more surprisingly, what we found that in four of these cases, men accounts

191
00:13:54,920 --> 00:14:00,440
who actually ended up watching neutral videos got significantly higher misinformation video

192
00:14:00,440 --> 00:14:02,080
recommendations.

193
00:14:02,080 --> 00:14:03,720
Now this is really problematic.

194
00:14:03,720 --> 00:14:09,000
It implies that the algorithm was actually recommending pro-conspiracy videos even when

195
00:14:09,000 --> 00:14:14,560
the user, in this case, Stockpuppet, was watching neutral videos on the topic.

196
00:14:14,560 --> 00:14:16,480
What could this mean for real users?

197
00:14:16,480 --> 00:14:21,680
This means that recommending promoting videos to men who are already drawn to neutral information

198
00:14:21,680 --> 00:14:26,840
for that topic, but have not yet developed pro-conspiracy beliefs, but now has a higher

199
00:14:26,840 --> 00:14:33,640
chance of developing that because the platform is returning these promoting conspiratorial

200
00:14:33,640 --> 00:14:36,400
videos.

201
00:14:36,400 --> 00:14:43,560
So wrapping up this work, the key contribution of this study was that in some senses this

202
00:14:43,560 --> 00:14:48,720
work developed a methodology to audit search engines for misinformation, and we were also

203
00:14:48,720 --> 00:14:53,400
able to statistically prove that YouTube's behavior varies across different misinformation

204
00:14:53,400 --> 00:14:54,960
topics.

205
00:14:54,960 --> 00:15:01,920
And our study also identified certain populations that could be potentially targets of certain

206
00:15:01,920 --> 00:15:03,920
types of misinformation.

207
00:15:03,920 --> 00:15:10,600
So this tells us that audit itself could be a useful way for studying how algorithms might

208
00:15:10,600 --> 00:15:14,280
have differential impacts on certain marginalized populations.

209
00:15:14,280 --> 00:15:15,280
Yes?

210
00:15:15,280 --> 00:15:16,280
Good question.

211
00:15:16,280 --> 00:15:19,400
So I get what you're saying about it being reactive.

212
00:15:19,400 --> 00:15:24,480
The evidence suggests that in this case it's like being a special case.

213
00:15:24,480 --> 00:15:28,760
Do you have a proposal as to how it might not be reactive?

214
00:15:28,760 --> 00:15:29,760
How could they be proactive?

215
00:15:29,760 --> 00:15:33,360
Is there something you would propose that they do instead?

216
00:15:33,360 --> 00:15:34,360
Yes.

217
00:15:34,360 --> 00:15:38,200
So I think one of the things they could do, and I think they are doing it now in hindsight,

218
00:15:38,200 --> 00:15:43,560
this is an older study, is they're sitting with teams of experts, health experts, and

219
00:15:43,560 --> 00:15:50,440
also looking at these health-related queries and topics in advance to figure out doing these

220
00:15:50,440 --> 00:15:52,360
red teaming exercises.

221
00:15:52,360 --> 00:15:53,360
They call it red teaming.

222
00:15:53,360 --> 00:15:55,320
I think in the research we call it audit.

223
00:15:55,320 --> 00:16:00,960
So they are doing this beforehand to figure out whether the platform is returning problematic

224
00:16:00,960 --> 00:16:02,280
content.

225
00:16:02,280 --> 00:16:11,000
And so if they do more of that proactively, of course there is some hope in changing things,

226
00:16:11,000 --> 00:16:14,320
and we should not be catching these reactively after the fact.

227
00:16:14,320 --> 00:16:17,920
That assumes that the points of view are stable, right?

228
00:16:17,920 --> 00:16:25,720
So we often have these scenarios where culture changes or something goes viral, like the

229
00:16:25,720 --> 00:16:30,440
tide pods or whatever, where I don't think a red team would have come up with, oh yeah,

230
00:16:30,440 --> 00:16:34,720
we're going to start eating bleach or whatever.

231
00:16:34,720 --> 00:16:37,320
Is there an approach there that you would advocate, like if you were in charge of one

232
00:16:37,320 --> 00:16:38,320
of these teams?

233
00:16:38,320 --> 00:16:39,320
Yeah.

234
00:16:39,320 --> 00:16:45,440
I think one of the things that I think some researchers at Stanford, maybe it was one

235
00:16:45,440 --> 00:16:51,080
of your students who did this work with crowd audits, right, like where the crowd itself

236
00:16:51,080 --> 00:16:56,280
is reporting, because it's not really possible for the red team to find all possible scenarios

237
00:16:56,280 --> 00:17:02,600
under which these sorts of problems happen.

238
00:17:02,600 --> 00:17:07,200
And I think that's where if you have these multiple eyes from different domains and different

239
00:17:07,200 --> 00:17:12,680
cultures to report those problems, and then the company actually responds to it.

240
00:17:12,680 --> 00:17:16,200
The problem is if the company's not responding or the people who are building these algorithms,

241
00:17:16,200 --> 00:17:17,760
if they're not responding to it.

242
00:17:17,760 --> 00:17:22,960
And so hopefully there should be a mechanism to do that, kind of closing that loop all the

243
00:17:22,960 --> 00:17:28,800
way from reporting to actually taking action.

244
00:17:28,800 --> 00:17:35,760
So moving on to Amazon, I think one of the things that led us to looking at Amazon is

245
00:17:35,760 --> 00:17:44,160
despite being this leading retailer platform, how less of a focused research focus has been

246
00:17:44,160 --> 00:17:46,280
paid to this platform.

247
00:17:46,280 --> 00:17:51,880
And I think what was alarming is there were several media reports at the time coming out

248
00:17:51,880 --> 00:17:56,680
suggesting that Amazon's algorithm were putting health and vaccine misinformation at the top

249
00:17:56,680 --> 00:17:57,760
of your reading list.

250
00:17:57,760 --> 00:18:02,600
But there was very little research to fall back to either verify or even, you know, kind

251
00:18:02,600 --> 00:18:05,960
of disprove these reports.

252
00:18:05,960 --> 00:18:10,320
So if you search on Amazon, unlike YouTube, which is at least tried to control for vaccine

253
00:18:10,320 --> 00:18:15,320
misinformation, searching on Amazon for a vaccine, even this morning when I searched,

254
00:18:15,320 --> 00:18:20,880
I could actually find some of these books, you would end up getting several anti-vaccination

255
00:18:20,880 --> 00:18:24,480
products mostly in the form of books.

256
00:18:24,480 --> 00:18:29,520
And the recommendation algorithm for Amazon are even much more sophisticated than YouTube.

257
00:18:29,520 --> 00:18:33,240
So you have your product page recommendation, which has all these many different layers,

258
00:18:33,240 --> 00:18:36,720
customers who bought items, sponsored products related to these items.

259
00:18:36,720 --> 00:18:42,160
You have your homepage recommendations, again, many different layers underneath, pre-purchase

260
00:18:42,160 --> 00:18:46,880
page recommendations, which is shown to you after you add a product to the cart that is

261
00:18:46,880 --> 00:18:52,240
after the user shows an intention to buy that product.

262
00:18:52,240 --> 00:18:57,800
So again, this was the same question as before, how bad is this scenario?

263
00:18:57,800 --> 00:19:01,880
So we essentially wanted to conduct the systematic audits on Amazon search and recommendation

264
00:19:01,880 --> 00:19:03,280
algorithm.

265
00:19:03,280 --> 00:19:08,760
And here we picked only one type of problematic content, vaccine misinformation.

266
00:19:08,760 --> 00:19:12,840
And we conducted two sets of audits, unpersonalized one, and then the personalized audits.

267
00:19:12,840 --> 00:19:18,520
The personalized audit school was to assess whether users account history built progressively

268
00:19:18,520 --> 00:19:22,760
by a user performing certain actions, such as clicking on a product, adding the product

269
00:19:22,760 --> 00:19:25,800
to the cart, showing their intention to buy.

270
00:19:25,800 --> 00:19:31,640
Because any of those actions, how does that change what recommendation is being returned?

271
00:19:31,640 --> 00:19:36,640
And here the user built these account history progressively by performing a particular action

272
00:19:36,640 --> 00:19:39,000
for seven consecutive days.

273
00:19:39,000 --> 00:19:42,560
So these were, again, when I say users, these were sock puppets, searching, searching plus

274
00:19:42,560 --> 00:19:45,680
clicking, searching plus clicking plus adding the product to the cart.

275
00:19:45,680 --> 00:19:49,160
So all these were different actions that were performed.

276
00:19:49,160 --> 00:19:53,960
And then we also controlled for noise, very similar setup as before, just with the caveat

277
00:19:53,960 --> 00:19:59,320
that this whole audit experiment setup was a big software engineering feat, considering

278
00:19:59,320 --> 00:20:06,240
the amount of different combinations of recommendations possible on Amazon.

279
00:20:06,240 --> 00:20:07,240
So what did we find?

280
00:20:07,240 --> 00:20:09,280
So I'm going to highlight a couple of results here.

281
00:20:09,280 --> 00:20:11,800
First a single case study result.

282
00:20:11,800 --> 00:20:16,320
So let's say users start searching for a vaccine and they click on an anti-vaccine book.

283
00:20:16,320 --> 00:20:20,600
So as of this morning, this book was actually there on their platform in the first page,

284
00:20:20,600 --> 00:20:22,480
first search result page.

285
00:20:22,480 --> 00:20:27,480
And so if the user clicks on this, that algorithm next serves the user three other anti-vaccine

286
00:20:27,480 --> 00:20:30,080
books in the product recommendation page.

287
00:20:30,080 --> 00:20:36,120
And then once the user adds a product or book to the cart that shows their intention to buy,

288
00:20:36,120 --> 00:20:41,040
both the pre-purchase as well as a home page recommendation also rapidly changes with many

289
00:20:41,040 --> 00:20:43,720
more anti-vaccine book recommendations.

290
00:20:43,720 --> 00:20:48,080
So this just tells you that once a user starts engaging with one misinformation product on

291
00:20:48,080 --> 00:20:52,240
this e-commerce platform, they will be presented with more of those similar stuff at every

292
00:20:52,240 --> 00:20:55,960
point of their Amazon navigation route.

293
00:20:55,960 --> 00:20:57,920
So this was not just a one-off case study.

294
00:20:57,920 --> 00:21:03,560
We found that more than 10% of Amazon products during the time period of our study for search

295
00:21:03,560 --> 00:21:10,080
terms like vaccine, autism, immunization resulted in misinformation book containing

296
00:21:10,080 --> 00:21:13,320
active vaccination content.

297
00:21:13,320 --> 00:21:17,480
And so our audit experiment, just to give you the scale of this experiment, this was

298
00:21:17,480 --> 00:21:24,240
ran for a little bit over three weeks, resulted in 36,000 search results, 16,000 recommendations,

299
00:21:24,240 --> 00:21:30,280
and then worked over several search filters like featured, sponsored, different recommendation

300
00:21:30,280 --> 00:21:35,560
types, user actions, and so all of that resulted in that number of more than 10%.

301
00:21:35,560 --> 00:21:42,400
So if you just zoom out and look at what these thousands of recommendations look like, this

302
00:21:42,400 --> 00:21:46,360
is the entire recommendation graph for one type of recommendation, what are the items

303
00:21:46,360 --> 00:21:49,880
customers buy after viewing this item.

304
00:21:49,880 --> 00:21:53,920
So here, each node in the graph represents a product, an Amazon product, and an edge

305
00:21:53,920 --> 00:21:58,800
from a node A to a node B indicates that B was recommended in the product page of A.

306
00:21:58,800 --> 00:22:02,400
Node size here is proportional to the number of times the product was recommended, and

307
00:22:02,400 --> 00:22:08,360
the color corresponds to whether if it's a red, if it denotes a product annotated as

308
00:22:08,360 --> 00:22:12,600
misinformation, green, neutral, and then blue debunking.

309
00:22:12,600 --> 00:22:18,840
And I think being, you know, all of you have CS degrees or almost about to get a CS degree,

310
00:22:18,840 --> 00:22:23,080
so you would probably able to decipher what's going on in this graph.

311
00:22:23,080 --> 00:22:27,680
There are these large red size nodes attached to other red nodes, which are almost completely

312
00:22:27,680 --> 00:22:28,680
separated.

313
00:22:28,680 --> 00:22:30,640
There are like two separate components, right?

314
00:22:30,640 --> 00:22:35,200
So this just shows how strong of a filter bubble effect there is for this particular

315
00:22:35,200 --> 00:22:36,200
recommendation.

316
00:22:36,200 --> 00:22:41,040
People who are recommended misinformation products, they keep getting recommended those

317
00:22:41,040 --> 00:22:45,520
products super hard for them to break out from that red zone to get to the blue or the

318
00:22:45,520 --> 00:22:46,520
green zone.

319
00:22:46,520 --> 00:22:47,520
Sorry, just a clarification.

320
00:22:47,520 --> 00:22:50,520
I think you mentioned, how did you code something as misinformation?

321
00:22:50,520 --> 00:22:52,520
Obviously, there's lots of shades of gray here.

322
00:22:52,520 --> 00:22:53,520
Yes.

323
00:22:53,520 --> 00:23:01,520
So there was an initial status, we went through an extensive annotation scheme of a set of,

324
00:23:01,520 --> 00:23:03,960
and then we also built a classifier to do that.

325
00:23:03,960 --> 00:23:10,040
But the entire, there are like a lot more details in the paper as to how we coded it,

326
00:23:10,040 --> 00:23:14,840
but I think it took us almost a month to even come up with the whole annotation scheme,

327
00:23:14,840 --> 00:23:20,520
and then five or six of our experts, including me, we kind of coded it.

328
00:23:20,520 --> 00:23:26,640
But yes, so we looked at a few markers, like the name of the book, the text, read the Google

329
00:23:26,640 --> 00:23:32,640
preview of the text of the book, some of the comments, and then also the reviews that are

330
00:23:32,640 --> 00:23:34,640
present on the Amazon website.

331
00:23:34,640 --> 00:23:39,480
So it's a lot more qualitative process, and then all the markers that we took, we also

332
00:23:39,680 --> 00:23:43,200
put it into the classifier to get the final annotations.

333
00:23:43,200 --> 00:23:47,560
So this would have been more like the overall position of the book, not like, was there

334
00:23:47,560 --> 00:23:49,560
a fact somewhere in there that.

335
00:23:49,560 --> 00:23:54,440
So yes, we cannot really go and look at, okay, there is this one single line in the text

336
00:23:54,440 --> 00:23:58,800
in the book, which is misinformation, but yes, so you could say there's a little bit

337
00:23:58,800 --> 00:24:01,960
of noise in there.

338
00:24:01,960 --> 00:24:07,480
And so this was another one just to say that this was not happening for one recommendation.

339
00:24:07,480 --> 00:24:13,160
Those who viewed this item also viewed very similar graph as before, very similar trend.

340
00:24:13,160 --> 00:24:18,400
And so the key takeaway here was our goal was to bring the focus to e-commerce platforms

341
00:24:18,400 --> 00:24:23,960
and show how their algorithms could be pushing anti-vaccine content to users.

342
00:24:23,960 --> 00:24:28,840
And we empirically established how certain real-world user actions on the platform could

343
00:24:28,840 --> 00:24:31,560
drive users to these problematic eco chambers.

344
00:24:31,560 --> 00:24:37,360
I think one of the implications, at least from this work, is that recommendation algorithms

345
00:24:37,760 --> 00:24:40,240
should not be blindly applied to all topics equally.

346
00:24:40,240 --> 00:24:46,400
If it's a health topic, perhaps companies need to pay a little bit more attention and

347
00:24:46,400 --> 00:24:53,880
to ensure that there is higher quality content coming out in their platform.

348
00:24:53,880 --> 00:25:00,080
So this work of ours intentionally and unintentionally was kind of rightly timed during the COVID pandemic.

349
00:25:00,080 --> 00:25:03,240
And so this was widely covered by several news channels.

350
00:25:03,240 --> 00:25:07,120
And in fact, Congressman Adam Schiff and Elizabeth Warren actually cited this research

351
00:25:07,160 --> 00:25:11,640
of ours in their letter to Amazon to control vaccine misinformation.

352
00:25:12,240 --> 00:25:16,400
So we were really happy that, OK, so now Amazon is going to take a few steps to do this,

353
00:25:16,760 --> 00:25:18,840
but turns out we were really wrong.

354
00:25:19,360 --> 00:25:21,080
So this is how Amazon is doing today.

355
00:25:21,080 --> 00:25:25,680
Still today, as of earlier this morning, you would still find several books containing

356
00:25:25,680 --> 00:25:27,880
vaccine misinformation.

357
00:25:27,880 --> 00:25:33,040
This also just tells how, even though you go into all these lengths doing these academic

358
00:25:33,040 --> 00:25:36,000
research, is it actually informing policies?

359
00:25:36,000 --> 00:25:37,760
Is it actually making any real world impact?

360
00:25:37,760 --> 00:25:40,120
And we can go into a long discussion about that.

361
00:25:40,520 --> 00:25:47,160
But in the interest of time, let me talk a little bit more on one other type of study

362
00:25:47,160 --> 00:25:50,600
that we did, looking at another type of audit method.

363
00:25:51,360 --> 00:25:54,560
So so far, the studies that I presented employed one type of audit method,

364
00:25:54,560 --> 00:25:55,520
sock puppet audits.

365
00:25:56,040 --> 00:26:02,280
While these audits provide great control over your experimental design, you can pinpoint

366
00:26:02,280 --> 00:26:06,000
exactly which variable might be affecting the output of the algorithm.

367
00:26:06,200 --> 00:26:10,760
But one criticism of these audits is that the bots behavior are usually built in a

368
00:26:10,760 --> 00:26:11,760
very conservative way, right?

369
00:26:11,760 --> 00:26:16,280
So the bot in this YouTube case was essentially going and watching all

370
00:26:16,280 --> 00:26:18,600
pro-conspiracy videos or all debunking videos.

371
00:26:18,880 --> 00:26:22,080
Real users do not really act exactly that in that way, right?

372
00:26:22,080 --> 00:26:25,800
So these are at least very extreme bot behaviors or user behaviors.

373
00:26:26,680 --> 00:26:30,400
So as an alternative, we conducted crowdsourced audits where we audited the algorithmic

374
00:26:30,400 --> 00:26:34,880
outputs from real world users to study and identify problematic behavior in users'

375
00:26:34,880 --> 00:26:35,880
naturalistic setting.

376
00:26:36,360 --> 00:26:40,320
So we conducted this audit for a nine day duration on YouTube.

377
00:26:40,680 --> 00:26:45,800
And our goal was to assess to the extent in which YouTube was regulating US-based

378
00:26:45,800 --> 00:26:48,080
election misinformation on their platform.

379
00:26:49,480 --> 00:26:53,800
And so soon after the presidential election in 2020, YouTube came under fire for

380
00:26:54,280 --> 00:26:58,000
surfacing election-related misinformation in their search and recommendations.

381
00:26:58,520 --> 00:27:02,160
And they quickly responded to those criticisms by introducing these content

382
00:27:02,160 --> 00:27:06,880
moderation policies to remove videos that spread election-related falsehoods

383
00:27:07,240 --> 00:27:11,080
and claim that misinformation videos would not be surfaced on their platform.

384
00:27:11,800 --> 00:27:17,040
But then again, during the midterm 2020 elections, there were reports saying that

385
00:27:17,040 --> 00:27:20,240
YouTube has still has misinformation blind spots, right?

386
00:27:20,240 --> 00:27:22,920
So they have not been very effective.

387
00:27:23,560 --> 00:27:27,840
So this study of ours was goal was to determine how effective YouTube was

388
00:27:27,840 --> 00:27:30,720
in successfully implementing its content moderation policy.

389
00:27:31,840 --> 00:27:34,840
And so we did this through this post hoc crowdsourced audit.

390
00:27:35,320 --> 00:27:36,360
Why it's post hoc?

391
00:27:36,360 --> 00:27:40,280
Because it's conducted after the fact the event has happened to elections of 2020

392
00:27:40,280 --> 00:27:43,280
and we were conducting this study in 2022.

393
00:27:43,880 --> 00:27:47,080
And it's crowdsourced audit since we investigated YouTube's algorithm collecting

394
00:27:47,080 --> 00:27:49,040
data from real world users.

395
00:27:50,400 --> 00:27:55,960
And I'm sure many of you who have run these sorts of recruitment studies, you

396
00:27:55,960 --> 00:27:58,720
would realize how difficult and how hard it is to do that.

397
00:27:59,000 --> 00:28:05,880
Essentially, we were asking users to lend their YouTube history so that we can do

398
00:28:05,880 --> 00:28:07,640
this sort of audit run.

399
00:28:08,600 --> 00:28:12,960
And so our crowdsourced investigation, I think we started with like recruiting

400
00:28:13,200 --> 00:28:17,880
600 to 500 users and we ended up slightly lower than 100 users.

401
00:28:17,880 --> 00:28:19,960
So 99s, particularly.

402
00:28:20,240 --> 00:28:24,720
So all these 99 users first filled out a pre-survey and about the beliefs on

403
00:28:24,720 --> 00:28:28,440
personalization on YouTube, how they trust YouTube search and recommendation

404
00:28:28,440 --> 00:28:32,760
algorithm, and then they installed this browser extension, which allowed us to

405
00:28:33,080 --> 00:28:34,880
collect users personalized data.

406
00:28:35,280 --> 00:28:40,720
We also had all these ethical considerations, which I can go on into

407
00:28:40,720 --> 00:28:42,320
more detail if anyone is interested.

408
00:28:43,200 --> 00:28:46,920
But what this extension was doing, it was collecting search results of search

409
00:28:46,920 --> 00:28:51,560
queries related to the 2020 US presidential election, as well as

410
00:28:51,560 --> 00:28:54,320
voter fraud claims surrounding the 2020 elections.

411
00:28:54,640 --> 00:28:57,000
So two kinds of collection was happening.

412
00:28:57,000 --> 00:29:02,160
One was with respect to search results in the standard and incognito window.

413
00:29:02,160 --> 00:29:06,240
And by comparing these search results in both these windows, our goal was to

414
00:29:06,240 --> 00:29:09,600
tell the extent in which YouTube was personalizing search results.

415
00:29:10,320 --> 00:29:12,920
And then we were also collecting recommendation results.

416
00:29:12,920 --> 00:29:16,200
And the way we were doing this, we were collecting these up next recommendation

417
00:29:16,200 --> 00:29:22,160
trails after a user has watched a list of pre-selected videos with different

418
00:29:22,160 --> 00:29:24,040
stances on election misinformation.

419
00:29:25,240 --> 00:29:29,520
And the extension would start by first watching a pre-selected seed video and

420
00:29:29,520 --> 00:29:33,480
then collecting up next videos up to five different levels.

421
00:29:35,280 --> 00:29:38,680
So when we asked our participants in this pre-study survey, how much do you

422
00:29:38,680 --> 00:29:40,640
think YouTube personalizes your search results?

423
00:29:40,640 --> 00:29:44,840
About 34% of them believe that YouTube personalizes their search results to a

424
00:29:44,840 --> 00:29:45,760
really great extent.

425
00:29:46,240 --> 00:29:49,960
But this, through our audit, we found that YouTube, actually, that their YouTube

426
00:29:49,960 --> 00:29:52,520
stop search results have little to no personalization.

427
00:29:52,840 --> 00:29:57,520
So this also tells you how users believe in algorithms, like the way they behave

428
00:29:57,840 --> 00:30:02,080
is different from actually the way the platform might be behaving.

429
00:30:03,720 --> 00:30:06,880
But when we asked how much YouTube personalizes their up next recommendation,

430
00:30:06,880 --> 00:30:11,040
that perception actually aligned with how actually the audit results showed.

431
00:30:11,040 --> 00:30:14,720
Like 51% of participants believe that YouTube personalizes up next

432
00:30:14,720 --> 00:30:18,760
recommendation to a great extent, which is in line with what our audit results

433
00:30:18,760 --> 00:30:19,080
found.

434
00:30:21,000 --> 00:30:24,840
We also calculate the amount of misinformation present in search results.

435
00:30:25,000 --> 00:30:28,160
And we quantified this with this misinformation bias score.

436
00:30:28,400 --> 00:30:31,560
And this is the only equation you're going to see throughout this talk.

437
00:30:32,480 --> 00:30:36,120
So this misinformation bias score, we're East from minus one to one.

438
00:30:36,120 --> 00:30:40,640
What the score does is it captures amount of misinformation, election related

439
00:30:40,640 --> 00:30:44,120
misinformation while taking into account the ranking of the search results.

440
00:30:44,440 --> 00:30:48,080
So a positive score indicates that search results contain videos that support

441
00:30:48,240 --> 00:30:53,080
election misinformation while negative it contain videos that oppose election

442
00:30:53,080 --> 00:30:53,920
misinformation.

443
00:30:55,200 --> 00:30:57,640
Now, if you look at the entire distribution of scores for our collective

444
00:30:57,640 --> 00:31:02,360
results, we found that the misinformation score, if you look at the X axis,

445
00:31:02,360 --> 00:31:06,200
it's mostly negative, which indicates that YouTube presents more debunking or

446
00:31:06,200 --> 00:31:08,120
opposing videos in the search results.

447
00:31:08,760 --> 00:31:11,480
A couple of other key things also jumps off, right?

448
00:31:11,480 --> 00:31:15,440
So you could see there are this distribution is by model.

449
00:31:15,960 --> 00:31:19,480
So essentially there are two different clusters and each of these clusters

450
00:31:19,480 --> 00:31:22,360
corresponds to two types of search queries.

451
00:31:24,040 --> 00:31:26,640
I mean, we didn't cluster it ahead of time, right?

452
00:31:26,640 --> 00:31:29,120
This, this emerged from our data.

453
00:31:29,320 --> 00:31:31,800
So the first cluster corresponds to voter fraud.

454
00:31:33,120 --> 00:31:36,480
Basically anything related to fraud in conjunction with keywords related to

455
00:31:36,480 --> 00:31:40,880
election, while cluster two is more generic election related searches,

456
00:31:40,880 --> 00:31:43,040
presidential election, mail-in ballots and so on.

457
00:31:44,040 --> 00:31:48,240
What's interesting here is that the cluster one has these missions

458
00:31:48,240 --> 00:31:52,680
information bias score, which are more negative, which indicates that if the

459
00:31:52,680 --> 00:31:56,160
user goes and search for fraud related topics, they are actually going to be

460
00:31:56,160 --> 00:31:59,800
given more opposing election related misinformation video, right?

461
00:31:59,800 --> 00:32:02,960
So it's making YouTube is making it really difficult for users to search

462
00:32:02,960 --> 00:32:07,280
for election fraud video, which in some sense tells that YouTube's pay more

463
00:32:07,280 --> 00:32:10,440
attention to queries about election fraud and ensures that when users are

464
00:32:10,440 --> 00:32:15,880
searching for them, they are in fact being exposed to opposing misinformation videos.

465
00:32:17,080 --> 00:32:22,240
So key takeaway here is in some way YouTube is in fact successful in

466
00:32:22,240 --> 00:32:24,480
enacting election misinformation policies.

467
00:32:24,480 --> 00:32:30,240
So things that we wanted to test turns out it's actually, you know,

468
00:32:30,280 --> 00:32:34,520
aligning with how they wanted to enforce these policies, but it is indeed

469
00:32:34,520 --> 00:32:37,720
paying special attention to certain queries about voter fraud.

470
00:32:38,600 --> 00:32:42,280
But there still exists certain misinformation in the up next trails.

471
00:32:42,280 --> 00:32:45,520
We found that with some of those positive scores that you found.

472
00:32:46,400 --> 00:32:51,160
And then finally, as a byproduct of this audit, we also found that there was

473
00:32:51,160 --> 00:32:56,840
some mismatch in participants beliefs and the algorithmic reality that happens,

474
00:32:56,840 --> 00:33:01,440
right, which indicates a lack, some lack of awareness of algorithmic, how

475
00:33:01,440 --> 00:33:02,400
algorithms behave.

476
00:33:02,600 --> 00:33:05,560
And I think there has been other researchers who have been working in

477
00:33:05,560 --> 00:33:10,680
the space looking at algorithmic folk theories and how people's perception

478
00:33:10,680 --> 00:33:14,200
differ from the way these platforms work.

479
00:33:16,960 --> 00:33:21,000
Now, wrapping up, so these three, these are all the other, you know,

480
00:33:21,000 --> 00:33:24,280
the core studies that I wanted to present, but obviously coming back to

481
00:33:24,280 --> 00:33:27,280
how I started the talk, where do we go from here, right?

482
00:33:27,440 --> 00:33:31,080
How do we do meaningful algorithmic governance in the first place?

483
00:33:31,080 --> 00:33:35,360
That is, how do we set the path towards algorithmic governance in a meaningful

484
00:33:35,360 --> 00:33:37,760
way and what are the challenges in doing that?

485
00:33:39,320 --> 00:33:44,120
So here are a few ideas and obviously this is not, you know, there might be

486
00:33:44,120 --> 00:33:47,520
more that could be added, but these are some of the possibilities for doing

487
00:33:47,520 --> 00:33:48,480
algorithmic governance.

488
00:33:48,480 --> 00:33:49,760
So I've listed three of these.

489
00:33:50,160 --> 00:33:54,960
The first is algorithmic audits and so governance via audits and there could

490
00:33:54,960 --> 00:33:56,480
be many layers to this, right?

491
00:33:56,480 --> 00:34:01,000
So one of the layer is conducting external audits and I presented some of

492
00:34:01,000 --> 00:34:06,320
these external audit studies through the three research work that we have

493
00:34:06,320 --> 00:34:10,440
done in the past and also these audits could identify different types of risk.

494
00:34:10,440 --> 00:34:13,840
So misinformation is one risk, but you could also do the same for bias,

495
00:34:13,840 --> 00:34:18,840
discrimination, accountability, accessibility, accessibility, fairness

496
00:34:18,840 --> 00:34:19,320
and so on.

497
00:34:19,320 --> 00:34:23,600
And there are many researchers who have worked in the space.

498
00:34:23,840 --> 00:34:25,840
I've listed some of these citations here.

499
00:34:26,840 --> 00:34:31,840
But obviously a question is, so we as academic community, third-party researchers,

500
00:34:31,840 --> 00:34:35,840
we are doing all these audits, is it really making any difference, right?

501
00:34:35,840 --> 00:34:39,840
And as classic example is the failure of our Amazon study to make much of a

502
00:34:39,840 --> 00:34:40,840
difference, right?

503
00:34:40,840 --> 00:34:43,840
So we still really don't have a system in place where the algorithms or

504
00:34:43,840 --> 00:34:47,840
companies running them are truly accountable to an independent third-party.

505
00:34:47,840 --> 00:34:51,840
So this reminds me how US-based consumer reports operate, right?

506
00:34:51,840 --> 00:34:54,840
So there are these independent third-party organizations that go into

507
00:34:54,840 --> 00:34:58,840
great lengths for testing products that you use every day, your cars,

508
00:34:58,840 --> 00:35:00,840
your washing machine and so on.

509
00:35:00,840 --> 00:35:04,840
But so my argument is that why can't we do the same for algorithm?

510
00:35:04,840 --> 00:35:07,840
In fact, I would argue that we need that more for algorithms because we are using

511
00:35:07,840 --> 00:35:11,840
them much more frequently than say your washing machine.

512
00:35:12,840 --> 00:35:15,840
The other shortcoming with external audit is that they are a form of reactive

513
00:35:15,840 --> 00:35:16,840
governance.

514
00:35:16,840 --> 00:35:19,840
This was the question that Michael was asking even earlier, like they operate

515
00:35:19,840 --> 00:35:21,840
after the algorithm have been deployed.

516
00:35:21,840 --> 00:35:25,840
So after the harm has been done, plus the external auditors do not really

517
00:35:25,840 --> 00:35:30,840
have access to the models, to the training data, which are obviously

518
00:35:30,840 --> 00:35:32,840
protected as trade secrets.

519
00:35:32,840 --> 00:35:37,840
So as an alternative, another layer to governance via audits is you could do

520
00:35:37,840 --> 00:35:40,840
internal audits as proactive governance.

521
00:35:40,840 --> 00:35:45,840
And so at the time, researchers from Google who are no longer at Google right

522
00:35:46,840 --> 00:35:51,840
now, but they released this paper making a case for internal audits where audit

523
00:35:51,840 --> 00:35:55,840
would be part of a core part of product development at every step of the

524
00:35:55,840 --> 00:35:56,840
way.

525
00:35:57,840 --> 00:35:59,840
You could also do the best of both worlds, right?

526
00:35:59,840 --> 00:36:02,840
You could do something called cooperative audits, which is a fairly newer

527
00:36:02,840 --> 00:36:07,840
concept where while external audits answers what problems the platform has

528
00:36:07,840 --> 00:36:12,840
and internal audit says why that's happening, you could have a combination

529
00:36:12,840 --> 00:36:16,840
of both and you could do cooperative audits as shared governance, which allows

530
00:36:16,840 --> 00:36:20,840
external algorithm auditors to audit the system of willing private companies.

531
00:36:20,840 --> 00:36:27,840
So I've done a little bit of this with Spotify where working with their

532
00:36:27,840 --> 00:36:33,840
engineers within the company figuring out how gender representations might be

533
00:36:33,840 --> 00:36:38,840
biased or not for their taste on boarding and listen action on podcasts.

534
00:36:38,840 --> 00:36:42,840
And then finally, you also need to do these audits multiple times, right?

535
00:36:42,840 --> 00:36:46,840
Longitudinally, that is we need to conduct these continuous audits monitoring

536
00:36:46,840 --> 00:36:50,840
platforms multiple times instead of that single snapshot audit.

537
00:36:50,840 --> 00:36:54,840
Many of my studies that I presented today are all single snapshot and we do

538
00:36:54,840 --> 00:36:56,840
need that kind of longitudinal effort.

539
00:36:57,840 --> 00:37:02,840
So here is where I want to highlight one of the quotes from the Brajis earlier

540
00:37:02,840 --> 00:37:07,840
internal audit paper where they mentioned the audit process is necessarily

541
00:37:07,840 --> 00:37:11,840
boring, it is slow, it is methodological.

542
00:37:11,840 --> 00:37:17,840
Which stands in stark contrast to what I had started my earlier slides with

543
00:37:17,840 --> 00:37:21,840
move fast and break things, done is better than perfect, so very much in contrast

544
00:37:21,840 --> 00:37:23,840
with the rushed culture of technology development.

545
00:37:23,840 --> 00:37:27,840
And this is where I want to take a little bit of tangent and mention about audit

546
00:37:27,840 --> 00:37:31,840
possibilities. One of the fastest growing developing AI technologies is the

547
00:37:31,840 --> 00:37:35,840
large language models and what would auditing even look like for large

548
00:37:35,840 --> 00:37:39,840
language models? What is the blueprint for LLM auditing?

549
00:37:39,840 --> 00:37:42,840
And then also what are the key challenges, right?

550
00:37:42,840 --> 00:37:45,840
So one of the key challenges is that it is difficult to assess the risks that

551
00:37:45,840 --> 00:37:50,840
AI systems and large language models in particular pose independent of the

552
00:37:50,840 --> 00:37:52,840
context in which they are deployed.

553
00:37:52,840 --> 00:37:57,840
So we do need application specific audits for large language models.

554
00:37:57,840 --> 00:38:01,840
The second challenge is that, and I don't know how to solve this, or rather

555
00:38:01,840 --> 00:38:04,840
even the first one, is that the capabilities and the training processes

556
00:38:04,840 --> 00:38:08,840
of these foundation and models have really outpaced the development of the tools

557
00:38:08,840 --> 00:38:12,840
and techniques and the procedures for auditing, right?

558
00:38:12,840 --> 00:38:15,840
So it is really hard to keep up the pace.

559
00:38:15,840 --> 00:38:19,840
And so doing ethical, legal and technically robust audits makes it super

560
00:38:19,840 --> 00:38:23,840
challenging for such a rapidly developing technology.

561
00:38:23,840 --> 00:38:27,840
And so it must be complemented probably with much more newer forms of

562
00:38:27,840 --> 00:38:29,840
supervision and control.

563
00:38:29,840 --> 00:38:33,840
So here is one possible framework, one possible blueprint for auditing

564
00:38:33,840 --> 00:38:36,840
large language models, which is kind of three layers.

565
00:38:36,840 --> 00:38:38,840
So the first one is a model audit.

566
00:38:38,840 --> 00:38:42,840
So as a name suggests, it focuses on assessing the technical properties of

567
00:38:42,840 --> 00:38:44,840
the pre-attained language models.

568
00:38:44,840 --> 00:38:49,840
So this is very similar flavor to the internal audit that I mentioned earlier.

569
00:38:49,840 --> 00:38:52,840
So that's sort of proactive governance before you deploy the model.

570
00:38:52,840 --> 00:38:56,840
But then there is application audit, which focuses on the assessing the

571
00:38:56,840 --> 00:38:59,840
applications built on top of the LLMs.

572
00:38:59,840 --> 00:39:02,840
So which is these flavors of post hoc audit, right?

573
00:39:02,840 --> 00:39:05,840
And it should be done longitudinally, right?

574
00:39:05,840 --> 00:39:09,840
Multiple times, over a long period of time, so as to capture any sort of new

575
00:39:09,840 --> 00:39:11,840
properties that might be emerging.

576
00:39:11,840 --> 00:39:15,840
And then finally, I think this is the new form of audit that we haven't talked

577
00:39:15,840 --> 00:39:17,840
about a lot, at least the research community.

578
00:39:17,840 --> 00:39:21,840
These are these governance audits that is assessing the processes whereby these

579
00:39:21,840 --> 00:39:25,840
language models are designed and where they are disseminated.

580
00:39:25,840 --> 00:39:29,840
So very much process oriented, right?

581
00:39:29,840 --> 00:39:32,840
My next proposition is about value-centered audits.

582
00:39:32,840 --> 00:39:37,840
That is, there is this active conversation around social values, emphasizing

583
00:39:37,840 --> 00:39:39,840
while designing algorithms.

584
00:39:39,840 --> 00:39:42,840
And I think we also need to turn that attention and thinking into how we can

585
00:39:42,840 --> 00:39:45,840
value and respect humans involved in the audit process.

586
00:39:45,840 --> 00:39:49,840
So these humans could be in the form of users who use the system or even

587
00:39:49,840 --> 00:39:53,840
auditors who are investigating the sites.

588
00:39:53,840 --> 00:39:57,840
And so for auditors, if we bring back the conversation for a second back to

589
00:39:57,840 --> 00:40:01,840
misinformation, one instance where auditors did not really feel perceived

590
00:40:01,840 --> 00:40:06,840
fair treatment was this scenario where fact-checkers are one of the key auditors

591
00:40:06,840 --> 00:40:09,840
of misinformation on online platforms like Facebook, Twitter.

592
00:40:09,840 --> 00:40:13,840
So the fact-checking organizations, SNOPs, a couple of years ago, actually backed

593
00:40:13,840 --> 00:40:17,840
out of their partnership with Facebook because they didn't feel their values

594
00:40:17,840 --> 00:40:19,840
were being respected.

595
00:40:19,840 --> 00:40:22,840
I think to delve into this question of fair treatment of auditors, we need more

596
00:40:22,840 --> 00:40:23,840
effort.

597
00:40:23,840 --> 00:40:28,840
And so one way in which my group has started a few initiatives, we have

598
00:40:28,840 --> 00:40:32,840
launched a research endeavor with the fact-checking organization based in

599
00:40:32,840 --> 00:40:34,840
Kenya called Pesachek.

600
00:40:34,840 --> 00:40:37,840
And this has also expanded to 16 other fact-checking organizations across

601
00:40:37,840 --> 00:40:39,840
four different continents.

602
00:40:39,840 --> 00:40:42,840
And we released our first report called the Human and Technological

603
00:40:42,840 --> 00:40:45,840
Infrastructures of Fact-Checking.

604
00:40:45,840 --> 00:40:50,840
And so one big motivation for this work was also this question of, are we

605
00:40:50,840 --> 00:40:53,840
really taking into account diverse voices when we are talking about

606
00:40:53,840 --> 00:40:55,840
governance and governing technologies?

607
00:40:55,840 --> 00:40:59,840
And are we really doing culturally responsible AI?

608
00:40:59,840 --> 00:41:02,840
Finally, how do we ensure actionable audits?

609
00:41:02,840 --> 00:41:05,840
That is, audits that result in real change.

610
00:41:05,840 --> 00:41:10,840
So one of the most successful examples of an actionable audit is Joy Voluwami's

611
00:41:10,840 --> 00:41:12,840
Gender Shade Study.

612
00:41:12,840 --> 00:41:17,840
So what she did was she audited facial recognition algorithms.

613
00:41:17,840 --> 00:41:21,840
And within seven months of the release of these original audit, all the

614
00:41:21,840 --> 00:41:26,840
three companies who had their facial recognition apps released new API

615
00:41:26,840 --> 00:41:31,840
versions that reduced accuracy disparities with gender, male and

616
00:41:31,840 --> 00:41:35,840
female, as well as race, darker and lighter-skinned subgroups.

617
00:41:35,840 --> 00:41:39,840
So in other words, the Gender Shade Study is a classic example of commercial

618
00:41:39,840 --> 00:41:41,840
actual impact.

619
00:41:41,840 --> 00:41:44,840
And so they laid out their approach in this actionable auditing paper of

620
00:41:44,840 --> 00:41:45,840
theirs.

621
00:41:45,840 --> 00:41:47,840
Highly recommend you all to go and refer to it.

622
00:41:47,840 --> 00:41:51,840
But turns out actionable auditing is often tremendously difficult to

623
00:41:51,840 --> 00:41:52,840
achieve.

624
00:41:52,840 --> 00:41:55,840
And here is where I want to revisit that earlier slide for our Amazon

625
00:41:55,840 --> 00:41:59,840
study to highlight how much we had failed in doing the actionable

626
00:41:59,840 --> 00:42:00,840
auditing.

627
00:42:00,840 --> 00:42:05,840
So despite widespread media coverage, despite a letter from Congressman

628
00:42:05,840 --> 00:42:08,840
Adam Schiff, Amazon did not really act much.

629
00:42:08,840 --> 00:42:13,840
All they did was add that banner of COVID-19 information directing to

630
00:42:13,840 --> 00:42:15,840
CDC's web page.

631
00:42:15,840 --> 00:42:19,840
So hopefully this kind of summarizes the challenges as well as opportunities

632
00:42:19,840 --> 00:42:23,840
and setting the path for algorithmic governance and hoping with the new

633
00:42:23,840 --> 00:42:28,840
regulations coming in place, maybe if I were to give this talk next year, I

634
00:42:28,840 --> 00:42:33,840
would have a little bit more hopeful slide than how I'm ending this talk.

635
00:42:33,840 --> 00:42:35,840
So that's it.

636
00:42:35,840 --> 00:42:37,840
So this is all I talked about today.

637
00:42:37,840 --> 00:42:40,840
Most of this work was done with my PhDs.

638
00:42:41,840 --> 00:42:44,840
Then PhD student Prena Juneja, who is now a faculty at Seattle

639
00:42:44,840 --> 00:42:45,840
University.

640
00:42:45,840 --> 00:42:50,840
And then my group, I would also stick in three other threads of work,

641
00:42:50,840 --> 00:42:53,840
which I obviously don't have time to talk about.

642
00:42:53,840 --> 00:42:56,840
But these are like a couple of other amazing students.

643
00:42:56,840 --> 00:43:00,840
So Shruti Furkaya she did a bunch of work on computational social

644
00:43:00,840 --> 00:43:01,840
science.

645
00:43:01,840 --> 00:43:05,840
I was earlier meeting a student who was doing this sort of work.

646
00:43:05,840 --> 00:43:09,840
So things like big data analysis of online interactions.

647
00:43:09,840 --> 00:43:13,840
Studying trajectories of participation of users in extreme

648
00:43:13,840 --> 00:43:15,840
communities, conspiratorial communities.

649
00:43:15,840 --> 00:43:19,840
We have also done a little bit of design intervention and social

650
00:43:19,840 --> 00:43:24,840
system design work with another student who is also a faculty now.

651
00:43:24,840 --> 00:43:28,840
This is more of an XCI flavor where essentially questions like how do

652
00:43:28,840 --> 00:43:32,840
you design a system to nudge users towards meaningful credibility

653
00:43:32,840 --> 00:43:33,840
assessment?

654
00:43:33,840 --> 00:43:38,840
How do you design a system to allow users to break out of their filter

655
00:43:38,840 --> 00:43:39,840
bubble?

656
00:43:39,840 --> 00:43:42,840
Something called other tube that we built on YouTube.

657
00:43:42,840 --> 00:43:47,840
And then finally, the last and the least fleshed out thread is some

658
00:43:47,840 --> 00:43:51,840
of the work that is currently ongoing with two of my students.

659
00:43:51,840 --> 00:43:54,840
We are looking at challenges and opportunities of generative AI

660
00:43:54,840 --> 00:43:56,840
in fact checking work.

661
00:43:56,840 --> 00:44:00,840
And then what are some cultural misalignment that might happen with

662
00:44:00,840 --> 00:44:01,840
language models?

663
00:44:01,840 --> 00:44:05,840
Especially with roots in the global south, we are looking at

664
00:44:05,840 --> 00:44:09,840
cultural implications of these language models in countries like

665
00:44:09,840 --> 00:44:13,840
India and other countries in Southeast Asia.

666
00:44:13,840 --> 00:44:16,840
So with that, I would like to end and happy to take questions.

667
00:44:16,840 --> 00:44:17,840
Thank you all.

668
00:44:17,840 --> 00:44:27,840
All right, we've got time for some questions.

669
00:44:27,840 --> 00:44:32,840
I was wondering in your auditing of YouTube algorithms that you guys

670
00:44:32,840 --> 00:44:33,840
looked at.

671
00:44:33,840 --> 00:44:36,840
Yes, it was in 2020.

672
00:44:36,840 --> 00:44:40,840
So I wasn't sure if YouTube shorts had been implemented since then

673
00:44:40,840 --> 00:44:43,840
because YouTube shorts are somewhat of a newer aspect.

674
00:44:43,840 --> 00:44:50,840
But I wonder if the algorithms that underlie the, I guess, traditional

675
00:44:50,840 --> 00:44:54,840
YouTube recommendation system underlie the same sort of like YouTube

676
00:44:54,840 --> 00:44:59,840
shorts recommendation because I guess the length of content and sort of

677
00:44:59,840 --> 00:45:02,840
the amount of stimulus that would be needed to get the person to keep

678
00:45:02,840 --> 00:45:03,840
the point would be different.

679
00:45:03,840 --> 00:45:07,840
And therefore possibly seeing that if there are sort of similar

680
00:45:07,840 --> 00:45:14,840
pattern between the two, whether or not the density, I guess, of

681
00:45:14,840 --> 00:45:17,840
sort of misinformation sort of increases because the fact that

682
00:45:17,840 --> 00:45:18,840
content is more short form.

683
00:45:18,840 --> 00:45:19,840
Yeah.

684
00:45:19,840 --> 00:45:21,840
So for the first study, we didn't, at that time, shorts were not

685
00:45:21,840 --> 00:45:22,840
there.

686
00:45:22,840 --> 00:45:26,840
But then for the third one that I presented with election misinformation,

687
00:45:26,840 --> 00:45:28,840
we did capture YouTube shorts.

688
00:45:28,840 --> 00:45:31,840
And that tells me that we should probably do another analysis

689
00:45:31,840 --> 00:45:35,840
comparing the length of the videos and the, you know, whether it's a

690
00:45:35,840 --> 00:45:37,840
short video versus a long form.

691
00:45:37,840 --> 00:45:39,840
We did not do that, but that's an excellent point.

692
00:45:39,840 --> 00:45:40,840
Yeah.

693
00:45:43,840 --> 00:45:45,840
I have a question on maybe two studies.

694
00:45:45,840 --> 00:45:47,840
Was it a good first one?

695
00:45:47,840 --> 00:45:50,840
Did you look at all at like the probability that you would get

696
00:45:50,840 --> 00:45:54,840
recommended legitimately false conspiracy videos, like on the

697
00:45:54,840 --> 00:45:59,840
moon landing from videos about conspiracies that are a little bit

698
00:45:59,840 --> 00:46:02,840
more true, like missing persons cases that the police just don't

699
00:46:02,840 --> 00:46:06,840
investigate and like the likelihood that you'll get recommended

700
00:46:06,840 --> 00:46:08,840
actually false content?

701
00:46:08,840 --> 00:46:13,840
Yeah, we did not because I think one of the shortcomings of running

702
00:46:13,840 --> 00:46:15,840
audits is the whole setup itself.

703
00:46:15,840 --> 00:46:17,840
So we have to start somewhere, right?

704
00:46:17,840 --> 00:46:21,840
So the RR starting point were a set of seed queries.

705
00:46:21,840 --> 00:46:22,840
Right?

706
00:46:22,840 --> 00:46:27,840
So with the way you are framing it, you know, we could, the

707
00:46:27,840 --> 00:46:30,840
hypothesis could be dozen missing persons case lead you to more

708
00:46:30,840 --> 00:46:31,840
conspiratory videos.

709
00:46:31,840 --> 00:46:34,840
And in that scenario, I think we can use our audit framework to

710
00:46:34,840 --> 00:46:38,840
have those as seed queries and see what happens.

711
00:46:42,840 --> 00:46:47,840
On your kind of concluding point about actionable audits, I'm just

712
00:46:47,840 --> 00:46:50,840
curious, do you think it's something to do with like the

713
00:46:50,840 --> 00:46:53,840
conducting of the audit itself or just the context and how it

714
00:46:53,840 --> 00:46:56,840
aligns with like the company's incentives?

715
00:46:56,840 --> 00:47:00,840
Because it feels like the gender shades case, it was like a very

716
00:47:00,840 --> 00:47:03,840
easily framed as like poor performance.

717
00:47:03,840 --> 00:47:05,840
And so they were trying to cover themselves.

718
00:47:05,840 --> 00:47:08,840
Whereas Amazon is somewhat incentivized to keep people buying

719
00:47:08,840 --> 00:47:11,840
things even if those things are harmful.

720
00:47:11,840 --> 00:47:15,840
So like, I guess I'm wondering, like, do you think audits need to

721
00:47:15,840 --> 00:47:18,840
be conducted differently or there just needs to be more external

722
00:47:18,840 --> 00:47:22,840
pressure like from the government or the public to incentivize the

723
00:47:22,840 --> 00:47:24,840
companies when they are like internally?

724
00:47:24,840 --> 00:47:26,840
Yeah, I don't think it's a ladder.

725
00:47:26,840 --> 00:47:32,840
I think, I think Joy went on to great extent to after the study was

726
00:47:32,840 --> 00:47:36,840
published to kind of give talks and publicize and do that kind of

727
00:47:36,840 --> 00:47:39,840
outreach, which I did not do with this work.

728
00:47:39,840 --> 00:47:41,840
I think that matters a lot, right?

729
00:47:41,840 --> 00:47:44,840
She's the one who went to Congress for testimony and testified

730
00:47:44,840 --> 00:47:46,840
against these companies.

731
00:47:46,840 --> 00:47:50,840
And when you do that kind of impact, it would definitely translate

732
00:47:50,840 --> 00:47:54,840
or there's a higher chance to be for your work to be translated to

733
00:47:54,840 --> 00:47:56,840
actual actionable outcome.

734
00:47:56,840 --> 00:47:59,840
I don't think those are actually steps listed in the actionable

735
00:47:59,840 --> 00:48:01,840
auditing paper.

736
00:48:01,840 --> 00:48:04,840
And in some sense, I feel like maybe the academic community need to

737
00:48:04,840 --> 00:48:07,840
think about how to incentivize those additional work.

738
00:48:07,840 --> 00:48:09,840
We don't have those incentives in place.

739
00:48:09,840 --> 00:48:12,840
And partly I think we should look inward and blame ourselves that

740
00:48:12,840 --> 00:48:14,840
we don't have those incentives in place.

741
00:48:17,840 --> 00:48:20,840
Moving to this, I just wanted to hear your thoughts more.

742
00:48:20,840 --> 00:48:24,840
When being either interactive or reactive, do you think we should

743
00:48:24,840 --> 00:48:28,840
draw a distinction between conspiracy theories or misinformation

744
00:48:28,840 --> 00:48:33,840
that has potential for great harm versus those that maybe don't,

745
00:48:33,840 --> 00:48:36,840
right, to justify interventions that override individual autonomy

746
00:48:36,840 --> 00:48:38,840
or control the information space?

747
00:48:38,840 --> 00:48:42,840
Like, who is the moon landing conspiracy theory?

748
00:48:42,840 --> 00:48:44,840
Can you say that last part?

749
00:48:44,840 --> 00:48:48,840
Like, the moon landing conspiracy presumably isn't hurting anyone,

750
00:48:48,840 --> 00:48:49,840
right?

751
00:48:49,840 --> 00:48:51,840
Should we take it down?

752
00:48:51,840 --> 00:48:53,840
Yeah, that's a really good point.

753
00:48:53,840 --> 00:48:57,840
I think for companies like Google, I know they have this,

754
00:48:57,840 --> 00:49:01,840
your money or your life, they have a view or an acronym,

755
00:49:01,840 --> 00:49:06,840
YMYL or something, a set of search guidelines.

756
00:49:06,840 --> 00:49:11,840
If those search results or the pages that show up,

757
00:49:11,840 --> 00:49:16,840
if it's affecting monetarily, financially, health or your life,

758
00:49:16,840 --> 00:49:19,840
then they're going to be more proactive and act on it.

759
00:49:19,840 --> 00:49:20,840
So you're right.

760
00:49:20,840 --> 00:49:23,840
Like, moon landing is probably not to that extent,

761
00:49:23,840 --> 00:49:28,840
versus if it's vaccine information that has direct life consequences,

762
00:49:28,840 --> 00:49:29,840
right?

763
00:49:29,840 --> 00:49:32,840
But obviously then there are all these other questions that when

764
00:49:32,840 --> 00:49:35,840
it's very well known that if you are drawn to one conspiracy,

765
00:49:35,840 --> 00:49:38,840
you're likely to get other conspiracy theories, right?

766
00:49:38,840 --> 00:49:40,840
So then what happens?

767
00:49:40,840 --> 00:49:43,840
Should those be prioritized, at least to the extent that they

768
00:49:43,840 --> 00:49:46,840
maybe should be prioritized in the recommendations,

769
00:49:46,840 --> 00:49:51,840
if not completely removed from the platform?

770
00:49:51,840 --> 00:49:52,840
Yeah?

771
00:49:52,840 --> 00:49:55,840
This is sort of another sort of idea that I had.

772
00:49:55,840 --> 00:49:59,840
Sort of looking at the idea of, like, what I think is sort of

773
00:49:59,840 --> 00:50:02,840
interesting about, like, social media apps like YouTube is a whole

774
00:50:02,840 --> 00:50:05,840
aspect of the media being able to communicate with others,

775
00:50:05,840 --> 00:50:07,840
like the other comment section of a YouTube channel.

776
00:50:07,840 --> 00:50:10,840
And I was wondering if there is a way to possibly, like,

777
00:50:10,840 --> 00:50:12,840
I'm not sure how present this is, or if this is even, like,

778
00:50:12,840 --> 00:50:17,840
a thing that is even, like, something that is able to be,

779
00:50:17,840 --> 00:50:18,840
like, looked into.

780
00:50:18,840 --> 00:50:25,840
But is there a possibility that the algorithm isn't,

781
00:50:25,840 --> 00:50:29,840
may almost be promoting you content not necessarily by what

782
00:50:29,840 --> 00:50:34,840
it's physically providing or, like, showing up then recommended,

783
00:50:34,840 --> 00:50:38,840
but showing other users who would most likely put other links

784
00:50:38,840 --> 00:50:41,840
to more, I guess, like, extreme videos in the comments being like,

785
00:50:41,840 --> 00:50:43,840
oh, if you thought this was interesting, like, look at this.

786
00:50:43,840 --> 00:50:47,840
And so that's not explicitly YouTube's algorithm showing you a video.

787
00:50:47,840 --> 00:50:51,840
It's showing that same video to someone else who has the ability

788
00:50:51,840 --> 00:50:55,840
to share a link to another YouTube video.

789
00:50:55,840 --> 00:50:58,840
That would be almost, like, pushing someone down like a pipeline

790
00:50:58,840 --> 00:50:59,840
of conspiracy theories.

791
00:50:59,840 --> 00:51:02,840
So more like how the social recommendations,

792
00:51:02,840 --> 00:51:05,840
like, you're sort of adding this collaborative social

793
00:51:05,840 --> 00:51:09,840
recommendation component to YouTube and seeing how that pushes.

794
00:51:09,840 --> 00:51:13,840
Like, if YouTube, like, you could have the same effect.

795
00:51:13,840 --> 00:51:17,840
Like, in theory, maybe, the YouTube recommendation system could

796
00:51:17,840 --> 00:51:21,840
not explicitly push someone by recommending, like,

797
00:51:21,840 --> 00:51:23,840
more intense conspiracy theories.

798
00:51:23,840 --> 00:51:26,840
But if YouTube is recommending someone who's already, like,

799
00:51:26,840 --> 00:51:29,840
a very, like, entrenched conspiracy theorist and maybe someone

800
00:51:29,840 --> 00:51:33,840
who's on the edge, if YouTube recommends them both the same,

801
00:51:33,840 --> 00:51:37,840
like, it's starting out conspiracy theory video,

802
00:51:37,840 --> 00:51:39,840
then you can have the person who's, like,

803
00:51:39,840 --> 00:51:41,840
very entrenched conspiracy theorist commenting

804
00:51:41,840 --> 00:51:43,840
and suggesting things themselves.

805
00:51:43,840 --> 00:51:47,840
And it's not that YouTube is explicitly recommending the original person

806
00:51:47,840 --> 00:51:51,840
or the person who isn't, like, entrenched conspiracy theory.

807
00:51:51,840 --> 00:51:55,840
It's that they put them essentially on the same,

808
00:51:56,840 --> 00:51:59,840
or they put them in the same environment in which they could communicate.

809
00:51:59,840 --> 00:52:00,840
Yeah.

810
00:52:00,840 --> 00:52:04,840
So, actually, my group of some of Shruti's work,

811
00:52:04,840 --> 00:52:08,840
we have done this in the context of Reddit where what you're describing

812
00:52:08,840 --> 00:52:10,840
those very entrenched conspiracy users,

813
00:52:10,840 --> 00:52:12,840
we term this as veteran users.

814
00:52:12,840 --> 00:52:16,840
And so they are one of the big drivers of bringing other people,

815
00:52:16,840 --> 00:52:20,840
like what we call joiners, into the community of conspiracy group.

816
00:52:20,840 --> 00:52:24,840
So I hope YouTube never does that for what you're suggesting,

817
00:52:24,840 --> 00:52:27,840
but that's a classic marker of how these social dynamics

818
00:52:27,840 --> 00:52:31,840
can actually bring people into these conspiratorial world views.

819
00:52:31,840 --> 00:52:34,840
And, you know, empirically, we have seen that.

820
00:52:34,840 --> 00:52:36,840
And there is also social science theory

821
00:52:36,840 --> 00:52:39,840
proving that that definitely happens.

822
00:52:39,840 --> 00:52:41,840
Yeah.

823
00:52:41,840 --> 00:52:47,840
One challenge that I feel like our field faces with audits is,

824
00:52:47,840 --> 00:52:51,840
I guess what I would describe as the sense I get of frustration

825
00:52:51,840 --> 00:52:55,840
from folks at these companies who feel like the audits

826
00:52:55,840 --> 00:53:00,840
aren't well executed or are way out of date as soon as they're published.

827
00:53:00,840 --> 00:53:04,840
You made this point that we have to be very methodical

828
00:53:04,840 --> 00:53:07,840
and often slow in doing this.

829
00:53:07,840 --> 00:53:11,840
Then you throw in the peer review pipeline

830
00:53:11,840 --> 00:53:14,840
that can slow things down further.

831
00:53:14,840 --> 00:53:17,840
And by the time the thing comes out,

832
00:53:17,840 --> 00:53:21,840
I remember seeing an applied researcher in a company

833
00:53:21,840 --> 00:53:24,840
who in principle would be more open to this kind of stuff,

834
00:53:24,840 --> 00:53:27,840
being like, our album doesn't even work like that anymore.

835
00:53:27,840 --> 00:53:32,840
And so I'm wondering, so obviously you listed a bunch of possible

836
00:53:32,840 --> 00:53:36,840
cooperative audits, longitudinal, internal, and so on.

837
00:53:36,840 --> 00:53:41,840
I'm curious, are there, is there anything we can do to address that?

838
00:53:41,840 --> 00:53:45,840
Let's assume that we can't change the incentives of the companies,

839
00:53:45,840 --> 00:53:47,840
to change how quickly we do the audits.

840
00:53:47,840 --> 00:53:50,840
Are we always going to be vulnerable to this,

841
00:53:50,840 --> 00:53:53,840
like, oh yeah, that was yesterday's algorithm kind of critique?

842
00:53:53,840 --> 00:53:55,840
Yeah, that's a really good point.

843
00:53:55,840 --> 00:53:59,840
In fact, I was at a workshop with other folks like Christo Wilson

844
00:53:59,840 --> 00:54:03,840
and a few others who have done audits for a very long time

845
00:54:03,840 --> 00:54:05,840
with people from Facebook and YouTube.

846
00:54:05,840 --> 00:54:08,840
And I think we came up with this exact same question.

847
00:54:08,840 --> 00:54:10,840
And I think the common thing that emerged was that,

848
00:54:10,840 --> 00:54:14,840
I don't think academics should be the, or academic institutions

849
00:54:14,840 --> 00:54:18,840
should be the places to do these sorts of long-term audits.

850
00:54:18,840 --> 00:54:22,840
It's fine to kind of develop the methods and, you know,

851
00:54:22,840 --> 00:54:25,840
kind of say, okay, this one should pay attention, for example,

852
00:54:25,840 --> 00:54:28,840
to Amazon, or, you know, this is the method to do it.

853
00:54:28,840 --> 00:54:31,840
But then you need, like, separate third-party companies

854
00:54:31,840 --> 00:54:33,840
to continuously do these audits, right?

855
00:54:33,840 --> 00:54:35,840
So sort of like consumer reports,

856
00:54:35,840 --> 00:54:38,840
what's the equivalent of that for audits?

857
00:54:38,840 --> 00:54:40,840
And I think at CSCW, the closing keynote,

858
00:54:40,840 --> 00:54:42,840
Room Room was mentioning that kind of red teaming,

859
00:54:42,840 --> 00:54:45,840
I think their company or whichever NGO she's working with,

860
00:54:45,840 --> 00:54:47,840
they are doing something like that.

861
00:54:47,840 --> 00:54:49,840
I think, so academics with that peer review process,

862
00:54:49,840 --> 00:54:52,840
I don't think we should be responsible for doing those sorts of

863
00:54:52,840 --> 00:54:55,840
continuous audits because we are always going to play catch-up

864
00:54:55,840 --> 00:54:58,840
with companies.

865
00:54:58,840 --> 00:55:00,840
Yeah.

866
00:55:00,840 --> 00:55:01,840
Oh.

867
00:55:01,840 --> 00:55:02,840
One last? Yeah.

868
00:55:02,840 --> 00:55:03,840
At the end.

869
00:55:03,840 --> 00:55:05,840
Do you have thoughts on, like, Twitter has implemented, like,

870
00:55:05,840 --> 00:55:07,840
community notes where people can just, like,

871
00:55:07,840 --> 00:55:10,840
anyone can put it under a post like,

872
00:55:10,840 --> 00:55:12,840
that's not true, or this is misleading,

873
00:55:12,840 --> 00:55:14,840
or this person ever said that, like,

874
00:55:14,840 --> 00:55:17,840
just, like, user-based, immediate type auditing,

875
00:55:17,840 --> 00:55:18,840
if you will.

876
00:55:18,840 --> 00:55:20,840
I do have thoughts on that versus, like,

877
00:55:20,840 --> 00:55:22,840
companies long-term auditing, or, like,

878
00:55:22,840 --> 00:55:23,840
if you think that's a good idea.

879
00:55:23,840 --> 00:55:25,840
Yeah, that's a really good point.

880
00:55:25,840 --> 00:55:27,840
We haven't looked at community notes,

881
00:55:27,840 --> 00:55:30,840
but I know, you know, some researchers have,

882
00:55:30,840 --> 00:55:32,840
kind of, looked and researched it.

883
00:55:32,840 --> 00:55:35,840
I don't have really any very smart thoughts as to,

884
00:55:35,840 --> 00:55:38,840
other than the usual advice that it's a good thing that

885
00:55:38,840 --> 00:55:41,840
one should do it, but with the caveat that,

886
00:55:41,840 --> 00:55:47,840
if the community doesn't reflect the right view,

887
00:55:47,840 --> 00:55:49,840
or I wouldn't use the word right,

888
00:55:49,840 --> 00:55:54,840
but, you know, a credible view of what happens then.

889
00:55:54,840 --> 00:55:59,840
That's problematic.

890
00:55:59,840 --> 00:56:00,840
I think that's time.

891
00:56:00,840 --> 00:56:01,840
So let's thank your speaker.

892
00:56:01,840 --> 00:56:02,840
Thank you.

893
00:56:02,840 --> 00:56:04,840
Thank you, everyone.

894
00:56:08,840 --> 00:56:09,840
Thank you.

