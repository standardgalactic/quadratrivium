{"text": " We're glad to have Angela Fan today with us here. And she's a research scientist at Meta AI Research in New York, focusing on research in text generation mainly. And currently she's working on language modeling and developing the line AI agents, metaproducts, and recent research products include no language left behind, which she'll be talking briefly about today, universal speech translation for unwritten languages, as well as Lama2. So give it up for Angela, I guess. All right, thank you all so much. So yeah, when I got this email, I was like, oh, I should probably talk about Lama2. But then I noticed you have Sharon, who will like, you know, is like a 10x better speaker than me. So I was like, okay, like maybe not Lama2. But then I thought I maybe would cover this project that we did called No Language Left Behind, which could be very, also very relevant to this class. And so when you think about a lot of text generation technology, most of it, until fairly recently, has been really focused on English. But there are actually more than 3000 written languages worldwide. And for me, this is extremely personally meaningful because actually English is my third language. So it's really important. Yeah, so it's really also very personally meaningful. And when you think about some of the multilingual technology that permeates, it's not like we've never worked on multilingual, right? Actually, when speaking about generative AI, I actually think translation is one of the most commercially successful and widespread applications of generative AI. I mean, ultimately, translation models, they are, you know, like conditional language models. And so when you think about like traveling or something like that, or my sister is taking Spanish, so like just like doing her Spanish homework, we have a lot of tools that exist today. So things like Google Translate cover around 130 languages, Microsoft Translate about 110. This might be a little bit outdated since I pulled the statistics a little bit ago. But the project for No Language Left Behind, it started from like a very simple ask, like, okay, there's 3000 languages worldwide. Maybe it'll be like pretty hard to get to all 3000, since some of them are pretty rare and not spoken by many. But there are still like hundreds of languages spoken by millions and millions of people. And so we were like, okay, no big deal. Like, let's just start from the 100-ish that we have today and just go for like a doubling. Like, what would it take to actually be able to double this kind of coverage? And of course, you know, just saying that you support a bunch of languages is not the goal. You actually want to create high quality safe translations that would be usable by people just like if you're going on vacation today, you're kind of instinctive to whip out your phone and get on the Google Translate app. And so kind of the backdrop to this project was that there's actually a lot of progress in translation. So historically, there's been a lot of focus on what we call higher resource languages. And these are not necessarily languages that are spoken by the most people in the world. But when we say higher resource, it means the most amount of data. And so you can think about things like Europarl or, you know, translations from the European Parliament. And those served as the foundation for a lot, a lot of translation development. And more recently, there's been a great focus on low resource languages, and it's been driven across the research community with groups like Ghana NLP, Masekane, America's NLP. And these are all really exciting developments. And so these have led to a lot of development of new data sets, as well as criticisms of existing data sets, and also work on new languages, and usually languages that people kind of speak and they care a lot about. And we found this like really, really exciting. And so looking at a lot of this, a bunch of us got together at fair and started thinking like, okay, we actually speak some pretty low resource languages from like Catalan to Ossamese and so on. And so we started this as kind of like a big, passionate research project. And so today, I want to cover a little bit about our high level approach to this problem, which is a little bit interdisciplinary. I want to talk about how we actually created the data sets to be able to support this kind of work. Of course, I want to talk about the models, since this is a class about transformers. One note here, I think that's actually very interesting in terms of translation as like a research direction, is that actually a lot of innovations have been done in translation. The original transformer paper, I think is one of them, and which makes always translation a quite interesting area to work on. Because I feel like it's a very mature research area as well. So it kind of is like, okay, if your architecture works in translation, it probably works very generally. So that's also one of the things that excites me about translation research. Then I want to talk about evaluation, like how are we actually measuring and ensuring the quality of these translations are good and safe for people. And then I want to end with a little bit of like, you know, high level thoughts about future directions and things that I hope that we can work on in the future. So I want to start with our approach. I think the most important thing in research is to know that we're working on a real problem, especially when it's really close to people like translation. And I think in many areas, like when I was working on on-device AI, for example, I feel like I had like a research problem in mind, but it was like very, very disconnected from the practical problem of actually putting models on phones. And so this was something that was really important to us. And so we actually started the project by kind of like focusing on a social sciences type approach or, you know, sociology type approach. And we actually did a lot of interviews with low resource speakers. And so we met with about 44 different native speakers that spoke 36 different languages across North America. I will say that a lot of them are like immigrants to the US, since that was kind of like the easiest kind of cohort to recruit. And we learned a lot of different things about how they approach low resource languages, but also the kind of technological need that they have. Because I think it's easy to be like, hey, I have this cool background, like I have this cool problem, and I want to solve it, but I think it's very important to actually like talk to the people if this is a problem that needs to be solved. And so we learned that there's great fear in general that low resource languages might be undergoing a state of decline, partially because a lot of education is shifting to languages like Hindi or like English or Mandarin Chinese, for example. And there's a lot of excitement to be included in existing translation systems. And people said they have always tried to use Google translator, Microsoft translate in their existing languages. But ultimately they found that the quality is really insufficient for reliable usage. So if you think about like, well, I was going to say when I was in high school, but you're all probably like substantially younger than me. So maybe like, you know, 10, so years ago, you know, and you tried to use Google translate for your Spanish homework, like your Spanish teacher could always identify that like, you know, it was not a human written translation until you would get marks off. But that's not really the case for some of the high resource languages today. And so I think as with all things in machine learning, it really starts from a data perspective. Like why can't we just train models in hundreds of languages or large language models in hundreds of languages? It's because we don't have the data to support it. And so I want to talk first about evaluation data sets because I think it's extremely important to nail evaluation. And then I'll talk about training. So for an evaluation data set for this work, we started this Flora's effort, it stands for Facebook low resource, I guess we're called meta now, but I didn't think more as was like a very good, a renaming so we're still calling it Flora's. So this was something we originally started for just two languages in this first paper at EMLP, many years ago. So it was just for Napoleon Sinhalo. And we later extended it to incorporate two more languages in a release afterwards. You know, we thought a lot about, okay, like Flora's was really useful for the community. How can we extend it to 100 languages? And so that was this follow up work that we did, I think we had at ACL or WMT. And then in this project, we were like, okay, how can we go from Flora's 101 to Flora's 200 to really go for the doubling effect? And so what is Flora's? Well, it's in the name. It's a focus on low resource languages. So we do include some higher resource languages like German or Hindi or so on, almost for calibration effect as well. But the majority of the focus is on these lower and mid resource languages. It's the first large scale, many to many machine translation evaluation data set, which means that we take all of the sentences in English and then we translate them to all of the languages, which means that you would be able to evaluate any cross pair of languages. So for example, like Chinese to French, I lived in France for many years. So it's like very personally relevant to me. Of course, 200 languages also in the name, there's a broad diversity of different domains and topics. I think this is important when designing an evaluation data set, which is like very top of mind for anybody interested in language modeling research, because like the way people train machine translation models and the way people use them are often like very different. And so if you only benchmark your data set, for example, on news, which is very common in translation research, then you don't really pick up the fact that people talk about such a wide variety of things and have like different casual conversations that they need translated official documents and so on. It's also document level data set. This is not something that I think the community is like broadly leveraging right now. But the way it's translated is that you can have document level context. And so translators are provided the entire document to translate from. We also provide the entire document for evaluation and we translate like multiple sentences from the same paragraph. And so this was like a potential to research direction that we wanted to make sure we covered models that needed like potentially more context because a lot of translation work is done at the sentence level. So how do we actually ensure that this data set was high quality? So the first step is that we take a document. Well, actually, first step is like alignment on language standards. So this is very important because when you're translating French or Chinese, I think most people have a strong understanding of like what it means to produce like a good French or good Chinese. And there are a lot of professional translators hired in these languages. But when you go to lower resource languages, it's not necessarily the case that there's like a glowing translation industry around translating a lower resource language. And so one of the first things is actually to align on like what is a high quality translation? And so there's actually a lot of challenges here. So there are certain low resource languages where there's different competing language standards or there's like very high variance in different regions on how languages are spoken. And so this step is a pretty critical one. So then what we do is we take the document, we send it to one group of translators and they do the first translation step. Then we do some automatic checking, you know, like if the input sentence was like 10 words and the output sentence is like 300 words, it's like most likely something went wrong, and so we send it back. Otherwise, we'll send it onwards to a separate, completely independent set of translators that do review. And so they try to rate the quality of this. And if the quality doesn't pass the sufficient bar, it gets sent back to the original set of translators to edit and they kind of go through and like address all of the feedback. And then if it's good enough, then it enters our data set. And so there's many challenges here. The first one, of course, is just like finding translators and also finding more translators. There was a certain issue that we ran into, for example, that in a certain country that the internet was not available. And so, you know, it's a lot of recruitment. The other one, of course, is language standardization. I think I briefly mentioned this before, but there's a lot of different challenges in just understanding like what is a high quality translation. For example, the low resource language, Breton. There's like two competing groups on like, how do you write Breton? So it's like very difficult to resolve some of those things. And the final thing is that there's actually a lot of variation, even in languages like Arabic, like the Arabic, like Moroccan Arabic is very different from, you know, Jordanian Arabic and so on. And there are also certain regions that they speak the same language, but due to historical reasons, they write in different scripts. And so one of the things we actually did was like, if there are languages written in multiple scripts, we actually supported the collection of a multiple script evaluation. And I think this is really important because if you're building an underlying technology and you only choose one, then I think you risk like just kind of like naturally supporting one over the other when we really should be like kind of a more neutral technology provider. And so this is something that we we explored a lot as well as exploring different variants of Arabic. This is also open source. If you just go to this link, you can just like download all of the all of the text files for this. With evaluation done, I want to talk a little bit about how we collected some of these training data sets. The first thing I want to talk about is this data set we created called NLBCD. And the idea of this is like it's a really seed data set of high quality translations and languages that really don't have anything. Why? Because, well, you can't start from nothing, you know, you got a bootstrap from somewhere. A lot of people have been using the Bible as a way to bootstrap, but it's very limited domain, obviously very religious text. And so we created this data set NLBCD for languages that really don't have anything to get started from. It's only about 5,000 sentences, so it's nothing crazy, but it supports a lot of different use cases like training language identification models or sentence encoders, engram language models, like all of these things that I'm about to talk about in our data set pipeline. So it covers 43 languages, about 6,000 sentences. And the way we decided to sample it is focused on really general content. So Wikipedia has this article of like, hey, if you're going to start like a new Wikipedia in your new language, I think Wikipedia has like 309-ish Wikipedia's last I checked. Here's like a list of articles that every Wikipedia in a new language should have. And so that's where we sampled this original content from. And of course, it's also open source if you want to download it. So what we ended up doing to get large-scale training data is using mining. So this is not something we pioneered in this project. We have like a bunch of different previous work. So we started from Wikimatrix. We were like, hey, there's a lot of different sentences in Wikipedia and different languages that we should be able to match up. And so we tried to do that with Wikipedia to get machine translation training data. We extended that to the web in the CCMatrix project, and then we extended it to very, very large-scale mining on all cross-pairs in this project on beyond English-centric multilingual machine translation. We really tried to ditch like English as a central pivot language. And so the way this whole data mining thing works is that it focuses on sentence alignment. So everyone is probably super familiar with this because this is how language models are built now. But it's like you take Common Crawl or any other open source dump of the web. I don't know, like Red Pajama or like whatever you want to CCNet, whatever you want to use these days. And you take all of the data, you extract all of the text, you know, a lot of HTML parsing and so on goes into it. And the idea is that we want to try to find matching text that could be a translation. So we shatter it all into sentences, we embed them with different sentence encoder models, and then we do a match to try to understand in a multilingual space if the sentences match. And so one of the biggest challenges to this is that the quality of the sentence encoding is very important. So if your sentence encoding is not very accurate, then it's impossible to match in this multidimensional space the idea of like the meaning being the same. And so one of the big things we tried to do here in this project was try to improve the quality of the sentence encoders. And so one of the big things that we did was train sentence encoders with mask language modeling. You see that on the left. But we also use multilingual distillation, which you see on the right. And so previous approaches to sentence encoders and the trend in the research community for a while was to really try to embed all languages in the same sentence encoder model. So projects like XLMR, for example, are in that direction. I think it's pretty widely used. The challenge with this when you're training a low resource model is that a lot of your high resource data just overwhelms your low resource data. And so you don't end up with a very high quality sentence encoder for those languages. So what we ended up doing is we had a multilingual teacher model and we distilled a bunch of student models that are specialized to different language families that are low resource. And so this enables the quality to be pretty high. And so the way that distillation works is that the teacher and the student model both see the same data and then we try to minimize the cosine loss between the sentence embeddings that they produce. I think an important question that you can ask here is why do you need to do multilingual distillation? Why can't you just train a bunch of different student models, like one per language family, like why even care about distillation? And the reason is because if you're going to use a bunch of sentence encoders for mining, the important thing is that they all exist in the same embedding space. Like if you train one separate model and another separate model, there's nothing constraining them so that you can mine all of the data against each other. And so one of the things we found is that by starting everything from the same teacher model and trying to use this cosine loss to minimize the distance between embeddings, you are able to have this constrained space where you can mine every language against every other, even if you have different student models. And so this graph on the Y axis, it shows the error rate of mining. And so lower is better. And on the X axis, it shows a bunch of different low resource languages. So for example, the first one is Urdu, the second one is Telugu, third one is Tagalog, and so on. And so the gray bar here is the original laser paper. So this is a paper we put out maybe in 2018-ish and we had all of these languages with count of them as included. But as you can see, the error rate is extremely, extremely high for these languages. So even though they were included, couldn't really be used for high quality. And the blue bar is the laser model that we trained based on the technique I just described in the previous slide. And you can see that I think the most important point is that you can barely see the blue bars. So it was very effective even for these previous languages that people had thought we had previously embedded. And then so now how does this kind of thing fit into a whole data pipeline around this approach? So one of the most important things is when you download the data from the web, you don't really know what language it's in. And so this is part of all of the large scale data cleaning that goes into training large language models today. And so the way we identify different languages is through like simple classification models called language identification models. And I think it's a classification model. And so people think it's easier than it actually is. But I think some of the major challenges are that there's so many different languages. They're written in many different ways and web text is very casual. And so it can be very difficult to actually train a good classification model that can generalize to them. And so what we did is, we had our LID training data and we produced a language identification model LID. And then we actually did human evaluation to label errors coming from the LID system to iteratively improve this on web text itself to improve the quality of this specific model. Then after we produce this LID model, then we insert like all of our common crawl where the web arrow is coming in and we do a ton of filtering and cleaning. And this produces a huge corpus of different monolingual data that you can then use for training anything. Afterwards, we train our encoder, what I described on the previous text, and then we convert this monolingual data into what we call mined by texts. So these are a huge data set of things that we think are translations of each other. And then finally, what we do is we actually try to validate that these are real mined by texts by training very small bilingual multilingual, sorry bilingual translation models in order to see what the quality is like. And I think this is important because the data development cycle and the end task that it's being used for, you don't want to completely separate it. An analogy to large language model training today is that when you're doing your pre-training, you don't want someone to just deliver you a data, like the data mix of your different data sets is very important. And it's pretty similar here. And I think one of the highlights that we did here is really focused on the human evaluation of the language identification model because that actually improves the quality of all of the underlying data if you just more accurately know what language it's in. And this entire data pipeline is actually open source in this library and we had an MNLP paper describing it. The reason why I thought this was important is that because I think data cleaning is actually such a fundamental underlying thing that drives model quality and people's data pipelines. It's like, I had this script and this other thing and so it's actually, I think very important to be able to recreate it and rerun it as part of almost like your research that you would do as follow-up work. And so that's why we open sourced it. A few reflection things. For low resource languages, even though we did a large scale mining, I think monolingual data is the limiting factor. Like there are many languages that do not have like a huge amount of text written online. And so it can be very challenging to get a large amount. Further, I think languages and unique scripts can be extremely hard to get good representations of if you don't have very much data. There are certain languages as well where they were historically written in a new script but now the government would like to write it in a totally new one like the old cheeky script, for example. And so there's not a lot of content to represent these scripts. So it's hard to learn representations. And then further, a lot of the content we create, it's even after mining, it's a fairly limited domain, often religious content. Okay, so with data discussed, I wanna segue a little bit into some of the modeling work just to kind of start with like a high level picture. I think there's like three major challenges when you talk about like large scale multi-lingual modeling. And these pretty much apply to language models as well. The first one is effective data augmentation for low resource languages. Like how can you prevent the low resource language data from just being completely drowned out by the time you've seen like all of your words of German or Russian? I think there's also a question of like scalability of the model. So even if you train very large scale models, how do you prevent the representations of different languages from interfering with each other? And that leads to the last point as well of like if you give the model very limited capacity, then of course it may not have the capacity to model all of these different languages. And so you also need to accelerate the scale of the model. And so preliminary for those who may not have seen a translation system before, I don't know how many of you that practically is. So we use standard sequence-to-sequence models. So the input text, the like coral thing is like what you wanna translate and there's a transformer decoder model that then with a tension mechanism goes to a transformer decoder model. And then it decodes autoregressively the actual translation, which you can see here in yellow. And so I wanna talk a little bit about like how the data looks as we feed it into the models. So there's a few different ways that you might wanna think about data. So you wanna be like, okay, did a human look at it and decide that like these two sentences are translations or are they noisy? Also, is it limited in size? Another thing you can think about is like is the data quality dependent on some other factor? And so that's like the model dependent thing in which case like the data quality may be capped by the quality of that dependency. And so I think you can think a little bit like the ideal data set. It would be like humans have reviewed every bit of it. It's not noisy at all. We have an infinite amount and it doesn't have any dependencies on any other models. It's just like pure quality. But in reality, like closer to what we have are these. So we have a bunch of different data sources. We have the seed data that I discussed like way back in the talk where it's a small amount of like really high quality human aligned data. But the only problem is that it's limited in size. It's like 6,000 sentences per language. We have the public by text. So this is data that people have created over many years of working in translation. You know, you can download it from like the opus corpus for example, mostly has not been reviewed by humans. So pretty extremely noisy. In many languages, it's just coming from the Bible. So the size is quite limited. You have our mind data. So this is not human aligned either. And but it does have a model dependency, you know, it's dependent on the quality of the sentence encoders. And we have two other sources of data from back translation. So the idea of back translation, it's a model augmentation technique heavily used in machine translation where you use a model to produce like pseudo translations like silver data. And we use two different techniques to produce these back translations that also are dependent on the underlying model used to make the translations. And so this is a picture of like our high level of different data sources and like how you wanna think about the quality and the different axes. And so if we put them all together, what do we get? So the Y axis here is the number of training pairs and the X axis here is the language is sorted by resource. So you can see like on the left hand side, you have your low resource languages like Wolof and on your right hand side, you've got your high resource languages like French. The peak is English, of course. And so if you just look at what's available publicly, this is a distribution you get. And you'll see like a huge, huge fall off pretty quickly. And then if you add in the data that we have created for mining and back translation, our goal is basically to like make the distribution a little bit more uniform. It's very hard on the extremely low resource side, of course, but to make it a little bit more uniform so that you don't just immediately, you know, overfit on your low resource languages before you've even seen like three shards of your German data. With that kind of data strategy in mind, I wanna talk a little bit about mixture of experts. So this is something that we explored quite aggressively in the translation space for a number of years. You know, we could have this equal conversation about some of the debates going on on like, do you want sparse or dense architectures for large language models? But essentially mixture of experts, it enables massive scale because you don't have to just scale like you're kind of your dense trunk model, but you can have like a bunch of different separate experts that you activate per token. It also allows you to avoid language interference because the idea is that the different experts, they could specialize to specific languages. Unfortunately, it adds a ton of capacity so it becomes pretty easy to overfit. So I wanna talk a little bit about this overfitting phenomenon. So the top set of graphs that we're gonna talk about is for the language Congo and then the bottom set of languages is French. So you really wanna compare like a low resource language on top with a high resource language on bottom. So if you just take your dense model, traditional transformer sequence to sequence architecture, that's the graph that you're showing, right? So there's a little bit of overfitting on the low resource language, but you can pretty much regularize this with standard dropout techniques, right? So there's not a big problem and on French, you basically have no real problem. However, the minute you switch from like a dense architecture to a token level MOE architecture, you just have experienced a massive overfitting on the low resource language. So the green line here is like just demonstrating without dropout the overfitting. And then if you add dropout, you get a little bit better performance, but it's still overfitting quite a bit. Like essentially by like 12K updates, there's no real point in continuing training, like you're burning GPU basically. And so one of the things we actually worked on quite a bit was like trying to figure out how to properly regularize these MOE architectures with this specific masking technique on the gating function that decides like which MOE to route, sorry, which expert to route to and your MOE architecture to just try to pull back some of this overfitting effect. So if you look in the top right graph, the purple line, you still see some successful regularization. Another thing that we did to control the overfitting effect that's actually quite being used in language models today as well is curriculum learning. And the idea of this is like, how are we going to stage when languages are introduced? And so what we did was we tried to train a vanilla model and then we started to measure when the languages begin to overfit. And then we basically bucket them into different sections. And so for high resource languages like French, you want to start it early and it needs to be trained the entire way. But for a lower resource language like Wolof, after maybe like a hundred K updates, it's done. So the rest of the time is just overfitting. And so it actually gets worse the more you train it. So what we did is we moved some of those lower resource languages and we inserted them much later into the training schedule. So you start training your high resource, then you start training your low, your mid resource, and then your low resource, and then your very low resource. And so by the end, everything in theory has trained and is not as overfit as it would be without this kind of technique. So I want to show some results. So first I want to show results on existing datasets. So before we get to 200 languages, like let's just talk about 100 languages. And so this is the Flores 101 DevTest. It's important to compare to this because this is where like existing benchmarks in the community lie. Whereas on 200, of course, we can put up anything. Because it's the first work on that. So the first column is translating out of English. So English to Chinese, English to Icelandic, anything like that. The second column is translating into English. So Chinese to English. The third column, XXYY, it's translating any cross pair are not involving English. And the last column is the average. So if you look at the first set of rows, this is a comparison on models that cover 87 different languages. So there was this paper MTAM 100. There was also this deep net paper. So you can see the average blue score. Blue is a standard translation metric, essentially a metric of word overlap. So we're looking at blue score here. And so you can see the last row NLB 200. Even though we cover 200 languages, the blue score is substantially above some of the existing work. Now, if we look at 101 languages, only the Delta LM paper from Microsoft at the time covered that number of languages. And so if you compare on all of the different cross sets, similarly, you see that there's no language left behind model is much stronger in terms of blue. One thing really quick on the variance of these blue numbers, I think it's important to understand is something statistically significant or not. I think about 0.5 blue is kind of like the general plus minus that you'll see. And so if it's above that, it's usually a statistically significant metric improvement. So now I want to talk a little bit about Flora's 200 results. So here's similar, like the first chunk of columns translating out of English, then next chunk is translating into English, then you have your cross pairs, and then you have your average. So we have this blue metric as well. We also have a character level metric based on CHRF++ that's commonly used in the translation community. So I think looking at these numbers, of course, there's no baseline work to compare to on the previous slide. And so when we get to human evaluation in a little bit, it'll be more concrete. But I think generally one of the rules of thumb I have for these types of numbers is around 30 is pretty reasonably becomes usable. And I think another thing, if you compare these supervised pairs to zero shot pairs, I think we don't see a huge drop-off on zero shot, which indicates the model has some sort of generalization, even if it didn't see that translation pair directly during training. Another way to calibrate some of this is to compare to Google Translate. And so if you compare to Google Translate, no language to left behind is quite a bit better at translating into English and not as good as translating out of English, although if you like average across everything, it's a little bit better. I want to talk a little bit about human evaluation as well to complement some of our discussion on automatic evaluation. And so I think automatic metrics fast, really good for research and duration, impossible to move forward without, but human evaluation is really the real deal here. And so we had this paper at Amptox on how to make this human evaluation very consistent and scalable across different language pairs. I think this goes back to the kind of evaluation data set point that I was making at the beginning of the talk, where if you're a professional German translator, you're really good at evaluating the quality of your German translation. But beyond that, there's not a lot of consistency. And if you evaluate translation on a five point scale, a five translating between two languages and a three translating between other two languages, are those really comparable? And so we had this entire experiment methodology on how we might want to make this a little bit more comparable. So I want to show some results now on this. So the y-axis here, so the metric is called XSTS, some metric for how we're doing this human evaluation. The y-axis here is actually the delta. So anything is a five point scale. So it's a delta, not the raw score. The x-axis here is a bunch of different translation directions that we evaluated. So the gray set is translating into English. The green set is translating non-English directions, so like French to Oluf. And then the blue set is translating out of English. And so what you're looking for is like a positive delta indicates that our modeling architecture is much better. So what the delta is between is like a baseline transformer model just trained on all of our data versus like the final no language left behind model that we created. So the data is actually the same for both of them. That's how we get all 200 languages. So we're just measuring here the human eval of the modeling improvements. As you can see, most of the delta is pretty noticeable. Some of them not so much like, I don't know, Zulu to English. We didn't seem to improve very much, but in general, it's an improvement detectable by human evaluation. You might also ask, OK, what is the statistically significant difference here between about 0.2 to 0.3 plus or minus is something that's pretty noticeable. And above 0.5, it's very noticeable. One of the things that I also want to get at in evaluation is that there's many different facets of model evaluation. And I think if you look at all of the different LLM leader boards or the transparency reports or whatever, you'll begin to internalize this pretty quickly. But what we just looked at are just very high level summary numbers. And they don't really tell you what exactly are the errors and is it ultimately usable by people? Is it a safe thing that people can rely on? And so one of the things we really focused on is user safety. And some of that manifests in some of the toxicity work that we did. And the driving thing here is that not all errors in translation are made equal. So during COVID, there was this one that was really went viral circulating around. But the message during COVID is you've got to wash your hands. But the translation producer is like, you've got to hold hands, which I think is exactly the opposite of what you want to do. And other types of measurement errors are really important as well. So if you're telling someone how far they want to go, and you're like, hey, you want to travel five kilometers, and then your translation is like travel 500 kilometers, it's a completely different type of issue. And so what we did for toxicity, which is a big focus for this work, is that we collected different toxicity lists for all 200 languages. And so why do I care so much about toxicity? I think it's a user safety thing. So if you input some perfectly benign text, and then the output is profanity, I think it's just really unexpected. And it breaks a lot of trust in the system. And it's an extremely poor experience for people. That being said, it's also a very, very challenging thing, because it's extremely culturally specific. So things that are slurs or insults in certain languages, they don't really generalize across cultures, which means that things like this are very challenging to create. And I also was very interested in this direction, because I think it's broadly useful for all sorts of different type of detection things that you need to do, and also mitigation. And so even though we develop this in the context of translation, it can be used very broadly in other types of NLP applications. This is also open source, you can download it. You have to type in a little password that's in the GitHub repo, just so that you don't accidentally download and realize you have files of curse words all over your computer. Okay, so I wanna end a little bit with some thoughts about future directions. And before I get there, there's like a 190 page paper that writes up all of this in far greater detail, in case you're curious. So a few future directions that I think I'm really interested in, and some of these are also very applicable to things like speech, is that I think one of them is more explicit multilingual. So I think a lot of approaches to multilingual have been like, hey, we have this thing that's working well for one language, like let's try to scale it to a bunch of different languages, and then we're gonna put them all in the same modeling bucket, and just kind of like hope that the model learns all of these different representations. But I think there's a lot of potential room for explicitly bringing in, like the fact that you know it's multilingual into the architecture more. And so, you know, it's possible to capture more nuances between languages or different relationships between languages. And the other one is continued support for everyone. I think it's like something reflecting on this project is that, you know, going from 100 to 200 was already pretty challenging, but going beyond a lot of the techniques that we developed here are not necessarily that scalable. This is actually what inspired some of our work on speech translation as well. So if you recently saw like the seamless M4T release or like the unwritten languages, like we did a lot of modeling of Hokeum, and I think that goes into this direction really well, because many of the languages that people want to use are like spoken first languages and not necessarily like primarily written. And then I think the last thing that I'm still really passionate about is like continued increase ease of use and training of these models and like democratization for the community. So one of the things that we tried to do in this work is just like really, really clearly write down everything that we did and like open source, like even the data pipeline and things like that. And so that's where you get like all of the repos that I linked and, you know, like a huge write up. But I think if someone were to try to reproduce this for their own language, and many people have, like I'm not saying that that hasn't been, but it's like, if you wanted to like do this, it would be extremely, extremely hard because there's just like so much different things going on. So I think most of the, what we've seen is like, people have downloaded the base model and fine-tuned it for their own language, but it's pretty hard to just like add on many, many more languages to this system because of how complicated all of the moving parts are. And so I feel like something for the translation community overall is like, how do we simplify a lot of these things? And I think that's where like a lot of fundamental modeling innovation could help us get to. And so yeah, I got a chance to give this talk, but of course the work is like being done by a huge team of people that I've cited here. And yeah, if you want to use any of this or read more about it, like everything is linked from this main GitHub repo here in Fairseek, and you can like click on everything else afterwards. But yeah, maybe I'll go back to Stephen if we have any questions or anything else like that. All right, now thanks for the great talk. Yeah, if anybody has any questions, feel free to unmute and ask. Thank you. Did you consult with a lot of like native speakers for like, you know, profanities and this type of stuff? Like how are you able to get access to the, you know, low quality languages or low resource languages and make sure that translations are correct? Yeah, yeah, that's a really good question. I mean, I think it's the most important to consult like a bunch of native speakers across the entire development process. So part of our original thing was just like interviewing a bunch of people to understand like what they're looking for in a high quality translation. And then we have like an entire professional translation team hired, which took quite a long time to find to consult with along the process. And then right now, like we also have some of the things like toxicity lists are open to the community. So if you make like a pull request, we try to like, you know, validate that that's like a useful addition and then like try to merge it in as well. We have a question in the room. Let's see if that comes over soon. Oh, go ahead. So I'll speed, see if we should get it. Yeah. So like, did you spend most of your time in the data pipeline state? Yeah. Yeah, good question. I think the question is, did you spend most of your time in the data pipeline state? It ended up being about like kind of like 50-50 like data or more like driving work. And then like 50-50 on the other side like modeling and evaluation work. Because once like the data is set, then there is a lot and a lot of iteration on the modeling side to figure out like, okay, which, how much of the data should we use? Like how should we portion the data? How do we prevent overfitting? What is the right architecture? But a lot of work goes into the data because I think if you don't have high quality data, you just can't get a good model. And for data mining, how do you mine the data? Do you use like Selenium or how do you mine the web? Yeah, so for the web, we start with Common Crawl. So we downloaded all of the different dumps of Common Crawl and then we use HTML parser. I think now like, if you download, for example, the red pajama dataset, like they've done a lot of this like parsing and stuff. And then we have like large scale pipelines that are set up like you can use Spark, for example, to process these things, to like split all of the different sentences out, run your language identification. You know, you can do different heuristic cleaning. There are certain languages where it's like very actually, very challenging to identify what is a sentence. Like I think in Thai, there is no like period. So you have to like use different models to identify what is a sentence and like parse some of those things out. And then we end up with, you know, our monolingual data dump. What is Common Crawl? Is it software that you use for datasets? Oh, yeah, yeah. Common Crawl is kind of like an open source version of the web that runs, I think maybe quarterly. I would have to check. But yeah, if you go to like Common Crawl.org, you can download it, but warning, it's like very large. I have a question. I mean, you might have mentioned this briefly, but I'm wondering how chatGBT and GPT-4 does on this. Like does just more scale and pre-training data help as well for low resource machine translation? Yeah, yeah, good question. Actually, there have been some studies done on like how, you know, these systems work. I think for high resource languages, it's actually quite beneficial to scale up. I think part of that is because the models have some innate generalization. And so one of the challenges that people talk about different things in different languages. So like seeing that knowledge in another language can actually help the generalization. But on low resource languages, it's, yeah, the performance is pretty difficult, especially on some of these translation benchmarks. I also think that language models, in terms of being trained for like a translation objective, tend to score worse on translation benchmarks because language models are like approximately capturing the same thing or as translation models. So you really try to align the meaning a little bit more. But yeah, so I think for low resource, it's still pretty challenging. But yeah, one thing that's interesting is for most English language models, they can actually do a reasonable job at producing other languages because it's impossible to get rid of other languages in your English specific data. So things like French or German will work reasonably. So just to clarify, you said language models trained with a translation objective do better, right? Because, right? They tend to do better. Like if you fine tune for the translation task, it will tend to do better. Well, that makes sense compared to like, for example, some few shot in context examples. Right, right, exactly, exactly. And one other question is, do you see this being similar to, for example, fine tuning on particular expert domains, which might also have less data and low resource, and as well as domain specific jargon and so forth? Yeah, I mean, I think if we were to restart this project now, I think that would be one of the first things we also explored, or at least like an extremely strong baseline where if you like take some of the data and you try to fine tune or try to do domain adaptation, I think that's also where like some of the like retrieval type approaches go in for translation, but also large language modeling work where you try to have like a separate domain that you can like retrieve some text in for adaptation. I think all of those approaches are pretty promising. Arcade, any other questions? One quick one on the point of the video question. You're looking at one of the slides, I think you showed some peak results with zero shot that were higher than just the base model. Do you think that's because there might still be some overfitting on those low resource languages? Yeah, good question. So for our large scale mining, we don't mind like every single possible cross pair. So like Icelandic will love, it's probably like not like the most in demand translation direction. And so we did not mind like all 200 times 200 because it's like really producing like a combinatorial explosion. And so that's where the zero shot results come from where you don't need, we don't have training data directionally in that pair, but the model has seen both the input and the output. And so I think those results are pretty good. Well, they're good for certain languages, which I think goes to show like the generalization capability and it's not like as critical to have like every single pair covered, but many of them are not as good. And so you see overall the performance is lower, even though on certain languages, it can perform better than you expect. But that's because it has seen the input and the output. It's not zero shot on like completely unseen language. I have a question. But I wanted you to also, you know, do something related to transcription or audio information? Yeah, good question. So in this project, no, not so much transcription, but we had a follow up work that we released actually just like a month or so ago called seamless M4T, which is like a joint model for both speech and text translation. And that's where we do leverage a lot of audio transcription because that also has like, it helps us bridge, you know, like the spoken data and the text data to leverage both of them together. Wait, just to clarify, the supervised fine tuning it worked better, right, compared to other methods. So actually in this work, it was as a couple of years ago now. So supervised fine tuning wasn't as common as it was now. But I think in the literature, if you want to use like a large language model to do translation, it's currently best yet if you do some supervised fine tuning. I'm just wondering about that because the way as humans, right, we don't just learn by looking at pairs of the same thing in different languages and kind of memorizing how to map from one to the other. We kind of learn in a more unsupervised way where if we know both languages, then we can kind of naturally translate between them. But I guess it makes sense for an LLMY having supervised examples would help, yeah. Yeah, I mean, I think as like the base foundation model continues to improve in quality, I think that's where the quality will probably improve when you don't need less and less fine tuning. I mean, do you think that's like the open AI approach? Like if you have the best foundation model, then you don't need as much like domain specific fine tuning. I think like, you know, like at the start when I started working on text generation, there was like translation researchers and like summarization researchers and like question answering researchers and they like work very differently. But now it's like, it's all driven by the same underlying thing and you're not like a specialized summarization researcher anymore. Right, I think that makes a lot of sense. Do we have any other questions? Any questions? Ron, any more in-person questions? Oh, don't think so. Okay, great. All right. Well, thank you, Angela, for the very interesting and a great talk again and for taking the time. And we hope, yeah, we hope that you can keep in touch and if anybody has any other questions, feel free to get in touch with Angela. All right, thanks so much for having me today. Bye, everyone.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " We're glad to have Angela Fan today with us here.", "tokens": [50364, 492, 434, 5404, 281, 362, 20848, 18564, 965, 365, 505, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21359564009166898, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0333583764731884}, {"id": 1, "seek": 0, "start": 10.0, "end": 15.96, "text": " And she's a research scientist at Meta AI Research in New York, focusing on research", "tokens": [50864, 400, 750, 311, 257, 2132, 12662, 412, 6377, 64, 7318, 10303, 294, 1873, 3609, 11, 8416, 322, 2132, 51162], "temperature": 0.0, "avg_logprob": -0.21359564009166898, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0333583764731884}, {"id": 2, "seek": 0, "start": 15.96, "end": 18.28, "text": " in text generation mainly.", "tokens": [51162, 294, 2487, 5125, 8704, 13, 51278], "temperature": 0.0, "avg_logprob": -0.21359564009166898, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0333583764731884}, {"id": 3, "seek": 0, "start": 18.28, "end": 24.88, "text": " And currently she's working on language modeling and developing the line AI agents, metaproducts,", "tokens": [51278, 400, 4362, 750, 311, 1364, 322, 2856, 15983, 293, 6416, 264, 1622, 7318, 12554, 11, 1131, 569, 2323, 349, 82, 11, 51608], "temperature": 0.0, "avg_logprob": -0.21359564009166898, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0333583764731884}, {"id": 4, "seek": 0, "start": 24.88, "end": 29.2, "text": " and recent research products include no language left behind, which she'll be talking briefly", "tokens": [51608, 293, 5162, 2132, 3383, 4090, 572, 2856, 1411, 2261, 11, 597, 750, 603, 312, 1417, 10515, 51824], "temperature": 0.0, "avg_logprob": -0.21359564009166898, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.0333583764731884}, {"id": 5, "seek": 2920, "start": 29.2, "end": 35.76, "text": " about today, universal speech translation for unwritten languages, as well as Lama2.", "tokens": [50364, 466, 965, 11, 11455, 6218, 12853, 337, 517, 26859, 8650, 11, 382, 731, 382, 441, 2404, 17, 13, 50692], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 6, "seek": 2920, "start": 35.76, "end": 38.28, "text": " So give it up for Angela, I guess.", "tokens": [50692, 407, 976, 309, 493, 337, 20848, 11, 286, 2041, 13, 50818], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 7, "seek": 2920, "start": 38.28, "end": 41.480000000000004, "text": " All right, thank you all so much.", "tokens": [50818, 1057, 558, 11, 1309, 291, 439, 370, 709, 13, 50978], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 8, "seek": 2920, "start": 41.480000000000004, "end": 44.879999999999995, "text": " So yeah, when I got this email, I was like, oh, I should probably talk about Lama2.", "tokens": [50978, 407, 1338, 11, 562, 286, 658, 341, 3796, 11, 286, 390, 411, 11, 1954, 11, 286, 820, 1391, 751, 466, 441, 2404, 17, 13, 51148], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 9, "seek": 2920, "start": 44.879999999999995, "end": 48.92, "text": " But then I noticed you have Sharon, who will like, you know, is like a 10x better speaker", "tokens": [51148, 583, 550, 286, 5694, 291, 362, 28573, 11, 567, 486, 411, 11, 291, 458, 11, 307, 411, 257, 1266, 87, 1101, 8145, 51350], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 10, "seek": 2920, "start": 48.92, "end": 49.92, "text": " than me.", "tokens": [51350, 813, 385, 13, 51400], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 11, "seek": 2920, "start": 49.92, "end": 52.0, "text": " So I was like, okay, like maybe not Lama2.", "tokens": [51400, 407, 286, 390, 411, 11, 1392, 11, 411, 1310, 406, 441, 2404, 17, 13, 51504], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 12, "seek": 2920, "start": 52.0, "end": 55.6, "text": " But then I thought I maybe would cover this project that we did called No Language Left", "tokens": [51504, 583, 550, 286, 1194, 286, 1310, 576, 2060, 341, 1716, 300, 321, 630, 1219, 883, 24445, 16405, 51684], "temperature": 0.0, "avg_logprob": -0.22131390041775173, "compression_ratio": 1.6738351254480286, "no_speech_prob": 0.023537300527095795}, {"id": 13, "seek": 5560, "start": 55.6, "end": 59.88, "text": " Behind, which could be very, also very relevant to this class.", "tokens": [50364, 20475, 11, 597, 727, 312, 588, 11, 611, 588, 7340, 281, 341, 1508, 13, 50578], "temperature": 0.0, "avg_logprob": -0.1817875756157769, "compression_ratio": 1.56, "no_speech_prob": 0.019078770652413368}, {"id": 14, "seek": 5560, "start": 59.88, "end": 67.88, "text": " And so when you think about a lot of text generation technology, most of it, until fairly recently,", "tokens": [50578, 400, 370, 562, 291, 519, 466, 257, 688, 295, 2487, 5125, 2899, 11, 881, 295, 309, 11, 1826, 6457, 3938, 11, 50978], "temperature": 0.0, "avg_logprob": -0.1817875756157769, "compression_ratio": 1.56, "no_speech_prob": 0.019078770652413368}, {"id": 15, "seek": 5560, "start": 67.88, "end": 70.36, "text": " has been really focused on English.", "tokens": [50978, 575, 668, 534, 5178, 322, 3669, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1817875756157769, "compression_ratio": 1.56, "no_speech_prob": 0.019078770652413368}, {"id": 16, "seek": 5560, "start": 70.36, "end": 74.44, "text": " But there are actually more than 3000 written languages worldwide.", "tokens": [51102, 583, 456, 366, 767, 544, 813, 20984, 3720, 8650, 13485, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1817875756157769, "compression_ratio": 1.56, "no_speech_prob": 0.019078770652413368}, {"id": 17, "seek": 5560, "start": 74.44, "end": 80.04, "text": " And for me, this is extremely personally meaningful because actually English is my third language.", "tokens": [51306, 400, 337, 385, 11, 341, 307, 4664, 5665, 10995, 570, 767, 3669, 307, 452, 2636, 2856, 13, 51586], "temperature": 0.0, "avg_logprob": -0.1817875756157769, "compression_ratio": 1.56, "no_speech_prob": 0.019078770652413368}, {"id": 18, "seek": 5560, "start": 80.04, "end": 82.04, "text": " So it's really important.", "tokens": [51586, 407, 309, 311, 534, 1021, 13, 51686], "temperature": 0.0, "avg_logprob": -0.1817875756157769, "compression_ratio": 1.56, "no_speech_prob": 0.019078770652413368}, {"id": 19, "seek": 8204, "start": 83.04, "end": 86.88000000000001, "text": " Yeah, so it's really also very personally meaningful.", "tokens": [50414, 865, 11, 370, 309, 311, 534, 611, 588, 5665, 10995, 13, 50606], "temperature": 0.0, "avg_logprob": -0.16997188152653156, "compression_ratio": 1.734375, "no_speech_prob": 0.0009106211364269257}, {"id": 20, "seek": 8204, "start": 86.88000000000001, "end": 91.60000000000001, "text": " And when you think about some of the multilingual technology that permeates, it's not like we've", "tokens": [50606, 400, 562, 291, 519, 466, 512, 295, 264, 2120, 38219, 2899, 300, 30287, 1024, 11, 309, 311, 406, 411, 321, 600, 50842], "temperature": 0.0, "avg_logprob": -0.16997188152653156, "compression_ratio": 1.734375, "no_speech_prob": 0.0009106211364269257}, {"id": 21, "seek": 8204, "start": 91.60000000000001, "end": 93.80000000000001, "text": " never worked on multilingual, right?", "tokens": [50842, 1128, 2732, 322, 2120, 38219, 11, 558, 30, 50952], "temperature": 0.0, "avg_logprob": -0.16997188152653156, "compression_ratio": 1.734375, "no_speech_prob": 0.0009106211364269257}, {"id": 22, "seek": 8204, "start": 93.80000000000001, "end": 97.88000000000001, "text": " Actually, when speaking about generative AI, I actually think translation is one of the", "tokens": [50952, 5135, 11, 562, 4124, 466, 1337, 1166, 7318, 11, 286, 767, 519, 12853, 307, 472, 295, 264, 51156], "temperature": 0.0, "avg_logprob": -0.16997188152653156, "compression_ratio": 1.734375, "no_speech_prob": 0.0009106211364269257}, {"id": 23, "seek": 8204, "start": 97.88000000000001, "end": 102.0, "text": " most commercially successful and widespread applications of generative AI.", "tokens": [51156, 881, 41751, 4406, 293, 22679, 5821, 295, 1337, 1166, 7318, 13, 51362], "temperature": 0.0, "avg_logprob": -0.16997188152653156, "compression_ratio": 1.734375, "no_speech_prob": 0.0009106211364269257}, {"id": 24, "seek": 8204, "start": 102.0, "end": 107.24000000000001, "text": " I mean, ultimately, translation models, they are, you know, like conditional language models.", "tokens": [51362, 286, 914, 11, 6284, 11, 12853, 5245, 11, 436, 366, 11, 291, 458, 11, 411, 27708, 2856, 5245, 13, 51624], "temperature": 0.0, "avg_logprob": -0.16997188152653156, "compression_ratio": 1.734375, "no_speech_prob": 0.0009106211364269257}, {"id": 25, "seek": 10724, "start": 107.24, "end": 112.11999999999999, "text": " And so when you think about like traveling or something like that, or my sister is taking", "tokens": [50364, 400, 370, 562, 291, 519, 466, 411, 9712, 420, 746, 411, 300, 11, 420, 452, 4892, 307, 1940, 50608], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 26, "seek": 10724, "start": 112.11999999999999, "end": 116.03999999999999, "text": " Spanish, so like just like doing her Spanish homework, we have a lot of tools that exist", "tokens": [50608, 8058, 11, 370, 411, 445, 411, 884, 720, 8058, 14578, 11, 321, 362, 257, 688, 295, 3873, 300, 2514, 50804], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 27, "seek": 10724, "start": 116.03999999999999, "end": 117.03999999999999, "text": " today.", "tokens": [50804, 965, 13, 50854], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 28, "seek": 10724, "start": 117.03999999999999, "end": 121.83999999999999, "text": " So things like Google Translate cover around 130 languages, Microsoft Translate about 110.", "tokens": [50854, 407, 721, 411, 3329, 6531, 17593, 2060, 926, 19966, 8650, 11, 8116, 6531, 17593, 466, 20154, 13, 51094], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 29, "seek": 10724, "start": 121.83999999999999, "end": 127.67999999999999, "text": " This might be a little bit outdated since I pulled the statistics a little bit ago.", "tokens": [51094, 639, 1062, 312, 257, 707, 857, 36313, 1670, 286, 7373, 264, 12523, 257, 707, 857, 2057, 13, 51386], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 30, "seek": 10724, "start": 127.67999999999999, "end": 131.48, "text": " But the project for No Language Left Behind, it started from like a very simple ask, like,", "tokens": [51386, 583, 264, 1716, 337, 883, 24445, 16405, 20475, 11, 309, 1409, 490, 411, 257, 588, 2199, 1029, 11, 411, 11, 51576], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 31, "seek": 10724, "start": 131.48, "end": 134.32, "text": " okay, there's 3000 languages worldwide.", "tokens": [51576, 1392, 11, 456, 311, 20984, 8650, 13485, 13, 51718], "temperature": 0.0, "avg_logprob": -0.16815213260487613, "compression_ratio": 1.6258278145695364, "no_speech_prob": 0.005725447554141283}, {"id": 32, "seek": 13432, "start": 134.4, "end": 140.16, "text": " Maybe it'll be like pretty hard to get to all 3000, since some of them are pretty rare", "tokens": [50368, 2704, 309, 603, 312, 411, 1238, 1152, 281, 483, 281, 439, 20984, 11, 1670, 512, 295, 552, 366, 1238, 5892, 50656], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 33, "seek": 13432, "start": 140.16, "end": 141.88, "text": " and not spoken by many.", "tokens": [50656, 293, 406, 10759, 538, 867, 13, 50742], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 34, "seek": 13432, "start": 141.88, "end": 147.0, "text": " But there are still like hundreds of languages spoken by millions and millions of people.", "tokens": [50742, 583, 456, 366, 920, 411, 6779, 295, 8650, 10759, 538, 6803, 293, 6803, 295, 561, 13, 50998], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 35, "seek": 13432, "start": 147.0, "end": 148.92, "text": " And so we were like, okay, no big deal.", "tokens": [50998, 400, 370, 321, 645, 411, 11, 1392, 11, 572, 955, 2028, 13, 51094], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 36, "seek": 13432, "start": 148.92, "end": 154.16, "text": " Like, let's just start from the 100-ish that we have today and just go for like a doubling.", "tokens": [51094, 1743, 11, 718, 311, 445, 722, 490, 264, 2319, 12, 742, 300, 321, 362, 965, 293, 445, 352, 337, 411, 257, 33651, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 37, "seek": 13432, "start": 154.16, "end": 158.32, "text": " Like, what would it take to actually be able to double this kind of coverage?", "tokens": [51356, 1743, 11, 437, 576, 309, 747, 281, 767, 312, 1075, 281, 3834, 341, 733, 295, 9645, 30, 51564], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 38, "seek": 13432, "start": 158.32, "end": 161.68, "text": " And of course, you know, just saying that you support a bunch of languages is not the", "tokens": [51564, 400, 295, 1164, 11, 291, 458, 11, 445, 1566, 300, 291, 1406, 257, 3840, 295, 8650, 307, 406, 264, 51732], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 39, "seek": 13432, "start": 161.68, "end": 162.68, "text": " goal.", "tokens": [51732, 3387, 13, 51782], "temperature": 0.0, "avg_logprob": -0.1520802060464271, "compression_ratio": 1.7016949152542373, "no_speech_prob": 0.0044634523801505566}, {"id": 40, "seek": 16268, "start": 162.68, "end": 167.32, "text": " You actually want to create high quality safe translations that would be usable by people", "tokens": [50364, 509, 767, 528, 281, 1884, 1090, 3125, 3273, 37578, 300, 576, 312, 29975, 538, 561, 50596], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 41, "seek": 16268, "start": 167.32, "end": 171.0, "text": " just like if you're going on vacation today, you're kind of instinctive to whip out your", "tokens": [50596, 445, 411, 498, 291, 434, 516, 322, 12830, 965, 11, 291, 434, 733, 295, 16556, 488, 281, 22377, 484, 428, 50780], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 42, "seek": 16268, "start": 171.0, "end": 174.56, "text": " phone and get on the Google Translate app.", "tokens": [50780, 2593, 293, 483, 322, 264, 3329, 6531, 17593, 724, 13, 50958], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 43, "seek": 16268, "start": 174.56, "end": 178.92000000000002, "text": " And so kind of the backdrop to this project was that there's actually a lot of progress", "tokens": [50958, 400, 370, 733, 295, 264, 32697, 281, 341, 1716, 390, 300, 456, 311, 767, 257, 688, 295, 4205, 51176], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 44, "seek": 16268, "start": 178.92000000000002, "end": 180.24, "text": " in translation.", "tokens": [51176, 294, 12853, 13, 51242], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 45, "seek": 16268, "start": 180.24, "end": 184.64000000000001, "text": " So historically, there's been a lot of focus on what we call higher resource languages.", "tokens": [51242, 407, 16180, 11, 456, 311, 668, 257, 688, 295, 1879, 322, 437, 321, 818, 2946, 7684, 8650, 13, 51462], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 46, "seek": 16268, "start": 184.64000000000001, "end": 188.60000000000002, "text": " And these are not necessarily languages that are spoken by the most people in the world.", "tokens": [51462, 400, 613, 366, 406, 4725, 8650, 300, 366, 10759, 538, 264, 881, 561, 294, 264, 1002, 13, 51660], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 47, "seek": 16268, "start": 188.60000000000002, "end": 192.20000000000002, "text": " But when we say higher resource, it means the most amount of data.", "tokens": [51660, 583, 562, 321, 584, 2946, 7684, 11, 309, 1355, 264, 881, 2372, 295, 1412, 13, 51840], "temperature": 0.0, "avg_logprob": -0.12893198497259795, "compression_ratio": 1.7725856697819315, "no_speech_prob": 0.002180994488298893}, {"id": 48, "seek": 19220, "start": 192.2, "end": 196.88, "text": " And so you can think about things like Europarl or, you know, translations from the European", "tokens": [50364, 400, 370, 291, 393, 519, 466, 721, 411, 12201, 6843, 420, 11, 291, 458, 11, 37578, 490, 264, 6473, 50598], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 49, "seek": 19220, "start": 196.88, "end": 197.88, "text": " Parliament.", "tokens": [50598, 15538, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 50, "seek": 19220, "start": 197.88, "end": 202.2, "text": " And those served as the foundation for a lot, a lot of translation development.", "tokens": [50648, 400, 729, 7584, 382, 264, 7030, 337, 257, 688, 11, 257, 688, 295, 12853, 3250, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 51, "seek": 19220, "start": 202.2, "end": 206.35999999999999, "text": " And more recently, there's been a great focus on low resource languages, and it's been driven", "tokens": [50864, 400, 544, 3938, 11, 456, 311, 668, 257, 869, 1879, 322, 2295, 7684, 8650, 11, 293, 309, 311, 668, 9555, 51072], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 52, "seek": 19220, "start": 206.35999999999999, "end": 212.32, "text": " across the research community with groups like Ghana NLP, Masekane, America's NLP.", "tokens": [51072, 2108, 264, 2132, 1768, 365, 3935, 411, 38779, 426, 45196, 11, 376, 651, 74, 1929, 11, 3374, 311, 426, 45196, 13, 51370], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 53, "seek": 19220, "start": 212.32, "end": 214.76, "text": " And these are all really exciting developments.", "tokens": [51370, 400, 613, 366, 439, 534, 4670, 20862, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 54, "seek": 19220, "start": 214.76, "end": 219.92, "text": " And so these have led to a lot of development of new data sets, as well as criticisms of", "tokens": [51492, 400, 370, 613, 362, 4684, 281, 257, 688, 295, 3250, 295, 777, 1412, 6352, 11, 382, 731, 382, 48519, 295, 51750], "temperature": 0.0, "avg_logprob": -0.1513735505401111, "compression_ratio": 1.7231833910034602, "no_speech_prob": 0.00263010966591537}, {"id": 55, "seek": 21992, "start": 219.92, "end": 224.88, "text": " existing data sets, and also work on new languages, and usually languages that people", "tokens": [50364, 6741, 1412, 6352, 11, 293, 611, 589, 322, 777, 8650, 11, 293, 2673, 8650, 300, 561, 50612], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 56, "seek": 21992, "start": 224.88, "end": 227.11999999999998, "text": " kind of speak and they care a lot about.", "tokens": [50612, 733, 295, 1710, 293, 436, 1127, 257, 688, 466, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 57, "seek": 21992, "start": 227.11999999999998, "end": 229.79999999999998, "text": " And we found this like really, really exciting.", "tokens": [50724, 400, 321, 1352, 341, 411, 534, 11, 534, 4670, 13, 50858], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 58, "seek": 21992, "start": 229.79999999999998, "end": 234.32, "text": " And so looking at a lot of this, a bunch of us got together at fair and started thinking", "tokens": [50858, 400, 370, 1237, 412, 257, 688, 295, 341, 11, 257, 3840, 295, 505, 658, 1214, 412, 3143, 293, 1409, 1953, 51084], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 59, "seek": 21992, "start": 234.32, "end": 239.07999999999998, "text": " like, okay, we actually speak some pretty low resource languages from like Catalan to", "tokens": [51084, 411, 11, 1392, 11, 321, 767, 1710, 512, 1238, 2295, 7684, 8650, 490, 411, 9565, 14163, 281, 51322], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 60, "seek": 21992, "start": 239.07999999999998, "end": 241.04, "text": " Ossamese and so on.", "tokens": [51322, 422, 3810, 335, 1130, 293, 370, 322, 13, 51420], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 61, "seek": 21992, "start": 241.04, "end": 244.64, "text": " And so we started this as kind of like a big, passionate research project.", "tokens": [51420, 400, 370, 321, 1409, 341, 382, 733, 295, 411, 257, 955, 11, 11410, 2132, 1716, 13, 51600], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 62, "seek": 21992, "start": 244.64, "end": 249.51999999999998, "text": " And so today, I want to cover a little bit about our high level approach to this problem,", "tokens": [51600, 400, 370, 965, 11, 286, 528, 281, 2060, 257, 707, 857, 466, 527, 1090, 1496, 3109, 281, 341, 1154, 11, 51844], "temperature": 0.0, "avg_logprob": -0.16452190257884838, "compression_ratio": 1.804054054054054, "no_speech_prob": 0.00394322769716382}, {"id": 63, "seek": 24952, "start": 249.52, "end": 251.64000000000001, "text": " which is a little bit interdisciplinary.", "tokens": [50364, 597, 307, 257, 707, 857, 38280, 13, 50470], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 64, "seek": 24952, "start": 251.64000000000001, "end": 256.04, "text": " I want to talk about how we actually created the data sets to be able to support this kind", "tokens": [50470, 286, 528, 281, 751, 466, 577, 321, 767, 2942, 264, 1412, 6352, 281, 312, 1075, 281, 1406, 341, 733, 50690], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 65, "seek": 24952, "start": 256.04, "end": 257.04, "text": " of work.", "tokens": [50690, 295, 589, 13, 50740], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 66, "seek": 24952, "start": 257.04, "end": 261.76, "text": " Of course, I want to talk about the models, since this is a class about transformers.", "tokens": [50740, 2720, 1164, 11, 286, 528, 281, 751, 466, 264, 5245, 11, 1670, 341, 307, 257, 1508, 466, 4088, 433, 13, 50976], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 67, "seek": 24952, "start": 261.76, "end": 265.04, "text": " One note here, I think that's actually very interesting in terms of translation as like", "tokens": [50976, 1485, 3637, 510, 11, 286, 519, 300, 311, 767, 588, 1880, 294, 2115, 295, 12853, 382, 411, 51140], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 68, "seek": 24952, "start": 265.04, "end": 270.68, "text": " a research direction, is that actually a lot of innovations have been done in translation.", "tokens": [51140, 257, 2132, 3513, 11, 307, 300, 767, 257, 688, 295, 24283, 362, 668, 1096, 294, 12853, 13, 51422], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 69, "seek": 24952, "start": 270.68, "end": 275.12, "text": " The original transformer paper, I think is one of them, and which makes always translation", "tokens": [51422, 440, 3380, 31782, 3035, 11, 286, 519, 307, 472, 295, 552, 11, 293, 597, 1669, 1009, 12853, 51644], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 70, "seek": 24952, "start": 275.12, "end": 277.56, "text": " a quite interesting area to work on.", "tokens": [51644, 257, 1596, 1880, 1859, 281, 589, 322, 13, 51766], "temperature": 0.0, "avg_logprob": -0.16421501220218718, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.0012836456298828125}, {"id": 71, "seek": 27756, "start": 277.6, "end": 280.88, "text": " Because I feel like it's a very mature research area as well.", "tokens": [50366, 1436, 286, 841, 411, 309, 311, 257, 588, 14442, 2132, 1859, 382, 731, 13, 50530], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 72, "seek": 27756, "start": 280.88, "end": 284.76, "text": " So it kind of is like, okay, if your architecture works in translation, it probably works very", "tokens": [50530, 407, 309, 733, 295, 307, 411, 11, 1392, 11, 498, 428, 9482, 1985, 294, 12853, 11, 309, 1391, 1985, 588, 50724], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 73, "seek": 27756, "start": 284.76, "end": 285.76, "text": " generally.", "tokens": [50724, 5101, 13, 50774], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 74, "seek": 27756, "start": 285.76, "end": 289.76, "text": " So that's also one of the things that excites me about translation research.", "tokens": [50774, 407, 300, 311, 611, 472, 295, 264, 721, 300, 1624, 3324, 385, 466, 12853, 2132, 13, 50974], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 75, "seek": 27756, "start": 289.76, "end": 293.2, "text": " Then I want to talk about evaluation, like how are we actually measuring and ensuring", "tokens": [50974, 1396, 286, 528, 281, 751, 466, 13344, 11, 411, 577, 366, 321, 767, 13389, 293, 16882, 51146], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 76, "seek": 27756, "start": 293.2, "end": 297.2, "text": " the quality of these translations are good and safe for people.", "tokens": [51146, 264, 3125, 295, 613, 37578, 366, 665, 293, 3273, 337, 561, 13, 51346], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 77, "seek": 27756, "start": 297.2, "end": 300.84000000000003, "text": " And then I want to end with a little bit of like, you know, high level thoughts about", "tokens": [51346, 400, 550, 286, 528, 281, 917, 365, 257, 707, 857, 295, 411, 11, 291, 458, 11, 1090, 1496, 4598, 466, 51528], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 78, "seek": 27756, "start": 300.84000000000003, "end": 305.56, "text": " future directions and things that I hope that we can work on in the future.", "tokens": [51528, 2027, 11095, 293, 721, 300, 286, 1454, 300, 321, 393, 589, 322, 294, 264, 2027, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11146124085383628, "compression_ratio": 1.8110749185667752, "no_speech_prob": 0.0008826919365674257}, {"id": 79, "seek": 30556, "start": 305.56, "end": 308.12, "text": " So I want to start with our approach.", "tokens": [50364, 407, 286, 528, 281, 722, 365, 527, 3109, 13, 50492], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 80, "seek": 30556, "start": 308.12, "end": 312.36, "text": " I think the most important thing in research is to know that we're working on a real problem,", "tokens": [50492, 286, 519, 264, 881, 1021, 551, 294, 2132, 307, 281, 458, 300, 321, 434, 1364, 322, 257, 957, 1154, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 81, "seek": 30556, "start": 312.36, "end": 316.6, "text": " especially when it's really close to people like translation.", "tokens": [50704, 2318, 562, 309, 311, 534, 1998, 281, 561, 411, 12853, 13, 50916], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 82, "seek": 30556, "start": 316.6, "end": 320.76, "text": " And I think in many areas, like when I was working on on-device AI, for example, I feel", "tokens": [50916, 400, 286, 519, 294, 867, 3179, 11, 411, 562, 286, 390, 1364, 322, 322, 12, 40343, 573, 7318, 11, 337, 1365, 11, 286, 841, 51124], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 83, "seek": 30556, "start": 320.76, "end": 325.28, "text": " like I had like a research problem in mind, but it was like very, very disconnected from", "tokens": [51124, 411, 286, 632, 411, 257, 2132, 1154, 294, 1575, 11, 457, 309, 390, 411, 588, 11, 588, 29426, 490, 51350], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 84, "seek": 30556, "start": 325.28, "end": 328.32, "text": " the practical problem of actually putting models on phones.", "tokens": [51350, 264, 8496, 1154, 295, 767, 3372, 5245, 322, 10216, 13, 51502], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 85, "seek": 30556, "start": 328.32, "end": 330.8, "text": " And so this was something that was really important to us.", "tokens": [51502, 400, 370, 341, 390, 746, 300, 390, 534, 1021, 281, 505, 13, 51626], "temperature": 0.0, "avg_logprob": -0.11669737445421455, "compression_ratio": 1.7653429602888087, "no_speech_prob": 0.0023957518860697746}, {"id": 86, "seek": 33080, "start": 330.8, "end": 335.48, "text": " And so we actually started the project by kind of like focusing on a social sciences", "tokens": [50364, 400, 370, 321, 767, 1409, 264, 1716, 538, 733, 295, 411, 8416, 322, 257, 2093, 17677, 50598], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 87, "seek": 33080, "start": 335.48, "end": 339.2, "text": " type approach or, you know, sociology type approach.", "tokens": [50598, 2010, 3109, 420, 11, 291, 458, 11, 41744, 2010, 3109, 13, 50784], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 88, "seek": 33080, "start": 339.2, "end": 342.84000000000003, "text": " And we actually did a lot of interviews with low resource speakers.", "tokens": [50784, 400, 321, 767, 630, 257, 688, 295, 12318, 365, 2295, 7684, 9518, 13, 50966], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 89, "seek": 33080, "start": 342.84000000000003, "end": 348.28000000000003, "text": " And so we met with about 44 different native speakers that spoke 36 different languages", "tokens": [50966, 400, 370, 321, 1131, 365, 466, 16408, 819, 8470, 9518, 300, 7179, 8652, 819, 8650, 51238], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 90, "seek": 33080, "start": 348.28000000000003, "end": 350.36, "text": " across North America.", "tokens": [51238, 2108, 4067, 3374, 13, 51342], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 91, "seek": 33080, "start": 350.36, "end": 354.32, "text": " I will say that a lot of them are like immigrants to the US, since that was kind of like the", "tokens": [51342, 286, 486, 584, 300, 257, 688, 295, 552, 366, 411, 16598, 281, 264, 2546, 11, 1670, 300, 390, 733, 295, 411, 264, 51540], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 92, "seek": 33080, "start": 354.32, "end": 357.36, "text": " easiest kind of cohort to recruit.", "tokens": [51540, 12889, 733, 295, 28902, 281, 15119, 13, 51692], "temperature": 0.0, "avg_logprob": -0.10956303889934833, "compression_ratio": 1.7104247104247103, "no_speech_prob": 0.014932315796613693}, {"id": 93, "seek": 35736, "start": 357.36, "end": 361.96000000000004, "text": " And we learned a lot of different things about how they approach low resource languages,", "tokens": [50364, 400, 321, 3264, 257, 688, 295, 819, 721, 466, 577, 436, 3109, 2295, 7684, 8650, 11, 50594], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 94, "seek": 35736, "start": 361.96000000000004, "end": 364.24, "text": " but also the kind of technological need that they have.", "tokens": [50594, 457, 611, 264, 733, 295, 18439, 643, 300, 436, 362, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 95, "seek": 35736, "start": 364.24, "end": 368.36, "text": " Because I think it's easy to be like, hey, I have this cool background, like I have this", "tokens": [50708, 1436, 286, 519, 309, 311, 1858, 281, 312, 411, 11, 4177, 11, 286, 362, 341, 1627, 3678, 11, 411, 286, 362, 341, 50914], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 96, "seek": 35736, "start": 368.36, "end": 371.68, "text": " cool problem, and I want to solve it, but I think it's very important to actually like", "tokens": [50914, 1627, 1154, 11, 293, 286, 528, 281, 5039, 309, 11, 457, 286, 519, 309, 311, 588, 1021, 281, 767, 411, 51080], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 97, "seek": 35736, "start": 371.68, "end": 375.28000000000003, "text": " talk to the people if this is a problem that needs to be solved.", "tokens": [51080, 751, 281, 264, 561, 498, 341, 307, 257, 1154, 300, 2203, 281, 312, 13041, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 98, "seek": 35736, "start": 375.28000000000003, "end": 379.48, "text": " And so we learned that there's great fear in general that low resource languages might", "tokens": [51260, 400, 370, 321, 3264, 300, 456, 311, 869, 4240, 294, 2674, 300, 2295, 7684, 8650, 1062, 51470], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 99, "seek": 35736, "start": 379.48, "end": 385.96000000000004, "text": " be undergoing a state of decline, partially because a lot of education is shifting to", "tokens": [51470, 312, 40033, 257, 1785, 295, 15635, 11, 18886, 570, 257, 688, 295, 3309, 307, 17573, 281, 51794], "temperature": 0.0, "avg_logprob": -0.1326155084552187, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.001169083989225328}, {"id": 100, "seek": 38596, "start": 385.96, "end": 390.23999999999995, "text": " languages like Hindi or like English or Mandarin Chinese, for example.", "tokens": [50364, 8650, 411, 36225, 420, 411, 3669, 420, 42292, 4649, 11, 337, 1365, 13, 50578], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 101, "seek": 38596, "start": 390.23999999999995, "end": 394.64, "text": " And there's a lot of excitement to be included in existing translation systems.", "tokens": [50578, 400, 456, 311, 257, 688, 295, 14755, 281, 312, 5556, 294, 6741, 12853, 3652, 13, 50798], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 102, "seek": 38596, "start": 394.64, "end": 399.56, "text": " And people said they have always tried to use Google translator, Microsoft translate", "tokens": [50798, 400, 561, 848, 436, 362, 1009, 3031, 281, 764, 3329, 35223, 11, 8116, 13799, 51044], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 103, "seek": 38596, "start": 399.56, "end": 401.24, "text": " in their existing languages.", "tokens": [51044, 294, 641, 6741, 8650, 13, 51128], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 104, "seek": 38596, "start": 401.24, "end": 405.4, "text": " But ultimately they found that the quality is really insufficient for reliable usage.", "tokens": [51128, 583, 6284, 436, 1352, 300, 264, 3125, 307, 534, 41709, 337, 12924, 14924, 13, 51336], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 105, "seek": 38596, "start": 405.4, "end": 409.08, "text": " So if you think about like, well, I was going to say when I was in high school, but you're", "tokens": [51336, 407, 498, 291, 519, 466, 411, 11, 731, 11, 286, 390, 516, 281, 584, 562, 286, 390, 294, 1090, 1395, 11, 457, 291, 434, 51520], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 106, "seek": 38596, "start": 409.08, "end": 411.08, "text": " all probably like substantially younger than me.", "tokens": [51520, 439, 1391, 411, 30797, 7037, 813, 385, 13, 51620], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 107, "seek": 38596, "start": 411.08, "end": 414.67999999999995, "text": " So maybe like, you know, 10, so years ago, you know, and you tried to use Google translate", "tokens": [51620, 407, 1310, 411, 11, 291, 458, 11, 1266, 11, 370, 924, 2057, 11, 291, 458, 11, 293, 291, 3031, 281, 764, 3329, 13799, 51800], "temperature": 0.0, "avg_logprob": -0.18417761219081594, "compression_ratio": 1.7343283582089553, "no_speech_prob": 0.2799646854400635}, {"id": 108, "seek": 41468, "start": 414.68, "end": 418.64, "text": " for your Spanish homework, like your Spanish teacher could always identify that like, you", "tokens": [50364, 337, 428, 8058, 14578, 11, 411, 428, 8058, 5027, 727, 1009, 5876, 300, 411, 11, 291, 50562], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 109, "seek": 41468, "start": 418.64, "end": 421.96, "text": " know, it was not a human written translation until you would get marks off.", "tokens": [50562, 458, 11, 309, 390, 406, 257, 1952, 3720, 12853, 1826, 291, 576, 483, 10640, 766, 13, 50728], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 110, "seek": 41468, "start": 421.96, "end": 426.04, "text": " But that's not really the case for some of the high resource languages today.", "tokens": [50728, 583, 300, 311, 406, 534, 264, 1389, 337, 512, 295, 264, 1090, 7684, 8650, 965, 13, 50932], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 111, "seek": 41468, "start": 426.04, "end": 430.52, "text": " And so I think as with all things in machine learning, it really starts from a data perspective.", "tokens": [50932, 400, 370, 286, 519, 382, 365, 439, 721, 294, 3479, 2539, 11, 309, 534, 3719, 490, 257, 1412, 4585, 13, 51156], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 112, "seek": 41468, "start": 430.52, "end": 434.56, "text": " Like why can't we just train models in hundreds of languages or large language models in hundreds", "tokens": [51156, 1743, 983, 393, 380, 321, 445, 3847, 5245, 294, 6779, 295, 8650, 420, 2416, 2856, 5245, 294, 6779, 51358], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 113, "seek": 41468, "start": 434.56, "end": 435.56, "text": " of languages?", "tokens": [51358, 295, 8650, 30, 51408], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 114, "seek": 41468, "start": 435.56, "end": 437.84000000000003, "text": " It's because we don't have the data to support it.", "tokens": [51408, 467, 311, 570, 321, 500, 380, 362, 264, 1412, 281, 1406, 309, 13, 51522], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 115, "seek": 41468, "start": 437.84000000000003, "end": 442.48, "text": " And so I want to talk first about evaluation data sets because I think it's extremely important", "tokens": [51522, 400, 370, 286, 528, 281, 751, 700, 466, 13344, 1412, 6352, 570, 286, 519, 309, 311, 4664, 1021, 51754], "temperature": 0.0, "avg_logprob": -0.13016735298046167, "compression_ratio": 1.8151515151515152, "no_speech_prob": 0.0057268813252449036}, {"id": 116, "seek": 44248, "start": 442.48, "end": 444.84000000000003, "text": " to nail evaluation.", "tokens": [50364, 281, 10173, 13344, 13, 50482], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 117, "seek": 44248, "start": 444.84000000000003, "end": 447.16, "text": " And then I'll talk about training.", "tokens": [50482, 400, 550, 286, 603, 751, 466, 3097, 13, 50598], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 118, "seek": 44248, "start": 447.16, "end": 452.8, "text": " So for an evaluation data set for this work, we started this Flora's effort, it stands", "tokens": [50598, 407, 337, 364, 13344, 1412, 992, 337, 341, 589, 11, 321, 1409, 341, 3235, 3252, 311, 4630, 11, 309, 7382, 50880], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 119, "seek": 44248, "start": 452.8, "end": 456.88, "text": " for Facebook low resource, I guess we're called meta now, but I didn't think more as was like", "tokens": [50880, 337, 4384, 2295, 7684, 11, 286, 2041, 321, 434, 1219, 19616, 586, 11, 457, 286, 994, 380, 519, 544, 382, 390, 411, 51084], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 120, "seek": 44248, "start": 456.88, "end": 461.04, "text": " a very good, a renaming so we're still calling it Flora's.", "tokens": [51084, 257, 588, 665, 11, 257, 8124, 5184, 370, 321, 434, 920, 5141, 309, 3235, 3252, 311, 13, 51292], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 121, "seek": 44248, "start": 461.04, "end": 465.48, "text": " So this was something we originally started for just two languages in this first paper", "tokens": [51292, 407, 341, 390, 746, 321, 7993, 1409, 337, 445, 732, 8650, 294, 341, 700, 3035, 51514], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 122, "seek": 44248, "start": 465.48, "end": 468.32, "text": " at EMLP, many years ago.", "tokens": [51514, 412, 462, 12683, 47, 11, 867, 924, 2057, 13, 51656], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 123, "seek": 44248, "start": 468.32, "end": 470.56, "text": " So it was just for Napoleon Sinhalo.", "tokens": [51656, 407, 309, 390, 445, 337, 31694, 11187, 4947, 78, 13, 51768], "temperature": 0.0, "avg_logprob": -0.22517809053746665, "compression_ratio": 1.5878136200716846, "no_speech_prob": 0.0028876778669655323}, {"id": 124, "seek": 47056, "start": 470.56, "end": 475.28000000000003, "text": " And we later extended it to incorporate two more languages in a release afterwards.", "tokens": [50364, 400, 321, 1780, 10913, 309, 281, 16091, 732, 544, 8650, 294, 257, 4374, 10543, 13, 50600], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 125, "seek": 47056, "start": 475.28000000000003, "end": 480.44, "text": " You know, we thought a lot about, okay, like Flora's was really useful for the community.", "tokens": [50600, 509, 458, 11, 321, 1194, 257, 688, 466, 11, 1392, 11, 411, 3235, 3252, 311, 390, 534, 4420, 337, 264, 1768, 13, 50858], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 126, "seek": 47056, "start": 480.44, "end": 482.28000000000003, "text": " How can we extend it to 100 languages?", "tokens": [50858, 1012, 393, 321, 10101, 309, 281, 2319, 8650, 30, 50950], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 127, "seek": 47056, "start": 482.28000000000003, "end": 488.16, "text": " And so that was this follow up work that we did, I think we had at ACL or WMT.", "tokens": [50950, 400, 370, 300, 390, 341, 1524, 493, 589, 300, 321, 630, 11, 286, 519, 321, 632, 412, 43873, 420, 343, 44, 51, 13, 51244], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 128, "seek": 47056, "start": 488.16, "end": 493.48, "text": " And then in this project, we were like, okay, how can we go from Flora's 101 to Flora's", "tokens": [51244, 400, 550, 294, 341, 1716, 11, 321, 645, 411, 11, 1392, 11, 577, 393, 321, 352, 490, 3235, 3252, 311, 21055, 281, 3235, 3252, 311, 51510], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 129, "seek": 47056, "start": 493.48, "end": 496.64, "text": " 200 to really go for the doubling effect?", "tokens": [51510, 2331, 281, 534, 352, 337, 264, 33651, 1802, 30, 51668], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 130, "seek": 47056, "start": 496.64, "end": 497.64, "text": " And so what is Flora's?", "tokens": [51668, 400, 370, 437, 307, 3235, 3252, 311, 30, 51718], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 131, "seek": 47056, "start": 497.64, "end": 499.08, "text": " Well, it's in the name.", "tokens": [51718, 1042, 11, 309, 311, 294, 264, 1315, 13, 51790], "temperature": 0.0, "avg_logprob": -0.17209586261832802, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.006894522346556187}, {"id": 132, "seek": 49908, "start": 499.08, "end": 501.47999999999996, "text": " It's a focus on low resource languages.", "tokens": [50364, 467, 311, 257, 1879, 322, 2295, 7684, 8650, 13, 50484], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 133, "seek": 49908, "start": 501.47999999999996, "end": 506.71999999999997, "text": " So we do include some higher resource languages like German or Hindi or so on, almost for", "tokens": [50484, 407, 321, 360, 4090, 512, 2946, 7684, 8650, 411, 6521, 420, 36225, 420, 370, 322, 11, 1920, 337, 50746], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 134, "seek": 49908, "start": 506.71999999999997, "end": 509.03999999999996, "text": " calibration effect as well.", "tokens": [50746, 38732, 1802, 382, 731, 13, 50862], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 135, "seek": 49908, "start": 509.03999999999996, "end": 513.48, "text": " But the majority of the focus is on these lower and mid resource languages.", "tokens": [50862, 583, 264, 6286, 295, 264, 1879, 307, 322, 613, 3126, 293, 2062, 7684, 8650, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 136, "seek": 49908, "start": 513.48, "end": 519.1999999999999, "text": " It's the first large scale, many to many machine translation evaluation data set, which means", "tokens": [51084, 467, 311, 264, 700, 2416, 4373, 11, 867, 281, 867, 3479, 12853, 13344, 1412, 992, 11, 597, 1355, 51370], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 137, "seek": 49908, "start": 519.1999999999999, "end": 523.88, "text": " that we take all of the sentences in English and then we translate them to all of the languages,", "tokens": [51370, 300, 321, 747, 439, 295, 264, 16579, 294, 3669, 293, 550, 321, 13799, 552, 281, 439, 295, 264, 8650, 11, 51604], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 138, "seek": 49908, "start": 523.88, "end": 528.4399999999999, "text": " which means that you would be able to evaluate any cross pair of languages.", "tokens": [51604, 597, 1355, 300, 291, 576, 312, 1075, 281, 13059, 604, 3278, 6119, 295, 8650, 13, 51832], "temperature": 0.0, "avg_logprob": -0.09901060729191222, "compression_ratio": 1.845018450184502, "no_speech_prob": 0.01516579370945692}, {"id": 139, "seek": 52844, "start": 528.44, "end": 532.1600000000001, "text": " So for example, like Chinese to French, I lived in France for many years.", "tokens": [50364, 407, 337, 1365, 11, 411, 4649, 281, 5522, 11, 286, 5152, 294, 6190, 337, 867, 924, 13, 50550], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 140, "seek": 52844, "start": 532.1600000000001, "end": 534.08, "text": " So it's like very personally relevant to me.", "tokens": [50550, 407, 309, 311, 411, 588, 5665, 7340, 281, 385, 13, 50646], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 141, "seek": 52844, "start": 534.08, "end": 538.7600000000001, "text": " Of course, 200 languages also in the name, there's a broad diversity of different domains", "tokens": [50646, 2720, 1164, 11, 2331, 8650, 611, 294, 264, 1315, 11, 456, 311, 257, 4152, 8811, 295, 819, 25514, 50880], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 142, "seek": 52844, "start": 538.7600000000001, "end": 539.7600000000001, "text": " and topics.", "tokens": [50880, 293, 8378, 13, 50930], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 143, "seek": 52844, "start": 539.7600000000001, "end": 544.2, "text": " I think this is important when designing an evaluation data set, which is like very top", "tokens": [50930, 286, 519, 341, 307, 1021, 562, 14685, 364, 13344, 1412, 992, 11, 597, 307, 411, 588, 1192, 51152], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 144, "seek": 52844, "start": 544.2, "end": 550.12, "text": " of mind for anybody interested in language modeling research, because like the way people", "tokens": [51152, 295, 1575, 337, 4472, 3102, 294, 2856, 15983, 2132, 11, 570, 411, 264, 636, 561, 51448], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 145, "seek": 52844, "start": 550.12, "end": 555.32, "text": " train machine translation models and the way people use them are often like very different.", "tokens": [51448, 3847, 3479, 12853, 5245, 293, 264, 636, 561, 764, 552, 366, 2049, 411, 588, 819, 13, 51708], "temperature": 0.0, "avg_logprob": -0.12206387519836426, "compression_ratio": 1.678082191780822, "no_speech_prob": 0.0018662266666069627}, {"id": 146, "seek": 55532, "start": 555.32, "end": 559.24, "text": " And so if you only benchmark your data set, for example, on news, which is very common", "tokens": [50364, 400, 370, 498, 291, 787, 18927, 428, 1412, 992, 11, 337, 1365, 11, 322, 2583, 11, 597, 307, 588, 2689, 50560], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 147, "seek": 55532, "start": 559.24, "end": 563.72, "text": " in translation research, then you don't really pick up the fact that people talk about such", "tokens": [50560, 294, 12853, 2132, 11, 550, 291, 500, 380, 534, 1888, 493, 264, 1186, 300, 561, 751, 466, 1270, 50784], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 148, "seek": 55532, "start": 563.72, "end": 568.32, "text": " a wide variety of things and have like different casual conversations that they need translated", "tokens": [50784, 257, 4874, 5673, 295, 721, 293, 362, 411, 819, 13052, 7315, 300, 436, 643, 16805, 51014], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 149, "seek": 55532, "start": 568.32, "end": 571.0400000000001, "text": " official documents and so on.", "tokens": [51014, 4783, 8512, 293, 370, 322, 13, 51150], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 150, "seek": 55532, "start": 571.0400000000001, "end": 573.2, "text": " It's also document level data set.", "tokens": [51150, 467, 311, 611, 4166, 1496, 1412, 992, 13, 51258], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 151, "seek": 55532, "start": 573.2, "end": 576.84, "text": " This is not something that I think the community is like broadly leveraging right now.", "tokens": [51258, 639, 307, 406, 746, 300, 286, 519, 264, 1768, 307, 411, 19511, 32666, 558, 586, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 152, "seek": 55532, "start": 576.84, "end": 580.8000000000001, "text": " But the way it's translated is that you can have document level context.", "tokens": [51440, 583, 264, 636, 309, 311, 16805, 307, 300, 291, 393, 362, 4166, 1496, 4319, 13, 51638], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 153, "seek": 55532, "start": 580.8000000000001, "end": 584.44, "text": " And so translators are provided the entire document to translate from.", "tokens": [51638, 400, 370, 5105, 3391, 366, 5649, 264, 2302, 4166, 281, 13799, 490, 13, 51820], "temperature": 0.0, "avg_logprob": -0.11223156507625136, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.00044414898729883134}, {"id": 154, "seek": 58444, "start": 584.44, "end": 589.48, "text": " We also provide the entire document for evaluation and we translate like multiple sentences from", "tokens": [50364, 492, 611, 2893, 264, 2302, 4166, 337, 13344, 293, 321, 13799, 411, 3866, 16579, 490, 50616], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 155, "seek": 58444, "start": 589.48, "end": 590.7600000000001, "text": " the same paragraph.", "tokens": [50616, 264, 912, 18865, 13, 50680], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 156, "seek": 58444, "start": 590.7600000000001, "end": 594.8800000000001, "text": " And so this was like a potential to research direction that we wanted to make sure we covered", "tokens": [50680, 400, 370, 341, 390, 411, 257, 3995, 281, 2132, 3513, 300, 321, 1415, 281, 652, 988, 321, 5343, 50886], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 157, "seek": 58444, "start": 594.8800000000001, "end": 598.8800000000001, "text": " models that needed like potentially more context because a lot of translation work is done", "tokens": [50886, 5245, 300, 2978, 411, 7263, 544, 4319, 570, 257, 688, 295, 12853, 589, 307, 1096, 51086], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 158, "seek": 58444, "start": 598.8800000000001, "end": 600.96, "text": " at the sentence level.", "tokens": [51086, 412, 264, 8174, 1496, 13, 51190], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 159, "seek": 58444, "start": 600.96, "end": 604.0, "text": " So how do we actually ensure that this data set was high quality?", "tokens": [51190, 407, 577, 360, 321, 767, 5586, 300, 341, 1412, 992, 390, 1090, 3125, 30, 51342], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 160, "seek": 58444, "start": 604.0, "end": 606.72, "text": " So the first step is that we take a document.", "tokens": [51342, 407, 264, 700, 1823, 307, 300, 321, 747, 257, 4166, 13, 51478], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 161, "seek": 58444, "start": 606.72, "end": 610.32, "text": " Well, actually, first step is like alignment on language standards.", "tokens": [51478, 1042, 11, 767, 11, 700, 1823, 307, 411, 18515, 322, 2856, 7787, 13, 51658], "temperature": 0.0, "avg_logprob": -0.15002853258521157, "compression_ratio": 1.7622377622377623, "no_speech_prob": 0.00048769087879918516}, {"id": 162, "seek": 61032, "start": 610.32, "end": 615.2800000000001, "text": " So this is very important because when you're translating French or Chinese, I think most", "tokens": [50364, 407, 341, 307, 588, 1021, 570, 562, 291, 434, 35030, 5522, 420, 4649, 11, 286, 519, 881, 50612], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 163, "seek": 61032, "start": 615.2800000000001, "end": 619.36, "text": " people have a strong understanding of like what it means to produce like a good French", "tokens": [50612, 561, 362, 257, 2068, 3701, 295, 411, 437, 309, 1355, 281, 5258, 411, 257, 665, 5522, 50816], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 164, "seek": 61032, "start": 619.36, "end": 620.84, "text": " or good Chinese.", "tokens": [50816, 420, 665, 4649, 13, 50890], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 165, "seek": 61032, "start": 620.84, "end": 624.2, "text": " And there are a lot of professional translators hired in these languages.", "tokens": [50890, 400, 456, 366, 257, 688, 295, 4843, 5105, 3391, 13144, 294, 613, 8650, 13, 51058], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 166, "seek": 61032, "start": 624.2, "end": 628.32, "text": " But when you go to lower resource languages, it's not necessarily the case that there's", "tokens": [51058, 583, 562, 291, 352, 281, 3126, 7684, 8650, 11, 309, 311, 406, 4725, 264, 1389, 300, 456, 311, 51264], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 167, "seek": 61032, "start": 628.32, "end": 635.44, "text": " like a glowing translation industry around translating a lower resource language.", "tokens": [51264, 411, 257, 27064, 12853, 3518, 926, 35030, 257, 3126, 7684, 2856, 13, 51620], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 168, "seek": 61032, "start": 635.44, "end": 640.1600000000001, "text": " And so one of the first things is actually to align on like what is a high quality translation?", "tokens": [51620, 400, 370, 472, 295, 264, 700, 721, 307, 767, 281, 7975, 322, 411, 437, 307, 257, 1090, 3125, 12853, 30, 51856], "temperature": 0.0, "avg_logprob": -0.09358839703421308, "compression_ratio": 1.8379310344827586, "no_speech_prob": 0.0010644730646163225}, {"id": 169, "seek": 64016, "start": 640.16, "end": 642.6, "text": " And so there's actually a lot of challenges here.", "tokens": [50364, 400, 370, 456, 311, 767, 257, 688, 295, 4759, 510, 13, 50486], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 170, "seek": 64016, "start": 642.6, "end": 645.64, "text": " So there are certain low resource languages where there's different competing language", "tokens": [50486, 407, 456, 366, 1629, 2295, 7684, 8650, 689, 456, 311, 819, 15439, 2856, 50638], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 171, "seek": 64016, "start": 645.64, "end": 650.48, "text": " standards or there's like very high variance in different regions on how languages are", "tokens": [50638, 7787, 420, 456, 311, 411, 588, 1090, 21977, 294, 819, 10682, 322, 577, 8650, 366, 50880], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 172, "seek": 64016, "start": 650.48, "end": 651.6, "text": " spoken.", "tokens": [50880, 10759, 13, 50936], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 173, "seek": 64016, "start": 651.6, "end": 654.3199999999999, "text": " And so this step is a pretty critical one.", "tokens": [50936, 400, 370, 341, 1823, 307, 257, 1238, 4924, 472, 13, 51072], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 174, "seek": 64016, "start": 654.3199999999999, "end": 658.6, "text": " So then what we do is we take the document, we send it to one group of translators and", "tokens": [51072, 407, 550, 437, 321, 360, 307, 321, 747, 264, 4166, 11, 321, 2845, 309, 281, 472, 1594, 295, 5105, 3391, 293, 51286], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 175, "seek": 64016, "start": 658.6, "end": 660.92, "text": " they do the first translation step.", "tokens": [51286, 436, 360, 264, 700, 12853, 1823, 13, 51402], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 176, "seek": 64016, "start": 660.92, "end": 665.0, "text": " Then we do some automatic checking, you know, like if the input sentence was like 10 words", "tokens": [51402, 1396, 321, 360, 512, 12509, 8568, 11, 291, 458, 11, 411, 498, 264, 4846, 8174, 390, 411, 1266, 2283, 51606], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 177, "seek": 64016, "start": 665.0, "end": 669.4, "text": " and the output sentence is like 300 words, it's like most likely something went wrong,", "tokens": [51606, 293, 264, 5598, 8174, 307, 411, 6641, 2283, 11, 309, 311, 411, 881, 3700, 746, 1437, 2085, 11, 51826], "temperature": 0.0, "avg_logprob": -0.12108296506545123, "compression_ratio": 1.8312101910828025, "no_speech_prob": 0.0002693716960493475}, {"id": 178, "seek": 66940, "start": 669.88, "end": 671.4399999999999, "text": " and so we send it back.", "tokens": [50388, 293, 370, 321, 2845, 309, 646, 13, 50466], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 179, "seek": 66940, "start": 671.4399999999999, "end": 677.92, "text": " Otherwise, we'll send it onwards to a separate, completely independent set of translators", "tokens": [50466, 10328, 11, 321, 603, 2845, 309, 34230, 281, 257, 4994, 11, 2584, 6695, 992, 295, 5105, 3391, 50790], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 180, "seek": 66940, "start": 677.92, "end": 679.16, "text": " that do review.", "tokens": [50790, 300, 360, 3131, 13, 50852], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 181, "seek": 66940, "start": 679.16, "end": 681.36, "text": " And so they try to rate the quality of this.", "tokens": [50852, 400, 370, 436, 853, 281, 3314, 264, 3125, 295, 341, 13, 50962], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 182, "seek": 66940, "start": 681.36, "end": 685.48, "text": " And if the quality doesn't pass the sufficient bar, it gets sent back to the original set", "tokens": [50962, 400, 498, 264, 3125, 1177, 380, 1320, 264, 11563, 2159, 11, 309, 2170, 2279, 646, 281, 264, 3380, 992, 51168], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 183, "seek": 66940, "start": 685.48, "end": 690.68, "text": " of translators to edit and they kind of go through and like address all of the feedback.", "tokens": [51168, 295, 5105, 3391, 281, 8129, 293, 436, 733, 295, 352, 807, 293, 411, 2985, 439, 295, 264, 5824, 13, 51428], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 184, "seek": 66940, "start": 690.68, "end": 694.56, "text": " And then if it's good enough, then it enters our data set.", "tokens": [51428, 400, 550, 498, 309, 311, 665, 1547, 11, 550, 309, 18780, 527, 1412, 992, 13, 51622], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 185, "seek": 66940, "start": 694.56, "end": 696.4, "text": " And so there's many challenges here.", "tokens": [51622, 400, 370, 456, 311, 867, 4759, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14332475500591732, "compression_ratio": 1.8032128514056225, "no_speech_prob": 0.00446589570492506}, {"id": 186, "seek": 69640, "start": 696.4399999999999, "end": 701.28, "text": " The first one, of course, is just like finding translators and also finding more translators.", "tokens": [50366, 440, 700, 472, 11, 295, 1164, 11, 307, 445, 411, 5006, 5105, 3391, 293, 611, 5006, 544, 5105, 3391, 13, 50608], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 187, "seek": 69640, "start": 701.28, "end": 705.88, "text": " There was a certain issue that we ran into, for example, that in a certain country that", "tokens": [50608, 821, 390, 257, 1629, 2734, 300, 321, 5872, 666, 11, 337, 1365, 11, 300, 294, 257, 1629, 1941, 300, 50838], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 188, "seek": 69640, "start": 705.88, "end": 707.88, "text": " the internet was not available.", "tokens": [50838, 264, 4705, 390, 406, 2435, 13, 50938], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 189, "seek": 69640, "start": 707.88, "end": 711.4, "text": " And so, you know, it's a lot of recruitment.", "tokens": [50938, 400, 370, 11, 291, 458, 11, 309, 311, 257, 688, 295, 28240, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 190, "seek": 69640, "start": 711.4, "end": 714.04, "text": " The other one, of course, is language standardization.", "tokens": [51114, 440, 661, 472, 11, 295, 1164, 11, 307, 2856, 3832, 2144, 13, 51246], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 191, "seek": 69640, "start": 714.04, "end": 719.88, "text": " I think I briefly mentioned this before, but there's a lot of different challenges in just", "tokens": [51246, 286, 519, 286, 10515, 2835, 341, 949, 11, 457, 456, 311, 257, 688, 295, 819, 4759, 294, 445, 51538], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 192, "seek": 69640, "start": 719.88, "end": 722.48, "text": " understanding like what is a high quality translation.", "tokens": [51538, 3701, 411, 437, 307, 257, 1090, 3125, 12853, 13, 51668], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 193, "seek": 69640, "start": 722.48, "end": 725.0, "text": " For example, the low resource language, Breton.", "tokens": [51668, 1171, 1365, 11, 264, 2295, 7684, 2856, 11, 42000, 266, 13, 51794], "temperature": 0.0, "avg_logprob": -0.15761510152665395, "compression_ratio": 1.7852112676056338, "no_speech_prob": 0.00043037181603722274}, {"id": 194, "seek": 72500, "start": 725.0, "end": 728.44, "text": " There's like two competing groups on like, how do you write Breton?", "tokens": [50364, 821, 311, 411, 732, 15439, 3935, 322, 411, 11, 577, 360, 291, 2464, 42000, 266, 30, 50536], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 195, "seek": 72500, "start": 728.44, "end": 731.2, "text": " So it's like very difficult to resolve some of those things.", "tokens": [50536, 407, 309, 311, 411, 588, 2252, 281, 14151, 512, 295, 729, 721, 13, 50674], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 196, "seek": 72500, "start": 731.2, "end": 736.52, "text": " And the final thing is that there's actually a lot of variation, even in languages like Arabic,", "tokens": [50674, 400, 264, 2572, 551, 307, 300, 456, 311, 767, 257, 688, 295, 12990, 11, 754, 294, 8650, 411, 19938, 11, 50940], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 197, "seek": 72500, "start": 736.52, "end": 742.6, "text": " like the Arabic, like Moroccan Arabic is very different from, you know, Jordanian Arabic and so on.", "tokens": [50940, 411, 264, 19938, 11, 411, 30893, 7035, 19938, 307, 588, 819, 490, 11, 291, 458, 11, 10979, 952, 19938, 293, 370, 322, 13, 51244], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 198, "seek": 72500, "start": 742.6, "end": 747.4, "text": " And there are also certain regions that they speak the same language, but due to historical reasons,", "tokens": [51244, 400, 456, 366, 611, 1629, 10682, 300, 436, 1710, 264, 912, 2856, 11, 457, 3462, 281, 8584, 4112, 11, 51484], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 199, "seek": 72500, "start": 747.4, "end": 749.24, "text": " they write in different scripts.", "tokens": [51484, 436, 2464, 294, 819, 23294, 13, 51576], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 200, "seek": 72500, "start": 749.24, "end": 753.6, "text": " And so one of the things we actually did was like, if there are languages written in multiple scripts,", "tokens": [51576, 400, 370, 472, 295, 264, 721, 321, 767, 630, 390, 411, 11, 498, 456, 366, 8650, 3720, 294, 3866, 23294, 11, 51794], "temperature": 0.0, "avg_logprob": -0.13981036285855877, "compression_ratio": 1.8762541806020068, "no_speech_prob": 0.0009239473147317767}, {"id": 201, "seek": 75360, "start": 753.6, "end": 757.5600000000001, "text": " we actually supported the collection of a multiple script evaluation.", "tokens": [50364, 321, 767, 8104, 264, 5765, 295, 257, 3866, 5755, 13344, 13, 50562], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 202, "seek": 75360, "start": 757.5600000000001, "end": 761.88, "text": " And I think this is really important because if you're building an underlying technology", "tokens": [50562, 400, 286, 519, 341, 307, 534, 1021, 570, 498, 291, 434, 2390, 364, 14217, 2899, 50778], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 203, "seek": 75360, "start": 761.88, "end": 766.6, "text": " and you only choose one, then I think you risk like just kind of like naturally supporting", "tokens": [50778, 293, 291, 787, 2826, 472, 11, 550, 286, 519, 291, 3148, 411, 445, 733, 295, 411, 8195, 7231, 51014], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 204, "seek": 75360, "start": 766.6, "end": 771.72, "text": " one over the other when we really should be like kind of a more neutral technology provider.", "tokens": [51014, 472, 670, 264, 661, 562, 321, 534, 820, 312, 411, 733, 295, 257, 544, 10598, 2899, 12398, 13, 51270], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 205, "seek": 75360, "start": 771.72, "end": 777.44, "text": " And so this is something that we we explored a lot as well as exploring different variants of Arabic.", "tokens": [51270, 400, 370, 341, 307, 746, 300, 321, 321, 24016, 257, 688, 382, 731, 382, 12736, 819, 21669, 295, 19938, 13, 51556], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 206, "seek": 75360, "start": 777.44, "end": 778.48, "text": " This is also open source.", "tokens": [51556, 639, 307, 611, 1269, 4009, 13, 51608], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 207, "seek": 75360, "start": 778.48, "end": 783.5600000000001, "text": " If you just go to this link, you can just like download all of the all of the text files for this.", "tokens": [51608, 759, 291, 445, 352, 281, 341, 2113, 11, 291, 393, 445, 411, 5484, 439, 295, 264, 439, 295, 264, 2487, 7098, 337, 341, 13, 51862], "temperature": 0.0, "avg_logprob": -0.13350652158260345, "compression_ratio": 1.7949526813880126, "no_speech_prob": 0.0010467946995049715}, {"id": 208, "seek": 78356, "start": 783.76, "end": 789.04, "text": " With evaluation done, I want to talk a little bit about how we collected some of these training data sets.", "tokens": [50374, 2022, 13344, 1096, 11, 286, 528, 281, 751, 257, 707, 857, 466, 577, 321, 11087, 512, 295, 613, 3097, 1412, 6352, 13, 50638], "temperature": 0.0, "avg_logprob": -0.12577763470736417, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0008961628191173077}, {"id": 209, "seek": 78356, "start": 789.04, "end": 793.88, "text": " The first thing I want to talk about is this data set we created called NLBCD.", "tokens": [50638, 440, 700, 551, 286, 528, 281, 751, 466, 307, 341, 1412, 992, 321, 2942, 1219, 426, 43, 7869, 35, 13, 50880], "temperature": 0.0, "avg_logprob": -0.12577763470736417, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0008961628191173077}, {"id": 210, "seek": 78356, "start": 793.88, "end": 798.9599999999999, "text": " And the idea of this is like it's a really seed data set of high quality translations", "tokens": [50880, 400, 264, 1558, 295, 341, 307, 411, 309, 311, 257, 534, 8871, 1412, 992, 295, 1090, 3125, 37578, 51134], "temperature": 0.0, "avg_logprob": -0.12577763470736417, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0008961628191173077}, {"id": 211, "seek": 78356, "start": 798.9599999999999, "end": 801.16, "text": " and languages that really don't have anything.", "tokens": [51134, 293, 8650, 300, 534, 500, 380, 362, 1340, 13, 51244], "temperature": 0.0, "avg_logprob": -0.12577763470736417, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0008961628191173077}, {"id": 212, "seek": 78356, "start": 801.16, "end": 806.16, "text": " Why? Because, well, you can't start from nothing, you know, you got a bootstrap from somewhere.", "tokens": [51244, 1545, 30, 1436, 11, 731, 11, 291, 393, 380, 722, 490, 1825, 11, 291, 458, 11, 291, 658, 257, 11450, 372, 4007, 490, 4079, 13, 51494], "temperature": 0.0, "avg_logprob": -0.12577763470736417, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0008961628191173077}, {"id": 213, "seek": 78356, "start": 806.16, "end": 812.3199999999999, "text": " A lot of people have been using the Bible as a way to bootstrap, but it's very limited domain,", "tokens": [51494, 316, 688, 295, 561, 362, 668, 1228, 264, 6544, 382, 257, 636, 281, 11450, 372, 4007, 11, 457, 309, 311, 588, 5567, 9274, 11, 51802], "temperature": 0.0, "avg_logprob": -0.12577763470736417, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0008961628191173077}, {"id": 214, "seek": 81232, "start": 812.32, "end": 814.2800000000001, "text": " obviously very religious text.", "tokens": [50364, 2745, 588, 7185, 2487, 13, 50462], "temperature": 0.0, "avg_logprob": -0.2040251504003474, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.00033003470161929727}, {"id": 215, "seek": 81232, "start": 814.2800000000001, "end": 821.4000000000001, "text": " And so we created this data set NLBCD for languages that really don't have anything to get started from.", "tokens": [50462, 400, 370, 321, 2942, 341, 1412, 992, 426, 43, 7869, 35, 337, 8650, 300, 534, 500, 380, 362, 1340, 281, 483, 1409, 490, 13, 50818], "temperature": 0.0, "avg_logprob": -0.2040251504003474, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.00033003470161929727}, {"id": 216, "seek": 81232, "start": 821.4000000000001, "end": 826.5200000000001, "text": " It's only about 5,000 sentences, so it's nothing crazy, but it supports a lot of different use cases", "tokens": [50818, 467, 311, 787, 466, 1025, 11, 1360, 16579, 11, 370, 309, 311, 1825, 3219, 11, 457, 309, 9346, 257, 688, 295, 819, 764, 3331, 51074], "temperature": 0.0, "avg_logprob": -0.2040251504003474, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.00033003470161929727}, {"id": 217, "seek": 81232, "start": 826.5200000000001, "end": 831.72, "text": " like training language identification models or sentence encoders, engram language models,", "tokens": [51074, 411, 3097, 2856, 22065, 5245, 420, 8174, 2058, 378, 433, 11, 465, 1342, 2856, 5245, 11, 51334], "temperature": 0.0, "avg_logprob": -0.2040251504003474, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.00033003470161929727}, {"id": 218, "seek": 81232, "start": 831.72, "end": 834.8000000000001, "text": " like all of these things that I'm about to talk about in our data set pipeline.", "tokens": [51334, 411, 439, 295, 613, 721, 300, 286, 478, 466, 281, 751, 466, 294, 527, 1412, 992, 15517, 13, 51488], "temperature": 0.0, "avg_logprob": -0.2040251504003474, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.00033003470161929727}, {"id": 219, "seek": 81232, "start": 836.0400000000001, "end": 839.6800000000001, "text": " So it covers 43 languages, about 6,000 sentences.", "tokens": [51550, 407, 309, 10538, 17914, 8650, 11, 466, 1386, 11, 1360, 16579, 13, 51732], "temperature": 0.0, "avg_logprob": -0.2040251504003474, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.00033003470161929727}, {"id": 220, "seek": 83968, "start": 839.8, "end": 843.8399999999999, "text": " And the way we decided to sample it is focused on really general content.", "tokens": [50370, 400, 264, 636, 321, 3047, 281, 6889, 309, 307, 5178, 322, 534, 2674, 2701, 13, 50572], "temperature": 0.0, "avg_logprob": -0.15890166241189707, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0009998100576922297}, {"id": 221, "seek": 83968, "start": 843.8399999999999, "end": 848.4, "text": " So Wikipedia has this article of like, hey, if you're going to start like a new Wikipedia", "tokens": [50572, 407, 28999, 575, 341, 7222, 295, 411, 11, 4177, 11, 498, 291, 434, 516, 281, 722, 411, 257, 777, 28999, 50800], "temperature": 0.0, "avg_logprob": -0.15890166241189707, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0009998100576922297}, {"id": 222, "seek": 83968, "start": 848.4, "end": 853.8399999999999, "text": " in your new language, I think Wikipedia has like 309-ish Wikipedia's last I checked.", "tokens": [50800, 294, 428, 777, 2856, 11, 286, 519, 28999, 575, 411, 2217, 24, 12, 742, 28999, 311, 1036, 286, 10033, 13, 51072], "temperature": 0.0, "avg_logprob": -0.15890166241189707, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0009998100576922297}, {"id": 223, "seek": 83968, "start": 853.8399999999999, "end": 857.5999999999999, "text": " Here's like a list of articles that every Wikipedia in a new language should have.", "tokens": [51072, 1692, 311, 411, 257, 1329, 295, 11290, 300, 633, 28999, 294, 257, 777, 2856, 820, 362, 13, 51260], "temperature": 0.0, "avg_logprob": -0.15890166241189707, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0009998100576922297}, {"id": 224, "seek": 83968, "start": 857.5999999999999, "end": 860.64, "text": " And so that's where we sampled this original content from.", "tokens": [51260, 400, 370, 300, 311, 689, 321, 3247, 15551, 341, 3380, 2701, 490, 13, 51412], "temperature": 0.0, "avg_logprob": -0.15890166241189707, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0009998100576922297}, {"id": 225, "seek": 83968, "start": 860.64, "end": 864.8399999999999, "text": " And of course, it's also open source if you want to download it.", "tokens": [51412, 400, 295, 1164, 11, 309, 311, 611, 1269, 4009, 498, 291, 528, 281, 5484, 309, 13, 51622], "temperature": 0.0, "avg_logprob": -0.15890166241189707, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0009998100576922297}, {"id": 226, "seek": 86484, "start": 864.88, "end": 870.6, "text": " So what we ended up doing to get large-scale training data is using mining.", "tokens": [50366, 407, 437, 321, 4590, 493, 884, 281, 483, 2416, 12, 20033, 3097, 1412, 307, 1228, 15512, 13, 50652], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 227, "seek": 86484, "start": 870.6, "end": 873.2, "text": " So this is not something we pioneered in this project.", "tokens": [50652, 407, 341, 307, 406, 746, 321, 19761, 4073, 294, 341, 1716, 13, 50782], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 228, "seek": 86484, "start": 873.2, "end": 875.6, "text": " We have like a bunch of different previous work.", "tokens": [50782, 492, 362, 411, 257, 3840, 295, 819, 3894, 589, 13, 50902], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 229, "seek": 86484, "start": 875.6, "end": 877.8000000000001, "text": " So we started from Wikimatrix.", "tokens": [50902, 407, 321, 1409, 490, 23377, 332, 267, 6579, 13, 51012], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 230, "seek": 86484, "start": 877.8000000000001, "end": 881.6, "text": " We were like, hey, there's a lot of different sentences in Wikipedia", "tokens": [51012, 492, 645, 411, 11, 4177, 11, 456, 311, 257, 688, 295, 819, 16579, 294, 28999, 51202], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 231, "seek": 86484, "start": 881.6, "end": 884.2800000000001, "text": " and different languages that we should be able to match up.", "tokens": [51202, 293, 819, 8650, 300, 321, 820, 312, 1075, 281, 2995, 493, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 232, "seek": 86484, "start": 884.2800000000001, "end": 889.52, "text": " And so we tried to do that with Wikipedia to get machine translation training data.", "tokens": [51336, 400, 370, 321, 3031, 281, 360, 300, 365, 28999, 281, 483, 3479, 12853, 3097, 1412, 13, 51598], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 233, "seek": 86484, "start": 889.52, "end": 892.48, "text": " We extended that to the web in the CCMatrix project,", "tokens": [51598, 492, 10913, 300, 281, 264, 3670, 294, 264, 12630, 42325, 6579, 1716, 11, 51746], "temperature": 0.0, "avg_logprob": -0.12095531088406922, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0002958636323455721}, {"id": 234, "seek": 89248, "start": 892.52, "end": 896.6800000000001, "text": " and then we extended it to very, very large-scale mining on all cross-pairs", "tokens": [50366, 293, 550, 321, 10913, 309, 281, 588, 11, 588, 2416, 12, 20033, 15512, 322, 439, 3278, 12, 79, 4094, 50574], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 235, "seek": 89248, "start": 896.6800000000001, "end": 900.6800000000001, "text": " in this project on beyond English-centric multilingual machine translation.", "tokens": [50574, 294, 341, 1716, 322, 4399, 3669, 12, 45300, 2120, 38219, 3479, 12853, 13, 50774], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 236, "seek": 89248, "start": 900.6800000000001, "end": 904.32, "text": " We really tried to ditch like English as a central pivot language.", "tokens": [50774, 492, 534, 3031, 281, 25325, 411, 3669, 382, 257, 5777, 14538, 2856, 13, 50956], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 237, "seek": 89248, "start": 904.32, "end": 907.2, "text": " And so the way this whole data mining thing works", "tokens": [50956, 400, 370, 264, 636, 341, 1379, 1412, 15512, 551, 1985, 51100], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 238, "seek": 89248, "start": 907.2, "end": 909.6800000000001, "text": " is that it focuses on sentence alignment.", "tokens": [51100, 307, 300, 309, 16109, 322, 8174, 18515, 13, 51224], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 239, "seek": 89248, "start": 909.6800000000001, "end": 911.5600000000001, "text": " So everyone is probably super familiar with this", "tokens": [51224, 407, 1518, 307, 1391, 1687, 4963, 365, 341, 51318], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 240, "seek": 89248, "start": 911.5600000000001, "end": 913.32, "text": " because this is how language models are built now.", "tokens": [51318, 570, 341, 307, 577, 2856, 5245, 366, 3094, 586, 13, 51406], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 241, "seek": 89248, "start": 913.32, "end": 917.52, "text": " But it's like you take Common Crawl or any other open source dump of the web.", "tokens": [51406, 583, 309, 311, 411, 291, 747, 18235, 37877, 75, 420, 604, 661, 1269, 4009, 11430, 295, 264, 3670, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 242, "seek": 89248, "start": 917.52, "end": 920.5600000000001, "text": " I don't know, like Red Pajama or like whatever you want to CCNet,", "tokens": [51616, 286, 500, 380, 458, 11, 411, 4477, 430, 1805, 2404, 420, 411, 2035, 291, 528, 281, 12630, 31890, 11, 51768], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 243, "seek": 89248, "start": 920.5600000000001, "end": 922.32, "text": " whatever you want to use these days.", "tokens": [51768, 2035, 291, 528, 281, 764, 613, 1708, 13, 51856], "temperature": 0.0, "avg_logprob": -0.13432313291818504, "compression_ratio": 1.6742209631728044, "no_speech_prob": 0.00010228216706309468}, {"id": 244, "seek": 92232, "start": 922.32, "end": 925.96, "text": " And you take all of the data, you extract all of the text,", "tokens": [50364, 400, 291, 747, 439, 295, 264, 1412, 11, 291, 8947, 439, 295, 264, 2487, 11, 50546], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 245, "seek": 92232, "start": 925.96, "end": 928.72, "text": " you know, a lot of HTML parsing and so on goes into it.", "tokens": [50546, 291, 458, 11, 257, 688, 295, 17995, 21156, 278, 293, 370, 322, 1709, 666, 309, 13, 50684], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 246, "seek": 92232, "start": 928.72, "end": 931.6400000000001, "text": " And the idea is that we want to try to find matching text", "tokens": [50684, 400, 264, 1558, 307, 300, 321, 528, 281, 853, 281, 915, 14324, 2487, 50830], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 247, "seek": 92232, "start": 931.6400000000001, "end": 933.08, "text": " that could be a translation.", "tokens": [50830, 300, 727, 312, 257, 12853, 13, 50902], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 248, "seek": 92232, "start": 933.08, "end": 935.24, "text": " So we shatter it all into sentences,", "tokens": [50902, 407, 321, 402, 1161, 309, 439, 666, 16579, 11, 51010], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 249, "seek": 92232, "start": 935.24, "end": 938.36, "text": " we embed them with different sentence encoder models,", "tokens": [51010, 321, 12240, 552, 365, 819, 8174, 2058, 19866, 5245, 11, 51166], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 250, "seek": 92232, "start": 938.36, "end": 942.0400000000001, "text": " and then we do a match to try to understand in a multilingual space", "tokens": [51166, 293, 550, 321, 360, 257, 2995, 281, 853, 281, 1223, 294, 257, 2120, 38219, 1901, 51350], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 251, "seek": 92232, "start": 942.0400000000001, "end": 944.6800000000001, "text": " if the sentences match.", "tokens": [51350, 498, 264, 16579, 2995, 13, 51482], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 252, "seek": 92232, "start": 944.6800000000001, "end": 947.0, "text": " And so one of the biggest challenges to this", "tokens": [51482, 400, 370, 472, 295, 264, 3880, 4759, 281, 341, 51598], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 253, "seek": 92232, "start": 947.0, "end": 950.6, "text": " is that the quality of the sentence encoding is very important.", "tokens": [51598, 307, 300, 264, 3125, 295, 264, 8174, 43430, 307, 588, 1021, 13, 51778], "temperature": 0.0, "avg_logprob": -0.10685269037882487, "compression_ratio": 1.8125, "no_speech_prob": 3.425981049076654e-05}, {"id": 254, "seek": 95060, "start": 950.6, "end": 952.72, "text": " So if your sentence encoding is not very accurate,", "tokens": [50364, 407, 498, 428, 8174, 43430, 307, 406, 588, 8559, 11, 50470], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 255, "seek": 95060, "start": 952.72, "end": 955.76, "text": " then it's impossible to match in this multidimensional space", "tokens": [50470, 550, 309, 311, 6243, 281, 2995, 294, 341, 2120, 327, 332, 11075, 1901, 50622], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 256, "seek": 95060, "start": 955.76, "end": 958.48, "text": " the idea of like the meaning being the same.", "tokens": [50622, 264, 1558, 295, 411, 264, 3620, 885, 264, 912, 13, 50758], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 257, "seek": 95060, "start": 958.48, "end": 960.88, "text": " And so one of the big things we tried to do here", "tokens": [50758, 400, 370, 472, 295, 264, 955, 721, 321, 3031, 281, 360, 510, 50878], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 258, "seek": 95060, "start": 960.88, "end": 965.36, "text": " in this project was try to improve the quality of the sentence encoders.", "tokens": [50878, 294, 341, 1716, 390, 853, 281, 3470, 264, 3125, 295, 264, 8174, 2058, 378, 433, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 259, "seek": 95060, "start": 965.36, "end": 968.76, "text": " And so one of the big things that we did was train sentence encoders", "tokens": [51102, 400, 370, 472, 295, 264, 955, 721, 300, 321, 630, 390, 3847, 8174, 2058, 378, 433, 51272], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 260, "seek": 95060, "start": 968.76, "end": 970.24, "text": " with mask language modeling.", "tokens": [51272, 365, 6094, 2856, 15983, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 261, "seek": 95060, "start": 970.24, "end": 971.52, "text": " You see that on the left.", "tokens": [51346, 509, 536, 300, 322, 264, 1411, 13, 51410], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 262, "seek": 95060, "start": 971.52, "end": 976.4, "text": " But we also use multilingual distillation, which you see on the right.", "tokens": [51410, 583, 321, 611, 764, 2120, 38219, 42923, 399, 11, 597, 291, 536, 322, 264, 558, 13, 51654], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 263, "seek": 95060, "start": 976.4, "end": 979.84, "text": " And so previous approaches to sentence encoders", "tokens": [51654, 400, 370, 3894, 11587, 281, 8174, 2058, 378, 433, 51826], "temperature": 0.0, "avg_logprob": -0.1315799320445341, "compression_ratio": 1.8741007194244603, "no_speech_prob": 9.311369649367407e-05}, {"id": 264, "seek": 97984, "start": 979.84, "end": 982.24, "text": " and the trend in the research community for a while", "tokens": [50364, 293, 264, 6028, 294, 264, 2132, 1768, 337, 257, 1339, 50484], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 265, "seek": 97984, "start": 982.24, "end": 985.36, "text": " was to really try to embed all languages", "tokens": [50484, 390, 281, 534, 853, 281, 12240, 439, 8650, 50640], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 266, "seek": 97984, "start": 985.36, "end": 987.2, "text": " in the same sentence encoder model.", "tokens": [50640, 294, 264, 912, 8174, 2058, 19866, 2316, 13, 50732], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 267, "seek": 97984, "start": 987.2, "end": 990.88, "text": " So projects like XLMR, for example, are in that direction.", "tokens": [50732, 407, 4455, 411, 37210, 21173, 11, 337, 1365, 11, 366, 294, 300, 3513, 13, 50916], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 268, "seek": 97984, "start": 990.88, "end": 993.1600000000001, "text": " I think it's pretty widely used.", "tokens": [50916, 286, 519, 309, 311, 1238, 13371, 1143, 13, 51030], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 269, "seek": 97984, "start": 993.1600000000001, "end": 996.24, "text": " The challenge with this when you're training a low resource model", "tokens": [51030, 440, 3430, 365, 341, 562, 291, 434, 3097, 257, 2295, 7684, 2316, 51184], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 270, "seek": 97984, "start": 996.24, "end": 998.64, "text": " is that a lot of your high resource data", "tokens": [51184, 307, 300, 257, 688, 295, 428, 1090, 7684, 1412, 51304], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 271, "seek": 97984, "start": 998.64, "end": 1001.96, "text": " just overwhelms your low resource data.", "tokens": [51304, 445, 9103, 2592, 428, 2295, 7684, 1412, 13, 51470], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 272, "seek": 97984, "start": 1001.96, "end": 1005.0400000000001, "text": " And so you don't end up with a very high quality sentence encoder", "tokens": [51470, 400, 370, 291, 500, 380, 917, 493, 365, 257, 588, 1090, 3125, 8174, 2058, 19866, 51624], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 273, "seek": 97984, "start": 1005.0400000000001, "end": 1006.32, "text": " for those languages.", "tokens": [51624, 337, 729, 8650, 13, 51688], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 274, "seek": 97984, "start": 1006.32, "end": 1009.64, "text": " So what we ended up doing is we had a multilingual teacher model", "tokens": [51688, 407, 437, 321, 4590, 493, 884, 307, 321, 632, 257, 2120, 38219, 5027, 2316, 51854], "temperature": 0.0, "avg_logprob": -0.10410940113352306, "compression_ratio": 1.73, "no_speech_prob": 0.00022329797502607107}, {"id": 275, "seek": 100964, "start": 1009.68, "end": 1012.52, "text": " and we distilled a bunch of student models", "tokens": [50366, 293, 321, 1483, 6261, 257, 3840, 295, 3107, 5245, 50508], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 276, "seek": 100964, "start": 1012.52, "end": 1016.4399999999999, "text": " that are specialized to different language families", "tokens": [50508, 300, 366, 19813, 281, 819, 2856, 4466, 50704], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 277, "seek": 100964, "start": 1016.4399999999999, "end": 1017.68, "text": " that are low resource.", "tokens": [50704, 300, 366, 2295, 7684, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 278, "seek": 100964, "start": 1017.68, "end": 1020.1999999999999, "text": " And so this enables the quality to be pretty high.", "tokens": [50766, 400, 370, 341, 17077, 264, 3125, 281, 312, 1238, 1090, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 279, "seek": 100964, "start": 1020.1999999999999, "end": 1022.0, "text": " And so the way that distillation works", "tokens": [50892, 400, 370, 264, 636, 300, 42923, 399, 1985, 50982], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 280, "seek": 100964, "start": 1022.0, "end": 1025.4, "text": " is that the teacher and the student model both see the same data", "tokens": [50982, 307, 300, 264, 5027, 293, 264, 3107, 2316, 1293, 536, 264, 912, 1412, 51152], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 281, "seek": 100964, "start": 1025.4, "end": 1028.36, "text": " and then we try to minimize the cosine loss", "tokens": [51152, 293, 550, 321, 853, 281, 17522, 264, 23565, 4470, 51300], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 282, "seek": 100964, "start": 1028.36, "end": 1032.48, "text": " between the sentence embeddings that they produce.", "tokens": [51300, 1296, 264, 8174, 12240, 29432, 300, 436, 5258, 13, 51506], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 283, "seek": 100964, "start": 1032.48, "end": 1034.36, "text": " I think an important question that you can ask here", "tokens": [51506, 286, 519, 364, 1021, 1168, 300, 291, 393, 1029, 510, 51600], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 284, "seek": 100964, "start": 1034.36, "end": 1037.52, "text": " is why do you need to do multilingual distillation?", "tokens": [51600, 307, 983, 360, 291, 643, 281, 360, 2120, 38219, 42923, 399, 30, 51758], "temperature": 0.0, "avg_logprob": -0.11844545447308084, "compression_ratio": 1.8115384615384615, "no_speech_prob": 5.735915692639537e-05}, {"id": 285, "seek": 103752, "start": 1037.52, "end": 1041.28, "text": " Why can't you just train a bunch of different student models,", "tokens": [50364, 1545, 393, 380, 291, 445, 3847, 257, 3840, 295, 819, 3107, 5245, 11, 50552], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 286, "seek": 103752, "start": 1041.28, "end": 1042.72, "text": " like one per language family,", "tokens": [50552, 411, 472, 680, 2856, 1605, 11, 50624], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 287, "seek": 103752, "start": 1042.72, "end": 1045.04, "text": " like why even care about distillation?", "tokens": [50624, 411, 983, 754, 1127, 466, 42923, 399, 30, 50740], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 288, "seek": 103752, "start": 1045.04, "end": 1047.8799999999999, "text": " And the reason is because if you're going to use a bunch", "tokens": [50740, 400, 264, 1778, 307, 570, 498, 291, 434, 516, 281, 764, 257, 3840, 50882], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 289, "seek": 103752, "start": 1047.8799999999999, "end": 1049.96, "text": " of sentence encoders for mining,", "tokens": [50882, 295, 8174, 2058, 378, 433, 337, 15512, 11, 50986], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 290, "seek": 103752, "start": 1049.96, "end": 1051.8799999999999, "text": " the important thing is that they all exist", "tokens": [50986, 264, 1021, 551, 307, 300, 436, 439, 2514, 51082], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 291, "seek": 103752, "start": 1051.8799999999999, "end": 1054.16, "text": " in the same embedding space.", "tokens": [51082, 294, 264, 912, 12240, 3584, 1901, 13, 51196], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 292, "seek": 103752, "start": 1054.16, "end": 1055.68, "text": " Like if you train one separate model", "tokens": [51196, 1743, 498, 291, 3847, 472, 4994, 2316, 51272], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 293, "seek": 103752, "start": 1055.68, "end": 1056.8, "text": " and another separate model,", "tokens": [51272, 293, 1071, 4994, 2316, 11, 51328], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 294, "seek": 103752, "start": 1056.8, "end": 1059.16, "text": " there's nothing constraining them", "tokens": [51328, 456, 311, 1825, 11525, 1760, 552, 51446], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 295, "seek": 103752, "start": 1059.16, "end": 1061.96, "text": " so that you can mine all of the data against each other.", "tokens": [51446, 370, 300, 291, 393, 3892, 439, 295, 264, 1412, 1970, 1184, 661, 13, 51586], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 296, "seek": 103752, "start": 1061.96, "end": 1063.76, "text": " And so one of the things we found", "tokens": [51586, 400, 370, 472, 295, 264, 721, 321, 1352, 51676], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 297, "seek": 103752, "start": 1063.76, "end": 1066.6399999999999, "text": " is that by starting everything from the same teacher model", "tokens": [51676, 307, 300, 538, 2891, 1203, 490, 264, 912, 5027, 2316, 51820], "temperature": 0.0, "avg_logprob": -0.10848737444196428, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.00018517542048357427}, {"id": 298, "seek": 106664, "start": 1066.64, "end": 1068.2800000000002, "text": " and trying to use this cosine loss", "tokens": [50364, 293, 1382, 281, 764, 341, 23565, 4470, 50446], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 299, "seek": 106664, "start": 1068.2800000000002, "end": 1070.5200000000002, "text": " to minimize the distance between embeddings,", "tokens": [50446, 281, 17522, 264, 4560, 1296, 12240, 29432, 11, 50558], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 300, "seek": 106664, "start": 1070.5200000000002, "end": 1072.96, "text": " you are able to have this constrained space", "tokens": [50558, 291, 366, 1075, 281, 362, 341, 38901, 1901, 50680], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 301, "seek": 106664, "start": 1072.96, "end": 1076.0800000000002, "text": " where you can mine every language against every other,", "tokens": [50680, 689, 291, 393, 3892, 633, 2856, 1970, 633, 661, 11, 50836], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 302, "seek": 106664, "start": 1076.0800000000002, "end": 1078.8400000000001, "text": " even if you have different student models.", "tokens": [50836, 754, 498, 291, 362, 819, 3107, 5245, 13, 50974], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 303, "seek": 106664, "start": 1078.8400000000001, "end": 1081.76, "text": " And so this graph on the Y axis,", "tokens": [50974, 400, 370, 341, 4295, 322, 264, 398, 10298, 11, 51120], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 304, "seek": 106664, "start": 1081.76, "end": 1085.0400000000002, "text": " it shows the error rate of mining.", "tokens": [51120, 309, 3110, 264, 6713, 3314, 295, 15512, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 305, "seek": 106664, "start": 1085.0400000000002, "end": 1086.6000000000001, "text": " And so lower is better.", "tokens": [51284, 400, 370, 3126, 307, 1101, 13, 51362], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 306, "seek": 106664, "start": 1086.6000000000001, "end": 1087.8400000000001, "text": " And on the X axis,", "tokens": [51362, 400, 322, 264, 1783, 10298, 11, 51424], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 307, "seek": 106664, "start": 1087.8400000000001, "end": 1090.0400000000002, "text": " it shows a bunch of different low resource languages.", "tokens": [51424, 309, 3110, 257, 3840, 295, 819, 2295, 7684, 8650, 13, 51534], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 308, "seek": 106664, "start": 1090.0400000000002, "end": 1091.76, "text": " So for example, the first one is Urdu,", "tokens": [51534, 407, 337, 1365, 11, 264, 700, 472, 307, 9533, 769, 11, 51620], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 309, "seek": 106664, "start": 1091.76, "end": 1094.1200000000001, "text": " the second one is Telugu,", "tokens": [51620, 264, 1150, 472, 307, 27729, 13705, 11, 51738], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 310, "seek": 106664, "start": 1094.1200000000001, "end": 1096.0800000000002, "text": " third one is Tagalog, and so on.", "tokens": [51738, 2636, 472, 307, 11204, 44434, 11, 293, 370, 322, 13, 51836], "temperature": 0.0, "avg_logprob": -0.09639363429125618, "compression_ratio": 1.7794117647058822, "no_speech_prob": 0.00021976852440275252}, {"id": 311, "seek": 109608, "start": 1096.12, "end": 1099.76, "text": " And so the gray bar here is the original laser paper.", "tokens": [50366, 400, 370, 264, 10855, 2159, 510, 307, 264, 3380, 12530, 3035, 13, 50548], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 312, "seek": 109608, "start": 1099.76, "end": 1102.6799999999998, "text": " So this is a paper we put out maybe in 2018-ish", "tokens": [50548, 407, 341, 307, 257, 3035, 321, 829, 484, 1310, 294, 6096, 12, 742, 50694], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 313, "seek": 109608, "start": 1102.6799999999998, "end": 1104.28, "text": " and we had all of these languages", "tokens": [50694, 293, 321, 632, 439, 295, 613, 8650, 50774], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 314, "seek": 109608, "start": 1104.28, "end": 1105.72, "text": " with count of them as included.", "tokens": [50774, 365, 1207, 295, 552, 382, 5556, 13, 50846], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 315, "seek": 109608, "start": 1105.72, "end": 1106.56, "text": " But as you can see,", "tokens": [50846, 583, 382, 291, 393, 536, 11, 50888], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 316, "seek": 109608, "start": 1106.56, "end": 1109.1999999999998, "text": " the error rate is extremely, extremely high", "tokens": [50888, 264, 6713, 3314, 307, 4664, 11, 4664, 1090, 51020], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 317, "seek": 109608, "start": 1109.1999999999998, "end": 1110.1999999999998, "text": " for these languages.", "tokens": [51020, 337, 613, 8650, 13, 51070], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 318, "seek": 109608, "start": 1110.1999999999998, "end": 1112.32, "text": " So even though they were included,", "tokens": [51070, 407, 754, 1673, 436, 645, 5556, 11, 51176], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 319, "seek": 109608, "start": 1112.32, "end": 1114.52, "text": " couldn't really be used for high quality.", "tokens": [51176, 2809, 380, 534, 312, 1143, 337, 1090, 3125, 13, 51286], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 320, "seek": 109608, "start": 1114.52, "end": 1117.8799999999999, "text": " And the blue bar is the laser model that we trained", "tokens": [51286, 400, 264, 3344, 2159, 307, 264, 12530, 2316, 300, 321, 8895, 51454], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 321, "seek": 109608, "start": 1117.8799999999999, "end": 1120.76, "text": " based on the technique I just described in the previous slide.", "tokens": [51454, 2361, 322, 264, 6532, 286, 445, 7619, 294, 264, 3894, 4137, 13, 51598], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 322, "seek": 109608, "start": 1120.76, "end": 1122.3999999999999, "text": " And you can see that I think the most important point", "tokens": [51598, 400, 291, 393, 536, 300, 286, 519, 264, 881, 1021, 935, 51680], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 323, "seek": 109608, "start": 1122.3999999999999, "end": 1124.1599999999999, "text": " is that you can barely see the blue bars.", "tokens": [51680, 307, 300, 291, 393, 10268, 536, 264, 3344, 10228, 13, 51768], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 324, "seek": 109608, "start": 1124.1599999999999, "end": 1125.6799999999998, "text": " So it was very effective", "tokens": [51768, 407, 309, 390, 588, 4942, 51844], "temperature": 0.0, "avg_logprob": -0.09765609941984478, "compression_ratio": 1.7546583850931676, "no_speech_prob": 0.00016336551925633103}, {"id": 325, "seek": 112568, "start": 1125.68, "end": 1127.48, "text": " even for these previous languages", "tokens": [50364, 754, 337, 613, 3894, 8650, 50454], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 326, "seek": 112568, "start": 1127.48, "end": 1130.8, "text": " that people had thought we had previously embedded.", "tokens": [50454, 300, 561, 632, 1194, 321, 632, 8046, 16741, 13, 50620], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 327, "seek": 112568, "start": 1131.68, "end": 1133.5600000000002, "text": " And then so now how does this kind of thing", "tokens": [50664, 400, 550, 370, 586, 577, 775, 341, 733, 295, 551, 50758], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 328, "seek": 112568, "start": 1133.5600000000002, "end": 1136.68, "text": " fit into a whole data pipeline around this approach?", "tokens": [50758, 3318, 666, 257, 1379, 1412, 15517, 926, 341, 3109, 30, 50914], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 329, "seek": 112568, "start": 1136.68, "end": 1138.96, "text": " So one of the most important things", "tokens": [50914, 407, 472, 295, 264, 881, 1021, 721, 51028], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 330, "seek": 112568, "start": 1138.96, "end": 1141.48, "text": " is when you download the data from the web,", "tokens": [51028, 307, 562, 291, 5484, 264, 1412, 490, 264, 3670, 11, 51154], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 331, "seek": 112568, "start": 1141.48, "end": 1144.28, "text": " you don't really know what language it's in.", "tokens": [51154, 291, 500, 380, 534, 458, 437, 2856, 309, 311, 294, 13, 51294], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 332, "seek": 112568, "start": 1144.28, "end": 1147.3200000000002, "text": " And so this is part of all of the large scale data cleaning", "tokens": [51294, 400, 370, 341, 307, 644, 295, 439, 295, 264, 2416, 4373, 1412, 8924, 51446], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 333, "seek": 112568, "start": 1147.3200000000002, "end": 1150.48, "text": " that goes into training large language models today.", "tokens": [51446, 300, 1709, 666, 3097, 2416, 2856, 5245, 965, 13, 51604], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 334, "seek": 112568, "start": 1150.48, "end": 1153.64, "text": " And so the way we identify different languages", "tokens": [51604, 400, 370, 264, 636, 321, 5876, 819, 8650, 51762], "temperature": 0.0, "avg_logprob": -0.08569560971176415, "compression_ratio": 1.789272030651341, "no_speech_prob": 0.0003458028950262815}, {"id": 335, "seek": 115364, "start": 1153.76, "end": 1155.5600000000002, "text": " is through like simple classification models", "tokens": [50370, 307, 807, 411, 2199, 21538, 5245, 50460], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 336, "seek": 115364, "start": 1155.5600000000002, "end": 1158.2800000000002, "text": " called language identification models.", "tokens": [50460, 1219, 2856, 22065, 5245, 13, 50596], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 337, "seek": 115364, "start": 1158.2800000000002, "end": 1160.76, "text": " And I think it's a classification model.", "tokens": [50596, 400, 286, 519, 309, 311, 257, 21538, 2316, 13, 50720], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 338, "seek": 115364, "start": 1160.76, "end": 1164.48, "text": " And so people think it's easier than it actually is.", "tokens": [50720, 400, 370, 561, 519, 309, 311, 3571, 813, 309, 767, 307, 13, 50906], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 339, "seek": 115364, "start": 1164.48, "end": 1166.48, "text": " But I think some of the major challenges", "tokens": [50906, 583, 286, 519, 512, 295, 264, 2563, 4759, 51006], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 340, "seek": 115364, "start": 1166.48, "end": 1169.1200000000001, "text": " are that there's so many different languages.", "tokens": [51006, 366, 300, 456, 311, 370, 867, 819, 8650, 13, 51138], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 341, "seek": 115364, "start": 1169.1200000000001, "end": 1170.88, "text": " They're written in many different ways", "tokens": [51138, 814, 434, 3720, 294, 867, 819, 2098, 51226], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 342, "seek": 115364, "start": 1170.88, "end": 1173.8400000000001, "text": " and web text is very casual.", "tokens": [51226, 293, 3670, 2487, 307, 588, 13052, 13, 51374], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 343, "seek": 115364, "start": 1173.8400000000001, "end": 1175.1200000000001, "text": " And so it can be very difficult", "tokens": [51374, 400, 370, 309, 393, 312, 588, 2252, 51438], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 344, "seek": 115364, "start": 1175.1200000000001, "end": 1177.44, "text": " to actually train a good classification model", "tokens": [51438, 281, 767, 3847, 257, 665, 21538, 2316, 51554], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 345, "seek": 115364, "start": 1177.44, "end": 1178.96, "text": " that can generalize to them.", "tokens": [51554, 300, 393, 2674, 1125, 281, 552, 13, 51630], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 346, "seek": 115364, "start": 1178.96, "end": 1180.44, "text": " And so what we did is,", "tokens": [51630, 400, 370, 437, 321, 630, 307, 11, 51704], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 347, "seek": 115364, "start": 1180.44, "end": 1183.5200000000002, "text": " we had our LID training data", "tokens": [51704, 321, 632, 527, 441, 2777, 3097, 1412, 51858], "temperature": 0.0, "avg_logprob": -0.0769724240378728, "compression_ratio": 1.8740458015267176, "no_speech_prob": 0.0007316818227991462}, {"id": 348, "seek": 118352, "start": 1183.52, "end": 1187.4, "text": " and we produced a language identification model LID.", "tokens": [50364, 293, 321, 7126, 257, 2856, 22065, 2316, 441, 2777, 13, 50558], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 349, "seek": 118352, "start": 1187.4, "end": 1189.72, "text": " And then we actually did human evaluation", "tokens": [50558, 400, 550, 321, 767, 630, 1952, 13344, 50674], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 350, "seek": 118352, "start": 1189.72, "end": 1192.56, "text": " to label errors coming from the LID system", "tokens": [50674, 281, 7645, 13603, 1348, 490, 264, 441, 2777, 1185, 50816], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 351, "seek": 118352, "start": 1192.56, "end": 1195.76, "text": " to iteratively improve this on web text itself", "tokens": [50816, 281, 17138, 19020, 3470, 341, 322, 3670, 2487, 2564, 50976], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 352, "seek": 118352, "start": 1195.76, "end": 1198.72, "text": " to improve the quality of this specific model.", "tokens": [50976, 281, 3470, 264, 3125, 295, 341, 2685, 2316, 13, 51124], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 353, "seek": 118352, "start": 1198.72, "end": 1200.56, "text": " Then after we produce this LID model,", "tokens": [51124, 1396, 934, 321, 5258, 341, 441, 2777, 2316, 11, 51216], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 354, "seek": 118352, "start": 1200.56, "end": 1202.52, "text": " then we insert like all of our common crawl", "tokens": [51216, 550, 321, 8969, 411, 439, 295, 527, 2689, 24767, 51314], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 355, "seek": 118352, "start": 1202.52, "end": 1204.44, "text": " where the web arrow is coming in", "tokens": [51314, 689, 264, 3670, 11610, 307, 1348, 294, 51410], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 356, "seek": 118352, "start": 1204.44, "end": 1206.76, "text": " and we do a ton of filtering and cleaning.", "tokens": [51410, 293, 321, 360, 257, 2952, 295, 30822, 293, 8924, 13, 51526], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 357, "seek": 118352, "start": 1206.76, "end": 1209.28, "text": " And this produces a huge corpus of different", "tokens": [51526, 400, 341, 14725, 257, 2603, 1181, 31624, 295, 819, 51652], "temperature": 0.0, "avg_logprob": -0.08351418755271219, "compression_ratio": 1.7086614173228347, "no_speech_prob": 7.363990880548954e-05}, {"id": 358, "seek": 120928, "start": 1209.32, "end": 1211.44, "text": " monolingual data that you can then use", "tokens": [50366, 1108, 401, 278, 901, 1412, 300, 291, 393, 550, 764, 50472], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 359, "seek": 120928, "start": 1211.44, "end": 1213.8799999999999, "text": " for training anything.", "tokens": [50472, 337, 3097, 1340, 13, 50594], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 360, "seek": 120928, "start": 1213.8799999999999, "end": 1216.52, "text": " Afterwards, we train our encoder,", "tokens": [50594, 41357, 11, 321, 3847, 527, 2058, 19866, 11, 50726], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 361, "seek": 120928, "start": 1216.52, "end": 1218.2, "text": " what I described on the previous text,", "tokens": [50726, 437, 286, 7619, 322, 264, 3894, 2487, 11, 50810], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 362, "seek": 120928, "start": 1218.2, "end": 1220.28, "text": " and then we convert this monolingual data", "tokens": [50810, 293, 550, 321, 7620, 341, 1108, 401, 278, 901, 1412, 50914], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 363, "seek": 120928, "start": 1220.28, "end": 1222.44, "text": " into what we call mined by texts.", "tokens": [50914, 666, 437, 321, 818, 923, 292, 538, 15765, 13, 51022], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 364, "seek": 120928, "start": 1222.44, "end": 1224.52, "text": " So these are a huge data set of things", "tokens": [51022, 407, 613, 366, 257, 2603, 1412, 992, 295, 721, 51126], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 365, "seek": 120928, "start": 1224.52, "end": 1227.44, "text": " that we think are translations of each other.", "tokens": [51126, 300, 321, 519, 366, 37578, 295, 1184, 661, 13, 51272], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 366, "seek": 120928, "start": 1227.44, "end": 1230.68, "text": " And then finally, what we do is we actually try to validate", "tokens": [51272, 400, 550, 2721, 11, 437, 321, 360, 307, 321, 767, 853, 281, 29562, 51434], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 367, "seek": 120928, "start": 1230.68, "end": 1232.92, "text": " that these are real mined by texts", "tokens": [51434, 300, 613, 366, 957, 923, 292, 538, 15765, 51546], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 368, "seek": 120928, "start": 1232.92, "end": 1236.36, "text": " by training very small bilingual multilingual,", "tokens": [51546, 538, 3097, 588, 1359, 48757, 2120, 38219, 11, 51718], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 369, "seek": 120928, "start": 1236.36, "end": 1238.32, "text": " sorry bilingual translation models", "tokens": [51718, 2597, 48757, 12853, 5245, 51816], "temperature": 0.0, "avg_logprob": -0.15332974327935112, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0007204512949101627}, {"id": 370, "seek": 123832, "start": 1238.36, "end": 1240.8799999999999, "text": " in order to see what the quality is like.", "tokens": [50366, 294, 1668, 281, 536, 437, 264, 3125, 307, 411, 13, 50492], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 371, "seek": 123832, "start": 1240.8799999999999, "end": 1241.8799999999999, "text": " And I think this is important", "tokens": [50492, 400, 286, 519, 341, 307, 1021, 50542], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 372, "seek": 123832, "start": 1241.8799999999999, "end": 1244.84, "text": " because the data development cycle", "tokens": [50542, 570, 264, 1412, 3250, 6586, 50690], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 373, "seek": 123832, "start": 1244.84, "end": 1247.6799999999998, "text": " and the end task that it's being used for,", "tokens": [50690, 293, 264, 917, 5633, 300, 309, 311, 885, 1143, 337, 11, 50832], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 374, "seek": 123832, "start": 1247.6799999999998, "end": 1251.12, "text": " you don't want to completely separate it.", "tokens": [50832, 291, 500, 380, 528, 281, 2584, 4994, 309, 13, 51004], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 375, "seek": 123832, "start": 1251.12, "end": 1253.72, "text": " An analogy to large language model training today", "tokens": [51004, 1107, 21663, 281, 2416, 2856, 2316, 3097, 965, 51134], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 376, "seek": 123832, "start": 1253.72, "end": 1256.1599999999999, "text": " is that when you're doing your pre-training,", "tokens": [51134, 307, 300, 562, 291, 434, 884, 428, 659, 12, 17227, 1760, 11, 51256], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 377, "seek": 123832, "start": 1256.1599999999999, "end": 1258.76, "text": " you don't want someone to just deliver you a data,", "tokens": [51256, 291, 500, 380, 528, 1580, 281, 445, 4239, 291, 257, 1412, 11, 51386], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 378, "seek": 123832, "start": 1258.76, "end": 1260.96, "text": " like the data mix of your different data sets", "tokens": [51386, 411, 264, 1412, 2890, 295, 428, 819, 1412, 6352, 51496], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 379, "seek": 123832, "start": 1260.96, "end": 1261.8, "text": " is very important.", "tokens": [51496, 307, 588, 1021, 13, 51538], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 380, "seek": 123832, "start": 1261.8, "end": 1263.24, "text": " And it's pretty similar here.", "tokens": [51538, 400, 309, 311, 1238, 2531, 510, 13, 51610], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 381, "seek": 123832, "start": 1264.48, "end": 1267.52, "text": " And I think one of the highlights that we did here", "tokens": [51672, 400, 286, 519, 472, 295, 264, 14254, 300, 321, 630, 510, 51824], "temperature": 0.0, "avg_logprob": -0.10172942234919621, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.000518985150847584}, {"id": 382, "seek": 126752, "start": 1267.52, "end": 1270.12, "text": " is really focused on the human evaluation", "tokens": [50364, 307, 534, 5178, 322, 264, 1952, 13344, 50494], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 383, "seek": 126752, "start": 1270.12, "end": 1272.24, "text": " of the language identification model", "tokens": [50494, 295, 264, 2856, 22065, 2316, 50600], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 384, "seek": 126752, "start": 1272.24, "end": 1273.8799999999999, "text": " because that actually improves the quality", "tokens": [50600, 570, 300, 767, 24771, 264, 3125, 50682], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 385, "seek": 126752, "start": 1273.8799999999999, "end": 1275.6399999999999, "text": " of all of the underlying data", "tokens": [50682, 295, 439, 295, 264, 14217, 1412, 50770], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 386, "seek": 126752, "start": 1275.6399999999999, "end": 1278.32, "text": " if you just more accurately know what language it's in.", "tokens": [50770, 498, 291, 445, 544, 20095, 458, 437, 2856, 309, 311, 294, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 387, "seek": 126752, "start": 1279.6, "end": 1281.24, "text": " And this entire data pipeline", "tokens": [50968, 400, 341, 2302, 1412, 15517, 51050], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 388, "seek": 126752, "start": 1281.24, "end": 1283.04, "text": " is actually open source in this library", "tokens": [51050, 307, 767, 1269, 4009, 294, 341, 6405, 51140], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 389, "seek": 126752, "start": 1283.04, "end": 1285.48, "text": " and we had an MNLP paper describing it.", "tokens": [51140, 293, 321, 632, 364, 376, 45, 45196, 3035, 16141, 309, 13, 51262], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 390, "seek": 126752, "start": 1285.48, "end": 1287.2, "text": " The reason why I thought this was important", "tokens": [51262, 440, 1778, 983, 286, 1194, 341, 390, 1021, 51348], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 391, "seek": 126752, "start": 1287.2, "end": 1289.24, "text": " is that because I think data cleaning", "tokens": [51348, 307, 300, 570, 286, 519, 1412, 8924, 51450], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 392, "seek": 126752, "start": 1289.24, "end": 1291.56, "text": " is actually such a fundamental underlying thing", "tokens": [51450, 307, 767, 1270, 257, 8088, 14217, 551, 51566], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 393, "seek": 126752, "start": 1291.56, "end": 1294.8799999999999, "text": " that drives model quality and people's data pipelines.", "tokens": [51566, 300, 11754, 2316, 3125, 293, 561, 311, 1412, 40168, 13, 51732], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 394, "seek": 126752, "start": 1294.8799999999999, "end": 1296.6399999999999, "text": " It's like, I had this script and this other thing", "tokens": [51732, 467, 311, 411, 11, 286, 632, 341, 5755, 293, 341, 661, 551, 51820], "temperature": 0.0, "avg_logprob": -0.1161089307479276, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.00017947205924429}, {"id": 395, "seek": 129664, "start": 1297.64, "end": 1299.5200000000002, "text": " and so it's actually, I think very important", "tokens": [50414, 293, 370, 309, 311, 767, 11, 286, 519, 588, 1021, 50508], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 396, "seek": 129664, "start": 1299.5200000000002, "end": 1302.24, "text": " to be able to recreate it and rerun it", "tokens": [50508, 281, 312, 1075, 281, 25833, 309, 293, 43819, 409, 309, 50644], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 397, "seek": 129664, "start": 1302.24, "end": 1305.1200000000001, "text": " as part of almost like your research", "tokens": [50644, 382, 644, 295, 1920, 411, 428, 2132, 50788], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 398, "seek": 129664, "start": 1305.1200000000001, "end": 1307.5200000000002, "text": " that you would do as follow-up work.", "tokens": [50788, 300, 291, 576, 360, 382, 1524, 12, 1010, 589, 13, 50908], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 399, "seek": 129664, "start": 1307.5200000000002, "end": 1309.5200000000002, "text": " And so that's why we open sourced it.", "tokens": [50908, 400, 370, 300, 311, 983, 321, 1269, 11006, 1232, 309, 13, 51008], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 400, "seek": 129664, "start": 1310.5600000000002, "end": 1312.4, "text": " A few reflection things.", "tokens": [51060, 316, 1326, 12914, 721, 13, 51152], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 401, "seek": 129664, "start": 1313.5600000000002, "end": 1314.92, "text": " For low resource languages,", "tokens": [51210, 1171, 2295, 7684, 8650, 11, 51278], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 402, "seek": 129664, "start": 1314.92, "end": 1317.1200000000001, "text": " even though we did a large scale mining,", "tokens": [51278, 754, 1673, 321, 630, 257, 2416, 4373, 15512, 11, 51388], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 403, "seek": 129664, "start": 1317.1200000000001, "end": 1319.4, "text": " I think monolingual data is the limiting factor.", "tokens": [51388, 286, 519, 1108, 401, 278, 901, 1412, 307, 264, 22083, 5952, 13, 51502], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 404, "seek": 129664, "start": 1319.4, "end": 1321.8400000000001, "text": " Like there are many languages that do not have", "tokens": [51502, 1743, 456, 366, 867, 8650, 300, 360, 406, 362, 51624], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 405, "seek": 129664, "start": 1321.8400000000001, "end": 1324.72, "text": " like a huge amount of text written online.", "tokens": [51624, 411, 257, 2603, 2372, 295, 2487, 3720, 2950, 13, 51768], "temperature": 0.0, "avg_logprob": -0.14196693798727242, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.00010225289588561282}, {"id": 406, "seek": 132472, "start": 1324.8, "end": 1328.24, "text": " And so it can be very challenging to get a large amount.", "tokens": [50368, 400, 370, 309, 393, 312, 588, 7595, 281, 483, 257, 2416, 2372, 13, 50540], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 407, "seek": 132472, "start": 1328.24, "end": 1330.64, "text": " Further, I think languages and unique scripts", "tokens": [50540, 15364, 11, 286, 519, 8650, 293, 3845, 23294, 50660], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 408, "seek": 132472, "start": 1330.64, "end": 1334.24, "text": " can be extremely hard to get good representations of", "tokens": [50660, 393, 312, 4664, 1152, 281, 483, 665, 33358, 295, 50840], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 409, "seek": 132472, "start": 1334.24, "end": 1336.44, "text": " if you don't have very much data.", "tokens": [50840, 498, 291, 500, 380, 362, 588, 709, 1412, 13, 50950], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 410, "seek": 132472, "start": 1336.44, "end": 1337.8, "text": " There are certain languages as well", "tokens": [50950, 821, 366, 1629, 8650, 382, 731, 51018], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 411, "seek": 132472, "start": 1337.8, "end": 1340.44, "text": " where they were historically written in a new script", "tokens": [51018, 689, 436, 645, 16180, 3720, 294, 257, 777, 5755, 51150], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 412, "seek": 132472, "start": 1340.44, "end": 1342.64, "text": " but now the government would like to write it", "tokens": [51150, 457, 586, 264, 2463, 576, 411, 281, 2464, 309, 51260], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 413, "seek": 132472, "start": 1343.84, "end": 1346.72, "text": " in a totally new one like the old cheeky script, for example.", "tokens": [51320, 294, 257, 3879, 777, 472, 411, 264, 1331, 12839, 88, 5755, 11, 337, 1365, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 414, "seek": 132472, "start": 1346.72, "end": 1350.2, "text": " And so there's not a lot of content to represent these scripts.", "tokens": [51464, 400, 370, 456, 311, 406, 257, 688, 295, 2701, 281, 2906, 613, 23294, 13, 51638], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 415, "seek": 132472, "start": 1350.2, "end": 1352.52, "text": " So it's hard to learn representations.", "tokens": [51638, 407, 309, 311, 1152, 281, 1466, 33358, 13, 51754], "temperature": 0.0, "avg_logprob": -0.09145001896092149, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.00010883750655921176}, {"id": 416, "seek": 135252, "start": 1352.52, "end": 1355.76, "text": " And then further, a lot of the content we create,", "tokens": [50364, 400, 550, 3052, 11, 257, 688, 295, 264, 2701, 321, 1884, 11, 50526], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 417, "seek": 135252, "start": 1355.76, "end": 1358.76, "text": " it's even after mining, it's a fairly limited domain,", "tokens": [50526, 309, 311, 754, 934, 15512, 11, 309, 311, 257, 6457, 5567, 9274, 11, 50676], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 418, "seek": 135252, "start": 1358.76, "end": 1360.32, "text": " often religious content.", "tokens": [50676, 2049, 7185, 2701, 13, 50754], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 419, "seek": 135252, "start": 1361.52, "end": 1364.44, "text": " Okay, so with data discussed,", "tokens": [50814, 1033, 11, 370, 365, 1412, 7152, 11, 50960], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 420, "seek": 135252, "start": 1364.44, "end": 1368.48, "text": " I wanna segue a little bit into some of the modeling work", "tokens": [50960, 286, 1948, 33850, 257, 707, 857, 666, 512, 295, 264, 15983, 589, 51162], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 421, "seek": 135252, "start": 1368.48, "end": 1371.8799999999999, "text": " just to kind of start with like a high level picture.", "tokens": [51162, 445, 281, 733, 295, 722, 365, 411, 257, 1090, 1496, 3036, 13, 51332], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 422, "seek": 135252, "start": 1371.8799999999999, "end": 1374.04, "text": " I think there's like three major challenges", "tokens": [51332, 286, 519, 456, 311, 411, 1045, 2563, 4759, 51440], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 423, "seek": 135252, "start": 1374.04, "end": 1377.0, "text": " when you talk about like large scale multi-lingual modeling.", "tokens": [51440, 562, 291, 751, 466, 411, 2416, 4373, 4825, 12, 1688, 901, 15983, 13, 51588], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 424, "seek": 135252, "start": 1377.0, "end": 1380.76, "text": " And these pretty much apply to language models as well.", "tokens": [51588, 400, 613, 1238, 709, 3079, 281, 2856, 5245, 382, 731, 13, 51776], "temperature": 0.0, "avg_logprob": -0.11547915424619402, "compression_ratio": 1.6142322097378277, "no_speech_prob": 8.4790495748166e-05}, {"id": 425, "seek": 138076, "start": 1381.76, "end": 1384.2, "text": " The first one is effective data augmentation", "tokens": [50414, 440, 700, 472, 307, 4942, 1412, 14501, 19631, 50536], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 426, "seek": 138076, "start": 1384.2, "end": 1385.44, "text": " for low resource languages.", "tokens": [50536, 337, 2295, 7684, 8650, 13, 50598], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 427, "seek": 138076, "start": 1385.44, "end": 1389.12, "text": " Like how can you prevent the low resource language data", "tokens": [50598, 1743, 577, 393, 291, 4871, 264, 2295, 7684, 2856, 1412, 50782], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 428, "seek": 138076, "start": 1389.12, "end": 1391.12, "text": " from just being completely drowned out", "tokens": [50782, 490, 445, 885, 2584, 38233, 484, 50882], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 429, "seek": 138076, "start": 1391.12, "end": 1393.08, "text": " by the time you've seen like all of your words", "tokens": [50882, 538, 264, 565, 291, 600, 1612, 411, 439, 295, 428, 2283, 50980], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 430, "seek": 138076, "start": 1393.08, "end": 1394.8799999999999, "text": " of German or Russian?", "tokens": [50980, 295, 6521, 420, 7220, 30, 51070], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 431, "seek": 138076, "start": 1394.8799999999999, "end": 1397.52, "text": " I think there's also a question of like scalability", "tokens": [51070, 286, 519, 456, 311, 611, 257, 1168, 295, 411, 15664, 2310, 51202], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 432, "seek": 138076, "start": 1397.52, "end": 1398.36, "text": " of the model.", "tokens": [51202, 295, 264, 2316, 13, 51244], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 433, "seek": 138076, "start": 1398.36, "end": 1401.12, "text": " So even if you train very large scale models,", "tokens": [51244, 407, 754, 498, 291, 3847, 588, 2416, 4373, 5245, 11, 51382], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 434, "seek": 138076, "start": 1401.12, "end": 1402.68, "text": " how do you prevent the representations", "tokens": [51382, 577, 360, 291, 4871, 264, 33358, 51460], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 435, "seek": 138076, "start": 1402.68, "end": 1405.64, "text": " of different languages from interfering with each other?", "tokens": [51460, 295, 819, 8650, 490, 48721, 365, 1184, 661, 30, 51608], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 436, "seek": 138076, "start": 1405.64, "end": 1407.6, "text": " And that leads to the last point as well", "tokens": [51608, 400, 300, 6689, 281, 264, 1036, 935, 382, 731, 51706], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 437, "seek": 138076, "start": 1407.6, "end": 1410.72, "text": " of like if you give the model very limited capacity,", "tokens": [51706, 295, 411, 498, 291, 976, 264, 2316, 588, 5567, 6042, 11, 51862], "temperature": 0.0, "avg_logprob": -0.09350776672363281, "compression_ratio": 1.7639344262295082, "no_speech_prob": 9.607277024770156e-05}, {"id": 438, "seek": 141072, "start": 1410.72, "end": 1412.52, "text": " then of course it may not have the capacity", "tokens": [50364, 550, 295, 1164, 309, 815, 406, 362, 264, 6042, 50454], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 439, "seek": 141072, "start": 1412.52, "end": 1415.0, "text": " to model all of these different languages.", "tokens": [50454, 281, 2316, 439, 295, 613, 819, 8650, 13, 50578], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 440, "seek": 141072, "start": 1415.0, "end": 1418.32, "text": " And so you also need to accelerate the scale of the model.", "tokens": [50578, 400, 370, 291, 611, 643, 281, 21341, 264, 4373, 295, 264, 2316, 13, 50744], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 441, "seek": 141072, "start": 1419.28, "end": 1422.0, "text": " And so preliminary for those", "tokens": [50792, 400, 370, 28817, 337, 729, 50928], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 442, "seek": 141072, "start": 1423.3600000000001, "end": 1425.68, "text": " who may not have seen a translation system before,", "tokens": [50996, 567, 815, 406, 362, 1612, 257, 12853, 1185, 949, 11, 51112], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 443, "seek": 141072, "start": 1425.68, "end": 1428.04, "text": " I don't know how many of you that practically is.", "tokens": [51112, 286, 500, 380, 458, 577, 867, 295, 291, 300, 15667, 307, 13, 51230], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 444, "seek": 141072, "start": 1428.04, "end": 1430.64, "text": " So we use standard sequence-to-sequence models.", "tokens": [51230, 407, 321, 764, 3832, 8310, 12, 1353, 12, 11834, 655, 5245, 13, 51360], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 445, "seek": 141072, "start": 1430.64, "end": 1432.64, "text": " So the input text, the like coral thing", "tokens": [51360, 407, 264, 4846, 2487, 11, 264, 411, 24955, 551, 51460], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 446, "seek": 141072, "start": 1432.64, "end": 1434.2, "text": " is like what you wanna translate", "tokens": [51460, 307, 411, 437, 291, 1948, 13799, 51538], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 447, "seek": 141072, "start": 1434.2, "end": 1436.76, "text": " and there's a transformer decoder model", "tokens": [51538, 293, 456, 311, 257, 31782, 979, 19866, 2316, 51666], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 448, "seek": 141072, "start": 1436.76, "end": 1438.52, "text": " that then with a tension mechanism", "tokens": [51666, 300, 550, 365, 257, 8980, 7513, 51754], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 449, "seek": 141072, "start": 1438.52, "end": 1440.56, "text": " goes to a transformer decoder model.", "tokens": [51754, 1709, 281, 257, 31782, 979, 19866, 2316, 13, 51856], "temperature": 0.0, "avg_logprob": -0.11584409077962239, "compression_ratio": 1.7887323943661972, "no_speech_prob": 2.3918921215226874e-05}, {"id": 450, "seek": 144056, "start": 1440.56, "end": 1443.08, "text": " And then it decodes autoregressively", "tokens": [50364, 400, 550, 309, 979, 4789, 1476, 418, 3091, 3413, 50490], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 451, "seek": 144056, "start": 1443.08, "end": 1445.9199999999998, "text": " the actual translation, which you can see here in yellow.", "tokens": [50490, 264, 3539, 12853, 11, 597, 291, 393, 536, 510, 294, 5566, 13, 50632], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 452, "seek": 144056, "start": 1446.96, "end": 1450.0, "text": " And so I wanna talk a little bit about like how", "tokens": [50684, 400, 370, 286, 1948, 751, 257, 707, 857, 466, 411, 577, 50836], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 453, "seek": 144056, "start": 1450.0, "end": 1452.6799999999998, "text": " the data looks as we feed it into the models.", "tokens": [50836, 264, 1412, 1542, 382, 321, 3154, 309, 666, 264, 5245, 13, 50970], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 454, "seek": 144056, "start": 1452.6799999999998, "end": 1453.8799999999999, "text": " So there's a few different ways", "tokens": [50970, 407, 456, 311, 257, 1326, 819, 2098, 51030], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 455, "seek": 144056, "start": 1453.8799999999999, "end": 1455.48, "text": " that you might wanna think about data.", "tokens": [51030, 300, 291, 1062, 1948, 519, 466, 1412, 13, 51110], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 456, "seek": 144056, "start": 1455.48, "end": 1458.44, "text": " So you wanna be like, okay, did a human look at it", "tokens": [51110, 407, 291, 1948, 312, 411, 11, 1392, 11, 630, 257, 1952, 574, 412, 309, 51258], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 457, "seek": 144056, "start": 1458.44, "end": 1461.2, "text": " and decide that like these two sentences are translations", "tokens": [51258, 293, 4536, 300, 411, 613, 732, 16579, 366, 37578, 51396], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 458, "seek": 144056, "start": 1461.2, "end": 1462.56, "text": " or are they noisy?", "tokens": [51396, 420, 366, 436, 24518, 30, 51464], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 459, "seek": 144056, "start": 1462.56, "end": 1465.28, "text": " Also, is it limited in size?", "tokens": [51464, 2743, 11, 307, 309, 5567, 294, 2744, 30, 51600], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 460, "seek": 144056, "start": 1465.28, "end": 1466.6799999999998, "text": " Another thing you can think about is like", "tokens": [51600, 3996, 551, 291, 393, 519, 466, 307, 411, 51670], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 461, "seek": 144056, "start": 1466.6799999999998, "end": 1469.6399999999999, "text": " is the data quality dependent on some other factor?", "tokens": [51670, 307, 264, 1412, 3125, 12334, 322, 512, 661, 5952, 30, 51818], "temperature": 0.0, "avg_logprob": -0.10155538199604422, "compression_ratio": 1.7346938775510203, "no_speech_prob": 4.756144335260615e-05}, {"id": 462, "seek": 146964, "start": 1469.64, "end": 1471.2800000000002, "text": " And so that's like the model dependent thing", "tokens": [50364, 400, 370, 300, 311, 411, 264, 2316, 12334, 551, 50446], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 463, "seek": 146964, "start": 1471.2800000000002, "end": 1473.68, "text": " in which case like the data quality may be capped", "tokens": [50446, 294, 597, 1389, 411, 264, 1412, 3125, 815, 312, 1335, 3320, 50566], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 464, "seek": 146964, "start": 1473.68, "end": 1475.3600000000001, "text": " by the quality of that dependency.", "tokens": [50566, 538, 264, 3125, 295, 300, 33621, 13, 50650], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 465, "seek": 146964, "start": 1476.8400000000001, "end": 1478.92, "text": " And so I think you can think a little bit", "tokens": [50724, 400, 370, 286, 519, 291, 393, 519, 257, 707, 857, 50828], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 466, "seek": 146964, "start": 1478.92, "end": 1480.1200000000001, "text": " like the ideal data set.", "tokens": [50828, 411, 264, 7157, 1412, 992, 13, 50888], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 467, "seek": 146964, "start": 1480.1200000000001, "end": 1483.5600000000002, "text": " It would be like humans have reviewed every bit of it.", "tokens": [50888, 467, 576, 312, 411, 6255, 362, 18429, 633, 857, 295, 309, 13, 51060], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 468, "seek": 146964, "start": 1483.5600000000002, "end": 1484.88, "text": " It's not noisy at all.", "tokens": [51060, 467, 311, 406, 24518, 412, 439, 13, 51126], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 469, "seek": 146964, "start": 1484.88, "end": 1486.8000000000002, "text": " We have an infinite amount", "tokens": [51126, 492, 362, 364, 13785, 2372, 51222], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 470, "seek": 146964, "start": 1486.8000000000002, "end": 1489.48, "text": " and it doesn't have any dependencies on any other models.", "tokens": [51222, 293, 309, 1177, 380, 362, 604, 36606, 322, 604, 661, 5245, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 471, "seek": 146964, "start": 1489.48, "end": 1491.24, "text": " It's just like pure quality.", "tokens": [51356, 467, 311, 445, 411, 6075, 3125, 13, 51444], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 472, "seek": 146964, "start": 1491.24, "end": 1494.48, "text": " But in reality, like closer to what we have are these.", "tokens": [51444, 583, 294, 4103, 11, 411, 4966, 281, 437, 321, 362, 366, 613, 13, 51606], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 473, "seek": 146964, "start": 1494.48, "end": 1496.5600000000002, "text": " So we have a bunch of different data sources.", "tokens": [51606, 407, 321, 362, 257, 3840, 295, 819, 1412, 7139, 13, 51710], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 474, "seek": 146964, "start": 1496.5600000000002, "end": 1498.44, "text": " We have the seed data that I discussed", "tokens": [51710, 492, 362, 264, 8871, 1412, 300, 286, 7152, 51804], "temperature": 0.0, "avg_logprob": -0.10008492305360991, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.47526178706903e-05}, {"id": 475, "seek": 149844, "start": 1498.44, "end": 1500.16, "text": " like way back in the talk", "tokens": [50364, 411, 636, 646, 294, 264, 751, 50450], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 476, "seek": 149844, "start": 1500.16, "end": 1503.2, "text": " where it's a small amount of like really high quality", "tokens": [50450, 689, 309, 311, 257, 1359, 2372, 295, 411, 534, 1090, 3125, 50602], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 477, "seek": 149844, "start": 1503.2, "end": 1504.48, "text": " human aligned data.", "tokens": [50602, 1952, 17962, 1412, 13, 50666], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 478, "seek": 149844, "start": 1504.48, "end": 1506.88, "text": " But the only problem is that it's limited in size.", "tokens": [50666, 583, 264, 787, 1154, 307, 300, 309, 311, 5567, 294, 2744, 13, 50786], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 479, "seek": 149844, "start": 1506.88, "end": 1509.72, "text": " It's like 6,000 sentences per language.", "tokens": [50786, 467, 311, 411, 1386, 11, 1360, 16579, 680, 2856, 13, 50928], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 480, "seek": 149844, "start": 1509.72, "end": 1511.48, "text": " We have the public by text.", "tokens": [50928, 492, 362, 264, 1908, 538, 2487, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 481, "seek": 149844, "start": 1511.48, "end": 1513.68, "text": " So this is data that people have created", "tokens": [51016, 407, 341, 307, 1412, 300, 561, 362, 2942, 51126], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 482, "seek": 149844, "start": 1513.68, "end": 1515.56, "text": " over many years of working in translation.", "tokens": [51126, 670, 867, 924, 295, 1364, 294, 12853, 13, 51220], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 483, "seek": 149844, "start": 1515.56, "end": 1518.24, "text": " You know, you can download it from like the opus corpus", "tokens": [51220, 509, 458, 11, 291, 393, 5484, 309, 490, 411, 264, 999, 301, 1181, 31624, 51354], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 484, "seek": 149844, "start": 1518.24, "end": 1522.3600000000001, "text": " for example, mostly has not been reviewed by humans.", "tokens": [51354, 337, 1365, 11, 5240, 575, 406, 668, 18429, 538, 6255, 13, 51560], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 485, "seek": 149844, "start": 1522.3600000000001, "end": 1524.3600000000001, "text": " So pretty extremely noisy.", "tokens": [51560, 407, 1238, 4664, 24518, 13, 51660], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 486, "seek": 149844, "start": 1524.3600000000001, "end": 1526.88, "text": " In many languages, it's just coming from the Bible.", "tokens": [51660, 682, 867, 8650, 11, 309, 311, 445, 1348, 490, 264, 6544, 13, 51786], "temperature": 0.0, "avg_logprob": -0.10835800597916788, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.000163398843142204}, {"id": 487, "seek": 152688, "start": 1526.88, "end": 1528.7600000000002, "text": " So the size is quite limited.", "tokens": [50364, 407, 264, 2744, 307, 1596, 5567, 13, 50458], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 488, "seek": 152688, "start": 1528.7600000000002, "end": 1530.2800000000002, "text": " You have our mind data.", "tokens": [50458, 509, 362, 527, 1575, 1412, 13, 50534], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 489, "seek": 152688, "start": 1531.5200000000002, "end": 1533.64, "text": " So this is not human aligned either.", "tokens": [50596, 407, 341, 307, 406, 1952, 17962, 2139, 13, 50702], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 490, "seek": 152688, "start": 1534.64, "end": 1537.0800000000002, "text": " And but it does have a model dependency, you know,", "tokens": [50752, 400, 457, 309, 775, 362, 257, 2316, 33621, 11, 291, 458, 11, 50874], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 491, "seek": 152688, "start": 1537.0800000000002, "end": 1539.72, "text": " it's dependent on the quality of the sentence encoders.", "tokens": [50874, 309, 311, 12334, 322, 264, 3125, 295, 264, 8174, 2058, 378, 433, 13, 51006], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 492, "seek": 152688, "start": 1539.72, "end": 1543.72, "text": " And we have two other sources of data from back translation.", "tokens": [51006, 400, 321, 362, 732, 661, 7139, 295, 1412, 490, 646, 12853, 13, 51206], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 493, "seek": 152688, "start": 1543.72, "end": 1545.5200000000002, "text": " So the idea of back translation,", "tokens": [51206, 407, 264, 1558, 295, 646, 12853, 11, 51296], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 494, "seek": 152688, "start": 1545.5200000000002, "end": 1547.24, "text": " it's a model augmentation technique", "tokens": [51296, 309, 311, 257, 2316, 14501, 19631, 6532, 51382], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 495, "seek": 152688, "start": 1547.24, "end": 1549.16, "text": " heavily used in machine translation", "tokens": [51382, 10950, 1143, 294, 3479, 12853, 51478], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 496, "seek": 152688, "start": 1549.16, "end": 1552.48, "text": " where you use a model to produce like pseudo translations", "tokens": [51478, 689, 291, 764, 257, 2316, 281, 5258, 411, 35899, 37578, 51644], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 497, "seek": 152688, "start": 1552.48, "end": 1553.5600000000002, "text": " like silver data.", "tokens": [51644, 411, 8753, 1412, 13, 51698], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 498, "seek": 152688, "start": 1554.4, "end": 1555.96, "text": " And we use two different techniques", "tokens": [51740, 400, 321, 764, 732, 819, 7512, 51818], "temperature": 0.0, "avg_logprob": -0.14318664674836445, "compression_ratio": 1.7924528301886793, "no_speech_prob": 8.090987103059888e-05}, {"id": 499, "seek": 155596, "start": 1555.96, "end": 1557.72, "text": " to produce these back translations", "tokens": [50364, 281, 5258, 613, 646, 37578, 50452], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 500, "seek": 155596, "start": 1557.72, "end": 1560.3600000000001, "text": " that also are dependent on the underlying model", "tokens": [50452, 300, 611, 366, 12334, 322, 264, 14217, 2316, 50584], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 501, "seek": 155596, "start": 1560.3600000000001, "end": 1562.4, "text": " used to make the translations.", "tokens": [50584, 1143, 281, 652, 264, 37578, 13, 50686], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 502, "seek": 155596, "start": 1562.4, "end": 1564.2, "text": " And so this is a picture of like our high level", "tokens": [50686, 400, 370, 341, 307, 257, 3036, 295, 411, 527, 1090, 1496, 50776], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 503, "seek": 155596, "start": 1564.2, "end": 1565.1200000000001, "text": " of different data sources", "tokens": [50776, 295, 819, 1412, 7139, 50822], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 504, "seek": 155596, "start": 1565.1200000000001, "end": 1567.24, "text": " and like how you wanna think about the quality", "tokens": [50822, 293, 411, 577, 291, 1948, 519, 466, 264, 3125, 50928], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 505, "seek": 155596, "start": 1567.24, "end": 1569.0, "text": " and the different axes.", "tokens": [50928, 293, 264, 819, 35387, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 506, "seek": 155596, "start": 1569.0, "end": 1571.04, "text": " And so if we put them all together, what do we get?", "tokens": [51016, 400, 370, 498, 321, 829, 552, 439, 1214, 11, 437, 360, 321, 483, 30, 51118], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 507, "seek": 155596, "start": 1571.04, "end": 1574.8, "text": " So the Y axis here is the number of training pairs", "tokens": [51118, 407, 264, 398, 10298, 510, 307, 264, 1230, 295, 3097, 15494, 51306], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 508, "seek": 155596, "start": 1574.8, "end": 1578.28, "text": " and the X axis here is the language is sorted by resource.", "tokens": [51306, 293, 264, 1783, 10298, 510, 307, 264, 2856, 307, 25462, 538, 7684, 13, 51480], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 509, "seek": 155596, "start": 1578.28, "end": 1580.52, "text": " So you can see like on the left hand side,", "tokens": [51480, 407, 291, 393, 536, 411, 322, 264, 1411, 1011, 1252, 11, 51592], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 510, "seek": 155596, "start": 1580.52, "end": 1582.92, "text": " you have your low resource languages like Wolof", "tokens": [51592, 291, 362, 428, 2295, 7684, 8650, 411, 19925, 2670, 51712], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 511, "seek": 155596, "start": 1582.92, "end": 1584.68, "text": " and on your right hand side,", "tokens": [51712, 293, 322, 428, 558, 1011, 1252, 11, 51800], "temperature": 0.0, "avg_logprob": -0.11791380753753879, "compression_ratio": 1.8430034129692834, "no_speech_prob": 0.00014419190119951963}, {"id": 512, "seek": 158468, "start": 1584.68, "end": 1587.24, "text": " you've got your high resource languages like French.", "tokens": [50364, 291, 600, 658, 428, 1090, 7684, 8650, 411, 5522, 13, 50492], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 513, "seek": 158468, "start": 1587.24, "end": 1589.64, "text": " The peak is English, of course.", "tokens": [50492, 440, 10651, 307, 3669, 11, 295, 1164, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 514, "seek": 158468, "start": 1589.64, "end": 1592.28, "text": " And so if you just look at what's available publicly,", "tokens": [50612, 400, 370, 498, 291, 445, 574, 412, 437, 311, 2435, 14843, 11, 50744], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 515, "seek": 158468, "start": 1592.28, "end": 1594.16, "text": " this is a distribution you get.", "tokens": [50744, 341, 307, 257, 7316, 291, 483, 13, 50838], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 516, "seek": 158468, "start": 1594.16, "end": 1598.1200000000001, "text": " And you'll see like a huge, huge fall off pretty quickly.", "tokens": [50838, 400, 291, 603, 536, 411, 257, 2603, 11, 2603, 2100, 766, 1238, 2661, 13, 51036], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 517, "seek": 158468, "start": 1598.1200000000001, "end": 1601.48, "text": " And then if you add in the data that we have created", "tokens": [51036, 400, 550, 498, 291, 909, 294, 264, 1412, 300, 321, 362, 2942, 51204], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 518, "seek": 158468, "start": 1601.48, "end": 1603.1200000000001, "text": " for mining and back translation,", "tokens": [51204, 337, 15512, 293, 646, 12853, 11, 51286], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 519, "seek": 158468, "start": 1603.1200000000001, "end": 1605.8, "text": " our goal is basically to like make the distribution", "tokens": [51286, 527, 3387, 307, 1936, 281, 411, 652, 264, 7316, 51420], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 520, "seek": 158468, "start": 1605.8, "end": 1607.6000000000001, "text": " a little bit more uniform.", "tokens": [51420, 257, 707, 857, 544, 9452, 13, 51510], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 521, "seek": 158468, "start": 1607.6000000000001, "end": 1611.44, "text": " It's very hard on the extremely low resource side, of course,", "tokens": [51510, 467, 311, 588, 1152, 322, 264, 4664, 2295, 7684, 1252, 11, 295, 1164, 11, 51702], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 522, "seek": 158468, "start": 1611.44, "end": 1613.3200000000002, "text": " but to make it a little bit more uniform", "tokens": [51702, 457, 281, 652, 309, 257, 707, 857, 544, 9452, 51796], "temperature": 0.0, "avg_logprob": -0.11187060552698966, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.00032995734363794327}, {"id": 523, "seek": 161332, "start": 1613.32, "end": 1615.36, "text": " so that you don't just immediately, you know,", "tokens": [50364, 370, 300, 291, 500, 380, 445, 4258, 11, 291, 458, 11, 50466], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 524, "seek": 161332, "start": 1615.36, "end": 1617.36, "text": " overfit on your low resource languages", "tokens": [50466, 670, 6845, 322, 428, 2295, 7684, 8650, 50566], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 525, "seek": 161332, "start": 1617.36, "end": 1619.6, "text": " before you've even seen like three shards", "tokens": [50566, 949, 291, 600, 754, 1612, 411, 1045, 402, 2287, 50678], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 526, "seek": 161332, "start": 1619.6, "end": 1620.6, "text": " of your German data.", "tokens": [50678, 295, 428, 6521, 1412, 13, 50728], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 527, "seek": 161332, "start": 1622.32, "end": 1624.96, "text": " With that kind of data strategy in mind,", "tokens": [50814, 2022, 300, 733, 295, 1412, 5206, 294, 1575, 11, 50946], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 528, "seek": 161332, "start": 1624.96, "end": 1628.4399999999998, "text": " I wanna talk a little bit about mixture of experts.", "tokens": [50946, 286, 1948, 751, 257, 707, 857, 466, 9925, 295, 8572, 13, 51120], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 529, "seek": 161332, "start": 1628.4399999999998, "end": 1631.2, "text": " So this is something that we explored quite aggressively", "tokens": [51120, 407, 341, 307, 746, 300, 321, 24016, 1596, 32024, 51258], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 530, "seek": 161332, "start": 1631.2, "end": 1634.4399999999998, "text": " in the translation space for a number of years.", "tokens": [51258, 294, 264, 12853, 1901, 337, 257, 1230, 295, 924, 13, 51420], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 531, "seek": 161332, "start": 1634.4399999999998, "end": 1636.3999999999999, "text": " You know, we could have this equal conversation", "tokens": [51420, 509, 458, 11, 321, 727, 362, 341, 2681, 3761, 51518], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 532, "seek": 161332, "start": 1636.3999999999999, "end": 1637.76, "text": " about some of the debates going on", "tokens": [51518, 466, 512, 295, 264, 24203, 516, 322, 51586], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 533, "seek": 161332, "start": 1637.76, "end": 1640.36, "text": " on like, do you want sparse or dense architectures", "tokens": [51586, 322, 411, 11, 360, 291, 528, 637, 11668, 420, 18011, 6331, 1303, 51716], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 534, "seek": 161332, "start": 1640.36, "end": 1642.2, "text": " for large language models?", "tokens": [51716, 337, 2416, 2856, 5245, 30, 51808], "temperature": 0.0, "avg_logprob": -0.10620065835806039, "compression_ratio": 1.6375404530744337, "no_speech_prob": 5.9188871091464534e-05}, {"id": 535, "seek": 164220, "start": 1642.2, "end": 1646.0, "text": " But essentially mixture of experts, it enables massive scale", "tokens": [50364, 583, 4476, 9925, 295, 8572, 11, 309, 17077, 5994, 4373, 50554], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 536, "seek": 164220, "start": 1646.0, "end": 1648.04, "text": " because you don't have to just scale", "tokens": [50554, 570, 291, 500, 380, 362, 281, 445, 4373, 50656], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 537, "seek": 164220, "start": 1648.04, "end": 1650.56, "text": " like you're kind of your dense trunk model,", "tokens": [50656, 411, 291, 434, 733, 295, 428, 18011, 19849, 2316, 11, 50782], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 538, "seek": 164220, "start": 1650.56, "end": 1653.0, "text": " but you can have like a bunch of different separate experts", "tokens": [50782, 457, 291, 393, 362, 411, 257, 3840, 295, 819, 4994, 8572, 50904], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 539, "seek": 164220, "start": 1653.0, "end": 1654.52, "text": " that you activate per token.", "tokens": [50904, 300, 291, 13615, 680, 14862, 13, 50980], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 540, "seek": 164220, "start": 1656.04, "end": 1658.88, "text": " It also allows you to avoid language interference", "tokens": [51056, 467, 611, 4045, 291, 281, 5042, 2856, 24497, 51198], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 541, "seek": 164220, "start": 1658.88, "end": 1661.0, "text": " because the idea is that the different experts,", "tokens": [51198, 570, 264, 1558, 307, 300, 264, 819, 8572, 11, 51304], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 542, "seek": 164220, "start": 1661.0, "end": 1664.16, "text": " they could specialize to specific languages.", "tokens": [51304, 436, 727, 37938, 281, 2685, 8650, 13, 51462], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 543, "seek": 164220, "start": 1664.16, "end": 1666.32, "text": " Unfortunately, it adds a ton of capacity", "tokens": [51462, 8590, 11, 309, 10860, 257, 2952, 295, 6042, 51570], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 544, "seek": 164220, "start": 1666.32, "end": 1669.1200000000001, "text": " so it becomes pretty easy to overfit.", "tokens": [51570, 370, 309, 3643, 1238, 1858, 281, 670, 6845, 13, 51710], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 545, "seek": 164220, "start": 1669.1200000000001, "end": 1672.16, "text": " So I wanna talk a little bit about this overfitting", "tokens": [51710, 407, 286, 1948, 751, 257, 707, 857, 466, 341, 670, 69, 2414, 51862], "temperature": 0.0, "avg_logprob": -0.09606287909335777, "compression_ratio": 1.7084745762711864, "no_speech_prob": 6.202542863320559e-05}, {"id": 546, "seek": 167216, "start": 1672.16, "end": 1673.0, "text": " phenomenon.", "tokens": [50364, 14029, 13, 50406], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 547, "seek": 167216, "start": 1673.0, "end": 1676.3600000000001, "text": " So the top set of graphs that we're gonna talk about", "tokens": [50406, 407, 264, 1192, 992, 295, 24877, 300, 321, 434, 799, 751, 466, 50574], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 548, "seek": 167216, "start": 1676.3600000000001, "end": 1678.96, "text": " is for the language Congo", "tokens": [50574, 307, 337, 264, 2856, 42839, 50704], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 549, "seek": 167216, "start": 1678.96, "end": 1681.88, "text": " and then the bottom set of languages is French.", "tokens": [50704, 293, 550, 264, 2767, 992, 295, 8650, 307, 5522, 13, 50850], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 550, "seek": 167216, "start": 1681.88, "end": 1684.24, "text": " So you really wanna compare like a low resource language", "tokens": [50850, 407, 291, 534, 1948, 6794, 411, 257, 2295, 7684, 2856, 50968], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 551, "seek": 167216, "start": 1684.24, "end": 1687.1200000000001, "text": " on top with a high resource language on bottom.", "tokens": [50968, 322, 1192, 365, 257, 1090, 7684, 2856, 322, 2767, 13, 51112], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 552, "seek": 167216, "start": 1687.1200000000001, "end": 1688.92, "text": " So if you just take your dense model,", "tokens": [51112, 407, 498, 291, 445, 747, 428, 18011, 2316, 11, 51202], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 553, "seek": 167216, "start": 1688.92, "end": 1691.64, "text": " traditional transformer sequence to sequence architecture,", "tokens": [51202, 5164, 31782, 8310, 281, 8310, 9482, 11, 51338], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 554, "seek": 167216, "start": 1691.64, "end": 1693.72, "text": " that's the graph that you're showing, right?", "tokens": [51338, 300, 311, 264, 4295, 300, 291, 434, 4099, 11, 558, 30, 51442], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 555, "seek": 167216, "start": 1693.72, "end": 1695.5600000000002, "text": " So there's a little bit of overfitting", "tokens": [51442, 407, 456, 311, 257, 707, 857, 295, 670, 69, 2414, 51534], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 556, "seek": 167216, "start": 1695.5600000000002, "end": 1697.0400000000002, "text": " on the low resource language,", "tokens": [51534, 322, 264, 2295, 7684, 2856, 11, 51608], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 557, "seek": 167216, "start": 1697.0400000000002, "end": 1698.8400000000001, "text": " but you can pretty much regularize this", "tokens": [51608, 457, 291, 393, 1238, 709, 3890, 1125, 341, 51698], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 558, "seek": 167216, "start": 1698.8400000000001, "end": 1700.72, "text": " with standard dropout techniques, right?", "tokens": [51698, 365, 3832, 3270, 346, 7512, 11, 558, 30, 51792], "temperature": 0.0, "avg_logprob": -0.1024923184338738, "compression_ratio": 1.7953020134228188, "no_speech_prob": 0.0004725928883999586}, {"id": 559, "seek": 170072, "start": 1700.72, "end": 1703.04, "text": " So there's not a big problem and on French,", "tokens": [50364, 407, 456, 311, 406, 257, 955, 1154, 293, 322, 5522, 11, 50480], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 560, "seek": 170072, "start": 1703.04, "end": 1705.08, "text": " you basically have no real problem.", "tokens": [50480, 291, 1936, 362, 572, 957, 1154, 13, 50582], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 561, "seek": 170072, "start": 1706.16, "end": 1708.92, "text": " However, the minute you switch from like a dense architecture", "tokens": [50636, 2908, 11, 264, 3456, 291, 3679, 490, 411, 257, 18011, 9482, 50774], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 562, "seek": 170072, "start": 1708.92, "end": 1711.6000000000001, "text": " to a token level MOE architecture,", "tokens": [50774, 281, 257, 14862, 1496, 19290, 36, 9482, 11, 50908], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 563, "seek": 170072, "start": 1711.6000000000001, "end": 1714.1200000000001, "text": " you just have experienced a massive overfitting", "tokens": [50908, 291, 445, 362, 6751, 257, 5994, 670, 69, 2414, 51034], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 564, "seek": 170072, "start": 1714.1200000000001, "end": 1715.44, "text": " on the low resource language.", "tokens": [51034, 322, 264, 2295, 7684, 2856, 13, 51100], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 565, "seek": 170072, "start": 1715.44, "end": 1718.44, "text": " So the green line here is like just demonstrating", "tokens": [51100, 407, 264, 3092, 1622, 510, 307, 411, 445, 29889, 51250], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 566, "seek": 170072, "start": 1718.44, "end": 1720.16, "text": " without dropout the overfitting.", "tokens": [51250, 1553, 3270, 346, 264, 670, 69, 2414, 13, 51336], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 567, "seek": 170072, "start": 1720.16, "end": 1722.0, "text": " And then if you add dropout,", "tokens": [51336, 400, 550, 498, 291, 909, 3270, 346, 11, 51428], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 568, "seek": 170072, "start": 1722.0, "end": 1723.8, "text": " you get a little bit better performance,", "tokens": [51428, 291, 483, 257, 707, 857, 1101, 3389, 11, 51518], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 569, "seek": 170072, "start": 1723.8, "end": 1725.88, "text": " but it's still overfitting quite a bit.", "tokens": [51518, 457, 309, 311, 920, 670, 69, 2414, 1596, 257, 857, 13, 51622], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 570, "seek": 170072, "start": 1725.88, "end": 1729.64, "text": " Like essentially by like 12K updates,", "tokens": [51622, 1743, 4476, 538, 411, 2272, 42, 9205, 11, 51810], "temperature": 0.0, "avg_logprob": -0.14048034961407002, "compression_ratio": 1.6496598639455782, "no_speech_prob": 6.601908535230905e-05}, {"id": 571, "seek": 172964, "start": 1729.64, "end": 1731.5600000000002, "text": " there's no real point in continuing training,", "tokens": [50364, 456, 311, 572, 957, 935, 294, 9289, 3097, 11, 50460], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 572, "seek": 172964, "start": 1731.5600000000002, "end": 1733.6000000000001, "text": " like you're burning GPU basically.", "tokens": [50460, 411, 291, 434, 9488, 18407, 1936, 13, 50562], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 573, "seek": 172964, "start": 1734.5200000000002, "end": 1736.76, "text": " And so one of the things we actually worked on quite a bit", "tokens": [50608, 400, 370, 472, 295, 264, 721, 321, 767, 2732, 322, 1596, 257, 857, 50720], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 574, "seek": 172964, "start": 1736.76, "end": 1738.0, "text": " was like trying to figure out", "tokens": [50720, 390, 411, 1382, 281, 2573, 484, 50782], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 575, "seek": 172964, "start": 1738.0, "end": 1741.92, "text": " how to properly regularize these MOE architectures", "tokens": [50782, 577, 281, 6108, 3890, 1125, 613, 19290, 36, 6331, 1303, 50978], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 576, "seek": 172964, "start": 1741.92, "end": 1744.16, "text": " with this specific masking technique", "tokens": [50978, 365, 341, 2685, 31226, 6532, 51090], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 577, "seek": 172964, "start": 1744.16, "end": 1747.5200000000002, "text": " on the gating function that decides like which MOE to route,", "tokens": [51090, 322, 264, 290, 990, 2445, 300, 14898, 411, 597, 19290, 36, 281, 7955, 11, 51258], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 578, "seek": 172964, "start": 1747.5200000000002, "end": 1750.24, "text": " sorry, which expert to route to and your MOE architecture", "tokens": [51258, 2597, 11, 597, 5844, 281, 7955, 281, 293, 428, 19290, 36, 9482, 51394], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 579, "seek": 172964, "start": 1750.24, "end": 1753.4, "text": " to just try to pull back some of this overfitting effect.", "tokens": [51394, 281, 445, 853, 281, 2235, 646, 512, 295, 341, 670, 69, 2414, 1802, 13, 51552], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 580, "seek": 172964, "start": 1753.4, "end": 1757.24, "text": " So if you look in the top right graph, the purple line,", "tokens": [51552, 407, 498, 291, 574, 294, 264, 1192, 558, 4295, 11, 264, 9656, 1622, 11, 51744], "temperature": 0.0, "avg_logprob": -0.12689868230668325, "compression_ratio": 1.6387959866220736, "no_speech_prob": 6.603146175621077e-05}, {"id": 581, "seek": 175724, "start": 1757.44, "end": 1761.64, "text": " you still see some successful regularization.", "tokens": [50374, 291, 920, 536, 512, 4406, 3890, 2144, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 582, "seek": 175724, "start": 1762.96, "end": 1766.8, "text": " Another thing that we did to control the overfitting effect", "tokens": [50650, 3996, 551, 300, 321, 630, 281, 1969, 264, 670, 69, 2414, 1802, 50842], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 583, "seek": 175724, "start": 1766.8, "end": 1769.56, "text": " that's actually quite being used in language models today", "tokens": [50842, 300, 311, 767, 1596, 885, 1143, 294, 2856, 5245, 965, 50980], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 584, "seek": 175724, "start": 1769.56, "end": 1771.88, "text": " as well is curriculum learning.", "tokens": [50980, 382, 731, 307, 14302, 2539, 13, 51096], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 585, "seek": 175724, "start": 1771.88, "end": 1773.72, "text": " And the idea of this is like,", "tokens": [51096, 400, 264, 1558, 295, 341, 307, 411, 11, 51188], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 586, "seek": 175724, "start": 1773.72, "end": 1777.32, "text": " how are we going to stage when languages are introduced?", "tokens": [51188, 577, 366, 321, 516, 281, 3233, 562, 8650, 366, 7268, 30, 51368], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 587, "seek": 175724, "start": 1777.32, "end": 1780.68, "text": " And so what we did was we tried to train a vanilla model", "tokens": [51368, 400, 370, 437, 321, 630, 390, 321, 3031, 281, 3847, 257, 17528, 2316, 51536], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 588, "seek": 175724, "start": 1780.68, "end": 1781.96, "text": " and then we started to measure", "tokens": [51536, 293, 550, 321, 1409, 281, 3481, 51600], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 589, "seek": 175724, "start": 1781.96, "end": 1784.1200000000001, "text": " when the languages begin to overfit.", "tokens": [51600, 562, 264, 8650, 1841, 281, 670, 6845, 13, 51708], "temperature": 0.0, "avg_logprob": -0.10000329859116498, "compression_ratio": 1.7172995780590716, "no_speech_prob": 7.965606346260756e-05}, {"id": 590, "seek": 178412, "start": 1784.12, "end": 1787.6799999999998, "text": " And then we basically bucket them into different sections.", "tokens": [50364, 400, 550, 321, 1936, 13058, 552, 666, 819, 10863, 13, 50542], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 591, "seek": 178412, "start": 1787.6799999999998, "end": 1790.1999999999998, "text": " And so for high resource languages like French,", "tokens": [50542, 400, 370, 337, 1090, 7684, 8650, 411, 5522, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 592, "seek": 178412, "start": 1790.1999999999998, "end": 1791.4399999999998, "text": " you want to start it early", "tokens": [50668, 291, 528, 281, 722, 309, 2440, 50730], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 593, "seek": 178412, "start": 1791.4399999999998, "end": 1793.56, "text": " and it needs to be trained the entire way.", "tokens": [50730, 293, 309, 2203, 281, 312, 8895, 264, 2302, 636, 13, 50836], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 594, "seek": 178412, "start": 1793.56, "end": 1796.3999999999999, "text": " But for a lower resource language like Wolof,", "tokens": [50836, 583, 337, 257, 3126, 7684, 2856, 411, 19925, 2670, 11, 50978], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 595, "seek": 178412, "start": 1796.3999999999999, "end": 1799.4799999999998, "text": " after maybe like a hundred K updates, it's done.", "tokens": [50978, 934, 1310, 411, 257, 3262, 591, 9205, 11, 309, 311, 1096, 13, 51132], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 596, "seek": 178412, "start": 1799.4799999999998, "end": 1801.9199999999998, "text": " So the rest of the time is just overfitting.", "tokens": [51132, 407, 264, 1472, 295, 264, 565, 307, 445, 670, 69, 2414, 13, 51254], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 597, "seek": 178412, "start": 1801.9199999999998, "end": 1803.84, "text": " And so it actually gets worse the more you train it.", "tokens": [51254, 400, 370, 309, 767, 2170, 5324, 264, 544, 291, 3847, 309, 13, 51350], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 598, "seek": 178412, "start": 1803.84, "end": 1806.0, "text": " So what we did is we moved some of those lower resource", "tokens": [51350, 407, 437, 321, 630, 307, 321, 4259, 512, 295, 729, 3126, 7684, 51458], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 599, "seek": 178412, "start": 1806.0, "end": 1808.4799999999998, "text": " languages and we inserted them much later", "tokens": [51458, 8650, 293, 321, 27992, 552, 709, 1780, 51582], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 600, "seek": 178412, "start": 1808.4799999999998, "end": 1809.9199999999998, "text": " into the training schedule.", "tokens": [51582, 666, 264, 3097, 7567, 13, 51654], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 601, "seek": 178412, "start": 1809.9199999999998, "end": 1811.6, "text": " So you start training your high resource,", "tokens": [51654, 407, 291, 722, 3097, 428, 1090, 7684, 11, 51738], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 602, "seek": 178412, "start": 1811.6, "end": 1813.6399999999999, "text": " then you start training your low, your mid resource,", "tokens": [51738, 550, 291, 722, 3097, 428, 2295, 11, 428, 2062, 7684, 11, 51840], "temperature": 0.0, "avg_logprob": -0.11433577537536621, "compression_ratio": 1.9155844155844155, "no_speech_prob": 0.00022687185264658183}, {"id": 603, "seek": 181364, "start": 1813.64, "end": 1816.0800000000002, "text": " and then your low resource, and then your very low resource.", "tokens": [50364, 293, 550, 428, 2295, 7684, 11, 293, 550, 428, 588, 2295, 7684, 13, 50486], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 604, "seek": 181364, "start": 1816.0800000000002, "end": 1819.44, "text": " And so by the end, everything in theory has trained", "tokens": [50486, 400, 370, 538, 264, 917, 11, 1203, 294, 5261, 575, 8895, 50654], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 605, "seek": 181364, "start": 1819.44, "end": 1821.5600000000002, "text": " and is not as overfit as it would be", "tokens": [50654, 293, 307, 406, 382, 670, 6845, 382, 309, 576, 312, 50760], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 606, "seek": 181364, "start": 1821.5600000000002, "end": 1823.0400000000002, "text": " without this kind of technique.", "tokens": [50760, 1553, 341, 733, 295, 6532, 13, 50834], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 607, "seek": 181364, "start": 1824.1200000000001, "end": 1825.6000000000001, "text": " So I want to show some results.", "tokens": [50888, 407, 286, 528, 281, 855, 512, 3542, 13, 50962], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 608, "seek": 181364, "start": 1825.6000000000001, "end": 1828.88, "text": " So first I want to show results on existing datasets.", "tokens": [50962, 407, 700, 286, 528, 281, 855, 3542, 322, 6741, 42856, 13, 51126], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 609, "seek": 181364, "start": 1828.88, "end": 1830.8000000000002, "text": " So before we get to 200 languages,", "tokens": [51126, 407, 949, 321, 483, 281, 2331, 8650, 11, 51222], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 610, "seek": 181364, "start": 1830.8000000000002, "end": 1833.0800000000002, "text": " like let's just talk about 100 languages.", "tokens": [51222, 411, 718, 311, 445, 751, 466, 2319, 8650, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 611, "seek": 181364, "start": 1833.0800000000002, "end": 1835.8000000000002, "text": " And so this is the Flores 101 DevTest.", "tokens": [51336, 400, 370, 341, 307, 264, 3235, 2706, 21055, 9096, 51, 377, 13, 51472], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 612, "seek": 181364, "start": 1835.8000000000002, "end": 1837.3600000000001, "text": " It's important to compare to this", "tokens": [51472, 467, 311, 1021, 281, 6794, 281, 341, 51550], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 613, "seek": 181364, "start": 1837.3600000000001, "end": 1839.72, "text": " because this is where like existing benchmarks", "tokens": [51550, 570, 341, 307, 689, 411, 6741, 43751, 51668], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 614, "seek": 181364, "start": 1839.72, "end": 1841.3200000000002, "text": " in the community lie.", "tokens": [51668, 294, 264, 1768, 4544, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 615, "seek": 181364, "start": 1841.3200000000002, "end": 1843.6000000000001, "text": " Whereas on 200, of course, we can put up anything.", "tokens": [51748, 13813, 322, 2331, 11, 295, 1164, 11, 321, 393, 829, 493, 1340, 13, 51862], "temperature": 0.0, "avg_logprob": -0.12740345258970517, "compression_ratio": 1.7606557377049181, "no_speech_prob": 4.830706166103482e-05}, {"id": 616, "seek": 184360, "start": 1844.48, "end": 1846.24, "text": " Because it's the first work on that.", "tokens": [50408, 1436, 309, 311, 264, 700, 589, 322, 300, 13, 50496], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 617, "seek": 184360, "start": 1846.24, "end": 1851.12, "text": " So the first column is translating out of English.", "tokens": [50496, 407, 264, 700, 7738, 307, 35030, 484, 295, 3669, 13, 50740], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 618, "seek": 184360, "start": 1851.12, "end": 1854.84, "text": " So English to Chinese, English to Icelandic, anything like that.", "tokens": [50740, 407, 3669, 281, 4649, 11, 3669, 281, 28004, 299, 11, 1340, 411, 300, 13, 50926], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 619, "seek": 184360, "start": 1854.84, "end": 1857.32, "text": " The second column is translating into English.", "tokens": [50926, 440, 1150, 7738, 307, 35030, 666, 3669, 13, 51050], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 620, "seek": 184360, "start": 1857.32, "end": 1858.8799999999999, "text": " So Chinese to English.", "tokens": [51050, 407, 4649, 281, 3669, 13, 51128], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 621, "seek": 184360, "start": 1858.8799999999999, "end": 1863.1599999999999, "text": " The third column, XXYY, it's translating any cross pair", "tokens": [51128, 440, 2636, 7738, 11, 27050, 40570, 11, 309, 311, 35030, 604, 3278, 6119, 51342], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 622, "seek": 184360, "start": 1863.1599999999999, "end": 1864.6399999999999, "text": " are not involving English.", "tokens": [51342, 366, 406, 17030, 3669, 13, 51416], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 623, "seek": 184360, "start": 1864.6399999999999, "end": 1866.7199999999998, "text": " And the last column is the average.", "tokens": [51416, 400, 264, 1036, 7738, 307, 264, 4274, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 624, "seek": 184360, "start": 1866.7199999999998, "end": 1868.7199999999998, "text": " So if you look at the first set of rows,", "tokens": [51520, 407, 498, 291, 574, 412, 264, 700, 992, 295, 13241, 11, 51620], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 625, "seek": 184360, "start": 1868.7199999999998, "end": 1872.4399999999998, "text": " this is a comparison on models that cover 87 different languages.", "tokens": [51620, 341, 307, 257, 9660, 322, 5245, 300, 2060, 27990, 819, 8650, 13, 51806], "temperature": 0.0, "avg_logprob": -0.13515765850360578, "compression_ratio": 1.8436213991769548, "no_speech_prob": 0.00036796729546040297}, {"id": 626, "seek": 187244, "start": 1872.44, "end": 1874.72, "text": " So there was this paper MTAM 100.", "tokens": [50364, 407, 456, 390, 341, 3035, 37333, 2865, 2319, 13, 50478], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 627, "seek": 187244, "start": 1874.72, "end": 1876.68, "text": " There was also this deep net paper.", "tokens": [50478, 821, 390, 611, 341, 2452, 2533, 3035, 13, 50576], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 628, "seek": 187244, "start": 1876.68, "end": 1878.8400000000001, "text": " So you can see the average blue score.", "tokens": [50576, 407, 291, 393, 536, 264, 4274, 3344, 6175, 13, 50684], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 629, "seek": 187244, "start": 1878.8400000000001, "end": 1880.88, "text": " Blue is a standard translation metric,", "tokens": [50684, 8510, 307, 257, 3832, 12853, 20678, 11, 50786], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 630, "seek": 187244, "start": 1880.88, "end": 1883.6000000000001, "text": " essentially a metric of word overlap.", "tokens": [50786, 4476, 257, 20678, 295, 1349, 19959, 13, 50922], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 631, "seek": 187244, "start": 1883.6000000000001, "end": 1886.04, "text": " So we're looking at blue score here.", "tokens": [50922, 407, 321, 434, 1237, 412, 3344, 6175, 510, 13, 51044], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 632, "seek": 187244, "start": 1886.04, "end": 1889.52, "text": " And so you can see the last row NLB 200.", "tokens": [51044, 400, 370, 291, 393, 536, 264, 1036, 5386, 426, 43, 33, 2331, 13, 51218], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 633, "seek": 187244, "start": 1889.52, "end": 1891.68, "text": " Even though we cover 200 languages,", "tokens": [51218, 2754, 1673, 321, 2060, 2331, 8650, 11, 51326], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 634, "seek": 187244, "start": 1891.68, "end": 1893.92, "text": " the blue score is substantially above", "tokens": [51326, 264, 3344, 6175, 307, 30797, 3673, 51438], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 635, "seek": 187244, "start": 1893.92, "end": 1895.3600000000001, "text": " some of the existing work.", "tokens": [51438, 512, 295, 264, 6741, 589, 13, 51510], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 636, "seek": 187244, "start": 1895.3600000000001, "end": 1897.48, "text": " Now, if we look at 101 languages,", "tokens": [51510, 823, 11, 498, 321, 574, 412, 21055, 8650, 11, 51616], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 637, "seek": 187244, "start": 1897.48, "end": 1900.3200000000002, "text": " only the Delta LM paper from Microsoft at the time", "tokens": [51616, 787, 264, 18183, 46529, 3035, 490, 8116, 412, 264, 565, 51758], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 638, "seek": 187244, "start": 1900.3200000000002, "end": 1902.28, "text": " covered that number of languages.", "tokens": [51758, 5343, 300, 1230, 295, 8650, 13, 51856], "temperature": 0.0, "avg_logprob": -0.12533950805664062, "compression_ratio": 1.725, "no_speech_prob": 0.000269273150479421}, {"id": 639, "seek": 190228, "start": 1902.28, "end": 1905.72, "text": " And so if you compare on all of the different cross sets,", "tokens": [50364, 400, 370, 498, 291, 6794, 322, 439, 295, 264, 819, 3278, 6352, 11, 50536], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 640, "seek": 190228, "start": 1905.72, "end": 1909.24, "text": " similarly, you see that there's no language left behind model", "tokens": [50536, 14138, 11, 291, 536, 300, 456, 311, 572, 2856, 1411, 2261, 2316, 50712], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 641, "seek": 190228, "start": 1909.24, "end": 1911.84, "text": " is much stronger in terms of blue.", "tokens": [50712, 307, 709, 7249, 294, 2115, 295, 3344, 13, 50842], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 642, "seek": 190228, "start": 1911.84, "end": 1914.6, "text": " One thing really quick on the variance of these blue numbers,", "tokens": [50842, 1485, 551, 534, 1702, 322, 264, 21977, 295, 613, 3344, 3547, 11, 50980], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 643, "seek": 190228, "start": 1914.6, "end": 1916.3999999999999, "text": " I think it's important to understand", "tokens": [50980, 286, 519, 309, 311, 1021, 281, 1223, 51070], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 644, "seek": 190228, "start": 1916.3999999999999, "end": 1918.3999999999999, "text": " is something statistically significant or not.", "tokens": [51070, 307, 746, 36478, 4776, 420, 406, 13, 51170], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 645, "seek": 190228, "start": 1918.3999999999999, "end": 1923.3999999999999, "text": " I think about 0.5 blue is kind of like the general plus", "tokens": [51170, 286, 519, 466, 1958, 13, 20, 3344, 307, 733, 295, 411, 264, 2674, 1804, 51420], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 646, "seek": 190228, "start": 1923.3999999999999, "end": 1924.52, "text": " minus that you'll see.", "tokens": [51420, 3175, 300, 291, 603, 536, 13, 51476], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 647, "seek": 190228, "start": 1924.52, "end": 1927.84, "text": " And so if it's above that, it's usually", "tokens": [51476, 400, 370, 498, 309, 311, 3673, 300, 11, 309, 311, 2673, 51642], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 648, "seek": 190228, "start": 1927.84, "end": 1931.48, "text": " a statistically significant metric improvement.", "tokens": [51642, 257, 36478, 4776, 20678, 10444, 13, 51824], "temperature": 0.0, "avg_logprob": -0.15684556152860998, "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.00041051997686736286}, {"id": 649, "seek": 193148, "start": 1931.68, "end": 1934.84, "text": " So now I want to talk a little bit about Flora's 200 results.", "tokens": [50374, 407, 586, 286, 528, 281, 751, 257, 707, 857, 466, 3235, 3252, 311, 2331, 3542, 13, 50532], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 650, "seek": 193148, "start": 1934.84, "end": 1937.4, "text": " So here's similar, like the first chunk of columns", "tokens": [50532, 407, 510, 311, 2531, 11, 411, 264, 700, 16635, 295, 13766, 50660], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 651, "seek": 193148, "start": 1937.4, "end": 1939.32, "text": " translating out of English, then", "tokens": [50660, 35030, 484, 295, 3669, 11, 550, 50756], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 652, "seek": 193148, "start": 1939.32, "end": 1941.44, "text": " next chunk is translating into English,", "tokens": [50756, 958, 16635, 307, 35030, 666, 3669, 11, 50862], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 653, "seek": 193148, "start": 1941.44, "end": 1945.56, "text": " then you have your cross pairs, and then you have your average.", "tokens": [50862, 550, 291, 362, 428, 3278, 15494, 11, 293, 550, 291, 362, 428, 4274, 13, 51068], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 654, "seek": 193148, "start": 1945.56, "end": 1947.88, "text": " So we have this blue metric as well.", "tokens": [51068, 407, 321, 362, 341, 3344, 20678, 382, 731, 13, 51184], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 655, "seek": 193148, "start": 1947.88, "end": 1953.2, "text": " We also have a character level metric based on CHRF++", "tokens": [51184, 492, 611, 362, 257, 2517, 1496, 20678, 2361, 322, 5995, 49, 37, 25472, 51450], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 656, "seek": 193148, "start": 1953.2, "end": 1955.64, "text": " that's commonly used in the translation community.", "tokens": [51450, 300, 311, 12719, 1143, 294, 264, 12853, 1768, 13, 51572], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 657, "seek": 193148, "start": 1955.64, "end": 1957.44, "text": " So I think looking at these numbers, of course,", "tokens": [51572, 407, 286, 519, 1237, 412, 613, 3547, 11, 295, 1164, 11, 51662], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 658, "seek": 193148, "start": 1957.44, "end": 1960.88, "text": " there's no baseline work to compare to on the previous slide.", "tokens": [51662, 456, 311, 572, 20518, 589, 281, 6794, 281, 322, 264, 3894, 4137, 13, 51834], "temperature": 0.0, "avg_logprob": -0.19618249849508737, "compression_ratio": 1.7216494845360826, "no_speech_prob": 0.0005190797965042293}, {"id": 659, "seek": 196088, "start": 1960.92, "end": 1963.92, "text": " And so when we get to human evaluation in a little bit,", "tokens": [50366, 400, 370, 562, 321, 483, 281, 1952, 13344, 294, 257, 707, 857, 11, 50516], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 660, "seek": 196088, "start": 1963.92, "end": 1965.5600000000002, "text": " it'll be more concrete.", "tokens": [50516, 309, 603, 312, 544, 9859, 13, 50598], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 661, "seek": 196088, "start": 1965.5600000000002, "end": 1968.96, "text": " But I think generally one of the rules of thumb", "tokens": [50598, 583, 286, 519, 5101, 472, 295, 264, 4474, 295, 9298, 50768], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 662, "seek": 196088, "start": 1968.96, "end": 1971.92, "text": " I have for these types of numbers is around 30", "tokens": [50768, 286, 362, 337, 613, 3467, 295, 3547, 307, 926, 2217, 50916], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 663, "seek": 196088, "start": 1971.92, "end": 1977.0, "text": " is pretty reasonably becomes usable.", "tokens": [50916, 307, 1238, 23551, 3643, 29975, 13, 51170], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 664, "seek": 196088, "start": 1977.0, "end": 1980.96, "text": " And I think another thing, if you compare these supervised pairs", "tokens": [51170, 400, 286, 519, 1071, 551, 11, 498, 291, 6794, 613, 46533, 15494, 51368], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 665, "seek": 196088, "start": 1980.96, "end": 1984.6000000000001, "text": " to zero shot pairs, I think we don't see a huge drop-off", "tokens": [51368, 281, 4018, 3347, 15494, 11, 286, 519, 321, 500, 380, 536, 257, 2603, 3270, 12, 4506, 51550], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 666, "seek": 196088, "start": 1984.6000000000001, "end": 1986.7600000000002, "text": " on zero shot, which indicates the model has", "tokens": [51550, 322, 4018, 3347, 11, 597, 16203, 264, 2316, 575, 51658], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 667, "seek": 196088, "start": 1986.7600000000002, "end": 1989.2800000000002, "text": " some sort of generalization, even if it didn't see", "tokens": [51658, 512, 1333, 295, 2674, 2144, 11, 754, 498, 309, 994, 380, 536, 51784], "temperature": 0.0, "avg_logprob": -0.16055051903975637, "compression_ratio": 1.6090225563909775, "no_speech_prob": 6.012751691741869e-05}, {"id": 668, "seek": 198928, "start": 1989.28, "end": 1992.84, "text": " that translation pair directly during training.", "tokens": [50364, 300, 12853, 6119, 3838, 1830, 3097, 13, 50542], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 669, "seek": 198928, "start": 1992.84, "end": 1994.76, "text": " Another way to calibrate some of this", "tokens": [50542, 3996, 636, 281, 21583, 4404, 512, 295, 341, 50638], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 670, "seek": 198928, "start": 1994.76, "end": 1997.0, "text": " is to compare to Google Translate.", "tokens": [50638, 307, 281, 6794, 281, 3329, 6531, 17593, 13, 50750], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 671, "seek": 198928, "start": 1997.0, "end": 1999.32, "text": " And so if you compare to Google Translate,", "tokens": [50750, 400, 370, 498, 291, 6794, 281, 3329, 6531, 17593, 11, 50866], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 672, "seek": 198928, "start": 1999.32, "end": 2001.36, "text": " no language to left behind is quite a bit better", "tokens": [50866, 572, 2856, 281, 1411, 2261, 307, 1596, 257, 857, 1101, 50968], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 673, "seek": 198928, "start": 2001.36, "end": 2005.08, "text": " at translating into English and not as good as translating", "tokens": [50968, 412, 35030, 666, 3669, 293, 406, 382, 665, 382, 35030, 51154], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 674, "seek": 198928, "start": 2005.08, "end": 2008.24, "text": " out of English, although if you like average across everything,", "tokens": [51154, 484, 295, 3669, 11, 4878, 498, 291, 411, 4274, 2108, 1203, 11, 51312], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 675, "seek": 198928, "start": 2008.24, "end": 2011.6, "text": " it's a little bit better.", "tokens": [51312, 309, 311, 257, 707, 857, 1101, 13, 51480], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 676, "seek": 198928, "start": 2011.6, "end": 2014.24, "text": " I want to talk a little bit about human evaluation as well", "tokens": [51480, 286, 528, 281, 751, 257, 707, 857, 466, 1952, 13344, 382, 731, 51612], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 677, "seek": 198928, "start": 2014.24, "end": 2016.76, "text": " to complement some of our discussion", "tokens": [51612, 281, 17103, 512, 295, 527, 5017, 51738], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 678, "seek": 198928, "start": 2016.76, "end": 2018.8, "text": " on automatic evaluation.", "tokens": [51738, 322, 12509, 13344, 13, 51840], "temperature": 0.0, "avg_logprob": -0.15134780689821406, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.0003918188449461013}, {"id": 679, "seek": 201880, "start": 2018.8, "end": 2022.28, "text": " And so I think automatic metrics fast, really good", "tokens": [50364, 400, 370, 286, 519, 12509, 16367, 2370, 11, 534, 665, 50538], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 680, "seek": 201880, "start": 2022.28, "end": 2024.72, "text": " for research and duration, impossible to move forward", "tokens": [50538, 337, 2132, 293, 16365, 11, 6243, 281, 1286, 2128, 50660], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 681, "seek": 201880, "start": 2024.72, "end": 2029.3999999999999, "text": " without, but human evaluation is really the real deal here.", "tokens": [50660, 1553, 11, 457, 1952, 13344, 307, 534, 264, 957, 2028, 510, 13, 50894], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 682, "seek": 201880, "start": 2029.3999999999999, "end": 2032.0, "text": " And so we had this paper at Amptox", "tokens": [50894, 400, 370, 321, 632, 341, 3035, 412, 2012, 662, 5230, 51024], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 683, "seek": 201880, "start": 2032.0, "end": 2034.8, "text": " on how to make this human evaluation very consistent", "tokens": [51024, 322, 577, 281, 652, 341, 1952, 13344, 588, 8398, 51164], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 684, "seek": 201880, "start": 2034.8, "end": 2037.6, "text": " and scalable across different language pairs.", "tokens": [51164, 293, 38481, 2108, 819, 2856, 15494, 13, 51304], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 685, "seek": 201880, "start": 2037.6, "end": 2040.68, "text": " I think this goes back to the kind of evaluation data set", "tokens": [51304, 286, 519, 341, 1709, 646, 281, 264, 733, 295, 13344, 1412, 992, 51458], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 686, "seek": 201880, "start": 2040.68, "end": 2043.48, "text": " point that I was making at the beginning of the talk, where", "tokens": [51458, 935, 300, 286, 390, 1455, 412, 264, 2863, 295, 264, 751, 11, 689, 51598], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 687, "seek": 201880, "start": 2043.48, "end": 2046.0, "text": " if you're a professional German translator,", "tokens": [51598, 498, 291, 434, 257, 4843, 6521, 35223, 11, 51724], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 688, "seek": 201880, "start": 2046.0, "end": 2048.4, "text": " you're really good at evaluating the quality of your German", "tokens": [51724, 291, 434, 534, 665, 412, 27479, 264, 3125, 295, 428, 6521, 51844], "temperature": 0.0, "avg_logprob": -0.17323945208293637, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00021973770344629884}, {"id": 689, "seek": 204840, "start": 2048.44, "end": 2049.96, "text": " translation.", "tokens": [50366, 12853, 13, 50442], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 690, "seek": 204840, "start": 2049.96, "end": 2053.96, "text": " But beyond that, there's not a lot of consistency.", "tokens": [50442, 583, 4399, 300, 11, 456, 311, 406, 257, 688, 295, 14416, 13, 50642], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 691, "seek": 204840, "start": 2053.96, "end": 2058.28, "text": " And if you evaluate translation on a five point scale,", "tokens": [50642, 400, 498, 291, 13059, 12853, 322, 257, 1732, 935, 4373, 11, 50858], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 692, "seek": 204840, "start": 2058.28, "end": 2060.88, "text": " a five translating between two languages", "tokens": [50858, 257, 1732, 35030, 1296, 732, 8650, 50988], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 693, "seek": 204840, "start": 2060.88, "end": 2063.44, "text": " and a three translating between other two languages,", "tokens": [50988, 293, 257, 1045, 35030, 1296, 661, 732, 8650, 11, 51116], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 694, "seek": 204840, "start": 2063.44, "end": 2064.96, "text": " are those really comparable?", "tokens": [51116, 366, 729, 534, 25323, 30, 51192], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 695, "seek": 204840, "start": 2064.96, "end": 2067.7200000000003, "text": " And so we had this entire experiment methodology", "tokens": [51192, 400, 370, 321, 632, 341, 2302, 5120, 24850, 51330], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 696, "seek": 204840, "start": 2067.7200000000003, "end": 2070.6, "text": " on how we might want to make this a little bit more", "tokens": [51330, 322, 577, 321, 1062, 528, 281, 652, 341, 257, 707, 857, 544, 51474], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 697, "seek": 204840, "start": 2070.6, "end": 2072.32, "text": " comparable.", "tokens": [51474, 25323, 13, 51560], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 698, "seek": 204840, "start": 2072.32, "end": 2075.36, "text": " So I want to show some results now on this.", "tokens": [51560, 407, 286, 528, 281, 855, 512, 3542, 586, 322, 341, 13, 51712], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 699, "seek": 204840, "start": 2075.36, "end": 2077.6800000000003, "text": " So the y-axis here, so the metric", "tokens": [51712, 407, 264, 288, 12, 24633, 510, 11, 370, 264, 20678, 51828], "temperature": 0.0, "avg_logprob": -0.19141762597220285, "compression_ratio": 1.728, "no_speech_prob": 0.0013020881451666355}, {"id": 700, "seek": 207768, "start": 2077.72, "end": 2080.3599999999997, "text": " is called XSTS, some metric for how", "tokens": [50366, 307, 1219, 1783, 6840, 50, 11, 512, 20678, 337, 577, 50498], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 701, "seek": 207768, "start": 2080.3599999999997, "end": 2082.2799999999997, "text": " we're doing this human evaluation.", "tokens": [50498, 321, 434, 884, 341, 1952, 13344, 13, 50594], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 702, "seek": 207768, "start": 2082.2799999999997, "end": 2085.68, "text": " The y-axis here is actually the delta.", "tokens": [50594, 440, 288, 12, 24633, 510, 307, 767, 264, 8289, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 703, "seek": 207768, "start": 2085.68, "end": 2088.6, "text": " So anything is a five point scale.", "tokens": [50764, 407, 1340, 307, 257, 1732, 935, 4373, 13, 50910], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 704, "seek": 207768, "start": 2088.6, "end": 2092.08, "text": " So it's a delta, not the raw score.", "tokens": [50910, 407, 309, 311, 257, 8289, 11, 406, 264, 8936, 6175, 13, 51084], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 705, "seek": 207768, "start": 2092.08, "end": 2095.24, "text": " The x-axis here is a bunch of different translation directions", "tokens": [51084, 440, 2031, 12, 24633, 510, 307, 257, 3840, 295, 819, 12853, 11095, 51242], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 706, "seek": 207768, "start": 2095.24, "end": 2096.68, "text": " that we evaluated.", "tokens": [51242, 300, 321, 25509, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 707, "seek": 207768, "start": 2096.68, "end": 2099.8799999999997, "text": " So the gray set is translating into English.", "tokens": [51314, 407, 264, 10855, 992, 307, 35030, 666, 3669, 13, 51474], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 708, "seek": 207768, "start": 2099.8799999999997, "end": 2103.3199999999997, "text": " The green set is translating non-English directions,", "tokens": [51474, 440, 3092, 992, 307, 35030, 2107, 12, 31254, 1933, 11095, 11, 51646], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 709, "seek": 207768, "start": 2103.3199999999997, "end": 2107.12, "text": " so like French to Oluf.", "tokens": [51646, 370, 411, 5522, 281, 422, 2781, 69, 13, 51836], "temperature": 0.0, "avg_logprob": -0.16821045534951345, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0007307048072107136}, {"id": 710, "seek": 210712, "start": 2107.12, "end": 2110.04, "text": " And then the blue set is translating out of English.", "tokens": [50364, 400, 550, 264, 3344, 992, 307, 35030, 484, 295, 3669, 13, 50510], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 711, "seek": 210712, "start": 2110.04, "end": 2113.4, "text": " And so what you're looking for is like a positive delta", "tokens": [50510, 400, 370, 437, 291, 434, 1237, 337, 307, 411, 257, 3353, 8289, 50678], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 712, "seek": 210712, "start": 2113.4, "end": 2117.12, "text": " indicates that our modeling architecture is much better.", "tokens": [50678, 16203, 300, 527, 15983, 9482, 307, 709, 1101, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 713, "seek": 210712, "start": 2117.12, "end": 2121.64, "text": " So what the delta is between is like a baseline transformer", "tokens": [50864, 407, 437, 264, 8289, 307, 1296, 307, 411, 257, 20518, 31782, 51090], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 714, "seek": 210712, "start": 2121.64, "end": 2124.08, "text": " model just trained on all of our data", "tokens": [51090, 2316, 445, 8895, 322, 439, 295, 527, 1412, 51212], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 715, "seek": 210712, "start": 2124.08, "end": 2126.44, "text": " versus like the final no language left behind model", "tokens": [51212, 5717, 411, 264, 2572, 572, 2856, 1411, 2261, 2316, 51330], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 716, "seek": 210712, "start": 2126.44, "end": 2127.44, "text": " that we created.", "tokens": [51330, 300, 321, 2942, 13, 51380], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 717, "seek": 210712, "start": 2127.44, "end": 2129.68, "text": " So the data is actually the same for both of them.", "tokens": [51380, 407, 264, 1412, 307, 767, 264, 912, 337, 1293, 295, 552, 13, 51492], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 718, "seek": 210712, "start": 2129.68, "end": 2132.16, "text": " That's how we get all 200 languages.", "tokens": [51492, 663, 311, 577, 321, 483, 439, 2331, 8650, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 719, "seek": 210712, "start": 2132.16, "end": 2134.3199999999997, "text": " So we're just measuring here the human eval", "tokens": [51616, 407, 321, 434, 445, 13389, 510, 264, 1952, 1073, 304, 51724], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 720, "seek": 210712, "start": 2134.3199999999997, "end": 2136.3199999999997, "text": " of the modeling improvements.", "tokens": [51724, 295, 264, 15983, 13797, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09795441935139318, "compression_ratio": 1.75177304964539, "no_speech_prob": 0.0004873922443948686}, {"id": 721, "seek": 213632, "start": 2136.32, "end": 2141.4, "text": " As you can see, most of the delta is pretty noticeable.", "tokens": [50364, 1018, 291, 393, 536, 11, 881, 295, 264, 8289, 307, 1238, 26041, 13, 50618], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 722, "seek": 213632, "start": 2141.4, "end": 2147.0, "text": " Some of them not so much like, I don't know, Zulu to English.", "tokens": [50618, 2188, 295, 552, 406, 370, 709, 411, 11, 286, 500, 380, 458, 11, 1176, 12845, 281, 3669, 13, 50898], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 723, "seek": 213632, "start": 2147.0, "end": 2148.48, "text": " We didn't seem to improve very much,", "tokens": [50898, 492, 994, 380, 1643, 281, 3470, 588, 709, 11, 50972], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 724, "seek": 213632, "start": 2148.48, "end": 2151.04, "text": " but in general, it's an improvement detectable", "tokens": [50972, 457, 294, 2674, 11, 309, 311, 364, 10444, 5531, 712, 51100], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 725, "seek": 213632, "start": 2151.04, "end": 2152.52, "text": " by human evaluation.", "tokens": [51100, 538, 1952, 13344, 13, 51174], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 726, "seek": 213632, "start": 2152.52, "end": 2154.6800000000003, "text": " You might also ask, OK, what is the statistically", "tokens": [51174, 509, 1062, 611, 1029, 11, 2264, 11, 437, 307, 264, 36478, 51282], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 727, "seek": 213632, "start": 2154.6800000000003, "end": 2159.2000000000003, "text": " significant difference here between about 0.2 to 0.3", "tokens": [51282, 4776, 2649, 510, 1296, 466, 1958, 13, 17, 281, 1958, 13, 18, 51508], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 728, "seek": 213632, "start": 2159.2000000000003, "end": 2162.6400000000003, "text": " plus or minus is something that's pretty noticeable.", "tokens": [51508, 1804, 420, 3175, 307, 746, 300, 311, 1238, 26041, 13, 51680], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 729, "seek": 213632, "start": 2162.6400000000003, "end": 2165.2400000000002, "text": " And above 0.5, it's very noticeable.", "tokens": [51680, 400, 3673, 1958, 13, 20, 11, 309, 311, 588, 26041, 13, 51810], "temperature": 0.0, "avg_logprob": -0.20161041846642128, "compression_ratio": 1.5485074626865671, "no_speech_prob": 0.00011055796494474635}, {"id": 730, "seek": 216632, "start": 2167.04, "end": 2170.48, "text": " One of the things that I also want to get at in evaluation", "tokens": [50400, 1485, 295, 264, 721, 300, 286, 611, 528, 281, 483, 412, 294, 13344, 50572], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 731, "seek": 216632, "start": 2170.48, "end": 2175.6800000000003, "text": " is that there's many different facets of model evaluation.", "tokens": [50572, 307, 300, 456, 311, 867, 819, 49752, 295, 2316, 13344, 13, 50832], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 732, "seek": 216632, "start": 2175.6800000000003, "end": 2178.36, "text": " And I think if you look at all of the different LLM leader", "tokens": [50832, 400, 286, 519, 498, 291, 574, 412, 439, 295, 264, 819, 441, 43, 44, 5263, 50966], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 733, "seek": 216632, "start": 2178.36, "end": 2180.8, "text": " boards or the transparency reports or whatever,", "tokens": [50966, 13293, 420, 264, 17131, 7122, 420, 2035, 11, 51088], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 734, "seek": 216632, "start": 2180.8, "end": 2183.52, "text": " you'll begin to internalize this pretty quickly.", "tokens": [51088, 291, 603, 1841, 281, 6920, 1125, 341, 1238, 2661, 13, 51224], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 735, "seek": 216632, "start": 2183.52, "end": 2186.0800000000004, "text": " But what we just looked at are just very high level", "tokens": [51224, 583, 437, 321, 445, 2956, 412, 366, 445, 588, 1090, 1496, 51352], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 736, "seek": 216632, "start": 2186.0800000000004, "end": 2187.36, "text": " summary numbers.", "tokens": [51352, 12691, 3547, 13, 51416], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 737, "seek": 216632, "start": 2187.36, "end": 2190.36, "text": " And they don't really tell you what exactly are the errors", "tokens": [51416, 400, 436, 500, 380, 534, 980, 291, 437, 2293, 366, 264, 13603, 51566], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 738, "seek": 216632, "start": 2190.36, "end": 2192.52, "text": " and is it ultimately usable by people?", "tokens": [51566, 293, 307, 309, 6284, 29975, 538, 561, 30, 51674], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 739, "seek": 216632, "start": 2192.52, "end": 2195.4, "text": " Is it a safe thing that people can rely on?", "tokens": [51674, 1119, 309, 257, 3273, 551, 300, 561, 393, 10687, 322, 30, 51818], "temperature": 0.0, "avg_logprob": -0.16903995698498142, "compression_ratio": 1.651877133105802, "no_speech_prob": 0.00030527263879776}, {"id": 740, "seek": 219540, "start": 2195.44, "end": 2197.7200000000003, "text": " And so one of the things we really focused on", "tokens": [50366, 400, 370, 472, 295, 264, 721, 321, 534, 5178, 322, 50480], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 741, "seek": 219540, "start": 2197.7200000000003, "end": 2199.52, "text": " is user safety.", "tokens": [50480, 307, 4195, 4514, 13, 50570], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 742, "seek": 219540, "start": 2199.52, "end": 2203.04, "text": " And some of that manifests in some of the toxicity work", "tokens": [50570, 400, 512, 295, 300, 50252, 294, 512, 295, 264, 45866, 589, 50746], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 743, "seek": 219540, "start": 2203.04, "end": 2204.04, "text": " that we did.", "tokens": [50746, 300, 321, 630, 13, 50796], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 744, "seek": 219540, "start": 2204.04, "end": 2207.52, "text": " And the driving thing here is that not all errors in translation", "tokens": [50796, 400, 264, 4840, 551, 510, 307, 300, 406, 439, 13603, 294, 12853, 50970], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 745, "seek": 219540, "start": 2207.52, "end": 2208.52, "text": " are made equal.", "tokens": [50970, 366, 1027, 2681, 13, 51020], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 746, "seek": 219540, "start": 2208.52, "end": 2210.8, "text": " So during COVID, there was this one that was really", "tokens": [51020, 407, 1830, 4566, 11, 456, 390, 341, 472, 300, 390, 534, 51134], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 747, "seek": 219540, "start": 2210.8, "end": 2212.92, "text": " went viral circulating around.", "tokens": [51134, 1437, 16132, 39749, 926, 13, 51240], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 748, "seek": 219540, "start": 2212.92, "end": 2215.6800000000003, "text": " But the message during COVID is you've got to wash your hands.", "tokens": [51240, 583, 264, 3636, 1830, 4566, 307, 291, 600, 658, 281, 5675, 428, 2377, 13, 51378], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 749, "seek": 219540, "start": 2215.6800000000003, "end": 2218.36, "text": " But the translation producer is like, you've got to hold hands,", "tokens": [51378, 583, 264, 12853, 12314, 307, 411, 11, 291, 600, 658, 281, 1797, 2377, 11, 51512], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 750, "seek": 219540, "start": 2218.36, "end": 2222.1600000000003, "text": " which I think is exactly the opposite of what you want to do.", "tokens": [51512, 597, 286, 519, 307, 2293, 264, 6182, 295, 437, 291, 528, 281, 360, 13, 51702], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 751, "seek": 219540, "start": 2222.1600000000003, "end": 2223.8, "text": " And other types of measurement errors", "tokens": [51702, 400, 661, 3467, 295, 13160, 13603, 51784], "temperature": 0.0, "avg_logprob": -0.170997393572772, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00017390659195370972}, {"id": 752, "seek": 222380, "start": 2223.84, "end": 2225.5600000000004, "text": " are really important as well.", "tokens": [50366, 366, 534, 1021, 382, 731, 13, 50452], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 753, "seek": 222380, "start": 2225.5600000000004, "end": 2228.2400000000002, "text": " So if you're telling someone how far they want to go,", "tokens": [50452, 407, 498, 291, 434, 3585, 1580, 577, 1400, 436, 528, 281, 352, 11, 50586], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 754, "seek": 222380, "start": 2228.2400000000002, "end": 2230.76, "text": " and you're like, hey, you want to travel five kilometers,", "tokens": [50586, 293, 291, 434, 411, 11, 4177, 11, 291, 528, 281, 3147, 1732, 13904, 11, 50712], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 755, "seek": 222380, "start": 2230.76, "end": 2233.8, "text": " and then your translation is like travel 500 kilometers,", "tokens": [50712, 293, 550, 428, 12853, 307, 411, 3147, 5923, 13904, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 756, "seek": 222380, "start": 2233.8, "end": 2236.8, "text": " it's a completely different type of issue.", "tokens": [50864, 309, 311, 257, 2584, 819, 2010, 295, 2734, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 757, "seek": 222380, "start": 2236.8, "end": 2238.96, "text": " And so what we did for toxicity, which", "tokens": [51014, 400, 370, 437, 321, 630, 337, 45866, 11, 597, 51122], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 758, "seek": 222380, "start": 2238.96, "end": 2242.2000000000003, "text": " is a big focus for this work, is that we collected different", "tokens": [51122, 307, 257, 955, 1879, 337, 341, 589, 11, 307, 300, 321, 11087, 819, 51284], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 759, "seek": 222380, "start": 2242.2000000000003, "end": 2245.7200000000003, "text": " toxicity lists for all 200 languages.", "tokens": [51284, 45866, 14511, 337, 439, 2331, 8650, 13, 51460], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 760, "seek": 222380, "start": 2245.7200000000003, "end": 2247.88, "text": " And so why do I care so much about toxicity?", "tokens": [51460, 400, 370, 983, 360, 286, 1127, 370, 709, 466, 45866, 30, 51568], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 761, "seek": 222380, "start": 2247.88, "end": 2249.7200000000003, "text": " I think it's a user safety thing.", "tokens": [51568, 286, 519, 309, 311, 257, 4195, 4514, 551, 13, 51660], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 762, "seek": 222380, "start": 2249.7200000000003, "end": 2252.5600000000004, "text": " So if you input some perfectly benign text,", "tokens": [51660, 407, 498, 291, 4846, 512, 6239, 3271, 788, 2487, 11, 51802], "temperature": 0.0, "avg_logprob": -0.16721232378924333, "compression_ratio": 1.7552447552447552, "no_speech_prob": 0.0009694192558526993}, {"id": 763, "seek": 225256, "start": 2252.6, "end": 2254.44, "text": " and then the output is profanity,", "tokens": [50366, 293, 550, 264, 5598, 307, 1740, 282, 507, 11, 50458], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 764, "seek": 225256, "start": 2254.44, "end": 2256.72, "text": " I think it's just really unexpected.", "tokens": [50458, 286, 519, 309, 311, 445, 534, 13106, 13, 50572], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 765, "seek": 225256, "start": 2256.72, "end": 2259.12, "text": " And it breaks a lot of trust in the system.", "tokens": [50572, 400, 309, 9857, 257, 688, 295, 3361, 294, 264, 1185, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 766, "seek": 225256, "start": 2259.12, "end": 2262.44, "text": " And it's an extremely poor experience for people.", "tokens": [50692, 400, 309, 311, 364, 4664, 4716, 1752, 337, 561, 13, 50858], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 767, "seek": 225256, "start": 2262.44, "end": 2265.24, "text": " That being said, it's also a very, very challenging thing,", "tokens": [50858, 663, 885, 848, 11, 309, 311, 611, 257, 588, 11, 588, 7595, 551, 11, 50998], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 768, "seek": 225256, "start": 2265.24, "end": 2267.7599999999998, "text": " because it's extremely culturally specific.", "tokens": [50998, 570, 309, 311, 4664, 28879, 2685, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 769, "seek": 225256, "start": 2267.7599999999998, "end": 2272.36, "text": " So things that are slurs or insults in certain languages,", "tokens": [51124, 407, 721, 300, 366, 1061, 2156, 420, 15285, 82, 294, 1629, 8650, 11, 51354], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 770, "seek": 225256, "start": 2272.36, "end": 2275.7999999999997, "text": " they don't really generalize across cultures,", "tokens": [51354, 436, 500, 380, 534, 2674, 1125, 2108, 12951, 11, 51526], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 771, "seek": 225256, "start": 2275.7999999999997, "end": 2277.68, "text": " which means that things like this", "tokens": [51526, 597, 1355, 300, 721, 411, 341, 51620], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 772, "seek": 225256, "start": 2277.68, "end": 2280.2799999999997, "text": " are very challenging to create.", "tokens": [51620, 366, 588, 7595, 281, 1884, 13, 51750], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 773, "seek": 225256, "start": 2280.2799999999997, "end": 2282.24, "text": " And I also was very interested in this direction,", "tokens": [51750, 400, 286, 611, 390, 588, 3102, 294, 341, 3513, 11, 51848], "temperature": 0.0, "avg_logprob": -0.1348210282213106, "compression_ratio": 1.7581227436823104, "no_speech_prob": 0.00023038743529468775}, {"id": 774, "seek": 228224, "start": 2282.24, "end": 2284.8799999999997, "text": " because I think it's broadly useful for all sorts", "tokens": [50364, 570, 286, 519, 309, 311, 19511, 4420, 337, 439, 7527, 50496], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 775, "seek": 228224, "start": 2284.8799999999997, "end": 2287.3199999999997, "text": " of different type of detection things", "tokens": [50496, 295, 819, 2010, 295, 17784, 721, 50618], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 776, "seek": 228224, "start": 2287.3199999999997, "end": 2289.4399999999996, "text": " that you need to do, and also mitigation.", "tokens": [50618, 300, 291, 643, 281, 360, 11, 293, 611, 32649, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 777, "seek": 228224, "start": 2289.4399999999996, "end": 2291.0, "text": " And so even though we develop this", "tokens": [50724, 400, 370, 754, 1673, 321, 1499, 341, 50802], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 778, "seek": 228224, "start": 2291.0, "end": 2292.68, "text": " in the context of translation,", "tokens": [50802, 294, 264, 4319, 295, 12853, 11, 50886], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 779, "seek": 228224, "start": 2292.68, "end": 2296.72, "text": " it can be used very broadly in other types of NLP applications.", "tokens": [50886, 309, 393, 312, 1143, 588, 19511, 294, 661, 3467, 295, 426, 45196, 5821, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 780, "seek": 228224, "start": 2297.8799999999997, "end": 2300.0, "text": " This is also open source, you can download it.", "tokens": [51146, 639, 307, 611, 1269, 4009, 11, 291, 393, 5484, 309, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 781, "seek": 228224, "start": 2300.0, "end": 2301.7999999999997, "text": " You have to type in a little password", "tokens": [51252, 509, 362, 281, 2010, 294, 257, 707, 11524, 51342], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 782, "seek": 228224, "start": 2301.7999999999997, "end": 2303.52, "text": " that's in the GitHub repo,", "tokens": [51342, 300, 311, 294, 264, 23331, 49040, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 783, "seek": 228224, "start": 2303.52, "end": 2305.3999999999996, "text": " just so that you don't accidentally download", "tokens": [51428, 445, 370, 300, 291, 500, 380, 15715, 5484, 51522], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 784, "seek": 228224, "start": 2305.3999999999996, "end": 2307.6, "text": " and realize you have files of curse words", "tokens": [51522, 293, 4325, 291, 362, 7098, 295, 17139, 2283, 51632], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 785, "seek": 228224, "start": 2307.6, "end": 2309.12, "text": " all over your computer.", "tokens": [51632, 439, 670, 428, 3820, 13, 51708], "temperature": 0.0, "avg_logprob": -0.1042163334195576, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.00017394796304870397}, {"id": 786, "seek": 230912, "start": 2310.12, "end": 2311.7999999999997, "text": " Okay, so I wanna end a little bit", "tokens": [50414, 1033, 11, 370, 286, 1948, 917, 257, 707, 857, 50498], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 787, "seek": 230912, "start": 2311.7999999999997, "end": 2314.3199999999997, "text": " with some thoughts about future directions.", "tokens": [50498, 365, 512, 4598, 466, 2027, 11095, 13, 50624], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 788, "seek": 230912, "start": 2314.3199999999997, "end": 2316.08, "text": " And before I get there,", "tokens": [50624, 400, 949, 286, 483, 456, 11, 50712], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 789, "seek": 230912, "start": 2316.08, "end": 2319.7999999999997, "text": " there's like a 190 page paper that writes up", "tokens": [50712, 456, 311, 411, 257, 37609, 3028, 3035, 300, 13657, 493, 50898], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 790, "seek": 230912, "start": 2319.7999999999997, "end": 2322.24, "text": " all of this in far greater detail,", "tokens": [50898, 439, 295, 341, 294, 1400, 5044, 2607, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 791, "seek": 230912, "start": 2322.24, "end": 2323.72, "text": " in case you're curious.", "tokens": [51020, 294, 1389, 291, 434, 6369, 13, 51094], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 792, "seek": 230912, "start": 2325.0, "end": 2326.92, "text": " So a few future directions", "tokens": [51158, 407, 257, 1326, 2027, 11095, 51254], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 793, "seek": 230912, "start": 2326.92, "end": 2329.0, "text": " that I think I'm really interested in,", "tokens": [51254, 300, 286, 519, 286, 478, 534, 3102, 294, 11, 51358], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 794, "seek": 230912, "start": 2329.0, "end": 2331.3199999999997, "text": " and some of these are also very applicable", "tokens": [51358, 293, 512, 295, 613, 366, 611, 588, 21142, 51474], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 795, "seek": 230912, "start": 2331.3199999999997, "end": 2332.7599999999998, "text": " to things like speech,", "tokens": [51474, 281, 721, 411, 6218, 11, 51546], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 796, "seek": 230912, "start": 2332.7599999999998, "end": 2337.44, "text": " is that I think one of them is more explicit multilingual.", "tokens": [51546, 307, 300, 286, 519, 472, 295, 552, 307, 544, 13691, 2120, 38219, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1851138548417525, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.0005188820068724453}, {"id": 797, "seek": 233744, "start": 2337.7200000000003, "end": 2340.36, "text": " So I think a lot of approaches to multilingual", "tokens": [50378, 407, 286, 519, 257, 688, 295, 11587, 281, 2120, 38219, 50510], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 798, "seek": 233744, "start": 2340.36, "end": 2341.92, "text": " have been like, hey,", "tokens": [50510, 362, 668, 411, 11, 4177, 11, 50588], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 799, "seek": 233744, "start": 2341.92, "end": 2344.32, "text": " we have this thing that's working well for one language,", "tokens": [50588, 321, 362, 341, 551, 300, 311, 1364, 731, 337, 472, 2856, 11, 50708], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 800, "seek": 233744, "start": 2344.32, "end": 2346.48, "text": " like let's try to scale it", "tokens": [50708, 411, 718, 311, 853, 281, 4373, 309, 50816], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 801, "seek": 233744, "start": 2346.48, "end": 2347.88, "text": " to a bunch of different languages,", "tokens": [50816, 281, 257, 3840, 295, 819, 8650, 11, 50886], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 802, "seek": 233744, "start": 2347.88, "end": 2348.96, "text": " and then we're gonna put them all", "tokens": [50886, 293, 550, 321, 434, 799, 829, 552, 439, 50940], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 803, "seek": 233744, "start": 2348.96, "end": 2350.2000000000003, "text": " in the same modeling bucket,", "tokens": [50940, 294, 264, 912, 15983, 13058, 11, 51002], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 804, "seek": 233744, "start": 2350.2000000000003, "end": 2352.76, "text": " and just kind of like hope that the model learns", "tokens": [51002, 293, 445, 733, 295, 411, 1454, 300, 264, 2316, 27152, 51130], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 805, "seek": 233744, "start": 2352.76, "end": 2354.76, "text": " all of these different representations.", "tokens": [51130, 439, 295, 613, 819, 33358, 13, 51230], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 806, "seek": 233744, "start": 2354.76, "end": 2357.04, "text": " But I think there's a lot of potential room", "tokens": [51230, 583, 286, 519, 456, 311, 257, 688, 295, 3995, 1808, 51344], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 807, "seek": 233744, "start": 2357.04, "end": 2360.28, "text": " for explicitly bringing in,", "tokens": [51344, 337, 20803, 5062, 294, 11, 51506], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 808, "seek": 233744, "start": 2360.28, "end": 2362.16, "text": " like the fact that you know it's multilingual", "tokens": [51506, 411, 264, 1186, 300, 291, 458, 309, 311, 2120, 38219, 51600], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 809, "seek": 233744, "start": 2362.16, "end": 2365.32, "text": " into the architecture more.", "tokens": [51600, 666, 264, 9482, 544, 13, 51758], "temperature": 0.0, "avg_logprob": -0.12061122747567984, "compression_ratio": 1.7224199288256228, "no_speech_prob": 0.00020980287808924913}, {"id": 810, "seek": 236532, "start": 2365.32, "end": 2367.76, "text": " And so, you know,", "tokens": [50364, 400, 370, 11, 291, 458, 11, 50486], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 811, "seek": 236532, "start": 2367.76, "end": 2372.1200000000003, "text": " it's possible to capture more nuances between languages", "tokens": [50486, 309, 311, 1944, 281, 7983, 544, 38775, 1296, 8650, 50704], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 812, "seek": 236532, "start": 2372.1200000000003, "end": 2374.44, "text": " or different relationships between languages.", "tokens": [50704, 420, 819, 6159, 1296, 8650, 13, 50820], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 813, "seek": 236532, "start": 2375.6000000000004, "end": 2378.0800000000004, "text": " And the other one is continued support for everyone.", "tokens": [50878, 400, 264, 661, 472, 307, 7014, 1406, 337, 1518, 13, 51002], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 814, "seek": 236532, "start": 2378.0800000000004, "end": 2380.96, "text": " I think it's like something reflecting on this project", "tokens": [51002, 286, 519, 309, 311, 411, 746, 23543, 322, 341, 1716, 51146], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 815, "seek": 236532, "start": 2380.96, "end": 2383.52, "text": " is that, you know, going from 100 to 200", "tokens": [51146, 307, 300, 11, 291, 458, 11, 516, 490, 2319, 281, 2331, 51274], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 816, "seek": 236532, "start": 2383.52, "end": 2385.4, "text": " was already pretty challenging,", "tokens": [51274, 390, 1217, 1238, 7595, 11, 51368], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 817, "seek": 236532, "start": 2385.4, "end": 2387.32, "text": " but going beyond a lot of the techniques", "tokens": [51368, 457, 516, 4399, 257, 688, 295, 264, 7512, 51464], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 818, "seek": 236532, "start": 2387.32, "end": 2388.56, "text": " that we developed here", "tokens": [51464, 300, 321, 4743, 510, 51526], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 819, "seek": 236532, "start": 2388.56, "end": 2391.2400000000002, "text": " are not necessarily that scalable.", "tokens": [51526, 366, 406, 4725, 300, 38481, 13, 51660], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 820, "seek": 236532, "start": 2391.2400000000002, "end": 2393.32, "text": " This is actually what inspired some of our work", "tokens": [51660, 639, 307, 767, 437, 7547, 512, 295, 527, 589, 51764], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 821, "seek": 236532, "start": 2393.32, "end": 2395.28, "text": " on speech translation as well.", "tokens": [51764, 322, 6218, 12853, 382, 731, 13, 51862], "temperature": 0.0, "avg_logprob": -0.12975451041912212, "compression_ratio": 1.6426116838487972, "no_speech_prob": 9.605917148292065e-05}, {"id": 822, "seek": 239528, "start": 2395.28, "end": 2398.36, "text": " So if you recently saw like the seamless M4T release", "tokens": [50364, 407, 498, 291, 3938, 1866, 411, 264, 28677, 376, 19, 51, 4374, 50518], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 823, "seek": 239528, "start": 2398.36, "end": 2399.92, "text": " or like the unwritten languages,", "tokens": [50518, 420, 411, 264, 517, 26859, 8650, 11, 50596], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 824, "seek": 239528, "start": 2399.92, "end": 2401.84, "text": " like we did a lot of modeling of Hokeum,", "tokens": [50596, 411, 321, 630, 257, 688, 295, 15983, 295, 389, 2949, 449, 11, 50692], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 825, "seek": 239528, "start": 2401.84, "end": 2404.7200000000003, "text": " and I think that goes into this direction really well,", "tokens": [50692, 293, 286, 519, 300, 1709, 666, 341, 3513, 534, 731, 11, 50836], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 826, "seek": 239528, "start": 2404.7200000000003, "end": 2407.6800000000003, "text": " because many of the languages that people want to use", "tokens": [50836, 570, 867, 295, 264, 8650, 300, 561, 528, 281, 764, 50984], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 827, "seek": 239528, "start": 2407.6800000000003, "end": 2409.8, "text": " are like spoken first languages", "tokens": [50984, 366, 411, 10759, 700, 8650, 51090], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 828, "seek": 239528, "start": 2409.8, "end": 2412.0, "text": " and not necessarily like primarily written.", "tokens": [51090, 293, 406, 4725, 411, 10029, 3720, 13, 51200], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 829, "seek": 239528, "start": 2413.0800000000004, "end": 2414.2400000000002, "text": " And then I think the last thing", "tokens": [51254, 400, 550, 286, 519, 264, 1036, 551, 51312], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 830, "seek": 239528, "start": 2414.2400000000002, "end": 2415.96, "text": " that I'm still really passionate about", "tokens": [51312, 300, 286, 478, 920, 534, 11410, 466, 51398], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 831, "seek": 239528, "start": 2415.96, "end": 2418.8, "text": " is like continued increase ease of use", "tokens": [51398, 307, 411, 7014, 3488, 12708, 295, 764, 51540], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 832, "seek": 239528, "start": 2418.8, "end": 2420.28, "text": " and training of these models", "tokens": [51540, 293, 3097, 295, 613, 5245, 51614], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 833, "seek": 239528, "start": 2420.28, "end": 2423.0, "text": " and like democratization for the community.", "tokens": [51614, 293, 411, 37221, 2144, 337, 264, 1768, 13, 51750], "temperature": 0.0, "avg_logprob": -0.11811726324019893, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.2224881073925644e-05}, {"id": 834, "seek": 242300, "start": 2423.0, "end": 2425.44, "text": " So one of the things that we tried to do in this work", "tokens": [50364, 407, 472, 295, 264, 721, 300, 321, 3031, 281, 360, 294, 341, 589, 50486], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 835, "seek": 242300, "start": 2425.44, "end": 2428.08, "text": " is just like really, really clearly write down", "tokens": [50486, 307, 445, 411, 534, 11, 534, 4448, 2464, 760, 50618], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 836, "seek": 242300, "start": 2428.08, "end": 2430.2, "text": " everything that we did and like open source,", "tokens": [50618, 1203, 300, 321, 630, 293, 411, 1269, 4009, 11, 50724], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 837, "seek": 242300, "start": 2430.2, "end": 2432.84, "text": " like even the data pipeline and things like that.", "tokens": [50724, 411, 754, 264, 1412, 15517, 293, 721, 411, 300, 13, 50856], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 838, "seek": 242300, "start": 2432.84, "end": 2434.52, "text": " And so that's where you get like all of the repos", "tokens": [50856, 400, 370, 300, 311, 689, 291, 483, 411, 439, 295, 264, 1085, 329, 50940], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 839, "seek": 242300, "start": 2434.52, "end": 2438.04, "text": " that I linked and, you know, like a huge write up.", "tokens": [50940, 300, 286, 9408, 293, 11, 291, 458, 11, 411, 257, 2603, 2464, 493, 13, 51116], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 840, "seek": 242300, "start": 2438.04, "end": 2440.12, "text": " But I think if someone were to try to reproduce this", "tokens": [51116, 583, 286, 519, 498, 1580, 645, 281, 853, 281, 29501, 341, 51220], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 841, "seek": 242300, "start": 2440.12, "end": 2442.08, "text": " for their own language, and many people have,", "tokens": [51220, 337, 641, 1065, 2856, 11, 293, 867, 561, 362, 11, 51318], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 842, "seek": 242300, "start": 2442.08, "end": 2444.2, "text": " like I'm not saying that that hasn't been,", "tokens": [51318, 411, 286, 478, 406, 1566, 300, 300, 6132, 380, 668, 11, 51424], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 843, "seek": 242300, "start": 2444.2, "end": 2446.52, "text": " but it's like, if you wanted to like do this,", "tokens": [51424, 457, 309, 311, 411, 11, 498, 291, 1415, 281, 411, 360, 341, 11, 51540], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 844, "seek": 242300, "start": 2446.52, "end": 2449.48, "text": " it would be extremely, extremely hard", "tokens": [51540, 309, 576, 312, 4664, 11, 4664, 1152, 51688], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 845, "seek": 242300, "start": 2449.48, "end": 2452.4, "text": " because there's just like so much different things going on.", "tokens": [51688, 570, 456, 311, 445, 411, 370, 709, 819, 721, 516, 322, 13, 51834], "temperature": 0.0, "avg_logprob": -0.09113405927827087, "compression_ratio": 1.839116719242902, "no_speech_prob": 9.312129259342328e-05}, {"id": 846, "seek": 245240, "start": 2452.4, "end": 2454.44, "text": " So I think most of the, what we've seen is like,", "tokens": [50364, 407, 286, 519, 881, 295, 264, 11, 437, 321, 600, 1612, 307, 411, 11, 50466], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 847, "seek": 245240, "start": 2454.44, "end": 2455.96, "text": " people have downloaded the base model", "tokens": [50466, 561, 362, 21748, 264, 3096, 2316, 50542], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 848, "seek": 245240, "start": 2455.96, "end": 2458.04, "text": " and fine-tuned it for their own language,", "tokens": [50542, 293, 2489, 12, 83, 43703, 309, 337, 641, 1065, 2856, 11, 50646], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 849, "seek": 245240, "start": 2458.04, "end": 2460.96, "text": " but it's pretty hard to just like add on", "tokens": [50646, 457, 309, 311, 1238, 1152, 281, 445, 411, 909, 322, 50792], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 850, "seek": 245240, "start": 2460.96, "end": 2463.2000000000003, "text": " many, many more languages to this system", "tokens": [50792, 867, 11, 867, 544, 8650, 281, 341, 1185, 50904], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 851, "seek": 245240, "start": 2463.2000000000003, "end": 2466.0, "text": " because of how complicated all of the moving parts are.", "tokens": [50904, 570, 295, 577, 6179, 439, 295, 264, 2684, 3166, 366, 13, 51044], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 852, "seek": 245240, "start": 2466.0, "end": 2468.92, "text": " And so I feel like something for the translation community", "tokens": [51044, 400, 370, 286, 841, 411, 746, 337, 264, 12853, 1768, 51190], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 853, "seek": 245240, "start": 2468.92, "end": 2473.32, "text": " overall is like, how do we simplify a lot of these things?", "tokens": [51190, 4787, 307, 411, 11, 577, 360, 321, 20460, 257, 688, 295, 613, 721, 30, 51410], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 854, "seek": 245240, "start": 2473.32, "end": 2476.0, "text": " And I think that's where like a lot of fundamental modeling", "tokens": [51410, 400, 286, 519, 300, 311, 689, 411, 257, 688, 295, 8088, 15983, 51544], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 855, "seek": 245240, "start": 2476.0, "end": 2479.04, "text": " innovation could help us get to.", "tokens": [51544, 8504, 727, 854, 505, 483, 281, 13, 51696], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 856, "seek": 245240, "start": 2479.04, "end": 2481.44, "text": " And so yeah, I got a chance to give this talk,", "tokens": [51696, 400, 370, 1338, 11, 286, 658, 257, 2931, 281, 976, 341, 751, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10973080226353236, "compression_ratio": 1.7180327868852459, "no_speech_prob": 6.0117501561762765e-05}, {"id": 857, "seek": 248144, "start": 2481.44, "end": 2483.56, "text": " but of course the work is like being done", "tokens": [50364, 457, 295, 1164, 264, 589, 307, 411, 885, 1096, 50470], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 858, "seek": 248144, "start": 2483.56, "end": 2487.32, "text": " by a huge team of people that I've cited here.", "tokens": [50470, 538, 257, 2603, 1469, 295, 561, 300, 286, 600, 30134, 510, 13, 50658], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 859, "seek": 248144, "start": 2487.32, "end": 2490.48, "text": " And yeah, if you want to use any of this", "tokens": [50658, 400, 1338, 11, 498, 291, 528, 281, 764, 604, 295, 341, 50816], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 860, "seek": 248144, "start": 2490.48, "end": 2492.2000000000003, "text": " or read more about it, like everything is linked", "tokens": [50816, 420, 1401, 544, 466, 309, 11, 411, 1203, 307, 9408, 50902], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 861, "seek": 248144, "start": 2492.2000000000003, "end": 2495.92, "text": " from this main GitHub repo here in Fairseek,", "tokens": [50902, 490, 341, 2135, 23331, 49040, 510, 294, 12157, 405, 916, 11, 51088], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 862, "seek": 248144, "start": 2495.92, "end": 2498.56, "text": " and you can like click on everything else afterwards.", "tokens": [51088, 293, 291, 393, 411, 2052, 322, 1203, 1646, 10543, 13, 51220], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 863, "seek": 248144, "start": 2499.64, "end": 2502.68, "text": " But yeah, maybe I'll go back to Stephen", "tokens": [51274, 583, 1338, 11, 1310, 286, 603, 352, 646, 281, 13391, 51426], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 864, "seek": 248144, "start": 2502.68, "end": 2505.4, "text": " if we have any questions or anything else like that.", "tokens": [51426, 498, 321, 362, 604, 1651, 420, 1340, 1646, 411, 300, 13, 51562], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 865, "seek": 248144, "start": 2505.4, "end": 2506.84, "text": " All right, now thanks for the great talk.", "tokens": [51562, 1057, 558, 11, 586, 3231, 337, 264, 869, 751, 13, 51634], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 866, "seek": 248144, "start": 2506.84, "end": 2508.48, "text": " Yeah, if anybody has any questions,", "tokens": [51634, 865, 11, 498, 4472, 575, 604, 1651, 11, 51716], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 867, "seek": 248144, "start": 2508.48, "end": 2510.36, "text": " feel free to unmute and ask.", "tokens": [51716, 841, 1737, 281, 41445, 293, 1029, 13, 51810], "temperature": 0.0, "avg_logprob": -0.14086161237774472, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0002568817581050098}, {"id": 868, "seek": 251144, "start": 2511.44, "end": 2512.28, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50406], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 869, "seek": 251144, "start": 2517.28, "end": 2519.8, "text": " Did you consult with a lot of like native speakers", "tokens": [50656, 2589, 291, 7189, 365, 257, 688, 295, 411, 8470, 9518, 50782], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 870, "seek": 251144, "start": 2520.7200000000003, "end": 2523.8, "text": " for like, you know, profanities and this type of stuff?", "tokens": [50828, 337, 411, 11, 291, 458, 11, 1740, 282, 1088, 293, 341, 2010, 295, 1507, 30, 50982], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 871, "seek": 251144, "start": 2523.8, "end": 2527.6, "text": " Like how are you able to get access to the, you know,", "tokens": [50982, 1743, 577, 366, 291, 1075, 281, 483, 2105, 281, 264, 11, 291, 458, 11, 51172], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 872, "seek": 251144, "start": 2527.6, "end": 2530.92, "text": " low quality languages or low resource languages", "tokens": [51172, 2295, 3125, 8650, 420, 2295, 7684, 8650, 51338], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 873, "seek": 251144, "start": 2530.92, "end": 2533.68, "text": " and make sure that translations are correct?", "tokens": [51338, 293, 652, 988, 300, 37578, 366, 3006, 30, 51476], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 874, "seek": 251144, "start": 2533.68, "end": 2535.36, "text": " Yeah, yeah, that's a really good question.", "tokens": [51476, 865, 11, 1338, 11, 300, 311, 257, 534, 665, 1168, 13, 51560], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 875, "seek": 251144, "start": 2535.36, "end": 2537.12, "text": " I mean, I think it's the most important to consult", "tokens": [51560, 286, 914, 11, 286, 519, 309, 311, 264, 881, 1021, 281, 7189, 51648], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 876, "seek": 251144, "start": 2537.12, "end": 2538.64, "text": " like a bunch of native speakers", "tokens": [51648, 411, 257, 3840, 295, 8470, 9518, 51724], "temperature": 0.0, "avg_logprob": -0.21896612419272368, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0005190515657886863}, {"id": 877, "seek": 253864, "start": 2539.64, "end": 2541.72, "text": " across the entire development process.", "tokens": [50414, 2108, 264, 2302, 3250, 1399, 13, 50518], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 878, "seek": 253864, "start": 2541.72, "end": 2544.0, "text": " So part of our original thing was just like interviewing", "tokens": [50518, 407, 644, 295, 527, 3380, 551, 390, 445, 411, 26524, 50632], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 879, "seek": 253864, "start": 2544.0, "end": 2546.8399999999997, "text": " a bunch of people to understand like what they're looking for", "tokens": [50632, 257, 3840, 295, 561, 281, 1223, 411, 437, 436, 434, 1237, 337, 50774], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 880, "seek": 253864, "start": 2546.8399999999997, "end": 2548.64, "text": " in a high quality translation.", "tokens": [50774, 294, 257, 1090, 3125, 12853, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 881, "seek": 253864, "start": 2548.64, "end": 2552.16, "text": " And then we have like an entire professional translation team", "tokens": [50864, 400, 550, 321, 362, 411, 364, 2302, 4843, 12853, 1469, 51040], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 882, "seek": 253864, "start": 2552.16, "end": 2556.16, "text": " hired, which took quite a long time to find", "tokens": [51040, 13144, 11, 597, 1890, 1596, 257, 938, 565, 281, 915, 51240], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 883, "seek": 253864, "start": 2556.16, "end": 2560.12, "text": " to consult with along the process.", "tokens": [51240, 281, 7189, 365, 2051, 264, 1399, 13, 51438], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 884, "seek": 253864, "start": 2560.12, "end": 2563.0, "text": " And then right now, like we also have some of the things", "tokens": [51438, 400, 550, 558, 586, 11, 411, 321, 611, 362, 512, 295, 264, 721, 51582], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 885, "seek": 253864, "start": 2563.0, "end": 2565.24, "text": " like toxicity lists are open to the community.", "tokens": [51582, 411, 45866, 14511, 366, 1269, 281, 264, 1768, 13, 51694], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 886, "seek": 253864, "start": 2565.24, "end": 2566.7999999999997, "text": " So if you make like a pull request,", "tokens": [51694, 407, 498, 291, 652, 411, 257, 2235, 5308, 11, 51772], "temperature": 0.0, "avg_logprob": -0.08993997071918688, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0003052061074413359}, {"id": 887, "seek": 256680, "start": 2566.8, "end": 2568.76, "text": " we try to like, you know, validate that", "tokens": [50364, 321, 853, 281, 411, 11, 291, 458, 11, 29562, 300, 50462], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 888, "seek": 256680, "start": 2568.76, "end": 2570.1600000000003, "text": " that's like a useful addition", "tokens": [50462, 300, 311, 411, 257, 4420, 4500, 50532], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 889, "seek": 256680, "start": 2570.1600000000003, "end": 2573.4, "text": " and then like try to merge it in as well.", "tokens": [50532, 293, 550, 411, 853, 281, 22183, 309, 294, 382, 731, 13, 50694], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 890, "seek": 256680, "start": 2576.84, "end": 2578.1600000000003, "text": " We have a question in the room.", "tokens": [50866, 492, 362, 257, 1168, 294, 264, 1808, 13, 50932], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 891, "seek": 256680, "start": 2578.1600000000003, "end": 2580.5600000000004, "text": " Let's see if that comes over soon.", "tokens": [50932, 961, 311, 536, 498, 300, 1487, 670, 2321, 13, 51052], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 892, "seek": 256680, "start": 2580.5600000000004, "end": 2581.4, "text": " Oh, go ahead.", "tokens": [51052, 876, 11, 352, 2286, 13, 51094], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 893, "seek": 256680, "start": 2581.4, "end": 2583.0800000000004, "text": " So I'll speed, see if we should get it.", "tokens": [51094, 407, 286, 603, 3073, 11, 536, 498, 321, 820, 483, 309, 13, 51178], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 894, "seek": 256680, "start": 2583.0800000000004, "end": 2584.5600000000004, "text": " Yeah.", "tokens": [51178, 865, 13, 51252], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 895, "seek": 256680, "start": 2584.5600000000004, "end": 2586.52, "text": " So like, did you spend most of your time", "tokens": [51252, 407, 411, 11, 630, 291, 3496, 881, 295, 428, 565, 51350], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 896, "seek": 256680, "start": 2586.52, "end": 2587.96, "text": " in the data pipeline state?", "tokens": [51350, 294, 264, 1412, 15517, 1785, 30, 51422], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 897, "seek": 256680, "start": 2589.28, "end": 2590.1200000000003, "text": " Yeah.", "tokens": [51488, 865, 13, 51530], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 898, "seek": 256680, "start": 2591.1200000000003, "end": 2593.84, "text": " Yeah, good question.", "tokens": [51580, 865, 11, 665, 1168, 13, 51716], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 899, "seek": 256680, "start": 2593.84, "end": 2594.6800000000003, "text": " I think the question is,", "tokens": [51716, 286, 519, 264, 1168, 307, 11, 51758], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 900, "seek": 256680, "start": 2594.6800000000003, "end": 2595.6400000000003, "text": " did you spend most of your time", "tokens": [51758, 630, 291, 3496, 881, 295, 428, 565, 51806], "temperature": 0.0, "avg_logprob": -0.2575155154679173, "compression_ratio": 1.7377777777777779, "no_speech_prob": 7.961066148709506e-05}, {"id": 901, "seek": 259564, "start": 2595.64, "end": 2596.8399999999997, "text": " in the data pipeline state?", "tokens": [50364, 294, 264, 1412, 15517, 1785, 30, 50424], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 902, "seek": 259564, "start": 2596.8399999999997, "end": 2599.92, "text": " It ended up being about like kind of like 50-50", "tokens": [50424, 467, 4590, 493, 885, 466, 411, 733, 295, 411, 2625, 12, 2803, 50578], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 903, "seek": 259564, "start": 2599.92, "end": 2603.2799999999997, "text": " like data or more like driving work.", "tokens": [50578, 411, 1412, 420, 544, 411, 4840, 589, 13, 50746], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 904, "seek": 259564, "start": 2603.2799999999997, "end": 2605.44, "text": " And then like 50-50 on the other side", "tokens": [50746, 400, 550, 411, 2625, 12, 2803, 322, 264, 661, 1252, 50854], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 905, "seek": 259564, "start": 2605.44, "end": 2607.3199999999997, "text": " like modeling and evaluation work.", "tokens": [50854, 411, 15983, 293, 13344, 589, 13, 50948], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 906, "seek": 259564, "start": 2607.3199999999997, "end": 2609.2799999999997, "text": " Because once like the data is set,", "tokens": [50948, 1436, 1564, 411, 264, 1412, 307, 992, 11, 51046], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 907, "seek": 259564, "start": 2609.2799999999997, "end": 2611.48, "text": " then there is a lot and a lot of iteration", "tokens": [51046, 550, 456, 307, 257, 688, 293, 257, 688, 295, 24784, 51156], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 908, "seek": 259564, "start": 2611.48, "end": 2613.8799999999997, "text": " on the modeling side to figure out like, okay,", "tokens": [51156, 322, 264, 15983, 1252, 281, 2573, 484, 411, 11, 1392, 11, 51276], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 909, "seek": 259564, "start": 2613.8799999999997, "end": 2615.8399999999997, "text": " which, how much of the data should we use?", "tokens": [51276, 597, 11, 577, 709, 295, 264, 1412, 820, 321, 764, 30, 51374], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 910, "seek": 259564, "start": 2615.8399999999997, "end": 2617.0, "text": " Like how should we portion the data?", "tokens": [51374, 1743, 577, 820, 321, 8044, 264, 1412, 30, 51432], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 911, "seek": 259564, "start": 2617.0, "end": 2618.08, "text": " How do we prevent overfitting?", "tokens": [51432, 1012, 360, 321, 4871, 670, 69, 2414, 30, 51486], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 912, "seek": 259564, "start": 2618.08, "end": 2620.2, "text": " What is the right architecture?", "tokens": [51486, 708, 307, 264, 558, 9482, 30, 51592], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 913, "seek": 259564, "start": 2620.2, "end": 2622.52, "text": " But a lot of work goes into the data", "tokens": [51592, 583, 257, 688, 295, 589, 1709, 666, 264, 1412, 51708], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 914, "seek": 259564, "start": 2622.52, "end": 2624.68, "text": " because I think if you don't have high quality data,", "tokens": [51708, 570, 286, 519, 498, 291, 500, 380, 362, 1090, 3125, 1412, 11, 51816], "temperature": 0.0, "avg_logprob": -0.11069284254504788, "compression_ratio": 1.8221476510067114, "no_speech_prob": 0.0003052764222957194}, {"id": 915, "seek": 262468, "start": 2625.2799999999997, "end": 2627.3199999999997, "text": " you just can't get a good model.", "tokens": [50394, 291, 445, 393, 380, 483, 257, 665, 2316, 13, 50496], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 916, "seek": 262468, "start": 2629.08, "end": 2632.48, "text": " And for data mining, how do you mine the data?", "tokens": [50584, 400, 337, 1412, 15512, 11, 577, 360, 291, 3892, 264, 1412, 30, 50754], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 917, "seek": 262468, "start": 2632.48, "end": 2635.8799999999997, "text": " Do you use like Selenium or how do you mine the web?", "tokens": [50754, 1144, 291, 764, 411, 10736, 268, 2197, 420, 577, 360, 291, 3892, 264, 3670, 30, 50924], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 918, "seek": 262468, "start": 2637.16, "end": 2640.52, "text": " Yeah, so for the web, we start with Common Crawl.", "tokens": [50988, 865, 11, 370, 337, 264, 3670, 11, 321, 722, 365, 18235, 37877, 75, 13, 51156], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 919, "seek": 262468, "start": 2640.52, "end": 2643.24, "text": " So we downloaded all of the different dumps of Common Crawl", "tokens": [51156, 407, 321, 21748, 439, 295, 264, 819, 11430, 82, 295, 18235, 37877, 75, 51292], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 920, "seek": 262468, "start": 2643.24, "end": 2645.24, "text": " and then we use HTML parser.", "tokens": [51292, 293, 550, 321, 764, 17995, 21156, 260, 13, 51392], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 921, "seek": 262468, "start": 2645.24, "end": 2648.0, "text": " I think now like, if you download, for example,", "tokens": [51392, 286, 519, 586, 411, 11, 498, 291, 5484, 11, 337, 1365, 11, 51530], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 922, "seek": 262468, "start": 2648.0, "end": 2649.2, "text": " the red pajama dataset,", "tokens": [51530, 264, 2182, 33819, 2404, 28872, 11, 51590], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 923, "seek": 262468, "start": 2649.2, "end": 2652.12, "text": " like they've done a lot of this like parsing and stuff.", "tokens": [51590, 411, 436, 600, 1096, 257, 688, 295, 341, 411, 21156, 278, 293, 1507, 13, 51736], "temperature": 0.0, "avg_logprob": -0.19048741968666635, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0001397585729137063}, {"id": 924, "seek": 265212, "start": 2652.12, "end": 2655.24, "text": " And then we have like large scale pipelines", "tokens": [50364, 400, 550, 321, 362, 411, 2416, 4373, 40168, 50520], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 925, "seek": 265212, "start": 2655.24, "end": 2658.24, "text": " that are set up like you can use Spark, for example,", "tokens": [50520, 300, 366, 992, 493, 411, 291, 393, 764, 23424, 11, 337, 1365, 11, 50670], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 926, "seek": 265212, "start": 2658.24, "end": 2659.4, "text": " to process these things,", "tokens": [50670, 281, 1399, 613, 721, 11, 50728], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 927, "seek": 265212, "start": 2659.4, "end": 2661.72, "text": " to like split all of the different sentences out,", "tokens": [50728, 281, 411, 7472, 439, 295, 264, 819, 16579, 484, 11, 50844], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 928, "seek": 265212, "start": 2661.72, "end": 2663.6, "text": " run your language identification.", "tokens": [50844, 1190, 428, 2856, 22065, 13, 50938], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 929, "seek": 265212, "start": 2663.6, "end": 2666.48, "text": " You know, you can do different heuristic cleaning.", "tokens": [50938, 509, 458, 11, 291, 393, 360, 819, 415, 374, 3142, 8924, 13, 51082], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 930, "seek": 265212, "start": 2666.48, "end": 2668.52, "text": " There are certain languages where it's like very actually,", "tokens": [51082, 821, 366, 1629, 8650, 689, 309, 311, 411, 588, 767, 11, 51184], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 931, "seek": 265212, "start": 2668.52, "end": 2670.88, "text": " very challenging to identify what is a sentence.", "tokens": [51184, 588, 7595, 281, 5876, 437, 307, 257, 8174, 13, 51302], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 932, "seek": 265212, "start": 2670.88, "end": 2674.3199999999997, "text": " Like I think in Thai, there is no like period.", "tokens": [51302, 1743, 286, 519, 294, 19254, 11, 456, 307, 572, 411, 2896, 13, 51474], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 933, "seek": 265212, "start": 2674.3199999999997, "end": 2676.72, "text": " So you have to like use different models", "tokens": [51474, 407, 291, 362, 281, 411, 764, 819, 5245, 51594], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 934, "seek": 265212, "start": 2676.72, "end": 2678.12, "text": " to identify what is a sentence", "tokens": [51594, 281, 5876, 437, 307, 257, 8174, 51664], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 935, "seek": 265212, "start": 2678.12, "end": 2680.2799999999997, "text": " and like parse some of those things out.", "tokens": [51664, 293, 411, 48377, 512, 295, 729, 721, 484, 13, 51772], "temperature": 0.0, "avg_logprob": -0.13330334336010377, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.00020979970577172935}, {"id": 936, "seek": 268028, "start": 2680.48, "end": 2684.0800000000004, "text": " And then we end up with, you know, our monolingual data dump.", "tokens": [50374, 400, 550, 321, 917, 493, 365, 11, 291, 458, 11, 527, 1108, 401, 278, 901, 1412, 11430, 13, 50554], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 937, "seek": 268028, "start": 2686.28, "end": 2687.92, "text": " What is Common Crawl?", "tokens": [50664, 708, 307, 18235, 37877, 75, 30, 50746], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 938, "seek": 268028, "start": 2687.92, "end": 2691.6000000000004, "text": " Is it software that you use for datasets?", "tokens": [50746, 1119, 309, 4722, 300, 291, 764, 337, 42856, 30, 50930], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 939, "seek": 268028, "start": 2691.6000000000004, "end": 2692.44, "text": " Oh, yeah, yeah.", "tokens": [50930, 876, 11, 1338, 11, 1338, 13, 50972], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 940, "seek": 268028, "start": 2692.44, "end": 2694.76, "text": " Common Crawl is kind of like an open source version", "tokens": [50972, 18235, 37877, 75, 307, 733, 295, 411, 364, 1269, 4009, 3037, 51088], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 941, "seek": 268028, "start": 2694.76, "end": 2697.84, "text": " of the web that runs, I think maybe quarterly.", "tokens": [51088, 295, 264, 3670, 300, 6676, 11, 286, 519, 1310, 38633, 13, 51242], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 942, "seek": 268028, "start": 2697.84, "end": 2698.88, "text": " I would have to check.", "tokens": [51242, 286, 576, 362, 281, 1520, 13, 51294], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 943, "seek": 268028, "start": 2698.88, "end": 2700.92, "text": " But yeah, if you go to like Common Crawl.org,", "tokens": [51294, 583, 1338, 11, 498, 291, 352, 281, 411, 18235, 37877, 75, 13, 4646, 11, 51396], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 944, "seek": 268028, "start": 2700.92, "end": 2703.84, "text": " you can download it, but warning, it's like very large.", "tokens": [51396, 291, 393, 5484, 309, 11, 457, 9164, 11, 309, 311, 411, 588, 2416, 13, 51542], "temperature": 0.0, "avg_logprob": -0.2070601279275459, "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.0008960488485172391}, {"id": 945, "seek": 271028, "start": 2711.28, "end": 2712.6000000000004, "text": " I have a question.", "tokens": [50414, 286, 362, 257, 1168, 13, 50480], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 946, "seek": 271028, "start": 2712.6000000000004, "end": 2714.2000000000003, "text": " I mean, you might have mentioned this briefly,", "tokens": [50480, 286, 914, 11, 291, 1062, 362, 2835, 341, 10515, 11, 50560], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 947, "seek": 271028, "start": 2714.2000000000003, "end": 2717.8, "text": " but I'm wondering how chatGBT and GPT-4 does on this.", "tokens": [50560, 457, 286, 478, 6359, 577, 5081, 8769, 51, 293, 26039, 51, 12, 19, 775, 322, 341, 13, 50740], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 948, "seek": 271028, "start": 2717.8, "end": 2721.6000000000004, "text": " Like does just more scale and pre-training data help", "tokens": [50740, 1743, 775, 445, 544, 4373, 293, 659, 12, 17227, 1760, 1412, 854, 50930], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 949, "seek": 271028, "start": 2721.6000000000004, "end": 2725.4, "text": " as well for low resource machine translation?", "tokens": [50930, 382, 731, 337, 2295, 7684, 3479, 12853, 30, 51120], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 950, "seek": 271028, "start": 2725.4, "end": 2726.8, "text": " Yeah, yeah, good question.", "tokens": [51120, 865, 11, 1338, 11, 665, 1168, 13, 51190], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 951, "seek": 271028, "start": 2726.8, "end": 2728.2000000000003, "text": " Actually, there have been some studies done", "tokens": [51190, 5135, 11, 456, 362, 668, 512, 5313, 1096, 51260], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 952, "seek": 271028, "start": 2728.2000000000003, "end": 2730.36, "text": " on like how, you know, these systems work.", "tokens": [51260, 322, 411, 577, 11, 291, 458, 11, 613, 3652, 589, 13, 51368], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 953, "seek": 271028, "start": 2730.36, "end": 2732.1600000000003, "text": " I think for high resource languages,", "tokens": [51368, 286, 519, 337, 1090, 7684, 8650, 11, 51458], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 954, "seek": 271028, "start": 2732.1600000000003, "end": 2735.0, "text": " it's actually quite beneficial to scale up.", "tokens": [51458, 309, 311, 767, 1596, 14072, 281, 4373, 493, 13, 51600], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 955, "seek": 271028, "start": 2735.0, "end": 2737.36, "text": " I think part of that is because the models", "tokens": [51600, 286, 519, 644, 295, 300, 307, 570, 264, 5245, 51718], "temperature": 0.0, "avg_logprob": -0.3150897889625369, "compression_ratio": 1.5944055944055944, "no_speech_prob": 0.0006560345063917339}, {"id": 956, "seek": 273736, "start": 2737.56, "end": 2739.2400000000002, "text": " have some innate generalization.", "tokens": [50374, 362, 512, 41766, 2674, 2144, 13, 50458], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 957, "seek": 273736, "start": 2739.2400000000002, "end": 2740.96, "text": " And so one of the challenges that people talk", "tokens": [50458, 400, 370, 472, 295, 264, 4759, 300, 561, 751, 50544], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 958, "seek": 273736, "start": 2740.96, "end": 2742.84, "text": " about different things in different languages.", "tokens": [50544, 466, 819, 721, 294, 819, 8650, 13, 50638], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 959, "seek": 273736, "start": 2742.84, "end": 2745.6, "text": " So like seeing that knowledge in another language", "tokens": [50638, 407, 411, 2577, 300, 3601, 294, 1071, 2856, 50776], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 960, "seek": 273736, "start": 2745.6, "end": 2747.48, "text": " can actually help the generalization.", "tokens": [50776, 393, 767, 854, 264, 2674, 2144, 13, 50870], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 961, "seek": 273736, "start": 2747.48, "end": 2751.08, "text": " But on low resource languages, it's, yeah,", "tokens": [50870, 583, 322, 2295, 7684, 8650, 11, 309, 311, 11, 1338, 11, 51050], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 962, "seek": 273736, "start": 2751.08, "end": 2753.6800000000003, "text": " the performance is pretty difficult,", "tokens": [51050, 264, 3389, 307, 1238, 2252, 11, 51180], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 963, "seek": 273736, "start": 2753.6800000000003, "end": 2756.6, "text": " especially on some of these translation benchmarks.", "tokens": [51180, 2318, 322, 512, 295, 613, 12853, 43751, 13, 51326], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 964, "seek": 273736, "start": 2756.6, "end": 2759.04, "text": " I also think that language models,", "tokens": [51326, 286, 611, 519, 300, 2856, 5245, 11, 51448], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 965, "seek": 273736, "start": 2759.04, "end": 2762.0, "text": " in terms of being trained for like a translation objective,", "tokens": [51448, 294, 2115, 295, 885, 8895, 337, 411, 257, 12853, 10024, 11, 51596], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 966, "seek": 273736, "start": 2762.0, "end": 2764.32, "text": " tend to score worse on translation benchmarks", "tokens": [51596, 3928, 281, 6175, 5324, 322, 12853, 43751, 51712], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 967, "seek": 273736, "start": 2764.32, "end": 2766.48, "text": " because language models are like approximately", "tokens": [51712, 570, 2856, 5245, 366, 411, 10447, 51820], "temperature": 0.0, "avg_logprob": -0.30202118689272583, "compression_ratio": 1.8253424657534247, "no_speech_prob": 0.0008554283995181322}, {"id": 968, "seek": 276648, "start": 2767.48, "end": 2769.68, "text": " capturing the same thing or as translation models.", "tokens": [50414, 23384, 264, 912, 551, 420, 382, 12853, 5245, 13, 50524], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 969, "seek": 276648, "start": 2769.68, "end": 2772.64, "text": " So you really try to align the meaning a little bit more.", "tokens": [50524, 407, 291, 534, 853, 281, 7975, 264, 3620, 257, 707, 857, 544, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 970, "seek": 276648, "start": 2773.8, "end": 2775.2400000000002, "text": " But yeah, so I think for low resource,", "tokens": [50730, 583, 1338, 11, 370, 286, 519, 337, 2295, 7684, 11, 50802], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 971, "seek": 276648, "start": 2775.2400000000002, "end": 2777.48, "text": " it's still pretty challenging.", "tokens": [50802, 309, 311, 920, 1238, 7595, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 972, "seek": 276648, "start": 2777.48, "end": 2779.32, "text": " But yeah, one thing that's interesting", "tokens": [50914, 583, 1338, 11, 472, 551, 300, 311, 1880, 51006], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 973, "seek": 276648, "start": 2779.32, "end": 2781.32, "text": " is for most English language models,", "tokens": [51006, 307, 337, 881, 3669, 2856, 5245, 11, 51106], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 974, "seek": 276648, "start": 2781.32, "end": 2783.4, "text": " they can actually do a reasonable job", "tokens": [51106, 436, 393, 767, 360, 257, 10585, 1691, 51210], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 975, "seek": 276648, "start": 2783.4, "end": 2784.84, "text": " at producing other languages", "tokens": [51210, 412, 10501, 661, 8650, 51282], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 976, "seek": 276648, "start": 2784.84, "end": 2787.0, "text": " because it's impossible to get rid of other languages", "tokens": [51282, 570, 309, 311, 6243, 281, 483, 3973, 295, 661, 8650, 51390], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 977, "seek": 276648, "start": 2787.0, "end": 2788.72, "text": " in your English specific data.", "tokens": [51390, 294, 428, 3669, 2685, 1412, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 978, "seek": 276648, "start": 2788.72, "end": 2792.2, "text": " So things like French or German will work reasonably.", "tokens": [51476, 407, 721, 411, 5522, 420, 6521, 486, 589, 23551, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1878443017470098, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.00023032486205920577}, {"id": 979, "seek": 279648, "start": 2796.48, "end": 2799.4, "text": " So just to clarify, you said language models trained", "tokens": [50364, 407, 445, 281, 17594, 11, 291, 848, 2856, 5245, 8895, 50510], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 980, "seek": 279648, "start": 2799.4, "end": 2803.6, "text": " with a translation objective do better, right?", "tokens": [50510, 365, 257, 12853, 10024, 360, 1101, 11, 558, 30, 50720], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 981, "seek": 279648, "start": 2803.6, "end": 2806.04, "text": " Because, right?", "tokens": [50720, 1436, 11, 558, 30, 50842], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 982, "seek": 279648, "start": 2806.04, "end": 2806.88, "text": " They tend to do better.", "tokens": [50842, 814, 3928, 281, 360, 1101, 13, 50884], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 983, "seek": 279648, "start": 2806.88, "end": 2809.2, "text": " Like if you fine tune for the translation task,", "tokens": [50884, 1743, 498, 291, 2489, 10864, 337, 264, 12853, 5633, 11, 51000], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 984, "seek": 279648, "start": 2809.2, "end": 2810.76, "text": " it will tend to do better.", "tokens": [51000, 309, 486, 3928, 281, 360, 1101, 13, 51078], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 985, "seek": 279648, "start": 2810.76, "end": 2812.2400000000002, "text": " Well, that makes sense compared to like,", "tokens": [51078, 1042, 11, 300, 1669, 2020, 5347, 281, 411, 11, 51152], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 986, "seek": 279648, "start": 2812.2400000000002, "end": 2816.8, "text": " for example, some few shot in context examples.", "tokens": [51152, 337, 1365, 11, 512, 1326, 3347, 294, 4319, 5110, 13, 51380], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 987, "seek": 279648, "start": 2816.8, "end": 2819.08, "text": " Right, right, exactly, exactly.", "tokens": [51380, 1779, 11, 558, 11, 2293, 11, 2293, 13, 51494], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 988, "seek": 279648, "start": 2819.08, "end": 2820.64, "text": " And one other question is,", "tokens": [51494, 400, 472, 661, 1168, 307, 11, 51572], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 989, "seek": 279648, "start": 2823.76, "end": 2825.64, "text": " do you see this being similar to, for example,", "tokens": [51728, 360, 291, 536, 341, 885, 2531, 281, 11, 337, 1365, 11, 51822], "temperature": 0.0, "avg_logprob": -0.22212304567035876, "compression_ratio": 1.6970954356846473, "no_speech_prob": 0.000345841544913128}, {"id": 990, "seek": 282564, "start": 2825.64, "end": 2828.2, "text": " fine tuning on particular expert domains,", "tokens": [50364, 2489, 15164, 322, 1729, 5844, 25514, 11, 50492], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 991, "seek": 282564, "start": 2828.2, "end": 2833.2, "text": " which might also have less data and low resource,", "tokens": [50492, 597, 1062, 611, 362, 1570, 1412, 293, 2295, 7684, 11, 50742], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 992, "seek": 282564, "start": 2833.92, "end": 2836.72, "text": " and as well as domain specific jargon and so forth?", "tokens": [50778, 293, 382, 731, 382, 9274, 2685, 15181, 10660, 293, 370, 5220, 30, 50918], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 993, "seek": 282564, "start": 2838.7999999999997, "end": 2841.8799999999997, "text": " Yeah, I mean, I think if we were to restart this project now,", "tokens": [51022, 865, 11, 286, 914, 11, 286, 519, 498, 321, 645, 281, 21022, 341, 1716, 586, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 994, "seek": 282564, "start": 2841.8799999999997, "end": 2843.48, "text": " I think that would be one of the first things", "tokens": [51176, 286, 519, 300, 576, 312, 472, 295, 264, 700, 721, 51256], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 995, "seek": 282564, "start": 2843.48, "end": 2846.12, "text": " we also explored, or at least like an extremely strong", "tokens": [51256, 321, 611, 24016, 11, 420, 412, 1935, 411, 364, 4664, 2068, 51388], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 996, "seek": 282564, "start": 2846.12, "end": 2848.44, "text": " baseline where if you like take some of the data", "tokens": [51388, 20518, 689, 498, 291, 411, 747, 512, 295, 264, 1412, 51504], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 997, "seek": 282564, "start": 2848.44, "end": 2853.12, "text": " and you try to fine tune or try to do domain adaptation,", "tokens": [51504, 293, 291, 853, 281, 2489, 10864, 420, 853, 281, 360, 9274, 21549, 11, 51738], "temperature": 0.0, "avg_logprob": -0.1473780747886016, "compression_ratio": 1.6816326530612244, "no_speech_prob": 0.0007541424711234868}, {"id": 998, "seek": 285312, "start": 2853.12, "end": 2855.52, "text": " I think that's also where like some of the like retrieval", "tokens": [50364, 286, 519, 300, 311, 611, 689, 411, 512, 295, 264, 411, 19817, 3337, 50484], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 999, "seek": 285312, "start": 2855.52, "end": 2858.72, "text": " type approaches go in for translation,", "tokens": [50484, 2010, 11587, 352, 294, 337, 12853, 11, 50644], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1000, "seek": 285312, "start": 2858.72, "end": 2860.6, "text": " but also large language modeling work", "tokens": [50644, 457, 611, 2416, 2856, 15983, 589, 50738], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1001, "seek": 285312, "start": 2860.6, "end": 2862.52, "text": " where you try to have like a separate domain", "tokens": [50738, 689, 291, 853, 281, 362, 411, 257, 4994, 9274, 50834], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1002, "seek": 285312, "start": 2862.52, "end": 2865.92, "text": " that you can like retrieve some text in for adaptation.", "tokens": [50834, 300, 291, 393, 411, 30254, 512, 2487, 294, 337, 21549, 13, 51004], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1003, "seek": 285312, "start": 2865.92, "end": 2868.52, "text": " I think all of those approaches are pretty promising.", "tokens": [51004, 286, 519, 439, 295, 729, 11587, 366, 1238, 20257, 13, 51134], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1004, "seek": 285312, "start": 2874.52, "end": 2876.2799999999997, "text": " Arcade, any other questions?", "tokens": [51434, 21727, 762, 11, 604, 661, 1651, 30, 51522], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1005, "seek": 285312, "start": 2879.44, "end": 2881.56, "text": " One quick one on the point of the video question.", "tokens": [51680, 1485, 1702, 472, 322, 264, 935, 295, 264, 960, 1168, 13, 51786], "temperature": 0.0, "avg_logprob": -0.23798652317212976, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.00011407355486880988}, {"id": 1006, "seek": 288156, "start": 2881.56, "end": 2882.7599999999998, "text": " You're looking at one of the slides,", "tokens": [50364, 509, 434, 1237, 412, 472, 295, 264, 9788, 11, 50424], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1007, "seek": 288156, "start": 2882.7599999999998, "end": 2885.96, "text": " I think you showed some peak results with zero shot", "tokens": [50424, 286, 519, 291, 4712, 512, 10651, 3542, 365, 4018, 3347, 50584], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1008, "seek": 288156, "start": 2885.96, "end": 2890.2799999999997, "text": " that were higher than just the base model.", "tokens": [50584, 300, 645, 2946, 813, 445, 264, 3096, 2316, 13, 50800], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1009, "seek": 288156, "start": 2890.2799999999997, "end": 2892.88, "text": " Do you think that's because there might still be", "tokens": [50800, 1144, 291, 519, 300, 311, 570, 456, 1062, 920, 312, 50930], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1010, "seek": 288156, "start": 2892.88, "end": 2896.04, "text": " some overfitting on those low resource languages?", "tokens": [50930, 512, 670, 69, 2414, 322, 729, 2295, 7684, 8650, 30, 51088], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1011, "seek": 288156, "start": 2896.04, "end": 2897.0, "text": " Yeah, good question.", "tokens": [51088, 865, 11, 665, 1168, 13, 51136], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1012, "seek": 288156, "start": 2897.0, "end": 2899.72, "text": " So for our large scale mining,", "tokens": [51136, 407, 337, 527, 2416, 4373, 15512, 11, 51272], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1013, "seek": 288156, "start": 2899.72, "end": 2903.0, "text": " we don't mind like every single possible cross pair.", "tokens": [51272, 321, 500, 380, 1575, 411, 633, 2167, 1944, 3278, 6119, 13, 51436], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1014, "seek": 288156, "start": 2903.0, "end": 2906.4, "text": " So like Icelandic will love,", "tokens": [51436, 407, 411, 28004, 299, 486, 959, 11, 51606], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1015, "seek": 288156, "start": 2906.4, "end": 2908.36, "text": " it's probably like not like the most", "tokens": [51606, 309, 311, 1391, 411, 406, 411, 264, 881, 51704], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1016, "seek": 288156, "start": 2908.36, "end": 2910.08, "text": " in demand translation direction.", "tokens": [51704, 294, 4733, 12853, 3513, 13, 51790], "temperature": 0.0, "avg_logprob": -0.18587347942849863, "compression_ratio": 1.5955882352941178, "no_speech_prob": 0.00038578850217163563}, {"id": 1017, "seek": 291008, "start": 2910.08, "end": 2912.92, "text": " And so we did not mind like all 200 times 200", "tokens": [50364, 400, 370, 321, 630, 406, 1575, 411, 439, 2331, 1413, 2331, 50506], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1018, "seek": 291008, "start": 2912.92, "end": 2914.04, "text": " because it's like really producing", "tokens": [50506, 570, 309, 311, 411, 534, 10501, 50562], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1019, "seek": 291008, "start": 2914.04, "end": 2915.96, "text": " like a combinatorial explosion.", "tokens": [50562, 411, 257, 2512, 31927, 831, 15673, 13, 50658], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1020, "seek": 291008, "start": 2915.96, "end": 2918.4, "text": " And so that's where the zero shot results come from", "tokens": [50658, 400, 370, 300, 311, 689, 264, 4018, 3347, 3542, 808, 490, 50780], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1021, "seek": 291008, "start": 2918.4, "end": 2920.56, "text": " where you don't need,", "tokens": [50780, 689, 291, 500, 380, 643, 11, 50888], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1022, "seek": 291008, "start": 2920.56, "end": 2923.56, "text": " we don't have training data directionally in that pair,", "tokens": [50888, 321, 500, 380, 362, 3097, 1412, 3513, 379, 294, 300, 6119, 11, 51038], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1023, "seek": 291008, "start": 2923.56, "end": 2926.52, "text": " but the model has seen both the input and the output.", "tokens": [51038, 457, 264, 2316, 575, 1612, 1293, 264, 4846, 293, 264, 5598, 13, 51186], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1024, "seek": 291008, "start": 2926.52, "end": 2930.2, "text": " And so I think those results are pretty good.", "tokens": [51186, 400, 370, 286, 519, 729, 3542, 366, 1238, 665, 13, 51370], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1025, "seek": 291008, "start": 2930.2, "end": 2932.72, "text": " Well, they're good for certain languages,", "tokens": [51370, 1042, 11, 436, 434, 665, 337, 1629, 8650, 11, 51496], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1026, "seek": 291008, "start": 2932.72, "end": 2936.7599999999998, "text": " which I think goes to show like the generalization capability", "tokens": [51496, 597, 286, 519, 1709, 281, 855, 411, 264, 2674, 2144, 13759, 51698], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1027, "seek": 291008, "start": 2936.7599999999998, "end": 2938.7999999999997, "text": " and it's not like as critical", "tokens": [51698, 293, 309, 311, 406, 411, 382, 4924, 51800], "temperature": 0.0, "avg_logprob": -0.0974466914222354, "compression_ratio": 1.676056338028169, "no_speech_prob": 9.912095993058756e-05}, {"id": 1028, "seek": 293880, "start": 2938.8, "end": 2940.76, "text": " to have like every single pair covered,", "tokens": [50364, 281, 362, 411, 633, 2167, 6119, 5343, 11, 50462], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1029, "seek": 293880, "start": 2940.76, "end": 2942.44, "text": " but many of them are not as good.", "tokens": [50462, 457, 867, 295, 552, 366, 406, 382, 665, 13, 50546], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1030, "seek": 293880, "start": 2942.44, "end": 2944.84, "text": " And so you see overall the performance is lower,", "tokens": [50546, 400, 370, 291, 536, 4787, 264, 3389, 307, 3126, 11, 50666], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1031, "seek": 293880, "start": 2944.84, "end": 2946.0800000000004, "text": " even though on certain languages,", "tokens": [50666, 754, 1673, 322, 1629, 8650, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1032, "seek": 293880, "start": 2946.0800000000004, "end": 2948.0, "text": " it can perform better than you expect.", "tokens": [50728, 309, 393, 2042, 1101, 813, 291, 2066, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1033, "seek": 293880, "start": 2948.0, "end": 2950.1600000000003, "text": " But that's because it has seen the input and the output.", "tokens": [50824, 583, 300, 311, 570, 309, 575, 1612, 264, 4846, 293, 264, 5598, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1034, "seek": 293880, "start": 2950.1600000000003, "end": 2952.96, "text": " It's not zero shot on like completely unseen language.", "tokens": [50932, 467, 311, 406, 4018, 3347, 322, 411, 2584, 40608, 2856, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1035, "seek": 293880, "start": 2962.0, "end": 2963.1600000000003, "text": " I have a question.", "tokens": [51524, 286, 362, 257, 1168, 13, 51582], "temperature": 0.0, "avg_logprob": -0.1434136520732533, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.0010152522008866072}, {"id": 1036, "seek": 296316, "start": 2964.12, "end": 2968.12, "text": " But I wanted you to also, you know,", "tokens": [50412, 583, 286, 1415, 291, 281, 611, 11, 291, 458, 11, 50612], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1037, "seek": 296316, "start": 2968.12, "end": 2971.0, "text": " do something related to transcription", "tokens": [50612, 360, 746, 4077, 281, 35288, 50756], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1038, "seek": 296316, "start": 2971.0, "end": 2972.64, "text": " or audio information?", "tokens": [50756, 420, 6278, 1589, 30, 50838], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1039, "seek": 296316, "start": 2974.3999999999996, "end": 2975.64, "text": " Yeah, good question.", "tokens": [50926, 865, 11, 665, 1168, 13, 50988], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1040, "seek": 296316, "start": 2975.64, "end": 2978.96, "text": " So in this project, no, not so much transcription,", "tokens": [50988, 407, 294, 341, 1716, 11, 572, 11, 406, 370, 709, 35288, 11, 51154], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1041, "seek": 296316, "start": 2978.96, "end": 2980.7599999999998, "text": " but we had a follow up work that we released", "tokens": [51154, 457, 321, 632, 257, 1524, 493, 589, 300, 321, 4736, 51244], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1042, "seek": 296316, "start": 2980.7599999999998, "end": 2984.7999999999997, "text": " actually just like a month or so ago called seamless M4T,", "tokens": [51244, 767, 445, 411, 257, 1618, 420, 370, 2057, 1219, 28677, 376, 19, 51, 11, 51446], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1043, "seek": 296316, "start": 2984.7999999999997, "end": 2986.12, "text": " which is like a joint model", "tokens": [51446, 597, 307, 411, 257, 7225, 2316, 51512], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1044, "seek": 296316, "start": 2986.12, "end": 2988.3999999999996, "text": " for both speech and text translation.", "tokens": [51512, 337, 1293, 6218, 293, 2487, 12853, 13, 51626], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1045, "seek": 296316, "start": 2988.3999999999996, "end": 2991.44, "text": " And that's where we do leverage a lot of audio transcription", "tokens": [51626, 400, 300, 311, 689, 321, 360, 13982, 257, 688, 295, 6278, 35288, 51778], "temperature": 0.0, "avg_logprob": -0.21686416697279315, "compression_ratio": 1.588, "no_speech_prob": 0.002547619864344597}, {"id": 1046, "seek": 299144, "start": 2991.44, "end": 2994.32, "text": " because that also has like, it helps us bridge,", "tokens": [50364, 570, 300, 611, 575, 411, 11, 309, 3665, 505, 7283, 11, 50508], "temperature": 0.0, "avg_logprob": -0.21381771564483643, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.00010388113878434524}, {"id": 1047, "seek": 299144, "start": 2994.32, "end": 2996.44, "text": " you know, like the spoken data and the text data", "tokens": [50508, 291, 458, 11, 411, 264, 10759, 1412, 293, 264, 2487, 1412, 50614], "temperature": 0.0, "avg_logprob": -0.21381771564483643, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.00010388113878434524}, {"id": 1048, "seek": 299144, "start": 2996.44, "end": 2998.12, "text": " to leverage both of them together.", "tokens": [50614, 281, 13982, 1293, 295, 552, 1214, 13, 50698], "temperature": 0.0, "avg_logprob": -0.21381771564483643, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.00010388113878434524}, {"id": 1049, "seek": 299144, "start": 3008.2400000000002, "end": 3010.96, "text": " Wait, just to clarify, the supervised fine tuning", "tokens": [51204, 3802, 11, 445, 281, 17594, 11, 264, 46533, 2489, 15164, 51340], "temperature": 0.0, "avg_logprob": -0.21381771564483643, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.00010388113878434524}, {"id": 1050, "seek": 299144, "start": 3010.96, "end": 3015.56, "text": " it worked better, right, compared to other methods.", "tokens": [51340, 309, 2732, 1101, 11, 558, 11, 5347, 281, 661, 7150, 13, 51570], "temperature": 0.0, "avg_logprob": -0.21381771564483643, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.00010388113878434524}, {"id": 1051, "seek": 299144, "start": 3016.6, "end": 3019.12, "text": " So actually in this work, it was as a couple of years ago now.", "tokens": [51622, 407, 767, 294, 341, 589, 11, 309, 390, 382, 257, 1916, 295, 924, 2057, 586, 13, 51748], "temperature": 0.0, "avg_logprob": -0.21381771564483643, "compression_ratio": 1.5179487179487179, "no_speech_prob": 0.00010388113878434524}, {"id": 1052, "seek": 301912, "start": 3019.12, "end": 3023.52, "text": " So supervised fine tuning wasn't as common as it was now.", "tokens": [50364, 407, 46533, 2489, 15164, 2067, 380, 382, 2689, 382, 309, 390, 586, 13, 50584], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1053, "seek": 301912, "start": 3023.52, "end": 3025.24, "text": " But I think in the literature,", "tokens": [50584, 583, 286, 519, 294, 264, 10394, 11, 50670], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1054, "seek": 301912, "start": 3025.24, "end": 3026.96, "text": " if you want to use like a large language model", "tokens": [50670, 498, 291, 528, 281, 764, 411, 257, 2416, 2856, 2316, 50756], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1055, "seek": 301912, "start": 3026.96, "end": 3029.16, "text": " to do translation, it's currently best yet", "tokens": [50756, 281, 360, 12853, 11, 309, 311, 4362, 1151, 1939, 50866], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1056, "seek": 301912, "start": 3029.16, "end": 3031.6, "text": " if you do some supervised fine tuning.", "tokens": [50866, 498, 291, 360, 512, 46533, 2489, 15164, 13, 50988], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1057, "seek": 301912, "start": 3031.6, "end": 3033.12, "text": " I'm just wondering about that", "tokens": [50988, 286, 478, 445, 6359, 466, 300, 51064], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1058, "seek": 301912, "start": 3033.12, "end": 3034.7999999999997, "text": " because the way as humans, right,", "tokens": [51064, 570, 264, 636, 382, 6255, 11, 558, 11, 51148], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1059, "seek": 301912, "start": 3034.7999999999997, "end": 3039.3599999999997, "text": " we don't just learn by looking at pairs of the same thing", "tokens": [51148, 321, 500, 380, 445, 1466, 538, 1237, 412, 15494, 295, 264, 912, 551, 51376], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1060, "seek": 301912, "start": 3039.3599999999997, "end": 3040.2, "text": " in different languages", "tokens": [51376, 294, 819, 8650, 51418], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1061, "seek": 301912, "start": 3040.2, "end": 3044.3599999999997, "text": " and kind of memorizing how to map from one to the other.", "tokens": [51418, 293, 733, 295, 10560, 3319, 577, 281, 4471, 490, 472, 281, 264, 661, 13, 51626], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1062, "seek": 301912, "start": 3044.3599999999997, "end": 3046.24, "text": " We kind of learn in a more unsupervised way", "tokens": [51626, 492, 733, 295, 1466, 294, 257, 544, 2693, 12879, 24420, 636, 51720], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1063, "seek": 301912, "start": 3046.24, "end": 3048.4, "text": " where if we know both languages,", "tokens": [51720, 689, 498, 321, 458, 1293, 8650, 11, 51828], "temperature": 0.0, "avg_logprob": -0.09831458904125072, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0008028720621950924}, {"id": 1064, "seek": 304840, "start": 3048.4, "end": 3051.84, "text": " then we can kind of naturally translate between them.", "tokens": [50364, 550, 321, 393, 733, 295, 8195, 13799, 1296, 552, 13, 50536], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1065, "seek": 304840, "start": 3054.48, "end": 3057.12, "text": " But I guess it makes sense for an LLMY", "tokens": [50668, 583, 286, 2041, 309, 1669, 2020, 337, 364, 441, 43, 25859, 50800], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1066, "seek": 304840, "start": 3057.12, "end": 3061.28, "text": " having supervised examples would help, yeah.", "tokens": [50800, 1419, 46533, 5110, 576, 854, 11, 1338, 13, 51008], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1067, "seek": 304840, "start": 3061.28, "end": 3065.0, "text": " Yeah, I mean, I think as like the base foundation model", "tokens": [51008, 865, 11, 286, 914, 11, 286, 519, 382, 411, 264, 3096, 7030, 2316, 51194], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1068, "seek": 304840, "start": 3065.0, "end": 3066.8, "text": " continues to improve in quality,", "tokens": [51194, 6515, 281, 3470, 294, 3125, 11, 51284], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1069, "seek": 304840, "start": 3066.8, "end": 3069.2400000000002, "text": " I think that's where the quality will probably improve", "tokens": [51284, 286, 519, 300, 311, 689, 264, 3125, 486, 1391, 3470, 51406], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1070, "seek": 304840, "start": 3069.2400000000002, "end": 3071.1600000000003, "text": " when you don't need less and less fine tuning.", "tokens": [51406, 562, 291, 500, 380, 643, 1570, 293, 1570, 2489, 15164, 13, 51502], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1071, "seek": 304840, "start": 3071.1600000000003, "end": 3073.56, "text": " I mean, do you think that's like the open AI approach?", "tokens": [51502, 286, 914, 11, 360, 291, 519, 300, 311, 411, 264, 1269, 7318, 3109, 30, 51622], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1072, "seek": 304840, "start": 3073.56, "end": 3075.36, "text": " Like if you have the best foundation model,", "tokens": [51622, 1743, 498, 291, 362, 264, 1151, 7030, 2316, 11, 51712], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1073, "seek": 304840, "start": 3075.36, "end": 3078.04, "text": " then you don't need as much like domain specific fine tuning.", "tokens": [51712, 550, 291, 500, 380, 643, 382, 709, 411, 9274, 2685, 2489, 15164, 13, 51846], "temperature": 0.0, "avg_logprob": -0.11864767299862358, "compression_ratio": 1.7526881720430108, "no_speech_prob": 5.647882426274009e-05}, {"id": 1074, "seek": 307804, "start": 3078.08, "end": 3079.84, "text": " I think like, you know, like at the start", "tokens": [50366, 286, 519, 411, 11, 291, 458, 11, 411, 412, 264, 722, 50454], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1075, "seek": 307804, "start": 3079.84, "end": 3081.4, "text": " when I started working on text generation,", "tokens": [50454, 562, 286, 1409, 1364, 322, 2487, 5125, 11, 50532], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1076, "seek": 307804, "start": 3081.4, "end": 3082.84, "text": " there was like translation researchers", "tokens": [50532, 456, 390, 411, 12853, 10309, 50604], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1077, "seek": 307804, "start": 3082.84, "end": 3084.04, "text": " and like summarization researchers", "tokens": [50604, 293, 411, 14611, 2144, 10309, 50664], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1078, "seek": 307804, "start": 3084.04, "end": 3085.48, "text": " and like question answering researchers", "tokens": [50664, 293, 411, 1168, 13430, 10309, 50736], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1079, "seek": 307804, "start": 3085.48, "end": 3087.24, "text": " and they like work very differently.", "tokens": [50736, 293, 436, 411, 589, 588, 7614, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1080, "seek": 307804, "start": 3087.24, "end": 3088.68, "text": " But now it's like, it's all driven", "tokens": [50824, 583, 586, 309, 311, 411, 11, 309, 311, 439, 9555, 50896], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1081, "seek": 307804, "start": 3088.68, "end": 3090.0, "text": " by the same underlying thing", "tokens": [50896, 538, 264, 912, 14217, 551, 50962], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1082, "seek": 307804, "start": 3090.0, "end": 3091.68, "text": " and you're not like a specialized", "tokens": [50962, 293, 291, 434, 406, 411, 257, 19813, 51046], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1083, "seek": 307804, "start": 3091.68, "end": 3093.7599999999998, "text": " summarization researcher anymore.", "tokens": [51046, 14611, 2144, 21751, 3602, 13, 51150], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1084, "seek": 307804, "start": 3095.48, "end": 3098.6, "text": " Right, I think that makes a lot of sense.", "tokens": [51236, 1779, 11, 286, 519, 300, 1669, 257, 688, 295, 2020, 13, 51392], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1085, "seek": 307804, "start": 3101.2799999999997, "end": 3103.0, "text": " Do we have any other questions?", "tokens": [51526, 1144, 321, 362, 604, 661, 1651, 30, 51612], "temperature": 0.0, "avg_logprob": -0.1953648669379098, "compression_ratio": 1.7926829268292683, "no_speech_prob": 9.311774192610756e-05}, {"id": 1086, "seek": 310300, "start": 3104.0, "end": 3105.0, "text": " Any questions?", "tokens": [50414, 2639, 1651, 30, 50464], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1087, "seek": 310300, "start": 3112.0, "end": 3114.0, "text": " Ron, any more in-person questions?", "tokens": [50814, 9949, 11, 604, 544, 294, 12, 10813, 1651, 30, 50914], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1088, "seek": 310300, "start": 3116.0, "end": 3117.0, "text": " Oh, don't think so.", "tokens": [51014, 876, 11, 500, 380, 519, 370, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1089, "seek": 310300, "start": 3118.0, "end": 3119.0, "text": " Okay, great.", "tokens": [51114, 1033, 11, 869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1090, "seek": 310300, "start": 3119.0, "end": 3120.0, "text": " All right.", "tokens": [51164, 1057, 558, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1091, "seek": 310300, "start": 3120.0, "end": 3123.0, "text": " Well, thank you, Angela, for the very interesting", "tokens": [51214, 1042, 11, 1309, 291, 11, 20848, 11, 337, 264, 588, 1880, 51364], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1092, "seek": 310300, "start": 3123.0, "end": 3126.0, "text": " and a great talk again and for taking the time.", "tokens": [51364, 293, 257, 869, 751, 797, 293, 337, 1940, 264, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1093, "seek": 310300, "start": 3126.0, "end": 3131.0, "text": " And we hope, yeah, we hope that you can keep in touch", "tokens": [51514, 400, 321, 1454, 11, 1338, 11, 321, 1454, 300, 291, 393, 1066, 294, 2557, 51764], "temperature": 0.0, "avg_logprob": -0.3855106830596924, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.0011672191321849823}, {"id": 1094, "seek": 313100, "start": 3131.0, "end": 3134.0, "text": " and if anybody has any other questions,", "tokens": [50364, 293, 498, 4472, 575, 604, 661, 1651, 11, 50514], "temperature": 0.0, "avg_logprob": -0.16610502615207579, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.004390103276818991}, {"id": 1095, "seek": 313100, "start": 3134.0, "end": 3137.0, "text": " feel free to get in touch with Angela.", "tokens": [50514, 841, 1737, 281, 483, 294, 2557, 365, 20848, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16610502615207579, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.004390103276818991}, {"id": 1096, "seek": 313100, "start": 3137.0, "end": 3139.0, "text": " All right, thanks so much for having me today.", "tokens": [50664, 1057, 558, 11, 3231, 370, 709, 337, 1419, 385, 965, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16610502615207579, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.004390103276818991}, {"id": 1097, "seek": 313100, "start": 3139.0, "end": 3140.0, "text": " Bye, everyone.", "tokens": [50764, 4621, 11, 1518, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16610502615207579, "compression_ratio": 1.238938053097345, "no_speech_prob": 0.004390103276818991}], "language": "en"}