WEBVTT

00:00.000 --> 00:16.560
Being a speaker at this seminar series is, I mean, it means a lot to me personally.

00:16.560 --> 00:22.040
When I was an undergrad back in 2006, 2007, I've been meaning to learn about HCI, but

00:22.040 --> 00:26.420
I didn't really have that many resources, so I had to rely on online resources.

00:26.940 --> 00:31.340
This seminar series recordings have been posted online, and I think I've watched

00:31.340 --> 00:34.860
pretty much everything to really learn about HCI.

00:34.860 --> 00:38.740
And then I came as a master's student here in 2008 and

00:38.740 --> 00:42.940
took 547 pretty much for the entire two years I was here.

00:42.940 --> 00:49.300
And now I feel great that I get a chance to speak as a speaker, so this is great.

00:50.340 --> 00:54.020
Today I want to talk about interaction centric AI.

00:54.020 --> 01:00.340
This is a reprise of the New Europe's keynote talk that I gave two weeks ago

01:00.340 --> 01:02.700
in front of thousands of AI researchers.

01:02.700 --> 01:06.860
I tried to reframe it a little bit so that it's more customized for

01:06.860 --> 01:10.020
an HCI audience rather than an AI audience.

01:10.020 --> 01:15.300
But the idea is that I want to think about using human AI interaction

01:15.300 --> 01:18.060
at the center of developing AI technologies.

01:18.060 --> 01:21.180
And of course, I don't have to preach to the choir that human AI interaction

01:21.180 --> 01:26.340
actually matters, but diving deeper based on my experience of building

01:26.340 --> 01:30.300
these interactive systems in different contexts, like education, discussion,

01:30.300 --> 01:34.740
decision making, I want to dive deeper and report some of the detailed

01:34.740 --> 01:38.900
interactions that we've been observing and learning from.

01:38.900 --> 01:44.540
And think about what it means to design human AI interaction in various contexts

01:44.540 --> 01:48.460
and what are some action items moving forward as a community.

01:49.460 --> 01:53.060
So let's first start with some definitions and terms.

01:53.060 --> 02:00.300
I would say the dominant paradigm for developing AI technologies has been model centric.

02:00.300 --> 02:05.180
The idea is to build a model with high accuracy and we want to evaluate it

02:05.180 --> 02:08.740
against unseen examples for its generalizability.

02:08.740 --> 02:13.820
And benchmarks have been great in that they could help us competitively

02:14.220 --> 02:18.940
compare different models' performances, which could be useful in making

02:18.940 --> 02:22.260
scientific advances possible.

02:22.260 --> 02:25.900
And more recently, people have been talking a lot about data centric AI,

02:25.900 --> 02:30.780
where the idea is using this nicely performing model, what is a good,

02:30.780 --> 02:35.460
sort of robust and efficient data pipeline around it in terms of collection

02:35.460 --> 02:40.820
of the data, processing of it, cleaning of it, so that the model actually performs

02:40.820 --> 02:43.060
really well in different contexts.

02:43.100 --> 02:48.500
And here the focus is acquiring quality data and setting the pipeline in a way

02:48.500 --> 02:51.340
that really helps the machine perform its best.

02:52.980 --> 02:59.660
And these two paradigms are great, but then what is slightly missing is the user

02:59.660 --> 03:04.580
who's using these AI technologies and those who are affected by what the AI

03:04.580 --> 03:06.020
systems give you.

03:06.020 --> 03:12.140
So interaction centric AI is sort of my term in some contrast to model

03:12.140 --> 03:16.300
centric and data centric AI, where the goal would be basically what HCI

03:16.300 --> 03:20.780
researchers do in this context, like improving the user experience by

03:20.780 --> 03:23.980
building usable and useful applications.

03:23.980 --> 03:28.860
And the unit that we often grapple with is a human AI interaction.

03:28.860 --> 03:32.340
And you might be wondering, is this some sort of like marketing term?

03:32.340 --> 03:37.260
Like how is it different from human centric AI we've been talking all about?

03:37.260 --> 03:41.340
It's largely similar, so I'm not trying to say I invented this new term or

03:41.340 --> 03:45.820
anything, but I want to focus our attention to the interaction that is

03:45.820 --> 03:50.540
happening between humans and AI and the complex relationships and the dynamics

03:50.540 --> 03:54.620
that are happening between the two, rather than focusing on just the humans

03:54.620 --> 03:56.140
or machine alone.

03:56.140 --> 04:01.100
So I can say that that's the focus of where my discussion will be today.

04:03.260 --> 04:09.780
So let's say you're this AI researcher and your team has built this amazing model.

04:09.780 --> 04:14.900
So this is actually something that I copied and pasted from one of the

04:14.900 --> 04:16.500
diffusion models papers.

04:16.500 --> 04:20.540
I don't know what they actually mean, some of them I understand.

04:20.540 --> 04:25.300
But basically, this is what you have as an AI researcher.

04:25.300 --> 04:29.700
But what would a person using this kind of AI want to do with it?

04:29.700 --> 04:31.140
Here's an example.

04:31.140 --> 04:36.340
So this is a Twitch streamer in South Korea who was trying to use this

04:36.340 --> 04:41.940
diffusion-based text to image generation model to create this image of an

04:41.940 --> 04:46.420
animated character eating ramen with chopsticks with noodles around the

04:46.420 --> 04:47.300
character.

04:47.300 --> 04:52.500
So this is the roughly sketched out goal that the user company has.

04:52.500 --> 04:59.220
And he ended up spending two hours fiddling with text-based prompts to get at

04:59.220 --> 05:01.460
the final image that he wants.

05:01.460 --> 05:04.820
And this is somewhat similar to what Manish shared a couple of weeks ago at

05:04.820 --> 05:10.580
the HAI conference in terms of what he had to do with the prompt-based interface.

05:10.580 --> 05:14.500
And here, the entire two-hour journey was live streamed.

05:14.500 --> 05:19.060
So I want to kind of share a quick summary of what happened in that stream.

05:19.060 --> 05:22.740
And of course, we need something in the middle to bridge between the technology

05:23.380 --> 05:24.740
and the human user.

05:24.740 --> 05:29.540
And that's what we have, the prompt-based interface and interaction that's

05:29.540 --> 05:30.500
happening between the two.

05:31.380 --> 05:35.140
So the streamer started by something simple and obvious.

05:35.140 --> 05:37.780
The prompt says, eating ramen, and this is what he got.

05:37.780 --> 05:42.740
It's okay, it's kind of there, but the bowl is perhaps too large.

05:42.740 --> 05:44.820
The chopsticks are all to be placed.

05:44.820 --> 05:49.940
And he heard from somewhere that adding a full sentence might make things better.

05:49.940 --> 05:52.500
So he goes, she is eating ramen.

05:53.780 --> 05:58.900
She is eating ramen, for sure, but you can see that something's not quite right.

05:59.220 --> 06:03.220
So he keeps going on by adding more descriptions.

06:03.220 --> 06:06.660
And the prompt is definitely getting longer.

06:07.620 --> 06:12.340
And it seems that the AI is not quite getting how chopsticks should be used

06:12.340 --> 06:14.900
and how many should be used.

06:14.900 --> 06:20.020
So he keeps adding these descriptions to really explain what it means to use chopsticks.

06:20.340 --> 06:30.900
And to be fair, there's hair, there's chopsticks, there's noodles.

06:30.900 --> 06:33.060
So it's in computer graphics.

06:33.060 --> 06:35.860
Dealing with human hair, I heard, is a really tough challenge.

06:35.860 --> 06:41.860
And maybe for AI, it's also kind of struggling to deal with all these similar looking objects.

06:41.860 --> 06:47.620
And it doesn't really seem to get how to differentiate between chopsticks and noodles.

06:48.580 --> 06:52.340
And another interesting aspect was that since it was a live stream,

06:52.340 --> 06:59.060
the viewers were actively participating in recommending new prompts to try out,

06:59.060 --> 07:00.900
sharing their interpretations.

07:00.900 --> 07:05.860
And this is somewhat of a collaborative mental model construction process,

07:05.860 --> 07:08.260
if you will, as a group of people.

07:08.260 --> 07:10.660
They are really trying to figure out what's going on.

07:10.660 --> 07:14.180
And now the prompt is five lines long.

07:14.740 --> 07:19.140
And the service that this streamer was using was supporting variations,

07:19.140 --> 07:22.340
where you could pick an image and say create some variations.

07:22.340 --> 07:26.100
And he was referring to this interaction as variation gacha.

07:26.100 --> 07:30.340
So gacha is a Japanese word for like a random box or blind box.

07:30.340 --> 07:35.140
And this kind of tells us that how unpredictable this sort of interface is.

07:35.140 --> 07:40.740
Once you hit the generate button, the user doesn't really have a good sense of knowing what to expect.

07:41.220 --> 07:45.140
And this is the actual stream, as you can see, like his praying,

07:45.140 --> 07:50.020
and which also tells us about the usability of this sort of system.

07:50.020 --> 07:54.660
He doesn't have a good way of knowing what to expect, so that he actually has to pray.

07:56.660 --> 08:01.140
After two hours of hard work, this is the final image that he landed.

08:01.140 --> 08:04.580
And it looks pretty good, and he claims victory.

08:04.580 --> 08:09.380
But then look at what he had to do at the top, right?

08:09.700 --> 08:14.740
At the top, there are seven lines of prompt that he had to write.

08:15.620 --> 08:19.060
And arguably, this is natural language.

08:19.780 --> 08:22.420
But I would say this is really pseudo-natural language.

08:23.700 --> 08:27.380
And so this is basically the experience that he had to go through.

08:27.380 --> 08:29.220
So is this a good interface?

08:29.220 --> 08:34.180
And I sort of got inspired by Manish's discussion of discussing the usability

08:34.180 --> 08:36.900
of these text prompt-based interfaces.

08:37.540 --> 08:38.740
There are some good elements, right?

08:38.740 --> 08:39.780
It's quite intuitive.

08:39.780 --> 08:43.940
You can use natural language, or you believe natural language could be used.

08:44.500 --> 08:47.300
And the output is presented in a visual manner,

08:47.300 --> 08:51.540
which helps you kind of understand whether you got the image that you like or not,

08:51.540 --> 08:52.980
so that you can sort of debug.

08:53.780 --> 08:56.980
And there are some interactions that are supported,

08:56.980 --> 09:02.580
like variations and seeds and like words that should not be used and things like that.

09:03.540 --> 09:07.140
But there are many ways in which this interface actually fails

09:07.140 --> 09:08.820
to support what the actual user wants.

09:09.700 --> 09:11.860
He had to rely on trial and error.

09:11.860 --> 09:16.420
And just the fact that he had to spend two hours to get that image

09:16.420 --> 09:19.060
seems to suggest that something is really wrong.

09:19.940 --> 09:21.940
And of course, it was not really predictable

09:21.940 --> 09:26.180
and lack of specific feedback on the effect of what specific words

09:26.180 --> 09:29.860
in the prompt had influenced on the final outcome.

09:29.940 --> 09:33.060
These links were often missing, which made it really difficult.

09:34.340 --> 09:38.660
So is this really just a problem for these text-based prompt-based systems?

09:39.300 --> 09:44.340
I would say every AI application faces these interaction challenges.

09:44.340 --> 09:47.540
On the user side, when they first encounter these systems,

09:47.540 --> 09:51.700
they often have to struggle to kind of figure out how to make it work.

09:51.700 --> 09:57.140
Often people resort to misusing it, abusing it, and learning takes a long time.

09:57.700 --> 10:00.100
And part of it is really a design challenge.

10:01.380 --> 10:03.220
And we've seen other examples like this,

10:03.220 --> 10:06.260
where people don't have a good sense of what's happening

10:06.260 --> 10:11.060
in this algorithmically-generated systems and AI-powered systems.

10:11.060 --> 10:14.980
Like in the famous study of Facebook newsfeed users,

10:14.980 --> 10:20.020
more than half of the participants were not aware of the newsfeed curation algorithm's existence

10:20.020 --> 10:22.100
at all, which is far from being true.

10:23.060 --> 10:29.540
And on the right, what you see is in the pathologists' diagnosis scenario,

10:30.260 --> 10:33.140
often they would rely on some notion of similarity.

10:33.140 --> 10:37.380
So there are these algorithms that are designed to help people find similar images,

10:37.940 --> 10:44.900
but then the realization that the researchers had was that people had different notions of similarity.

10:44.900 --> 10:50.100
So a singular notion of similarity that was used in building an algorithm would not really suffice.

10:50.100 --> 10:55.700
So what they ended up doing was to support three different types of similarity interaction,

10:55.700 --> 11:00.900
and the user was able to kind of transition between these different terms in a fluid manner,

11:00.900 --> 11:04.580
which really gives more control and agency on the user side.

11:07.460 --> 11:12.420
And these, you know, put in a more simple sort of diagram manner,

11:12.420 --> 11:16.900
whether you are a creator or Facebook user pathologist,

11:17.460 --> 11:20.740
you seem to have some kind of a mental model of how the system works,

11:20.740 --> 11:25.300
a very sort of a classical sort of gap between what the user wants and the system wants.

11:25.300 --> 11:29.620
And obviously, the system is not behaving in a way that you really want.

11:29.620 --> 11:36.900
And this gap arguably seems to be larger with these more complex black box and deep learning based systems.

11:38.740 --> 11:42.020
And AI community has been tackling this problem as well.

11:42.100 --> 11:47.540
And, you know, some of the folks have been framing this as an alignment problem,

11:47.540 --> 11:52.260
which is about aligning the model's behavior with human intent.

11:52.900 --> 12:00.340
And for example, the famous chat GPT and the instruct GPT paradigm has been sort of open

12:00.340 --> 12:06.100
AI's response to the alignment problem, where their idea is, in addition to the, you know,

12:06.100 --> 12:11.860
basic large language model that they have, they would add this fine tuning layer with human feedback,

12:11.860 --> 12:17.060
which often involves asking people whether, you know, they were happy with the results they got.

12:17.060 --> 12:23.300
And the system kind of uses that feedback to train a reinforcement learning agent to

12:23.300 --> 12:28.660
do the fine tuning so that the resulting text aligns better with what the user wants.

12:28.660 --> 12:34.100
And they were seeing some success from it. And a quote from the paper is that

12:34.100 --> 12:39.460
making language models bigger does not inherently make them better at following a user's intent.

12:40.420 --> 12:46.580
And aligning language models with user intent on a wide range of tasks by fine tuning with human feedback.

12:46.580 --> 12:51.940
And of course, there's been a lot of discussion about whether this is really the most promising way

12:51.940 --> 12:57.940
to, you know, involve humans or alignment problem. But I think this is some progress towards that direction.

12:58.820 --> 13:06.900
But all of these examples, I would say, basically lead us to revisit these classical notions of

13:06.900 --> 13:13.300
Gulf of Execution and Gulf of Evaluation proposed by Don Norman back in the 1980s, right?

13:13.300 --> 13:19.300
As a user, they want to know what's going on with the system, and they want to have more control and agency.

13:19.300 --> 13:24.900
And on the evaluation side, when AI gives you some kind of result, they want to be able to

13:24.900 --> 13:31.700
understand it, interpret it, and want to get some explanation of it. And as an HCI researcher who's

13:31.700 --> 13:37.140
building these interactive systems, I feel like in often cases, I try to bridge these gaps.

13:37.140 --> 13:43.060
I come up with new ways of designing these social interactions and human AI interactions

13:43.060 --> 13:48.500
in a way that tries to bridge these gaps. And these are just some of the systems that I've been

13:48.500 --> 13:55.140
developing in different application domains. And I think many of them have somewhat succeeded

13:55.140 --> 13:59.220
in bridging these gaps. But other times, to be honest, we haven't done a good job of doing that.

14:00.180 --> 14:05.700
So what I want to do for the remaining time for this talk is to share some of these lessons,

14:05.700 --> 14:12.580
and some of them from positive experiences, but other times, bitter experiences by something

14:12.580 --> 14:18.260
that we haven't really done a good job of. And the main message that I want to send across

14:18.260 --> 14:23.860
is that beyond these point solutions for this system that works in this particular context,

14:23.860 --> 14:28.500
we've seen some success, I think as a field, we really need to start thinking about,

14:29.140 --> 14:34.900
can we do something more systematic and sustainable? Or empower designers and developers in thinking

14:34.900 --> 14:42.500
about can we develop these AI applications that are more usable and useful for more groups of people

14:42.500 --> 14:48.340
rather than having to reinvent the wheel each time someone has to develop these applications.

14:48.340 --> 14:51.540
And I think we're seeing too many of these cases where people are like,

14:52.260 --> 14:56.580
there's this cool model, let's build something around it, and it just gets released in a few

14:56.580 --> 15:01.780
days and realizes that people want it in a completely different way, people abuse it,

15:01.780 --> 15:05.860
a few days later, it goes down. We're seeing too many of these failure cases.

15:07.460 --> 15:12.100
So from the HCI point of view, I think HCI research can really advance this

15:13.460 --> 15:18.580
interaction-centric AI by contributing these generalizable building blocks for designing

15:18.580 --> 15:25.540
these systems and interface affordances. And AI research can also advance by embracing the idea

15:25.540 --> 15:31.540
of interaction-centric AI by rethinking models, architecture design, benchmarks, metrics, and

15:31.540 --> 15:39.380
research process. The part of it has to involve sort of broadening the perspective beyond just

15:39.380 --> 15:46.020
thinking about the model and the output that it generates to think about the users behind those

15:46.020 --> 15:51.220
and their mental models. And often there's not just a single user, but a group of user,

15:51.220 --> 15:56.020
community of user, a society of users. And there's also the temporal dimension,

15:56.020 --> 16:01.780
like before the user comes in and tries to use a system, we should be asking the questions about

16:01.780 --> 16:07.380
like, what's the task and who are these users and why and how. And during the interaction,

16:07.380 --> 16:12.420
we need to be thinking about presentation visualization. And the other way around as

16:12.420 --> 16:16.500
well, like interpretable results are being presented to the user. Do they have a way to

16:16.500 --> 16:22.260
provide feedback to the system? And also, it's never going to be just a single use, right? People

16:22.260 --> 16:27.940
would want to come back and use a system for a sustained amount of time. In those cases, people's

16:27.940 --> 16:35.220
mental model would evolve. And what does it mean for the system? And so I think this is sort of

16:35.220 --> 16:43.540
the ecosystem that I have in mind. And with these, I want to dive into these specific examples

16:43.540 --> 16:50.180
where we designed human-AI interactions. And I identify four major challenges

16:52.340 --> 16:57.940
in terms of human-AI interaction. The first one is about bridging the accuracy gap.

16:59.620 --> 17:04.660
So I'm on my sabbatical now. I'm working with this startup called Ringle, where they are

17:04.660 --> 17:10.260
basically Uber for language learning. They are matching tutors and tuties, and they have this

17:10.260 --> 17:16.500
video-based language tutoring session. So what we try to do here is to build this diagnostic

17:16.500 --> 17:21.940
service based on analyzing the chat-based tutoring session to give people personalized

17:21.940 --> 17:28.580
feedback and suggestions for improvement. But instead of going into the details of the service

17:28.580 --> 17:35.300
itself, I want to touch upon the case that we ran into when we were trying to run this automated

17:35.300 --> 17:42.100
speech recognition AI, which is crucial in sort of turning the video-based chat into text format,

17:42.660 --> 17:46.980
which is really required for us to run all these diagnostic algorithms on top of.

17:48.420 --> 17:56.020
And the standard metric of success in ASR would be word error rate, how correctly it can

17:56.580 --> 18:04.180
recover the original text. And on the tutor side, when we ran ASR on like hundreds and

18:04.180 --> 18:11.060
thousands of sessions, the average word error rate was around 8%. Can you take a guess as to what

18:11.060 --> 18:18.980
the number would have been for students? Obviously there's this white margin that's quite high,

18:19.060 --> 18:28.340
so you can imagine, 30. Yeah, we're seeing 23. So there's quite a bit of a gap. And this is an

18:28.340 --> 18:34.740
example of an accuracy gap where different groups of users are getting disproportionate results from

18:34.740 --> 18:41.620
the same AI. And the gap actually widens if we look at like the best tutor and the worst student

18:41.620 --> 18:47.380
when it comes to the performance of these models. But in terms of thinking about the interaction

18:47.380 --> 18:53.620
that these people are trying to have with this AI, I would argue that the students are the ones

18:53.620 --> 18:59.620
who really need this AI to work. Based on the accuracy of this AI, they want to kind of look

18:59.620 --> 19:06.420
at where they succeeded and failed and they want to learn and reflect. And with this low accuracy,

19:06.420 --> 19:11.940
they would really be struggling to come up with good action items and they might be frustrated,

19:11.940 --> 19:17.220
they might lose trust on the system. But interestingly, a lot of focus when it comes to

19:17.220 --> 19:24.180
model development is that we seem to be focusing on the 6%, like making the 6% better instead of

19:24.180 --> 19:30.420
narrowing the gap between 6 and 36%. And we have to really be asking like, what is the most important

19:30.420 --> 19:34.660
question in this context? And are we really focusing on the most important question here?

19:35.460 --> 19:42.580
And we see these other examples too, where Tyra and others have studied the machine translation

19:42.580 --> 19:48.100
that is being used in emergency rooms when it comes to discharge statements that are presented

19:48.100 --> 19:54.660
to patients and patients' families. And we see a huge disparity between different languages.

19:54.660 --> 20:01.380
And in the natural language processing community, this support for low resource languages has been

20:02.100 --> 20:07.700
a topic for research and there has been great efforts. And on the right is a famous example

20:07.700 --> 20:15.700
of gender shades, where the gender classification algorithm shows, again, an accuracy disparity

20:15.700 --> 20:24.900
between darker skin female versus lighter scale male. And of course, these diversity and inclusion

20:25.700 --> 20:30.980
efforts and low resource language support research in the AI community and in the community

20:30.980 --> 20:36.580
have been tackling these issues of accuracy gap, of course. But then I would argue that they could

20:37.380 --> 20:43.940
advance further by embracing more interaction-centric approach in trying to really see how in the real

20:43.940 --> 20:49.380
world people are interacting with these results and what kind of actual struggles that they have

20:49.380 --> 20:57.460
because of poor or good AI accuracy and what, as a community, how can we define the problem that's

20:57.460 --> 21:05.780
most important. And conceptually speaking, I feel like a good analogy might be the ceiling and floor

21:05.780 --> 21:12.740
analogy. The ceiling would be this primary user group who gets the best part of AI. And floor

21:12.740 --> 21:19.540
would be secondary user group who is disproportionately getting more negative impact of

21:19.540 --> 21:25.780
the same AI. And there's this accuracy gap. And often I feel like taking a model-centric approach

21:25.780 --> 21:32.340
incentivizes people and researchers to work on raising the ceiling. There could be a couple

21:32.340 --> 21:37.460
reasons for this. First of all, that's the sota number you get, which might be what you need to

21:37.460 --> 21:43.060
publish a paper out of it. Or the benchmarks that you're working with do not really have much data

21:43.940 --> 21:48.500
on the floor side. It's maybe more focused on the ceiling side. And that's why the ceiling is there

21:48.500 --> 21:54.500
in the first place. So it might be just incentivizing people to continue to push the boundaries of

21:54.500 --> 22:02.580
ceiling. And as a result, what we see is a lot of a widened accuracy gap. And if we take a more

22:02.580 --> 22:07.380
interaction-centric approach, I would argue that if we identify that narrowing this gap is a more

22:07.380 --> 22:13.620
important problem, we can narrow this accuracy gap. And it's not just a matter of accuracy,

22:13.620 --> 22:18.180
if you think about it. It's about experience, benefit, and value that people get out of

22:18.180 --> 22:26.420
interacting with this AI. So there was a first challenge about the accuracy gap and how thinking

22:26.420 --> 22:31.860
about how people interact with this AI can help us identify what problems are worth tackling.

22:32.660 --> 22:38.420
And second of all, I want to talk about when people actually use AI. And one of the

22:38.980 --> 22:45.460
anti-patterns of human-AI interaction is that people just stop using AI altogether or abandon it,

22:46.020 --> 22:52.500
which is something you might want to avoid as a system designer. And that's why it's important

22:52.500 --> 22:59.380
to think about how do we incentivize people to work with AI? And in most cases, people abandon

22:59.380 --> 23:05.460
using AI because it's not really giving them concrete value that they expect. And we explore

23:05.460 --> 23:11.860
this in the context of online education in this system called XS. So the problem that we wanted

23:11.860 --> 23:17.380
to focus here is that in online, let's say you want to learn some new concept like probability,

23:17.380 --> 23:22.980
there are lots of problems and answers you can find. But finding good explanations is

23:22.980 --> 23:29.140
surprisingly difficult. And generating high-quality explanations is costly and resource-intensive

23:29.140 --> 23:36.020
console. So we wanted to tackle this problem by building this online education platform,

23:36.020 --> 23:41.780
where people are presented with a problem and they solve this problem, they submit an answer,

23:43.540 --> 23:49.700
and they see an example that's presented by the system and they get a chance to rate how helpful

23:49.700 --> 23:57.460
the explanation that they saw was. And then they are getting a chance to sort of self-explain

23:57.460 --> 24:03.540
their own answer. So this is a pedagogically meaningful activity to be able to sort of explain

24:03.540 --> 24:09.940
your thought process, externalize it, and lots of research supports doing self-explanation.

24:10.740 --> 24:16.420
Okay, so fairly simple sort of front end in terms of the learner's experience. So what's

24:16.420 --> 24:23.380
happening behind the scene is that the system is collecting these explanations and ratings

24:23.380 --> 24:29.780
from learners, right? Since it's a live system, new learners keep coming in and provide new ratings

24:29.780 --> 24:36.020
and explanations. And we formulate this in a multi-armed bandit manner, which means that

24:36.020 --> 24:41.860
as a new explanation comes into the system, as a byproduct of humans' learning activity,

24:41.860 --> 24:47.860
a new arm gets added to the system. And what the system is doing is to determine this dynamic

24:47.860 --> 24:54.180
policy for what the most effective explanation would be for the next learner coming into the system.

24:55.780 --> 25:01.380
So if you're familiar with the reinforcement learning of concepts, we are navigating

25:01.380 --> 25:07.220
exploitation and exploration trade-off. Exploitation in the sense that the system wants to present the

25:07.220 --> 25:12.980
best explanations to the next learner coming into the system, but the system doesn't really know what

25:12.980 --> 25:18.980
the best explanations are until it collects some amount of ratings from people. So it has to do some

25:18.980 --> 25:25.220
exploration where it should collect this data. And to solve that, we use a technique called

25:25.220 --> 25:31.780
Thomson sampling. So what happens is the system keeps track of these policies and when a new

25:31.780 --> 25:36.740
explanation comes in and ratings come in, these things get updated and the policy

25:37.140 --> 25:44.260
of probabilistic policy gets updated so that it uses this distribution to determine

25:44.260 --> 25:54.340
what explanation to show to the next learner. So when we ran a study, these access-generated

25:54.340 --> 26:01.780
explanations were helpful in terms of helping people learn better. So when we compared against

26:01.860 --> 26:07.220
presenting no explanation at all and measured differences between pre-test and post-test

26:07.220 --> 26:13.860
results, we were seeing that people were gaining 3% increase in their scores. So just getting a

26:14.660 --> 26:21.060
chance to rethink the problem, I think still gave them some increase in their scores. And when they

26:21.060 --> 26:28.020
were seeing the instructor-generated explanation, which is, I guess, somewhat of an ideal case or

26:28.020 --> 26:34.420
the standard case, we're seeing 9% increase and with access, we're seeing 12% increase. So between

26:34.420 --> 26:39.620
these two conditions, it was not statistically significantly different, but there were certainly

26:39.620 --> 26:47.220
cases where access was picking explanations from learners that were even more powerful than the

26:47.220 --> 26:54.740
instructor-generated ones. So in this system, if we were to take a more model-centric approach,

26:54.740 --> 27:00.900
I think we might have built an AI that automatically generates high quality explanations.

27:02.340 --> 27:08.260
But instead, in taking an interaction-centric approach, I think the system we created is basically

27:08.260 --> 27:15.540
this co-learning system where the user, the learner, and AI are learning at the same time in a single

27:15.540 --> 27:22.420
system. So it's sort of an education-focused system of the game-with-the-purpose kind of setting,

27:22.420 --> 27:27.380
where organic benefits are provided to people who are interacting with the system,

27:27.380 --> 27:33.380
and the system is learning something useful out of it. And this is basically the mechanism that

27:33.380 --> 27:38.820
we have in that both sides are learning and explanation and feedback are establishing this

27:38.820 --> 27:45.460
loop. And this is the topic of my PhD thesis, and I explore this in the concept of learner sourcing,

27:45.460 --> 27:51.780
where learners as a crowd coming into the system are basically doing this by getting their individual

27:51.780 --> 27:57.620
benefits while they're providing something useful for the system to learn and do its thing better.

27:58.580 --> 28:05.380
So since then, I've been expanding this idea to a broad array of applications. So for example,

28:05.380 --> 28:11.860
can we use this kind of co-learning ideas to summarize how-to videos in terms of steps and

28:11.860 --> 28:17.780
sub-steps, or building a concept map out of an instructional video that shows relationships

28:17.780 --> 28:22.740
between different concepts, or helping learners come up with the solution plans

28:23.380 --> 28:28.500
in algorithmic problem-solving settings. And other researchers have taken on this idea

28:28.500 --> 28:33.540
in different application contexts as well. So I think we can try to really generalize this kind

28:33.540 --> 28:42.980
of idea of co-learning system design in different contexts. Moving on to the third challenge,

28:43.940 --> 28:50.740
is about beyond a single user. And often we think about a single user, a single AI interacting

28:50.740 --> 28:55.940
with each other. In real life, it would be much more complex and there would be diverse configurations.

28:56.980 --> 29:03.220
So how can we consider these social dynamics? And there could be various types of social dynamics,

29:03.780 --> 29:10.900
but one specific instance that we did in was group-based, chat-based discussion in a group.

29:11.780 --> 29:18.020
So we built this system called Solution Chat, where the idea is what if this AI agent could

29:18.740 --> 29:25.060
recommend real-time moderation messages to a group. So let's say a group is discussing,

29:25.060 --> 29:30.020
you know, what to do for the company retreat next week, and they're having a discussion.

29:30.020 --> 29:35.060
The system, in real time, based on the understanding of the discussion context,

29:35.060 --> 29:38.820
and also knowing what kind of messages would be useful for the group,

29:38.820 --> 29:44.020
based on our sort of literature survey of discussion and discussion-based education,

29:44.740 --> 29:52.340
it presents these recommendation messages, like any more ideas, or can this person share their

29:52.340 --> 29:58.100
opinions, you have been quiet for a while, or should we try to move on to the next stage,

29:58.100 --> 30:03.780
or thank you for your opinion. So these kinds of moderation messages are presented by the system

30:03.780 --> 30:10.020
in real time, just like what you get in smart replies in Gmail, for example. And as a moderator,

30:10.020 --> 30:16.500
you can just choose to accept any of the messages that you like, and discard the ones that you don't

30:16.500 --> 30:28.500
like. So a quick summary of the results of what we saw was that in our lab study with 55 users

30:28.500 --> 30:34.740
in 12 different groups, when we compared how many moderation messages were used in different groups,

30:34.740 --> 30:39.540
when we compared the baseline condition without these real-time recommendations versus

30:39.540 --> 30:45.300
solution chat or system, we're seeing a significant increase in the number of moderation messages

30:45.300 --> 30:50.340
that were present in the chat stream in the solution chat condition. But interestingly,

30:50.340 --> 30:55.700
you can see that the users manually typed moderation messages were actually decreasing

30:55.700 --> 31:04.980
in solution chat, but many of them were replaced by the accepting AI-generated recommendations.

31:07.380 --> 31:13.140
And furthermore, we had this great opportunity to actually release this system to over 2,000

31:13.140 --> 31:18.180
real-world users in a corporate education setting. So during COVID, a lot of these corporate

31:18.180 --> 31:24.580
education programs moved online, and this company that we worked with wanted to use these kinds of

31:24.580 --> 31:31.620
system to moderate hundreds of chat rooms that were doing discussion-based activity.

31:32.980 --> 31:39.860
And not surprisingly, just like the very first live stream prompt example that I mentioned,

31:39.860 --> 31:45.540
here again, people were collaboratively trying to understand the capabilities and limitations of

31:45.540 --> 31:51.540
AI when they were first presented with the system. So they were using the chat to test

31:51.540 --> 31:57.940
different messages, often things that they believe would be not working, and they would be

31:57.940 --> 32:03.620
sharing the results of, oh, this is working, this is not working, I think this does this well,

32:03.620 --> 32:10.420
but not that well. And it seems as a group does this kind of testing in the very first phase of

32:10.420 --> 32:16.660
their usage of the system, people have this shared expectation of the system, and that seems to sort

32:16.660 --> 32:23.540
of determine their further interactions with the system. And it was also notable how different

32:23.540 --> 32:28.500
groups had different expectations based on their limited experimentation that they did in the beginning.

32:30.340 --> 32:33.620
And there were some interesting social dynamics that we observed as well,

32:34.500 --> 32:41.380
like in how people use these AI recommendations to socially interact with each other.

32:42.340 --> 32:46.420
Some people were using AI as proxy. So one of the quotes that we had was,

32:46.420 --> 32:51.220
I didn't want to directly ask the person to stop talking. So the person relied on the AI

32:51.220 --> 32:58.820
recommended message to kind of send it. They still chose to send it, but it was their way of kind of

32:58.820 --> 33:07.540
softening the potential sort of dispute with the person. Other people were using AI as a reference.

33:07.620 --> 33:12.820
So what we were seeing is that it was a fairly simple technical pipeline that we had. So it was

33:12.820 --> 33:20.580
just a canned response. So people were sometimes not really fond of the tone of the message,

33:20.580 --> 33:25.300
style of the message that we showed. So the person said, I found no fun in the recommended

33:25.300 --> 33:30.900
messages because all the messages look the same. So in those cases, what people did was they still

33:31.460 --> 33:39.380
adopted the idea from the recommendation, but then rewrote it so that it feels more personal,

33:39.380 --> 33:46.820
and it feels more like it's coming from them, not AI. In other cases, AI seems to be adding

33:46.820 --> 33:53.220
a social burden. So in this excerpt, so one of the people said, I'm doubtful about the

33:53.780 --> 33:59.460
credibility of AI. And then the moderator picks this AI recommendation. Thanks for your opinion.

33:59.540 --> 34:04.100
Another person says, I also think negatively. Thanks for your opinion. Thanks for sharing a good

34:04.100 --> 34:09.460
opinion. Shall we go to the next topic? And then the moderator realizes he might have clicked,

34:09.460 --> 34:14.820
accept way too many times, and it was a little unnatural. So he stopped to kind of

34:14.820 --> 34:20.420
clarify and apologize for my unnatural words as I'm using AI recommendations.

34:21.220 --> 34:27.380
So while we were seeing how people were saving their time and cognitive effort in moderation

34:27.380 --> 34:32.500
could have decreased, it might have actually introduced other types of burden at the same time.

34:35.460 --> 34:39.700
Again, so if we were to build this kind of system in a more model-centric manner,

34:39.700 --> 34:44.580
I think a good alternative might have been automated discussion moderation, where AI

34:44.580 --> 34:52.500
would actually do all the moderation by itself. But instead, we chose to take a more AI-assisted

34:52.500 --> 34:58.340
moderation for obvious reasons. Users want to have more agency and control, and they wanted to

34:58.340 --> 35:04.340
keep their style of communication. So instead of handing over the entire control to AI,

35:04.900 --> 35:12.420
we still sort of gave that control to the human moderator who could kind of use it as an additional

35:12.420 --> 35:20.900
resource. Okay, so there was a third challenge. And moving on to the final challenge of supporting

35:20.900 --> 35:27.060
sustainable engagement. Here, the concern is that we want to think beyond this single

35:27.060 --> 35:34.020
session usage. And over time, how people react to these systems might change, their mental model

35:34.020 --> 35:38.260
might change, and how AI actually works might change. So we need to really think about this

35:38.260 --> 35:47.060
temporal dimension more carefully. And for this thread, we investigated in the context of

35:47.860 --> 35:54.260
novices making changes to websites that they're seeing. So for example, you might have a case

35:54.260 --> 35:59.220
where you visited this website that colors hurt your eyes, or you couldn't really find this button

35:59.220 --> 36:05.860
or tap it because it's too small, maybe you want to make it larger. But then people without

36:05.860 --> 36:12.580
expertise in HTML and CSS have difficulty doing this. So we thought by leveraging the power of

36:12.580 --> 36:18.420
large language models and so on, maybe we can support more natural language queries. So if a

36:18.420 --> 36:24.660
person says tone down the text, the system can kind of display these style recommendations that they

36:24.660 --> 36:33.300
can explore and select from that are about toning down the text. So the way the system works is

36:33.300 --> 36:40.260
if the user clicks and says make this larger, the system presents a set of design attributes that

36:40.260 --> 36:50.820
are about making something larger. And the user can say emphasize this part. It's somewhat ambiguous.

36:50.820 --> 36:56.180
There isn't a clear single design attribute that is about emphasis. So it presents these

36:57.220 --> 37:05.460
few recommendations that are about emphasizing something. So we built this by establishing

37:05.460 --> 37:12.660
this NLP pipeline and computer vision pipeline. On the NLP side, what it does is analyzing the

37:12.660 --> 37:19.700
user's query and mapping them with the style attributes that seem to be connected to what

37:19.700 --> 37:26.740
the user's intent is about. In terms of computer vision, we collected millions of web design

37:27.620 --> 37:34.020
elements to determine a good set of recommendations to show to the learner. So by combining those,

37:34.020 --> 37:39.940
we built this system. Again, so instead of going deep into the technical details of the system,

37:39.940 --> 37:46.660
I want to focus on the interaction dynamics. So we ran this user study with 40 people where we

37:46.660 --> 37:52.660
presented them with either stylet, which is the name of our system, versus the baseline, the

37:52.660 --> 37:57.220
Chrome developer tool, which is sort of the standard tool for making these style changes.

37:58.180 --> 38:04.340
So we compared these two groups. And we gave people two tasks. One is a well-defined task

38:04.340 --> 38:10.900
where we ask people to turn this before image into an after image. And then secondly, we had this

38:10.900 --> 38:16.100
open-ended task where we gave this blank slate and people were able to make any kind of change

38:16.100 --> 38:24.900
that they want. First, I want to share success stories. People were more successful in completing

38:24.980 --> 38:32.180
these design tasks when using stylet. 80% of the stylet users completed the task as opposed to only

38:32.180 --> 38:38.180
35% in Chrome developer tools. And these were complete novices in web design, no experience at all.

38:39.060 --> 38:44.500
And people completed the task in 35% less times. It was efficient to use stylet.

38:45.380 --> 38:51.300
Another interesting observation was that people were making same similar number of changes in

38:51.300 --> 38:58.740
both conditions. But in stylet condition, people were making more diverse changes, which means that

38:58.740 --> 39:04.740
it probably had to do with how stylet shows these multiple options for people to explore. And there

39:04.740 --> 39:10.100
was a conscious decision to not just show the most obvious one, but show somewhat related ones as

39:10.100 --> 39:16.260
well so that people could explore and tinker around different options. But then an unexpected

39:16.820 --> 39:22.260
finding was when we looked at people's self-confidence. Because we thought this kind of system would be

39:22.260 --> 39:27.860
useful for people's learning of the skills and confidence that they have about the skills,

39:27.860 --> 39:33.540
we asked people's self-confidence after each task. What we noted was that after the first task,

39:34.100 --> 39:38.980
in both conditions, people's self-confidence increased. But then in the second task,

39:38.980 --> 39:45.220
after the second task, users' self-confidence decreased for stylet while in the developer

39:45.220 --> 39:51.220
tool, it kept increasing. Why would that be the case? And we were seeing many cases where

39:51.220 --> 39:56.420
stylet users were frustrated that the only control that they had was natural language.

39:57.060 --> 40:01.220
Now they have some grasp of how it works. They wanted to do more fine-grained control more

40:01.220 --> 40:06.180
directly. And they wanted more specific things. But because they only had natural language,

40:06.740 --> 40:11.780
they sometimes just got frustrated. Whereas in the Chrome Developer Tools condition,

40:11.780 --> 40:16.260
people were just happy that they accomplished something with their own hands.

40:16.260 --> 40:22.020
And I think that is presented as a continued increase in self-confidence.

40:23.220 --> 40:31.860
And we know from HCI and CS147 that people's expertise and learnability really matters. And as

40:31.860 --> 40:39.140
they have more knowledge of the domain and the skill, they might need to get more advanced

40:39.140 --> 40:45.060
controls or being able to more directly manipulate what they are working on. So I think this had

40:45.060 --> 40:50.020
some interesting lessons in terms of thinking about the temporal dimension in that learners are

40:50.020 --> 40:57.060
changing. And other researchers have been reporting that considering these temporal dynamics is

40:57.060 --> 41:02.900
important. On the left, what you see is design researchers who have shown that there are these

41:02.900 --> 41:08.900
different stages of relationship that people have in technologies like self-tracking devices.

41:09.220 --> 41:14.420
First, they would start with initiation and experimentation, followed by intensifying and

41:14.420 --> 41:19.620
integration, and then stagnation and termination. And one of the design lessons might be that

41:20.180 --> 41:26.340
these might be more meta-level factors that really should be considered in design systems,

41:26.340 --> 41:31.780
in that even the same kind of intervention might need to be presented in different

41:31.780 --> 41:36.340
manners depending on what stage you are or what your expectation is with the system.

41:36.900 --> 41:42.260
On the right, what you're seeing is the guidelines for human-AI interaction, really influential

41:42.260 --> 41:48.420
work from Emershi et al. And they organize these guidelines for human-AI interaction

41:48.420 --> 41:53.860
in different categories but are organized in the temporal sort of aspect, like initially

41:53.860 --> 42:00.740
encounter with AI during interaction, when things go wrong, and over time. So taking into account

42:00.820 --> 42:06.180
this temporal dimension can really be powerful in supporting more sustainable engagement.

42:07.620 --> 42:14.100
And the related question might be, as people are relying more on these AI tools, like grammar

42:14.100 --> 42:20.660
fixes or even generating text, it's important to think about how people's mental model would

42:20.660 --> 42:27.540
change over time, and AI also changes over time too. And do we hit a point where people become

42:27.540 --> 42:34.100
maybe overly reliant in that maybe their grammar skills or writing skills do not improve anymore,

42:34.660 --> 42:40.100
but then without the tool, they actually might perform worse? And what is that dynamic? Or maybe

42:40.100 --> 42:44.580
over reliance is perfectly fine because if we believe these tools will be around the user all

42:44.580 --> 42:50.260
the time, maybe it's just the final outcome that matters. And I think we need more studies and

42:50.340 --> 42:55.780
analysis of the long-term engagement of users using these kind of technologies.

42:59.220 --> 43:03.700
And to kind of sum up, if we were to take a more model-centric approach here, I think we might

43:03.700 --> 43:11.220
have built a system that makes automatic design fixes to optimize a web page directly, and the

43:11.220 --> 43:17.380
system makes a fix and user can just use it. But instead, we took a more sort of interaction-centric

43:17.380 --> 43:23.780
route where we asked people to do sort of, you know, style change by themselves as the system

43:23.780 --> 43:29.460
was presenting these recommendations, and they still had to do the fix by themselves. But what

43:29.460 --> 43:35.460
we expected here was that people can then customize by seeing these attributes, they can learn,

43:35.460 --> 43:40.100
they can discover new ways of doing things, they can think around, which can empower them,

43:40.100 --> 43:45.140
especially in the more learning context, although the temporal dimension has to be more carefully

43:45.140 --> 43:51.780
taken into account. So these were the four challenges that I wanted to

43:51.780 --> 43:57.140
share today. And to kind of wrap up, I just wanted to pose two questions moving forward

43:58.020 --> 44:05.300
from the interaction-centric perspective as HCI researchers. So first is, how might we design

44:05.300 --> 44:12.820
these building blocks and interface affordances for new and upcoming AI models? Okay, so I think

44:12.900 --> 44:18.180
part of it is that instead of building these point solutions, I think we need to think about,

44:18.180 --> 44:23.700
are there any sort of generalizable frameworks, libraries, widgets, or interface affordances

44:24.340 --> 44:29.860
that we could come up with as a community that is really good at these kinds of things?

44:30.580 --> 44:36.420
And the second question is, does AI really require us to have these new things? I mean,

44:36.420 --> 44:42.020
can we just use existing design elements and frameworks to build AI applications?

44:42.820 --> 44:50.260
And I tend to think that we might need something new for these new and upcoming AI models,

44:50.260 --> 44:55.460
especially because they have these very different characteristics than the conventional systems

44:55.460 --> 45:01.060
that we have been building. They're more probabilistic, harder to predict, more black box in nature,

45:01.620 --> 45:07.700
yet seemingly more impactful and powerful in terms of what they do, hallucinating. All these

45:07.700 --> 45:13.460
properties packed together, I think we might really need to think about, what are the types

45:13.460 --> 45:20.340
of interaction affordances that are really built for supporting the usability of these

45:20.340 --> 45:28.660
AI-powered applications? So in this, I think as a community, we are making all these great

45:28.660 --> 45:33.300
advances, like making different types of contributions. And I tend to focus on more

45:33.860 --> 45:39.700
interactive systems and techniques, whereas other people focus on introducing new design

45:39.700 --> 45:44.820
processes and understandings. And I think all this work is needed. And some of the interesting

45:44.820 --> 45:51.860
examples of adding an interaction layer to these new types of models is in this example,

45:51.860 --> 45:58.260
Tailbrush, where the user can draw the level of fortune that they won in the character to have

45:58.260 --> 46:08.100
when they use generative models to generate a story. Or this AI chains work, which presents these

46:08.100 --> 46:15.620
primitives and workflows for putting together this workflow that can accomplish more complex tasks

46:15.620 --> 46:23.540
with these LLM prompts that a single prompt cannot really perform. And in my research group,

46:23.540 --> 46:29.220
with my PhD student, Tesu Kim, we have been investigating this idea of what would be more

46:29.220 --> 46:34.820
generalizable design framework. And thinking about input, model, and output, we have been

46:34.820 --> 46:41.860
thinking about the concepts of cells, generators, and lenses, and tried to introduce this standardized

46:41.860 --> 46:47.300
libraries and widgets that people can easily adopt in their AI applications. So for example,

46:47.300 --> 46:54.100
using this kind of framework, people can build a copywriting app, email app, or story writing

46:54.100 --> 46:59.860
app using pretty much the same kind of framework, which can save people's time while supporting

46:59.860 --> 47:05.220
the types of interactions like iterations and comparison and experimenting different outputs.

47:07.540 --> 47:12.980
And the second question, and the final question that I want to ask today is, how might we as an

47:12.980 --> 47:18.980
HCI community collaborate better with the AI community on these various things? And it was also

47:18.980 --> 47:25.140
the discussion that I was having a lot with today's meetings, and also with various AI researchers,

47:25.140 --> 47:30.660
especially in Europe. And in terms of community collaboration, of course, one of the important

47:30.660 --> 47:35.780
things is metrics. And there was also a great discussion at the HCI conference a couple weeks

47:35.780 --> 47:42.900
ago, hosted here at Stanford. And in the AI community, it cares a lot about model performance

47:42.900 --> 47:48.260
and generalization errors, where in HCI, we tend to focus on the human experience. So how do we

47:48.260 --> 47:54.900
really bridge the gap between the metrics? And what it means to do AI research with more human

47:54.900 --> 48:00.260
side metrics incorporated? What's the incentive for people to do that? And how do we encourage

48:00.260 --> 48:06.980
poor AI people to use these metrics, too? In terms of human input design, a lot of the comments

48:06.980 --> 48:12.500
that I was getting in terms of interaction-centric AI from AI researchers is that these ideas are

48:12.500 --> 48:18.740
great, but then I don't really know how to actually take action about it. And part of it is, in their

48:18.740 --> 48:25.540
model-building kind of work, how can I incorporate human feedback? And how do I use it in a meaningful

48:25.540 --> 48:31.220
way to really change the way the model actually works, rather than just getting more high-level

48:31.220 --> 48:40.500
design guidance? So one great direction for this might be, think about more making human feedback,

48:40.500 --> 48:44.980
more computationally feasible, so that this compatibility is actually satisfied.

48:46.500 --> 48:50.820
And lastly, we need to think about the change in design process as well. And in a lot of,

48:51.540 --> 48:57.460
this is Stanford D-School's user-centered design cycle. And I think in a lot of the AI research,

48:57.460 --> 49:02.500
what we're seeing is this prototype test kind of culture. You try something new, test it,

49:02.500 --> 49:08.820
iteratively improve it. But then one of the frustrations is that interaction often comes

49:08.820 --> 49:15.460
too late, right? There's this new cool model, and can you build an UI on top of it, is sort of the

49:15.540 --> 49:20.820
kind of discourse we get a lot. And I think interaction should not just be like an icing

49:20.820 --> 49:26.660
on the cake, but really something that can guide the entire design process or help people determine,

49:26.660 --> 49:32.260
is this the right problem to tackle in the first place? Or what kind of interaction should we try

49:32.260 --> 49:37.620
to support with AI? And based on that, think of what AI should do and should not do and how much

49:37.620 --> 49:44.980
AI should be used in a particular context. So that's all I wanted to share. And here's a summary

49:44.980 --> 49:49.140
of what I mentioned today, and I'd be happy to take any questions. Thank you.

49:54.660 --> 49:59.220
All right, so I'll check my recommendations of facilitating messages. If there are any more ideas.

50:01.140 --> 50:05.460
No? What do you think?

50:06.100 --> 50:09.620
It really sounded like an AI.

50:11.220 --> 50:21.060
I'll just click them all. I want to pull the mic on. I want to pull the thread a little bit on

50:21.060 --> 50:28.900
this notion of how to connect human feedback with the objective functions that you touched on near

50:28.900 --> 50:33.700
the end, because that's been rattling around in my head in much of the talk that you're giving,

50:33.700 --> 50:37.380
that if I think about what should AI researchers be doing differently,

50:38.980 --> 50:44.180
then you're asking, well, what's the proper model of the person in their system?

50:45.460 --> 50:49.300
And traditionally, the problem has been that human interaction is really expensive,

50:50.260 --> 50:54.420
just to collect annotated data. Or once you have it to be able to tune the model,

50:56.260 --> 51:02.500
you don't get that much of it. And so they often fall back on self-supervision, or as you've been

51:02.500 --> 51:08.420
talking about in the value alignment, they train an RL model to mimic a human and then let that go

51:08.420 --> 51:14.020
loose. And it seems like until, I think they're kind of, I want you to take a position on one of

51:14.020 --> 51:19.380
the two positions. One either is to say, look, we need to find strategies like that where we can

51:19.380 --> 51:26.900
create proxy humans, and that's how we hook into the objective functions, the loss functions, etc.

51:26.900 --> 51:32.100
The other alternative would be to say, no, we're going to find some other way to actually

51:32.100 --> 51:40.020
make human feedback at a scale and in a form that they can directly use in the models. I'm

51:40.020 --> 51:44.260
just curious, like, if you want to take a bet, where's your bet on that? Where should we be heading?

51:45.460 --> 51:51.700
Yeah, that's an excellent question. I would say, I mean, you asked me to take a position,

51:51.700 --> 51:58.580
but I would say both will be prevalent. And I like the letter much more. And I think that's more

51:58.580 --> 52:03.540
promising and sustainable. And for example, the reason I'm really interested in this, like,

52:03.540 --> 52:09.620
co-learning feedback loop between the human and the machine is that, you know, even if this super

52:09.620 --> 52:16.020
advanced AI comes along and let's say it presents this, like, super accurate explanations, people's

52:16.020 --> 52:21.780
self-explanation activity is still meaningful, right? Because that's how they could learn.

52:21.780 --> 52:27.620
And so I feel like, you know, we can really try to find these compatible

52:29.700 --> 52:35.940
mechanisms in which the human can get the benefit and get the incentive for doing what

52:35.940 --> 52:40.260
they are really good at and what is helpful for them, not necessarily trying to help the system

52:40.260 --> 52:46.340
or, you know, getting paid to system, paid to support the system per se. And at the same time,

52:46.340 --> 52:50.900
the system can use it for something meaningful. And on the system side, I think in the system,

52:50.900 --> 52:57.860
like, access that I presented, I was really happy when we landed at this technical solution where

52:57.860 --> 53:05.860
people's rating data could be almost directly piped into the feedback for the RL agent to kind

53:05.860 --> 53:11.300
of use as meaningful feedback. So I think that's just one example where this kind of worked out

53:11.300 --> 53:16.900
for this kind of context. And I think we need to really investigate more and think about, are there

53:17.460 --> 53:22.740
any generalizable mechanisms that this kind of approach could work in different contexts?

53:22.740 --> 53:27.380
This assumes that you have a large set of users you can draw on, like there are learners that

53:27.380 --> 53:32.100
are coming through your system. If I'm early on in the pipeline and I just kind of have V0,

53:32.100 --> 53:35.220
I don't have the users yet, are there strategies you would recommend?

53:36.020 --> 53:40.740
Yeah, yeah, excellent. So in that same access system, for instance, what we did was to

53:41.620 --> 53:49.780
insert the instructor-generated explanations as sort of the initial seed. And I was also imagining

53:49.780 --> 53:56.260
maybe using LLMs, for instance, we can plug in AI-generated ones to kind of avoid the cold

53:56.260 --> 54:02.340
start problem. And it would be interesting to see how, you know, in the same system, like AI-generated

54:02.340 --> 54:07.860
ones, instructor-generated ones, and learner-generated ones can kind of compete against each other

54:07.860 --> 54:11.780
until the system ultimately just focuses on what is best for learners.

54:12.820 --> 54:19.060
This is kind of a two-part question, going back to the like third challenge or like project you

54:19.060 --> 54:25.380
talked about, where there was that note about AI as proxy, like people kind of using that as

54:26.100 --> 54:30.740
like an excuse to make points, where maybe they wanted to do something but didn't want it to come

54:30.740 --> 54:38.180
off as them. So the first part of the question is like, in that case, did people want to,

54:39.140 --> 54:43.700
later it says people wanted the message to kind of sound like them, but in the case of the AI

54:43.700 --> 54:49.780
as proxy, did they want that to sound like them? Or were they wanting it to sound more artificial?

54:49.780 --> 54:55.940
And then second part of the question is, do you think there are more situations than just this

54:56.020 --> 55:02.500
where maybe we don't want the AI to feel super personable and maybe want the interaction to

55:02.500 --> 55:07.620
feel slightly more kind of mechanical or unnatural? Yeah, that's an excellent question. And I would

55:07.620 --> 55:14.900
say these were somewhat different use cases, and both I think are valuable and smite. And

55:16.340 --> 55:21.860
that again, I think in a more model-centric approach, we also kind of focus on trying to

55:22.660 --> 55:28.500
create these messages that are more like humans. And that could be effective in certain cases,

55:28.500 --> 55:34.260
but as you said, that might not really be what the users want, because in a proxy kind of setting,

55:34.260 --> 55:40.340
you might not actually want it to sound too personalized, because maybe the more canned

55:40.340 --> 55:48.980
message might actually work better in that context. And vice versa. So I think just being able to

55:48.980 --> 55:54.420
identify all these different needs that people have and expectations that people have and being

55:54.420 --> 56:00.820
able to somewhat fluidly support those, I think was really an interesting kind of observation

56:00.820 --> 56:06.580
that we had. And I think moving forward, one of the lessons was that this more personalizable

56:07.860 --> 56:12.100
message generation could be an interesting technology that could be potentially integrated,

56:12.100 --> 56:17.140
but that's not going to solve everything, because there are these other types of needs that will

56:17.140 --> 56:23.780
not be supported, even with the perfect personalizable style transfer. So yeah.

56:26.020 --> 56:31.780
Explaining stuff, I kept thinking about how what you described and sort of the challenges

56:31.780 --> 56:38.180
that we see with this new deep networks and models and how we interact with them are

56:38.820 --> 56:44.260
similar to how people used to interact with search engines, right? At the beginning, people were

56:44.340 --> 56:51.140
not as good as sort of figuring out how to query the search engine right. And over time, both

56:52.420 --> 56:57.220
we became better at querying the search engines, and then the search engines became better at

56:57.220 --> 57:03.540
sort of understanding how to interpret user queries. Do you see any similarities there? Is

57:03.540 --> 57:10.900
there something that's very unique to the challenges we face with this new models? Or is it just that

57:10.980 --> 57:17.060
we haven't had enough time to sort of adopt to each other in a way? Yeah.

57:17.060 --> 57:24.100
Excellent. Yeah. And I think it's a recurring theme as these new technologies come in. Initially,

57:24.100 --> 57:29.060
people would kind of struggle and they would need to learn how it actually works through trial and

57:29.060 --> 57:34.420
error and lots of like failed attempts. And that's what we're seeing with these like

57:34.420 --> 57:38.980
chat GPT, for instance, a lot of people are trying things out, reporting success and failure cases.

57:39.940 --> 57:46.660
So I do think there are certain similarities. What's more unique about what we're seeing right

57:46.660 --> 57:53.940
now is that due to the nature of like how black box, complex, unpredictable these models are,

57:53.940 --> 58:00.980
I think it just confuses people much more. And there's a question of, you know, is this really

58:00.980 --> 58:08.020
like a human learning problem to begin with, right? So if people take, do it more, and you know,

58:08.020 --> 58:13.220
if they had more time, will people be actually able to really get to a point where they could

58:13.220 --> 58:18.020
really easily create something that they like? Probably not. Right? So that's why I think we

58:18.020 --> 58:24.180
need both on the model side to kind of think about what are more interactable and learnable ways of,

58:24.180 --> 58:29.140
you know, architecting this kind of models in the first place. And also from the HCI point of view,

58:29.140 --> 58:35.060
what are these interaction mechanisms that could be added to these models in a way that

58:35.060 --> 58:40.180
it is actually more understandable and usable on the user side? Yeah. Thanks so much. Yeah.

58:41.140 --> 58:44.340
I think we're at about the time, but Duho will be here for a couple minutes after the talk for

58:44.340 --> 58:48.180
further questions. So let's thank him for speaking. Thank you.

