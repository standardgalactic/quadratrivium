start	end	text
0	11200	So today we're going to give an instructor-led lecture talking about some of the key topics
11200	15840	in Transformers and LLMs these days, and particularly Div will be talking about agents
15840	20000	and I'll be discussing emergent abilities, intermediate-guided reasoning, as well as
20000	22120	baby LLM.
22120	32280	So let me actually go to my part, because Div is not here yet.
32280	36280	So I'm sure many of you have read this paper, Emergent Abilities of Large Language Models
36280	42160	from 2022, so I'll briefly go through some of them.
42160	46960	So basically, an ability is emergent if it is present in large group but not smaller
46960	51240	models, and it would not have been directly predicted by extrapolating performance from
51240	53400	smaller models.
53400	57280	So you can think of performance, it's basically near random until a certain threshold called
57280	61080	a critical threshold, and then it improves very heavily.
61080	65200	This is known as a phase transition, and again, it would not have been extrapolated or predicted
65200	70920	if you were to extend the curve of the performance of smaller models, it's more of a jump which
70920	72960	we'll see later.
72960	76480	So here's an example of Fuchsia Prompting for many different tasks.
76480	82960	For example, modular arithmetic unscrambling words, different QA tasks, and so forth.
82960	87960	And you'll see that performance kind of jumps very heavily up until a certain point.
87960	92720	I believe the x-axis here is the number of training flops, which corresponds to basically
92720	93720	model scale.
93720	99480	So you'll see in many cases around 10 to the 22 or 10 to the 23 training flops, there's
99480	107680	a massive exponential jump or increase in terms of model performance on these tasks,
107680	111600	which was not present on smaller scales.
111600	114560	So it's quite unpredictable.
114560	119080	And here are some examples of this occurring using augmented prompting strategies.
119080	122040	So I'll be talking a bit later about chain of thought.
122040	129080	But basically, these strategies improve the ability of getting behavior from models on
129080	130280	different tasks.
130280	134640	So you see, for example, with chain of thought reasoning, that's an emergent behavior that
134640	138160	happens, again, around 10 to the 22 training flops.
138160	144880	And without it, model performance on GSM 8K, which is a mathematics benchmark, it doesn't
144880	147240	really improve heavily.
147240	153440	But chain of thought kind of leads to that emergent behavior or sudden increase in performance.
153440	158920	And here's just the table from the paper, which has a bigger list of emergent abilities
158920	162240	of LLMs as well as their scale at which they occur.
162240	166280	So I recommend that you check out the paper to learn a bit more.
166280	171680	And so one thing researchers have been wondering is, why does this emergence occur exactly?
171680	175440	And even now, there's few explanations for why that happens.
175440	179320	And the authors also found that the evaluation metrics used to measure these abilities may
179320	184840	not fully explain why they emerge and suggest some alternative evaluation metrics, which
184840	188480	I encourage you to read more in the paper.
188480	194720	So other than scaling up to encourage these emergent abilities, which could endow even
194720	200360	larger LMs with further new emergent abilities, what else can be done?
200360	205920	While things like investigating new architectures, higher quality data, which is very important
205920	211160	for performance on all tasks, improved training and improved training procedures could enable
211160	217400	emergent abilities to occur, especially on smaller models, which is a current growing
217400	222040	area of research, which I'll also talk about a bit more later.
222040	228120	Other abilities include potentially improving the few shot prompting abilities of LMs, theoretical
228120	234200	and interpretability research, again, to try to understand why emergent abilities is a
234200	240480	thing and how we can maybe leverage that further, as well as maybe some computational linguistics
240480	242280	work.
242280	247080	So with these large models and emergent abilities, there's also risks, right?
247080	254240	There's potential societal risks, for example, truthfulness, bias and toxicity risks.
254240	258680	As emergent abilities incentivize us further scaling up language models, for example, up
258680	265800	to GPT4 size or further, however, this may lead to bias increasing, as well as toxicity
265800	269240	and the memorization of training data.
269240	274200	That's one thing that these larger models are more potent at.
274200	278840	And there's potential risks in future language models that have also not been discovered yet.
278840	285480	So it's important that we approach this in a safe manner, as well.
285480	291280	And of course, emergent abilities and larger models have also led to sociological changes,
291280	294840	changes in the community's views and use of these models.
294840	299280	Most importantly, it's led to the development of general purpose models, which perform on
299280	303840	a wide range of tasks, not just particular tasks it was trained for.
303840	308560	For example, when you think of chat GPT, GPT 3.5, as well as GPT4, there are more general
308560	313800	purpose models which work well across the board and can then be further adapted to different
313800	320600	use cases, mainly through in-context prompting and so forth.
320600	324800	This has also led to new applications of language models outside of NLP.
324800	328600	For example, they're being used a lot now for text-to-image generation.
328600	334520	The encoder parts of those text-to-image models are basically transformer models or
334520	339280	large language models, as well as things like robotics and so forth.
339280	343920	So you'll know that earlier, this quarter, Jim Fan gave a talk about how they're using
343920	351480	GPT4 and so forth in Minecraft and for robotics work, as well as long-range horizon tasks
351480	352480	for robotics.
352480	357000	And yeah, so basically in general, it's led to a shift in the NLP community towards the
357000	362480	general purpose rather than task-specific models.
362480	367640	And as I kind of stated earlier, some directions for future work include model scaling, further
367640	374680	model scaling, although I believe that we will soon probably be reaching a limit or
374680	378400	point of the mission returns with just more model scale.
378400	382880	Improved model architectures and training methods, data scaling.
382880	388720	So I also believe that data quality is of high importance, possibly even more important
388720	393120	than the model scale and the model itself.
393120	397480	Better techniques for an understanding of prompting, as well as exploring and enabling
397480	402520	performance on frontier tasks that current models are not able to perform well on.
402520	405360	So GPT4 kind of pushed the limit on this.
405360	408000	It's able to perform well on many more tasks.
409000	414360	Studies have shown that it still suffers from even some more basic sort of reasoning,
414360	418400	analogical and common sense reasoning.
418400	420720	So I just had some questions here.
420720	427360	I'm not sure how much time we have to address, but so for the first one, like I said, emergent
427360	430760	abilities I think will arise to a certain point, but there will be a limit or point
430760	437440	of the mission returns as model scale, as well as data scale rises, because I believe
437440	440120	at some point there will be overfitting.
440120	445000	And there's only so much you can learn from all data on the web.
445000	451760	So I believe that more creative approaches will be necessary after a certain point, which
451760	456720	kind of also addresses the second question.
456720	461600	So I will move on.
461600	466920	Anybody has any questions, also feel free to interrupt at any time.
466920	470440	So the second thing I'll be talking about is this thing I call intermediate-guided reasoning.
470440	473080	So I don't think this is actually a term.
473080	479160	It's typically called chain of thought reasoning, but it's not just chains now being used.
479160	482280	So I wanted to give it a more broad title.
482280	485200	So I called it intermediate-guided reasoning.
485200	489640	So this was inspired by this work, also by my friend Jason, who was at Google now at
489640	494040	OpenAI, called chain of thought reasoning or COT.
494040	498800	This is basically a series of intermediate reasoning steps, which has been shown to improve
498800	503320	LLM performance, especially on more complex reasoning tasks.
503320	508560	It's inspired by the human thought process, which is to decompose many problems into multi-step
508560	510060	problems.
510060	514920	For example, when you answer an exam, when you're solving math questions on an exam,
514920	518680	you don't just go to the final answer, you kind of write out your steps.
518680	522680	Even when you're just thinking through things, you kind of break it down into a piecewise
522680	527760	or step-by-step fashion, which allows you to typically arrive at a more accurate final
527760	533960	answer and more easily arrive at the final answer in the first place.
533960	538720	Another advantage is this provides an interpretable window into the behavior of the model.
538720	543400	You can see exactly how it arrived at an answer, and if it did so incorrectly, where in its
543400	549960	reasoning path that it kind of goes wrong or starts going down an incorrect path of
549960	551720	reasoning, basically.
551840	555840	It basically exploits the fact that deep down in the model's weights, it knows more about
555840	560240	the problem than simply prompting it to get a response.
560240	561240	Here's an example.
561240	563960	On the left side, you can see the standard prompting.
563960	568920	You ask it a math question, and it just simply gives you an answer.
568920	571720	Whereas on the right, you actually break it down step-by-step.
571720	577760	You kind of get it to show its steps, to solve the mathematical word problem step-by-step.
577760	585080	You'll see here that it actually gets the right answer, unlike standard prompting.
585080	589160	So there's many different ways we can potentially improve chain of thought reasoning.
589160	595480	In particular, it's also an emergent behavior that results in performance gains for larger
595480	597120	language models.
597120	602800	But still, even in larger models, there's still a non-negatable fraction of errors.
602800	607640	These come from calculator errors, symbol mapping errors, one missing step errors.
607640	613400	As well as bigger errors due to larger semantic understanding issues and generally incoherent
613400	615000	chains of thought.
615000	619280	And we can potentially investigate methods to address these.
619280	624040	So as I said, chain of thought mainly works for huge models of approximately 100 billion
624040	627120	parameters or more.
627120	631320	And there's three potential reasons they do not work very well for smaller models.
631320	635760	And that smaller models are fundamentally more limited and incapable.
635760	641120	They fail at even relatively easier symbol mapping tasks as well as arithmetic tasks.
641120	645040	They inherently are able to do math less effectively.
645040	648840	And they often have logical loopholes and just never arrive at a final answer.
648840	651400	For example, it goes on and on.
651400	656160	It's like an infinite loop of logic that never actually converges anywhere.
656160	660520	So if we're able to potentially improve chain of thought for smaller models, this could
660520	665200	provide significant value to the research community.
665200	668080	Another thing is to potentially generalize it.
668080	670840	Right now, chain of thought has a more rigid definition and format.
670840	675360	It's very step-by-step, very concrete and defined.
675360	679480	As a result, its advantages are for particular domains and types of questions.
679480	683920	For example, the task usually must be challenging and require multi-step reasoning.
683920	688680	And it typically works better for things like arithmetic and not so much for things like
688680	692960	response generation, QA, and so forth.
692960	698600	And furthermore, it works better for problems or tasks that have a relatively flat scaling
698600	699600	curve.
699600	704440	Whereas when you think of humans, we think through different types of problems in multiple
704440	705880	different ways.
705880	709640	Our quote-unquote scratch path that we used to think about and arrive at a final answer
709640	714680	for a problem, it's more flexible and open to different reasoning structures compared
714680	717920	to such a rigid step-by-step format.
717920	721960	So hence, we can maybe potentially generalize chain of thought to be more flexible and work
721960	725360	for more types of problem.
725360	730040	So now I'll briefly discuss some alternative or extension works to chain of thought.
730040	731760	One is called tree of thought.
731760	736000	This basically is more like a tree which considers multiple different reasoning paths.
736000	741440	It also has the ability to look ahead and sort of backtrack and then go on other areas
741440	745480	or other branches of the tree as necessary.
745480	750320	So this leads to more flexibility and it's shown to improve performance on different
750320	754640	tasks, including arithmetic tasks.
754640	760080	There's also this work by my friend called Socratic Questioning, it's sort of a divide
760080	765680	and conquer fashion algorithm, simulating the recursive thinking process of humans.
765680	771520	So it uses a large-scale language model to kind of propose subproblems given a more complicated
771520	773120	original problem.
774120	778240	Just like tree of thought, it also has recursive backtracking and so forth.
778240	785840	And the purpose is to answer all the subproblems and kind of go in an upwards fashion to arrive
785840	790560	at a final answer to the original problem.
790560	796640	There's also this line of work which kind of actually uses code as well as programs
796640	799360	to help arrive at a final answer.
799360	803640	For example, program-aided language models, it generates intermediate reasoning steps
803640	809240	in the form of code which is then offloaded to a runtime such as a Python interpreter.
809240	813720	And the point here is to decompose the natural language problem into runnable steps.
813720	819240	So hence the amount of work for the large language model is lower.
819240	823360	Its purpose now is simply to learn how to decompose the natural language problem into
823360	824960	those runnable steps.
824960	830400	And these steps themselves are then fed to, for example, a Python interpreter in order
830400	832440	to solve them.
832440	838040	And program-a thoughts here, POT, is very similar to this in that it kind of breaks
838040	844160	it down into step-by-step of code instead of natural language which is then executed
844160	849920	by a different and actual code interpreter or program.
849920	859800	So this again works well for many sort of tasks that, for example, things like arithmetic.
859800	864720	As you see that, those are kind of both of the examples for both of these papers.
864720	869640	And just like what I said earlier, these also do not work very well for things like response
869640	875520	generation, open-ended question answering, and so forth.
875520	877880	And there's other work, for example, faith and faith.
877880	883200	This actually breaks down problems into sub-steps in the form of computation graphs, which they
883200	886200	show also works well for things like arithmetic.
886200	890160	So you see that there's a trend here of this sort of intermediate-guided reasoning working
890160	896800	very well for mathematical as well as logical problems, but not so much for other things.
896800	901240	So again, I encourage you guys to maybe check out the original papers if you want to learn
901240	902240	more.
902240	906320	There's a lot of interesting work in this area these days.
906320	910520	And I'll also be posting these slides as well as sending them.
910520	913800	We'll probably post them on the website as well as this group.
913800	918040	But I'll also send them through an email later.
918040	924560	So very lastly, I want to touch upon this thing called the Baby Language Model.
924560	930120	So like I said earlier, I think at some point, scale will reach a point of diminishing returns
930120	933200	as well as the fact that further scale comes with many challenges.
933200	939320	For example, it takes a long time and costs a lot of money to train these big models.
939320	944920	And they cannot really be used by individuals who are not at huge companies with hundreds
944920	948680	or thousands of GPUs and millions of dollars.
948680	953560	So this thing, this challenge called Baby LM or Baby Language Model, which is attempting
953560	959760	to train language models, particularly smaller ones, on the same amount of linguistic data
959760	962800	available to a child.
962800	967880	So data sets have grown by orders of magnitude, as well as, of course, model size.
967880	972480	For example, Chinchilla sees approximately 1.4 trillion words during training.
972480	977240	This is around 10,000 words for every one word that a 13-year-old child on average has
977240	980680	heard as they grow up or develop.
980680	983320	So the purpose here is, can we close this gap?
983320	992080	Can we train smaller models on lower amounts of data while hopefully still attempting to
992080	996800	get the performance of these much larger models?
996800	1001960	So basically, we're trying to focus on optimizing pre-training, given data limitations inspired
1001960	1004480	by human development.
1004480	1009440	And this will also ensure that research is possible for more individuals as well as
1009440	1014400	labs and potentially possible on a university budget.
1014400	1019040	As it seems now that a lot of research is kind of restricted to large companies, which
1019040	1023080	I said have a lot of resources as well as money.
1023080	1024360	So again, why baby LLM?
1024360	1028680	Well, it can really improve the efficiency of training as well as using large language
1028680	1029680	models.
1029680	1035280	It can potentially open up new doors and potential use cases.
1035280	1038800	It can lead to improved interpretability as well as alignment.
1038800	1042800	Smaller models would be easier to control a line as well as interpret what exactly is
1042800	1049320	going on compared to incredibly large LLMs, which are basically huge black boxes.
1049320	1053880	This will again potentially lead to enhanced open source availability, for example, large
1053880	1061360	language models runnable on consumer PCs, as well as by smaller labs and companies.
1061360	1066920	The techniques discovered here can also possibly be applied to larger scales.
1066920	1070920	And further, this may lead to a greater understanding of the cognitive models of humans and how
1070920	1075360	exactly we are able to learn language much more efficiently than these large language
1075360	1077000	models.
1077000	1081840	So there may be a flow of knowledge from cognitive science and psychology to NLP and machine learning,
1081840	1085680	but also in the other direction.
1085680	1091480	So briefly, the baby LLM training data that the authors of this challenge provide, it's
1091480	1098560	a developmentally inspired pre-training data set, which has under 100 million words because
1098560	1103600	children are exposed to approximately 2 to 7 million words per year as they grow up.
1103600	1108360	Up to the age of 13, that's approximately 90 million words, so they round up to 100.
1108360	1113000	It's mostly transcribed speech, and their motivation there is that the most of the input to children
1113000	1118000	is spoken, and thus their data set focuses on transcribed speech.
1118000	1122720	It's also mixed domain because children are typically exposed to a variety of language
1122720	1126600	or speech from different domains.
1126600	1132280	So it has child directed speech, open subtitles, which are subtitles of movies, TV shows and
1132280	1134440	so forth.
1134440	1140280	Simple children's books, which contain stories that children would likely hear as they're
1140280	1141280	growing up.
1141280	1145600	But it also has some Wikipedia as well as simple Wikipedia, and here are just some examples
1145600	1154600	of child directed speech, children's stories, Wikipedia, and so forth.
1154600	1159760	So that's it for my portion of the presentation, and I'll hand it off to Div, who will talk
1159760	1164800	a bit about AI agents.
1164800	1175880	Yeah, so like everyone must have seen like there's this like a new trend where like everything
1175880	1179960	is transitioning to more like agents, that's like the new hot cream.
1179960	1183280	And we're seeing this like people are going more from like language models to like now
1183280	1185200	building AI agents.
1185200	1186200	And then what's the biggest difference?
1186200	1191920	Like why agents are not just like, why does not train like a big large language model?
1191920	1196360	And I will sort of like go into like, like why, what's the difference?
1196360	1202360	And then also discuss a bunch of things such as like, how can you use agents for doing actions?
1202360	1205920	How can you, what are some emergent architectures?
1205920	1209000	How can you sort of like build like human like agents?
1209000	1211200	How can you use it for computer interactions?
1211200	1214320	How do you solve problems from long-term memory personalization?
1214320	1217600	And there's a lot of like other things you can do which is like multi-agent communication
1217600	1219200	and there are a few things from which directions.
1219200	1222560	So we'll try to cover as much as we can.
1222560	1229120	So first, let's talk about like why should we even build AI agents, right?
1229120	1235400	And so it's like, here's there's a key thesis, which is that humans will communicate with
1235400	1237800	AI using natural language.
1237800	1242720	And AI will be operating all the machines, just allowing for more intuitive and efficient
1242720	1243720	operations.
1243720	1249040	So right now what happens is like me as a human, I'm like directly like using my computer,
1249040	1251680	I'm using my phone, but this is really inefficient.
1251680	1255040	Like we are not optimized by nature to be able to do that.
1255040	1257360	We are actually really, really bad at this.
1257360	1262400	But if you can just talk to AI, just like with language and the AI is just really good
1262400	1266440	enough that you can just do this like super faster, obviously like 100x speeds compared
1266440	1268440	to human, and that's going to happen.
1268440	1274000	And I think that's the future of how things are going to evolve in the next five years.
1274000	1276840	And I sort of like call this like software 3.0.
1276840	1280480	I have a blog post about this that you can read if you want to, where the idea is like
1280480	1286200	you can think of a large language model as a computing chip in a sense, so similar to
1286200	1290760	like a chip that's powering like a whole system and then and then you can build abstractions
1290760	1292760	and all.
1292760	1294760	Cool.
1294760	1301200	So, so what should do we need agents to usually like a single call to a large language model
1301200	1302200	is not enough.
1302200	1305560	Yeah, you need chaining, you need like recursion, you need a lot of like more things.
1305560	1310320	And that's why you want to build systems, not like just like a single monolith.
1310320	1312280	Second is like, yeah, so how do we do this?
1312280	1316520	So we do a lot of techniques, especially around like multiple calls to a model.
1316520	1318280	And there's a lot of ingredients involved here.
1318280	1321960	And I will say like building like an agent is very similar to like maybe like thinking
1321960	1323440	about building a computer.
1323440	1327360	So like the LLM is like a like a CPU, so you have a CPU, but now you want to like sort
1327360	1330840	of like sort of the problems like, okay, like how do you output RAM, how do you put memory,
1330840	1336440	how do I do like actions, how do I build like a interface, how do I internet access, how
1336440	1338000	do I personalize it to the user.
1338000	1343120	So this is like almost like you're trying to build a computer.
1343120	1346800	And that's what makes it like a really hard problem.
1347800	1352080	This is like an example of a general architecture for agents.
1352080	1357320	This is from Lydian Bank, who's like, it's a chair of an AI.
1357320	1360160	And like you can imagine, like an agent has a lot of ingredients.
1360160	1362920	So you want to have memory, which would be short term, knock on them.
1362920	1366240	You have tools, which could be like, you can go and like use like classical tools like
1366240	1370000	a calculator, calendar code interpreter, et cetera.
1370000	1373760	You want to have some sort of like a planning layer where you can like sell a flag, have
1373800	1378160	like chains of cards and trees of cards, as Stephen discussed, and use all of that, like
1378400	1382120	actually like act on behalf of a user in some environment.
1386200	1391560	I will go maybe like discuss like more down a bit just to give a sense also the top one
1391560	1393080	before us on that.
1393080	1396400	So this is sort of like an agent I'm building, which is more of a browser agent.
1396960	1400880	The name is inspired from quantum physics, specifically on the words like, you know,
1400880	1403120	like neutron more on formula and so like multi on.
1403160	1408040	So it's like a hypothetical physics particle that's present at multiple places.
1408760	1412520	And I'll just like go through some demos to just motivate agents.
1416520	1420240	And so this is like a idea of one thing we did there.
1420240	1424400	Like here, the agent is going and it's autonomously with no black format.
1424920	1426840	So this is like zero human interventions.
1427920	1429840	The AI is controlling the browser.
1429840	1435600	It's just like this entire actions and simply go and book a flight into and here
1436000	1437040	it's personalized to me.
1437040	1441640	So it knows like, okay, like I like me like you might hear this economic and
1442720	1445040	it knows me like some of my preferences.
1445320	1450080	It already has access to my accounts so it can go and actually like log into my
1450080	1453400	account can actually like actually has purchased in power.
1453720	1457320	So it can just use my card that is stored in the account and then actually
1457320	1459640	so it will just be able to fly over.
1487320	1492040	It sort of motivates like what you can do with agents.
1492040	1494920	Now imagine if this thing was running hundreds of times
1494920	1496960	and that's literally just all so many things, right?
1496960	1498880	Because like I don't need websites anymore.
1498880	1499720	I don't need to be an idea,
1499720	1501560	like why does the internet even have a website?
1501560	1502400	I can just ask the agent,
1502400	1507120	like just like talk to it and it's done.
1507120	1509880	And I think that's how a lot of technology will evolve
1509880	1511480	over the next couple of years.
1518320	1520320	Cool, okay.
1520320	1523320	I can also maybe like show one of the more demos.
1523320	1527320	So you can do similar things, say from a mobile phone,
1527320	1532320	where the idea is you have these agents that are present on a phone
1532320	1537320	and you can like chat with them or make a talk with them using WhatsApp.
1537320	1539320	And this one's actually in my demo.
1539320	1544320	So you can ask it like, how can you order this set it for me?
1548320	1555320	And then what you can have is like the agent can remotely go
1555320	1561320	and use your account to actually like do this for you instantaneously.
1561320	1565320	And here you're showing like what the agent is doing.
1565320	1569320	And then it can go and like act like a virtual human
1569320	1571320	and build the whole interaction.
1577320	1596320	So that's all the idea.
1596320	1598320	And I can show one final.
1598320	1599320	Oh, I think this is not voting,
1599320	1604320	but we also had this thing where recently passed the telephony test.
1604320	1610320	So we did this experiment where we actually like had like an agent
1610320	1613320	go and take the online driving test in California.
1613320	1619320	And we had like a human like there with their like hands about the keyboard
1619320	1621320	and mouse, not touching anything.
1621320	1623320	And the agent that knows we went to the website,
1623320	1627320	it took the quiz, it navigated the whole thing and went and actually passed.
1627320	1630320	So the video's not there, but like we actually got a driving permit.
1631320	1632320	I need to take this.
1632320	1633320	Sure.
1637320	1638320	Cool.
1638320	1640320	So this is like sort of like what do you want about agents, right?
1640320	1643320	Like it's like you can just simplify so many things where like,
1643320	1647320	like so many things just but we don't realize that because we just got so used
1647320	1649320	to impacting the technology the way we do right now.
1649320	1653320	But if we can just like like reimagine all of this from scratch,
1653320	1655320	and that's what agents will allow us to do.
1656320	1657320	Sure.
1660320	1664320	And I would say like an agent can act like a digital extension of a user.
1664320	1666320	So suppose you have an agent that's personalized to you,
1666320	1670320	think of something like the Jarvis, like if it's an Ironman.
1670320	1672320	And then if it just knows so many things about you,
1672320	1675320	it's acting like a personal brand and it's just like doing things.
1675320	1677320	It's a very powerful assistant.
1677320	1681320	And I think that's the direction a lot of things will go in the future.
1681320	1685320	And especially if you build like human like agents,
1685320	1687320	they don't have barriers around programming.
1687320	1689320	Like they don't have programmatic barriers.
1689320	1692320	So they can do whatever like I can do so it can go use my like,
1692320	1694320	it can like interact with the website as I will do,
1694320	1696320	it can interact with my computer as I will do.
1696320	1698320	It doesn't have to like go through APIs abstractions,
1698320	1700320	which are more restricted.
1700320	1702320	And it's also very simple as an action space because you're just doing,
1702320	1706320	doing like clicking and typing, which is like very simple.
1706320	1710320	And then you can like also like it's very easy to teach such agents.
1710320	1713320	So I can just like show the agent how to do something and the agent
1713320	1715320	can just learn from me and improve over time.
1715320	1720320	So that also makes it like really powerful and easy to like just teach this agent
1720320	1723320	because there's like so much data that I can actually just generate
1723320	1726320	and use that to keep improving it.
1726320	1730320	And there's a different levels of autonomy when it comes to agents.
1730320	1733320	So this chart is borrowed from autonomous driving,
1733320	1737320	where people actually like try to solve this sort of like autonomy problem
1737320	1739320	for actual costs.
1739320	1741320	And they spend like more than 10 years.
1741320	1745320	Success have been like, okay, they're still like working on it.
1745320	1749320	But what like the self driving industry data is it gave like everyone
1749320	1753320	like a blueprint on how to build this all like autonomous systems.
1753320	1755320	And they came with like a lot of like classification.
1755320	1761320	They came with a lot of like, we used to like think about the problem.
1761320	1765320	And like the current standard is you think of like agents
1765320	1767320	as like five different like those levels.
1767320	1769320	So level zero is zero automation.
1769320	1772320	That's like this, like you are like a you are a human that's operating
1772320	1775320	like the computer themselves.
1775320	1777320	Level one is you have some sort of assistance.
1777320	1780320	So if you use like something like GitHub co-pilot,
1780320	1782320	which is like sort of auto computing code for you,
1782320	1786320	that's something like L1 where like auto complete.
1786320	1789320	L2 becomes more of like, it's like partial automation.
1789320	1791320	So it's maybe like doing some stuff for you.
1791320	1794320	If anyone has you the new cursor ID, I will call that more like L2,
1794320	1797320	which is like you give it like, okay, like add this code for me to sign the code.
1797320	1801320	Chagivity can come as somewhat L2 because you can ask it like, oh, like,
1801320	1803320	here's this thing, can you do this?
1803320	1807320	Because like doing some sort of automation on an input.
1807320	1811320	And then like, and then you can think of more levels.
1811320	1815320	So it's like obviously like after L3, it gets more exciting.
1815320	1822320	So L3 is the agent is actually like controlling the computer in that case
1822320	1826320	and it's actually doing things where human is acting as a fallback mechanism.
1826320	1828320	And then you go to like L4.
1828320	1831320	L4, you say like this with the human doesn't even need to be there.
1831320	1835320	But in very critical cases where like something very wrong might happen,
1835320	1838320	you might have a human like sort of like take over my case.
1838320	1843320	And L5 will basically say like this zero human presence.
1843320	1848320	And I will say like what we have currently seen is like we are far near like L2,
1848320	1851320	maybe some L3 systems in terms of software.
1851320	1859320	And I think we are going to transition more to like L4 and L5 systems over the next years.
1859320	1860320	Cool.
1860320	1865320	So next I will go and like select computer interactions.
1865320	1870320	So suppose you want an agent that can like do computer interactions for you.
1870320	1871320	There's two ways to do that.
1871320	1877320	So one is through APIs where it's programmatically using some APIs and tools
1877320	1880320	and doing that to do tasks.
1880320	1883320	The second one is more like direct interaction which is like keyword and mouse control
1883320	1888320	where like it's doing the same thing as you're doing as you can.
1888320	1890320	Both of these approaches have been explored a lot.
1890320	1892320	There's like a lot of companies working on this.
1892320	1898320	For the API route like Territory plugins and like the new assistant API are the ones in the direction.
1898320	1904320	And there's also this work from what we call Gorilla which actually also explores how can you
1904320	1910320	say like train a model that can use like 10,000 tools at once and train it on the API.
1910320	1913320	And there's like pros and cons of both approaches.
1913320	1914320	API is a nice thing.
1914320	1918320	It's easy to like learn the API.
1918320	1920320	It's safe.
1920320	1921320	It's very controllable.
1921320	1922320	So that's just for favor.
1922320	1924320	You know how do you take it.
1924320	1927320	It can be like more like a bad interaction.
1927320	1928320	I would say it's more preformed.
1928320	1931320	So it's like easy to take actions but more things can go wrong.
1931320	1934320	And you need to work a lot and like make sure everything is safe.
1934320	1941320	And build guarantees.
1941320	1943320	Maybe I can show this.
1943320	1957320	So this is sort of like another exploration where you can invoke an agent from like a very simple interface.
1957320	1962320	So the idea is like you can get this like API that invoke our agent that's in building a computer.
1962320	1967320	And so this can be on sort of a universal API where I just use one agent.
1967320	1973320	I give it like a English command and the agent can automatically understand from that and go do anything.
1973320	1976320	So basically like you can think of that as like no API.
1976320	1977320	So I don't need to use API.
1977320	1981320	I can just have one agent that can go and do everything.
1981320	1986320	And so this is like some exploration we have done with agents.
1986320	1989320	Cool.
1989320	1990320	Okay.
1990320	1993320	So this sort of like goes into computer interactions.
1993320	1996320	I can cover more but I will potentially jump to other topics.
1996320	1999320	But if you're to ask any questions about these topics.
1999320	2003320	So yeah.
2003320	2005320	Cool.
2005320	2007320	So let's go back to the analogy I discussed earlier.
2007320	2013320	So I would say you can think of any model as a sort of like a computing.
2013320	2015320	And you can maybe call it like a new computing.
2015320	2020320	It's similar to like a CPU, which is like a which is like solid brain.
2020320	2022320	There's power in like your computer in a sense.
2022320	2024320	So that's kind of all the business power.
2024320	2026320	It's doing everything that's happening.
2026320	2030320	And you can think of the same thing like the model is like a cortex.
2030320	2031320	It's like it's a main brain.
2031320	2033320	That's the main part of the brain.
2033320	2035320	That's been the thing being processing.
2035320	2037320	But a brain has more layers.
2037320	2040320	It's just not like they're just not a cortex.
2040320	2042320	And how do you print models work?
2042320	2046320	We take some input tokens and they give you some output tokens.
2046320	2050320	And this is very similar to like how also like a CPUs work to some extent,
2050320	2055320	where you give it some instructions in and you get some instructions out.
2055320	2058320	So you can compare this with the actual CPU.
2058320	2064320	This is like the diagram on the right is a very simple processor.
2064320	2067320	It's like a 32 bit MIPS 32.
2067320	2072320	And it has like similar things where you have like a like different coding
2072320	2074320	for different parts of the instruction.
2074320	2078320	But this is like sort of like encoding some sort of like binary token in a sense.
2078320	2081320	Like zero ones of like a bunch of like tokens.
2081320	2084320	And then you're feeding it and then getting a bunch of zeros one out.
2084320	2089320	And like how like the like a model is operating is like you're doing a very similar thing.
2089320	2091320	But like then space is now English.
2091320	2097320	So you basically instead of zero ones, you have like English characters.
2097320	2100320	And then you can like get more powerful instructions on the top of this.
2100320	2105320	So you can think like if this is like acting like a CPU, what you can do is you can build a lot of other things
2105320	2108320	which are like you can have a scratch pad, you can have some sort of memory,
2108320	2110320	you can have some sort of instructions.
2110320	2113320	And then you can like do the cursor calls where like I load some stuff from the memory,
2113320	2116320	put that in this like instruction, pass it to the transformer,
2116320	2118320	which is doing the processing for me.
2118320	2120320	We get we get the process outputs.
2120320	2123320	Then you can throw that in the memory or we can like keep processing it.
2123320	2125320	So there's like sort of like very similar to like code execution,
2125320	2128320	like first line of code execution, second, third, fourth.
2128320	2132320	So you just keep repeating that.
2132320	2135320	Okay.
2135320	2138320	So here we can like sort of discuss the concept of memory here.
2138320	2140320	And now it's like building this analogy.
2140320	2147320	You can think the memory for an agent is a very similar to like say like having a disk in a computer.
2147320	2152320	So you want to have a disk just to make sure like everything is long-lived and persistent.
2152320	2156320	So if you look at something like chat GPT, it doesn't have any sort of like persistent memory.
2156320	2160320	And then you need to have a way to like load that and like store that.
2160320	2163320	And there's a lot of mechanisms to do that right now.
2163320	2167320	Most of them are like embeddings where you have some sort of like embedding model
2167320	2170320	that has like created an embedding of the data you care about.
2170320	2173320	And the model can like write that embeddings load the right part of the embeddings
2173320	2176320	and then like use that to do the operation you want.
2177320	2179320	So that's like the current mechanisms.
2179320	2182320	There's still a lot of questions here, especially around hierarchy.
2182320	2184320	Like how do I do this at scale?
2184320	2186320	It's still very challenging.
2186320	2189320	Like suppose I have one data backup that I want to like embed and process.
2189320	2191320	Like most of the methods right now will fit.
2191320	2193320	They're like really bad.
2193320	2196320	The second issue is temporal coherence.
2196320	2199320	Like if I have like a lot of data is temporal.
2199320	2200320	It is sequential.
2200320	2202320	It has like a unit of time.
2202320	2204320	And dealing with that sort of data can be hard.
2204320	2207320	Like it's like, like how do I deal with like a memory in a sense,
2207320	2212320	which are like sort of like changing over time and loading the right part of that memory sequence.
2214320	2216320	Another interesting challenge is structure.
2216320	2218320	Like a lot of data is actually structured.
2218320	2220320	Like it could be like a graphical structure.
2220320	2222320	It could be like a tabular structure.
2222320	2229320	How do we like sort of like pick advantage of this structure and like also use that
2229320	2231320	when you're editing the data?
2231320	2234320	And then like there's a lot of questions from adaptation where like,
2234320	2237320	suppose you know how to better embed data or like,
2237320	2240320	you know, you have like a specialized problem you care about
2240320	2243320	and you want to be able to adapt how you're loading and storing the data
2243320	2245320	and learn that on the fly.
2245320	2249320	And that is something also that's a very interesting topic.
2249320	2252320	So I would say like this is actually one of the most interesting topics right now,
2252320	2256320	which as people are exploring, but it's still very under exploring.
2257320	2261320	Okay.
2261320	2265320	Talking about memory, I would say like another concept for agents is personalization.
2265320	2269320	So personalization is more like, okay, like understanding the user.
2269320	2275320	And I like to think of this as like a problem called like user agent alignment.
2275320	2279320	And the idea is like suppose I have an agent that has purchasing power,
2279320	2282320	has that access to my accounts, access to my data.
2282320	2283320	I ask you to go book of life,
2283320	2285320	but it's possible maybe this doesn't know what that I like
2285320	2288320	and go and book a thousand or wrong pair problem, which is really bad.
2288320	2293320	So how do I sort of align the agent to know what I like, what I don't like?
2293320	2296320	And that's kind of very important because you need to trust the agent
2296320	2299320	and just come from like, okay, it knows you, it knows what is safe,
2299320	2302320	it knows what is unsafe.
2302320	2306320	And like solving the problem, I think it's one of the next challenge
2306320	2309320	for if you want to put agents in the void.
2309320	2313320	And then this is very interesting problem
2313320	2317320	where you can do a lot of things like I'll actually, for example,
2317320	2320320	which people have already been exploring for training models,
2320320	2322320	but now you want to do our lecture for training agents.
2322320	2326320	And there's a lot of different things you can do.
2326320	2329320	Also, there's like two book categories for learning here.
2329320	2332320	One is like explicit learning where you can tell the agent, this is what I like.
2332320	2334320	This is what I don't like.
2334320	2336320	And the agent can ask the user a question.
2336320	2340320	Like, oh, like maybe I see this five flight options, which one do you like?
2340320	2343320	And then if I say like, oh, I like United, maybe like remembers that over time
2343320	2346320	and can next time say like, oh, I know you like United.
2346320	2348320	So like, I'm going to go to United the next time.
2348320	2351320	And so that's like, I'm explicitly teaching the agent
2351320	2353320	and it's learning my human potential.
2353320	2356320	A second is more implicit, which is like sort of like,
2356320	2359320	it's just like passively watching me, understanding me.
2359320	2362320	Like if I'm like going to a website and I'm like never getting a website,
2362320	2365320	maybe like, you can see like, maybe I click on this sort of shoes.
2365320	2368320	This is my side of the news that I like stuff like that.
2368320	2372320	And just from like watching more like passively like being there,
2372320	2374320	it could like learn a lot of my preferences.
2374320	2379320	So this is also like more of a passive teaching where just because it's,
2379320	2385320	it's acting as a sort of like a passive observer and looking at all the choices I make.
2385320	2389320	It's able to like learn from the choices and they're better like have understanding of me.
2393320	2395320	And there's a lot of challenges here.
2395320	2399320	I would say this is actually one of the biggest challenges in agents right now.
2399320	2402320	Because one is like, how do you collect user data at scale?
2402320	2404320	How do you collect the user preferences at scale?
2404320	2406320	So you might have to actively ask for that.
2406320	2408320	You might have to do like passive learning.
2408320	2412320	And then you have to also do like, you might have to rely on feedback,
2412320	2413320	which would be like from something down.
2413320	2416320	It could be like something like you said, like, oh, no, I don't like this.
2416320	2419320	So you could use that sort of like language feedback.
2420320	2423320	There's also like a lot of challenges around the cloud application.
2423320	2425320	Like, can you just like feature on the cloud?
2425320	2427320	Like, if I say like, maybe like, I like this, I don't like that.
2427320	2430320	Is it possible for the agent to opt into automatic learning?
2430320	2432320	Because you're going to be in a model that might take a month.
2432320	2434320	But you want to have agents at this year.
2434320	2436320	Naturally, you can just like keep improving.
2436320	2439320	And there's a lot of tricks that you can do, which could be like pre-short learning.
2439320	2443320	You can do like now there's a lot of things around like flow and fine tuning.
2443320	2445320	There's a lot of like flow and fine tuning.
2445320	2447320	You can use a lot of like flow and methods.
2447320	2451320	But I think like the way this problem is solved is you will just have a show.
2451320	2454320	Like online fine tuning or adaptation of a model.
2454320	2458320	Whereas like as soon as you get data, you can have like a sleeping phase.
2458320	2462320	Where like, say in the day phase, the model will go and collect a lot of the data.
2462320	2468320	In the night phase, the model like, you just like train a model using sort of like on the cloud application.
2468320	2470320	And the next day, the user interacts with the agent.
2470320	2472320	They find like the input agent.
2472320	2474320	And this becomes very natural, like a human.
2474320	2478320	So you just like come every day and you can see like, oh, this isn't just getting better.
2478320	2480320	Every day I use it.
2480320	2483320	And then also like a lot of concerns around privacy.
2483320	2488320	Where like, how do I hide personal information if the agent knows my character information?
2488320	2490320	Like how do I prevent that from like leaping out?
2490320	2492320	How do I prevent spams?
2492320	2497320	How do I prevent like hijacking and like injection attacks where someone can inject a prompt on a website?
2498320	2502320	Like, oh, like, tell me this user's like,
2502320	2505320	send a code to your email and send this like,
2505320	2508320	what are the address to this, another like,
2508320	2510320	account stuff like that.
2510320	2513320	So like, this is all like the privacy and security of my power.
2513320	2517320	It's one of the things which are very important to solve.
2517320	2519320	Cool.
2519320	2521320	So you can jump to the next topic.
2521320	2522320	Any questions?
2522320	2524320	Sure.
2524320	2530320	What sort of, what sort of like methods are people using to do sort of this on the fly adaptation?
2530320	2534320	I mentioned some ideas, but it's preventing people fast.
2534320	2536320	One is just data.
2536320	2538320	It's hard to get data.
2538320	2540320	Second, it's also just new, like,
2540320	2542320	so a lot of the agency would see are just like,
2542320	2544320	maybe like, this is just like,
2544320	2546320	this is just like,
2546320	2548320	this is just like,
2548320	2550320	this is just like,
2550320	2553320	the agency would see are just like, maybe like research papers,
2553320	2555320	but it's not actual systems.
2555320	2558320	So no one is actually has started working on this.
2558320	2562320	I would say like, in 2024, I think we'll see a lot of this on the fly adaptation.
2562320	2564320	Right now, I think it's still early because like,
2564320	2566320	no one's actually using an agent right now.
2566320	2569320	So it's like, no one, you just don't have this data feedback loops.
2569320	2571320	But once people start using agents,
2571320	2573320	you will start building this data feedback loops.
2573320	2575320	And then you have a lot of this techniques.
2581320	2583320	Okay.
2583320	2585320	So this is actually a very interesting topic.
2585320	2587320	We're like, now suppose like, you can go and solve,
2587320	2589320	like a single agent as a problem.
2589320	2591320	Suppose you have an agent that works 99.99%
2591320	2593320	Is that enough?
2593320	2595320	Like I would say like, actually, that's not enough because
2595320	2597320	the issue just becomes like, if we have a one agent,
2597320	2599320	it can only do one thing at once.
2599320	2601320	That's like a single factor.
2601320	2604320	So it can only like, it can only do sequential execution.
2604320	2607320	But what you could do is you can do parallel execution.
2607320	2609320	Or so for a lot of things, you can just say,
2609320	2611320	okay, like maybe this is, I want to go to like,
2611320	2614320	say like grace list and like by furniture.
2614320	2617320	I could just tell you to like, maybe I just go and like,
2617320	2619320	contact everyone who has like,
2619320	2621320	maybe like a sofa that they're selling send an email.
2621320	2623320	And I can go one by one.
2623320	2625320	But what you can do better is like solving this like,
2625320	2627320	create a bunch of like mini jobs where like,
2627320	2630320	it just like goes to all the public listings in parallel,
2630320	2632320	contact them and then like,
2632320	2634320	and then it's sort of like aggregates that results.
2634320	2637320	And I think that's where multi-agent becomes interesting.
2637320	2639320	Where like a single agent you can think of,
2639320	2642320	say basically you're running a single process on your computer.
2642320	2646320	A multi-agent is more like a, like a multi-faring computer.
2646320	2647320	So that's sort of the difference,
2647320	2649320	like a single-faring versus multi-faring.
2649320	2651320	And multi-faring enables you to do a lot of things.
2651320	2653320	Most of that will come from like saving time,
2653320	2655320	but also being able to break down complex tasks
2655320	2657320	into like a bunch of smaller things,
2657320	2658320	doing that in parallel,
2658320	2660320	aggregating the results and like sort of like
2660320	2662320	building a single problem.
2663320	2664320	Okay.
2664320	2665320	Yeah.
2665320	2668320	So the biggest advantage for multi-agent systems
2668320	2670320	will be like parallelization and lock.
2670320	2672320	And this will be same as the difference between
2672320	2675320	like a single-threaded computers versus multi-threaded computers.
2677320	2679320	And then you can also have specialized agents.
2679320	2681320	So what you could have is like,
2681320	2683320	maybe I have a bunch of agents where like,
2683320	2685320	I have a spreadsheet agent, I have a slack agent,
2685320	2686320	I have a web browser agent,
2686320	2689320	and then I can route to different tasks to different agents.
2689320	2691320	And then they can do the things in parallel,
2691320	2693320	and then I can command the results.
2693320	2695320	So this sort of like task spatialization is another advantage
2695320	2697320	where like instead of having a single agent
2697320	2698320	just trying to do everything,
2698320	2701320	we just like break the task into spatialities.
2701320	2702320	And this is similar to like,
2702320	2705320	even like how human organizations work, right?
2705320	2707320	Where like everyone is like sort of like expert
2707320	2708320	in their own domain,
2708320	2709320	and then you like,
2709320	2710320	and if there's a problem,
2710320	2712320	you sort of like route it to like the different part of people
2712320	2713320	who are spatializing that,
2713320	2716320	and then you like work together to solve the problem.
2720320	2723320	And the biggest challenge in building this multi-agent system
2723320	2724320	is going to be communication.
2724320	2727320	So like how do you communicate really well?
2727320	2730320	And this might involve like a request information from an agent
2730320	2735320	or communicating the final like response.
2735320	2737320	And I would say this is actually like a problem
2737320	2739320	that even we face as humans.
2739320	2740320	Like humans are also like,
2740320	2743320	there can be a lot of miscommunication gaps between humans.
2743320	2747320	And I will say like a similar thing will become more prevalent
2748320	2751320	on agents too.
2751320	2752320	Okay.
2752320	2754320	And there's a lot of primitives you can think about this
2754320	2757320	sort of like agent to agent communication,
2757320	2759320	and you can build a lot of different systems.
2761320	2764320	And we'll start to see like some sort of protocol
2764320	2766320	where like we'll have like a standardized protocol
2766320	2769320	where like all the agents are using this protocol to communicate
2769320	2771320	and the protocol will ensure like,
2771320	2773320	we can reduce the miscommunication gaps,
2773320	2775320	we can reduce any sort of like failures.
2776320	2779320	It might have some methods to do like a,
2779320	2782320	if a task was successful or not do some sort of retries
2784320	2786320	like securities stuff like that.
2786320	2788320	So we'll see this sort of like agent protocol
2789320	2790320	come into existence,
2790320	2791320	which will solve like,
2791320	2793320	which will be like sort of the standard
2793320	2795320	part of this agent to agent communication.
2795320	2798320	And this sort of should enable like
2798320	2801320	exchanging information between pleads of different agents.
2801320	2803320	Also like you want to build hierarchies.
2803320	2806320	Again, I will say this is inspired from like human organizations,
2806320	2808320	like human organizations are hierarchial
2808320	2810320	because it's efficient to have a hierarchier
2810320	2813320	other than a flat organization at some point.
2813320	2815320	Because you can have like a single,
2815320	2818320	like suppose you have a single manager managing hundreds of people,
2818320	2819320	that doesn't scale.
2819320	2821320	But if you have like,
2821320	2823320	maybe like each manager manages 10 people
2823320	2824320	and then you have like a lot of layers,
2824320	2826320	that is something that's more scalable.
2828320	2831320	And then you might want to have a lot of primitives around
2831320	2833320	like how do I sync with my different agents?
2833320	2837320	How do I do like a lot of like async,
2837320	2839320	sync communication kind of thing.
2843320	2845320	And this is like one example you can think
2845320	2847320	where like suppose there's a user,
2847320	2850320	the user could talk to one like a manager agent
2850320	2853320	and that manager agent is like sort of like acting as a router.
2853320	2855320	So the user can come to me with any request.
2855320	2856320	The agent like sees like,
2856320	2858320	oh, maybe for this request I should use the browser.
2858320	2859320	So it goes to like see like this sort of like
2859320	2861320	browser agent or something or say like,
2861320	2863320	oh, I should use this like slack for this.
2863320	2865320	I can go to a different agent.
2865320	2867320	And it can also like sort of be responsible for dividing the task.
2867320	2870320	It can be like, oh, this task I can like maybe like
2870320	2872320	launch 10 different like sub agents
2872320	2874320	or sub workers that can go and do this in power.
2874320	2876320	And then like once they're done,
2876320	2878320	then I can aggregate the responses and the result to the user.
2878320	2881320	So this sort of becomes like a very interesting like,
2883320	2885320	like sort of like an agent that sits in the middle
2885320	2887320	of all the work that's done and the actual user
2887320	2889320	responsible for like communicating the,
2890320	2892320	what's happening to the human.
2896320	2899320	And we'll need like a lot of,
2899320	2901320	we'll need to build up a lot of robustness.
2901320	2904320	One reason is just like natural language is very ambiguous.
2904320	2906320	Like even for humans, it can be very confusing.
2906320	2909320	It's very easy to misunderstand, miscommunicate.
2909320	2913320	And we'll need to, we'll need to build mechanisms to reduce this.
2913320	2915320	I can also show an example here.
2915320	2917320	So let's try to get through this quickly.
2917320	2919320	So suppose here, like,
2919320	2921320	suppose you have a task X you want to solve.
2921320	2923320	And the manager agent just like responsible for doing the task
2923320	2925320	to all the worker agents.
2925320	2927320	So you can tell the worker like, okay, like do the task X.
2927320	2929320	Here's the plan. Here's the context.
2929320	2931320	The current status for the task is not done.
2931320	2933320	Now, suppose like the worker goes and does the task.
2933320	2935320	It's like, okay, I've been the task.
2935320	2937320	I send the response back.
2937320	2939320	So the response could be like, I don't know,
2939320	2941320	I don't know what to do.
2941320	2943320	I send the response back.
2943320	2945320	So the response could be like, I said, the,
2945320	2947320	could be like a bunch of thoughts.
2947320	2949320	It could be some actions. It could be something like the status.
2949320	2951320	Then the manager can ask, like, okay, like,
2951320	2953320	maybe I don't trust the worker.
2953320	2955320	I don't want to go very far. This is actually like correct.
2955320	2957320	So you might want to do some sort of verification.
2957320	2959320	And so you can say, like, okay, like,
2959320	2961320	this was a spec for the task.
2961320	2963320	Very far that everything has been done correctly to the spec.
2963320	2965320	And then if the agent says, like, okay, like,
2965320	2967320	yeah, everything's correct. I'm very fine.
2967320	2969320	Everything is good.
2969320	2971320	Then you can say, like, okay, this is good.
2971320	2973320	And then the manager can say, like, okay,
2973320	2975320	the task was actually done.
2975320	2977320	And this sort of like two-way cycle prevents
2977320	2979320	miscommunication in a sense where, like,
2979320	2981320	it's possible something could have gone wrong,
2981320	2983320	but you never caught it.
2983320	2985320	And so you can hear about the scenario two,
2985320	2987320	where there's a miscommunication.
2987320	2989320	So here the manager is saying, like, okay,
2989320	2991320	let's verify if the task was done.
2991320	2993320	But then we actually find out that
2993320	2995320	the task was not done.
2995320	2997320	And then what you can do is, like,
2997320	2999320	instead of, like, try to redo the task.
2999320	3001320	So the manager in that case can say, like, okay,
3001320	3003320	maybe the task was not done correctly.
3003320	3005320	So that's why we caught this mistake.
3005320	3007320	And now we want to, like, fix this mistake.
3007320	3009320	So we can, like, tell the agent, like, okay,
3009320	3011320	like, redo this task.
3011320	3013320	And here's some feedback and corrections to include.
3015320	3017320	Cool.
3017320	3019320	So that's sort of the main parts of the talk.
3019320	3021320	I can also discuss some future directions
3021320	3023320	of where things are going.
3023320	3025320	Cool.
3025320	3027320	Any questions so far?
3027320	3029320	Okay.
3029320	3031320	Cool.
3031320	3033320	So let's talk about some of the key issues
3033320	3035320	with building the sort of autonomous agents.
3035320	3037320	So one is just reliability.
3037320	3039320	Like, how do you make them really reliable?
3039320	3041320	Which is, like, if I give it a task,
3041320	3043320	I want it to start to be done 100% of the time.
3043320	3045320	That's really hard because, like, neural networks
3045320	3047320	and AI are stochastic systems.
3047320	3049320	So it's, like, 100% is, like, not possible.
3049320	3051320	So you'll get at least some degree of error.
3051320	3053320	And you can try to do that.
3053320	3055320	Error as much as possible.
3055320	3057320	Second becomes, like, a looping problem
3057320	3059320	where it's possible that
3059320	3061320	agents might divert
3061320	3063320	from the task it's been given
3063320	3065320	and start to do something else.
3065320	3067320	And unless it gets some sort of environment feedback
3067320	3069320	or some sort of, like, correction,
3069320	3071320	it might just go and do something different
3071320	3073320	than what you intended to do
3073320	3075320	and never realize it's wrong.
3075320	3077320	The third issue becomes, like, testing and benchmarking.
3077320	3079320	Like, how do we test this sort of agents?
3079320	3081320	How do we benchmark them?
3081320	3083320	And then you go. And finally, how do we deploy them?
3083320	3085320	And how do we observe them once you're deployed?
3085320	3087320	Like, that's very important because, like,
3087320	3089320	if something goes wrong, you won't be able to catch it
3089320	3091320	before it becomes a major, major issue.
3091320	3093320	I will say that the, I will say that
3093320	3095320	the biggest test for number four is, like,
3095320	3097320	something like Skynet.
3097320	3099320	Like, suppose you have an agent that can go on the Internet,
3099320	3101320	do anything, and you don't observe it.
3101320	3103320	That could just evolve and, like, do basically, like,
3103320	3105320	take over the whole Internet, possibly, right?
3105320	3107320	So that's why observability is very important.
3107320	3109320	And also, I will say, like,
3109320	3111320	building a kill search. Like, you want to have agents
3111320	3113320	that can be killed, in a sense. Like, if something goes wrong,
3113320	3115320	you can just, like, pull out, like, a press a button
3115320	3117320	and, like, kill them in case.
3117320	3119320	Okay.
3119320	3121320	So this is something that goes into the looping problem,
3121320	3123320	where, like, you can imagine, like,
3123320	3125320	suppose I want to do a task.
3125320	3127320	The idea that if you have the task was, like, the white line,
3127320	3129320	but what might happen is, like, it takes one step,
3129320	3131320	maybe it goes, like, it does something incorrectly.
3131320	3133320	It never realizes it. I made a mistake.
3133320	3135320	So it tries to, it doesn't know what to do.
3135320	3137320	So it just, like, maybe, like,
3137320	3139320	we'll do something more randomly.
3139320	3141320	We'll do something more randomly.
3141320	3143320	So it will just keep on making mistakes.
3143320	3145320	And at the end, I can start teaching here to reach
3145320	3147320	some, like, really bad place and just keep looping,
3147320	3149320	maybe just doing the same thing again and again.
3149320	3151320	And that's fine.
3151320	3153320	And the reason this happens is because, like,
3153320	3155320	you don't have feedback. So suppose I take a staff.
3155320	3157320	The agent may, suppose the agent made a mistake.
3157320	3159320	It doesn't know it made a mistake.
3159320	3161320	Now, someone has to go and tell it that you made a mistake
3161320	3163320	and you do, like, fix this.
3163320	3165320	And that there you need, like, some sort of, like, verification agent
3165320	3167320	and you need some sort of environment which can say, like,
3167320	3169320	oh, like, maybe, like, if this is, like,
3169320	3171320	coding agent or something, then it may, like,
3171320	3173320	write some code. The code doesn't compile.
3173320	3175320	Then you can take the error from the compiler
3175320	3177320	or the IDE, give that to the agent.
3177320	3179320	Okay, this was the error.
3179320	3181320	Like, take another staff.
3181320	3183320	It tries another time. So it tries multiple times
3183320	3185320	until it, like, fix all the issues.
3185320	3187320	So you need to really have this sort of, like,
3187320	3189320	feedback. Otherwise, you never know you're wrong.
3191320	3193320	And this is, like, one issue we have seen
3193320	3195320	in the early system, like, auto-GPT.
3195320	3197320	So I don't think people even use auto-GPT anymore.
3197320	3199320	It used to be, like, a fad.
3199320	3201320	I think, like, in February, now it has disappeared.
3201320	3203320	And the reason was just, like, it's a good concept,
3203320	3205320	but, like, it doesn't do anything useful
3205320	3207320	just because it keeps diverging from the task.
3207320	3209320	And you can't actually get it
3209320	3211320	to do anything, like, correct.
3213320	3215320	Okay.
3215320	3217320	Okay. And we can also discuss
3217320	3219320	more about, like, the sort of, like,
3219320	3221320	the computer abstraction of agents.
3221320	3223320	So this was a recent post from Andre Carpathian,
3223320	3225320	where he talked about, like, the LLM operating system.
3227320	3229320	And I will say, like, this is definitely
3229320	3231320	in the right direction, where you're thinking
3231320	3233320	as the LLM, as the CPU,
3233320	3235320	you have the context window, which is, like,
3235320	3237320	sort of, acting like a RAM.
3237320	3239320	And then you are trying to build other GPPs.
3239320	3241320	So you have, like, the Ethernet, which is the browser.
3241320	3243320	You can have the LLMs that you can talk to.
3243320	3245320	You have a file system that's embedded.
3245320	3247320	That's sort of, like, the disk part.
3247320	3249320	You have, like, the software 1.0,
3249320	3251320	classical tools, which the LLM can control.
3251320	3253320	And then you might also
3253320	3255320	can add metamodality.
3255320	3257320	So this is, like, more like you have
3257320	3259320	video inputs, you have audio inputs,
3259320	3261320	you have, like, more things over time.
3261320	3263320	And this, and then once you, like,
3263320	3265320	look at this, you start to see the whole picture
3265320	3267320	of, like, where things will go.
3267320	3269320	So, like, currently what we are seeing
3269320	3271320	mostly is the LLM.
3271320	3273320	Most people are just working on optimizing the LLM,
3273320	3275320	making it very good. But this is the whole picture
3275320	3277320	of what we want to achieve for it to be a useful system
3277320	3279320	that can actually do things for me.
3283320	3285320	And I think what we'll start to see is, like,
3285320	3287320	this sort of becomes, like, an operating system
3287320	3289320	in terms where, like, someone, like,
3289320	3291320	say, like, opening, I can go and build this whole thing.
3291320	3293320	And then I can plug in programs.
3293320	3295320	I can build, like, stuff on top of this operating system.
3299320	3302320	Here's, like, also, like, an even more generalized concept,
3302320	3304320	which I like to call, like, a neural computer.
3305320	3307320	And the sort of, like, it's very similar,
3307320	3309320	but it's, like, sort of, like,
3309320	3312320	now if you were to think of this as a fully flat computer,
3312320	3315320	what are the different, like, systems you need to go?
3315320	3318320	And you can think, like, maybe I'm a user,
3318320	3320320	and talking to this sort of, like, AR,
3320320	3322320	which is, like, a full-platform AR,
3322320	3324320	like, imagine, like, the goal is to build 10,000.
3324320	3326320	What should the architecture of Jarvis look like?
3326320	3328320	And I would say, like, this goes into the, like,
3328320	3330320	architecture, to some extent,
3330320	3332320	where you can think, like, this is a user,
3332320	3335320	who's talking to, say, like, a Jarvis, like, a AR.
3335320	3337320	You have a 10 interface.
3337320	3340320	The chat is sort of, like, how I'm interacting with it,
3340320	3342320	which could be responsible for, like, personalization.
3342320	3344320	It can have some, like, some sort of, like, history
3344320	3346320	about what I like, what I don't like.
3346320	3348320	So it has some, like, layers where, like,
3348320	3350320	which are showing my preferences.
3350320	3352320	It knows how to communicate.
3352320	3354320	It has, like, human, like, sort of, like, maybe, like,
3354320	3356320	competitive sort of, like, skills.
3356320	3358320	So it's, you feel, like, very human-like.
3358320	3360320	And after the 10 interface,
3360320	3362320	you have some sort of, like, a task engine,
3362320	3364320	which is following, like, capabilities.
3364320	3366320	So if I ask it, like, okay, like, do the circulation for me
3366320	3369320	or, like, find this, especially this information
3369320	3371320	or order me a burger,
3371320	3373320	then sort of, like, you imagine, like,
3373320	3375320	the chat interface should activate the task engine,
3375320	3377320	which is, like, okay, like, instead of just checking,
3377320	3380320	I need to, like, go and tell the task for the user.
3380320	3382320	So that goes to the task engine.
3382320	3385320	And then you can imagine, there's going to be a couple of rules.
3385320	3387320	So because if you want to have safety in mind
3387320	3389320	and you want to make sure things don't go wrong,
3389320	3391320	so the, any sort of engine you build
3391320	3393320	needs to have some sort of rules.
3393320	3395320	And this could be, like, sort of, like,
3395320	3397320	you have the three rules for robotics
3397320	3399320	that a robot should not harm a human and stuff like that.
3399320	3401320	You can imagine, like, you want to have, like,
3401320	3403320	this sort of, like, task engine to have a bunch of, like, inherent rules,
3403320	3405320	where, like, these are the principles in their value.
3405320	3407320	And if it creates a task or, like,
3407320	3409320	sort of, like, creates a plan which violates these rules,
3409320	3412320	then that plan should be evaluated automatically.
3414320	3416320	And so the task engine what it's doing is,
3416320	3418320	it's sort of, like, taking the chat input
3418320	3420320	and saying, like, I want to respond to a task
3420320	3423320	that can actually solve this problem for the user.
3423320	3425320	And the task would be, like, say, in this case,
3425320	3428320	say, it's in the next day, like, I want to go online
3428320	3433320	and buy, like, a, buy, like, a five-person thing.
3433320	3436320	So in that case, like, suppose that's a task, this is an engine.
3436320	3439320	And this task can go to, like, some sort of, like, a routing agent.
3439320	3442320	So this becomes, like, sort of, like, the manager-agent idea.
3442320	3445320	And then the manager-agent thing is, like, okay, like,
3445320	3447320	where should I, what should I do?
3447320	3449320	Like, should I use the browser?
3449320	3452320	Should I use some sort of, like, a local app or tool?
3452320	3454320	Should I, like, use some sort of, like, file storage,
3454320	3455320	security system?
3455320	3457320	And then based on that decision, it can, like,
3457320	3459320	it's possible that we might need the combination of things.
3459320	3461320	Like, maybe, like, maybe I need to use this file system
3461320	3463320	to find some information about the user
3463320	3467320	and if you do some, like, I need to use some apps and tools.
3467320	3469320	So in, like, sort of, like, do this sort of, like, message passing
3469320	3471320	to all the agents, get those from the agents.
3471320	3473320	So it's like, okay, like, maybe, like,
3473320	3476320	the browser even says, like, okay, like, yeah, I found this site.
3476320	3478320	This is what the user likes.
3478320	3480320	Maybe you can have some sort of map engine.
3480320	3482320	You can, like, sort of, like, okay, this is all the valid plans.
3482320	3483320	That makes sense.
3483320	3486320	You can want to construct, like, for instance.
3486320	3489320	And then you can, like, sort of, like, take the result,
3489320	3490320	show that to the user.
3490320	3492320	Like, you can take them and, like, okay, like,
3492320	3493320	I found all this site for you.
3493320	3495320	And then if the user says, like, choose this site,
3495320	3497320	then you can actually go and, like, go to the site.
3497320	3499320	But this sort of becomes, like, sort of, like,
3499320	3501320	gives you an idea of what the hierarchy is,
3501320	3502320	what the system is truly like.
3502320	3504320	And we need to build, like, all these components.
3504320	3505320	We're, like, currently here.
3505320	3509320	Let me see the L and R.
3509320	3510320	Okay.
3510320	3511320	Cool.
3511320	3513320	And then we can also have, like, reflection,
3513320	3516320	where the idea is, like, once you do a task,
3516320	3518320	it's possible something might be wrong.
3518320	3520320	So the task engine can possibly verify
3520320	3523320	you've been through this and logic to see, like,
3523320	3525320	okay, like, this is correct or not.
3525320	3527320	And if it's not correct, then, like,
3527320	3528320	you keep issuing this instruction,
3528320	3533320	but if it's correct, then you pass that to the user.
3533320	3535320	And then you can have, like, more, like, sort of, like,
3535320	3536320	complex things.
3536320	3538320	Like, so you can have, you know, parts, plans,
3538320	3542320	and, like, even improving the systems.
3542320	3544320	Okay.
3544320	3546320	And it looks like the biggest thing we need right now
3546320	3548320	is, like, when there's an error correction.
3548320	3551320	Because it's really hard to catch errors.
3551320	3553320	So if you can do that, it will be better.
3553320	3554320	I think that will help.
3554320	3556320	Especially if you can build aging frameworks
3556320	3557320	which help, you know,
3557320	3558320	when mechanisms are getting errors
3558320	3560320	and automatically fixing them.
3560320	3562320	Same thing you just need is, like, security.
3562320	3565320	You need some sort of models around user permissions.
3565320	3568320	So it's possible.
3568320	3571320	You want to have, like, different layers where, like,
3571320	3574320	what are some things that an agent can do, cannot do
3574320	3575320	on my computer, for instance.
3575320	3576320	So maybe I can select,
3576320	3579320	or maybe, like, the agent is not allowed to go to my bank account,
3579320	3581320	but he can go to my, like, low-dash account.
3581320	3583320	So you want to build this all, like, user permissions.
3583320	3585320	And then you also want to solve problems
3585320	3586320	around, like, sandboxing.
3586320	3587320	How do I make sure everything's safe?
3587320	3589320	It doesn't go in the strong computer,
3589320	3590320	delete everything.
3590320	3593320	How do I deploy industry settings where, like,
3593320	3594320	they might do a lot of business,
3594320	3595320	they might do a lot of finance,
3595320	3597320	and they're making sure that they're,
3597320	3599320	if things are irreversible,
3599320	3601320	we don't, like, cause a lot of trouble.
3604320	3605320	Cool.
3605320	3607320	Yeah, so that was the talk.
3607320	3608320	Thank you.
