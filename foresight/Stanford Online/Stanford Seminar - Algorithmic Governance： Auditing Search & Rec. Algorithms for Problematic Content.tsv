start	end	text
0	15240	So this work is part of a larger thread of work centered around algorithmic governance.
15240	22400	That is, how do we audit and measure problematic content like mis-disinformation, online extremism,
22400	26920	and how do we do that on online platforms, and then specifically the search and recommendation
26920	29720	algorithms driving these platforms.
29720	34080	So as I was preparing for this talk, I thought of opening it up in a slightly unconventional
34080	39440	way with this particular quote, which I'm going to read a little bit, which says, mankind
39440	44720	barely noticed when the concept of massively organized information quietly emerged to become
44720	50480	a means of social control, a weapon of war, and a roadmap for group destruction.
50480	54720	Any guesses as to which year of the world this quote describes?
54720	63640	Is it the world, is it a current world, 2020s, 2000s, 1900s?
63640	67960	Some people could guess just by the word mankind in previous talk.
67960	73160	Yeah, that's absolutely accurate.
73160	78440	So this is actually Edwin Black's description of the world of the 1930s, 1940s, during the
78440	83160	World War II, and the invention of the punch card, which was at that time the new wave
83160	86360	of automation and data collection.
86360	91040	And what is more surprising is that this quote is still true in today's 21st century world,
91040	96640	and we have seen many new scholarships, and I'm sure folks here have read and are familiar
96640	101640	with these books, many of these outline the harms of automation and data collection.
101640	106320	And more recently, this discussion has shifted to the harms posed by generative AI and large
106320	107320	language models.
107320	111360	So I'm going to read this quote a little bit more of what Edwin Black had to say in his
111360	114680	book, which is a lot of parallels to a present-day world.
114680	119080	So he goes on to say, the unique igniting event was the most fateful day of the last
119080	124760	century when Adolf Hitler came to power, but for the first time in history, an anti-Semayet
124760	128960	had automation on his side, so that was what was different, and the automation was in the
128960	131720	form of IBM punch cards.
131720	136960	So he goes on to say that IBM was self-criped by a special immoral corporate mantra, if
136960	139680	it can be done, it should be done.
139680	143720	So we should take a pause and kind of re-read this phrase, if it can be done, it should
143720	150200	be done, which is parallels very re-resemblance to the motives and culture practiced in modern-day
150200	155560	technology companies, and I'm sure being in the Silicon Valley, you all probably have
155560	160000	seen some of this while interning at some of these companies, move fast and break things,
160000	164480	done is better than perfect, what would you do if you weren't afraid.
164480	168800	And I argue that the sort of rushed culture, the sort of culture of disruption and speed
168800	173760	often poses a great challenges in governing these technologies and conducting thoughtful
173760	179480	medicalist audits, and in turn this is what makes algorithmic governance a difficult problem.
179480	183400	And I'll come back to these ideas later on towards the end of the talk, but let me first
183400	189360	dive into the meat of this presentation, a couple of studies which we did in this realm
189360	191040	of algorithmic governance.
191040	196560	So here I want to focus on two types of algorithms, search and recommendations, and two specific
196920	200080	platforms, YouTube and Amazon.
200080	206040	And our focus was only one type of problem, misinformation particularly.
206040	210520	Now you might wonder why search and recommendation algorithms, so now the world is moving towards
210520	216040	large language models, why we should focus on these old technologies, search and recommendations.
216040	220680	So users generally have an unwavering trust in search engines, so several scholarly work,
220680	225560	some of these cited here have actually shown that these ranking of search results have
226040	230440	really dramatic effect on users' attitudes, their preferences, their behaviors.
230440	235160	In fact, bias search rankings are so powerful they can even shift voting preferences of
235160	239080	undecided waters by so much so as 20%.
239080	243440	And users don't even show awareness of that kind of manipulation happening in their search
243440	244440	results.
244440	249120	So this should tell you how powerful search algorithms are, and then that is why we should
249120	253640	keep studying them despite the other new shiny technologies coming our way.
253720	257560	For recommendation algorithms, I hope I don't have to make a case here because you see those
257560	262960	almost everywhere starting from what recommendations, what movies you should watch, what products
262960	267080	one should buy, which campaign you should donate to, they are almost everywhere.
267080	269000	And they also have these reinforcing effect, right?
269000	274240	So the more you like, watch certain content, the more you share, the more you get those
274240	276440	types of content.
276440	281160	Now this might be harmless say when you're going out this weekend and trying to buy Christmas
281200	286480	decorations, but these things can get ugly quite quickly when say you are browsing for
286480	292040	vaccine information, health information or climate information.
292040	294720	Now why audit for misinformation in particular, right?
294720	300440	So currently there is this disproportionate focus on AI bias and fairness and tech journalist
300440	305560	Karen Howe captures this notion very well in her article where she mentions how often
305560	310760	responsible AI teams and companies are pigeonholed into targeting AI bias.
310760	314360	Now don't get me wrong, so bias and fairness are indeed important topics that one should
314360	320080	pursue, but tackling just AI bias draws away attention from fixing much bigger problems
320080	325400	of other types of harmful information such as misinformation, extremism, conspiratorial
325400	326480	content.
326480	331000	So this is what is underlying motivation behind the set of studies that we did.
331000	336840	And with that, let's dive into this first study, the YouTube audit work.
337120	341680	A major motivation for this YouTube study was coming from these frequent headlines that
341680	346440	I was noticing a few years ago, how YouTube is driving people in the internet's darkest
346440	351240	corners, and then there were these opinion pieces talking about YouTube being the great
351240	353640	radicalizer.
353640	358240	But YouTube was also responding with articles saying that they will be reducing conspiracy
358240	363400	theory recommendations or making it much harder to find those on the platform.
363400	368640	And all these questions and all these reports are really anecdotal, like empirically how
368640	369920	bad is this, right?
369920	372200	Do we really know that?
372200	377360	And this is what I studied to try to do, verify these anecdotal claims that does YouTube really
377360	380160	surface these problematic content.
380160	384840	And in order to do this, we conducted these systematic audits on YouTube search and recommendation
384840	389480	algorithms and we picked one type of problematic content, conspiracy theories.
389480	392600	Now what really is an audit, right?
392600	397720	I have talked about mentioned audit a couple of times so far, but how do you audit algorithms?
397720	402480	I'm sure some of you in the audience might be familiar with the concept of audit.
402480	406040	For those of you who don't know, I'm going to give you a quick definition through an
406040	410920	example and also say that this is a thriving field of research, lots of work has been done
410920	412800	in this space.
412800	416960	And this was one of the earliest example of audits coming from the social science world.
416960	422000	And it's also one of my favorite ones from this 2004 paper, where researchers conducted
422000	427120	this very clever field experiment to investigate employment discrimination.
427120	431720	That is, they audited the labor market for racial discrimination.
431720	436720	So what they did was they responded with fictitious resumes to help wanted ads in the Boston
436720	438960	and Chicago newspapers.
438960	443280	To manipulate the perception of race, what they did was they kept everything else in
443280	446800	each of these resumes constant, only they changed the names.
446800	451960	And the names were either very African American sounding names such as Laquisha or Jamal or
451960	457080	very white sounding names such as Emily or Greg, and hence the name of this paper.
457080	461160	The results showed that there was significant discrimination against African American names
461160	468040	while white names received 50% more callbacks for interviews compared to the African American
468040	471800	names despite everything else being constant in those resumes.
471800	475640	So this is a core idea behind audits, that is, you would keep everything else constant
475640	479880	and then you manipulate a single variable to determine how that change would affect
479880	481520	the algorithm.
481520	486560	So if you translate this into the context of YouTube, you would manipulate a variable
486560	491520	to determine whether the search and recommendation algorithm returns different results, say when
491520	497680	someone's age, their other demographic attributes, gender, their watch history, where they are
497680	504520	searching from, the geolocation, if that differs, what happens with the research results.
504520	508000	So to answer this, we set up this elaborate audit framework, which broadly looked like
508000	511800	this, where we had programmed bots, or in other words, we were conducting these sock
511800	512800	puppet audits.
512800	517800	So these were bots or sock puppets, which behave like normal users logging into YouTube,
517800	522200	running queries on the search platform while at the same time, a script at the back end
522200	528200	were collecting whatever search and recommendation results the platform was returning.
528200	532080	So we started with selecting search topics, and a goal here was to make sure that the
532080	536280	topics are indeed high impact, that is, lots of people are searching for these topics,
536280	540320	so they should be popular, and they should be topics which also have the potential to
540320	542880	return false conspiracies.
542880	547600	So we did some more background work, referring to Wikipedia, comparing with Google Trends,
547600	553920	and we came up with this list of five different topics, 9-11 conspiracies, vaccine controversies,
553920	558400	moon landing conspiracies, chemtrail, and then flat earth.
558400	563240	And then we audited three components of YouTube, the up next video, the top five recommendations,
563240	567520	and also YouTube's search results.
567520	572000	For demographics, we checked for four different age groups and two different types of gender,
572000	577840	male and female, and to emulate this, we had to create eight different sock puppet combination
577840	579080	accounts.
579080	584960	And then for geolocation, we found this hot and cold regions, that is, we call these
584960	588480	hot and cold regions because these are the regions which have the highest or the lowest
588480	590720	interest for that particular topic.
590720	596680	And we found these hot and cold regions comparing with Google Trends' interest over time graph.
596680	598840	So this is how it looked like for all the topics.
598840	604480	So for example, if you pick the flatter theories, Montana was a hot region or a high interest
604480	609320	showing region, while New Jersey is a low interest or cold region.
609320	614480	So once we had all these parameters, the demographic geolocation and all these parameters, we essentially
614480	620520	created bot accounts or sock puppets and programmed these accounts to keep firing queries on YouTube.
620520	626240	For geolocation, these bots fired queries from IP addresses of these locations.
626240	630520	Now one very important thing that we had to do for running these audits is, throughout
630520	633840	these audit experiments, you would have to control for noise to ensure that the effect
633840	639320	that you're observing is actually from the algorithm is not because of the noise that
639320	642760	might have been introduced while running the experiment.
642760	649480	So we controlled for browser noise by selecting one single version of Firefox browser.
649480	654240	We made sure that the YouTube searches are happening simultaneously to control for temporal
654240	657320	effects and so on.
657320	665400	So all of that audit run resulted in about 56,000, more than 56,000 videos capturing about
665400	668080	3,000 unique videos.
668080	673400	And then was the hard part, right, the manual annotations for this data set.
673400	677480	And I can go into detail of why we went with manual annotations and how we annotated if
677480	682120	anyone is interested in the Q&A round, but essentially this resulted in kind of three
682120	685520	sets of annotations promoting neutral and debunking.
685520	689800	And there was a lot of thought process that went into these annotation scheme, why these
689800	694240	three class made sense for this purpose and so on.
694240	699240	And then we performed statistical comparison tests to essentially find out what's the result
699240	700800	of these audits.
700800	703000	So let's look at some of these.
703000	704000	So what did we find?
704160	709680	We found that for brand new accounts, demography and geolocation do not really have an effect
709680	714640	on the amount of misinformation or the type of conspiracy theories that these platforms
714640	719440	are or YouTube is returning.
719440	720440	This is encouraging.
720440	722160	This is what we want the platform to do, right?
722160	727640	So it tells us that unlike those reports which were blaming YouTube for returning conspiracy
727640	732480	theories when it's a brand new account, turns out the demography and geolocation do not
732480	734480	really have an effect.
734480	741200	But once accounts builds a history by watching both demography and geolocation starts exerting
741200	749120	an effect on the recommendation for certain combination of topics, stances and component.
749120	751000	You might be thinking, okay, this is expected, right?
751000	755720	This is what we, how we think the platform would behave.
755720	760080	But turns out there are a little bit more nuanced results when we dig deeper into these,
760080	761920	the actual audit outcomes.
761920	767680	So for example, for the 9-11 topic, if the sock puppets watched YouTube videos promoting
767680	773200	line of conspiracy, you would end up getting more of these promoting videos in the recommendations.
773200	777560	But if the topic is something different, so for surprisingly for vaccine topic, the effect
777560	779040	was completely opposite.
779040	783400	If you watch anti-vaccine videos, YouTube ended up recommending you debunking videos
783400	787040	in the up next and top five recommendations.
787040	792840	So this, at least from these observations, it tells us that YouTube in some way is handling
792840	795440	misinformation in a much more reactive way.
795440	799800	It's modifying its search and recommendation algorithm selectively based on what reactions
799800	802320	is getting from the media and technology critics.
802320	806880	So we know that there was a lot of pushback for vaccine-related misinformation, and it
806880	810800	appears that they have gone and fixed that, but they have not done that universally for
810800	814320	other problematic topics.
814320	818440	We also found that certain demographics were prone to conspiracy video recommendations.
818440	824400	So for example, among eight of those demographic cases in all but one case, men accounts, that
824400	829040	is, bought accounts who had gender set as male were recommended more misinformation
829040	830640	videos.
830640	834920	And perhaps more surprisingly, what we found that in four of these cases, men accounts
834920	840440	who actually ended up watching neutral videos got significantly higher misinformation video
840440	842080	recommendations.
842080	843720	Now this is really problematic.
843720	849000	It implies that the algorithm was actually recommending pro-conspiracy videos even when
849000	854560	the user, in this case, Stockpuppet, was watching neutral videos on the topic.
854560	856480	What could this mean for real users?
856480	861680	This means that recommending promoting videos to men who are already drawn to neutral information
861680	866840	for that topic, but have not yet developed pro-conspiracy beliefs, but now has a higher
866840	873640	chance of developing that because the platform is returning these promoting conspiratorial
873640	876400	videos.
876400	883560	So wrapping up this work, the key contribution of this study was that in some senses this
883560	888720	work developed a methodology to audit search engines for misinformation, and we were also
888720	893400	able to statistically prove that YouTube's behavior varies across different misinformation
893400	894960	topics.
894960	901920	And our study also identified certain populations that could be potentially targets of certain
901920	903920	types of misinformation.
903920	910600	So this tells us that audit itself could be a useful way for studying how algorithms might
910600	914280	have differential impacts on certain marginalized populations.
914280	915280	Yes?
915280	916280	Good question.
916280	919400	So I get what you're saying about it being reactive.
919400	924480	The evidence suggests that in this case it's like being a special case.
924480	928760	Do you have a proposal as to how it might not be reactive?
928760	929760	How could they be proactive?
929760	933360	Is there something you would propose that they do instead?
933360	934360	Yes.
934360	938200	So I think one of the things they could do, and I think they are doing it now in hindsight,
938200	943560	this is an older study, is they're sitting with teams of experts, health experts, and
943560	950440	also looking at these health-related queries and topics in advance to figure out doing these
950440	952360	red teaming exercises.
952360	953360	They call it red teaming.
953360	955320	I think in the research we call it audit.
955320	960960	So they are doing this beforehand to figure out whether the platform is returning problematic
960960	962280	content.
962280	971000	And so if they do more of that proactively, of course there is some hope in changing things,
971000	974320	and we should not be catching these reactively after the fact.
974320	977920	That assumes that the points of view are stable, right?
977920	985720	So we often have these scenarios where culture changes or something goes viral, like the
985720	990440	tide pods or whatever, where I don't think a red team would have come up with, oh yeah,
990440	994720	we're going to start eating bleach or whatever.
994720	997320	Is there an approach there that you would advocate, like if you were in charge of one
997320	998320	of these teams?
998320	999320	Yeah.
999320	1005440	I think one of the things that I think some researchers at Stanford, maybe it was one
1005440	1011080	of your students who did this work with crowd audits, right, like where the crowd itself
1011080	1016280	is reporting, because it's not really possible for the red team to find all possible scenarios
1016280	1022600	under which these sorts of problems happen.
1022600	1027200	And I think that's where if you have these multiple eyes from different domains and different
1027200	1032680	cultures to report those problems, and then the company actually responds to it.
1032680	1036200	The problem is if the company's not responding or the people who are building these algorithms,
1036200	1037760	if they're not responding to it.
1037760	1042960	And so hopefully there should be a mechanism to do that, kind of closing that loop all the
1042960	1048800	way from reporting to actually taking action.
1048800	1055760	So moving on to Amazon, I think one of the things that led us to looking at Amazon is
1055760	1064160	despite being this leading retailer platform, how less of a focused research focus has been
1064160	1066280	paid to this platform.
1066280	1071880	And I think what was alarming is there were several media reports at the time coming out
1071880	1076680	suggesting that Amazon's algorithm were putting health and vaccine misinformation at the top
1076680	1077760	of your reading list.
1077760	1082600	But there was very little research to fall back to either verify or even, you know, kind
1082600	1085960	of disprove these reports.
1085960	1090320	So if you search on Amazon, unlike YouTube, which is at least tried to control for vaccine
1090320	1095320	misinformation, searching on Amazon for a vaccine, even this morning when I searched,
1095320	1100880	I could actually find some of these books, you would end up getting several anti-vaccination
1100880	1104480	products mostly in the form of books.
1104480	1109520	And the recommendation algorithm for Amazon are even much more sophisticated than YouTube.
1109520	1113240	So you have your product page recommendation, which has all these many different layers,
1113240	1116720	customers who bought items, sponsored products related to these items.
1116720	1122160	You have your homepage recommendations, again, many different layers underneath, pre-purchase
1122160	1126880	page recommendations, which is shown to you after you add a product to the cart that is
1126880	1132240	after the user shows an intention to buy that product.
1132240	1137800	So again, this was the same question as before, how bad is this scenario?
1137800	1141880	So we essentially wanted to conduct the systematic audits on Amazon search and recommendation
1141880	1143280	algorithm.
1143280	1148760	And here we picked only one type of problematic content, vaccine misinformation.
1148760	1152840	And we conducted two sets of audits, unpersonalized one, and then the personalized audits.
1152840	1158520	The personalized audit school was to assess whether users account history built progressively
1158520	1162760	by a user performing certain actions, such as clicking on a product, adding the product
1162760	1165800	to the cart, showing their intention to buy.
1165800	1171640	Because any of those actions, how does that change what recommendation is being returned?
1171640	1176640	And here the user built these account history progressively by performing a particular action
1176640	1179000	for seven consecutive days.
1179000	1182560	So these were, again, when I say users, these were sock puppets, searching, searching plus
1182560	1185680	clicking, searching plus clicking plus adding the product to the cart.
1185680	1189160	So all these were different actions that were performed.
1189160	1193960	And then we also controlled for noise, very similar setup as before, just with the caveat
1193960	1199320	that this whole audit experiment setup was a big software engineering feat, considering
1199320	1206240	the amount of different combinations of recommendations possible on Amazon.
1206240	1207240	So what did we find?
1207240	1209280	So I'm going to highlight a couple of results here.
1209280	1211800	First a single case study result.
1211800	1216320	So let's say users start searching for a vaccine and they click on an anti-vaccine book.
1216320	1220600	So as of this morning, this book was actually there on their platform in the first page,
1220600	1222480	first search result page.
1222480	1227480	And so if the user clicks on this, that algorithm next serves the user three other anti-vaccine
1227480	1230080	books in the product recommendation page.
1230080	1236120	And then once the user adds a product or book to the cart that shows their intention to buy,
1236120	1241040	both the pre-purchase as well as a home page recommendation also rapidly changes with many
1241040	1243720	more anti-vaccine book recommendations.
1243720	1248080	So this just tells you that once a user starts engaging with one misinformation product on
1248080	1252240	this e-commerce platform, they will be presented with more of those similar stuff at every
1252240	1255960	point of their Amazon navigation route.
1255960	1257920	So this was not just a one-off case study.
1257920	1263560	We found that more than 10% of Amazon products during the time period of our study for search
1263560	1270080	terms like vaccine, autism, immunization resulted in misinformation book containing
1270080	1273320	active vaccination content.
1273320	1277480	And so our audit experiment, just to give you the scale of this experiment, this was
1277480	1284240	ran for a little bit over three weeks, resulted in 36,000 search results, 16,000 recommendations,
1284240	1290280	and then worked over several search filters like featured, sponsored, different recommendation
1290280	1295560	types, user actions, and so all of that resulted in that number of more than 10%.
1295560	1302400	So if you just zoom out and look at what these thousands of recommendations look like, this
1302400	1306360	is the entire recommendation graph for one type of recommendation, what are the items
1306360	1309880	customers buy after viewing this item.
1309880	1313920	So here, each node in the graph represents a product, an Amazon product, and an edge
1313920	1318800	from a node A to a node B indicates that B was recommended in the product page of A.
1318800	1322400	Node size here is proportional to the number of times the product was recommended, and
1322400	1328360	the color corresponds to whether if it's a red, if it denotes a product annotated as
1328360	1332600	misinformation, green, neutral, and then blue debunking.
1332600	1338840	And I think being, you know, all of you have CS degrees or almost about to get a CS degree,
1338840	1343080	so you would probably able to decipher what's going on in this graph.
1343080	1347680	There are these large red size nodes attached to other red nodes, which are almost completely
1347680	1348680	separated.
1348680	1350640	There are like two separate components, right?
1350640	1355200	So this just shows how strong of a filter bubble effect there is for this particular
1355200	1356200	recommendation.
1356200	1361040	People who are recommended misinformation products, they keep getting recommended those
1361040	1365520	products super hard for them to break out from that red zone to get to the blue or the
1365520	1366520	green zone.
1366520	1367520	Sorry, just a clarification.
1367520	1370520	I think you mentioned, how did you code something as misinformation?
1370520	1372520	Obviously, there's lots of shades of gray here.
1372520	1373520	Yes.
1373520	1381520	So there was an initial status, we went through an extensive annotation scheme of a set of,
1381520	1383960	and then we also built a classifier to do that.
1383960	1390040	But the entire, there are like a lot more details in the paper as to how we coded it,
1390040	1394840	but I think it took us almost a month to even come up with the whole annotation scheme,
1394840	1400520	and then five or six of our experts, including me, we kind of coded it.
1400520	1406640	But yes, so we looked at a few markers, like the name of the book, the text, read the Google
1406640	1412640	preview of the text of the book, some of the comments, and then also the reviews that are
1412640	1414640	present on the Amazon website.
1414640	1419480	So it's a lot more qualitative process, and then all the markers that we took, we also
1419680	1423200	put it into the classifier to get the final annotations.
1423200	1427560	So this would have been more like the overall position of the book, not like, was there
1427560	1429560	a fact somewhere in there that.
1429560	1434440	So yes, we cannot really go and look at, okay, there is this one single line in the text
1434440	1438800	in the book, which is misinformation, but yes, so you could say there's a little bit
1438800	1441960	of noise in there.
1441960	1447480	And so this was another one just to say that this was not happening for one recommendation.
1447480	1453160	Those who viewed this item also viewed very similar graph as before, very similar trend.
1453160	1458400	And so the key takeaway here was our goal was to bring the focus to e-commerce platforms
1458400	1463960	and show how their algorithms could be pushing anti-vaccine content to users.
1463960	1468840	And we empirically established how certain real-world user actions on the platform could
1468840	1471560	drive users to these problematic eco chambers.
1471560	1477360	I think one of the implications, at least from this work, is that recommendation algorithms
1477760	1480240	should not be blindly applied to all topics equally.
1480240	1486400	If it's a health topic, perhaps companies need to pay a little bit more attention and
1486400	1493880	to ensure that there is higher quality content coming out in their platform.
1493880	1500080	So this work of ours intentionally and unintentionally was kind of rightly timed during the COVID pandemic.
1500080	1503240	And so this was widely covered by several news channels.
1503240	1507120	And in fact, Congressman Adam Schiff and Elizabeth Warren actually cited this research
1507160	1511640	of ours in their letter to Amazon to control vaccine misinformation.
1512240	1516400	So we were really happy that, OK, so now Amazon is going to take a few steps to do this,
1516760	1518840	but turns out we were really wrong.
1519360	1521080	So this is how Amazon is doing today.
1521080	1525680	Still today, as of earlier this morning, you would still find several books containing
1525680	1527880	vaccine misinformation.
1527880	1533040	This also just tells how, even though you go into all these lengths doing these academic
1533040	1536000	research, is it actually informing policies?
1536000	1537760	Is it actually making any real world impact?
1537760	1540120	And we can go into a long discussion about that.
1540520	1547160	But in the interest of time, let me talk a little bit more on one other type of study
1547160	1550600	that we did, looking at another type of audit method.
1551360	1554560	So so far, the studies that I presented employed one type of audit method,
1554560	1555520	sock puppet audits.
1556040	1562280	While these audits provide great control over your experimental design, you can pinpoint
1562280	1566000	exactly which variable might be affecting the output of the algorithm.
1566200	1570760	But one criticism of these audits is that the bots behavior are usually built in a
1570760	1571760	very conservative way, right?
1571760	1576280	So the bot in this YouTube case was essentially going and watching all
1576280	1578600	pro-conspiracy videos or all debunking videos.
1578880	1582080	Real users do not really act exactly that in that way, right?
1582080	1585800	So these are at least very extreme bot behaviors or user behaviors.
1586680	1590400	So as an alternative, we conducted crowdsourced audits where we audited the algorithmic
1590400	1594880	outputs from real world users to study and identify problematic behavior in users'
1594880	1595880	naturalistic setting.
1596360	1600320	So we conducted this audit for a nine day duration on YouTube.
1600680	1605800	And our goal was to assess to the extent in which YouTube was regulating US-based
1605800	1608080	election misinformation on their platform.
1609480	1613800	And so soon after the presidential election in 2020, YouTube came under fire for
1614280	1618000	surfacing election-related misinformation in their search and recommendations.
1618520	1622160	And they quickly responded to those criticisms by introducing these content
1622160	1626880	moderation policies to remove videos that spread election-related falsehoods
1627240	1631080	and claim that misinformation videos would not be surfaced on their platform.
1631800	1637040	But then again, during the midterm 2020 elections, there were reports saying that
1637040	1640240	YouTube has still has misinformation blind spots, right?
1640240	1642920	So they have not been very effective.
1643560	1647840	So this study of ours was goal was to determine how effective YouTube was
1647840	1650720	in successfully implementing its content moderation policy.
1651840	1654840	And so we did this through this post hoc crowdsourced audit.
1655320	1656360	Why it's post hoc?
1656360	1660280	Because it's conducted after the fact the event has happened to elections of 2020
1660280	1663280	and we were conducting this study in 2022.
1663880	1667080	And it's crowdsourced audit since we investigated YouTube's algorithm collecting
1667080	1669040	data from real world users.
1670400	1675960	And I'm sure many of you who have run these sorts of recruitment studies, you
1675960	1678720	would realize how difficult and how hard it is to do that.
1679000	1685880	Essentially, we were asking users to lend their YouTube history so that we can do
1685880	1687640	this sort of audit run.
1688600	1692960	And so our crowdsourced investigation, I think we started with like recruiting
1693200	1697880	600 to 500 users and we ended up slightly lower than 100 users.
1697880	1699960	So 99s, particularly.
1700240	1704720	So all these 99 users first filled out a pre-survey and about the beliefs on
1704720	1708440	personalization on YouTube, how they trust YouTube search and recommendation
1708440	1712760	algorithm, and then they installed this browser extension, which allowed us to
1713080	1714880	collect users personalized data.
1715280	1720720	We also had all these ethical considerations, which I can go on into
1720720	1722320	more detail if anyone is interested.
1723200	1726920	But what this extension was doing, it was collecting search results of search
1726920	1731560	queries related to the 2020 US presidential election, as well as
1731560	1734320	voter fraud claims surrounding the 2020 elections.
1734640	1737000	So two kinds of collection was happening.
1737000	1742160	One was with respect to search results in the standard and incognito window.
1742160	1746240	And by comparing these search results in both these windows, our goal was to
1746240	1749600	tell the extent in which YouTube was personalizing search results.
1750320	1752920	And then we were also collecting recommendation results.
1752920	1756200	And the way we were doing this, we were collecting these up next recommendation
1756200	1762160	trails after a user has watched a list of pre-selected videos with different
1762160	1764040	stances on election misinformation.
1765240	1769520	And the extension would start by first watching a pre-selected seed video and
1769520	1773480	then collecting up next videos up to five different levels.
1775280	1778680	So when we asked our participants in this pre-study survey, how much do you
1778680	1780640	think YouTube personalizes your search results?
1780640	1784840	About 34% of them believe that YouTube personalizes their search results to a
1784840	1785760	really great extent.
1786240	1789960	But this, through our audit, we found that YouTube, actually, that their YouTube
1789960	1792520	stop search results have little to no personalization.
1792840	1797520	So this also tells you how users believe in algorithms, like the way they behave
1797840	1802080	is different from actually the way the platform might be behaving.
1803720	1806880	But when we asked how much YouTube personalizes their up next recommendation,
1806880	1811040	that perception actually aligned with how actually the audit results showed.
1811040	1814720	Like 51% of participants believe that YouTube personalizes up next
1814720	1818760	recommendation to a great extent, which is in line with what our audit results
1818760	1819080	found.
1821000	1824840	We also calculate the amount of misinformation present in search results.
1825000	1828160	And we quantified this with this misinformation bias score.
1828400	1831560	And this is the only equation you're going to see throughout this talk.
1832480	1836120	So this misinformation bias score, we're East from minus one to one.
1836120	1840640	What the score does is it captures amount of misinformation, election related
1840640	1844120	misinformation while taking into account the ranking of the search results.
1844440	1848080	So a positive score indicates that search results contain videos that support
1848240	1853080	election misinformation while negative it contain videos that oppose election
1853080	1853920	misinformation.
1855200	1857640	Now, if you look at the entire distribution of scores for our collective
1857640	1862360	results, we found that the misinformation score, if you look at the X axis,
1862360	1866200	it's mostly negative, which indicates that YouTube presents more debunking or
1866200	1868120	opposing videos in the search results.
1868760	1871480	A couple of other key things also jumps off, right?
1871480	1875440	So you could see there are this distribution is by model.
1875960	1879480	So essentially there are two different clusters and each of these clusters
1879480	1882360	corresponds to two types of search queries.
1884040	1886640	I mean, we didn't cluster it ahead of time, right?
1886640	1889120	This, this emerged from our data.
1889320	1891800	So the first cluster corresponds to voter fraud.
1893120	1896480	Basically anything related to fraud in conjunction with keywords related to
1896480	1900880	election, while cluster two is more generic election related searches,
1900880	1903040	presidential election, mail-in ballots and so on.
1904040	1908240	What's interesting here is that the cluster one has these missions
1908240	1912680	information bias score, which are more negative, which indicates that if the
1912680	1916160	user goes and search for fraud related topics, they are actually going to be
1916160	1919800	given more opposing election related misinformation video, right?
1919800	1922960	So it's making YouTube is making it really difficult for users to search
1922960	1927280	for election fraud video, which in some sense tells that YouTube's pay more
1927280	1930440	attention to queries about election fraud and ensures that when users are
1930440	1935880	searching for them, they are in fact being exposed to opposing misinformation videos.
1937080	1942240	So key takeaway here is in some way YouTube is in fact successful in
1942240	1944480	enacting election misinformation policies.
1944480	1950240	So things that we wanted to test turns out it's actually, you know,
1950280	1954520	aligning with how they wanted to enforce these policies, but it is indeed
1954520	1957720	paying special attention to certain queries about voter fraud.
1958600	1962280	But there still exists certain misinformation in the up next trails.
1962280	1965520	We found that with some of those positive scores that you found.
1966400	1971160	And then finally, as a byproduct of this audit, we also found that there was
1971160	1976840	some mismatch in participants beliefs and the algorithmic reality that happens,
1976840	1981440	right, which indicates a lack, some lack of awareness of algorithmic, how
1981440	1982400	algorithms behave.
1982600	1985560	And I think there has been other researchers who have been working in
1985560	1990680	the space looking at algorithmic folk theories and how people's perception
1990680	1994200	differ from the way these platforms work.
1996960	2001000	Now, wrapping up, so these three, these are all the other, you know,
2001000	2004280	the core studies that I wanted to present, but obviously coming back to
2004280	2007280	how I started the talk, where do we go from here, right?
2007440	2011080	How do we do meaningful algorithmic governance in the first place?
2011080	2015360	That is, how do we set the path towards algorithmic governance in a meaningful
2015360	2017760	way and what are the challenges in doing that?
2019320	2024120	So here are a few ideas and obviously this is not, you know, there might be
2024120	2027520	more that could be added, but these are some of the possibilities for doing
2027520	2028480	algorithmic governance.
2028480	2029760	So I've listed three of these.
2030160	2034960	The first is algorithmic audits and so governance via audits and there could
2034960	2036480	be many layers to this, right?
2036480	2041000	So one of the layer is conducting external audits and I presented some of
2041000	2046320	these external audit studies through the three research work that we have
2046320	2050440	done in the past and also these audits could identify different types of risk.
2050440	2053840	So misinformation is one risk, but you could also do the same for bias,
2053840	2058840	discrimination, accountability, accessibility, accessibility, fairness
2058840	2059320	and so on.
2059320	2063600	And there are many researchers who have worked in the space.
2063840	2065840	I've listed some of these citations here.
2066840	2071840	But obviously a question is, so we as academic community, third-party researchers,
2071840	2075840	we are doing all these audits, is it really making any difference, right?
2075840	2079840	And as classic example is the failure of our Amazon study to make much of a
2079840	2080840	difference, right?
2080840	2083840	So we still really don't have a system in place where the algorithms or
2083840	2087840	companies running them are truly accountable to an independent third-party.
2087840	2091840	So this reminds me how US-based consumer reports operate, right?
2091840	2094840	So there are these independent third-party organizations that go into
2094840	2098840	great lengths for testing products that you use every day, your cars,
2098840	2100840	your washing machine and so on.
2100840	2104840	But so my argument is that why can't we do the same for algorithm?
2104840	2107840	In fact, I would argue that we need that more for algorithms because we are using
2107840	2111840	them much more frequently than say your washing machine.
2112840	2115840	The other shortcoming with external audit is that they are a form of reactive
2115840	2116840	governance.
2116840	2119840	This was the question that Michael was asking even earlier, like they operate
2119840	2121840	after the algorithm have been deployed.
2121840	2125840	So after the harm has been done, plus the external auditors do not really
2125840	2130840	have access to the models, to the training data, which are obviously
2130840	2132840	protected as trade secrets.
2132840	2137840	So as an alternative, another layer to governance via audits is you could do
2137840	2140840	internal audits as proactive governance.
2140840	2145840	And so at the time, researchers from Google who are no longer at Google right
2146840	2151840	now, but they released this paper making a case for internal audits where audit
2151840	2155840	would be part of a core part of product development at every step of the
2155840	2156840	way.
2157840	2159840	You could also do the best of both worlds, right?
2159840	2162840	You could do something called cooperative audits, which is a fairly newer
2162840	2167840	concept where while external audits answers what problems the platform has
2167840	2172840	and internal audit says why that's happening, you could have a combination
2172840	2176840	of both and you could do cooperative audits as shared governance, which allows
2176840	2180840	external algorithm auditors to audit the system of willing private companies.
2180840	2187840	So I've done a little bit of this with Spotify where working with their
2187840	2193840	engineers within the company figuring out how gender representations might be
2193840	2198840	biased or not for their taste on boarding and listen action on podcasts.
2198840	2202840	And then finally, you also need to do these audits multiple times, right?
2202840	2206840	Longitudinally, that is we need to conduct these continuous audits monitoring
2206840	2210840	platforms multiple times instead of that single snapshot audit.
2210840	2214840	Many of my studies that I presented today are all single snapshot and we do
2214840	2216840	need that kind of longitudinal effort.
2217840	2222840	So here is where I want to highlight one of the quotes from the Brajis earlier
2222840	2227840	internal audit paper where they mentioned the audit process is necessarily
2227840	2231840	boring, it is slow, it is methodological.
2231840	2237840	Which stands in stark contrast to what I had started my earlier slides with
2237840	2241840	move fast and break things, done is better than perfect, so very much in contrast
2241840	2243840	with the rushed culture of technology development.
2243840	2247840	And this is where I want to take a little bit of tangent and mention about audit
2247840	2251840	possibilities. One of the fastest growing developing AI technologies is the
2251840	2255840	large language models and what would auditing even look like for large
2255840	2259840	language models? What is the blueprint for LLM auditing?
2259840	2262840	And then also what are the key challenges, right?
2262840	2265840	So one of the key challenges is that it is difficult to assess the risks that
2265840	2270840	AI systems and large language models in particular pose independent of the
2270840	2272840	context in which they are deployed.
2272840	2277840	So we do need application specific audits for large language models.
2277840	2281840	The second challenge is that, and I don't know how to solve this, or rather
2281840	2284840	even the first one, is that the capabilities and the training processes
2284840	2288840	of these foundation and models have really outpaced the development of the tools
2288840	2292840	and techniques and the procedures for auditing, right?
2292840	2295840	So it is really hard to keep up the pace.
2295840	2299840	And so doing ethical, legal and technically robust audits makes it super
2299840	2303840	challenging for such a rapidly developing technology.
2303840	2307840	And so it must be complemented probably with much more newer forms of
2307840	2309840	supervision and control.
2309840	2313840	So here is one possible framework, one possible blueprint for auditing
2313840	2316840	large language models, which is kind of three layers.
2316840	2318840	So the first one is a model audit.
2318840	2322840	So as a name suggests, it focuses on assessing the technical properties of
2322840	2324840	the pre-attained language models.
2324840	2329840	So this is very similar flavor to the internal audit that I mentioned earlier.
2329840	2332840	So that's sort of proactive governance before you deploy the model.
2332840	2336840	But then there is application audit, which focuses on the assessing the
2336840	2339840	applications built on top of the LLMs.
2339840	2342840	So which is these flavors of post hoc audit, right?
2342840	2345840	And it should be done longitudinally, right?
2345840	2349840	Multiple times, over a long period of time, so as to capture any sort of new
2349840	2351840	properties that might be emerging.
2351840	2355840	And then finally, I think this is the new form of audit that we haven't talked
2355840	2357840	about a lot, at least the research community.
2357840	2361840	These are these governance audits that is assessing the processes whereby these
2361840	2365840	language models are designed and where they are disseminated.
2365840	2369840	So very much process oriented, right?
2369840	2372840	My next proposition is about value-centered audits.
2372840	2377840	That is, there is this active conversation around social values, emphasizing
2377840	2379840	while designing algorithms.
2379840	2382840	And I think we also need to turn that attention and thinking into how we can
2382840	2385840	value and respect humans involved in the audit process.
2385840	2389840	So these humans could be in the form of users who use the system or even
2389840	2393840	auditors who are investigating the sites.
2393840	2397840	And so for auditors, if we bring back the conversation for a second back to
2397840	2401840	misinformation, one instance where auditors did not really feel perceived
2401840	2406840	fair treatment was this scenario where fact-checkers are one of the key auditors
2406840	2409840	of misinformation on online platforms like Facebook, Twitter.
2409840	2413840	So the fact-checking organizations, SNOPs, a couple of years ago, actually backed
2413840	2417840	out of their partnership with Facebook because they didn't feel their values
2417840	2419840	were being respected.
2419840	2422840	I think to delve into this question of fair treatment of auditors, we need more
2422840	2423840	effort.
2423840	2428840	And so one way in which my group has started a few initiatives, we have
2428840	2432840	launched a research endeavor with the fact-checking organization based in
2432840	2434840	Kenya called Pesachek.
2434840	2437840	And this has also expanded to 16 other fact-checking organizations across
2437840	2439840	four different continents.
2439840	2442840	And we released our first report called the Human and Technological
2442840	2445840	Infrastructures of Fact-Checking.
2445840	2450840	And so one big motivation for this work was also this question of, are we
2450840	2453840	really taking into account diverse voices when we are talking about
2453840	2455840	governance and governing technologies?
2455840	2459840	And are we really doing culturally responsible AI?
2459840	2462840	Finally, how do we ensure actionable audits?
2462840	2465840	That is, audits that result in real change.
2465840	2470840	So one of the most successful examples of an actionable audit is Joy Voluwami's
2470840	2472840	Gender Shade Study.
2472840	2477840	So what she did was she audited facial recognition algorithms.
2477840	2481840	And within seven months of the release of these original audit, all the
2481840	2486840	three companies who had their facial recognition apps released new API
2486840	2491840	versions that reduced accuracy disparities with gender, male and
2491840	2495840	female, as well as race, darker and lighter-skinned subgroups.
2495840	2499840	So in other words, the Gender Shade Study is a classic example of commercial
2499840	2501840	actual impact.
2501840	2504840	And so they laid out their approach in this actionable auditing paper of
2504840	2505840	theirs.
2505840	2507840	Highly recommend you all to go and refer to it.
2507840	2511840	But turns out actionable auditing is often tremendously difficult to
2511840	2512840	achieve.
2512840	2515840	And here is where I want to revisit that earlier slide for our Amazon
2515840	2519840	study to highlight how much we had failed in doing the actionable
2519840	2520840	auditing.
2520840	2525840	So despite widespread media coverage, despite a letter from Congressman
2525840	2528840	Adam Schiff, Amazon did not really act much.
2528840	2533840	All they did was add that banner of COVID-19 information directing to
2533840	2535840	CDC's web page.
2535840	2539840	So hopefully this kind of summarizes the challenges as well as opportunities
2539840	2543840	and setting the path for algorithmic governance and hoping with the new
2543840	2548840	regulations coming in place, maybe if I were to give this talk next year, I
2548840	2553840	would have a little bit more hopeful slide than how I'm ending this talk.
2553840	2555840	So that's it.
2555840	2557840	So this is all I talked about today.
2557840	2560840	Most of this work was done with my PhDs.
2561840	2564840	Then PhD student Prena Juneja, who is now a faculty at Seattle
2564840	2565840	University.
2565840	2570840	And then my group, I would also stick in three other threads of work,
2570840	2573840	which I obviously don't have time to talk about.
2573840	2576840	But these are like a couple of other amazing students.
2576840	2580840	So Shruti Furkaya she did a bunch of work on computational social
2580840	2581840	science.
2581840	2585840	I was earlier meeting a student who was doing this sort of work.
2585840	2589840	So things like big data analysis of online interactions.
2589840	2593840	Studying trajectories of participation of users in extreme
2593840	2595840	communities, conspiratorial communities.
2595840	2599840	We have also done a little bit of design intervention and social
2599840	2604840	system design work with another student who is also a faculty now.
2604840	2608840	This is more of an XCI flavor where essentially questions like how do
2608840	2612840	you design a system to nudge users towards meaningful credibility
2612840	2613840	assessment?
2613840	2618840	How do you design a system to allow users to break out of their filter
2618840	2619840	bubble?
2619840	2622840	Something called other tube that we built on YouTube.
2622840	2627840	And then finally, the last and the least fleshed out thread is some
2627840	2631840	of the work that is currently ongoing with two of my students.
2631840	2634840	We are looking at challenges and opportunities of generative AI
2634840	2636840	in fact checking work.
2636840	2640840	And then what are some cultural misalignment that might happen with
2640840	2641840	language models?
2641840	2645840	Especially with roots in the global south, we are looking at
2645840	2649840	cultural implications of these language models in countries like
2649840	2653840	India and other countries in Southeast Asia.
2653840	2656840	So with that, I would like to end and happy to take questions.
2656840	2657840	Thank you all.
2657840	2667840	All right, we've got time for some questions.
2667840	2672840	I was wondering in your auditing of YouTube algorithms that you guys
2672840	2673840	looked at.
2673840	2676840	Yes, it was in 2020.
2676840	2680840	So I wasn't sure if YouTube shorts had been implemented since then
2680840	2683840	because YouTube shorts are somewhat of a newer aspect.
2683840	2690840	But I wonder if the algorithms that underlie the, I guess, traditional
2690840	2694840	YouTube recommendation system underlie the same sort of like YouTube
2694840	2699840	shorts recommendation because I guess the length of content and sort of
2699840	2702840	the amount of stimulus that would be needed to get the person to keep
2702840	2703840	the point would be different.
2703840	2707840	And therefore possibly seeing that if there are sort of similar
2707840	2714840	pattern between the two, whether or not the density, I guess, of
2714840	2717840	sort of misinformation sort of increases because the fact that
2717840	2718840	content is more short form.
2718840	2719840	Yeah.
2719840	2721840	So for the first study, we didn't, at that time, shorts were not
2721840	2722840	there.
2722840	2726840	But then for the third one that I presented with election misinformation,
2726840	2728840	we did capture YouTube shorts.
2728840	2731840	And that tells me that we should probably do another analysis
2731840	2735840	comparing the length of the videos and the, you know, whether it's a
2735840	2737840	short video versus a long form.
2737840	2739840	We did not do that, but that's an excellent point.
2739840	2740840	Yeah.
2743840	2745840	I have a question on maybe two studies.
2745840	2747840	Was it a good first one?
2747840	2750840	Did you look at all at like the probability that you would get
2750840	2754840	recommended legitimately false conspiracy videos, like on the
2754840	2759840	moon landing from videos about conspiracies that are a little bit
2759840	2762840	more true, like missing persons cases that the police just don't
2762840	2766840	investigate and like the likelihood that you'll get recommended
2766840	2768840	actually false content?
2768840	2773840	Yeah, we did not because I think one of the shortcomings of running
2773840	2775840	audits is the whole setup itself.
2775840	2777840	So we have to start somewhere, right?
2777840	2781840	So the RR starting point were a set of seed queries.
2781840	2782840	Right?
2782840	2787840	So with the way you are framing it, you know, we could, the
2787840	2790840	hypothesis could be dozen missing persons case lead you to more
2790840	2791840	conspiratory videos.
2791840	2794840	And in that scenario, I think we can use our audit framework to
2794840	2798840	have those as seed queries and see what happens.
2802840	2807840	On your kind of concluding point about actionable audits, I'm just
2807840	2810840	curious, do you think it's something to do with like the
2810840	2813840	conducting of the audit itself or just the context and how it
2813840	2816840	aligns with like the company's incentives?
2816840	2820840	Because it feels like the gender shades case, it was like a very
2820840	2823840	easily framed as like poor performance.
2823840	2825840	And so they were trying to cover themselves.
2825840	2828840	Whereas Amazon is somewhat incentivized to keep people buying
2828840	2831840	things even if those things are harmful.
2831840	2835840	So like, I guess I'm wondering, like, do you think audits need to
2835840	2838840	be conducted differently or there just needs to be more external
2838840	2842840	pressure like from the government or the public to incentivize the
2842840	2844840	companies when they are like internally?
2844840	2846840	Yeah, I don't think it's a ladder.
2846840	2852840	I think, I think Joy went on to great extent to after the study was
2852840	2856840	published to kind of give talks and publicize and do that kind of
2856840	2859840	outreach, which I did not do with this work.
2859840	2861840	I think that matters a lot, right?
2861840	2864840	She's the one who went to Congress for testimony and testified
2864840	2866840	against these companies.
2866840	2870840	And when you do that kind of impact, it would definitely translate
2870840	2874840	or there's a higher chance to be for your work to be translated to
2874840	2876840	actual actionable outcome.
2876840	2879840	I don't think those are actually steps listed in the actionable
2879840	2881840	auditing paper.
2881840	2884840	And in some sense, I feel like maybe the academic community need to
2884840	2887840	think about how to incentivize those additional work.
2887840	2889840	We don't have those incentives in place.
2889840	2892840	And partly I think we should look inward and blame ourselves that
2892840	2894840	we don't have those incentives in place.
2897840	2900840	Moving to this, I just wanted to hear your thoughts more.
2900840	2904840	When being either interactive or reactive, do you think we should
2904840	2908840	draw a distinction between conspiracy theories or misinformation
2908840	2913840	that has potential for great harm versus those that maybe don't,
2913840	2916840	right, to justify interventions that override individual autonomy
2916840	2918840	or control the information space?
2918840	2922840	Like, who is the moon landing conspiracy theory?
2922840	2924840	Can you say that last part?
2924840	2928840	Like, the moon landing conspiracy presumably isn't hurting anyone,
2928840	2929840	right?
2929840	2931840	Should we take it down?
2931840	2933840	Yeah, that's a really good point.
2933840	2937840	I think for companies like Google, I know they have this,
2937840	2941840	your money or your life, they have a view or an acronym,
2941840	2946840	YMYL or something, a set of search guidelines.
2946840	2951840	If those search results or the pages that show up,
2951840	2956840	if it's affecting monetarily, financially, health or your life,
2956840	2959840	then they're going to be more proactive and act on it.
2959840	2960840	So you're right.
2960840	2963840	Like, moon landing is probably not to that extent,
2963840	2968840	versus if it's vaccine information that has direct life consequences,
2968840	2969840	right?
2969840	2972840	But obviously then there are all these other questions that when
2972840	2975840	it's very well known that if you are drawn to one conspiracy,
2975840	2978840	you're likely to get other conspiracy theories, right?
2978840	2980840	So then what happens?
2980840	2983840	Should those be prioritized, at least to the extent that they
2983840	2986840	maybe should be prioritized in the recommendations,
2986840	2991840	if not completely removed from the platform?
2991840	2992840	Yeah?
2992840	2995840	This is sort of another sort of idea that I had.
2995840	2999840	Sort of looking at the idea of, like, what I think is sort of
2999840	3002840	interesting about, like, social media apps like YouTube is a whole
3002840	3005840	aspect of the media being able to communicate with others,
3005840	3007840	like the other comment section of a YouTube channel.
3007840	3010840	And I was wondering if there is a way to possibly, like,
3010840	3012840	I'm not sure how present this is, or if this is even, like,
3012840	3017840	a thing that is even, like, something that is able to be,
3017840	3018840	like, looked into.
3018840	3025840	But is there a possibility that the algorithm isn't,
3025840	3029840	may almost be promoting you content not necessarily by what
3029840	3034840	it's physically providing or, like, showing up then recommended,
3034840	3038840	but showing other users who would most likely put other links
3038840	3041840	to more, I guess, like, extreme videos in the comments being like,
3041840	3043840	oh, if you thought this was interesting, like, look at this.
3043840	3047840	And so that's not explicitly YouTube's algorithm showing you a video.
3047840	3051840	It's showing that same video to someone else who has the ability
3051840	3055840	to share a link to another YouTube video.
3055840	3058840	That would be almost, like, pushing someone down like a pipeline
3058840	3059840	of conspiracy theories.
3059840	3062840	So more like how the social recommendations,
3062840	3065840	like, you're sort of adding this collaborative social
3065840	3069840	recommendation component to YouTube and seeing how that pushes.
3069840	3073840	Like, if YouTube, like, you could have the same effect.
3073840	3077840	Like, in theory, maybe, the YouTube recommendation system could
3077840	3081840	not explicitly push someone by recommending, like,
3081840	3083840	more intense conspiracy theories.
3083840	3086840	But if YouTube is recommending someone who's already, like,
3086840	3089840	a very, like, entrenched conspiracy theorist and maybe someone
3089840	3093840	who's on the edge, if YouTube recommends them both the same,
3093840	3097840	like, it's starting out conspiracy theory video,
3097840	3099840	then you can have the person who's, like,
3099840	3101840	very entrenched conspiracy theorist commenting
3101840	3103840	and suggesting things themselves.
3103840	3107840	And it's not that YouTube is explicitly recommending the original person
3107840	3111840	or the person who isn't, like, entrenched conspiracy theory.
3111840	3115840	It's that they put them essentially on the same,
3116840	3119840	or they put them in the same environment in which they could communicate.
3119840	3120840	Yeah.
3120840	3124840	So, actually, my group of some of Shruti's work,
3124840	3128840	we have done this in the context of Reddit where what you're describing
3128840	3130840	those very entrenched conspiracy users,
3130840	3132840	we term this as veteran users.
3132840	3136840	And so they are one of the big drivers of bringing other people,
3136840	3140840	like what we call joiners, into the community of conspiracy group.
3140840	3144840	So I hope YouTube never does that for what you're suggesting,
3144840	3147840	but that's a classic marker of how these social dynamics
3147840	3151840	can actually bring people into these conspiratorial world views.
3151840	3154840	And, you know, empirically, we have seen that.
3154840	3156840	And there is also social science theory
3156840	3159840	proving that that definitely happens.
3159840	3161840	Yeah.
3161840	3167840	One challenge that I feel like our field faces with audits is,
3167840	3171840	I guess what I would describe as the sense I get of frustration
3171840	3175840	from folks at these companies who feel like the audits
3175840	3180840	aren't well executed or are way out of date as soon as they're published.
3180840	3184840	You made this point that we have to be very methodical
3184840	3187840	and often slow in doing this.
3187840	3191840	Then you throw in the peer review pipeline
3191840	3194840	that can slow things down further.
3194840	3197840	And by the time the thing comes out,
3197840	3201840	I remember seeing an applied researcher in a company
3201840	3204840	who in principle would be more open to this kind of stuff,
3204840	3207840	being like, our album doesn't even work like that anymore.
3207840	3212840	And so I'm wondering, so obviously you listed a bunch of possible
3212840	3216840	cooperative audits, longitudinal, internal, and so on.
3216840	3221840	I'm curious, are there, is there anything we can do to address that?
3221840	3225840	Let's assume that we can't change the incentives of the companies,
3225840	3227840	to change how quickly we do the audits.
3227840	3230840	Are we always going to be vulnerable to this,
3230840	3233840	like, oh yeah, that was yesterday's algorithm kind of critique?
3233840	3235840	Yeah, that's a really good point.
3235840	3239840	In fact, I was at a workshop with other folks like Christo Wilson
3239840	3243840	and a few others who have done audits for a very long time
3243840	3245840	with people from Facebook and YouTube.
3245840	3248840	And I think we came up with this exact same question.
3248840	3250840	And I think the common thing that emerged was that,
3250840	3254840	I don't think academics should be the, or academic institutions
3254840	3258840	should be the places to do these sorts of long-term audits.
3258840	3262840	It's fine to kind of develop the methods and, you know,
3262840	3265840	kind of say, okay, this one should pay attention, for example,
3265840	3268840	to Amazon, or, you know, this is the method to do it.
3268840	3271840	But then you need, like, separate third-party companies
3271840	3273840	to continuously do these audits, right?
3273840	3275840	So sort of like consumer reports,
3275840	3278840	what's the equivalent of that for audits?
3278840	3280840	And I think at CSCW, the closing keynote,
3280840	3282840	Room Room was mentioning that kind of red teaming,
3282840	3285840	I think their company or whichever NGO she's working with,
3285840	3287840	they are doing something like that.
3287840	3289840	I think, so academics with that peer review process,
3289840	3292840	I don't think we should be responsible for doing those sorts of
3292840	3295840	continuous audits because we are always going to play catch-up
3295840	3298840	with companies.
3298840	3300840	Yeah.
3300840	3301840	Oh.
3301840	3302840	One last? Yeah.
3302840	3303840	At the end.
3303840	3305840	Do you have thoughts on, like, Twitter has implemented, like,
3305840	3307840	community notes where people can just, like,
3307840	3310840	anyone can put it under a post like,
3310840	3312840	that's not true, or this is misleading,
3312840	3314840	or this person ever said that, like,
3314840	3317840	just, like, user-based, immediate type auditing,
3317840	3318840	if you will.
3318840	3320840	I do have thoughts on that versus, like,
3320840	3322840	companies long-term auditing, or, like,
3322840	3323840	if you think that's a good idea.
3323840	3325840	Yeah, that's a really good point.
3325840	3327840	We haven't looked at community notes,
3327840	3330840	but I know, you know, some researchers have,
3330840	3332840	kind of, looked and researched it.
3332840	3335840	I don't have really any very smart thoughts as to,
3335840	3338840	other than the usual advice that it's a good thing that
3338840	3341840	one should do it, but with the caveat that,
3341840	3347840	if the community doesn't reflect the right view,
3347840	3349840	or I wouldn't use the word right,
3349840	3354840	but, you know, a credible view of what happens then.
3354840	3359840	That's problematic.
3359840	3360840	I think that's time.
3360840	3361840	So let's thank your speaker.
3361840	3362840	Thank you.
3362840	3364840	Thank you, everyone.
3368840	3369840	Thank you.
