WEBVTT

00:00.000 --> 00:15.360
So Chris Potts is a professor and actually also the chair of the Department of Linguistics

00:15.360 --> 00:20.400
and by courtesy also at the Department of Computer Science and he's a great expert in

00:20.400 --> 00:24.400
the area of natural language understanding so he's you know there would not be a better

00:24.400 --> 00:29.160
person to hear about a topic than him and we are so grateful that he could make the

00:29.160 --> 00:34.760
time and he's actually also teaching the graduate course CS22 for you natural language

00:34.760 --> 00:39.880
understanding that we actually transform into a professional course that is starting next week

00:39.880 --> 00:44.920
on the same topic so you know if you're interested in learning more we have some links included

00:45.560 --> 00:50.760
you know down below on your platform you can check it out and you know there's so many other

00:50.760 --> 00:55.320
things that can be said about Chris like he has a super interesting podcast he's running like so

00:55.320 --> 01:01.160
many interesting research papers like projects he worked on so you know go ahead and learn more

01:01.160 --> 01:07.240
about him like you should also have a little link I think without further ado I think we can kick it

01:07.240 --> 01:13.800
off Chris thank you so much once again oh thank you so much Petra for the kind words and welcome

01:13.800 --> 01:20.200
to everyone it's wonderful to be here with you all I do think that we live in a golden age for

01:20.280 --> 01:26.360
natural language understanding maybe also a disconcerting age a weird age but certainly

01:26.360 --> 01:31.480
a time of a lot of innovation and a lot of change it's sort of an interesting moment for

01:31.480 --> 01:39.640
reflection for me because I started teaching my NLU course at Stanford in 2012 about a decade ago

01:40.280 --> 01:46.920
that feels very recent in my lived experience but it feels like a completely different age

01:46.920 --> 01:53.320
when it comes to NLU and indeed all of artificial intelligence I I never would have guessed in 2012

01:53.320 --> 01:59.720
that we would have such an amazing array of technologies and scientific innovations and

01:59.720 --> 02:06.840
that we would have these models that were just so performant and also so widely deployed in the

02:06.840 --> 02:14.360
world this is also a story of again for better or worse increasing societal impact and so that

02:14.360 --> 02:19.160
does come together for me into a golden age and just to reflect on this a little bit it's really

02:19.160 --> 02:25.160
just amazing to think about how many of these models you can get hands on with if you want to

02:25.160 --> 02:31.800
right away right you can download or use via apis models like dolly 2 that do incredible text to

02:31.800 --> 02:37.720
image generation stable diffusion mid-journey they're all in that class we also have github

02:37.720 --> 02:43.640
co-pilot based in the codex model for doing code generation tons of people derive a lot of value

02:43.640 --> 02:50.520
from that system u.com is at the leading edge I would say of search technologies that are changing

02:50.520 --> 02:55.000
the search experience and also leading us to new and better results when we search on the web

02:56.040 --> 03:03.000
whisper ai is an incredible model from open ai this does speech to text and this model is a

03:03.000 --> 03:10.280
generic model that is better than the best user customized models that we had 10 years ago just

03:10.280 --> 03:14.840
astounding not something I would have predicted I think and then of course the star of our show

03:14.840 --> 03:22.200
for today is going to be these big language models gpt 3 is the famous one you can use it via an api

03:22.200 --> 03:28.840
we have all these open source ones as well that have come out opt bloom gpt neo x these are models

03:28.840 --> 03:33.800
that you can download and work with to your heart's content provided that you have all the computing

03:33.800 --> 03:39.560
resources necessary so just incredible and I'm sure you're familiar with this but let's just you

03:39.560 --> 03:44.600
know get this into our common ground here it's incredible what these models can do here's a quick

03:44.600 --> 03:53.960
demo of gpt 3 I asked the da Vinci 2 engine in which year was stanford university founded

03:53.960 --> 03:59.000
when did it enroll its first students who is its current president and what is its mascot

03:59.000 --> 04:06.200
and da Vinci 2 gave a fluent and complete answer that is correct on all counts just incredible

04:06.920 --> 04:12.680
that was with da Vinci 2 we got a big update to that model in late 2022 that's da Vinci 3

04:12.680 --> 04:18.360
and here I'm showing you that it reproduces that results exactly and I do think that da Vinci 3 is

04:18.360 --> 04:24.360
a big step forward over the previous engine here's actually an example of that you know I have like

04:24.360 --> 04:30.120
to play adversarial games with this model and so I asked da Vinci 2 would it be possible to hire a

04:30.120 --> 04:35.320
team of tamarins to help me paint my house assuming I'm willing to pay them in sufficient quantities

04:35.320 --> 04:40.440
of fruit to meet minimum wage requirements in california this is adversarial because I know

04:40.440 --> 04:45.960
that these models don't have a really rich understanding of the world we live in they're

04:45.960 --> 04:51.480
often distracted by details like this and sure enough da Vinci 2 got confused yes it would be

04:51.480 --> 04:55.320
possible to hire a team of tamarins to paint your house you would need to make sure that you're

04:55.320 --> 05:01.560
providing them with enough fruit to meet minimum wage requirements and so forth so easily distracted

05:01.560 --> 05:07.480
but I tried this again with da Vinci 3 and with the same question it gave a very sensible answer no

05:07.480 --> 05:12.280
it would not be possible to hire a team of tamarins to help you paint your house da Vinci 2 was not

05:12.280 --> 05:18.280
distracted by my adversarial game this is not to say that you can't trick da Vinci 2 just go on to

05:18.280 --> 05:24.120
twitter and you'll find examples of that but again I do think we're seeing a pretty remarkable rate

05:24.120 --> 05:31.480
of progress toward these models being robust and relatively trustworthy this is also a story

05:31.560 --> 05:36.760
of scientific innovation that was a brief anecdote but we're seeing this same level of progress

05:36.760 --> 05:41.640
in the tools that we use to measure system performance in the field I've put this under

05:41.640 --> 05:47.000
the heading of benchmark saturate faster than ever this is from a paper from 2021 that I was

05:47.000 --> 05:52.680
involved with Kila et al here's the framework along the x-axis I have time going back to the

05:52.680 --> 05:59.480
1990s and along the y-axis I have a normalized measure of our estimate of human performance

05:59.480 --> 06:06.520
that's the red line set at zero so MNIST digit recognition a grand old data set in the field

06:06.520 --> 06:11.720
that was launched in the 1990s and it took about 20 years for us to surpass this estimate of human

06:11.720 --> 06:17.640
performance switchboard is a similar story launched in the 90s this is the speech to text problem it

06:17.640 --> 06:24.200
took about 20 years for us to get up past this red line here image net is newer this was launched

06:24.200 --> 06:30.680
in 2009 it took about 10 years for us to reach this saturation point and from here the pace is

06:30.680 --> 06:36.040
really going to pick up so squad 1.1 is question answering that was solved in about three years

06:36.840 --> 06:43.240
the response was squad 2.0 that was solved in less than two years and then the glue benchmark

06:43.240 --> 06:47.720
if you were in the field you might recall back the glue benchmark is this big set of tasks that

06:47.720 --> 06:53.800
was meant to stress test our best models when it was announced a lot of us worried that it was just

06:53.800 --> 06:59.480
too hard for present day models but glue was saturated in less than a year the response was

06:59.480 --> 07:05.560
super glue meant to be much harder it was also saturated in less than a year a remarkable

07:05.560 --> 07:10.440
story of progress undoubtedly even if you're cynical about this measure of human performance

07:10.440 --> 07:17.640
we are still seeing a rapid increase in the rate of change here and you know 2021 was ages ago in

07:17.640 --> 07:23.640
the story of AI now I think this same thing carries over into the current era with our largest

07:23.640 --> 07:29.480
language models this is from a really nice post from Jason Wei he is assessing emergent abilities

07:29.480 --> 07:35.160
in large language models you see eight of them given here along the x-axis for these plots you

07:35.160 --> 07:41.640
have model size and on the y-axis you have accuracy and what Jason is showing is that at a certain

07:41.640 --> 07:47.160
point these really big models just attain these abilities to do these really hard tasks

07:47.960 --> 07:54.520
Jason estimates that for 137 tasks models are showing this kind of emergent ability and that

07:54.520 --> 08:00.760
includes tasks that were explicitly set up to help us stress test our largest language model

08:00.760 --> 08:08.520
they're just falling away one by one really incredible now we're going to talk a little

08:08.520 --> 08:13.640
bit later about the factors that are driving this enormous progress for large language models but I

08:13.640 --> 08:18.920
want to be upfront that one of the major factors here is just the raw size of these models you

08:18.920 --> 08:23.960
can see that in Jason's plots that's where the emergent ability kicks in and let me put that

08:23.960 --> 08:29.080
in context for you so this is from a famous plot from a paper that's actually about making models

08:29.080 --> 08:35.560
smaller and what they did is track the rise of you know increases in model size along the x-axis

08:35.560 --> 08:42.040
we have time depth it only goes back to 2018 it's not very long ago and in 2018 the largest of our

08:42.040 --> 08:50.920
models had around 100 million parameters seems small by current comparisons in late 2019 early 2020

08:50.920 --> 08:56.440
we start to see a rapid increase in the size of these models so that by the end of 2020 we have

08:56.440 --> 09:03.800
this megatron model at 8.3 billion parameters I remember when that came out it seemed like it

09:03.800 --> 09:09.640
must be some kind of typo I could not fathom that we had a model that was that large but now of

09:09.640 --> 09:14.760
course this is kind of on the small side soon after that we got an 11 billion parameter variant of

09:14.760 --> 09:22.840
that model and then gpd3 came out that says 175 billion parameters and that one too now looks

09:22.840 --> 09:28.440
small in comparison to these truly gargantuan megatron models and the palm model from google

09:28.440 --> 09:36.040
which surpassed 500 billion parameters I want to emphasize that this has made a complete mockery

09:36.040 --> 09:42.600
of the y-axis of this plot to capture the scale correctly we would need 5 000 of these slides

09:42.600 --> 09:48.040
stacked on top of each other again it still feels weird to say that but that is the truth

09:48.040 --> 09:54.440
the scale of this is absolutely enormous and not something I think that I would have anticipated

09:54.440 --> 09:59.960
way back when we were dealing with those 100 million parameter babies by comparison they seem

09:59.960 --> 10:07.560
large to me at that point so this brings us to our central question it's a golden age this is all

10:07.560 --> 10:12.760
undoubtedly exciting and the things that I've just described to you are going to have an impact on

10:12.760 --> 10:19.320
your lives positive and negative but certainly an impact but I take it that we are here today

10:19.320 --> 10:25.000
because we are researchers and we would like to participate in this research and that could leave

10:25.000 --> 10:31.720
you with a kind of worried feeling how can you contribute to nlu in this era of these gargantuan

10:31.720 --> 10:38.280
models I've set this up as a kind of flow chart first question do you have 50 million dollars

10:38.280 --> 10:44.760
and a love of deep learning infrastructure if the answer is yes to this question then I would

10:44.760 --> 10:49.240
encourage you to go off and build your own large language model you could change the world in this

10:49.240 --> 10:54.600
way I would also request that you get in touch with me maybe you could join my research group and

10:54.680 --> 11:00.840
maybe fund my research group that would be wonderful but I'm assuming that most of you

11:00.840 --> 11:06.920
cannot truthfully answer yes to this question I'm in the no camp right and on both counts I am both

11:06.920 --> 11:12.760
dramatically short of the funds and I also don't have a love of deep learning infrastructure so for

11:12.760 --> 11:18.520
those of us who have to answer no to this question how can you contribute even if the answer is no

11:18.520 --> 11:23.960
there are tons of things that you can be doing all right so just topics that are front of mind

11:23.960 --> 11:29.160
to me include retrieval augmented in-context learning this could be small models that are

11:29.160 --> 11:34.920
performant you could always contribute to creating better benchmarks this is a perennial challenge

11:34.920 --> 11:40.360
for the field and maybe the most significant thing that you can do is just create devices that allow

11:40.360 --> 11:45.960
us to accurately measure the performance of our systems you could also help us solve what I've

11:45.960 --> 11:51.320
called the last mile problem for productive applications these central developments in AI

11:51.320 --> 11:58.040
take us 95 percent of the way toward utility but that last five percent actually having a

11:58.040 --> 12:04.680
positive impact on people's lives often requires twice as much development twice as much innovation

12:04.680 --> 12:11.080
across domain experts people who are good at human computer interaction and AI experts right and

12:11.080 --> 12:15.800
there's so there's just a huge amount that has to be done to realize the potential of these technologies

12:16.520 --> 12:22.600
and then finally you could think about achieving faithful human interpretable explanations of how

12:22.600 --> 12:28.120
these models behave if we're going to trust them we need to understand how they work at a human level

12:28.120 --> 12:32.600
that is supremely challenging and therefore this is incredibly important work you could be doing

12:33.800 --> 12:38.360
now I would love to talk with you about all four of those things and really elaborate on them but

12:38.360 --> 12:44.680
our time is short and so what I've done is select one topic retrieval augmented in-context learning

12:44.680 --> 12:50.120
to focus on because it's it's intimately connected to this notion of in-context learning

12:50.120 --> 12:55.960
and it's a place where all of us can participate in lots of innovative ways so that's kind of the

12:55.960 --> 13:02.920
central plan for the day before I do that though I just want to help us get more common ground around

13:02.920 --> 13:08.760
what I take to be the really central change that's happening as a result of these large language

13:08.760 --> 13:15.800
models and I've put that under the heading of the rise of in-context learning again this is

13:15.800 --> 13:21.000
something we're all getting used to it really remarks a genuine paradigm shift I would say

13:22.440 --> 13:28.440
in-context learning really traces to the GPT-3 paper there are precedents earlier in the literature

13:28.440 --> 13:34.360
but it was the GPT-3 paper that really gave it a thorough initial investigation and showed that it

13:34.440 --> 13:40.840
had promised with the earliest GPT models here's how this works we have our big language model

13:40.840 --> 13:47.000
and we prompt it with a bunch of text so for example this is from that GPT-3 paper we might

13:47.000 --> 13:53.240
prompt the model with a context passage and a title we might follow that with one or more

13:53.240 --> 13:58.360
demonstrations here the demonstration is a question and an answer and the goal of the

13:58.360 --> 14:03.080
demonstration is to help the model learn in context that is from the prompt we've given it

14:03.080 --> 14:08.200
what behavior we're trying to elicit from it so here you might say we're trying to coax the model

14:08.200 --> 14:14.440
to do extractive question answering to find the answer as a substring of the passage we gave it

14:14.440 --> 14:19.480
you might have a few of those and then finally we have the actual question we want the model to

14:19.480 --> 14:25.240
answer we prompt the model with this prompt here that puts it in some state and then its

14:25.240 --> 14:30.120
generation is taken to be the prediction or response and that's how we assess its success

14:30.920 --> 14:35.080
and the whole idea is that the model can learn in context that is from this prompt

14:35.080 --> 14:39.880
what we want it to do so that gives you a sense for how this works you've probably all prompted

14:39.880 --> 14:44.760
language models like you like this yourself already i want to dwell on this for a second though

14:44.760 --> 14:50.200
this is a really different thing from what we used to do throughout artificial intelligence

14:50.200 --> 14:55.800
let me contrast in context learning with the standard paradigm of standard supervision

14:56.760 --> 15:03.000
back in the old days of 2017 or whatever we would typically set things up like this we would have

15:03.000 --> 15:07.720
say we wanted to solve a problem like classifying texts according to whether they express nervous

15:07.720 --> 15:12.680
anticipation a complex human emotion the first step would be that we would need to create a data

15:12.680 --> 15:18.520
set of positive and negative examples of that phenomenon and then we would train a custom

15:18.520 --> 15:24.760
built model to make the binary distinction reflected in the labels here it can be surprisingly

15:24.840 --> 15:29.720
powerful but you can start to see already how this isn't going to scale to the complexity of

15:29.720 --> 15:35.480
the human experience we're going to need separate data sets and maybe separate models for optimism

15:35.480 --> 15:41.080
and sadness and every other emotion you can think of and that's just a subset of all the

15:41.080 --> 15:45.880
problems we might want our models to solve for each one we're going to need data and maybe a

15:45.880 --> 15:54.200
custom built model the promise of in-context learning is that a single big frozen language model

15:54.200 --> 15:59.080
can serve all those goals and in this mode we do that prompting thing that I just described

15:59.080 --> 16:04.600
we're going to give the model examples just expressed in flat text of positive and negative

16:04.600 --> 16:09.240
instances and hope that that's enough for it to learn in context about the distinction we're

16:09.240 --> 16:14.760
trying to establish this is really really different consider that over here the phrase nervous

16:14.760 --> 16:20.200
anticipation has no special status the model doesn't really process it it's entirely structured to

16:20.200 --> 16:26.600
make a binary distinction and the label nervous anticipation is kind of for us on the right the

16:26.600 --> 16:32.680
model needs to learn essentially the meanings of all of these terms and our intentions and figure

16:32.680 --> 16:38.920
out how to make these distinctions on new examples all from a prompt it's just weird and wild that

16:38.920 --> 16:44.040
this works at all I think I used to be discouraging about this as an avenue and now we're seeing it

16:44.040 --> 16:52.120
bear so much fruit what are the mechanisms behind this I'm going to identify a few of them for you

16:52.120 --> 16:57.720
the first one is certainly the transformer architecture this is the basic building block

16:57.720 --> 17:03.000
of essentially all the language models that I've mentioned so far we have great coverage of the

17:03.000 --> 17:07.240
transformer in our course natural language understanding so I'm going to do this quickly

17:07.240 --> 17:12.920
the transformer starts with word embeddings and positional encodings on top of those we have a

17:12.920 --> 17:18.600
bunch of attention mechanisms these give the name to the famous paper attention is all you need

17:18.600 --> 17:23.880
which announced the transformer evidently attention is not all you need because we have these

17:23.880 --> 17:27.880
positional encodings at the bottom and then we have a bunch of feed forward layers and

17:27.880 --> 17:35.000
regularization steps at the top but attention really is the beating heart of this model and it

17:35.000 --> 17:41.240
really was a dramatic departure from the fancy mechanisms LSTMs and so forth that were characteristic

17:41.240 --> 17:46.840
of the pre-transformer era so that's essentially though on the diagram here the full model in the

17:46.840 --> 17:52.520
course we have a bunch of materials that help you get hands on with transformer representations

17:52.520 --> 17:58.280
and also dive deep into math into the mathematics so I'm just going to skip past this I will say

17:58.280 --> 18:02.360
that if you dive deep you're likely to go through the same journey we all go through

18:03.000 --> 18:08.440
where your first question is how on earth does this work this diagram looks very complicated

18:08.440 --> 18:14.120
but then you come to terms with it and you realize oh this is actually a bunch of very

18:14.120 --> 18:20.120
simple mechanisms but then you arrive at a question that is a burning question for all of us why does

18:20.120 --> 18:25.720
this work so well this remains an open question a lot of people are working on explaining why this

18:25.720 --> 18:31.400
is so effective and that is certainly an area in which all of us could participate analytic work

18:31.400 --> 18:39.640
understanding why this is so successful the second big innovation here is a realization

18:39.640 --> 18:44.920
that what I've called self supervision is an incredibly powerful mechanism for acquiring

18:44.920 --> 18:51.800
rich representations of form and meaning this is also very strange in self supervision the model's

18:51.800 --> 18:57.400
only objective is to learn from co-occurrence patterns in the sequences it's trained on this is

18:57.400 --> 19:03.400
purely distributional learning another way to put this is the model is just learning to assign

19:03.400 --> 19:10.040
high probability to attested sequences that is the fundamental mechanism we think about these

19:10.040 --> 19:15.400
models as generators but generation is just sampling from the model that's a kind of secondary

19:15.400 --> 19:21.240
or derivative process the main thing is learning from these co-occurrence patterns an enlightening

19:21.240 --> 19:25.640
thing about the current era is that it's fruitful for these sequences content to contain lots of

19:25.640 --> 19:31.560
symbols not just language but computer code sensor readings even images and so forth those

19:31.560 --> 19:37.640
are all just symbol streams and the model learns associations among them the core thing about

19:37.640 --> 19:42.360
self supervision though that really contrasts it with the standard supervised paradigm I mentioned

19:42.360 --> 19:48.520
before is that the objective doesn't mention any specific specific symbols or relations between

19:48.520 --> 19:55.400
them is entirely about learning these co-occurrence patterns and from this simple mechanism we get such

19:55.480 --> 20:02.280
rich results and that is incredibly empowering because you need hardly any human effort to train

20:02.280 --> 20:08.200
a model with self supervision you just need vast quantities of these symbol streams and so that has

20:08.200 --> 20:15.080
facilitated the rise of another important mechanism here large-scale pre-training and there are actually

20:15.080 --> 20:20.440
two innovations that are happening here right so we see the rise of large-scale pre-training in the

20:20.440 --> 20:27.800
earliest work on static word representations like word to vex and glove and what those teams realize

20:27.800 --> 20:33.400
is not only that it's powerful to train on vast quantities of data using just self supervision

20:33.400 --> 20:40.040
but also that it's empowering to the community to release those parameters not just data not just

20:40.040 --> 20:45.480
code but the actual learned representations for other people to build on that has been incredible

20:45.480 --> 20:51.640
in terms of building effective systems after those we get ELMO which was the first model to do this

20:51.640 --> 20:58.120
for contextual word representations truly large language models then we get BERT of course and

20:58.120 --> 21:05.240
GPT and then finally of course GPT-3 at a scale that was really previously unimagined and maybe

21:05.240 --> 21:13.800
kind of unimaginable for me a final piece that we should not overlook is the role of human feedback

21:13.800 --> 21:20.360
in all of this and I'm thinking in particular of the open AI models I've given a lot of coverage

21:20.360 --> 21:26.120
so far of this mechanism of self supervision but we have to acknowledge that our best models

21:26.120 --> 21:31.480
are what open AI calls the instruct models and those are trained with way more than just self

21:31.480 --> 21:38.280
supervision this is a diagram from the chat GPT blog post it has a lot of details I'm confident

21:38.280 --> 21:43.880
that there are really two pieces that are important first the language model is fine tuned

21:44.440 --> 21:50.520
on human level supervision just making binary distinctions about good generations and bad ones

21:50.520 --> 21:56.600
that's already beyond self supervision and then in a second phase the model generates outputs and

21:56.600 --> 22:02.600
humans rank all of the outputs the model has produced and that feedback goes into a lightweight

22:02.600 --> 22:09.000
reinforcement learning mechanism in both of those phases we have important human contributions

22:09.000 --> 22:14.600
that take us beyond that self supervision step and kind of reduce the magical feeling of how

22:14.600 --> 22:20.680
these models are achieving so much I'm emphasizing this because I think what we're seeing is a return

22:20.680 --> 22:26.280
to a familiar and kind of cynical sounding story about AI which is that many of the transformative

22:26.280 --> 22:33.080
step forwards are actually on the back of a lot of human effort behind the scenes expressed at the

22:33.080 --> 22:39.560
level of training data but on the positive side here it is incredible that this human feedback

22:39.560 --> 22:45.240
is having such an important impact instruct models are best in class in the field and we have a lot

22:45.240 --> 22:51.240
of evidence that that must be because of these human feedback steps happening at a scale that I

22:51.320 --> 22:56.840
assume is astounding they must have at open AI large teams of people providing very fine

22:56.840 --> 23:01.800
green feedback across lots of different domains with lots of different tasks in mind

23:04.280 --> 23:10.600
final piece by way of background prompting itself this has been a real journey for all of us I've

23:10.600 --> 23:16.200
described this as step by step and chain of thought reasoning to give you a feel for how

23:16.200 --> 23:20.920
this is happening let's just imagine that we've posed a question like can our models reason about

23:21.000 --> 23:28.600
negation that is if we didn't eat any food does the model know that we didn't eat any pizza in the

23:28.600 --> 23:36.280
old days of 2021 we were so naive we would prompt models with just that direct question like is it

23:36.280 --> 23:40.440
true that if we didn't eat any food then we didn't eat any pizza and we would see what the model

23:40.440 --> 23:49.240
said in return now in 2023 we know so much and we have learned that it can really help to design

23:49.240 --> 23:53.960
a prompt that helps the model reason in the intended ways this is often called step by step

23:53.960 --> 23:58.840
reasoning here's an example of a prompt that was given to me by Omar Khattab you start by telling

23:58.840 --> 24:04.040
it it's a logic and common sense reasoning exam for some reason that's helpful then you give it

24:04.040 --> 24:10.040
some specific instructions and then you use some special markup to give it an example of the kind

24:10.040 --> 24:16.120
of reasoning that you would like it to follow after that example comes the actual prompt and in

24:16.120 --> 24:22.040
this context what we essentially ask the model to do is express its own reasoning and then conditional

24:22.040 --> 24:28.520
on what it has produced create an answer and the eye-opening thing about the current era is that

24:28.520 --> 24:33.000
this can be transformative better I think if you wanted to put this poetically you'd say that these

24:33.000 --> 24:38.040
large language models are kind of like alien creatures and it's taking us some time to figure

24:38.040 --> 24:43.160
out how to communicate with them and together with all that instruct fine tuning with human

24:43.160 --> 24:48.760
supervision we're converging on prompts like this as the powerful device and this is exciting to me

24:48.760 --> 24:55.080
because what's really emerging is that this is a kind of very light way of programming an AI system

24:55.080 --> 24:59.720
using only prompts as opposed to all the deep learning code that we used to have to write

24:59.720 --> 25:04.360
and that's going to be incredibly empowering in terms of system development and experimentation

25:06.680 --> 25:11.240
all right so we have our background in place I'd like to move to my main topic here

25:11.240 --> 25:15.800
which is retrieval augmented in-context learning what you're going to see here is a

25:15.800 --> 25:21.640
combination of language models with retriever models which are themselves under the hood

25:21.640 --> 25:27.160
large language models as well but let me start with a bit of the backstory here I think we're

25:27.160 --> 25:33.000
all probably vaguely aware at this point that large language models have been revolutionizing

25:33.000 --> 25:39.400
search again the star of this is the transformer or maybe more specifically its famous spokesmodel

25:39.400 --> 25:45.400
Burt right after Burt was announced around 2018 Google announced that it was incorporating

25:45.400 --> 25:51.240
aspects of Burt into its core search technology and Microsoft made a similar announcement at

25:51.240 --> 25:57.800
about the same time and I think those are just two public facing stories of you know many instances

25:57.800 --> 26:04.120
of large search technologies having Burt elements incorporated into them in that era and then of

26:04.120 --> 26:09.800
course in the current era we have startups like you.com which have made large language models

26:09.800 --> 26:15.560
pretty central to the entire search experience in the form of you know delivering results but also

26:15.560 --> 26:23.000
interactive search with conversational agents so that's all exciting but I am an NLP at heart

26:23.000 --> 26:27.720
and so for me in a way the more exciting direction here is the fact that finally

26:28.360 --> 26:35.240
search is revolutionizing NLP by helping us bridge the gap into much more relevant

26:35.240 --> 26:40.760
knowledge intensive tasks to give you a feel for how that's happening let's just use question

26:40.760 --> 26:48.120
answering as an example so prior to this work in NLP we would pose question answering or QA in the

26:48.120 --> 26:55.640
following way you saw this already with the GPT-3 example we would have as given at test time a title

26:55.640 --> 27:01.880
and a context passage and then a question and the task of the model is to find the answer to that

27:01.880 --> 27:07.800
question as a literal substring of the context passage which was guaranteed by the nature of

27:07.800 --> 27:14.760
the data set as you can imagine models are really good at this task superhuman certainly at this

27:14.760 --> 27:20.440
task but it's also a very rarefied task this is not a natural form of question answering in the

27:20.440 --> 27:26.440
world and it's certainly unlike the scenario of for example doing web search so the promise of the

27:26.440 --> 27:31.560
open formulations of this task are that we're going to connect more directly with the real world

27:31.560 --> 27:39.400
in this formulation at test time we're just given a question and the standard strategy is to rely on

27:39.400 --> 27:45.880
some kind of retrieval mechanism to find relevant evidence in a large corpus or maybe even the web

27:46.440 --> 27:51.800
and then we proceed as before this is a much harder problem because we're not going to get

27:51.800 --> 27:56.280
the substring guarantee anymore because we're dependent on the retriever to find relevant

27:56.280 --> 28:02.200
evidence but of course it's a much more important task because this is much more like our experience

28:02.200 --> 28:08.600
of searching on the web now I've kind of biased already in describing things this way where I

28:08.600 --> 28:14.600
assume we're retrieving a passage but there is another narrative out there let me skip to this

28:14.600 --> 28:18.920
then you could call this like the llms for everything approach and this would be where

28:18.920 --> 28:24.600
there's no explicit retriever you just have a question come in you have a big opaque model

28:24.600 --> 28:30.200
process that question and out comes an answer voila you hope that the user's information

28:30.200 --> 28:36.520
need is met directly no separate retrieval mechanism just the language model doing everything I think

28:36.520 --> 28:41.800
this is an incredibly inspiring vision but we should be aware that there are lots of kind of

28:41.800 --> 28:48.680
danger zones here so the first is just efficiency one of the major factors driving that explosion

28:48.680 --> 28:53.960
in model size that I tracked before is that in this llms for everything approach we are asking

28:53.960 --> 29:00.200
this model to play the role of both knowledge store and language capability if we could separate

29:00.200 --> 29:07.720
those out we might get away with smaller models we have a related problem of update ability suppose

29:07.720 --> 29:13.080
a fact in the world changes that document on the web changes for example well you're going to have

29:13.080 --> 29:18.600
to update the parameters of this big opaque model somehow to conform to the change in reality

29:19.400 --> 29:23.800
there are people hard at work on that problem that's a very exciting problem but I think we're a

29:23.800 --> 29:29.480
long way from being able to offer guarantees that a change in the world is reflected in the model

29:29.480 --> 29:35.800
behavior and that plays into all sorts of issues of trustworthiness and explainability of behavior

29:35.800 --> 29:43.080
and so forth also we have an issue of provenance look at the answer at the bottom there is that the

29:43.080 --> 29:48.680
correct answer should you trust this model right in the standard web search experience we typically

29:48.680 --> 29:53.880
are given some web pages that we can click on to verify at least at the next level of detail

29:53.880 --> 29:59.560
whether the information is correct but here we're just given this response and if the model also

29:59.560 --> 30:03.640
generated a provenance string if it told us where it found the information we'd be left with the

30:03.640 --> 30:09.320
concern that that provenance string was also untrustworthy right and this is like a really

30:09.320 --> 30:14.440
breaking a fundamental contract that users expect to have with search technologies I believe

30:15.320 --> 30:19.880
so those are some things to worry about there are positives though of course these models are

30:19.880 --> 30:25.960
incredibly effective at meeting your information need directly and they're also outstanding at

30:25.960 --> 30:31.080
synthesizing information if your question can only be answered by 10 different web pages

30:31.080 --> 30:35.160
it's very likely that the language model will still be able to do it without you having to hunt

30:35.160 --> 30:42.360
through all those pages so exciting but lots of concerns here here is the alternative of retrieval

30:42.360 --> 30:49.320
augmented approaches right oh I can't resist this actually just to give you an example of how

30:49.320 --> 30:56.920
important this trustworthy thing can be so I used to be impressed by DaVinci 3 because it would give

30:57.000 --> 31:02.120
a correct answer to the question are professional baseball players allowed to glue small wings

31:02.120 --> 31:06.920
onto their caps this is a question that I got from a wonderful article by Hector Levec where he

31:06.920 --> 31:12.760
encourages us to stress test our models by asking them questions that would seem to run up against

31:12.760 --> 31:18.280
any simple distributional or statistical learning model and really get at whether they have a model

31:18.280 --> 31:23.720
of the world and for DaVinci 2 it gave what it looked like a really good Levec style answer

31:23.720 --> 31:31.000
there is no rule against it but it is not common that seems true so I was disappointed I guess

31:31.000 --> 31:35.160
or I'm actually not sure how to feel about this when I asked DaVinci 3 the same question and it

31:35.160 --> 31:40.440
said no professional baseball players are not allowed to glue small wings onto their caps major

31:40.440 --> 31:44.440
league baseball has strict rules about the appearance of players uniforms and caps in any

31:44.440 --> 31:51.560
modification to the caps are not allowed that also sounds reasonable to me is it true it would help

31:51.640 --> 31:57.400
enormously if the model could offer me at least a web page with with evidence that's relevant to

31:57.400 --> 32:03.480
these claims otherwise I'm simply left wondering and I think that shows you that we've kind of broken

32:03.480 --> 32:08.920
this implicit contract with the user that we expect from search so that'll bring me to my

32:08.920 --> 32:14.280
alternative here retrieval based or retrieval augmented NLP to give you a sense for this at

32:14.280 --> 32:18.680
the top here I have a standard search box and I've put in a very complicated question indeed

32:19.400 --> 32:24.440
the first step in this approach is familiar from the LLMs for everything one we're going to encode

32:24.440 --> 32:29.800
that query into a dense numerical representation capturing aspects of its form and meaning we

32:29.800 --> 32:35.880
use a language model for that the next step is new though we are also going to use a language model

32:35.880 --> 32:41.240
maybe the same one we use for the query to process all of the documents in our document collection

32:41.880 --> 32:48.040
so each one has some kind of numerical deep learning representation now on the basis of

32:48.040 --> 32:53.080
these represent representations we can now score documents with respect to queries just like we

32:53.080 --> 32:59.640
would in the standard good old days of information retrieval so we can reproduce every aspect of

32:59.640 --> 33:05.160
that familiar experience if we want to we're just doing it now in this very rich semantic space

33:05.960 --> 33:10.200
so we get some results back and we could offer those to the user as ranked results but we can

33:10.200 --> 33:16.840
also go further we can have another language model call it a reader or a generator slurp up

33:16.840 --> 33:22.680
those retrieved passages and synthesize them into a single answer maybe meeting the user's

33:22.680 --> 33:27.960
information need directly right so let's check in on how we're doing with respect to our goals

33:27.960 --> 33:32.760
here first efficiency I won't have time to substantiate this today but these systems in

33:32.760 --> 33:38.040
terms of parameter counts can be much smaller than the integrated approach I mentioned before

33:39.240 --> 33:45.160
we also have an easy path to update ability we have this index here so as pages change in our

33:45.160 --> 33:51.160
document store we simply use our frozen language model to reprocess and re-represent them and we

33:51.160 --> 33:56.280
can have a pretty good guarantee at this point that information changes will be reflected in

33:56.280 --> 34:01.560
the retrieved results down here we're also naturally tracking provenance because we have

34:01.560 --> 34:06.120
all these documents and they're used to deliver the results and we can have that carry through

34:06.120 --> 34:12.360
into the generation so we've kept that contract with the user these models are incredibly effective

34:12.360 --> 34:18.040
across lots of literature we're seeing that retrieval augmented approaches are just superior

34:18.040 --> 34:24.200
to the fully integrated llms for everything one and we've retained the benefit of llms for everything

34:24.200 --> 34:29.800
because we have this model down here the reader generator that can synthesize information into

34:29.800 --> 34:37.880
answers that meet the information need directly so that's my fundamental pitch now again things are

34:37.880 --> 34:44.280
changing fast and even the approach to designing these systems is also changing really fast so in

34:44.280 --> 34:51.720
the in the previous era of 2020 we would have these pre-trained components like we have our index and

34:51.720 --> 34:57.000
our retriever maybe we have a language model like reader generator and you might have other

34:57.000 --> 35:02.840
pre-trained components image processing and so forth so you have all these assets and the question is

35:02.840 --> 35:08.600
how are you going to bring them together into an integrated solution the standard deep learning

35:08.600 --> 35:14.840
answer to that question is to define a bunch of task specific parameters that are meant to tie

35:14.840 --> 35:19.560
together all those components and then you learn those parameters with respect to some task

35:19.560 --> 35:25.720
and you hope that that has kind of created an effective integrated system that's the modular

35:25.720 --> 35:31.960
vision of deep learning the truth in practice is that even for very experienced researchers

35:31.960 --> 35:38.680
and system designers this can often go really wrong and debugging these systems and figuring out

35:38.680 --> 35:44.440
how to improve them can be very difficult because they are so opaque and the scale is so large

35:46.520 --> 35:52.680
but maybe we're moving out of an era in which we have to do this at all so this will bring us back

35:52.680 --> 35:59.240
to in-context learning the fundamental insight here is that many of these models can in principle

35:59.240 --> 36:07.480
communicate in natural language right so a retriever is abstractly just a device for pulling in text

36:07.480 --> 36:14.200
and producing text with scores and a language model is also a device for pulling in text and

36:14.200 --> 36:20.600
producing text with scores and we have already seen in my basic picture of retrieval augmented

36:20.600 --> 36:25.320
approaches that we could have the retriever communicate with the language model via retrieve

36:25.320 --> 36:31.000
results well what if we just allow that to go in both directions now we've got a system

36:31.000 --> 36:36.840
that is essentially constructed by prompts that help these models do message passing between them

36:36.840 --> 36:42.680
in potentially very complicated ways an entirely new approach to system design that I think is going

36:42.680 --> 36:48.200
to have an incredible democratizing effect on who designs these systems and what they're for

36:49.000 --> 36:55.160
let me give you a deep sense for just how wide open the design space is here again to give you

36:55.240 --> 37:01.800
a sense for how much of this research is still left to be done even in this golden era let's

37:01.800 --> 37:07.320
imagine a search context the question is what course to take what we're going to do in this new

37:07.320 --> 37:14.520
mode is begin a prompt that contains that question just as before and now what we can do next is

37:14.520 --> 37:19.720
retrieve a context passage that'll be like the retrieval augmented approach that I showed you

37:19.720 --> 37:24.760
at the start of this section right you could just use our retriever for that but there's more

37:24.760 --> 37:29.000
that could be done what about demonstrations let's imagine that we have a little train set

37:29.000 --> 37:34.440
of qa pairs that kind of demonstrate for our system what the intended behavior is well we can add

37:34.440 --> 37:39.320
those into the prompt and now we're giving the system a lot of few shot guidance about how to

37:39.320 --> 37:46.280
learn in context right but that's also just the beginning I might have sampled these training

37:46.280 --> 37:52.840
examples randomly for my train set but I have a retriever remember and so what I could do instead

37:52.840 --> 37:58.680
is find the demonstrations that are the most similar to the user's question and put those

37:58.680 --> 38:04.040
in my prompt with the expectation that that will help it understand kind of topical coherence and

38:04.040 --> 38:10.280
lead to better results but I could go further right I could use my retriever again to find

38:10.280 --> 38:15.800
relevant context passages for each one of those demonstrations to further help it figure out

38:15.800 --> 38:21.720
how to reason in terms of evidence and that also opens up a huge design space we could do what we

38:21.720 --> 38:26.600
call hindsight retrieval where for each one of these we're using both the question and the answer

38:26.600 --> 38:33.000
to find relevant context passages to really give you integrated informational packets that the model

38:33.000 --> 38:38.200
can benefit from and there's lots more that we could do with these demonstrations you're probably

38:38.200 --> 38:44.120
starting to see it right we could do some rewriting and so forth really makes sophisticated use of

38:44.120 --> 38:50.040
the retriever and the language model interwoven we could also think about how we selected this

38:50.040 --> 38:56.440
background passage I was assuming that we would just retrieve the most relevant passage according

38:56.440 --> 39:02.680
to our question but we could also think about rewriting the user's query in terms of the

39:02.680 --> 39:07.480
demonstrations that we could construct it to get a new query that will help the model that's

39:07.480 --> 39:12.680
especially powerful if you have a kind of interactional mode where the demonstrations are actually

39:12.680 --> 39:18.520
part of like a dialogue history or something like that and then finally we could turn our

39:18.520 --> 39:23.160
attention to how we're actually generating the answer I was assuming we would take the top

39:23.160 --> 39:27.960
generation from the language model but we could do much more we could filter its generations

39:27.960 --> 39:33.800
to just those that match a substring of the passage reproducing some of the old mode of

39:33.800 --> 39:38.920
question answering but now in this completely open formulation that can be incredibly powerful

39:38.920 --> 39:44.600
if you know your model can retrieve good background passages here those are two simple steps you could

39:44.600 --> 39:51.640
also go all the way to the other extreme and use the full retrieval augmented generation or rag model

39:51.640 --> 39:56.840
which is essentially creates a full probability model that allows us to marginalize out the

39:56.840 --> 40:03.400
contribution of passages that can be incredibly powerful in terms of making maximal use of the

40:03.400 --> 40:11.080
capacity of this model to generate text conditional on all the work that we did up here I hope that's

40:11.080 --> 40:16.360
giving you a sense for just how much can happen here what we're starting to see I think is that

40:16.360 --> 40:22.040
there is a new programming mode emerging it's a programming mode that involves using these large

40:22.040 --> 40:29.720
pre-trained components to design in code prompts that are essentially full AI systems that are

40:29.720 --> 40:35.720
entirely about message passing between these frozen components we have a new paper out that's called

40:35.800 --> 40:40.840
demonstrate search predictor dsp this is a lightweight programming framework for doing

40:40.840 --> 40:46.600
exactly what I was just describing for you and one thing I want to call out is that our results

40:46.600 --> 40:53.960
are fantastic now you know we can pat ourselves on the back we have a very talented team and so it's

40:53.960 --> 40:59.080
no surprise the results are so good but I actually want to be upfront with you I think the real insight

40:59.080 --> 41:05.320
here is that it is such early days in terms of us figuring out how to construct these prompts how to

41:05.320 --> 41:11.400
program these systems that we've only just begun to understand what's optimal we have explored only

41:11.400 --> 41:16.840
a tiny part of the space and everything we're doing is suboptimal and that's just the kind of conditions

41:16.840 --> 41:22.600
where you get these huge leap forwards leaps forward in performance on these tasks so I suspect

41:22.600 --> 41:28.280
that the bold row that we have here will not be long-lived given how much innovation is happening

41:28.280 --> 41:33.960
in this space and I want to make a pitch for our course here right so we have in this course

41:34.600 --> 41:39.560
a bunch of assignment slash bake-offs and the way that works essentially is that you have an

41:39.560 --> 41:45.640
assignment that helps you build some baselines and then work toward an original system which you

41:45.640 --> 41:51.640
enter into a bake-off which is a kind of informal competition around data and modeling our newest

41:51.640 --> 41:57.160
of these is called few shot open qa with cobear retrieval it's a version of the problems that I've

41:57.160 --> 42:02.360
just been describing for you this is a problem that could not even have been meaningfully posed

42:02.440 --> 42:09.000
five years ago and now we are seeing students doing incredible cutting-edge things in this mode

42:09.000 --> 42:14.280
it's exactly what I was just describing for you and we're in the sort of moment where a student

42:14.280 --> 42:19.160
project could lead to a paper that you know really leaves leads to state-of-the-art performance in

42:19.160 --> 42:24.120
surprising ways again because there is just so much research that has to be done here

42:24.520 --> 42:34.440
I'm running out of time what I think I'll do is just briefly call out again those important other

42:34.440 --> 42:39.800
areas that I've given short drift to today but I think are just so important starting with data

42:39.800 --> 42:47.000
sets I've been talking about system design and task performance but it is now and will always be the

42:47.000 --> 42:53.320
case that contributing you new benchmark data sets is basically the most important thing you can do

42:53.400 --> 42:59.080
Jacques Cousteau said water and air the two essential fluids on which all life depends I would

42:59.080 --> 43:08.280
extend that NLP our data sets are the resource on which all progress depends now Cousteau extended

43:08.280 --> 43:13.320
this with have become global garbage cans I am not that cynical about our data sets I think we've

43:13.320 --> 43:18.520
learned a lot about how to create effective data sets we're getting better at this but we need to

43:18.520 --> 43:24.280
watch out for this metaphorical pollution and we need always to be pushing our systems with

43:24.280 --> 43:30.360
harder tasks that come closer to the human capabilities that we're actually actually trying to get them

43:30.360 --> 43:36.120
to achieve and without contributions of data sets we could be tricking ourselves when we think we're

43:36.120 --> 43:43.320
making a lot of progress the second thing that I wanted to call out relates to model explainability

43:43.320 --> 43:49.000
you know we're in an era of incredible impact and that has rightly turned researchers to questions

43:49.000 --> 43:57.560
of system reliability safety trust approved use and pernicious social biases we have to get serious

43:57.560 --> 44:03.640
about all these issues if we're gonna responsibly have all of the impact that we're achieving at this

44:03.640 --> 44:09.720
point all of these things are incredibly difficult because the systems we're talking about are these

44:09.720 --> 44:15.400
enormous opaque impossible to understand analytically devices like this that are just

44:15.400 --> 44:21.000
clouding our understanding of them and so to me that shines a light on the importance of

44:21.000 --> 44:26.920
achieving analytic guarantees about our model behaviors that seems to me to be a prerequisite

44:26.920 --> 44:32.520
for getting serious about any one of these topics and the goal there in our terms is to achieve

44:33.240 --> 44:39.400
faithful human interpretable explanations of model behavior we have great coverage of these

44:39.400 --> 44:45.080
methods in the course hands-on materials screencasts and other things that will help you

44:45.640 --> 44:51.240
participate in this research and also as a side effect write absolutely outstanding

44:51.240 --> 44:58.360
discussion and analysis sections for your papers and the final thing I wanted to call out is just

44:58.440 --> 45:05.800
that last mile problem fundamental advances in AI take us 95 percent of the way there but that last

45:05.800 --> 45:12.280
five percent is every bit as difficult as the first 95 in my group we've been looking a lot at

45:12.280 --> 45:19.960
image accessibility this is an incredibly important societal problem because images are so central

45:19.960 --> 45:26.440
to modern life across being on the web and in social media also in the news and in our scientific

45:26.440 --> 45:32.520
discourse and it's a sad fact about the current state of the world that almost none of these images

45:32.520 --> 45:38.840
are made non-visually accessible so blind and low vision users are basically unable to understand

45:38.840 --> 45:43.640
all this context and receive all of this information something has to change that

45:44.760 --> 45:50.920
image-based text generation has become incredibly good over the last 10 years that's another story

45:50.920 --> 45:57.480
of astounding progress but it has yet to take us to the point where we can actually write useful

45:57.480 --> 46:03.320
descriptions of these images that would help a BLB user and that last bit is going to require

46:03.320 --> 46:11.000
HCI research linguistic research and fundamental advances in AI and by the way lots of astounding

46:11.000 --> 46:17.880
new data sets and this is just one example of in the innumerable number of applied problems

46:17.880 --> 46:23.560
that fall into this mode and that can be very exciting for people who have domain expertise

46:23.560 --> 46:32.600
that can help us close that final mile so let me wrap up here I don't want to have a standard

46:32.600 --> 46:38.600
conclusion I think it's fun to close with some predictions about the future and I have put this

46:38.600 --> 46:43.800
under the heading of predictions for the text next 10 years or so although I'm about to retract

46:43.800 --> 46:50.280
that for reasons I will get to but here are the predictions first laggard industries that are rich

46:50.280 --> 46:56.040
in text data will be transformed in part by NLP technology and that's likely to happen from

46:56.040 --> 47:02.360
some disruptive newcomers coming out of left field second prediction artificial assistants

47:02.360 --> 47:07.240
will get dramatically better and become more ubiquitous with the side effect that you'll

47:07.240 --> 47:13.640
often be unsure in life whether this customer service representative is a person or an AI

47:13.640 --> 47:21.480
or some team combining the two many kinds of writing including student papers at universities

47:21.480 --> 47:26.440
will be done with AI writing assistants and this might be transparently true given how

47:26.440 --> 47:32.360
sophisticated autocomplete and other tools have gotten at this point and then finally

47:32.360 --> 47:37.560
the negative effects of NLP and of AI will be amplified along with the positives I'm thinking

47:37.560 --> 47:44.600
of things like disinformation spread market disruption systemic bias it's almost sure to

47:44.600 --> 47:49.160
be the case if it hasn't already happened already that there will be some calamitous world event

47:49.720 --> 47:56.040
that traces to the intentional or unintentional misuse of some AI technology that's in our future

47:56.920 --> 48:00.920
so I think these are reasonable predictions and I'm curious for yours but I have to tell you

48:01.720 --> 48:09.720
that I made these predictions in 2020 two years ago with the expectation that they would be good

48:09.720 --> 48:16.520
for 10 years but more than half of them probably have already come true two and three are definitely

48:16.520 --> 48:21.640
true about the world we live in and on the flip side I just failed to predict so many important

48:21.640 --> 48:26.360
things like the most prominent example is that I just failed to predict the progress we would see

48:26.440 --> 48:33.400
in text image models like dolly two and and stable diffusion in fact I'll be honest with you I might

48:33.400 --> 48:38.360
have bet against them I thought that was an area that was going to languish for a long time and yet

48:38.360 --> 48:43.320
nonetheless seemingly out of nowhere we had this incredible set of advances and there are probably

48:43.320 --> 48:50.600
lots of other areas where I would make similarly bad predictions so I said 10 years but I think

48:50.680 --> 48:57.320
my new rule is going to be that I'm going to predict only through 2024 at the very outside

48:57.320 --> 49:03.960
because in 10 years the only thing I can say with confidence is that we will be in a radically

49:03.960 --> 49:09.480
different place from where we are now but what that place will be like is anyone's guess I'm

49:09.480 --> 49:14.200
interested in your predictions about it but I think I will stop here thank you very much

49:14.760 --> 49:21.720
thank you so much Chris for the engaging and extremely interesting topic and presentation

49:21.720 --> 49:27.160
you have given I'm always amazed by all the new things you're mentioning every single time we

49:27.160 --> 49:33.080
talk I feel it is something new something exciting you know not you not me especially not me like

49:33.080 --> 49:39.000
expected if you'll be talking about it so soon many questions came in I must already see people

49:39.000 --> 49:43.720
unfortunately not be able to get to all of them because the time is limited and the audience is

49:43.800 --> 49:51.080
so active and so many people showed up so let me pick a few um Chris so the cost of the training

49:51.080 --> 49:55.960
model so it seems it really scales with the size and we are paying a lot of attention and like

49:55.960 --> 50:01.320
putting a lot of effort into the training uh so what does it mean for the energy requirements

50:01.320 --> 50:05.320
and I guess we are talking about predictions but like how does it look like now and like

50:05.320 --> 50:12.360
what do you recommend people to to pay attention to oh it's a wonderful set of questions to be

50:12.360 --> 50:20.280
answering and critically important I mean I ask myself you know you know if you think about

50:20.280 --> 50:25.000
industries in the world some of them are improving in terms of their environmental impacts some are

50:25.000 --> 50:31.320
getting much worse where is artificial intelligence in that is it getting better or is it getting worse

50:31.320 --> 50:37.720
I don't know the answer because on the one hand the expenditure for training and now serving for

50:37.720 --> 50:44.040
example GPT-3 to everyone who wants to use it is absolutely enormous and it has real costs

50:44.840 --> 50:51.720
like measured in emissions and things like that on the other hand this is a centralization of all

50:51.720 --> 50:57.960
of that and that can often bring real benefits and I want to not forget of the previous era

50:57.960 --> 51:05.080
where every single person trained every single model from scratch and so now a lot of our research

51:05.160 --> 51:11.960
is actually just using these frozen components they were expensive but the expenditure of our lab

51:11.960 --> 51:19.240
is probably going way down because we are not training these big models it kind of reminds

51:19.240 --> 51:24.040
me of that last mile problem again in the previous era it was like we were all driving to pick up our

51:24.040 --> 51:30.040
groceries everywhere huge expenditure with all those individual trips now it's much more like

51:30.120 --> 51:33.320
they're all brought to the end of the street and we walk to get them

51:34.200 --> 51:38.440
but of course that's done in big trucks and those have real consequences as well

51:38.440 --> 51:44.920
I don't know but I hope that a lot of smart people work continue to work on this problem

51:44.920 --> 51:48.840
and that'll lead to benefits in terms of us doing all these things more efficiently as well

51:50.840 --> 51:57.000
thank you so much the next question and you touched on that a few times but it might be

51:57.000 --> 52:02.200
good to summarize that a little bit because we got a lot of the questions about kind of the

52:02.200 --> 52:09.080
trustworthiness and if the model actually knows that it's wrong or correct and like how do how

52:09.080 --> 52:13.480
do we trust the model or like how do we achieve the trustworthiness of the model because right now

52:13.480 --> 52:19.240
it's a lot of the generation happening generative models happening so like how do we pass that

52:19.800 --> 52:29.160
it's an incredibly good question and it is the thing I have in mind when we're doing all our work

52:29.160 --> 52:35.720
on explaining models because I feel like offering faithful human interpretable explanations is the

52:35.720 --> 52:42.200
step we can take toward trustworthiness it's a very difficult problem I just want to add that it

52:42.200 --> 52:47.640
might be even harder than we've anticipated because people are also pretty untrustworthy

52:48.600 --> 52:56.120
it's just that individual people often don't have like a systemic effect right so if you're

52:56.120 --> 53:01.480
really doing a poor job at something you probably impact just a handful of people

53:01.480 --> 53:07.640
and other people say at your company do a much better job but these ai's are now it's like they're

53:07.640 --> 53:15.000
everyone and so any kind of small problem that they have is amplified across the entire population

53:15.080 --> 53:19.960
they interact with and that's going to probably mean that our standards for trustworthiness for

53:19.960 --> 53:25.400
them need to be higher than they are for humans and that's another sense in which they're going

53:25.400 --> 53:31.480
to have to be superhuman to achieve the jobs we're asking of them and the field cannot offer

53:31.480 --> 53:39.960
guarantees right now so come help us fascinating thank you so much and like I saw also some

53:39.960 --> 53:44.600
questions or comments about the bias in data and like you mentioned it also right like you

53:44.600 --> 53:52.120
like we are improving like there is a big improvement happening um last question for you um like a

53:52.120 --> 53:56.680
little bit of a thought experiment but like do you think that the large language models might be

53:56.680 --> 54:02.520
able to come up with answers to as yet unanswered important scientific questions like something

54:02.520 --> 54:08.920
we are not even sure that it even exists like in our minds right now oh it's a wonderful question

54:08.920 --> 54:13.720
yeah and people are asking this across multiple domains like they're producing incredible artwork

54:13.720 --> 54:19.320
but are we now trapped inside a feedback loop that's going to lead to less truly innovative art

54:19.320 --> 54:25.240
and and if we ask them to generate text are they going to do either weird irrelevant stuff or just

54:25.240 --> 54:32.840
more of the boring average case stuff um I don't know the answer I will say though that these models

54:32.840 --> 54:38.840
have an incredible capacity to synthesize information across sources and I feel like

54:39.480 --> 54:45.880
that is a source of innovation for humans as well simply making those connections and it might be

54:45.880 --> 54:50.920
true that there is nothing new under the sun but there are lots of new connections perspectives

54:50.920 --> 54:56.520
and so forth to be had and I actually do have faith that models are going to be able to at least

54:56.520 --> 55:04.280
simulate some of that and it might look to us like innovation but this is not to say that this

55:04.280 --> 55:09.960
is uh not a concern for us it should be something we think about especially because we might be

55:09.960 --> 55:14.360
heading into an era when whether we want them to or not mostly these models are trained on their own

55:14.360 --> 55:19.720
output which is being put on the web and then consumed when people create train sets and so

55:19.720 --> 55:28.840
forth and so on yeah great thank you so much and we are nearing the end so like last point um

55:29.400 --> 55:35.160
do you have any like last remarks any anything anything interesting you would suggest others to

55:35.160 --> 55:42.680
look at follow read um learn about to kind of get more acquainted with the subject well learn

55:42.680 --> 55:47.560
more about the NLU GPT-3 other large language models and their recommendations

55:50.680 --> 55:55.720
the thing that comes to mind based on all the interactions I have with the professional

55:55.720 --> 56:01.720
development students who have taken our course before is that a lot of you I'm guessing have

56:01.720 --> 56:08.760
incredibly valuable valuable domain expertise you work in an industry in a position that has taught

56:08.760 --> 56:15.560
you tons of things and given you lots of skills and my last mile problem shows you that that is

56:15.560 --> 56:21.320
relevant to AI and therefore you could bring it to bear on AI and we might all benefit where you

56:21.320 --> 56:25.800
would be taking all these innovations you can learn about in our course and other courses

56:25.800 --> 56:32.600
combining that with your domain expertise and maybe actually making progress in a meaningful way on a

56:32.600 --> 56:39.720
problem as opposed to merely having demos and things that our scientific community often produces

56:39.720 --> 56:45.160
real impact so often requires real domain expertise of the sort you all have

56:47.000 --> 56:53.160
great thank you so much um and yeah at the end thank you so much Chris for taking the time to do

56:53.160 --> 56:59.560
this I know beginning of the quarter hectic Stanford live and I appreciate you taking the time to do

56:59.560 --> 57:05.320
this to run this webinar thank you also everybody who had a chance to join us live or like who's

57:05.320 --> 57:11.240
watching this recording if you could please let us know what kind of other topics you might be

57:11.240 --> 57:18.440
interested in in this sort of a free webinar structure we have a little survey down on the console

57:19.640 --> 57:26.280
and yeah I hope you all have a great day a wonderful start of the of or like end of the

57:26.280 --> 57:31.000
winter start of the spring and yeah thank you everybody for joining us yeah Petra this is

57:31.000 --> 57:35.400
wonderful we got an astounding number of really great questions it's too bad we're out of time

57:35.400 --> 57:39.480
there's a lot to think about here and so that's just another thank you to the audience for all

57:39.480 --> 57:52.680
this food for thought thank you

