WEBVTT

00:00.000 --> 00:12.800
Hi. I'm delighted to have with us here today my old friend Professor Fei Fei Li. Fei Fei

00:12.800 --> 00:18.640
is a professor of computer science at Stanford University and also co-director of HAI, the

00:18.640 --> 00:25.880
Human-Centered AI Institute. And previously, she also was responsible for AI at Google

00:25.880 --> 00:29.960
Cloud as a chief scientist for the division. It's great to have you here, Fei Fei.

00:29.960 --> 00:32.240
Thank you, Andrew. Very happy to be here.

00:32.240 --> 00:36.000
So, I guess, actually, how long have you known each other? I've lost track.

00:36.000 --> 00:41.680
Definitely more than a decade. I mean, I've known your work, right, before we even met.

00:41.680 --> 00:50.560
And I came to Stanford 2009, but we started talking 2007, so 15 years.

00:50.560 --> 00:56.640
And I can still have very clear memories of how stressful it was when, collectively,

00:56.640 --> 01:00.720
you know, a bunch of us, me, Chris Manning, a bunch of us, were trying to figure out how

01:00.720 --> 01:02.960
to recruit you to come to Stanford.

01:02.960 --> 01:10.120
It wasn't hard. I just needed to sort out my students and life, but it's hard to resist

01:10.120 --> 01:11.120
Stanford.

01:11.120 --> 01:14.440
It wasn't really great having you as a friend and colleague here.

01:14.440 --> 01:21.480
Yeah, me too. It's been a long time. And we're very lucky to be the generation seeing

01:21.480 --> 01:23.120
AI is great progress.

01:23.120 --> 01:28.720
Okay, so there was something about your background that always found inspiring, which is, you

01:28.720 --> 01:34.560
know, today people are entering AI from all walks of life. And sometimes people still wonder,

01:34.560 --> 01:40.720
oh, I majored in something or other. Is AI a right path for me? So I thought one of the

01:40.720 --> 01:44.120
most interesting parts of your background was that you actually started out not studying

01:44.120 --> 01:49.560
computer science or AI, but you started out studying physics and then had this path to

01:49.560 --> 01:54.880
becoming, you know, one of the most globally recognizable AI scientists. So how did you

01:54.880 --> 01:57.480
make that switch from physics to AI?

01:57.480 --> 02:02.120
Right. Well, that's a great question, Andrew, especially both of us are passionate about

02:02.120 --> 02:10.320
young people's future and their coming to the world of AI. The truth is, if I could

02:10.320 --> 02:16.720
enter AI back then more than 20 years ago, today, anybody can enter AI because AI has

02:16.720 --> 02:27.120
become such a prevalent and globally impactful technology. But myself, maybe I was an accident.

02:27.120 --> 02:35.760
So I have always been a physics kid or STEM kids. I'm sure you were too. But physics was

02:35.760 --> 02:41.240
my passion all the way through, you know, middle school, high school, college. I went

02:41.240 --> 02:48.120
to Princeton and majored in physics. And one thing physics has taught me till today is

02:48.120 --> 02:55.720
really the passion for asking big questions, the passion for seeking North stars. And I

02:55.720 --> 03:01.400
was really having fun as a physics student at Princeton. And one thing I did was reading

03:01.480 --> 03:09.760
up stories and just writings of great physicists of the 20th century and just hear about what

03:09.760 --> 03:16.760
they think about, you know, the world, especially people like Albert Einstein, Roger Penrose,

03:16.760 --> 03:25.720
you know, Erring Schrodinger. And it was really funny to notice that many of the writings

03:25.720 --> 03:32.360
towards the later half of the career of these great physicists were not about just the atomic

03:32.360 --> 03:41.600
world or the physical world, but pondering about equally audacious questions like life,

03:41.600 --> 03:46.440
like intelligence, like human conditions. You know, Schrodinger wrote this book, What

03:46.440 --> 03:56.400
Is Life? And Roger Penrose wrote this book, Emperor's New Mind, right? And that really

03:56.400 --> 04:03.200
got me very curious about the topic of intelligence. So one thing led to another during college

04:03.200 --> 04:11.800
time, I did the intern at a couple of neuroscience labs, and especially vision related. And I

04:11.800 --> 04:19.120
was like, wow, this is just as audacious a question to ask as the beginning of the universe,

04:19.120 --> 04:26.880
or what is matter made of. And that got me to switch from undergraduate degree in physics

04:26.880 --> 04:34.960
to graduate degree in AI, even though I don't know about you, during our time, AI was a

04:34.960 --> 04:41.520
dirty word. It was AI winter, so it was more machine learning and computer vision and computation

04:41.520 --> 04:46.080
on neuroscience. Yeah, I know. Honestly, I think when when I was in undergrad, I was

04:46.080 --> 04:51.840
too busy writing code, I just, you know, managed to blithely ignore the AI winter and just kept

04:51.840 --> 04:56.760
on coding. Yeah, well, I was too busy solving PDE equations.

04:56.760 --> 05:04.960
And so actually, do you do you have an audacious question now? Yes, my audacious question is

05:04.960 --> 05:12.760
still intelligence. I think since Alan Turing, humanity has not fully understand what is

05:12.760 --> 05:21.600
the fundamental computing principles behind intelligence. You know, we, we, we today we

05:21.600 --> 05:28.200
use the words AI, we use the word AGI. But at the end of the day, I still dream of a

05:28.280 --> 05:35.680
set of simple equations or simple principles that can define the process of intelligence,

05:35.680 --> 05:42.520
whether it's animal intelligence or machine intelligence. And this is similar to physics.

05:42.520 --> 05:49.520
For example, many people have joined the analogy of flying, right? Are we replicating birds

05:49.520 --> 05:55.000
flying, or are we building an airplane? And a lot of people ask the question of the relationship

05:55.040 --> 06:03.800
between AI and brain. And to me, whether we're building a bird or replicating a bird or building

06:03.800 --> 06:11.280
an airplane, at the end of the day, aerodynamics and physics that govern the process of flying.

06:11.280 --> 06:16.760
And I do believe one day we'll discover that. I sometimes think about this, you know, one

06:16.760 --> 06:21.480
learning algorithm hypothesis, could a lot of intelligence, maybe not all, but a lot of

06:21.560 --> 06:27.440
it be explained by one or a very simple machine learning principles. And it feels like we're

06:27.440 --> 06:32.840
still so far from cracking that nut. But in the weekends, when I have spare time, when

06:32.840 --> 06:35.960
I think about learning algorithms and where they could go, this is one of the things I

06:35.960 --> 06:38.560
still, you know, I'm excited about, right, just thinking about.

06:38.560 --> 06:46.160
I totally agree. I still feel like we are pre Newtonian. If we're doing physics analogy

06:46.160 --> 06:52.560
before Newton, there has been great physics, great physicists, a lot of phenomenology, a

06:52.560 --> 06:59.200
lot of studies of how the astrobodies move and all that. But it was Newton who start to

06:59.200 --> 07:05.920
write the very simple laws. And I think we are still going to that very exciting coming

07:05.920 --> 07:13.000
of age of AI as a basic science. And we're pre Newton, in my opinion.

07:13.320 --> 07:17.320
It's really nice to hear you talk about how despite machine learning and AI having come so

07:17.320 --> 07:23.000
far, it still feels like there are a lot more unanswered questions, a lot more work to be done

07:23.000 --> 07:27.560
by maybe some of the people joining the field today than work that's already been done.

07:27.560 --> 07:33.800
Absolutely. I mean, let's let's calculate. It's only what 60 years about. It's a very

07:33.800 --> 07:41.720
nascent field modern his physics and chemistry and biology are all hundreds of years. Right. So

07:41.720 --> 07:50.440
I think it is very, it is very exciting to be entering the field of science of intelligence

07:51.080 --> 07:58.120
and studying AI today. Yeah, I think I remember chatting with the late Professor John McCarthy

07:58.120 --> 08:03.800
who had coined the term artificial intelligence. And boy, the field has changed since when,

08:03.800 --> 08:10.600
you know, he conceived of it at the workshop and came up the term AI. But maybe another 10 years

08:10.600 --> 08:15.080
from now, you know, maybe someone watching this will come up with a new set of ideas.

08:15.080 --> 08:19.320
And then we'll be saying, boy, AI sure is different than what you know,

08:19.320 --> 08:22.600
you and I thought it would be. That's an exciting future to build towards.

08:22.600 --> 08:29.720
Yeah, I'm sure Newton would have not dreamed of Einstein. So, you know, our evolution of science

08:30.520 --> 08:37.720
sometimes takes strides, sometimes takes a while. And I think we're absolutely in an exciting phase

08:37.720 --> 08:45.080
of AI right now. You know, it's interesting hearing you paint this grand vision for AI.

08:46.120 --> 08:49.720
Going back a little bit, there was one other piece of your background that I found, you know,

08:51.080 --> 08:56.360
inspiring, which is when you're just getting started, I've heard you speak about how,

08:57.240 --> 09:01.880
you know, you're a physics student, but not only that, you also you're also running a laundromat

09:02.440 --> 09:07.400
to pay for school. And so just tell us more about that.

09:09.720 --> 09:17.160
So I came to this country, to New Jersey, actually, when I was 15. And one thing great

09:17.160 --> 09:23.560
about being in New Jersey is it was close to Princeton. So I often just take a weekend trip

09:23.560 --> 09:30.440
with my parents and to admire the place where Einstein spent most of his career in the latter

09:30.440 --> 09:38.120
half of his life. But, you know, with typical immigrant life, and it was tough. And by the time

09:38.120 --> 09:46.280
I entered Princeton, my parents didn't speak English. And one thing led to another, it turns out

09:46.280 --> 09:52.600
running a dry cleaner might be the best option for my family, especially for me to lead that

09:52.600 --> 09:58.040
business because it's a weekend business. If it's a weekday business, it would be hard for me to be

09:58.040 --> 10:05.480
a student. And it's actually, believe it or not, running a dry cleaning shop is very machine heavy,

10:05.480 --> 10:13.960
which is good for a STEM student like me. So we decided to open a dry cleaner shop in

10:14.920 --> 10:22.040
a small town in New Jersey called Persephone, New Jersey. It turned out we were physically not too

10:22.040 --> 10:29.560
far from Bell Labs and where lots of early convolutional neural network research was

10:29.560 --> 10:35.800
happening, but I had no idea. Actually, a summer intern at the AT&T Bell Labs way back.

10:35.800 --> 10:41.800
That's right, with Rob Shapiro. With Michael Curran was my mentor. And Rob Shapiro invented

10:41.800 --> 10:48.120
boosting great algorithms. So your coding AI, I was trying to cling to. No, that was only much

10:48.200 --> 10:57.240
later in my life. Did I start interrunning? Yeah. And then it was seven years. I did that for

10:57.880 --> 11:02.280
the entire undergrad and most of my grad school and I hired my parents.

11:04.280 --> 11:09.960
Yeah, no, that's really inspiring. I know you've been brilliant at doing exciting work all your

11:09.960 --> 11:16.120
life. And I think the story of running a laundromat to globally prominent computer scientists,

11:16.120 --> 11:21.960
I hope that inspires some people watching this that no matter where you are, there's plenty

11:21.960 --> 11:28.600
of room for young everyone. Don't even notice, my high school job was an office admin.

11:30.600 --> 11:35.720
And so to this day, I remember doing a lot of photocopying. And the exciting part was using

11:35.720 --> 11:40.600
this shredder. That was a glamorous one. But I was doing so much photocopying in high school,

11:40.600 --> 11:45.800
I thought, boy, if only I could build a robot to do this photocopying, maybe I could do something

11:46.360 --> 11:54.920
Did you succeed? I'm still working on it. We'll see. And then when people think about

11:54.920 --> 12:01.880
you and the work you've done, one of the huge successes everyone thinks about is ImageNet,

12:01.880 --> 12:06.840
where Hub established early benchmark for computer vision. It was really completely

12:06.840 --> 12:11.960
instrumental to the modern rise of deep learning in computer vision. One thing I

12:11.960 --> 12:16.120
bet not many people know about is how you actually got started on ImageNet.

12:16.120 --> 12:23.080
So tell us the origin story of ImageNet. Yeah, well, Andrew, that's a good question,

12:23.080 --> 12:29.640
because a lot of people see ImageNet as just labeling a ton of images. But where we began

12:29.640 --> 12:35.640
was really going after a North Star, brings back my physics background. So when I entered

12:35.640 --> 12:41.960
grad school, when did you enter grad school? Which year? 97. Okay, I was three years later

12:41.960 --> 12:50.280
than you, 2000. And that was a very exciting period, because I was in the computer vision

12:50.280 --> 12:56.200
and computational neuroscience lab of Pietro Peronna and Christoph Koch at Caltech. And

12:56.200 --> 13:03.480
leading up to that, there has been, first of all, two things was very exciting. One is that the world

13:04.120 --> 13:09.400
AI at that point wasn't called AI, computer vision or natural language processing,

13:09.400 --> 13:16.920
has found its lingua de franco, its machine learning, statistical modeling as a new tool

13:16.920 --> 13:22.760
has emerged, right? I mean, it's been around. And I remember when the idea of applying machine

13:22.760 --> 13:27.720
learning to computer vision, that was like a controversial thing. Right, and I was the first

13:27.800 --> 13:33.400
generation of graduate students who were embracing all the base net, all the inference

13:33.400 --> 13:41.560
algorithms and all that. And that was one exciting happening. A second exciting happening that

13:42.520 --> 13:48.680
most people don't know and don't appreciate, is that a couple of decades, probably more than

13:48.680 --> 13:55.320
two or three decades of incredible cognitive science and cognitive neuroscience work

13:55.320 --> 14:01.080
in the field of vision, in the world of vision, human vision, that has really established a couple

14:01.080 --> 14:07.000
of really critical north star problems, just understanding human visual processing and human

14:07.000 --> 14:13.320
intelligence. And one of them is the recognition of understanding of natural objects and natural

14:13.320 --> 14:19.880
things, because a lot of the psychology and cognitive science work is pointing to us,

14:20.520 --> 14:29.160
that is an innately optimized, whatever that word is, functionality and ability of human

14:29.160 --> 14:40.600
intelligence. It's more robust, faster and more nuanced than we had thought. We even find neural

14:40.600 --> 14:51.160
correlates, brain areas devoted to faces or places or body parts. So these two things led to my PhD

14:51.160 --> 15:01.000
study of using machine learning methods to work on real-world object recognition. But it became

15:01.000 --> 15:08.680
very painful, very quickly, that we are coming, banging against one of the most,

15:10.600 --> 15:16.840
continue to be the most important challenge in AI and machine learning is the lack of

15:16.840 --> 15:25.960
generalizability. You can design a beautiful model all you want if you're overfitting that model.

15:26.520 --> 15:31.400
I remember when it used to be possible to publish a computer vision paper showing your

15:31.400 --> 15:39.320
works on one image. Exactly. Yeah, it's just the overfitting, the models are not very expressive

15:39.880 --> 15:50.040
and we lack the data. And we also, as a field, was betting on making the variables very rich by

15:50.600 --> 15:56.760
hand-engineered features. Remember, every variable carrying a ton of semantic meaning,

15:56.760 --> 16:05.800
but with hand-engineered features. And then towards the end of my PhD, my advisor,

16:05.800 --> 16:12.600
Pietro and I start to look at each other and say, well, boy, we need more data. If we believe in

16:12.600 --> 16:19.160
this North Star problem of object recognition, and we look back at the tools we have,

16:20.120 --> 16:25.800
mathematically speaking, we're overfitting every model we're encountering. We need to take a fresh

16:25.800 --> 16:32.520
look at this. So one thing led to another. He and I decided we'll just do a, at that point,

16:32.520 --> 16:38.920
we think it was a large-scale data project called Caltech 101. I remember the data set. I wrote

16:38.920 --> 16:44.520
papers using your Caltech 101 data set way back. You did. You and your early graduate student.

16:44.520 --> 16:50.040
It helped benefit a lot of researchers. Yeah. Caltech 101 data set. That was me and my mom

16:50.040 --> 16:57.560
labeling images on a couple of undergrads. But that was, it was the early days of internet.

16:57.560 --> 17:04.280
So suddenly the availability of data was a new thing. You suddenly, I remember Pietro still

17:04.280 --> 17:10.760
have this super expensive digital camera. I think it was Canon or something like $6,000

17:11.560 --> 17:18.680
walking around Caltech taking pictures. But we are the internet generation. I go to Google

17:18.680 --> 17:23.800
Image Search. I start to see these thousands and tens of thousands of images. And I tell Pietro,

17:23.800 --> 17:29.480
let's just download. Of course, it's all that easy to download. So one thing led to another.

17:29.480 --> 17:37.080
We built this Caltech 101 data set of 101 object categories. And about, I would say,

17:37.080 --> 17:43.080
30 to 50, 30,000 pictures. I think it's really interesting that,

17:44.040 --> 17:49.800
you know, even though everyone's heard of ImageNet today, even you kind of took a couple

17:49.800 --> 17:54.440
of iterations where you did Caltech 101. And that was a success. Lots of people used it.

17:54.440 --> 17:59.640
But it's the, even the early learnings from building Caltech 101 that gave you the basis to

17:59.640 --> 18:07.000
build what turned out to be even, an even bigger success. Right. Except that by the time we start,

18:07.000 --> 18:12.920
I became an assistant professor. We started to look at the problem. I realized it's way

18:12.920 --> 18:18.760
bigger than we think. Just mathematically speaking, Caltech 101 was not sufficient to,

18:18.760 --> 18:25.800
to power the, the algorithms. We decided to image, to do ImageNet. And that was the time people

18:25.800 --> 18:32.600
start to think we're, we're doing too much. Right. It's, it's just too crazy. The idea of

18:32.600 --> 18:40.200
downloading the entire Internet of images and mapping out all the English nouns was a little bit,

18:41.560 --> 18:47.240
I start to get a lot of pushback. I remember at one of the CVPR conference when I presented the

18:47.240 --> 18:55.000
early idea of ImageNet, a couple of researchers publicly questioned and said, said, if you cannot

18:55.000 --> 19:02.120
recognize one category of object, let's say the chair you're sitting in, how do you imagine,

19:02.120 --> 19:10.360
or what's the use of a dataset of 22,000 classes of 15 million images? Yeah. But, but in the end,

19:10.360 --> 19:15.640
you know, that giant dataset unlocked a lot of value for, you know, countless number of

19:15.640 --> 19:20.280
researchers around the world. So that, so that works. Well, I, I think it was the combination

19:20.280 --> 19:28.760
of betting on the right North Star problem and the data that drives it. So it was a fun process.

19:28.760 --> 19:35.240
Yeah. And, and, you know, to me, one of the, when I think about that story, it seems like one of

19:35.240 --> 19:41.400
those examples where, you know, sometimes people feel like they should only work on projects without

19:41.400 --> 19:47.160
the huge thing at the first outset, but I feel like for people working in machine learning,

19:47.160 --> 19:52.280
if your first project is a bit smaller, it's totally fine. Have a good win, use the learnings

19:52.280 --> 19:57.080
to build up to even bigger things. And then sometimes you get a, you know, ImageNet size win,

19:57.080 --> 20:04.280
all of it. Yeah. Well, but in the meantime, I think it's also important to be driven by

20:05.080 --> 20:12.040
an audacious goal, though, you know, you can size your problem or size your project as local

20:12.040 --> 20:19.880
milestones and, and, and so on along this journey. But I also look at some of our current students,

20:19.880 --> 20:29.080
they're so peer pressured by this current climate of publishing nonstop. It becomes more incremental

20:29.080 --> 20:37.000
papers to just get into a publication for the sake of it. And I, I personally always push my

20:37.000 --> 20:41.480
students to ask the question, what is the North Star that's driving you? Yeah, that's true. Yeah.

20:42.200 --> 20:45.560
And you're right. You know, for myself, when I do research over the years, I've always

20:46.280 --> 20:52.200
pretty much done what I'm excited about, where I want to, you know, try to push the view forward.

20:52.200 --> 20:56.200
Doesn't it don't listen to people, have to listen to people, let them shape your opinion. But in

20:56.200 --> 21:01.080
the end, I think the best research is, let the world shape their opinion. But in the end,

21:01.080 --> 21:05.400
drive things forward using their own opinion. Totally agree. Yeah. It's your own inner fire,

21:05.400 --> 21:12.840
right? So as your research program developed, you've wound up taking your, let's say, foundations

21:12.840 --> 21:19.240
in computer vision and neuroscience and applying to all sorts of topics, including your very visibly

21:19.240 --> 21:24.680
healthcare, looking at neuroscience applications. Would love to hear a bit more about that.

21:24.680 --> 21:31.320
Yeah, happy to. I think the evolution of my research in computer vision also kind of follows

21:31.320 --> 21:38.360
the evolution of visual intelligence in animals. And there are two topics that truly excites me.

21:38.360 --> 21:46.200
One is, what is a truly impactful application area that would help human lives? And that's my

21:46.200 --> 21:54.280
healthcare work. The other one is, what is vision at the end of the day about? And that brings me to

21:55.640 --> 22:02.840
the, trying to close the loop between perception and robotic learning. So on the healthcare side,

22:04.600 --> 22:11.000
you know, one thing, Andrew, there was a number that shocked me about 10 years ago when I met

22:11.000 --> 22:18.120
my long-term collaborator, Dr. Arnie Milstein at Stanford Medical School. And that number is about

22:18.120 --> 22:28.120
a quarter of a million Americans die of medical errors every year. I had never imagined a number

22:28.120 --> 22:36.360
being that high due to medical errors. There are many, many reasons, but we can rest assured most

22:36.360 --> 22:44.760
of the reasons are not intentional. These are her errors of unintended mistakes and so on.

22:44.760 --> 22:50.520
For example? That's a mind-boggling number. I think it's made about 40,000 deaths a year from

22:50.520 --> 22:54.280
automotive accidents, which is completely tragic. And this is even vastly tragic.

22:54.280 --> 23:00.120
I was going to say that. I'm glad you brought it up. Just one example, one number within that

23:00.120 --> 23:07.080
mind-boggling number is the number of hospital-acquired infection resulted in fatality is

23:07.880 --> 23:19.160
more than 95,000. That's 2.5 times than the death of car accidents. And in this particular case,

23:19.160 --> 23:26.200
hospital-acquired infection is a result of many things, but in large, a lack of good

23:26.200 --> 23:33.560
hand hygiene practice. So if you look at WHO, there has been a lot of protocols about clinicians'

23:33.560 --> 23:42.120
hand hygiene practice. But in real healthcare delivery, when things get busy and when the

23:42.120 --> 23:47.640
process is tedious and when there is a lack of feedback system, you still make a lot of mistakes.

23:48.280 --> 23:56.680
Another tragic medical fact is that more than $70 billion every year are spent in

23:57.640 --> 24:03.880
in fall resulted injuries and fatalities. And most of this happened to elderlies at home,

24:03.880 --> 24:10.040
but also in the hospital rooms. And these are huge issues. And when Arnie and I got together

24:10.680 --> 24:19.720
back in 2012, it was the height of a self-driving car, let's say not hype, but what's the word,

24:19.960 --> 24:27.640
right word, excitement in Silicon Valley. And then we look at the technology of smart sensing

24:27.640 --> 24:34.280
cameras, LiDARs, radars, whatever, smart sensors, machine learning algorithm,

24:34.280 --> 24:41.960
and holistic understanding of a complex environment with high stakes for human lives.

24:42.920 --> 24:48.760
I was looking at all that for self-driving car and realized in healthcare delivery,

24:49.560 --> 24:56.680
we have the same situation. Much of the process, the human behavior process of healthcare is in

24:56.680 --> 25:05.320
the dark. And if we could have smart sensors, be it in patient rooms or senior homes, to help our

25:05.320 --> 25:11.640
clinicians and patients to stay safer, that would be amazing. So Arnie and I embarked on this,

25:11.720 --> 25:18.440
what we call ambient intelligence research agenda. But one thing I learned, which probably will lead

25:18.440 --> 25:28.040
to our other topics, is as soon as you're applying AI to real human conditions, there's a lot of human

25:28.040 --> 25:33.800
issues in addition to machine learning issues. For example, privacy. And I remember reading some

25:33.800 --> 25:38.200
of your papers with Arnie and found it really interesting how you could build and deploy

25:38.200 --> 25:45.560
systems that were relatively privacy preserving. Yeah, well, thank you. Well, the first iteration

25:45.560 --> 25:52.120
of that technology is we use cameras that do not capture RGB information. You've used a lot of that

25:52.120 --> 25:58.840
in self-driving cars, the depth cameras, for example. And there you preserve a lot of privacy

26:00.360 --> 26:05.960
information just by not seeing the faces and the identity of the people. But what's really

26:06.040 --> 26:13.080
interesting over the past decade is the changes of technology is actually giving us a bigger tool set

26:13.080 --> 26:21.320
for privacy preserved computing in this condition. For example, on device inference,

26:21.960 --> 26:27.080
you know, as the chip's getting more and more powerful, if you don't have to transmit any data

26:27.080 --> 26:32.760
through the network and to the central server, you help people better. Federated learning,

26:32.760 --> 26:40.520
we know it's still early stage, but that's another potential tool for privacy preserved computing

26:40.520 --> 26:47.320
and then differential privacy and also encryption technologies. So we're starting to see

26:47.960 --> 26:54.840
that human demand, you know, privacy and other issues is driving actually a new wave of machine

26:54.840 --> 27:00.440
learning technology in ambient intelligence in healthcare. Yeah, that's great. Yeah, I've been

27:00.440 --> 27:06.440
encouraged to see the, you know, real practical applications of differential privacy that are

27:06.440 --> 27:11.320
actually real. Federated learning, as you said, probably the PR is a little bit ahead of the

27:11.320 --> 27:16.120
reality, but I think we'll get there. But it's interesting how consumers in the last several

27:16.120 --> 27:21.160
years have fortunately gotten much more knowledgeable about privacy and are increasingly

27:21.640 --> 27:26.360
so important. I think the public is also making us to be better scientists.

27:27.080 --> 27:32.840
Yeah, yeah. And I think, and I think ultimately, you know, people understanding AI holds everyone,

27:32.840 --> 27:39.080
including us, but holds everyone accountable for really doing the right thing. Yeah, yeah. And,

27:39.080 --> 27:44.920
you know, and on that note, one of the really interesting pieces of work you've been doing has

27:44.920 --> 27:53.320
been leading several efforts to help educate legislators or help governments, especially U.S.

27:53.320 --> 27:58.920
government, work towards better laws and better regulation, especially as it relates to AI.

28:00.200 --> 28:04.520
This sounds like very important. And I suspect some days of the week,

28:04.520 --> 28:07.560
I would get somewhat frustrating work, but we'd love to hear more about that.

28:08.360 --> 28:15.560
Yeah, so I think first of all, I have to credit many, many people. So about four years ago,

28:15.560 --> 28:21.640
and I was actually finishing my sabbatical from Google time, I was very privileged to work with

28:21.640 --> 28:31.880
so many businesses, you know, enterprise developers, just just a large number and variety of vertical

28:31.880 --> 28:40.280
industries and realizing AI's human impact. And that was when many faculty leaders at Stanford

28:40.280 --> 28:46.040
and also just our president provost, former president and former provost all get together

28:46.200 --> 28:54.360
realize there is a role, historical role that Stanford needs to play in the advances of AI.

28:54.360 --> 29:02.600
We were part of the part of the birthplace of AI, you know, a lot of work our previous

29:03.240 --> 29:09.000
generation have done and a lot of work you've done and some of our work I've done led to AI,

29:09.000 --> 29:18.360
today's AI, what is our historical opportunity and responsibility? With that, we believe that the

29:18.360 --> 29:24.440
next generation of AI education and research and policy needs to be human centered. And

29:25.240 --> 29:33.240
having established the Human Center Institute, what we call HAI, one of the work that really took me

29:33.240 --> 29:43.480
outside of my comfort zone or any expertise, is really a deeper engagement with policy thinkers

29:43.480 --> 29:48.920
and makers. Because, you know, we're here in Silicon Valley and there is a culture in Silicon

29:48.920 --> 29:55.720
Valley is we just keep making things and the law will catch up by itself. But AI is impacting human

29:55.720 --> 30:06.440
lives and sometimes negatively so rapidly that it is not good for any of us if we the experts

30:07.080 --> 30:12.760
are not at the table with the policy thinkers and makers to really try to make this technology

30:12.760 --> 30:16.920
better for the people. I mean, we're talking about fairness, we're talking about privacy.

30:17.640 --> 30:27.080
We also are talking about the brain drain of AI to industry and the concentration of data and

30:27.080 --> 30:36.600
compute in a small number of technology companies. All these are really part of the changes of our

30:36.600 --> 30:43.240
time. Some are really exciting changes, some have profound impact that we cannot necessarily

30:43.880 --> 30:51.160
predict yet. So one of the policy work that Stanford HAI has very proudly engaged in is

30:51.160 --> 30:59.000
we were the one of the leading universities that lobbied a bill called the National AI Research

30:59.000 --> 31:05.720
Cloud Task Force Bill. It changed the name from Research Cloud to Research Resource. So now the

31:05.720 --> 31:12.680
bill's acronym is NAIR, National AI Research Resource. And this bill is calling for a task force

31:13.480 --> 31:20.200
to put together a roadmap for America's public sector, especially higher education and research

31:21.480 --> 31:30.280
sector to increase their access to resource for AI compute and AI data. It really is

31:30.360 --> 31:39.800
aimed to rejuvenate America's ecosystem in AI innovation and research. And I'm on the 12-person

31:39.800 --> 31:47.640
task force for under Biden administration for this bill. And we hope that's a piece of policy that

31:48.520 --> 31:56.120
is not a regulatory policy, it's more an incentive policy to build and rejuvenate ecosystems.

31:56.120 --> 32:01.320
I'm glad that you're doing this to help shape U.S. policy and this type of making sure enough

32:01.320 --> 32:07.160
resources allocated to ensure healthy development of AI feels like this is something that every

32:07.160 --> 32:15.000
country needs at this point. So just from the things that you are doing by yourself, not to

32:15.000 --> 32:20.520
speak of the things that the global AI community is doing, there's just so much going on in AI

32:20.520 --> 32:26.520
right now. So many opportunities, so much excitement. I found that for someone getting

32:26.520 --> 32:30.280
started in machine learning for the first time, sometimes there's so much going on,

32:30.280 --> 32:37.240
it can almost be a little bit overwhelming. What advice do you have for someone getting

32:37.240 --> 32:43.640
started in machine learning? Good question, Andrew. I'm sure you have great advice. You're one of the

32:43.640 --> 32:50.600
world-known advocates for AI machine learning education, so I do get this question a lot as

32:50.600 --> 32:56.920
well. And one thing you're totally right is AI really today feels different from our time.

32:57.960 --> 33:04.520
During our time... Just further, I thought, now is still our time. That's true, when we were

33:05.400 --> 33:13.160
starting in AI. I love that, exactly. We're still part of this. When we get started,

33:13.240 --> 33:20.120
entrance to AI and machine learning was relatively narrow. You almost have to start from computer

33:20.120 --> 33:27.560
science and go. As a physics major, I still had to wedge myself into the computer science track

33:27.560 --> 33:34.840
or electrical engineering track to get to AI. But today, I actually think that there is a

33:34.920 --> 33:41.880
remaining aspect of AI that creates entry points for people from all walks of life.

33:43.000 --> 33:51.560
On the technical side, I think it's obvious that there's just an incredible plethora of

33:51.560 --> 33:58.280
resources out there on the internet, from Coursera to YouTube to TikTok to GitHub.

33:59.240 --> 34:06.280
There's just so much that students worldwide can learn about AI and machine learning compared

34:06.280 --> 34:12.360
to the time we began learning machine learning. And also, any campuses, we're not talking about

34:12.360 --> 34:19.080
just college campuses. We're talking about high school campuses. Even sometimes earlier, we're

34:19.080 --> 34:29.640
starting to see more available classes and resources. I do encourage those of the young

34:29.640 --> 34:37.960
people with a technical interest and resource and opportunity to embrace these resources,

34:37.960 --> 34:45.640
because it's a lot of fun. But having said that, for those of you who are not coming from a technical

34:45.640 --> 34:51.960
angle, who still are passionate about AI, whether it's the downstream application or

34:51.960 --> 35:00.520
the creativity it engenders, or the policy and social angle or important social problems,

35:00.520 --> 35:09.480
whether it's digital economics or the governance or history, ethics, political sciences,

35:10.200 --> 35:18.280
there, I do invite you to join us, because there is a lot of work to be done. There's a lot of

35:20.040 --> 35:27.480
unknown questions. For example, my colleague at HAI are questioning, are trying to find answers on,

35:27.480 --> 35:36.040
how do you define our economy in the digital age? What does it mean when robots and software

35:36.200 --> 35:44.040
are participating in the workflow more and more? How do you measure our economy? That's not an AI

35:44.040 --> 35:50.520
coding question. That is an AI impact question. We're looking at the incredible advances of

35:50.520 --> 35:57.960
generative AI, and there will be more. What does that mean for creativity and to the creators,

35:57.960 --> 36:06.760
from music to art to writing? There is a lot of concerns, and I think it's rightfully so,

36:06.760 --> 36:14.040
but in the meantime, it takes people together to figure this out, and also to use this new tool.

36:16.280 --> 36:22.360
In short, I just think it's a very exciting time, and anybody with any walks of life,

36:23.000 --> 36:25.960
as long as you're passionate about this, there's a role to play.

36:26.680 --> 36:30.760
Yeah, and that's really exciting. We're going to talk about economics, think about my conversations

36:30.760 --> 36:37.720
with Professor Eric Brynoson, studying the impact of AI on the economy. From what you're saying,

36:37.720 --> 36:44.840
and I agree, it seems like no matter what your current interests are, AI is such a general purpose

36:44.840 --> 36:52.120
technology that the combination of your current interests and AI is often promising. I find that

36:52.120 --> 36:57.880
even for learners that may not yet have a specific interest, if you find your way into AI,

36:57.880 --> 37:03.720
start learning things, often the interests will evolve, and then you can start to craft your own

37:03.720 --> 37:10.200
path. And given where AI is today, there's still so much room and so much need for a lot more people

37:10.200 --> 37:16.040
to craft their own paths to do this exciting work that I think the world still needs a lot more of.

37:16.040 --> 37:20.760
Totally agree, yeah. So, one piece of work that you did that I thought was very cool was starting

37:20.760 --> 37:26.520
a program, initially called Sailors, and then later AI for all, which was really reaching out

37:26.520 --> 37:32.200
to high school and even younger students to try to give them more opportunities in AI,

37:32.200 --> 37:35.880
including people of all walks of life. We'd love to hear more about that.

37:35.880 --> 37:46.280
Yeah, well, this is in the spirit of this conversation. That was back in 2015. There was

37:46.360 --> 37:52.520
starting to be a lot of excitement of AI, but there was also starting to be this talk about

37:53.800 --> 38:00.120
killer robot coming next door, terminators coming. And I was, at that time, Andrew,

38:01.080 --> 38:07.800
I was the director of Steveria Lab, and I was thinking, you know, we know how far we are from

38:07.800 --> 38:13.400
terminators coming, and that seemed to be a really a little bit of far-fetched concern,

38:13.480 --> 38:19.320
but I was living my work life with a real concern I felt no one was talking about,

38:19.320 --> 38:25.080
which was the lack of representation in AI. At that time, I guess after Daphne has left,

38:25.080 --> 38:35.000
I was the only woman faculty at Steveria Lab, and we're having very small, around 15% of women

38:35.000 --> 38:42.040
graduate students, and we really don't see anybody from the underrepresented minority groups

38:42.040 --> 38:50.280
in Stanford AI program, and this is a national or even worldwide issue, so it wasn't just Stanford.

38:50.280 --> 38:55.800
Frankly, it still needs a lot of work today. Exactly. So how do we do this? Well, I got together

38:55.800 --> 39:05.000
with my former student Olga Rosakowski and also a long-term educator of STEM topics,

39:05.000 --> 39:12.520
Dr. Rick Sommer from a Stanford pre-collegiate study program, and thought about inviting

39:12.520 --> 39:18.920
high schoolers at that time, women, high school young women, to participate in a summer program

39:18.920 --> 39:28.360
to inspire them to learn AI, and that was how it started in 2015 and 2017. We got a lot of

39:28.360 --> 39:34.760
encouragement and support from people like Jensen and Laurie Huang and Melinda Gates,

39:34.760 --> 39:41.960
and we formed the National Nonprofit AI for All, which is really committed to training or

39:43.480 --> 39:51.240
helping tomorrow's leaders, shaping tomorrow's leaders for AI from students of all walks of

39:51.240 --> 39:56.840
life, especially the traditionally underserved and underrepresented communities, and

39:57.800 --> 40:05.480
you know, till today, we've had many, many summer camps and summer programs across the country. More

40:05.480 --> 40:16.120
than 15 universities are involved, and we have online curriculum to encourage students as well as

40:16.120 --> 40:23.720
college pathway programs to continue support these students' career by matching them with

40:23.720 --> 40:30.760
internships and mentors, so it's a it's a continued effort of encouraging students of all walks of

40:30.760 --> 40:35.880
life. And I remember back then, I think your group was printing these really cool t-shirts that

40:35.880 --> 40:42.040
asked the question, AI will change the world, who will change AI, and I thought the answer of

40:42.040 --> 40:47.640
making sure everyone can come in and participate, that was a great answer. Yeah, still an important

40:47.640 --> 40:55.080
question today. So that's a great thought, and I think that takes us to the end of the interview.

40:56.120 --> 41:03.000
Any final thoughts for people watching this? Still, that this is a very nascent field.

41:03.000 --> 41:08.280
As you said, Andrew, we are still in the middle of this. I still feel there's just so many questions

41:08.280 --> 41:14.920
that, you know, I wake up excited to work on with my students in the lab, and I think there's a lot

41:14.920 --> 41:21.080
more opportunities for the young people out there who want to learn and contribute and shape

41:21.080 --> 41:28.040
tomorrow's AI. Well said, that's very inspiring. Really great to chat with you, and thank you.

41:28.040 --> 41:41.160
Thank you, it's fun to have these conversations.

