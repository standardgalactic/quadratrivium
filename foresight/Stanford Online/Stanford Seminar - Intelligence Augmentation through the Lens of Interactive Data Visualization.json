{"text": " Just such a treat to be back, I spend many hours on that side of the room, so it's wild to be on this side of the room and going, whoa, there was actually like monitors up here, like that's how the speakers kept track of where in their talk they were, so that's good to know. This is sort of the first set of talks I've given since the pandemic, and so I thought it was a really great opportunity to talk about some new ideas that have been on my mind, and particularly with all of you as my captive audience, I thought that I would use this talk as an opportunity to think out loud about what the role of HCI should be in the face of all of this really incredible rapid progress that AI and ML have made, particularly kind of scoped in the last six months or so. As I was trying to think about what the role of HCI should be, I was reminded of this figure from Jonathan Gruden's 2009 article in triple AI about how AI and HCI are two fields that are divided by a common focus. As you can see in moments where AI makes a lot of progress, it's almost like the pendulum swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches of human intelligence augmentation or amplification, but also I think HCI is in a more established stronger position than it's ever been in the past, and so I really think it's our responsibility to think about what that counterbalance to ever-increased automation should be. So I often, in moments like this, like to sort of turn to history and ground myself, and so if we cast back to the first AI winter with Sutherland's sketchpad, right around that time there was this foundational paper written by Lick Leiter at MIT titled Man Computers in Biosas, and I think the gendering is unfortunate and unfortunately reflective of the times, but nevertheless in this paper, Lick Leiter put forth this really compelling vision about the ways in which a computer could interact with us through this intuitive, guided trial and error procedure, turning up solutions and revealing unexpected turns in the reasoning, and I was really tempted to put this sort of side-by-side with this very recent demo that OpenAI released with ChadGPT plus plugins where you can upload this music.csv data set and then start to have this very natural language interaction to ask what are the columns in the data set, how many rows in there in the data set, and then even say, can you give me some basic visualizations of this data set, and it thinks a little bit, it's working real hard, and there you go, it produces sort of three visualizations and even starts to give you maybe something that looks like an explanation, and I wonder, is it time to roll out our mission accomplished banners? Like have we achieved Lick Leiter's vision to think in interaction with a computer in the same way that we think with a colleague whose competence supplements our own? Now, I don't think it's time to roll out the mission accomplished banners, but I'm hopeful that the reason it's not that is not just my sort of hope that we haven't been put out of jobs, but rather that there is something more to do. So two years after Lick Leiter's man-computer symbiosis, Douglas Engelbart wrote up this really incredible framework called augmenting human intellect, and right in the introduction of this piece, we already start to see how Engelbart is defining a much more expansive role of human augmentation. So the idea is not just about problem solving, which he does mention right at the end there, to derive solutions to a problem, but it's also about using computers to help us think. It's to increase our capacity to approach a complex problem situation, to gain comprehension really about this thinking and not just the problem solving pieces, and really what I like is how he thinks we'll get there. Certainly there will be sophisticated methods, high powered electronic aids, but to me the part that really resonates in his prescription here is streamlined terminology and notation, and that's going to be a theme of my talk here, certainly one of the themes that underlies my group's work. And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the top right-hand corner, on this system called B2. So this is a Jupyter notebook, it's a very commonly used data science environment where people can start to write code in the style of a Python REPL, but what B2 does is saying, well, in addition to that sort of linear style of data science analysis and programming, there's a lot of value in a more visual analysis dashboard style interface like Tableau. And so what B2 tries to do is bring these two pieces together. So you can see once I've invoked B2 it adds this on the sidebar, and I can start to issue regular sort of Python, you know, pandas commands like looking at the data frame, getting a, you know, a sense of how many rows there are, what the columns are, and now I can start to write some code to do a little bit of data transformation and visualization. Notice here in all of these steps, you know, when I'm authoring a visualization, I don't have to specify what that visualization should look like, right? I'm just calling these .viz methods on the data frame, and B2 behind the scenes is figuring out what sort of visualization actually makes sense based on the history of the transformations that were performed on the data frame. So in the case of year, for instance, if I've grouped by year, the most sensible visualization to produce is a histogram of the number of counts of data records across years. You might have also noticed in the video that if I click the fields on the right hand side there, that it automatically produces an equivalent visualization, but it doesn't stop there. It adds, you know, the code and tags them with these little, you know, yellow emojis to indicate that there's actually sort of a common shared representation here, right? Clicking on the sidebar not only produces the visualization, but produces the equivalent code as well. And what's interesting is that these visualizations aren't just output mechanisms, but I can start to interact with them to sort of do this cross filtering interaction. So all the other bars update to reflect the data shown in the highlighted bars, and B2 is keeping this as an interaction log that is semantically meaningful to me. So this interaction log doesn't comprise mouse clicks and keystrokes and things like that, but it's expressing data queries, right? Which states have been selected? And I can use that data query to perform subsequent sort of analyses based on my interactive results. So I can say, great, I'm gonna, you know, copy some code to the clipboard, paste it in as a data query to look at what the interactive selection should be, and then, you know, proceed with some other sort of visual analysis. And so as we're sort of, you know, looking at these two forms of interaction, I was trying to figure out, well, some things feel the same, right? I've got that kind of conversational back and forth. I'm sure on the left-hand side with ChatGPT, it's a more natural language conversation. On the right-hand side, it's more of a repel conversation. But also, things feel qualitatively different. And how do I actually kind of characterize what is the same and what is the difference? And I thought really hard about it. And I realized that actually, maybe what still matters is direct manipulation, right? And by direct manipulation, I don't mean just the sort of Ben Schneiderman version of the term, which is, you know, associated with graphical user interfaces and having a representation on screen that you can manipulate and undo redo and things like that. But what I mean here is the deeper treatment of direct manipulation that three cognitive scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about the mid-1980s. So in particular, in Hutchins et al's treatment of direct manipulation, they sort of, you know, imagine direct manipulation to be this cognitive process between a user's goals and the user interface. And, you know, they identify this gulf of execution that exists when a user has to translate their goals into commands that they execute on the user interface. And similarly, a return gulf of evaluation when a user has to figure out, well, did the UI do the thing that I was expecting it to do? Right? I'm seeing a lot of nods in the audience because, you know, if you've had experience in user interaction design, user experience, you've maybe experienced these terms gulf of evaluation and execution. But what I find interesting in this 1985 paper is that they went one level deeper. So in particular, they identified this idea of a semantic distance, which is basically how users take the fuzzy notions in their head and translate those into the nouns and verbs of the user interface, right? So going, doing that sort of sense-meaning operation of transforming your intentions into the particular actions that might exist in the user interface. And in addition to the semantic distance, they identified what I love. I love this term in articulatory distance, right? So it's not necessarily the meaning that we care about anymore, but the way in which we're conveying that meaning through the UI. And this is particularly important because you might have several user interfaces that all express the same semantics, right? You can conduct the same set of, you know, operations with them, the same nouns and verbs. But the way you do that might be different because one interface might be graphical, the other one might be textual, another one might be conversational, gesture-oriented, etc. And their claim in this paper was that that articulation, the form of that meaning is really, really important, just as important as the semantics. And of course, these distances exist on the Gulf of Evaluation as well. So the articulatory distance is how do I perceive the changes that occurred in the UI and start to bring meaning to that perceptual operation by interpreting and evaluating the degree to which they met my goals. So this is actually going to give us the kind of conceptual machinery for the rest of the talk. And it's a little bit dense. And so I want to return to sort of the prior two examples and think about how they manifest these two kinds of distances. So in the case of the chat GPT example, you know, if we start with semantic distance, I would say that, well, the semantics aren't really well defined, right? They're not really explicit. Because what these models have done is they've learned over, you know, vast corpuses of text, often just text that is present on the internet. And so what they've learned is this latent space that is very ambiguous in the semantics that are encoded in that latent space. So as a user, it's hard for me to know how to translate my intentions into something that the system can understand because I don't know what it is the system knows about the world. But as I'm sure many of us are aware, like prompt engineering is a thing, right? So if I figure out exactly how to craft my, you know, natural language expression, suddenly I can get the model to very rapidly almost zero shot adopt the semantics that I want. And that feels like a very powerful, you know, affordance that we've not necessarily had before. On the other side, you know, the semantic distance in the Jupiter notebook in B2 had explicitly defined semantics, right? We have the explicit semantics of pandas on the data frame of the visualization library of being able to click on the fields in a graphical user interface to produce visualizations. And every time I did that, I had the shared representation of the code. So either I would offer the code and it would produce a visualization or if the system produced some code, I could go in and comment and uncomment entries or tweak the code in a particular way and things like that. And so it gave me the shared representation that allowed me to bridge between input and output mechanisms really, really easily. With articulatory distance in chat GPT, right, natural language, it's been enormously powerful because it's reduced the sort of learning threshold for a lot of things, right? So if I don't know exactly what it is I want or how to sort of pose it to the question, I can lean into the ambiguity of natural language and chat GPT catches up to my intentions pretty rapidly, which is great. But conversely, sometimes I know exactly what it is I want. And it's really frustrating to have to express precise operations through the ambiguity of natural language. And then as a result, because of the fact that natural language is the only mechanism so far by which we can interact with many of these models, there's a disconnect if your output is visual, like the case of visualization. So I can't interact with the visualizations in any way to do subsequent back and forth interactions with the model. Now, I don't think the second point is sort of a fundamental limitation, but it's certainly, you know, the state of where we are today. And on the other hand, with, you know, Jupiter Notebook and B2, with the articulatory distance, we've got basically the inverse of this, right? We've got a nice precise programmatic syntax. So if I know that syntax, I can work really, really efficiently, right? Sort of a common affordance of many sort of command line style interfaces. But I really need to learn that syntax to be effective. And in some cases with pearly design syntaxes, which I might maybe argue, Pandas is an example of, right? I constantly have to look up the documentation for, right? There's a learning curve associated with it that slows people down. Yeah, Michael. Yeah, so the reason I put it, I think this is a great question, you know, what lies in semantic and articulatory. And oftentimes it is quite a fuzzy distinction. The reason I put this in articulatory is my experience with Pandas oftentimes is I know what it is I want to do, right? I know the sort of operation I want to perform on my, on my data frame. I just don't know the specific syntax that I need to look up. Exactly, exactly. But certainly, you know, if, if you don't know what it is you want to do, then the affordances of natural language absolutely help because you can kind of, you know, pose things in really fuzzy ways and, and kind of iterate towards your outcome. And, and I think you see some of this ambiguity in, in sort of, you know, the distinction between semantic and articulatory distance here with this, this last point where because there are consistent semantics that actually has this knock on effect on the articulation because now there's a shared representation of input and output and that simplifies that articulatory distance as well. So there's not quite that disconnect that we see on the chat GPT side. And so, you know, that's a, that's, you know, I found semantic and articulatory distances to be a really helpful sort of framework and I wanted to use it to sort of analyze the very last step in the output that that demo produced. So it, it, you know, it's basically this, this thing that masquerades as an explanation of the visualizations that chat GPT produced. But if you actually look at what it says, right, here's some basic visualizations. Number one, a histogram of song durations colon. This shows the distribution of song durations in seconds. All right, fair enough. Scatter plot of song hotness versus artist familiarity. This shows the relationship between song hotness and artist familiarity. Well, I would hope so. And then bar chart of the top 10 most frequent artist names. This shows the top 10 most frequent artist names in the data set, right. These are not really explanations, but they're pretty provocative or evocative in the potential that these models might have in allowing us to produce these textual descriptions of visual artifacts. And certainly, you know, a lot of, of, of people, certainly lots of big tech companies have thought about the ways in which you could use all kinds of machine learning models, not just LLMs to do the sort of rich description of visual content and particularly for this sort of these accessibility use cases, like how do you describe these kinds of artifacts to people who are blind or have low vision. And lots of people have studied the degree to which these models are effective and found maybe unsurprisingly that they're not terribly effective right now, right. So here is a quote from a participant from one of our studies who says, you know, the reader wouldn't get much insight from texts like this, which not only, you know, is problematic because it doesn't effectively convey information, but more troublingly, it actually increases the burden that readers face when they're trying to make sense of this output, right. There's a lot of sort of noise that gets added to that experience. Another participant, you know, says very, very interestingly, the problem with these textual descriptions is also that it robs me of control of consuming the data, right. A participant, another participant said, I want to have the time and space to interpret the numbers for myself before I read any kind of textual description that does the analysis for me. And so to me, these sound very similar to issues associated with a semantic and an articulatory distance, right. That first quote talking about, well, these texts aren't conveying anything interesting. The second set talking about, well, I want to have that time and space, I want to be able to control the form with which that text is conveyed to me. And so I want to dig into how we might address these two distances in the case of accessibility. But before I do that, I want to give us a sense of how people who are blind or have low vision experience, you know, the internet and graphical interfaces today. So I'm going to turn things over to my PhD student, Jonathan Zhang, who will give us a quick demo of an assistive technology called a screen reader that basically narrates on-screen content. So here I can demonstrate what the accessible HTML version of our paper looks like to a screen reader. So as you can see, what a screen reader does is it basically sort of linearizes the operation of, you know, reading, perceiving, understanding graphical content on a user interface. And in particular, you might notice that the narration was actually quite rapid, right. And this is actually a slowed down version of what, you know, proficient screen reader users use, which is often much, much faster. But what is interesting about the screen reader use case is that it forces that linearity, right. And the key challenge in figuring out the articulatory distance in the case of accessibility is how do you take visualizations that probably all of us in the audience have slightly subtly different ways of reading, right. Maybe some of you start by reading the title, then moving to the axes, then looking at, you know, the shapes, while others might start by looking at the most salient trend and then start to, you know, map out to what the axes and legends and stuff like that are. How do we take all of that rich diversity, but linearize it? So the people who use screen readers can nevertheless have that same, you know, choice in meeting a visualization, but under these conditions. And so the way we have chosen to do that is basically by restructuring the content of a visualization into a text-oriented hierarchy. So at the top, at the root of this hierarchy is just a summary of the chart, probably the trends that are shown in the chart. And then the hierarchy branches off into the individual sort of data fields or the encodings in this case, right. The x-axis, the y-axis, the legend and things like that. And then people can start to drill down in ways that maintain some correspondence with the visual artifacts. So one step below, you know, the x-axis is stepping through them by the major ticks, right. One step below the major ticks would be minor ticks and then ultimately you would get to the individual data points. So let me throw things back to Jonathan to give us a demo of how this works. A scatter plot of Penguin data. And to a screen reader, our system represents this scatter plot as a keyboard navigable data structure that contains text descriptions at varying levels of detail. So when a screen reader user first encounters this visualization on a page, they'll be able to read off a high-level alt text description of what the chart is. So and if they're interested in getting more detail about this visualization, they can dive in by pressing the down arrow key to descend one level in the hierarchy and access descriptions about the different encodings of the scatter plot. So I'm going to press the down arrow key. I can press the left and right arrow keys to flip through descriptions of the other axes and legends. Cool, so let's say I am interested in getting more information about the x-axis. I can use the left arrow key to navigate back to the x-axis description and then press down one more time to descend a level of detail into the x-axis. So on this level underneath the x-axis, I'm accessing descriptions of intervals along the x-axis and it's reading out to me how many data values are contained within each interval. So by pressing left and right, I can kind of get a sense of the distribution of data along the x-axis. So let's say I am interested in this range from 190 to 200. I can then press down arrow again to dive into the individual data points that are contained within this interval. So let's say that instead of moving up and down this hierarchical structure, I would rather just move around the x-y grid in the scatter plot, as if I were kind of feeling around a tactile graphic, for example. I can start by navigating over to the grid view of the scatter plot. And once I descend into this part of the hierarchy, I can use the WASD keys to move up and down different squares along the grid. And so similarly to before, it's starting off by giving me the number of data values that are contained in that square so that I can get a sense of the distribution of the data. And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges. And this was the first time he felt like he actually understood and could build a mental model of what it was that a scatter plot was representing. We saw these sorts of comments reflected in user studies that we ran about how the form of this textual output really influenced participants' mental model of what the data was, what the trends were, and things like that. So one participant, for instance, said, I now know how to drill down and up between different layers in the data to get an overall picture. And it gives me a different way of thinking. And another one said, I'm thinking more in spatial terms because this is just a new method for navigating and moving through the grid and drilling down to information and things like that. And so what I find interesting here is that at every step, the semantic content stayed exactly the same. And there wasn't even very rich semantic content. It was a range and then a count of the data values. All we manipulated was that articulation, that form, giving it a hierarchical nature, adding all of these different navigational affordances, and just manipulating the articulation had this huge impact on people's mental models of the data. And I think that we're really just at the tip of the iceberg of these more accessible structures. Currently, in my group, we're thinking about just the impact that token order has on how people using screen readers build up those mental models. If you're constantly prompting them with the range first rather than the actual data values, does that introduce friction to their capacity to build that mental model and things like that? But in all of this, where is semantic distance? How do we actually start to make that textual descriptions more interesting and meaningful? And this is where I think LLMs can really help us. For one reason, it's because there's just a sheer amount of textual content we need to be able to produce that is infeasible to expect a human to sort of manually author. But there are other sort of implications that we'll touch upon really shortly. But before we can get LLMs to actually sort of produce the content we want, what we need to do is shift from that very latent space with implicit semantics to a set of explicit semantics. We need to impose a conceptual model onto our LLMs. Or another way of saying that is we need to get the LLMs to understand what a good textual description of a visualization is. And so that's what my then PhD student, Alan Lungard, set out to do. We ran a crowdsource study where we got sort of 2,000 descriptions of charts. And through qualitative coding, we realized there are basically four kinds of semantic content that textual descriptions should convey. The first most primitive layer is basically just the sort of construction details of the chart. What are the titles, the labels, the scales, the units, etc. And accessibility best practices say that this is some of the most important content to convey because it gives people sort of important milestones and landmarks. One level above that are the sort of statistical properties like minimum, maximum, outliers, and things like that. And then one level above that is probably what is cited people we consider the real value of visualization to be. The perceptual and cognitive characteristics like complex trends and patterns, things that automated statistical methods we typically think of as not being sufficient at. And then finally, the fourth and highest level are what journalists often consider to be the real value of visualization, which is the narration that gets associated with it. What is the data story that you're able to tell through the visualization? Can you explain what you're seeing, the causal mechanisms, etc., etc. Now, another reason I think LLMs are really suited for this sort of semantic bridging task is because when we asked sighted and blind people what their preferences were, when it came to these four layers, four levels, we saw really distinct preferences. In the case of sighted people, because we've got our own visual perception doing that sort of bridging of the Gulf of Evaluation, sighted people tended to want higher and higher levels of content being conveyed through text. Blind readers, on the other hand, were pretty significantly divergent. For many of them, they didn't want those level three and four, particularly the level four captions at all, because they wanted that time and space to do the interpretation for themselves. And so here, this visualization to me conveys that LLMs can help us or machine learning models can help us think about sort of personalizing the semantics of a user interface in a way that maybe we haven't had the opportunity to study so far. There's been a lot of work in personalization, but it's often been at that level of the articulation, changing the sizes of buttons and adapting color palettes and things like that. And there's maybe an opportunity now to use LLMs to actually change what the nouns, the verbs, the concepts of a user interface are much more fundamentally. And so the way we're going about doing this in the case of textual descriptions is we're going to be releasing very soon a data set of about, actually now we're over 12,000 pairs of chart captions. And we've generated some of these captions and we've crowdsourced some of these captions. And we started to train baseline models to do this task. And one of the interesting features here is how do we represent the semantics of a chart to a large language model, right? One way could just be let's treat the chart as an image, right? This is just a set of pixels. And unsurprisingly, the baseline models don't do very well at that because a chart is a much richer kind of artifact than just an image, right? It's got all this rich structure. So then we said, great, let's look at a data table or let's look at a scene graph, which is just a fancy way of saying the SVG associated with the chart. And a priori, we would have thought, well, the scene graph is maybe like a good in-between between the computational affordances of data table and capturing some of those perceptual characteristics. Turns out for the LLMs we trained that were all transformer models, they did equivalently well on those two representations. And so one of the things my group is working on right now is a new way of representing visualizations that more directly encode some of those perceptual operations that are otherwise currently implicit in a scene graph that grammar of graphics libraries like VegaLite or GGplot perform. But what's interesting in all of this to me is that through these generative models, the goal has been how do we impose a conceptual model onto them, right? How do we bring some explicit semantics? And I think we're just scratching the surface here as well because I think the chart example case is a really great one where a lot of these representations of charts that we've got right now, the grammar of graphics, for instance, were designed for people to author, right? So we're really good at figuring out how to design programming languages, domain specific languages, to emphasize the cognitive characteristics that are important for human authors. Things like, you know, the cognitive dimensions of notation that cares about, you know, how viscous is the programming language? How many premature commitments is the programming language enforced? But I don't know what it means to design a representation to be suitable for an LLM to operate over, right? Do we restructure the programming language more fundamentally to make it tractable for an LLM? Maybe. So in addition to generative models, my group has also been working with predictive models. And here I think the bridging task is really not about imposing a conceptual model, but bridging it or aligning it to the ones that we already have. And often the way that a lot of this work happens is through the lens of model interpretability. So here is a very popular set of techniques called saliency maps. The idea behind saliency maps is they're trying to depict the most important input features for a particular outcome. So in this case, you know, this is an image, the label should be toy terrier, and here's what a variety of different kinds of saliency methods believe to be, you know, the most important pixels to produce that outcome. Now, I look at these visualizations and I go, well, you know, is it telling me something? Maybe, right? And maybe the reason I believe it's telling me something is because I'm the one doing the perception and interpretive tasks, right? Like if I look at some of those visualizations on the bottom, I go, oh, like, it looks like the dog snout is really important to the classification of a toy terrier or the spots. But it's not actually the saliency method that is doing that interpretation for me. I'm the one bringing meaning to those lit pixels, right? And so as a result, if we think about that gulf of evaluation, it's not the saliency method that's helping bridge that gulf in any way, which is why saliency maps for now have been these tools that we just use in a very ad hoc way that require a lot of manual effort to make sense of. And so a question that my student, Angie Boggast, has been focused on is how do we scaffold that semantic sense making operation, right? Providing some additional structure to help sort of scale it up to make it more reproducible and things like that. And what she's developed is these set of metrics that are very analogous to ideas of precision and recall, but are operating at the level of input features and interpretability. So in many data sets, right, you've got some set of ground truth human annotated features. And what shared interest is looking at is what is the overlap between what a saliency method considers as being important to the classification and what the humans, the human annotators thought was important. And there's actually three different ways that these overlaps can manifest. The first is a sort of ground truth coverage, which is very analogous to ideas of recall, right? It's how much of the ground truth does the model incorporate in its prediction or what is the proportion of the ground truth region that is covered by the saliency region. And if we look at some examples of low coverage on the top and high coverage at the bottom, we can see then the case of low ground truth coverage is actually very little overlap, right, between the ground truth, the yellow region, and the salient region in orange. But I often find that it's actually the high coverage regions that are more interesting to analyze. So if we compare, you know, cases where the model was correct on the right with the green label and cases where the model was incorrect with the red label, we can see in the case of correct high ground truth coverage, there are instances where the model relies not just on the object, like in this case with the cab, but a lot of contextual information as well to ultimately make that correct prediction. But on the flip side, right, with the laptop, the model is doing the same thing, but here the context is actually throwing it off, right? It's actually confusing the model because it's accounting for too much of that context in its decision making. Another kind of coverage is something we call saliency coverage, and this is more akin to precision, right, which is how strictly is the model relying only on ground truth features to make its sort of prediction. And again, you know, if we look at low and high coverage in the case of low coverage, we can see again pretty disjoint sorts of sets. But in the case of the high coverage regions, we can see that, you know, in the case of high saliency coverage, it basically means that the salient regions are a strict subset of the ground truth coverage. But the difference between a correct and incorrect prediction is whether that subset was sufficient to make the correct classification or not, right? So in the case of the Maltese dog, it did indeed only need to look at the head to make that correct prediction. But in the case of the Dalmatian, it probably should have accounted for more of that dog's head or some of the other characteristics associated with the dog. By focusing only on the snout, it ended up sort of arriving at the incorrect sort of classification. And finally, the last metric is something that is very familiar IOU, the intersection over the union. This is sort of the strictest shared interest metric. It's really measuring how aligned the model's behavior is with human reasoning. So if you look at some examples, again, you know, low coverage at the top, we can see, you know, in incorrect cases, totally distinct disjoint sets again. But in a correct instance, I actually find that pretty interesting, right? Low IOU coverage, but it got a correct classification. Now, one could say maybe it got lucky. But potentially, what the signal there is, is that maybe all the model needs is a tiny bit of a wheel associated with a horse, right, to make the prediction that is actually a horse cart and not just a horse, right? And on the flip side, with high coverage, you know, Newfoundland, great, you know, total, total alignment. But in this case, right, incorrect classification, even though there was high coverage, this might suggest, you know, genuinely difficult to classify images, right, even for people. Because if I look at that, a pickup truck seems a totally reasonable guess to have made about the image. I don't know that I've got enough sort of visual information there to call that a snowplow. So shared interest basically gives us a mechanism to start to scaffold and structure, bridge that semantic distance, right? People no longer necessarily need to manually start to analyze these things. And in fact, you know, we analyzed lots of different models across, you know, both vision and natural language and found that different combinations of these shared interest metrics, along with figuring out whether the prediction was correct or not, actually surfaced eight kinds of repeating patterns in model behavior. So we can see human aligned and some of these others we also looked at earlier, right, context confusion, context dependent and so forth. And all of these give us sort of semantics that we can start to play around with through different articulations. So one articulation of these semantics might be a very traditional visual analytics interface, right, where I've got all the different kinds of images that I care about. This is a system we built to help a board certified dermatologist make sense of this melanoma detection model. And you've got, you know, query widgets on the top to sort and filter. You can use, you know, these histograms of the shared interest metrics to really drill into the data. But what was maybe most interesting was what the dermatologist said when they started to analyze that recurring pattern of context dependent cases. So in particular, when they switched to these context dependent cases, the dermatologist started to wonder if the model is seeing something we are not truly appreciating in the clinical image. Maybe there are subtle changes we don't yet understand that the model does, right at the boundaries of the skin region and things like that. And so, you know, to me, this is alluding to the fact of, well, can we as domain experts learn something about our problem domain based on how it is models are operating? And I think we see this more clearly in another articulation of shared interest semantics. Here, what we're doing is basically using shared interest to interactively probe or query that latent space. So we're brushing and using that brushed region as ground truth and then calculating the IOU coverage to figure out what are all the classes that maximize IOU coverage for that brush ground truth. So we can see if I brush over hand, a lot of the classes that get returned are things that are often associated with hands like laptops and cleavers and interestingly enough, hen. So I guess a lot of the images in the ImageNet, you know, data set have people holding hens, right, which is, I guess, kind of interesting. But more maybe profoundly is we could ask a question like, what is the essence of a dog, right? What is the minimal amount of region that I would need to brush for the model to still be convinced that what it is classifying as a dog? So I could start with the whole dog and then brush just on its head and sure, you know, querying which shared interest still returns, you know, dog classes. But then I could use a smaller brush and brush just on the nose and it still returns, you know, German shepherd and sheepdog and Tibetan terrier and things like that. So it seems like according to the model, all it really needs to know about, you know, an object in the image is the sort of shape of its nose or something associated with its nose to be able to classify whether it is or is not a dog. And this seems like a really sort of toy example, but it reflects some of the things that real world scientists are doing. So in particular, you know, there's a researcher at the University of Washington, Julia Parrish that runs this grand crowdsource data collection project around seabird deaths. And the way they train their participants to figure out how to do bird classification is by asking them to measure, you know, the bird beaks and the bird feet and things like that. And so I think it's really interesting that we're seeing maybe some of those sorts of representations creep up in how a model is making its decisions as well. And so where I want to end is sort of being most speculative and where I think, you know, there's scope for HCI to sort of grow. And so, you know, we looked at generative models and imposing a conceptual model on them. We looked at predictive models where the idea was to align conceptual models. But what I think, you know, we're hearing from that dermatologist we're seeing in that last case study which shared interest is the potential to use machine learning models to basically discover new representations of particular problem domains, right? And, you know, again, at my most speculative, I don't know what I would call these, but I would maybe call them abstraction models, right, where the goal of these models is not to produce some particular outcome that I care about, but to maximize what are the different ways of representing the world, right? What are all the diverse abstractions that we could learn about a problem domain like classifying dogs or classifying seabirds or things like that. And I think this is a really interesting opportunity to use machine learning to essentially advance our understanding, advance our science. But I want to be careful here because we've already seen, you know, through this talk, but also in the broader discourse, how generative and predictive models can sort of muddy that, that gulf of evaluation, right? Lots of people are starting to anthropomorphize these models, you know, some people think these models are representing general intelligence or conscience or things like that. And there's a potential with, you know, these abstraction models to make this problem worse by muddying the question of, well, how do we know what we know, right? Like what counts as evidence? Is it evidence because, you know, the model has learned that representation? And how do we validate what that evidence is? In the case of representations that are designed or interpreted or theorized by people, we know how to consider that to be evidence, right? But I don't know what it means for a learned representation to count as evidence. And as all sorts of problems in machine learning, this is not necessarily a problem that is unique to machine learning. So here are three visualizations that were used to discuss the COVID-19 pandemic right at the, the, the peak of the first wave in the summer of 2020. And I'm curious if anything pops out at you, like any reason, you know, to be curious or suspect of, of these visualizations, right? Like no, right? Probably not. Like these seem pretty legitimate, right? Like our world and data, very legitimate data source, right? And if you look at, look at some of these two other visualizations, you might go, you know what, actually the one on the right, that looks like something in maybe a policy briefing or something, right? It looks very sophisticated, lots of good annotation, you know, a style and aesthetic that looks very sort of sophisticated. But you may be catching what I'm alluding to, which is the fact that all three visualizations were used by people on social media to advance the argument that, you know, our response to COVID was overblown. Not that COVID was a hoax, but that our reaction to it was, was way too extreme. That COVID wasn't as serious an issue as it might initially seem. And I want to be really careful about what I'm, when I'm doing here with these charts, because certainly some of the people that were distributing this were bad actors who were ideologically motivated. But through a very long, laborious ethnography, ethnographic process that we conducted, spending six months on five different Facebook groups, we found that a lot of people who were producing visualizations like that were actually displaying many hallmarks of citizen data science. So they were really many of them filling gaps in, in information sort of collection, because they were situated in rural parts of the country where, you know, there wasn't a lot of good data collection. So many members of these groups were hosting, you know, webcasts, live seminars of how to download data from the government website, how to clean it and excel, how to visualize it and things like that. And, and most surprisingly to us, many of them were engaged in discussion that looked like peer review, right? They were critically assessing data sources, discussing metrics, making arguments for which metrics were, were better or not. But all of this was sort of inflected through a sort of frustration with mainstream institutions and maybe even distrust of those institutions as well, right? But ultimately what these groups cared about was bolstering a sense of social unity and civic engagement, right? So this quote I, I find particularly sort of reflective of that sense of, you know, it's incumbent on all of us to hold our elected officials to account so that they make better decisions through data, right? I'm speaking to you as a neighbor, as a mama bear, right? So this is not some sort of ideologically motivated individual who is, who is, you know, trying to be a bad actor. This is just an engaged member of the citizenry. And similarly, you know, oftentimes they were actually more sophisticated than scientists can be. So many of these members were very reflexive about their own data analysis, data gathering process, right? So someone says, you know, I've never claimed to have no bias, right? I'm human, of course I'm biased, here are my biases. Whereas in science, often we like to portray ourselves as being very objective, you know, arbiters of truth. And so in many ways, you know, what was happening in these groups is, is perhaps more sophisticated than what was happening in science and public health at the time. But the question is, so what does this have to do with sort of bridging semantic distances and abstraction models? Well, I think what was happening in, in those groups was, you know, they, they, they disagreed with the definitions of some of these metrics, right? They were living in rural communities. And so the metrics that public health officials were using to, to, you know, define the, the state and scale of the pandemic was not reflected in their lived experience. They were turning around and well, it didn't seem like COVID was an issue, right? And so our colleagues in the humanities and social sciences often advocate for adopting what they call an interpretive, interpretivist lens, right? The idea that knowledge is subjective, it's socially constructed, and that it's composed of many diff, different diverse perspectives that we have to figure out ways to synthesize together. And while that idea has been adopted in pockets of visualization and HCI and CS, so far, I think it's largely been on the qualitative side, because if we think about how to do computation, we have to, you know, we're forced into making decisions about the world and how to represent that world and computational data structures. And what I think abstraction models allow us to do is start to push, but you know, push that boundary a little bit, right? Rather than being focused on developing a model that produces a single best outcome, we might instead be looking to a world in which we are training sort of ecosystems of abstraction models, where we're forcing them to learn really different representations of the world or of a problem domain, and then leaving it up to people to figure out how to synthesize between those learned representations for, you know, some particular policy goal or, you know, thing that they want to optimize for. So with that, I'm happy to take questions about any of what I talked about. Thank you very much. This image that made me make this decision, so do you think the results would be different if you used, like, iFixations in that comparison? That's an interesting question. We haven't considered iFixations for the salency map work, but certainly I think your intuition is right in the sense that, you know, the current way that we've modeled shared interest is pretty brittle, right? It's operating at the level of abstraction of, like, pixels in an image, and how meaningful are pixels really? And so what Angie is working on right now is a way to raise the level of abstraction that shared interest is working on. So in many of these domains, like, you know, ImageNet, the task that we're asking models to do, the labeling task, actually inherits from a much richer knowledge graph or taxonomy or hierarchy or things like that. But right now, at least, you know, there's a little bit of work in hierarchical learning, but most of the predictive models are just learning at the finest level of detail, right? So we're throwing away all that rich information that might be really relevant to how a person is making a decision. So maybe what I care about is not whether it's a Chihuahua or a golden retriever or a laboratory retriever. I might care, is it a dog or really sometimes is it just an object, right? And so what does it look like to do shared interest in more meaningful abstraction space rather than pixels is something we're working on. Yeah, great question. Thanks. Yeah, Will. Thank you for the great talk, Arvin. So going back to Jupyter notebooks and ChatGPT, you talked about how, right, ChatGPT can shell out to some of these nice plugins like for Excel or whatever to try and help people do natural language data science and that there's this articulatory distance due to the difficulty of learning an API. But conversely, you could say tools like co-pilot are sort of the parallel to overcoming that articulatory distance by almost in some sense, what is the same interface expressing a natural language but just in a code comment and then getting back code, right? But just I guess the only difference is its code you can see as opposed to code that's running in some back end that you don't see. And I'm curious if you think there's sort of a synthesis of these two poles, an interface that can take the best of both worlds and offers conversation but still provides access to the code or encourages people to understand the annoying representations. Yeah, absolutely. I thought really hard about which of those examples I wanted to use as the kind of foil to B2. So I did very seriously consider a co-pilot and I sort of agree with your analysis that it's, I think, a much better example of how to integrate the capacity of these LLMs. And I think there's opportunity to push that even further where what I would often want is really targeted mechanisms to introduce ambiguity, right? Right now, the little that I've used co-pilot, it's almost at the level of, well, it's going to produce the whole function, the whole whatever. And often what I want is it to be the sort of parallel prototyper for me, right? I want to introduce, say, a hole in my program and then go, I don't know that I want that hole to be filled in with just one specific outcome, but I want it to produce the whole space and for me to go, well, I want a little bit of this and a little bit of that and so on and so forth. So yeah, I totally agree with there being some really interesting medium of these things. Cool. Yeah, I like that idea. Yeah. Hey, really exciting talk. I'm wondering towards your kind of vision for these abstraction models, I'm wondering like obviously kind of from a human-interpreter interaction perspective, we know like representation matters so much, right? Like isomorphs of representation very much change how people can approach a problem or understand it. But I guess the ways in which they vary and the benefits of these different representations are tied very much to human cognition and perception. And I'm wondering, you know, in some of the examples you're showing and a lot of work in machine learning, we're sort of training things based upon that output. Yeah. And I'm wondering like, you know, are there ways that we can get at more of how people are thinking versus just how they output and how do we get there? Yeah, I love this question. And the reason I love it is also the reason I love sort of that Hutchins et al description of direct manipulation, right? I find the terms that they use there, particularly these two's distances, really evocative terms. Because to me, a distance is something that I would want to measure, right? But so far at least, as far as I know, those terms have largely been descriptive, right? As you saw in my talk, like I use them to be very analytic, but I'm not able to be generative with them in, you know, a very systematic way. So certainly a lot of the work that my group is trying to do right now is in visualization, you know, there's a lot of work that we've inherited in methods from sort of vision science. So we run these studies of human perception. And increasingly, the field is starting to get to, well, how do we start to measure cognition, right? Can we model sort of a decision making task and start to, you know, operationalize that through experimental design? And so we're starting to push in some of those directions as well, but scope to sort of, you know, interaction in a Jupyter notebook, but then starting to see, you know, the impact that interaction has on sort of the downstream analyses people would do, and then see if that actually maps to, you know, their goals or things like that. Absolutely. Yeah. So I'm curious about the, just continue on this line of perception up through cognition, you know, going back to the sort of like, or 10 Cleveland McGill kinds of stuff, the automatic processing was very key to the design of visualizations, especially early on, that the notion was that my encodings were supposed to map on to almost like system one interpretation, right? Like when I see the scatterplot and you know, encoding distance in the following way, I'm gonna draw the correct conclusion. And it's interesting to me that sort of through the transformations you've started to pursue, we're not trying to like encode those into a similar mapping for audio, but instead directly doing the cognition on behalf of the individual. And those seem like orthogonal directions one could go. I'm curious how we find the right point in the design space. I think this is a fantastic question. So the way my group is starting to think about this of like, how do we find the sort of right balance of who is doing the perception, who is doing the interpretation is starting to consider some of these modalities and concert to better understand what the relative affordances of these modalities are. So in particular, Jonathan, who you saw in the demos is leading some really, really cool work right now around what if I'm sort of specifying the visual, the audio, the sort of sonified audio and the textual audio side by side, and then I'm playing them sort of simultaneously through. Do I want, you know, there to be sort of perceptual redundancy where the sonification is sort of emphasizing what is, you know, described in the texture? Do I want these modalities to be complementary? And, you know, sort of TBD, but I think there's some really exciting sort of questions for us to sort of dig into space. Are there similar pre attentive principles for audio? There must be. As far as I know, so I'm, you know, we're just starting to look in the sort of sonification literature. Yeah, as far as we can tell sound is a very, very different perceptual sense than vision. And so even the sort of, you know, basic sort of visual encoding paradigm where I take a data field, I map it to, you know, position color size that breaks down very rapidly for audio. So oftentimes really all the people are able to sort of, you know, detect differences in our sort of pitch and loudness. And even then our fidelity at that is very, very low. And so there might be some pre attentive characteristics. We're certainly looking at some early work in HCI. I think Stephen Brewster had done around sort of ear cons, you know, discreet sort of representations of icons, but through audio and things like that. So there may be some of that there. But at least so far we're so early in our own work that we don't know. Interesting. Okay, thank you. Yeah. I think we're about at time. So if you have additional questions, please mob him after the talk. Thank you, Arvin for joining us. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.44, "text": " Just such a treat to be back, I spend many hours on that side of the room, so it's wild", "tokens": [50364, 1449, 1270, 257, 2387, 281, 312, 646, 11, 286, 3496, 867, 2496, 322, 300, 1252, 295, 264, 1808, 11, 370, 309, 311, 4868, 51186], "temperature": 0.0, "avg_logprob": -0.20156969342912948, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.05914333090186119}, {"id": 1, "seek": 0, "start": 16.44, "end": 19.84, "text": " to be on this side of the room and going, whoa, there was actually like monitors up here,", "tokens": [51186, 281, 312, 322, 341, 1252, 295, 264, 1808, 293, 516, 11, 13310, 11, 456, 390, 767, 411, 26518, 493, 510, 11, 51356], "temperature": 0.0, "avg_logprob": -0.20156969342912948, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.05914333090186119}, {"id": 2, "seek": 0, "start": 19.84, "end": 25.44, "text": " like that's how the speakers kept track of where in their talk they were, so that's good", "tokens": [51356, 411, 300, 311, 577, 264, 9518, 4305, 2837, 295, 689, 294, 641, 751, 436, 645, 11, 370, 300, 311, 665, 51636], "temperature": 0.0, "avg_logprob": -0.20156969342912948, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.05914333090186119}, {"id": 3, "seek": 0, "start": 25.44, "end": 27.28, "text": " to know.", "tokens": [51636, 281, 458, 13, 51728], "temperature": 0.0, "avg_logprob": -0.20156969342912948, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.05914333090186119}, {"id": 4, "seek": 2728, "start": 27.6, "end": 31.44, "text": " This is sort of the first set of talks I've given since the pandemic, and so I thought", "tokens": [50380, 639, 307, 1333, 295, 264, 700, 992, 295, 6686, 286, 600, 2212, 1670, 264, 5388, 11, 293, 370, 286, 1194, 50572], "temperature": 0.0, "avg_logprob": -0.1440813700358073, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.04946587607264519}, {"id": 5, "seek": 2728, "start": 31.44, "end": 36.72, "text": " it was a really great opportunity to talk about some new ideas that have been on my", "tokens": [50572, 309, 390, 257, 534, 869, 2650, 281, 751, 466, 512, 777, 3487, 300, 362, 668, 322, 452, 50836], "temperature": 0.0, "avg_logprob": -0.1440813700358073, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.04946587607264519}, {"id": 6, "seek": 2728, "start": 36.72, "end": 42.0, "text": " mind, and particularly with all of you as my captive audience, I thought that I would", "tokens": [50836, 1575, 11, 293, 4098, 365, 439, 295, 291, 382, 452, 41762, 4034, 11, 286, 1194, 300, 286, 576, 51100], "temperature": 0.0, "avg_logprob": -0.1440813700358073, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.04946587607264519}, {"id": 7, "seek": 2728, "start": 42.0, "end": 48.84, "text": " use this talk as an opportunity to think out loud about what the role of HCI should be", "tokens": [51100, 764, 341, 751, 382, 364, 2650, 281, 519, 484, 6588, 466, 437, 264, 3090, 295, 389, 25240, 820, 312, 51442], "temperature": 0.0, "avg_logprob": -0.1440813700358073, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.04946587607264519}, {"id": 8, "seek": 2728, "start": 48.84, "end": 55.760000000000005, "text": " in the face of all of this really incredible rapid progress that AI and ML have made, particularly", "tokens": [51442, 294, 264, 1851, 295, 439, 295, 341, 534, 4651, 7558, 4205, 300, 7318, 293, 21601, 362, 1027, 11, 4098, 51788], "temperature": 0.0, "avg_logprob": -0.1440813700358073, "compression_ratio": 1.7131782945736433, "no_speech_prob": 0.04946587607264519}, {"id": 9, "seek": 5576, "start": 55.76, "end": 61.92, "text": " kind of scoped in the last six months or so.", "tokens": [50364, 733, 295, 795, 27277, 294, 264, 1036, 2309, 2493, 420, 370, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1559408454484837, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.005214220844209194}, {"id": 10, "seek": 5576, "start": 61.92, "end": 68.48, "text": " As I was trying to think about what the role of HCI should be, I was reminded of this figure", "tokens": [50672, 1018, 286, 390, 1382, 281, 519, 466, 437, 264, 3090, 295, 389, 25240, 820, 312, 11, 286, 390, 15920, 295, 341, 2573, 51000], "temperature": 0.0, "avg_logprob": -0.1559408454484837, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.005214220844209194}, {"id": 11, "seek": 5576, "start": 68.48, "end": 75.4, "text": " from Jonathan Gruden's 2009 article in triple AI about how AI and HCI are two fields that", "tokens": [51000, 490, 15471, 2606, 32940, 311, 11453, 7222, 294, 15508, 7318, 466, 577, 7318, 293, 389, 25240, 366, 732, 7909, 300, 51346], "temperature": 0.0, "avg_logprob": -0.1559408454484837, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.005214220844209194}, {"id": 12, "seek": 5576, "start": 75.4, "end": 77.88, "text": " are divided by a common focus.", "tokens": [51346, 366, 6666, 538, 257, 2689, 1879, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1559408454484837, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.005214220844209194}, {"id": 13, "seek": 5576, "start": 77.88, "end": 83.08, "text": " As you can see in moments where AI makes a lot of progress, it's almost like the pendulum", "tokens": [51470, 1018, 291, 393, 536, 294, 6065, 689, 7318, 1669, 257, 688, 295, 4205, 11, 309, 311, 1920, 411, 264, 44103, 51730], "temperature": 0.0, "avg_logprob": -0.1559408454484837, "compression_ratio": 1.5064935064935066, "no_speech_prob": 0.005214220844209194}, {"id": 14, "seek": 8308, "start": 83.12, "end": 91.0, "text": " swings towards ever-increased amounts of automation, perhaps at the expense of more HCI-esque approaches", "tokens": [50366, 32386, 3030, 1562, 12, 4647, 265, 1937, 11663, 295, 17769, 11, 4317, 412, 264, 18406, 295, 544, 389, 25240, 12, 47457, 11587, 50760], "temperature": 0.0, "avg_logprob": -0.13616649082728793, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.003942726645618677}, {"id": 15, "seek": 8308, "start": 91.0, "end": 97.96, "text": " of human intelligence augmentation or amplification, but also I think HCI is in a more established", "tokens": [50760, 295, 1952, 7599, 14501, 19631, 420, 9731, 3774, 11, 457, 611, 286, 519, 389, 25240, 307, 294, 257, 544, 7545, 51108], "temperature": 0.0, "avg_logprob": -0.13616649082728793, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.003942726645618677}, {"id": 16, "seek": 8308, "start": 97.96, "end": 107.2, "text": " stronger position than it's ever been in the past, and so I really think it's our responsibility", "tokens": [51108, 7249, 2535, 813, 309, 311, 1562, 668, 294, 264, 1791, 11, 293, 370, 286, 534, 519, 309, 311, 527, 6357, 51570], "temperature": 0.0, "avg_logprob": -0.13616649082728793, "compression_ratio": 1.5463917525773196, "no_speech_prob": 0.003942726645618677}, {"id": 17, "seek": 10720, "start": 107.28, "end": 113.12, "text": " to think about what that counterbalance to ever-increased automation should be.", "tokens": [50368, 281, 519, 466, 437, 300, 5682, 29215, 281, 1562, 12, 4647, 265, 1937, 17769, 820, 312, 13, 50660], "temperature": 0.0, "avg_logprob": -0.22334149198712044, "compression_ratio": 1.5793357933579335, "no_speech_prob": 0.4716586768627167}, {"id": 18, "seek": 10720, "start": 113.12, "end": 118.24000000000001, "text": " So I often, in moments like this, like to sort of turn to history and ground myself,", "tokens": [50660, 407, 286, 2049, 11, 294, 6065, 411, 341, 11, 411, 281, 1333, 295, 1261, 281, 2503, 293, 2727, 2059, 11, 50916], "temperature": 0.0, "avg_logprob": -0.22334149198712044, "compression_ratio": 1.5793357933579335, "no_speech_prob": 0.4716586768627167}, {"id": 19, "seek": 10720, "start": 118.24000000000001, "end": 123.52000000000001, "text": " and so if we cast back to the first AI winter with Sutherland's sketchpad, right around", "tokens": [50916, 293, 370, 498, 321, 4193, 646, 281, 264, 700, 7318, 6355, 365, 318, 17696, 1661, 311, 12325, 13647, 11, 558, 926, 51180], "temperature": 0.0, "avg_logprob": -0.22334149198712044, "compression_ratio": 1.5793357933579335, "no_speech_prob": 0.4716586768627167}, {"id": 20, "seek": 10720, "start": 123.52000000000001, "end": 129.32, "text": " that time there was this foundational paper written by Lick Leiter at MIT titled Man", "tokens": [51180, 300, 565, 456, 390, 341, 32195, 3035, 3720, 538, 441, 618, 1456, 1681, 412, 13100, 19841, 2458, 51470], "temperature": 0.0, "avg_logprob": -0.22334149198712044, "compression_ratio": 1.5793357933579335, "no_speech_prob": 0.4716586768627167}, {"id": 21, "seek": 10720, "start": 129.32, "end": 134.32, "text": " Computers in Biosas, and I think the gendering is unfortunate and unfortunately reflective", "tokens": [51470, 37804, 433, 294, 363, 2717, 296, 11, 293, 286, 519, 264, 290, 521, 1794, 307, 17843, 293, 7015, 28931, 51720], "temperature": 0.0, "avg_logprob": -0.22334149198712044, "compression_ratio": 1.5793357933579335, "no_speech_prob": 0.4716586768627167}, {"id": 22, "seek": 13432, "start": 134.4, "end": 140.4, "text": " of the times, but nevertheless in this paper, Lick Leiter put forth this really compelling", "tokens": [50368, 295, 264, 1413, 11, 457, 26924, 294, 341, 3035, 11, 441, 618, 1456, 1681, 829, 5220, 341, 534, 20050, 50668], "temperature": 0.0, "avg_logprob": -0.15742639952068088, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.010645348578691483}, {"id": 23, "seek": 13432, "start": 140.4, "end": 147.9, "text": " vision about the ways in which a computer could interact with us through this intuitive,", "tokens": [50668, 5201, 466, 264, 2098, 294, 597, 257, 3820, 727, 4648, 365, 505, 807, 341, 21769, 11, 51043], "temperature": 0.0, "avg_logprob": -0.15742639952068088, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.010645348578691483}, {"id": 24, "seek": 13432, "start": 147.9, "end": 154.66, "text": " guided trial and error procedure, turning up solutions and revealing unexpected turns", "tokens": [51043, 19663, 7308, 293, 6713, 10747, 11, 6246, 493, 6547, 293, 23983, 13106, 4523, 51381], "temperature": 0.0, "avg_logprob": -0.15742639952068088, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.010645348578691483}, {"id": 25, "seek": 13432, "start": 154.66, "end": 159.24, "text": " in the reasoning, and I was really tempted to put this sort of side-by-side with this", "tokens": [51381, 294, 264, 21577, 11, 293, 286, 390, 534, 29941, 281, 829, 341, 1333, 295, 1252, 12, 2322, 12, 1812, 365, 341, 51610], "temperature": 0.0, "avg_logprob": -0.15742639952068088, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.010645348578691483}, {"id": 26, "seek": 15924, "start": 159.32000000000002, "end": 165.56, "text": " very recent demo that OpenAI released with ChadGPT plus plugins where you can upload", "tokens": [50368, 588, 5162, 10723, 300, 7238, 48698, 4736, 365, 22268, 38, 47, 51, 1804, 33759, 689, 291, 393, 6580, 50680], "temperature": 0.0, "avg_logprob": -0.16648072594994898, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.37668782472610474}, {"id": 27, "seek": 15924, "start": 165.56, "end": 170.56, "text": " this music.csv data set and then start to have this very natural language interaction", "tokens": [50680, 341, 1318, 13, 14368, 85, 1412, 992, 293, 550, 722, 281, 362, 341, 588, 3303, 2856, 9285, 50930], "temperature": 0.0, "avg_logprob": -0.16648072594994898, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.37668782472610474}, {"id": 28, "seek": 15924, "start": 170.56, "end": 174.96, "text": " to ask what are the columns in the data set, how many rows in there in the data set, and", "tokens": [50930, 281, 1029, 437, 366, 264, 13766, 294, 264, 1412, 992, 11, 577, 867, 13241, 294, 456, 294, 264, 1412, 992, 11, 293, 51150], "temperature": 0.0, "avg_logprob": -0.16648072594994898, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.37668782472610474}, {"id": 29, "seek": 15924, "start": 174.96, "end": 180.24, "text": " then even say, can you give me some basic visualizations of this data set, and it thinks", "tokens": [51150, 550, 754, 584, 11, 393, 291, 976, 385, 512, 3875, 5056, 14455, 295, 341, 1412, 992, 11, 293, 309, 7309, 51414], "temperature": 0.0, "avg_logprob": -0.16648072594994898, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.37668782472610474}, {"id": 30, "seek": 15924, "start": 180.24, "end": 186.4, "text": " a little bit, it's working real hard, and there you go, it produces sort of three visualizations", "tokens": [51414, 257, 707, 857, 11, 309, 311, 1364, 957, 1152, 11, 293, 456, 291, 352, 11, 309, 14725, 1333, 295, 1045, 5056, 14455, 51722], "temperature": 0.0, "avg_logprob": -0.16648072594994898, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.37668782472610474}, {"id": 31, "seek": 18640, "start": 186.44, "end": 192.24, "text": " and even starts to give you maybe something that looks like an explanation, and I wonder,", "tokens": [50366, 293, 754, 3719, 281, 976, 291, 1310, 746, 300, 1542, 411, 364, 10835, 11, 293, 286, 2441, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1371665394634282, "compression_ratio": 1.7300380228136882, "no_speech_prob": 0.013215875253081322}, {"id": 32, "seek": 18640, "start": 192.24, "end": 196.84, "text": " is it time to roll out our mission accomplished banners? Like have we achieved Lick Leiter's", "tokens": [50656, 307, 309, 565, 281, 3373, 484, 527, 4447, 15419, 272, 25792, 30, 1743, 362, 321, 11042, 441, 618, 1456, 1681, 311, 50886], "temperature": 0.0, "avg_logprob": -0.1371665394634282, "compression_ratio": 1.7300380228136882, "no_speech_prob": 0.013215875253081322}, {"id": 33, "seek": 18640, "start": 196.84, "end": 202.16, "text": " vision to think in interaction with a computer in the same way that we think with a colleague", "tokens": [50886, 5201, 281, 519, 294, 9285, 365, 257, 3820, 294, 264, 912, 636, 300, 321, 519, 365, 257, 13532, 51152], "temperature": 0.0, "avg_logprob": -0.1371665394634282, "compression_ratio": 1.7300380228136882, "no_speech_prob": 0.013215875253081322}, {"id": 34, "seek": 18640, "start": 202.16, "end": 207.76, "text": " whose competence supplements our own? Now, I don't think it's time to roll out the mission", "tokens": [51152, 6104, 39965, 26645, 527, 1065, 30, 823, 11, 286, 500, 380, 519, 309, 311, 565, 281, 3373, 484, 264, 4447, 51432], "temperature": 0.0, "avg_logprob": -0.1371665394634282, "compression_ratio": 1.7300380228136882, "no_speech_prob": 0.013215875253081322}, {"id": 35, "seek": 18640, "start": 207.76, "end": 211.84, "text": " accomplished banners, but I'm hopeful that the reason it's not that is not just my sort", "tokens": [51432, 15419, 272, 25792, 11, 457, 286, 478, 20531, 300, 264, 1778, 309, 311, 406, 300, 307, 406, 445, 452, 1333, 51636], "temperature": 0.0, "avg_logprob": -0.1371665394634282, "compression_ratio": 1.7300380228136882, "no_speech_prob": 0.013215875253081322}, {"id": 36, "seek": 21184, "start": 211.88, "end": 217.96, "text": " of hope that we haven't been put out of jobs, but rather that there is something more to do.", "tokens": [50366, 295, 1454, 300, 321, 2378, 380, 668, 829, 484, 295, 4782, 11, 457, 2831, 300, 456, 307, 746, 544, 281, 360, 13, 50670], "temperature": 0.0, "avg_logprob": -0.14202716133811258, "compression_ratio": 1.5232067510548524, "no_speech_prob": 0.011150130070745945}, {"id": 37, "seek": 21184, "start": 217.96, "end": 224.96, "text": " So two years after Lick Leiter's man-computer symbiosis, Douglas Engelbart wrote up this", "tokens": [50670, 407, 732, 924, 934, 441, 618, 1456, 1681, 311, 587, 12, 1112, 13849, 43700, 48783, 11, 23010, 2469, 338, 47504, 4114, 493, 341, 51020], "temperature": 0.0, "avg_logprob": -0.14202716133811258, "compression_ratio": 1.5232067510548524, "no_speech_prob": 0.011150130070745945}, {"id": 38, "seek": 21184, "start": 224.96, "end": 230.24, "text": " really incredible framework called augmenting human intellect, and right in the introduction", "tokens": [51020, 534, 4651, 8388, 1219, 29919, 278, 1952, 10058, 11, 293, 558, 294, 264, 9339, 51284], "temperature": 0.0, "avg_logprob": -0.14202716133811258, "compression_ratio": 1.5232067510548524, "no_speech_prob": 0.011150130070745945}, {"id": 39, "seek": 21184, "start": 230.24, "end": 236.44, "text": " of this piece, we already start to see how Engelbart is defining a much more expansive", "tokens": [51284, 295, 341, 2522, 11, 321, 1217, 722, 281, 536, 577, 2469, 338, 47504, 307, 17827, 257, 709, 544, 46949, 51594], "temperature": 0.0, "avg_logprob": -0.14202716133811258, "compression_ratio": 1.5232067510548524, "no_speech_prob": 0.011150130070745945}, {"id": 40, "seek": 23644, "start": 236.48, "end": 242.68, "text": " role of human augmentation. So the idea is not just about problem solving, which he does", "tokens": [50366, 3090, 295, 1952, 14501, 19631, 13, 407, 264, 1558, 307, 406, 445, 466, 1154, 12606, 11, 597, 415, 775, 50676], "temperature": 0.0, "avg_logprob": -0.16092785750285232, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.1559057980775833}, {"id": 41, "seek": 23644, "start": 242.68, "end": 247.64, "text": " mention right at the end there, to derive solutions to a problem, but it's also about", "tokens": [50676, 2152, 558, 412, 264, 917, 456, 11, 281, 28446, 6547, 281, 257, 1154, 11, 457, 309, 311, 611, 466, 50924], "temperature": 0.0, "avg_logprob": -0.16092785750285232, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.1559057980775833}, {"id": 42, "seek": 23644, "start": 247.64, "end": 254.64, "text": " using computers to help us think. It's to increase our capacity to approach a complex", "tokens": [50924, 1228, 10807, 281, 854, 505, 519, 13, 467, 311, 281, 3488, 527, 6042, 281, 3109, 257, 3997, 51274], "temperature": 0.0, "avg_logprob": -0.16092785750285232, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.1559057980775833}, {"id": 43, "seek": 23644, "start": 255.68, "end": 261.04, "text": " problem situation, to gain comprehension really about this thinking and not just the problem", "tokens": [51326, 1154, 2590, 11, 281, 6052, 44991, 534, 466, 341, 1953, 293, 406, 445, 264, 1154, 51594], "temperature": 0.0, "avg_logprob": -0.16092785750285232, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.1559057980775833}, {"id": 44, "seek": 23644, "start": 261.04, "end": 266.28, "text": " solving pieces, and really what I like is how he thinks we'll get there. Certainly there", "tokens": [51594, 12606, 3755, 11, 293, 534, 437, 286, 411, 307, 577, 415, 7309, 321, 603, 483, 456, 13, 16628, 456, 51856], "temperature": 0.0, "avg_logprob": -0.16092785750285232, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.1559057980775833}, {"id": 45, "seek": 26628, "start": 266.28, "end": 270.91999999999996, "text": " will be sophisticated methods, high powered electronic aids, but to me the part that really", "tokens": [50364, 486, 312, 16950, 7150, 11, 1090, 17786, 10092, 28447, 11, 457, 281, 385, 264, 644, 300, 534, 50596], "temperature": 0.0, "avg_logprob": -0.1652635998196072, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.0034810821525752544}, {"id": 46, "seek": 26628, "start": 270.91999999999996, "end": 276.79999999999995, "text": " resonates in his prescription here is streamlined terminology and notation, and that's going", "tokens": [50596, 41051, 294, 702, 22456, 510, 307, 48155, 27575, 293, 24657, 11, 293, 300, 311, 516, 50890], "temperature": 0.0, "avg_logprob": -0.1652635998196072, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.0034810821525752544}, {"id": 47, "seek": 26628, "start": 276.79999999999995, "end": 283.44, "text": " to be a theme of my talk here, certainly one of the themes that underlies my group's work.", "tokens": [50890, 281, 312, 257, 6314, 295, 452, 751, 510, 11, 3297, 472, 295, 264, 13544, 300, 833, 24119, 452, 1594, 311, 589, 13, 51222], "temperature": 0.0, "avg_logprob": -0.1652635998196072, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.0034810821525752544}, {"id": 48, "seek": 26628, "start": 283.44, "end": 289.0, "text": " And so in contrast to that chat GPT demo, a few years ago I had the pleasure to work", "tokens": [51222, 400, 370, 294, 8712, 281, 300, 5081, 26039, 51, 10723, 11, 257, 1326, 924, 2057, 286, 632, 264, 6834, 281, 589, 51500], "temperature": 0.0, "avg_logprob": -0.1652635998196072, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.0034810821525752544}, {"id": 49, "seek": 26628, "start": 289.0, "end": 292.88, "text": " with some collaborators at Berkeley, Yifan Wu and Joe Hellerstein, who you see in the", "tokens": [51500, 365, 512, 39789, 412, 23684, 11, 398, 351, 282, 17287, 293, 6807, 634, 4658, 9089, 11, 567, 291, 536, 294, 264, 51694], "temperature": 0.0, "avg_logprob": -0.1652635998196072, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.0034810821525752544}, {"id": 50, "seek": 29288, "start": 292.92, "end": 298.44, "text": " top right-hand corner, on this system called B2. So this is a Jupyter notebook, it's a", "tokens": [50366, 1192, 558, 12, 5543, 4538, 11, 322, 341, 1185, 1219, 363, 17, 13, 407, 341, 307, 257, 22125, 88, 391, 21060, 11, 309, 311, 257, 50642], "temperature": 0.0, "avg_logprob": -0.1577962755082964, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.010646680369973183}, {"id": 51, "seek": 29288, "start": 298.44, "end": 303.71999999999997, "text": " very commonly used data science environment where people can start to write code in the", "tokens": [50642, 588, 12719, 1143, 1412, 3497, 2823, 689, 561, 393, 722, 281, 2464, 3089, 294, 264, 50906], "temperature": 0.0, "avg_logprob": -0.1577962755082964, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.010646680369973183}, {"id": 52, "seek": 29288, "start": 303.71999999999997, "end": 309.84, "text": " style of a Python REPL, but what B2 does is saying, well, in addition to that sort of", "tokens": [50906, 3758, 295, 257, 15329, 31511, 43, 11, 457, 437, 363, 17, 775, 307, 1566, 11, 731, 11, 294, 4500, 281, 300, 1333, 295, 51212], "temperature": 0.0, "avg_logprob": -0.1577962755082964, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.010646680369973183}, {"id": 53, "seek": 29288, "start": 309.84, "end": 315.71999999999997, "text": " linear style of data science analysis and programming, there's a lot of value in a more", "tokens": [51212, 8213, 3758, 295, 1412, 3497, 5215, 293, 9410, 11, 456, 311, 257, 688, 295, 2158, 294, 257, 544, 51506], "temperature": 0.0, "avg_logprob": -0.1577962755082964, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.010646680369973183}, {"id": 54, "seek": 29288, "start": 315.71999999999997, "end": 321.0, "text": " visual analysis dashboard style interface like Tableau. And so what B2 tries to do is", "tokens": [51506, 5056, 5215, 18342, 3758, 9226, 411, 25535, 1459, 13, 400, 370, 437, 363, 17, 9898, 281, 360, 307, 51770], "temperature": 0.0, "avg_logprob": -0.1577962755082964, "compression_ratio": 1.6014760147601477, "no_speech_prob": 0.010646680369973183}, {"id": 55, "seek": 32100, "start": 321.04, "end": 326.16, "text": " bring these two pieces together. So you can see once I've invoked B2 it adds this on the", "tokens": [50366, 1565, 613, 732, 3755, 1214, 13, 407, 291, 393, 536, 1564, 286, 600, 1048, 9511, 363, 17, 309, 10860, 341, 322, 264, 50622], "temperature": 0.0, "avg_logprob": -0.11360845151154891, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.0044670021161437035}, {"id": 56, "seek": 32100, "start": 326.16, "end": 331.96, "text": " sidebar, and I can start to issue regular sort of Python, you know, pandas commands", "tokens": [50622, 1252, 5356, 11, 293, 286, 393, 722, 281, 2734, 3890, 1333, 295, 15329, 11, 291, 458, 11, 4565, 296, 16901, 50912], "temperature": 0.0, "avg_logprob": -0.11360845151154891, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.0044670021161437035}, {"id": 57, "seek": 32100, "start": 331.96, "end": 336.96, "text": " like looking at the data frame, getting a, you know, a sense of how many rows there are,", "tokens": [50912, 411, 1237, 412, 264, 1412, 3920, 11, 1242, 257, 11, 291, 458, 11, 257, 2020, 295, 577, 867, 13241, 456, 366, 11, 51162], "temperature": 0.0, "avg_logprob": -0.11360845151154891, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.0044670021161437035}, {"id": 58, "seek": 32100, "start": 336.96, "end": 341.6, "text": " what the columns are, and now I can start to write some code to do a little bit of data", "tokens": [51162, 437, 264, 13766, 366, 11, 293, 586, 286, 393, 722, 281, 2464, 512, 3089, 281, 360, 257, 707, 857, 295, 1412, 51394], "temperature": 0.0, "avg_logprob": -0.11360845151154891, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.0044670021161437035}, {"id": 59, "seek": 32100, "start": 341.6, "end": 346.12, "text": " transformation and visualization. Notice here in all of these steps, you know, when I'm", "tokens": [51394, 9887, 293, 25801, 13, 13428, 510, 294, 439, 295, 613, 4439, 11, 291, 458, 11, 562, 286, 478, 51620], "temperature": 0.0, "avg_logprob": -0.11360845151154891, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.0044670021161437035}, {"id": 60, "seek": 34612, "start": 346.16, "end": 350.8, "text": " authoring a visualization, I don't have to specify what that visualization should look", "tokens": [50366, 3793, 278, 257, 25801, 11, 286, 500, 380, 362, 281, 16500, 437, 300, 25801, 820, 574, 50598], "temperature": 0.0, "avg_logprob": -0.11800610192931525, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.004754241090267897}, {"id": 61, "seek": 34612, "start": 350.8, "end": 356.8, "text": " like, right? I'm just calling these .viz methods on the data frame, and B2 behind the scenes", "tokens": [50598, 411, 11, 558, 30, 286, 478, 445, 5141, 613, 2411, 85, 590, 7150, 322, 264, 1412, 3920, 11, 293, 363, 17, 2261, 264, 8026, 50898], "temperature": 0.0, "avg_logprob": -0.11800610192931525, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.004754241090267897}, {"id": 62, "seek": 34612, "start": 356.8, "end": 362.0, "text": " is figuring out what sort of visualization actually makes sense based on the history", "tokens": [50898, 307, 15213, 484, 437, 1333, 295, 25801, 767, 1669, 2020, 2361, 322, 264, 2503, 51158], "temperature": 0.0, "avg_logprob": -0.11800610192931525, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.004754241090267897}, {"id": 63, "seek": 34612, "start": 362.0, "end": 367.52, "text": " of the transformations that were performed on the data frame. So in the case of year,", "tokens": [51158, 295, 264, 34852, 300, 645, 10332, 322, 264, 1412, 3920, 13, 407, 294, 264, 1389, 295, 1064, 11, 51434], "temperature": 0.0, "avg_logprob": -0.11800610192931525, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.004754241090267897}, {"id": 64, "seek": 34612, "start": 367.52, "end": 372.4, "text": " for instance, if I've grouped by year, the most sensible visualization to produce is", "tokens": [51434, 337, 5197, 11, 498, 286, 600, 41877, 538, 1064, 11, 264, 881, 25380, 25801, 281, 5258, 307, 51678], "temperature": 0.0, "avg_logprob": -0.11800610192931525, "compression_ratio": 1.6926070038910506, "no_speech_prob": 0.004754241090267897}, {"id": 65, "seek": 37240, "start": 372.44, "end": 378.2, "text": " a histogram of the number of counts of data records across years. You might have also noticed", "tokens": [50366, 257, 49816, 295, 264, 1230, 295, 14893, 295, 1412, 7724, 2108, 924, 13, 509, 1062, 362, 611, 5694, 50654], "temperature": 0.0, "avg_logprob": -0.1236202010401973, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0028889125678688288}, {"id": 66, "seek": 37240, "start": 378.2, "end": 383.0, "text": " in the video that if I click the fields on the right hand side there, that it automatically", "tokens": [50654, 294, 264, 960, 300, 498, 286, 2052, 264, 7909, 322, 264, 558, 1011, 1252, 456, 11, 300, 309, 6772, 50894], "temperature": 0.0, "avg_logprob": -0.1236202010401973, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0028889125678688288}, {"id": 67, "seek": 37240, "start": 383.0, "end": 388.23999999999995, "text": " produces an equivalent visualization, but it doesn't stop there. It adds, you know, the code", "tokens": [50894, 14725, 364, 10344, 25801, 11, 457, 309, 1177, 380, 1590, 456, 13, 467, 10860, 11, 291, 458, 11, 264, 3089, 51156], "temperature": 0.0, "avg_logprob": -0.1236202010401973, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0028889125678688288}, {"id": 68, "seek": 37240, "start": 388.23999999999995, "end": 394.84, "text": " and tags them with these little, you know, yellow emojis to indicate that there's actually sort of", "tokens": [51156, 293, 18632, 552, 365, 613, 707, 11, 291, 458, 11, 5566, 19611, 40371, 281, 13330, 300, 456, 311, 767, 1333, 295, 51486], "temperature": 0.0, "avg_logprob": -0.1236202010401973, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0028889125678688288}, {"id": 69, "seek": 37240, "start": 394.84, "end": 401.08, "text": " a common shared representation here, right? Clicking on the sidebar not only produces the", "tokens": [51486, 257, 2689, 5507, 10290, 510, 11, 558, 30, 8230, 278, 322, 264, 1252, 5356, 406, 787, 14725, 264, 51798], "temperature": 0.0, "avg_logprob": -0.1236202010401973, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.0028889125678688288}, {"id": 70, "seek": 40108, "start": 401.08, "end": 406.44, "text": " visualization, but produces the equivalent code as well. And what's interesting is that these", "tokens": [50364, 25801, 11, 457, 14725, 264, 10344, 3089, 382, 731, 13, 400, 437, 311, 1880, 307, 300, 613, 50632], "temperature": 0.0, "avg_logprob": -0.12210935966990819, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001648180652409792}, {"id": 71, "seek": 40108, "start": 406.44, "end": 411.91999999999996, "text": " visualizations aren't just output mechanisms, but I can start to interact with them to sort of do", "tokens": [50632, 5056, 14455, 3212, 380, 445, 5598, 15902, 11, 457, 286, 393, 722, 281, 4648, 365, 552, 281, 1333, 295, 360, 50906], "temperature": 0.0, "avg_logprob": -0.12210935966990819, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001648180652409792}, {"id": 72, "seek": 40108, "start": 411.91999999999996, "end": 418.03999999999996, "text": " this cross filtering interaction. So all the other bars update to reflect the data shown in the", "tokens": [50906, 341, 3278, 30822, 9285, 13, 407, 439, 264, 661, 10228, 5623, 281, 5031, 264, 1412, 4898, 294, 264, 51212], "temperature": 0.0, "avg_logprob": -0.12210935966990819, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001648180652409792}, {"id": 73, "seek": 40108, "start": 418.03999999999996, "end": 424.64, "text": " highlighted bars, and B2 is keeping this as an interaction log that is semantically meaningful", "tokens": [51212, 17173, 10228, 11, 293, 363, 17, 307, 5145, 341, 382, 364, 9285, 3565, 300, 307, 4361, 49505, 10995, 51542], "temperature": 0.0, "avg_logprob": -0.12210935966990819, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001648180652409792}, {"id": 74, "seek": 40108, "start": 424.64, "end": 429.68, "text": " to me. So this interaction log doesn't comprise mouse clicks and keystrokes and things like that,", "tokens": [51542, 281, 385, 13, 407, 341, 9285, 3565, 1177, 380, 16802, 908, 9719, 18521, 293, 2141, 27616, 5993, 293, 721, 411, 300, 11, 51794], "temperature": 0.0, "avg_logprob": -0.12210935966990819, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.001648180652409792}, {"id": 75, "seek": 42968, "start": 429.92, "end": 435.04, "text": " but it's expressing data queries, right? Which states have been selected? And I can use that", "tokens": [50376, 457, 309, 311, 22171, 1412, 24109, 11, 558, 30, 3013, 4368, 362, 668, 8209, 30, 400, 286, 393, 764, 300, 50632], "temperature": 0.0, "avg_logprob": -0.09992683635038488, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00027798733208328485}, {"id": 76, "seek": 42968, "start": 435.04, "end": 441.52, "text": " data query to perform subsequent sort of analyses based on my interactive results. So I can say,", "tokens": [50632, 1412, 14581, 281, 2042, 19962, 1333, 295, 37560, 2361, 322, 452, 15141, 3542, 13, 407, 286, 393, 584, 11, 50956], "temperature": 0.0, "avg_logprob": -0.09992683635038488, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00027798733208328485}, {"id": 77, "seek": 42968, "start": 441.52, "end": 447.84000000000003, "text": " great, I'm gonna, you know, copy some code to the clipboard, paste it in as a data query to look", "tokens": [50956, 869, 11, 286, 478, 799, 11, 291, 458, 11, 5055, 512, 3089, 281, 264, 7353, 3787, 11, 9163, 309, 294, 382, 257, 1412, 14581, 281, 574, 51272], "temperature": 0.0, "avg_logprob": -0.09992683635038488, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00027798733208328485}, {"id": 78, "seek": 42968, "start": 447.84000000000003, "end": 453.28000000000003, "text": " at what the interactive selection should be, and then, you know, proceed with some other sort of", "tokens": [51272, 412, 437, 264, 15141, 9450, 820, 312, 11, 293, 550, 11, 291, 458, 11, 8991, 365, 512, 661, 1333, 295, 51544], "temperature": 0.0, "avg_logprob": -0.09992683635038488, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00027798733208328485}, {"id": 79, "seek": 42968, "start": 453.28000000000003, "end": 459.44, "text": " visual analysis. And so as we're sort of, you know, looking at these two forms of interaction,", "tokens": [51544, 5056, 5215, 13, 400, 370, 382, 321, 434, 1333, 295, 11, 291, 458, 11, 1237, 412, 613, 732, 6422, 295, 9285, 11, 51852], "temperature": 0.0, "avg_logprob": -0.09992683635038488, "compression_ratio": 1.7445255474452555, "no_speech_prob": 0.00027798733208328485}, {"id": 80, "seek": 45944, "start": 459.6, "end": 464.71999999999997, "text": " I was trying to figure out, well, some things feel the same, right? I've got that kind of", "tokens": [50372, 286, 390, 1382, 281, 2573, 484, 11, 731, 11, 512, 721, 841, 264, 912, 11, 558, 30, 286, 600, 658, 300, 733, 295, 50628], "temperature": 0.0, "avg_logprob": -0.14283074623297068, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0007319581927731633}, {"id": 81, "seek": 45944, "start": 464.71999999999997, "end": 469.68, "text": " conversational back and forth. I'm sure on the left-hand side with ChatGPT, it's a more natural", "tokens": [50628, 2615, 1478, 646, 293, 5220, 13, 286, 478, 988, 322, 264, 1411, 12, 5543, 1252, 365, 27503, 38, 47, 51, 11, 309, 311, 257, 544, 3303, 50876], "temperature": 0.0, "avg_logprob": -0.14283074623297068, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0007319581927731633}, {"id": 82, "seek": 45944, "start": 469.68, "end": 473.8, "text": " language conversation. On the right-hand side, it's more of a repel conversation. But also,", "tokens": [50876, 2856, 3761, 13, 1282, 264, 558, 12, 5543, 1252, 11, 309, 311, 544, 295, 257, 1085, 338, 3761, 13, 583, 611, 11, 51082], "temperature": 0.0, "avg_logprob": -0.14283074623297068, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0007319581927731633}, {"id": 83, "seek": 45944, "start": 473.8, "end": 479.48, "text": " things feel qualitatively different. And how do I actually kind of characterize what is the same", "tokens": [51082, 721, 841, 31312, 356, 819, 13, 400, 577, 360, 286, 767, 733, 295, 38463, 437, 307, 264, 912, 51366], "temperature": 0.0, "avg_logprob": -0.14283074623297068, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0007319581927731633}, {"id": 84, "seek": 45944, "start": 479.48, "end": 484.0, "text": " and what is the difference? And I thought really hard about it. And I realized that actually,", "tokens": [51366, 293, 437, 307, 264, 2649, 30, 400, 286, 1194, 534, 1152, 466, 309, 13, 400, 286, 5334, 300, 767, 11, 51592], "temperature": 0.0, "avg_logprob": -0.14283074623297068, "compression_ratio": 1.752808988764045, "no_speech_prob": 0.0007319581927731633}, {"id": 85, "seek": 48400, "start": 484.48, "end": 490.24, "text": " maybe what still matters is direct manipulation, right? And by direct manipulation, I don't mean", "tokens": [50388, 1310, 437, 920, 7001, 307, 2047, 26475, 11, 558, 30, 400, 538, 2047, 26475, 11, 286, 500, 380, 914, 50676], "temperature": 0.0, "avg_logprob": -0.0760291062512444, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006288125645369291}, {"id": 86, "seek": 48400, "start": 491.28, "end": 495.44, "text": " just the sort of Ben Schneiderman version of the term, which is, you know, associated with", "tokens": [50728, 445, 264, 1333, 295, 3964, 30343, 47811, 3037, 295, 264, 1433, 11, 597, 307, 11, 291, 458, 11, 6615, 365, 50936], "temperature": 0.0, "avg_logprob": -0.0760291062512444, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006288125645369291}, {"id": 87, "seek": 48400, "start": 495.44, "end": 500.96, "text": " graphical user interfaces and having a representation on screen that you can manipulate and undo", "tokens": [50936, 35942, 4195, 28416, 293, 1419, 257, 10290, 322, 2568, 300, 291, 393, 20459, 293, 23779, 51212], "temperature": 0.0, "avg_logprob": -0.0760291062512444, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006288125645369291}, {"id": 88, "seek": 48400, "start": 500.96, "end": 506.24, "text": " redo and things like that. But what I mean here is the deeper treatment of direct manipulation", "tokens": [51212, 29956, 293, 721, 411, 300, 13, 583, 437, 286, 914, 510, 307, 264, 7731, 5032, 295, 2047, 26475, 51476], "temperature": 0.0, "avg_logprob": -0.0760291062512444, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006288125645369291}, {"id": 89, "seek": 48400, "start": 506.24, "end": 511.68, "text": " that three cognitive scientists, Ed Hutchins, Jim Holland, and Don Norman, wrote in about", "tokens": [51476, 300, 1045, 15605, 7708, 11, 3977, 48499, 1292, 11, 6637, 27201, 11, 293, 1468, 30475, 11, 4114, 294, 466, 51748], "temperature": 0.0, "avg_logprob": -0.0760291062512444, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.006288125645369291}, {"id": 90, "seek": 51168, "start": 511.68, "end": 518.64, "text": " the mid-1980s. So in particular, in Hutchins et al's treatment of direct manipulation, they sort", "tokens": [50364, 264, 2062, 12, 3405, 4702, 82, 13, 407, 294, 1729, 11, 294, 48499, 1292, 1030, 419, 311, 5032, 295, 2047, 26475, 11, 436, 1333, 50712], "temperature": 0.0, "avg_logprob": -0.0885806187339451, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0006461208104155958}, {"id": 91, "seek": 51168, "start": 518.64, "end": 524.32, "text": " of, you know, imagine direct manipulation to be this cognitive process between a user's goals", "tokens": [50712, 295, 11, 291, 458, 11, 3811, 2047, 26475, 281, 312, 341, 15605, 1399, 1296, 257, 4195, 311, 5493, 50996], "temperature": 0.0, "avg_logprob": -0.0885806187339451, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0006461208104155958}, {"id": 92, "seek": 51168, "start": 524.32, "end": 530.88, "text": " and the user interface. And, you know, they identify this gulf of execution that exists when a user", "tokens": [50996, 293, 264, 4195, 9226, 13, 400, 11, 291, 458, 11, 436, 5876, 341, 290, 5757, 295, 15058, 300, 8198, 562, 257, 4195, 51324], "temperature": 0.0, "avg_logprob": -0.0885806187339451, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0006461208104155958}, {"id": 93, "seek": 51168, "start": 530.88, "end": 536.8, "text": " has to translate their goals into commands that they execute on the user interface. And similarly,", "tokens": [51324, 575, 281, 13799, 641, 5493, 666, 16901, 300, 436, 14483, 322, 264, 4195, 9226, 13, 400, 14138, 11, 51620], "temperature": 0.0, "avg_logprob": -0.0885806187339451, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0006461208104155958}, {"id": 94, "seek": 53680, "start": 536.88, "end": 542.3199999999999, "text": " a return gulf of evaluation when a user has to figure out, well, did the UI do the thing that", "tokens": [50368, 257, 2736, 290, 5757, 295, 13344, 562, 257, 4195, 575, 281, 2573, 484, 11, 731, 11, 630, 264, 15682, 360, 264, 551, 300, 50640], "temperature": 0.0, "avg_logprob": -0.05607268316992398, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.0035930376034229994}, {"id": 95, "seek": 53680, "start": 542.3199999999999, "end": 546.7199999999999, "text": " I was expecting it to do? Right? I'm seeing a lot of nods in the audience because, you know, if you've", "tokens": [50640, 286, 390, 9650, 309, 281, 360, 30, 1779, 30, 286, 478, 2577, 257, 688, 295, 15224, 82, 294, 264, 4034, 570, 11, 291, 458, 11, 498, 291, 600, 50860], "temperature": 0.0, "avg_logprob": -0.05607268316992398, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.0035930376034229994}, {"id": 96, "seek": 53680, "start": 546.7199999999999, "end": 552.88, "text": " had experience in user interaction design, user experience, you've maybe experienced these terms", "tokens": [50860, 632, 1752, 294, 4195, 9285, 1715, 11, 4195, 1752, 11, 291, 600, 1310, 6751, 613, 2115, 51168], "temperature": 0.0, "avg_logprob": -0.05607268316992398, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.0035930376034229994}, {"id": 97, "seek": 53680, "start": 552.88, "end": 558.88, "text": " gulf of evaluation and execution. But what I find interesting in this 1985 paper is that they went", "tokens": [51168, 290, 5757, 295, 13344, 293, 15058, 13, 583, 437, 286, 915, 1880, 294, 341, 28962, 3035, 307, 300, 436, 1437, 51468], "temperature": 0.0, "avg_logprob": -0.05607268316992398, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.0035930376034229994}, {"id": 98, "seek": 53680, "start": 558.88, "end": 564.9599999999999, "text": " one level deeper. So in particular, they identified this idea of a semantic distance,", "tokens": [51468, 472, 1496, 7731, 13, 407, 294, 1729, 11, 436, 9234, 341, 1558, 295, 257, 47982, 4560, 11, 51772], "temperature": 0.0, "avg_logprob": -0.05607268316992398, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.0035930376034229994}, {"id": 99, "seek": 56496, "start": 564.96, "end": 572.64, "text": " which is basically how users take the fuzzy notions in their head and translate those into", "tokens": [50364, 597, 307, 1936, 577, 5022, 747, 264, 34710, 35799, 294, 641, 1378, 293, 13799, 729, 666, 50748], "temperature": 0.0, "avg_logprob": -0.08880891118730817, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.00031499608303420246}, {"id": 100, "seek": 56496, "start": 572.64, "end": 579.6800000000001, "text": " the nouns and verbs of the user interface, right? So going, doing that sort of sense-meaning operation", "tokens": [50748, 264, 48184, 293, 30051, 295, 264, 4195, 9226, 11, 558, 30, 407, 516, 11, 884, 300, 1333, 295, 2020, 12, 1398, 8415, 6916, 51100], "temperature": 0.0, "avg_logprob": -0.08880891118730817, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.00031499608303420246}, {"id": 101, "seek": 56496, "start": 579.6800000000001, "end": 586.32, "text": " of transforming your intentions into the particular actions that might exist in the user interface.", "tokens": [51100, 295, 27210, 428, 19354, 666, 264, 1729, 5909, 300, 1062, 2514, 294, 264, 4195, 9226, 13, 51432], "temperature": 0.0, "avg_logprob": -0.08880891118730817, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.00031499608303420246}, {"id": 102, "seek": 56496, "start": 587.12, "end": 591.6, "text": " And in addition to the semantic distance, they identified what I love. I love this term in", "tokens": [51472, 400, 294, 4500, 281, 264, 47982, 4560, 11, 436, 9234, 437, 286, 959, 13, 286, 959, 341, 1433, 294, 51696], "temperature": 0.0, "avg_logprob": -0.08880891118730817, "compression_ratio": 1.6991150442477876, "no_speech_prob": 0.00031499608303420246}, {"id": 103, "seek": 59160, "start": 591.6, "end": 596.72, "text": " articulatory distance, right? So it's not necessarily the meaning that we care about anymore,", "tokens": [50364, 15228, 425, 4745, 4560, 11, 558, 30, 407, 309, 311, 406, 4725, 264, 3620, 300, 321, 1127, 466, 3602, 11, 50620], "temperature": 0.0, "avg_logprob": -0.05699574738218073, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.0031721601262688637}, {"id": 104, "seek": 59160, "start": 596.72, "end": 602.08, "text": " but the way in which we're conveying that meaning through the UI. And this is particularly important", "tokens": [50620, 457, 264, 636, 294, 597, 321, 434, 18053, 1840, 300, 3620, 807, 264, 15682, 13, 400, 341, 307, 4098, 1021, 50888], "temperature": 0.0, "avg_logprob": -0.05699574738218073, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.0031721601262688637}, {"id": 105, "seek": 59160, "start": 602.08, "end": 608.4, "text": " because you might have several user interfaces that all express the same semantics, right? You can", "tokens": [50888, 570, 291, 1062, 362, 2940, 4195, 28416, 300, 439, 5109, 264, 912, 4361, 45298, 11, 558, 30, 509, 393, 51204], "temperature": 0.0, "avg_logprob": -0.05699574738218073, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.0031721601262688637}, {"id": 106, "seek": 59160, "start": 608.4, "end": 614.72, "text": " conduct the same set of, you know, operations with them, the same nouns and verbs. But the way you", "tokens": [51204, 6018, 264, 912, 992, 295, 11, 291, 458, 11, 7705, 365, 552, 11, 264, 912, 48184, 293, 30051, 13, 583, 264, 636, 291, 51520], "temperature": 0.0, "avg_logprob": -0.05699574738218073, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.0031721601262688637}, {"id": 107, "seek": 59160, "start": 614.72, "end": 619.6, "text": " do that might be different because one interface might be graphical, the other one might be textual,", "tokens": [51520, 360, 300, 1062, 312, 819, 570, 472, 9226, 1062, 312, 35942, 11, 264, 661, 472, 1062, 312, 2487, 901, 11, 51764], "temperature": 0.0, "avg_logprob": -0.05699574738218073, "compression_ratio": 1.779783393501805, "no_speech_prob": 0.0031721601262688637}, {"id": 108, "seek": 61960, "start": 619.6, "end": 624.72, "text": " another one might be conversational, gesture-oriented, etc. And their claim in this paper was that", "tokens": [50364, 1071, 472, 1062, 312, 2615, 1478, 11, 22252, 12, 27414, 11, 5183, 13, 400, 641, 3932, 294, 341, 3035, 390, 300, 50620], "temperature": 0.0, "avg_logprob": -0.0678695595782736, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.00018519345030654222}, {"id": 109, "seek": 61960, "start": 624.72, "end": 630.8000000000001, "text": " that articulation, the form of that meaning is really, really important, just as important as", "tokens": [50620, 300, 15228, 2776, 11, 264, 1254, 295, 300, 3620, 307, 534, 11, 534, 1021, 11, 445, 382, 1021, 382, 50924], "temperature": 0.0, "avg_logprob": -0.0678695595782736, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.00018519345030654222}, {"id": 110, "seek": 61960, "start": 630.8000000000001, "end": 636.16, "text": " the semantics. And of course, these distances exist on the Gulf of Evaluation as well. So the", "tokens": [50924, 264, 4361, 45298, 13, 400, 295, 1164, 11, 613, 22182, 2514, 322, 264, 23033, 295, 462, 46504, 382, 731, 13, 407, 264, 51192], "temperature": 0.0, "avg_logprob": -0.0678695595782736, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.00018519345030654222}, {"id": 111, "seek": 61960, "start": 636.16, "end": 642.88, "text": " articulatory distance is how do I perceive the changes that occurred in the UI and start to bring", "tokens": [51192, 15228, 425, 4745, 4560, 307, 577, 360, 286, 20281, 264, 2962, 300, 11068, 294, 264, 15682, 293, 722, 281, 1565, 51528], "temperature": 0.0, "avg_logprob": -0.0678695595782736, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.00018519345030654222}, {"id": 112, "seek": 64288, "start": 642.88, "end": 650.64, "text": " meaning to that perceptual operation by interpreting and evaluating the degree to which", "tokens": [50364, 3620, 281, 300, 43276, 901, 6916, 538, 37395, 293, 27479, 264, 4314, 281, 597, 50752], "temperature": 0.0, "avg_logprob": -0.06477968801151622, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.008844863623380661}, {"id": 113, "seek": 64288, "start": 650.64, "end": 656.64, "text": " they met my goals. So this is actually going to give us the kind of conceptual machinery for", "tokens": [50752, 436, 1131, 452, 5493, 13, 407, 341, 307, 767, 516, 281, 976, 505, 264, 733, 295, 24106, 27302, 337, 51052], "temperature": 0.0, "avg_logprob": -0.06477968801151622, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.008844863623380661}, {"id": 114, "seek": 64288, "start": 656.64, "end": 661.92, "text": " the rest of the talk. And it's a little bit dense. And so I want to return to sort of the prior two", "tokens": [51052, 264, 1472, 295, 264, 751, 13, 400, 309, 311, 257, 707, 857, 18011, 13, 400, 370, 286, 528, 281, 2736, 281, 1333, 295, 264, 4059, 732, 51316], "temperature": 0.0, "avg_logprob": -0.06477968801151622, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.008844863623380661}, {"id": 115, "seek": 64288, "start": 661.92, "end": 667.6, "text": " examples and think about how they manifest these two kinds of distances. So in the case of the", "tokens": [51316, 5110, 293, 519, 466, 577, 436, 10067, 613, 732, 3685, 295, 22182, 13, 407, 294, 264, 1389, 295, 264, 51600], "temperature": 0.0, "avg_logprob": -0.06477968801151622, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.008844863623380661}, {"id": 116, "seek": 66760, "start": 667.6, "end": 672.5600000000001, "text": " chat GPT example, you know, if we start with semantic distance, I would say that, well, the", "tokens": [50364, 5081, 26039, 51, 1365, 11, 291, 458, 11, 498, 321, 722, 365, 47982, 4560, 11, 286, 576, 584, 300, 11, 731, 11, 264, 50612], "temperature": 0.0, "avg_logprob": -0.07536562796561949, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.01132938638329506}, {"id": 117, "seek": 66760, "start": 672.5600000000001, "end": 677.6800000000001, "text": " semantics aren't really well defined, right? They're not really explicit. Because what these models", "tokens": [50612, 4361, 45298, 3212, 380, 534, 731, 7642, 11, 558, 30, 814, 434, 406, 534, 13691, 13, 1436, 437, 613, 5245, 50868], "temperature": 0.0, "avg_logprob": -0.07536562796561949, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.01132938638329506}, {"id": 118, "seek": 66760, "start": 677.6800000000001, "end": 683.84, "text": " have done is they've learned over, you know, vast corpuses of text, often just text that is present", "tokens": [50868, 362, 1096, 307, 436, 600, 3264, 670, 11, 291, 458, 11, 8369, 1181, 79, 8355, 295, 2487, 11, 2049, 445, 2487, 300, 307, 1974, 51176], "temperature": 0.0, "avg_logprob": -0.07536562796561949, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.01132938638329506}, {"id": 119, "seek": 66760, "start": 683.84, "end": 688.96, "text": " on the internet. And so what they've learned is this latent space that is very ambiguous in the", "tokens": [51176, 322, 264, 4705, 13, 400, 370, 437, 436, 600, 3264, 307, 341, 48994, 1901, 300, 307, 588, 39465, 294, 264, 51432], "temperature": 0.0, "avg_logprob": -0.07536562796561949, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.01132938638329506}, {"id": 120, "seek": 66760, "start": 688.96, "end": 694.24, "text": " semantics that are encoded in that latent space. So as a user, it's hard for me to know how to", "tokens": [51432, 4361, 45298, 300, 366, 2058, 12340, 294, 300, 48994, 1901, 13, 407, 382, 257, 4195, 11, 309, 311, 1152, 337, 385, 281, 458, 577, 281, 51696], "temperature": 0.0, "avg_logprob": -0.07536562796561949, "compression_ratio": 1.7214285714285715, "no_speech_prob": 0.01132938638329506}, {"id": 121, "seek": 69424, "start": 694.32, "end": 699.36, "text": " translate my intentions into something that the system can understand because I don't know what", "tokens": [50368, 13799, 452, 19354, 666, 746, 300, 264, 1185, 393, 1223, 570, 286, 500, 380, 458, 437, 50620], "temperature": 0.0, "avg_logprob": -0.06501281470583196, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0019263885915279388}, {"id": 122, "seek": 69424, "start": 699.36, "end": 706.72, "text": " it is the system knows about the world. But as I'm sure many of us are aware, like prompt engineering", "tokens": [50620, 309, 307, 264, 1185, 3255, 466, 264, 1002, 13, 583, 382, 286, 478, 988, 867, 295, 505, 366, 3650, 11, 411, 12391, 7043, 50988], "temperature": 0.0, "avg_logprob": -0.06501281470583196, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0019263885915279388}, {"id": 123, "seek": 69424, "start": 706.72, "end": 712.4, "text": " is a thing, right? So if I figure out exactly how to craft my, you know, natural language", "tokens": [50988, 307, 257, 551, 11, 558, 30, 407, 498, 286, 2573, 484, 2293, 577, 281, 8448, 452, 11, 291, 458, 11, 3303, 2856, 51272], "temperature": 0.0, "avg_logprob": -0.06501281470583196, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0019263885915279388}, {"id": 124, "seek": 69424, "start": 712.4, "end": 717.76, "text": " expression, suddenly I can get the model to very rapidly almost zero shot adopt the semantics that", "tokens": [51272, 6114, 11, 5800, 286, 393, 483, 264, 2316, 281, 588, 12910, 1920, 4018, 3347, 6878, 264, 4361, 45298, 300, 51540], "temperature": 0.0, "avg_logprob": -0.06501281470583196, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0019263885915279388}, {"id": 125, "seek": 69424, "start": 717.76, "end": 723.12, "text": " I want. And that feels like a very powerful, you know, affordance that we've not necessarily had", "tokens": [51540, 286, 528, 13, 400, 300, 3417, 411, 257, 588, 4005, 11, 291, 458, 11, 6157, 719, 300, 321, 600, 406, 4725, 632, 51808], "temperature": 0.0, "avg_logprob": -0.06501281470583196, "compression_ratio": 1.6372881355932203, "no_speech_prob": 0.0019263885915279388}, {"id": 126, "seek": 72312, "start": 723.12, "end": 730.72, "text": " before. On the other side, you know, the semantic distance in the Jupiter notebook in B2 had explicitly", "tokens": [50364, 949, 13, 1282, 264, 661, 1252, 11, 291, 458, 11, 264, 47982, 4560, 294, 264, 24567, 21060, 294, 363, 17, 632, 20803, 50744], "temperature": 0.0, "avg_logprob": -0.06661871129816228, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.00024532192037440836}, {"id": 127, "seek": 72312, "start": 730.72, "end": 735.6, "text": " defined semantics, right? We have the explicit semantics of pandas on the data frame of the", "tokens": [50744, 7642, 4361, 45298, 11, 558, 30, 492, 362, 264, 13691, 4361, 45298, 295, 4565, 296, 322, 264, 1412, 3920, 295, 264, 50988], "temperature": 0.0, "avg_logprob": -0.06661871129816228, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.00024532192037440836}, {"id": 128, "seek": 72312, "start": 735.6, "end": 740.96, "text": " visualization library of being able to click on the fields in a graphical user interface to produce", "tokens": [50988, 25801, 6405, 295, 885, 1075, 281, 2052, 322, 264, 7909, 294, 257, 35942, 4195, 9226, 281, 5258, 51256], "temperature": 0.0, "avg_logprob": -0.06661871129816228, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.00024532192037440836}, {"id": 129, "seek": 72312, "start": 740.96, "end": 747.04, "text": " visualizations. And every time I did that, I had the shared representation of the code. So either I", "tokens": [51256, 5056, 14455, 13, 400, 633, 565, 286, 630, 300, 11, 286, 632, 264, 5507, 10290, 295, 264, 3089, 13, 407, 2139, 286, 51560], "temperature": 0.0, "avg_logprob": -0.06661871129816228, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.00024532192037440836}, {"id": 130, "seek": 72312, "start": 747.04, "end": 750.48, "text": " would offer the code and it would produce a visualization or if the system produced some", "tokens": [51560, 576, 2626, 264, 3089, 293, 309, 576, 5258, 257, 25801, 420, 498, 264, 1185, 7126, 512, 51732], "temperature": 0.0, "avg_logprob": -0.06661871129816228, "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.00024532192037440836}, {"id": 131, "seek": 75048, "start": 750.48, "end": 756.32, "text": " code, I could go in and comment and uncomment entries or tweak the code in a particular way", "tokens": [50364, 3089, 11, 286, 727, 352, 294, 293, 2871, 293, 8585, 518, 23041, 420, 29879, 264, 3089, 294, 257, 1729, 636, 50656], "temperature": 0.0, "avg_logprob": -0.08128013721732207, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0003249945002608001}, {"id": 132, "seek": 75048, "start": 756.32, "end": 761.44, "text": " and things like that. And so it gave me the shared representation that allowed me to bridge", "tokens": [50656, 293, 721, 411, 300, 13, 400, 370, 309, 2729, 385, 264, 5507, 10290, 300, 4350, 385, 281, 7283, 50912], "temperature": 0.0, "avg_logprob": -0.08128013721732207, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0003249945002608001}, {"id": 133, "seek": 75048, "start": 761.44, "end": 769.2, "text": " between input and output mechanisms really, really easily. With articulatory distance in chat", "tokens": [50912, 1296, 4846, 293, 5598, 15902, 534, 11, 534, 3612, 13, 2022, 15228, 425, 4745, 4560, 294, 5081, 51300], "temperature": 0.0, "avg_logprob": -0.08128013721732207, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0003249945002608001}, {"id": 134, "seek": 75048, "start": 769.2, "end": 776.0, "text": " GPT, right, natural language, it's been enormously powerful because it's reduced the sort of learning", "tokens": [51300, 26039, 51, 11, 558, 11, 3303, 2856, 11, 309, 311, 668, 39669, 4005, 570, 309, 311, 9212, 264, 1333, 295, 2539, 51640], "temperature": 0.0, "avg_logprob": -0.08128013721732207, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0003249945002608001}, {"id": 135, "seek": 77600, "start": 776.08, "end": 781.28, "text": " threshold for a lot of things, right? So if I don't know exactly what it is I want or how to", "tokens": [50368, 14678, 337, 257, 688, 295, 721, 11, 558, 30, 407, 498, 286, 500, 380, 458, 2293, 437, 309, 307, 286, 528, 420, 577, 281, 50628], "temperature": 0.0, "avg_logprob": -0.058100649651060716, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01098429411649704}, {"id": 136, "seek": 77600, "start": 781.28, "end": 787.44, "text": " sort of pose it to the question, I can lean into the ambiguity of natural language and chat GPT", "tokens": [50628, 1333, 295, 10774, 309, 281, 264, 1168, 11, 286, 393, 11659, 666, 264, 46519, 295, 3303, 2856, 293, 5081, 26039, 51, 50936], "temperature": 0.0, "avg_logprob": -0.058100649651060716, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01098429411649704}, {"id": 137, "seek": 77600, "start": 787.44, "end": 793.76, "text": " catches up to my intentions pretty rapidly, which is great. But conversely, sometimes I know exactly", "tokens": [50936, 25496, 493, 281, 452, 19354, 1238, 12910, 11, 597, 307, 869, 13, 583, 2615, 736, 11, 2171, 286, 458, 2293, 51252], "temperature": 0.0, "avg_logprob": -0.058100649651060716, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01098429411649704}, {"id": 138, "seek": 77600, "start": 793.76, "end": 799.76, "text": " what it is I want. And it's really frustrating to have to express precise operations through the", "tokens": [51252, 437, 309, 307, 286, 528, 13, 400, 309, 311, 534, 16522, 281, 362, 281, 5109, 13600, 7705, 807, 264, 51552], "temperature": 0.0, "avg_logprob": -0.058100649651060716, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.01098429411649704}, {"id": 139, "seek": 79976, "start": 799.76, "end": 806.0, "text": " ambiguity of natural language. And then as a result, because of the fact that natural language", "tokens": [50364, 46519, 295, 3303, 2856, 13, 400, 550, 382, 257, 1874, 11, 570, 295, 264, 1186, 300, 3303, 2856, 50676], "temperature": 0.0, "avg_logprob": -0.05795559176692256, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.017975647002458572}, {"id": 140, "seek": 79976, "start": 806.0, "end": 810.3199999999999, "text": " is the only mechanism so far by which we can interact with many of these models, there's a", "tokens": [50676, 307, 264, 787, 7513, 370, 1400, 538, 597, 321, 393, 4648, 365, 867, 295, 613, 5245, 11, 456, 311, 257, 50892], "temperature": 0.0, "avg_logprob": -0.05795559176692256, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.017975647002458572}, {"id": 141, "seek": 79976, "start": 810.3199999999999, "end": 815.76, "text": " disconnect if your output is visual, like the case of visualization. So I can't interact with", "tokens": [50892, 14299, 498, 428, 5598, 307, 5056, 11, 411, 264, 1389, 295, 25801, 13, 407, 286, 393, 380, 4648, 365, 51164], "temperature": 0.0, "avg_logprob": -0.05795559176692256, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.017975647002458572}, {"id": 142, "seek": 79976, "start": 815.76, "end": 821.04, "text": " the visualizations in any way to do subsequent back and forth interactions with the model. Now,", "tokens": [51164, 264, 5056, 14455, 294, 604, 636, 281, 360, 19962, 646, 293, 5220, 13280, 365, 264, 2316, 13, 823, 11, 51428], "temperature": 0.0, "avg_logprob": -0.05795559176692256, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.017975647002458572}, {"id": 143, "seek": 79976, "start": 821.04, "end": 825.28, "text": " I don't think the second point is sort of a fundamental limitation, but it's certainly, you", "tokens": [51428, 286, 500, 380, 519, 264, 1150, 935, 307, 1333, 295, 257, 8088, 27432, 11, 457, 309, 311, 3297, 11, 291, 51640], "temperature": 0.0, "avg_logprob": -0.05795559176692256, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.017975647002458572}, {"id": 144, "seek": 82528, "start": 825.28, "end": 831.28, "text": " know, the state of where we are today. And on the other hand, with, you know, Jupiter Notebook and", "tokens": [50364, 458, 11, 264, 1785, 295, 689, 321, 366, 965, 13, 400, 322, 264, 661, 1011, 11, 365, 11, 291, 458, 11, 24567, 11633, 2939, 293, 50664], "temperature": 0.0, "avg_logprob": -0.10058209203904675, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0019871273543685675}, {"id": 145, "seek": 82528, "start": 831.28, "end": 836.48, "text": " B2, with the articulatory distance, we've got basically the inverse of this, right? We've got a", "tokens": [50664, 363, 17, 11, 365, 264, 15228, 425, 4745, 4560, 11, 321, 600, 658, 1936, 264, 17340, 295, 341, 11, 558, 30, 492, 600, 658, 257, 50924], "temperature": 0.0, "avg_logprob": -0.10058209203904675, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0019871273543685675}, {"id": 146, "seek": 82528, "start": 836.48, "end": 842.16, "text": " nice precise programmatic syntax. So if I know that syntax, I can work really, really efficiently,", "tokens": [50924, 1481, 13600, 1461, 25915, 28431, 13, 407, 498, 286, 458, 300, 28431, 11, 286, 393, 589, 534, 11, 534, 19621, 11, 51208], "temperature": 0.0, "avg_logprob": -0.10058209203904675, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0019871273543685675}, {"id": 147, "seek": 82528, "start": 842.16, "end": 848.56, "text": " right? Sort of a common affordance of many sort of command line style interfaces. But I really", "tokens": [51208, 558, 30, 26149, 295, 257, 2689, 6157, 719, 295, 867, 1333, 295, 5622, 1622, 3758, 28416, 13, 583, 286, 534, 51528], "temperature": 0.0, "avg_logprob": -0.10058209203904675, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0019871273543685675}, {"id": 148, "seek": 82528, "start": 848.56, "end": 854.3199999999999, "text": " need to learn that syntax to be effective. And in some cases with pearly design syntaxes, which", "tokens": [51528, 643, 281, 1466, 300, 28431, 281, 312, 4942, 13, 400, 294, 512, 3331, 365, 37320, 356, 1715, 28431, 279, 11, 597, 51816], "temperature": 0.0, "avg_logprob": -0.10058209203904675, "compression_ratio": 1.6689655172413793, "no_speech_prob": 0.0019871273543685675}, {"id": 149, "seek": 85432, "start": 854.4000000000001, "end": 859.84, "text": " I might maybe argue, Pandas is an example of, right? I constantly have to look up the documentation", "tokens": [50368, 286, 1062, 1310, 9695, 11, 16995, 296, 307, 364, 1365, 295, 11, 558, 30, 286, 6460, 362, 281, 574, 493, 264, 14333, 50640], "temperature": 0.0, "avg_logprob": -0.12649419148763022, "compression_ratio": 1.4427860696517414, "no_speech_prob": 0.0006460660952143371}, {"id": 150, "seek": 85432, "start": 859.84, "end": 864.8000000000001, "text": " for, right? There's a learning curve associated with it that slows people down. Yeah, Michael.", "tokens": [50640, 337, 11, 558, 30, 821, 311, 257, 2539, 7605, 6615, 365, 309, 300, 35789, 561, 760, 13, 865, 11, 5116, 13, 50888], "temperature": 0.0, "avg_logprob": -0.12649419148763022, "compression_ratio": 1.4427860696517414, "no_speech_prob": 0.0006460660952143371}, {"id": 151, "seek": 85432, "start": 877.44, "end": 882.0, "text": " Yeah, so the reason I put it, I think this is a great question, you know, what lies in semantic", "tokens": [51520, 865, 11, 370, 264, 1778, 286, 829, 309, 11, 286, 519, 341, 307, 257, 869, 1168, 11, 291, 458, 11, 437, 9134, 294, 47982, 51748], "temperature": 0.0, "avg_logprob": -0.12649419148763022, "compression_ratio": 1.4427860696517414, "no_speech_prob": 0.0006460660952143371}, {"id": 152, "seek": 88200, "start": 882.0, "end": 887.04, "text": " and articulatory. And oftentimes it is quite a fuzzy distinction. The reason I put this in", "tokens": [50364, 293, 15228, 425, 4745, 13, 400, 18349, 309, 307, 1596, 257, 34710, 16844, 13, 440, 1778, 286, 829, 341, 294, 50616], "temperature": 0.0, "avg_logprob": -0.08246697918061287, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.008573794737458229}, {"id": 153, "seek": 88200, "start": 887.04, "end": 893.12, "text": " articulatory is my experience with Pandas oftentimes is I know what it is I want to do, right? I know", "tokens": [50616, 15228, 425, 4745, 307, 452, 1752, 365, 16995, 296, 18349, 307, 286, 458, 437, 309, 307, 286, 528, 281, 360, 11, 558, 30, 286, 458, 50920], "temperature": 0.0, "avg_logprob": -0.08246697918061287, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.008573794737458229}, {"id": 154, "seek": 88200, "start": 893.12, "end": 897.92, "text": " the sort of operation I want to perform on my, on my data frame. I just don't know the specific", "tokens": [50920, 264, 1333, 295, 6916, 286, 528, 281, 2042, 322, 452, 11, 322, 452, 1412, 3920, 13, 286, 445, 500, 380, 458, 264, 2685, 51160], "temperature": 0.0, "avg_logprob": -0.08246697918061287, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.008573794737458229}, {"id": 155, "seek": 88200, "start": 897.92, "end": 904.88, "text": " syntax that I need to look up. Exactly, exactly. But certainly, you know, if, if you don't know", "tokens": [51160, 28431, 300, 286, 643, 281, 574, 493, 13, 7587, 11, 2293, 13, 583, 3297, 11, 291, 458, 11, 498, 11, 498, 291, 500, 380, 458, 51508], "temperature": 0.0, "avg_logprob": -0.08246697918061287, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.008573794737458229}, {"id": 156, "seek": 88200, "start": 904.88, "end": 909.28, "text": " what it is you want to do, then the affordances of natural language absolutely help because you", "tokens": [51508, 437, 309, 307, 291, 528, 281, 360, 11, 550, 264, 6157, 2676, 295, 3303, 2856, 3122, 854, 570, 291, 51728], "temperature": 0.0, "avg_logprob": -0.08246697918061287, "compression_ratio": 1.7328519855595668, "no_speech_prob": 0.008573794737458229}, {"id": 157, "seek": 90928, "start": 909.28, "end": 914.9599999999999, "text": " can kind of, you know, pose things in really fuzzy ways and, and kind of iterate towards your outcome.", "tokens": [50364, 393, 733, 295, 11, 291, 458, 11, 10774, 721, 294, 534, 34710, 2098, 293, 11, 293, 733, 295, 44497, 3030, 428, 9700, 13, 50648], "temperature": 0.0, "avg_logprob": -0.0883716548885311, "compression_ratio": 1.8458646616541354, "no_speech_prob": 0.00010552916501183063}, {"id": 158, "seek": 90928, "start": 917.6, "end": 921.8399999999999, "text": " And, and I think you see some of this ambiguity in, in sort of, you know, the distinction between", "tokens": [50780, 400, 11, 293, 286, 519, 291, 536, 512, 295, 341, 46519, 294, 11, 294, 1333, 295, 11, 291, 458, 11, 264, 16844, 1296, 50992], "temperature": 0.0, "avg_logprob": -0.0883716548885311, "compression_ratio": 1.8458646616541354, "no_speech_prob": 0.00010552916501183063}, {"id": 159, "seek": 90928, "start": 921.8399999999999, "end": 927.28, "text": " semantic and articulatory distance here with this, this last point where because there are consistent", "tokens": [50992, 47982, 293, 15228, 425, 4745, 4560, 510, 365, 341, 11, 341, 1036, 935, 689, 570, 456, 366, 8398, 51264], "temperature": 0.0, "avg_logprob": -0.0883716548885311, "compression_ratio": 1.8458646616541354, "no_speech_prob": 0.00010552916501183063}, {"id": 160, "seek": 90928, "start": 927.28, "end": 933.68, "text": " semantics that actually has this knock on effect on the articulation because now there's a shared", "tokens": [51264, 4361, 45298, 300, 767, 575, 341, 6728, 322, 1802, 322, 264, 15228, 2776, 570, 586, 456, 311, 257, 5507, 51584], "temperature": 0.0, "avg_logprob": -0.0883716548885311, "compression_ratio": 1.8458646616541354, "no_speech_prob": 0.00010552916501183063}, {"id": 161, "seek": 90928, "start": 933.68, "end": 938.72, "text": " representation of input and output and that simplifies that articulatory distance as well.", "tokens": [51584, 10290, 295, 4846, 293, 5598, 293, 300, 6883, 11221, 300, 15228, 425, 4745, 4560, 382, 731, 13, 51836], "temperature": 0.0, "avg_logprob": -0.0883716548885311, "compression_ratio": 1.8458646616541354, "no_speech_prob": 0.00010552916501183063}, {"id": 162, "seek": 93928, "start": 939.28, "end": 944.9599999999999, "text": " So there's not quite that disconnect that we see on the chat GPT side. And so, you know, that's a,", "tokens": [50364, 407, 456, 311, 406, 1596, 300, 14299, 300, 321, 536, 322, 264, 5081, 26039, 51, 1252, 13, 400, 370, 11, 291, 458, 11, 300, 311, 257, 11, 50648], "temperature": 0.0, "avg_logprob": -0.096265198634221, "compression_ratio": 1.7282229965156795, "no_speech_prob": 9.026177576743066e-05}, {"id": 163, "seek": 93928, "start": 944.9599999999999, "end": 951.1999999999999, "text": " that's, you know, I found semantic and articulatory distances to be a really helpful sort of framework", "tokens": [50648, 300, 311, 11, 291, 458, 11, 286, 1352, 47982, 293, 15228, 425, 4745, 22182, 281, 312, 257, 534, 4961, 1333, 295, 8388, 50960], "temperature": 0.0, "avg_logprob": -0.096265198634221, "compression_ratio": 1.7282229965156795, "no_speech_prob": 9.026177576743066e-05}, {"id": 164, "seek": 93928, "start": 951.1999999999999, "end": 957.8399999999999, "text": " and I wanted to use it to sort of analyze the very last step in the output that that demo produced. So", "tokens": [50960, 293, 286, 1415, 281, 764, 309, 281, 1333, 295, 12477, 264, 588, 1036, 1823, 294, 264, 5598, 300, 300, 10723, 7126, 13, 407, 51292], "temperature": 0.0, "avg_logprob": -0.096265198634221, "compression_ratio": 1.7282229965156795, "no_speech_prob": 9.026177576743066e-05}, {"id": 165, "seek": 93928, "start": 957.8399999999999, "end": 962.9599999999999, "text": " it, it, you know, it's basically this, this thing that masquerades as an explanation of the", "tokens": [51292, 309, 11, 309, 11, 291, 458, 11, 309, 311, 1936, 341, 11, 341, 551, 300, 2300, 8035, 2977, 382, 364, 10835, 295, 264, 51548], "temperature": 0.0, "avg_logprob": -0.096265198634221, "compression_ratio": 1.7282229965156795, "no_speech_prob": 9.026177576743066e-05}, {"id": 166, "seek": 93928, "start": 962.9599999999999, "end": 968.4, "text": " visualizations that chat GPT produced. But if you actually look at what it says, right, here's some", "tokens": [51548, 5056, 14455, 300, 5081, 26039, 51, 7126, 13, 583, 498, 291, 767, 574, 412, 437, 309, 1619, 11, 558, 11, 510, 311, 512, 51820], "temperature": 0.0, "avg_logprob": -0.096265198634221, "compression_ratio": 1.7282229965156795, "no_speech_prob": 9.026177576743066e-05}, {"id": 167, "seek": 96840, "start": 968.4, "end": 973.52, "text": " basic visualizations. Number one, a histogram of song durations colon. This shows the distribution", "tokens": [50364, 3875, 5056, 14455, 13, 5118, 472, 11, 257, 49816, 295, 2153, 4861, 763, 8255, 13, 639, 3110, 264, 7316, 50620], "temperature": 0.0, "avg_logprob": -0.11277917453220912, "compression_ratio": 1.89453125, "no_speech_prob": 0.001925980788655579}, {"id": 168, "seek": 96840, "start": 973.52, "end": 978.72, "text": " of song durations in seconds. All right, fair enough. Scatter plot of song hotness versus artist", "tokens": [50620, 295, 2153, 4861, 763, 294, 3949, 13, 1057, 558, 11, 3143, 1547, 13, 2747, 1161, 7542, 295, 2153, 2368, 1287, 5717, 5748, 50880], "temperature": 0.0, "avg_logprob": -0.11277917453220912, "compression_ratio": 1.89453125, "no_speech_prob": 0.001925980788655579}, {"id": 169, "seek": 96840, "start": 978.72, "end": 983.6, "text": " familiarity. This shows the relationship between song hotness and artist familiarity. Well, I would", "tokens": [50880, 49828, 13, 639, 3110, 264, 2480, 1296, 2153, 2368, 1287, 293, 5748, 49828, 13, 1042, 11, 286, 576, 51124], "temperature": 0.0, "avg_logprob": -0.11277917453220912, "compression_ratio": 1.89453125, "no_speech_prob": 0.001925980788655579}, {"id": 170, "seek": 96840, "start": 983.6, "end": 989.12, "text": " hope so. And then bar chart of the top 10 most frequent artist names. This shows the top 10 most", "tokens": [51124, 1454, 370, 13, 400, 550, 2159, 6927, 295, 264, 1192, 1266, 881, 18004, 5748, 5288, 13, 639, 3110, 264, 1192, 1266, 881, 51400], "temperature": 0.0, "avg_logprob": -0.11277917453220912, "compression_ratio": 1.89453125, "no_speech_prob": 0.001925980788655579}, {"id": 171, "seek": 96840, "start": 989.12, "end": 995.28, "text": " frequent artist names in the data set, right. These are not really explanations, but they're", "tokens": [51400, 18004, 5748, 5288, 294, 264, 1412, 992, 11, 558, 13, 1981, 366, 406, 534, 28708, 11, 457, 436, 434, 51708], "temperature": 0.0, "avg_logprob": -0.11277917453220912, "compression_ratio": 1.89453125, "no_speech_prob": 0.001925980788655579}, {"id": 172, "seek": 99528, "start": 995.28, "end": 1000.72, "text": " pretty provocative or evocative in the potential that these models might have in allowing us to", "tokens": [50364, 1238, 47663, 420, 1073, 905, 1166, 294, 264, 3995, 300, 613, 5245, 1062, 362, 294, 8293, 505, 281, 50636], "temperature": 0.0, "avg_logprob": -0.09160050639399776, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0010002932976931334}, {"id": 173, "seek": 99528, "start": 1000.72, "end": 1006.4, "text": " produce these textual descriptions of visual artifacts. And certainly, you know, a lot of,", "tokens": [50636, 5258, 613, 2487, 901, 24406, 295, 5056, 24617, 13, 400, 3297, 11, 291, 458, 11, 257, 688, 295, 11, 50920], "temperature": 0.0, "avg_logprob": -0.09160050639399776, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0010002932976931334}, {"id": 174, "seek": 99528, "start": 1006.4, "end": 1011.8399999999999, "text": " of, of people, certainly lots of big tech companies have thought about the ways in which you could use", "tokens": [50920, 295, 11, 295, 561, 11, 3297, 3195, 295, 955, 7553, 3431, 362, 1194, 466, 264, 2098, 294, 597, 291, 727, 764, 51192], "temperature": 0.0, "avg_logprob": -0.09160050639399776, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0010002932976931334}, {"id": 175, "seek": 99528, "start": 1012.64, "end": 1017.4399999999999, "text": " all kinds of machine learning models, not just LLMs to do the sort of rich description of visual", "tokens": [51232, 439, 3685, 295, 3479, 2539, 5245, 11, 406, 445, 441, 43, 26386, 281, 360, 264, 1333, 295, 4593, 3855, 295, 5056, 51472], "temperature": 0.0, "avg_logprob": -0.09160050639399776, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0010002932976931334}, {"id": 176, "seek": 99528, "start": 1017.4399999999999, "end": 1023.12, "text": " content and particularly for this sort of these accessibility use cases, like how do you describe", "tokens": [51472, 2701, 293, 4098, 337, 341, 1333, 295, 613, 15002, 764, 3331, 11, 411, 577, 360, 291, 6786, 51756], "temperature": 0.0, "avg_logprob": -0.09160050639399776, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0010002932976931334}, {"id": 177, "seek": 102312, "start": 1023.2, "end": 1029.12, "text": " these kinds of artifacts to people who are blind or have low vision. And lots of people have studied", "tokens": [50368, 613, 3685, 295, 24617, 281, 561, 567, 366, 6865, 420, 362, 2295, 5201, 13, 400, 3195, 295, 561, 362, 9454, 50664], "temperature": 0.0, "avg_logprob": -0.06518838188864969, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.001926322584040463}, {"id": 178, "seek": 102312, "start": 1029.12, "end": 1033.76, "text": " the degree to which these models are effective and found maybe unsurprisingly that they're not", "tokens": [50664, 264, 4314, 281, 597, 613, 5245, 366, 4942, 293, 1352, 1310, 2693, 374, 34408, 300, 436, 434, 406, 50896], "temperature": 0.0, "avg_logprob": -0.06518838188864969, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.001926322584040463}, {"id": 179, "seek": 102312, "start": 1033.76, "end": 1039.12, "text": " terribly effective right now, right. So here is a quote from a participant from one of our studies", "tokens": [50896, 22903, 4942, 558, 586, 11, 558, 13, 407, 510, 307, 257, 6513, 490, 257, 24950, 490, 472, 295, 527, 5313, 51164], "temperature": 0.0, "avg_logprob": -0.06518838188864969, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.001926322584040463}, {"id": 180, "seek": 102312, "start": 1039.12, "end": 1043.68, "text": " who says, you know, the reader wouldn't get much insight from texts like this, which not only,", "tokens": [51164, 567, 1619, 11, 291, 458, 11, 264, 15149, 2759, 380, 483, 709, 11269, 490, 15765, 411, 341, 11, 597, 406, 787, 11, 51392], "temperature": 0.0, "avg_logprob": -0.06518838188864969, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.001926322584040463}, {"id": 181, "seek": 102312, "start": 1043.68, "end": 1048.64, "text": " you know, is problematic because it doesn't effectively convey information, but more troublingly,", "tokens": [51392, 291, 458, 11, 307, 19011, 570, 309, 1177, 380, 8659, 16965, 1589, 11, 457, 544, 38080, 356, 11, 51640], "temperature": 0.0, "avg_logprob": -0.06518838188864969, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.001926322584040463}, {"id": 182, "seek": 104864, "start": 1048.64, "end": 1053.1200000000001, "text": " it actually increases the burden that readers face when they're trying to make sense", "tokens": [50364, 309, 767, 8637, 264, 12578, 300, 17147, 1851, 562, 436, 434, 1382, 281, 652, 2020, 50588], "temperature": 0.0, "avg_logprob": -0.06557827729445237, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0018099014414474368}, {"id": 183, "seek": 104864, "start": 1053.1200000000001, "end": 1058.3200000000002, "text": " of this output, right. There's a lot of sort of noise that gets added to that experience.", "tokens": [50588, 295, 341, 5598, 11, 558, 13, 821, 311, 257, 688, 295, 1333, 295, 5658, 300, 2170, 3869, 281, 300, 1752, 13, 50848], "temperature": 0.0, "avg_logprob": -0.06557827729445237, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0018099014414474368}, {"id": 184, "seek": 104864, "start": 1059.1200000000001, "end": 1063.8400000000001, "text": " Another participant, you know, says very, very interestingly, the problem with these textual", "tokens": [50888, 3996, 24950, 11, 291, 458, 11, 1619, 588, 11, 588, 25873, 11, 264, 1154, 365, 613, 2487, 901, 51124], "temperature": 0.0, "avg_logprob": -0.06557827729445237, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0018099014414474368}, {"id": 185, "seek": 104864, "start": 1063.8400000000001, "end": 1070.5600000000002, "text": " descriptions is also that it robs me of control of consuming the data, right. A participant,", "tokens": [51124, 24406, 307, 611, 300, 309, 744, 929, 385, 295, 1969, 295, 19867, 264, 1412, 11, 558, 13, 316, 24950, 11, 51460], "temperature": 0.0, "avg_logprob": -0.06557827729445237, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0018099014414474368}, {"id": 186, "seek": 104864, "start": 1070.5600000000002, "end": 1075.2800000000002, "text": " another participant said, I want to have the time and space to interpret the numbers for myself", "tokens": [51460, 1071, 24950, 848, 11, 286, 528, 281, 362, 264, 565, 293, 1901, 281, 7302, 264, 3547, 337, 2059, 51696], "temperature": 0.0, "avg_logprob": -0.06557827729445237, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0018099014414474368}, {"id": 187, "seek": 107528, "start": 1075.28, "end": 1081.76, "text": " before I read any kind of textual description that does the analysis for me. And so to me,", "tokens": [50364, 949, 286, 1401, 604, 733, 295, 2487, 901, 3855, 300, 775, 264, 5215, 337, 385, 13, 400, 370, 281, 385, 11, 50688], "temperature": 0.0, "avg_logprob": -0.06945212263810008, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00023774798319209367}, {"id": 188, "seek": 107528, "start": 1081.76, "end": 1087.28, "text": " these sound very similar to issues associated with a semantic and an articulatory distance,", "tokens": [50688, 613, 1626, 588, 2531, 281, 2663, 6615, 365, 257, 47982, 293, 364, 15228, 425, 4745, 4560, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06945212263810008, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00023774798319209367}, {"id": 189, "seek": 107528, "start": 1087.28, "end": 1092.24, "text": " right. That first quote talking about, well, these texts aren't conveying anything interesting.", "tokens": [50964, 558, 13, 663, 700, 6513, 1417, 466, 11, 731, 11, 613, 15765, 3212, 380, 18053, 1840, 1340, 1880, 13, 51212], "temperature": 0.0, "avg_logprob": -0.06945212263810008, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00023774798319209367}, {"id": 190, "seek": 107528, "start": 1092.96, "end": 1097.36, "text": " The second set talking about, well, I want to have that time and space, I want to be able to", "tokens": [51248, 440, 1150, 992, 1417, 466, 11, 731, 11, 286, 528, 281, 362, 300, 565, 293, 1901, 11, 286, 528, 281, 312, 1075, 281, 51468], "temperature": 0.0, "avg_logprob": -0.06945212263810008, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00023774798319209367}, {"id": 191, "seek": 107528, "start": 1097.36, "end": 1104.24, "text": " control the form with which that text is conveyed to me. And so I want to dig into how we might", "tokens": [51468, 1969, 264, 1254, 365, 597, 300, 2487, 307, 49340, 281, 385, 13, 400, 370, 286, 528, 281, 2528, 666, 577, 321, 1062, 51812], "temperature": 0.0, "avg_logprob": -0.06945212263810008, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.00023774798319209367}, {"id": 192, "seek": 110424, "start": 1104.24, "end": 1109.04, "text": " address these two distances in the case of accessibility. But before I do that, I want to", "tokens": [50364, 2985, 613, 732, 22182, 294, 264, 1389, 295, 15002, 13, 583, 949, 286, 360, 300, 11, 286, 528, 281, 50604], "temperature": 0.0, "avg_logprob": -0.1331072541856274, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0008035438950173557}, {"id": 193, "seek": 110424, "start": 1109.04, "end": 1115.44, "text": " give us a sense of how people who are blind or have low vision experience, you know, the", "tokens": [50604, 976, 505, 257, 2020, 295, 577, 561, 567, 366, 6865, 420, 362, 2295, 5201, 1752, 11, 291, 458, 11, 264, 50924], "temperature": 0.0, "avg_logprob": -0.1331072541856274, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0008035438950173557}, {"id": 194, "seek": 110424, "start": 1116.24, "end": 1121.36, "text": " internet and graphical interfaces today. So I'm going to turn things over to my PhD student,", "tokens": [50964, 4705, 293, 35942, 28416, 965, 13, 407, 286, 478, 516, 281, 1261, 721, 670, 281, 452, 14476, 3107, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1331072541856274, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0008035438950173557}, {"id": 195, "seek": 110424, "start": 1121.36, "end": 1126.72, "text": " Jonathan Zhang, who will give us a quick demo of an assistive technology called a screen reader", "tokens": [51220, 15471, 17729, 11, 567, 486, 976, 505, 257, 1702, 10723, 295, 364, 4255, 488, 2899, 1219, 257, 2568, 15149, 51488], "temperature": 0.0, "avg_logprob": -0.1331072541856274, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0008035438950173557}, {"id": 196, "seek": 110424, "start": 1126.72, "end": 1129.1200000000001, "text": " that basically narrates on-screen content.", "tokens": [51488, 300, 1936, 6397, 1024, 322, 12, 12439, 2701, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1331072541856274, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.0008035438950173557}, {"id": 197, "seek": 113424, "start": 1134.24, "end": 1151.1200000000001, "text": " So here I can demonstrate what the accessible HTML version of our paper looks like to a screen reader.", "tokens": [50364, 407, 510, 286, 393, 11698, 437, 264, 9515, 17995, 3037, 295, 527, 3035, 1542, 411, 281, 257, 2568, 15149, 13, 51208], "temperature": 0.0, "avg_logprob": -0.28360039254893427, "compression_ratio": 1.146067415730337, "no_speech_prob": 0.0849963054060936}, {"id": 198, "seek": 116424, "start": 1164.96, "end": 1175.84, "text": " So as you can see, what a screen reader does is it basically sort of linearizes the operation", "tokens": [50400, 407, 382, 291, 393, 536, 11, 437, 257, 2568, 15149, 775, 307, 309, 1936, 1333, 295, 8213, 5660, 264, 6916, 50944], "temperature": 0.0, "avg_logprob": -0.1243189609411991, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.011681749485433102}, {"id": 199, "seek": 116424, "start": 1175.84, "end": 1183.68, "text": " of, you know, reading, perceiving, understanding graphical content on a user interface. And in", "tokens": [50944, 295, 11, 291, 458, 11, 3760, 11, 9016, 2123, 11, 3701, 35942, 2701, 322, 257, 4195, 9226, 13, 400, 294, 51336], "temperature": 0.0, "avg_logprob": -0.1243189609411991, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.011681749485433102}, {"id": 200, "seek": 116424, "start": 1183.68, "end": 1187.92, "text": " particular, you might notice that the narration was actually quite rapid, right. And this is", "tokens": [51336, 1729, 11, 291, 1062, 3449, 300, 264, 43299, 390, 767, 1596, 7558, 11, 558, 13, 400, 341, 307, 51548], "temperature": 0.0, "avg_logprob": -0.1243189609411991, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.011681749485433102}, {"id": 201, "seek": 118792, "start": 1187.92, "end": 1194.96, "text": " actually a slowed down version of what, you know, proficient screen reader users use, which is often", "tokens": [50364, 767, 257, 32057, 760, 3037, 295, 437, 11, 291, 458, 11, 1740, 24549, 2568, 15149, 5022, 764, 11, 597, 307, 2049, 50716], "temperature": 0.0, "avg_logprob": -0.07676197601868226, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.014057536609470844}, {"id": 202, "seek": 118792, "start": 1194.96, "end": 1201.04, "text": " much, much faster. But what is interesting about the screen reader use case is that it forces that", "tokens": [50716, 709, 11, 709, 4663, 13, 583, 437, 307, 1880, 466, 264, 2568, 15149, 764, 1389, 307, 300, 309, 5874, 300, 51020], "temperature": 0.0, "avg_logprob": -0.07676197601868226, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.014057536609470844}, {"id": 203, "seek": 118792, "start": 1201.04, "end": 1207.04, "text": " linearity, right. And the key challenge in figuring out the articulatory distance in the case of", "tokens": [51020, 8213, 507, 11, 558, 13, 400, 264, 2141, 3430, 294, 15213, 484, 264, 15228, 425, 4745, 4560, 294, 264, 1389, 295, 51320], "temperature": 0.0, "avg_logprob": -0.07676197601868226, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.014057536609470844}, {"id": 204, "seek": 118792, "start": 1207.04, "end": 1212.16, "text": " accessibility is how do you take visualizations that probably all of us in the audience have", "tokens": [51320, 15002, 307, 577, 360, 291, 747, 5056, 14455, 300, 1391, 439, 295, 505, 294, 264, 4034, 362, 51576], "temperature": 0.0, "avg_logprob": -0.07676197601868226, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.014057536609470844}, {"id": 205, "seek": 118792, "start": 1212.16, "end": 1216.5600000000002, "text": " slightly subtly different ways of reading, right. Maybe some of you start by reading the title,", "tokens": [51576, 4748, 7257, 356, 819, 2098, 295, 3760, 11, 558, 13, 2704, 512, 295, 291, 722, 538, 3760, 264, 4876, 11, 51796], "temperature": 0.0, "avg_logprob": -0.07676197601868226, "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.014057536609470844}, {"id": 206, "seek": 121656, "start": 1216.56, "end": 1221.6, "text": " then moving to the axes, then looking at, you know, the shapes, while others might start by", "tokens": [50364, 550, 2684, 281, 264, 35387, 11, 550, 1237, 412, 11, 291, 458, 11, 264, 10854, 11, 1339, 2357, 1062, 722, 538, 50616], "temperature": 0.0, "avg_logprob": -0.08225397405953243, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.001098324661143124}, {"id": 207, "seek": 121656, "start": 1221.6, "end": 1227.28, "text": " looking at the most salient trend and then start to, you know, map out to what the axes and legends", "tokens": [50616, 1237, 412, 264, 881, 1845, 1196, 6028, 293, 550, 722, 281, 11, 291, 458, 11, 4471, 484, 281, 437, 264, 35387, 293, 27695, 50900], "temperature": 0.0, "avg_logprob": -0.08225397405953243, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.001098324661143124}, {"id": 208, "seek": 121656, "start": 1227.28, "end": 1233.28, "text": " and stuff like that are. How do we take all of that rich diversity, but linearize it? So the", "tokens": [50900, 293, 1507, 411, 300, 366, 13, 1012, 360, 321, 747, 439, 295, 300, 4593, 8811, 11, 457, 8213, 1125, 309, 30, 407, 264, 51200], "temperature": 0.0, "avg_logprob": -0.08225397405953243, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.001098324661143124}, {"id": 209, "seek": 121656, "start": 1233.28, "end": 1238.8, "text": " people who use screen readers can nevertheless have that same, you know, choice in meeting a", "tokens": [51200, 561, 567, 764, 2568, 17147, 393, 26924, 362, 300, 912, 11, 291, 458, 11, 3922, 294, 3440, 257, 51476], "temperature": 0.0, "avg_logprob": -0.08225397405953243, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.001098324661143124}, {"id": 210, "seek": 121656, "start": 1238.8, "end": 1245.44, "text": " visualization, but under these conditions. And so the way we have chosen to do that is basically", "tokens": [51476, 25801, 11, 457, 833, 613, 4487, 13, 400, 370, 264, 636, 321, 362, 8614, 281, 360, 300, 307, 1936, 51808], "temperature": 0.0, "avg_logprob": -0.08225397405953243, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.001098324661143124}, {"id": 211, "seek": 124544, "start": 1245.44, "end": 1252.88, "text": " by restructuring the content of a visualization into a text-oriented hierarchy. So at the top,", "tokens": [50364, 538, 1472, 1757, 1345, 264, 2701, 295, 257, 25801, 666, 257, 2487, 12, 27414, 22333, 13, 407, 412, 264, 1192, 11, 50736], "temperature": 0.0, "avg_logprob": -0.06951430776844854, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0004044114029966295}, {"id": 212, "seek": 124544, "start": 1252.88, "end": 1257.6000000000001, "text": " at the root of this hierarchy is just a summary of the chart, probably the trends that are shown", "tokens": [50736, 412, 264, 5593, 295, 341, 22333, 307, 445, 257, 12691, 295, 264, 6927, 11, 1391, 264, 13892, 300, 366, 4898, 50972], "temperature": 0.0, "avg_logprob": -0.06951430776844854, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0004044114029966295}, {"id": 213, "seek": 124544, "start": 1257.6000000000001, "end": 1263.76, "text": " in the chart. And then the hierarchy branches off into the individual sort of data fields", "tokens": [50972, 294, 264, 6927, 13, 400, 550, 264, 22333, 14770, 766, 666, 264, 2609, 1333, 295, 1412, 7909, 51280], "temperature": 0.0, "avg_logprob": -0.06951430776844854, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0004044114029966295}, {"id": 214, "seek": 124544, "start": 1263.76, "end": 1268.96, "text": " or the encodings in this case, right. The x-axis, the y-axis, the legend and things like that.", "tokens": [51280, 420, 264, 2058, 378, 1109, 294, 341, 1389, 11, 558, 13, 440, 2031, 12, 24633, 11, 264, 288, 12, 24633, 11, 264, 9451, 293, 721, 411, 300, 13, 51540], "temperature": 0.0, "avg_logprob": -0.06951430776844854, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0004044114029966295}, {"id": 215, "seek": 124544, "start": 1268.96, "end": 1275.1200000000001, "text": " And then people can start to drill down in ways that maintain some correspondence with the visual", "tokens": [51540, 400, 550, 561, 393, 722, 281, 11392, 760, 294, 2098, 300, 6909, 512, 38135, 365, 264, 5056, 51848], "temperature": 0.0, "avg_logprob": -0.06951430776844854, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0004044114029966295}, {"id": 216, "seek": 127512, "start": 1275.12, "end": 1281.6, "text": " artifacts. So one step below, you know, the x-axis is stepping through them by the major ticks,", "tokens": [50364, 24617, 13, 407, 472, 1823, 2507, 11, 291, 458, 11, 264, 2031, 12, 24633, 307, 16821, 807, 552, 538, 264, 2563, 42475, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09359962851912887, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0008828432764858007}, {"id": 217, "seek": 127512, "start": 1281.6, "end": 1285.1999999999998, "text": " right. One step below the major ticks would be minor ticks and then ultimately you would get", "tokens": [50688, 558, 13, 1485, 1823, 2507, 264, 2563, 42475, 576, 312, 6696, 42475, 293, 550, 6284, 291, 576, 483, 50868], "temperature": 0.0, "avg_logprob": -0.09359962851912887, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0008828432764858007}, {"id": 218, "seek": 127512, "start": 1285.1999999999998, "end": 1289.76, "text": " to the individual data points. So let me throw things back to Jonathan to give us a demo of how", "tokens": [50868, 281, 264, 2609, 1412, 2793, 13, 407, 718, 385, 3507, 721, 646, 281, 15471, 281, 976, 505, 257, 10723, 295, 577, 51096], "temperature": 0.0, "avg_logprob": -0.09359962851912887, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0008828432764858007}, {"id": 219, "seek": 127512, "start": 1289.76, "end": 1297.9199999999998, "text": " this works. A scatter plot of Penguin data. And to a screen reader, our system represents this", "tokens": [51096, 341, 1985, 13, 316, 34951, 7542, 295, 49562, 1412, 13, 400, 281, 257, 2568, 15149, 11, 527, 1185, 8855, 341, 51504], "temperature": 0.0, "avg_logprob": -0.09359962851912887, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0008828432764858007}, {"id": 220, "seek": 127512, "start": 1297.9199999999998, "end": 1304.4799999999998, "text": " scatter plot as a keyboard navigable data structure that contains text descriptions at", "tokens": [51504, 34951, 7542, 382, 257, 10186, 7407, 712, 1412, 3877, 300, 8306, 2487, 24406, 412, 51832], "temperature": 0.0, "avg_logprob": -0.09359962851912887, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0008828432764858007}, {"id": 221, "seek": 130448, "start": 1304.48, "end": 1311.2, "text": " varying levels of detail. So when a screen reader user first encounters this visualization on a page,", "tokens": [50364, 22984, 4358, 295, 2607, 13, 407, 562, 257, 2568, 15149, 4195, 700, 26310, 341, 25801, 322, 257, 3028, 11, 50700], "temperature": 0.0, "avg_logprob": -0.09435675825391497, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0025503598153591156}, {"id": 222, "seek": 130448, "start": 1311.2, "end": 1316.72, "text": " they'll be able to read off a high-level alt text description of what the chart is. So", "tokens": [50700, 436, 603, 312, 1075, 281, 1401, 766, 257, 1090, 12, 12418, 4955, 2487, 3855, 295, 437, 264, 6927, 307, 13, 407, 50976], "temperature": 0.0, "avg_logprob": -0.09435675825391497, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0025503598153591156}, {"id": 223, "seek": 130448, "start": 1319.84, "end": 1324.96, "text": " and if they're interested in getting more detail about this visualization, they can dive in by", "tokens": [51132, 293, 498, 436, 434, 3102, 294, 1242, 544, 2607, 466, 341, 25801, 11, 436, 393, 9192, 294, 538, 51388], "temperature": 0.0, "avg_logprob": -0.09435675825391497, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0025503598153591156}, {"id": 224, "seek": 130448, "start": 1324.96, "end": 1331.84, "text": " pressing the down arrow key to descend one level in the hierarchy and access descriptions about", "tokens": [51388, 12417, 264, 760, 11610, 2141, 281, 16333, 472, 1496, 294, 264, 22333, 293, 2105, 24406, 466, 51732], "temperature": 0.0, "avg_logprob": -0.09435675825391497, "compression_ratio": 1.7149321266968325, "no_speech_prob": 0.0025503598153591156}, {"id": 225, "seek": 133184, "start": 1331.84, "end": 1335.76, "text": " the different encodings of the scatter plot. So I'm going to press the down arrow key.", "tokens": [50364, 264, 819, 2058, 378, 1109, 295, 264, 34951, 7542, 13, 407, 286, 478, 516, 281, 1886, 264, 760, 11610, 2141, 13, 50560], "temperature": 0.0, "avg_logprob": -0.11693556649344308, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.012818125076591969}, {"id": 226, "seek": 133184, "start": 1339.76, "end": 1345.12, "text": " I can press the left and right arrow keys to flip through descriptions of the other axes and", "tokens": [50760, 286, 393, 1886, 264, 1411, 293, 558, 11610, 9317, 281, 7929, 807, 24406, 295, 264, 661, 35387, 293, 51028], "temperature": 0.0, "avg_logprob": -0.11693556649344308, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.012818125076591969}, {"id": 227, "seek": 133184, "start": 1345.12, "end": 1360.6399999999999, "text": " legends. Cool, so let's say I am interested in getting more information about the x-axis. I can", "tokens": [51028, 27695, 13, 8561, 11, 370, 718, 311, 584, 286, 669, 3102, 294, 1242, 544, 1589, 466, 264, 2031, 12, 24633, 13, 286, 393, 51804], "temperature": 0.0, "avg_logprob": -0.11693556649344308, "compression_ratio": 1.5193370165745856, "no_speech_prob": 0.012818125076591969}, {"id": 228, "seek": 136064, "start": 1361.3600000000001, "end": 1366.8000000000002, "text": " use the left arrow key to navigate back to the x-axis description and then press down one more", "tokens": [50400, 764, 264, 1411, 11610, 2141, 281, 12350, 646, 281, 264, 2031, 12, 24633, 3855, 293, 550, 1886, 760, 472, 544, 50672], "temperature": 0.0, "avg_logprob": -0.0806574092970954, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.0026311432011425495}, {"id": 229, "seek": 136064, "start": 1366.8000000000002, "end": 1381.3600000000001, "text": " time to descend a level of detail into the x-axis. So on this level underneath the x-axis, I'm", "tokens": [50672, 565, 281, 16333, 257, 1496, 295, 2607, 666, 264, 2031, 12, 24633, 13, 407, 322, 341, 1496, 7223, 264, 2031, 12, 24633, 11, 286, 478, 51400], "temperature": 0.0, "avg_logprob": -0.0806574092970954, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.0026311432011425495}, {"id": 230, "seek": 136064, "start": 1381.3600000000001, "end": 1388.24, "text": " accessing descriptions of intervals along the x-axis and it's reading out to me how many data", "tokens": [51400, 26440, 24406, 295, 26651, 2051, 264, 2031, 12, 24633, 293, 309, 311, 3760, 484, 281, 385, 577, 867, 1412, 51744], "temperature": 0.0, "avg_logprob": -0.0806574092970954, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.0026311432011425495}, {"id": 231, "seek": 138824, "start": 1388.24, "end": 1392.64, "text": " values are contained within each interval. So by pressing left and right, I can kind of get", "tokens": [50364, 4190, 366, 16212, 1951, 1184, 15035, 13, 407, 538, 12417, 1411, 293, 558, 11, 286, 393, 733, 295, 483, 50584], "temperature": 0.0, "avg_logprob": -0.061758317809174026, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.003483009757474065}, {"id": 232, "seek": 138824, "start": 1392.64, "end": 1406.24, "text": " a sense of the distribution of data along the x-axis. So let's say I am interested in this", "tokens": [50584, 257, 2020, 295, 264, 7316, 295, 1412, 2051, 264, 2031, 12, 24633, 13, 407, 718, 311, 584, 286, 669, 3102, 294, 341, 51264], "temperature": 0.0, "avg_logprob": -0.061758317809174026, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.003483009757474065}, {"id": 233, "seek": 138824, "start": 1406.24, "end": 1412.8, "text": " range from 190 to 200. I can then press down arrow again to dive into the individual data points", "tokens": [51264, 3613, 490, 37609, 281, 2331, 13, 286, 393, 550, 1886, 760, 11610, 797, 281, 9192, 666, 264, 2609, 1412, 2793, 51592], "temperature": 0.0, "avg_logprob": -0.061758317809174026, "compression_ratio": 1.4684210526315788, "no_speech_prob": 0.003483009757474065}, {"id": 234, "seek": 141280, "start": 1412.8, "end": 1428.56, "text": " that are contained within this interval. So let's say that instead of moving up and down", "tokens": [50364, 300, 366, 16212, 1951, 341, 15035, 13, 407, 718, 311, 584, 300, 2602, 295, 2684, 493, 293, 760, 51152], "temperature": 0.0, "avg_logprob": -0.07371170841046233, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.018542908132076263}, {"id": 235, "seek": 141280, "start": 1428.56, "end": 1434.1599999999999, "text": " this hierarchical structure, I would rather just move around the x-y grid in the scatter plot,", "tokens": [51152, 341, 35250, 804, 3877, 11, 286, 576, 2831, 445, 1286, 926, 264, 2031, 12, 88, 10748, 294, 264, 34951, 7542, 11, 51432], "temperature": 0.0, "avg_logprob": -0.07371170841046233, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.018542908132076263}, {"id": 236, "seek": 141280, "start": 1434.1599999999999, "end": 1440.72, "text": " as if I were kind of feeling around a tactile graphic, for example. I can start by navigating", "tokens": [51432, 382, 498, 286, 645, 733, 295, 2633, 926, 257, 47319, 14089, 11, 337, 1365, 13, 286, 393, 722, 538, 32054, 51760], "temperature": 0.0, "avg_logprob": -0.07371170841046233, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.018542908132076263}, {"id": 237, "seek": 144072, "start": 1440.72, "end": 1451.2, "text": " over to the grid view of the scatter plot. And once I descend into this part of the hierarchy,", "tokens": [50364, 670, 281, 264, 10748, 1910, 295, 264, 34951, 7542, 13, 400, 1564, 286, 16333, 666, 341, 644, 295, 264, 22333, 11, 50888], "temperature": 0.0, "avg_logprob": -0.07403333981831868, "compression_ratio": 1.34375, "no_speech_prob": 0.007575875613838434}, {"id": 238, "seek": 144072, "start": 1451.2, "end": 1457.76, "text": " I can use the WASD keys to move up and down different squares along the grid.", "tokens": [50888, 286, 393, 764, 264, 28984, 35, 9317, 281, 1286, 493, 293, 760, 819, 19368, 2051, 264, 10748, 13, 51216], "temperature": 0.0, "avg_logprob": -0.07403333981831868, "compression_ratio": 1.34375, "no_speech_prob": 0.007575875613838434}, {"id": 239, "seek": 145776, "start": 1457.92, "end": 1471.6, "text": " And so similarly to before, it's starting off by giving me the number of data values that are", "tokens": [50372, 400, 370, 14138, 281, 949, 11, 309, 311, 2891, 766, 538, 2902, 385, 264, 1230, 295, 1412, 4190, 300, 366, 51056], "temperature": 0.0, "avg_logprob": -0.10925643891096115, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.005219439975917339}, {"id": 240, "seek": 145776, "start": 1471.6, "end": 1476.32, "text": " contained in that square so that I can get a sense of the distribution of the data.", "tokens": [51056, 16212, 294, 300, 3732, 370, 300, 286, 393, 483, 257, 2020, 295, 264, 7316, 295, 264, 1412, 13, 51292], "temperature": 0.0, "avg_logprob": -0.10925643891096115, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.005219439975917339}, {"id": 241, "seek": 145776, "start": 1477.68, "end": 1485.92, "text": " And so we designed this in collaboration with a blind HCI researcher named Daniel Hodges.", "tokens": [51360, 400, 370, 321, 4761, 341, 294, 9363, 365, 257, 6865, 389, 25240, 21751, 4926, 8033, 45151, 2880, 13, 51772], "temperature": 0.0, "avg_logprob": -0.10925643891096115, "compression_ratio": 1.4833333333333334, "no_speech_prob": 0.005219439975917339}, {"id": 242, "seek": 148592, "start": 1485.92, "end": 1492.72, "text": " And this was the first time he felt like he actually understood and could build a mental", "tokens": [50364, 400, 341, 390, 264, 700, 565, 415, 2762, 411, 415, 767, 7320, 293, 727, 1322, 257, 4973, 50704], "temperature": 0.0, "avg_logprob": -0.06000356674194336, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00033529018401168287}, {"id": 243, "seek": 148592, "start": 1492.72, "end": 1499.1200000000001, "text": " model of what it was that a scatter plot was representing. We saw these sorts of comments", "tokens": [50704, 2316, 295, 437, 309, 390, 300, 257, 34951, 7542, 390, 13460, 13, 492, 1866, 613, 7527, 295, 3053, 51024], "temperature": 0.0, "avg_logprob": -0.06000356674194336, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00033529018401168287}, {"id": 244, "seek": 148592, "start": 1499.1200000000001, "end": 1505.3600000000001, "text": " reflected in user studies that we ran about how the form of this textual output really", "tokens": [51024, 15502, 294, 4195, 5313, 300, 321, 5872, 466, 577, 264, 1254, 295, 341, 2487, 901, 5598, 534, 51336], "temperature": 0.0, "avg_logprob": -0.06000356674194336, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00033529018401168287}, {"id": 245, "seek": 148592, "start": 1505.3600000000001, "end": 1510.72, "text": " influenced participants' mental model of what the data was, what the trends were, and things like that.", "tokens": [51336, 15269, 10503, 6, 4973, 2316, 295, 437, 264, 1412, 390, 11, 437, 264, 13892, 645, 11, 293, 721, 411, 300, 13, 51604], "temperature": 0.0, "avg_logprob": -0.06000356674194336, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00033529018401168287}, {"id": 246, "seek": 148592, "start": 1510.72, "end": 1515.52, "text": " So one participant, for instance, said, I now know how to drill down and up between different", "tokens": [51604, 407, 472, 24950, 11, 337, 5197, 11, 848, 11, 286, 586, 458, 577, 281, 11392, 760, 293, 493, 1296, 819, 51844], "temperature": 0.0, "avg_logprob": -0.06000356674194336, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.00033529018401168287}, {"id": 247, "seek": 151552, "start": 1515.6, "end": 1521.6, "text": " layers in the data to get an overall picture. And it gives me a different way of thinking.", "tokens": [50368, 7914, 294, 264, 1412, 281, 483, 364, 4787, 3036, 13, 400, 309, 2709, 385, 257, 819, 636, 295, 1953, 13, 50668], "temperature": 0.0, "avg_logprob": -0.07271836657042897, "compression_ratio": 1.72, "no_speech_prob": 0.0005356171168386936}, {"id": 248, "seek": 151552, "start": 1521.6, "end": 1526.4, "text": " And another one said, I'm thinking more in spatial terms because this is just a new method", "tokens": [50668, 400, 1071, 472, 848, 11, 286, 478, 1953, 544, 294, 23598, 2115, 570, 341, 307, 445, 257, 777, 3170, 50908], "temperature": 0.0, "avg_logprob": -0.07271836657042897, "compression_ratio": 1.72, "no_speech_prob": 0.0005356171168386936}, {"id": 249, "seek": 151552, "start": 1526.4, "end": 1532.32, "text": " for navigating and moving through the grid and drilling down to information and things like that.", "tokens": [50908, 337, 32054, 293, 2684, 807, 264, 10748, 293, 26290, 760, 281, 1589, 293, 721, 411, 300, 13, 51204], "temperature": 0.0, "avg_logprob": -0.07271836657042897, "compression_ratio": 1.72, "no_speech_prob": 0.0005356171168386936}, {"id": 250, "seek": 151552, "start": 1532.32, "end": 1539.12, "text": " And so what I find interesting here is that at every step, the semantic content stayed exactly", "tokens": [51204, 400, 370, 437, 286, 915, 1880, 510, 307, 300, 412, 633, 1823, 11, 264, 47982, 2701, 9181, 2293, 51544], "temperature": 0.0, "avg_logprob": -0.07271836657042897, "compression_ratio": 1.72, "no_speech_prob": 0.0005356171168386936}, {"id": 251, "seek": 151552, "start": 1539.12, "end": 1544.16, "text": " the same. And there wasn't even very rich semantic content. It was a range and then a count of the", "tokens": [51544, 264, 912, 13, 400, 456, 2067, 380, 754, 588, 4593, 47982, 2701, 13, 467, 390, 257, 3613, 293, 550, 257, 1207, 295, 264, 51796], "temperature": 0.0, "avg_logprob": -0.07271836657042897, "compression_ratio": 1.72, "no_speech_prob": 0.0005356171168386936}, {"id": 252, "seek": 154416, "start": 1544.16, "end": 1550.64, "text": " data values. All we manipulated was that articulation, that form, giving it a hierarchical", "tokens": [50364, 1412, 4190, 13, 1057, 321, 37161, 390, 300, 15228, 2776, 11, 300, 1254, 11, 2902, 309, 257, 35250, 804, 50688], "temperature": 0.0, "avg_logprob": -0.08726339125901125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0014099356485530734}, {"id": 253, "seek": 154416, "start": 1550.64, "end": 1556.8000000000002, "text": " nature, adding all of these different navigational affordances, and just manipulating the articulation", "tokens": [50688, 3687, 11, 5127, 439, 295, 613, 819, 7407, 1478, 6157, 2676, 11, 293, 445, 40805, 264, 15228, 2776, 50996], "temperature": 0.0, "avg_logprob": -0.08726339125901125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0014099356485530734}, {"id": 254, "seek": 154416, "start": 1556.8000000000002, "end": 1562.64, "text": " had this huge impact on people's mental models of the data. And I think that we're really just at", "tokens": [50996, 632, 341, 2603, 2712, 322, 561, 311, 4973, 5245, 295, 264, 1412, 13, 400, 286, 519, 300, 321, 434, 534, 445, 412, 51288], "temperature": 0.0, "avg_logprob": -0.08726339125901125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0014099356485530734}, {"id": 255, "seek": 154416, "start": 1562.64, "end": 1568.0800000000002, "text": " the tip of the iceberg of these more accessible structures. Currently, in my group, we're thinking", "tokens": [51288, 264, 4125, 295, 264, 38880, 295, 613, 544, 9515, 9227, 13, 19964, 11, 294, 452, 1594, 11, 321, 434, 1953, 51560], "temperature": 0.0, "avg_logprob": -0.08726339125901125, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0014099356485530734}, {"id": 256, "seek": 156808, "start": 1568.08, "end": 1574.24, "text": " about just the impact that token order has on how people using screen readers build up those", "tokens": [50364, 466, 445, 264, 2712, 300, 14862, 1668, 575, 322, 577, 561, 1228, 2568, 17147, 1322, 493, 729, 50672], "temperature": 0.0, "avg_logprob": -0.06752963860829671, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.02516750991344452}, {"id": 257, "seek": 156808, "start": 1574.24, "end": 1579.6799999999998, "text": " mental models. If you're constantly prompting them with the range first rather than the actual data", "tokens": [50672, 4973, 5245, 13, 759, 291, 434, 6460, 12391, 278, 552, 365, 264, 3613, 700, 2831, 813, 264, 3539, 1412, 50944], "temperature": 0.0, "avg_logprob": -0.06752963860829671, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.02516750991344452}, {"id": 258, "seek": 156808, "start": 1579.6799999999998, "end": 1585.52, "text": " values, does that introduce friction to their capacity to build that mental model and things", "tokens": [50944, 4190, 11, 775, 300, 5366, 17710, 281, 641, 6042, 281, 1322, 300, 4973, 2316, 293, 721, 51236], "temperature": 0.0, "avg_logprob": -0.06752963860829671, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.02516750991344452}, {"id": 259, "seek": 156808, "start": 1585.52, "end": 1592.8799999999999, "text": " like that? But in all of this, where is semantic distance? How do we actually start to make that", "tokens": [51236, 411, 300, 30, 583, 294, 439, 295, 341, 11, 689, 307, 47982, 4560, 30, 1012, 360, 321, 767, 722, 281, 652, 300, 51604], "temperature": 0.0, "avg_logprob": -0.06752963860829671, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.02516750991344452}, {"id": 260, "seek": 159288, "start": 1592.88, "end": 1598.0, "text": " textual descriptions more interesting and meaningful? And this is where I think LLMs can", "tokens": [50364, 2487, 901, 24406, 544, 1880, 293, 10995, 30, 400, 341, 307, 689, 286, 519, 441, 43, 26386, 393, 50620], "temperature": 0.0, "avg_logprob": -0.07608391414178867, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.1776781976222992}, {"id": 261, "seek": 159288, "start": 1598.0, "end": 1604.24, "text": " really help us. For one reason, it's because there's just a sheer amount of textual content we need", "tokens": [50620, 534, 854, 505, 13, 1171, 472, 1778, 11, 309, 311, 570, 456, 311, 445, 257, 23061, 2372, 295, 2487, 901, 2701, 321, 643, 50932], "temperature": 0.0, "avg_logprob": -0.07608391414178867, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.1776781976222992}, {"id": 262, "seek": 159288, "start": 1604.24, "end": 1610.4, "text": " to be able to produce that is infeasible to expect a human to sort of manually author.", "tokens": [50932, 281, 312, 1075, 281, 5258, 300, 307, 1536, 68, 296, 964, 281, 2066, 257, 1952, 281, 1333, 295, 16945, 3793, 13, 51240], "temperature": 0.0, "avg_logprob": -0.07608391414178867, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.1776781976222992}, {"id": 263, "seek": 159288, "start": 1610.4, "end": 1614.88, "text": " But there are other sort of implications that we'll touch upon really shortly.", "tokens": [51240, 583, 456, 366, 661, 1333, 295, 16602, 300, 321, 603, 2557, 3564, 534, 13392, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07608391414178867, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.1776781976222992}, {"id": 264, "seek": 159288, "start": 1614.88, "end": 1619.3600000000001, "text": " But before we can get LLMs to actually sort of produce the content we want,", "tokens": [51464, 583, 949, 321, 393, 483, 441, 43, 26386, 281, 767, 1333, 295, 5258, 264, 2701, 321, 528, 11, 51688], "temperature": 0.0, "avg_logprob": -0.07608391414178867, "compression_ratio": 1.6862745098039216, "no_speech_prob": 0.1776781976222992}, {"id": 265, "seek": 161936, "start": 1619.36, "end": 1625.84, "text": " what we need to do is shift from that very latent space with implicit semantics to a set of explicit", "tokens": [50364, 437, 321, 643, 281, 360, 307, 5513, 490, 300, 588, 48994, 1901, 365, 26947, 4361, 45298, 281, 257, 992, 295, 13691, 50688], "temperature": 0.0, "avg_logprob": -0.07154536955427415, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00018515929696150124}, {"id": 266, "seek": 161936, "start": 1625.84, "end": 1632.7199999999998, "text": " semantics. We need to impose a conceptual model onto our LLMs. Or another way of saying that is", "tokens": [50688, 4361, 45298, 13, 492, 643, 281, 26952, 257, 24106, 2316, 3911, 527, 441, 43, 26386, 13, 1610, 1071, 636, 295, 1566, 300, 307, 51032], "temperature": 0.0, "avg_logprob": -0.07154536955427415, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00018515929696150124}, {"id": 267, "seek": 161936, "start": 1632.7199999999998, "end": 1637.76, "text": " we need to get the LLMs to understand what a good textual description of a visualization is.", "tokens": [51032, 321, 643, 281, 483, 264, 441, 43, 26386, 281, 1223, 437, 257, 665, 2487, 901, 3855, 295, 257, 25801, 307, 13, 51284], "temperature": 0.0, "avg_logprob": -0.07154536955427415, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00018515929696150124}, {"id": 268, "seek": 161936, "start": 1638.56, "end": 1644.4799999999998, "text": " And so that's what my then PhD student, Alan Lungard, set out to do. We ran a crowdsource study", "tokens": [51324, 400, 370, 300, 311, 437, 452, 550, 14476, 3107, 11, 16442, 441, 1063, 515, 11, 992, 484, 281, 360, 13, 492, 5872, 257, 26070, 2948, 2979, 51620], "temperature": 0.0, "avg_logprob": -0.07154536955427415, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.00018515929696150124}, {"id": 269, "seek": 164448, "start": 1644.48, "end": 1651.52, "text": " where we got sort of 2,000 descriptions of charts. And through qualitative coding,", "tokens": [50364, 689, 321, 658, 1333, 295, 568, 11, 1360, 24406, 295, 17767, 13, 400, 807, 31312, 17720, 11, 50716], "temperature": 0.0, "avg_logprob": -0.097594420115153, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.010648615658283234}, {"id": 270, "seek": 164448, "start": 1651.52, "end": 1656.0, "text": " we realized there are basically four kinds of semantic content that textual descriptions should", "tokens": [50716, 321, 5334, 456, 366, 1936, 1451, 3685, 295, 47982, 2701, 300, 2487, 901, 24406, 820, 50940], "temperature": 0.0, "avg_logprob": -0.097594420115153, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.010648615658283234}, {"id": 271, "seek": 164448, "start": 1656.0, "end": 1662.08, "text": " convey. The first most primitive layer is basically just the sort of construction details of the chart.", "tokens": [50940, 16965, 13, 440, 700, 881, 28540, 4583, 307, 1936, 445, 264, 1333, 295, 6435, 4365, 295, 264, 6927, 13, 51244], "temperature": 0.0, "avg_logprob": -0.097594420115153, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.010648615658283234}, {"id": 272, "seek": 164448, "start": 1662.08, "end": 1668.48, "text": " What are the titles, the labels, the scales, the units, etc. And accessibility best practices say", "tokens": [51244, 708, 366, 264, 12992, 11, 264, 16949, 11, 264, 17408, 11, 264, 6815, 11, 5183, 13, 400, 15002, 1151, 7525, 584, 51564], "temperature": 0.0, "avg_logprob": -0.097594420115153, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.010648615658283234}, {"id": 273, "seek": 164448, "start": 1668.48, "end": 1673.68, "text": " that this is some of the most important content to convey because it gives people sort of important", "tokens": [51564, 300, 341, 307, 512, 295, 264, 881, 1021, 2701, 281, 16965, 570, 309, 2709, 561, 1333, 295, 1021, 51824], "temperature": 0.0, "avg_logprob": -0.097594420115153, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.010648615658283234}, {"id": 274, "seek": 167368, "start": 1673.76, "end": 1679.2, "text": " milestones and landmarks. One level above that are the sort of statistical properties like minimum,", "tokens": [50368, 42038, 293, 26962, 82, 13, 1485, 1496, 3673, 300, 366, 264, 1333, 295, 22820, 7221, 411, 7285, 11, 50640], "temperature": 0.0, "avg_logprob": -0.06983155071145238, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.0015007774345576763}, {"id": 275, "seek": 167368, "start": 1679.2, "end": 1684.88, "text": " maximum, outliers, and things like that. And then one level above that is probably what is cited", "tokens": [50640, 6674, 11, 484, 23646, 11, 293, 721, 411, 300, 13, 400, 550, 472, 1496, 3673, 300, 307, 1391, 437, 307, 30134, 50924], "temperature": 0.0, "avg_logprob": -0.06983155071145238, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.0015007774345576763}, {"id": 276, "seek": 167368, "start": 1684.88, "end": 1690.0800000000002, "text": " people we consider the real value of visualization to be. The perceptual and cognitive characteristics", "tokens": [50924, 561, 321, 1949, 264, 957, 2158, 295, 25801, 281, 312, 13, 440, 43276, 901, 293, 15605, 10891, 51184], "temperature": 0.0, "avg_logprob": -0.06983155071145238, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.0015007774345576763}, {"id": 277, "seek": 167368, "start": 1690.0800000000002, "end": 1695.68, "text": " like complex trends and patterns, things that automated statistical methods we typically think", "tokens": [51184, 411, 3997, 13892, 293, 8294, 11, 721, 300, 18473, 22820, 7150, 321, 5850, 519, 51464], "temperature": 0.0, "avg_logprob": -0.06983155071145238, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.0015007774345576763}, {"id": 278, "seek": 167368, "start": 1695.68, "end": 1701.52, "text": " of as not being sufficient at. And then finally, the fourth and highest level are what journalists", "tokens": [51464, 295, 382, 406, 885, 11563, 412, 13, 400, 550, 2721, 11, 264, 6409, 293, 6343, 1496, 366, 437, 19535, 51756], "temperature": 0.0, "avg_logprob": -0.06983155071145238, "compression_ratio": 1.7544483985765125, "no_speech_prob": 0.0015007774345576763}, {"id": 279, "seek": 170152, "start": 1701.52, "end": 1706.24, "text": " often consider to be the real value of visualization, which is the narration that gets associated", "tokens": [50364, 2049, 1949, 281, 312, 264, 957, 2158, 295, 25801, 11, 597, 307, 264, 43299, 300, 2170, 6615, 50600], "temperature": 0.0, "avg_logprob": -0.08365949562617711, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.00026115518994629383}, {"id": 280, "seek": 170152, "start": 1706.24, "end": 1710.8, "text": " with it. What is the data story that you're able to tell through the visualization? Can you explain", "tokens": [50600, 365, 309, 13, 708, 307, 264, 1412, 1657, 300, 291, 434, 1075, 281, 980, 807, 264, 25801, 30, 1664, 291, 2903, 50828], "temperature": 0.0, "avg_logprob": -0.08365949562617711, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.00026115518994629383}, {"id": 281, "seek": 170152, "start": 1710.8, "end": 1716.6399999999999, "text": " what you're seeing, the causal mechanisms, etc., etc. Now, another reason I think LLMs are really", "tokens": [50828, 437, 291, 434, 2577, 11, 264, 38755, 15902, 11, 5183, 7933, 5183, 13, 823, 11, 1071, 1778, 286, 519, 441, 43, 26386, 366, 534, 51120], "temperature": 0.0, "avg_logprob": -0.08365949562617711, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.00026115518994629383}, {"id": 282, "seek": 170152, "start": 1716.6399999999999, "end": 1722.8799999999999, "text": " suited for this sort of semantic bridging task is because when we asked sighted and blind people", "tokens": [51120, 24736, 337, 341, 1333, 295, 47982, 16362, 3249, 5633, 307, 570, 562, 321, 2351, 7860, 292, 293, 6865, 561, 51432], "temperature": 0.0, "avg_logprob": -0.08365949562617711, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.00026115518994629383}, {"id": 283, "seek": 170152, "start": 1722.8799999999999, "end": 1728.56, "text": " what their preferences were, when it came to these four layers, four levels, we saw really", "tokens": [51432, 437, 641, 21910, 645, 11, 562, 309, 1361, 281, 613, 1451, 7914, 11, 1451, 4358, 11, 321, 1866, 534, 51716], "temperature": 0.0, "avg_logprob": -0.08365949562617711, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.00026115518994629383}, {"id": 284, "seek": 172856, "start": 1728.56, "end": 1735.6, "text": " distinct preferences. In the case of sighted people, because we've got our own visual perception", "tokens": [50364, 10644, 21910, 13, 682, 264, 1389, 295, 7860, 292, 561, 11, 570, 321, 600, 658, 527, 1065, 5056, 12860, 50716], "temperature": 0.0, "avg_logprob": -0.07727800716053355, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.001597433933056891}, {"id": 285, "seek": 172856, "start": 1736.6399999999999, "end": 1743.12, "text": " doing that sort of bridging of the Gulf of Evaluation, sighted people tended to want higher", "tokens": [50768, 884, 300, 1333, 295, 16362, 3249, 295, 264, 23033, 295, 462, 46504, 11, 7860, 292, 561, 34732, 281, 528, 2946, 51092], "temperature": 0.0, "avg_logprob": -0.07727800716053355, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.001597433933056891}, {"id": 286, "seek": 172856, "start": 1743.12, "end": 1748.24, "text": " and higher levels of content being conveyed through text. Blind readers, on the other hand,", "tokens": [51092, 293, 2946, 4358, 295, 2701, 885, 49340, 807, 2487, 13, 34126, 17147, 11, 322, 264, 661, 1011, 11, 51348], "temperature": 0.0, "avg_logprob": -0.07727800716053355, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.001597433933056891}, {"id": 287, "seek": 172856, "start": 1748.24, "end": 1754.0, "text": " were pretty significantly divergent. For many of them, they didn't want those level three and four,", "tokens": [51348, 645, 1238, 10591, 18558, 6930, 13, 1171, 867, 295, 552, 11, 436, 994, 380, 528, 729, 1496, 1045, 293, 1451, 11, 51636], "temperature": 0.0, "avg_logprob": -0.07727800716053355, "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.001597433933056891}, {"id": 288, "seek": 175400, "start": 1754.0, "end": 1758.8, "text": " particularly the level four captions at all, because they wanted that time and space to do", "tokens": [50364, 4098, 264, 1496, 1451, 44832, 412, 439, 11, 570, 436, 1415, 300, 565, 293, 1901, 281, 360, 50604], "temperature": 0.0, "avg_logprob": -0.0734658187085932, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0022511950228363276}, {"id": 289, "seek": 175400, "start": 1758.8, "end": 1768.08, "text": " the interpretation for themselves. And so here, this visualization to me conveys that LLMs can", "tokens": [50604, 264, 14174, 337, 2969, 13, 400, 370, 510, 11, 341, 25801, 281, 385, 18053, 749, 300, 441, 43, 26386, 393, 51068], "temperature": 0.0, "avg_logprob": -0.0734658187085932, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0022511950228363276}, {"id": 290, "seek": 175400, "start": 1768.08, "end": 1773.2, "text": " help us or machine learning models can help us think about sort of personalizing the semantics", "tokens": [51068, 854, 505, 420, 3479, 2539, 5245, 393, 854, 505, 519, 466, 1333, 295, 2973, 3319, 264, 4361, 45298, 51324], "temperature": 0.0, "avg_logprob": -0.0734658187085932, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0022511950228363276}, {"id": 291, "seek": 175400, "start": 1773.2, "end": 1779.12, "text": " of a user interface in a way that maybe we haven't had the opportunity to study so far. There's been", "tokens": [51324, 295, 257, 4195, 9226, 294, 257, 636, 300, 1310, 321, 2378, 380, 632, 264, 2650, 281, 2979, 370, 1400, 13, 821, 311, 668, 51620], "temperature": 0.0, "avg_logprob": -0.0734658187085932, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0022511950228363276}, {"id": 292, "seek": 177912, "start": 1779.12, "end": 1784.3999999999999, "text": " a lot of work in personalization, but it's often been at that level of the articulation,", "tokens": [50364, 257, 688, 295, 589, 294, 2973, 2144, 11, 457, 309, 311, 2049, 668, 412, 300, 1496, 295, 264, 15228, 2776, 11, 50628], "temperature": 0.0, "avg_logprob": -0.07637758336515509, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.004608211573213339}, {"id": 293, "seek": 177912, "start": 1784.3999999999999, "end": 1789.52, "text": " changing the sizes of buttons and adapting color palettes and things like that. And there's maybe", "tokens": [50628, 4473, 264, 11602, 295, 9905, 293, 34942, 2017, 3984, 16049, 293, 721, 411, 300, 13, 400, 456, 311, 1310, 50884], "temperature": 0.0, "avg_logprob": -0.07637758336515509, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.004608211573213339}, {"id": 294, "seek": 177912, "start": 1789.52, "end": 1797.76, "text": " an opportunity now to use LLMs to actually change what the nouns, the verbs, the concepts of a user", "tokens": [50884, 364, 2650, 586, 281, 764, 441, 43, 26386, 281, 767, 1319, 437, 264, 48184, 11, 264, 30051, 11, 264, 10392, 295, 257, 4195, 51296], "temperature": 0.0, "avg_logprob": -0.07637758336515509, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.004608211573213339}, {"id": 295, "seek": 177912, "start": 1797.76, "end": 1803.12, "text": " interface are much more fundamentally. And so the way we're going about doing this in the case of", "tokens": [51296, 9226, 366, 709, 544, 17879, 13, 400, 370, 264, 636, 321, 434, 516, 466, 884, 341, 294, 264, 1389, 295, 51564], "temperature": 0.0, "avg_logprob": -0.07637758336515509, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.004608211573213339}, {"id": 296, "seek": 177912, "start": 1803.12, "end": 1808.9599999999998, "text": " textual descriptions is we're going to be releasing very soon a data set of about, actually now we're", "tokens": [51564, 2487, 901, 24406, 307, 321, 434, 516, 281, 312, 16327, 588, 2321, 257, 1412, 992, 295, 466, 11, 767, 586, 321, 434, 51856], "temperature": 0.0, "avg_logprob": -0.07637758336515509, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.004608211573213339}, {"id": 297, "seek": 180896, "start": 1808.96, "end": 1813.76, "text": " over 12,000 pairs of chart captions. And we've generated some of these captions and we've crowdsourced", "tokens": [50364, 670, 2272, 11, 1360, 15494, 295, 6927, 44832, 13, 400, 321, 600, 10833, 512, 295, 613, 44832, 293, 321, 600, 26070, 396, 1232, 50604], "temperature": 0.0, "avg_logprob": -0.0737334220640121, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.000803865201305598}, {"id": 298, "seek": 180896, "start": 1813.76, "end": 1819.2, "text": " some of these captions. And we started to train baseline models to do this task. And one of the", "tokens": [50604, 512, 295, 613, 44832, 13, 400, 321, 1409, 281, 3847, 20518, 5245, 281, 360, 341, 5633, 13, 400, 472, 295, 264, 50876], "temperature": 0.0, "avg_logprob": -0.0737334220640121, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.000803865201305598}, {"id": 299, "seek": 180896, "start": 1819.2, "end": 1825.92, "text": " interesting features here is how do we represent the semantics of a chart to a large language model,", "tokens": [50876, 1880, 4122, 510, 307, 577, 360, 321, 2906, 264, 4361, 45298, 295, 257, 6927, 281, 257, 2416, 2856, 2316, 11, 51212], "temperature": 0.0, "avg_logprob": -0.0737334220640121, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.000803865201305598}, {"id": 300, "seek": 180896, "start": 1825.92, "end": 1830.64, "text": " right? One way could just be let's treat the chart as an image, right? This is just a set of pixels.", "tokens": [51212, 558, 30, 1485, 636, 727, 445, 312, 718, 311, 2387, 264, 6927, 382, 364, 3256, 11, 558, 30, 639, 307, 445, 257, 992, 295, 18668, 13, 51448], "temperature": 0.0, "avg_logprob": -0.0737334220640121, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.000803865201305598}, {"id": 301, "seek": 180896, "start": 1831.52, "end": 1836.96, "text": " And unsurprisingly, the baseline models don't do very well at that because a chart is a much richer", "tokens": [51492, 400, 2693, 374, 34408, 11, 264, 20518, 5245, 500, 380, 360, 588, 731, 412, 300, 570, 257, 6927, 307, 257, 709, 29021, 51764], "temperature": 0.0, "avg_logprob": -0.0737334220640121, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.000803865201305598}, {"id": 302, "seek": 183696, "start": 1836.96, "end": 1841.92, "text": " kind of artifact than just an image, right? It's got all this rich structure. So then we said,", "tokens": [50364, 733, 295, 34806, 813, 445, 364, 3256, 11, 558, 30, 467, 311, 658, 439, 341, 4593, 3877, 13, 407, 550, 321, 848, 11, 50612], "temperature": 0.0, "avg_logprob": -0.09294751485188803, "compression_ratio": 1.6288659793814433, "no_speech_prob": 0.00019712859648279846}, {"id": 303, "seek": 183696, "start": 1841.92, "end": 1846.16, "text": " great, let's look at a data table or let's look at a scene graph, which is just a fancy way of", "tokens": [50612, 869, 11, 718, 311, 574, 412, 257, 1412, 3199, 420, 718, 311, 574, 412, 257, 4145, 4295, 11, 597, 307, 445, 257, 10247, 636, 295, 50824], "temperature": 0.0, "avg_logprob": -0.09294751485188803, "compression_ratio": 1.6288659793814433, "no_speech_prob": 0.00019712859648279846}, {"id": 304, "seek": 183696, "start": 1846.16, "end": 1851.2, "text": " saying the SVG associated with the chart. And a priori, we would have thought, well, the scene", "tokens": [50824, 1566, 264, 31910, 38, 6615, 365, 264, 6927, 13, 400, 257, 4059, 72, 11, 321, 576, 362, 1194, 11, 731, 11, 264, 4145, 51076], "temperature": 0.0, "avg_logprob": -0.09294751485188803, "compression_ratio": 1.6288659793814433, "no_speech_prob": 0.00019712859648279846}, {"id": 305, "seek": 183696, "start": 1851.2, "end": 1856.4, "text": " graph is maybe like a good in-between between the computational affordances of data table and", "tokens": [51076, 4295, 307, 1310, 411, 257, 665, 294, 12, 32387, 1296, 264, 28270, 6157, 2676, 295, 1412, 3199, 293, 51336], "temperature": 0.0, "avg_logprob": -0.09294751485188803, "compression_ratio": 1.6288659793814433, "no_speech_prob": 0.00019712859648279846}, {"id": 306, "seek": 183696, "start": 1856.4, "end": 1862.48, "text": " capturing some of those perceptual characteristics. Turns out for the LLMs we trained that were", "tokens": [51336, 23384, 512, 295, 729, 43276, 901, 10891, 13, 29524, 484, 337, 264, 441, 43, 26386, 321, 8895, 300, 645, 51640], "temperature": 0.0, "avg_logprob": -0.09294751485188803, "compression_ratio": 1.6288659793814433, "no_speech_prob": 0.00019712859648279846}, {"id": 307, "seek": 186248, "start": 1862.48, "end": 1867.3600000000001, "text": " all transformer models, they did equivalently well on those two representations. And so one of", "tokens": [50364, 439, 31782, 5245, 11, 436, 630, 9052, 2276, 731, 322, 729, 732, 33358, 13, 400, 370, 472, 295, 50608], "temperature": 0.0, "avg_logprob": -0.09112712189003273, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.00145476043689996}, {"id": 308, "seek": 186248, "start": 1867.3600000000001, "end": 1872.8, "text": " the things my group is working on right now is a new way of representing visualizations that more", "tokens": [50608, 264, 721, 452, 1594, 307, 1364, 322, 558, 586, 307, 257, 777, 636, 295, 13460, 5056, 14455, 300, 544, 50880], "temperature": 0.0, "avg_logprob": -0.09112712189003273, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.00145476043689996}, {"id": 309, "seek": 186248, "start": 1872.8, "end": 1879.1200000000001, "text": " directly encode some of those perceptual operations that are otherwise currently implicit in a scene", "tokens": [50880, 3838, 2058, 1429, 512, 295, 729, 43276, 901, 7705, 300, 366, 5911, 4362, 26947, 294, 257, 4145, 51196], "temperature": 0.0, "avg_logprob": -0.09112712189003273, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.00145476043689996}, {"id": 310, "seek": 186248, "start": 1879.1200000000001, "end": 1886.4, "text": " graph that grammar of graphics libraries like VegaLite or GGplot perform. But what's interesting", "tokens": [51196, 4295, 300, 22317, 295, 11837, 15148, 411, 48796, 43, 642, 420, 42240, 564, 310, 2042, 13, 583, 437, 311, 1880, 51560], "temperature": 0.0, "avg_logprob": -0.09112712189003273, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.00145476043689996}, {"id": 311, "seek": 186248, "start": 1886.4, "end": 1891.68, "text": " in all of this to me is that through these generative models, the goal has been how do we impose", "tokens": [51560, 294, 439, 295, 341, 281, 385, 307, 300, 807, 613, 1337, 1166, 5245, 11, 264, 3387, 575, 668, 577, 360, 321, 26952, 51824], "temperature": 0.0, "avg_logprob": -0.09112712189003273, "compression_ratio": 1.685121107266436, "no_speech_prob": 0.00145476043689996}, {"id": 312, "seek": 189168, "start": 1891.76, "end": 1898.16, "text": " a conceptual model onto them, right? How do we bring some explicit semantics? And I think we're", "tokens": [50368, 257, 24106, 2316, 3911, 552, 11, 558, 30, 1012, 360, 321, 1565, 512, 13691, 4361, 45298, 30, 400, 286, 519, 321, 434, 50688], "temperature": 0.0, "avg_logprob": -0.06680303149753147, "compression_ratio": 1.6870748299319729, "no_speech_prob": 0.0006876839906908572}, {"id": 313, "seek": 189168, "start": 1898.16, "end": 1903.68, "text": " just scratching the surface here as well because I think the chart example case is a really great", "tokens": [50688, 445, 29699, 264, 3753, 510, 382, 731, 570, 286, 519, 264, 6927, 1365, 1389, 307, 257, 534, 869, 50964], "temperature": 0.0, "avg_logprob": -0.06680303149753147, "compression_ratio": 1.6870748299319729, "no_speech_prob": 0.0006876839906908572}, {"id": 314, "seek": 189168, "start": 1903.68, "end": 1908.24, "text": " one where a lot of these representations of charts that we've got right now, the grammar of graphics,", "tokens": [50964, 472, 689, 257, 688, 295, 613, 33358, 295, 17767, 300, 321, 600, 658, 558, 586, 11, 264, 22317, 295, 11837, 11, 51192], "temperature": 0.0, "avg_logprob": -0.06680303149753147, "compression_ratio": 1.6870748299319729, "no_speech_prob": 0.0006876839906908572}, {"id": 315, "seek": 189168, "start": 1908.24, "end": 1912.96, "text": " for instance, were designed for people to author, right? So we're really good at figuring out how", "tokens": [51192, 337, 5197, 11, 645, 4761, 337, 561, 281, 3793, 11, 558, 30, 407, 321, 434, 534, 665, 412, 15213, 484, 577, 51428], "temperature": 0.0, "avg_logprob": -0.06680303149753147, "compression_ratio": 1.6870748299319729, "no_speech_prob": 0.0006876839906908572}, {"id": 316, "seek": 189168, "start": 1912.96, "end": 1918.0800000000002, "text": " to design programming languages, domain specific languages, to emphasize the cognitive characteristics", "tokens": [51428, 281, 1715, 9410, 8650, 11, 9274, 2685, 8650, 11, 281, 16078, 264, 15605, 10891, 51684], "temperature": 0.0, "avg_logprob": -0.06680303149753147, "compression_ratio": 1.6870748299319729, "no_speech_prob": 0.0006876839906908572}, {"id": 317, "seek": 191808, "start": 1918.08, "end": 1922.56, "text": " that are important for human authors. Things like, you know, the cognitive dimensions of notation", "tokens": [50364, 300, 366, 1021, 337, 1952, 16552, 13, 9514, 411, 11, 291, 458, 11, 264, 15605, 12819, 295, 24657, 50588], "temperature": 0.0, "avg_logprob": -0.06376931044432493, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0005526767345145345}, {"id": 318, "seek": 191808, "start": 1922.56, "end": 1926.8799999999999, "text": " that cares about, you know, how viscous is the programming language? How many premature commitments", "tokens": [50588, 300, 12310, 466, 11, 291, 458, 11, 577, 1452, 39939, 307, 264, 9410, 2856, 30, 1012, 867, 34877, 26230, 50804], "temperature": 0.0, "avg_logprob": -0.06376931044432493, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0005526767345145345}, {"id": 319, "seek": 191808, "start": 1926.8799999999999, "end": 1931.84, "text": " is the programming language enforced? But I don't know what it means to design a representation", "tokens": [50804, 307, 264, 9410, 2856, 40953, 30, 583, 286, 500, 380, 458, 437, 309, 1355, 281, 1715, 257, 10290, 51052], "temperature": 0.0, "avg_logprob": -0.06376931044432493, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0005526767345145345}, {"id": 320, "seek": 191808, "start": 1931.84, "end": 1938.24, "text": " to be suitable for an LLM to operate over, right? Do we restructure the programming language more", "tokens": [51052, 281, 312, 12873, 337, 364, 441, 43, 44, 281, 9651, 670, 11, 558, 30, 1144, 321, 1472, 2885, 264, 9410, 2856, 544, 51372], "temperature": 0.0, "avg_logprob": -0.06376931044432493, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0005526767345145345}, {"id": 321, "seek": 191808, "start": 1938.24, "end": 1944.72, "text": " fundamentally to make it tractable for an LLM? Maybe. So in addition to generative models,", "tokens": [51372, 17879, 281, 652, 309, 24207, 712, 337, 364, 441, 43, 44, 30, 2704, 13, 407, 294, 4500, 281, 1337, 1166, 5245, 11, 51696], "temperature": 0.0, "avg_logprob": -0.06376931044432493, "compression_ratio": 1.740072202166065, "no_speech_prob": 0.0005526767345145345}, {"id": 322, "seek": 194472, "start": 1944.72, "end": 1950.56, "text": " my group has also been working with predictive models. And here I think the bridging task is", "tokens": [50364, 452, 1594, 575, 611, 668, 1364, 365, 35521, 5245, 13, 400, 510, 286, 519, 264, 16362, 3249, 5633, 307, 50656], "temperature": 0.0, "avg_logprob": -0.05383929146660699, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.0010647914605215192}, {"id": 323, "seek": 194472, "start": 1950.56, "end": 1957.04, "text": " really not about imposing a conceptual model, but bridging it or aligning it to the ones that we", "tokens": [50656, 534, 406, 466, 40288, 257, 24106, 2316, 11, 457, 16362, 3249, 309, 420, 419, 9676, 309, 281, 264, 2306, 300, 321, 50980], "temperature": 0.0, "avg_logprob": -0.05383929146660699, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.0010647914605215192}, {"id": 324, "seek": 194472, "start": 1957.04, "end": 1962.64, "text": " already have. And often the way that a lot of this work happens is through the lens of model", "tokens": [50980, 1217, 362, 13, 400, 2049, 264, 636, 300, 257, 688, 295, 341, 589, 2314, 307, 807, 264, 6765, 295, 2316, 51260], "temperature": 0.0, "avg_logprob": -0.05383929146660699, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.0010647914605215192}, {"id": 325, "seek": 194472, "start": 1962.64, "end": 1970.48, "text": " interpretability. So here is a very popular set of techniques called saliency maps. The idea behind", "tokens": [51260, 7302, 2310, 13, 407, 510, 307, 257, 588, 3743, 992, 295, 7512, 1219, 1845, 7848, 11317, 13, 440, 1558, 2261, 51652], "temperature": 0.0, "avg_logprob": -0.05383929146660699, "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.0010647914605215192}, {"id": 326, "seek": 197048, "start": 1970.48, "end": 1978.0, "text": " saliency maps is they're trying to depict the most important input features for a particular", "tokens": [50364, 1845, 7848, 11317, 307, 436, 434, 1382, 281, 31553, 264, 881, 1021, 4846, 4122, 337, 257, 1729, 50740], "temperature": 0.0, "avg_logprob": -0.06151629487673441, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.001324628246948123}, {"id": 327, "seek": 197048, "start": 1978.0, "end": 1983.28, "text": " outcome. So in this case, you know, this is an image, the label should be toy terrier, and here's", "tokens": [50740, 9700, 13, 407, 294, 341, 1389, 11, 291, 458, 11, 341, 307, 364, 3256, 11, 264, 7645, 820, 312, 12058, 1796, 7326, 11, 293, 510, 311, 51004], "temperature": 0.0, "avg_logprob": -0.06151629487673441, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.001324628246948123}, {"id": 328, "seek": 197048, "start": 1983.28, "end": 1988.56, "text": " what a variety of different kinds of saliency methods believe to be, you know, the most important", "tokens": [51004, 437, 257, 5673, 295, 819, 3685, 295, 1845, 7848, 7150, 1697, 281, 312, 11, 291, 458, 11, 264, 881, 1021, 51268], "temperature": 0.0, "avg_logprob": -0.06151629487673441, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.001324628246948123}, {"id": 329, "seek": 197048, "start": 1988.56, "end": 1997.84, "text": " pixels to produce that outcome. Now, I look at these visualizations and I go, well, you know,", "tokens": [51268, 18668, 281, 5258, 300, 9700, 13, 823, 11, 286, 574, 412, 613, 5056, 14455, 293, 286, 352, 11, 731, 11, 291, 458, 11, 51732], "temperature": 0.0, "avg_logprob": -0.06151629487673441, "compression_ratio": 1.646551724137931, "no_speech_prob": 0.001324628246948123}, {"id": 330, "seek": 199784, "start": 1997.84, "end": 2004.3999999999999, "text": " is it telling me something? Maybe, right? And maybe the reason I believe it's telling me something", "tokens": [50364, 307, 309, 3585, 385, 746, 30, 2704, 11, 558, 30, 400, 1310, 264, 1778, 286, 1697, 309, 311, 3585, 385, 746, 50692], "temperature": 0.0, "avg_logprob": -0.07450003623962402, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0018670130521059036}, {"id": 331, "seek": 199784, "start": 2004.3999999999999, "end": 2010.72, "text": " is because I'm the one doing the perception and interpretive tasks, right? Like if I look at some", "tokens": [50692, 307, 570, 286, 478, 264, 472, 884, 264, 12860, 293, 7302, 488, 9608, 11, 558, 30, 1743, 498, 286, 574, 412, 512, 51008], "temperature": 0.0, "avg_logprob": -0.07450003623962402, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0018670130521059036}, {"id": 332, "seek": 199784, "start": 2010.72, "end": 2015.12, "text": " of those visualizations on the bottom, I go, oh, like, it looks like the dog snout is really", "tokens": [51008, 295, 729, 5056, 14455, 322, 264, 2767, 11, 286, 352, 11, 1954, 11, 411, 11, 309, 1542, 411, 264, 3000, 2406, 346, 307, 534, 51228], "temperature": 0.0, "avg_logprob": -0.07450003623962402, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0018670130521059036}, {"id": 333, "seek": 199784, "start": 2015.12, "end": 2020.56, "text": " important to the classification of a toy terrier or the spots. But it's not actually the saliency", "tokens": [51228, 1021, 281, 264, 21538, 295, 257, 12058, 1796, 7326, 420, 264, 10681, 13, 583, 309, 311, 406, 767, 264, 1845, 7848, 51500], "temperature": 0.0, "avg_logprob": -0.07450003623962402, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0018670130521059036}, {"id": 334, "seek": 199784, "start": 2020.56, "end": 2025.6, "text": " method that is doing that interpretation for me. I'm the one bringing meaning to those lit pixels,", "tokens": [51500, 3170, 300, 307, 884, 300, 14174, 337, 385, 13, 286, 478, 264, 472, 5062, 3620, 281, 729, 7997, 18668, 11, 51752], "temperature": 0.0, "avg_logprob": -0.07450003623962402, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.0018670130521059036}, {"id": 335, "seek": 202560, "start": 2026.1599999999999, "end": 2032.32, "text": " right? And so as a result, if we think about that gulf of evaluation, it's not the saliency", "tokens": [50392, 558, 30, 400, 370, 382, 257, 1874, 11, 498, 321, 519, 466, 300, 290, 5757, 295, 13344, 11, 309, 311, 406, 264, 1845, 7848, 50700], "temperature": 0.0, "avg_logprob": -0.07278073535246007, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0008556445245631039}, {"id": 336, "seek": 202560, "start": 2032.32, "end": 2038.08, "text": " method that's helping bridge that gulf in any way, which is why saliency maps for now have been", "tokens": [50700, 3170, 300, 311, 4315, 7283, 300, 290, 5757, 294, 604, 636, 11, 597, 307, 983, 1845, 7848, 11317, 337, 586, 362, 668, 50988], "temperature": 0.0, "avg_logprob": -0.07278073535246007, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0008556445245631039}, {"id": 337, "seek": 202560, "start": 2038.08, "end": 2043.84, "text": " these tools that we just use in a very ad hoc way that require a lot of manual effort to make sense", "tokens": [50988, 613, 3873, 300, 321, 445, 764, 294, 257, 588, 614, 16708, 636, 300, 3651, 257, 688, 295, 9688, 4630, 281, 652, 2020, 51276], "temperature": 0.0, "avg_logprob": -0.07278073535246007, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0008556445245631039}, {"id": 338, "seek": 202560, "start": 2043.84, "end": 2050.7999999999997, "text": " of. And so a question that my student, Angie Boggast, has been focused on is how do we scaffold", "tokens": [51276, 295, 13, 400, 370, 257, 1168, 300, 452, 3107, 11, 48829, 24339, 70, 525, 11, 575, 668, 5178, 322, 307, 577, 360, 321, 44094, 51624], "temperature": 0.0, "avg_logprob": -0.07278073535246007, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0008556445245631039}, {"id": 339, "seek": 205080, "start": 2050.8, "end": 2055.92, "text": " that semantic sense making operation, right? Providing some additional structure to help", "tokens": [50364, 300, 47982, 2020, 1455, 6916, 11, 558, 30, 15685, 2819, 512, 4497, 3877, 281, 854, 50620], "temperature": 0.0, "avg_logprob": -0.08073742606423118, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0019262520363554358}, {"id": 340, "seek": 205080, "start": 2056.5600000000004, "end": 2061.6800000000003, "text": " sort of scale it up to make it more reproducible and things like that. And what she's developed", "tokens": [50652, 1333, 295, 4373, 309, 493, 281, 652, 309, 544, 11408, 32128, 293, 721, 411, 300, 13, 400, 437, 750, 311, 4743, 50908], "temperature": 0.0, "avg_logprob": -0.08073742606423118, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0019262520363554358}, {"id": 341, "seek": 205080, "start": 2061.6800000000003, "end": 2067.44, "text": " is these set of metrics that are very analogous to ideas of precision and recall, but are operating", "tokens": [50908, 307, 613, 992, 295, 16367, 300, 366, 588, 16660, 563, 281, 3487, 295, 18356, 293, 9901, 11, 457, 366, 7447, 51196], "temperature": 0.0, "avg_logprob": -0.08073742606423118, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0019262520363554358}, {"id": 342, "seek": 205080, "start": 2067.44, "end": 2072.96, "text": " at the level of input features and interpretability. So in many data sets, right, you've got some set", "tokens": [51196, 412, 264, 1496, 295, 4846, 4122, 293, 7302, 2310, 13, 407, 294, 867, 1412, 6352, 11, 558, 11, 291, 600, 658, 512, 992, 51472], "temperature": 0.0, "avg_logprob": -0.08073742606423118, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0019262520363554358}, {"id": 343, "seek": 205080, "start": 2072.96, "end": 2078.0800000000004, "text": " of ground truth human annotated features. And what shared interest is looking at is what is the", "tokens": [51472, 295, 2727, 3494, 1952, 25339, 770, 4122, 13, 400, 437, 5507, 1179, 307, 1237, 412, 307, 437, 307, 264, 51728], "temperature": 0.0, "avg_logprob": -0.08073742606423118, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0019262520363554358}, {"id": 344, "seek": 207808, "start": 2078.08, "end": 2083.2, "text": " overlap between what a saliency method considers as being important to the classification and what", "tokens": [50364, 19959, 1296, 437, 257, 1845, 7848, 3170, 33095, 382, 885, 1021, 281, 264, 21538, 293, 437, 50620], "temperature": 0.0, "avg_logprob": -0.0711088634672619, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00043049806845374405}, {"id": 345, "seek": 207808, "start": 2083.2, "end": 2088.24, "text": " the humans, the human annotators thought was important. And there's actually three different", "tokens": [50620, 264, 6255, 11, 264, 1952, 25339, 3391, 1194, 390, 1021, 13, 400, 456, 311, 767, 1045, 819, 50872], "temperature": 0.0, "avg_logprob": -0.0711088634672619, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00043049806845374405}, {"id": 346, "seek": 207808, "start": 2088.24, "end": 2094.24, "text": " ways that these overlaps can manifest. The first is a sort of ground truth coverage, which is very", "tokens": [50872, 2098, 300, 613, 15986, 2382, 393, 10067, 13, 440, 700, 307, 257, 1333, 295, 2727, 3494, 9645, 11, 597, 307, 588, 51172], "temperature": 0.0, "avg_logprob": -0.0711088634672619, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00043049806845374405}, {"id": 347, "seek": 207808, "start": 2094.24, "end": 2099.36, "text": " analogous to ideas of recall, right? It's how much of the ground truth does the model incorporate", "tokens": [51172, 16660, 563, 281, 3487, 295, 9901, 11, 558, 30, 467, 311, 577, 709, 295, 264, 2727, 3494, 775, 264, 2316, 16091, 51428], "temperature": 0.0, "avg_logprob": -0.0711088634672619, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00043049806845374405}, {"id": 348, "seek": 207808, "start": 2099.36, "end": 2104.0, "text": " in its prediction or what is the proportion of the ground truth region that is covered by the", "tokens": [51428, 294, 1080, 17630, 420, 437, 307, 264, 16068, 295, 264, 2727, 3494, 4458, 300, 307, 5343, 538, 264, 51660], "temperature": 0.0, "avg_logprob": -0.0711088634672619, "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.00043049806845374405}, {"id": 349, "seek": 210400, "start": 2104.0, "end": 2109.2, "text": " saliency region. And if we look at some examples of low coverage on the top and high coverage at the", "tokens": [50364, 1845, 7848, 4458, 13, 400, 498, 321, 574, 412, 512, 5110, 295, 2295, 9645, 322, 264, 1192, 293, 1090, 9645, 412, 264, 50624], "temperature": 0.0, "avg_logprob": -0.07091741726316254, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.0007320499862544239}, {"id": 350, "seek": 210400, "start": 2109.2, "end": 2114.64, "text": " bottom, we can see then the case of low ground truth coverage is actually very little overlap,", "tokens": [50624, 2767, 11, 321, 393, 536, 550, 264, 1389, 295, 2295, 2727, 3494, 9645, 307, 767, 588, 707, 19959, 11, 50896], "temperature": 0.0, "avg_logprob": -0.07091741726316254, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.0007320499862544239}, {"id": 351, "seek": 210400, "start": 2114.64, "end": 2120.4, "text": " right, between the ground truth, the yellow region, and the salient region in orange. But I often find", "tokens": [50896, 558, 11, 1296, 264, 2727, 3494, 11, 264, 5566, 4458, 11, 293, 264, 1845, 1196, 4458, 294, 7671, 13, 583, 286, 2049, 915, 51184], "temperature": 0.0, "avg_logprob": -0.07091741726316254, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.0007320499862544239}, {"id": 352, "seek": 210400, "start": 2120.4, "end": 2125.6, "text": " that it's actually the high coverage regions that are more interesting to analyze. So if we compare,", "tokens": [51184, 300, 309, 311, 767, 264, 1090, 9645, 10682, 300, 366, 544, 1880, 281, 12477, 13, 407, 498, 321, 6794, 11, 51444], "temperature": 0.0, "avg_logprob": -0.07091741726316254, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.0007320499862544239}, {"id": 353, "seek": 210400, "start": 2125.6, "end": 2131.2, "text": " you know, cases where the model was correct on the right with the green label and cases where the", "tokens": [51444, 291, 458, 11, 3331, 689, 264, 2316, 390, 3006, 322, 264, 558, 365, 264, 3092, 7645, 293, 3331, 689, 264, 51724], "temperature": 0.0, "avg_logprob": -0.07091741726316254, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.0007320499862544239}, {"id": 354, "seek": 213120, "start": 2131.2, "end": 2136.64, "text": " model was incorrect with the red label, we can see in the case of correct high ground truth coverage,", "tokens": [50364, 2316, 390, 18424, 365, 264, 2182, 7645, 11, 321, 393, 536, 294, 264, 1389, 295, 3006, 1090, 2727, 3494, 9645, 11, 50636], "temperature": 0.0, "avg_logprob": -0.08222082683018275, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.00021652586292475462}, {"id": 355, "seek": 213120, "start": 2136.64, "end": 2141.04, "text": " there are instances where the model relies not just on the object, like in this case with the", "tokens": [50636, 456, 366, 14519, 689, 264, 2316, 30910, 406, 445, 322, 264, 2657, 11, 411, 294, 341, 1389, 365, 264, 50856], "temperature": 0.0, "avg_logprob": -0.08222082683018275, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.00021652586292475462}, {"id": 356, "seek": 213120, "start": 2141.04, "end": 2147.8399999999997, "text": " cab, but a lot of contextual information as well to ultimately make that correct prediction. But", "tokens": [50856, 5487, 11, 457, 257, 688, 295, 35526, 1589, 382, 731, 281, 6284, 652, 300, 3006, 17630, 13, 583, 51196], "temperature": 0.0, "avg_logprob": -0.08222082683018275, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.00021652586292475462}, {"id": 357, "seek": 213120, "start": 2147.8399999999997, "end": 2153.9199999999996, "text": " on the flip side, right, with the laptop, the model is doing the same thing, but here the context", "tokens": [51196, 322, 264, 7929, 1252, 11, 558, 11, 365, 264, 10732, 11, 264, 2316, 307, 884, 264, 912, 551, 11, 457, 510, 264, 4319, 51500], "temperature": 0.0, "avg_logprob": -0.08222082683018275, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.00021652586292475462}, {"id": 358, "seek": 213120, "start": 2153.9199999999996, "end": 2158.0, "text": " is actually throwing it off, right? It's actually confusing the model because it's accounting for", "tokens": [51500, 307, 767, 10238, 309, 766, 11, 558, 30, 467, 311, 767, 13181, 264, 2316, 570, 309, 311, 19163, 337, 51704], "temperature": 0.0, "avg_logprob": -0.08222082683018275, "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.00021652586292475462}, {"id": 359, "seek": 215800, "start": 2158.0, "end": 2163.68, "text": " too much of that context in its decision making. Another kind of coverage is something we call", "tokens": [50364, 886, 709, 295, 300, 4319, 294, 1080, 3537, 1455, 13, 3996, 733, 295, 9645, 307, 746, 321, 818, 50648], "temperature": 0.0, "avg_logprob": -0.06840893957349989, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.219540177378803e-05}, {"id": 360, "seek": 215800, "start": 2163.68, "end": 2168.48, "text": " saliency coverage, and this is more akin to precision, right, which is how strictly is the", "tokens": [50648, 1845, 7848, 9645, 11, 293, 341, 307, 544, 47540, 281, 18356, 11, 558, 11, 597, 307, 577, 20792, 307, 264, 50888], "temperature": 0.0, "avg_logprob": -0.06840893957349989, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.219540177378803e-05}, {"id": 361, "seek": 215800, "start": 2168.48, "end": 2174.8, "text": " model relying only on ground truth features to make its sort of prediction. And again, you know,", "tokens": [50888, 2316, 24140, 787, 322, 2727, 3494, 4122, 281, 652, 1080, 1333, 295, 17630, 13, 400, 797, 11, 291, 458, 11, 51204], "temperature": 0.0, "avg_logprob": -0.06840893957349989, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.219540177378803e-05}, {"id": 362, "seek": 215800, "start": 2174.8, "end": 2180.4, "text": " if we look at low and high coverage in the case of low coverage, we can see again pretty disjoint", "tokens": [51204, 498, 321, 574, 412, 2295, 293, 1090, 9645, 294, 264, 1389, 295, 2295, 9645, 11, 321, 393, 536, 797, 1238, 717, 48613, 51484], "temperature": 0.0, "avg_logprob": -0.06840893957349989, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.219540177378803e-05}, {"id": 363, "seek": 218040, "start": 2181.12, "end": 2187.36, "text": " sorts of sets. But in the case of the high coverage regions, we can see that, you know,", "tokens": [50400, 7527, 295, 6352, 13, 583, 294, 264, 1389, 295, 264, 1090, 9645, 10682, 11, 321, 393, 536, 300, 11, 291, 458, 11, 50712], "temperature": 0.0, "avg_logprob": -0.07301325457436698, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.05663378909230232}, {"id": 364, "seek": 218040, "start": 2187.36, "end": 2193.92, "text": " in the case of high saliency coverage, it basically means that the salient regions are a strict subset", "tokens": [50712, 294, 264, 1389, 295, 1090, 1845, 7848, 9645, 11, 309, 1936, 1355, 300, 264, 1845, 1196, 10682, 366, 257, 10910, 25993, 51040], "temperature": 0.0, "avg_logprob": -0.07301325457436698, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.05663378909230232}, {"id": 365, "seek": 218040, "start": 2193.92, "end": 2198.56, "text": " of the ground truth coverage. But the difference between a correct and incorrect prediction is", "tokens": [51040, 295, 264, 2727, 3494, 9645, 13, 583, 264, 2649, 1296, 257, 3006, 293, 18424, 17630, 307, 51272], "temperature": 0.0, "avg_logprob": -0.07301325457436698, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.05663378909230232}, {"id": 366, "seek": 218040, "start": 2198.56, "end": 2205.36, "text": " whether that subset was sufficient to make the correct classification or not, right? So in the", "tokens": [51272, 1968, 300, 25993, 390, 11563, 281, 652, 264, 3006, 21538, 420, 406, 11, 558, 30, 407, 294, 264, 51612], "temperature": 0.0, "avg_logprob": -0.07301325457436698, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.05663378909230232}, {"id": 367, "seek": 218040, "start": 2205.36, "end": 2210.2400000000002, "text": " case of the Maltese dog, it did indeed only need to look at the head to make that correct prediction.", "tokens": [51612, 1389, 295, 264, 376, 3198, 1130, 3000, 11, 309, 630, 6451, 787, 643, 281, 574, 412, 264, 1378, 281, 652, 300, 3006, 17630, 13, 51856], "temperature": 0.0, "avg_logprob": -0.07301325457436698, "compression_ratio": 1.9126984126984128, "no_speech_prob": 0.05663378909230232}, {"id": 368, "seek": 221040, "start": 2210.56, "end": 2216.96, "text": " But in the case of the Dalmatian, it probably should have accounted for more of that dog's head or", "tokens": [50372, 583, 294, 264, 1389, 295, 264, 17357, 15677, 952, 11, 309, 1391, 820, 362, 43138, 337, 544, 295, 300, 3000, 311, 1378, 420, 50692], "temperature": 0.0, "avg_logprob": -0.06809665959909422, "compression_ratio": 1.6978417266187051, "no_speech_prob": 7.721538713667542e-05}, {"id": 369, "seek": 221040, "start": 2216.96, "end": 2221.28, "text": " some of the other characteristics associated with the dog. By focusing only on the snout,", "tokens": [50692, 512, 295, 264, 661, 10891, 6615, 365, 264, 3000, 13, 3146, 8416, 787, 322, 264, 2406, 346, 11, 50908], "temperature": 0.0, "avg_logprob": -0.06809665959909422, "compression_ratio": 1.6978417266187051, "no_speech_prob": 7.721538713667542e-05}, {"id": 370, "seek": 221040, "start": 2221.28, "end": 2227.44, "text": " it ended up sort of arriving at the incorrect sort of classification. And finally, the last metric", "tokens": [50908, 309, 4590, 493, 1333, 295, 22436, 412, 264, 18424, 1333, 295, 21538, 13, 400, 2721, 11, 264, 1036, 20678, 51216], "temperature": 0.0, "avg_logprob": -0.06809665959909422, "compression_ratio": 1.6978417266187051, "no_speech_prob": 7.721538713667542e-05}, {"id": 371, "seek": 221040, "start": 2227.44, "end": 2231.92, "text": " is something that is very familiar IOU, the intersection over the union. This is sort of", "tokens": [51216, 307, 746, 300, 307, 588, 4963, 286, 4807, 11, 264, 15236, 670, 264, 11671, 13, 639, 307, 1333, 295, 51440], "temperature": 0.0, "avg_logprob": -0.06809665959909422, "compression_ratio": 1.6978417266187051, "no_speech_prob": 7.721538713667542e-05}, {"id": 372, "seek": 221040, "start": 2231.92, "end": 2236.88, "text": " the strictest shared interest metric. It's really measuring how aligned the model's behavior is", "tokens": [51440, 264, 10910, 377, 5507, 1179, 20678, 13, 467, 311, 534, 13389, 577, 17962, 264, 2316, 311, 5223, 307, 51688], "temperature": 0.0, "avg_logprob": -0.06809665959909422, "compression_ratio": 1.6978417266187051, "no_speech_prob": 7.721538713667542e-05}, {"id": 373, "seek": 223688, "start": 2236.88, "end": 2241.76, "text": " with human reasoning. So if you look at some examples, again, you know, low coverage at the", "tokens": [50364, 365, 1952, 21577, 13, 407, 498, 291, 574, 412, 512, 5110, 11, 797, 11, 291, 458, 11, 2295, 9645, 412, 264, 50608], "temperature": 0.0, "avg_logprob": -0.06785774645598039, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.004466991405934095}, {"id": 374, "seek": 223688, "start": 2241.76, "end": 2249.52, "text": " top, we can see, you know, in incorrect cases, totally distinct disjoint sets again. But in a", "tokens": [50608, 1192, 11, 321, 393, 536, 11, 291, 458, 11, 294, 18424, 3331, 11, 3879, 10644, 717, 48613, 6352, 797, 13, 583, 294, 257, 50996], "temperature": 0.0, "avg_logprob": -0.06785774645598039, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.004466991405934095}, {"id": 375, "seek": 223688, "start": 2249.52, "end": 2254.7200000000003, "text": " correct instance, I actually find that pretty interesting, right? Low IOU coverage, but it", "tokens": [50996, 3006, 5197, 11, 286, 767, 915, 300, 1238, 1880, 11, 558, 30, 17078, 286, 4807, 9645, 11, 457, 309, 51256], "temperature": 0.0, "avg_logprob": -0.06785774645598039, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.004466991405934095}, {"id": 376, "seek": 223688, "start": 2254.7200000000003, "end": 2259.92, "text": " got a correct classification. Now, one could say maybe it got lucky. But potentially, what", "tokens": [51256, 658, 257, 3006, 21538, 13, 823, 11, 472, 727, 584, 1310, 309, 658, 6356, 13, 583, 7263, 11, 437, 51516], "temperature": 0.0, "avg_logprob": -0.06785774645598039, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.004466991405934095}, {"id": 377, "seek": 223688, "start": 2259.92, "end": 2266.32, "text": " the signal there is, is that maybe all the model needs is a tiny bit of a wheel associated with", "tokens": [51516, 264, 6358, 456, 307, 11, 307, 300, 1310, 439, 264, 2316, 2203, 307, 257, 5870, 857, 295, 257, 5589, 6615, 365, 51836], "temperature": 0.0, "avg_logprob": -0.06785774645598039, "compression_ratio": 1.6535714285714285, "no_speech_prob": 0.004466991405934095}, {"id": 378, "seek": 226632, "start": 2266.32, "end": 2272.0, "text": " a horse, right, to make the prediction that is actually a horse cart and not just a horse, right?", "tokens": [50364, 257, 6832, 11, 558, 11, 281, 652, 264, 17630, 300, 307, 767, 257, 6832, 5467, 293, 406, 445, 257, 6832, 11, 558, 30, 50648], "temperature": 0.0, "avg_logprob": -0.09296342422222269, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00023045316629577428}, {"id": 379, "seek": 226632, "start": 2272.0, "end": 2278.0800000000004, "text": " And on the flip side, with high coverage, you know, Newfoundland, great, you know, total,", "tokens": [50648, 400, 322, 264, 7929, 1252, 11, 365, 1090, 9645, 11, 291, 458, 11, 1873, 17493, 1661, 11, 869, 11, 291, 458, 11, 3217, 11, 50952], "temperature": 0.0, "avg_logprob": -0.09296342422222269, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00023045316629577428}, {"id": 380, "seek": 226632, "start": 2278.0800000000004, "end": 2282.96, "text": " total alignment. But in this case, right, incorrect classification, even though there was high", "tokens": [50952, 3217, 18515, 13, 583, 294, 341, 1389, 11, 558, 11, 18424, 21538, 11, 754, 1673, 456, 390, 1090, 51196], "temperature": 0.0, "avg_logprob": -0.09296342422222269, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00023045316629577428}, {"id": 381, "seek": 226632, "start": 2282.96, "end": 2288.56, "text": " coverage, this might suggest, you know, genuinely difficult to classify images, right, even for", "tokens": [51196, 9645, 11, 341, 1062, 3402, 11, 291, 458, 11, 17839, 2252, 281, 33872, 5267, 11, 558, 11, 754, 337, 51476], "temperature": 0.0, "avg_logprob": -0.09296342422222269, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00023045316629577428}, {"id": 382, "seek": 226632, "start": 2288.56, "end": 2293.6000000000004, "text": " people. Because if I look at that, a pickup truck seems a totally reasonable guess to have made", "tokens": [51476, 561, 13, 1436, 498, 286, 574, 412, 300, 11, 257, 25328, 5898, 2544, 257, 3879, 10585, 2041, 281, 362, 1027, 51728], "temperature": 0.0, "avg_logprob": -0.09296342422222269, "compression_ratio": 1.7236363636363636, "no_speech_prob": 0.00023045316629577428}, {"id": 383, "seek": 229360, "start": 2293.6, "end": 2297.68, "text": " about the image. I don't know that I've got enough sort of visual information there to call", "tokens": [50364, 466, 264, 3256, 13, 286, 500, 380, 458, 300, 286, 600, 658, 1547, 1333, 295, 5056, 1589, 456, 281, 818, 50568], "temperature": 0.0, "avg_logprob": -0.058023108885838434, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0035924913827329874}, {"id": 384, "seek": 229360, "start": 2297.68, "end": 2303.92, "text": " that a snowplow. So shared interest basically gives us a mechanism to start to scaffold and", "tokens": [50568, 300, 257, 5756, 564, 305, 13, 407, 5507, 1179, 1936, 2709, 505, 257, 7513, 281, 722, 281, 44094, 293, 50880], "temperature": 0.0, "avg_logprob": -0.058023108885838434, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0035924913827329874}, {"id": 385, "seek": 229360, "start": 2303.92, "end": 2309.7599999999998, "text": " structure, bridge that semantic distance, right? People no longer necessarily need to manually", "tokens": [50880, 3877, 11, 7283, 300, 47982, 4560, 11, 558, 30, 3432, 572, 2854, 4725, 643, 281, 16945, 51172], "temperature": 0.0, "avg_logprob": -0.058023108885838434, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0035924913827329874}, {"id": 386, "seek": 229360, "start": 2309.7599999999998, "end": 2315.8399999999997, "text": " start to analyze these things. And in fact, you know, we analyzed lots of different models across,", "tokens": [51172, 722, 281, 12477, 613, 721, 13, 400, 294, 1186, 11, 291, 458, 11, 321, 28181, 3195, 295, 819, 5245, 2108, 11, 51476], "temperature": 0.0, "avg_logprob": -0.058023108885838434, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0035924913827329874}, {"id": 387, "seek": 229360, "start": 2315.8399999999997, "end": 2320.7999999999997, "text": " you know, both vision and natural language and found that different combinations of these shared", "tokens": [51476, 291, 458, 11, 1293, 5201, 293, 3303, 2856, 293, 1352, 300, 819, 21267, 295, 613, 5507, 51724], "temperature": 0.0, "avg_logprob": -0.058023108885838434, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0035924913827329874}, {"id": 388, "seek": 232080, "start": 2320.8, "end": 2325.04, "text": " interest metrics, along with figuring out whether the prediction was correct or not,", "tokens": [50364, 1179, 16367, 11, 2051, 365, 15213, 484, 1968, 264, 17630, 390, 3006, 420, 406, 11, 50576], "temperature": 0.0, "avg_logprob": -0.06582012176513671, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.00043050904059782624}, {"id": 389, "seek": 232080, "start": 2325.04, "end": 2330.8, "text": " actually surfaced eight kinds of repeating patterns in model behavior. So we can see human", "tokens": [50576, 767, 9684, 3839, 3180, 3685, 295, 18617, 8294, 294, 2316, 5223, 13, 407, 321, 393, 536, 1952, 50864], "temperature": 0.0, "avg_logprob": -0.06582012176513671, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.00043050904059782624}, {"id": 390, "seek": 232080, "start": 2330.8, "end": 2336.2400000000002, "text": " aligned and some of these others we also looked at earlier, right, context confusion,", "tokens": [50864, 17962, 293, 512, 295, 613, 2357, 321, 611, 2956, 412, 3071, 11, 558, 11, 4319, 15075, 11, 51136], "temperature": 0.0, "avg_logprob": -0.06582012176513671, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.00043050904059782624}, {"id": 391, "seek": 232080, "start": 2336.2400000000002, "end": 2341.76, "text": " context dependent and so forth. And all of these give us sort of semantics that we can start to", "tokens": [51136, 4319, 12334, 293, 370, 5220, 13, 400, 439, 295, 613, 976, 505, 1333, 295, 4361, 45298, 300, 321, 393, 722, 281, 51412], "temperature": 0.0, "avg_logprob": -0.06582012176513671, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.00043050904059782624}, {"id": 392, "seek": 232080, "start": 2341.76, "end": 2347.04, "text": " play around with through different articulations. So one articulation of these semantics might be", "tokens": [51412, 862, 926, 365, 807, 819, 15228, 4136, 13, 407, 472, 15228, 2776, 295, 613, 4361, 45298, 1062, 312, 51676], "temperature": 0.0, "avg_logprob": -0.06582012176513671, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.00043050904059782624}, {"id": 393, "seek": 234704, "start": 2347.04, "end": 2352.48, "text": " a very traditional visual analytics interface, right, where I've got all the different kinds of", "tokens": [50364, 257, 588, 5164, 5056, 15370, 9226, 11, 558, 11, 689, 286, 600, 658, 439, 264, 819, 3685, 295, 50636], "temperature": 0.0, "avg_logprob": -0.061647224426269534, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.000939800520427525}, {"id": 394, "seek": 234704, "start": 2353.2, "end": 2357.92, "text": " images that I care about. This is a system we built to help a board certified dermatologist", "tokens": [50672, 5267, 300, 286, 1127, 466, 13, 639, 307, 257, 1185, 321, 3094, 281, 854, 257, 3150, 18580, 43706, 9201, 50908], "temperature": 0.0, "avg_logprob": -0.061647224426269534, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.000939800520427525}, {"id": 395, "seek": 234704, "start": 2357.92, "end": 2362.56, "text": " make sense of this melanoma detection model. And you've got, you know, query widgets on the top", "tokens": [50908, 652, 2020, 295, 341, 47969, 6440, 17784, 2316, 13, 400, 291, 600, 658, 11, 291, 458, 11, 14581, 43355, 322, 264, 1192, 51140], "temperature": 0.0, "avg_logprob": -0.061647224426269534, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.000939800520427525}, {"id": 396, "seek": 234704, "start": 2362.56, "end": 2368.72, "text": " to sort and filter. You can use, you know, these histograms of the shared interest metrics to really", "tokens": [51140, 281, 1333, 293, 6608, 13, 509, 393, 764, 11, 291, 458, 11, 613, 49816, 82, 295, 264, 5507, 1179, 16367, 281, 534, 51448], "temperature": 0.0, "avg_logprob": -0.061647224426269534, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.000939800520427525}, {"id": 397, "seek": 234704, "start": 2368.72, "end": 2373.52, "text": " drill into the data. But what was maybe most interesting was what the dermatologist said", "tokens": [51448, 11392, 666, 264, 1412, 13, 583, 437, 390, 1310, 881, 1880, 390, 437, 264, 43706, 9201, 848, 51688], "temperature": 0.0, "avg_logprob": -0.061647224426269534, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.000939800520427525}, {"id": 398, "seek": 237352, "start": 2373.52, "end": 2381.68, "text": " when they started to analyze that recurring pattern of context dependent cases. So in particular,", "tokens": [50364, 562, 436, 1409, 281, 12477, 300, 32279, 5102, 295, 4319, 12334, 3331, 13, 407, 294, 1729, 11, 50772], "temperature": 0.0, "avg_logprob": -0.08452259302139283, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0005192430689930916}, {"id": 399, "seek": 237352, "start": 2381.68, "end": 2387.7599999999998, "text": " when they switched to these context dependent cases, the dermatologist started to wonder if the", "tokens": [50772, 562, 436, 16858, 281, 613, 4319, 12334, 3331, 11, 264, 43706, 9201, 1409, 281, 2441, 498, 264, 51076], "temperature": 0.0, "avg_logprob": -0.08452259302139283, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0005192430689930916}, {"id": 400, "seek": 237352, "start": 2387.7599999999998, "end": 2392.64, "text": " model is seeing something we are not truly appreciating in the clinical image. Maybe there", "tokens": [51076, 2316, 307, 2577, 746, 321, 366, 406, 4908, 3616, 990, 294, 264, 9115, 3256, 13, 2704, 456, 51320], "temperature": 0.0, "avg_logprob": -0.08452259302139283, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0005192430689930916}, {"id": 401, "seek": 237352, "start": 2392.64, "end": 2399.2, "text": " are subtle changes we don't yet understand that the model does, right at the boundaries of the", "tokens": [51320, 366, 13743, 2962, 321, 500, 380, 1939, 1223, 300, 264, 2316, 775, 11, 558, 412, 264, 13180, 295, 264, 51648], "temperature": 0.0, "avg_logprob": -0.08452259302139283, "compression_ratio": 1.7227272727272727, "no_speech_prob": 0.0005192430689930916}, {"id": 402, "seek": 239920, "start": 2399.2799999999997, "end": 2406.24, "text": " skin region and things like that. And so, you know, to me, this is alluding to the fact of,", "tokens": [50368, 3178, 4458, 293, 721, 411, 300, 13, 400, 370, 11, 291, 458, 11, 281, 385, 11, 341, 307, 439, 33703, 281, 264, 1186, 295, 11, 50716], "temperature": 0.0, "avg_logprob": -0.08339904438365589, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0002453401393722743}, {"id": 403, "seek": 239920, "start": 2406.24, "end": 2412.3199999999997, "text": " well, can we as domain experts learn something about our problem domain based on how it is", "tokens": [50716, 731, 11, 393, 321, 382, 9274, 8572, 1466, 746, 466, 527, 1154, 9274, 2361, 322, 577, 309, 307, 51020], "temperature": 0.0, "avg_logprob": -0.08339904438365589, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0002453401393722743}, {"id": 404, "seek": 239920, "start": 2412.3199999999997, "end": 2417.7599999999998, "text": " models are operating? And I think we see this more clearly in another articulation of shared", "tokens": [51020, 5245, 366, 7447, 30, 400, 286, 519, 321, 536, 341, 544, 4448, 294, 1071, 15228, 2776, 295, 5507, 51292], "temperature": 0.0, "avg_logprob": -0.08339904438365589, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0002453401393722743}, {"id": 405, "seek": 239920, "start": 2417.7599999999998, "end": 2422.7999999999997, "text": " interest semantics. Here, what we're doing is basically using shared interest to interactively", "tokens": [51292, 1179, 4361, 45298, 13, 1692, 11, 437, 321, 434, 884, 307, 1936, 1228, 5507, 1179, 281, 4648, 3413, 51544], "temperature": 0.0, "avg_logprob": -0.08339904438365589, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0002453401393722743}, {"id": 406, "seek": 239920, "start": 2422.7999999999997, "end": 2429.04, "text": " probe or query that latent space. So we're brushing and using that brushed region as ground truth", "tokens": [51544, 22715, 420, 14581, 300, 48994, 1901, 13, 407, 321, 434, 33130, 293, 1228, 300, 40694, 4458, 382, 2727, 3494, 51856], "temperature": 0.0, "avg_logprob": -0.08339904438365589, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0002453401393722743}, {"id": 407, "seek": 242904, "start": 2429.36, "end": 2435.84, "text": " and then calculating the IOU coverage to figure out what are all the classes that maximize IOU", "tokens": [50380, 293, 550, 28258, 264, 286, 4807, 9645, 281, 2573, 484, 437, 366, 439, 264, 5359, 300, 19874, 286, 4807, 50704], "temperature": 0.0, "avg_logprob": -0.11336948560631793, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0002779911446850747}, {"id": 408, "seek": 242904, "start": 2435.84, "end": 2442.72, "text": " coverage for that brush ground truth. So we can see if I brush over hand, a lot of the classes", "tokens": [50704, 9645, 337, 300, 5287, 2727, 3494, 13, 407, 321, 393, 536, 498, 286, 5287, 670, 1011, 11, 257, 688, 295, 264, 5359, 51048], "temperature": 0.0, "avg_logprob": -0.11336948560631793, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0002779911446850747}, {"id": 409, "seek": 242904, "start": 2442.72, "end": 2447.7599999999998, "text": " that get returned are things that are often associated with hands like laptops and cleavers", "tokens": [51048, 300, 483, 8752, 366, 721, 300, 366, 2049, 6615, 365, 2377, 411, 27642, 293, 1233, 64, 840, 51300], "temperature": 0.0, "avg_logprob": -0.11336948560631793, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0002779911446850747}, {"id": 410, "seek": 242904, "start": 2447.7599999999998, "end": 2454.32, "text": " and interestingly enough, hen. So I guess a lot of the images in the ImageNet, you know, data set", "tokens": [51300, 293, 25873, 1547, 11, 22253, 13, 407, 286, 2041, 257, 688, 295, 264, 5267, 294, 264, 29903, 31890, 11, 291, 458, 11, 1412, 992, 51628], "temperature": 0.0, "avg_logprob": -0.11336948560631793, "compression_ratio": 1.6995515695067265, "no_speech_prob": 0.0002779911446850747}, {"id": 411, "seek": 245432, "start": 2454.4, "end": 2461.84, "text": " have people holding hens, right, which is, I guess, kind of interesting. But more maybe profoundly", "tokens": [50368, 362, 561, 5061, 276, 694, 11, 558, 11, 597, 307, 11, 286, 2041, 11, 733, 295, 1880, 13, 583, 544, 1310, 39954, 50740], "temperature": 0.0, "avg_logprob": -0.09564835826555888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0019872658886015415}, {"id": 412, "seek": 245432, "start": 2461.84, "end": 2468.0, "text": " is we could ask a question like, what is the essence of a dog, right? What is the minimal", "tokens": [50740, 307, 321, 727, 1029, 257, 1168, 411, 11, 437, 307, 264, 12801, 295, 257, 3000, 11, 558, 30, 708, 307, 264, 13206, 51048], "temperature": 0.0, "avg_logprob": -0.09564835826555888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0019872658886015415}, {"id": 413, "seek": 245432, "start": 2468.0, "end": 2474.0800000000004, "text": " amount of region that I would need to brush for the model to still be convinced that what it is", "tokens": [51048, 2372, 295, 4458, 300, 286, 576, 643, 281, 5287, 337, 264, 2316, 281, 920, 312, 12561, 300, 437, 309, 307, 51352], "temperature": 0.0, "avg_logprob": -0.09564835826555888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0019872658886015415}, {"id": 414, "seek": 245432, "start": 2474.0800000000004, "end": 2479.28, "text": " classifying as a dog? So I could start with the whole dog and then brush just on its head and", "tokens": [51352, 1508, 5489, 382, 257, 3000, 30, 407, 286, 727, 722, 365, 264, 1379, 3000, 293, 550, 5287, 445, 322, 1080, 1378, 293, 51612], "temperature": 0.0, "avg_logprob": -0.09564835826555888, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0019872658886015415}, {"id": 415, "seek": 247928, "start": 2479.28, "end": 2484.1600000000003, "text": " sure, you know, querying which shared interest still returns, you know, dog classes. But then I", "tokens": [50364, 988, 11, 291, 458, 11, 7083, 1840, 597, 5507, 1179, 920, 11247, 11, 291, 458, 11, 3000, 5359, 13, 583, 550, 286, 50608], "temperature": 0.0, "avg_logprob": -0.09641037773840207, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0009695687331259251}, {"id": 416, "seek": 247928, "start": 2484.1600000000003, "end": 2491.6000000000004, "text": " could use a smaller brush and brush just on the nose and it still returns, you know, German shepherd", "tokens": [50608, 727, 764, 257, 4356, 5287, 293, 5287, 445, 322, 264, 6690, 293, 309, 920, 11247, 11, 291, 458, 11, 6521, 40317, 50980], "temperature": 0.0, "avg_logprob": -0.09641037773840207, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0009695687331259251}, {"id": 417, "seek": 247928, "start": 2491.6000000000004, "end": 2496.88, "text": " and sheepdog and Tibetan terrier and things like that. So it seems like according to the model,", "tokens": [50980, 293, 14213, 14833, 293, 44963, 1796, 7326, 293, 721, 411, 300, 13, 407, 309, 2544, 411, 4650, 281, 264, 2316, 11, 51244], "temperature": 0.0, "avg_logprob": -0.09641037773840207, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0009695687331259251}, {"id": 418, "seek": 247928, "start": 2497.44, "end": 2505.84, "text": " all it really needs to know about, you know, an object in the image is the sort of shape of its", "tokens": [51272, 439, 309, 534, 2203, 281, 458, 466, 11, 291, 458, 11, 364, 2657, 294, 264, 3256, 307, 264, 1333, 295, 3909, 295, 1080, 51692], "temperature": 0.0, "avg_logprob": -0.09641037773840207, "compression_ratio": 1.7168141592920354, "no_speech_prob": 0.0009695687331259251}, {"id": 419, "seek": 250584, "start": 2505.84, "end": 2511.36, "text": " nose or something associated with its nose to be able to classify whether it is or is not a dog.", "tokens": [50364, 6690, 420, 746, 6615, 365, 1080, 6690, 281, 312, 1075, 281, 33872, 1968, 309, 307, 420, 307, 406, 257, 3000, 13, 50640], "temperature": 0.0, "avg_logprob": -0.07323095975098787, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.001987283118069172}, {"id": 420, "seek": 250584, "start": 2511.36, "end": 2517.04, "text": " And this seems like a really sort of toy example, but it reflects some of the things that real", "tokens": [50640, 400, 341, 2544, 411, 257, 534, 1333, 295, 12058, 1365, 11, 457, 309, 18926, 512, 295, 264, 721, 300, 957, 50924], "temperature": 0.0, "avg_logprob": -0.07323095975098787, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.001987283118069172}, {"id": 421, "seek": 250584, "start": 2517.04, "end": 2523.2000000000003, "text": " world scientists are doing. So in particular, you know, there's a researcher at the University of", "tokens": [50924, 1002, 7708, 366, 884, 13, 407, 294, 1729, 11, 291, 458, 11, 456, 311, 257, 21751, 412, 264, 3535, 295, 51232], "temperature": 0.0, "avg_logprob": -0.07323095975098787, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.001987283118069172}, {"id": 422, "seek": 250584, "start": 2523.2000000000003, "end": 2529.36, "text": " Washington, Julia Parrish that runs this grand crowdsource data collection project around seabird", "tokens": [51232, 6149, 11, 18551, 47890, 742, 300, 6676, 341, 2697, 26070, 2948, 1412, 5765, 1716, 926, 369, 455, 1271, 51540], "temperature": 0.0, "avg_logprob": -0.07323095975098787, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.001987283118069172}, {"id": 423, "seek": 250584, "start": 2529.36, "end": 2534.88, "text": " deaths. And the way they train their participants to figure out how to do bird classification", "tokens": [51540, 13027, 13, 400, 264, 636, 436, 3847, 641, 10503, 281, 2573, 484, 577, 281, 360, 5255, 21538, 51816], "temperature": 0.0, "avg_logprob": -0.07323095975098787, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.001987283118069172}, {"id": 424, "seek": 253488, "start": 2534.96, "end": 2539.84, "text": " is by asking them to measure, you know, the bird beaks and the bird feet and things like that.", "tokens": [50368, 307, 538, 3365, 552, 281, 3481, 11, 291, 458, 11, 264, 5255, 312, 5461, 293, 264, 5255, 3521, 293, 721, 411, 300, 13, 50612], "temperature": 0.0, "avg_logprob": -0.06167362116965927, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0003919278096873313}, {"id": 425, "seek": 253488, "start": 2539.84, "end": 2543.84, "text": " And so I think it's really interesting that we're seeing maybe some of those sorts of", "tokens": [50612, 400, 370, 286, 519, 309, 311, 534, 1880, 300, 321, 434, 2577, 1310, 512, 295, 729, 7527, 295, 50812], "temperature": 0.0, "avg_logprob": -0.06167362116965927, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0003919278096873313}, {"id": 426, "seek": 253488, "start": 2543.84, "end": 2552.48, "text": " representations creep up in how a model is making its decisions as well. And so where I want to end", "tokens": [50812, 33358, 9626, 493, 294, 577, 257, 2316, 307, 1455, 1080, 5327, 382, 731, 13, 400, 370, 689, 286, 528, 281, 917, 51244], "temperature": 0.0, "avg_logprob": -0.06167362116965927, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0003919278096873313}, {"id": 427, "seek": 253488, "start": 2552.48, "end": 2558.48, "text": " is sort of being most speculative and where I think, you know, there's scope for HCI to sort of", "tokens": [51244, 307, 1333, 295, 885, 881, 49415, 293, 689, 286, 519, 11, 291, 458, 11, 456, 311, 11923, 337, 389, 25240, 281, 1333, 295, 51544], "temperature": 0.0, "avg_logprob": -0.06167362116965927, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0003919278096873313}, {"id": 428, "seek": 253488, "start": 2558.48, "end": 2563.6800000000003, "text": " grow. And so, you know, we looked at generative models and imposing a conceptual model on them.", "tokens": [51544, 1852, 13, 400, 370, 11, 291, 458, 11, 321, 2956, 412, 1337, 1166, 5245, 293, 40288, 257, 24106, 2316, 322, 552, 13, 51804], "temperature": 0.0, "avg_logprob": -0.06167362116965927, "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.0003919278096873313}, {"id": 429, "seek": 256368, "start": 2563.68, "end": 2569.12, "text": " We looked at predictive models where the idea was to align conceptual models. But what I think,", "tokens": [50364, 492, 2956, 412, 35521, 5245, 689, 264, 1558, 390, 281, 7975, 24106, 5245, 13, 583, 437, 286, 519, 11, 50636], "temperature": 0.0, "avg_logprob": -0.0756995349849036, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.00013133377069607377}, {"id": 430, "seek": 256368, "start": 2569.12, "end": 2573.8399999999997, "text": " you know, we're hearing from that dermatologist we're seeing in that last case study which shared", "tokens": [50636, 291, 458, 11, 321, 434, 4763, 490, 300, 43706, 9201, 321, 434, 2577, 294, 300, 1036, 1389, 2979, 597, 5507, 50872], "temperature": 0.0, "avg_logprob": -0.0756995349849036, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.00013133377069607377}, {"id": 431, "seek": 256368, "start": 2573.8399999999997, "end": 2581.44, "text": " interest is the potential to use machine learning models to basically discover new representations", "tokens": [50872, 1179, 307, 264, 3995, 281, 764, 3479, 2539, 5245, 281, 1936, 4411, 777, 33358, 51252], "temperature": 0.0, "avg_logprob": -0.0756995349849036, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.00013133377069607377}, {"id": 432, "seek": 256368, "start": 2581.44, "end": 2587.2, "text": " of particular problem domains, right? And, you know, again, at my most speculative, I don't know", "tokens": [51252, 295, 1729, 1154, 25514, 11, 558, 30, 400, 11, 291, 458, 11, 797, 11, 412, 452, 881, 49415, 11, 286, 500, 380, 458, 51540], "temperature": 0.0, "avg_logprob": -0.0756995349849036, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.00013133377069607377}, {"id": 433, "seek": 256368, "start": 2587.2, "end": 2591.44, "text": " what I would call these, but I would maybe call them abstraction models, right, where the goal of", "tokens": [51540, 437, 286, 576, 818, 613, 11, 457, 286, 576, 1310, 818, 552, 37765, 5245, 11, 558, 11, 689, 264, 3387, 295, 51752], "temperature": 0.0, "avg_logprob": -0.0756995349849036, "compression_ratio": 1.6968641114982579, "no_speech_prob": 0.00013133377069607377}, {"id": 434, "seek": 259144, "start": 2591.44, "end": 2596.88, "text": " these models is not to produce some particular outcome that I care about, but to maximize what", "tokens": [50364, 613, 5245, 307, 406, 281, 5258, 512, 1729, 9700, 300, 286, 1127, 466, 11, 457, 281, 19874, 437, 50636], "temperature": 0.0, "avg_logprob": -0.055437744788403784, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0006461271550506353}, {"id": 435, "seek": 259144, "start": 2596.88, "end": 2601.44, "text": " are the different ways of representing the world, right? What are all the diverse abstractions that", "tokens": [50636, 366, 264, 819, 2098, 295, 13460, 264, 1002, 11, 558, 30, 708, 366, 439, 264, 9521, 12649, 626, 300, 50864], "temperature": 0.0, "avg_logprob": -0.055437744788403784, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0006461271550506353}, {"id": 436, "seek": 259144, "start": 2601.44, "end": 2606.7200000000003, "text": " we could learn about a problem domain like classifying dogs or classifying seabirds or things", "tokens": [50864, 321, 727, 1466, 466, 257, 1154, 9274, 411, 1508, 5489, 7197, 420, 1508, 5489, 369, 455, 1271, 82, 420, 721, 51128], "temperature": 0.0, "avg_logprob": -0.055437744788403784, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0006461271550506353}, {"id": 437, "seek": 259144, "start": 2606.7200000000003, "end": 2612.0, "text": " like that. And I think this is a really interesting opportunity to use machine learning to essentially", "tokens": [51128, 411, 300, 13, 400, 286, 519, 341, 307, 257, 534, 1880, 2650, 281, 764, 3479, 2539, 281, 4476, 51392], "temperature": 0.0, "avg_logprob": -0.055437744788403784, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0006461271550506353}, {"id": 438, "seek": 259144, "start": 2612.0, "end": 2617.92, "text": " advance our understanding, advance our science. But I want to be careful here because we've already", "tokens": [51392, 7295, 527, 3701, 11, 7295, 527, 3497, 13, 583, 286, 528, 281, 312, 5026, 510, 570, 321, 600, 1217, 51688], "temperature": 0.0, "avg_logprob": -0.055437744788403784, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0006461271550506353}, {"id": 439, "seek": 261792, "start": 2618.0, "end": 2622.4, "text": " seen, you know, through this talk, but also in the broader discourse, how generative and predictive", "tokens": [50368, 1612, 11, 291, 458, 11, 807, 341, 751, 11, 457, 611, 294, 264, 13227, 23938, 11, 577, 1337, 1166, 293, 35521, 50588], "temperature": 0.0, "avg_logprob": -0.07992398201882302, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0016481045167893171}, {"id": 440, "seek": 261792, "start": 2622.4, "end": 2627.12, "text": " models can sort of muddy that, that gulf of evaluation, right? Lots of people are starting", "tokens": [50588, 5245, 393, 1333, 295, 38540, 300, 11, 300, 290, 5757, 295, 13344, 11, 558, 30, 15908, 295, 561, 366, 2891, 50824], "temperature": 0.0, "avg_logprob": -0.07992398201882302, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0016481045167893171}, {"id": 441, "seek": 261792, "start": 2627.12, "end": 2632.32, "text": " to anthropomorphize these models, you know, some people think these models are representing general", "tokens": [50824, 281, 22727, 32702, 1125, 613, 5245, 11, 291, 458, 11, 512, 561, 519, 613, 5245, 366, 13460, 2674, 51084], "temperature": 0.0, "avg_logprob": -0.07992398201882302, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0016481045167893171}, {"id": 442, "seek": 261792, "start": 2632.32, "end": 2637.12, "text": " intelligence or conscience or things like that. And there's a potential with, you know, these", "tokens": [51084, 7599, 420, 20537, 420, 721, 411, 300, 13, 400, 456, 311, 257, 3995, 365, 11, 291, 458, 11, 613, 51324], "temperature": 0.0, "avg_logprob": -0.07992398201882302, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0016481045167893171}, {"id": 443, "seek": 261792, "start": 2637.12, "end": 2643.52, "text": " abstraction models to make this problem worse by muddying the question of, well, how do we know", "tokens": [51324, 37765, 5245, 281, 652, 341, 1154, 5324, 538, 8933, 67, 1840, 264, 1168, 295, 11, 731, 11, 577, 360, 321, 458, 51644], "temperature": 0.0, "avg_logprob": -0.07992398201882302, "compression_ratio": 1.7518248175182483, "no_speech_prob": 0.0016481045167893171}, {"id": 444, "seek": 264352, "start": 2643.6, "end": 2650.64, "text": " what we know, right? Like what counts as evidence? Is it evidence because, you know, the model has", "tokens": [50368, 437, 321, 458, 11, 558, 30, 1743, 437, 14893, 382, 4467, 30, 1119, 309, 4467, 570, 11, 291, 458, 11, 264, 2316, 575, 50720], "temperature": 0.0, "avg_logprob": -0.05668048171309738, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.011326530948281288}, {"id": 445, "seek": 264352, "start": 2650.64, "end": 2656.96, "text": " learned that representation? And how do we validate what that evidence is? In the case of", "tokens": [50720, 3264, 300, 10290, 30, 400, 577, 360, 321, 29562, 437, 300, 4467, 307, 30, 682, 264, 1389, 295, 51036], "temperature": 0.0, "avg_logprob": -0.05668048171309738, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.011326530948281288}, {"id": 446, "seek": 264352, "start": 2656.96, "end": 2662.24, "text": " representations that are designed or interpreted or theorized by people, we know how to consider that", "tokens": [51036, 33358, 300, 366, 4761, 420, 26749, 420, 27423, 1602, 538, 561, 11, 321, 458, 577, 281, 1949, 300, 51300], "temperature": 0.0, "avg_logprob": -0.05668048171309738, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.011326530948281288}, {"id": 447, "seek": 264352, "start": 2662.24, "end": 2667.36, "text": " to be evidence, right? But I don't know what it means for a learned representation to count as", "tokens": [51300, 281, 312, 4467, 11, 558, 30, 583, 286, 500, 380, 458, 437, 309, 1355, 337, 257, 3264, 10290, 281, 1207, 382, 51556], "temperature": 0.0, "avg_logprob": -0.05668048171309738, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.011326530948281288}, {"id": 448, "seek": 264352, "start": 2667.36, "end": 2672.24, "text": " evidence. And as all sorts of problems in machine learning, this is not necessarily a problem that", "tokens": [51556, 4467, 13, 400, 382, 439, 7527, 295, 2740, 294, 3479, 2539, 11, 341, 307, 406, 4725, 257, 1154, 300, 51800], "temperature": 0.0, "avg_logprob": -0.05668048171309738, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.011326530948281288}, {"id": 449, "seek": 267224, "start": 2672.24, "end": 2679.9199999999996, "text": " is unique to machine learning. So here are three visualizations that were used to discuss the", "tokens": [50364, 307, 3845, 281, 3479, 2539, 13, 407, 510, 366, 1045, 5056, 14455, 300, 645, 1143, 281, 2248, 264, 50748], "temperature": 0.0, "avg_logprob": -0.07836076191493443, "compression_ratio": 1.4083769633507854, "no_speech_prob": 0.0002694132272154093}, {"id": 450, "seek": 267224, "start": 2679.9199999999996, "end": 2685.68, "text": " COVID-19 pandemic right at the, the, the peak of the first wave in the summer of 2020.", "tokens": [50748, 4566, 12, 3405, 5388, 558, 412, 264, 11, 264, 11, 264, 10651, 295, 264, 700, 5772, 294, 264, 4266, 295, 4808, 13, 51036], "temperature": 0.0, "avg_logprob": -0.07836076191493443, "compression_ratio": 1.4083769633507854, "no_speech_prob": 0.0002694132272154093}, {"id": 451, "seek": 267224, "start": 2688.08, "end": 2695.2799999999997, "text": " And I'm curious if anything pops out at you, like any reason, you know, to be curious or", "tokens": [51156, 400, 286, 478, 6369, 498, 1340, 16795, 484, 412, 291, 11, 411, 604, 1778, 11, 291, 458, 11, 281, 312, 6369, 420, 51516], "temperature": 0.0, "avg_logprob": -0.07836076191493443, "compression_ratio": 1.4083769633507854, "no_speech_prob": 0.0002694132272154093}, {"id": 452, "seek": 269528, "start": 2695.36, "end": 2705.84, "text": " suspect of, of these visualizations, right? Like no, right? Probably not. Like these seem pretty", "tokens": [50368, 9091, 295, 11, 295, 613, 5056, 14455, 11, 558, 30, 1743, 572, 11, 558, 30, 9210, 406, 13, 1743, 613, 1643, 1238, 50892], "temperature": 0.0, "avg_logprob": -0.1137033541177966, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.005554342642426491}, {"id": 453, "seek": 269528, "start": 2705.84, "end": 2711.6800000000003, "text": " legitimate, right? Like our world and data, very legitimate data source, right? And if you look at,", "tokens": [50892, 17956, 11, 558, 30, 1743, 527, 1002, 293, 1412, 11, 588, 17956, 1412, 4009, 11, 558, 30, 400, 498, 291, 574, 412, 11, 51184], "temperature": 0.0, "avg_logprob": -0.1137033541177966, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.005554342642426491}, {"id": 454, "seek": 269528, "start": 2711.6800000000003, "end": 2715.52, "text": " look at some of these two other visualizations, you might go, you know what, actually the one on the", "tokens": [51184, 574, 412, 512, 295, 613, 732, 661, 5056, 14455, 11, 291, 1062, 352, 11, 291, 458, 437, 11, 767, 264, 472, 322, 264, 51376], "temperature": 0.0, "avg_logprob": -0.1137033541177966, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.005554342642426491}, {"id": 455, "seek": 269528, "start": 2715.52, "end": 2719.6800000000003, "text": " right, that looks like something in maybe a policy briefing or something, right? It looks very", "tokens": [51376, 558, 11, 300, 1542, 411, 746, 294, 1310, 257, 3897, 28878, 420, 746, 11, 558, 30, 467, 1542, 588, 51584], "temperature": 0.0, "avg_logprob": -0.1137033541177966, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.005554342642426491}, {"id": 456, "seek": 271968, "start": 2719.68, "end": 2727.12, "text": " sophisticated, lots of good annotation, you know, a style and aesthetic that looks very sort of", "tokens": [50364, 16950, 11, 3195, 295, 665, 48654, 11, 291, 458, 11, 257, 3758, 293, 20092, 300, 1542, 588, 1333, 295, 50736], "temperature": 0.0, "avg_logprob": -0.06371835561899039, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.08505255728960037}, {"id": 457, "seek": 271968, "start": 2727.12, "end": 2732.8799999999997, "text": " sophisticated. But you may be catching what I'm alluding to, which is the fact that all three", "tokens": [50736, 16950, 13, 583, 291, 815, 312, 16124, 437, 286, 478, 439, 33703, 281, 11, 597, 307, 264, 1186, 300, 439, 1045, 51024], "temperature": 0.0, "avg_logprob": -0.06371835561899039, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.08505255728960037}, {"id": 458, "seek": 271968, "start": 2732.8799999999997, "end": 2740.7999999999997, "text": " visualizations were used by people on social media to advance the argument that, you know,", "tokens": [51024, 5056, 14455, 645, 1143, 538, 561, 322, 2093, 3021, 281, 7295, 264, 6770, 300, 11, 291, 458, 11, 51420], "temperature": 0.0, "avg_logprob": -0.06371835561899039, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.08505255728960037}, {"id": 459, "seek": 271968, "start": 2740.7999999999997, "end": 2747.2, "text": " our response to COVID was overblown. Not that COVID was a hoax, but that our reaction to it", "tokens": [51420, 527, 4134, 281, 4566, 390, 670, 5199, 648, 13, 1726, 300, 4566, 390, 257, 1106, 2797, 11, 457, 300, 527, 5480, 281, 309, 51740], "temperature": 0.0, "avg_logprob": -0.06371835561899039, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.08505255728960037}, {"id": 460, "seek": 274720, "start": 2747.2, "end": 2754.0, "text": " was, was way too extreme. That COVID wasn't as serious an issue as it might initially seem.", "tokens": [50364, 390, 11, 390, 636, 886, 8084, 13, 663, 4566, 2067, 380, 382, 3156, 364, 2734, 382, 309, 1062, 9105, 1643, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1042223744008733, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0005111550563015044}, {"id": 461, "seek": 274720, "start": 2754.56, "end": 2760.08, "text": " And I want to be really careful about what I'm, when I'm doing here with these charts, because", "tokens": [50732, 400, 286, 528, 281, 312, 534, 5026, 466, 437, 286, 478, 11, 562, 286, 478, 884, 510, 365, 613, 17767, 11, 570, 51008], "temperature": 0.0, "avg_logprob": -0.1042223744008733, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0005111550563015044}, {"id": 462, "seek": 274720, "start": 2760.08, "end": 2764.72, "text": " certainly some of the people that were distributing this were bad actors who were ideologically", "tokens": [51008, 3297, 512, 295, 264, 561, 300, 645, 41406, 341, 645, 1578, 10037, 567, 645, 1153, 17157, 51240], "temperature": 0.0, "avg_logprob": -0.1042223744008733, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0005111550563015044}, {"id": 463, "seek": 274720, "start": 2764.72, "end": 2771.9199999999996, "text": " motivated. But through a very long, laborious ethnography, ethnographic process that we", "tokens": [51240, 14515, 13, 583, 807, 257, 588, 938, 11, 5938, 851, 42589, 5820, 11, 42589, 12295, 1399, 300, 321, 51600], "temperature": 0.0, "avg_logprob": -0.1042223744008733, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.0005111550563015044}, {"id": 464, "seek": 277192, "start": 2771.92, "end": 2777.6800000000003, "text": " conducted, spending six months on five different Facebook groups, we found that a lot of people", "tokens": [50364, 13809, 11, 6434, 2309, 2493, 322, 1732, 819, 4384, 3935, 11, 321, 1352, 300, 257, 688, 295, 561, 50652], "temperature": 0.0, "avg_logprob": -0.08405291706050208, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.031128549948334694}, {"id": 465, "seek": 277192, "start": 2777.6800000000003, "end": 2782.56, "text": " who were producing visualizations like that were actually displaying many hallmarks of citizen", "tokens": [50652, 567, 645, 10501, 5056, 14455, 411, 300, 645, 767, 36834, 867, 6500, 37307, 295, 13326, 50896], "temperature": 0.0, "avg_logprob": -0.08405291706050208, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.031128549948334694}, {"id": 466, "seek": 277192, "start": 2782.56, "end": 2788.0, "text": " data science. So they were really many of them filling gaps in, in information sort of collection,", "tokens": [50896, 1412, 3497, 13, 407, 436, 645, 534, 867, 295, 552, 10623, 15031, 294, 11, 294, 1589, 1333, 295, 5765, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08405291706050208, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.031128549948334694}, {"id": 467, "seek": 277192, "start": 2788.0, "end": 2792.4, "text": " because they were situated in rural parts of the country where, you know, there wasn't a lot of good", "tokens": [51168, 570, 436, 645, 30143, 294, 11165, 3166, 295, 264, 1941, 689, 11, 291, 458, 11, 456, 2067, 380, 257, 688, 295, 665, 51388], "temperature": 0.0, "avg_logprob": -0.08405291706050208, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.031128549948334694}, {"id": 468, "seek": 277192, "start": 2792.4, "end": 2797.92, "text": " data collection. So many members of these groups were hosting, you know, webcasts, live seminars of", "tokens": [51388, 1412, 5765, 13, 407, 867, 2679, 295, 613, 3935, 645, 16058, 11, 291, 458, 11, 3670, 3734, 82, 11, 1621, 43112, 295, 51664], "temperature": 0.0, "avg_logprob": -0.08405291706050208, "compression_ratio": 1.7437722419928825, "no_speech_prob": 0.031128549948334694}, {"id": 469, "seek": 279792, "start": 2797.92, "end": 2802.8, "text": " how to download data from the government website, how to clean it and excel, how to visualize it and", "tokens": [50364, 577, 281, 5484, 1412, 490, 264, 2463, 3144, 11, 577, 281, 2541, 309, 293, 24015, 11, 577, 281, 23273, 309, 293, 50608], "temperature": 0.0, "avg_logprob": -0.07586338406517393, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.001283950638025999}, {"id": 470, "seek": 279792, "start": 2802.8, "end": 2808.88, "text": " things like that. And, and most surprisingly to us, many of them were engaged in discussion that", "tokens": [50608, 721, 411, 300, 13, 400, 11, 293, 881, 17600, 281, 505, 11, 867, 295, 552, 645, 8237, 294, 5017, 300, 50912], "temperature": 0.0, "avg_logprob": -0.07586338406517393, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.001283950638025999}, {"id": 471, "seek": 279792, "start": 2808.88, "end": 2814.56, "text": " looked like peer review, right? They were critically assessing data sources, discussing metrics,", "tokens": [50912, 2956, 411, 15108, 3131, 11, 558, 30, 814, 645, 22797, 34348, 1412, 7139, 11, 10850, 16367, 11, 51196], "temperature": 0.0, "avg_logprob": -0.07586338406517393, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.001283950638025999}, {"id": 472, "seek": 279792, "start": 2815.2000000000003, "end": 2819.92, "text": " making arguments for which metrics were, were better or not. But all of this was sort of", "tokens": [51228, 1455, 12869, 337, 597, 16367, 645, 11, 645, 1101, 420, 406, 13, 583, 439, 295, 341, 390, 1333, 295, 51464], "temperature": 0.0, "avg_logprob": -0.07586338406517393, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.001283950638025999}, {"id": 473, "seek": 279792, "start": 2819.92, "end": 2826.16, "text": " inflected through a sort of frustration with mainstream institutions and maybe even distrust", "tokens": [51464, 1536, 1809, 292, 807, 257, 1333, 295, 20491, 365, 15960, 8142, 293, 1310, 754, 1483, 22326, 51776], "temperature": 0.0, "avg_logprob": -0.07586338406517393, "compression_ratio": 1.7184115523465704, "no_speech_prob": 0.001283950638025999}, {"id": 474, "seek": 282616, "start": 2826.16, "end": 2832.64, "text": " of those institutions as well, right? But ultimately what these groups cared about was bolstering a", "tokens": [50364, 295, 729, 8142, 382, 731, 11, 558, 30, 583, 6284, 437, 613, 3935, 19779, 466, 390, 8986, 372, 1794, 257, 50688], "temperature": 0.0, "avg_logprob": -0.07457098467596646, "compression_ratio": 1.6769759450171822, "no_speech_prob": 0.000570105214137584}, {"id": 475, "seek": 282616, "start": 2832.64, "end": 2838.72, "text": " sense of social unity and civic engagement, right? So this quote I, I find particularly sort of", "tokens": [50688, 2020, 295, 2093, 18205, 293, 29089, 8742, 11, 558, 30, 407, 341, 6513, 286, 11, 286, 915, 4098, 1333, 295, 50992], "temperature": 0.0, "avg_logprob": -0.07457098467596646, "compression_ratio": 1.6769759450171822, "no_speech_prob": 0.000570105214137584}, {"id": 476, "seek": 282616, "start": 2838.72, "end": 2843.6, "text": " reflective of that sense of, you know, it's incumbent on all of us to hold our elected officials to", "tokens": [50992, 28931, 295, 300, 2020, 295, 11, 291, 458, 11, 309, 311, 45539, 322, 439, 295, 505, 281, 1797, 527, 11776, 9798, 281, 51236], "temperature": 0.0, "avg_logprob": -0.07457098467596646, "compression_ratio": 1.6769759450171822, "no_speech_prob": 0.000570105214137584}, {"id": 477, "seek": 282616, "start": 2843.6, "end": 2848.56, "text": " account so that they make better decisions through data, right? I'm speaking to you as a neighbor,", "tokens": [51236, 2696, 370, 300, 436, 652, 1101, 5327, 807, 1412, 11, 558, 30, 286, 478, 4124, 281, 291, 382, 257, 5987, 11, 51484], "temperature": 0.0, "avg_logprob": -0.07457098467596646, "compression_ratio": 1.6769759450171822, "no_speech_prob": 0.000570105214137584}, {"id": 478, "seek": 282616, "start": 2848.56, "end": 2855.12, "text": " as a mama bear, right? So this is not some sort of ideologically motivated individual who is,", "tokens": [51484, 382, 257, 18775, 6155, 11, 558, 30, 407, 341, 307, 406, 512, 1333, 295, 1153, 17157, 14515, 2609, 567, 307, 11, 51812], "temperature": 0.0, "avg_logprob": -0.07457098467596646, "compression_ratio": 1.6769759450171822, "no_speech_prob": 0.000570105214137584}, {"id": 479, "seek": 285512, "start": 2855.12, "end": 2859.52, "text": " who is, you know, trying to be a bad actor. This is just an engaged member of the citizenry.", "tokens": [50364, 567, 307, 11, 291, 458, 11, 1382, 281, 312, 257, 1578, 8747, 13, 639, 307, 445, 364, 8237, 4006, 295, 264, 13326, 627, 13, 50584], "temperature": 0.0, "avg_logprob": -0.07847410284954569, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0001583988923812285}, {"id": 480, "seek": 285512, "start": 2860.4, "end": 2865.52, "text": " And similarly, you know, oftentimes they were actually more sophisticated than scientists", "tokens": [50628, 400, 14138, 11, 291, 458, 11, 18349, 436, 645, 767, 544, 16950, 813, 7708, 50884], "temperature": 0.0, "avg_logprob": -0.07847410284954569, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0001583988923812285}, {"id": 481, "seek": 285512, "start": 2865.52, "end": 2870.72, "text": " can be. So many of these members were very reflexive about their own data analysis,", "tokens": [50884, 393, 312, 13, 407, 867, 295, 613, 2679, 645, 588, 23802, 488, 466, 641, 1065, 1412, 5215, 11, 51144], "temperature": 0.0, "avg_logprob": -0.07847410284954569, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0001583988923812285}, {"id": 482, "seek": 285512, "start": 2870.72, "end": 2875.7599999999998, "text": " data gathering process, right? So someone says, you know, I've never claimed to have no bias,", "tokens": [51144, 1412, 13519, 1399, 11, 558, 30, 407, 1580, 1619, 11, 291, 458, 11, 286, 600, 1128, 12941, 281, 362, 572, 12577, 11, 51396], "temperature": 0.0, "avg_logprob": -0.07847410284954569, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0001583988923812285}, {"id": 483, "seek": 285512, "start": 2875.7599999999998, "end": 2880.56, "text": " right? I'm human, of course I'm biased, here are my biases. Whereas in science, often we like to", "tokens": [51396, 558, 30, 286, 478, 1952, 11, 295, 1164, 286, 478, 28035, 11, 510, 366, 452, 32152, 13, 13813, 294, 3497, 11, 2049, 321, 411, 281, 51636], "temperature": 0.0, "avg_logprob": -0.07847410284954569, "compression_ratio": 1.6557971014492754, "no_speech_prob": 0.0001583988923812285}, {"id": 484, "seek": 288056, "start": 2880.56, "end": 2886.24, "text": " portray ourselves as being very objective, you know, arbiters of truth. And so in many ways,", "tokens": [50364, 15676, 4175, 382, 885, 588, 10024, 11, 291, 458, 11, 14931, 433, 295, 3494, 13, 400, 370, 294, 867, 2098, 11, 50648], "temperature": 0.0, "avg_logprob": -0.05310448356296705, "compression_ratio": 1.7896678966789668, "no_speech_prob": 0.0003568917454686016}, {"id": 485, "seek": 288056, "start": 2886.24, "end": 2892.72, "text": " you know, what was happening in these groups is, is perhaps more sophisticated than what was happening", "tokens": [50648, 291, 458, 11, 437, 390, 2737, 294, 613, 3935, 307, 11, 307, 4317, 544, 16950, 813, 437, 390, 2737, 50972], "temperature": 0.0, "avg_logprob": -0.05310448356296705, "compression_ratio": 1.7896678966789668, "no_speech_prob": 0.0003568917454686016}, {"id": 486, "seek": 288056, "start": 2892.72, "end": 2896.88, "text": " in science and public health at the time. But the question is, so what does this have to do with", "tokens": [50972, 294, 3497, 293, 1908, 1585, 412, 264, 565, 13, 583, 264, 1168, 307, 11, 370, 437, 775, 341, 362, 281, 360, 365, 51180], "temperature": 0.0, "avg_logprob": -0.05310448356296705, "compression_ratio": 1.7896678966789668, "no_speech_prob": 0.0003568917454686016}, {"id": 487, "seek": 288056, "start": 2896.88, "end": 2902.64, "text": " sort of bridging semantic distances and abstraction models? Well, I think what was happening in,", "tokens": [51180, 1333, 295, 16362, 3249, 47982, 22182, 293, 37765, 5245, 30, 1042, 11, 286, 519, 437, 390, 2737, 294, 11, 51468], "temperature": 0.0, "avg_logprob": -0.05310448356296705, "compression_ratio": 1.7896678966789668, "no_speech_prob": 0.0003568917454686016}, {"id": 488, "seek": 288056, "start": 2902.64, "end": 2908.4, "text": " in those groups was, you know, they, they, they disagreed with the definitions of some of these", "tokens": [51468, 294, 729, 3935, 390, 11, 291, 458, 11, 436, 11, 436, 11, 436, 23926, 292, 365, 264, 21988, 295, 512, 295, 613, 51756], "temperature": 0.0, "avg_logprob": -0.05310448356296705, "compression_ratio": 1.7896678966789668, "no_speech_prob": 0.0003568917454686016}, {"id": 489, "seek": 290840, "start": 2908.48, "end": 2914.2400000000002, "text": " metrics, right? They were living in rural communities. And so the metrics that public", "tokens": [50368, 16367, 11, 558, 30, 814, 645, 2647, 294, 11165, 4456, 13, 400, 370, 264, 16367, 300, 1908, 50656], "temperature": 0.0, "avg_logprob": -0.07218837275088412, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0016480976482853293}, {"id": 490, "seek": 290840, "start": 2914.2400000000002, "end": 2919.92, "text": " health officials were using to, to, you know, define the, the state and scale of the pandemic", "tokens": [50656, 1585, 9798, 645, 1228, 281, 11, 281, 11, 291, 458, 11, 6964, 264, 11, 264, 1785, 293, 4373, 295, 264, 5388, 50940], "temperature": 0.0, "avg_logprob": -0.07218837275088412, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0016480976482853293}, {"id": 491, "seek": 290840, "start": 2919.92, "end": 2923.76, "text": " was not reflected in their lived experience. They were turning around and well,", "tokens": [50940, 390, 406, 15502, 294, 641, 5152, 1752, 13, 814, 645, 6246, 926, 293, 731, 11, 51132], "temperature": 0.0, "avg_logprob": -0.07218837275088412, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0016480976482853293}, {"id": 492, "seek": 290840, "start": 2923.76, "end": 2928.48, "text": " it didn't seem like COVID was an issue, right? And so our colleagues in the humanities and", "tokens": [51132, 309, 994, 380, 1643, 411, 4566, 390, 364, 2734, 11, 558, 30, 400, 370, 527, 7734, 294, 264, 36140, 293, 51368], "temperature": 0.0, "avg_logprob": -0.07218837275088412, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0016480976482853293}, {"id": 493, "seek": 290840, "start": 2928.48, "end": 2934.7200000000003, "text": " social sciences often advocate for adopting what they call an interpretive, interpretivist lens,", "tokens": [51368, 2093, 17677, 2049, 14608, 337, 32328, 437, 436, 818, 364, 7302, 488, 11, 7302, 592, 468, 6765, 11, 51680], "temperature": 0.0, "avg_logprob": -0.07218837275088412, "compression_ratio": 1.6555555555555554, "no_speech_prob": 0.0016480976482853293}, {"id": 494, "seek": 293472, "start": 2934.72, "end": 2939.2799999999997, "text": " right? The idea that knowledge is subjective, it's socially constructed,", "tokens": [50364, 558, 30, 440, 1558, 300, 3601, 307, 25972, 11, 309, 311, 21397, 17083, 11, 50592], "temperature": 0.0, "avg_logprob": -0.08569229090655292, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.0002453068154864013}, {"id": 495, "seek": 293472, "start": 2939.2799999999997, "end": 2943.9199999999996, "text": " and that it's composed of many diff, different diverse perspectives that we have to figure out", "tokens": [50592, 293, 300, 309, 311, 18204, 295, 867, 7593, 11, 819, 9521, 16766, 300, 321, 362, 281, 2573, 484, 50824], "temperature": 0.0, "avg_logprob": -0.08569229090655292, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.0002453068154864013}, {"id": 496, "seek": 293472, "start": 2943.9199999999996, "end": 2949.68, "text": " ways to synthesize together. And while that idea has been adopted in pockets of visualization and", "tokens": [50824, 2098, 281, 26617, 1125, 1214, 13, 400, 1339, 300, 1558, 575, 668, 12175, 294, 16491, 295, 25801, 293, 51112], "temperature": 0.0, "avg_logprob": -0.08569229090655292, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.0002453068154864013}, {"id": 497, "seek": 293472, "start": 2949.68, "end": 2955.52, "text": " HCI and CS, so far, I think it's largely been on the qualitative side, because if we think about", "tokens": [51112, 389, 25240, 293, 9460, 11, 370, 1400, 11, 286, 519, 309, 311, 11611, 668, 322, 264, 31312, 1252, 11, 570, 498, 321, 519, 466, 51404], "temperature": 0.0, "avg_logprob": -0.08569229090655292, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.0002453068154864013}, {"id": 498, "seek": 293472, "start": 2955.52, "end": 2962.08, "text": " how to do computation, we have to, you know, we're forced into making decisions about the world and", "tokens": [51404, 577, 281, 360, 24903, 11, 321, 362, 281, 11, 291, 458, 11, 321, 434, 7579, 666, 1455, 5327, 466, 264, 1002, 293, 51732], "temperature": 0.0, "avg_logprob": -0.08569229090655292, "compression_ratio": 1.6325088339222615, "no_speech_prob": 0.0002453068154864013}, {"id": 499, "seek": 296208, "start": 2962.08, "end": 2966.96, "text": " how to represent that world and computational data structures. And what I think abstraction models", "tokens": [50364, 577, 281, 2906, 300, 1002, 293, 28270, 1412, 9227, 13, 400, 437, 286, 519, 37765, 5245, 50608], "temperature": 0.0, "avg_logprob": -0.05805912843117347, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005882970872335136}, {"id": 500, "seek": 296208, "start": 2966.96, "end": 2972.48, "text": " allow us to do is start to push, but you know, push that boundary a little bit, right? Rather than", "tokens": [50608, 2089, 505, 281, 360, 307, 722, 281, 2944, 11, 457, 291, 458, 11, 2944, 300, 12866, 257, 707, 857, 11, 558, 30, 16571, 813, 50884], "temperature": 0.0, "avg_logprob": -0.05805912843117347, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005882970872335136}, {"id": 501, "seek": 296208, "start": 2972.48, "end": 2978.0, "text": " being focused on developing a model that produces a single best outcome, we might instead be looking", "tokens": [50884, 885, 5178, 322, 6416, 257, 2316, 300, 14725, 257, 2167, 1151, 9700, 11, 321, 1062, 2602, 312, 1237, 51160], "temperature": 0.0, "avg_logprob": -0.05805912843117347, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005882970872335136}, {"id": 502, "seek": 296208, "start": 2978.0, "end": 2982.72, "text": " to a world in which we are training sort of ecosystems of abstraction models, where we're", "tokens": [51160, 281, 257, 1002, 294, 597, 321, 366, 3097, 1333, 295, 32647, 295, 37765, 5245, 11, 689, 321, 434, 51396], "temperature": 0.0, "avg_logprob": -0.05805912843117347, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005882970872335136}, {"id": 503, "seek": 296208, "start": 2982.72, "end": 2988.08, "text": " forcing them to learn really different representations of the world or of a problem domain,", "tokens": [51396, 19030, 552, 281, 1466, 534, 819, 33358, 295, 264, 1002, 420, 295, 257, 1154, 9274, 11, 51664], "temperature": 0.0, "avg_logprob": -0.05805912843117347, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005882970872335136}, {"id": 504, "seek": 298808, "start": 2988.08, "end": 2992.24, "text": " and then leaving it up to people to figure out how to synthesize between those learned", "tokens": [50364, 293, 550, 5012, 309, 493, 281, 561, 281, 2573, 484, 577, 281, 26617, 1125, 1296, 729, 3264, 50572], "temperature": 0.0, "avg_logprob": -0.09777774185430808, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0003350682382006198}, {"id": 505, "seek": 298808, "start": 2992.24, "end": 2997.7599999999998, "text": " representations for, you know, some particular policy goal or, you know,", "tokens": [50572, 33358, 337, 11, 291, 458, 11, 512, 1729, 3897, 3387, 420, 11, 291, 458, 11, 50848], "temperature": 0.0, "avg_logprob": -0.09777774185430808, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0003350682382006198}, {"id": 506, "seek": 298808, "start": 2999.84, "end": 3005.68, "text": " thing that they want to optimize for. So with that, I'm happy to take questions about any of", "tokens": [50952, 551, 300, 436, 528, 281, 19719, 337, 13, 407, 365, 300, 11, 286, 478, 2055, 281, 747, 1651, 466, 604, 295, 51244], "temperature": 0.0, "avg_logprob": -0.09777774185430808, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0003350682382006198}, {"id": 507, "seek": 300568, "start": 3005.7599999999998, "end": 3007.2799999999997, "text": " what I talked about. Thank you very much.", "tokens": [50368, 437, 286, 2825, 466, 13, 1044, 291, 588, 709, 13, 50444], "temperature": 0.0, "avg_logprob": -0.23218906626981847, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.03108040802180767}, {"id": 508, "seek": 300568, "start": 3031.2799999999997, "end": 3035.12, "text": " This image that made me make this decision, so do you think the results would be different", "tokens": [51644, 639, 3256, 300, 1027, 385, 652, 341, 3537, 11, 370, 360, 291, 519, 264, 3542, 576, 312, 819, 51836], "temperature": 0.0, "avg_logprob": -0.23218906626981847, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.03108040802180767}, {"id": 509, "seek": 303512, "start": 3035.12, "end": 3042.72, "text": " if you used, like, iFixations in that comparison? That's an interesting question. We haven't", "tokens": [50364, 498, 291, 1143, 11, 411, 11, 741, 37, 970, 763, 294, 300, 9660, 30, 663, 311, 364, 1880, 1168, 13, 492, 2378, 380, 50744], "temperature": 0.0, "avg_logprob": -0.13402424256006876, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0008293043938465416}, {"id": 510, "seek": 303512, "start": 3042.72, "end": 3051.2, "text": " considered iFixations for the salency map work, but certainly I think your intuition is right in", "tokens": [50744, 4888, 741, 37, 970, 763, 337, 264, 1845, 3020, 4471, 589, 11, 457, 3297, 286, 519, 428, 24002, 307, 558, 294, 51168], "temperature": 0.0, "avg_logprob": -0.13402424256006876, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0008293043938465416}, {"id": 511, "seek": 303512, "start": 3051.2, "end": 3057.04, "text": " the sense that, you know, the current way that we've modeled shared interest is pretty brittle,", "tokens": [51168, 264, 2020, 300, 11, 291, 458, 11, 264, 2190, 636, 300, 321, 600, 37140, 5507, 1179, 307, 1238, 49325, 11, 51460], "temperature": 0.0, "avg_logprob": -0.13402424256006876, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0008293043938465416}, {"id": 512, "seek": 303512, "start": 3057.04, "end": 3061.68, "text": " right? It's operating at the level of abstraction of, like, pixels in an image, and how meaningful", "tokens": [51460, 558, 30, 467, 311, 7447, 412, 264, 1496, 295, 37765, 295, 11, 411, 11, 18668, 294, 364, 3256, 11, 293, 577, 10995, 51692], "temperature": 0.0, "avg_logprob": -0.13402424256006876, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0008293043938465416}, {"id": 513, "seek": 306168, "start": 3061.68, "end": 3067.6, "text": " are pixels really? And so what Angie is working on right now is a way to raise the", "tokens": [50364, 366, 18668, 534, 30, 400, 370, 437, 48829, 307, 1364, 322, 558, 586, 307, 257, 636, 281, 5300, 264, 50660], "temperature": 0.0, "avg_logprob": -0.06029614457139024, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.00041726414929144084}, {"id": 514, "seek": 306168, "start": 3067.6, "end": 3072.16, "text": " level of abstraction that shared interest is working on. So in many of these domains, like,", "tokens": [50660, 1496, 295, 37765, 300, 5507, 1179, 307, 1364, 322, 13, 407, 294, 867, 295, 613, 25514, 11, 411, 11, 50888], "temperature": 0.0, "avg_logprob": -0.06029614457139024, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.00041726414929144084}, {"id": 515, "seek": 306168, "start": 3072.16, "end": 3078.08, "text": " you know, ImageNet, the task that we're asking models to do, the labeling task,", "tokens": [50888, 291, 458, 11, 29903, 31890, 11, 264, 5633, 300, 321, 434, 3365, 5245, 281, 360, 11, 264, 40244, 5633, 11, 51184], "temperature": 0.0, "avg_logprob": -0.06029614457139024, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.00041726414929144084}, {"id": 516, "seek": 306168, "start": 3078.08, "end": 3084.24, "text": " actually inherits from a much richer knowledge graph or taxonomy or hierarchy or things like that.", "tokens": [51184, 767, 9484, 1208, 490, 257, 709, 29021, 3601, 4295, 420, 3366, 23423, 420, 22333, 420, 721, 411, 300, 13, 51492], "temperature": 0.0, "avg_logprob": -0.06029614457139024, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.00041726414929144084}, {"id": 517, "seek": 306168, "start": 3084.24, "end": 3088.0, "text": " But right now, at least, you know, there's a little bit of work in hierarchical learning,", "tokens": [51492, 583, 558, 586, 11, 412, 1935, 11, 291, 458, 11, 456, 311, 257, 707, 857, 295, 589, 294, 35250, 804, 2539, 11, 51680], "temperature": 0.0, "avg_logprob": -0.06029614457139024, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.00041726414929144084}, {"id": 518, "seek": 308800, "start": 3088.0, "end": 3093.6, "text": " but most of the predictive models are just learning at the finest level of detail, right?", "tokens": [50364, 457, 881, 295, 264, 35521, 5245, 366, 445, 2539, 412, 264, 28141, 1496, 295, 2607, 11, 558, 30, 50644], "temperature": 0.0, "avg_logprob": -0.07950216376263163, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.001867244252935052}, {"id": 519, "seek": 308800, "start": 3093.6, "end": 3099.84, "text": " So we're throwing away all that rich information that might be really relevant to how a person is", "tokens": [50644, 407, 321, 434, 10238, 1314, 439, 300, 4593, 1589, 300, 1062, 312, 534, 7340, 281, 577, 257, 954, 307, 50956], "temperature": 0.0, "avg_logprob": -0.07950216376263163, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.001867244252935052}, {"id": 520, "seek": 308800, "start": 3099.84, "end": 3104.88, "text": " making a decision. So maybe what I care about is not whether it's a Chihuahua or a golden retriever", "tokens": [50956, 1455, 257, 3537, 13, 407, 1310, 437, 286, 1127, 466, 307, 406, 1968, 309, 311, 257, 761, 44344, 545, 4398, 420, 257, 9729, 19817, 331, 51208], "temperature": 0.0, "avg_logprob": -0.07950216376263163, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.001867244252935052}, {"id": 521, "seek": 308800, "start": 3104.88, "end": 3110.32, "text": " or a laboratory retriever. I might care, is it a dog or really sometimes is it just an object,", "tokens": [51208, 420, 257, 16523, 19817, 331, 13, 286, 1062, 1127, 11, 307, 309, 257, 3000, 420, 534, 2171, 307, 309, 445, 364, 2657, 11, 51480], "temperature": 0.0, "avg_logprob": -0.07950216376263163, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.001867244252935052}, {"id": 522, "seek": 308800, "start": 3110.32, "end": 3115.92, "text": " right? And so what does it look like to do shared interest in more meaningful abstraction space", "tokens": [51480, 558, 30, 400, 370, 437, 775, 309, 574, 411, 281, 360, 5507, 1179, 294, 544, 10995, 37765, 1901, 51760], "temperature": 0.0, "avg_logprob": -0.07950216376263163, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.001867244252935052}, {"id": 523, "seek": 311592, "start": 3115.92, "end": 3120.2400000000002, "text": " rather than pixels is something we're working on. Yeah, great question. Thanks. Yeah, Will.", "tokens": [50364, 2831, 813, 18668, 307, 746, 321, 434, 1364, 322, 13, 865, 11, 869, 1168, 13, 2561, 13, 865, 11, 3099, 13, 50580], "temperature": 0.0, "avg_logprob": -0.13145749181763738, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.001345357159152627}, {"id": 524, "seek": 311592, "start": 3121.6800000000003, "end": 3127.04, "text": " Thank you for the great talk, Arvin. So going back to Jupyter notebooks and ChatGPT,", "tokens": [50652, 1044, 291, 337, 264, 869, 751, 11, 1587, 4796, 13, 407, 516, 646, 281, 22125, 88, 391, 43782, 293, 27503, 38, 47, 51, 11, 50920], "temperature": 0.0, "avg_logprob": -0.13145749181763738, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.001345357159152627}, {"id": 525, "seek": 311592, "start": 3127.92, "end": 3134.16, "text": " you talked about how, right, ChatGPT can shell out to some of these nice plugins like for Excel or", "tokens": [50964, 291, 2825, 466, 577, 11, 558, 11, 27503, 38, 47, 51, 393, 8720, 484, 281, 512, 295, 613, 1481, 33759, 411, 337, 19060, 420, 51276], "temperature": 0.0, "avg_logprob": -0.13145749181763738, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.001345357159152627}, {"id": 526, "seek": 311592, "start": 3134.16, "end": 3139.44, "text": " whatever to try and help people do natural language data science and that there's this", "tokens": [51276, 2035, 281, 853, 293, 854, 561, 360, 3303, 2856, 1412, 3497, 293, 300, 456, 311, 341, 51540], "temperature": 0.0, "avg_logprob": -0.13145749181763738, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.001345357159152627}, {"id": 527, "seek": 311592, "start": 3139.44, "end": 3144.16, "text": " articulatory distance due to the difficulty of learning an API. But conversely, you could say", "tokens": [51540, 15228, 425, 4745, 4560, 3462, 281, 264, 10360, 295, 2539, 364, 9362, 13, 583, 2615, 736, 11, 291, 727, 584, 51776], "temperature": 0.0, "avg_logprob": -0.13145749181763738, "compression_ratio": 1.5616438356164384, "no_speech_prob": 0.001345357159152627}, {"id": 528, "seek": 314416, "start": 3144.16, "end": 3149.68, "text": " tools like co-pilot are sort of the parallel to overcoming that articulatory distance by", "tokens": [50364, 3873, 411, 598, 12, 79, 31516, 366, 1333, 295, 264, 8952, 281, 38047, 300, 15228, 425, 4745, 4560, 538, 50640], "temperature": 0.0, "avg_logprob": -0.08533672852949663, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.032060407102108}, {"id": 529, "seek": 314416, "start": 3149.68, "end": 3153.6, "text": " almost in some sense, what is the same interface expressing a natural language but just in a", "tokens": [50640, 1920, 294, 512, 2020, 11, 437, 307, 264, 912, 9226, 22171, 257, 3303, 2856, 457, 445, 294, 257, 50836], "temperature": 0.0, "avg_logprob": -0.08533672852949663, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.032060407102108}, {"id": 530, "seek": 314416, "start": 3153.6, "end": 3158.72, "text": " code comment and then getting back code, right? But just I guess the only difference is its code", "tokens": [50836, 3089, 2871, 293, 550, 1242, 646, 3089, 11, 558, 30, 583, 445, 286, 2041, 264, 787, 2649, 307, 1080, 3089, 51092], "temperature": 0.0, "avg_logprob": -0.08533672852949663, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.032060407102108}, {"id": 531, "seek": 314416, "start": 3158.72, "end": 3162.56, "text": " you can see as opposed to code that's running in some back end that you don't see. And I'm curious", "tokens": [51092, 291, 393, 536, 382, 8851, 281, 3089, 300, 311, 2614, 294, 512, 646, 917, 300, 291, 500, 380, 536, 13, 400, 286, 478, 6369, 51284], "temperature": 0.0, "avg_logprob": -0.08533672852949663, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.032060407102108}, {"id": 532, "seek": 314416, "start": 3162.56, "end": 3167.52, "text": " if you think there's sort of a synthesis of these two poles, an interface that can take the best of", "tokens": [51284, 498, 291, 519, 456, 311, 1333, 295, 257, 30252, 295, 613, 732, 24760, 11, 364, 9226, 300, 393, 747, 264, 1151, 295, 51532], "temperature": 0.0, "avg_logprob": -0.08533672852949663, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.032060407102108}, {"id": 533, "seek": 314416, "start": 3167.52, "end": 3172.48, "text": " both worlds and offers conversation but still provides access to the code or encourages people", "tokens": [51532, 1293, 13401, 293, 7736, 3761, 457, 920, 6417, 2105, 281, 264, 3089, 420, 28071, 561, 51780], "temperature": 0.0, "avg_logprob": -0.08533672852949663, "compression_ratio": 1.7492354740061162, "no_speech_prob": 0.032060407102108}, {"id": 534, "seek": 317248, "start": 3172.48, "end": 3178.8, "text": " to understand the annoying representations. Yeah, absolutely. I thought really hard about", "tokens": [50364, 281, 1223, 264, 11304, 33358, 13, 865, 11, 3122, 13, 286, 1194, 534, 1152, 466, 50680], "temperature": 0.0, "avg_logprob": -0.12254263559977213, "compression_ratio": 1.5041666666666667, "no_speech_prob": 0.0021819481626152992}, {"id": 535, "seek": 317248, "start": 3179.52, "end": 3186.72, "text": " which of those examples I wanted to use as the kind of foil to B2. So I did very seriously", "tokens": [50716, 597, 295, 729, 5110, 286, 1415, 281, 764, 382, 264, 733, 295, 22444, 281, 363, 17, 13, 407, 286, 630, 588, 6638, 51076], "temperature": 0.0, "avg_logprob": -0.12254263559977213, "compression_ratio": 1.5041666666666667, "no_speech_prob": 0.0021819481626152992}, {"id": 536, "seek": 317248, "start": 3186.72, "end": 3193.76, "text": " consider a co-pilot and I sort of agree with your analysis that it's, I think, a much better", "tokens": [51076, 1949, 257, 598, 12, 79, 31516, 293, 286, 1333, 295, 3986, 365, 428, 5215, 300, 309, 311, 11, 286, 519, 11, 257, 709, 1101, 51428], "temperature": 0.0, "avg_logprob": -0.12254263559977213, "compression_ratio": 1.5041666666666667, "no_speech_prob": 0.0021819481626152992}, {"id": 537, "seek": 317248, "start": 3193.76, "end": 3200.88, "text": " example of how to integrate the capacity of these LLMs. And I think there's opportunity", "tokens": [51428, 1365, 295, 577, 281, 13365, 264, 6042, 295, 613, 441, 43, 26386, 13, 400, 286, 519, 456, 311, 2650, 51784], "temperature": 0.0, "avg_logprob": -0.12254263559977213, "compression_ratio": 1.5041666666666667, "no_speech_prob": 0.0021819481626152992}, {"id": 538, "seek": 320088, "start": 3200.88, "end": 3207.2000000000003, "text": " to push that even further where what I would often want is really targeted mechanisms to", "tokens": [50364, 281, 2944, 300, 754, 3052, 689, 437, 286, 576, 2049, 528, 307, 534, 15045, 15902, 281, 50680], "temperature": 0.0, "avg_logprob": -0.08457622528076172, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.006690504495054483}, {"id": 539, "seek": 320088, "start": 3207.2000000000003, "end": 3213.52, "text": " introduce ambiguity, right? Right now, the little that I've used co-pilot, it's almost at the level", "tokens": [50680, 5366, 46519, 11, 558, 30, 1779, 586, 11, 264, 707, 300, 286, 600, 1143, 598, 12, 79, 31516, 11, 309, 311, 1920, 412, 264, 1496, 50996], "temperature": 0.0, "avg_logprob": -0.08457622528076172, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.006690504495054483}, {"id": 540, "seek": 320088, "start": 3213.52, "end": 3220.7200000000003, "text": " of, well, it's going to produce the whole function, the whole whatever. And often what I want is it", "tokens": [50996, 295, 11, 731, 11, 309, 311, 516, 281, 5258, 264, 1379, 2445, 11, 264, 1379, 2035, 13, 400, 2049, 437, 286, 528, 307, 309, 51356], "temperature": 0.0, "avg_logprob": -0.08457622528076172, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.006690504495054483}, {"id": 541, "seek": 320088, "start": 3220.7200000000003, "end": 3226.6400000000003, "text": " to be the sort of parallel prototyper for me, right? I want to introduce, say, a hole in my program", "tokens": [51356, 281, 312, 264, 1333, 295, 8952, 46219, 610, 337, 385, 11, 558, 30, 286, 528, 281, 5366, 11, 584, 11, 257, 5458, 294, 452, 1461, 51652], "temperature": 0.0, "avg_logprob": -0.08457622528076172, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.006690504495054483}, {"id": 542, "seek": 322664, "start": 3226.64, "end": 3231.04, "text": " and then go, I don't know that I want that hole to be filled in with just one specific", "tokens": [50364, 293, 550, 352, 11, 286, 500, 380, 458, 300, 286, 528, 300, 5458, 281, 312, 6412, 294, 365, 445, 472, 2685, 50584], "temperature": 0.0, "avg_logprob": -0.10095444703713441, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.004069507122039795}, {"id": 543, "seek": 322664, "start": 3232.08, "end": 3236.96, "text": " outcome, but I want it to produce the whole space and for me to go, well, I want a little bit of", "tokens": [50636, 9700, 11, 457, 286, 528, 309, 281, 5258, 264, 1379, 1901, 293, 337, 385, 281, 352, 11, 731, 11, 286, 528, 257, 707, 857, 295, 50880], "temperature": 0.0, "avg_logprob": -0.10095444703713441, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.004069507122039795}, {"id": 544, "seek": 322664, "start": 3236.96, "end": 3243.52, "text": " this and a little bit of that and so on and so forth. So yeah, I totally agree with there being", "tokens": [50880, 341, 293, 257, 707, 857, 295, 300, 293, 370, 322, 293, 370, 5220, 13, 407, 1338, 11, 286, 3879, 3986, 365, 456, 885, 51208], "temperature": 0.0, "avg_logprob": -0.10095444703713441, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.004069507122039795}, {"id": 545, "seek": 322664, "start": 3243.52, "end": 3248.08, "text": " some really interesting medium of these things. Cool. Yeah, I like that idea.", "tokens": [51208, 512, 534, 1880, 6399, 295, 613, 721, 13, 8561, 13, 865, 11, 286, 411, 300, 1558, 13, 51436], "temperature": 0.0, "avg_logprob": -0.10095444703713441, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.004069507122039795}, {"id": 546, "seek": 322664, "start": 3250.0, "end": 3256.4, "text": " Yeah. Hey, really exciting talk. I'm wondering towards your kind of vision for these abstraction", "tokens": [51532, 865, 13, 1911, 11, 534, 4670, 751, 13, 286, 478, 6359, 3030, 428, 733, 295, 5201, 337, 613, 37765, 51852], "temperature": 0.0, "avg_logprob": -0.10095444703713441, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.004069507122039795}, {"id": 547, "seek": 325640, "start": 3256.48, "end": 3262.4, "text": " models, I'm wondering like obviously kind of from a human-interpreter interaction perspective,", "tokens": [50368, 5245, 11, 286, 478, 6359, 411, 2745, 733, 295, 490, 257, 1952, 12, 5106, 3712, 391, 9285, 4585, 11, 50664], "temperature": 0.0, "avg_logprob": -0.1253804142035327, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0032720095477998257}, {"id": 548, "seek": 325640, "start": 3262.4, "end": 3268.32, "text": " we know like representation matters so much, right? Like isomorphs of representation very much", "tokens": [50664, 321, 458, 411, 10290, 7001, 370, 709, 11, 558, 30, 1743, 307, 32702, 82, 295, 10290, 588, 709, 50960], "temperature": 0.0, "avg_logprob": -0.1253804142035327, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0032720095477998257}, {"id": 549, "seek": 325640, "start": 3268.32, "end": 3274.08, "text": " change how people can approach a problem or understand it. But I guess the ways in which", "tokens": [50960, 1319, 577, 561, 393, 3109, 257, 1154, 420, 1223, 309, 13, 583, 286, 2041, 264, 2098, 294, 597, 51248], "temperature": 0.0, "avg_logprob": -0.1253804142035327, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0032720095477998257}, {"id": 550, "seek": 325640, "start": 3274.08, "end": 3280.48, "text": " they vary and the benefits of these different representations are tied very much to human", "tokens": [51248, 436, 10559, 293, 264, 5311, 295, 613, 819, 33358, 366, 9601, 588, 709, 281, 1952, 51568], "temperature": 0.0, "avg_logprob": -0.1253804142035327, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0032720095477998257}, {"id": 551, "seek": 325640, "start": 3280.48, "end": 3285.52, "text": " cognition and perception. And I'm wondering, you know, in some of the examples you're showing and", "tokens": [51568, 46905, 293, 12860, 13, 400, 286, 478, 6359, 11, 291, 458, 11, 294, 512, 295, 264, 5110, 291, 434, 4099, 293, 51820], "temperature": 0.0, "avg_logprob": -0.1253804142035327, "compression_ratio": 1.7007299270072993, "no_speech_prob": 0.0032720095477998257}, {"id": 552, "seek": 328552, "start": 3285.52, "end": 3290.64, "text": " a lot of work in machine learning, we're sort of training things based upon that output. Yeah.", "tokens": [50364, 257, 688, 295, 589, 294, 3479, 2539, 11, 321, 434, 1333, 295, 3097, 721, 2361, 3564, 300, 5598, 13, 865, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12321794242189642, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00035695068072527647}, {"id": 553, "seek": 328552, "start": 3290.64, "end": 3295.44, "text": " And I'm wondering like, you know, are there ways that we can get at more of how people", "tokens": [50620, 400, 286, 478, 6359, 411, 11, 291, 458, 11, 366, 456, 2098, 300, 321, 393, 483, 412, 544, 295, 577, 561, 50860], "temperature": 0.0, "avg_logprob": -0.12321794242189642, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00035695068072527647}, {"id": 554, "seek": 328552, "start": 3295.44, "end": 3301.2, "text": " are thinking versus just how they output and how do we get there? Yeah, I love this question.", "tokens": [50860, 366, 1953, 5717, 445, 577, 436, 5598, 293, 577, 360, 321, 483, 456, 30, 865, 11, 286, 959, 341, 1168, 13, 51148], "temperature": 0.0, "avg_logprob": -0.12321794242189642, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00035695068072527647}, {"id": 555, "seek": 328552, "start": 3302.0, "end": 3308.16, "text": " And the reason I love it is also the reason I love sort of that Hutchins et al description of", "tokens": [51188, 400, 264, 1778, 286, 959, 309, 307, 611, 264, 1778, 286, 959, 1333, 295, 300, 48499, 1292, 1030, 419, 3855, 295, 51496], "temperature": 0.0, "avg_logprob": -0.12321794242189642, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00035695068072527647}, {"id": 556, "seek": 328552, "start": 3308.16, "end": 3312.48, "text": " direct manipulation, right? I find the terms that they use there, particularly these two's", "tokens": [51496, 2047, 26475, 11, 558, 30, 286, 915, 264, 2115, 300, 436, 764, 456, 11, 4098, 613, 732, 311, 51712], "temperature": 0.0, "avg_logprob": -0.12321794242189642, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.00035695068072527647}, {"id": 557, "seek": 331248, "start": 3312.48, "end": 3318.2400000000002, "text": " distances, really evocative terms. Because to me, a distance is something that I would want to", "tokens": [50364, 22182, 11, 534, 1073, 905, 1166, 2115, 13, 1436, 281, 385, 11, 257, 4560, 307, 746, 300, 286, 576, 528, 281, 50652], "temperature": 0.0, "avg_logprob": -0.09628453621497521, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00047260650899261236}, {"id": 558, "seek": 331248, "start": 3318.2400000000002, "end": 3324.8, "text": " measure, right? But so far at least, as far as I know, those terms have largely been descriptive,", "tokens": [50652, 3481, 11, 558, 30, 583, 370, 1400, 412, 1935, 11, 382, 1400, 382, 286, 458, 11, 729, 2115, 362, 11611, 668, 42585, 11, 50980], "temperature": 0.0, "avg_logprob": -0.09628453621497521, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00047260650899261236}, {"id": 559, "seek": 331248, "start": 3324.8, "end": 3330.56, "text": " right? As you saw in my talk, like I use them to be very analytic, but I'm not able to be generative", "tokens": [50980, 558, 30, 1018, 291, 1866, 294, 452, 751, 11, 411, 286, 764, 552, 281, 312, 588, 40358, 11, 457, 286, 478, 406, 1075, 281, 312, 1337, 1166, 51268], "temperature": 0.0, "avg_logprob": -0.09628453621497521, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00047260650899261236}, {"id": 560, "seek": 331248, "start": 3330.56, "end": 3338.08, "text": " with them in, you know, a very systematic way. So certainly a lot of the work that my group is", "tokens": [51268, 365, 552, 294, 11, 291, 458, 11, 257, 588, 27249, 636, 13, 407, 3297, 257, 688, 295, 264, 589, 300, 452, 1594, 307, 51644], "temperature": 0.0, "avg_logprob": -0.09628453621497521, "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00047260650899261236}, {"id": 561, "seek": 333808, "start": 3338.08, "end": 3343.44, "text": " trying to do right now is in visualization, you know, there's a lot of work that we've", "tokens": [50364, 1382, 281, 360, 558, 586, 307, 294, 25801, 11, 291, 458, 11, 456, 311, 257, 688, 295, 589, 300, 321, 600, 50632], "temperature": 0.0, "avg_logprob": -0.07766816375452444, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0014316585147753358}, {"id": 562, "seek": 333808, "start": 3344.24, "end": 3349.68, "text": " inherited in methods from sort of vision science. So we run these studies of human perception.", "tokens": [50672, 27091, 294, 7150, 490, 1333, 295, 5201, 3497, 13, 407, 321, 1190, 613, 5313, 295, 1952, 12860, 13, 50944], "temperature": 0.0, "avg_logprob": -0.07766816375452444, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0014316585147753358}, {"id": 563, "seek": 333808, "start": 3351.36, "end": 3356.56, "text": " And increasingly, the field is starting to get to, well, how do we start to measure cognition,", "tokens": [51028, 400, 12980, 11, 264, 2519, 307, 2891, 281, 483, 281, 11, 731, 11, 577, 360, 321, 722, 281, 3481, 46905, 11, 51288], "temperature": 0.0, "avg_logprob": -0.07766816375452444, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0014316585147753358}, {"id": 564, "seek": 333808, "start": 3356.56, "end": 3362.3199999999997, "text": " right? Can we model sort of a decision making task and start to, you know, operationalize", "tokens": [51288, 558, 30, 1664, 321, 2316, 1333, 295, 257, 3537, 1455, 5633, 293, 722, 281, 11, 291, 458, 11, 16607, 1125, 51576], "temperature": 0.0, "avg_logprob": -0.07766816375452444, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0014316585147753358}, {"id": 565, "seek": 333808, "start": 3362.3199999999997, "end": 3366.7999999999997, "text": " that through experimental design? And so we're starting to push in some of those directions", "tokens": [51576, 300, 807, 17069, 1715, 30, 400, 370, 321, 434, 2891, 281, 2944, 294, 512, 295, 729, 11095, 51800], "temperature": 0.0, "avg_logprob": -0.07766816375452444, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.0014316585147753358}, {"id": 566, "seek": 336680, "start": 3366.8, "end": 3371.44, "text": " as well, but scope to sort of, you know, interaction in a Jupyter notebook, but then", "tokens": [50364, 382, 731, 11, 457, 11923, 281, 1333, 295, 11, 291, 458, 11, 9285, 294, 257, 22125, 88, 391, 21060, 11, 457, 550, 50596], "temperature": 0.0, "avg_logprob": -0.09845007790459527, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0004727059858851135}, {"id": 567, "seek": 336680, "start": 3371.44, "end": 3375.84, "text": " starting to see, you know, the impact that interaction has on sort of the downstream", "tokens": [50596, 2891, 281, 536, 11, 291, 458, 11, 264, 2712, 300, 9285, 575, 322, 1333, 295, 264, 30621, 50816], "temperature": 0.0, "avg_logprob": -0.09845007790459527, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0004727059858851135}, {"id": 568, "seek": 336680, "start": 3375.84, "end": 3381.52, "text": " analyses people would do, and then see if that actually maps to, you know, their goals or things", "tokens": [50816, 37560, 561, 576, 360, 11, 293, 550, 536, 498, 300, 767, 11317, 281, 11, 291, 458, 11, 641, 5493, 420, 721, 51100], "temperature": 0.0, "avg_logprob": -0.09845007790459527, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0004727059858851135}, {"id": 569, "seek": 336680, "start": 3381.52, "end": 3394.48, "text": " like that. Absolutely. Yeah. So I'm curious about the, just continue on this line of", "tokens": [51100, 411, 300, 13, 7021, 13, 865, 13, 407, 286, 478, 6369, 466, 264, 11, 445, 2354, 322, 341, 1622, 295, 51748], "temperature": 0.0, "avg_logprob": -0.09845007790459527, "compression_ratio": 1.6556603773584906, "no_speech_prob": 0.0004727059858851135}, {"id": 570, "seek": 339448, "start": 3394.48, "end": 3400.08, "text": " perception up through cognition, you know, going back to the sort of like,", "tokens": [50364, 12860, 493, 807, 46905, 11, 291, 458, 11, 516, 646, 281, 264, 1333, 295, 411, 11, 50644], "temperature": 0.0, "avg_logprob": -0.18160214878263928, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.010981392115354538}, {"id": 571, "seek": 339448, "start": 3400.8, "end": 3407.84, "text": " or 10 Cleveland McGill kinds of stuff, the automatic processing was very key to the design", "tokens": [50680, 420, 1266, 27846, 21865, 373, 3685, 295, 1507, 11, 264, 12509, 9007, 390, 588, 2141, 281, 264, 1715, 51032], "temperature": 0.0, "avg_logprob": -0.18160214878263928, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.010981392115354538}, {"id": 572, "seek": 339448, "start": 3407.84, "end": 3413.68, "text": " of visualizations, especially early on, that the notion was that my encodings were supposed to", "tokens": [51032, 295, 5056, 14455, 11, 2318, 2440, 322, 11, 300, 264, 10710, 390, 300, 452, 2058, 378, 1109, 645, 3442, 281, 51324], "temperature": 0.0, "avg_logprob": -0.18160214878263928, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.010981392115354538}, {"id": 573, "seek": 339448, "start": 3413.68, "end": 3420.96, "text": " map on to almost like system one interpretation, right? Like when I see the scatterplot and", "tokens": [51324, 4471, 322, 281, 1920, 411, 1185, 472, 14174, 11, 558, 30, 1743, 562, 286, 536, 264, 34951, 564, 310, 293, 51688], "temperature": 0.0, "avg_logprob": -0.18160214878263928, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.010981392115354538}, {"id": 574, "seek": 342096, "start": 3420.96, "end": 3424.56, "text": " you know, encoding distance in the following way, I'm gonna draw the correct conclusion.", "tokens": [50364, 291, 458, 11, 43430, 4560, 294, 264, 3480, 636, 11, 286, 478, 799, 2642, 264, 3006, 10063, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11893911361694336, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.07582376152276993}, {"id": 575, "seek": 342096, "start": 3424.56, "end": 3431.52, "text": " And it's interesting to me that sort of through the transformations you've started to pursue,", "tokens": [50544, 400, 309, 311, 1880, 281, 385, 300, 1333, 295, 807, 264, 34852, 291, 600, 1409, 281, 12392, 11, 50892], "temperature": 0.0, "avg_logprob": -0.11893911361694336, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.07582376152276993}, {"id": 576, "seek": 342096, "start": 3431.52, "end": 3439.6, "text": " we're not trying to like encode those into a similar mapping for audio, but instead", "tokens": [50892, 321, 434, 406, 1382, 281, 411, 2058, 1429, 729, 666, 257, 2531, 18350, 337, 6278, 11, 457, 2602, 51296], "temperature": 0.0, "avg_logprob": -0.11893911361694336, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.07582376152276993}, {"id": 577, "seek": 342096, "start": 3439.6, "end": 3445.36, "text": " directly doing the cognition on behalf of the individual. And those seem like orthogonal", "tokens": [51296, 3838, 884, 264, 46905, 322, 9490, 295, 264, 2609, 13, 400, 729, 1643, 411, 41488, 51584], "temperature": 0.0, "avg_logprob": -0.11893911361694336, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.07582376152276993}, {"id": 578, "seek": 342096, "start": 3445.36, "end": 3450.88, "text": " directions one could go. I'm curious how we find the right point in the design space.", "tokens": [51584, 11095, 472, 727, 352, 13, 286, 478, 6369, 577, 321, 915, 264, 558, 935, 294, 264, 1715, 1901, 13, 51860], "temperature": 0.0, "avg_logprob": -0.11893911361694336, "compression_ratio": 1.651685393258427, "no_speech_prob": 0.07582376152276993}, {"id": 579, "seek": 345088, "start": 3451.84, "end": 3457.28, "text": " I think this is a fantastic question. So the way my group is starting to think about this", "tokens": [50412, 286, 519, 341, 307, 257, 5456, 1168, 13, 407, 264, 636, 452, 1594, 307, 2891, 281, 519, 466, 341, 50684], "temperature": 0.0, "avg_logprob": -0.09358759024708542, "compression_ratio": 1.7896995708154506, "no_speech_prob": 5.828158828080632e-05}, {"id": 580, "seek": 345088, "start": 3457.92, "end": 3461.76, "text": " of like, how do we find the sort of right balance of who is doing the perception,", "tokens": [50716, 295, 411, 11, 577, 360, 321, 915, 264, 1333, 295, 558, 4772, 295, 567, 307, 884, 264, 12860, 11, 50908], "temperature": 0.0, "avg_logprob": -0.09358759024708542, "compression_ratio": 1.7896995708154506, "no_speech_prob": 5.828158828080632e-05}, {"id": 581, "seek": 345088, "start": 3461.76, "end": 3466.7200000000003, "text": " who is doing the interpretation is starting to consider some of these modalities and concert", "tokens": [50908, 567, 307, 884, 264, 14174, 307, 2891, 281, 1949, 512, 295, 613, 1072, 16110, 293, 8543, 51156], "temperature": 0.0, "avg_logprob": -0.09358759024708542, "compression_ratio": 1.7896995708154506, "no_speech_prob": 5.828158828080632e-05}, {"id": 582, "seek": 345088, "start": 3466.7200000000003, "end": 3470.56, "text": " to better understand what the relative affordances of these modalities are.", "tokens": [51156, 281, 1101, 1223, 437, 264, 4972, 6157, 2676, 295, 613, 1072, 16110, 366, 13, 51348], "temperature": 0.0, "avg_logprob": -0.09358759024708542, "compression_ratio": 1.7896995708154506, "no_speech_prob": 5.828158828080632e-05}, {"id": 583, "seek": 345088, "start": 3470.56, "end": 3475.28, "text": " So in particular, Jonathan, who you saw in the demos is leading some really,", "tokens": [51348, 407, 294, 1729, 11, 15471, 11, 567, 291, 1866, 294, 264, 33788, 307, 5775, 512, 534, 11, 51584], "temperature": 0.0, "avg_logprob": -0.09358759024708542, "compression_ratio": 1.7896995708154506, "no_speech_prob": 5.828158828080632e-05}, {"id": 584, "seek": 347528, "start": 3475.28, "end": 3483.0400000000004, "text": " really cool work right now around what if I'm sort of specifying the visual, the audio,", "tokens": [50364, 534, 1627, 589, 558, 586, 926, 437, 498, 286, 478, 1333, 295, 1608, 5489, 264, 5056, 11, 264, 6278, 11, 50752], "temperature": 0.0, "avg_logprob": -0.10139668642819583, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0041967034339904785}, {"id": 585, "seek": 347528, "start": 3483.0400000000004, "end": 3487.84, "text": " the sort of sonified audio and the textual audio side by side, and then I'm playing them", "tokens": [50752, 264, 1333, 295, 1872, 2587, 6278, 293, 264, 2487, 901, 6278, 1252, 538, 1252, 11, 293, 550, 286, 478, 2433, 552, 50992], "temperature": 0.0, "avg_logprob": -0.10139668642819583, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0041967034339904785}, {"id": 586, "seek": 347528, "start": 3487.84, "end": 3493.6800000000003, "text": " sort of simultaneously through. Do I want, you know, there to be sort of perceptual redundancy", "tokens": [50992, 1333, 295, 16561, 807, 13, 1144, 286, 528, 11, 291, 458, 11, 456, 281, 312, 1333, 295, 43276, 901, 27830, 6717, 51284], "temperature": 0.0, "avg_logprob": -0.10139668642819583, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0041967034339904785}, {"id": 587, "seek": 347528, "start": 3493.6800000000003, "end": 3499.52, "text": " where the sonification is sort of emphasizing what is, you know, described in the texture?", "tokens": [51284, 689, 264, 1872, 3774, 307, 1333, 295, 45550, 437, 307, 11, 291, 458, 11, 7619, 294, 264, 8091, 30, 51576], "temperature": 0.0, "avg_logprob": -0.10139668642819583, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0041967034339904785}, {"id": 588, "seek": 349952, "start": 3499.52, "end": 3506.56, "text": " Do I want these modalities to be complementary? And, you know, sort of TBD, but I think there's", "tokens": [50364, 1144, 286, 528, 613, 1072, 16110, 281, 312, 40705, 30, 400, 11, 291, 458, 11, 1333, 295, 29711, 35, 11, 457, 286, 519, 456, 311, 50716], "temperature": 0.0, "avg_logprob": -0.12074927416714755, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.008979763835668564}, {"id": 589, "seek": 349952, "start": 3506.56, "end": 3511.6, "text": " some really exciting sort of questions for us to sort of dig into space.", "tokens": [50716, 512, 534, 4670, 1333, 295, 1651, 337, 505, 281, 1333, 295, 2528, 666, 1901, 13, 50968], "temperature": 0.0, "avg_logprob": -0.12074927416714755, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.008979763835668564}, {"id": 590, "seek": 349952, "start": 3511.6, "end": 3514.72, "text": " Are there similar pre attentive principles for audio? There must be.", "tokens": [50968, 2014, 456, 2531, 659, 43661, 9156, 337, 6278, 30, 821, 1633, 312, 13, 51124], "temperature": 0.0, "avg_logprob": -0.12074927416714755, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.008979763835668564}, {"id": 591, "seek": 349952, "start": 3516.0, "end": 3520.48, "text": " As far as I know, so I'm, you know, we're just starting to look in the sort of sonification", "tokens": [51188, 1018, 1400, 382, 286, 458, 11, 370, 286, 478, 11, 291, 458, 11, 321, 434, 445, 2891, 281, 574, 294, 264, 1333, 295, 1872, 3774, 51412], "temperature": 0.0, "avg_logprob": -0.12074927416714755, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.008979763835668564}, {"id": 592, "seek": 349952, "start": 3520.48, "end": 3526.72, "text": " literature. Yeah, as far as we can tell sound is a very, very different perceptual sense", "tokens": [51412, 10394, 13, 865, 11, 382, 1400, 382, 321, 393, 980, 1626, 307, 257, 588, 11, 588, 819, 43276, 901, 2020, 51724], "temperature": 0.0, "avg_logprob": -0.12074927416714755, "compression_ratio": 1.5954198473282444, "no_speech_prob": 0.008979763835668564}, {"id": 593, "seek": 352672, "start": 3527.52, "end": 3533.4399999999996, "text": " than vision. And so even the sort of, you know, basic sort of visual encoding paradigm where I", "tokens": [50404, 813, 5201, 13, 400, 370, 754, 264, 1333, 295, 11, 291, 458, 11, 3875, 1333, 295, 5056, 43430, 24709, 689, 286, 50700], "temperature": 0.0, "avg_logprob": -0.09255982408619891, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.0018094676779583097}, {"id": 594, "seek": 352672, "start": 3533.4399999999996, "end": 3539.6, "text": " take a data field, I map it to, you know, position color size that breaks down very rapidly for audio.", "tokens": [50700, 747, 257, 1412, 2519, 11, 286, 4471, 309, 281, 11, 291, 458, 11, 2535, 2017, 2744, 300, 9857, 760, 588, 12910, 337, 6278, 13, 51008], "temperature": 0.0, "avg_logprob": -0.09255982408619891, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.0018094676779583097}, {"id": 595, "seek": 352672, "start": 3540.3199999999997, "end": 3546.3999999999996, "text": " So oftentimes really all the people are able to sort of, you know, detect differences in our sort", "tokens": [51044, 407, 18349, 534, 439, 264, 561, 366, 1075, 281, 1333, 295, 11, 291, 458, 11, 5531, 7300, 294, 527, 1333, 51348], "temperature": 0.0, "avg_logprob": -0.09255982408619891, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.0018094676779583097}, {"id": 596, "seek": 352672, "start": 3546.3999999999996, "end": 3553.6, "text": " of pitch and loudness. And even then our fidelity at that is very, very low. And so there might be", "tokens": [51348, 295, 7293, 293, 6588, 1287, 13, 400, 754, 550, 527, 46404, 412, 300, 307, 588, 11, 588, 2295, 13, 400, 370, 456, 1062, 312, 51708], "temperature": 0.0, "avg_logprob": -0.09255982408619891, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.0018094676779583097}, {"id": 597, "seek": 355360, "start": 3553.6, "end": 3559.12, "text": " some pre attentive characteristics. We're certainly looking at some early work in HCI.", "tokens": [50364, 512, 659, 43661, 10891, 13, 492, 434, 3297, 1237, 412, 512, 2440, 589, 294, 389, 25240, 13, 50640], "temperature": 0.0, "avg_logprob": -0.13575130751153, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002755688736215234}, {"id": 598, "seek": 355360, "start": 3559.12, "end": 3567.12, "text": " I think Stephen Brewster had done around sort of ear cons, you know, discreet sort of representations", "tokens": [50640, 286, 519, 13391, 42906, 3120, 632, 1096, 926, 1333, 295, 1273, 1014, 11, 291, 458, 11, 2983, 4751, 1333, 295, 33358, 51040], "temperature": 0.0, "avg_logprob": -0.13575130751153, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002755688736215234}, {"id": 599, "seek": 355360, "start": 3567.12, "end": 3571.04, "text": " of icons, but through audio and things like that. So there may be some of that there.", "tokens": [51040, 295, 23308, 11, 457, 807, 6278, 293, 721, 411, 300, 13, 407, 456, 815, 312, 512, 295, 300, 456, 13, 51236], "temperature": 0.0, "avg_logprob": -0.13575130751153, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002755688736215234}, {"id": 600, "seek": 355360, "start": 3572.0, "end": 3575.92, "text": " But at least so far we're so early in our own work that we don't know.", "tokens": [51284, 583, 412, 1935, 370, 1400, 321, 434, 370, 2440, 294, 527, 1065, 589, 300, 321, 500, 380, 458, 13, 51480], "temperature": 0.0, "avg_logprob": -0.13575130751153, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002755688736215234}, {"id": 601, "seek": 355360, "start": 3575.92, "end": 3577.52, "text": " Interesting. Okay, thank you. Yeah.", "tokens": [51480, 14711, 13, 1033, 11, 1309, 291, 13, 865, 13, 51560], "temperature": 0.0, "avg_logprob": -0.13575130751153, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002755688736215234}, {"id": 602, "seek": 355360, "start": 3579.6, "end": 3583.2799999999997, "text": " I think we're about at time. So if you have additional questions, please mob him after", "tokens": [51664, 286, 519, 321, 434, 466, 412, 565, 13, 407, 498, 291, 362, 4497, 1651, 11, 1767, 4298, 796, 934, 51848], "temperature": 0.0, "avg_logprob": -0.13575130751153, "compression_ratio": 1.6306620209059233, "no_speech_prob": 0.002755688736215234}, {"id": 603, "seek": 358328, "start": 3583.28, "end": 3592.8, "text": " the talk. Thank you, Arvin for joining us. Thank you very much.", "tokens": [50364, 264, 751, 13, 1044, 291, 11, 1587, 4796, 337, 5549, 505, 13, 1044, 291, 588, 709, 13, 50840], "temperature": 0.0, "avg_logprob": -0.2756887197494507, "compression_ratio": 1.0161290322580645, "no_speech_prob": 0.011824038811028004}], "language": "en"}