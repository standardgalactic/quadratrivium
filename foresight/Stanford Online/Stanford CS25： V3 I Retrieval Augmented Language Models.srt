1
00:00:00,000 --> 00:00:11,840
Hey guys, welcome to our last lecture of this quarter.

2
00:00:11,840 --> 00:00:15,520
And we're very happy to have Dawa here.

3
00:00:15,520 --> 00:00:22,120
He's the CEO of Contextual AI, the Enterprise LLM company, as well as an adjunct professor

4
00:00:22,120 --> 00:00:25,120
in symbolic systems here at Stanford.

5
00:00:25,120 --> 00:00:29,160
And previously, he was the head of research at Clicking Base, and before that a research

6
00:00:29,160 --> 00:00:32,520
scientist at Facebook AI Research.

7
00:00:32,520 --> 00:00:37,400
He received his PhD in masters from the University of Cambridge, as well as a master's in logic

8
00:00:37,400 --> 00:00:42,320
from the University of Amsterdam, and studied philosophy and incognitive AI in undergrad.

9
00:00:42,320 --> 00:00:48,040
And his work focuses on machine learning as well as NLP, specifically on developing better

10
00:00:48,040 --> 00:00:54,440
models for language understanding and generation, and better tools for evaluation and many times.

11
00:00:54,440 --> 00:00:57,480
Yeah, give it up for Adela.

12
00:00:58,480 --> 00:01:00,200
Thank you.

13
00:01:00,200 --> 00:01:04,120
So I guess I have to sort of stand here in the corner, so people can see me on this

14
00:01:04,120 --> 00:01:05,120
move as well.

15
00:01:05,120 --> 00:01:10,600
Yeah, thanks so much for having me here.

16
00:01:10,600 --> 00:01:12,280
So I asked Steven what I should talk about.

17
00:01:12,280 --> 00:01:17,000
There were a couple of things I could talk about, multi-modality or evaluation.

18
00:01:17,000 --> 00:01:23,520
And this was the preferred topic, I guess, because the others were already covered.

19
00:01:23,520 --> 00:01:28,600
So yeah, I'm very happy to talk to you about everything retrieval augmentation.

20
00:01:28,600 --> 00:01:33,080
I think this is really one of the coolest topics right now in our field.

21
00:01:33,080 --> 00:01:37,800
So I'll just give you an overview of what's been happening and what I think are the interesting

22
00:01:37,800 --> 00:01:40,920
questions to think about.

23
00:01:40,920 --> 00:01:46,520
So first of all, obviously, in case you've missed it, we are in the age of language models.

24
00:01:46,520 --> 00:01:51,200
And I just wanted to do a quick poll here in this not super big audience.

25
00:01:51,200 --> 00:01:57,120
I guess there's more people on the Zoom, but who invented language models?

26
00:01:57,120 --> 00:02:00,920
If you thought OpenAI, then I'm angry with you.

27
00:02:00,920 --> 00:02:04,280
So actually, this is a very, very old idea.

28
00:02:04,280 --> 00:02:10,000
So the idea is just you take a sequence and you factorize out the token probabilities.

29
00:02:10,000 --> 00:02:13,040
And so it wasn't invented by OpenAI.

30
00:02:13,040 --> 00:02:14,840
It's not like a few years old.

31
00:02:14,840 --> 00:02:18,000
It's actually several decades old.

32
00:02:18,000 --> 00:02:21,520
So I'm bringing this up because I was talking to someone and they were like, OpenAI invented

33
00:02:21,520 --> 00:02:22,520
language models.

34
00:02:22,520 --> 00:02:25,640
And I was like, they're kidding me, right?

35
00:02:25,640 --> 00:02:31,480
So I went back to the literature and this is the oldest one I could find actually, 1991

36
00:02:31,480 --> 00:02:33,720
first neural language model.

37
00:02:33,720 --> 00:02:39,760
There's a very nice paper from 2003 from Bengio where they actually have like word embeddings

38
00:02:39,760 --> 00:02:42,120
and everything already in there.

39
00:02:42,120 --> 00:02:46,040
So obviously, these are LLMs, not LLMs.

40
00:02:46,040 --> 00:02:51,000
And as it turns out, if you make them really big and you parameterize them with these massive

41
00:02:51,000 --> 00:02:55,800
neural nets, then you get something really powerful that really shows emergent properties.

42
00:02:55,800 --> 00:02:59,720
And that's why we're also excited in this stuff.

43
00:02:59,720 --> 00:03:04,400
So if we think about this from like a classic CS perspective, there's input output, right?

44
00:03:04,400 --> 00:03:07,360
There's this kind of thing in the middle, it's the generator.

45
00:03:07,360 --> 00:03:12,920
So we take a sequence, the input sequence, and then the task of the model is to predict

46
00:03:12,920 --> 00:03:14,600
the next token.

47
00:03:14,600 --> 00:03:17,360
Very, very simple model.

48
00:03:17,360 --> 00:03:21,480
And so, you know, that's why it was so easy to come up with this in 1991 already because

49
00:03:21,480 --> 00:03:24,080
it's like the idea is very intuitive.

50
00:03:24,080 --> 00:03:29,920
But for a long time, what was really broken with this was the user interface.

51
00:03:29,920 --> 00:03:35,160
And this, I think a lot of people kind of misunderstand what chat GPT was about.

52
00:03:35,160 --> 00:03:37,200
That's really what chat GPT fixed.

53
00:03:37,200 --> 00:03:41,840
So that initially you had to come up with these very weird prompts in order to get your

54
00:03:41,840 --> 00:03:44,880
language model to do what you wanted it to do.

55
00:03:44,880 --> 00:03:46,840
And humans are terrible at this, right?

56
00:03:46,840 --> 00:03:51,200
So we're much better at sort of telling people or things around us what we want, right?

57
00:03:51,200 --> 00:03:56,200
So if we have a dog, we say sit, we don't prompt it in a very weird way so that it sits,

58
00:03:56,200 --> 00:03:57,200
right?

59
00:03:57,200 --> 00:03:58,480
And it's the same with the language model.

60
00:03:58,480 --> 00:04:03,720
If you wanted to generate some red lyrics in the style of a pirate or Shakespeare or

61
00:04:03,720 --> 00:04:08,080
something, then you tell it generate some red lyrics in the style of a pirate, right?

62
00:04:08,080 --> 00:04:13,680
So that kind of instruction data actually turns out to be super, super rare in just web

63
00:04:13,680 --> 00:04:14,680
data.

64
00:04:14,680 --> 00:04:18,280
So what you need to do is you need to fix the user interface to the language model.

65
00:04:18,280 --> 00:04:23,560
And the classic recipe for doing that is the sequence basically that chat GPT used.

66
00:04:23,560 --> 00:04:25,160
So you prompt the model in a specific way.

67
00:04:25,160 --> 00:04:29,840
You will instruction fine tune the model and you can do some alignment or LHF, whatever

68
00:04:29,840 --> 00:04:31,960
you do on top of that.

69
00:04:31,960 --> 00:04:32,960
So that's the first thing.

70
00:04:32,960 --> 00:04:37,920
So now you have a working language model with a working user interface.

71
00:04:37,920 --> 00:04:40,560
So are we done then?

72
00:04:40,560 --> 00:04:41,560
Obviously we're not.

73
00:04:41,560 --> 00:04:45,160
So right now language models are kind of taking the world by storm.

74
00:04:45,160 --> 00:04:49,040
But if you talk to anyone, especially in an enterprise, for example, where they have very

75
00:04:49,040 --> 00:04:54,480
strict accuracy requirements, they will tell you that they can't really productionize this

76
00:04:54,480 --> 00:04:55,720
yet.

77
00:04:55,720 --> 00:04:58,680
And the reason is because there are all these familiar problems, probably a bunch of you

78
00:04:58,680 --> 00:05:03,480
are working on these problems right now around hallucination.

79
00:05:03,480 --> 00:05:07,120
So these models, they kind of make up stuff very often with very high confidence, which

80
00:05:07,120 --> 00:05:11,440
is even more scary in a way attribution.

81
00:05:11,440 --> 00:05:15,520
So we don't really know why these models are saying what they're saying stillness.

82
00:05:15,520 --> 00:05:16,520
They go out of date.

83
00:05:16,520 --> 00:05:20,320
And so this was a big problem with sort of chat GPT, not knowing anything that happened

84
00:05:20,320 --> 00:05:22,160
after a certain cutoff date.

85
00:05:22,160 --> 00:05:24,080
And they keep updating it every once in a while.

86
00:05:24,080 --> 00:05:29,040
But you want to have a system that's always completely up to date that never goes still.

87
00:05:29,040 --> 00:05:31,600
You want to be able to revise the information in the system.

88
00:05:31,600 --> 00:05:37,320
So if you're a European organization, you have to worry about GDPR, which means that

89
00:05:37,320 --> 00:05:42,240
you need to be able to remove information from the language model or maybe revise facts,

90
00:05:42,240 --> 00:05:43,960
which we don't really know how to do.

91
00:05:43,960 --> 00:05:50,040
So again, this is a very interesting area of study for a lot of folks, model editing.

92
00:05:50,040 --> 00:05:53,520
But so this is something that we really want to be able to fix.

93
00:05:53,520 --> 00:05:57,880
And then there's this big question of how do you customize these models?

94
00:05:57,880 --> 00:05:59,880
So different people have different use cases.

95
00:05:59,880 --> 00:06:00,880
You have different data.

96
00:06:00,880 --> 00:06:04,560
If you're a company or if you want to have a language model on your own data, how do

97
00:06:04,560 --> 00:06:06,840
you make it work on your own data?

98
00:06:06,840 --> 00:06:12,160
So one of the solutions that everybody has started using right now is to couple it to

99
00:06:12,160 --> 00:06:13,360
an external memory.

100
00:06:13,360 --> 00:06:17,200
So that's really just rag, right?

101
00:06:17,200 --> 00:06:19,840
This whole lecture is basically about rag.

102
00:06:19,840 --> 00:06:25,880
But the way to understand what is going on here is we have this generator just like before.

103
00:06:25,880 --> 00:06:27,800
We have the input and the prompt just like before.

104
00:06:27,800 --> 00:06:32,720
But now instead of just giving those two things, we give this additional context.

105
00:06:32,720 --> 00:06:37,600
So we contextualize the language model using things we've retrieved.

106
00:06:37,600 --> 00:06:40,680
And the retriever is very often pretty simple.

107
00:06:40,680 --> 00:06:43,600
It's just a query in the document encoder.

108
00:06:43,600 --> 00:06:48,040
And then you get a bunch of documents, you give them as context to the model.

109
00:06:48,040 --> 00:06:51,200
So super simple architecture.

110
00:06:51,200 --> 00:06:56,040
And I think it's useful to think about it from the perspective of these two separate

111
00:06:56,080 --> 00:06:57,840
paradigms.

112
00:06:57,840 --> 00:07:01,280
So if you've ever taken an exam, I'm sure you have, right?

113
00:07:01,280 --> 00:07:04,040
You can have a closed book exam where you have to memorize all of this, so you have

114
00:07:04,040 --> 00:07:08,600
to cram all the knowledge into your parameters, your neurons.

115
00:07:08,600 --> 00:07:12,160
Or you have an open book exam where you have all of this information in the book that you

116
00:07:12,160 --> 00:07:14,960
can access when you do the exam.

117
00:07:14,960 --> 00:07:16,840
So it's a very similar thing with rag, right?

118
00:07:16,840 --> 00:07:20,200
You can just make it an open book setting where you can give it access to this external

119
00:07:20,280 --> 00:07:25,920
information, Wikipedia or something else, or basically the entire internet, and then

120
00:07:25,920 --> 00:07:31,680
have the language model do its job without having to memorize all of it in its parameters.

121
00:07:31,680 --> 00:07:36,800
So Dr. I think useful distinction here is that cramming everything into your parameters,

122
00:07:36,800 --> 00:07:38,640
that's the parametric approach.

123
00:07:38,640 --> 00:07:44,120
So what we're doing with rag is we're adding this non-parametric retrieval component.

124
00:07:44,120 --> 00:07:49,080
So you might call this semi-parametric if you want to give this a name.

125
00:07:50,600 --> 00:07:54,920
All right, so why does that actually solve these issues?

126
00:07:54,920 --> 00:07:59,720
And so the answer is basically that if you have this separate index, right, this separate

127
00:07:59,720 --> 00:08:04,480
retriever, you can swap it in, you can swap it out, you can replace it with a new index

128
00:08:04,480 --> 00:08:06,480
so you can really customize it.

129
00:08:06,480 --> 00:08:12,800
And so you can customize your language model system for what the user really wants to see.

130
00:08:12,800 --> 00:08:17,040
And then obviously you can update this index, so it doesn't really go still and you can

131
00:08:17,040 --> 00:08:21,400
revise it if everything goes wrong, if anything goes wrong.

132
00:08:21,400 --> 00:08:23,560
The other thing you get is grounding.

133
00:08:23,560 --> 00:08:27,480
So that's initially why I became interested in this kind of architecture, because I was

134
00:08:27,480 --> 00:08:31,080
thinking a lot about grounding and multimodality and things like that, and actually one really

135
00:08:31,080 --> 00:08:35,760
nice way to ground things is to find some other information that you can ground your

136
00:08:35,760 --> 00:08:36,920
generation in.

137
00:08:36,920 --> 00:08:41,880
So you really want the language model to only say things that it has evidence for in this

138
00:08:41,880 --> 00:08:46,720
other piece of text, or even multimodal data that it retrieves separately.

139
00:08:46,720 --> 00:08:50,120
So if you do that, then you get less hallucination, because you can always point back to your

140
00:08:50,120 --> 00:08:53,160
source, it's always grounded in your source.

141
00:08:53,160 --> 00:08:56,080
And you get attribution, because you don't know why the model is saying what it's saying

142
00:08:56,080 --> 00:09:00,880
is because it founded this thing here, is that clear?

143
00:09:00,880 --> 00:09:02,800
All right.

144
00:09:02,800 --> 00:09:09,200
So for the rest of this lecture, we're going to talk about this basic architecture.

145
00:09:09,200 --> 00:09:12,920
And so it kind of looks like a pretty simple thing, right?

146
00:09:12,920 --> 00:09:17,360
But there are actually lots and lots of questions you can ask about what the system should really

147
00:09:17,360 --> 00:09:19,000
look like.

148
00:09:19,000 --> 00:09:23,200
And this doesn't even cover half the questions you can ask.

149
00:09:23,200 --> 00:09:28,000
So it really is about how do we optimize this entire system?

150
00:09:28,000 --> 00:09:33,600
So we have the separate components, the retriever, the generator, and then there are things like

151
00:09:33,600 --> 00:09:38,880
this query encoder, how do we encode queries, how do we do the retrieval, do we update the

152
00:09:38,880 --> 00:09:43,440
document encoder, or how do we actually define a document, right?

153
00:09:43,440 --> 00:09:48,720
Is it like a full document or is it a paragraph or a chunk or a sentence or a couple of words?

154
00:09:48,720 --> 00:09:51,280
So there are lots of questions to ask.

155
00:09:51,280 --> 00:09:56,800
And as you'll see, there are lots of possible answers to these questions as well.

156
00:09:56,800 --> 00:10:01,040
So this is what we'll cover.

157
00:10:01,040 --> 00:10:06,320
So there are lots of architectures going into these questions.

158
00:10:06,320 --> 00:10:11,320
And I think as we go through them, it's useful for you to think about what happens during

159
00:10:11,320 --> 00:10:14,440
training time and what happens during test time, right?

160
00:10:14,440 --> 00:10:20,720
So during training time, it's really, okay, we have the language model, we have this retriever.

161
00:10:20,720 --> 00:10:21,920
Which one do we update?

162
00:10:21,920 --> 00:10:23,040
How do we update them?

163
00:10:23,040 --> 00:10:24,880
How do we train this entire system?

164
00:10:24,880 --> 00:10:27,120
Do we maybe not train it at all?

165
00:10:27,120 --> 00:10:28,600
Do we pre-train it from scratch?

166
00:10:28,600 --> 00:10:33,160
Do we initialize it with components that were already separately trained?

167
00:10:33,160 --> 00:10:36,400
These are the kinds of questions that you have to answer if you want to design a system

168
00:10:36,400 --> 00:10:38,000
like this.

169
00:10:38,000 --> 00:10:41,760
And then during test time, you have this entire system, right?

170
00:10:41,760 --> 00:10:47,720
So actually multiple models in a way that are working together.

171
00:10:47,720 --> 00:10:49,800
So there's also different things you can do there, right?

172
00:10:49,800 --> 00:10:54,480
So give it different indices during test time or manipulate kind of how you're sampling

173
00:10:54,480 --> 00:10:56,760
things like that.

174
00:10:56,760 --> 00:11:01,360
So the starting point for all of this stuff, I think if you ask someone now, like, what

175
00:11:01,400 --> 00:11:04,720
is RAG, they will think of this thing.

176
00:11:04,720 --> 00:11:08,400
So this is frozen RAG basically.

177
00:11:08,400 --> 00:11:09,800
There's no training here at all.

178
00:11:09,800 --> 00:11:13,560
So going back to this question of train time, test time, there's only test time here.

179
00:11:13,560 --> 00:11:17,840
Train time happens separately with these kind of black box models that we don't necessarily

180
00:11:17,840 --> 00:11:19,080
have control over, right?

181
00:11:19,080 --> 00:11:24,120
So there's this document embedding model as whatever is currently at the top of some

182
00:11:24,160 --> 00:11:26,760
open source leaderboard.

183
00:11:26,760 --> 00:11:34,160
You use that to get some vectors that you then use to create this vector database.

184
00:11:34,160 --> 00:11:38,400
And then the vector database just does search and it gives the information from the search

185
00:11:38,400 --> 00:11:39,840
to the language model.

186
00:11:39,840 --> 00:11:43,720
And it just passes it as the context, right?

187
00:11:43,720 --> 00:11:47,800
So this only works because of in-context learning.

188
00:11:47,880 --> 00:11:54,240
And I think as a machine learner myself, this feels very inelegant.

189
00:11:54,240 --> 00:12:01,320
So what this lecture is about is, can we do better than this frozen thing?

190
00:12:01,320 --> 00:12:05,160
So let's start from the left side of this.

191
00:12:05,160 --> 00:12:09,760
OK, if we want to outperform this frozen thing itself with just the vector database,

192
00:12:09,760 --> 00:12:14,200
what would that look like from a retrieval perspective?

193
00:12:14,200 --> 00:12:18,080
And the starting point for everything retrieval is TFIDF.

194
00:12:18,080 --> 00:12:21,280
Does everybody know what TFIDF is?

195
00:12:21,280 --> 00:12:21,720
No?

196
00:12:21,720 --> 00:12:23,080
OK.

197
00:12:23,080 --> 00:12:29,400
So TFIDF is basically a sparse retrieval method where you have a score function that

198
00:12:29,400 --> 00:12:33,280
looks at documents and queries, so E and Q.

199
00:12:33,280 --> 00:12:35,280
And then there are basically two terms that matter.

200
00:12:35,280 --> 00:12:40,680
One is the TF, the term frequency, and the other is the IDF, the inverse document frequency.

201
00:12:40,720 --> 00:12:45,160
So this inverse document frequency is actually a really nice idea from Karen Spark-Jones,

202
00:12:45,160 --> 00:12:46,560
really underrated researcher.

203
00:12:46,560 --> 00:12:49,080
She's done some amazing work.

204
00:12:49,080 --> 00:12:53,560
But the basic idea is that you want to look at the words that are very special,

205
00:12:53,560 --> 00:12:55,880
so that don't occur in lots of different documents.

206
00:12:55,880 --> 00:12:59,440
And so the overlap between the word the doesn't really matter, right?

207
00:12:59,440 --> 00:13:01,520
Like the occurs everywhere.

208
00:13:01,520 --> 00:13:04,040
So you want to have sort of the special words.

209
00:13:04,040 --> 00:13:06,440
So that's what TFIDF does in a nutshell.

210
00:13:06,440 --> 00:13:10,040
It gives you a score for document query overlap.

211
00:13:10,040 --> 00:13:13,760
And then you can do all kinds of things here with how you weigh it.

212
00:13:13,760 --> 00:13:17,680
So there's all these weird different parameters like this B and things like that

213
00:13:17,680 --> 00:13:22,360
that allow you to make it better than just having the TFIDF score.

214
00:13:22,360 --> 00:13:24,520
So there's a couple of tweaks you can do there.

215
00:13:24,520 --> 00:13:29,400
So BM25, actually, in case you're wondering, stands for Best Match 25.

216
00:13:29,400 --> 00:13:34,000
So I tried to discover where does the 25 actually come from?

217
00:13:34,000 --> 00:13:39,160
That's because the preceding 24 experiments failed.

218
00:13:39,160 --> 00:13:41,520
So it's literally the 25th one that seemed to work.

219
00:13:41,520 --> 00:13:44,080
And that's why it's called BM25.

220
00:13:44,080 --> 00:13:44,840
Bizarre.

221
00:13:44,840 --> 00:13:48,760
But so this is sparse retrieval.

222
00:13:48,760 --> 00:13:50,000
It's just counting words.

223
00:13:50,000 --> 00:13:53,880
So you have this massive, massive vector of all these word occurrences.

224
00:13:53,880 --> 00:13:56,400
It's sparse because most words never occur.

225
00:13:56,400 --> 00:14:02,200
So it's sort of like a vector of vocabulary size dimensions.

226
00:14:02,200 --> 00:14:04,880
So most of that is obviously zero.

227
00:14:04,880 --> 00:14:07,120
But so that's actually kind of a nice property.

228
00:14:07,120 --> 00:14:12,440
If you want to do fast search on a CPU, because on a CPU sparse product,

229
00:14:12,440 --> 00:14:14,440
it's very easy to compute.

230
00:14:14,440 --> 00:14:21,160
So this is used in the system called Dr. QA, which is really one of the first

231
00:14:21,160 --> 00:14:27,320
neural instances of this open domain, sort of open book question answer in paradigm.

232
00:14:27,320 --> 00:14:31,760
So you have a question like how many of our cells in habitants, blah, blah.

233
00:14:31,760 --> 00:14:35,280
So you want to ask basically Wikipedia what the answer is for this.

234
00:14:35,280 --> 00:14:38,960
So then you have this document retriever based on the sparse.

235
00:14:38,960 --> 00:14:42,760
So BM25, I think in this case, retrieval methods.

236
00:14:42,760 --> 00:14:45,480
You pass that to this.

237
00:14:45,480 --> 00:14:50,640
I think this was still by LSTM at the time, a document reader model.

238
00:14:50,640 --> 00:14:54,280
And then that model gives you the answer.

239
00:14:54,280 --> 00:14:58,080
So this, I think, is really the first instance of having sort of this separation

240
00:14:58,080 --> 00:15:03,000
between a retrieval and a generator system that you use for answering complicated

241
00:15:03,000 --> 00:15:07,200
questions based on sort of open domain knowledge.

242
00:15:07,200 --> 00:15:12,800
So after the sparse stuff, there was a bunch of work on dense retrieval.

243
00:15:12,800 --> 00:15:17,080
And so the advantage of dense retrieval, so this is just like word embeddings,

244
00:15:17,080 --> 00:15:20,240
basically vectors, like they're dense now, no longer sparse.

245
00:15:20,240 --> 00:15:24,080
So they're much smaller in terms of dimensionality.

246
00:15:24,080 --> 00:15:28,960
And a nice advantage of dense retrieval is that it's not really about specific words.

247
00:15:28,960 --> 00:15:35,040
So if they're synonyms, you can still find the relevant document,

248
00:15:35,040 --> 00:15:37,720
which you couldn't really do with a sparse representation.

249
00:15:37,720 --> 00:15:43,400
So that's really the advantage of dense is that you get like semantic similarity.

250
00:15:43,400 --> 00:15:46,040
So you can do this over word embeddings.

251
00:15:46,040 --> 00:15:47,440
That doesn't really work all that well.

252
00:15:47,440 --> 00:15:50,240
But at the time that people started thinking about this,

253
00:15:50,240 --> 00:15:51,400
Bert was already out there.

254
00:15:51,400 --> 00:15:54,360
And Bert is really great for giving you a vector representation

255
00:15:54,360 --> 00:15:56,320
for an entire sequence of words.

256
00:15:56,320 --> 00:15:59,560
So it sent this representation or a passage representation.

257
00:15:59,560 --> 00:16:05,200
So there are all these cool systems like ORCA and DPR, the dense passage retriever,

258
00:16:05,200 --> 00:16:11,800
where they essentially use the retrieval as a kind of latent variable in the system.

259
00:16:11,800 --> 00:16:16,440
And the way to get the latent variable to work, to be good enough essentially

260
00:16:16,440 --> 00:16:21,640
to train the entire system, is to pre-train the retriever on relevant information.

261
00:16:21,640 --> 00:16:25,640
So for ORCA, they do something called inverse close.

262
00:16:25,640 --> 00:16:30,280
So they do kind of a close task where you want to find passages

263
00:16:30,280 --> 00:16:33,480
that are sort of relevant to the preceding passage.

264
00:16:33,480 --> 00:16:36,680
And in DPR, they just train it on the supervised thing.

265
00:16:36,680 --> 00:16:40,840
But really, the core idea here is that, as you can see in this graph here,

266
00:16:40,840 --> 00:16:44,560
you can do better than BM25 if you add lots of documents.

267
00:16:44,560 --> 00:16:47,080
And the way you compute the score function is much simpler.

268
00:16:47,080 --> 00:16:51,000
It's just a dot product, right?

269
00:16:51,000 --> 00:16:53,960
So the nice thing about dot products

270
00:16:53,960 --> 00:16:58,760
is that you can do them very, very efficiently on the GPU as well

271
00:16:58,760 --> 00:17:00,560
if you know what you're doing.

272
00:17:00,560 --> 00:17:05,200
So what you really want to get at is Maximum Inner Product Search MIPS.

273
00:17:05,200 --> 00:17:09,160
So this is one of the kind of core ideas of a lot of this stuff.

274
00:17:09,160 --> 00:17:14,560
And you can do MIPS with ANN, Approximate Neighbor Search.

275
00:17:14,560 --> 00:17:18,960
And so there's this really brilliant piece of work out of there.

276
00:17:18,960 --> 00:17:22,200
For my colleagues at the time, I called FACE,

277
00:17:22,200 --> 00:17:26,160
which really underlies all of these modern vector databases, right?

278
00:17:26,160 --> 00:17:30,440
So all the popular ones are sort of re-implementations of this FACE idea.

279
00:17:30,440 --> 00:17:33,680
One is in Rust, one is in Go, but it's all basically the same idea.

280
00:17:33,680 --> 00:17:36,200
It's just FACE.

281
00:17:36,200 --> 00:17:39,800
And so FACE really powers a lot of this stuff.

282
00:17:39,800 --> 00:17:42,720
And whenever somebody tells you something about a vector database,

283
00:17:42,720 --> 00:17:48,200
just think about FACE, very fast dot product.

284
00:17:48,200 --> 00:17:51,080
So obviously, you can go beyond dot product.

285
00:17:51,080 --> 00:17:51,800
Yes.

286
00:17:51,800 --> 00:17:53,600
What is FACE?

287
00:17:53,600 --> 00:17:55,240
What is FACE?

288
00:17:55,240 --> 00:17:59,000
So it's an open source library, Facebook AI similarity search.

289
00:17:59,000 --> 00:18:00,280
Yeah, it's something.

290
00:18:00,280 --> 00:18:03,080
Yes.

291
00:18:03,080 --> 00:18:06,440
No, so it's just basic off the shelf, ANN algorithms.

292
00:18:10,720 --> 00:18:13,160
Yeah, so there are all kinds of different,

293
00:18:13,160 --> 00:18:16,960
I don't know if you would like product quantization and things like that.

294
00:18:16,960 --> 00:18:20,400
So you have a bunch of vectors.

295
00:18:20,400 --> 00:18:24,920
And you can just compute the full dot product, which is sort of inefficient, right?

296
00:18:24,920 --> 00:18:29,400
So what you can do is try to compress subspaces of the vector

297
00:18:29,400 --> 00:18:33,120
and then just look at the kind of centroids.

298
00:18:33,120 --> 00:18:36,560
So you can quantize sub-vectors of the full vector

299
00:18:36,560 --> 00:18:42,280
and then do much faster search over just the centroids.

300
00:18:42,280 --> 00:18:43,000
It's a good question.

301
00:18:43,000 --> 00:18:43,800
Any other questions?

302
00:18:47,560 --> 00:18:48,480
All right.

303
00:18:48,480 --> 00:18:50,640
So about this dot product idea, right?

304
00:18:50,640 --> 00:18:55,920
So what we have here is some people call this a Siamese network, I guess it is, right?

305
00:18:55,920 --> 00:19:00,720
So you have two different BERT models or whatever your encoder is here.

306
00:19:00,720 --> 00:19:02,480
And then at the end, you get these two vectors

307
00:19:02,480 --> 00:19:05,760
and then you just do dot products where you get one single score.

308
00:19:05,760 --> 00:19:08,080
But you can do all kinds of much fancier things

309
00:19:08,080 --> 00:19:13,360
if you're willing to give up on this buy encoder approach.

310
00:19:13,360 --> 00:19:18,440
So a really nice example from one of your colleagues here at Stanford is Colbert.

311
00:19:19,400 --> 00:19:22,880
So what this does is late interaction.

312
00:19:22,880 --> 00:19:25,640
So instead of just having this dot product here,

313
00:19:25,640 --> 00:19:30,560
you have a kind of more complicated version of computing score

314
00:19:30,560 --> 00:19:35,080
where you aggregate over sort of maximum similarity scores between different words.

315
00:19:35,080 --> 00:19:37,920
So I only recently actually discovered that this is called Colbert

316
00:19:37,920 --> 00:19:40,560
because of the late night show Colbert.

317
00:19:40,560 --> 00:19:46,520
So it's sort of Omar's joke actually, this name, but just so you know, if you run into it.

318
00:19:48,920 --> 00:19:55,240
So, but I think if we look at kind of where the state of the art has been going now,

319
00:19:55,240 --> 00:19:58,920
one of the nice things about inspector databases is that they're super efficient, right?

320
00:19:58,920 --> 00:20:02,120
So dot product is much more efficient than this late interaction stuff,

321
00:20:02,120 --> 00:20:04,760
especially if you do the approximate nearest neighbor search.

322
00:20:05,880 --> 00:20:07,880
But there's been some really cool work.

323
00:20:07,880 --> 00:20:14,440
So things like SPLATE, they basically have sparse meat dents in a way.

324
00:20:14,440 --> 00:20:15,960
So one of the big problems, as I said,

325
00:20:16,040 --> 00:20:19,320
with sparse is that you can't really handle synonyms and things like that.

326
00:20:19,320 --> 00:20:23,320
But what you could do is take a dense model, like a bird model,

327
00:20:23,320 --> 00:20:26,600
look at kind of this one word in your sequence,

328
00:20:26,600 --> 00:20:29,400
try to see which other words fit in the same slot.

329
00:20:29,400 --> 00:20:30,840
So that gives you the synonyms.

330
00:20:31,640 --> 00:20:35,880
So now you can give all these synonyms to a sparse vector,

331
00:20:35,880 --> 00:20:37,960
and then you can just do sparse dot product.

332
00:20:37,960 --> 00:20:40,760
And so have a much, much more efficient way to do search

333
00:20:41,720 --> 00:20:47,080
without sort of giving up on all the cool stuff that you get from a dense representation.

334
00:20:47,960 --> 00:20:49,080
So that's one thing.

335
00:20:49,080 --> 00:20:52,360
And this other idea I really like is called dragon.

336
00:20:53,640 --> 00:20:57,720
So this side, I think, is really the best generalized dense retriever.

337
00:20:57,720 --> 00:20:59,640
So if you want to take something off the shelf right now

338
00:20:59,640 --> 00:21:01,560
and just go to hugging face or something,

339
00:21:01,560 --> 00:21:06,440
then this dragon or dragon plus is probably the thing you want to use for a dense retriever.

340
00:21:06,440 --> 00:21:11,400
And the way they train this is through this progressive data augmentation strategy

341
00:21:11,400 --> 00:21:13,480
to make the model better and better over time

342
00:21:13,480 --> 00:21:15,320
by sampling very difficult negatives.

343
00:21:16,360 --> 00:21:19,480
And that gives you very good representations.

344
00:21:20,840 --> 00:21:22,520
And so the other thing about this,

345
00:21:22,520 --> 00:21:27,080
I think this is the only sort of final point about retrieval in general,

346
00:21:27,080 --> 00:21:29,400
is that what we see happening right now,

347
00:21:29,400 --> 00:21:32,040
if you look at sort of the developer community around drag,

348
00:21:32,040 --> 00:21:34,760
is that they're all doing hybrid search right now.

349
00:21:34,840 --> 00:21:38,600
So you can actually just combine the search results from your sparse,

350
00:21:38,600 --> 00:21:41,160
be in 25 or whatever thing or displayed,

351
00:21:41,160 --> 00:21:43,000
and you can combine them with your dragon.

352
00:21:43,960 --> 00:21:47,080
And then you'll get this ranking that works even better.

353
00:21:47,080 --> 00:21:48,760
So then you kind of get best of both worlds,

354
00:21:48,760 --> 00:21:51,800
but then you get all these questions about how do you combine the results.

355
00:21:53,960 --> 00:21:55,560
Any questions on this part?

356
00:21:57,160 --> 00:21:58,120
Oh, can you hear me?

357
00:21:58,920 --> 00:21:59,160
Yes.

358
00:21:59,880 --> 00:22:00,360
Oh, sorry.

359
00:22:00,920 --> 00:22:05,480
On the earlier slide, has there been any work on benchmark,

360
00:22:05,480 --> 00:22:10,920
how much less hallucination rag incurs over closed book question answering,

361
00:22:10,920 --> 00:22:14,280
for example, directly asking the large language model the question,

362
00:22:14,280 --> 00:22:16,360
has there been any benchmarking studies in this?

363
00:22:17,400 --> 00:22:20,680
Yeah, so there's a great paper, if I can say so myself,

364
00:22:20,680 --> 00:22:23,640
on the fact that retrieval augmentation reduces hallucination.

365
00:22:24,360 --> 00:22:25,880
It's from 2021, I think.

366
00:22:26,840 --> 00:22:30,040
So yeah, you can just find, if you literally look for

367
00:22:30,040 --> 00:22:33,080
retrieval augmentation reduces hallucination, then you'll find the paper.

368
00:22:33,960 --> 00:22:34,520
Oh, thank you.

369
00:22:37,880 --> 00:22:42,040
Well, let's see, is there a picture of your dance approach,

370
00:22:42,040 --> 00:22:43,640
and why do you need swaps?

371
00:22:44,200 --> 00:22:51,320
Yeah, so very often you want to have a very precise word overlap

372
00:22:51,320 --> 00:22:53,720
for things where you don't want to have the synonyms

373
00:22:53,720 --> 00:22:55,240
or the kind of nearest neighbors.

374
00:22:55,240 --> 00:22:59,480
So if there's like a brand name or something like that,

375
00:23:00,040 --> 00:23:02,520
then like let's say your brand is Apple,

376
00:23:02,520 --> 00:23:04,360
you don't want to find stuff about Paris.

377
00:23:05,080 --> 00:23:07,320
So that's what you would do with a dance retriever.

378
00:23:08,360 --> 00:23:12,040
So it really kind of depends on what you want to use it for.

379
00:23:12,040 --> 00:23:13,880
That's why hybrid is probably the way to go.

380
00:23:15,720 --> 00:23:16,440
It's a good question.

381
00:23:17,400 --> 00:23:24,040
But with a dance, it's contextualized in many ways.

382
00:23:24,040 --> 00:23:28,040
Should it realize Apple, the company, would be different from that?

383
00:23:28,040 --> 00:23:28,600
No.

384
00:23:28,600 --> 00:23:31,400
So if they were actually contextualized, then yes,

385
00:23:31,400 --> 00:23:34,200
but very often it's a frozen retrieval system.

386
00:23:35,000 --> 00:23:37,640
That's one of the problems with all the frozen rag stuff.

387
00:23:42,040 --> 00:23:43,640
I might be missing something very, very soon.

388
00:23:43,640 --> 00:24:01,160
So the sort of document and the query, they're the same, right?

389
00:24:01,160 --> 00:24:03,320
So they're either sparse or they're dense.

390
00:24:03,320 --> 00:24:04,840
But so if they're sparse,

391
00:24:04,840 --> 00:24:07,800
the components of the vector are literally the other words.

392
00:24:07,800 --> 00:24:15,880
And you just finalized when you're thinking about the thing that creates the names?

393
00:24:17,800 --> 00:24:19,640
How are you getting expressed here?

394
00:24:19,640 --> 00:24:21,160
So it literally counts, right?

395
00:24:22,040 --> 00:24:26,680
So basically it's one big matrix of documents as rows

396
00:24:26,680 --> 00:24:29,240
and the columns are the words in the documents.

397
00:24:29,240 --> 00:24:32,280
And then you just count how often a word occurs in a document.

398
00:24:33,000 --> 00:24:34,200
So that's a sparse.

399
00:24:40,520 --> 00:24:41,000
Yeah.

400
00:24:41,000 --> 00:24:46,440
And so in the field, we call them sparse embeddings or sparse retrieval

401
00:24:46,440 --> 00:24:48,280
because most of that vector is zero.

402
00:24:49,560 --> 00:24:51,880
Because most words don't occur in that document.

403
00:24:54,040 --> 00:24:55,000
Does that make sense?

404
00:24:58,600 --> 00:24:58,840
Cool.

405
00:24:59,800 --> 00:25:04,920
So let's talk about doing slightly better.

406
00:25:04,920 --> 00:25:08,840
So going back to Steven's question about, okay, we have this kind of retrieval thing,

407
00:25:08,840 --> 00:25:14,360
but how do we actually make this retriever good for the context that is going to be used in?

408
00:25:14,360 --> 00:25:17,560
And so can we contextualize the retriever for the generator?

409
00:25:18,280 --> 00:25:22,120
Even if it's a generator where we might not have access to the weights.

410
00:25:22,120 --> 00:25:26,520
So it could be a GP4 model, we just send it to some API, we get some stuff back.

411
00:25:27,480 --> 00:25:31,240
And so one paper I would like is called Replug.

412
00:25:32,360 --> 00:25:34,920
So just to kind of explain what this looks like.

413
00:25:34,920 --> 00:25:39,720
So you have this context, you have a retriever that we do the standard retrieval step with.

414
00:25:39,720 --> 00:25:40,920
This is a dense retriever.

415
00:25:41,960 --> 00:25:46,840
And now, sorry, and now you compute the likelihood.

416
00:25:46,840 --> 00:25:51,320
So basically just normalize the scores that you get for the top K documents

417
00:25:51,320 --> 00:25:53,000
to get a distribution here.

418
00:25:53,000 --> 00:25:59,080
And then you'll give each one of the retrieved documents separately to this generator,

419
00:25:59,080 --> 00:26:00,600
to your language model.

420
00:26:00,600 --> 00:26:05,320
So you can look at the perplexity of the correct answer for that language model.

421
00:26:06,040 --> 00:26:10,280
So now we have these two probability distributions, or two likelihoods essentially,

422
00:26:10,280 --> 00:26:13,880
and we can minimize the KL divergence to make sure that we can actually

423
00:26:14,520 --> 00:26:20,200
retrieve the documents that lead to the lowest perplexity on the right answer for the language model.

424
00:26:21,400 --> 00:26:24,920
So super simple idea, works really, really well.

425
00:26:26,360 --> 00:26:30,920
And the nice thing about this is completely agnostic of what happens upstream.

426
00:26:30,920 --> 00:26:34,600
So this will work for any sort of encoder decoder for any language model.

427
00:26:35,720 --> 00:26:41,320
What you need is a perplexity score, but for most language models you can get that,

428
00:26:41,320 --> 00:26:42,520
not necessarily all of them.

429
00:26:43,080 --> 00:26:43,880
So that's one thing.

430
00:26:43,880 --> 00:26:46,280
And then there's this other really nice approach.

431
00:26:47,000 --> 00:26:57,400
So in the retriever, you're literally updating the dense representations,

432
00:26:58,040 --> 00:26:58,280
right?

433
00:26:58,280 --> 00:27:00,840
So your encoder basically for your dense representation.

434
00:27:00,840 --> 00:27:02,920
That's a good question, we'll get into that a little bit more.

435
00:27:04,600 --> 00:27:09,800
So there's another paper on in-context retrieval augmented language models,

436
00:27:09,800 --> 00:27:15,400
where the whole paper is basically about just doing BM25 and just giving stuff directly

437
00:27:15,400 --> 00:27:17,960
through the context of the language model and things kind of work.

438
00:27:17,960 --> 00:27:24,280
So it's sort of frozen rag, but even more primitive in a way where the retriever is

439
00:27:24,920 --> 00:27:28,120
this very old sparse algorithm, but it works really, really well.

440
00:27:28,920 --> 00:27:33,160
But then they have this really awesome section where they show that you can just

441
00:27:33,160 --> 00:27:40,200
have this reranker on top of the BM25 results, and you can backdrop into this reranker.

442
00:27:40,200 --> 00:27:43,160
So now you still keep the language model completely fixed.

443
00:27:43,160 --> 00:27:46,680
So that's sort of this part of the loss here.

444
00:27:46,680 --> 00:27:49,320
So you have kind of a stop gradient on the parameters data.

445
00:27:49,320 --> 00:27:50,680
That's just your language model.

446
00:27:51,240 --> 00:27:56,920
But now you have this kind of rank function here that you can backdrop into, right?

447
00:27:56,920 --> 00:27:58,200
So that's your reranker.

448
00:27:58,200 --> 00:28:01,720
It's basically it can be a burden model or anything like that that works on top of the things

449
00:28:01,720 --> 00:28:04,040
you initially retrieved from your BM25.

450
00:28:04,040 --> 00:28:07,240
And now you have this birth reranker that you can backdrop into.

451
00:28:08,760 --> 00:28:11,320
So this also works really, really nice.

452
00:28:11,320 --> 00:28:16,760
So we're slowly progressing towards having a system that is much more optimized for

453
00:28:16,760 --> 00:28:21,960
being properly retrieval-augmented in a way where it's useful and contextualized for what

454
00:28:21,960 --> 00:28:22,920
you want to use it for.

455
00:28:24,840 --> 00:28:28,280
So yeah, just to point out kind of what that looks like with this reranker.

456
00:28:28,280 --> 00:28:30,840
So you just have this extra step essentially, right?

457
00:28:30,840 --> 00:28:34,920
So we have our retriever, then we have our reranker, then we have our generator and our output.

458
00:28:36,600 --> 00:28:38,680
Any grades to the language model?

459
00:28:39,800 --> 00:28:41,000
No, not necessarily.

460
00:28:42,280 --> 00:28:45,400
So for this one, you do, yeah.

461
00:28:45,400 --> 00:28:47,160
But so for a redeplug, you don't, right?

462
00:28:48,680 --> 00:28:49,560
Yeah, for this one.

463
00:28:49,560 --> 00:28:51,960
Yeah, yeah, yeah.

464
00:28:51,960 --> 00:28:53,880
So basically, yeah, you need to get-

465
00:28:53,880 --> 00:28:55,560
Do you guys provide them?

466
00:28:55,560 --> 00:28:56,200
Not all of them.

467
00:28:57,160 --> 00:29:01,080
Some of them do, but yeah, there are all kinds of tricks you can do on top of that.

468
00:29:03,960 --> 00:29:09,240
So basically, the question is how do we get sort of gradients flowing into this, right?

469
00:29:09,240 --> 00:29:14,040
So if you don't actually have access to the full parameters of the model so that you can backprop

470
00:29:14,040 --> 00:29:20,120
all the way through it, then you can do a reinforce style loss on the retrieval,

471
00:29:20,120 --> 00:29:24,360
and then you just pass the kind of log likelihood if you have access to that,

472
00:29:24,360 --> 00:29:26,040
or some other kind of black box function.

473
00:29:31,480 --> 00:29:39,080
All right, so the next thing you can do is to optimize both the retriever and the generator.

474
00:29:40,200 --> 00:29:46,600
And so this really starts getting to the proper kind of contextualization of the entire architecture

475
00:29:46,600 --> 00:29:48,840
where you want everything to work together, right?

476
00:29:48,840 --> 00:29:53,240
So rather than having this frozen thing where everything is basically not aware that the other

477
00:29:53,240 --> 00:29:54,040
part exists, right?

478
00:29:54,040 --> 00:29:55,240
It's like two halves of the brain.

479
00:29:55,240 --> 00:29:56,760
They're not talking to each other.

480
00:29:56,760 --> 00:29:58,920
One is your retriever, the other is your language model.

481
00:29:58,920 --> 00:29:59,720
There's no connection.

482
00:29:59,720 --> 00:30:03,400
They're just like sort of like something is thrown over the fence and then you hope for the best.

483
00:30:03,960 --> 00:30:07,160
So instead of that, we have everything much closer and learning together.

484
00:30:08,120 --> 00:30:16,280
So one of the first ways of doing this with the generator was ragged retrieval augmented

485
00:30:16,280 --> 00:30:19,160
generation, which we did at fair in 2020.

486
00:30:20,440 --> 00:30:23,720
And it's very similar to what we've already seen.

487
00:30:23,720 --> 00:30:27,320
We basically have this retriever here that works over different documents.

488
00:30:27,320 --> 00:30:33,560
You get some score function that gets given to this generator that generates the answer.

489
00:30:33,560 --> 00:30:38,040
And now you want to backdrop all the way and update your generator as well, right?

490
00:30:38,040 --> 00:30:41,880
So in the previous two architectures, we saw you keep the generator fixed,

491
00:30:41,880 --> 00:30:46,040
you backdrop into your retriever, but here we update everything.

492
00:30:46,600 --> 00:30:52,280
Well, not exactly everything as you'll see, but we'll also update the part of the retriever

493
00:30:52,280 --> 00:30:53,080
and the generator.

494
00:30:54,440 --> 00:30:59,080
So in this ragged model, we actually have two different ways of doing this.

495
00:31:00,040 --> 00:31:02,360
It's probably something that when we talk about this,

496
00:31:03,480 --> 00:31:07,880
if you think about this long enough, then you'll think like, okay, but when actually do I need to

497
00:31:07,880 --> 00:31:14,120
retrieve? Do I retrieve every time I generate a new token or do I just retrieve once and then

498
00:31:14,120 --> 00:31:20,600
generate an entire sequence, right? Or maybe I want to retrieve every end tokens, right?

499
00:31:20,600 --> 00:31:23,400
So these are hypergrams, or maybe I want to learn when to retrieve,

500
00:31:23,400 --> 00:31:25,720
as we'll see that's also something people have done.

501
00:31:26,360 --> 00:31:28,840
So these are two different ways to do it.

502
00:31:30,040 --> 00:31:35,000
And what we do in this paper, basically the whole point of the paper is that this frozen thing

503
00:31:35,000 --> 00:31:37,320
doesn't really work all that well, right?

504
00:31:37,320 --> 00:31:42,920
So I think what people call rag now is usually referred to the frozen thing,

505
00:31:43,480 --> 00:31:47,320
but the whole paper basically would never have been accepted anywhere if we had just done the

506
00:31:47,320 --> 00:31:52,280
frozen thing, right? The whole point of the paper is that you want to optimize it.

507
00:31:52,280 --> 00:31:56,920
And so at my company contextual, we call this frozen thing Frankenstein's monster,

508
00:31:56,920 --> 00:32:00,280
because it's really like you cobble together these different pieces, right?

509
00:32:00,280 --> 00:32:04,600
You sort of, yeah, it's really like Frankenstein, you just put it together and then it sort of

510
00:32:04,600 --> 00:32:08,600
walks, you know, but it doesn't really have to solve, it doesn't really actually work,

511
00:32:08,600 --> 00:32:13,640
because not the real thing. So that's great for everyone here, I think,

512
00:32:13,640 --> 00:32:17,960
because there are so many opportunities to do better than what most people are using right now.

513
00:32:17,960 --> 00:32:26,680
So one of the limitations of the original rag architecture is that it only supports a very

514
00:32:26,680 --> 00:32:32,360
small cave. So if you have lots and lots of documents, then the problem is that you have

515
00:32:32,360 --> 00:32:38,040
to fit all of them in the context, but how do you really get that to fit, right?

516
00:32:38,040 --> 00:32:45,080
So one thing you can do is you first encode things so that you get one single representation,

517
00:32:45,560 --> 00:32:49,720
or only diffuse for the top level representations, then you concatenate those,

518
00:32:49,720 --> 00:32:55,880
and then you just feed them to the decoder. So this is FID fusion and decoder. And as you can see,

519
00:32:55,880 --> 00:33:03,960
this scales to a much higher number of passages. And that leads to corresponding improvements in

520
00:33:03,960 --> 00:33:10,520
the scores that you care about. So that's a really cool idea. And so we're slowly moving

521
00:33:10,520 --> 00:33:15,240
towards more decoder-only architectures. So in rag, we have this barred model,

522
00:33:15,240 --> 00:33:19,400
it's sort of an encoder-decoder architecture, but here you just have this decoder that

523
00:33:19,400 --> 00:33:28,520
does some fancy attention over stuff that you retrieved before. And so another pure decoder

524
00:33:28,520 --> 00:33:36,600
language model architecture is this one, KNNLM, which I think is very elegant in its simplicity.

525
00:33:36,600 --> 00:33:42,040
So it's basically, you just have a normal language model, but you interpolate the normal

526
00:33:42,040 --> 00:33:48,360
language model weights with things that you retrieved. So basically, you have some sort

527
00:33:48,360 --> 00:33:54,040
of prompt, right? So like Obama's birthplace is, you go to your big corpus, you find similar things,

528
00:33:54,760 --> 00:34:00,360
you look at the words that come next to the similar things, you rank that thing,

529
00:34:00,360 --> 00:34:06,200
you sample your top K, you renormalize that. So now you have a bunch of scores, and now you

530
00:34:06,200 --> 00:34:11,800
can just interpolate between your retrieved kind of non-parametric memory scores and your

531
00:34:11,800 --> 00:34:16,600
parametric language model scores. So this is very late fusion in a sense, right? At the very

532
00:34:16,600 --> 00:34:22,040
end, you combine these two, and it allows you to reweight the pure language model probabilities

533
00:34:22,040 --> 00:34:27,960
or like this. So this works really well, and it scales especially well if you have a huge

534
00:34:28,920 --> 00:34:33,560
retrieval corpus. And so if you have trillions and trillions of tokens in there, you can have

535
00:34:33,560 --> 00:34:39,160
a much smaller language model that does not that much heavy lifting because you can really rely on

536
00:34:39,160 --> 00:34:45,880
this big source corpus that you're working from. And so that idea was exploited by this paper called

537
00:34:45,880 --> 00:34:52,680
Retro out of DeepMind, where they showed that you can have a 25 times smaller retrieval augmented

538
00:34:52,680 --> 00:34:58,360
language model trained from scratch, so really pre-trained entirely from scratch, that outperforms

539
00:34:58,440 --> 00:35:03,880
this 25 times bigger language model on the same data in terms of complexity, which is pretty

540
00:35:03,880 --> 00:35:09,800
impressive. So this architecture is much more efficient than a parametric model because you

541
00:35:09,800 --> 00:35:15,640
can rely on this external memory. So if your external memory is big enough, you can get pretty

542
00:35:15,640 --> 00:35:22,120
huge gains. So there was a lot of excitement about Retro when it was announced, but this is a DeepMind

543
00:35:22,120 --> 00:35:27,320
paper, so there's really no open source, nothing really to validate that this actually works.

544
00:35:28,600 --> 00:35:33,720
And so very recently, there has been a bit of work from NVIDIA called Retro++,

545
00:35:35,320 --> 00:35:40,760
where they have this hybrid between the Retro architecture and then they do basically RAG,

546
00:35:40,760 --> 00:35:46,440
sort of they put the top one or the top K results in the context of the language model after all.

547
00:35:46,440 --> 00:35:51,880
So it's sort of a crossover between RAG and Retro, and they showed some really nice results here,

548
00:35:51,880 --> 00:35:57,880
but I think it's sort of pointing to this big flaw, I think, is that why is there still no

549
00:35:57,880 --> 00:36:03,880
good open source Retro model? It probably tells you something about whether it actually really

550
00:36:03,880 --> 00:36:09,800
works. I spent a lot of time in my career trying to reproduce DeepMind papers that didn't necessarily

551
00:36:09,800 --> 00:36:17,640
always work. And so I think the same is true for Retro, and that's why we need to do this

552
00:36:17,640 --> 00:36:20,680
in-context RAG on top of Retro to actually get it to work.

553
00:36:32,680 --> 00:36:37,880
So doing retrieval over that big corpus is not that difficult, actually.

554
00:36:39,480 --> 00:36:44,680
So there are even distributed face packages, you can just do everything yourself.

555
00:36:45,640 --> 00:36:51,480
Yeah, so in terms of compute, it's actually not that hard anymore to reproduce something like this,

556
00:36:52,280 --> 00:36:56,280
but I've tried several times and it's not really reproducible.

557
00:36:57,240 --> 00:37:01,400
So the only way to get it to work is if you do this in-context RAG on top of the Retro thing,

558
00:37:01,400 --> 00:37:06,040
and then as you can see here in the results, then it actually gives you a gain over the pure GPT

559
00:37:06,040 --> 00:37:10,920
model. So it starts from a GPT and then they kind of retrofit as they call it the GPT model.

560
00:37:11,880 --> 00:37:16,600
So in short, I think there's still a lot of work to be done in pre-training these systems,

561
00:37:16,600 --> 00:37:21,640
really from scratch. And Retro kind of showed that it might be possible, but we don't necessarily

562
00:37:21,640 --> 00:37:26,440
know exactly how to do it the right way. And this is really one of the interesting open questions.

563
00:37:28,360 --> 00:37:29,560
Any questions on that?

564
00:37:33,880 --> 00:37:34,360
Online?

565
00:37:38,920 --> 00:37:39,400
No? Okay.

566
00:37:41,160 --> 00:37:51,880
Then we'll move on. So let's go all the way with the contextualization now. So with Retro and with

567
00:37:51,880 --> 00:37:59,080
RAG, what we actually did is we only updated the query encoder. So updating the document

568
00:37:59,080 --> 00:38:06,280
encoder is very expensive. So one of the first papers, actually kind of the OG of the non-frozen

569
00:38:06,280 --> 00:38:11,880
dense retrieval augmented methods is this paper called Realm. This is really like visionary

570
00:38:11,880 --> 00:38:19,240
work. This was basically the first kind of version that did this properly, where they updated it

571
00:38:19,240 --> 00:38:25,480
all the way, including the document encoder. So can someone explain to me why it's expensive to

572
00:38:25,480 --> 00:38:33,960
update the document encoder? So let's say we have a trillion tokens in our corpus.

573
00:38:34,920 --> 00:38:41,000
So now we go all the way. So we basically do a forward pass. We get a gradient at the end.

574
00:38:41,000 --> 00:38:45,160
Now we back propagate the gradient through the retriever. We update the query encoder.

575
00:38:45,160 --> 00:38:50,040
Now we have to update the document encoder. So what do we then need to do after we've updated

576
00:38:50,040 --> 00:38:56,760
the document encoder? We need to re-encode the entire internet. So basically every single gradient

577
00:38:56,760 --> 00:39:02,920
update, we have to re-encode whatever our index is. So if this is like trillions of tokens, it's like

578
00:39:02,920 --> 00:39:07,480
re-encoding the internet after every batch update. So that's not very efficient.

579
00:39:09,240 --> 00:39:14,680
Well, I think it does look like we've got a very general international change. So if you learn

580
00:39:14,680 --> 00:39:20,040
digital or other sort of stuff, like if you basically take your old activations and that sounds

581
00:39:20,040 --> 00:39:24,120
like a long, unpredictable change to your entire business. Yeah.

582
00:39:26,920 --> 00:39:31,960
Yeah, that's one way to do it. So there are a bunch of different ways to update the

583
00:39:31,960 --> 00:39:37,880
document encoder. So what they do in Realm is they basically do it for te batches.

584
00:39:38,520 --> 00:39:42,680
Then they stop, they re-encode the entire internet, and then they train again.

585
00:39:43,400 --> 00:39:48,520
So it's sort of asynchronous updates. They have this very fancy sort of sharding mechanisms

586
00:39:48,520 --> 00:39:54,520
where they take down certain parts of their entire index and then update them kind of on the fly.

587
00:39:55,720 --> 00:39:59,880
So you can do it. It's just very expensive. So one of the things that a lot of people have been

588
00:39:59,880 --> 00:40:06,360
thinking about, not exactly the Laura idea, but similar versions of that are around, like,

589
00:40:06,360 --> 00:40:10,760
can you make it more efficient so that you don't have to do this asynchronously?

590
00:40:12,920 --> 00:40:18,600
So one of the downsides of this Realm architecture is that it's really just a BERT model, but then

591
00:40:18,600 --> 00:40:22,360
you do this retrieval augmentation on a BERT model with other BERT models. So it's not really

592
00:40:22,360 --> 00:40:28,200
generative. It's not really gen AI in the modern paradigm. But if you want to read one paper

593
00:40:29,160 --> 00:40:35,320
on this topic, like, this is a very good one to read. The other one that is really, really good

594
00:40:35,320 --> 00:40:44,520
to read is this paper called Atlas. So Atlas is, so this is out of fare with a bunch of folks,

595
00:40:44,520 --> 00:40:51,080
the folks who did, like, RAG, and the folks who did FID, and really a brilliant set of people.

596
00:40:51,080 --> 00:40:57,160
And this is really a comprehensive analysis of everything that's happening in this architecture.

597
00:40:57,240 --> 00:41:01,160
So the first question they really look at is, how do we train this retriever? So we've seen

598
00:41:01,160 --> 00:41:07,080
a couple of versions of this, but which one actually works better? They haven't really

599
00:41:07,080 --> 00:41:12,200
been compared in a head-to-head setting. So one thing is we have this FID style sort of

600
00:41:12,200 --> 00:41:18,040
attention distillation. So that's really too complicated to go into detail here, but the

601
00:41:18,040 --> 00:41:25,320
others are actually very simple. So one is this loss we've basically seen before. So we've seen

602
00:41:25,320 --> 00:41:30,200
this, I think, with the in-context RAG one. So we have a stop gradient on the language model,

603
00:41:30,200 --> 00:41:36,200
and then we update the retriever. The other one is what we've seen with Replug. So this is basically

604
00:41:36,200 --> 00:41:43,480
exactly the Replug loss. So we have the KL divergence of the documents and sort of the

605
00:41:43,480 --> 00:41:48,920
improvement that you see when you give it that document. The other thing they have is basically

606
00:41:48,920 --> 00:41:54,520
the inverse of that one. So if I take this one document out, how does that affect my

607
00:41:55,560 --> 00:42:02,360
perplexity of the language model? And so this one I think is actually quite elegant because

608
00:42:02,360 --> 00:42:08,280
that really gets to like, how valuable is this one single document for me answering this question

609
00:42:08,280 --> 00:42:17,000
correctly? So they compare all of these different versions, and what you can see is that the kind

610
00:42:17,080 --> 00:42:21,640
of Replug style loss and this leave one out loss, they performed a lot better than all of these

611
00:42:21,640 --> 00:42:26,120
others. So this fixed retriever or no joint pre-training, these are really kind of the

612
00:42:26,120 --> 00:42:32,520
baseline sort of frozen RAG models or closed book. And as you can see, you can do really a lot better

613
00:42:33,160 --> 00:42:38,920
if you optimize things. And so this leave one out thing is probably the best I would say.

614
00:42:40,040 --> 00:42:45,000
So then the other question is how do you actually like train that entire system? Like what data or

615
00:42:45,000 --> 00:42:50,600
what tasks do you train this on? So they also experiment with a bunch of different versions.

616
00:42:50,600 --> 00:42:57,960
So one is doing prefix lm, if you're familiar with that. So they basically take a chunk that

617
00:42:57,960 --> 00:43:03,240
occurs somewhere on the internet, and then they predict the next chunk from that chunk. So it's

618
00:43:03,240 --> 00:43:08,280
really like sentence to sentence. So maybe like skip thought back in the day, but now you have

619
00:43:08,280 --> 00:43:14,680
this retrieval step where you predict the next sentence. Then they just do T5 styles or denoising.

620
00:43:14,680 --> 00:43:20,200
So that's mass language modeling if you're familiar with T5. And then they have this title for section

621
00:43:20,200 --> 00:43:26,120
generation piece. So I think the takeaway from this table is basically that whatever you do here,

622
00:43:26,120 --> 00:43:31,160
so they're using T5 models. So whatever you do here needs to be the same that your language

623
00:43:31,160 --> 00:43:40,440
model expects. So for T5, that's T5 style loss. And then the next sort of final question that

624
00:43:40,440 --> 00:43:45,480
they look into going back to what we talked about, how exactly do we update this retriever?

625
00:43:46,200 --> 00:43:51,560
So do we have to update the document encoder? Or do we maybe have to do some sort of re-ranking?

626
00:43:52,200 --> 00:43:58,360
Or do we maybe just update the query? And quite surprisingly, I think they find that just updating

627
00:43:58,360 --> 00:44:04,360
the query. So like in your original RAD paper is actually already basically good enough in many

628
00:44:04,360 --> 00:44:10,040
cases. So that's nice because it's much more efficient if you don't have to update your documents

629
00:44:10,040 --> 00:44:16,520
all the time. I think the real question here though is like, how good is your document representation

630
00:44:16,520 --> 00:44:21,720
to begin with? So you need to have a very, very high quality embedding model for this to work.

631
00:44:21,720 --> 00:44:26,440
If you don't have that, then this will not work. But if you do have that, then you get a very nice

632
00:44:26,440 --> 00:44:36,280
kind of query site fine tuning thing. So the Atlas paper is about trying to do few shop

633
00:44:37,240 --> 00:44:41,720
sort of language modeling tasks. So it's how many examples are given in the context.

634
00:44:46,680 --> 00:44:52,680
Yeah, so the main takeaway here is that if you compare like the close book equivalent model

635
00:44:52,680 --> 00:45:01,000
to the retrieval augmented model, you see very big improvements. That's really the only takeaway

636
00:45:01,320 --> 00:45:09,480
of this entire section. But I think that that's really saying something in terms of what we should

637
00:45:09,480 --> 00:45:20,360
be thinking about. How much time do I have until? Okay. All right. Other questions?

638
00:45:20,360 --> 00:45:37,080
Yeah, so there can be different. So in Atlas, the Atlas basically tries everything. So they also

639
00:45:37,080 --> 00:45:42,520
tried to see what happens if I train this on Wikipedia, but I swap in like a sort of common

640
00:45:42,520 --> 00:45:49,560
crawl index. And I think so in Atlas, but also in retro domain finding is just the more the better.

641
00:45:50,760 --> 00:45:56,040
So it's really just like the bigger your index, the more likely you are to find the exact

642
00:45:56,040 --> 00:45:59,480
right thing and then make the right prediction.

643
00:46:05,080 --> 00:46:11,320
Any other questions on this? Oh, yeah. Sorry. This is a question about the generator in the,

644
00:46:11,320 --> 00:46:20,760
I guess, the RAG system. So recently I saw a paper on Mistral 7B. So it introduces a lot of these

645
00:46:21,400 --> 00:46:26,680
new architectural changes like the sliding window attention to handle longer sequences at a smaller

646
00:46:26,680 --> 00:46:32,840
cost and the group query attention for faster inference. I'd like to like know your thoughts on

647
00:46:33,400 --> 00:46:39,560
designing a generator specifically for RAG, leveraging, for example, where Mistral 7B

648
00:46:39,560 --> 00:46:44,440
currently is. Because for example, like the sliding window attention, I could see how that

649
00:46:44,440 --> 00:46:51,240
could be adapted to the RAG case. Yeah. So maybe you're agreed on sort of what makes Mistral's

650
00:46:51,240 --> 00:46:55,720
special is a bit different from mine. So I don't think that the sliding attention window thing is

651
00:46:55,720 --> 00:46:59,880
actually that interesting. The reason Mistral works so well is because it's trained on a lot of data.

652
00:47:00,680 --> 00:47:04,520
You can do that more efficiently because you have sliding window attention. So you don't need to

653
00:47:04,520 --> 00:47:11,960
attend to everything. But so to answer your question, I guess you're asking sort of about the

654
00:47:11,960 --> 00:47:17,640
architecture of the generator if you know that there's going to be a retriever. So I think

655
00:47:18,360 --> 00:47:25,560
that's basically what Retro tried to do. So Retro actually, some of the people on the Retro paper

656
00:47:25,560 --> 00:47:32,760
are at Mistral now. So they have this chunk cross attention idea here. So you basically

657
00:47:32,760 --> 00:47:37,800
have a language model, but the way it does attention over the things you retrieve in your

658
00:47:37,800 --> 00:47:46,280
Retro architecture, they kind of get integrated into a model, not using the standard attention

659
00:47:46,280 --> 00:47:53,160
mechanism, but using this slightly different chunk cross attention. Okay. So I think the

660
00:47:53,160 --> 00:47:59,480
sliding window attention point I was trying to get at was that it uses a fixed window so that

661
00:47:59,480 --> 00:48:05,720
whenever you're doing the query key computation with the query vectors and the key vectors,

662
00:48:05,720 --> 00:48:13,480
you're using a fixed window attention. So I think my idea was to actually, one, use a dynamic window

663
00:48:13,480 --> 00:48:20,360
because for example, the rag case, if you use a fixed window when you're doing attention, it is

664
00:48:20,360 --> 00:48:27,240
possible that you actually are leaving, you're only looking at a fixed span of information. So if you

665
00:48:27,240 --> 00:48:33,320
could maybe adapt Mistral so that you could make it better for the rag case in, for example,

666
00:48:33,320 --> 00:48:39,400
the making the fixed window size, the dynamic window. Yeah. Yeah, I think it's an interesting

667
00:48:39,400 --> 00:48:46,840
idea. So for me, what Mistral is doing with the sliding window, that's basically like a confnet.

668
00:48:47,400 --> 00:48:52,280
So we had all these convolutional like light confnets where we would have word embeddings,

669
00:48:52,280 --> 00:48:57,080
and you would do confolutions over it and then pull, and then you would still get the information

670
00:48:57,080 --> 00:49:02,120
out. So it's not that the sliding window prohibits you from looking earlier, it's just that that

671
00:49:02,120 --> 00:49:10,760
happens higher up in your transformers. So I think that definitely is an interesting

672
00:49:10,760 --> 00:49:18,440
direction to think in. Yeah, so I think it's like not too crazy to say, are there any architectural

673
00:49:18,440 --> 00:49:23,320
changes that we can introduce into these seven billion parameter models so that they could be

674
00:49:23,320 --> 00:49:32,200
better adapted to the rag case? Yeah, so there might be. Yeah, I think one question is just how

675
00:49:32,200 --> 00:49:38,760
do you do the attention over things you've retrieved, which is what you're doing. Yeah, thanks.

676
00:49:40,600 --> 00:49:47,560
So just to make sure I understand, I mean in this retro model, you're retrieving each block,

677
00:49:48,520 --> 00:49:53,960
and when you struggle about putting a retrieval in the context, are you saying that you'll need to

678
00:49:53,960 --> 00:50:00,520
do it at the beginning and you don't do it at the block? Yeah, so in context, so it's not exactly

679
00:50:00,520 --> 00:50:09,160
every layer, so it's every token, so every step basically, not every block, so it doesn't make

680
00:50:09,240 --> 00:50:18,360
sense. So it's not every layer that you're doing a retrieval. Yeah, so every step. So this is kind

681
00:50:18,360 --> 00:50:24,600
of like what rag token is, so you retrieve every token, so you generate and then you can retrieve

682
00:50:24,600 --> 00:50:29,640
again, or in the case of retro, you can generate like a chunk and then you retrieve chunks again.

683
00:50:31,080 --> 00:50:35,480
If you look at the in-context case, you retrieve once at the beginning and then you give it.

684
00:50:36,200 --> 00:50:51,800
So here you don't actually give it as context at all, like directly to the model, right,

685
00:50:51,800 --> 00:50:56,040
so here you let the decoder kind of attend over it.

686
00:50:56,040 --> 00:51:06,600
Yeah, so I don't think cross-attention really works, yeah.

687
00:51:11,800 --> 00:51:12,600
Other questions?

688
00:51:13,800 --> 00:51:19,560
Yeah, inside the in-context case, the retrieving of the retriever is not necessarily,

689
00:51:19,720 --> 00:51:26,840
because of the large distribution loss, so I'm wondering inside of the cases, like what cases

690
00:51:27,640 --> 00:51:34,120
are really necessary need to evenize updates, or anyways updates for this argument.

691
00:51:35,160 --> 00:51:40,520
Yeah, so you do want to update the retriever, right, but only part of the retriever is necessary to

692
00:51:40,520 --> 00:51:49,400
be updated for a lot of these cases, but so I think it, so these are very specific data sets,

693
00:51:49,400 --> 00:51:54,920
right, natural questions, Wizard of Wikipedia and Fever, so they're really very kind of knowledge

694
00:51:54,920 --> 00:52:01,640
intensive tasks, so in that case, if you already have a very good system like DPR that is specifically

695
00:52:01,640 --> 00:52:07,640
pre-trained for those tasks, then you only need to update the query encoder, but so I would expect

696
00:52:07,640 --> 00:52:13,320
that if you move beyond this to kind of general language modeling things like retro, then you

697
00:52:13,320 --> 00:52:18,200
probably do want to update the document encoder, at least in a way where you can skate it.

698
00:52:19,400 --> 00:52:27,320
So I believe that in this part, it's very knowledge intensive, and actually a couple of

699
00:52:28,120 --> 00:52:37,080
very important topics, as long as we have a good office around knowledge of what

700
00:52:37,960 --> 00:52:46,600
many of the documents by those good models. Yeah, but so you need to learn how to kind of query

701
00:52:46,600 --> 00:52:53,400
into that index, right, so if you don't do that, then yeah, you don't get really good

702
00:52:53,400 --> 00:52:57,880
performance, so that's sort of like your closed book performance, right, if you just have the

703
00:52:57,880 --> 00:53:02,680
language model and you're just like, what does the parametric model on its own without the

704
00:53:02,680 --> 00:53:07,400
retrieval, what does it actually know? As you can see, there are pretty big gaps there, right.

705
00:53:07,720 --> 00:53:15,320
Other questions? Otherwise, I will cover other questions.

706
00:53:18,840 --> 00:53:25,880
No? Hello? Yeah, go for it. Okay, question, like so what about like more hierarchical retrieval,

707
00:53:25,880 --> 00:53:30,120
like I suppose there'll be methods trying to not just retrieve a single chunk, but there's some

708
00:53:30,120 --> 00:53:35,160
kind of like groups of chunks or something, or some rise versions. There's been some

709
00:53:35,160 --> 00:53:40,040
interesting work on doing that where you first tried to find, so you can have multiple indices

710
00:53:40,040 --> 00:53:43,720
and they can kind of cascade, right, so first you want to find the relevant document,

711
00:53:43,720 --> 00:53:47,480
so you have some document representation and then within that document you want to find the

712
00:53:47,480 --> 00:53:53,000
relevant chunk, so you can do it sort of that direction, you can also do it in reverse, I think

713
00:53:53,000 --> 00:53:57,480
I have something on a slide there where you can find the chunk and then sort of expand

714
00:53:58,360 --> 00:54:04,120
the context around it and then give that to the language model. So I think yeah,

715
00:54:04,120 --> 00:54:06,200
there are all kinds of interesting things you can do there.

716
00:54:08,360 --> 00:54:14,520
Cool, thanks, I guess another thing just like can you compare RAG versus like long context

717
00:54:14,520 --> 00:54:20,200
so efforts, so like there are lots of things like around just having a really long context and the

718
00:54:20,200 --> 00:54:28,120
extreme it could replace RAG, but I don't know like if it takes. Yeah, so everybody understands

719
00:54:28,120 --> 00:54:33,160
this question, right, so there's a trend where we want to have very long context language models,

720
00:54:33,160 --> 00:54:37,960
so that basically you can like take Harry Potter or something, just put it in the context and then

721
00:54:37,960 --> 00:54:42,920
ask a question like what is the name of like Harry Potter's owl or something, right, and then it can

722
00:54:42,920 --> 00:54:49,320
just attend over the entire thing. So attending over all of Harry Potter to answer that one question

723
00:54:49,320 --> 00:54:55,800
is super inefficient, right, so most of Harry Potter has nothing to do with the owl, so but you

724
00:54:55,800 --> 00:55:01,080
are still kind of reading it if you do it with the long context window, so that's why I think

725
00:55:01,080 --> 00:55:06,200
the doing it the RAG way where you have this non-parametric component is a much more efficient

726
00:55:06,200 --> 00:55:10,920
way to solve this problem, and if you actually look at the literature on long context windows,

727
00:55:11,800 --> 00:55:17,800
the way they solve the problem of scaling the attention mechanism is by making it very sparse,

728
00:55:18,440 --> 00:55:22,600
so they're basically turning it in, so that's a different kind of sparse, but they're turning

729
00:55:22,600 --> 00:55:28,200
it into a non-parametric retrieval problem kind of behind the scenes, so they're not actually

730
00:55:28,200 --> 00:55:32,440
all that different, if you want to scale long context then you're going to move towards a RAG-style

731
00:55:32,440 --> 00:55:42,600
architecture. Cool, thanks. All right, so let's talk about some other interesting questions,

732
00:55:43,240 --> 00:55:49,560
so one thing and I already alluded to this is when do we actually retrieve, so very if we're

733
00:55:49,560 --> 00:55:55,720
doing like if we want to like retrieve every token that's also very inefficient because I probably

734
00:55:55,720 --> 00:56:01,480
don't have to retrieve to generate the, right, I can probably do that on my own with the language

735
00:56:01,480 --> 00:56:07,160
model, it's sort of a waste to go and retrieve stuff, but if I only retrieve once at the beginning

736
00:56:07,160 --> 00:56:13,000
of the sequence that's probably also not great, so what we ideally want to be able to do is to say,

737
00:56:13,000 --> 00:56:16,680
okay sometimes I want to retrieve, sometimes I don't want to retrieve and I'm going to learn

738
00:56:16,680 --> 00:56:24,600
when I want to kind of expend the compute budget on doing the retrieval, so a nice paper where they

739
00:56:24,600 --> 00:56:29,480
have a stab at, this is called FLARE for active retrieval augmentation where they basically have

740
00:56:29,480 --> 00:56:34,840
the language model decide when it should do a search and what it should do the search for,

741
00:56:36,520 --> 00:56:42,440
so I think this fits in a general trend that you can see in the field around kind of agents,

742
00:56:42,440 --> 00:56:49,160
so we can talk a little bit more about that too. So this other question that I think we've also

743
00:56:49,160 --> 00:56:53,720
kind of covered already here is how do we train this at scale, so we can do these asynchronous

744
00:56:53,720 --> 00:56:59,560
updates, we can do re-rankers, we can do query side only, there's this really nice paper which is

745
00:56:59,560 --> 00:57:07,400
quite close I think to the idea you proposed where you first use BM25 to create a batch basically

746
00:57:07,400 --> 00:57:14,440
where everything is very similar in terms of what you've retrieved and now you have this kind of

747
00:57:14,440 --> 00:57:19,480
in-batch update, so it's sort of like a re-ranker where you encode the information that is just in

748
00:57:19,480 --> 00:57:24,840
your batch using this other model and now you can update this model on the fly, so you don't have

749
00:57:24,840 --> 00:57:30,600
to worry too much about doing the full kind of document side update and again here what really

750
00:57:30,600 --> 00:57:34,840
matters is like how big is your index, if you have an amazing index you can basically solve

751
00:57:34,840 --> 00:57:40,600
any problem just by looking it up, so rather than cramming it into your parameters you can just find

752
00:57:40,600 --> 00:57:50,120
it. This is a really nice paper called SILO, so one of the interesting things I think that's going

753
00:57:50,120 --> 00:57:55,000
to happen in the next year or two around language models is there and you've seen this already,

754
00:57:55,000 --> 00:57:59,880
there's a bunch of lawsuits against OpenAI and other places around where does the data exactly

755
00:57:59,880 --> 00:58:07,560
come from, so one very elegant solution I think is to have a RAG system that you train on data that

756
00:58:07,560 --> 00:58:14,120
you know is safe, so you can train that thing on Wikipedia but now during test time you can give it

757
00:58:14,120 --> 00:58:20,280
a data store that has maybe slightly riskier information in it, so this massive index of

758
00:58:20,280 --> 00:58:26,760
all the stuff on the internet including some things that are maybe higher risk, you can still

759
00:58:26,760 --> 00:58:31,720
have them in your index but your language model, your retrieval augmented language model I should

760
00:58:31,720 --> 00:58:35,640
say, you know that that thing is safe because it was trained on data that is public domain,

761
00:58:36,360 --> 00:58:40,360
so that's what they do in SILO and they show that that works really well, so that's

762
00:58:41,240 --> 00:58:46,520
one possible solution to a lot of the kind of compliance and legal risk around language model

763
00:58:46,520 --> 00:58:56,920
deployments. There's a great paper also from one of your colleagues around context getting lost in

764
00:58:56,920 --> 00:59:01,160
the middle, I think this is also kind of a fascinating phenomenon, this is on a frozen RAG system

765
00:59:02,120 --> 00:59:09,800
but language models are very similar to humans in what things they pay attention to, so if you

766
00:59:09,800 --> 00:59:14,680
give them a bunch of things that you retrieve, what they will look at are the first things you

767
00:59:14,680 --> 00:59:20,040
list and the last things you list and they will sort of ignore the middle, so if it actually

768
00:59:20,040 --> 00:59:25,240
respected the rank function then this curve would get down all the way, but it sort of goes up,

769
00:59:25,880 --> 00:59:32,040
so I think that's a very interesting observation which kind of shows that how brittle

770
00:59:32,760 --> 00:59:38,120
these these systems can be, so if you have a frozen RAG system it can be very very brittle where

771
00:59:38,120 --> 00:59:42,680
like the order of the retrieved context matters a lot in whether you get the right answer or not.

772
00:59:44,520 --> 00:59:49,960
It doesn't work on treating this as a very funny problem in the sense that the colleagues come back

773
00:59:49,960 --> 00:59:55,400
or those of like specifically going towards the interpretation, I'm going to try it out for that

774
00:59:55,400 --> 01:00:07,080
period that's going to inter-product with just the RAG. Yeah, so what I just described, somebody

775
01:00:07,080 --> 01:00:11,720
asked like how do you actually, so I said there are other ways to do this and then the question

776
01:00:11,720 --> 01:00:18,040
was how do you do that, so the way you do that is using reinforce, so yeah there has been work on

777
01:00:18,040 --> 01:00:23,480
doing that, so some of the older papers were playing with this, but one of the big problems with,

778
01:00:25,400 --> 01:00:30,440
so I think the replug solution is sort of more elegant for solving that problem

779
01:00:31,000 --> 01:00:35,640
because you actually do signal from the language model and if you just do reinforce it's very

780
01:00:35,640 --> 01:00:41,480
high variance, so it's going to be super finicky if you don't want to destroy your index,

781
01:00:43,080 --> 01:00:44,040
but people have tried it.

782
01:00:49,000 --> 01:00:56,920
So there's some really nice work from OpenAI where they basically show and again we're sort of like

783
01:00:56,920 --> 01:01:02,440
thinking more and more about agents here, where they show something very similar to the flare

784
01:01:02,440 --> 01:01:06,840
results from earlier with active retrieval that doesn't necessarily have to be some index that

785
01:01:06,840 --> 01:01:12,520
you only can read just some web search, and obviously in this case you don't really have

786
01:01:12,520 --> 01:01:16,840
access to the web search necessarily, so Bing or whatever they use here is not going to update

787
01:01:16,840 --> 01:01:22,200
these parameters, but I just wanted to kind of put this in your mind like this is another thing

788
01:01:22,200 --> 01:01:28,600
you can do, and if we take this really to the general form then you can think of language

789
01:01:28,600 --> 01:01:35,000
models as just tool users, so rather than just retrieval augmenting language models we can tool

790
01:01:35,000 --> 01:01:39,640
augment language models and retrieval is just one of the many tools that language models have access

791
01:01:39,640 --> 01:01:46,440
to, we can have re-rankers and things on top of the outputs of these tools, and so one of the big

792
01:01:46,440 --> 01:01:52,920
questions I think is how do you actually get the system to learn stuff, so we're going to need RL if

793
01:01:52,920 --> 01:02:01,880
we want the system to really learn how to take these actions properly, and so yeah this has been

794
01:02:01,880 --> 01:02:07,320
taken to the extreme in this sort of self-reg architecture where they have this sort of retrieval

795
01:02:07,320 --> 01:02:12,280
step and it's active and then you criticize it and then you basically do some natural language

796
01:02:12,280 --> 01:02:16,840
inference and all of that just with one language model to answer the questions.

797
01:02:18,760 --> 01:02:23,080
So the other missing piece, so I'm just kind of going through a bunch of open questions

798
01:02:23,640 --> 01:02:28,280
that people have looked at, but feel free to interrupt me if there's anything you want to know,

799
01:02:29,400 --> 01:02:33,880
but so instruction tuning we established at the beginning of the lecture that this is pretty

800
01:02:33,880 --> 01:02:40,600
important for getting things to work, so fixing the user interface, but the instruction tuning

801
01:02:40,600 --> 01:02:44,920
has almost always only happened on the language model and not on the entire system,

802
01:02:44,920 --> 01:02:49,880
so I think one of the interesting things that people are looking at now with things like RADT

803
01:02:49,880 --> 01:02:54,360
and Instruct Retro is how can we instruction fine-tune an entire retrieval augmented system,

804
01:02:54,360 --> 01:02:59,720
so all the way into the retrieval step can we generate data so that that also follows the

805
01:02:59,720 --> 01:03:03,720
instructions properly, which currently doesn't happen in any of these model architectures.

806
01:03:03,720 --> 01:03:10,040
And then finally, I think I would be remiss if I didn't really talk about

807
01:03:10,760 --> 01:03:14,840
what people call advanced RAG, so like the developer community has been really doing

808
01:03:14,840 --> 01:03:20,360
some awesome stuff, so like frameworks like Lamaindex and Langchain and there's all these

809
01:03:20,360 --> 01:03:24,920
open source vector databases like Chroma and Weaviate and they're all sort of about making

810
01:03:24,920 --> 01:03:30,280
RAG really easy, but this is all frozen RAG, right, but even with frozen RAG you've been

811
01:03:30,280 --> 01:03:36,120
really doing incredible things, so we mentioned some of these already, so child-parent recursive

812
01:03:36,120 --> 01:03:41,480
retrievers, so you find small parts and then you give the big parts around it to the language model,

813
01:03:41,480 --> 01:03:46,200
you can do hybrid search where we use reciprocal rank fusion, so we have like different search

814
01:03:46,200 --> 01:03:50,360
results that we didn't combine before we give the final thing to the language model.

815
01:03:51,000 --> 01:03:55,800
There's zero shot like a large language model remanker, so basically the score function is not,

816
01:03:55,800 --> 01:03:59,000
it doesn't come from your retrieval, it comes directly from the language model,

817
01:03:59,960 --> 01:04:04,840
and then hypothetical document embeddings which I think is a really cool idea, so you just,

818
01:04:05,560 --> 01:04:11,400
basically you fix hallucination through hallucination, so you get a question, then you let

819
01:04:11,400 --> 01:04:15,720
the language model hallucinate a bunch of possible answers, then you go and search for

820
01:04:15,720 --> 01:04:20,120
nearest neighbors to the possible answers and you give those as context and then it gives the right

821
01:04:20,120 --> 01:04:26,040
answer based on that, so it was really like hallucinating answers, I think it's a brilliant

822
01:04:26,120 --> 01:04:32,840
solution, so there's a lot of stuff happening in the kind of frozen rack community too that I think

823
01:04:32,840 --> 01:04:40,440
is very interesting to look at, so just to wrap up, kind of looking at the future of this stuff,

824
01:04:41,640 --> 01:04:45,960
there are still lots of very interesting open questions, so if you're a student thinking about

825
01:04:45,960 --> 01:04:54,200
how to solve any of these, I think you can have quite a lot of impact, so how exactly do we do

826
01:04:54,200 --> 01:04:59,240
the pretraining of this architecture and do we even need to pretrain, I think even retro kind of

827
01:04:59,240 --> 01:05:04,840
shows that you don't necessarily have to pretrain, so maybe there's something wrong with how we do

828
01:05:04,840 --> 01:05:09,800
that, what do skating laws look like, so I think there's a really interesting question here around

829
01:05:09,800 --> 01:05:15,560
if I have a huge index and a very rich encoder of all the information in that index, maybe I can

830
01:05:15,560 --> 01:05:20,760
move, so basically decouple all the memorization to this index, so I have a language model that

831
01:05:20,760 --> 01:05:25,240
doesn't know anything, it just speaks English, it just sort of reasons on top, but it has no

832
01:05:25,240 --> 01:05:29,480
knowledge because that always comes from this retriever, if you can do something like that then

833
01:05:29,480 --> 01:05:34,600
you get very interesting scaling trade-offs, so you can have a tiny language model and do your

834
01:05:34,600 --> 01:05:39,880
retrieval to do a lot of the heavy lifting with your retrieval, which is nice because that's a

835
01:05:39,880 --> 01:05:45,160
cached computation, so you already have the embeddings, you just need to do the dot product,

836
01:05:45,960 --> 01:05:49,240
so it's much more efficient than kind of self-attention in the language model.

837
01:05:50,840 --> 01:05:57,000
Can we move beyond by encoders, so vector databases, I like people who build vector

838
01:05:57,000 --> 01:06:03,720
databases, but I'm not sure how long we're going to keep vector databases, because I think

839
01:06:04,280 --> 01:06:09,400
re-rankers probably work just as well and the N25 is much more efficient than a vector database,

840
01:06:10,920 --> 01:06:17,560
so I don't really see why we need dedicated vector databases, so what we're seeing, but maybe

841
01:06:17,560 --> 01:06:23,480
this is a bit of a critique of Silicon Valley investment strategies and things like that, but

842
01:06:23,480 --> 01:06:29,000
a lot of these vector database companies are basically becoming database companies now,

843
01:06:29,000 --> 01:06:34,680
so they are adding all this sparse stuff because the dense thing is not enough, and as it turns out

844
01:06:34,680 --> 01:06:40,280
there are a lot of pretty good sparse databases out there already, like Postgres and things like

845
01:06:40,280 --> 01:06:46,200
that, and there are also all adding vectors to their databases, so I think that's all going to

846
01:06:46,200 --> 01:06:55,080
kind of coalesce into databases. So I think there are some interesting things to look at

847
01:06:55,800 --> 01:07:01,960
for kind of the data, so through this instruction problem, can we generate much better data for

848
01:07:01,960 --> 01:07:07,080
training RAG systems synthetically, and then I think there's this massive open question around

849
01:07:07,080 --> 01:07:11,560
how we actually measure whether the RAG system is any good, so right now we just look at downstream

850
01:07:11,560 --> 01:07:17,720
performance, which is sort of okay, but if you mess up the retrieval it's very hard to measure,

851
01:07:18,920 --> 01:07:23,800
but how to measure whether your retrieval is right is also very difficult, so there are some

852
01:07:23,800 --> 01:07:28,520
frameworks where they try to take like the harmonic mean of your retrieval accuracy and your language

853
01:07:28,520 --> 01:07:34,040
model accuracy, but I think those are also very shoddy because we don't really have very good

854
01:07:34,040 --> 01:07:38,840
data sets to measure that on, so I think that's a very cool problem to work on as well.

855
01:07:39,720 --> 01:07:45,720
So the other problem that I personally am always very excited about is multimodality,

856
01:07:47,000 --> 01:07:53,720
and so why would we stop with RAG systems with just text, so you can do the same with images,

857
01:07:54,280 --> 01:07:59,320
you can augment language models with vision, so we did this work on lens where we have a

858
01:07:59,320 --> 01:08:05,720
language model enhanced to see, where you can just give kind of a computer vision pipeline,

859
01:08:05,720 --> 01:08:10,040
just like a retrieval pipeline and give that to a frozen language model and pass it to the

860
01:08:10,040 --> 01:08:15,320
context and that system actually is an amazing visual question answering system. It's close to

861
01:08:15,320 --> 01:08:20,760
state-of-the-art sort of flamingo from DeepMind, which is also very hard to reproduce because

862
01:08:20,760 --> 01:08:28,840
there's no open source version of that, so we've done some early work on this in 2021 where we

863
01:08:28,840 --> 01:08:34,280
have this cross-modal retrieval and there's some more recent work out of FAIR where they also look

864
01:08:34,280 --> 01:08:38,920
at this, so I think that's really like if you look at the trend in the field like multimodality

865
01:08:38,920 --> 01:08:43,560
with GPD4 or V and things like that is really a hot topic, so everything is kind of going in

866
01:08:43,560 --> 01:08:52,040
that direction, so it's an interesting thing to think about. So overall I think it would be nice

867
01:08:52,040 --> 01:08:58,280
if everybody sort of moves away from RAG 1.0, the frozen Frankenstein RAG and moves towards this much

868
01:08:58,280 --> 01:09:03,960
more kind of optimized version RAG 2.0, so it's really about systems over models, right, it's

869
01:09:03,960 --> 01:09:07,800
not just your language model when you're retriever and they're kind of separate, it's about thinking

870
01:09:07,800 --> 01:09:12,600
from the from a system's perspective about the entire thing and the problem you're trying to solve

871
01:09:12,600 --> 01:09:17,640
and so I think that really is the way that in deep learning things have always progressed,

872
01:09:17,640 --> 01:09:22,360
where if you optimize the system end-to-end that's always going to win out, like back in the day in

873
01:09:22,360 --> 01:09:26,840
computer vision or NLP, we have like parsers and scene parsers and all this kind of stuff and all

874
01:09:26,840 --> 01:09:32,600
of that just doesn't exist anymore now because we optimize the system end-to-end and so that's

875
01:09:32,600 --> 01:09:37,640
what's going to happen here too. So if we take that to the extreme, like there's a chunker thing

876
01:09:37,640 --> 01:09:42,040
in your documents, right, like cutting it up into pieces, like you could backdrop into debt,

877
01:09:42,040 --> 01:09:49,560
like why not? Somebody should really do that and so yeah, I think like trading off cost and quality

878
01:09:50,360 --> 01:09:54,200
and zero-shot domain generalization, that's really like where this stuff is going to come in, right,

879
01:09:54,200 --> 01:09:59,080
so language models right now, they're amazing but very often they're way too expensive for being

880
01:09:59,080 --> 01:10:03,800
deployed somewhere where you can actually make money from them if you're in a company. So what

881
01:10:03,800 --> 01:10:08,760
you want to do is make it much more efficient and have the right cost quality trade-off and the

882
01:10:08,760 --> 01:10:13,640
easiest way I can think of is to do it through retrieval augmentation but obviously I'm very biased.

883
01:10:15,720 --> 01:10:22,040
So yeah, that was all I had actually. So if you're interested in this, I'm at Samford so I can

884
01:10:22,040 --> 01:10:26,760
work with you on research projects on these topics or if you want you can also join contextual

885
01:10:26,760 --> 01:10:34,040
because we work on this stuff every day. Thank you. Well, sorry, I had a question from earlier.

886
01:10:35,880 --> 01:10:41,240
Yeah, I think you said something really, really, I think really super helpful earlier about

887
01:10:41,240 --> 01:10:46,600
Miss Jill 7B. You talked about, you compared the sliding window attention to convolutional neural

888
01:10:46,600 --> 01:10:50,040
networks and I do see the parallel because with convolutional neural networks you have

889
01:10:50,680 --> 01:10:54,760
several layers of, several different layers of convolutional layers and the top convolutional

890
01:10:54,760 --> 01:11:00,120
layers are able to see a larger receptive field than the bottom convolutional layers

891
01:11:00,120 --> 01:11:07,720
and with convolutional layers you're able to tune the filter sizes and the strides so you're able

892
01:11:07,720 --> 01:11:12,520
to see a different receptive field and I was wondering if you could see that same innovation

893
01:11:12,520 --> 01:11:17,960
in Miss Jill 7B by tuning because you have different transformer layers and if each transformer

894
01:11:17,960 --> 01:11:23,000
layer will have a span over a different set of tokens and you can tune I guess the transformer

895
01:11:23,000 --> 01:11:27,720
architecture the way you tune those convolutional layers, the filter sizes, the receptive field,

896
01:11:27,720 --> 01:11:31,720
perhaps we can do some optimization in the transformer realm that we have already done

897
01:11:31,720 --> 01:11:37,880
in convolution layers. Yeah, I think that's a good idea. There's a great paper on light

898
01:11:37,880 --> 01:11:44,040
convolutions I think from Michael Auli and David Gange and a bunch of people where it's basically

899
01:11:44,760 --> 01:11:49,560
this came out at exactly the same time as the transformer and the transformer is slightly

900
01:11:49,640 --> 01:11:54,840
more optimized for GPU computation but the convolutional model was actually slightly

901
01:11:54,840 --> 01:12:01,400
better than the transformer so it's definitely worth exploring. Okay, cool, thanks.

902
01:12:01,400 --> 01:12:18,520
Yeah, so it depends on the problem I think what you probably want to do is

903
01:12:18,520 --> 01:12:24,280
is sort of cast a white net with VM25 and then just narrow it down with dense search

904
01:12:25,080 --> 01:12:29,560
so you often see that kind of as a two-stage process where the first one is kind of noisy

905
01:12:29,560 --> 01:12:34,120
you can add noise actually to your retrieval and then you use the dense one to filter it down.

906
01:12:36,760 --> 01:12:42,680
Yeah, everyone's trying to maybe adapt their plug-in model to almost

907
01:12:42,680 --> 01:12:49,240
the only specific area. I think there are many two ways of project one way is to use

908
01:12:49,240 --> 01:12:55,080
some instrument tuning in some kind of learning way or functionally like tuning that and another way

909
01:12:55,080 --> 01:13:02,520
is to the main project of this lecture is using the virtual augmented way. So I wonder if I

910
01:13:03,160 --> 01:13:10,680
know a message of virtual augmented way, do you think the capacity or the quality of virtual

911
01:13:10,680 --> 01:13:15,640
augmented way can be matched with those tuning methods I think or kind of learning?

912
01:13:15,640 --> 01:13:21,960
Yeah, so I think actually what's going to happen is that all of this will come together right so

913
01:13:22,920 --> 01:13:28,440
if you actually train things like end-to-end, rack 2.0 style then you can also fine-tune that system

914
01:13:29,000 --> 01:13:35,720
on some use case end-to-end. So why would you just take the retrieval augmented system if you can

915
01:13:35,720 --> 01:13:39,640
also fine-tune it on the thing you care about? So I think in the end everybody's going to do

916
01:13:39,640 --> 01:13:43,640
all of those things and then there's questions like how do you do that efficiently so that's

917
01:13:43,640 --> 01:13:50,520
why you would use adapters or things like that. I think there was another question.

918
01:13:52,920 --> 01:13:57,320
I'm curious about hardware, you say it's going to become database kind of thing,

919
01:13:57,320 --> 01:14:04,440
in fact it was part of the database but what about retrieval hardware and you know it's mine

920
01:14:04,440 --> 01:14:11,160
because we've got so much of the learning part but what about because it's huge,

921
01:14:12,360 --> 01:14:17,000
tree as I said so do you have any ideas it's just a database problem?

922
01:14:17,000 --> 01:14:24,280
So I don't know if I'm allowed to say this exactly actually but so one of the biggest chip

923
01:14:24,280 --> 01:14:29,880
manufacturers that recently their stock has done really well they have some dedicated retrieval

924
01:14:29,880 --> 01:14:40,200
hardware coming out I think sooner it might already be out. So yeah very efficient dense

925
01:14:40,200 --> 01:14:48,280
retrieval is a very big business. Other questions?

926
01:14:59,160 --> 01:15:04,920
Yes I think so if you take it to the extreme so one of the big problems right now is that

927
01:15:04,920 --> 01:15:08,440
if you contextualize an existing language model that already hallucinates

928
01:15:09,160 --> 01:15:13,000
then then it's going to be kind of hard to get rid of the hallucination right so if you do

929
01:15:13,000 --> 01:15:19,880
replug on GPT-4 GPT-4 might still hallucinate so you could basically just ignore all the stuff

930
01:15:19,880 --> 01:15:24,200
you retrieved and just do whatever it wants anyway so that's one of the reasons why you

931
01:15:24,200 --> 01:15:28,680
want to train the system end to end and if you take that to the extreme where like I said right

932
01:15:28,680 --> 01:15:34,200
if you can just have the language model only reason and speak so it knows English and reasoning

933
01:15:34,200 --> 01:15:39,640
but it has no knowledge which all comes from somewhere else then you can't hallucinate and so

934
01:15:39,640 --> 01:15:51,080
it's really all grounded in whatever is in your index but they're so they're about hallucination

935
01:15:51,080 --> 01:15:55,320
I'm sort of frustrated that a lot of people in the field misunderstand what hallucination even

936
01:15:55,320 --> 01:16:00,520
means like so a lot of people are conflating hallucination with a correctness or incorrectness

937
01:16:00,520 --> 01:16:04,520
so they're like oh the model made a mistake it hallucinated it's like no it made a mistake

938
01:16:05,320 --> 01:16:10,280
that's different from hallucination hallucination I think is very specific kind of I've retrieved

939
01:16:10,280 --> 01:16:16,600
something so I have some sort of counterfactual ground truth and what I'm saying does not correspond

940
01:16:16,600 --> 01:16:24,600
to that ground truth and so yeah I think there's a bunch of folks at Stanford also working on better

941
01:16:24,600 --> 01:16:27,480
measurements of hallucination and definitions and things like that

942
01:16:37,560 --> 01:16:43,960
yeah awesome ground truth right so so hallucination is really like there there is something that is

943
01:16:43,960 --> 01:16:49,000
true right so so if we're talking about like hallucination and yeah so if we're talking about

944
01:16:49,000 --> 01:16:53,640
just general parametric language models then sort of the ground truth is whatever we consider to be

945
01:16:53,640 --> 01:17:01,960
true right but we had to work for like language models making mistakes before it was called major

946
01:17:01,960 --> 01:17:14,120
mistakes yeah ground truth I guess this is solving that helps me to question that path

947
01:17:14,760 --> 01:17:19,560
are you working on ground truth per se that's around you know if I generate the building

948
01:17:20,120 --> 01:17:26,680
saying oh well I'm a president I mean everything all the time are you sharing work on that on this

949
01:17:27,720 --> 01:17:33,960
yeah so so I like the sort of silo uh mansion error as well so I think the whole point is that you

950
01:17:33,960 --> 01:17:40,120
can you can have different indices and different definitions of ground truth and so um I think

951
01:17:40,120 --> 01:17:46,600
you could say I only trust uh archive or I only trust like peer review papers and not just archive

952
01:17:47,480 --> 01:17:51,240
so you can make decisions in your architecture during test time about what you

953
01:17:51,240 --> 01:17:58,440
define as ground truth and I also think actually that and there's a bunch of work I think happening

954
01:17:58,440 --> 01:18:03,400
on this right now you can control for how how grounded you want it to be in your ground truth

955
01:18:03,960 --> 01:18:08,920
so that's another kind of misconception about hallucinations like sometimes hallucinations

956
01:18:08,920 --> 01:18:13,000
are actually good right if you have a creative writing assistant and you wanted to come up with

957
01:18:13,000 --> 01:18:18,440
some cool new ideas you want the language model to hallucinate uh so I think what you want to have

958
01:18:18,440 --> 01:18:22,680
is kind of a tunable knob where you say like oh now you can hallucinate and now maybe you should

959
01:18:22,680 --> 01:18:26,840
like really tell me the truth only

960
01:18:31,720 --> 01:18:34,840
anything else

961
01:18:35,160 --> 01:18:47,960
yeah so but the temperature that's just about how you sample right so how flat your your distribution

962
01:18:47,960 --> 01:18:57,640
is that you sample from yes but so even if you have a low temperature it can still come up with

963
01:18:57,640 --> 01:19:02,360
random stuff right so it just says that then you're very likely to do like really sampling

964
01:19:02,920 --> 01:19:14,200
um so so I think what you want to get at is is something more sophisticated than that

965
01:19:16,920 --> 01:19:22,200
yeah I like the question

