1
00:00:00,000 --> 00:00:10,000
We're glad to have Angela Fan today with us here.

2
00:00:10,000 --> 00:00:15,960
And she's a research scientist at Meta AI Research in New York, focusing on research

3
00:00:15,960 --> 00:00:18,280
in text generation mainly.

4
00:00:18,280 --> 00:00:24,880
And currently she's working on language modeling and developing the line AI agents, metaproducts,

5
00:00:24,880 --> 00:00:29,200
and recent research products include no language left behind, which she'll be talking briefly

6
00:00:29,200 --> 00:00:35,760
about today, universal speech translation for unwritten languages, as well as Lama2.

7
00:00:35,760 --> 00:00:38,280
So give it up for Angela, I guess.

8
00:00:38,280 --> 00:00:41,480
All right, thank you all so much.

9
00:00:41,480 --> 00:00:44,880
So yeah, when I got this email, I was like, oh, I should probably talk about Lama2.

10
00:00:44,880 --> 00:00:48,920
But then I noticed you have Sharon, who will like, you know, is like a 10x better speaker

11
00:00:48,920 --> 00:00:49,920
than me.

12
00:00:49,920 --> 00:00:52,000
So I was like, okay, like maybe not Lama2.

13
00:00:52,000 --> 00:00:55,600
But then I thought I maybe would cover this project that we did called No Language Left

14
00:00:55,600 --> 00:00:59,880
Behind, which could be very, also very relevant to this class.

15
00:00:59,880 --> 00:01:07,880
And so when you think about a lot of text generation technology, most of it, until fairly recently,

16
00:01:07,880 --> 00:01:10,360
has been really focused on English.

17
00:01:10,360 --> 00:01:14,440
But there are actually more than 3000 written languages worldwide.

18
00:01:14,440 --> 00:01:20,040
And for me, this is extremely personally meaningful because actually English is my third language.

19
00:01:20,040 --> 00:01:22,040
So it's really important.

20
00:01:23,040 --> 00:01:26,880
Yeah, so it's really also very personally meaningful.

21
00:01:26,880 --> 00:01:31,600
And when you think about some of the multilingual technology that permeates, it's not like we've

22
00:01:31,600 --> 00:01:33,800
never worked on multilingual, right?

23
00:01:33,800 --> 00:01:37,880
Actually, when speaking about generative AI, I actually think translation is one of the

24
00:01:37,880 --> 00:01:42,000
most commercially successful and widespread applications of generative AI.

25
00:01:42,000 --> 00:01:47,240
I mean, ultimately, translation models, they are, you know, like conditional language models.

26
00:01:47,240 --> 00:01:52,120
And so when you think about like traveling or something like that, or my sister is taking

27
00:01:52,120 --> 00:01:56,040
Spanish, so like just like doing her Spanish homework, we have a lot of tools that exist

28
00:01:56,040 --> 00:01:57,040
today.

29
00:01:57,040 --> 00:02:01,840
So things like Google Translate cover around 130 languages, Microsoft Translate about 110.

30
00:02:01,840 --> 00:02:07,680
This might be a little bit outdated since I pulled the statistics a little bit ago.

31
00:02:07,680 --> 00:02:11,480
But the project for No Language Left Behind, it started from like a very simple ask, like,

32
00:02:11,480 --> 00:02:14,320
okay, there's 3000 languages worldwide.

33
00:02:14,400 --> 00:02:20,160
Maybe it'll be like pretty hard to get to all 3000, since some of them are pretty rare

34
00:02:20,160 --> 00:02:21,880
and not spoken by many.

35
00:02:21,880 --> 00:02:27,000
But there are still like hundreds of languages spoken by millions and millions of people.

36
00:02:27,000 --> 00:02:28,920
And so we were like, okay, no big deal.

37
00:02:28,920 --> 00:02:34,160
Like, let's just start from the 100-ish that we have today and just go for like a doubling.

38
00:02:34,160 --> 00:02:38,320
Like, what would it take to actually be able to double this kind of coverage?

39
00:02:38,320 --> 00:02:41,680
And of course, you know, just saying that you support a bunch of languages is not the

40
00:02:41,680 --> 00:02:42,680
goal.

41
00:02:42,680 --> 00:02:47,320
You actually want to create high quality safe translations that would be usable by people

42
00:02:47,320 --> 00:02:51,000
just like if you're going on vacation today, you're kind of instinctive to whip out your

43
00:02:51,000 --> 00:02:54,560
phone and get on the Google Translate app.

44
00:02:54,560 --> 00:02:58,920
And so kind of the backdrop to this project was that there's actually a lot of progress

45
00:02:58,920 --> 00:03:00,240
in translation.

46
00:03:00,240 --> 00:03:04,640
So historically, there's been a lot of focus on what we call higher resource languages.

47
00:03:04,640 --> 00:03:08,600
And these are not necessarily languages that are spoken by the most people in the world.

48
00:03:08,600 --> 00:03:12,200
But when we say higher resource, it means the most amount of data.

49
00:03:12,200 --> 00:03:16,880
And so you can think about things like Europarl or, you know, translations from the European

50
00:03:16,880 --> 00:03:17,880
Parliament.

51
00:03:17,880 --> 00:03:22,200
And those served as the foundation for a lot, a lot of translation development.

52
00:03:22,200 --> 00:03:26,360
And more recently, there's been a great focus on low resource languages, and it's been driven

53
00:03:26,360 --> 00:03:32,320
across the research community with groups like Ghana NLP, Masekane, America's NLP.

54
00:03:32,320 --> 00:03:34,760
And these are all really exciting developments.

55
00:03:34,760 --> 00:03:39,920
And so these have led to a lot of development of new data sets, as well as criticisms of

56
00:03:39,920 --> 00:03:44,880
existing data sets, and also work on new languages, and usually languages that people

57
00:03:44,880 --> 00:03:47,120
kind of speak and they care a lot about.

58
00:03:47,120 --> 00:03:49,800
And we found this like really, really exciting.

59
00:03:49,800 --> 00:03:54,320
And so looking at a lot of this, a bunch of us got together at fair and started thinking

60
00:03:54,320 --> 00:03:59,080
like, okay, we actually speak some pretty low resource languages from like Catalan to

61
00:03:59,080 --> 00:04:01,040
Ossamese and so on.

62
00:04:01,040 --> 00:04:04,640
And so we started this as kind of like a big, passionate research project.

63
00:04:04,640 --> 00:04:09,520
And so today, I want to cover a little bit about our high level approach to this problem,

64
00:04:09,520 --> 00:04:11,640
which is a little bit interdisciplinary.

65
00:04:11,640 --> 00:04:16,040
I want to talk about how we actually created the data sets to be able to support this kind

66
00:04:16,040 --> 00:04:17,040
of work.

67
00:04:17,040 --> 00:04:21,760
Of course, I want to talk about the models, since this is a class about transformers.

68
00:04:21,760 --> 00:04:25,040
One note here, I think that's actually very interesting in terms of translation as like

69
00:04:25,040 --> 00:04:30,680
a research direction, is that actually a lot of innovations have been done in translation.

70
00:04:30,680 --> 00:04:35,120
The original transformer paper, I think is one of them, and which makes always translation

71
00:04:35,120 --> 00:04:37,560
a quite interesting area to work on.

72
00:04:37,600 --> 00:04:40,880
Because I feel like it's a very mature research area as well.

73
00:04:40,880 --> 00:04:44,760
So it kind of is like, okay, if your architecture works in translation, it probably works very

74
00:04:44,760 --> 00:04:45,760
generally.

75
00:04:45,760 --> 00:04:49,760
So that's also one of the things that excites me about translation research.

76
00:04:49,760 --> 00:04:53,200
Then I want to talk about evaluation, like how are we actually measuring and ensuring

77
00:04:53,200 --> 00:04:57,200
the quality of these translations are good and safe for people.

78
00:04:57,200 --> 00:05:00,840
And then I want to end with a little bit of like, you know, high level thoughts about

79
00:05:00,840 --> 00:05:05,560
future directions and things that I hope that we can work on in the future.

80
00:05:05,560 --> 00:05:08,120
So I want to start with our approach.

81
00:05:08,120 --> 00:05:12,360
I think the most important thing in research is to know that we're working on a real problem,

82
00:05:12,360 --> 00:05:16,600
especially when it's really close to people like translation.

83
00:05:16,600 --> 00:05:20,760
And I think in many areas, like when I was working on on-device AI, for example, I feel

84
00:05:20,760 --> 00:05:25,280
like I had like a research problem in mind, but it was like very, very disconnected from

85
00:05:25,280 --> 00:05:28,320
the practical problem of actually putting models on phones.

86
00:05:28,320 --> 00:05:30,800
And so this was something that was really important to us.

87
00:05:30,800 --> 00:05:35,480
And so we actually started the project by kind of like focusing on a social sciences

88
00:05:35,480 --> 00:05:39,200
type approach or, you know, sociology type approach.

89
00:05:39,200 --> 00:05:42,840
And we actually did a lot of interviews with low resource speakers.

90
00:05:42,840 --> 00:05:48,280
And so we met with about 44 different native speakers that spoke 36 different languages

91
00:05:48,280 --> 00:05:50,360
across North America.

92
00:05:50,360 --> 00:05:54,320
I will say that a lot of them are like immigrants to the US, since that was kind of like the

93
00:05:54,320 --> 00:05:57,360
easiest kind of cohort to recruit.

94
00:05:57,360 --> 00:06:01,960
And we learned a lot of different things about how they approach low resource languages,

95
00:06:01,960 --> 00:06:04,240
but also the kind of technological need that they have.

96
00:06:04,240 --> 00:06:08,360
Because I think it's easy to be like, hey, I have this cool background, like I have this

97
00:06:08,360 --> 00:06:11,680
cool problem, and I want to solve it, but I think it's very important to actually like

98
00:06:11,680 --> 00:06:15,280
talk to the people if this is a problem that needs to be solved.

99
00:06:15,280 --> 00:06:19,480
And so we learned that there's great fear in general that low resource languages might

100
00:06:19,480 --> 00:06:25,960
be undergoing a state of decline, partially because a lot of education is shifting to

101
00:06:25,960 --> 00:06:30,240
languages like Hindi or like English or Mandarin Chinese, for example.

102
00:06:30,240 --> 00:06:34,640
And there's a lot of excitement to be included in existing translation systems.

103
00:06:34,640 --> 00:06:39,560
And people said they have always tried to use Google translator, Microsoft translate

104
00:06:39,560 --> 00:06:41,240
in their existing languages.

105
00:06:41,240 --> 00:06:45,400
But ultimately they found that the quality is really insufficient for reliable usage.

106
00:06:45,400 --> 00:06:49,080
So if you think about like, well, I was going to say when I was in high school, but you're

107
00:06:49,080 --> 00:06:51,080
all probably like substantially younger than me.

108
00:06:51,080 --> 00:06:54,680
So maybe like, you know, 10, so years ago, you know, and you tried to use Google translate

109
00:06:54,680 --> 00:06:58,640
for your Spanish homework, like your Spanish teacher could always identify that like, you

110
00:06:58,640 --> 00:07:01,960
know, it was not a human written translation until you would get marks off.

111
00:07:01,960 --> 00:07:06,040
But that's not really the case for some of the high resource languages today.

112
00:07:06,040 --> 00:07:10,520
And so I think as with all things in machine learning, it really starts from a data perspective.

113
00:07:10,520 --> 00:07:14,560
Like why can't we just train models in hundreds of languages or large language models in hundreds

114
00:07:14,560 --> 00:07:15,560
of languages?

115
00:07:15,560 --> 00:07:17,840
It's because we don't have the data to support it.

116
00:07:17,840 --> 00:07:22,480
And so I want to talk first about evaluation data sets because I think it's extremely important

117
00:07:22,480 --> 00:07:24,840
to nail evaluation.

118
00:07:24,840 --> 00:07:27,160
And then I'll talk about training.

119
00:07:27,160 --> 00:07:32,800
So for an evaluation data set for this work, we started this Flora's effort, it stands

120
00:07:32,800 --> 00:07:36,880
for Facebook low resource, I guess we're called meta now, but I didn't think more as was like

121
00:07:36,880 --> 00:07:41,040
a very good, a renaming so we're still calling it Flora's.

122
00:07:41,040 --> 00:07:45,480
So this was something we originally started for just two languages in this first paper

123
00:07:45,480 --> 00:07:48,320
at EMLP, many years ago.

124
00:07:48,320 --> 00:07:50,560
So it was just for Napoleon Sinhalo.

125
00:07:50,560 --> 00:07:55,280
And we later extended it to incorporate two more languages in a release afterwards.

126
00:07:55,280 --> 00:08:00,440
You know, we thought a lot about, okay, like Flora's was really useful for the community.

127
00:08:00,440 --> 00:08:02,280
How can we extend it to 100 languages?

128
00:08:02,280 --> 00:08:08,160
And so that was this follow up work that we did, I think we had at ACL or WMT.

129
00:08:08,160 --> 00:08:13,480
And then in this project, we were like, okay, how can we go from Flora's 101 to Flora's

130
00:08:13,480 --> 00:08:16,640
200 to really go for the doubling effect?

131
00:08:16,640 --> 00:08:17,640
And so what is Flora's?

132
00:08:17,640 --> 00:08:19,080
Well, it's in the name.

133
00:08:19,080 --> 00:08:21,480
It's a focus on low resource languages.

134
00:08:21,480 --> 00:08:26,720
So we do include some higher resource languages like German or Hindi or so on, almost for

135
00:08:26,720 --> 00:08:29,040
calibration effect as well.

136
00:08:29,040 --> 00:08:33,480
But the majority of the focus is on these lower and mid resource languages.

137
00:08:33,480 --> 00:08:39,200
It's the first large scale, many to many machine translation evaluation data set, which means

138
00:08:39,200 --> 00:08:43,880
that we take all of the sentences in English and then we translate them to all of the languages,

139
00:08:43,880 --> 00:08:48,440
which means that you would be able to evaluate any cross pair of languages.

140
00:08:48,440 --> 00:08:52,160
So for example, like Chinese to French, I lived in France for many years.

141
00:08:52,160 --> 00:08:54,080
So it's like very personally relevant to me.

142
00:08:54,080 --> 00:08:58,760
Of course, 200 languages also in the name, there's a broad diversity of different domains

143
00:08:58,760 --> 00:08:59,760
and topics.

144
00:08:59,760 --> 00:09:04,200
I think this is important when designing an evaluation data set, which is like very top

145
00:09:04,200 --> 00:09:10,120
of mind for anybody interested in language modeling research, because like the way people

146
00:09:10,120 --> 00:09:15,320
train machine translation models and the way people use them are often like very different.

147
00:09:15,320 --> 00:09:19,240
And so if you only benchmark your data set, for example, on news, which is very common

148
00:09:19,240 --> 00:09:23,720
in translation research, then you don't really pick up the fact that people talk about such

149
00:09:23,720 --> 00:09:28,320
a wide variety of things and have like different casual conversations that they need translated

150
00:09:28,320 --> 00:09:31,040
official documents and so on.

151
00:09:31,040 --> 00:09:33,200
It's also document level data set.

152
00:09:33,200 --> 00:09:36,840
This is not something that I think the community is like broadly leveraging right now.

153
00:09:36,840 --> 00:09:40,800
But the way it's translated is that you can have document level context.

154
00:09:40,800 --> 00:09:44,440
And so translators are provided the entire document to translate from.

155
00:09:44,440 --> 00:09:49,480
We also provide the entire document for evaluation and we translate like multiple sentences from

156
00:09:49,480 --> 00:09:50,760
the same paragraph.

157
00:09:50,760 --> 00:09:54,880
And so this was like a potential to research direction that we wanted to make sure we covered

158
00:09:54,880 --> 00:09:58,880
models that needed like potentially more context because a lot of translation work is done

159
00:09:58,880 --> 00:10:00,960
at the sentence level.

160
00:10:00,960 --> 00:10:04,000
So how do we actually ensure that this data set was high quality?

161
00:10:04,000 --> 00:10:06,720
So the first step is that we take a document.

162
00:10:06,720 --> 00:10:10,320
Well, actually, first step is like alignment on language standards.

163
00:10:10,320 --> 00:10:15,280
So this is very important because when you're translating French or Chinese, I think most

164
00:10:15,280 --> 00:10:19,360
people have a strong understanding of like what it means to produce like a good French

165
00:10:19,360 --> 00:10:20,840
or good Chinese.

166
00:10:20,840 --> 00:10:24,200
And there are a lot of professional translators hired in these languages.

167
00:10:24,200 --> 00:10:28,320
But when you go to lower resource languages, it's not necessarily the case that there's

168
00:10:28,320 --> 00:10:35,440
like a glowing translation industry around translating a lower resource language.

169
00:10:35,440 --> 00:10:40,160
And so one of the first things is actually to align on like what is a high quality translation?

170
00:10:40,160 --> 00:10:42,600
And so there's actually a lot of challenges here.

171
00:10:42,600 --> 00:10:45,640
So there are certain low resource languages where there's different competing language

172
00:10:45,640 --> 00:10:50,480
standards or there's like very high variance in different regions on how languages are

173
00:10:50,480 --> 00:10:51,600
spoken.

174
00:10:51,600 --> 00:10:54,320
And so this step is a pretty critical one.

175
00:10:54,320 --> 00:10:58,600
So then what we do is we take the document, we send it to one group of translators and

176
00:10:58,600 --> 00:11:00,920
they do the first translation step.

177
00:11:00,920 --> 00:11:05,000
Then we do some automatic checking, you know, like if the input sentence was like 10 words

178
00:11:05,000 --> 00:11:09,400
and the output sentence is like 300 words, it's like most likely something went wrong,

179
00:11:09,880 --> 00:11:11,440
and so we send it back.

180
00:11:11,440 --> 00:11:17,920
Otherwise, we'll send it onwards to a separate, completely independent set of translators

181
00:11:17,920 --> 00:11:19,160
that do review.

182
00:11:19,160 --> 00:11:21,360
And so they try to rate the quality of this.

183
00:11:21,360 --> 00:11:25,480
And if the quality doesn't pass the sufficient bar, it gets sent back to the original set

184
00:11:25,480 --> 00:11:30,680
of translators to edit and they kind of go through and like address all of the feedback.

185
00:11:30,680 --> 00:11:34,560
And then if it's good enough, then it enters our data set.

186
00:11:34,560 --> 00:11:36,400
And so there's many challenges here.

187
00:11:36,440 --> 00:11:41,280
The first one, of course, is just like finding translators and also finding more translators.

188
00:11:41,280 --> 00:11:45,880
There was a certain issue that we ran into, for example, that in a certain country that

189
00:11:45,880 --> 00:11:47,880
the internet was not available.

190
00:11:47,880 --> 00:11:51,400
And so, you know, it's a lot of recruitment.

191
00:11:51,400 --> 00:11:54,040
The other one, of course, is language standardization.

192
00:11:54,040 --> 00:11:59,880
I think I briefly mentioned this before, but there's a lot of different challenges in just

193
00:11:59,880 --> 00:12:02,480
understanding like what is a high quality translation.

194
00:12:02,480 --> 00:12:05,000
For example, the low resource language, Breton.

195
00:12:05,000 --> 00:12:08,440
There's like two competing groups on like, how do you write Breton?

196
00:12:08,440 --> 00:12:11,200
So it's like very difficult to resolve some of those things.

197
00:12:11,200 --> 00:12:16,520
And the final thing is that there's actually a lot of variation, even in languages like Arabic,

198
00:12:16,520 --> 00:12:22,600
like the Arabic, like Moroccan Arabic is very different from, you know, Jordanian Arabic and so on.

199
00:12:22,600 --> 00:12:27,400
And there are also certain regions that they speak the same language, but due to historical reasons,

200
00:12:27,400 --> 00:12:29,240
they write in different scripts.

201
00:12:29,240 --> 00:12:33,600
And so one of the things we actually did was like, if there are languages written in multiple scripts,

202
00:12:33,600 --> 00:12:37,560
we actually supported the collection of a multiple script evaluation.

203
00:12:37,560 --> 00:12:41,880
And I think this is really important because if you're building an underlying technology

204
00:12:41,880 --> 00:12:46,600
and you only choose one, then I think you risk like just kind of like naturally supporting

205
00:12:46,600 --> 00:12:51,720
one over the other when we really should be like kind of a more neutral technology provider.

206
00:12:51,720 --> 00:12:57,440
And so this is something that we we explored a lot as well as exploring different variants of Arabic.

207
00:12:57,440 --> 00:12:58,480
This is also open source.

208
00:12:58,480 --> 00:13:03,560
If you just go to this link, you can just like download all of the all of the text files for this.

209
00:13:03,760 --> 00:13:09,040
With evaluation done, I want to talk a little bit about how we collected some of these training data sets.

210
00:13:09,040 --> 00:13:13,880
The first thing I want to talk about is this data set we created called NLBCD.

211
00:13:13,880 --> 00:13:18,960
And the idea of this is like it's a really seed data set of high quality translations

212
00:13:18,960 --> 00:13:21,160
and languages that really don't have anything.

213
00:13:21,160 --> 00:13:26,160
Why? Because, well, you can't start from nothing, you know, you got a bootstrap from somewhere.

214
00:13:26,160 --> 00:13:32,320
A lot of people have been using the Bible as a way to bootstrap, but it's very limited domain,

215
00:13:32,320 --> 00:13:34,280
obviously very religious text.

216
00:13:34,280 --> 00:13:41,400
And so we created this data set NLBCD for languages that really don't have anything to get started from.

217
00:13:41,400 --> 00:13:46,520
It's only about 5,000 sentences, so it's nothing crazy, but it supports a lot of different use cases

218
00:13:46,520 --> 00:13:51,720
like training language identification models or sentence encoders, engram language models,

219
00:13:51,720 --> 00:13:54,800
like all of these things that I'm about to talk about in our data set pipeline.

220
00:13:56,040 --> 00:13:59,680
So it covers 43 languages, about 6,000 sentences.

221
00:13:59,800 --> 00:14:03,840
And the way we decided to sample it is focused on really general content.

222
00:14:03,840 --> 00:14:08,400
So Wikipedia has this article of like, hey, if you're going to start like a new Wikipedia

223
00:14:08,400 --> 00:14:13,840
in your new language, I think Wikipedia has like 309-ish Wikipedia's last I checked.

224
00:14:13,840 --> 00:14:17,600
Here's like a list of articles that every Wikipedia in a new language should have.

225
00:14:17,600 --> 00:14:20,640
And so that's where we sampled this original content from.

226
00:14:20,640 --> 00:14:24,840
And of course, it's also open source if you want to download it.

227
00:14:24,880 --> 00:14:30,600
So what we ended up doing to get large-scale training data is using mining.

228
00:14:30,600 --> 00:14:33,200
So this is not something we pioneered in this project.

229
00:14:33,200 --> 00:14:35,600
We have like a bunch of different previous work.

230
00:14:35,600 --> 00:14:37,800
So we started from Wikimatrix.

231
00:14:37,800 --> 00:14:41,600
We were like, hey, there's a lot of different sentences in Wikipedia

232
00:14:41,600 --> 00:14:44,280
and different languages that we should be able to match up.

233
00:14:44,280 --> 00:14:49,520
And so we tried to do that with Wikipedia to get machine translation training data.

234
00:14:49,520 --> 00:14:52,480
We extended that to the web in the CCMatrix project,

235
00:14:52,520 --> 00:14:56,680
and then we extended it to very, very large-scale mining on all cross-pairs

236
00:14:56,680 --> 00:15:00,680
in this project on beyond English-centric multilingual machine translation.

237
00:15:00,680 --> 00:15:04,320
We really tried to ditch like English as a central pivot language.

238
00:15:04,320 --> 00:15:07,200
And so the way this whole data mining thing works

239
00:15:07,200 --> 00:15:09,680
is that it focuses on sentence alignment.

240
00:15:09,680 --> 00:15:11,560
So everyone is probably super familiar with this

241
00:15:11,560 --> 00:15:13,320
because this is how language models are built now.

242
00:15:13,320 --> 00:15:17,520
But it's like you take Common Crawl or any other open source dump of the web.

243
00:15:17,520 --> 00:15:20,560
I don't know, like Red Pajama or like whatever you want to CCNet,

244
00:15:20,560 --> 00:15:22,320
whatever you want to use these days.

245
00:15:22,320 --> 00:15:25,960
And you take all of the data, you extract all of the text,

246
00:15:25,960 --> 00:15:28,720
you know, a lot of HTML parsing and so on goes into it.

247
00:15:28,720 --> 00:15:31,640
And the idea is that we want to try to find matching text

248
00:15:31,640 --> 00:15:33,080
that could be a translation.

249
00:15:33,080 --> 00:15:35,240
So we shatter it all into sentences,

250
00:15:35,240 --> 00:15:38,360
we embed them with different sentence encoder models,

251
00:15:38,360 --> 00:15:42,040
and then we do a match to try to understand in a multilingual space

252
00:15:42,040 --> 00:15:44,680
if the sentences match.

253
00:15:44,680 --> 00:15:47,000
And so one of the biggest challenges to this

254
00:15:47,000 --> 00:15:50,600
is that the quality of the sentence encoding is very important.

255
00:15:50,600 --> 00:15:52,720
So if your sentence encoding is not very accurate,

256
00:15:52,720 --> 00:15:55,760
then it's impossible to match in this multidimensional space

257
00:15:55,760 --> 00:15:58,480
the idea of like the meaning being the same.

258
00:15:58,480 --> 00:16:00,880
And so one of the big things we tried to do here

259
00:16:00,880 --> 00:16:05,360
in this project was try to improve the quality of the sentence encoders.

260
00:16:05,360 --> 00:16:08,760
And so one of the big things that we did was train sentence encoders

261
00:16:08,760 --> 00:16:10,240
with mask language modeling.

262
00:16:10,240 --> 00:16:11,520
You see that on the left.

263
00:16:11,520 --> 00:16:16,400
But we also use multilingual distillation, which you see on the right.

264
00:16:16,400 --> 00:16:19,840
And so previous approaches to sentence encoders

265
00:16:19,840 --> 00:16:22,240
and the trend in the research community for a while

266
00:16:22,240 --> 00:16:25,360
was to really try to embed all languages

267
00:16:25,360 --> 00:16:27,200
in the same sentence encoder model.

268
00:16:27,200 --> 00:16:30,880
So projects like XLMR, for example, are in that direction.

269
00:16:30,880 --> 00:16:33,160
I think it's pretty widely used.

270
00:16:33,160 --> 00:16:36,240
The challenge with this when you're training a low resource model

271
00:16:36,240 --> 00:16:38,640
is that a lot of your high resource data

272
00:16:38,640 --> 00:16:41,960
just overwhelms your low resource data.

273
00:16:41,960 --> 00:16:45,040
And so you don't end up with a very high quality sentence encoder

274
00:16:45,040 --> 00:16:46,320
for those languages.

275
00:16:46,320 --> 00:16:49,640
So what we ended up doing is we had a multilingual teacher model

276
00:16:49,680 --> 00:16:52,520
and we distilled a bunch of student models

277
00:16:52,520 --> 00:16:56,440
that are specialized to different language families

278
00:16:56,440 --> 00:16:57,680
that are low resource.

279
00:16:57,680 --> 00:17:00,200
And so this enables the quality to be pretty high.

280
00:17:00,200 --> 00:17:02,000
And so the way that distillation works

281
00:17:02,000 --> 00:17:05,400
is that the teacher and the student model both see the same data

282
00:17:05,400 --> 00:17:08,360
and then we try to minimize the cosine loss

283
00:17:08,360 --> 00:17:12,480
between the sentence embeddings that they produce.

284
00:17:12,480 --> 00:17:14,360
I think an important question that you can ask here

285
00:17:14,360 --> 00:17:17,520
is why do you need to do multilingual distillation?

286
00:17:17,520 --> 00:17:21,280
Why can't you just train a bunch of different student models,

287
00:17:21,280 --> 00:17:22,720
like one per language family,

288
00:17:22,720 --> 00:17:25,040
like why even care about distillation?

289
00:17:25,040 --> 00:17:27,880
And the reason is because if you're going to use a bunch

290
00:17:27,880 --> 00:17:29,960
of sentence encoders for mining,

291
00:17:29,960 --> 00:17:31,880
the important thing is that they all exist

292
00:17:31,880 --> 00:17:34,160
in the same embedding space.

293
00:17:34,160 --> 00:17:35,680
Like if you train one separate model

294
00:17:35,680 --> 00:17:36,800
and another separate model,

295
00:17:36,800 --> 00:17:39,160
there's nothing constraining them

296
00:17:39,160 --> 00:17:41,960
so that you can mine all of the data against each other.

297
00:17:41,960 --> 00:17:43,760
And so one of the things we found

298
00:17:43,760 --> 00:17:46,640
is that by starting everything from the same teacher model

299
00:17:46,640 --> 00:17:48,280
and trying to use this cosine loss

300
00:17:48,280 --> 00:17:50,520
to minimize the distance between embeddings,

301
00:17:50,520 --> 00:17:52,960
you are able to have this constrained space

302
00:17:52,960 --> 00:17:56,080
where you can mine every language against every other,

303
00:17:56,080 --> 00:17:58,840
even if you have different student models.

304
00:17:58,840 --> 00:18:01,760
And so this graph on the Y axis,

305
00:18:01,760 --> 00:18:05,040
it shows the error rate of mining.

306
00:18:05,040 --> 00:18:06,600
And so lower is better.

307
00:18:06,600 --> 00:18:07,840
And on the X axis,

308
00:18:07,840 --> 00:18:10,040
it shows a bunch of different low resource languages.

309
00:18:10,040 --> 00:18:11,760
So for example, the first one is Urdu,

310
00:18:11,760 --> 00:18:14,120
the second one is Telugu,

311
00:18:14,120 --> 00:18:16,080
third one is Tagalog, and so on.

312
00:18:16,120 --> 00:18:19,760
And so the gray bar here is the original laser paper.

313
00:18:19,760 --> 00:18:22,680
So this is a paper we put out maybe in 2018-ish

314
00:18:22,680 --> 00:18:24,280
and we had all of these languages

315
00:18:24,280 --> 00:18:25,720
with count of them as included.

316
00:18:25,720 --> 00:18:26,560
But as you can see,

317
00:18:26,560 --> 00:18:29,200
the error rate is extremely, extremely high

318
00:18:29,200 --> 00:18:30,200
for these languages.

319
00:18:30,200 --> 00:18:32,320
So even though they were included,

320
00:18:32,320 --> 00:18:34,520
couldn't really be used for high quality.

321
00:18:34,520 --> 00:18:37,880
And the blue bar is the laser model that we trained

322
00:18:37,880 --> 00:18:40,760
based on the technique I just described in the previous slide.

323
00:18:40,760 --> 00:18:42,400
And you can see that I think the most important point

324
00:18:42,400 --> 00:18:44,160
is that you can barely see the blue bars.

325
00:18:44,160 --> 00:18:45,680
So it was very effective

326
00:18:45,680 --> 00:18:47,480
even for these previous languages

327
00:18:47,480 --> 00:18:50,800
that people had thought we had previously embedded.

328
00:18:51,680 --> 00:18:53,560
And then so now how does this kind of thing

329
00:18:53,560 --> 00:18:56,680
fit into a whole data pipeline around this approach?

330
00:18:56,680 --> 00:18:58,960
So one of the most important things

331
00:18:58,960 --> 00:19:01,480
is when you download the data from the web,

332
00:19:01,480 --> 00:19:04,280
you don't really know what language it's in.

333
00:19:04,280 --> 00:19:07,320
And so this is part of all of the large scale data cleaning

334
00:19:07,320 --> 00:19:10,480
that goes into training large language models today.

335
00:19:10,480 --> 00:19:13,640
And so the way we identify different languages

336
00:19:13,760 --> 00:19:15,560
is through like simple classification models

337
00:19:15,560 --> 00:19:18,280
called language identification models.

338
00:19:18,280 --> 00:19:20,760
And I think it's a classification model.

339
00:19:20,760 --> 00:19:24,480
And so people think it's easier than it actually is.

340
00:19:24,480 --> 00:19:26,480
But I think some of the major challenges

341
00:19:26,480 --> 00:19:29,120
are that there's so many different languages.

342
00:19:29,120 --> 00:19:30,880
They're written in many different ways

343
00:19:30,880 --> 00:19:33,840
and web text is very casual.

344
00:19:33,840 --> 00:19:35,120
And so it can be very difficult

345
00:19:35,120 --> 00:19:37,440
to actually train a good classification model

346
00:19:37,440 --> 00:19:38,960
that can generalize to them.

347
00:19:38,960 --> 00:19:40,440
And so what we did is,

348
00:19:40,440 --> 00:19:43,520
we had our LID training data

349
00:19:43,520 --> 00:19:47,400
and we produced a language identification model LID.

350
00:19:47,400 --> 00:19:49,720
And then we actually did human evaluation

351
00:19:49,720 --> 00:19:52,560
to label errors coming from the LID system

352
00:19:52,560 --> 00:19:55,760
to iteratively improve this on web text itself

353
00:19:55,760 --> 00:19:58,720
to improve the quality of this specific model.

354
00:19:58,720 --> 00:20:00,560
Then after we produce this LID model,

355
00:20:00,560 --> 00:20:02,520
then we insert like all of our common crawl

356
00:20:02,520 --> 00:20:04,440
where the web arrow is coming in

357
00:20:04,440 --> 00:20:06,760
and we do a ton of filtering and cleaning.

358
00:20:06,760 --> 00:20:09,280
And this produces a huge corpus of different

359
00:20:09,320 --> 00:20:11,440
monolingual data that you can then use

360
00:20:11,440 --> 00:20:13,880
for training anything.

361
00:20:13,880 --> 00:20:16,520
Afterwards, we train our encoder,

362
00:20:16,520 --> 00:20:18,200
what I described on the previous text,

363
00:20:18,200 --> 00:20:20,280
and then we convert this monolingual data

364
00:20:20,280 --> 00:20:22,440
into what we call mined by texts.

365
00:20:22,440 --> 00:20:24,520
So these are a huge data set of things

366
00:20:24,520 --> 00:20:27,440
that we think are translations of each other.

367
00:20:27,440 --> 00:20:30,680
And then finally, what we do is we actually try to validate

368
00:20:30,680 --> 00:20:32,920
that these are real mined by texts

369
00:20:32,920 --> 00:20:36,360
by training very small bilingual multilingual,

370
00:20:36,360 --> 00:20:38,320
sorry bilingual translation models

371
00:20:38,360 --> 00:20:40,880
in order to see what the quality is like.

372
00:20:40,880 --> 00:20:41,880
And I think this is important

373
00:20:41,880 --> 00:20:44,840
because the data development cycle

374
00:20:44,840 --> 00:20:47,680
and the end task that it's being used for,

375
00:20:47,680 --> 00:20:51,120
you don't want to completely separate it.

376
00:20:51,120 --> 00:20:53,720
An analogy to large language model training today

377
00:20:53,720 --> 00:20:56,160
is that when you're doing your pre-training,

378
00:20:56,160 --> 00:20:58,760
you don't want someone to just deliver you a data,

379
00:20:58,760 --> 00:21:00,960
like the data mix of your different data sets

380
00:21:00,960 --> 00:21:01,800
is very important.

381
00:21:01,800 --> 00:21:03,240
And it's pretty similar here.

382
00:21:04,480 --> 00:21:07,520
And I think one of the highlights that we did here

383
00:21:07,520 --> 00:21:10,120
is really focused on the human evaluation

384
00:21:10,120 --> 00:21:12,240
of the language identification model

385
00:21:12,240 --> 00:21:13,880
because that actually improves the quality

386
00:21:13,880 --> 00:21:15,640
of all of the underlying data

387
00:21:15,640 --> 00:21:18,320
if you just more accurately know what language it's in.

388
00:21:19,600 --> 00:21:21,240
And this entire data pipeline

389
00:21:21,240 --> 00:21:23,040
is actually open source in this library

390
00:21:23,040 --> 00:21:25,480
and we had an MNLP paper describing it.

391
00:21:25,480 --> 00:21:27,200
The reason why I thought this was important

392
00:21:27,200 --> 00:21:29,240
is that because I think data cleaning

393
00:21:29,240 --> 00:21:31,560
is actually such a fundamental underlying thing

394
00:21:31,560 --> 00:21:34,880
that drives model quality and people's data pipelines.

395
00:21:34,880 --> 00:21:36,640
It's like, I had this script and this other thing

396
00:21:37,640 --> 00:21:39,520
and so it's actually, I think very important

397
00:21:39,520 --> 00:21:42,240
to be able to recreate it and rerun it

398
00:21:42,240 --> 00:21:45,120
as part of almost like your research

399
00:21:45,120 --> 00:21:47,520
that you would do as follow-up work.

400
00:21:47,520 --> 00:21:49,520
And so that's why we open sourced it.

401
00:21:50,560 --> 00:21:52,400
A few reflection things.

402
00:21:53,560 --> 00:21:54,920
For low resource languages,

403
00:21:54,920 --> 00:21:57,120
even though we did a large scale mining,

404
00:21:57,120 --> 00:21:59,400
I think monolingual data is the limiting factor.

405
00:21:59,400 --> 00:22:01,840
Like there are many languages that do not have

406
00:22:01,840 --> 00:22:04,720
like a huge amount of text written online.

407
00:22:04,800 --> 00:22:08,240
And so it can be very challenging to get a large amount.

408
00:22:08,240 --> 00:22:10,640
Further, I think languages and unique scripts

409
00:22:10,640 --> 00:22:14,240
can be extremely hard to get good representations of

410
00:22:14,240 --> 00:22:16,440
if you don't have very much data.

411
00:22:16,440 --> 00:22:17,800
There are certain languages as well

412
00:22:17,800 --> 00:22:20,440
where they were historically written in a new script

413
00:22:20,440 --> 00:22:22,640
but now the government would like to write it

414
00:22:23,840 --> 00:22:26,720
in a totally new one like the old cheeky script, for example.

415
00:22:26,720 --> 00:22:30,200
And so there's not a lot of content to represent these scripts.

416
00:22:30,200 --> 00:22:32,520
So it's hard to learn representations.

417
00:22:32,520 --> 00:22:35,760
And then further, a lot of the content we create,

418
00:22:35,760 --> 00:22:38,760
it's even after mining, it's a fairly limited domain,

419
00:22:38,760 --> 00:22:40,320
often religious content.

420
00:22:41,520 --> 00:22:44,440
Okay, so with data discussed,

421
00:22:44,440 --> 00:22:48,480
I wanna segue a little bit into some of the modeling work

422
00:22:48,480 --> 00:22:51,880
just to kind of start with like a high level picture.

423
00:22:51,880 --> 00:22:54,040
I think there's like three major challenges

424
00:22:54,040 --> 00:22:57,000
when you talk about like large scale multi-lingual modeling.

425
00:22:57,000 --> 00:23:00,760
And these pretty much apply to language models as well.

426
00:23:01,760 --> 00:23:04,200
The first one is effective data augmentation

427
00:23:04,200 --> 00:23:05,440
for low resource languages.

428
00:23:05,440 --> 00:23:09,120
Like how can you prevent the low resource language data

429
00:23:09,120 --> 00:23:11,120
from just being completely drowned out

430
00:23:11,120 --> 00:23:13,080
by the time you've seen like all of your words

431
00:23:13,080 --> 00:23:14,880
of German or Russian?

432
00:23:14,880 --> 00:23:17,520
I think there's also a question of like scalability

433
00:23:17,520 --> 00:23:18,360
of the model.

434
00:23:18,360 --> 00:23:21,120
So even if you train very large scale models,

435
00:23:21,120 --> 00:23:22,680
how do you prevent the representations

436
00:23:22,680 --> 00:23:25,640
of different languages from interfering with each other?

437
00:23:25,640 --> 00:23:27,600
And that leads to the last point as well

438
00:23:27,600 --> 00:23:30,720
of like if you give the model very limited capacity,

439
00:23:30,720 --> 00:23:32,520
then of course it may not have the capacity

440
00:23:32,520 --> 00:23:35,000
to model all of these different languages.

441
00:23:35,000 --> 00:23:38,320
And so you also need to accelerate the scale of the model.

442
00:23:39,280 --> 00:23:42,000
And so preliminary for those

443
00:23:43,360 --> 00:23:45,680
who may not have seen a translation system before,

444
00:23:45,680 --> 00:23:48,040
I don't know how many of you that practically is.

445
00:23:48,040 --> 00:23:50,640
So we use standard sequence-to-sequence models.

446
00:23:50,640 --> 00:23:52,640
So the input text, the like coral thing

447
00:23:52,640 --> 00:23:54,200
is like what you wanna translate

448
00:23:54,200 --> 00:23:56,760
and there's a transformer decoder model

449
00:23:56,760 --> 00:23:58,520
that then with a tension mechanism

450
00:23:58,520 --> 00:24:00,560
goes to a transformer decoder model.

451
00:24:00,560 --> 00:24:03,080
And then it decodes autoregressively

452
00:24:03,080 --> 00:24:05,920
the actual translation, which you can see here in yellow.

453
00:24:06,960 --> 00:24:10,000
And so I wanna talk a little bit about like how

454
00:24:10,000 --> 00:24:12,680
the data looks as we feed it into the models.

455
00:24:12,680 --> 00:24:13,880
So there's a few different ways

456
00:24:13,880 --> 00:24:15,480
that you might wanna think about data.

457
00:24:15,480 --> 00:24:18,440
So you wanna be like, okay, did a human look at it

458
00:24:18,440 --> 00:24:21,200
and decide that like these two sentences are translations

459
00:24:21,200 --> 00:24:22,560
or are they noisy?

460
00:24:22,560 --> 00:24:25,280
Also, is it limited in size?

461
00:24:25,280 --> 00:24:26,680
Another thing you can think about is like

462
00:24:26,680 --> 00:24:29,640
is the data quality dependent on some other factor?

463
00:24:29,640 --> 00:24:31,280
And so that's like the model dependent thing

464
00:24:31,280 --> 00:24:33,680
in which case like the data quality may be capped

465
00:24:33,680 --> 00:24:35,360
by the quality of that dependency.

466
00:24:36,840 --> 00:24:38,920
And so I think you can think a little bit

467
00:24:38,920 --> 00:24:40,120
like the ideal data set.

468
00:24:40,120 --> 00:24:43,560
It would be like humans have reviewed every bit of it.

469
00:24:43,560 --> 00:24:44,880
It's not noisy at all.

470
00:24:44,880 --> 00:24:46,800
We have an infinite amount

471
00:24:46,800 --> 00:24:49,480
and it doesn't have any dependencies on any other models.

472
00:24:49,480 --> 00:24:51,240
It's just like pure quality.

473
00:24:51,240 --> 00:24:54,480
But in reality, like closer to what we have are these.

474
00:24:54,480 --> 00:24:56,560
So we have a bunch of different data sources.

475
00:24:56,560 --> 00:24:58,440
We have the seed data that I discussed

476
00:24:58,440 --> 00:25:00,160
like way back in the talk

477
00:25:00,160 --> 00:25:03,200
where it's a small amount of like really high quality

478
00:25:03,200 --> 00:25:04,480
human aligned data.

479
00:25:04,480 --> 00:25:06,880
But the only problem is that it's limited in size.

480
00:25:06,880 --> 00:25:09,720
It's like 6,000 sentences per language.

481
00:25:09,720 --> 00:25:11,480
We have the public by text.

482
00:25:11,480 --> 00:25:13,680
So this is data that people have created

483
00:25:13,680 --> 00:25:15,560
over many years of working in translation.

484
00:25:15,560 --> 00:25:18,240
You know, you can download it from like the opus corpus

485
00:25:18,240 --> 00:25:22,360
for example, mostly has not been reviewed by humans.

486
00:25:22,360 --> 00:25:24,360
So pretty extremely noisy.

487
00:25:24,360 --> 00:25:26,880
In many languages, it's just coming from the Bible.

488
00:25:26,880 --> 00:25:28,760
So the size is quite limited.

489
00:25:28,760 --> 00:25:30,280
You have our mind data.

490
00:25:31,520 --> 00:25:33,640
So this is not human aligned either.

491
00:25:34,640 --> 00:25:37,080
And but it does have a model dependency, you know,

492
00:25:37,080 --> 00:25:39,720
it's dependent on the quality of the sentence encoders.

493
00:25:39,720 --> 00:25:43,720
And we have two other sources of data from back translation.

494
00:25:43,720 --> 00:25:45,520
So the idea of back translation,

495
00:25:45,520 --> 00:25:47,240
it's a model augmentation technique

496
00:25:47,240 --> 00:25:49,160
heavily used in machine translation

497
00:25:49,160 --> 00:25:52,480
where you use a model to produce like pseudo translations

498
00:25:52,480 --> 00:25:53,560
like silver data.

499
00:25:54,400 --> 00:25:55,960
And we use two different techniques

500
00:25:55,960 --> 00:25:57,720
to produce these back translations

501
00:25:57,720 --> 00:26:00,360
that also are dependent on the underlying model

502
00:26:00,360 --> 00:26:02,400
used to make the translations.

503
00:26:02,400 --> 00:26:04,200
And so this is a picture of like our high level

504
00:26:04,200 --> 00:26:05,120
of different data sources

505
00:26:05,120 --> 00:26:07,240
and like how you wanna think about the quality

506
00:26:07,240 --> 00:26:09,000
and the different axes.

507
00:26:09,000 --> 00:26:11,040
And so if we put them all together, what do we get?

508
00:26:11,040 --> 00:26:14,800
So the Y axis here is the number of training pairs

509
00:26:14,800 --> 00:26:18,280
and the X axis here is the language is sorted by resource.

510
00:26:18,280 --> 00:26:20,520
So you can see like on the left hand side,

511
00:26:20,520 --> 00:26:22,920
you have your low resource languages like Wolof

512
00:26:22,920 --> 00:26:24,680
and on your right hand side,

513
00:26:24,680 --> 00:26:27,240
you've got your high resource languages like French.

514
00:26:27,240 --> 00:26:29,640
The peak is English, of course.

515
00:26:29,640 --> 00:26:32,280
And so if you just look at what's available publicly,

516
00:26:32,280 --> 00:26:34,160
this is a distribution you get.

517
00:26:34,160 --> 00:26:38,120
And you'll see like a huge, huge fall off pretty quickly.

518
00:26:38,120 --> 00:26:41,480
And then if you add in the data that we have created

519
00:26:41,480 --> 00:26:43,120
for mining and back translation,

520
00:26:43,120 --> 00:26:45,800
our goal is basically to like make the distribution

521
00:26:45,800 --> 00:26:47,600
a little bit more uniform.

522
00:26:47,600 --> 00:26:51,440
It's very hard on the extremely low resource side, of course,

523
00:26:51,440 --> 00:26:53,320
but to make it a little bit more uniform

524
00:26:53,320 --> 00:26:55,360
so that you don't just immediately, you know,

525
00:26:55,360 --> 00:26:57,360
overfit on your low resource languages

526
00:26:57,360 --> 00:26:59,600
before you've even seen like three shards

527
00:26:59,600 --> 00:27:00,600
of your German data.

528
00:27:02,320 --> 00:27:04,960
With that kind of data strategy in mind,

529
00:27:04,960 --> 00:27:08,440
I wanna talk a little bit about mixture of experts.

530
00:27:08,440 --> 00:27:11,200
So this is something that we explored quite aggressively

531
00:27:11,200 --> 00:27:14,440
in the translation space for a number of years.

532
00:27:14,440 --> 00:27:16,400
You know, we could have this equal conversation

533
00:27:16,400 --> 00:27:17,760
about some of the debates going on

534
00:27:17,760 --> 00:27:20,360
on like, do you want sparse or dense architectures

535
00:27:20,360 --> 00:27:22,200
for large language models?

536
00:27:22,200 --> 00:27:26,000
But essentially mixture of experts, it enables massive scale

537
00:27:26,000 --> 00:27:28,040
because you don't have to just scale

538
00:27:28,040 --> 00:27:30,560
like you're kind of your dense trunk model,

539
00:27:30,560 --> 00:27:33,000
but you can have like a bunch of different separate experts

540
00:27:33,000 --> 00:27:34,520
that you activate per token.

541
00:27:36,040 --> 00:27:38,880
It also allows you to avoid language interference

542
00:27:38,880 --> 00:27:41,000
because the idea is that the different experts,

543
00:27:41,000 --> 00:27:44,160
they could specialize to specific languages.

544
00:27:44,160 --> 00:27:46,320
Unfortunately, it adds a ton of capacity

545
00:27:46,320 --> 00:27:49,120
so it becomes pretty easy to overfit.

546
00:27:49,120 --> 00:27:52,160
So I wanna talk a little bit about this overfitting

547
00:27:52,160 --> 00:27:53,000
phenomenon.

548
00:27:53,000 --> 00:27:56,360
So the top set of graphs that we're gonna talk about

549
00:27:56,360 --> 00:27:58,960
is for the language Congo

550
00:27:58,960 --> 00:28:01,880
and then the bottom set of languages is French.

551
00:28:01,880 --> 00:28:04,240
So you really wanna compare like a low resource language

552
00:28:04,240 --> 00:28:07,120
on top with a high resource language on bottom.

553
00:28:07,120 --> 00:28:08,920
So if you just take your dense model,

554
00:28:08,920 --> 00:28:11,640
traditional transformer sequence to sequence architecture,

555
00:28:11,640 --> 00:28:13,720
that's the graph that you're showing, right?

556
00:28:13,720 --> 00:28:15,560
So there's a little bit of overfitting

557
00:28:15,560 --> 00:28:17,040
on the low resource language,

558
00:28:17,040 --> 00:28:18,840
but you can pretty much regularize this

559
00:28:18,840 --> 00:28:20,720
with standard dropout techniques, right?

560
00:28:20,720 --> 00:28:23,040
So there's not a big problem and on French,

561
00:28:23,040 --> 00:28:25,080
you basically have no real problem.

562
00:28:26,160 --> 00:28:28,920
However, the minute you switch from like a dense architecture

563
00:28:28,920 --> 00:28:31,600
to a token level MOE architecture,

564
00:28:31,600 --> 00:28:34,120
you just have experienced a massive overfitting

565
00:28:34,120 --> 00:28:35,440
on the low resource language.

566
00:28:35,440 --> 00:28:38,440
So the green line here is like just demonstrating

567
00:28:38,440 --> 00:28:40,160
without dropout the overfitting.

568
00:28:40,160 --> 00:28:42,000
And then if you add dropout,

569
00:28:42,000 --> 00:28:43,800
you get a little bit better performance,

570
00:28:43,800 --> 00:28:45,880
but it's still overfitting quite a bit.

571
00:28:45,880 --> 00:28:49,640
Like essentially by like 12K updates,

572
00:28:49,640 --> 00:28:51,560
there's no real point in continuing training,

573
00:28:51,560 --> 00:28:53,600
like you're burning GPU basically.

574
00:28:54,520 --> 00:28:56,760
And so one of the things we actually worked on quite a bit

575
00:28:56,760 --> 00:28:58,000
was like trying to figure out

576
00:28:58,000 --> 00:29:01,920
how to properly regularize these MOE architectures

577
00:29:01,920 --> 00:29:04,160
with this specific masking technique

578
00:29:04,160 --> 00:29:07,520
on the gating function that decides like which MOE to route,

579
00:29:07,520 --> 00:29:10,240
sorry, which expert to route to and your MOE architecture

580
00:29:10,240 --> 00:29:13,400
to just try to pull back some of this overfitting effect.

581
00:29:13,400 --> 00:29:17,240
So if you look in the top right graph, the purple line,

582
00:29:17,440 --> 00:29:21,640
you still see some successful regularization.

583
00:29:22,960 --> 00:29:26,800
Another thing that we did to control the overfitting effect

584
00:29:26,800 --> 00:29:29,560
that's actually quite being used in language models today

585
00:29:29,560 --> 00:29:31,880
as well is curriculum learning.

586
00:29:31,880 --> 00:29:33,720
And the idea of this is like,

587
00:29:33,720 --> 00:29:37,320
how are we going to stage when languages are introduced?

588
00:29:37,320 --> 00:29:40,680
And so what we did was we tried to train a vanilla model

589
00:29:40,680 --> 00:29:41,960
and then we started to measure

590
00:29:41,960 --> 00:29:44,120
when the languages begin to overfit.

591
00:29:44,120 --> 00:29:47,680
And then we basically bucket them into different sections.

592
00:29:47,680 --> 00:29:50,200
And so for high resource languages like French,

593
00:29:50,200 --> 00:29:51,440
you want to start it early

594
00:29:51,440 --> 00:29:53,560
and it needs to be trained the entire way.

595
00:29:53,560 --> 00:29:56,400
But for a lower resource language like Wolof,

596
00:29:56,400 --> 00:29:59,480
after maybe like a hundred K updates, it's done.

597
00:29:59,480 --> 00:30:01,920
So the rest of the time is just overfitting.

598
00:30:01,920 --> 00:30:03,840
And so it actually gets worse the more you train it.

599
00:30:03,840 --> 00:30:06,000
So what we did is we moved some of those lower resource

600
00:30:06,000 --> 00:30:08,480
languages and we inserted them much later

601
00:30:08,480 --> 00:30:09,920
into the training schedule.

602
00:30:09,920 --> 00:30:11,600
So you start training your high resource,

603
00:30:11,600 --> 00:30:13,640
then you start training your low, your mid resource,

604
00:30:13,640 --> 00:30:16,080
and then your low resource, and then your very low resource.

605
00:30:16,080 --> 00:30:19,440
And so by the end, everything in theory has trained

606
00:30:19,440 --> 00:30:21,560
and is not as overfit as it would be

607
00:30:21,560 --> 00:30:23,040
without this kind of technique.

608
00:30:24,120 --> 00:30:25,600
So I want to show some results.

609
00:30:25,600 --> 00:30:28,880
So first I want to show results on existing datasets.

610
00:30:28,880 --> 00:30:30,800
So before we get to 200 languages,

611
00:30:30,800 --> 00:30:33,080
like let's just talk about 100 languages.

612
00:30:33,080 --> 00:30:35,800
And so this is the Flores 101 DevTest.

613
00:30:35,800 --> 00:30:37,360
It's important to compare to this

614
00:30:37,360 --> 00:30:39,720
because this is where like existing benchmarks

615
00:30:39,720 --> 00:30:41,320
in the community lie.

616
00:30:41,320 --> 00:30:43,600
Whereas on 200, of course, we can put up anything.

617
00:30:44,480 --> 00:30:46,240
Because it's the first work on that.

618
00:30:46,240 --> 00:30:51,120
So the first column is translating out of English.

619
00:30:51,120 --> 00:30:54,840
So English to Chinese, English to Icelandic, anything like that.

620
00:30:54,840 --> 00:30:57,320
The second column is translating into English.

621
00:30:57,320 --> 00:30:58,880
So Chinese to English.

622
00:30:58,880 --> 00:31:03,160
The third column, XXYY, it's translating any cross pair

623
00:31:03,160 --> 00:31:04,640
are not involving English.

624
00:31:04,640 --> 00:31:06,720
And the last column is the average.

625
00:31:06,720 --> 00:31:08,720
So if you look at the first set of rows,

626
00:31:08,720 --> 00:31:12,440
this is a comparison on models that cover 87 different languages.

627
00:31:12,440 --> 00:31:14,720
So there was this paper MTAM 100.

628
00:31:14,720 --> 00:31:16,680
There was also this deep net paper.

629
00:31:16,680 --> 00:31:18,840
So you can see the average blue score.

630
00:31:18,840 --> 00:31:20,880
Blue is a standard translation metric,

631
00:31:20,880 --> 00:31:23,600
essentially a metric of word overlap.

632
00:31:23,600 --> 00:31:26,040
So we're looking at blue score here.

633
00:31:26,040 --> 00:31:29,520
And so you can see the last row NLB 200.

634
00:31:29,520 --> 00:31:31,680
Even though we cover 200 languages,

635
00:31:31,680 --> 00:31:33,920
the blue score is substantially above

636
00:31:33,920 --> 00:31:35,360
some of the existing work.

637
00:31:35,360 --> 00:31:37,480
Now, if we look at 101 languages,

638
00:31:37,480 --> 00:31:40,320
only the Delta LM paper from Microsoft at the time

639
00:31:40,320 --> 00:31:42,280
covered that number of languages.

640
00:31:42,280 --> 00:31:45,720
And so if you compare on all of the different cross sets,

641
00:31:45,720 --> 00:31:49,240
similarly, you see that there's no language left behind model

642
00:31:49,240 --> 00:31:51,840
is much stronger in terms of blue.

643
00:31:51,840 --> 00:31:54,600
One thing really quick on the variance of these blue numbers,

644
00:31:54,600 --> 00:31:56,400
I think it's important to understand

645
00:31:56,400 --> 00:31:58,400
is something statistically significant or not.

646
00:31:58,400 --> 00:32:03,400
I think about 0.5 blue is kind of like the general plus

647
00:32:03,400 --> 00:32:04,520
minus that you'll see.

648
00:32:04,520 --> 00:32:07,840
And so if it's above that, it's usually

649
00:32:07,840 --> 00:32:11,480
a statistically significant metric improvement.

650
00:32:11,680 --> 00:32:14,840
So now I want to talk a little bit about Flora's 200 results.

651
00:32:14,840 --> 00:32:17,400
So here's similar, like the first chunk of columns

652
00:32:17,400 --> 00:32:19,320
translating out of English, then

653
00:32:19,320 --> 00:32:21,440
next chunk is translating into English,

654
00:32:21,440 --> 00:32:25,560
then you have your cross pairs, and then you have your average.

655
00:32:25,560 --> 00:32:27,880
So we have this blue metric as well.

656
00:32:27,880 --> 00:32:33,200
We also have a character level metric based on CHRF++

657
00:32:33,200 --> 00:32:35,640
that's commonly used in the translation community.

658
00:32:35,640 --> 00:32:37,440
So I think looking at these numbers, of course,

659
00:32:37,440 --> 00:32:40,880
there's no baseline work to compare to on the previous slide.

660
00:32:40,920 --> 00:32:43,920
And so when we get to human evaluation in a little bit,

661
00:32:43,920 --> 00:32:45,560
it'll be more concrete.

662
00:32:45,560 --> 00:32:48,960
But I think generally one of the rules of thumb

663
00:32:48,960 --> 00:32:51,920
I have for these types of numbers is around 30

664
00:32:51,920 --> 00:32:57,000
is pretty reasonably becomes usable.

665
00:32:57,000 --> 00:33:00,960
And I think another thing, if you compare these supervised pairs

666
00:33:00,960 --> 00:33:04,600
to zero shot pairs, I think we don't see a huge drop-off

667
00:33:04,600 --> 00:33:06,760
on zero shot, which indicates the model has

668
00:33:06,760 --> 00:33:09,280
some sort of generalization, even if it didn't see

669
00:33:09,280 --> 00:33:12,840
that translation pair directly during training.

670
00:33:12,840 --> 00:33:14,760
Another way to calibrate some of this

671
00:33:14,760 --> 00:33:17,000
is to compare to Google Translate.

672
00:33:17,000 --> 00:33:19,320
And so if you compare to Google Translate,

673
00:33:19,320 --> 00:33:21,360
no language to left behind is quite a bit better

674
00:33:21,360 --> 00:33:25,080
at translating into English and not as good as translating

675
00:33:25,080 --> 00:33:28,240
out of English, although if you like average across everything,

676
00:33:28,240 --> 00:33:31,600
it's a little bit better.

677
00:33:31,600 --> 00:33:34,240
I want to talk a little bit about human evaluation as well

678
00:33:34,240 --> 00:33:36,760
to complement some of our discussion

679
00:33:36,760 --> 00:33:38,800
on automatic evaluation.

680
00:33:38,800 --> 00:33:42,280
And so I think automatic metrics fast, really good

681
00:33:42,280 --> 00:33:44,720
for research and duration, impossible to move forward

682
00:33:44,720 --> 00:33:49,400
without, but human evaluation is really the real deal here.

683
00:33:49,400 --> 00:33:52,000
And so we had this paper at Amptox

684
00:33:52,000 --> 00:33:54,800
on how to make this human evaluation very consistent

685
00:33:54,800 --> 00:33:57,600
and scalable across different language pairs.

686
00:33:57,600 --> 00:34:00,680
I think this goes back to the kind of evaluation data set

687
00:34:00,680 --> 00:34:03,480
point that I was making at the beginning of the talk, where

688
00:34:03,480 --> 00:34:06,000
if you're a professional German translator,

689
00:34:06,000 --> 00:34:08,400
you're really good at evaluating the quality of your German

690
00:34:08,440 --> 00:34:09,960
translation.

691
00:34:09,960 --> 00:34:13,960
But beyond that, there's not a lot of consistency.

692
00:34:13,960 --> 00:34:18,280
And if you evaluate translation on a five point scale,

693
00:34:18,280 --> 00:34:20,880
a five translating between two languages

694
00:34:20,880 --> 00:34:23,440
and a three translating between other two languages,

695
00:34:23,440 --> 00:34:24,960
are those really comparable?

696
00:34:24,960 --> 00:34:27,720
And so we had this entire experiment methodology

697
00:34:27,720 --> 00:34:30,600
on how we might want to make this a little bit more

698
00:34:30,600 --> 00:34:32,320
comparable.

699
00:34:32,320 --> 00:34:35,360
So I want to show some results now on this.

700
00:34:35,360 --> 00:34:37,680
So the y-axis here, so the metric

701
00:34:37,720 --> 00:34:40,360
is called XSTS, some metric for how

702
00:34:40,360 --> 00:34:42,280
we're doing this human evaluation.

703
00:34:42,280 --> 00:34:45,680
The y-axis here is actually the delta.

704
00:34:45,680 --> 00:34:48,600
So anything is a five point scale.

705
00:34:48,600 --> 00:34:52,080
So it's a delta, not the raw score.

706
00:34:52,080 --> 00:34:55,240
The x-axis here is a bunch of different translation directions

707
00:34:55,240 --> 00:34:56,680
that we evaluated.

708
00:34:56,680 --> 00:34:59,880
So the gray set is translating into English.

709
00:34:59,880 --> 00:35:03,320
The green set is translating non-English directions,

710
00:35:03,320 --> 00:35:07,120
so like French to Oluf.

711
00:35:07,120 --> 00:35:10,040
And then the blue set is translating out of English.

712
00:35:10,040 --> 00:35:13,400
And so what you're looking for is like a positive delta

713
00:35:13,400 --> 00:35:17,120
indicates that our modeling architecture is much better.

714
00:35:17,120 --> 00:35:21,640
So what the delta is between is like a baseline transformer

715
00:35:21,640 --> 00:35:24,080
model just trained on all of our data

716
00:35:24,080 --> 00:35:26,440
versus like the final no language left behind model

717
00:35:26,440 --> 00:35:27,440
that we created.

718
00:35:27,440 --> 00:35:29,680
So the data is actually the same for both of them.

719
00:35:29,680 --> 00:35:32,160
That's how we get all 200 languages.

720
00:35:32,160 --> 00:35:34,320
So we're just measuring here the human eval

721
00:35:34,320 --> 00:35:36,320
of the modeling improvements.

722
00:35:36,320 --> 00:35:41,400
As you can see, most of the delta is pretty noticeable.

723
00:35:41,400 --> 00:35:47,000
Some of them not so much like, I don't know, Zulu to English.

724
00:35:47,000 --> 00:35:48,480
We didn't seem to improve very much,

725
00:35:48,480 --> 00:35:51,040
but in general, it's an improvement detectable

726
00:35:51,040 --> 00:35:52,520
by human evaluation.

727
00:35:52,520 --> 00:35:54,680
You might also ask, OK, what is the statistically

728
00:35:54,680 --> 00:35:59,200
significant difference here between about 0.2 to 0.3

729
00:35:59,200 --> 00:36:02,640
plus or minus is something that's pretty noticeable.

730
00:36:02,640 --> 00:36:05,240
And above 0.5, it's very noticeable.

731
00:36:07,040 --> 00:36:10,480
One of the things that I also want to get at in evaluation

732
00:36:10,480 --> 00:36:15,680
is that there's many different facets of model evaluation.

733
00:36:15,680 --> 00:36:18,360
And I think if you look at all of the different LLM leader

734
00:36:18,360 --> 00:36:20,800
boards or the transparency reports or whatever,

735
00:36:20,800 --> 00:36:23,520
you'll begin to internalize this pretty quickly.

736
00:36:23,520 --> 00:36:26,080
But what we just looked at are just very high level

737
00:36:26,080 --> 00:36:27,360
summary numbers.

738
00:36:27,360 --> 00:36:30,360
And they don't really tell you what exactly are the errors

739
00:36:30,360 --> 00:36:32,520
and is it ultimately usable by people?

740
00:36:32,520 --> 00:36:35,400
Is it a safe thing that people can rely on?

741
00:36:35,440 --> 00:36:37,720
And so one of the things we really focused on

742
00:36:37,720 --> 00:36:39,520
is user safety.

743
00:36:39,520 --> 00:36:43,040
And some of that manifests in some of the toxicity work

744
00:36:43,040 --> 00:36:44,040
that we did.

745
00:36:44,040 --> 00:36:47,520
And the driving thing here is that not all errors in translation

746
00:36:47,520 --> 00:36:48,520
are made equal.

747
00:36:48,520 --> 00:36:50,800
So during COVID, there was this one that was really

748
00:36:50,800 --> 00:36:52,920
went viral circulating around.

749
00:36:52,920 --> 00:36:55,680
But the message during COVID is you've got to wash your hands.

750
00:36:55,680 --> 00:36:58,360
But the translation producer is like, you've got to hold hands,

751
00:36:58,360 --> 00:37:02,160
which I think is exactly the opposite of what you want to do.

752
00:37:02,160 --> 00:37:03,800
And other types of measurement errors

753
00:37:03,840 --> 00:37:05,560
are really important as well.

754
00:37:05,560 --> 00:37:08,240
So if you're telling someone how far they want to go,

755
00:37:08,240 --> 00:37:10,760
and you're like, hey, you want to travel five kilometers,

756
00:37:10,760 --> 00:37:13,800
and then your translation is like travel 500 kilometers,

757
00:37:13,800 --> 00:37:16,800
it's a completely different type of issue.

758
00:37:16,800 --> 00:37:18,960
And so what we did for toxicity, which

759
00:37:18,960 --> 00:37:22,200
is a big focus for this work, is that we collected different

760
00:37:22,200 --> 00:37:25,720
toxicity lists for all 200 languages.

761
00:37:25,720 --> 00:37:27,880
And so why do I care so much about toxicity?

762
00:37:27,880 --> 00:37:29,720
I think it's a user safety thing.

763
00:37:29,720 --> 00:37:32,560
So if you input some perfectly benign text,

764
00:37:32,600 --> 00:37:34,440
and then the output is profanity,

765
00:37:34,440 --> 00:37:36,720
I think it's just really unexpected.

766
00:37:36,720 --> 00:37:39,120
And it breaks a lot of trust in the system.

767
00:37:39,120 --> 00:37:42,440
And it's an extremely poor experience for people.

768
00:37:42,440 --> 00:37:45,240
That being said, it's also a very, very challenging thing,

769
00:37:45,240 --> 00:37:47,760
because it's extremely culturally specific.

770
00:37:47,760 --> 00:37:52,360
So things that are slurs or insults in certain languages,

771
00:37:52,360 --> 00:37:55,800
they don't really generalize across cultures,

772
00:37:55,800 --> 00:37:57,680
which means that things like this

773
00:37:57,680 --> 00:38:00,280
are very challenging to create.

774
00:38:00,280 --> 00:38:02,240
And I also was very interested in this direction,

775
00:38:02,240 --> 00:38:04,880
because I think it's broadly useful for all sorts

776
00:38:04,880 --> 00:38:07,320
of different type of detection things

777
00:38:07,320 --> 00:38:09,440
that you need to do, and also mitigation.

778
00:38:09,440 --> 00:38:11,000
And so even though we develop this

779
00:38:11,000 --> 00:38:12,680
in the context of translation,

780
00:38:12,680 --> 00:38:16,720
it can be used very broadly in other types of NLP applications.

781
00:38:17,880 --> 00:38:20,000
This is also open source, you can download it.

782
00:38:20,000 --> 00:38:21,800
You have to type in a little password

783
00:38:21,800 --> 00:38:23,520
that's in the GitHub repo,

784
00:38:23,520 --> 00:38:25,400
just so that you don't accidentally download

785
00:38:25,400 --> 00:38:27,600
and realize you have files of curse words

786
00:38:27,600 --> 00:38:29,120
all over your computer.

787
00:38:30,120 --> 00:38:31,800
Okay, so I wanna end a little bit

788
00:38:31,800 --> 00:38:34,320
with some thoughts about future directions.

789
00:38:34,320 --> 00:38:36,080
And before I get there,

790
00:38:36,080 --> 00:38:39,800
there's like a 190 page paper that writes up

791
00:38:39,800 --> 00:38:42,240
all of this in far greater detail,

792
00:38:42,240 --> 00:38:43,720
in case you're curious.

793
00:38:45,000 --> 00:38:46,920
So a few future directions

794
00:38:46,920 --> 00:38:49,000
that I think I'm really interested in,

795
00:38:49,000 --> 00:38:51,320
and some of these are also very applicable

796
00:38:51,320 --> 00:38:52,760
to things like speech,

797
00:38:52,760 --> 00:38:57,440
is that I think one of them is more explicit multilingual.

798
00:38:57,720 --> 00:39:00,360
So I think a lot of approaches to multilingual

799
00:39:00,360 --> 00:39:01,920
have been like, hey,

800
00:39:01,920 --> 00:39:04,320
we have this thing that's working well for one language,

801
00:39:04,320 --> 00:39:06,480
like let's try to scale it

802
00:39:06,480 --> 00:39:07,880
to a bunch of different languages,

803
00:39:07,880 --> 00:39:08,960
and then we're gonna put them all

804
00:39:08,960 --> 00:39:10,200
in the same modeling bucket,

805
00:39:10,200 --> 00:39:12,760
and just kind of like hope that the model learns

806
00:39:12,760 --> 00:39:14,760
all of these different representations.

807
00:39:14,760 --> 00:39:17,040
But I think there's a lot of potential room

808
00:39:17,040 --> 00:39:20,280
for explicitly bringing in,

809
00:39:20,280 --> 00:39:22,160
like the fact that you know it's multilingual

810
00:39:22,160 --> 00:39:25,320
into the architecture more.

811
00:39:25,320 --> 00:39:27,760
And so, you know,

812
00:39:27,760 --> 00:39:32,120
it's possible to capture more nuances between languages

813
00:39:32,120 --> 00:39:34,440
or different relationships between languages.

814
00:39:35,600 --> 00:39:38,080
And the other one is continued support for everyone.

815
00:39:38,080 --> 00:39:40,960
I think it's like something reflecting on this project

816
00:39:40,960 --> 00:39:43,520
is that, you know, going from 100 to 200

817
00:39:43,520 --> 00:39:45,400
was already pretty challenging,

818
00:39:45,400 --> 00:39:47,320
but going beyond a lot of the techniques

819
00:39:47,320 --> 00:39:48,560
that we developed here

820
00:39:48,560 --> 00:39:51,240
are not necessarily that scalable.

821
00:39:51,240 --> 00:39:53,320
This is actually what inspired some of our work

822
00:39:53,320 --> 00:39:55,280
on speech translation as well.

823
00:39:55,280 --> 00:39:58,360
So if you recently saw like the seamless M4T release

824
00:39:58,360 --> 00:39:59,920
or like the unwritten languages,

825
00:39:59,920 --> 00:40:01,840
like we did a lot of modeling of Hokeum,

826
00:40:01,840 --> 00:40:04,720
and I think that goes into this direction really well,

827
00:40:04,720 --> 00:40:07,680
because many of the languages that people want to use

828
00:40:07,680 --> 00:40:09,800
are like spoken first languages

829
00:40:09,800 --> 00:40:12,000
and not necessarily like primarily written.

830
00:40:13,080 --> 00:40:14,240
And then I think the last thing

831
00:40:14,240 --> 00:40:15,960
that I'm still really passionate about

832
00:40:15,960 --> 00:40:18,800
is like continued increase ease of use

833
00:40:18,800 --> 00:40:20,280
and training of these models

834
00:40:20,280 --> 00:40:23,000
and like democratization for the community.

835
00:40:23,000 --> 00:40:25,440
So one of the things that we tried to do in this work

836
00:40:25,440 --> 00:40:28,080
is just like really, really clearly write down

837
00:40:28,080 --> 00:40:30,200
everything that we did and like open source,

838
00:40:30,200 --> 00:40:32,840
like even the data pipeline and things like that.

839
00:40:32,840 --> 00:40:34,520
And so that's where you get like all of the repos

840
00:40:34,520 --> 00:40:38,040
that I linked and, you know, like a huge write up.

841
00:40:38,040 --> 00:40:40,120
But I think if someone were to try to reproduce this

842
00:40:40,120 --> 00:40:42,080
for their own language, and many people have,

843
00:40:42,080 --> 00:40:44,200
like I'm not saying that that hasn't been,

844
00:40:44,200 --> 00:40:46,520
but it's like, if you wanted to like do this,

845
00:40:46,520 --> 00:40:49,480
it would be extremely, extremely hard

846
00:40:49,480 --> 00:40:52,400
because there's just like so much different things going on.

847
00:40:52,400 --> 00:40:54,440
So I think most of the, what we've seen is like,

848
00:40:54,440 --> 00:40:55,960
people have downloaded the base model

849
00:40:55,960 --> 00:40:58,040
and fine-tuned it for their own language,

850
00:40:58,040 --> 00:41:00,960
but it's pretty hard to just like add on

851
00:41:00,960 --> 00:41:03,200
many, many more languages to this system

852
00:41:03,200 --> 00:41:06,000
because of how complicated all of the moving parts are.

853
00:41:06,000 --> 00:41:08,920
And so I feel like something for the translation community

854
00:41:08,920 --> 00:41:13,320
overall is like, how do we simplify a lot of these things?

855
00:41:13,320 --> 00:41:16,000
And I think that's where like a lot of fundamental modeling

856
00:41:16,000 --> 00:41:19,040
innovation could help us get to.

857
00:41:19,040 --> 00:41:21,440
And so yeah, I got a chance to give this talk,

858
00:41:21,440 --> 00:41:23,560
but of course the work is like being done

859
00:41:23,560 --> 00:41:27,320
by a huge team of people that I've cited here.

860
00:41:27,320 --> 00:41:30,480
And yeah, if you want to use any of this

861
00:41:30,480 --> 00:41:32,200
or read more about it, like everything is linked

862
00:41:32,200 --> 00:41:35,920
from this main GitHub repo here in Fairseek,

863
00:41:35,920 --> 00:41:38,560
and you can like click on everything else afterwards.

864
00:41:39,640 --> 00:41:42,680
But yeah, maybe I'll go back to Stephen

865
00:41:42,680 --> 00:41:45,400
if we have any questions or anything else like that.

866
00:41:45,400 --> 00:41:46,840
All right, now thanks for the great talk.

867
00:41:46,840 --> 00:41:48,480
Yeah, if anybody has any questions,

868
00:41:48,480 --> 00:41:50,360
feel free to unmute and ask.

869
00:41:51,440 --> 00:41:52,280
Thank you.

870
00:41:57,280 --> 00:41:59,800
Did you consult with a lot of like native speakers

871
00:42:00,720 --> 00:42:03,800
for like, you know, profanities and this type of stuff?

872
00:42:03,800 --> 00:42:07,600
Like how are you able to get access to the, you know,

873
00:42:07,600 --> 00:42:10,920
low quality languages or low resource languages

874
00:42:10,920 --> 00:42:13,680
and make sure that translations are correct?

875
00:42:13,680 --> 00:42:15,360
Yeah, yeah, that's a really good question.

876
00:42:15,360 --> 00:42:17,120
I mean, I think it's the most important to consult

877
00:42:17,120 --> 00:42:18,640
like a bunch of native speakers

878
00:42:19,640 --> 00:42:21,720
across the entire development process.

879
00:42:21,720 --> 00:42:24,000
So part of our original thing was just like interviewing

880
00:42:24,000 --> 00:42:26,840
a bunch of people to understand like what they're looking for

881
00:42:26,840 --> 00:42:28,640
in a high quality translation.

882
00:42:28,640 --> 00:42:32,160
And then we have like an entire professional translation team

883
00:42:32,160 --> 00:42:36,160
hired, which took quite a long time to find

884
00:42:36,160 --> 00:42:40,120
to consult with along the process.

885
00:42:40,120 --> 00:42:43,000
And then right now, like we also have some of the things

886
00:42:43,000 --> 00:42:45,240
like toxicity lists are open to the community.

887
00:42:45,240 --> 00:42:46,800
So if you make like a pull request,

888
00:42:46,800 --> 00:42:48,760
we try to like, you know, validate that

889
00:42:48,760 --> 00:42:50,160
that's like a useful addition

890
00:42:50,160 --> 00:42:53,400
and then like try to merge it in as well.

891
00:42:56,840 --> 00:42:58,160
We have a question in the room.

892
00:42:58,160 --> 00:43:00,560
Let's see if that comes over soon.

893
00:43:00,560 --> 00:43:01,400
Oh, go ahead.

894
00:43:01,400 --> 00:43:03,080
So I'll speed, see if we should get it.

895
00:43:03,080 --> 00:43:04,560
Yeah.

896
00:43:04,560 --> 00:43:06,520
So like, did you spend most of your time

897
00:43:06,520 --> 00:43:07,960
in the data pipeline state?

898
00:43:09,280 --> 00:43:10,120
Yeah.

899
00:43:11,120 --> 00:43:13,840
Yeah, good question.

900
00:43:13,840 --> 00:43:14,680
I think the question is,

901
00:43:14,680 --> 00:43:15,640
did you spend most of your time

902
00:43:15,640 --> 00:43:16,840
in the data pipeline state?

903
00:43:16,840 --> 00:43:19,920
It ended up being about like kind of like 50-50

904
00:43:19,920 --> 00:43:23,280
like data or more like driving work.

905
00:43:23,280 --> 00:43:25,440
And then like 50-50 on the other side

906
00:43:25,440 --> 00:43:27,320
like modeling and evaluation work.

907
00:43:27,320 --> 00:43:29,280
Because once like the data is set,

908
00:43:29,280 --> 00:43:31,480
then there is a lot and a lot of iteration

909
00:43:31,480 --> 00:43:33,880
on the modeling side to figure out like, okay,

910
00:43:33,880 --> 00:43:35,840
which, how much of the data should we use?

911
00:43:35,840 --> 00:43:37,000
Like how should we portion the data?

912
00:43:37,000 --> 00:43:38,080
How do we prevent overfitting?

913
00:43:38,080 --> 00:43:40,200
What is the right architecture?

914
00:43:40,200 --> 00:43:42,520
But a lot of work goes into the data

915
00:43:42,520 --> 00:43:44,680
because I think if you don't have high quality data,

916
00:43:45,280 --> 00:43:47,320
you just can't get a good model.

917
00:43:49,080 --> 00:43:52,480
And for data mining, how do you mine the data?

918
00:43:52,480 --> 00:43:55,880
Do you use like Selenium or how do you mine the web?

919
00:43:57,160 --> 00:44:00,520
Yeah, so for the web, we start with Common Crawl.

920
00:44:00,520 --> 00:44:03,240
So we downloaded all of the different dumps of Common Crawl

921
00:44:03,240 --> 00:44:05,240
and then we use HTML parser.

922
00:44:05,240 --> 00:44:08,000
I think now like, if you download, for example,

923
00:44:08,000 --> 00:44:09,200
the red pajama dataset,

924
00:44:09,200 --> 00:44:12,120
like they've done a lot of this like parsing and stuff.

925
00:44:12,120 --> 00:44:15,240
And then we have like large scale pipelines

926
00:44:15,240 --> 00:44:18,240
that are set up like you can use Spark, for example,

927
00:44:18,240 --> 00:44:19,400
to process these things,

928
00:44:19,400 --> 00:44:21,720
to like split all of the different sentences out,

929
00:44:21,720 --> 00:44:23,600
run your language identification.

930
00:44:23,600 --> 00:44:26,480
You know, you can do different heuristic cleaning.

931
00:44:26,480 --> 00:44:28,520
There are certain languages where it's like very actually,

932
00:44:28,520 --> 00:44:30,880
very challenging to identify what is a sentence.

933
00:44:30,880 --> 00:44:34,320
Like I think in Thai, there is no like period.

934
00:44:34,320 --> 00:44:36,720
So you have to like use different models

935
00:44:36,720 --> 00:44:38,120
to identify what is a sentence

936
00:44:38,120 --> 00:44:40,280
and like parse some of those things out.

937
00:44:40,480 --> 00:44:44,080
And then we end up with, you know, our monolingual data dump.

938
00:44:46,280 --> 00:44:47,920
What is Common Crawl?

939
00:44:47,920 --> 00:44:51,600
Is it software that you use for datasets?

940
00:44:51,600 --> 00:44:52,440
Oh, yeah, yeah.

941
00:44:52,440 --> 00:44:54,760
Common Crawl is kind of like an open source version

942
00:44:54,760 --> 00:44:57,840
of the web that runs, I think maybe quarterly.

943
00:44:57,840 --> 00:44:58,880
I would have to check.

944
00:44:58,880 --> 00:45:00,920
But yeah, if you go to like Common Crawl.org,

945
00:45:00,920 --> 00:45:03,840
you can download it, but warning, it's like very large.

946
00:45:11,280 --> 00:45:12,600
I have a question.

947
00:45:12,600 --> 00:45:14,200
I mean, you might have mentioned this briefly,

948
00:45:14,200 --> 00:45:17,800
but I'm wondering how chatGBT and GPT-4 does on this.

949
00:45:17,800 --> 00:45:21,600
Like does just more scale and pre-training data help

950
00:45:21,600 --> 00:45:25,400
as well for low resource machine translation?

951
00:45:25,400 --> 00:45:26,800
Yeah, yeah, good question.

952
00:45:26,800 --> 00:45:28,200
Actually, there have been some studies done

953
00:45:28,200 --> 00:45:30,360
on like how, you know, these systems work.

954
00:45:30,360 --> 00:45:32,160
I think for high resource languages,

955
00:45:32,160 --> 00:45:35,000
it's actually quite beneficial to scale up.

956
00:45:35,000 --> 00:45:37,360
I think part of that is because the models

957
00:45:37,560 --> 00:45:39,240
have some innate generalization.

958
00:45:39,240 --> 00:45:40,960
And so one of the challenges that people talk

959
00:45:40,960 --> 00:45:42,840
about different things in different languages.

960
00:45:42,840 --> 00:45:45,600
So like seeing that knowledge in another language

961
00:45:45,600 --> 00:45:47,480
can actually help the generalization.

962
00:45:47,480 --> 00:45:51,080
But on low resource languages, it's, yeah,

963
00:45:51,080 --> 00:45:53,680
the performance is pretty difficult,

964
00:45:53,680 --> 00:45:56,600
especially on some of these translation benchmarks.

965
00:45:56,600 --> 00:45:59,040
I also think that language models,

966
00:45:59,040 --> 00:46:02,000
in terms of being trained for like a translation objective,

967
00:46:02,000 --> 00:46:04,320
tend to score worse on translation benchmarks

968
00:46:04,320 --> 00:46:06,480
because language models are like approximately

969
00:46:07,480 --> 00:46:09,680
capturing the same thing or as translation models.

970
00:46:09,680 --> 00:46:12,640
So you really try to align the meaning a little bit more.

971
00:46:13,800 --> 00:46:15,240
But yeah, so I think for low resource,

972
00:46:15,240 --> 00:46:17,480
it's still pretty challenging.

973
00:46:17,480 --> 00:46:19,320
But yeah, one thing that's interesting

974
00:46:19,320 --> 00:46:21,320
is for most English language models,

975
00:46:21,320 --> 00:46:23,400
they can actually do a reasonable job

976
00:46:23,400 --> 00:46:24,840
at producing other languages

977
00:46:24,840 --> 00:46:27,000
because it's impossible to get rid of other languages

978
00:46:27,000 --> 00:46:28,720
in your English specific data.

979
00:46:28,720 --> 00:46:32,200
So things like French or German will work reasonably.

980
00:46:36,480 --> 00:46:39,400
So just to clarify, you said language models trained

981
00:46:39,400 --> 00:46:43,600
with a translation objective do better, right?

982
00:46:43,600 --> 00:46:46,040
Because, right?

983
00:46:46,040 --> 00:46:46,880
They tend to do better.

984
00:46:46,880 --> 00:46:49,200
Like if you fine tune for the translation task,

985
00:46:49,200 --> 00:46:50,760
it will tend to do better.

986
00:46:50,760 --> 00:46:52,240
Well, that makes sense compared to like,

987
00:46:52,240 --> 00:46:56,800
for example, some few shot in context examples.

988
00:46:56,800 --> 00:46:59,080
Right, right, exactly, exactly.

989
00:46:59,080 --> 00:47:00,640
And one other question is,

990
00:47:03,760 --> 00:47:05,640
do you see this being similar to, for example,

991
00:47:05,640 --> 00:47:08,200
fine tuning on particular expert domains,

992
00:47:08,200 --> 00:47:13,200
which might also have less data and low resource,

993
00:47:13,920 --> 00:47:16,720
and as well as domain specific jargon and so forth?

994
00:47:18,800 --> 00:47:21,880
Yeah, I mean, I think if we were to restart this project now,

995
00:47:21,880 --> 00:47:23,480
I think that would be one of the first things

996
00:47:23,480 --> 00:47:26,120
we also explored, or at least like an extremely strong

997
00:47:26,120 --> 00:47:28,440
baseline where if you like take some of the data

998
00:47:28,440 --> 00:47:33,120
and you try to fine tune or try to do domain adaptation,

999
00:47:33,120 --> 00:47:35,520
I think that's also where like some of the like retrieval

1000
00:47:35,520 --> 00:47:38,720
type approaches go in for translation,

1001
00:47:38,720 --> 00:47:40,600
but also large language modeling work

1002
00:47:40,600 --> 00:47:42,520
where you try to have like a separate domain

1003
00:47:42,520 --> 00:47:45,920
that you can like retrieve some text in for adaptation.

1004
00:47:45,920 --> 00:47:48,520
I think all of those approaches are pretty promising.

1005
00:47:54,520 --> 00:47:56,280
Arcade, any other questions?

1006
00:47:59,440 --> 00:48:01,560
One quick one on the point of the video question.

1007
00:48:01,560 --> 00:48:02,760
You're looking at one of the slides,

1008
00:48:02,760 --> 00:48:05,960
I think you showed some peak results with zero shot

1009
00:48:05,960 --> 00:48:10,280
that were higher than just the base model.

1010
00:48:10,280 --> 00:48:12,880
Do you think that's because there might still be

1011
00:48:12,880 --> 00:48:16,040
some overfitting on those low resource languages?

1012
00:48:16,040 --> 00:48:17,000
Yeah, good question.

1013
00:48:17,000 --> 00:48:19,720
So for our large scale mining,

1014
00:48:19,720 --> 00:48:23,000
we don't mind like every single possible cross pair.

1015
00:48:23,000 --> 00:48:26,400
So like Icelandic will love,

1016
00:48:26,400 --> 00:48:28,360
it's probably like not like the most

1017
00:48:28,360 --> 00:48:30,080
in demand translation direction.

1018
00:48:30,080 --> 00:48:32,920
And so we did not mind like all 200 times 200

1019
00:48:32,920 --> 00:48:34,040
because it's like really producing

1020
00:48:34,040 --> 00:48:35,960
like a combinatorial explosion.

1021
00:48:35,960 --> 00:48:38,400
And so that's where the zero shot results come from

1022
00:48:38,400 --> 00:48:40,560
where you don't need,

1023
00:48:40,560 --> 00:48:43,560
we don't have training data directionally in that pair,

1024
00:48:43,560 --> 00:48:46,520
but the model has seen both the input and the output.

1025
00:48:46,520 --> 00:48:50,200
And so I think those results are pretty good.

1026
00:48:50,200 --> 00:48:52,720
Well, they're good for certain languages,

1027
00:48:52,720 --> 00:48:56,760
which I think goes to show like the generalization capability

1028
00:48:56,760 --> 00:48:58,800
and it's not like as critical

1029
00:48:58,800 --> 00:49:00,760
to have like every single pair covered,

1030
00:49:00,760 --> 00:49:02,440
but many of them are not as good.

1031
00:49:02,440 --> 00:49:04,840
And so you see overall the performance is lower,

1032
00:49:04,840 --> 00:49:06,080
even though on certain languages,

1033
00:49:06,080 --> 00:49:08,000
it can perform better than you expect.

1034
00:49:08,000 --> 00:49:10,160
But that's because it has seen the input and the output.

1035
00:49:10,160 --> 00:49:12,960
It's not zero shot on like completely unseen language.

1036
00:49:22,000 --> 00:49:23,160
I have a question.

1037
00:49:24,120 --> 00:49:28,120
But I wanted you to also, you know,

1038
00:49:28,120 --> 00:49:31,000
do something related to transcription

1039
00:49:31,000 --> 00:49:32,640
or audio information?

1040
00:49:34,400 --> 00:49:35,640
Yeah, good question.

1041
00:49:35,640 --> 00:49:38,960
So in this project, no, not so much transcription,

1042
00:49:38,960 --> 00:49:40,760
but we had a follow up work that we released

1043
00:49:40,760 --> 00:49:44,800
actually just like a month or so ago called seamless M4T,

1044
00:49:44,800 --> 00:49:46,120
which is like a joint model

1045
00:49:46,120 --> 00:49:48,400
for both speech and text translation.

1046
00:49:48,400 --> 00:49:51,440
And that's where we do leverage a lot of audio transcription

1047
00:49:51,440 --> 00:49:54,320
because that also has like, it helps us bridge,

1048
00:49:54,320 --> 00:49:56,440
you know, like the spoken data and the text data

1049
00:49:56,440 --> 00:49:58,120
to leverage both of them together.

1050
00:50:08,240 --> 00:50:10,960
Wait, just to clarify, the supervised fine tuning

1051
00:50:10,960 --> 00:50:15,560
it worked better, right, compared to other methods.

1052
00:50:16,600 --> 00:50:19,120
So actually in this work, it was as a couple of years ago now.

1053
00:50:19,120 --> 00:50:23,520
So supervised fine tuning wasn't as common as it was now.

1054
00:50:23,520 --> 00:50:25,240
But I think in the literature,

1055
00:50:25,240 --> 00:50:26,960
if you want to use like a large language model

1056
00:50:26,960 --> 00:50:29,160
to do translation, it's currently best yet

1057
00:50:29,160 --> 00:50:31,600
if you do some supervised fine tuning.

1058
00:50:31,600 --> 00:50:33,120
I'm just wondering about that

1059
00:50:33,120 --> 00:50:34,800
because the way as humans, right,

1060
00:50:34,800 --> 00:50:39,360
we don't just learn by looking at pairs of the same thing

1061
00:50:39,360 --> 00:50:40,200
in different languages

1062
00:50:40,200 --> 00:50:44,360
and kind of memorizing how to map from one to the other.

1063
00:50:44,360 --> 00:50:46,240
We kind of learn in a more unsupervised way

1064
00:50:46,240 --> 00:50:48,400
where if we know both languages,

1065
00:50:48,400 --> 00:50:51,840
then we can kind of naturally translate between them.

1066
00:50:54,480 --> 00:50:57,120
But I guess it makes sense for an LLMY

1067
00:50:57,120 --> 00:51:01,280
having supervised examples would help, yeah.

1068
00:51:01,280 --> 00:51:05,000
Yeah, I mean, I think as like the base foundation model

1069
00:51:05,000 --> 00:51:06,800
continues to improve in quality,

1070
00:51:06,800 --> 00:51:09,240
I think that's where the quality will probably improve

1071
00:51:09,240 --> 00:51:11,160
when you don't need less and less fine tuning.

1072
00:51:11,160 --> 00:51:13,560
I mean, do you think that's like the open AI approach?

1073
00:51:13,560 --> 00:51:15,360
Like if you have the best foundation model,

1074
00:51:15,360 --> 00:51:18,040
then you don't need as much like domain specific fine tuning.

1075
00:51:18,080 --> 00:51:19,840
I think like, you know, like at the start

1076
00:51:19,840 --> 00:51:21,400
when I started working on text generation,

1077
00:51:21,400 --> 00:51:22,840
there was like translation researchers

1078
00:51:22,840 --> 00:51:24,040
and like summarization researchers

1079
00:51:24,040 --> 00:51:25,480
and like question answering researchers

1080
00:51:25,480 --> 00:51:27,240
and they like work very differently.

1081
00:51:27,240 --> 00:51:28,680
But now it's like, it's all driven

1082
00:51:28,680 --> 00:51:30,000
by the same underlying thing

1083
00:51:30,000 --> 00:51:31,680
and you're not like a specialized

1084
00:51:31,680 --> 00:51:33,760
summarization researcher anymore.

1085
00:51:35,480 --> 00:51:38,600
Right, I think that makes a lot of sense.

1086
00:51:41,280 --> 00:51:43,000
Do we have any other questions?

1087
00:51:44,000 --> 00:51:45,000
Any questions?

1088
00:51:52,000 --> 00:51:54,000
Ron, any more in-person questions?

1089
00:51:56,000 --> 00:51:57,000
Oh, don't think so.

1090
00:51:58,000 --> 00:51:59,000
Okay, great.

1091
00:51:59,000 --> 00:52:00,000
All right.

1092
00:52:00,000 --> 00:52:03,000
Well, thank you, Angela, for the very interesting

1093
00:52:03,000 --> 00:52:06,000
and a great talk again and for taking the time.

1094
00:52:06,000 --> 00:52:11,000
And we hope, yeah, we hope that you can keep in touch

1095
00:52:11,000 --> 00:52:14,000
and if anybody has any other questions,

1096
00:52:14,000 --> 00:52:17,000
feel free to get in touch with Angela.

1097
00:52:17,000 --> 00:52:19,000
All right, thanks so much for having me today.

1098
00:52:19,000 --> 00:52:20,000
Bye, everyone.

