Hi everyone, thank you very much for the invitation to speak at this CVPR workshop.
I'm very excited to be presenting my research.
I will talk about shape learning in biomedical imaging, while by shape learning I mean machine
learning for data that are shapes.
One data point is one shape.
And specifically, we will be interested in the shapes that we see in biomedical images.
For example, the shapes of the brains that we can see at the images at the bottom of
this slide.
So we're interested in the shapes of biological structures that appear in biomedical images.
These biological structures can be found at the macroscopic scale all the way down to the
nanoscopic scale, going from organs, such as the brains that we just saw, all the way
down to tissues, cells, organelles, and molecules.
And in 2022, we are actually very lucky, because in order to study these biological structures
that are at the heart of life, we can just look at them.
We have access to a wide range of imaging techniques that allow us to observe these
biological structures.
At the macroscopic scale, for example, we have MRI, or magnetic resonance imaging, that
allows us to observe human brains.
But all the way down to the nanoscopic scale, we have other techniques, ending with cryo-electron
microscopy, which allows us to look at biomolecules, such as the human ribosome.
Now the point that I want to make with this slide and with this presentation is that in
all of these images, there is one feature that is very interesting in the sense that
it can be linked to the biology.
This feature is the shape.
The shape of the biological structure that is being imaged contains enormous information
on the biology and the biophysics of what is happening in living organisms.
Let me give you an example of this.
Actually, let me give you a few examples on how shapes of biological structure contain
meaningful information.
Few examples in the context of one pathology called Alzheimer's disease.
At the macroscopic scale first, Alzheimer's disease is linked to shape changes.
If we look at the simulation that I had on the first slide, we see the evolution of
a brain through time.
Actually, we see three slices of a 3D brain evolving through time.
This is a simulation of a brain with Alzheimer's disease.
You can see on the simulation that the brain changes shape as it evolves through time.
The ventricles, which are the structure in the center of this brain, are becoming larger
and larger, so that's the area in black that is in prison.
As the brain undergoes atrophy, meaning the brain matter around this ventricle is becoming
smaller and smaller.
This atrophy is a macroscopic shape change that is characteristic of Alzheimer's disease.
Now you may ask, where does this atrophy come from?
Where do these macroscopic shape changes come from?
Actually, at the microscopic scale now, if we look at cells, so at the neurons, these
neurons are dying.
The other reasons were why we see this atrophy at the macroscopic scale.
When a neuron is dying, it changes shape.
As you can see on this video of a dying neuron, it's retracting its axons.
At the microscopic scale, for the scale of cells, Alzheimer's disease is linked to shape
changes.
What's even more interesting is that at the macroscopic scale, there is also a shape problem.
We have these two proteins, eta-amyloid and tau, that are changing shape in patient with
Alzheimer's disease.
Because they have changed shape, they will agglomerate either inside the neurons, so
we see in blue, that the proteins agglomerate inside the neuron, or they will agglomerate
outside the neuron, and these agglomerations is what will cause the neuron to die.
A shape problem at the nanoscopic scale is creating the death of the neurons, which is
creating a microscopic shape change, which is also leading to a macroscopic shape change
with the atrophy of the brain that we can see on MRI.
We have illustrated that biological shape contains biologically relevant information,
or medically relevant information, that we can go from biological shapes to biomedical
insights.
Actually, biophysics tells us that the healthy or the pathological state of the biological
structure will change its shape, or generally that the function of a biological structure
will change its shape.
For example, blood cells have a different shape than brain cells or neurons, because
they have different functions.
In the context of this talk, we are interested in inverting that model, in learning biophysics
or biology, from the data that we have, which are the shapes.
So we do statistics and machine learning on shapes, which is what we will call shape learning.
Let me show you how we can do shape learning in biomedical imaging, by showing you the
architecture of a typical research project in our lab.
Usually, it starts with a biomedical question.
For example, what is the acceleration of the atrophy that we see in the brains of patients
with Alzheimer's disease?
It will be interesting to know what's the magnitude of these accelerated brain shape
changes, because it could give ideas to build automatic diagnosis procedure.
Another biomedical question could be, how do cells move?
What you see on this slide, a moving cell, so called a migrating cell.
How can cell move is a very important question when one studies metastatic cancer, when the
cancerous cells propagate in the human body.
All in all, it starts with a biomedical question.
Then there is a step of image acquisition, acquiring the MRIs, acquiring the microscopy
images, that is done by collaborators of all that.
Then there is an important step of shape reconstruction.
Yes, because the data that we get are images or videos, so the shape has not been extracted
yet.
We made it an algorithm of contour detection, edge detection, or segmentation to reconstruct
the shape.
Then there is a step of shape modeling.
You could also think of it as shape featurization.
Let's say we have extracted the border of the outline of that cell.
How do we want to represent this shape in a computer?
There are different choices that one can make, and we're going to see a few of them.
And lastly, when we have represented the shape, we have chosen a feature to represent the shape.
We can do statistics, machine learning, and deep learning on these data points that are
shapes.
That's what they call shape learning.
In the context of this talk, we will focus on the last two of this pipeline of this cycle.
How can we represent a shape in a computer?
And then when we have represented the notion of shape in a computer, how can we do machine
learning on shapes?
Let's start with how we represent the notion of shape in a computer.
So shape modeling, our shape representation is interesting because we have different
choices as to how to model the shape of the biological structure.
But irrespective of these choices, very often, we end up computing with manifolds.
What is a manifold?
On the left, you have a visual definition of what is a manifold.
We can generally define a manifold as a generalization of a vector space that is allowed to be curved.
On the left, for example, you can see a 2D vector space, also called a plane, that is
not curved.
It's linear or flat.
And just below it, we have an example of a 2D manifold M that is itself allowed to be
curved.
You can see that there is some curvature there.
Now, this is interesting for us because, as I said, we can try different models of shapes,
and representations of shapes, but very often, we will see this manifold M appear.
Let me give you examples of how manifolds enter the computations.
First, let's look at the shapes themselves.
Let's say we are interested in shape of this cat, and more precisely, the shape of the
surface that is defining the cat.
The shape of the surface is a generalization of a vector space that is allowed to be curved,
and so the shape itself is the manifold in this case, so I write M equal to the shape.
The manifold is the shape.
If you want to compute on these shapes, for example, define a hit map or a scalar field
on this shape, you are working with defining a function on the manifold.
Another example is a bit more abstract.
The manifold can be the space of all the shapes.
In the first case, one shape is a manifold.
In the second case, we consider the space of all possible shapes, but you have illustrated
with this sphere is actually the shape space of all the possible triangles, so you can
see if you look, maybe you can see, that I have superimposed a little triangle on this
sphere, because here there is a triangle.
This sphere actually represents the space of all the triangular shapes, so all the visible
shapes.
Now, this illustration is the shape space of triangles, but more generally, the shape
space of surfaces is also a manifold.
Continuing, you could be interested in the space of shape motions.
For example, how your shape translates and rotates.
Here you have a cancer cell translating and rotating.
If you want to do statistics on all the possible motions that your shape can perform, then you're
doing statistics on, again, a manifold M. That is this time, the space of shape motions.
And lastly, you could be interested in how shapes deform, or a smooth deformation of
the cancer cell, as you can see on the left, but also other objects like functional maps
that explain how shapes can deform.
And interestingly, again, if you choose this shape representation, you end up with a space
of shapes deformation, that is a manifold.
There are other models of shape that I will not cover here, but it seems that these four
models of shapes are quite common, and all of them require thinking about manifolds.
Thinking about manifolds, what can we do with manifolds?
Let's think about generalizing basic operations, and for example, generating the definition
of a new manifold.
On the center of the screen, I put the sphere.
The sphere is one example of manifolds that we had just before.
It can represent the shape space of triangles.
The one point of the sphere is the shape of a triangle.
You can also think of it as one point of the sphere is a shape motion.
How do I compute the mean of two triangles?
That will be the two real points here.
Or how do I compute the mean of two shape transformations?
That will be the two blue points here.
I could use the traditional definitions of statistics, and I can use the traditional
definition of mean and compute the mean as this orange point, which would be computed
as the middle of the two blue points.
But in that case, if I apply traditional statistics, there is a problem because you see the mean
in orange does not belong to the manifold.
Now, if the manifold represents the shape space of triangles, what we are saying in applying
traditional statistics is that the mean of two triangles is not even a triangle.
So traditional statistics do not really apply in this setting.
We would like to generalize operations, statistics, machine learning, and deep learning to manifolds.
So that when I try to compute the mean of these two blue points, that again, the two
triangle shapes, two shape deformations, I end up with a orange point that is at least
part of the data space that I'm interested in.
So we'd like to generalize all of these operations to manipulate.
Why would we want to do that instead of, for example, just using traditional statistics
and then projecting back on the manifold?
This is because knowing that the data space is a manifold is information.
Knowing the geometry of the shape space, of the space of shape motions, of the space of
shape deformations, that is information.
And any information that we can incorporate in the learning algorithm is welcome.
So in other words, we would like to use the geometry of manifolds as an inductive bias
in our analysis.
So let's see how we can do this.
So I've just introduced shape modeling.
I've introduced four models of shapes, four representations of shapes, and all of them
require us to compute on manifolds.
Now let's move to shape deep learning, which is a way of computing on manifolds, and see
how we can define shape deep learning for these manifold spaces.
So let's think about what is deep learning first.
I've put four big categories of subfields of deep learning, or machine learning in general,
supervised learning, unsupervised learning, reinforcement learning, and optimization.
What is supervised deep learning in the context of shape learning?
In supervised learning, the goal is to learn a map from an input space to an output space
Y. Now let's rename the input space X into Mx, to denote that the input space can be a manifold.
And let's rename the output space Y as MY to denote that the output space can be a manifold.
So this is a setting that will appear if we want to do shape deep learning.
As an example, let's say Mx is just an image, a 2D image of a 3D object.
In that case, it's not, I mean, it's a special case of a manifold, but it's not a curved manifold.
And let's say Y is the space of the pose of the subject.
In that case, MY would be a manifold.
That's an example of shape learning when we want to predict an element of MY, therefore an element of a manifold.
You can have all the cases where Mx is the manifold.
For example, you see the shape of the brain and you want to predict if that brain has Alzheimer's disease.
In that case, Mx is the space of all possible brain shapes and Y is the probability of having Alzheimer's disease.
And you can also have a case where Mx is the manifold and MY is the manifold.
In unsupervised learning, the goal is to find patterns in the data that belong to the data space M.
Let's say we want to find patterns in a data space of shapes, then M is a manifold.
And so we need to generalize unsupervised deep learning to unsupervised learning to manifolds.
An example of unsupervised learning would be if we want to find clusters of shapes.
We need to find a method that can perform clustering of manifolds.
In reinforcement learning, same story.
Let's say we want to learn the policy by that gives the priority of applying an action on the state space.
Then each of these spaces can also be a manifold or both into a manifold.
For example, we want to learn how a shape should move to optimize a certain feature.
And lastly, what is underpinning a lot of development in deep learning is the broader field of optimization.
And same thing, we may wonder how this can generalize to manifolds.
Let's say we want to find the optimal shape that verifies a certain feature.
In that case, we want to perform an optimization where the parameter that is varying is actually varying on a manifold.
So we want to perform optimization of manifold.
The example here, let's say we want to find the optimal deformation that can map one shape onto the other.
The deformation is an element of the manifold of the patients.
And therefore, this will be an example of optimization on manifolds.
So how can we generalize all of these wide fields of deep learning to manifolds?
And that's actually a trick that I call the vector space manifold conversion trick.
And the trick is as follows.
You can realize that all of these machine learning, such as deep learning algorithms,
rely on the same basic building blocks.
By basic building blocks, I mean some sets of abstract elements and some set of abstract operations.
Now, if we can convert these building blocks from their definition on vector spaces to their definitions on manifold,
then we have a translation in the sense of language translation that allow us to beautifully convert any type of statistical learning,
machine learning, and deep learning algorithm to manifolds.
So let's do this.
What are the basic building blocks that are at the core of every statistical learning, machine learning, and deep learning algorithm?
Well, in terms of the elements, almost all of these algorithms use either points or vectors.
So point on the data space, in our case, one point in your shape, one point can be a deformation.
They also use vectors.
Think about gradient descent.
The gradient is a vector anchored at a point on the manifold.
And then in terms of operations, there are three operations that come over and over again in these algorithms,
which are computing a straight line, computing a distance, and performing a net issue.
So straight line, linear regression, our principal component analysis, we are using straight lines.
So we need a way of computing straight lines.
The squared distance, in many loss functions, we use a notion of squared distance between the ground truth and our prediction.
Very often the L2 distance.
And then the addition.
For example, when we add a gradient to an estimate, we are adding the gradient vector to the estimate point.
So we are adding the black arrow to this blue point when we get an orange point.
Now, assuming that most of the learning algorithms uses these basic elements and operations,
if we can generalize or convert these elements and operations to manifold, then we have our conversion checks.
I'm going to introduce the conversion.
Yes, we want to convert to manifold.
The points are just points.
They are the elements of our manifolds.
What was a vector on the vector space becomes a tangent vector on the manifold.
So on the vector space, point and vectors are approximately the same.
On the manifold, points and tangent vectors are two different, very different elements.
On the left, the registration on the left, you can see the tangent vector in black that is anchored at the point in blue on the manifold.
So we have generalized basic elements.
Now let's generalize the operations.
The straight line becomes a geodesic on M.
I'm not telling you how we compute the geodesic, I'm telling you the translation or conversion of the operation.
For example, on the registration on the left, you can see the dotted black line is the geodesic between the blue point and the orange point.
The square distance between two points on the manifold becomes the square geodesic distance between the same two points.
We're basically the length of the square length of the geodesic that is connecting these two points.
And lastly, we need a generalization of the notion of addition.
On the vector space, we can add a vector, for example, a vector in black to a point here in blue.
On the manifold, this operation is now called the exponential map.
And it is, again, just rated on the left, while we add not a vector, but tangent vector, so the tangent vector in black to the point in blue.
And performing this addition operation allows us to reach another point on the manifold, which is the point in orange.
So that is the vector space manifold conversion.
And this trick, we implemented it in this open source package called GM Stats, which generalizes all of these operations that we know very well on vector spaces to manifolds.
And for different types of manifolds, specifically the different types of manifold that I showed you in the context of shape representation and shape models.
It's a Python package that is available online at these links, and we created it with three objectives.
First, to teach hands-on geometric learning.
So computing on manifolds can be hard in practice, even though the word manifold comes over and over again in manifold learning, for example.
The basic operations on manifolds are sometimes hard to implement.
And one of the reasons is that they're not necessarily taught in class besides textbooks.
So with this software, you can teach hands-on manifolds and geometric learning.
The second objective is to support research in geometric learning or learning on manifolds.
So different researchers reach out to us who publish, they have published papers and they want to implement them methods in GM Stats.
So as to make them methods better, more available to the general public.
So that's on the contributor side supporting the research.
And the last objective is to democratize the use of geometric learning, in other words, learning on manifolds.
Because once these methods are incorporated into GM Stats, from a user's perspective, you only need to know the conversion that I just presented you on the previous slide,
and you can use all of these algorithms.
So you do not need to know how a geodesic is implemented.
You just need to know that the geodesic is a generalization of a straight line.
So it allows to democratize the use of these methods.
Now other libraries that implement computations on manifolds, learning on manifolds and optimization on manifolds, you can see them here on the screen.
The first three, the one that have opt in it and MacDodge, they are focused on optimization on manifolds.
For example, to minimize a criterion and the parameter minimizing this criterion is an element of the manifold.
And then all the others are Python packages that perform computations and learning on manifolds, but they focus on a special type of manifolds.
For example, the manifold of SPD mattresses, tree rotations, disruptions.
By contrast, we do not do fancy optimizations in GM Stats, but we try to implement a very wide range of manifolds, including the manifolds that represent shapes and shapespaces.
So let me give you an example of how you can use this software to compute with shape representations and let's say shape motions.
So let's say you're interested in knowing how a shape evolved from being in an orientation R1 and translation or position T1.
And how it evolves from this original pose to an end pose, which we could call R1 for rotation R2, for rotation two and T2 for translation two.
So when we compute with rotation and translations, as we do with motions and by the way, it should be 3D translation, not 2D translation.
We are computing with elements that belong to the manifold and that gave the manifold is M equal SE3 and that stands for special Euclidean group in three dimension.
In other words, the group of all 3D translations and 3D rotations.
This is a good snippet using GM Stats.
What you really have to do is instantiate the manifold that you are interested in.
Here we want to work with SE3, so we instantiate SE3 as an object of the class special Euclidean in dimension three.
We extract what's called a metric.
It's a Riemannian metric that allows us to perform the computations.
And then we call metric dot geodesic.
Again, we do not need to know how the geodesic is implemented.
We just need to know that the geodesic is the generalization of the straight line.
And therefore, we can compute the geodesic that starts with an initial point, rotation one, translation one, and has an initial tangent vector.
And on the right, you have an illustration of the geodesic.
Pose one corresponds to R1T1, rotation one, translation one.
It's represented by one very small frame with three axes because we can use the translation, translation one, to place this frame in 3D.
And the rotation, rotation one to orient the frame in 3D.
But therefore, one little frame corresponds to one point in SE3.
And you see here a trajectory of frames that correspond to the geodesic on the manifold SE3 linking pose one to pose two.
So I told you we implemented a lot of different manifolds in GM stats.
And what we end up with is actually a numerical taxonomy of manifolds.
So we use object-oriented programming to create this hierarchy of all the possible of many manifolds that you could be interested in working with.
So this hierarchy is built as follows on the root of the hierarchy of the tree.
You have the most abstract concept of manifold that is just a manifold.
And we list all the different attributes or methods that a manifold item object should have.
And as you go down this hierarchy, you find more and more specialized or more and more concrete manifolds.
For example, one level down, you found the manifold matrix lead group.
Lead group is a special case of a manifold.
It's a manifold that has an algebraic group structure.
So therefore, matrix lead group is on the second level of the hierarchy.
It inherits from the Python class manifold because it's a special case of a manifold.
And then you go down this hierarchy and at the very bottom of this hierarchy, you have the leaves.
And these are manifolds that you can actually instantiate.
So for example, there will be somewhere in there, the special ectogen proof that we just used,
which would inherit from matrix lead group because it's a special type of matrix lead group.
And again, matrix lead group itself inherits from manifolds.
So with this project that started with representing shapes and doing statistical learning on shapes,
we actually ended up creating a computational representation of differential geometry.
Okay, so let me wrap this up a little bit and show you how we can mix these two.
So how we can use a model of shape to how can we can perform a deep learning algorithm
on a manifold that would, for example, represent the shape space.
And the example I'm going to choose is variational encoders.
How do variational encoders work on manifolds?
Zooming out a little bit, let me show you an overview of the research that I'm interested in.
In the previous slides, I was showing you the CRT of computational differential geometry,
which is the ERT of manifolds.
I'm interested in generalizing statistical learning, machine learning and deep learning
on these exotic data spaces. In other words, I'm interested in filling the table that you see on
this slide, where in this table, the different rows represent different manifolds. So each of the row
in this table corresponds to one of the nodes that you had in the previous CRT.
There are different types of exotic data spaces, if you wish.
And in this table, the different columns represents different fields of statistics,
machine learning and deep learning. We're going to talk about theorizing variational encoders to
manifolds. So therefore, we are in the column dimension reduction and deep learning.
So dimension reduction and variational encoders. I'd like to present a geometric perspective
on them as to emphasize where manifolds come in. On this first line of this table,
you can see two classes of dimension reduction methods. The first classes, the first class of
methods, are the methods that seek to perform dimension reduction on an ambient space that is
a vector space. Specifically, let's say the vector space is R2, and we have some data points,
which are the dark green data points. We can think of principal component analysis that seeks to
learn a linear subspace that would be the light green line within this linear space that is R2.
And you have different dimension reduction methods that work like this, that wish to
learn a linear subspace within a linear space. In the manifold world, we remember that linear
subspace, or that linear line, is converted to geodesic. And so you have equivalent methods
that allow to learn the equivalent of a line that is a geodesic on a manifold. So this time,
the ambient space is a manifold and will be this clear. And we're interested in learning
principal directions of variations of the data, the data of the dark green point. And we want to
learn principal directions of variations of this data that will be the equivalent of the lines on
the left, so that are the geodesic. The geodesic on the sphere is a great circle, and you can see in
light green, a great circle. In the context of variational tranquillers, the vector space case,
we are not linear. We're using deep learning, and most of the time we are highly nonlinear.
Therefore, we need another row in this table to complete the overview of dimension
reduction methods from a geometric perspective. And then we talk about the one that is on the
bottom left, which include, which is a class of methods that include variational tranquillers.
These class of methods operate in an ambient space, that is, again, a vector space. It's again
represented as R2 here. But instead of learning variations of the data that are linear, as this
here would do, instead, we are allowed to learn a nonlinear subspace M. So you see the dark
light green line is now nonlinear. So I'm going to present how we can generalize this VAE,
presented from this geometric perspective here, to geometric VAE, or VAE on manifolds,
which is what is shown on the bottom right. So now the setting is the ambient space is a manifold.
So the sphere represents the ambient space, that is the manifold, to which we know that the data are,
in which we know that the data are. And now we don't only restrict ourselves to learning
a geodesic subspace, as the road just above was doing, but we allow to learn any nonlinear subspace,
or non-geodesic submanifolds, and within the manifold M.
But how does this work? Let me review variational tranquillers. I put
equations on the left for completeness. I'm not going to comment on them too much.
Variational tranquillers start with a generative model of data in Rd, that's the ambient space
that we had before. The data assumed to belong to a vector space. And we can think of variational
tranquillers as a way to invert a generative model with latent variables. And typically,
a generative model with latent particles, such as the one that is in the first equation.
So we have xi, that's our data in the vector space Rd, that is assumed to be generated
with a generative model with latent variables. But the latent variable is xi, they are going to
represent the low dimensional representation of the xi. So this low dimensional representation,
xi, pass through a function, f, this parameter mu and w, this function is usually represented by a
neural network, that is called the decoder, variational tranquillers, to which noise is added.
So that's the generative model that we assume has generated data in variational tranquillers.
From a geometric perspective, xi are assumed to belong to a lower dimensional latent space,
that I write Rl. Then this xi passes through the function f of mu and w,
which can be the neural network. And by passing through this function, it becomes
an element of the higher dimensional space Rd. So this is the blue point on the illustration.
And then noise is added to the model. Now, if we were to pass the whole latent space Rl
through the function f mu and w, we would get this nonlinear light green curve that represents
the subspace, the nonlinear subspace that we are learning. That's a geometric explanation
of the generative model of the data that is at the foundation of variational tranquillers.
Now, with GAE, we do not observe, we do not know what is f of mu and w.
We model it as a decoder, but we do not know what are the weights of the decoder.
And we also, we do not know what are the xi that are associated to each xi. So,
given only the xi's, we would like to be able to learn the low-dimensional representations
xi together with the decoder, the model. And this is done with this architecture that you
might be familiar with. On the left, you have the traditional architecture of the variational
tranquillers. On the right, you have the last function that is used to train it. So in the
architecture of the GAE, the second half represents the decoder. It is the representation of the
generative model that takes an element of the lower-dimensional latent space in green
and outputs the function f of that latent variable. And on the left, we add an encoder
that is able to perform the operation of going from xi to a normal size approximation
approximates representation of the posterior of the xi. Anyway, this is all trained with the
last function that is called the elbow. It stands for evidence lower bound. It's a lower bound of
the likelihood, of the log likelihood. And I've given its expression here as a sum of a reconstruction,
a term and a regularization term, because these are the terms that we have to generalize to manifold
to have a version of variational tranquillers that works on manifold.
So let's try to do this, to generalize variational tranquillers to manifold. We will have to generalize
two elements. First, the generative model and second, the last function. Now, looking a little bit
at the generative model and the last functions, we observe that they are built from basic elements
and operations that we now know how to converge to manifold. So specifically in the generative model,
we see there is a plus, which is an addition. And the generalization of the addition of vector
spaces is the exponential map of manifold. In the last function also, we see that there is a square
distance here, a squared L2 distance between x and f of c. Now that's something we can generalize
to manifold to. And this is how we go to manifold variational tranquillers. It's conceptually easy.
We replace the addition by the exponential map. And now we have a generative model that outputs
points on the manifold. And that goes like this. We find again zi on the latent space RL. And now
our decoder is going to be able to map zi on the point on manifold to which we add noise by adding
a tangent vector to the point in group. So we have generalized the generative model to
manifolds. We need to generalize the inference, which is how do we learn in this generative model.
We have a very similar architecture that has an encoder and a decoder. But then what really changes
now is the loss function that you can still formulate as an elbow of a different log likelihood,
just because the generative model is different. And I've put in red the terms that change,
because the generative model is different, the log likelihood is different, the evidence lower
bound or the elbow is different. And therefore you can show that it has this formula.
We could have guessed this formula by converting the elbow plus as it was written on vector spaces
to its expression on manifolds. And so this allows us to learn non-giardasec
sub-manifolds of a given manifold. I think the only competing method that was able to do that
before was relying on Monte Carlo, for example, from the posterior. And therefore
so using VAE we're able to learn non-giardasec sub-manifold of a manifold. We're also able to
give some insights of what was observed in the literature about VAE, which was about this
statement that you can see by Chao Edel in 2018. But I was saying that in VAE the experiment showed
that these models represent real image data with manifolds that have surprisingly little curvature.
Long story short, we are able through a geometric analysis of geometric VAE
that gives insight on VAE to explain why they find latent spaces with shockingly little curvature.
So to conclude, this is the type of pipeline that we're interested in our lab, we want to do shape
learning in biomedical imaging. I've explained how shape modeling gives rise to
overwhelming operations on excessive data spaces that are manifolds, and therefore how statistical
learning, machine learning, and deep learning can generalize to these spaces if you want to shape
deep learning. Many thanks to our lab at UCSB and many thanks to the organization that are funding
this research, and I'll be happy to answer any questions.
