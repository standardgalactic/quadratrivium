WEBVTT

00:00.000 --> 00:07.960
Hi everyone, thank you very much for the invitation to speak at this CVPR workshop.

00:07.960 --> 00:12.280
I'm very excited to be presenting my research.

00:12.280 --> 00:19.440
I will talk about shape learning in biomedical imaging, while by shape learning I mean machine

00:19.440 --> 00:23.400
learning for data that are shapes.

00:23.400 --> 00:26.520
One data point is one shape.

00:26.520 --> 00:33.560
And specifically, we will be interested in the shapes that we see in biomedical images.

00:33.560 --> 00:38.800
For example, the shapes of the brains that we can see at the images at the bottom of

00:38.800 --> 00:41.800
this slide.

00:41.800 --> 00:47.920
So we're interested in the shapes of biological structures that appear in biomedical images.

00:47.920 --> 00:53.320
These biological structures can be found at the macroscopic scale all the way down to the

00:53.320 --> 01:00.320
nanoscopic scale, going from organs, such as the brains that we just saw, all the way

01:00.320 --> 01:05.760
down to tissues, cells, organelles, and molecules.

01:05.760 --> 01:14.400
And in 2022, we are actually very lucky, because in order to study these biological structures

01:14.400 --> 01:21.480
that are at the heart of life, we can just look at them.

01:21.480 --> 01:27.960
We have access to a wide range of imaging techniques that allow us to observe these

01:27.960 --> 01:30.520
biological structures.

01:30.520 --> 01:37.200
At the macroscopic scale, for example, we have MRI, or magnetic resonance imaging, that

01:37.200 --> 01:41.680
allows us to observe human brains.

01:41.680 --> 01:47.440
But all the way down to the nanoscopic scale, we have other techniques, ending with cryo-electron

01:47.440 --> 01:54.480
microscopy, which allows us to look at biomolecules, such as the human ribosome.

01:54.480 --> 02:01.120
Now the point that I want to make with this slide and with this presentation is that in

02:01.120 --> 02:07.960
all of these images, there is one feature that is very interesting in the sense that

02:07.960 --> 02:11.400
it can be linked to the biology.

02:11.400 --> 02:14.520
This feature is the shape.

02:14.520 --> 02:20.800
The shape of the biological structure that is being imaged contains enormous information

02:20.800 --> 02:26.840
on the biology and the biophysics of what is happening in living organisms.

02:26.840 --> 02:29.880
Let me give you an example of this.

02:29.880 --> 02:37.920
Actually, let me give you a few examples on how shapes of biological structure contain

02:37.920 --> 02:39.280
meaningful information.

02:39.760 --> 02:46.520
Few examples in the context of one pathology called Alzheimer's disease.

02:46.520 --> 02:53.680
At the macroscopic scale first, Alzheimer's disease is linked to shape changes.

02:53.680 --> 02:58.880
If we look at the simulation that I had on the first slide, we see the evolution of

02:58.880 --> 03:00.480
a brain through time.

03:00.480 --> 03:07.440
Actually, we see three slices of a 3D brain evolving through time.

03:07.440 --> 03:11.080
This is a simulation of a brain with Alzheimer's disease.

03:11.080 --> 03:17.800
You can see on the simulation that the brain changes shape as it evolves through time.

03:17.800 --> 03:23.400
The ventricles, which are the structure in the center of this brain, are becoming larger

03:23.400 --> 03:28.160
and larger, so that's the area in black that is in prison.

03:28.160 --> 03:33.880
As the brain undergoes atrophy, meaning the brain matter around this ventricle is becoming

03:33.880 --> 03:36.120
smaller and smaller.

03:36.120 --> 03:44.320
This atrophy is a macroscopic shape change that is characteristic of Alzheimer's disease.

03:44.320 --> 03:48.760
Now you may ask, where does this atrophy come from?

03:48.760 --> 03:52.880
Where do these macroscopic shape changes come from?

03:52.880 --> 03:59.640
Actually, at the microscopic scale now, if we look at cells, so at the neurons, these

03:59.640 --> 04:01.160
neurons are dying.

04:01.160 --> 04:07.360
The other reasons were why we see this atrophy at the macroscopic scale.

04:07.360 --> 04:11.640
When a neuron is dying, it changes shape.

04:11.640 --> 04:18.360
As you can see on this video of a dying neuron, it's retracting its axons.

04:18.360 --> 04:25.640
At the microscopic scale, for the scale of cells, Alzheimer's disease is linked to shape

04:25.640 --> 04:28.480
changes.

04:28.480 --> 04:34.240
What's even more interesting is that at the macroscopic scale, there is also a shape problem.

04:34.240 --> 04:41.640
We have these two proteins, eta-amyloid and tau, that are changing shape in patient with

04:41.640 --> 04:43.920
Alzheimer's disease.

04:43.920 --> 04:50.240
Because they have changed shape, they will agglomerate either inside the neurons, so

04:50.240 --> 04:57.240
we see in blue, that the proteins agglomerate inside the neuron, or they will agglomerate

04:57.240 --> 05:05.240
outside the neuron, and these agglomerations is what will cause the neuron to die.

05:05.240 --> 05:13.560
A shape problem at the nanoscopic scale is creating the death of the neurons, which is

05:13.560 --> 05:21.760
creating a microscopic shape change, which is also leading to a macroscopic shape change

05:21.760 --> 05:28.360
with the atrophy of the brain that we can see on MRI.

05:28.360 --> 05:37.080
We have illustrated that biological shape contains biologically relevant information,

05:37.080 --> 05:43.920
or medically relevant information, that we can go from biological shapes to biomedical

05:43.920 --> 05:44.920
insights.

05:45.280 --> 05:51.360
Actually, biophysics tells us that the healthy or the pathological state of the biological

05:51.360 --> 05:58.440
structure will change its shape, or generally that the function of a biological structure

05:58.440 --> 06:00.160
will change its shape.

06:00.160 --> 06:06.960
For example, blood cells have a different shape than brain cells or neurons, because

06:06.960 --> 06:09.240
they have different functions.

06:09.240 --> 06:15.320
In the context of this talk, we are interested in inverting that model, in learning biophysics

06:15.320 --> 06:20.840
or biology, from the data that we have, which are the shapes.

06:20.840 --> 06:30.120
So we do statistics and machine learning on shapes, which is what we will call shape learning.

06:30.120 --> 06:37.840
Let me show you how we can do shape learning in biomedical imaging, by showing you the

06:37.840 --> 06:41.960
architecture of a typical research project in our lab.

06:41.960 --> 06:45.760
Usually, it starts with a biomedical question.

06:45.760 --> 06:52.800
For example, what is the acceleration of the atrophy that we see in the brains of patients

06:52.800 --> 06:54.240
with Alzheimer's disease?

06:54.240 --> 07:01.080
It will be interesting to know what's the magnitude of these accelerated brain shape

07:01.080 --> 07:08.200
changes, because it could give ideas to build automatic diagnosis procedure.

07:08.200 --> 07:11.920
Another biomedical question could be, how do cells move?

07:11.920 --> 07:19.120
What you see on this slide, a moving cell, so called a migrating cell.

07:19.120 --> 07:25.120
How can cell move is a very important question when one studies metastatic cancer, when the

07:25.120 --> 07:28.840
cancerous cells propagate in the human body.

07:28.840 --> 07:32.440
All in all, it starts with a biomedical question.

07:32.440 --> 07:38.400
Then there is a step of image acquisition, acquiring the MRIs, acquiring the microscopy

07:38.400 --> 07:43.400
images, that is done by collaborators of all that.

07:43.400 --> 07:46.560
Then there is an important step of shape reconstruction.

07:46.560 --> 07:55.400
Yes, because the data that we get are images or videos, so the shape has not been extracted

07:55.400 --> 07:56.400
yet.

07:56.400 --> 08:02.440
We made it an algorithm of contour detection, edge detection, or segmentation to reconstruct

08:02.440 --> 08:05.240
the shape.

08:05.240 --> 08:07.680
Then there is a step of shape modeling.

08:07.680 --> 08:11.040
You could also think of it as shape featurization.

08:11.040 --> 08:16.600
Let's say we have extracted the border of the outline of that cell.

08:16.600 --> 08:20.680
How do we want to represent this shape in a computer?

08:21.320 --> 08:26.240
There are different choices that one can make, and we're going to see a few of them.

08:26.240 --> 08:33.320
And lastly, when we have represented the shape, we have chosen a feature to represent the shape.

08:33.320 --> 08:38.040
We can do statistics, machine learning, and deep learning on these data points that are

08:38.040 --> 08:39.040
shapes.

08:39.040 --> 08:43.040
That's what they call shape learning.

08:43.040 --> 08:49.880
In the context of this talk, we will focus on the last two of this pipeline of this cycle.

08:49.880 --> 08:53.400
How can we represent a shape in a computer?

08:53.400 --> 08:58.520
And then when we have represented the notion of shape in a computer, how can we do machine

08:58.520 --> 09:02.880
learning on shapes?

09:02.880 --> 09:08.160
Let's start with how we represent the notion of shape in a computer.

09:08.160 --> 09:14.840
So shape modeling, our shape representation is interesting because we have different

09:14.840 --> 09:20.760
choices as to how to model the shape of the biological structure.

09:20.760 --> 09:28.600
But irrespective of these choices, very often, we end up computing with manifolds.

09:28.600 --> 09:30.160
What is a manifold?

09:30.160 --> 09:35.320
On the left, you have a visual definition of what is a manifold.

09:35.320 --> 09:44.120
We can generally define a manifold as a generalization of a vector space that is allowed to be curved.

09:44.120 --> 09:49.760
On the left, for example, you can see a 2D vector space, also called a plane, that is

09:49.760 --> 09:51.160
not curved.

09:51.160 --> 09:53.360
It's linear or flat.

09:53.360 --> 10:01.120
And just below it, we have an example of a 2D manifold M that is itself allowed to be

10:01.120 --> 10:02.120
curved.

10:02.120 --> 10:04.840
You can see that there is some curvature there.

10:04.840 --> 10:11.400
Now, this is interesting for us because, as I said, we can try different models of shapes,

10:11.400 --> 10:17.920
and representations of shapes, but very often, we will see this manifold M appear.

10:17.920 --> 10:23.480
Let me give you examples of how manifolds enter the computations.

10:23.480 --> 10:27.880
First, let's look at the shapes themselves.

10:27.880 --> 10:34.080
Let's say we are interested in shape of this cat, and more precisely, the shape of the

10:34.080 --> 10:37.440
surface that is defining the cat.

10:37.440 --> 10:43.080
The shape of the surface is a generalization of a vector space that is allowed to be curved,

10:43.080 --> 10:50.960
and so the shape itself is the manifold in this case, so I write M equal to the shape.

10:50.960 --> 10:53.040
The manifold is the shape.

10:53.040 --> 10:58.440
If you want to compute on these shapes, for example, define a hit map or a scalar field

10:58.440 --> 11:05.840
on this shape, you are working with defining a function on the manifold.

11:05.840 --> 11:09.480
Another example is a bit more abstract.

11:09.480 --> 11:13.760
The manifold can be the space of all the shapes.

11:13.760 --> 11:18.480
In the first case, one shape is a manifold.

11:18.480 --> 11:24.600
In the second case, we consider the space of all possible shapes, but you have illustrated

11:24.600 --> 11:31.200
with this sphere is actually the shape space of all the possible triangles, so you can

11:31.200 --> 11:37.080
see if you look, maybe you can see, that I have superimposed a little triangle on this

11:37.080 --> 11:40.840
sphere, because here there is a triangle.

11:40.840 --> 11:47.440
This sphere actually represents the space of all the triangular shapes, so all the visible

11:47.440 --> 11:48.440
shapes.

11:48.440 --> 11:53.400
Now, this illustration is the shape space of triangles, but more generally, the shape

11:53.400 --> 11:58.200
space of surfaces is also a manifold.

11:58.560 --> 12:03.040
Continuing, you could be interested in the space of shape motions.

12:03.040 --> 12:06.840
For example, how your shape translates and rotates.

12:06.840 --> 12:11.080
Here you have a cancer cell translating and rotating.

12:11.080 --> 12:16.640
If you want to do statistics on all the possible motions that your shape can perform, then you're

12:16.640 --> 12:23.200
doing statistics on, again, a manifold M. That is this time, the space of shape motions.

12:24.120 --> 12:30.240
And lastly, you could be interested in how shapes deform, or a smooth deformation of

12:30.240 --> 12:35.000
the cancer cell, as you can see on the left, but also other objects like functional maps

12:35.000 --> 12:39.720
that explain how shapes can deform.

12:39.720 --> 12:45.320
And interestingly, again, if you choose this shape representation, you end up with a space

12:45.320 --> 12:49.880
of shapes deformation, that is a manifold.

12:49.880 --> 12:54.280
There are other models of shape that I will not cover here, but it seems that these four

12:54.280 --> 13:03.280
models of shapes are quite common, and all of them require thinking about manifolds.

13:03.280 --> 13:07.080
Thinking about manifolds, what can we do with manifolds?

13:07.080 --> 13:12.880
Let's think about generalizing basic operations, and for example, generating the definition

13:12.880 --> 13:15.480
of a new manifold.

13:15.480 --> 13:18.280
On the center of the screen, I put the sphere.

13:18.280 --> 13:21.920
The sphere is one example of manifolds that we had just before.

13:21.920 --> 13:25.880
It can represent the shape space of triangles.

13:25.880 --> 13:29.800
The one point of the sphere is the shape of a triangle.

13:29.800 --> 13:35.880
You can also think of it as one point of the sphere is a shape motion.

13:35.880 --> 13:40.160
How do I compute the mean of two triangles?

13:40.160 --> 13:42.960
That will be the two real points here.

13:42.960 --> 13:46.560
Or how do I compute the mean of two shape transformations?

13:46.560 --> 13:49.960
That will be the two blue points here.

13:49.960 --> 13:54.560
I could use the traditional definitions of statistics, and I can use the traditional

13:54.560 --> 14:00.600
definition of mean and compute the mean as this orange point, which would be computed

14:00.600 --> 14:03.200
as the middle of the two blue points.

14:03.200 --> 14:09.240
But in that case, if I apply traditional statistics, there is a problem because you see the mean

14:09.240 --> 14:12.120
in orange does not belong to the manifold.

14:12.600 --> 14:18.560
Now, if the manifold represents the shape space of triangles, what we are saying in applying

14:18.560 --> 14:24.200
traditional statistics is that the mean of two triangles is not even a triangle.

14:24.200 --> 14:29.640
So traditional statistics do not really apply in this setting.

14:29.640 --> 14:37.240
We would like to generalize operations, statistics, machine learning, and deep learning to manifolds.

14:37.240 --> 14:42.600
So that when I try to compute the mean of these two blue points, that again, the two

14:42.600 --> 14:48.800
triangle shapes, two shape deformations, I end up with a orange point that is at least

14:48.800 --> 14:53.040
part of the data space that I'm interested in.

14:53.040 --> 14:57.560
So we'd like to generalize all of these operations to manipulate.

14:57.560 --> 15:02.520
Why would we want to do that instead of, for example, just using traditional statistics

15:02.520 --> 15:06.000
and then projecting back on the manifold?

15:06.040 --> 15:12.040
This is because knowing that the data space is a manifold is information.

15:12.040 --> 15:17.680
Knowing the geometry of the shape space, of the space of shape motions, of the space of

15:17.680 --> 15:20.360
shape deformations, that is information.

15:20.360 --> 15:25.640
And any information that we can incorporate in the learning algorithm is welcome.

15:25.640 --> 15:30.480
So in other words, we would like to use the geometry of manifolds as an inductive bias

15:30.480 --> 15:34.120
in our analysis.

15:34.120 --> 15:36.840
So let's see how we can do this.

15:36.840 --> 15:39.920
So I've just introduced shape modeling.

15:39.920 --> 15:47.880
I've introduced four models of shapes, four representations of shapes, and all of them

15:47.880 --> 15:51.400
require us to compute on manifolds.

15:51.400 --> 15:57.360
Now let's move to shape deep learning, which is a way of computing on manifolds, and see

15:57.360 --> 16:02.720
how we can define shape deep learning for these manifold spaces.

16:05.120 --> 16:08.600
So let's think about what is deep learning first.

16:08.600 --> 16:16.880
I've put four big categories of subfields of deep learning, or machine learning in general,

16:16.880 --> 16:23.120
supervised learning, unsupervised learning, reinforcement learning, and optimization.

16:23.120 --> 16:27.480
What is supervised deep learning in the context of shape learning?

16:27.480 --> 16:34.080
In supervised learning, the goal is to learn a map from an input space to an output space

16:34.560 --> 16:43.720
Y. Now let's rename the input space X into Mx, to denote that the input space can be a manifold.

16:43.720 --> 16:52.760
And let's rename the output space Y as MY to denote that the output space can be a manifold.

16:52.760 --> 16:57.840
So this is a setting that will appear if we want to do shape deep learning.

16:57.840 --> 17:04.760
As an example, let's say Mx is just an image, a 2D image of a 3D object.

17:04.760 --> 17:09.960
In that case, it's not, I mean, it's a special case of a manifold, but it's not a curved manifold.

17:09.960 --> 17:15.400
And let's say Y is the space of the pose of the subject.

17:15.400 --> 17:18.080
In that case, MY would be a manifold.

17:18.080 --> 17:26.040
That's an example of shape learning when we want to predict an element of MY, therefore an element of a manifold.

17:26.080 --> 17:30.160
You can have all the cases where Mx is the manifold.

17:30.160 --> 17:36.560
For example, you see the shape of the brain and you want to predict if that brain has Alzheimer's disease.

17:36.560 --> 17:44.560
In that case, Mx is the space of all possible brain shapes and Y is the probability of having Alzheimer's disease.

17:44.560 --> 17:51.000
And you can also have a case where Mx is the manifold and MY is the manifold.

17:51.040 --> 17:59.400
In unsupervised learning, the goal is to find patterns in the data that belong to the data space M.

17:59.400 --> 18:05.840
Let's say we want to find patterns in a data space of shapes, then M is a manifold.

18:05.840 --> 18:12.800
And so we need to generalize unsupervised deep learning to unsupervised learning to manifolds.

18:12.800 --> 18:17.680
An example of unsupervised learning would be if we want to find clusters of shapes.

18:17.680 --> 18:22.880
We need to find a method that can perform clustering of manifolds.

18:22.880 --> 18:25.400
In reinforcement learning, same story.

18:25.400 --> 18:33.000
Let's say we want to learn the policy by that gives the priority of applying an action on the state space.

18:33.000 --> 18:38.920
Then each of these spaces can also be a manifold or both into a manifold.

18:38.920 --> 18:44.960
For example, we want to learn how a shape should move to optimize a certain feature.

18:44.960 --> 18:53.200
And lastly, what is underpinning a lot of development in deep learning is the broader field of optimization.

18:53.200 --> 18:57.920
And same thing, we may wonder how this can generalize to manifolds.

18:57.920 --> 19:04.000
Let's say we want to find the optimal shape that verifies a certain feature.

19:04.000 --> 19:10.960
In that case, we want to perform an optimization where the parameter that is varying is actually varying on a manifold.

19:10.960 --> 19:14.280
So we want to perform optimization of manifold.

19:14.280 --> 19:21.720
The example here, let's say we want to find the optimal deformation that can map one shape onto the other.

19:21.720 --> 19:26.880
The deformation is an element of the manifold of the patients.

19:26.880 --> 19:32.440
And therefore, this will be an example of optimization on manifolds.

19:32.440 --> 19:41.200
So how can we generalize all of these wide fields of deep learning to manifolds?

19:41.200 --> 19:47.160
And that's actually a trick that I call the vector space manifold conversion trick.

19:47.160 --> 19:49.440
And the trick is as follows.

19:49.440 --> 19:55.280
You can realize that all of these machine learning, such as deep learning algorithms,

19:55.280 --> 19:58.920
rely on the same basic building blocks.

19:58.920 --> 20:06.560
By basic building blocks, I mean some sets of abstract elements and some set of abstract operations.

20:06.560 --> 20:16.440
Now, if we can convert these building blocks from their definition on vector spaces to their definitions on manifold,

20:16.440 --> 20:24.640
then we have a translation in the sense of language translation that allow us to beautifully convert any type of statistical learning,

20:24.640 --> 20:28.880
machine learning, and deep learning algorithm to manifolds.

20:28.880 --> 20:30.280
So let's do this.

20:30.280 --> 20:39.480
What are the basic building blocks that are at the core of every statistical learning, machine learning, and deep learning algorithm?

20:39.480 --> 20:47.880
Well, in terms of the elements, almost all of these algorithms use either points or vectors.

20:47.880 --> 20:54.640
So point on the data space, in our case, one point in your shape, one point can be a deformation.

20:54.640 --> 20:56.640
They also use vectors.

20:56.640 --> 20:59.000
Think about gradient descent.

20:59.000 --> 21:04.360
The gradient is a vector anchored at a point on the manifold.

21:04.360 --> 21:12.320
And then in terms of operations, there are three operations that come over and over again in these algorithms,

21:12.320 --> 21:18.480
which are computing a straight line, computing a distance, and performing a net issue.

21:18.480 --> 21:24.400
So straight line, linear regression, our principal component analysis, we are using straight lines.

21:24.400 --> 21:26.960
So we need a way of computing straight lines.

21:27.000 --> 21:36.600
The squared distance, in many loss functions, we use a notion of squared distance between the ground truth and our prediction.

21:36.600 --> 21:39.160
Very often the L2 distance.

21:39.160 --> 21:41.280
And then the addition.

21:41.280 --> 21:49.280
For example, when we add a gradient to an estimate, we are adding the gradient vector to the estimate point.

21:49.280 --> 21:55.160
So we are adding the black arrow to this blue point when we get an orange point.

21:55.160 --> 22:01.600
Now, assuming that most of the learning algorithms uses these basic elements and operations,

22:01.600 --> 22:09.720
if we can generalize or convert these elements and operations to manifold, then we have our conversion checks.

22:09.720 --> 22:12.560
I'm going to introduce the conversion.

22:12.560 --> 22:15.680
Yes, we want to convert to manifold.

22:15.680 --> 22:18.040
The points are just points.

22:18.040 --> 22:20.640
They are the elements of our manifolds.

22:20.640 --> 22:26.400
What was a vector on the vector space becomes a tangent vector on the manifold.

22:26.400 --> 22:31.000
So on the vector space, point and vectors are approximately the same.

22:31.000 --> 22:36.720
On the manifold, points and tangent vectors are two different, very different elements.

22:36.720 --> 22:45.640
On the left, the registration on the left, you can see the tangent vector in black that is anchored at the point in blue on the manifold.

22:45.640 --> 22:48.920
So we have generalized basic elements.

22:48.920 --> 22:51.960
Now let's generalize the operations.

22:51.960 --> 22:56.440
The straight line becomes a geodesic on M.

22:56.440 --> 23:03.080
I'm not telling you how we compute the geodesic, I'm telling you the translation or conversion of the operation.

23:03.080 --> 23:14.520
For example, on the registration on the left, you can see the dotted black line is the geodesic between the blue point and the orange point.

23:14.520 --> 23:22.240
The square distance between two points on the manifold becomes the square geodesic distance between the same two points.

23:22.240 --> 23:31.320
We're basically the length of the square length of the geodesic that is connecting these two points.

23:31.320 --> 23:35.200
And lastly, we need a generalization of the notion of addition.

23:35.200 --> 23:42.920
On the vector space, we can add a vector, for example, a vector in black to a point here in blue.

23:42.920 --> 23:46.920
On the manifold, this operation is now called the exponential map.

23:46.920 --> 23:56.320
And it is, again, just rated on the left, while we add not a vector, but tangent vector, so the tangent vector in black to the point in blue.

23:56.320 --> 24:05.160
And performing this addition operation allows us to reach another point on the manifold, which is the point in orange.

24:05.160 --> 24:11.360
So that is the vector space manifold conversion.

24:11.360 --> 24:25.120
And this trick, we implemented it in this open source package called GM Stats, which generalizes all of these operations that we know very well on vector spaces to manifolds.

24:25.120 --> 24:35.520
And for different types of manifolds, specifically the different types of manifold that I showed you in the context of shape representation and shape models.

24:35.520 --> 24:43.320
It's a Python package that is available online at these links, and we created it with three objectives.

24:43.320 --> 24:46.200
First, to teach hands-on geometric learning.

24:46.200 --> 24:55.120
So computing on manifolds can be hard in practice, even though the word manifold comes over and over again in manifold learning, for example.

24:55.120 --> 24:59.840
The basic operations on manifolds are sometimes hard to implement.

24:59.840 --> 25:05.800
And one of the reasons is that they're not necessarily taught in class besides textbooks.

25:05.800 --> 25:11.720
So with this software, you can teach hands-on manifolds and geometric learning.

25:11.720 --> 25:17.320
The second objective is to support research in geometric learning or learning on manifolds.

25:17.320 --> 25:25.640
So different researchers reach out to us who publish, they have published papers and they want to implement them methods in GM Stats.

25:25.640 --> 25:30.480
So as to make them methods better, more available to the general public.

25:30.480 --> 25:35.160
So that's on the contributor side supporting the research.

25:35.160 --> 25:42.440
And the last objective is to democratize the use of geometric learning, in other words, learning on manifolds.

25:42.440 --> 25:53.520
Because once these methods are incorporated into GM Stats, from a user's perspective, you only need to know the conversion that I just presented you on the previous slide,

25:53.520 --> 25:56.000
and you can use all of these algorithms.

25:56.000 --> 25:59.960
So you do not need to know how a geodesic is implemented.

25:59.960 --> 26:04.720
You just need to know that the geodesic is a generalization of a straight line.

26:04.720 --> 26:10.480
So it allows to democratize the use of these methods.

26:10.480 --> 26:21.280
Now other libraries that implement computations on manifolds, learning on manifolds and optimization on manifolds, you can see them here on the screen.

26:21.320 --> 26:29.680
The first three, the one that have opt in it and MacDodge, they are focused on optimization on manifolds.

26:29.680 --> 26:38.280
For example, to minimize a criterion and the parameter minimizing this criterion is an element of the manifold.

26:38.280 --> 26:48.840
And then all the others are Python packages that perform computations and learning on manifolds, but they focus on a special type of manifolds.

26:48.840 --> 26:53.720
For example, the manifold of SPD mattresses, tree rotations, disruptions.

26:53.720 --> 27:09.440
By contrast, we do not do fancy optimizations in GM Stats, but we try to implement a very wide range of manifolds, including the manifolds that represent shapes and shapespaces.

27:09.440 --> 27:19.720
So let me give you an example of how you can use this software to compute with shape representations and let's say shape motions.

27:19.720 --> 27:31.480
So let's say you're interested in knowing how a shape evolved from being in an orientation R1 and translation or position T1.

27:31.480 --> 27:45.640
And how it evolves from this original pose to an end pose, which we could call R1 for rotation R2, for rotation two and T2 for translation two.

27:45.640 --> 27:55.120
So when we compute with rotation and translations, as we do with motions and by the way, it should be 3D translation, not 2D translation.

27:55.120 --> 28:07.160
We are computing with elements that belong to the manifold and that gave the manifold is M equal SE3 and that stands for special Euclidean group in three dimension.

28:07.160 --> 28:12.160
In other words, the group of all 3D translations and 3D rotations.

28:12.160 --> 28:15.640
This is a good snippet using GM Stats.

28:15.640 --> 28:21.920
What you really have to do is instantiate the manifold that you are interested in.

28:21.920 --> 28:32.760
Here we want to work with SE3, so we instantiate SE3 as an object of the class special Euclidean in dimension three.

28:32.760 --> 28:34.880
We extract what's called a metric.

28:34.880 --> 28:38.840
It's a Riemannian metric that allows us to perform the computations.

28:38.840 --> 28:42.560
And then we call metric dot geodesic.

28:42.560 --> 28:45.880
Again, we do not need to know how the geodesic is implemented.

28:45.880 --> 28:50.640
We just need to know that the geodesic is the generalization of the straight line.

28:50.680 --> 28:59.760
And therefore, we can compute the geodesic that starts with an initial point, rotation one, translation one, and has an initial tangent vector.

28:59.760 --> 29:04.000
And on the right, you have an illustration of the geodesic.

29:04.000 --> 29:09.440
Pose one corresponds to R1T1, rotation one, translation one.

29:09.440 --> 29:20.600
It's represented by one very small frame with three axes because we can use the translation, translation one, to place this frame in 3D.

29:20.840 --> 29:25.920
And the rotation, rotation one to orient the frame in 3D.

29:25.920 --> 29:31.720
But therefore, one little frame corresponds to one point in SE3.

29:31.720 --> 29:43.520
And you see here a trajectory of frames that correspond to the geodesic on the manifold SE3 linking pose one to pose two.

29:43.520 --> 29:50.480
So I told you we implemented a lot of different manifolds in GM stats.

29:50.480 --> 29:55.560
And what we end up with is actually a numerical taxonomy of manifolds.

29:55.560 --> 30:06.040
So we use object-oriented programming to create this hierarchy of all the possible of many manifolds that you could be interested in working with.

30:06.040 --> 30:12.120
So this hierarchy is built as follows on the root of the hierarchy of the tree.

30:12.120 --> 30:17.320
You have the most abstract concept of manifold that is just a manifold.

30:17.320 --> 30:25.080
And we list all the different attributes or methods that a manifold item object should have.

30:25.080 --> 30:32.680
And as you go down this hierarchy, you find more and more specialized or more and more concrete manifolds.

30:32.680 --> 30:38.600
For example, one level down, you found the manifold matrix lead group.

30:38.600 --> 30:41.560
Lead group is a special case of a manifold.

30:41.560 --> 30:45.720
It's a manifold that has an algebraic group structure.

30:45.720 --> 30:49.560
So therefore, matrix lead group is on the second level of the hierarchy.

30:49.560 --> 30:56.600
It inherits from the Python class manifold because it's a special case of a manifold.

30:56.600 --> 31:02.200
And then you go down this hierarchy and at the very bottom of this hierarchy, you have the leaves.

31:02.200 --> 31:05.600
And these are manifolds that you can actually instantiate.

31:05.600 --> 31:11.600
So for example, there will be somewhere in there, the special ectogen proof that we just used,

31:11.600 --> 31:16.960
which would inherit from matrix lead group because it's a special type of matrix lead group.

31:16.960 --> 31:21.520
And again, matrix lead group itself inherits from manifolds.

31:21.520 --> 31:27.440
So with this project that started with representing shapes and doing statistical learning on shapes,

31:27.440 --> 31:34.160
we actually ended up creating a computational representation of differential geometry.

31:36.320 --> 31:42.800
Okay, so let me wrap this up a little bit and show you how we can mix these two.

31:43.520 --> 31:52.160
So how we can use a model of shape to how can we can perform a deep learning algorithm

31:52.160 --> 31:56.000
on a manifold that would, for example, represent the shape space.

31:57.360 --> 32:01.760
And the example I'm going to choose is variational encoders.

32:01.840 --> 32:05.920
How do variational encoders work on manifolds?

32:08.160 --> 32:14.240
Zooming out a little bit, let me show you an overview of the research that I'm interested in.

32:15.040 --> 32:22.080
In the previous slides, I was showing you the CRT of computational differential geometry,

32:22.080 --> 32:23.840
which is the ERT of manifolds.

32:25.040 --> 32:29.840
I'm interested in generalizing statistical learning, machine learning and deep learning

32:29.920 --> 32:36.400
on these exotic data spaces. In other words, I'm interested in filling the table that you see on

32:36.400 --> 32:44.640
this slide, where in this table, the different rows represent different manifolds. So each of the row

32:44.640 --> 32:49.440
in this table corresponds to one of the nodes that you had in the previous CRT.

32:50.080 --> 32:52.960
There are different types of exotic data spaces, if you wish.

32:53.840 --> 32:58.400
And in this table, the different columns represents different fields of statistics,

32:58.720 --> 33:04.800
machine learning and deep learning. We're going to talk about theorizing variational encoders to

33:04.800 --> 33:10.640
manifolds. So therefore, we are in the column dimension reduction and deep learning.

33:12.960 --> 33:18.800
So dimension reduction and variational encoders. I'd like to present a geometric perspective

33:19.440 --> 33:26.800
on them as to emphasize where manifolds come in. On this first line of this table,

33:27.760 --> 33:36.480
you can see two classes of dimension reduction methods. The first classes, the first class of

33:36.480 --> 33:42.240
methods, are the methods that seek to perform dimension reduction on an ambient space that is

33:42.240 --> 33:49.200
a vector space. Specifically, let's say the vector space is R2, and we have some data points,

33:49.200 --> 33:56.400
which are the dark green data points. We can think of principal component analysis that seeks to

33:56.480 --> 34:04.800
learn a linear subspace that would be the light green line within this linear space that is R2.

34:05.440 --> 34:10.480
And you have different dimension reduction methods that work like this, that wish to

34:10.480 --> 34:18.240
learn a linear subspace within a linear space. In the manifold world, we remember that linear

34:19.200 --> 34:26.880
subspace, or that linear line, is converted to geodesic. And so you have equivalent methods

34:26.880 --> 34:34.080
that allow to learn the equivalent of a line that is a geodesic on a manifold. So this time,

34:34.080 --> 34:40.240
the ambient space is a manifold and will be this clear. And we're interested in learning

34:40.240 --> 34:47.360
principal directions of variations of the data, the data of the dark green point. And we want to

34:47.360 --> 34:52.800
learn principal directions of variations of this data that will be the equivalent of the lines on

34:52.800 --> 34:58.560
the left, so that are the geodesic. The geodesic on the sphere is a great circle, and you can see in

34:58.560 --> 35:05.680
light green, a great circle. In the context of variational tranquillers, the vector space case,

35:06.240 --> 35:11.360
we are not linear. We're using deep learning, and most of the time we are highly nonlinear.

35:12.320 --> 35:18.320
Therefore, we need another row in this table to complete the overview of dimension

35:18.320 --> 35:23.760
reduction methods from a geometric perspective. And then we talk about the one that is on the

35:23.760 --> 35:30.320
bottom left, which include, which is a class of methods that include variational tranquillers.

35:31.120 --> 35:37.600
These class of methods operate in an ambient space, that is, again, a vector space. It's again

35:38.400 --> 35:46.160
represented as R2 here. But instead of learning variations of the data that are linear, as this

35:46.160 --> 35:54.320
here would do, instead, we are allowed to learn a nonlinear subspace M. So you see the dark

35:54.320 --> 36:04.960
light green line is now nonlinear. So I'm going to present how we can generalize this VAE,

36:04.960 --> 36:12.880
presented from this geometric perspective here, to geometric VAE, or VAE on manifolds,

36:12.880 --> 36:20.000
which is what is shown on the bottom right. So now the setting is the ambient space is a manifold.

36:20.640 --> 36:28.160
So the sphere represents the ambient space, that is the manifold, to which we know that the data are,

36:28.160 --> 36:33.520
in which we know that the data are. And now we don't only restrict ourselves to learning

36:33.520 --> 36:41.040
a geodesic subspace, as the road just above was doing, but we allow to learn any nonlinear subspace,

36:42.000 --> 36:47.520
or non-geodesic submanifolds, and within the manifold M.

36:49.760 --> 36:55.280
But how does this work? Let me review variational tranquillers. I put

36:56.240 --> 37:00.640
equations on the left for completeness. I'm not going to comment on them too much.

37:01.600 --> 37:08.880
Variational tranquillers start with a generative model of data in Rd, that's the ambient space

37:08.880 --> 37:14.960
that we had before. The data assumed to belong to a vector space. And we can think of variational

37:14.960 --> 37:23.840
tranquillers as a way to invert a generative model with latent variables. And typically,

37:23.840 --> 37:28.640
a generative model with latent particles, such as the one that is in the first equation.

37:29.360 --> 37:36.240
So we have xi, that's our data in the vector space Rd, that is assumed to be generated

37:36.240 --> 37:42.560
with a generative model with latent variables. But the latent variable is xi, they are going to

37:42.560 --> 37:49.520
represent the low dimensional representation of the xi. So this low dimensional representation,

37:49.520 --> 37:57.360
xi, pass through a function, f, this parameter mu and w, this function is usually represented by a

37:57.360 --> 38:04.000
neural network, that is called the decoder, variational tranquillers, to which noise is added.

38:04.800 --> 38:10.000
So that's the generative model that we assume has generated data in variational tranquillers.

38:11.040 --> 38:18.720
From a geometric perspective, xi are assumed to belong to a lower dimensional latent space,

38:19.440 --> 38:26.960
that I write Rl. Then this xi passes through the function f of mu and w,

38:26.960 --> 38:31.520
which can be the neural network. And by passing through this function, it becomes

38:31.520 --> 38:37.120
an element of the higher dimensional space Rd. So this is the blue point on the illustration.

38:37.920 --> 38:45.680
And then noise is added to the model. Now, if we were to pass the whole latent space Rl

38:46.400 --> 38:54.720
through the function f mu and w, we would get this nonlinear light green curve that represents

38:54.720 --> 39:01.120
the subspace, the nonlinear subspace that we are learning. That's a geometric explanation

39:01.120 --> 39:06.960
of the generative model of the data that is at the foundation of variational tranquillers.

39:07.760 --> 39:13.200
Now, with GAE, we do not observe, we do not know what is f of mu and w.

39:14.400 --> 39:18.160
We model it as a decoder, but we do not know what are the weights of the decoder.

39:18.960 --> 39:26.320
And we also, we do not know what are the xi that are associated to each xi. So,

39:26.320 --> 39:33.520
given only the xi's, we would like to be able to learn the low-dimensional representations

39:33.520 --> 39:42.800
xi together with the decoder, the model. And this is done with this architecture that you

39:42.800 --> 39:47.680
might be familiar with. On the left, you have the traditional architecture of the variational

39:47.680 --> 39:53.680
tranquillers. On the right, you have the last function that is used to train it. So in the

39:53.680 --> 40:02.320
architecture of the GAE, the second half represents the decoder. It is the representation of the

40:02.320 --> 40:08.160
generative model that takes an element of the lower-dimensional latent space in green

40:08.160 --> 40:15.040
and outputs the function f of that latent variable. And on the left, we add an encoder

40:15.600 --> 40:22.640
that is able to perform the operation of going from xi to a normal size approximation

40:22.640 --> 40:29.040
approximates representation of the posterior of the xi. Anyway, this is all trained with the

40:29.040 --> 40:35.280
last function that is called the elbow. It stands for evidence lower bound. It's a lower bound of

40:35.280 --> 40:42.000
the likelihood, of the log likelihood. And I've given its expression here as a sum of a reconstruction,

40:42.880 --> 40:48.720
a term and a regularization term, because these are the terms that we have to generalize to manifold

40:49.600 --> 40:53.280
to have a version of variational tranquillers that works on manifold.

40:53.600 --> 41:03.200
So let's try to do this, to generalize variational tranquillers to manifold. We will have to generalize

41:03.200 --> 41:12.400
two elements. First, the generative model and second, the last function. Now, looking a little bit

41:12.400 --> 41:20.000
at the generative model and the last functions, we observe that they are built from basic elements

41:20.000 --> 41:26.320
and operations that we now know how to converge to manifold. So specifically in the generative model,

41:26.320 --> 41:32.240
we see there is a plus, which is an addition. And the generalization of the addition of vector

41:32.240 --> 41:38.320
spaces is the exponential map of manifold. In the last function also, we see that there is a square

41:38.320 --> 41:46.640
distance here, a squared L2 distance between x and f of c. Now that's something we can generalize

41:46.640 --> 41:57.280
to manifold to. And this is how we go to manifold variational tranquillers. It's conceptually easy.

41:57.280 --> 42:04.880
We replace the addition by the exponential map. And now we have a generative model that outputs

42:04.880 --> 42:14.320
points on the manifold. And that goes like this. We find again zi on the latent space RL. And now

42:14.320 --> 42:22.400
our decoder is going to be able to map zi on the point on manifold to which we add noise by adding

42:22.400 --> 42:31.040
a tangent vector to the point in group. So we have generalized the generative model to

42:31.040 --> 42:38.080
manifolds. We need to generalize the inference, which is how do we learn in this generative model.

42:39.040 --> 42:46.720
We have a very similar architecture that has an encoder and a decoder. But then what really changes

42:46.720 --> 42:53.760
now is the loss function that you can still formulate as an elbow of a different log likelihood,

42:53.760 --> 42:59.520
just because the generative model is different. And I've put in red the terms that change,

43:00.160 --> 43:05.120
because the generative model is different, the log likelihood is different, the evidence lower

43:05.200 --> 43:10.800
bound or the elbow is different. And therefore you can show that it has this formula.

43:12.160 --> 43:19.360
We could have guessed this formula by converting the elbow plus as it was written on vector spaces

43:19.360 --> 43:28.320
to its expression on manifolds. And so this allows us to learn non-giardasec

43:28.960 --> 43:37.360
sub-manifolds of a given manifold. I think the only competing method that was able to do that

43:37.360 --> 43:41.440
before was relying on Monte Carlo, for example, from the posterior. And therefore

43:45.760 --> 43:53.680
so using VAE we're able to learn non-giardasec sub-manifold of a manifold. We're also able to

43:53.680 --> 44:02.720
give some insights of what was observed in the literature about VAE, which was about this

44:02.720 --> 44:10.160
statement that you can see by Chao Edel in 2018. But I was saying that in VAE the experiment showed

44:10.720 --> 44:16.320
that these models represent real image data with manifolds that have surprisingly little curvature.

44:17.200 --> 44:23.600
Long story short, we are able through a geometric analysis of geometric VAE

44:23.600 --> 44:30.800
that gives insight on VAE to explain why they find latent spaces with shockingly little curvature.

44:33.440 --> 44:38.400
So to conclude, this is the type of pipeline that we're interested in our lab, we want to do shape

44:38.400 --> 44:45.360
learning in biomedical imaging. I've explained how shape modeling gives rise to

44:46.320 --> 44:52.560
overwhelming operations on excessive data spaces that are manifolds, and therefore how statistical

44:52.560 --> 44:57.600
learning, machine learning, and deep learning can generalize to these spaces if you want to shape

44:57.600 --> 45:06.640
deep learning. Many thanks to our lab at UCSB and many thanks to the organization that are funding

45:06.640 --> 45:10.720
this research, and I'll be happy to answer any questions.

