1
00:00:00,000 --> 00:00:07,960
Hi everyone, thank you very much for the invitation to speak at this CVPR workshop.

2
00:00:07,960 --> 00:00:12,280
I'm very excited to be presenting my research.

3
00:00:12,280 --> 00:00:19,440
I will talk about shape learning in biomedical imaging, while by shape learning I mean machine

4
00:00:19,440 --> 00:00:23,400
learning for data that are shapes.

5
00:00:23,400 --> 00:00:26,520
One data point is one shape.

6
00:00:26,520 --> 00:00:33,560
And specifically, we will be interested in the shapes that we see in biomedical images.

7
00:00:33,560 --> 00:00:38,800
For example, the shapes of the brains that we can see at the images at the bottom of

8
00:00:38,800 --> 00:00:41,800
this slide.

9
00:00:41,800 --> 00:00:47,920
So we're interested in the shapes of biological structures that appear in biomedical images.

10
00:00:47,920 --> 00:00:53,320
These biological structures can be found at the macroscopic scale all the way down to the

11
00:00:53,320 --> 00:01:00,320
nanoscopic scale, going from organs, such as the brains that we just saw, all the way

12
00:01:00,320 --> 00:01:05,760
down to tissues, cells, organelles, and molecules.

13
00:01:05,760 --> 00:01:14,400
And in 2022, we are actually very lucky, because in order to study these biological structures

14
00:01:14,400 --> 00:01:21,480
that are at the heart of life, we can just look at them.

15
00:01:21,480 --> 00:01:27,960
We have access to a wide range of imaging techniques that allow us to observe these

16
00:01:27,960 --> 00:01:30,520
biological structures.

17
00:01:30,520 --> 00:01:37,200
At the macroscopic scale, for example, we have MRI, or magnetic resonance imaging, that

18
00:01:37,200 --> 00:01:41,680
allows us to observe human brains.

19
00:01:41,680 --> 00:01:47,440
But all the way down to the nanoscopic scale, we have other techniques, ending with cryo-electron

20
00:01:47,440 --> 00:01:54,480
microscopy, which allows us to look at biomolecules, such as the human ribosome.

21
00:01:54,480 --> 00:02:01,120
Now the point that I want to make with this slide and with this presentation is that in

22
00:02:01,120 --> 00:02:07,960
all of these images, there is one feature that is very interesting in the sense that

23
00:02:07,960 --> 00:02:11,400
it can be linked to the biology.

24
00:02:11,400 --> 00:02:14,520
This feature is the shape.

25
00:02:14,520 --> 00:02:20,800
The shape of the biological structure that is being imaged contains enormous information

26
00:02:20,800 --> 00:02:26,840
on the biology and the biophysics of what is happening in living organisms.

27
00:02:26,840 --> 00:02:29,880
Let me give you an example of this.

28
00:02:29,880 --> 00:02:37,920
Actually, let me give you a few examples on how shapes of biological structure contain

29
00:02:37,920 --> 00:02:39,280
meaningful information.

30
00:02:39,760 --> 00:02:46,520
Few examples in the context of one pathology called Alzheimer's disease.

31
00:02:46,520 --> 00:02:53,680
At the macroscopic scale first, Alzheimer's disease is linked to shape changes.

32
00:02:53,680 --> 00:02:58,880
If we look at the simulation that I had on the first slide, we see the evolution of

33
00:02:58,880 --> 00:03:00,480
a brain through time.

34
00:03:00,480 --> 00:03:07,440
Actually, we see three slices of a 3D brain evolving through time.

35
00:03:07,440 --> 00:03:11,080
This is a simulation of a brain with Alzheimer's disease.

36
00:03:11,080 --> 00:03:17,800
You can see on the simulation that the brain changes shape as it evolves through time.

37
00:03:17,800 --> 00:03:23,400
The ventricles, which are the structure in the center of this brain, are becoming larger

38
00:03:23,400 --> 00:03:28,160
and larger, so that's the area in black that is in prison.

39
00:03:28,160 --> 00:03:33,880
As the brain undergoes atrophy, meaning the brain matter around this ventricle is becoming

40
00:03:33,880 --> 00:03:36,120
smaller and smaller.

41
00:03:36,120 --> 00:03:44,320
This atrophy is a macroscopic shape change that is characteristic of Alzheimer's disease.

42
00:03:44,320 --> 00:03:48,760
Now you may ask, where does this atrophy come from?

43
00:03:48,760 --> 00:03:52,880
Where do these macroscopic shape changes come from?

44
00:03:52,880 --> 00:03:59,640
Actually, at the microscopic scale now, if we look at cells, so at the neurons, these

45
00:03:59,640 --> 00:04:01,160
neurons are dying.

46
00:04:01,160 --> 00:04:07,360
The other reasons were why we see this atrophy at the macroscopic scale.

47
00:04:07,360 --> 00:04:11,640
When a neuron is dying, it changes shape.

48
00:04:11,640 --> 00:04:18,360
As you can see on this video of a dying neuron, it's retracting its axons.

49
00:04:18,360 --> 00:04:25,640
At the microscopic scale, for the scale of cells, Alzheimer's disease is linked to shape

50
00:04:25,640 --> 00:04:28,480
changes.

51
00:04:28,480 --> 00:04:34,240
What's even more interesting is that at the macroscopic scale, there is also a shape problem.

52
00:04:34,240 --> 00:04:41,640
We have these two proteins, eta-amyloid and tau, that are changing shape in patient with

53
00:04:41,640 --> 00:04:43,920
Alzheimer's disease.

54
00:04:43,920 --> 00:04:50,240
Because they have changed shape, they will agglomerate either inside the neurons, so

55
00:04:50,240 --> 00:04:57,240
we see in blue, that the proteins agglomerate inside the neuron, or they will agglomerate

56
00:04:57,240 --> 00:05:05,240
outside the neuron, and these agglomerations is what will cause the neuron to die.

57
00:05:05,240 --> 00:05:13,560
A shape problem at the nanoscopic scale is creating the death of the neurons, which is

58
00:05:13,560 --> 00:05:21,760
creating a microscopic shape change, which is also leading to a macroscopic shape change

59
00:05:21,760 --> 00:05:28,360
with the atrophy of the brain that we can see on MRI.

60
00:05:28,360 --> 00:05:37,080
We have illustrated that biological shape contains biologically relevant information,

61
00:05:37,080 --> 00:05:43,920
or medically relevant information, that we can go from biological shapes to biomedical

62
00:05:43,920 --> 00:05:44,920
insights.

63
00:05:45,280 --> 00:05:51,360
Actually, biophysics tells us that the healthy or the pathological state of the biological

64
00:05:51,360 --> 00:05:58,440
structure will change its shape, or generally that the function of a biological structure

65
00:05:58,440 --> 00:06:00,160
will change its shape.

66
00:06:00,160 --> 00:06:06,960
For example, blood cells have a different shape than brain cells or neurons, because

67
00:06:06,960 --> 00:06:09,240
they have different functions.

68
00:06:09,240 --> 00:06:15,320
In the context of this talk, we are interested in inverting that model, in learning biophysics

69
00:06:15,320 --> 00:06:20,840
or biology, from the data that we have, which are the shapes.

70
00:06:20,840 --> 00:06:30,120
So we do statistics and machine learning on shapes, which is what we will call shape learning.

71
00:06:30,120 --> 00:06:37,840
Let me show you how we can do shape learning in biomedical imaging, by showing you the

72
00:06:37,840 --> 00:06:41,960
architecture of a typical research project in our lab.

73
00:06:41,960 --> 00:06:45,760
Usually, it starts with a biomedical question.

74
00:06:45,760 --> 00:06:52,800
For example, what is the acceleration of the atrophy that we see in the brains of patients

75
00:06:52,800 --> 00:06:54,240
with Alzheimer's disease?

76
00:06:54,240 --> 00:07:01,080
It will be interesting to know what's the magnitude of these accelerated brain shape

77
00:07:01,080 --> 00:07:08,200
changes, because it could give ideas to build automatic diagnosis procedure.

78
00:07:08,200 --> 00:07:11,920
Another biomedical question could be, how do cells move?

79
00:07:11,920 --> 00:07:19,120
What you see on this slide, a moving cell, so called a migrating cell.

80
00:07:19,120 --> 00:07:25,120
How can cell move is a very important question when one studies metastatic cancer, when the

81
00:07:25,120 --> 00:07:28,840
cancerous cells propagate in the human body.

82
00:07:28,840 --> 00:07:32,440
All in all, it starts with a biomedical question.

83
00:07:32,440 --> 00:07:38,400
Then there is a step of image acquisition, acquiring the MRIs, acquiring the microscopy

84
00:07:38,400 --> 00:07:43,400
images, that is done by collaborators of all that.

85
00:07:43,400 --> 00:07:46,560
Then there is an important step of shape reconstruction.

86
00:07:46,560 --> 00:07:55,400
Yes, because the data that we get are images or videos, so the shape has not been extracted

87
00:07:55,400 --> 00:07:56,400
yet.

88
00:07:56,400 --> 00:08:02,440
We made it an algorithm of contour detection, edge detection, or segmentation to reconstruct

89
00:08:02,440 --> 00:08:05,240
the shape.

90
00:08:05,240 --> 00:08:07,680
Then there is a step of shape modeling.

91
00:08:07,680 --> 00:08:11,040
You could also think of it as shape featurization.

92
00:08:11,040 --> 00:08:16,600
Let's say we have extracted the border of the outline of that cell.

93
00:08:16,600 --> 00:08:20,680
How do we want to represent this shape in a computer?

94
00:08:21,320 --> 00:08:26,240
There are different choices that one can make, and we're going to see a few of them.

95
00:08:26,240 --> 00:08:33,320
And lastly, when we have represented the shape, we have chosen a feature to represent the shape.

96
00:08:33,320 --> 00:08:38,040
We can do statistics, machine learning, and deep learning on these data points that are

97
00:08:38,040 --> 00:08:39,040
shapes.

98
00:08:39,040 --> 00:08:43,040
That's what they call shape learning.

99
00:08:43,040 --> 00:08:49,880
In the context of this talk, we will focus on the last two of this pipeline of this cycle.

100
00:08:49,880 --> 00:08:53,400
How can we represent a shape in a computer?

101
00:08:53,400 --> 00:08:58,520
And then when we have represented the notion of shape in a computer, how can we do machine

102
00:08:58,520 --> 00:09:02,880
learning on shapes?

103
00:09:02,880 --> 00:09:08,160
Let's start with how we represent the notion of shape in a computer.

104
00:09:08,160 --> 00:09:14,840
So shape modeling, our shape representation is interesting because we have different

105
00:09:14,840 --> 00:09:20,760
choices as to how to model the shape of the biological structure.

106
00:09:20,760 --> 00:09:28,600
But irrespective of these choices, very often, we end up computing with manifolds.

107
00:09:28,600 --> 00:09:30,160
What is a manifold?

108
00:09:30,160 --> 00:09:35,320
On the left, you have a visual definition of what is a manifold.

109
00:09:35,320 --> 00:09:44,120
We can generally define a manifold as a generalization of a vector space that is allowed to be curved.

110
00:09:44,120 --> 00:09:49,760
On the left, for example, you can see a 2D vector space, also called a plane, that is

111
00:09:49,760 --> 00:09:51,160
not curved.

112
00:09:51,160 --> 00:09:53,360
It's linear or flat.

113
00:09:53,360 --> 00:10:01,120
And just below it, we have an example of a 2D manifold M that is itself allowed to be

114
00:10:01,120 --> 00:10:02,120
curved.

115
00:10:02,120 --> 00:10:04,840
You can see that there is some curvature there.

116
00:10:04,840 --> 00:10:11,400
Now, this is interesting for us because, as I said, we can try different models of shapes,

117
00:10:11,400 --> 00:10:17,920
and representations of shapes, but very often, we will see this manifold M appear.

118
00:10:17,920 --> 00:10:23,480
Let me give you examples of how manifolds enter the computations.

119
00:10:23,480 --> 00:10:27,880
First, let's look at the shapes themselves.

120
00:10:27,880 --> 00:10:34,080
Let's say we are interested in shape of this cat, and more precisely, the shape of the

121
00:10:34,080 --> 00:10:37,440
surface that is defining the cat.

122
00:10:37,440 --> 00:10:43,080
The shape of the surface is a generalization of a vector space that is allowed to be curved,

123
00:10:43,080 --> 00:10:50,960
and so the shape itself is the manifold in this case, so I write M equal to the shape.

124
00:10:50,960 --> 00:10:53,040
The manifold is the shape.

125
00:10:53,040 --> 00:10:58,440
If you want to compute on these shapes, for example, define a hit map or a scalar field

126
00:10:58,440 --> 00:11:05,840
on this shape, you are working with defining a function on the manifold.

127
00:11:05,840 --> 00:11:09,480
Another example is a bit more abstract.

128
00:11:09,480 --> 00:11:13,760
The manifold can be the space of all the shapes.

129
00:11:13,760 --> 00:11:18,480
In the first case, one shape is a manifold.

130
00:11:18,480 --> 00:11:24,600
In the second case, we consider the space of all possible shapes, but you have illustrated

131
00:11:24,600 --> 00:11:31,200
with this sphere is actually the shape space of all the possible triangles, so you can

132
00:11:31,200 --> 00:11:37,080
see if you look, maybe you can see, that I have superimposed a little triangle on this

133
00:11:37,080 --> 00:11:40,840
sphere, because here there is a triangle.

134
00:11:40,840 --> 00:11:47,440
This sphere actually represents the space of all the triangular shapes, so all the visible

135
00:11:47,440 --> 00:11:48,440
shapes.

136
00:11:48,440 --> 00:11:53,400
Now, this illustration is the shape space of triangles, but more generally, the shape

137
00:11:53,400 --> 00:11:58,200
space of surfaces is also a manifold.

138
00:11:58,560 --> 00:12:03,040
Continuing, you could be interested in the space of shape motions.

139
00:12:03,040 --> 00:12:06,840
For example, how your shape translates and rotates.

140
00:12:06,840 --> 00:12:11,080
Here you have a cancer cell translating and rotating.

141
00:12:11,080 --> 00:12:16,640
If you want to do statistics on all the possible motions that your shape can perform, then you're

142
00:12:16,640 --> 00:12:23,200
doing statistics on, again, a manifold M. That is this time, the space of shape motions.

143
00:12:24,120 --> 00:12:30,240
And lastly, you could be interested in how shapes deform, or a smooth deformation of

144
00:12:30,240 --> 00:12:35,000
the cancer cell, as you can see on the left, but also other objects like functional maps

145
00:12:35,000 --> 00:12:39,720
that explain how shapes can deform.

146
00:12:39,720 --> 00:12:45,320
And interestingly, again, if you choose this shape representation, you end up with a space

147
00:12:45,320 --> 00:12:49,880
of shapes deformation, that is a manifold.

148
00:12:49,880 --> 00:12:54,280
There are other models of shape that I will not cover here, but it seems that these four

149
00:12:54,280 --> 00:13:03,280
models of shapes are quite common, and all of them require thinking about manifolds.

150
00:13:03,280 --> 00:13:07,080
Thinking about manifolds, what can we do with manifolds?

151
00:13:07,080 --> 00:13:12,880
Let's think about generalizing basic operations, and for example, generating the definition

152
00:13:12,880 --> 00:13:15,480
of a new manifold.

153
00:13:15,480 --> 00:13:18,280
On the center of the screen, I put the sphere.

154
00:13:18,280 --> 00:13:21,920
The sphere is one example of manifolds that we had just before.

155
00:13:21,920 --> 00:13:25,880
It can represent the shape space of triangles.

156
00:13:25,880 --> 00:13:29,800
The one point of the sphere is the shape of a triangle.

157
00:13:29,800 --> 00:13:35,880
You can also think of it as one point of the sphere is a shape motion.

158
00:13:35,880 --> 00:13:40,160
How do I compute the mean of two triangles?

159
00:13:40,160 --> 00:13:42,960
That will be the two real points here.

160
00:13:42,960 --> 00:13:46,560
Or how do I compute the mean of two shape transformations?

161
00:13:46,560 --> 00:13:49,960
That will be the two blue points here.

162
00:13:49,960 --> 00:13:54,560
I could use the traditional definitions of statistics, and I can use the traditional

163
00:13:54,560 --> 00:14:00,600
definition of mean and compute the mean as this orange point, which would be computed

164
00:14:00,600 --> 00:14:03,200
as the middle of the two blue points.

165
00:14:03,200 --> 00:14:09,240
But in that case, if I apply traditional statistics, there is a problem because you see the mean

166
00:14:09,240 --> 00:14:12,120
in orange does not belong to the manifold.

167
00:14:12,600 --> 00:14:18,560
Now, if the manifold represents the shape space of triangles, what we are saying in applying

168
00:14:18,560 --> 00:14:24,200
traditional statistics is that the mean of two triangles is not even a triangle.

169
00:14:24,200 --> 00:14:29,640
So traditional statistics do not really apply in this setting.

170
00:14:29,640 --> 00:14:37,240
We would like to generalize operations, statistics, machine learning, and deep learning to manifolds.

171
00:14:37,240 --> 00:14:42,600
So that when I try to compute the mean of these two blue points, that again, the two

172
00:14:42,600 --> 00:14:48,800
triangle shapes, two shape deformations, I end up with a orange point that is at least

173
00:14:48,800 --> 00:14:53,040
part of the data space that I'm interested in.

174
00:14:53,040 --> 00:14:57,560
So we'd like to generalize all of these operations to manipulate.

175
00:14:57,560 --> 00:15:02,520
Why would we want to do that instead of, for example, just using traditional statistics

176
00:15:02,520 --> 00:15:06,000
and then projecting back on the manifold?

177
00:15:06,040 --> 00:15:12,040
This is because knowing that the data space is a manifold is information.

178
00:15:12,040 --> 00:15:17,680
Knowing the geometry of the shape space, of the space of shape motions, of the space of

179
00:15:17,680 --> 00:15:20,360
shape deformations, that is information.

180
00:15:20,360 --> 00:15:25,640
And any information that we can incorporate in the learning algorithm is welcome.

181
00:15:25,640 --> 00:15:30,480
So in other words, we would like to use the geometry of manifolds as an inductive bias

182
00:15:30,480 --> 00:15:34,120
in our analysis.

183
00:15:34,120 --> 00:15:36,840
So let's see how we can do this.

184
00:15:36,840 --> 00:15:39,920
So I've just introduced shape modeling.

185
00:15:39,920 --> 00:15:47,880
I've introduced four models of shapes, four representations of shapes, and all of them

186
00:15:47,880 --> 00:15:51,400
require us to compute on manifolds.

187
00:15:51,400 --> 00:15:57,360
Now let's move to shape deep learning, which is a way of computing on manifolds, and see

188
00:15:57,360 --> 00:16:02,720
how we can define shape deep learning for these manifold spaces.

189
00:16:05,120 --> 00:16:08,600
So let's think about what is deep learning first.

190
00:16:08,600 --> 00:16:16,880
I've put four big categories of subfields of deep learning, or machine learning in general,

191
00:16:16,880 --> 00:16:23,120
supervised learning, unsupervised learning, reinforcement learning, and optimization.

192
00:16:23,120 --> 00:16:27,480
What is supervised deep learning in the context of shape learning?

193
00:16:27,480 --> 00:16:34,080
In supervised learning, the goal is to learn a map from an input space to an output space

194
00:16:34,560 --> 00:16:43,720
Y. Now let's rename the input space X into Mx, to denote that the input space can be a manifold.

195
00:16:43,720 --> 00:16:52,760
And let's rename the output space Y as MY to denote that the output space can be a manifold.

196
00:16:52,760 --> 00:16:57,840
So this is a setting that will appear if we want to do shape deep learning.

197
00:16:57,840 --> 00:17:04,760
As an example, let's say Mx is just an image, a 2D image of a 3D object.

198
00:17:04,760 --> 00:17:09,960
In that case, it's not, I mean, it's a special case of a manifold, but it's not a curved manifold.

199
00:17:09,960 --> 00:17:15,400
And let's say Y is the space of the pose of the subject.

200
00:17:15,400 --> 00:17:18,080
In that case, MY would be a manifold.

201
00:17:18,080 --> 00:17:26,040
That's an example of shape learning when we want to predict an element of MY, therefore an element of a manifold.

202
00:17:26,080 --> 00:17:30,160
You can have all the cases where Mx is the manifold.

203
00:17:30,160 --> 00:17:36,560
For example, you see the shape of the brain and you want to predict if that brain has Alzheimer's disease.

204
00:17:36,560 --> 00:17:44,560
In that case, Mx is the space of all possible brain shapes and Y is the probability of having Alzheimer's disease.

205
00:17:44,560 --> 00:17:51,000
And you can also have a case where Mx is the manifold and MY is the manifold.

206
00:17:51,040 --> 00:17:59,400
In unsupervised learning, the goal is to find patterns in the data that belong to the data space M.

207
00:17:59,400 --> 00:18:05,840
Let's say we want to find patterns in a data space of shapes, then M is a manifold.

208
00:18:05,840 --> 00:18:12,800
And so we need to generalize unsupervised deep learning to unsupervised learning to manifolds.

209
00:18:12,800 --> 00:18:17,680
An example of unsupervised learning would be if we want to find clusters of shapes.

210
00:18:17,680 --> 00:18:22,880
We need to find a method that can perform clustering of manifolds.

211
00:18:22,880 --> 00:18:25,400
In reinforcement learning, same story.

212
00:18:25,400 --> 00:18:33,000
Let's say we want to learn the policy by that gives the priority of applying an action on the state space.

213
00:18:33,000 --> 00:18:38,920
Then each of these spaces can also be a manifold or both into a manifold.

214
00:18:38,920 --> 00:18:44,960
For example, we want to learn how a shape should move to optimize a certain feature.

215
00:18:44,960 --> 00:18:53,200
And lastly, what is underpinning a lot of development in deep learning is the broader field of optimization.

216
00:18:53,200 --> 00:18:57,920
And same thing, we may wonder how this can generalize to manifolds.

217
00:18:57,920 --> 00:19:04,000
Let's say we want to find the optimal shape that verifies a certain feature.

218
00:19:04,000 --> 00:19:10,960
In that case, we want to perform an optimization where the parameter that is varying is actually varying on a manifold.

219
00:19:10,960 --> 00:19:14,280
So we want to perform optimization of manifold.

220
00:19:14,280 --> 00:19:21,720
The example here, let's say we want to find the optimal deformation that can map one shape onto the other.

221
00:19:21,720 --> 00:19:26,880
The deformation is an element of the manifold of the patients.

222
00:19:26,880 --> 00:19:32,440
And therefore, this will be an example of optimization on manifolds.

223
00:19:32,440 --> 00:19:41,200
So how can we generalize all of these wide fields of deep learning to manifolds?

224
00:19:41,200 --> 00:19:47,160
And that's actually a trick that I call the vector space manifold conversion trick.

225
00:19:47,160 --> 00:19:49,440
And the trick is as follows.

226
00:19:49,440 --> 00:19:55,280
You can realize that all of these machine learning, such as deep learning algorithms,

227
00:19:55,280 --> 00:19:58,920
rely on the same basic building blocks.

228
00:19:58,920 --> 00:20:06,560
By basic building blocks, I mean some sets of abstract elements and some set of abstract operations.

229
00:20:06,560 --> 00:20:16,440
Now, if we can convert these building blocks from their definition on vector spaces to their definitions on manifold,

230
00:20:16,440 --> 00:20:24,640
then we have a translation in the sense of language translation that allow us to beautifully convert any type of statistical learning,

231
00:20:24,640 --> 00:20:28,880
machine learning, and deep learning algorithm to manifolds.

232
00:20:28,880 --> 00:20:30,280
So let's do this.

233
00:20:30,280 --> 00:20:39,480
What are the basic building blocks that are at the core of every statistical learning, machine learning, and deep learning algorithm?

234
00:20:39,480 --> 00:20:47,880
Well, in terms of the elements, almost all of these algorithms use either points or vectors.

235
00:20:47,880 --> 00:20:54,640
So point on the data space, in our case, one point in your shape, one point can be a deformation.

236
00:20:54,640 --> 00:20:56,640
They also use vectors.

237
00:20:56,640 --> 00:20:59,000
Think about gradient descent.

238
00:20:59,000 --> 00:21:04,360
The gradient is a vector anchored at a point on the manifold.

239
00:21:04,360 --> 00:21:12,320
And then in terms of operations, there are three operations that come over and over again in these algorithms,

240
00:21:12,320 --> 00:21:18,480
which are computing a straight line, computing a distance, and performing a net issue.

241
00:21:18,480 --> 00:21:24,400
So straight line, linear regression, our principal component analysis, we are using straight lines.

242
00:21:24,400 --> 00:21:26,960
So we need a way of computing straight lines.

243
00:21:27,000 --> 00:21:36,600
The squared distance, in many loss functions, we use a notion of squared distance between the ground truth and our prediction.

244
00:21:36,600 --> 00:21:39,160
Very often the L2 distance.

245
00:21:39,160 --> 00:21:41,280
And then the addition.

246
00:21:41,280 --> 00:21:49,280
For example, when we add a gradient to an estimate, we are adding the gradient vector to the estimate point.

247
00:21:49,280 --> 00:21:55,160
So we are adding the black arrow to this blue point when we get an orange point.

248
00:21:55,160 --> 00:22:01,600
Now, assuming that most of the learning algorithms uses these basic elements and operations,

249
00:22:01,600 --> 00:22:09,720
if we can generalize or convert these elements and operations to manifold, then we have our conversion checks.

250
00:22:09,720 --> 00:22:12,560
I'm going to introduce the conversion.

251
00:22:12,560 --> 00:22:15,680
Yes, we want to convert to manifold.

252
00:22:15,680 --> 00:22:18,040
The points are just points.

253
00:22:18,040 --> 00:22:20,640
They are the elements of our manifolds.

254
00:22:20,640 --> 00:22:26,400
What was a vector on the vector space becomes a tangent vector on the manifold.

255
00:22:26,400 --> 00:22:31,000
So on the vector space, point and vectors are approximately the same.

256
00:22:31,000 --> 00:22:36,720
On the manifold, points and tangent vectors are two different, very different elements.

257
00:22:36,720 --> 00:22:45,640
On the left, the registration on the left, you can see the tangent vector in black that is anchored at the point in blue on the manifold.

258
00:22:45,640 --> 00:22:48,920
So we have generalized basic elements.

259
00:22:48,920 --> 00:22:51,960
Now let's generalize the operations.

260
00:22:51,960 --> 00:22:56,440
The straight line becomes a geodesic on M.

261
00:22:56,440 --> 00:23:03,080
I'm not telling you how we compute the geodesic, I'm telling you the translation or conversion of the operation.

262
00:23:03,080 --> 00:23:14,520
For example, on the registration on the left, you can see the dotted black line is the geodesic between the blue point and the orange point.

263
00:23:14,520 --> 00:23:22,240
The square distance between two points on the manifold becomes the square geodesic distance between the same two points.

264
00:23:22,240 --> 00:23:31,320
We're basically the length of the square length of the geodesic that is connecting these two points.

265
00:23:31,320 --> 00:23:35,200
And lastly, we need a generalization of the notion of addition.

266
00:23:35,200 --> 00:23:42,920
On the vector space, we can add a vector, for example, a vector in black to a point here in blue.

267
00:23:42,920 --> 00:23:46,920
On the manifold, this operation is now called the exponential map.

268
00:23:46,920 --> 00:23:56,320
And it is, again, just rated on the left, while we add not a vector, but tangent vector, so the tangent vector in black to the point in blue.

269
00:23:56,320 --> 00:24:05,160
And performing this addition operation allows us to reach another point on the manifold, which is the point in orange.

270
00:24:05,160 --> 00:24:11,360
So that is the vector space manifold conversion.

271
00:24:11,360 --> 00:24:25,120
And this trick, we implemented it in this open source package called GM Stats, which generalizes all of these operations that we know very well on vector spaces to manifolds.

272
00:24:25,120 --> 00:24:35,520
And for different types of manifolds, specifically the different types of manifold that I showed you in the context of shape representation and shape models.

273
00:24:35,520 --> 00:24:43,320
It's a Python package that is available online at these links, and we created it with three objectives.

274
00:24:43,320 --> 00:24:46,200
First, to teach hands-on geometric learning.

275
00:24:46,200 --> 00:24:55,120
So computing on manifolds can be hard in practice, even though the word manifold comes over and over again in manifold learning, for example.

276
00:24:55,120 --> 00:24:59,840
The basic operations on manifolds are sometimes hard to implement.

277
00:24:59,840 --> 00:25:05,800
And one of the reasons is that they're not necessarily taught in class besides textbooks.

278
00:25:05,800 --> 00:25:11,720
So with this software, you can teach hands-on manifolds and geometric learning.

279
00:25:11,720 --> 00:25:17,320
The second objective is to support research in geometric learning or learning on manifolds.

280
00:25:17,320 --> 00:25:25,640
So different researchers reach out to us who publish, they have published papers and they want to implement them methods in GM Stats.

281
00:25:25,640 --> 00:25:30,480
So as to make them methods better, more available to the general public.

282
00:25:30,480 --> 00:25:35,160
So that's on the contributor side supporting the research.

283
00:25:35,160 --> 00:25:42,440
And the last objective is to democratize the use of geometric learning, in other words, learning on manifolds.

284
00:25:42,440 --> 00:25:53,520
Because once these methods are incorporated into GM Stats, from a user's perspective, you only need to know the conversion that I just presented you on the previous slide,

285
00:25:53,520 --> 00:25:56,000
and you can use all of these algorithms.

286
00:25:56,000 --> 00:25:59,960
So you do not need to know how a geodesic is implemented.

287
00:25:59,960 --> 00:26:04,720
You just need to know that the geodesic is a generalization of a straight line.

288
00:26:04,720 --> 00:26:10,480
So it allows to democratize the use of these methods.

289
00:26:10,480 --> 00:26:21,280
Now other libraries that implement computations on manifolds, learning on manifolds and optimization on manifolds, you can see them here on the screen.

290
00:26:21,320 --> 00:26:29,680
The first three, the one that have opt in it and MacDodge, they are focused on optimization on manifolds.

291
00:26:29,680 --> 00:26:38,280
For example, to minimize a criterion and the parameter minimizing this criterion is an element of the manifold.

292
00:26:38,280 --> 00:26:48,840
And then all the others are Python packages that perform computations and learning on manifolds, but they focus on a special type of manifolds.

293
00:26:48,840 --> 00:26:53,720
For example, the manifold of SPD mattresses, tree rotations, disruptions.

294
00:26:53,720 --> 00:27:09,440
By contrast, we do not do fancy optimizations in GM Stats, but we try to implement a very wide range of manifolds, including the manifolds that represent shapes and shapespaces.

295
00:27:09,440 --> 00:27:19,720
So let me give you an example of how you can use this software to compute with shape representations and let's say shape motions.

296
00:27:19,720 --> 00:27:31,480
So let's say you're interested in knowing how a shape evolved from being in an orientation R1 and translation or position T1.

297
00:27:31,480 --> 00:27:45,640
And how it evolves from this original pose to an end pose, which we could call R1 for rotation R2, for rotation two and T2 for translation two.

298
00:27:45,640 --> 00:27:55,120
So when we compute with rotation and translations, as we do with motions and by the way, it should be 3D translation, not 2D translation.

299
00:27:55,120 --> 00:28:07,160
We are computing with elements that belong to the manifold and that gave the manifold is M equal SE3 and that stands for special Euclidean group in three dimension.

300
00:28:07,160 --> 00:28:12,160
In other words, the group of all 3D translations and 3D rotations.

301
00:28:12,160 --> 00:28:15,640
This is a good snippet using GM Stats.

302
00:28:15,640 --> 00:28:21,920
What you really have to do is instantiate the manifold that you are interested in.

303
00:28:21,920 --> 00:28:32,760
Here we want to work with SE3, so we instantiate SE3 as an object of the class special Euclidean in dimension three.

304
00:28:32,760 --> 00:28:34,880
We extract what's called a metric.

305
00:28:34,880 --> 00:28:38,840
It's a Riemannian metric that allows us to perform the computations.

306
00:28:38,840 --> 00:28:42,560
And then we call metric dot geodesic.

307
00:28:42,560 --> 00:28:45,880
Again, we do not need to know how the geodesic is implemented.

308
00:28:45,880 --> 00:28:50,640
We just need to know that the geodesic is the generalization of the straight line.

309
00:28:50,680 --> 00:28:59,760
And therefore, we can compute the geodesic that starts with an initial point, rotation one, translation one, and has an initial tangent vector.

310
00:28:59,760 --> 00:29:04,000
And on the right, you have an illustration of the geodesic.

311
00:29:04,000 --> 00:29:09,440
Pose one corresponds to R1T1, rotation one, translation one.

312
00:29:09,440 --> 00:29:20,600
It's represented by one very small frame with three axes because we can use the translation, translation one, to place this frame in 3D.

313
00:29:20,840 --> 00:29:25,920
And the rotation, rotation one to orient the frame in 3D.

314
00:29:25,920 --> 00:29:31,720
But therefore, one little frame corresponds to one point in SE3.

315
00:29:31,720 --> 00:29:43,520
And you see here a trajectory of frames that correspond to the geodesic on the manifold SE3 linking pose one to pose two.

316
00:29:43,520 --> 00:29:50,480
So I told you we implemented a lot of different manifolds in GM stats.

317
00:29:50,480 --> 00:29:55,560
And what we end up with is actually a numerical taxonomy of manifolds.

318
00:29:55,560 --> 00:30:06,040
So we use object-oriented programming to create this hierarchy of all the possible of many manifolds that you could be interested in working with.

319
00:30:06,040 --> 00:30:12,120
So this hierarchy is built as follows on the root of the hierarchy of the tree.

320
00:30:12,120 --> 00:30:17,320
You have the most abstract concept of manifold that is just a manifold.

321
00:30:17,320 --> 00:30:25,080
And we list all the different attributes or methods that a manifold item object should have.

322
00:30:25,080 --> 00:30:32,680
And as you go down this hierarchy, you find more and more specialized or more and more concrete manifolds.

323
00:30:32,680 --> 00:30:38,600
For example, one level down, you found the manifold matrix lead group.

324
00:30:38,600 --> 00:30:41,560
Lead group is a special case of a manifold.

325
00:30:41,560 --> 00:30:45,720
It's a manifold that has an algebraic group structure.

326
00:30:45,720 --> 00:30:49,560
So therefore, matrix lead group is on the second level of the hierarchy.

327
00:30:49,560 --> 00:30:56,600
It inherits from the Python class manifold because it's a special case of a manifold.

328
00:30:56,600 --> 00:31:02,200
And then you go down this hierarchy and at the very bottom of this hierarchy, you have the leaves.

329
00:31:02,200 --> 00:31:05,600
And these are manifolds that you can actually instantiate.

330
00:31:05,600 --> 00:31:11,600
So for example, there will be somewhere in there, the special ectogen proof that we just used,

331
00:31:11,600 --> 00:31:16,960
which would inherit from matrix lead group because it's a special type of matrix lead group.

332
00:31:16,960 --> 00:31:21,520
And again, matrix lead group itself inherits from manifolds.

333
00:31:21,520 --> 00:31:27,440
So with this project that started with representing shapes and doing statistical learning on shapes,

334
00:31:27,440 --> 00:31:34,160
we actually ended up creating a computational representation of differential geometry.

335
00:31:36,320 --> 00:31:42,800
Okay, so let me wrap this up a little bit and show you how we can mix these two.

336
00:31:43,520 --> 00:31:52,160
So how we can use a model of shape to how can we can perform a deep learning algorithm

337
00:31:52,160 --> 00:31:56,000
on a manifold that would, for example, represent the shape space.

338
00:31:57,360 --> 00:32:01,760
And the example I'm going to choose is variational encoders.

339
00:32:01,840 --> 00:32:05,920
How do variational encoders work on manifolds?

340
00:32:08,160 --> 00:32:14,240
Zooming out a little bit, let me show you an overview of the research that I'm interested in.

341
00:32:15,040 --> 00:32:22,080
In the previous slides, I was showing you the CRT of computational differential geometry,

342
00:32:22,080 --> 00:32:23,840
which is the ERT of manifolds.

343
00:32:25,040 --> 00:32:29,840
I'm interested in generalizing statistical learning, machine learning and deep learning

344
00:32:29,920 --> 00:32:36,400
on these exotic data spaces. In other words, I'm interested in filling the table that you see on

345
00:32:36,400 --> 00:32:44,640
this slide, where in this table, the different rows represent different manifolds. So each of the row

346
00:32:44,640 --> 00:32:49,440
in this table corresponds to one of the nodes that you had in the previous CRT.

347
00:32:50,080 --> 00:32:52,960
There are different types of exotic data spaces, if you wish.

348
00:32:53,840 --> 00:32:58,400
And in this table, the different columns represents different fields of statistics,

349
00:32:58,720 --> 00:33:04,800
machine learning and deep learning. We're going to talk about theorizing variational encoders to

350
00:33:04,800 --> 00:33:10,640
manifolds. So therefore, we are in the column dimension reduction and deep learning.

351
00:33:12,960 --> 00:33:18,800
So dimension reduction and variational encoders. I'd like to present a geometric perspective

352
00:33:19,440 --> 00:33:26,800
on them as to emphasize where manifolds come in. On this first line of this table,

353
00:33:27,760 --> 00:33:36,480
you can see two classes of dimension reduction methods. The first classes, the first class of

354
00:33:36,480 --> 00:33:42,240
methods, are the methods that seek to perform dimension reduction on an ambient space that is

355
00:33:42,240 --> 00:33:49,200
a vector space. Specifically, let's say the vector space is R2, and we have some data points,

356
00:33:49,200 --> 00:33:56,400
which are the dark green data points. We can think of principal component analysis that seeks to

357
00:33:56,480 --> 00:34:04,800
learn a linear subspace that would be the light green line within this linear space that is R2.

358
00:34:05,440 --> 00:34:10,480
And you have different dimension reduction methods that work like this, that wish to

359
00:34:10,480 --> 00:34:18,240
learn a linear subspace within a linear space. In the manifold world, we remember that linear

360
00:34:19,200 --> 00:34:26,880
subspace, or that linear line, is converted to geodesic. And so you have equivalent methods

361
00:34:26,880 --> 00:34:34,080
that allow to learn the equivalent of a line that is a geodesic on a manifold. So this time,

362
00:34:34,080 --> 00:34:40,240
the ambient space is a manifold and will be this clear. And we're interested in learning

363
00:34:40,240 --> 00:34:47,360
principal directions of variations of the data, the data of the dark green point. And we want to

364
00:34:47,360 --> 00:34:52,800
learn principal directions of variations of this data that will be the equivalent of the lines on

365
00:34:52,800 --> 00:34:58,560
the left, so that are the geodesic. The geodesic on the sphere is a great circle, and you can see in

366
00:34:58,560 --> 00:35:05,680
light green, a great circle. In the context of variational tranquillers, the vector space case,

367
00:35:06,240 --> 00:35:11,360
we are not linear. We're using deep learning, and most of the time we are highly nonlinear.

368
00:35:12,320 --> 00:35:18,320
Therefore, we need another row in this table to complete the overview of dimension

369
00:35:18,320 --> 00:35:23,760
reduction methods from a geometric perspective. And then we talk about the one that is on the

370
00:35:23,760 --> 00:35:30,320
bottom left, which include, which is a class of methods that include variational tranquillers.

371
00:35:31,120 --> 00:35:37,600
These class of methods operate in an ambient space, that is, again, a vector space. It's again

372
00:35:38,400 --> 00:35:46,160
represented as R2 here. But instead of learning variations of the data that are linear, as this

373
00:35:46,160 --> 00:35:54,320
here would do, instead, we are allowed to learn a nonlinear subspace M. So you see the dark

374
00:35:54,320 --> 00:36:04,960
light green line is now nonlinear. So I'm going to present how we can generalize this VAE,

375
00:36:04,960 --> 00:36:12,880
presented from this geometric perspective here, to geometric VAE, or VAE on manifolds,

376
00:36:12,880 --> 00:36:20,000
which is what is shown on the bottom right. So now the setting is the ambient space is a manifold.

377
00:36:20,640 --> 00:36:28,160
So the sphere represents the ambient space, that is the manifold, to which we know that the data are,

378
00:36:28,160 --> 00:36:33,520
in which we know that the data are. And now we don't only restrict ourselves to learning

379
00:36:33,520 --> 00:36:41,040
a geodesic subspace, as the road just above was doing, but we allow to learn any nonlinear subspace,

380
00:36:42,000 --> 00:36:47,520
or non-geodesic submanifolds, and within the manifold M.

381
00:36:49,760 --> 00:36:55,280
But how does this work? Let me review variational tranquillers. I put

382
00:36:56,240 --> 00:37:00,640
equations on the left for completeness. I'm not going to comment on them too much.

383
00:37:01,600 --> 00:37:08,880
Variational tranquillers start with a generative model of data in Rd, that's the ambient space

384
00:37:08,880 --> 00:37:14,960
that we had before. The data assumed to belong to a vector space. And we can think of variational

385
00:37:14,960 --> 00:37:23,840
tranquillers as a way to invert a generative model with latent variables. And typically,

386
00:37:23,840 --> 00:37:28,640
a generative model with latent particles, such as the one that is in the first equation.

387
00:37:29,360 --> 00:37:36,240
So we have xi, that's our data in the vector space Rd, that is assumed to be generated

388
00:37:36,240 --> 00:37:42,560
with a generative model with latent variables. But the latent variable is xi, they are going to

389
00:37:42,560 --> 00:37:49,520
represent the low dimensional representation of the xi. So this low dimensional representation,

390
00:37:49,520 --> 00:37:57,360
xi, pass through a function, f, this parameter mu and w, this function is usually represented by a

391
00:37:57,360 --> 00:38:04,000
neural network, that is called the decoder, variational tranquillers, to which noise is added.

392
00:38:04,800 --> 00:38:10,000
So that's the generative model that we assume has generated data in variational tranquillers.

393
00:38:11,040 --> 00:38:18,720
From a geometric perspective, xi are assumed to belong to a lower dimensional latent space,

394
00:38:19,440 --> 00:38:26,960
that I write Rl. Then this xi passes through the function f of mu and w,

395
00:38:26,960 --> 00:38:31,520
which can be the neural network. And by passing through this function, it becomes

396
00:38:31,520 --> 00:38:37,120
an element of the higher dimensional space Rd. So this is the blue point on the illustration.

397
00:38:37,920 --> 00:38:45,680
And then noise is added to the model. Now, if we were to pass the whole latent space Rl

398
00:38:46,400 --> 00:38:54,720
through the function f mu and w, we would get this nonlinear light green curve that represents

399
00:38:54,720 --> 00:39:01,120
the subspace, the nonlinear subspace that we are learning. That's a geometric explanation

400
00:39:01,120 --> 00:39:06,960
of the generative model of the data that is at the foundation of variational tranquillers.

401
00:39:07,760 --> 00:39:13,200
Now, with GAE, we do not observe, we do not know what is f of mu and w.

402
00:39:14,400 --> 00:39:18,160
We model it as a decoder, but we do not know what are the weights of the decoder.

403
00:39:18,960 --> 00:39:26,320
And we also, we do not know what are the xi that are associated to each xi. So,

404
00:39:26,320 --> 00:39:33,520
given only the xi's, we would like to be able to learn the low-dimensional representations

405
00:39:33,520 --> 00:39:42,800
xi together with the decoder, the model. And this is done with this architecture that you

406
00:39:42,800 --> 00:39:47,680
might be familiar with. On the left, you have the traditional architecture of the variational

407
00:39:47,680 --> 00:39:53,680
tranquillers. On the right, you have the last function that is used to train it. So in the

408
00:39:53,680 --> 00:40:02,320
architecture of the GAE, the second half represents the decoder. It is the representation of the

409
00:40:02,320 --> 00:40:08,160
generative model that takes an element of the lower-dimensional latent space in green

410
00:40:08,160 --> 00:40:15,040
and outputs the function f of that latent variable. And on the left, we add an encoder

411
00:40:15,600 --> 00:40:22,640
that is able to perform the operation of going from xi to a normal size approximation

412
00:40:22,640 --> 00:40:29,040
approximates representation of the posterior of the xi. Anyway, this is all trained with the

413
00:40:29,040 --> 00:40:35,280
last function that is called the elbow. It stands for evidence lower bound. It's a lower bound of

414
00:40:35,280 --> 00:40:42,000
the likelihood, of the log likelihood. And I've given its expression here as a sum of a reconstruction,

415
00:40:42,880 --> 00:40:48,720
a term and a regularization term, because these are the terms that we have to generalize to manifold

416
00:40:49,600 --> 00:40:53,280
to have a version of variational tranquillers that works on manifold.

417
00:40:53,600 --> 00:41:03,200
So let's try to do this, to generalize variational tranquillers to manifold. We will have to generalize

418
00:41:03,200 --> 00:41:12,400
two elements. First, the generative model and second, the last function. Now, looking a little bit

419
00:41:12,400 --> 00:41:20,000
at the generative model and the last functions, we observe that they are built from basic elements

420
00:41:20,000 --> 00:41:26,320
and operations that we now know how to converge to manifold. So specifically in the generative model,

421
00:41:26,320 --> 00:41:32,240
we see there is a plus, which is an addition. And the generalization of the addition of vector

422
00:41:32,240 --> 00:41:38,320
spaces is the exponential map of manifold. In the last function also, we see that there is a square

423
00:41:38,320 --> 00:41:46,640
distance here, a squared L2 distance between x and f of c. Now that's something we can generalize

424
00:41:46,640 --> 00:41:57,280
to manifold to. And this is how we go to manifold variational tranquillers. It's conceptually easy.

425
00:41:57,280 --> 00:42:04,880
We replace the addition by the exponential map. And now we have a generative model that outputs

426
00:42:04,880 --> 00:42:14,320
points on the manifold. And that goes like this. We find again zi on the latent space RL. And now

427
00:42:14,320 --> 00:42:22,400
our decoder is going to be able to map zi on the point on manifold to which we add noise by adding

428
00:42:22,400 --> 00:42:31,040
a tangent vector to the point in group. So we have generalized the generative model to

429
00:42:31,040 --> 00:42:38,080
manifolds. We need to generalize the inference, which is how do we learn in this generative model.

430
00:42:39,040 --> 00:42:46,720
We have a very similar architecture that has an encoder and a decoder. But then what really changes

431
00:42:46,720 --> 00:42:53,760
now is the loss function that you can still formulate as an elbow of a different log likelihood,

432
00:42:53,760 --> 00:42:59,520
just because the generative model is different. And I've put in red the terms that change,

433
00:43:00,160 --> 00:43:05,120
because the generative model is different, the log likelihood is different, the evidence lower

434
00:43:05,200 --> 00:43:10,800
bound or the elbow is different. And therefore you can show that it has this formula.

435
00:43:12,160 --> 00:43:19,360
We could have guessed this formula by converting the elbow plus as it was written on vector spaces

436
00:43:19,360 --> 00:43:28,320
to its expression on manifolds. And so this allows us to learn non-giardasec

437
00:43:28,960 --> 00:43:37,360
sub-manifolds of a given manifold. I think the only competing method that was able to do that

438
00:43:37,360 --> 00:43:41,440
before was relying on Monte Carlo, for example, from the posterior. And therefore

439
00:43:45,760 --> 00:43:53,680
so using VAE we're able to learn non-giardasec sub-manifold of a manifold. We're also able to

440
00:43:53,680 --> 00:44:02,720
give some insights of what was observed in the literature about VAE, which was about this

441
00:44:02,720 --> 00:44:10,160
statement that you can see by Chao Edel in 2018. But I was saying that in VAE the experiment showed

442
00:44:10,720 --> 00:44:16,320
that these models represent real image data with manifolds that have surprisingly little curvature.

443
00:44:17,200 --> 00:44:23,600
Long story short, we are able through a geometric analysis of geometric VAE

444
00:44:23,600 --> 00:44:30,800
that gives insight on VAE to explain why they find latent spaces with shockingly little curvature.

445
00:44:33,440 --> 00:44:38,400
So to conclude, this is the type of pipeline that we're interested in our lab, we want to do shape

446
00:44:38,400 --> 00:44:45,360
learning in biomedical imaging. I've explained how shape modeling gives rise to

447
00:44:46,320 --> 00:44:52,560
overwhelming operations on excessive data spaces that are manifolds, and therefore how statistical

448
00:44:52,560 --> 00:44:57,600
learning, machine learning, and deep learning can generalize to these spaces if you want to shape

449
00:44:57,600 --> 00:45:06,640
deep learning. Many thanks to our lab at UCSB and many thanks to the organization that are funding

450
00:45:06,640 --> 00:45:10,720
this research, and I'll be happy to answer any questions.

