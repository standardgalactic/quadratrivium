1
00:00:00,000 --> 00:00:07,680
So I'm going to be talking about the book indeed, but I guess the most interesting part

2
00:00:07,680 --> 00:00:12,800
of what I have to say is about chat GPT, and so I changed the title.

3
00:00:12,800 --> 00:00:14,840
This is the book.

4
00:00:14,840 --> 00:00:18,520
The subtitle is Artificial Intelligence Without Fear.

5
00:00:18,520 --> 00:00:23,640
So we certainly don't need to be worried about the supposed fact that machines will one day

6
00:00:23,640 --> 00:00:25,320
rule the world.

7
00:00:25,720 --> 00:00:33,920
AI is a set of algorithms, of algorithms which belong to a certain kind of applied mathematics.

8
00:00:33,920 --> 00:00:36,000
And these algorithms are very good.

9
00:00:36,000 --> 00:00:37,520
They can do wonderful things.

10
00:00:37,520 --> 00:00:42,800
And the fear that people have, and we are aiming in the book to set aside this fear,

11
00:00:42,800 --> 00:00:48,800
is that one day there will be an algorithm of this sort which is able to provide an

12
00:00:48,800 --> 00:00:53,480
intelligence which surpasses the intelligence of human beings.

13
00:00:53,480 --> 00:01:00,520
And then once we have an AI algorithm like that, it would be able to write a new AI algorithm

14
00:01:00,520 --> 00:01:06,000
which would be even more intelligent, and then we have an explosion of ever more intelligent

15
00:01:06,000 --> 00:01:12,960
AIs, and eventually they would be able to use their intelligence to replace human beings

16
00:01:12,960 --> 00:01:17,320
and to rule the world or the galaxy or the whole universe in principle.

17
00:01:17,320 --> 00:01:18,720
This is nonsense.

18
00:01:18,720 --> 00:01:20,200
This will never happen.

19
00:01:20,280 --> 00:01:25,920
AI algorithms will be always much lower in intelligence than human beings.

20
00:01:25,920 --> 00:01:31,560
Indeed, they will never have intelligence like the intelligence of human beings, because

21
00:01:31,560 --> 00:01:37,600
they will always be what is called narrow AI, which means that they are intelligent only

22
00:01:37,600 --> 00:01:43,200
in relation to one specific activity, for instance, playing the game of Go, and they

23
00:01:43,200 --> 00:01:48,200
will never have the kind of general intelligence which we have and which would be needed to

24
00:01:48,200 --> 00:01:49,760
take over the world.

25
00:01:49,760 --> 00:01:53,240
So that's what we mean by artificial intelligence without fear.

26
00:01:53,240 --> 00:01:59,800
There will never be the singularity when AI explodes and becomes more intelligent and

27
00:01:59,800 --> 00:02:02,080
more powerful than we are.

28
00:02:02,080 --> 00:02:06,440
So this is another way of formulating the main theses of the book, which rests upon

29
00:02:06,440 --> 00:02:09,680
the mathematics of complex systems.

30
00:02:09,680 --> 00:02:15,720
Complex systems, and that means all systems involving organisms, your brain, your digestive

31
00:02:15,720 --> 00:02:20,360
system, you yourself, the system formed now by the people in this room.

32
00:02:20,360 --> 00:02:23,480
All of these complex systems have evolutionary properties.

33
00:02:23,480 --> 00:02:25,960
What that means is that they can change.

34
00:02:25,960 --> 00:02:32,120
They can acquire new elements, new types of elements, new types of interactions.

35
00:02:32,120 --> 00:02:38,480
And any model which can predict the behavior of a system breaks when you have new types

36
00:02:38,480 --> 00:02:41,320
of phase space, they say, in physics.

37
00:02:41,320 --> 00:02:45,000
We can't model complex systems mathematically.

38
00:02:45,000 --> 00:02:51,680
Therefore, we can't emulate such systems inside a computer that follows trivially.

39
00:02:51,680 --> 00:02:53,920
So this is the main thesis of the book.

40
00:02:53,920 --> 00:02:57,240
The main chapter is about the mathematics of complex systems.

41
00:02:57,240 --> 00:03:00,120
The rest of the book is about many things.

42
00:03:00,120 --> 00:03:03,180
It's about intelligence, which I'm going to talk about next.

43
00:03:03,180 --> 00:03:05,760
Human intelligence, what makes it special?

44
00:03:05,760 --> 00:03:11,840
It's about attempts to emulate human intelligence by means of modeling in some sense biologically

45
00:03:11,840 --> 00:03:12,840
the human brain.

46
00:03:12,840 --> 00:03:14,960
I'm going to talk about that.

47
00:03:15,680 --> 00:03:21,840
And then I'm going to focus my energies on chat GPT, which is, as I say, something glorious,

48
00:03:21,840 --> 00:03:25,800
but it's also really, really, really bad.

49
00:03:25,800 --> 00:03:28,640
And I'll try and prove that with some examples.

50
00:03:28,640 --> 00:03:35,280
So an example of a system which is changing its phase space is the system of creating

51
00:03:35,280 --> 00:03:43,120
spam, which is a system run by evil people whose life is devoted to creating these horrible

52
00:03:43,160 --> 00:03:45,280
things called spam.

53
00:03:45,280 --> 00:03:47,480
We can stop the spam using AI.

54
00:03:47,480 --> 00:03:51,720
We can build spam filters, which are narrow AI in two senses.

55
00:03:51,720 --> 00:03:58,320
One, they only filter out spam, but two, they only filter out spam of a sort and sort.

56
00:03:58,320 --> 00:04:03,280
And as soon as new types of spam come down the pipeline, then the spam filters won't

57
00:04:03,280 --> 00:04:04,280
work.

58
00:04:04,280 --> 00:04:09,560
And this is what I mean by the impossibility of predicting the future, predicting future

59
00:04:09,560 --> 00:04:11,240
behavior of a complex system.

60
00:04:11,240 --> 00:04:15,880
Even a complex system as familiar as the system of spam creation.

61
00:04:15,880 --> 00:04:17,080
All right.

62
00:04:17,080 --> 00:04:23,240
So AI is always limited to simple systems in a technical sense.

63
00:04:23,240 --> 00:04:31,000
So an AI algorithm like chat GPT is a huge mathematical polynomial function with billions

64
00:04:31,000 --> 00:04:32,320
of parameters.

65
00:04:32,320 --> 00:04:40,920
Google Translate is not based upon those complex systems which are human languages.

66
00:04:40,920 --> 00:04:48,720
It's based on a frozen set of data, a corpus taken from the 96 or so human languages which

67
00:04:48,720 --> 00:04:51,160
Google Translate translates.

68
00:04:51,160 --> 00:04:55,720
And that corpus is then turned into a simple system.

69
00:04:55,720 --> 00:05:03,040
And then Google Translate uses very large algorithms to create polynomial functions

70
00:05:03,040 --> 00:05:08,240
which can take an input in German and yield an output in English.

71
00:05:08,240 --> 00:05:14,200
And that's a mathematical application to binary vectors made up of zeros and ones which can

72
00:05:14,200 --> 00:05:19,360
be translated as English sentences and binary vectors made up of zeros and ones which can

73
00:05:19,360 --> 00:05:22,320
be translated as German sentences.

74
00:05:22,320 --> 00:05:24,120
Google Translate is dumb.

75
00:05:24,120 --> 00:05:27,480
It doesn't know anything about meaning or semantics.

76
00:05:27,480 --> 00:05:29,560
It doesn't know what it's talking about.

77
00:05:29,560 --> 00:05:34,080
It just performs a certain mathematical calculation.

78
00:05:34,080 --> 00:05:39,200
Simply rather simple because it has to compute inside a Turing machine which is a relatively

79
00:05:39,200 --> 00:05:45,040
simple kind of environment but incredibly long as an algorithm which explains why it's

80
00:05:45,040 --> 00:05:47,520
able to perform such impressive feat.

81
00:05:47,520 --> 00:05:49,960
So there is glory to Google Translate.

82
00:05:49,960 --> 00:05:54,680
I think Google Translate is fantastic but it's not going to take over the world or anything

83
00:05:54,680 --> 00:05:55,680
like that.

84
00:05:55,680 --> 00:05:56,680
All right.

85
00:05:56,680 --> 00:05:57,680
Now how does this work?

86
00:05:57,680 --> 00:06:02,040
How does an algorithm like Google Translate work?

87
00:06:02,040 --> 00:06:07,680
Well many people think that all you need is enough training data and then these things

88
00:06:07,680 --> 00:06:17,280
called deep neural networks can be trained to use statistics in order to predict patterns

89
00:06:17,280 --> 00:06:20,000
in those large bodies of data.

90
00:06:20,000 --> 00:06:25,520
But this isn't quite right and even a lot of people in the AI world don't appreciate

91
00:06:25,520 --> 00:06:31,920
this shortfall in the idea that all we need is mere quantity of data.

92
00:06:31,920 --> 00:06:38,880
What we need is to be able to sample data which has a variance which is the same as

93
00:06:38,880 --> 00:06:40,400
the target data.

94
00:06:40,400 --> 00:06:46,000
So if we're going to take the sample data and use it to predict patterns in the target

95
00:06:46,000 --> 00:06:53,040
data then the sample data has to be statistically like, it has to be a typical sample in other

96
00:06:53,040 --> 00:06:55,400
words, like the target data.

97
00:06:55,400 --> 00:06:58,880
It must be representative of the target data.

98
00:06:58,880 --> 00:07:03,200
So what that means is that it has to have the same distribution of the target data and

99
00:07:03,200 --> 00:07:06,600
this is the bell curve which is the simplest kind of distribution.

100
00:07:06,600 --> 00:07:12,600
There are other kinds of distribution but the data you have has to have the same distribution

101
00:07:12,600 --> 00:07:16,600
as the target data you're applying to and that's what Google Translate does.

102
00:07:16,600 --> 00:07:23,200
It takes samples from all the world's languages and it is able to take them as representative

103
00:07:23,200 --> 00:07:28,120
of the patterns in this frozen corpus that they use as a starting point.

104
00:07:28,120 --> 00:07:32,360
Now there are target domains where there is no distribution and so there is no way in

105
00:07:32,360 --> 00:07:35,160
which we can get representative sample data.

106
00:07:35,160 --> 00:07:41,160
So this is true in an emergency room in a hospital in a big city.

107
00:07:41,160 --> 00:07:45,320
You just can't predict how much blood will be needed or how many beds will be needed

108
00:07:45,320 --> 00:07:49,360
or how many doctors will be needed even an hour ahead.

109
00:07:49,360 --> 00:07:51,960
But it's true also of any conversation.

110
00:07:51,960 --> 00:07:56,320
You can't predict what your conversation partner will say next.

111
00:07:56,360 --> 00:07:59,200
Alright so this is an overview of what I'm going to talk about.

112
00:07:59,200 --> 00:08:04,760
First of all I'm going to talk about human intelligence, actually animal and human intelligence.

113
00:08:04,760 --> 00:08:10,520
Then I'm going to talk about the real reason why computers will never take over the world

114
00:08:10,520 --> 00:08:16,560
which is the fact that they will never want to take over the world because algorithms can't

115
00:08:16,560 --> 00:08:17,560
want.

116
00:08:17,560 --> 00:08:20,040
They can only do what you tell them to do.

117
00:08:20,040 --> 00:08:26,200
Then I'll talk a little bit about Nick Bostrom and his idea that we can build a super-intelligent

118
00:08:26,200 --> 00:08:31,480
AI algorithm by emulating the whole brain of the human being.

119
00:08:31,480 --> 00:08:38,320
And then finally I'll talk about the really funny story of chat GPT.

120
00:08:38,320 --> 00:08:46,680
Alright so the big difference between organisms and simple systems is in one word it's thermodynamics.

121
00:08:46,680 --> 00:08:52,760
So in other words it's a matter of physics which involves energy and we and all animals

122
00:08:52,760 --> 00:08:58,200
survive because we have the drive to acquire energy from the environment.

123
00:08:58,200 --> 00:09:02,600
Now we humans do this in a very complicated way involving things like supermarkets and

124
00:09:02,600 --> 00:09:08,120
farms but every animal has a way of sucking energy out of the environment.

125
00:09:08,120 --> 00:09:10,480
Every plant does this with the sun.

126
00:09:10,480 --> 00:09:12,480
Even computers are driven in a certain sense.

127
00:09:12,480 --> 00:09:18,080
They take energy from the environment but only because we give it to them and no one

128
00:09:18,080 --> 00:09:19,520
gives us energy.

129
00:09:19,520 --> 00:09:22,600
No one gives animals energy we have to go and find it ourselves.

130
00:09:22,600 --> 00:09:30,120
If there is a surface of energy in our environment, in the ancestral environment of human beings

131
00:09:30,120 --> 00:09:36,760
then we become obese because we like eating and so we keep eating and this eventually

132
00:09:36,760 --> 00:09:41,240
will mean that we will eat so much that we use up all the energy in the environment and

133
00:09:41,240 --> 00:09:43,240
then we die.

134
00:09:43,240 --> 00:09:51,360
So gradually we moved out of the areas of the world where there was lots of food into

135
00:09:51,360 --> 00:09:57,040
areas of the world which were cold and barren and so we had to find ways of surviving in

136
00:09:57,040 --> 00:09:59,720
much harsher environments.

137
00:09:59,720 --> 00:10:08,200
That's why through a long series of faltering steps we created civilization, police, armies

138
00:10:08,200 --> 00:10:13,560
all the other things which make it possible for us to survive in a world where we are

139
00:10:13,560 --> 00:10:20,880
competing with other groups for limited food supplies.

140
00:10:20,880 --> 00:10:26,480
What civilization does, what social norms do is channel the excess dry which human beings

141
00:10:26,480 --> 00:10:27,480
have.

142
00:10:27,480 --> 00:10:34,320
In other words we become more rational and less instinctive.

143
00:10:34,320 --> 00:10:43,200
But we are still always seeking for energy but now because we have found ways of solving

144
00:10:43,200 --> 00:10:47,160
the energy problem through supermarkets and farms and so on.

145
00:10:47,160 --> 00:10:53,920
We can do other things, we can build orchestras, we can go to talks about chat GPT, we can

146
00:10:53,920 --> 00:10:59,360
play with chat GPT, we can watch the traffic through the window, we're always doing something.

147
00:10:59,360 --> 00:11:03,880
We're doing one damn thing after another and that's the same with animals too.

148
00:11:03,880 --> 00:11:09,760
We never stop, there's no tendency towards equilibrium.

149
00:11:09,760 --> 00:11:18,640
As long as we're alive we are doing one damn thing after another so no convergence on equilibrium.

150
00:11:18,640 --> 00:11:25,160
This is thermodynamically remarkable that there are entities on the planet which are

151
00:11:25,160 --> 00:11:33,440
decreasing entropy by taking energy out of the environment and replacing it with cathedrals

152
00:11:33,480 --> 00:11:37,800
or with airplanes or supermarkets.

153
00:11:37,800 --> 00:11:46,880
Now so as we go through life not approaching any equilibrium we are constantly changing

154
00:11:46,880 --> 00:11:51,480
our state, changing the phase space.

155
00:11:51,480 --> 00:11:56,120
So if we're in an orchestra and we're under the command of the conductor we have one phase

156
00:11:56,120 --> 00:12:03,400
space but then suddenly we have a pain in our arm and we run outside and go to the doctor

157
00:12:03,400 --> 00:12:08,000
because we think there's something wrong with our arm and we're in another phase space.

158
00:12:08,000 --> 00:12:13,560
Any kind of change like that and such changes happen all the time would break any kind of

159
00:12:13,560 --> 00:12:22,400
predictive machine because predictive machines have to use mathematical equations of a mathematically

160
00:12:22,400 --> 00:12:28,480
rather simple sort and they can't cope with multiple ever-changing phase spaces.

161
00:12:28,480 --> 00:12:34,440
This is if you want to predict the behavior of an entity where you have a Cartesian coordinate

162
00:12:34,440 --> 00:12:40,320
system telling you what its behavior is but then suddenly it changes the behavior so that

163
00:12:40,320 --> 00:12:44,200
you need a further dimension and a different coordinate system.

164
00:12:44,200 --> 00:12:50,600
Your predictive attempt would fail because you've changed the phase space.

165
00:12:50,600 --> 00:12:54,440
Alright now there are in fact three kinds of drivenness.

166
00:12:54,440 --> 00:13:01,040
There's animate drivenness which is organisms, animals and humans particularly.

167
00:13:01,040 --> 00:13:09,200
There is inanimate drivenness so the tides take energy from the moon I guess and the

168
00:13:09,200 --> 00:13:12,720
whole earth takes energy from the sun.

169
00:13:12,720 --> 00:13:18,080
Machines get energy given to them so we give coal to the steam engine, we give electricity

170
00:13:18,080 --> 00:13:19,920
to the computer.

171
00:13:19,920 --> 00:13:25,760
This is external drivenness and external drivenness means that the external supplier of energy

172
00:13:25,760 --> 00:13:29,840
which is typically a human being is in control of the machine.

173
00:13:29,840 --> 00:13:34,320
That's another reason why machines will never rule the world.

174
00:13:34,320 --> 00:13:41,360
Alright so we have natural drivenness and artificial drivenness and artificial drivenness

175
00:13:41,360 --> 00:13:45,240
means steam engines, laptops, tanks and so on.

176
00:13:45,240 --> 00:13:47,760
Ice drivenness depends on human drivenness.

177
00:13:47,760 --> 00:13:50,960
We want to have the steam engine do something for us.

178
00:13:50,960 --> 00:13:56,280
If it's not doing anything for us we're not going to feed it energy anymore and that's

179
00:13:56,280 --> 00:13:58,400
what happens.

180
00:13:58,400 --> 00:14:05,120
So somebody forgot to maintain this entity and so we don't need to supply it with energy

181
00:14:05,120 --> 00:14:08,960
anymore.

182
00:14:08,960 --> 00:14:14,120
And this is how Schrodinger expresses this matter.

183
00:14:14,120 --> 00:14:18,840
Now of course eventually we do not escape the decay to equilibrium, there comes a point

184
00:14:18,840 --> 00:14:23,480
where we go over the cliff and then we're dead jack.

185
00:14:23,480 --> 00:14:28,360
But until then it's one damn thing after another.

186
00:14:28,360 --> 00:14:34,440
Alright now so machines need energy from the environment and they create energy.

187
00:14:34,440 --> 00:14:40,120
So a computer if it's switched on but not being used is a heater, it's giving off heat

188
00:14:40,120 --> 00:14:45,360
and this is another reason why what we're talking about now is thermodynamics.

189
00:14:45,360 --> 00:14:52,000
And this aspect of computers is often neglected but it's another factor in the question whether

190
00:14:52,000 --> 00:14:54,400
computers would ever take over the world.

191
00:14:54,400 --> 00:15:01,480
So we already know that the crypto coin industry is using significant amounts of energy, significant

192
00:15:01,480 --> 00:15:04,800
fractions of the energy which humans need to live.

193
00:15:04,800 --> 00:15:10,680
If we have computers of anything like the power that people conceive then there would

194
00:15:10,680 --> 00:15:17,560
be an energy problem and that would mean that this power would be reduced one way or another.

195
00:15:17,560 --> 00:15:22,480
But of course we'll never get even near there.

196
00:15:22,480 --> 00:15:26,640
We will never see even the attempt to take over the world by machines because they cannot

197
00:15:26,640 --> 00:15:28,480
want anything.

198
00:15:28,480 --> 00:15:35,560
Alright so we produce energy storing molecules called ATP from the sun and from food and

199
00:15:35,560 --> 00:15:40,880
so forth and then we use that energy to survive and reproduce and to do all the things that

200
00:15:40,880 --> 00:15:46,160
we do such as wave our arms when we're speaking and things like that.

201
00:15:46,160 --> 00:15:48,920
Now we come to intelligence.

202
00:15:48,920 --> 00:15:59,040
So primal intelligence we find in both animals and humans.

203
00:15:59,040 --> 00:16:04,560
And then there is a kind of intelligence that we call objectifying which is exclusive to

204
00:16:04,560 --> 00:16:10,440
humans and which is the reason why we're able to build supermarkets and farms and airports

205
00:16:10,440 --> 00:16:17,760
and all of those other things that enable us to do more than survive.

206
00:16:17,760 --> 00:16:24,800
So primal intelligence is what animals do when they're in their ancestral environments

207
00:16:24,800 --> 00:16:27,920
and they're acquiring food.

208
00:16:27,920 --> 00:16:30,600
If there is food around then they just use it.

209
00:16:30,600 --> 00:16:35,360
If they have to go chasing food, finding food because their available resources have been

210
00:16:35,360 --> 00:16:41,680
used up then they have to still use their primal intelligence but they have to use their

211
00:16:41,680 --> 00:16:46,960
primal intelligence in order to find new food which means they need at least two aspects

212
00:16:46,960 --> 00:16:50,280
of intelligence which plants don't have.

213
00:16:50,280 --> 00:16:56,160
One is they need to have conscious perception because they need to be able to identify new

214
00:16:56,160 --> 00:17:03,880
food as food rather than as something which looks like food but which is in fact poison

215
00:17:03,880 --> 00:17:08,400
or are just an accident of similarity of shape.

216
00:17:08,400 --> 00:17:16,920
And so they display ever more powerful versions of primal intelligence as they become more

217
00:17:17,920 --> 00:17:23,720
complicated, more ambitious in their attempts to find new food and eventually they go hunting

218
00:17:23,720 --> 00:17:30,600
in teams and then they develop a crude or language, a proto language to organize the

219
00:17:30,600 --> 00:17:34,240
other members of the team so that they know what's going on when they're hunting large

220
00:17:34,240 --> 00:17:36,680
animals for instance.

221
00:17:36,680 --> 00:17:42,560
And so they become to some degree adaptive but always within the ancestral environment.

222
00:17:42,560 --> 00:17:47,280
The adaptiveness is their ability to find new food and of course if they fail to find

223
00:17:47,280 --> 00:17:54,960
new food then they're dead and this applies to all kinds of animals from parrots to humans

224
00:17:54,960 --> 00:18:00,320
in the ancestral state.

225
00:18:00,320 --> 00:18:06,240
So we don't learn primal intelligence, it's innate, it's instinct and the characteristic

226
00:18:06,240 --> 00:18:12,400
of human beings is that they have abandoned, they've lost most of their instincts and instead

227
00:18:12,400 --> 00:18:17,280
we have civilization, we have social norms, social control and so on.

228
00:18:17,280 --> 00:18:26,360
And it's a marker for intelligence in the sense that it doesn't act by trial and error

229
00:18:26,360 --> 00:18:35,240
or by, I don't know, some alternative to trial and error which would involve checking samples,

230
00:18:35,240 --> 00:18:36,760
it's immediate.

231
00:18:36,760 --> 00:18:40,600
As soon as they see something which looks like food immediately they know that that could

232
00:18:40,600 --> 00:18:46,720
be food and the typical characteristic feature of something's being intelligent is that it's

233
00:18:46,720 --> 00:18:50,640
a response which happens immediately.

234
00:18:50,640 --> 00:18:57,640
Alright so these are the features of primal intelligence and so you can't train anything

235
00:18:57,640 --> 00:19:01,880
to have it, either it has it or doesn't or it doesn't.

236
00:19:01,880 --> 00:19:07,000
And non-human animals have just the goals of their ancestral environment to find food

237
00:19:07,000 --> 00:19:13,640
in that environment, to survive when competitors try and steal the food so they have the ability

238
00:19:13,640 --> 00:19:20,440
to fight or the ability to flee and they ignore everything which is not responding to their

239
00:19:20,440 --> 00:19:23,120
biological needs.

240
00:19:23,120 --> 00:19:30,880
Their world is just that which is relevant to eating, fighting, fleeing and so forth.

241
00:19:30,880 --> 00:19:35,760
Now higher animals, as I've already said, can develop something like a proto-language

242
00:19:35,800 --> 00:19:41,840
so birds have elaborate signalling systems for instance.

243
00:19:41,840 --> 00:19:46,840
Many animals have developed elaborate tracking skills for seeking the food which they need

244
00:19:46,840 --> 00:19:55,320
in order to survive and they've even developed something like wanting so they want to find

245
00:19:55,320 --> 00:19:57,960
food when they're hungry.

246
00:19:57,960 --> 00:20:03,640
But it's always within the ancestral environment so they don't build new kinds of buildings

247
00:20:03,640 --> 00:20:12,600
because they don't build buildings really and that's the big difference of course when

248
00:20:12,600 --> 00:20:17,560
we move to the case of humans.

249
00:20:17,560 --> 00:20:22,360
So we had to survive in tough environments that meant that we had to go outside our ancestral

250
00:20:22,360 --> 00:20:27,880
environment which means that we had to abandon practically all of the instincts that kept

251
00:20:27,880 --> 00:20:32,920
us alive in the ancestral environment and work out new ways of living.

252
00:20:32,920 --> 00:20:38,000
And that meant that we had to develop things like curiosity but we had to develop other

253
00:20:38,000 --> 00:20:49,720
kinds of capabilities and one way of grouping these capabilities so everything I've said

254
00:20:49,720 --> 00:20:57,400
so far is pretty standard but the term objectifying intelligence is a new term which we formulated

255
00:20:57,400 --> 00:21:03,560
in response to Husserl's way, I'm switching suddenly to philosophy, of understanding the

256
00:21:03,560 --> 00:21:06,560
way language and the mind works.

257
00:21:06,560 --> 00:21:13,760
So he talks about objectifying acts and what he means by is acts directed towards objects

258
00:21:13,760 --> 00:21:18,640
typically other people but it might also be things like tables chairs or it might be things

259
00:21:18,640 --> 00:21:25,120
in the future or in the past things which are distant in each case we have this objectifying

260
00:21:25,120 --> 00:21:33,640
intelligence and for humans this goes beyond any biological need, it can extend towards

261
00:21:33,640 --> 00:21:40,480
the future, it can extend towards the opera, it can extend towards the planet Mars independently

262
00:21:40,480 --> 00:21:46,760
of any biological need which is the reverse situation from what we find among animals.

263
00:21:46,760 --> 00:21:51,560
So we're moving into new kinds of contexts all the time, we're able to keep track of

264
00:21:51,560 --> 00:21:56,840
objects as we move from one context to another or we're able to switch targeting completely

265
00:21:56,840 --> 00:22:04,440
to a new set of objects and a new set of norms and so forth and this happens sometimes in

266
00:22:04,440 --> 00:22:11,600
a given in a single conversation so that reminds me of what we were talking about last Christmas

267
00:22:11,600 --> 00:22:17,040
about the rotten cheese that had made me so sick just before the Covid panic started.

268
00:22:17,040 --> 00:22:23,560
We jumped around in just one longish part sentence between multiple context you all

269
00:22:23,560 --> 00:22:29,400
follow what I was referring to even though you've never heard this I'm not sure now

270
00:22:29,400 --> 00:22:33,760
what would happen if I fed this into chat GPT.

271
00:22:33,760 --> 00:22:41,400
So there's no Markov property here one of the reasons why computers are not able to predict

272
00:22:41,400 --> 00:22:46,800
the future in a realm like human conversation is because human conversations don't have

273
00:22:46,800 --> 00:22:53,440
the Markov property and our mathematical resources to model processes nearly always

274
00:22:53,440 --> 00:22:59,120
rely on the Markov property that's missing.

275
00:22:59,120 --> 00:23:04,720
Alright so how did objectifying intelligence evolve the answer is over millions of years

276
00:23:04,720 --> 00:23:13,960
and certain parts of it I can talk about here so one important part I've already mentioned

277
00:23:14,320 --> 00:23:18,520
because we have these proto languages and eventually have language in its fully formed

278
00:23:18,520 --> 00:23:25,160
state we can engage in all kinds of shared agency so we can plan on going to the moon

279
00:23:25,160 --> 00:23:31,800
or we can build a cathedral or we can well we started by building walls to keep us safe

280
00:23:31,800 --> 00:23:38,040
against our enemies building a wall like that involved some considerable shared agency at

281
00:23:38,080 --> 00:23:44,720
that time and this is one of the oldest five walls on earth on top by Google I guess I

282
00:23:44,720 --> 00:23:51,040
could chat chat GPT to alright so these are some of the marks of objectifying intelligence

283
00:23:51,040 --> 00:23:57,320
and I'll go through this quite quickly so as I say it doesn't depend upon our biological

284
00:23:57,320 --> 00:24:01,520
state you can move in any cultural world you can move in the world of mathematics you can

285
00:24:01,520 --> 00:24:09,480
move in the world of plant biology it's completely open and it involves categorical thinking

286
00:24:09,480 --> 00:24:16,640
already from infancy so children can recognize categories they have an infant metaphysics

287
00:24:16,640 --> 00:24:22,360
and we have a world model which is built out of these categories and the relations between

288
00:24:22,360 --> 00:24:28,760
objects in different categories for instance the causal relations but then also the relations

289
00:24:28,760 --> 00:24:33,240
having to do with ethics for instance that if you bump into a chair you don't need to

290
00:24:33,240 --> 00:24:38,680
apologize to the chair but if you bump into a human being you probably need to apologize

291
00:24:38,680 --> 00:24:46,640
and we have a theory of mind or inter subjectivity so you are all objects I am an object for you

292
00:24:46,640 --> 00:24:55,120
I can also be an object for myself in being an object for me under the category of person I

293
00:24:55,360 --> 00:25:01,360
appreciate automatically without reasoning about it that you have beliefs and desires and so

294
00:25:01,360 --> 00:25:10,400
forth we can plan so objectifying intelligence allows us to plan for the future and we can

295
00:25:10,400 --> 00:25:21,200
plan together to build an airport or a moon landing or whatever it might be and then finally a

296
00:25:21,360 --> 00:25:27,840
feature of objectifying intelligence is that while we typically target objects that we believe to

297
00:25:27,840 --> 00:25:35,120
exist we can cancel belief and we can imagine and we do that when we plan when we have ambitious

298
00:25:35,120 --> 00:25:39,760
plans we plan going beyond the planet earth but we can also do it when we're writing fiction

299
00:25:40,560 --> 00:25:46,400
and this is this is an ability way way beyond anything which animals have chat gpt has this

300
00:25:46,960 --> 00:25:53,760
ability it but it doesn't need to suspend belief because it doesn't have any belief in the in

301
00:25:53,760 --> 00:26:02,800
the beginning it can mimic writing imaginative texts and then we once we build these new environments

302
00:26:02,800 --> 00:26:10,240
we can live in them culturally including in scientific environments so we can build an

303
00:26:10,240 --> 00:26:15,600
environment to serve a certain purpose for instance studying disease or whatever it might be

304
00:26:16,640 --> 00:26:23,120
all right now we come to the missing a i will and we'll talk a little bit about my hero nick

305
00:26:23,120 --> 00:26:30,160
bostrom who wrote a book called superintelligence in this book he says all philosophers should

306
00:26:30,160 --> 00:26:40,320
give up their job and work with him to prevent the singularity and this we this is a rational act

307
00:26:40,320 --> 00:26:47,760
because once a i become superintelligent it will be able to do better philosophy than we can do now

308
00:26:47,760 --> 00:26:56,320
anyway so preventing the intelligence well anyway you get the idea so now he thinks that

309
00:26:56,320 --> 00:27:04,080
this singularity could exist and that there is a ticking time bomb which is the a i this chat gpt

310
00:27:04,080 --> 00:27:09,920
plotting to take over the world it's already ticking and we don't know how far away we are from

311
00:27:09,920 --> 00:27:17,840
the great cataclysmic events when a i will machines will join together and and take over

312
00:27:17,840 --> 00:27:25,360
the universe but he worries a lot about it and the problem is as i say that computers can't want

313
00:27:26,080 --> 00:27:31,760
and so they can't want to take over the world they do not have a will now bostrom talks quite a

314
00:27:31,760 --> 00:27:38,160
bit about goals of machines in his book but he never explains how computers can have goals

315
00:27:39,040 --> 00:27:47,840
what he does is refer to this man yudkowski who i understand does good work in a i ethics

316
00:27:49,280 --> 00:27:57,120
and so and he apologizes he refers to yudkowski's work on the machine will but he wants to distance

317
00:27:57,120 --> 00:28:01,680
himself because he appreciates that it's not really quite clear what yudkowski is trying to say

318
00:28:02,480 --> 00:28:08,800
and you can decide for yourself so this is what he says it you will notice that he doesn't tell us

319
00:28:08,800 --> 00:28:15,440
how a goal system will come into existence he just tells us about what a goal system is like

320
00:28:16,080 --> 00:28:20,560
and he tells us only about the goal system goal system that he himself would like

321
00:28:21,520 --> 00:28:27,040
not about the goal system which a machine if it could have goals which of course it can't

322
00:28:27,120 --> 00:28:33,680
so it's a goal system containing only decisions super goals in and beliefs with all sub goal

323
00:28:33,680 --> 00:28:38,960
content being identical with beliefs about which events are predicted to lead to other events

324
00:28:38,960 --> 00:28:46,000
and all desirability being identical with with leads to supergoldness if you can understand that

325
00:28:46,000 --> 00:28:52,640
then you're a better man or woman than i i have no idea what he's talking about and that's why

326
00:28:52,640 --> 00:28:58,960
bostrom apologized because he didn't have any idea and he goes on like this so the content of this

327
00:28:58,960 --> 00:29:06,800
goal system is our wish if we knew more thought faster were more people we wished we were had

328
00:29:06,800 --> 00:29:13,600
drawn up further together where the extrapolation converges and so on it's complete i don't understand

329
00:29:13,680 --> 00:29:22,560
what it is and it goes on so now why is a machine will and bostrom did not find a

330
00:29:22,560 --> 00:29:26,560
count of a machine well i don't believe that there is a good account of how a machine could have a

331
00:29:26,560 --> 00:29:33,600
will outside the cases i'm going to talk about in in talking about charting pt later on there is

332
00:29:33,600 --> 00:29:41,840
something like a will that i will explain in a minute so without a will the machine could never

333
00:29:41,840 --> 00:29:48,000
become an autonomous agent and if it can never become an autonomous agent then it can never

334
00:29:48,000 --> 00:29:54,720
pursue goals and if it's not autonomous it can never be either moral or immoral you can only

335
00:29:54,720 --> 00:29:59,600
be moral if you can take responsibility for your actions and you can only take responsibility if you

336
00:29:59,600 --> 00:30:05,360
will them if we will them which is what we would do in writing the software they're not your goals

337
00:30:05,360 --> 00:30:13,200
and you look you do not have a will you're just following our will so how do we understand the

338
00:30:13,200 --> 00:30:18,880
human will now here i'm going to do some more philosophy this is a man called max sheila who

339
00:30:18,880 --> 00:30:25,360
was a very influential philosopher the turn of the last century and one of his students with

340
00:30:25,360 --> 00:30:31,920
edith stein who is one of the i wanted to say father figures but i guess i should say mother

341
00:30:31,920 --> 00:30:40,080
figures of feminine of female philosophy who was also a saint so she died in auschwitz and

342
00:30:41,120 --> 00:30:48,960
was canonized and he was a saint too oh he's a saint both of them were very

343
00:30:48,960 --> 00:30:55,040
influent very much influenced by max sheila he whitey was habity tats jaunschlift is about

344
00:30:55,040 --> 00:31:01,280
max sheila's word it's also about thomas equinas of course but it's about sheila primarily and

345
00:31:01,280 --> 00:31:07,680
this is rather an amazing feat for a teacher to have two of his students become canonized

346
00:31:09,200 --> 00:31:15,920
and but so but sheila is interesting for other reasons so this is his big book about ethics

347
00:31:16,560 --> 00:31:22,160
and basically he distinguishes ethics into two categories first of all there's formal ethics

348
00:31:22,160 --> 00:31:28,400
which is cant and the like where you have imperatives that you have to follow and they are

349
00:31:28,400 --> 00:31:34,640
to be followed on the basis of rational arguments and then you have sheila's own version of ethics

350
00:31:34,640 --> 00:31:42,000
which he calls material ethics which is based on feelings value feelings every normal person

351
00:31:42,000 --> 00:31:46,000
experiences value feelings all the time even if it's just thirst

352
00:31:46,000 --> 00:31:59,840
but there are some people psychopaths who are value blind and so sheila on this basis

353
00:31:59,840 --> 00:32:07,280
tries to give an account of the will and his example is a rescue scenario where a man sees

354
00:32:07,280 --> 00:32:14,080
a drowning child and jumps in to rescue the child so it's a perfectly general account of the will

355
00:32:14,080 --> 00:32:20,000
and it could be applied also if you're playing chess the decision to move your night in a certain

356
00:32:20,000 --> 00:32:27,360
direction would fit his schema for what the will is like and so this is the chess scenario

357
00:32:28,080 --> 00:32:33,760
i'm going to talk about the jumping in scenario it consists of four stages but we're only going

358
00:32:33,760 --> 00:32:39,040
to talk about two of them there's a fifth stage where you do actually jump in but this is what

359
00:32:39,040 --> 00:32:46,880
is involved in the will to jump in and more precisely the act of will takes place at the end

360
00:32:46,880 --> 00:32:52,560
here and that there is uh i'll give you a picture in a minute so you see the drowning child it's not

361
00:32:52,560 --> 00:32:58,240
just perception you also begin to have value feelings you feel that there is something which

362
00:32:58,240 --> 00:33:06,880
needs to be done here you might call that a moral affordance and then you draw the value consequence

363
00:33:07,600 --> 00:33:13,360
in the sense that you you you watch the child you realize that she's going to drown and you

364
00:33:13,360 --> 00:33:20,720
realize that this would be a bad thing and then you decide to act now this is this is a complex

365
00:33:20,720 --> 00:33:28,720
phenomenon making a decision so you decide to jump in to save the child and this decision is

366
00:33:28,720 --> 00:33:35,040
based on knowing that you can swim that you can swim well enough in the current to save the child

367
00:33:35,120 --> 00:33:39,200
you have enough time to save the child so this part of the deciding is kind of

368
00:33:39,200 --> 00:33:48,000
rational part combined with value feelings but there are other parts so 3a is forming an intention

369
00:33:48,640 --> 00:33:55,520
to save the child and to view the child as worth saving something that ought to be preserved

370
00:33:57,040 --> 00:34:03,040
and then part of the decision making process is delivering how to how to perform the rescue

371
00:34:03,680 --> 00:34:11,520
but then the important part is resolving to take that course of action and here we're dealing with

372
00:34:11,520 --> 00:34:18,880
something which is a physiological change in the brain and that physiological change in the brain

373
00:34:19,840 --> 00:34:25,600
is it starts you off it starts you moving so it's an act of will which has a real consequence

374
00:34:25,600 --> 00:34:31,440
or rather it's one side of an act of will because you have to have a physiological change also which

375
00:34:31,440 --> 00:34:43,280
triggers the bodily movement so 3c the final very very tiny sliver of your deciding process when you

376
00:34:43,280 --> 00:34:50,320
actually resolve to take the course of action in the full sense that your body starts moving

377
00:34:50,320 --> 00:34:56,240
is practically just the other side of the coin from your body sending signals to your feet

378
00:34:56,240 --> 00:35:03,280
that they need to start running and so we can see this roughly as taking this shape you have something

379
00:35:03,280 --> 00:35:12,480
going on in the brain up here and you have something going on in the arm down here as your you move

380
00:35:12,480 --> 00:35:21,600
out towards a swim I guess I should have taken feet here and that whole thing then is the is the

381
00:35:21,600 --> 00:35:29,520
act of will it's a combination of a very very rapid triggering event in the brain and a very

382
00:35:29,520 --> 00:35:34,240
very rapid signaling event to the relevant part of the body where the triggering event still has

383
00:35:34,240 --> 00:35:40,720
something rational about it now we know very little about the brain and we can't predict any

384
00:35:40,720 --> 00:35:46,320
practically speaking we can't predict any of this and so we can't emulate it in a machine or in an

385
00:35:46,320 --> 00:35:54,480
algorithm and so we can't describe it mathematically and you can check by looking in textbooks of

386
00:35:54,480 --> 00:36:00,640
neurophysiology there's very little in the way of mathematics all right now why is human well

387
00:36:00,640 --> 00:36:05,360
so important well because of hunting and all of those important things which kept us alive during

388
00:36:05,360 --> 00:36:10,240
the eight million years when we were involving ourselves evolving ourselves to a present to

389
00:36:10,240 --> 00:36:16,480
present state now hunting involves tracking and tracking is really difficult and that's because

390
00:36:16,480 --> 00:36:23,040
as you hunt the tiger the tiger is responding to you changing your environment as you change

391
00:36:23,040 --> 00:36:30,320
his environment hiding behind trees performing tricks I don't know what tigers do but all the time

392
00:36:30,320 --> 00:36:37,200
that you're moving around targeting the the tiger you're changing your face space and if you try to

393
00:36:37,200 --> 00:36:44,320
do that with stationary sensors sending one-dimensional signals to a machine you'll get nowhere you will

394
00:36:44,320 --> 00:36:50,640
never be able to hunt a lion a tiger and and we have a section of the book which describes

395
00:36:50,640 --> 00:36:56,080
mathematically why something like tracking an animal or tracking a human being in a forest

396
00:36:56,080 --> 00:37:01,280
or something is going to be way beyond the power of a computer so you have to spot the man with the

397
00:37:01,280 --> 00:37:11,600
gun say he's well he's here and he has to spot the the bird that he's going to shoot and keep track

398
00:37:11,600 --> 00:37:17,200
of the bird all right now the other reason why human well is so important there are many reasons

399
00:37:17,200 --> 00:37:23,760
i'm just going to talk about two of them this is the second one conversation human conversation as

400
00:37:23,760 --> 00:37:32,720
we saw is unpredictable how do we manage human conversation chat box created for bank telephone

401
00:37:32,720 --> 00:37:39,280
conversation with customers after 50 years are still now i want to say crap but i wouldn't say

402
00:37:39,280 --> 00:37:47,920
crap in it polite audience they're not not good 50 years why because conversation is really hard

403
00:37:47,920 --> 00:37:54,800
it's harder than tracking a lion and the the the reason why it's hard is because conversations

404
00:37:54,800 --> 00:38:00,080
rely on context so much and there are many different kinds of contexts including multiple

405
00:38:00,080 --> 00:38:07,040
contexts in a single conversation as you talk about oh how bad it was in the covid era and so

406
00:38:07,040 --> 00:38:14,080
you can shift the the context i just did now i've shifted the context to be about this particular

407
00:38:14,800 --> 00:38:21,680
it's not really a conversation it's a one-sided harangue but um i'm now making what i'm saying

408
00:38:22,240 --> 00:38:30,400
the context for what i'm saying and i just made the that context the context anyway um so our

409
00:38:30,400 --> 00:38:37,360
goals will change but we always have goals it's the goals which keep the conversation alive my

410
00:38:37,360 --> 00:38:43,280
goal is to convince you of certain things that's why i'm becoming so involved and that's some of

411
00:38:43,360 --> 00:38:49,040
you may be becoming involved and we'll respond later i hope so that's what keeps conversation

412
00:38:49,040 --> 00:38:53,520
alive everybody has goals their goals evolve through the conversation but without goals there

413
00:38:53,520 --> 00:39:00,880
would be no conversation chat gpt has no goals well actually that's not quite true i will explain

414
00:39:00,880 --> 00:39:08,240
in what sense chat gpt has a goal in a minute so how can you build a general intelligence a

415
00:39:08,240 --> 00:39:14,880
machine intelligence that can do any of this um so the will will not arise by itself some people

416
00:39:14,880 --> 00:39:20,240
claim that if you put all the computers together in a big internet system it will somehow evolve a

417
00:39:20,240 --> 00:39:27,520
will that's just it's happy talk and you can't program a goal system not even you kowski can

418
00:39:27,520 --> 00:39:33,680
program a goal system we can in some cases if you want to win at the game of go you can program a

419
00:39:33,680 --> 00:39:40,160
goal system you can't program a goal system to win a conversation and if you don't believe me try it

420
00:39:40,160 --> 00:39:47,760
with your spouse makes heaps core of each step in a conversation see who wins it will not work

421
00:39:49,280 --> 00:39:58,400
all right so what are the proposed methods uh to i think i'm near 45 minutes is that correct

422
00:39:58,400 --> 00:40:05,280
but that's fine just it's interesting okay um well that's good to hear all right so the old way of

423
00:40:05,280 --> 00:40:10,880
doing ai was expert systems based on logic then came stochastic systems which are based on statistics

424
00:40:10,880 --> 00:40:16,800
which we've been talking about that's chat gpt boss room have this idea of whole brain emulation

425
00:40:16,800 --> 00:40:24,240
i think i'm going to skip that um because it's full of nonsense uh that is the the the the funny

426
00:40:24,240 --> 00:40:31,360
chapter in the book and i'll give you just one joke um which is not me it's boston and he didn't

427
00:40:31,360 --> 00:40:36,560
realize it was funny and then we will have we won't talk about artificial life at all we'll go

428
00:40:36,560 --> 00:40:41,920
straight to chat gpt so this is the most boston's book and he thought that you could scan the brain

429
00:40:43,360 --> 00:40:47,680
the problem with that is that to scan the brain you need to kill the patient and so that you're

430
00:40:47,680 --> 00:40:52,480
scanning something which is static so you can never find the dynamic patterns in the brain and

431
00:40:52,480 --> 00:40:58,400
that's just one of the problems so and um we don't know anything about the molecular

432
00:40:58,960 --> 00:41:08,800
configuration of cells and um and some people think that we can do ai in in the general genuinely

433
00:41:08,800 --> 00:41:14,080
intelligent sense if we use quantum computers but quantum computers are turing machines too

434
00:41:14,720 --> 00:41:19,440
they're just a lot quicker and we we haven't built one yet practically speaking it's a dead end

435
00:41:20,080 --> 00:41:26,240
maybe a dead end uh he also talks about biological enhancement of existing brains so you can maybe

436
00:41:26,240 --> 00:41:36,320
make superintelligence by selective breeding uh you get i don't know um so you you get a lot of

437
00:41:36,320 --> 00:41:43,120
people to breed and then you select only a small number of embryos that the clever ones so you have

438
00:41:43,120 --> 00:41:49,840
a really clever way selecting intelligent embryos which i don't know about and then he says if we do

439
00:41:49,840 --> 00:42:00,000
that we can raise the iq level by 24.3 iq points that is the silliest thing that was ever said

440
00:42:00,000 --> 00:42:07,120
by anybody working in biology or in any anything near biology it's um anyway it's it's not good now

441
00:42:08,080 --> 00:42:19,440
uh so that basically his whole thing doesn't work um so let's talk about chat gp t and um

442
00:42:19,440 --> 00:42:25,440
i i really mean it when i say it's glorious and it's really a fantastic thing and i like ai

443
00:42:25,440 --> 00:42:32,640
generally i just i'm aware that it's always going to be narrow ai now chat gp t is narrow ai too

444
00:42:32,640 --> 00:42:43,120
can only do one thing um so let's talk about the misery and i imagine all of you have played

445
00:42:43,120 --> 00:42:49,600
with chat gp t if not you should certainly play with chat gp t for a bit and you will find that it

446
00:42:49,600 --> 00:42:57,760
does odd things so that it makes stuff up for instance and now this is an example where it

447
00:42:57,760 --> 00:43:03,440
realizes that it's not really intelligent you can't do something which even a not very intelligent

448
00:43:03,440 --> 00:43:12,320
human being can do so i asked it to send me five a list of five single authored papers on medical ai

449
00:43:13,840 --> 00:43:21,120
and it said no he can't do that but then he gave me a list or sorry it gave me a whole paragraph

450
00:43:21,120 --> 00:43:26,640
of stuff that i didn't want to know so telling me about ai applications in medicine and so on which

451
00:43:26,640 --> 00:43:33,680
i knew anyway it it wants to be nice as it were gets anyway you'll see why it wants to be nice in

452
00:43:33,680 --> 00:43:38,960
a minute and it couldn't give me an answer so to the question i wanted which is an easy question

453
00:43:39,520 --> 00:43:44,880
so it gave me an answer to a different question but then i asked it again a few seconds later the

454
00:43:44,880 --> 00:43:52,880
very same question and it gave me five single authored papers on medical ai sure here are five

455
00:43:52,880 --> 00:44:00,240
single authored papers on medical ai ai so the first problem is that two of them have et al in the

456
00:44:00,240 --> 00:44:08,480
author list now even an ignorant person who understands the request will know that this is a bad

457
00:44:09,600 --> 00:44:15,120
first step in answering that request but it got three right out of five which is a good score

458
00:44:16,000 --> 00:44:23,840
for these difficult questions so and as i say any human intelligence would find this

459
00:44:23,840 --> 00:44:28,960
request is a trivial and it failed but the the next problem is that none of the five papers that

460
00:44:28,960 --> 00:44:38,720
it requested exists it made them up so it can't even make up a single authored paper at random

461
00:44:38,720 --> 00:44:49,360
it it failed on two of them and i'll try another one so i um in that this way i i this was a serious

462
00:44:49,360 --> 00:44:55,440
question i wanted to know the answer uh so i have an iphone 11 and i thinking about buying an iphone

463
00:44:55,440 --> 00:45:03,280
14 so i asked it and it said sorry the iphone 14 is not yet released this was on 17th of march

464
00:45:03,280 --> 00:45:10,960
2023 and then it gave me all sorts of information that i didn't ask for about iphone 13 and so

465
00:45:10,960 --> 00:45:17,840
but two minutes later i tell it but the iphone 14 was released four months ago

466
00:45:18,960 --> 00:45:24,000
and so it says i apologize for the confusion you're right and so so that's not a good sign either

467
00:45:24,880 --> 00:45:33,760
now i i've done a lot of work i know a lot about barry smith and so i can ask you all

468
00:45:33,760 --> 00:45:38,640
such sorts of questions and work out the score of how often you get things right and it's it's

469
00:45:38,640 --> 00:45:46,000
less than 50 so here we have the question who wrote that which i wrote i wrote this phd thesis so i

470
00:45:46,000 --> 00:45:52,160
want the i want the answer barry smith so it gives me the answer kevin molligan who is a close friend

471
00:45:52,160 --> 00:45:59,120
we've written things together but he did not write my phd dissertation so i tell it to try again

472
00:45:59,840 --> 00:46:06,400
and then it says that my phd dissertation was written by a famous philosopher from the 1950s

473
00:46:06,400 --> 00:46:13,760
1960s which was when i was a boy a little boy uh so he didn't write it and so i tell it to try again

474
00:46:13,760 --> 00:46:19,120
it goes back to kevin molligan and i say are you sure yes i'm sure that kevin molligan wrote

475
00:46:19,680 --> 00:46:24,880
are you sure you're sure and then he apologizes again and he says that it was actually written

476
00:46:24,880 --> 00:46:32,960
by john michael croiss who i'd never heard of from that moment but it turns out that as

477
00:46:32,960 --> 00:46:37,600
chat gpt says he was a philosopher which is correct he was a professor of philosophy at the

478
00:46:37,600 --> 00:46:43,600
university of frankfurt which is incorrect he was born in 1943 which is correct in boston which is

479
00:46:43,600 --> 00:46:52,240
incorrect and so so he gets a little bit of truth about the non-author of my phd dissertation which

480
00:46:52,240 --> 00:47:00,000
is worth less than zero to me except that i can prove that there are things going on here that

481
00:47:00,000 --> 00:47:06,480
shouldn't be going on now let's try this one all the swiss people in this room will know that there

482
00:47:06,480 --> 00:47:16,000
is the orna loch and it's an old tongue basically an interesting old tongue so i asked it what is

483
00:47:16,000 --> 00:47:23,600
the orna loch and it said i'm sorry but i'm not sure which specific orna loch you are referring to

484
00:47:23,600 --> 00:47:30,880
as there may be different places or things with this name however one possible reference is to the

485
00:47:30,880 --> 00:47:39,280
orna lochschaft so it changed the subject it says it's in switzerland in a district in the canton

486
00:47:39,280 --> 00:47:46,480
of ori that encompasses the valleys of schekenthal and oeseren do those valleys exist anybody know

487
00:47:48,560 --> 00:47:55,040
the valleys exist good the name orna lochschaft literally means the district of the ori valley

488
00:47:55,760 --> 00:48:03,600
that is not true i'm assuming you can correct me here another reference is the orna loch cave

489
00:48:03,600 --> 00:48:10,240
in austria there is no such cave now if you could provide more information on the specific orna

490
00:48:10,240 --> 00:48:16,080
loch you are referring to i'd be happy to provide more information so i said could you provide me

491
00:48:16,080 --> 00:48:23,280
with more information about the orna lochschaft in switzerland which doesn't exist and it gave me

492
00:48:23,280 --> 00:48:30,960
two whole pages of tourism information other notable attractions include the historic town of

493
00:48:30,960 --> 00:48:36,880
aldorf and the aldermat i have no idea whether any of these things exist either but the orna loch

494
00:48:36,880 --> 00:48:44,240
shaft does not exist and you can check by asking google there is nothing there isn't a single entry

495
00:48:44,240 --> 00:48:49,600
which is a kind of miracle for any strings that you might give to google it can usually think of

496
00:48:49,600 --> 00:48:57,280
something but here there's nothing so is there an orna loch shaft i think no it made it up all

497
00:48:57,280 --> 00:49:05,200
right now there is a a very nice um slide deck by yang luqun who is one of the real experts in

498
00:49:07,360 --> 00:49:13,840
the sarcastic ai he's also one of the people who we cite in our book as also believing that

499
00:49:13,840 --> 00:49:18,880
there is a lot of nonsense being talked about the singularity machines taking over the world

500
00:49:20,080 --> 00:49:28,080
here he he gives a mathematical argument why these hallucinations they're called

501
00:49:28,080 --> 00:49:37,920
non-nonsense that uh genomes that the chat gpt throws up um the the reason is a mathematical one

502
00:49:38,480 --> 00:49:45,200
and it's so the mathematics we think is not quite right the formula needs to take account of

503
00:49:45,200 --> 00:49:51,600
length of input and length of output because the likelihood of error goes up for longer inputs

504
00:49:51,600 --> 00:49:58,320
and longer outputs which seems reasonable but this is a first step the probability of a of a

505
00:49:58,320 --> 00:50:07,040
chat gpt output being correct is one minus e raised to the power n and that means it's this red

506
00:50:07,040 --> 00:50:15,600
area here they are the correct answers and he thinks that this exponential divergence is not

507
00:50:15,600 --> 00:50:25,280
fixable so chat gpt is dead jack because if they can't fix this nonsense no one will trust chat gpt

508
00:50:25,280 --> 00:50:29,760
and it will be replaced by something quite different and no one knows what that is because the

509
00:50:29,760 --> 00:50:37,920
four large language models which is what chat gpt is the google one the the bing one uh i've

510
00:50:37,920 --> 00:50:42,800
forgotten the facebook one i guess they all use the same principles and they all have the same

511
00:50:42,800 --> 00:50:52,320
error code they all generate stuff uh that they make up all right now that's the misery of chat

512
00:50:52,320 --> 00:50:59,120
gpt and it should feel miserable now because i just declared it dead and i should really be investing

513
00:50:59,120 --> 00:51:06,320
i should be shorting stock which relies on chat gpt being alive in say six months but i'm not doing

514
00:51:06,320 --> 00:51:12,400
that all right so let's see how it works and why it is fantastic why it's a really a miracle which

515
00:51:12,400 --> 00:51:20,560
surprised me so i'm not pleased with it at all but it it did something which is important so

516
00:51:21,360 --> 00:51:27,360
how did we go the answer is through an ai method called reinforcement learning which is a method

517
00:51:27,360 --> 00:51:34,560
which works well for games like go and the way it works is that for a game like you know you can

518
00:51:34,560 --> 00:51:40,880
go you can define a reward system for each move and it can be a reward system which

519
00:51:40,880 --> 00:51:48,560
whether rewards can be assigned by the computer now if you can do that you can play the game

520
00:51:51,440 --> 00:51:56,320
over and over again billions of times inside the computer you don't need human beings so

521
00:51:56,320 --> 00:52:02,560
they're still trying to crack the game of dota 2 which is apparently a leading esport game i've

522
00:52:02,560 --> 00:52:09,040
no idea what esport means but dota 2 exists they still haven't cracked it but they're trying to

523
00:52:09,040 --> 00:52:17,120
crack it with a software algorithm called open ai 5 which usually wins against humans and this can

524
00:52:17,120 --> 00:52:25,920
play 180 years worth of dota 2 games in a single day if you can do that you can do you can perform

525
00:52:26,000 --> 00:52:37,040
miracles in principles such as beating dota 2 so can you do it for conversation three months ago i

526
00:52:37,040 --> 00:52:44,800
would have said no impossible and i just said it 10 minutes ago chat gpt showed how you can apply

527
00:52:44,800 --> 00:52:52,800
reinforcement learning to what looks like conversations now how did it do that so what that

528
00:52:52,800 --> 00:52:59,920
means is that we are doing a little bit like emulating human will because the alpha go has to

529
00:52:59,920 --> 00:53:05,600
want to win the game of go in some sense of want it has to emulate the kind of want that you have

530
00:53:05,600 --> 00:53:13,840
when you play a game and want to win so how does it work well you need a reward system and i i put

531
00:53:13,840 --> 00:53:20,160
this in that's what i used to believe i still believe it really but um chat gp has unsettled my

532
00:53:20,160 --> 00:53:27,920
conviction so chat gp found a chat gpt found a way to use reinforcement learning to emulate two

533
00:53:27,920 --> 00:53:34,000
persons human conversation inside a computer and it says to here this is a big deal and i mean that

534
00:53:34,000 --> 00:53:43,360
in a positive way now how does chat gpt work you give it any string and it will work out from its

535
00:53:43,360 --> 00:53:50,560
really powerful knowledge of language what the next lightliest next syllable is

536
00:53:51,440 --> 00:53:56,960
that's why it sometimes takes time when it's chatting as it were so if you say the best thing

537
00:53:56,960 --> 00:54:02,320
about ai is its ability to then it will say learn because that's the next most probable

538
00:54:04,400 --> 00:54:09,600
output and now it doesn't always take the most probable because if it did it would go around

539
00:54:09,680 --> 00:54:18,000
circles so sometimes it has a random kick down the hierarchy all right now notice that it doesn't

540
00:54:18,000 --> 00:54:22,960
understand anything it just has an incredibly powerful knowledge of the patterns of language

541
00:54:22,960 --> 00:54:28,960
which enables it to know what the next syllable will be will be most likely after any given string

542
00:54:30,800 --> 00:54:36,720
and so this is how it was built so the first part is creating this wonderful

543
00:54:37,440 --> 00:54:43,200
patterned model of language not just english but other languages too and i won't talk about that

544
00:54:43,200 --> 00:54:49,360
that's that that's the same kind of training that you find in google translate and and there are

545
00:54:49,360 --> 00:54:55,360
then three steps which i'll go through one by one so we have prompts and these come from being

546
00:54:56,560 --> 00:55:00,320
so they're being questions we don't know what they are and that's a little bit fishy

547
00:55:01,040 --> 00:55:06,720
so some people would like to see the prompts because chat gpt4 is claiming that it can beat

548
00:55:06,720 --> 00:55:12,720
humans in medical exams and some people think that the answers to the medical exam questions were in

549
00:55:12,720 --> 00:55:20,400
the prompt database that was used to train chat gpt in which case the being able to beat humans

550
00:55:20,480 --> 00:55:30,560
would be worth nothing um so and then you have people i some people say it was 40 contractors

551
00:55:30,560 --> 00:55:34,640
but again that's a secret it may have been many more they were all in india so they didn't cost

552
00:55:34,640 --> 00:55:41,120
as much as if they've been in theory for instance and they were hired to write responses to those

553
00:55:41,120 --> 00:55:52,240
prompts now so the when was michael jackson born that kind of prompt and and many other prompts but

554
00:55:52,240 --> 00:55:57,280
only a limited number and we don't know what they are but some people say 13 000 prompts

555
00:55:58,640 --> 00:56:05,120
so that's the first step now the second step so you've you've you've got the prompts from being

556
00:56:05,120 --> 00:56:10,080
and you've got the outputs from the people in india who are paid to respond to these prompts

557
00:56:10,160 --> 00:56:19,840
producing text then the next stage is that you pay a labeler it's called but it means an evaluator

558
00:56:20,560 --> 00:56:29,600
to rank the outputs created in the first stage now you can't rank outputs when you're talking to your

559
00:56:29,600 --> 00:56:37,280
spouse in a conversation but you and that's why we can't create a reward system for conversation

560
00:56:37,280 --> 00:56:45,280
because you can't rank outputs in conversations but the chat gpt found a way to rank outputs by

561
00:56:45,280 --> 00:56:52,720
paying somebody to label them with a score of one two three or four points so four points means it's

562
00:56:52,720 --> 00:57:01,920
a good output and one point means it's a bad output and i believe that the reason why chat gpt very

563
00:57:01,920 --> 00:57:09,600
often says honestly i don't know the answer to your question so it didn't say i have no idea what

564
00:57:09,600 --> 00:57:16,000
the orna lock is it told me to think about the orna lock shaft which has all kinds of rivers running

565
00:57:16,000 --> 00:57:21,920
through and doesn't exist why did it go to that trouble of giving me all this tourist information

566
00:57:21,920 --> 00:57:31,040
because the labelers poor things think that a response who says i don't know what you want

567
00:57:32,080 --> 00:57:39,680
i'm sorry is less reward worthy than a long list of tourism information about a non-existing

568
00:57:39,680 --> 00:57:44,000
village because it's a long list of tourist information that must be worth four points

569
00:57:44,000 --> 00:57:53,200
and so chat gpt basically is being bribed by the labelers to reward long outputs which are

570
00:57:54,000 --> 00:58:00,320
kind and gentle and so this is why it throws up so much rubbish the machine will always try to

571
00:58:00,320 --> 00:58:05,280
have nice friendly output but the machine can play response prompt response prompt response

572
00:58:05,280 --> 00:58:10,160
prompt response games with itself billions of times every day for weeks and that that is what

573
00:58:10,160 --> 00:58:15,760
it did and it cost a lot of money to do all that training and then it can give answers that's how

574
00:58:15,760 --> 00:58:18,720
it does it and so the lesson

