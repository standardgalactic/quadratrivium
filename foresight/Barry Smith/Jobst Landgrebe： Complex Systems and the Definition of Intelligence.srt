1
00:00:00,000 --> 00:00:02,960
Hello, everyone, and thanks for participating.

2
00:00:02,960 --> 00:00:04,920
My name is Jobs Danke, but I'm by training

3
00:00:04,920 --> 00:00:07,480
physician and mathematician, but I also

4
00:00:07,480 --> 00:00:10,040
study philosophy, I've come back to do

5
00:00:10,040 --> 00:00:12,520
philosophical research work as well.

6
00:00:12,520 --> 00:00:15,680
So today I'm going to talk about artificial intelligence,

7
00:00:15,680 --> 00:00:18,000
intelligent pseudo definitions.

8
00:00:18,000 --> 00:00:20,360
But before doing this, I need to introduce

9
00:00:20,360 --> 00:00:23,360
our view of complex systems, because we

10
00:00:23,360 --> 00:00:26,120
will need this later on in the talk.

11
00:00:26,120 --> 00:00:29,160
So I've been starting with two slides about complex systems,

12
00:00:29,160 --> 00:00:32,800
and then we'll move to the definitions of our intelligence.

13
00:00:32,800 --> 00:00:36,120
So what is a complex system?

14
00:00:36,120 --> 00:00:39,280
So maybe why do we need to understand a complex system?

15
00:00:39,280 --> 00:00:42,320
Because the animal and the human mind body

16
00:00:42,320 --> 00:00:48,560
continue, which produce intelligence, are complex systems.

17
00:00:48,560 --> 00:00:53,080
So even primal intelligence, which

18
00:00:53,080 --> 00:00:57,480
is the intelligence of a bird or a mammalian animal,

19
00:00:57,480 --> 00:01:00,480
the non-human, has primal intelligence

20
00:01:00,480 --> 00:01:03,320
that gets produced by a complex system.

21
00:01:03,320 --> 00:01:05,720
So what is a complex system?

22
00:01:05,720 --> 00:01:09,080
So let's start with the Newtonian system.

23
00:01:09,080 --> 00:01:13,240
So Newtonian systems are systems in which one

24
00:01:13,240 --> 00:01:16,240
can apply models of physics.

25
00:01:16,240 --> 00:01:21,200
And they have become extended quite a bit in the 19th century

26
00:01:21,200 --> 00:01:24,560
with thermodynamics and statistical mechanics.

27
00:01:24,560 --> 00:01:29,480
And then another bit by quantum theory and also

28
00:01:29,480 --> 00:01:31,560
general theory of relativity.

29
00:01:31,560 --> 00:01:33,800
But they still remain Newtonian systems.

30
00:01:33,800 --> 00:01:36,480
So what is a Newtonian system?

31
00:01:36,480 --> 00:01:38,200
But by the way, can you hear me all right?

32
00:01:38,200 --> 00:01:40,400
Does it work?

33
00:01:40,400 --> 00:01:42,960
OK, I assume yes.

34
00:01:42,960 --> 00:01:47,880
So Newtonian systems are made up by a small set of element

35
00:01:47,880 --> 00:01:48,880
types.

36
00:01:48,880 --> 00:01:51,320
For example, the solar system is made up

37
00:01:51,320 --> 00:01:56,200
by the sun and the planets, which are not many elements.

38
00:01:56,200 --> 00:02:00,800
The way the elements interact, they

39
00:02:00,800 --> 00:02:05,920
interact by the four basic forces or interaction types

40
00:02:05,920 --> 00:02:08,040
that are known in physics.

41
00:02:08,040 --> 00:02:12,120
And in this case of the solar system, it's gravitation.

42
00:02:12,120 --> 00:02:14,640
And actually, only gravitation.

43
00:02:14,640 --> 00:02:17,960
All the other forces don't matter for the solar system.

44
00:02:17,960 --> 00:02:19,680
At least not with regard to the way

45
00:02:19,680 --> 00:02:21,920
the planets move around the sun.

46
00:02:21,920 --> 00:02:26,320
And they interact in a uniform and isotropic way.

47
00:02:26,320 --> 00:02:35,440
So the force that is interacting here, the gravitation,

48
00:02:35,440 --> 00:02:41,200
has its effect in a symmetric way all around the sun.

49
00:02:41,200 --> 00:02:46,160
And it is the same everywhere.

50
00:02:46,160 --> 00:02:48,880
I mean, it gets weaker and weaker.

51
00:02:48,920 --> 00:02:52,920
But in a law-like fashion.

52
00:02:52,920 --> 00:02:54,680
Also, there is no force overlay.

53
00:02:54,680 --> 00:02:57,160
So there are other forces, like electromagnetic forces.

54
00:02:57,160 --> 00:03:00,440
There's, for example, light that comes out of the sun.

55
00:03:00,440 --> 00:03:04,720
But it doesn't interact significantly with gravitation.

56
00:03:04,720 --> 00:03:07,480
So if I model the way the planet moves around the sun,

57
00:03:07,480 --> 00:03:11,840
I don't need to take into account other forces than gravitation.

58
00:03:11,840 --> 00:03:19,640
The phase space in which the elements are placed or occur

59
00:03:19,640 --> 00:03:23,080
is deterministic and ergodic.

60
00:03:23,080 --> 00:03:26,480
So ergodicity is shown in a small inset here.

61
00:03:26,480 --> 00:03:34,680
So an ergodic phase space means that all accessible micro space

62
00:03:34,680 --> 00:03:39,640
states of the space are actually probable over a long time.

63
00:03:39,640 --> 00:03:45,640
So basically, in simple words, I can get to everywhere in this space

64
00:03:45,640 --> 00:03:50,360
with the same probability if I wait long enough.

65
00:03:50,360 --> 00:03:53,200
For example, if I have gas in a bottle,

66
00:03:53,200 --> 00:03:56,920
the molecules of the gas will distribute

67
00:03:56,920 --> 00:04:00,800
equi-probably over the volume of the bottle.

68
00:04:00,800 --> 00:04:05,680
And so it has to be such a phase space is ergodic,

69
00:04:05,680 --> 00:04:08,120
whereas there are also non-ergodic spaces to which

70
00:04:08,120 --> 00:04:11,000
we'll get back in a minute.

71
00:04:11,000 --> 00:04:13,800
Such Newtonian systems are non-driven.

72
00:04:13,800 --> 00:04:23,000
Drivenness means that there is no force flowing through the system.

73
00:04:23,000 --> 00:04:31,680
So for example, a steam engine has a driven aspect.

74
00:04:31,680 --> 00:04:36,480
Because all the time while it's driving or under energy,

75
00:04:36,480 --> 00:04:39,680
all the time energy is entering into the steam engine

76
00:04:39,680 --> 00:04:41,320
by the burning of the coals.

77
00:04:41,320 --> 00:04:43,200
And then it's been dissipated.

78
00:04:43,200 --> 00:04:47,160
And in Newtonian systems, that's not the case.

79
00:04:47,160 --> 00:04:49,880
Such systems have no evolution properties.

80
00:04:49,880 --> 00:04:52,840
So they don't obtain new element types.

81
00:04:52,840 --> 00:04:54,800
They have the element types they have.

82
00:04:54,800 --> 00:04:58,600
So yes, this solar system could get a new planet

83
00:04:58,600 --> 00:05:02,080
because there could be a big asteroid, could approach the sun,

84
00:05:02,080 --> 00:05:04,200
and could be attracted by the sun,

85
00:05:04,200 --> 00:05:10,920
and then start to orbit around the sun in the way the planets do.

86
00:05:10,920 --> 00:05:12,720
But that wouldn't be a new element type.

87
00:05:12,720 --> 00:05:15,160
It would just be a new element.

88
00:05:15,160 --> 00:05:17,080
And they have fixed boundary conditions.

89
00:05:17,080 --> 00:05:21,840
That is, if the solar system is four light-years away

90
00:05:21,840 --> 00:05:25,400
from the next solar system, which is alpha centauri,

91
00:05:25,400 --> 00:05:28,520
now if it would just be displaced by one or two light-years,

92
00:05:28,520 --> 00:05:31,960
or even three light-years, this wouldn't change anything.

93
00:05:31,960 --> 00:05:35,560
So basically, I can take the solar system out of its context

94
00:05:35,560 --> 00:05:38,800
and move it away, and it wouldn't change anything.

95
00:05:38,800 --> 00:05:42,840
Of course, if I would move it very, very close to alpha centauri,

96
00:05:42,840 --> 00:05:45,200
then the sun of alpha centauri and our sun

97
00:05:45,200 --> 00:05:47,520
would start to interact by gravitation,

98
00:05:47,520 --> 00:05:50,880
and then very terrible events could happen.

99
00:05:50,880 --> 00:05:55,320
But basically, Ceteris paribus,

100
00:05:55,320 --> 00:05:58,800
I can just take such a Newtonian system out of its context.

101
00:05:58,800 --> 00:06:01,400
Now, complex systems are completely different.

102
00:06:01,440 --> 00:06:04,800
They depend on multiple arbitrary element types.

103
00:06:04,800 --> 00:06:08,520
They have different interaction types between elements.

104
00:06:08,520 --> 00:06:09,640
They have force overlay.

105
00:06:09,640 --> 00:06:12,720
So that means that several forces act at the same time

106
00:06:12,720 --> 00:06:14,680
and also interact.

107
00:06:14,680 --> 00:06:19,200
The phase spaces that they have cannot be predicted

108
00:06:19,200 --> 00:06:22,120
from their system elements, and they are non-ergodic.

109
00:06:22,120 --> 00:06:26,440
So they behave in a way that the microstates

110
00:06:26,440 --> 00:06:29,840
are not accessed over a long time with the same probability.

111
00:06:29,840 --> 00:06:31,240
They're also driven.

112
00:06:31,240 --> 00:06:33,680
So they have inner or external drive.

113
00:06:33,680 --> 00:06:35,840
External drive is, for example, the steam engine

114
00:06:35,840 --> 00:06:37,920
that gets heated from coal.

115
00:06:37,920 --> 00:06:41,040
Internal drive is what humans or bacteria have.

116
00:06:41,040 --> 00:06:45,240
This is the drive to reproduce and also to survive.

117
00:06:45,240 --> 00:06:48,240
And drivenness means that there's a flow of energy

118
00:06:48,240 --> 00:06:50,320
flowing through the system all the time

119
00:06:50,320 --> 00:06:53,080
and that this energy is dissipating.

120
00:06:53,080 --> 00:06:55,160
And they lack an equilibrium state

121
00:06:55,160 --> 00:06:57,080
to which they would constantly be converging.

122
00:06:57,080 --> 00:07:00,320
So a driven system doesn't come to equilibrium.

123
00:07:00,320 --> 00:07:02,800
It's always it goes on.

124
00:07:02,800 --> 00:07:05,400
But when the system, when an organism dies,

125
00:07:05,400 --> 00:07:08,120
then it stops being driven and then it also

126
00:07:08,120 --> 00:07:11,400
converges towards an equilibrium state, which

127
00:07:11,400 --> 00:07:14,040
is, in this case, entropy.

128
00:07:14,040 --> 00:07:16,880
Also, complex systems have evolutionary properties.

129
00:07:16,880 --> 00:07:19,760
So they can evolve new element types.

130
00:07:19,760 --> 00:07:22,160
And they have non-fixable boundary conditions.

131
00:07:22,160 --> 00:07:24,760
So they are context dependent.

132
00:07:24,760 --> 00:07:29,200
You can read this comparison of complex and classically

133
00:07:29,200 --> 00:07:31,720
returning systems in turn at a very good book

134
00:07:31,720 --> 00:07:34,920
about complex systems.

135
00:07:34,920 --> 00:07:36,680
So let's look at some examples.

136
00:07:36,680 --> 00:07:40,040
We have those seven properties of complex systems.

137
00:07:40,040 --> 00:07:43,440
And the solar system has none of these properties

138
00:07:43,440 --> 00:07:45,680
because it's not a complex system.

139
00:07:45,680 --> 00:07:48,480
The steam engine has one property, it is driven.

140
00:07:48,480 --> 00:07:52,160
However, to reason about the steam engine,

141
00:07:52,160 --> 00:07:55,200
in many ways, you can abstract from its drivenness.

142
00:07:55,200 --> 00:07:59,080
So for example, the velocity of the steam engine,

143
00:07:59,080 --> 00:08:05,200
if it's used to drive a train, is

144
00:08:05,200 --> 00:08:08,240
proportional to the pressure that it builds up and so on.

145
00:08:08,240 --> 00:08:10,920
So this property, if it's the only driven property,

146
00:08:10,920 --> 00:08:12,920
you can abstract from it for many predictions

147
00:08:12,920 --> 00:08:15,440
you want to make about the behavior.

148
00:08:15,440 --> 00:08:22,760
Pryon is protein that can infect the brain

149
00:08:22,760 --> 00:08:24,600
and cause damage in the brain.

150
00:08:24,600 --> 00:08:27,480
You have heard of Jacob Kreuzfeld disease.

151
00:08:27,480 --> 00:08:32,480
And you may also heard of bovine spongiform encephalopathy,

152
00:08:32,480 --> 00:08:34,600
which is also a prior disease.

153
00:08:34,600 --> 00:08:39,360
And it has only two complex system properties, namely

154
00:08:39,360 --> 00:08:43,920
force overlay and a non-negotic phase space.

155
00:08:43,920 --> 00:08:47,080
And it has also a non-fixed boundary conditions,

156
00:08:47,080 --> 00:08:48,760
but it lacks all the others.

157
00:08:48,760 --> 00:08:52,440
But then as soon as I get to the virus,

158
00:08:52,440 --> 00:08:55,000
I almost have all the properties.

159
00:08:55,000 --> 00:08:58,120
Viruses, although not driven, because it cannot synthesize

160
00:08:58,120 --> 00:08:59,040
energy.

161
00:08:59,040 --> 00:09:01,440
And then with the most primitive organism,

162
00:09:01,440 --> 00:09:04,040
I have all the properties of a complex system.

163
00:09:04,040 --> 00:09:10,000
So it is important to realize that most systems in nature

164
00:09:10,000 --> 00:09:13,640
are complex or deterministically chaotic.

165
00:09:13,640 --> 00:09:18,240
And so basically, there's only a very little Newtonian systems

166
00:09:18,240 --> 00:09:19,440
out there.

167
00:09:19,440 --> 00:09:22,880
And most of the Newtonian systems that we master

168
00:09:23,440 --> 00:09:25,840
are technically devices that we have designed out there.

169
00:09:25,840 --> 00:09:27,080
All of them are Newtonian.

170
00:09:27,080 --> 00:09:29,600
All of them are built using equations

171
00:09:29,600 --> 00:09:31,400
that we have designed ourselves.

172
00:09:31,400 --> 00:09:33,360
And that's what we really understand and master.

173
00:09:33,360 --> 00:09:36,920
But nature is chaotic and complex.

174
00:09:36,920 --> 00:09:39,520
And humans react irrationally to it often.

175
00:09:39,520 --> 00:09:43,560
So you can, if you think of how, for example, we are now

176
00:09:43,560 --> 00:09:47,000
reacting to this virus.

177
00:09:47,000 --> 00:09:50,840
What it's causing is complex, but the reaction is irrational.

178
00:09:50,840 --> 00:09:54,640
And that's because we feel that we cannot control it.

179
00:09:54,640 --> 00:09:56,840
On the next page, you can see.

180
00:09:56,840 --> 00:09:59,280
So I will give you the opportunity

181
00:09:59,280 --> 00:10:01,440
to ask questions after this slide.

182
00:10:01,440 --> 00:10:04,880
So on the next slide, we see now the problem

183
00:10:04,880 --> 00:10:08,240
that complex systems pose to machine learning algorithms.

184
00:10:08,240 --> 00:10:10,120
So basically, machine learning algorithms

185
00:10:10,120 --> 00:10:14,640
cannot model complex systems because such algorithms

186
00:10:14,640 --> 00:10:18,200
are large auto-parameterized differential equations,

187
00:10:18,200 --> 00:10:20,400
partially auto-parameterized.

188
00:10:20,400 --> 00:10:23,200
And let's look at the problems of machine learning models.

189
00:10:23,200 --> 00:10:25,160
So on the left-hand side, you see the problems

190
00:10:25,160 --> 00:10:26,720
that everybody know.

191
00:10:26,720 --> 00:10:29,440
So that they optimize problem-specific loss

192
00:10:29,440 --> 00:10:32,000
functions that don't generalize well,

193
00:10:32,000 --> 00:10:35,280
that they narrowly depend on the selected training samples

194
00:10:35,280 --> 00:10:38,240
and the specific annotations of these samples,

195
00:10:38,240 --> 00:10:40,360
that they fail upon heterogeneous annotation

196
00:10:40,360 --> 00:10:41,400
of identical input.

197
00:10:41,400 --> 00:10:44,680
So identical input by different output,

198
00:10:44,680 --> 00:10:46,480
it will be very stressful, so to speak.

199
00:10:46,480 --> 00:10:49,520
I mean, it will not train well.

200
00:10:49,560 --> 00:10:53,200
They fail on sparsely-populated sample space parts,

201
00:10:53,200 --> 00:10:56,520
which is very often a very big problem

202
00:10:56,520 --> 00:11:00,600
and that explains why very often machine learning algorithms

203
00:11:00,600 --> 00:11:01,800
need so many samples.

204
00:11:01,800 --> 00:11:04,920
Because if you look at the Curse of Dimensionality,

205
00:11:04,920 --> 00:11:08,160
which you can, for example, read in Trevor Haste's

206
00:11:08,160 --> 00:11:11,480
wonderful book about statistical learning,

207
00:11:11,480 --> 00:11:13,600
you will see in chapter one or chapter two

208
00:11:13,600 --> 00:11:15,440
where he explains the Curse of Dimensionality

209
00:11:15,440 --> 00:11:18,920
that very quickly, you come from a topological perspective

210
00:11:18,960 --> 00:11:22,480
to a very sparsely-populated sample space areas.

211
00:11:22,480 --> 00:11:25,680
And in such areas, the machine doesn't learn anything.

212
00:11:25,680 --> 00:11:27,400
And that's very interesting because humans

213
00:11:27,400 --> 00:11:31,120
and also animals are very good at applying

214
00:11:31,120 --> 00:11:34,120
the intelligence to sparsely-populated sample space.

215
00:11:34,120 --> 00:11:36,600
It's basically, that's the key of intelligence.

216
00:11:36,600 --> 00:11:40,080
Intelligence means the ability to react to new situations.

217
00:11:40,080 --> 00:11:42,840
And machine learning models completely fail in new situations,

218
00:11:42,840 --> 00:11:45,920
which are such sparsely-populated sample space.

219
00:11:45,920 --> 00:11:47,920
They cannot be guaranteed to move

220
00:11:47,920 --> 00:11:49,800
to corrected outcomes.

221
00:11:49,800 --> 00:11:52,640
So if you have an error, erroneous behavior

222
00:11:52,640 --> 00:11:56,800
of such a system, and you try to correct the system

223
00:11:56,800 --> 00:11:59,760
once you've noted the error, it's very hard to guarantee

224
00:11:59,760 --> 00:12:01,480
that it moves to the corrected outcome.

225
00:12:01,480 --> 00:12:04,640
And while it's doing this, it may start to make new mistakes

226
00:12:04,640 --> 00:12:07,320
because remember that the model itself is nothing

227
00:12:07,320 --> 00:12:10,680
but a hyperplane in a k-dimensional space.

228
00:12:10,680 --> 00:12:14,720
And this hyperplane, of course, I mean,

229
00:12:15,400 --> 00:12:20,200
if you change its shape to get a certain effect,

230
00:12:20,200 --> 00:12:22,360
you may get other effects you don't want to.

231
00:12:23,400 --> 00:12:26,280
Such models cannot perceive their own failure, of course.

232
00:12:27,280 --> 00:12:29,240
So they cannot really raise exceptions.

233
00:12:29,240 --> 00:12:30,400
Well, they can raise an exception

234
00:12:30,400 --> 00:12:35,080
if the data don't match the input type, but not much more.

235
00:12:35,080 --> 00:12:37,880
They cannot model far-reaching relationships.

236
00:12:37,880 --> 00:12:40,360
They cannot model semantics of mental type,

237
00:12:40,360 --> 00:12:41,880
objectification semantics.

238
00:12:41,880 --> 00:12:43,640
Here I'm using the German word, I apologize.

239
00:12:43,640 --> 00:12:48,000
So it's object type, mental types they cannot use.

240
00:12:48,000 --> 00:12:49,360
And that's why they fail at image

241
00:12:49,360 --> 00:12:52,480
or language interpretation, they fail completely.

242
00:12:52,480 --> 00:12:57,320
And they, of course, have not a sufficient exactness

243
00:12:57,320 --> 00:12:58,760
for really critical settings.

244
00:13:00,360 --> 00:13:02,760
That's the general problems that have been known

245
00:13:02,760 --> 00:13:05,840
for machine learning, even before neural networks

246
00:13:05,840 --> 00:13:08,720
were invented already, I mean,

247
00:13:08,720 --> 00:13:10,760
before the deep neural networks were invented.

248
00:13:10,760 --> 00:13:14,120
Even in the 1970s, when there was just logistic regression

249
00:13:14,120 --> 00:13:17,240
and perceptron approach, it was already clear

250
00:13:17,240 --> 00:13:21,160
to every statistician that these are the problems

251
00:13:21,160 --> 00:13:22,400
of such models.

252
00:13:22,400 --> 00:13:24,840
On the other hand, there are also problems

253
00:13:24,840 --> 00:13:28,200
that are less well-known, that are described in our book.

254
00:13:28,200 --> 00:13:30,920
There will be no singularity that we expect to publish this year.

255
00:13:30,920 --> 00:13:34,520
It's with the, currently with the publisher for review,

256
00:13:34,520 --> 00:13:38,680
which are related to the fact that human behavior

257
00:13:38,720 --> 00:13:42,840
is emanating from a complex system, the human.

258
00:13:42,840 --> 00:13:46,400
And so if you look at what the problem is here,

259
00:13:46,400 --> 00:13:51,400
first of all, machine learning models require ergodicity

260
00:13:52,480 --> 00:13:55,480
because the samples must be assumed to be drawn

261
00:13:55,480 --> 00:13:57,360
from a representative distribution.

262
00:13:57,360 --> 00:14:00,960
So if you want to train a complex system,

263
00:14:00,960 --> 00:14:03,200
you need to have a representative distribution

264
00:14:03,200 --> 00:14:05,880
because next time you draw a sample,

265
00:14:05,880 --> 00:14:07,480
if it doesn't come from the distribution

266
00:14:07,480 --> 00:14:09,520
which you used to train, the model will fail

267
00:14:09,520 --> 00:14:11,560
because of reason number four.

268
00:14:11,560 --> 00:14:14,920
However, reason number one on the right-hand side

269
00:14:14,920 --> 00:14:18,680
gives you states that complex systems

270
00:14:18,680 --> 00:14:21,200
create non-ergodic distributions.

271
00:14:21,200 --> 00:14:23,880
And so when you draw from, when you make a photo,

272
00:14:23,880 --> 00:14:25,600
so to speak, from a non-complex,

273
00:14:25,600 --> 00:14:29,080
from complex system behavior-derived situation,

274
00:14:30,000 --> 00:14:33,560
this photograph, apparently in quote marks,

275
00:14:33,560 --> 00:14:35,480
is basically always skewed.

276
00:14:35,480 --> 00:14:36,640
That's the most important thing

277
00:14:36,640 --> 00:14:38,360
you have to learn today.

278
00:14:38,360 --> 00:14:40,760
So when you record human behavior,

279
00:14:40,760 --> 00:14:43,200
this behavior is never representative.

280
00:14:43,200 --> 00:14:44,880
Because, so in other words,

281
00:14:44,880 --> 00:14:47,280
there is no multivariate distribution to draw from

282
00:14:47,280 --> 00:14:49,320
because each situation is new.

283
00:14:49,320 --> 00:14:52,120
And the samples that you can draw are always outdated.

284
00:14:52,120 --> 00:14:56,000
So what you do is you train with this huge snap

285
00:14:56,000 --> 00:14:58,040
when you sample from a complex system.

286
00:14:58,040 --> 00:15:00,080
So the sample space is always sparse.

287
00:15:00,080 --> 00:15:03,280
I priori, that's really bad for complex systems

288
00:15:03,280 --> 00:15:05,960
and that's bad for ML algorithms.

289
00:15:05,960 --> 00:15:06,800
They cannot deal with this.

290
00:15:06,800 --> 00:15:08,240
They need repetitive pattern.

291
00:15:09,280 --> 00:15:11,960
Of course, then this is really the take-home message

292
00:15:11,960 --> 00:15:15,080
number one, that if you have a situation

293
00:15:15,080 --> 00:15:18,000
or an environment created by a complex system,

294
00:15:18,000 --> 00:15:22,080
then this is always a non-ergodic distribution

295
00:15:22,080 --> 00:15:24,520
deriving from a non-ergodic process.

296
00:15:24,520 --> 00:15:27,080
And such a distribution can never be represented.

297
00:15:28,280 --> 00:15:31,560
So also they don't have evolutionary property.

298
00:15:31,560 --> 00:15:33,120
They cannot model evolutionary properties

299
00:15:33,120 --> 00:15:36,720
because they model a fixed input-output relationship.

300
00:15:36,720 --> 00:15:38,720
But when you have evolutionary properties,

301
00:15:38,720 --> 00:15:41,200
input-output relationships change all the time.

302
00:15:42,880 --> 00:15:45,760
Because they are differentiable models,

303
00:15:45,760 --> 00:15:49,640
they cannot model non-continuous operators or functionals.

304
00:15:49,640 --> 00:15:50,880
And also they can, of course,

305
00:15:50,880 --> 00:15:52,840
not model non-isotropic forces.

306
00:15:54,760 --> 00:15:58,360
They cannot model elements specific interaction types.

307
00:15:58,360 --> 00:16:01,280
And also force overlay or interaction

308
00:16:01,280 --> 00:16:03,040
is very hard to model.

309
00:16:03,040 --> 00:16:07,680
So for them,

310
00:16:07,680 --> 00:16:09,760
and also they cannot model drivenness,

311
00:16:09,760 --> 00:16:11,960
which is energy dissipation.

312
00:16:11,960 --> 00:16:15,320
To give you a very simple example why this doesn't work,

313
00:16:15,320 --> 00:16:20,320
we haven't found any way to model the flow of water

314
00:16:20,440 --> 00:16:24,240
into, for example, water reservoirs.

315
00:16:24,240 --> 00:16:25,840
So turbulence.

316
00:16:25,840 --> 00:16:28,400
Turbulence is one of the most simplest

317
00:16:30,600 --> 00:16:32,400
natural phenomena of drivenness

318
00:16:32,400 --> 00:16:35,160
because the energy from the water flow dissipates

319
00:16:35,160 --> 00:16:38,600
in the reservoir to which the water is flowing,

320
00:16:38,600 --> 00:16:40,280
but we cannot model it.

321
00:16:40,280 --> 00:16:42,720
And so we cannot model how the energy

322
00:16:42,720 --> 00:16:45,480
that is the kinetic energy of the water distributes

323
00:16:46,440 --> 00:16:49,080
into the reservoir.

324
00:16:50,160 --> 00:16:55,160
And also complex systems cannot model contextuality,

325
00:16:55,320 --> 00:16:58,320
sorry, context-free machine learning models,

326
00:16:58,320 --> 00:17:00,640
cannot model contextuality.

327
00:17:00,640 --> 00:17:02,600
Because complex systems are always contextual,

328
00:17:02,600 --> 00:17:05,760
but machine learning models are always context-free.

329
00:17:05,760 --> 00:17:09,040
And so you have a discrepancy between the context-freeness

330
00:17:09,040 --> 00:17:10,080
of the machine learning model

331
00:17:10,080 --> 00:17:12,520
and the contextuality of the complex system.

332
00:17:12,520 --> 00:17:13,800
So this is a very,

333
00:17:13,800 --> 00:17:16,600
I don't know how long I spoke, maybe 15 minutes,

334
00:17:16,600 --> 00:17:19,120
in very short words.

335
00:17:19,120 --> 00:17:24,120
That's what complex systems,

336
00:17:24,120 --> 00:17:28,360
the problem is complex systems modeling in machine learning.

337
00:17:31,480 --> 00:17:33,320
Hello, are you still there?

338
00:17:33,320 --> 00:17:34,640
I'm still here.

339
00:17:34,640 --> 00:17:38,640
So now the students are required to challenge you.

340
00:17:42,040 --> 00:17:44,080
I have a question.

341
00:17:44,080 --> 00:17:44,920
Yes.

342
00:17:46,480 --> 00:17:51,480
So the issues raised here were the problems

343
00:17:52,320 --> 00:17:56,120
of modeling complex systems in the context

344
00:17:56,120 --> 00:17:58,080
of neural networks and deep learning.

345
00:17:58,520 --> 00:17:59,360
Machine learning.

346
00:17:59,360 --> 00:18:01,680
Machine learning, all right.

347
00:18:01,680 --> 00:18:04,760
But the thing is humans also haven't been able

348
00:18:04,760 --> 00:18:06,560
to model turbulence.

349
00:18:06,560 --> 00:18:08,840
And humans haven't been able to,

350
00:18:08,840 --> 00:18:11,440
so to an extent we've been able to model

351
00:18:11,440 --> 00:18:12,800
weather forecast systems.

352
00:18:12,800 --> 00:18:15,320
So we use certain sets of differential equations there.

353
00:18:17,320 --> 00:18:21,320
But if ML systems can, to a certain extent,

354
00:18:21,320 --> 00:18:23,120
as was mentioned in the slide,

355
00:18:23,120 --> 00:18:25,600
auto-parameterized differential equations model

356
00:18:26,600 --> 00:18:30,600
some extent of that, then why should it be considered

357
00:18:30,600 --> 00:18:33,600
a sign of no intelligence,

358
00:18:33,600 --> 00:18:35,600
even though humans have also not been able to do

359
00:18:35,600 --> 00:18:36,600
the same thing?

360
00:18:36,600 --> 00:18:41,600
So why this requirement from machine learning?

361
00:18:44,600 --> 00:18:48,600
So I'm not saying that it is a sign of intelligence.

362
00:18:48,600 --> 00:18:51,600
So now we are not talking about intelligence right now.

363
00:18:51,600 --> 00:18:54,600
We are only talking about what can be modeled with machine.

364
00:18:55,600 --> 00:18:58,600
And what I have not said is I've skipped

365
00:18:58,600 --> 00:18:59,600
a little bit of content.

366
00:18:59,600 --> 00:19:03,600
So what I've not said is that all the phenomena

367
00:19:03,600 --> 00:19:07,600
that are complex in nature can't be modeled using mathematics.

368
00:19:07,600 --> 00:19:10,600
So of course we cannot model complex systems mathematics.

369
00:19:10,600 --> 00:19:13,600
And so, but that's not because we are not intelligent,

370
00:19:13,600 --> 00:19:17,600
but because complex system modeling seems to be

371
00:19:17,600 --> 00:19:21,600
beyond the hardware that human intelligence possesses.

372
00:19:21,600 --> 00:19:23,600
So but that doesn't mean that humans are not intelligent.

373
00:19:23,600 --> 00:19:25,600
It just means that we can't model.

374
00:19:25,600 --> 00:19:27,600
There's no way to model complex systems,

375
00:19:27,600 --> 00:19:30,600
neither with paper and pen mathematics,

376
00:19:30,600 --> 00:19:33,600
nor with whatever kind of algorithm.

377
00:19:33,600 --> 00:19:36,600
And so there, but that doesn't mean that we are not intelligent.

378
00:19:36,600 --> 00:19:39,600
It just means that this is beyond our modeling capability.

379
00:19:39,600 --> 00:19:42,600
But this taken aside,

380
00:19:42,600 --> 00:19:46,600
when it comes to creating ML models,

381
00:19:46,600 --> 00:19:51,600
and we want to model our, or let's say animal intelligence,

382
00:19:51,600 --> 00:19:54,600
then we would have to model the output of a complex system,

383
00:19:54,600 --> 00:19:57,600
which not even humans can let alone machines.

384
00:19:59,600 --> 00:20:00,600
Okay.

385
00:20:00,600 --> 00:20:01,600
All right.

386
00:20:01,600 --> 00:20:04,600
But at least humans have been able to approximate certain

387
00:20:04,600 --> 00:20:06,600
complex systems like weather forecast systems.

388
00:20:07,600 --> 00:20:08,600
So yes.

389
00:20:08,600 --> 00:20:10,600
So that's, so go ahead.

390
00:20:11,600 --> 00:20:13,600
So my question then would be,

391
00:20:15,600 --> 00:20:19,600
what do you think of the potential for machine learning systems

392
00:20:19,600 --> 00:20:22,600
to approximate rather than create an exact model?

393
00:20:22,600 --> 00:20:24,600
So, so a very good question.

394
00:20:24,600 --> 00:20:28,600
So whether forecast models are approximative machine learning models,

395
00:20:28,600 --> 00:20:29,600
right?

396
00:20:29,600 --> 00:20:31,600
So many of them are built using machine learning.

397
00:20:31,600 --> 00:20:34,600
So of course it's possible to approximate certain

398
00:20:34,600 --> 00:20:36,600
complex phenomena using machine learning,

399
00:20:36,600 --> 00:20:38,600
but it's quite limited what you can achieve.

400
00:20:38,600 --> 00:20:40,600
Yeah, you can achieve some.

401
00:20:40,600 --> 00:20:42,600
The question is always how good is the approximation

402
00:20:42,600 --> 00:20:44,600
and what can you technically do with it?

403
00:20:44,600 --> 00:20:47,600
And so if you look at the tetanosphere we have,

404
00:20:47,600 --> 00:20:49,600
all the technical gadgets that surround us,

405
00:20:49,600 --> 00:20:52,600
that make our life so much easier and better,

406
00:20:52,600 --> 00:20:54,600
most of them are exact.

407
00:20:55,600 --> 00:20:58,600
So mobile phones are exact, bridges are exact, trains are exact,

408
00:20:58,600 --> 00:21:00,600
airplanes are exact, cars are exact.

409
00:21:00,600 --> 00:21:04,600
So we don't have so many approximative models in the

410
00:21:04,600 --> 00:21:05,600
tetanosphere.

411
00:21:05,600 --> 00:21:08,600
So one, so Google, Google advertising plays is

412
00:21:08,600 --> 00:21:11,600
approximative, but why can Google afford it?

413
00:21:11,600 --> 00:21:15,600
Because nobody gets killed if I get shown a female

414
00:21:16,600 --> 00:21:19,600
lipstick ad, right?

415
00:21:19,600 --> 00:21:22,600
So nobody gets killed if I get shown a lipstick ad,

416
00:21:22,600 --> 00:21:24,600
so they can afford to do it.

417
00:21:24,600 --> 00:21:28,600
But if they would use these algorithms in intensive care,

418
00:21:28,600 --> 00:21:31,600
unit machinery, they would kill people.

419
00:21:31,600 --> 00:21:32,600
So that's the point.

420
00:21:32,600 --> 00:21:35,600
The point is approximative modeling is fine,

421
00:21:35,600 --> 00:21:38,600
but you have to decide in practice for what purposes

422
00:21:38,600 --> 00:21:40,600
you can use it and where you can't use it.

423
00:21:40,600 --> 00:21:41,600
Perfect.

424
00:21:41,600 --> 00:21:42,600
Thank you.

425
00:21:42,600 --> 00:21:43,600
Thank you.

426
00:21:43,600 --> 00:21:44,600
Thank you very much.

427
00:21:46,600 --> 00:21:48,600
Any more questions?

428
00:21:54,600 --> 00:21:56,600
So I guess I'll try.

429
00:21:56,600 --> 00:22:01,600
So if you take a, we use this example already a couple of

430
00:22:01,600 --> 00:22:04,600
times, if you take a laptop and throw it into a river,

431
00:22:04,600 --> 00:22:09,600
then the laptop no longer behaves in such a way that

432
00:22:09,600 --> 00:22:11,600
it is a simple system.

433
00:22:11,600 --> 00:22:14,600
It starts to decay.

434
00:22:14,600 --> 00:22:17,600
And that's because the laptop plus the river is a

435
00:22:17,600 --> 00:22:19,600
complex system.

436
00:22:19,600 --> 00:22:22,600
Is that a correct account?

437
00:22:22,600 --> 00:22:27,600
Well, I mean the system as a whole, so when the laptop

438
00:22:27,600 --> 00:22:31,600
drops into the river or creek or whatever,

439
00:22:31,600 --> 00:22:34,600
the creek has mechanical energy in the water.

440
00:22:34,600 --> 00:22:39,600
And this mechanical energy will start to attack the

441
00:22:39,600 --> 00:22:40,600
structure of the laptop.

442
00:22:40,600 --> 00:22:44,600
And then of course chemical processes that are not in

443
00:22:44,600 --> 00:22:48,600
equilibrium because the water is driven is driven will

444
00:22:48,600 --> 00:22:50,600
also attack the laptop.

445
00:22:50,600 --> 00:22:54,600
So the laptop in a way will become part of a

446
00:22:54,600 --> 00:22:57,600
complex system, but on its own it will not be a

447
00:22:57,600 --> 00:22:58,600
complex system.

448
00:22:58,600 --> 00:23:01,600
So if you take it out and those forces stop working

449
00:23:01,600 --> 00:23:06,600
on it, then it will stay in this more or less in a

450
00:23:06,600 --> 00:23:08,600
certain form of decay.

451
00:23:08,600 --> 00:23:11,600
Although in the long term, even if you leave a laptop

452
00:23:11,600 --> 00:23:14,600
standing somewhere without water, it will also be

453
00:23:14,600 --> 00:23:16,600
part of a complex system because there will also be

454
00:23:16,600 --> 00:23:19,600
energy working on it, but much less so.

455
00:23:19,600 --> 00:23:22,600
So if it's in a building, to begin with it will take

456
00:23:22,600 --> 00:23:24,600
one or 200 years before the building is broken and

457
00:23:24,600 --> 00:23:27,600
then energy can really work on the laptop.

458
00:23:27,600 --> 00:23:32,600
Can you go back to slide two?

459
00:23:32,600 --> 00:23:34,600
Yes.

460
00:23:34,600 --> 00:23:37,600
So you give C elegans as an example of a complex

461
00:23:37,600 --> 00:23:38,600
system.

462
00:23:38,600 --> 00:23:41,600
I assume you would give a human being as an example of

463
00:23:41,600 --> 00:23:42,600
a complex system.

464
00:23:42,600 --> 00:23:43,600
Yeah, of course.

465
00:23:43,600 --> 00:23:44,600
Yes.

466
00:23:44,600 --> 00:23:46,600
Now, but can you tell me how a human being has

467
00:23:46,600 --> 00:23:48,600
evolutionary properties?

468
00:23:48,600 --> 00:23:49,600
Yes.

469
00:23:49,600 --> 00:23:55,600
So the evolutionary properties of living organisms

470
00:23:55,600 --> 00:24:03,600
constitute, come from the fact that new types of

471
00:24:03,600 --> 00:24:08,600
molecules can be created in adult organisms.

472
00:24:08,600 --> 00:24:12,600
So if you have an adult organism by its ability to

473
00:24:12,600 --> 00:24:15,600
react to the environment, it can create new types of

474
00:24:15,600 --> 00:24:19,600
macromolecules that it hasn't had before, like new

475
00:24:19,600 --> 00:24:23,600
memories and all new combinations of elements.

476
00:24:23,600 --> 00:24:26,600
And that's all it can actually change the way it

477
00:24:26,600 --> 00:24:29,600
methylates its DNA and therefore change the way it

478
00:24:29,600 --> 00:24:34,600
will, the inheritance will work if it becomes

479
00:24:34,600 --> 00:24:37,600
progenitor of new organisms.

480
00:24:37,600 --> 00:24:41,600
So this is the way that new elements and element

481
00:24:41,600 --> 00:24:46,600
interactions can arise in adult organisms.

482
00:24:46,600 --> 00:24:47,600
Good.

483
00:24:47,600 --> 00:24:52,600
Okay, any more questions from anybody?

484
00:24:52,600 --> 00:24:54,600
Yes, I will have one.

485
00:24:54,600 --> 00:24:57,600
I believe it's a very boring, usual philosophical

486
00:24:57,600 --> 00:25:02,600
question, but when we say that complex systems are

487
00:25:02,600 --> 00:25:03,600
not...

488
00:25:03,600 --> 00:25:07,600
We don't have a mathematical modeling for them.

489
00:25:07,600 --> 00:25:08,600
Is this correct?

490
00:25:08,600 --> 00:25:09,600
Yes.

491
00:25:09,600 --> 00:25:12,600
Is this intended to be one of our epistemic

492
00:25:12,600 --> 00:25:16,600
lacks or something which happens in nature

493
00:25:16,600 --> 00:25:21,600
unregarded of some very, very complex mathematical

494
00:25:21,600 --> 00:25:25,600
model, which at the moment we don't have, but...

495
00:25:25,600 --> 00:25:26,600
This is a very good question.

496
00:25:26,600 --> 00:25:30,600
So we don't know this, so we can't really give an answer

497
00:25:30,600 --> 00:25:31,600
to this question.

498
00:25:31,600 --> 00:25:32,600
It could be speculative.

499
00:25:32,600 --> 00:25:36,600
However, my view is that our ability for mathematical

500
00:25:36,600 --> 00:25:40,600
modeling is an evolutionary adaptation of humans.

501
00:25:40,600 --> 00:25:44,600
So very much like language is the way our hands work,

502
00:25:44,600 --> 00:25:46,600
our evolutionary adaptations.

503
00:25:46,600 --> 00:25:49,600
And it is a special evolutionary adaptation of our

504
00:25:49,600 --> 00:25:54,600
mind, the extent of which also strongly varies

505
00:25:54,600 --> 00:25:55,600
between individuals.

506
00:25:55,600 --> 00:25:58,600
So there are only out of a thousand, only one or two

507
00:25:58,600 --> 00:26:01,600
individuals are usually mathematically gifted.

508
00:26:01,600 --> 00:26:05,600
And so it has a high variance, but everybody with an IQ

509
00:26:05,600 --> 00:26:07,600
above 80 can count.

510
00:26:07,600 --> 00:26:11,600
And this skill is limited.

511
00:26:11,600 --> 00:26:15,600
And I think it's limited by the forces that shaped it

512
00:26:15,600 --> 00:26:16,600
during evolution.

513
00:26:16,600 --> 00:26:22,600
And so I think that certain aspects of nature are just

514
00:26:22,600 --> 00:26:26,600
too much, so to speak, for the structure of this skill.

515
00:26:26,600 --> 00:26:29,600
And this seems very plausible, given the history of

516
00:26:29,600 --> 00:26:32,600
mathematics so far.

517
00:26:32,600 --> 00:26:37,600
However, because in the end, all the mathematical objects

518
00:26:37,600 --> 00:26:42,600
that we know can be reduced to numbers and are very

519
00:26:42,600 --> 00:26:45,600
complicated combinations of them.

520
00:26:45,600 --> 00:26:47,600
So even, for example, if the invention of calculus by

521
00:26:47,600 --> 00:26:51,600
Newton and Leibniz seems like a very big step, the way

522
00:26:51,600 --> 00:26:53,600
they did it was quite geometrical.

523
00:26:53,600 --> 00:26:57,600
And great inventions are always great, but it's still

524
00:26:57,600 --> 00:27:00,600
linked, of course, to the history of mathematics.

525
00:27:00,600 --> 00:27:03,600
So I don't think that we will ever be able to model fully

526
00:27:03,600 --> 00:27:06,600
model complex systems, but we were able to, to some extent,

527
00:27:06,600 --> 00:27:12,600
of course, approximately model aspects of them.

528
00:27:12,600 --> 00:27:15,600
So in other words, I think the structural deficit.

529
00:27:15,600 --> 00:27:16,600
Yes, yes.

530
00:27:16,600 --> 00:27:19,600
Thank you.

531
00:27:19,600 --> 00:27:21,600
I also have another question.

532
00:27:21,600 --> 00:27:22,600
Yes.

533
00:27:22,600 --> 00:27:27,600
So if we consider the fact that how mathematics have evolved

534
00:27:27,600 --> 00:27:32,600
so far, and also the fact that mathematical modeling has

535
00:27:32,600 --> 00:27:38,600
up to a point tried to represent, in a way, the randomness

536
00:27:38,600 --> 00:27:45,600
that might be an attribute of these complex systems.

537
00:27:45,600 --> 00:27:50,600
So we have already made some, find out some mathematical

538
00:27:50,600 --> 00:27:53,600
ways through statistics and stochastic processes in order

539
00:27:53,600 --> 00:27:55,600
to study randomness.

540
00:27:55,600 --> 00:27:57,600
Yeah, of course.

541
00:27:57,600 --> 00:28:02,600
And so do you think that this is like a big step, a small

542
00:28:02,600 --> 00:28:06,600
step to accomplish something even bigger in the future?

543
00:28:06,600 --> 00:28:09,600
Because up until a point, we also thought that randomness

544
00:28:09,600 --> 00:28:12,600
was something that we couldn't even explain.

545
00:28:12,600 --> 00:28:15,600
And computationally.

546
00:28:15,600 --> 00:28:17,600
That's, that's not fully true.

547
00:28:17,600 --> 00:28:22,600
For example, there is no mathematical model for a true

548
00:28:22,600 --> 00:28:25,600
natural number generator, a random number generator.

549
00:28:25,600 --> 00:28:29,600
So a true random number generator, which produces true

550
00:28:29,600 --> 00:28:32,600
random events can only be constructed by using a physical

551
00:28:32,600 --> 00:28:36,600
device, namely normally one uses a Geiger counter, which is

552
00:28:36,600 --> 00:28:40,600
counting radioactive decay events, because they are truly

553
00:28:40,600 --> 00:28:41,600
random.

554
00:28:41,600 --> 00:28:43,600
So we cannot simulate randomness.

555
00:28:43,600 --> 00:28:46,600
Of course, we can model certain aspects of randomness.

556
00:28:46,600 --> 00:28:49,600
And this we can do since a couple of hundred years.

557
00:28:49,600 --> 00:28:53,600
And at the end of the 1980th century, we have started to

558
00:28:53,600 --> 00:28:57,600
understand how calculus can be applied to this.

559
00:28:57,600 --> 00:29:02,600
But the way that we model randomness is only applies to

560
00:29:02,600 --> 00:29:04,600
classical Newtonian systems.

561
00:29:04,600 --> 00:29:07,600
And when we get out of Newtonian systems, there are actually

562
00:29:07,600 --> 00:29:11,600
no examples to convincingly model a complex system behavior.

563
00:29:11,600 --> 00:29:15,600
So they are, of course, like one of your colleagues mentioned,

564
00:29:15,600 --> 00:29:19,600
approximative models of randomness, such as, or

565
00:29:19,600 --> 00:29:23,600
stochastic behavior, such as weather forecasting, but they

566
00:29:23,600 --> 00:29:26,600
have a very short-term forecast window.

567
00:29:26,600 --> 00:29:29,600
And they are not very, depending on in which landscape you are,

568
00:29:29,600 --> 00:29:31,600
they almost don't work at all.

569
00:29:31,600 --> 00:29:35,600
So near the sea or in the mountains, they almost do not work.

570
00:29:35,600 --> 00:29:39,600
And basically, they work for regular continental climate with

571
00:29:39,600 --> 00:29:42,600
strong determinants over very short durations.

572
00:29:42,600 --> 00:29:49,600
And so the fact that we have probability theory doesn't

573
00:29:49,600 --> 00:29:51,600
really help with complex systems.

574
00:29:51,600 --> 00:29:54,600
Turbulence, which I already mentioned, is a very good example

575
00:29:54,600 --> 00:29:59,600
because the mathematician who had found the best way so far of

576
00:29:59,600 --> 00:30:04,600
dealing with turbulence was actually probabilistic

577
00:30:04,600 --> 00:30:07,600
theoretician, was Kolmogorov, who invented the Kolmogorov-Smilnov

578
00:30:07,600 --> 00:30:10,600
theorem, one of the greatest mathematical geniuses of the

579
00:30:10,600 --> 00:30:11,600
20th century.

580
00:30:11,600 --> 00:30:14,600
In the 1940s, he tried to model turbulence.

581
00:30:14,600 --> 00:30:17,600
And while his model is, you can see it when our book will

582
00:30:17,600 --> 00:30:20,600
appear, it's explained in the book, while his model is very

583
00:30:20,600 --> 00:30:22,600
aesthetically highly valuable.

584
00:30:22,600 --> 00:30:24,600
It's very beautiful.

585
00:30:24,600 --> 00:30:27,600
It fails to model the reality of turbulence.

586
00:30:27,600 --> 00:30:32,600
And so there is no example for exact modeling or good

587
00:30:33,600 --> 00:30:35,600
approximative modeling of complex systems.

588
00:30:35,600 --> 00:30:36,600
And I don't know.

589
00:30:36,600 --> 00:30:44,600
It's actually, if you know how the theory of probability works,

590
00:30:44,600 --> 00:30:47,600
how the distribution are approximated, that it's already

591
00:30:47,600 --> 00:30:51,600
very, very hard actually to figure out or mathematically

592
00:30:51,600 --> 00:30:56,600
impossible to calculate mixed high-dimensional distributions.

593
00:30:56,600 --> 00:30:59,600
You will see that this is very, very far away.

594
00:30:59,600 --> 00:31:02,600
I don't know if it's possible.

595
00:31:02,600 --> 00:31:03,600
Okay.

596
00:31:03,600 --> 00:31:05,600
Thank you for your...

597
00:31:05,600 --> 00:31:06,600
Okay.

598
00:31:06,600 --> 00:31:09,600
I think we'll have yours to continue now for a bit.

599
00:31:09,600 --> 00:31:12,600
So now we look at intelligence.

600
00:31:12,600 --> 00:31:16,600
And you will see in the course of the second part why we needed

601
00:31:16,600 --> 00:31:18,600
to talk about complex systems.

602
00:31:18,600 --> 00:31:21,600
So this here, what you see is the standard definition of the

603
00:31:21,600 --> 00:31:25,600
artificial general intelligence community of intelligence.

604
00:31:25,600 --> 00:31:26,600
And this is...

605
00:31:26,600 --> 00:31:30,600
So they have a verbal definition which goes intelligence

606
00:31:30,600 --> 00:31:33,600
measures an agent's ability to achieve goals in a wide range

607
00:31:33,600 --> 00:31:34,600
of environments.

608
00:31:34,600 --> 00:31:39,600
That's the standard definition that everybody accepts.

609
00:31:39,600 --> 00:31:44,600
And so this ability to achieve goals is defined by a utility

610
00:31:44,600 --> 00:31:45,600
function.

611
00:31:45,600 --> 00:31:50,600
And this utility is here V of an agent pi depending on the

612
00:31:50,600 --> 00:31:52,600
environment of the agent's mu.

613
00:31:52,600 --> 00:31:56,600
And it's defined as the expectation of the sum of the

614
00:31:56,600 --> 00:32:00,600
rewards the agent is going to have, which is actually normed

615
00:32:00,600 --> 00:32:04,600
to be equal less to one.

616
00:32:04,600 --> 00:32:08,600
And so mu is a binary description of the environment of

617
00:32:08,600 --> 00:32:09,600
the agent.

618
00:32:09,600 --> 00:32:13,600
And this environment may be a fired abstract structure that

619
00:32:13,600 --> 00:32:15,600
is manipulated inside a computer.

620
00:32:15,600 --> 00:32:19,600
For example, if you have a theorem prover, then this would

621
00:32:19,600 --> 00:32:22,600
be one environment or it may relate to something in the

622
00:32:22,600 --> 00:32:23,600
physical world.

623
00:32:23,600 --> 00:32:28,600
For example, a nuclear power station that has broken down

624
00:32:28,600 --> 00:32:32,600
and where you now want to clean up the nuclear power station

625
00:32:32,600 --> 00:32:33,600
with a robot.

626
00:32:33,600 --> 00:32:35,600
Because it's not an environment where we would like to use

627
00:32:35,600 --> 00:32:36,600
humans.

628
00:32:36,600 --> 00:32:40,600
And by the way, there's a great documentation about

629
00:32:40,600 --> 00:32:41,600
Chernobyl.

630
00:32:41,600 --> 00:32:43,600
They tried to use robots.

631
00:32:43,600 --> 00:32:47,600
But the problem was that the transistors broke immediately

632
00:32:47,600 --> 00:32:49,600
because radiation was too strong.

633
00:32:49,600 --> 00:32:53,600
So the radiation destroyed the transistors inside the robot

634
00:32:53,600 --> 00:32:55,600
and then they stopped working.

635
00:32:55,600 --> 00:32:58,600
So they had to use humans after all.

636
00:32:58,600 --> 00:33:03,600
And in either case, no matter whether it's a physical

637
00:33:03,600 --> 00:33:09,600
scenario or artificial one, this vector takes a form of a

638
00:33:09,600 --> 00:33:10,600
binary string.

639
00:33:10,600 --> 00:33:17,600
And it's a description which plays a role in the HATTA

640
00:33:17,600 --> 00:33:18,600
model.

641
00:33:18,600 --> 00:33:21,600
And E is the expectation of the rewards.

642
00:33:21,600 --> 00:33:24,600
And what is a reward we will see it on the next slide.

643
00:33:24,600 --> 00:33:26,600
So this is a basic definition.

644
00:33:26,600 --> 00:33:31,600
Now, first of all, let's consider what's that this definition

645
00:33:31,600 --> 00:33:39,600
is actually mathematically broken, which I find kind of sad

646
00:33:39,600 --> 00:33:41,600
because they should at least get this right.

647
00:33:41,600 --> 00:33:44,600
But basically, this is really accepted in the AGI community.

648
00:33:44,600 --> 00:33:47,600
Now, one has to see that the AGI community is made up mainly by

649
00:33:47,600 --> 00:33:49,600
computer scientists and mathematicians.

650
00:33:49,600 --> 00:33:53,600
And computer scientists like physicists and engineers, they

651
00:33:53,600 --> 00:33:56,600
tend to not look so carefully at mathematical equations.

652
00:33:56,600 --> 00:34:02,600
But actually the definition of expectation is the sum of a

653
00:34:02,600 --> 00:34:06,600
variable, which is actually multiplied by the probability of

654
00:34:06,600 --> 00:34:09,600
this variable for each step.

655
00:34:09,600 --> 00:34:12,600
So every finite amount of steps, or here is actually even

656
00:34:12,600 --> 00:34:16,600
infinite, but it doesn't matter if you have some steps, you

657
00:34:16,600 --> 00:34:23,600
should calculate the reward of the variable by summing the

658
00:34:23,600 --> 00:34:26,600
product of the probability with the variable, which is the

659
00:34:26,600 --> 00:34:27,600
reward.

660
00:34:27,600 --> 00:34:29,600
And here they don't do this.

661
00:34:29,600 --> 00:34:32,600
They actually take the expectation of that is already

662
00:34:32,600 --> 00:34:33,600
summed up.

663
00:34:34,600 --> 00:34:38,600
So it's funny because if you wanted to implement this, it's

664
00:34:38,600 --> 00:34:41,600
actually not implementable because mathematically wrong, which

665
00:34:41,600 --> 00:34:44,600
I found quite interesting.

666
00:34:44,600 --> 00:34:48,600
But so in other words, the operator E of the expectation has

667
00:34:48,600 --> 00:34:51,600
to be applied not to the sum of the values of a random

668
00:34:51,600 --> 00:34:54,600
variable, but directly to the values of the variable, as shown

669
00:34:54,600 --> 00:34:56,600
here in this equation.

670
00:34:56,600 --> 00:34:59,600
And otherwise, you cannot take account of the norming

671
00:34:59,600 --> 00:35:00,600
denominator.

672
00:35:00,600 --> 00:35:04,600
So pi i is smaller than 1, and therefore it's a denominator.

673
00:35:04,600 --> 00:35:07,600
If you multiply by a number smaller than 1, it's the same

674
00:35:07,600 --> 00:35:09,600
as dividing by this number.

675
00:35:09,600 --> 00:35:13,600
And so usually you need a denominator.

676
00:35:13,600 --> 00:35:15,600
And if you don't have a denominator, you cannot get the

677
00:35:15,600 --> 00:35:19,600
sum below 1, of course.

678
00:35:19,600 --> 00:35:25,600
Because this is just mathematically silly.

679
00:35:25,600 --> 00:35:28,600
But they've published it, and the viewers have accepted it,

680
00:35:28,600 --> 00:35:31,600
and it gets cited hundreds and hundreds of times.

681
00:35:31,600 --> 00:35:32,600
I'm astonished.

682
00:35:32,600 --> 00:35:35,600
But anyhow, a lot of crap gets cited a lot.

683
00:35:35,600 --> 00:35:37,600
So it seems to be human nature.

684
00:35:37,600 --> 00:35:41,600
But it's remarkable that nobody has criticized this.

685
00:35:41,600 --> 00:35:46,600
Taking this aside and just ignoring the definition

686
00:35:46,600 --> 00:35:49,600
problems and imagining they had used the proper definition

687
00:35:49,600 --> 00:35:53,600
of expectation, which is actually you can read up in any

688
00:35:53,600 --> 00:35:56,600
very basic textbook of statistics.

689
00:35:57,600 --> 00:36:00,600
I mean, actually, it's cool when you learn to calculate

690
00:36:00,600 --> 00:36:04,600
the probability of throwing the number 7 with 2 dice.

691
00:36:04,600 --> 00:36:05,600
You learn this.

692
00:36:05,600 --> 00:36:07,600
So I was kind of astonished.

693
00:36:07,600 --> 00:36:11,600
Anyhow, in simplified terms, when we ignore this, the equation

694
00:36:11,600 --> 00:36:15,600
1 set in agent pi reacting to environment distribution mu

695
00:36:15,600 --> 00:36:18,600
obtains a finite reward, which corresponds to the expectation

696
00:36:18,600 --> 00:36:22,600
of reward it achieves over all the steps it undertakes.

697
00:36:22,600 --> 00:36:27,600
And actually, the higher this is, the better the utility.

698
00:36:27,600 --> 00:36:32,600
And so Barry asked me to insert something about reward.

699
00:36:32,600 --> 00:36:36,600
Of course, the reward here has no meaning for the machine.

700
00:36:36,600 --> 00:36:39,600
The machine is just a calculating machine.

701
00:36:39,600 --> 00:36:40,600
It's a Turing machine.

702
00:36:40,600 --> 00:36:49,600
It can just apply amounts of electricity to certain circuits.

703
00:36:49,600 --> 00:36:54,600
But by doing this, so reward has no meaning for the machine.

704
00:36:54,600 --> 00:36:58,600
That's just a mathematical concept to express an optimization

705
00:36:58,600 --> 00:36:59,600
problem.

706
00:36:59,600 --> 00:37:04,600
So this year, how can I maximize the utility?

707
00:37:04,600 --> 00:37:09,600
I can maximize this utility by doing something with this reward

708
00:37:09,600 --> 00:37:10,600
variable.

709
00:37:10,600 --> 00:37:12,600
And that's all it says.

710
00:37:12,600 --> 00:37:16,600
So reward doesn't mean what is reward for you so that I invite

711
00:37:16,600 --> 00:37:21,600
you to a good beer in the evening or anything or bring you

712
00:37:21,600 --> 00:37:24,600
some flowers or so, what humans experience as reward.

713
00:37:24,600 --> 00:37:27,600
It's just a way of formulating the mathematical optimization

714
00:37:27,600 --> 00:37:29,600
problem.

715
00:37:29,600 --> 00:37:30,600
OK.

716
00:37:30,600 --> 00:37:36,600
So on the next slide, what do they do with this utility?

717
00:37:36,600 --> 00:37:39,600
So here you can see this utility term again.

718
00:37:39,600 --> 00:37:42,600
But now it is in a bigger equation, which defines intelligence.

719
00:37:42,600 --> 00:37:49,600
And this uppercase epsilon, it's a mathematical uppercase epsilon

720
00:37:49,600 --> 00:37:55,600
as a function of the agent is defined as the sum of the utility

721
00:37:55,600 --> 00:37:59,600
multiplied with another factor.

722
00:37:59,600 --> 00:38:01,600
And what is this factor?

723
00:38:01,600 --> 00:38:07,600
So this factor contains k, which is a Kolmogorov complexity

724
00:38:07,600 --> 00:38:09,600
function.

725
00:38:09,600 --> 00:38:11,600
We already heard about Kolmogorov.

726
00:38:11,600 --> 00:38:16,600
He did not only work on probability distributions, but also on

727
00:38:16,600 --> 00:38:20,600
information theory and the complexity function.

728
00:38:20,600 --> 00:38:24,600
He invented it and it's indicating the complexity of the algorithm

729
00:38:24,600 --> 00:38:28,600
executed by the agent pi to represent the environment mu.

730
00:38:28,600 --> 00:38:32,600
So it's basically how many calculation steps are needed to

731
00:38:32,600 --> 00:38:33,600
represent mu.

732
00:38:33,600 --> 00:38:36,600
And as you can see, this is a negative exponent.

733
00:38:36,600 --> 00:38:38,600
So that means that this is a parallelizing factor.

734
00:38:38,600 --> 00:38:43,600
So the more steps I need, the more complex my representation

735
00:38:43,600 --> 00:38:47,600
algorithm is, the lower the intelligence will be.

736
00:38:47,600 --> 00:38:50,600
And that's pretty, I think, acceptable.

737
00:38:50,600 --> 00:38:55,600
Because so if you imagine a more intelligent individual will

738
00:38:55,600 --> 00:38:58,600
find it easier to achieve a goal than a stupid individual.

739
00:38:58,600 --> 00:39:01,600
So of course, you know this from everyday life.

740
00:39:01,600 --> 00:39:04,600
And so this says something similar.

741
00:39:04,600 --> 00:39:09,600
It says, so if you have a high utility for a given environment,

742
00:39:09,600 --> 00:39:13,600
but I needed a million steps to achieve it, then I'm less

743
00:39:13,600 --> 00:39:17,600
intelligent than somebody who achieved the same utility with a

744
00:39:17,600 --> 00:39:20,600
shorter number of steps.

745
00:39:20,600 --> 00:39:24,600
So further, a couple of more remarks.

746
00:39:24,600 --> 00:39:25,600
So mu is the environment.

747
00:39:25,600 --> 00:39:27,600
What is uppercase u?

748
00:39:27,600 --> 00:39:30,600
Uppercase u is a set of environment descriptions that the

749
00:39:30,600 --> 00:39:32,600
machine can process.

750
00:39:32,600 --> 00:39:37,600
So for example, if the machine is the AlphaGo machine, then all

751
00:39:37,600 --> 00:39:42,600
the situations are settings on the gold board and nothing more.

752
00:39:42,600 --> 00:39:46,600
But of course, an AGI agent would be able hopefully to process

753
00:39:46,600 --> 00:39:49,600
more different situations that at least what they hope.

754
00:39:49,600 --> 00:39:53,600
And so u is just a set of environments the agent could in

755
00:39:53,600 --> 00:39:55,600
theory be processing.

756
00:39:55,600 --> 00:39:58,600
Let me give you an example from the animal kingdom.

757
00:39:58,600 --> 00:40:04,600
So for example, a rat has quite a huge set u because a rat can

758
00:40:04,600 --> 00:40:06,600
adapt to very many environments.

759
00:40:06,600 --> 00:40:09,600
And therefore, it is also to be found all around the globe

760
00:40:09,600 --> 00:40:13,600
all mode, not in the Antarctic and not in the Sahara, I believe,

761
00:40:13,600 --> 00:40:15,600
but in many, many moderate environments.

762
00:40:15,600 --> 00:40:19,600
A rat is to be found, whereas other animals are very specialized

763
00:40:19,600 --> 00:40:21,600
and are only found in certain environments.

764
00:40:21,600 --> 00:40:25,600
And this is what this uppercase u is supposed to say.

765
00:40:26,600 --> 00:40:32,600
So basically the definition of intelligence shows to you the

766
00:40:32,600 --> 00:40:37,600
most efficient possible algorithm to achieve a certain utility.

767
00:40:37,600 --> 00:40:43,600
Also, the summation over all environments prevents a random hit.

768
00:40:43,600 --> 00:40:49,600
So because you have to perform this in many environments, you can

769
00:40:49,600 --> 00:40:53,600
guarantee that the equation doesn't give you a distorted outcome.

770
00:40:53,600 --> 00:40:58,600
So if the definition of the utility function would be

771
00:40:58,600 --> 00:41:02,600
mathematically sound, which you could easily do by replacing it

772
00:41:02,600 --> 00:41:06,600
with this, then the entire equation would mathematically

773
00:41:06,600 --> 00:41:08,600
make sense.

774
00:41:08,600 --> 00:41:13,600
So it is a proper definition of a weighted utility function.

775
00:41:13,600 --> 00:41:17,600
That's what it basically is.

776
00:41:17,600 --> 00:41:22,600
Before we take apart these two equations, now I've just

777
00:41:22,600 --> 00:41:29,600
criticized them from a mathematical point of view.

778
00:41:29,600 --> 00:41:33,600
Are there any questions regarding the two equations before I

779
00:41:33,600 --> 00:41:37,600
continue dissecting them?

780
00:41:37,600 --> 00:41:40,600
I have two questions when I was looking at this formula.

781
00:41:40,600 --> 00:41:45,600
So I was just looking at V as the author says,

782
00:41:45,600 --> 00:41:49,600
ability to achieve the value, the capital V.

783
00:41:49,600 --> 00:41:54,600
But then in the formula on the second slide, on this one.

784
00:41:54,600 --> 00:42:01,600
So why is Gogomolo complexity function as exponential?

785
00:42:01,600 --> 00:42:05,600
And the V is just like linear.

786
00:42:05,600 --> 00:42:09,600
Because it's just a negative exponent.

787
00:42:09,600 --> 00:42:13,600
So it means you just divide.

788
00:42:13,600 --> 00:42:16,600
This means 1 divided by 2 exponent k.

789
00:42:16,600 --> 00:42:21,600
So it's just used to actually penalize the utility function.

790
00:42:21,600 --> 00:42:27,600
But why didn't they just penalize with the inverse value?

791
00:42:27,600 --> 00:42:33,600
Why did they choose the exponential growth?

792
00:42:33,600 --> 00:42:36,600
To make it perfect, maybe.

793
00:42:36,600 --> 00:42:38,600
Yeah, maybe.

794
00:42:38,600 --> 00:42:41,600
Actually, I haven't thought about it.

795
00:42:41,600 --> 00:42:45,600
But it's basically the way that the Gogomolo complexity function

796
00:42:45,600 --> 00:42:49,600
always gets applied in theoretical informatics.

797
00:42:49,600 --> 00:42:54,600
So this form is when you read a textbook of theoretical informatics,

798
00:42:54,600 --> 00:42:57,600
you always see it represented like this.

799
00:42:57,600 --> 00:43:03,600
So that you get a smooth representation of the entire function.

800
00:43:03,600 --> 00:43:12,600
So I think it works pretty well to weigh whatever you want to weigh

801
00:43:12,600 --> 00:43:16,600
by the amount of work that you need to put into obtain it.

802
00:43:16,600 --> 00:43:20,600
Yeah, they argue that extensively in the paper anyway.

803
00:43:20,600 --> 00:43:26,600
But the second question is this capital U, the set of environment descriptions.

804
00:43:26,600 --> 00:43:28,600
How is this supposed to be interpreted?

805
00:43:28,600 --> 00:43:31,600
Because it's written as a sum.

806
00:43:31,600 --> 00:43:37,600
But I would take that in general, this is like a manifold in the best case.

807
00:43:37,600 --> 00:43:40,600
So it's a sum over the elements of a set.

808
00:43:40,600 --> 00:43:41,600
That's fine mathematically.

809
00:43:41,600 --> 00:43:47,600
So this is just an index of all the possible terms you can get.

810
00:43:47,600 --> 00:43:52,600
And let's say you would have three different environments.

811
00:43:52,600 --> 00:43:59,600
What if it has a ball or a sphere of different environments?

812
00:43:59,600 --> 00:44:01,600
So you just integrate?

813
00:44:01,600 --> 00:44:03,600
No, I don't think so.

814
00:44:03,600 --> 00:44:08,600
Because you basically can always, this is theoretical informatics.

815
00:44:08,600 --> 00:44:13,600
So that means that you can, no matter what from a functional analysis point of view,

816
00:44:13,600 --> 00:44:17,600
no matter what is the way the environments are,

817
00:44:17,600 --> 00:44:20,600
they are always compressed into binary vectors.

818
00:44:20,600 --> 00:44:26,600
And then you can just create a series of binary vectors or a set of binary vectors.

819
00:44:26,600 --> 00:44:28,600
And this is just one binary vector out of a set.

820
00:44:28,600 --> 00:44:32,600
So it's a countable set of binary vectors, no matter what.

821
00:44:32,600 --> 00:44:38,600
Maybe you should explain what a binary vector is.

822
00:44:38,600 --> 00:44:44,600
Yeah, so for the non-n estimaticians, it's just a vector of ones and zeros.

823
00:44:44,600 --> 00:44:50,600
So remember that a Turing machine can only deal with ones and zeros.

824
00:44:50,600 --> 00:44:56,600
It's like a big tape on which you can write ones and zeros and can change the ones and zeros.

825
00:44:56,600 --> 00:45:01,600
And this is basically, from the perspective of theoretical informatics,

826
00:45:01,600 --> 00:45:06,600
it's just a set of such binary vectors.

827
00:45:06,600 --> 00:45:12,600
Sorry, I think it would have been a bit more helpful if the input and output spaces were also defined.

828
00:45:12,600 --> 00:45:18,600
Because now it's clear to me what, so the inputs are basically strings, as far as I understand it.

829
00:45:18,600 --> 00:45:20,600
Is it?

830
00:45:20,600 --> 00:45:24,600
Yeah, so otherwise the mu is a bit hard to interpret.

831
00:45:24,600 --> 00:45:26,600
Yeah, the mu, yeah, you're right.

832
00:45:26,600 --> 00:45:34,600
So I have not defined, actually it's interesting why I've just copied the definition from their paper,

833
00:45:34,600 --> 00:45:42,600
but you're right, usually one would have to use a functional analysis type of definition of the input space at the output.

834
00:45:42,600 --> 00:45:44,600
Yeah, exactly.

835
00:45:44,600 --> 00:45:50,600
And I could easily have done this, but there are two reasons why they didn't do it for sloppiness,

836
00:45:50,600 --> 00:45:56,600
and I didn't do it because it's just too implicit for me because it's what I do all the time, but thank you.

837
00:45:56,600 --> 00:46:00,600
I think we should add this next time, it's a good point.

838
00:46:00,600 --> 00:46:01,600
Yeah.

839
00:46:01,600 --> 00:46:06,600
Okay, so anyhow, this is a definition.

840
00:46:06,600 --> 00:46:09,600
Now let's move on and look at the problem.

841
00:46:09,600 --> 00:46:14,600
So first of all, there is a verbose definition before the equation,

842
00:46:14,600 --> 00:46:19,600
which says intelligence measures an agent's ability to achieve goals in a wide range of environments.

843
00:46:19,600 --> 00:46:27,600
So first of all, the definition captures just one part of one of the standard definitions of primary intelligence.

844
00:46:27,600 --> 00:46:38,600
So primary intelligence says that you have to adapt to new environments suddenly and without being trained upfront.

845
00:46:38,600 --> 00:46:45,600
So you have to be spontaneously able to adapt to a new environment suddenly, quickly.

846
00:46:45,600 --> 00:46:52,600
And this definition just captures the adaptation.

847
00:46:52,600 --> 00:46:59,600
Now, what is more important is that the definition is very broad because it allows also this organism,

848
00:47:00,600 --> 00:47:08,600
a small worm that is one millimeter long and has a thousand cells and only 300 neurons to be intelligent,

849
00:47:08,600 --> 00:47:12,600
because what it can do, it can forage and reproduce in complex environments,

850
00:47:12,600 --> 00:47:18,600
so it can live in fruit, in vegetables, in mushrooms, in soil, and so on.

851
00:47:18,600 --> 00:47:21,600
It can use snails and stugs as migration vectors.

852
00:47:21,600 --> 00:47:28,600
So it can really live in many environments and it can also reproduce there.

853
00:47:28,600 --> 00:47:34,600
So I think it would be intelligent, according to the AGI definition.

854
00:47:34,600 --> 00:47:38,600
And even probably the amoeba here, this is a nice amoeba.

855
00:47:38,600 --> 00:47:43,600
It's called kaos kaolinensis because it never has the same shape.

856
00:47:43,600 --> 00:47:50,600
Here's just a drawing of it, but in the next second it will look different because it moves around by changing its shape.

857
00:47:50,600 --> 00:47:55,600
And it can also live in many, many environments and thrive wonderfully there.

858
00:47:55,600 --> 00:48:01,600
It can reproduce, it can find food, but it's just one cell.

859
00:48:01,600 --> 00:48:06,600
So it's one of the most primitive, well, it's the most primitive eukaryotes,

860
00:48:06,600 --> 00:48:10,600
but directly next to yeast comes already this amoeba.

861
00:48:10,600 --> 00:48:12,600
It would also be intelligent in this definition.

862
00:48:12,600 --> 00:48:15,600
So I think the definition of intelligence is too weak.

863
00:48:15,600 --> 00:48:16,600
Why is it so weak?

864
00:48:16,600 --> 00:48:21,600
Because in the book where this intelligence definition is used,

865
00:48:21,600 --> 00:48:28,600
that's the book by Götze and Pinache, which is called Artificial General Intelligence,

866
00:48:28,600 --> 00:48:31,600
they discuss many other definitions.

867
00:48:31,600 --> 00:48:37,600
But the problem is that if they use definition like the one very simple one of human intelligence,

868
00:48:37,600 --> 00:48:43,600
intelligence is the capability that enables us to speak, for example.

869
00:48:43,600 --> 00:48:47,600
That enables the language that humans can use.

870
00:48:47,600 --> 00:48:51,600
I mean, it's not my definition, but if you would, but they propose this definition,

871
00:48:51,600 --> 00:48:56,600
then they would automatically fail in generating artificial intelligence.

872
00:48:56,600 --> 00:49:00,600
So they've basically chosen a definition that doesn't yield intelligence,

873
00:49:00,600 --> 00:49:04,600
so that they claim now that they have an intelligence.

874
00:49:04,600 --> 00:49:09,600
But they've actually on purpose used a very stupid definition that is not intelligence.

875
00:49:09,600 --> 00:49:19,600
Now let's go and look at what time it is and look at other problems of the AGI definition.

876
00:49:19,600 --> 00:49:21,600
So let's first look at perception.

877
00:49:21,600 --> 00:49:27,600
So if you have this vector mu as a measure of complexity of environment,

878
00:49:27,600 --> 00:49:34,600
this vector mu presupposes that the environment can be represented by using a binary vector.

879
00:49:34,600 --> 00:49:38,600
In some artificial environments, such a binary representation may be adequate,

880
00:49:38,600 --> 00:49:43,600
but in natural environments, we have signals emanating from complex systems.

881
00:49:43,600 --> 00:49:53,600
And therefore, the signals need to be actively interpreted and reassessed all the time.

882
00:49:53,600 --> 00:49:57,600
And also the observation needs to be continuously adapted to the input

883
00:49:57,600 --> 00:50:02,600
as the agent takes account of the interpretation of each antecedent observation.

884
00:50:02,600 --> 00:50:12,600
And also an animal interpretation depends on previously experienced mental material,

885
00:50:12,600 --> 00:50:13,600
so memories.

886
00:50:13,600 --> 00:50:21,600
And for example, this tiger observing the prey, it does actually all the time update its observations.

887
00:50:21,600 --> 00:50:25,600
It does active perception or shields, I think a female tiger.

888
00:50:25,600 --> 00:50:29,600
And if there would be a puppet next to a young tiger,

889
00:50:29,600 --> 00:50:34,600
the young tiger would not be as good at observing those animals, those prey animals,

890
00:50:34,600 --> 00:50:36,600
because it has that experience.

891
00:50:36,600 --> 00:50:42,600
So the experience stored in the memory of the tiger also helps it to react better to what the animals are doing

892
00:50:42,600 --> 00:50:47,600
and to single out, for example, one animal to hunt it down.

893
00:50:47,600 --> 00:50:52,600
So the predator, if the predator is just sitting there and observing the prey,

894
00:50:52,600 --> 00:50:57,600
it's already acting to improve and adapt the perception of the prey.

895
00:50:57,600 --> 00:51:02,600
So perception is not static, but a dynamic process of constant iterative feedback loops

896
00:51:02,600 --> 00:51:08,600
between sensory and motor neuron circuits, and we cannot even know.

897
00:51:08,600 --> 00:51:14,600
So if we think of training an ML algorithm, we need to know the tuples that we used to train,

898
00:51:14,600 --> 00:51:18,600
but we don't even know when the cycle begins and ends.

899
00:51:18,600 --> 00:51:23,600
So the cycles can be very fast, and we don't know because we can only observe the overall behavior,

900
00:51:23,600 --> 00:51:31,600
but we don't know how to determine the tuples that constitute the perception process.

901
00:51:31,600 --> 00:51:37,600
So our JJ Gibson, a very important psychologist and philosopher says,

902
00:51:37,600 --> 00:51:41,600
normal activity of perception is to explore the world.

903
00:51:41,600 --> 00:51:44,600
So perception depends on more than just sensory stimulus.

904
00:51:44,600 --> 00:51:50,600
So the view that perception is just a result of sensory stimulus is completely outdated.

905
00:51:50,600 --> 00:51:55,600
So when we give input to computers, it's just sensory input, but that's not real.

906
00:51:55,600 --> 00:51:57,600
That's not perception.

907
00:51:57,600 --> 00:52:02,600
Perception requires purposeful activity, direct manipulation of the object,

908
00:52:02,600 --> 00:52:07,600
and innate or quiet knowledge of the expected patterns of reality.

909
00:52:07,600 --> 00:52:17,600
So I need categorical, predefined ability to deal with the environment and also quiet knowledge.

910
00:52:17,600 --> 00:52:22,600
And this manipulation of the object doesn't mean that I need to touch them,

911
00:52:22,600 --> 00:52:31,600
but the tiger can also manipulate these animals, prey animals in her imagination.

912
00:52:31,600 --> 00:52:36,600
So she can imagine that maybe this animal would now lean down to drink from water source

913
00:52:36,600 --> 00:52:41,600
and whether then maybe it would be a good moment to attack this animal and so on.

914
00:52:42,600 --> 00:52:48,600
So this is highly interactive and mu, this static vector mu,

915
00:52:48,600 --> 00:52:54,600
which basically models, for example, the input from a sensor doesn't capture any of this,

916
00:52:54,600 --> 00:52:57,600
does not capture any of what I've just said about perception,

917
00:52:57,600 --> 00:53:00,600
what we know about animal and human perception.

918
00:53:00,600 --> 00:53:06,600
Here's another example of perception, which is much more complex than this one

919
00:53:06,600 --> 00:53:10,600
because it involves dialogue, it involves observation of a dialogue,

920
00:53:10,600 --> 00:53:18,600
it involves probably, yeah, it has many, many interesting aspects

921
00:53:18,600 --> 00:53:22,600
that show how complicated perception is.

922
00:53:22,600 --> 00:53:31,600
And so, for example, what does Tony Curtis, who is here acting as a woman,

923
00:53:31,600 --> 00:53:33,600
think about Marilyn Monroe in this moment?

924
00:53:33,600 --> 00:53:36,600
I don't know, but it's certainly an interesting question.

925
00:53:36,600 --> 00:53:43,600
And anyhow, so perception is not modeled by this environment variable mu.

926
00:53:43,600 --> 00:53:48,600
On the next slide, we see the next problem, which is activity.

927
00:53:48,600 --> 00:53:55,600
So the steps of the utility function that you've seen here are results of,

928
00:53:55,600 --> 00:53:58,600
so each step is an activity which yields a reward.

929
00:53:58,600 --> 00:54:03,600
So in chess or in Go, you get a reward for making a certain move.

930
00:54:03,600 --> 00:54:07,600
Now, the steps of the utility function that are described by the Hutter definition

931
00:54:07,600 --> 00:54:14,600
of artificial intelligence, there are a linear sequence of discrete machine actions.

932
00:54:14,600 --> 00:54:21,600
But human motor acts are actually interactions of perception motor activity.

933
00:54:21,600 --> 00:54:25,600
They involve at every stage a dense synergy of multiple body systems

934
00:54:25,600 --> 00:54:27,600
at multiple levels of granularity.

935
00:54:27,600 --> 00:54:34,600
And that's, for example, if you think of human manufacturing activities

936
00:54:34,600 --> 00:54:39,600
where they have to use, like surgery, when you have to actually

937
00:54:39,600 --> 00:54:46,600
feel very exactly what you're doing or in certain steps in the construction

938
00:54:46,600 --> 00:54:50,600
of even of cars, fine motor, it's called in German.

939
00:54:50,600 --> 00:54:53,600
Barry, do you know the word in English?

940
00:54:54,600 --> 00:54:59,600
I don't. Sorry. Precision engineering would be...

941
00:54:59,600 --> 00:55:01,600
Yeah, probably.

942
00:55:01,600 --> 00:55:06,600
So we don't know how to make machines do this because we don't know

943
00:55:06,600 --> 00:55:13,600
how the circuitry between a perception and motor action work.

944
00:55:13,600 --> 00:55:16,600
So we know that there is some feedback loop, blah, blah, blah,

945
00:55:16,600 --> 00:55:18,600
but we don't know the details of it.

946
00:55:18,600 --> 00:55:23,600
And if you look at the most advanced textbooks or papers that are available

947
00:55:23,600 --> 00:55:32,600
about animal fine motor action, we have no clue how the animals

948
00:55:32,600 --> 00:55:35,600
do these fine motor actions or know how we do it.

949
00:55:35,600 --> 00:55:37,600
We have no models for it.

950
00:55:37,600 --> 00:55:42,600
And so because we have no models for it, we cannot do it in a computer.

951
00:55:42,600 --> 00:55:47,600
And so the activities that happen in real environments

952
00:55:47,600 --> 00:55:52,600
are much more complicated than those linear sequence of steps.

953
00:55:52,600 --> 00:55:57,600
And the interaction between sensory and motor activity

954
00:55:57,600 --> 00:56:01,600
could potentially scrub the linear sequence of effort, effort,

955
00:56:01,600 --> 00:56:05,600
and interest in neural signal events, but the coupling of such a sequence

956
00:56:05,600 --> 00:56:08,600
to any sort of reward is very indirect.

957
00:56:08,600 --> 00:56:13,600
So yes, in reality, of course, those things happen one after the other,

958
00:56:13,600 --> 00:56:16,600
but very, very quickly in a very complex session.

959
00:56:16,600 --> 00:56:19,600
And so we don't know how to cover this to reward.

960
00:56:19,600 --> 00:56:25,600
And so probably the sequence in reward terms that we need

961
00:56:25,600 --> 00:56:31,600
would probably not be possible to construct the correct reward sequence.

962
00:56:31,600 --> 00:56:34,600
Another aspect is that mammals can overcome

963
00:56:34,600 --> 00:56:36,600
massive negative rewards to achieve the goal.

964
00:56:36,600 --> 00:56:41,600
So here I have an example from animal psychology.

965
00:56:41,600 --> 00:56:44,600
So it's a cocaine self-administration experiment

966
00:56:44,600 --> 00:56:49,600
where the rats to get cocaine, they have to traverse a heated plate,

967
00:56:49,600 --> 00:56:51,600
which is burning their feet.

968
00:56:51,600 --> 00:56:54,600
So they have to damage themselves to get to the cocaine,

969
00:56:54,600 --> 00:57:00,600
but they still do it because they have primary intelligence.

970
00:57:00,600 --> 00:57:04,600
So they know that they must cross the heat plate to get to the cocaine,

971
00:57:04,600 --> 00:57:09,600
that they must press a button to get the administration of cocaine.

972
00:57:09,600 --> 00:57:12,600
And they get a short-term reward for this,

973
00:57:12,600 --> 00:57:15,600
but they don't have a net long-term reward.

974
00:57:15,600 --> 00:57:21,600
And so such a behavior is very, very hard to model with a reward function.

975
00:57:21,600 --> 00:57:26,600
And if you think many of you are maybe in the middle of their PhD thesis,

976
00:57:26,600 --> 00:57:32,600
so the PhD thesis process is not like the cocaine self-administration,

977
00:57:32,600 --> 00:57:34,600
but there's a lot of negative stuff you have to cope with

978
00:57:34,600 --> 00:57:38,600
over a long time before you get a reward, that's for sure.

979
00:57:38,600 --> 00:57:42,600
And I don't know how this can be modeled with such a reward model,

980
00:57:42,600 --> 00:57:44,600
at least being very hard.

981
00:57:44,600 --> 00:57:46,600
Speaking of reward,

982
00:57:46,600 --> 00:57:51,600
so what is even more important is that the reward pattern

983
00:57:51,600 --> 00:57:54,600
that we see is some of a reward.

984
00:57:54,600 --> 00:58:01,600
It is actually unable to model reward patterns that we encounter in real life,

985
00:58:01,600 --> 00:58:06,600
because first of all, the system that certifies the harder definition

986
00:58:06,600 --> 00:58:08,600
will always be situation-specific.

987
00:58:08,600 --> 00:58:12,600
So it will work only in the context where a human has already been at work

988
00:58:12,600 --> 00:58:15,600
in preparing appropriate rewards.

989
00:58:15,600 --> 00:58:17,600
There is no general or universal reward.

990
00:58:17,600 --> 00:58:22,600
So the reward, for example, that the machine receives for playing the game of Go,

991
00:58:22,600 --> 00:58:24,600
are points.

992
00:58:24,600 --> 00:58:27,600
And the algorithm is trying to find a functional or an operator

993
00:58:27,600 --> 00:58:29,600
that maximizes the number of points.

994
00:58:29,600 --> 00:58:34,600
So it's just a derivative of a very long equation that you have to find.

995
00:58:35,600 --> 00:58:40,600
And that has nothing to do with universal or general intelligence.

996
00:58:40,600 --> 00:58:45,600
The reason is that the mathematical definition that Hatter provides

997
00:58:45,600 --> 00:58:51,600
for the environment mu that must be matched by the reward.

998
00:58:51,600 --> 00:58:56,600
So it's not possible to find a reward that works for all environments,

999
00:58:56,600 --> 00:58:59,600
whereas humans have as a reward,

1000
00:58:59,600 --> 00:59:04,600
the main rewards are to survive and to reproduce.

1001
00:59:04,600 --> 00:59:13,600
And survival and reproduction can come very indirectly in highly evolved societies.

1002
00:59:13,600 --> 00:59:19,600
So to sit in a room and do mathematical equations all day long

1003
00:59:19,600 --> 00:59:23,600
for survival and reproduction, that's quite abstract

1004
00:59:23,600 --> 00:59:27,600
or to do paintings of art or to compose music.

1005
00:59:27,600 --> 00:59:36,600
And so there is a way of humans to delay the reward of reproduction and survival

1006
00:59:36,600 --> 00:59:42,600
very far off and to do activities that seem to be non-connected to it.

1007
00:59:42,600 --> 00:59:47,600
And that seems to be at least required for objectifying intelligence or human intelligence.

1008
00:59:47,600 --> 00:59:52,600
And we don't know how we can model this with reward.

1009
00:59:52,600 --> 01:00:02,600
Also further problem arises that the assumption is made that all rewards of one agent

1010
01:00:02,600 --> 01:00:08,600
must be of the same type for every step under a given environment.

1011
01:00:08,600 --> 01:00:13,600
So if we go back to the equation, r is only indexed by the step,

1012
01:00:13,600 --> 01:00:18,600
but it cannot change its type, otherwise it would need the second index.

1013
01:00:18,600 --> 01:00:25,600
So there's only one type. Yes, this type can obtain different values,

1014
01:00:25,600 --> 01:00:30,600
but probably in animal and human behavior there are many different types of rewards.

1015
01:00:30,600 --> 01:00:34,600
So we cannot model this.

1016
01:00:34,600 --> 01:00:37,600
Then the question is, couldn't we create a sequence of rewards

1017
01:00:37,600 --> 01:00:41,600
adequate for learning the behavior of a complex system?

1018
01:00:41,600 --> 01:00:45,600
So the problem here would be that such a reward sequence,

1019
01:00:45,600 --> 01:00:48,600
you would then imagine many, many rewards in a sequence

1020
01:00:48,600 --> 01:00:52,600
and they would have to give situation-specific rewards.

1021
01:00:52,600 --> 01:00:56,600
That's because each step on a complex system model trajectory

1022
01:00:56,600 --> 01:01:01,600
would have to be able to deal with an unexpected situation

1023
01:01:01,600 --> 01:01:05,600
because when the AI system interacts with its own environment,

1024
01:01:05,600 --> 01:01:09,600
that will change the environment.

1025
01:01:09,600 --> 01:01:14,600
And so this change of the environment will create an unexpected situation

1026
01:01:14,600 --> 01:01:16,600
and may require different reward.

1027
01:01:16,600 --> 01:01:20,600
So you cannot really find a reward part or trajectory

1028
01:01:20,600 --> 01:01:24,600
because at each step a different reward would be needed to correspond

1029
01:01:24,600 --> 01:01:28,600
to the emanations from the complex system that form the environment.

1030
01:01:28,600 --> 01:01:33,600
So that has to do with the evolutionary character of complex systems.

1031
01:01:33,600 --> 01:01:38,600
And such a temporal reward sequence would obviously not follow a Markovian pattern.

1032
01:01:38,600 --> 01:01:43,600
And of course, speaking of probability theory, if we go back to this slide here,

1033
01:01:43,600 --> 01:01:50,600
Markovian pattern is of course a pattern that applies only to classical Newtonian systems.

1034
01:01:50,600 --> 01:01:53,600
Only Newtonian systems have the Markov property.

1035
01:01:53,600 --> 01:01:58,600
Complex systems don't have the Markov property, but without the Markov property,

1036
01:01:58,600 --> 01:02:03,600
you cannot achieve predictive modeling in stochastic equations.

1037
01:02:03,600 --> 01:02:07,600
So stochastic differential equations or other stochastic process models,

1038
01:02:07,600 --> 01:02:09,600
they always need a Markov pattern.

1039
01:02:09,600 --> 01:02:20,600
And without the Markov pattern in which the reward would depend only on the previous or some previous steps.

1040
01:02:20,600 --> 01:02:24,600
But if you don't have this pattern, but you have a dependency on many earlier steps

1041
01:02:24,600 --> 01:02:28,600
and long-term dispositions of the organism and also short-term intentions,

1042
01:02:28,600 --> 01:02:31,600
you can't find a reward sequence.

1043
01:02:31,600 --> 01:02:44,600
And so, therefore, the reward sequence would need to correspond to complex emanations

1044
01:02:44,600 --> 01:02:48,600
relating to situations varying as a successive test unfold.

1045
01:02:48,600 --> 01:02:53,600
And therefore, the reward sequence itself has to be complex

1046
01:02:53,600 --> 01:02:57,600
and thereby it would have all the properties of a complex system emanation.

1047
01:02:57,600 --> 01:03:05,600
So that's the interesting thing, that to give rewards to an intelligent system in a complex setting,

1048
01:03:05,600 --> 01:03:10,600
the reward sequence itself would have all the properties of a complex system emanation.

1049
01:03:10,600 --> 01:03:16,600
So it would have the same properties that my stream of language that I'm currently giving to you has.

1050
01:03:16,600 --> 01:03:21,600
And we have no mathematical models to create such sequences

1051
01:03:21,600 --> 01:03:26,600
because we have no mathematical models for complex system emanations.

1052
01:03:26,600 --> 01:03:33,600
And so we couldn't create the reward sequence that would be needed for the intelligent system

1053
01:03:33,600 --> 01:03:36,600
to cope with a complex system situation.

1054
01:03:36,600 --> 01:03:40,600
And that's a problem, I think, a very important problem to see this,

1055
01:03:40,600 --> 01:03:45,600
that the reward approach, why reward is mathematically attractive

1056
01:03:45,600 --> 01:03:54,600
because it allows to state the intelligence problem like an optimization problem for optimization theory.

1057
01:03:54,600 --> 01:04:00,600
It doesn't create a realistic sequence of rewards that correspond in any way

1058
01:04:00,600 --> 01:04:05,600
to what we experience or animals experience when they obtain rewards for their behavior.

1059
01:04:05,600 --> 01:04:11,600
Actually, the reward that an animal and foraging animal receives, which is the food

1060
01:04:11,600 --> 01:04:14,600
and for which the animal does very interesting things.

1061
01:04:14,600 --> 01:04:18,600
So if you read modern books about foraging, it has been found out that parrots, for example,

1062
01:04:18,600 --> 01:04:31,600
invent new patterns of shouts and vocal noises they make to describe sources for food.

1063
01:04:31,600 --> 01:04:38,600
And while they search for those food sources, which they have to do every day,

1064
01:04:38,600 --> 01:04:41,600
they create those sound patterns.

1065
01:04:41,600 --> 01:04:45,600
And this process is highly complex and it has nothing to do.

1066
01:04:45,600 --> 01:04:50,600
And the way they get to the reward has nothing to do at all with linear reward sequence

1067
01:04:50,600 --> 01:04:53,600
that we see in those models.

1068
01:04:53,600 --> 01:04:56,600
And so that's why I call these pseudo-definitions of intelligence

1069
01:04:56,600 --> 01:05:06,600
because they just define something that can be actually put into a model optimization algorithm,

1070
01:05:06,600 --> 01:05:14,600
a numerical model optimization algorithm, like a dual or something like this.

1071
01:05:14,600 --> 01:05:20,600
But in reality, it has nothing to do with real intelligence, not even animal intelligence.

1072
01:05:20,600 --> 01:05:26,600
Now, I think before I go to the last slide, I would like to give you the opportunity

1073
01:05:26,600 --> 01:05:30,600
to ask one more round of questions.

1074
01:05:30,600 --> 01:05:33,600
Yeah, I have one question, Professor.

1075
01:05:34,600 --> 01:05:43,600
The thing is, so here the argument is that the reward sequence is extremely complex

1076
01:05:43,600 --> 01:05:48,600
and since it is not Markovian, you cannot somehow model this.

1077
01:05:48,600 --> 01:05:54,600
Now, if I were to take the example of a robot in a factory,

1078
01:05:54,600 --> 01:05:57,600
let's say a robot that knows how to load boxes, unload boxes,

1079
01:05:57,600 --> 01:06:01,600
get the boxes onto some other conveyor belt, et cetera, et cetera.

1080
01:06:01,600 --> 01:06:06,600
Currently, there are reinforcement learning algorithms that are able to do this

1081
01:06:06,600 --> 01:06:12,600
pretty effectively in these factories, just replacing entirely with robots that do this work.

1082
01:06:12,600 --> 01:06:16,600
So there, of course, the reward sequence is very clear

1083
01:06:16,600 --> 01:06:21,600
because you have just one mechanistic task that you keep doing throughout your life,

1084
01:06:21,600 --> 01:06:24,600
well, throughout your robot life.

1085
01:06:24,600 --> 01:06:34,600
My question here was, what if we could create a set of rewards for different tasks

1086
01:06:34,600 --> 01:06:37,600
and model that together into a specific robot?

1087
01:06:37,600 --> 01:06:40,600
So, for example, let's say loading and loading boxes, opening doors,

1088
01:06:40,600 --> 01:06:43,600
walking, sitting down, different movements.

1089
01:06:43,600 --> 01:06:47,600
Now, I'm not suggesting for a single second that humans learn this

1090
01:06:47,600 --> 01:06:50,600
by this sort of discretized reward sequence.

1091
01:06:50,600 --> 01:06:55,600
I'm not suggesting this at all because I don't know how humans learn it.

1092
01:06:55,600 --> 01:06:58,600
But as far as teaching robots that is concerned,

1093
01:06:58,600 --> 01:07:03,600
don't you think the reward sequence then would just be a set of different rewards

1094
01:07:03,600 --> 01:07:07,600
for different tasks, but they would all be modeled into the same algorithm.

1095
01:07:07,600 --> 01:07:11,600
So, in a sense, what I'm arguing is the whole is the sum of its parts.

1096
01:07:11,600 --> 01:07:13,600
That's what I'm trying to say.

1097
01:07:13,600 --> 01:07:19,600
So, you're actually already giving a small preview of the next slide.

1098
01:07:19,600 --> 01:07:23,600
So, it's never what you're saying.

1099
01:07:23,600 --> 01:07:29,600
So, basically, what you describe is, of course, that what is a factory?

1100
01:07:29,600 --> 01:07:32,600
A factory is a Newtonian system.

1101
01:07:32,600 --> 01:07:35,600
So, a factory has all those properties.

1102
01:07:35,600 --> 01:07:41,600
And so, therefore, machine learning can be very, very efficient

1103
01:07:41,600 --> 01:07:47,600
in implicitly modeling Newtonian equations, motion equations, also sensory equations.

1104
01:07:47,600 --> 01:07:54,600
So, therefore, because in a factory, all processes have the Markov property

1105
01:07:54,600 --> 01:07:59,600
and because there's no force overlay, the elements are well defined and so on,

1106
01:07:59,600 --> 01:08:01,600
that will work pretty well.

1107
01:08:01,600 --> 01:08:06,600
And that's why also machine learning is, I think, the best application of machine learning

1108
01:08:06,600 --> 01:08:12,600
that is currently not visible very much is actually manufacturing and mining

1109
01:08:12,600 --> 01:08:18,600
and other mechanical activities where humans still play a role,

1110
01:08:18,600 --> 01:08:23,600
but where they will with the exception of this fine motor or sensoric behavior,

1111
01:08:23,600 --> 01:08:28,600
which is very hard to model, they will be replaced by robots more and more.

1112
01:08:28,600 --> 01:08:34,600
You're completely right, and that's because here, machine learning is used to model Newtonian systems.

1113
01:08:34,600 --> 01:08:40,600
That's the first answer. The second answer is that it's essentially the way to build AI properly

1114
01:08:40,600 --> 01:08:46,600
is to do it as you described, and I will come to this once the other questions are answered.

1115
01:08:46,600 --> 01:08:52,600
Oh, okay, okay. Thank you so much.

1116
01:08:52,600 --> 01:08:54,600
Is there any?

1117
01:08:54,600 --> 01:08:58,600
I have one, but I'll give the students chance to butt in first.

1118
01:09:07,600 --> 01:09:13,600
I would just remark maybe that the authors were aware of this argument that you are saying

1119
01:09:13,600 --> 01:09:22,600
because they said that at no point they are trying to compare their definition to human intelligence.

1120
01:09:22,600 --> 01:09:32,600
I think they perceive the machine intelligence just as a composite of different tasks

1121
01:09:32,600 --> 01:09:36,600
as Ravidi was saying earlier.

1122
01:09:36,600 --> 01:09:43,600
If you read Hutter's and Schmitt-Huber's papers about general intelligence, I disagree.

1123
01:09:43,600 --> 01:09:48,600
They believe that they can create general intelligence, and many of them, I don't know whether Schmitt-Huber does,

1124
01:09:48,600 --> 01:09:55,600
but many of them also believe in the singularity, which I think shows.

1125
01:09:55,600 --> 01:10:01,600
I think that you should really get this AGI book, or I can send it to you, or Barry can send it to you,

1126
01:10:01,600 --> 01:10:11,600
this AGI book by Gertse Penachin, which is one of the most important consensus readers

1127
01:10:11,600 --> 01:10:14,600
where all the big shots of AGI have published papers.

1128
01:10:14,600 --> 01:10:19,600
Yes, of course they say that it's not human intelligence, but they say it's a real intelligence.

1129
01:10:19,600 --> 01:10:26,600
What I'm saying is no, what you're suggesting is not a real intelligence, but it's just a kind of amoeba intelligence,

1130
01:10:26,600 --> 01:10:28,600
and probably not even that.

1131
01:10:28,600 --> 01:10:36,600
I think that because on the next slide you will see how I define what one can do with AGI,

1132
01:10:36,600 --> 01:10:41,600
and when I show this to people from this community, I get heavily attacked.

1133
01:10:41,600 --> 01:10:46,600
Oh, that's old school, we can do much better, we can do general intelligence, and so on.

1134
01:10:46,600 --> 01:10:57,600
If I understood correctly, they were also saying that, of course, the Chinese argument is completely right.

1135
01:10:57,600 --> 01:11:07,600
I think where they were defending the critique, maybe that's at the end.

1136
01:11:08,600 --> 01:11:12,600
But I don't even argue with the Chinese room argument here.

1137
01:11:12,600 --> 01:11:20,600
I just say that even the most basic form of intelligence that we as humans perceive as intelligent

1138
01:11:20,600 --> 01:11:29,600
would just see the behavior of a dog or another mammal, that this behavior cannot never be achieved by these equations.

1139
01:11:29,600 --> 01:11:36,600
So what can be achieved by these equations is actually what neural networks already do,

1140
01:11:36,600 --> 01:11:47,600
and that is because this here is a recipe to actually...

1141
01:11:47,600 --> 01:11:55,600
Yeah, and so loss function minimizes loss, and here we maximize reward, but it's basically optimization, prescription,

1142
01:11:55,600 --> 01:12:04,600
and we achieve this by applying these models to achieve what is shown on slide three, with all the pros and cons.

1143
01:12:04,600 --> 01:12:09,600
Now, I'm saying that these ML models are very useful, but they have nothing to do with intelligence,

1144
01:12:09,600 --> 01:12:14,600
and that's basically...

1145
01:12:14,600 --> 01:12:19,600
I mean, when I discuss, you know that I have an AI company, when I discuss with clever customers,

1146
01:12:19,600 --> 01:12:23,600
I mean their customers who want to buy intelligence, and I let them under the illusion,

1147
01:12:23,600 --> 01:12:28,600
but others who are clever to say, but that's not really intelligence, I say, no, these are artificial instincts.

1148
01:12:29,600 --> 01:12:38,600
And this is basically what we do, what we still do, even if we use such optimization procedures as those described by these models here,

1149
01:12:38,600 --> 01:12:41,600
we still only obtain a narrow AI.

1150
01:12:41,600 --> 01:12:45,600
And with this, Barry, you had a question before I go through the narrow AI.

1151
01:12:45,600 --> 01:12:49,600
Yes, so I have a number of questions now, unfortunately.

1152
01:12:49,600 --> 01:12:53,600
So what would leg Hutter say to the following objection?

1153
01:12:53,600 --> 01:12:59,600
You define intelligence as the ability to achieve rewards in a wide range of environments,

1154
01:12:59,600 --> 01:13:05,600
but AlphaGo can only achieve rewards in one kind of environment, which is the Go board.

1155
01:13:05,600 --> 01:13:07,600
Yeah.

1156
01:13:07,600 --> 01:13:10,600
So it's not a wide range at all.

1157
01:13:10,600 --> 01:13:12,600
Yeah, yeah.

1158
01:13:12,600 --> 01:13:16,600
Yeah, I believe that even actually, you're right.

1159
01:13:16,600 --> 01:13:22,600
So I believe that even this definition, intelligence so will not be achievable.

1160
01:13:22,600 --> 01:13:27,600
So the verbal definition is actually in conflict with the equations.

1161
01:13:27,600 --> 01:13:29,600
Yeah, you're right.

1162
01:13:29,600 --> 01:13:32,600
And now just a little correction, sorry.

1163
01:13:32,600 --> 01:13:34,600
Sorry, just a little correction.

1164
01:13:34,600 --> 01:13:39,600
AlphaZero actually can succeed in a variety of game environments.

1165
01:13:39,600 --> 01:13:43,600
AlphaZero can play chess, it can play Go, it can play checker.

1166
01:13:43,600 --> 01:13:45,600
I'm just talking about AlphaGo.

1167
01:13:45,600 --> 01:13:47,600
You're right.

1168
01:13:47,600 --> 01:13:50,600
Not AlphaGo, but AlphaZero, which was recently released.

1169
01:13:50,600 --> 01:13:53,600
Yeah, AlphaZero can play different games.

1170
01:13:53,600 --> 01:13:57,600
And actually, there was already in 2014 a precursor for AlphaZero,

1171
01:13:57,600 --> 01:14:04,600
which could play all the Atari games, which was also trained with reward learning.

1172
01:14:04,600 --> 01:14:08,600
It could play strategic games, but it could play Pong and all the ones

1173
01:14:08,600 --> 01:14:10,600
that give you a series of points.

1174
01:14:10,600 --> 01:14:14,600
But now OpenAI has come out with StarCraft games too.

1175
01:14:14,600 --> 01:14:17,600
So they are getting better at these complex environments.

1176
01:14:17,600 --> 01:14:21,600
Well, these are actually not complex environments.

1177
01:14:21,600 --> 01:14:25,600
So these are still Newtonian environments.

1178
01:14:25,600 --> 01:14:31,600
Because the reason this is very important, if you could hold on with your questions a minute.

1179
01:14:31,600 --> 01:14:35,600
So the reason why these are Newtonian environments is that also,

1180
01:14:35,600 --> 01:14:39,600
for example, games like strategy games, like civilization,

1181
01:14:39,600 --> 01:14:42,600
they have also been beat by reinforcement learning.

1182
01:14:42,600 --> 01:14:44,600
But the reason is, of course, what is civilization?

1183
01:14:44,600 --> 01:14:51,600
Civilization is a set of rules and equations applied to a certain pattern.

1184
01:14:51,600 --> 01:14:55,600
And even if this pattern is created by random, it's created as a multivariate

1185
01:14:55,600 --> 01:14:58,600
random distribution, usually multivariate.

1186
01:14:58,600 --> 01:15:01,600
And so it's multivariate Gaussian.

1187
01:15:01,600 --> 01:15:05,600
And then, of course, you can take points from this distribution

1188
01:15:05,600 --> 01:15:07,600
and put them into a set of rules.

1189
01:15:07,600 --> 01:15:12,600
Then you create a very complex set of events, but it's still a Newtonian universe.

1190
01:15:12,600 --> 01:15:16,600
And therefore, you can also train in the same place for the ego shooter games.

1191
01:15:16,600 --> 01:15:20,600
And therefore, you can train AI that is beating this.

1192
01:15:20,600 --> 01:15:25,600
But the point is, since 1950, we have heard predictions about free-moving robots.

1193
01:15:25,600 --> 01:15:27,600
Why don't we have them?

1194
01:15:27,600 --> 01:15:29,600
I mean, we have them, of course, in controlled environments.

1195
01:15:29,600 --> 01:15:33,600
But why do we never encounter free-moving robots in our streets?

1196
01:15:33,600 --> 01:15:36,600
Because these are real complex environments.

1197
01:15:36,600 --> 01:15:38,600
Yeah, yeah, yeah.

1198
01:15:38,600 --> 01:15:40,600
Thank you.

1199
01:15:40,600 --> 01:15:43,600
I just have one quick question for anybody.

1200
01:15:43,600 --> 01:15:52,600
So why do they use the word universal in the title of the Laguta paper?

1201
01:15:52,600 --> 01:15:55,600
Because it sounds good.

1202
01:15:55,600 --> 01:15:57,600
All right.

1203
01:15:57,600 --> 01:16:02,600
Okay, that's what I guess the answer would be.

1204
01:16:02,600 --> 01:16:03,600
I'm not sure.

1205
01:16:03,600 --> 01:16:05,600
Jopster, am I wrong?

1206
01:16:05,600 --> 01:16:06,600
Please, correct me.

1207
01:16:06,600 --> 01:16:07,600
Probably.

1208
01:16:07,600 --> 01:16:13,600
I mean, there's a wonderful paper by Johannes from 2005,

1209
01:16:13,600 --> 01:16:17,600
which says why most of scientific research results are wrong.

1210
01:16:17,600 --> 01:16:21,600
And the biggest reason is, of course, bias.

1211
01:16:21,600 --> 01:16:23,600
And sounding good is also a form of bias.

1212
01:16:23,600 --> 01:16:26,600
So anyhow, but I'm not a pessimist for AI.

1213
01:16:26,600 --> 01:16:29,600
I remember that I made my little trunk for my life.

1214
01:16:29,600 --> 01:16:33,600
Jopster, we have one student, Peter Bottaroni,

1215
01:16:33,600 --> 01:16:35,600
who would like to ask a question.

1216
01:16:35,600 --> 01:16:36,600
Yeah, please go ahead.

1217
01:16:36,600 --> 01:16:37,600
Yes, thank you.

1218
01:16:37,600 --> 01:16:39,600
Actually, there are two questions.

1219
01:16:39,600 --> 01:16:44,600
The first is related with the discussion with the RID.

1220
01:16:44,600 --> 01:16:49,600
In the sense, okay, then we say that a complex environment

1221
01:16:49,600 --> 01:16:51,600
is an environment that is not...

1222
01:16:51,600 --> 01:16:54,600
We cannot model mathematically, right?

1223
01:16:54,600 --> 01:16:56,600
Yes, yes, basically, yes.

1224
01:16:56,600 --> 01:16:59,600
Okay, then every complex environment cannot be something

1225
01:16:59,600 --> 01:17:01,600
that we create with a software.

1226
01:17:01,600 --> 01:17:06,600
Then a complex environment should be a real environment,

1227
01:17:06,600 --> 01:17:09,600
in the sense that everything that is...

1228
01:17:09,600 --> 01:17:12,600
Even the more complex game that we can play

1229
01:17:12,600 --> 01:17:17,600
is not considered a complex environment, right?

1230
01:17:17,600 --> 01:17:21,600
Unless what we sometimes have is that we have

1231
01:17:21,600 --> 01:17:24,600
natural human behavior integrated as movies

1232
01:17:24,600 --> 01:17:26,600
into an environment, into a game.

1233
01:17:26,600 --> 01:17:29,600
And then the player in some strategy games,

1234
01:17:29,600 --> 01:17:31,600
which I used to play 20 years ago,

1235
01:17:31,600 --> 01:17:33,600
there was a movie sequence in the game embedded,

1236
01:17:33,600 --> 01:17:36,600
and you had to interpret the movie scene correctly

1237
01:17:36,600 --> 01:17:38,600
to continue to play the game.

1238
01:17:38,600 --> 01:17:41,600
And of course, the behavior of humans during that scene,

1239
01:17:41,600 --> 01:17:44,600
because this was a film of naturally behaving humans,

1240
01:17:44,600 --> 01:17:46,600
where they were playing a role, but still,

1241
01:17:46,600 --> 01:17:49,600
that was, of course, complex system behavior,

1242
01:17:49,600 --> 01:17:51,600
but then the rest of the game was not,

1243
01:17:51,600 --> 01:17:53,600
and that's still like it.

1244
01:17:53,600 --> 01:17:56,600
Poker would be a complex game, would it not?

1245
01:17:56,600 --> 01:18:01,600
Poker, or yes, poker played with humans is a complex game.

1246
01:18:01,600 --> 01:18:03,600
Which one? Sorry?

1247
01:18:03,600 --> 01:18:06,600
If you play a round of poker against human opponents...

1248
01:18:06,600 --> 01:18:09,600
Ah, okay, okay, yeah, yeah, yeah, yeah, okay.

1249
01:18:09,600 --> 01:18:13,600
But chess with human opponents is not a complex game.

1250
01:18:13,600 --> 01:18:16,600
Correct. Yeah, good.

1251
01:18:16,600 --> 01:18:21,600
And the second question is related with the reward definition,

1252
01:18:21,600 --> 01:18:24,600
in the sense that, at least to me,

1253
01:18:24,600 --> 01:18:29,600
it seems that we define the rewards with something that is given.

1254
01:18:29,600 --> 01:18:32,600
We define the reward for the robot or for the machine.

1255
01:18:32,600 --> 01:18:33,600
Yes.

1256
01:18:33,600 --> 01:18:37,600
Actually, the human sometimes creates some reward or some goal

1257
01:18:37,600 --> 01:18:41,600
by itself, maybe some sub-goal or sub-reward

1258
01:18:41,600 --> 01:18:45,600
in order to achieve a bigger goal.

1259
01:18:45,600 --> 01:18:46,600
Correct.

1260
01:18:46,600 --> 01:18:51,600
Is it possible to model this creation of intermediate goal

1261
01:18:51,600 --> 01:18:54,600
or final goal as a machine, according to you?

1262
01:18:54,600 --> 01:18:56,600
Yes, so it has already been done.

1263
01:18:56,600 --> 01:18:59,600
So you can, for example, you can have, as you know,

1264
01:18:59,600 --> 01:19:01,600
you can have meta models in machine learning.

1265
01:19:01,600 --> 01:19:05,600
So you can have, for example, several reinforcement learning models

1266
01:19:05,600 --> 01:19:10,600
that use different reinforcement rewards from a choice of reward.

1267
01:19:11,600 --> 01:19:15,600
And so you can optimize from a certain reward.

1268
01:19:15,600 --> 01:19:18,600
You can try out several reward types in parallel

1269
01:19:18,600 --> 01:19:22,600
and find the best model by varying the reward type.

1270
01:19:22,600 --> 01:19:26,600
And then you can use adversarial learning, maybe,

1271
01:19:26,600 --> 01:19:29,600
to drive the choice of the reward type, and so on.

1272
01:19:29,600 --> 01:19:31,600
Yeah, so there are ways to do this,

1273
01:19:31,600 --> 01:19:36,600
but they will always only solve problems in non-compact environment.

1274
01:19:36,600 --> 01:19:37,600
Okay.

1275
01:19:37,600 --> 01:19:39,600
Okay, thank you.

1276
01:19:39,600 --> 01:19:40,600
Good.

1277
01:19:40,600 --> 01:19:43,600
So now last slide.

1278
01:19:43,600 --> 01:19:46,600
So you remember that I'm making money with AI

1279
01:19:46,600 --> 01:19:47,600
and I'm a big fan of AI.

1280
01:19:47,600 --> 01:19:51,600
It's not that I'm negative against it, yeah?

1281
01:19:51,600 --> 01:19:54,600
So what can we do with the analytical engine?

1282
01:19:54,600 --> 01:19:56,600
So the analytical engine is the first name

1283
01:19:56,600 --> 01:19:58,600
that was given to a Turing machine.

1284
01:19:58,600 --> 01:20:02,600
Charles Babbage was the one who built the first computer

1285
01:20:02,600 --> 01:20:05,600
in the 19th century, around 1850,

1286
01:20:05,600 --> 01:20:08,600
and it could already do computations.

1287
01:20:09,600 --> 01:20:19,600
And Anna Lovelace said that an analytical engine

1288
01:20:19,600 --> 01:20:21,600
has no pretensions to originate anything.

1289
01:20:21,600 --> 01:20:26,600
It can only do whatever we know how to order it to perform.

1290
01:20:26,600 --> 01:20:32,600
And so what is now, is that also true for the Turing machine?

1291
01:20:32,600 --> 01:20:34,600
So Alan Turing says that, of course,

1292
01:20:34,600 --> 01:20:37,600
an analytical engine is a Turing machine.

1293
01:20:37,600 --> 01:20:39,600
And what can we do with it?

1294
01:20:39,600 --> 01:20:43,600
And so I think we can do a lot.

1295
01:20:43,600 --> 01:20:45,600
And we can do what one of you just said,

1296
01:20:45,600 --> 01:20:47,600
namely we can engineer by composition.

1297
01:20:47,600 --> 01:20:51,600
So that's an outcome that we want to achieve.

1298
01:20:51,600 --> 01:20:56,600
And here you see many operators and functionals

1299
01:20:56,600 --> 01:20:58,600
which are chained together.

1300
01:20:59,600 --> 01:21:06,600
And these are an upper case data as an operator

1301
01:21:06,600 --> 01:21:08,600
and a lower case data as a function.

1302
01:21:08,600 --> 01:21:12,600
For the non-mathematicians, a function is a relation

1303
01:21:12,600 --> 01:21:21,600
that takes an input vector and creates an output number.

1304
01:21:21,600 --> 01:21:24,600
The operator can also create an output vector

1305
01:21:24,600 --> 01:21:26,600
from an input vector.

1306
01:21:26,600 --> 01:21:29,600
So they act together here in a chain

1307
01:21:29,600 --> 01:21:34,600
to yield the final result, which is y or t hat.

1308
01:21:34,600 --> 01:21:40,600
And the superscripts that you can see here,

1309
01:21:40,600 --> 01:21:46,600
Teta, Kappa, Lambda, there are prior knowledge

1310
01:21:46,600 --> 01:21:48,600
that can be configured into the functions

1311
01:21:48,600 --> 01:21:51,600
and operated explicitly or via training tools.

1312
01:21:51,600 --> 01:21:56,600
So for example, this could be just a set of rules

1313
01:21:56,600 --> 01:21:58,600
and this could be the parallelization of the rules.

1314
01:21:58,600 --> 01:22:00,600
This could be a function that has been trained

1315
01:22:00,600 --> 01:22:04,600
like a spam filter and where the parameter indicates

1316
01:22:04,600 --> 01:22:06,600
which training tools for you.

1317
01:22:06,600 --> 01:22:09,600
And so I believe that it's very important

1318
01:22:09,600 --> 01:22:11,600
that you make the prior knowledge

1319
01:22:11,600 --> 01:22:13,600
that you gave to the machine explicit.

1320
01:22:13,600 --> 01:22:17,600
So either you make it explicit if you have, for example,

1321
01:22:17,600 --> 01:22:19,600
mechanical theory improving component,

1322
01:22:19,600 --> 01:22:22,600
let's say this one here, or maybe this one

1323
01:22:22,600 --> 01:22:24,600
would be mechanical theory improver,

1324
01:22:24,600 --> 01:22:26,600
then you need some axioms for the mechanical theory improver.

1325
01:22:26,600 --> 01:22:28,600
And those axioms are its configuration

1326
01:22:28,600 --> 01:22:31,600
and you need to know which axioms you give to it

1327
01:22:31,600 --> 01:22:33,600
so that you know how it will behave.

1328
01:22:33,600 --> 01:22:36,600
Or this would be a functional, now the functional,

1329
01:22:36,600 --> 01:22:38,600
of course, is trained by using training tools

1330
01:22:38,600 --> 01:22:40,600
and other meta parameter.

1331
01:22:40,600 --> 01:22:43,600
They are this and also those you have to know pretty well.

1332
01:22:43,600 --> 01:22:46,600
So for example, we do customer correspondence automation,

1333
01:22:46,600 --> 01:22:48,600
but we cannot use the way that,

1334
01:22:48,600 --> 01:22:51,600
so if a company gives us an email that it received

1335
01:22:51,600 --> 01:22:54,600
and also gives us a reaction to the email that they created,

1336
01:22:54,600 --> 01:22:57,600
we cannot use these two bits for training

1337
01:22:57,600 --> 01:23:00,600
because the outcomes that are created by the company are erratic.

1338
01:23:00,600 --> 01:23:02,600
That has to do with many factors.

1339
01:23:02,600 --> 01:23:04,600
It has to do with human error,

1340
01:23:04,600 --> 01:23:07,600
but it has also to do with, for example,

1341
01:23:07,600 --> 01:23:11,600
they have a period where they have understaffed.

1342
01:23:11,600 --> 01:23:15,600
So they just give a stereotype answer to all the letters

1343
01:23:15,600 --> 01:23:17,600
and later on, and they say,

1344
01:23:17,600 --> 01:23:19,600
we will get back to you later,

1345
01:23:19,600 --> 01:23:21,600
but we cannot react for the next three weeks.

1346
01:23:21,600 --> 01:23:23,600
So you have a heterogeneous outcome

1347
01:23:23,600 --> 01:23:27,600
which is not very well related to the input.

1348
01:23:27,600 --> 01:23:30,600
And so therefore, you have to very carefully curate

1349
01:23:30,600 --> 01:23:32,600
the training material that you use

1350
01:23:32,600 --> 01:23:35,600
to train such complex chains of functions and operators.

1351
01:23:35,600 --> 01:23:37,600
But if you do this,

1352
01:23:37,600 --> 01:23:41,600
then you can achieve very impressive results.

1353
01:23:41,600 --> 01:23:43,600
For example, we have automated

1354
01:23:43,600 --> 01:23:45,600
in the insurance industry,

1355
01:23:45,600 --> 01:23:48,600
there are builds that are, for example,

1356
01:23:48,600 --> 01:23:51,600
created by when a car gets repaired,

1357
01:23:51,600 --> 01:23:53,600
a bill is created.

1358
01:23:53,600 --> 01:23:56,600
We have created an algorithm that can automatically evaluate

1359
01:23:56,600 --> 01:23:59,600
whether the bill is correct.

1360
01:23:59,600 --> 01:24:02,600
And that's very hard because it's usually done by a technician

1361
01:24:02,600 --> 01:24:05,600
who looks at the bill and figures out

1362
01:24:05,600 --> 01:24:08,600
whether the workshop repaired the car in the right way.

1363
01:24:08,600 --> 01:24:10,600
And what we do is that we take the bill

1364
01:24:10,600 --> 01:24:13,600
and transform the bill into mathematical logic.

1365
01:24:13,600 --> 01:24:16,600
And then we have also, we get for this car,

1366
01:24:16,600 --> 01:24:20,600
the car repair instructions by the manufacturer of the car,

1367
01:24:20,600 --> 01:24:23,600
which we also transform into mathematical logic.

1368
01:24:23,600 --> 01:24:26,600
And then we do mechanical theory improving

1369
01:24:26,600 --> 01:24:28,600
to prove for each step

1370
01:24:28,600 --> 01:24:30,600
what this step corresponds to a step

1371
01:24:30,600 --> 01:24:33,600
that is also described in the repair instruction.

1372
01:24:33,600 --> 01:24:36,600
This way we have a mechanical evaluation

1373
01:24:36,600 --> 01:24:39,600
of the correctness of the repair of a car.

1374
01:24:39,600 --> 01:24:41,600
And so this is quite impressive

1375
01:24:41,600 --> 01:24:44,600
because it's a very demanding technical skill

1376
01:24:44,600 --> 01:24:46,600
that I'm not able to perform.

1377
01:24:46,600 --> 01:24:49,600
So I cannot perform what this computer program can

1378
01:24:49,600 --> 01:24:53,600
because I don't know enough about car repair.

1379
01:24:53,600 --> 01:24:56,600
Of course, the computer knows nothing about car repair,

1380
01:24:56,600 --> 01:24:59,600
but it can formalize the repair bill from the workshop

1381
01:24:59,600 --> 01:25:01,600
and it can formalize the repair instruction

1382
01:25:01,600 --> 01:25:03,600
into mathematical logic.

1383
01:25:03,600 --> 01:25:05,600
And then all it needs to do is to make a method

1384
01:25:05,600 --> 01:25:08,600
to establish mathematical, logical equivalence

1385
01:25:08,600 --> 01:25:13,600
between a repair step and an instruction step.

1386
01:25:13,600 --> 01:25:17,600
And this is just an example for what something

1387
01:25:17,600 --> 01:25:20,600
that can achieve with Turing machines.

1388
01:25:20,600 --> 01:25:23,600
And there are many, many other impressive examples.

1389
01:25:23,600 --> 01:25:28,600
But my opinion is that you need to do that intelligent behavior

1390
01:25:28,600 --> 01:25:30,600
is very hard to reproduce with the machine

1391
01:25:30,600 --> 01:25:32,600
that you have to make conscious decision.

1392
01:25:32,600 --> 01:25:34,600
You have to carefully select the training material

1393
01:25:34,600 --> 01:25:38,600
and you usually need a chain of algorithms that work together,

1394
01:25:38,600 --> 01:25:40,600
which you orchestrate somehow.

1395
01:25:40,600 --> 01:25:43,600
And then you can, that requires some work.

1396
01:25:43,600 --> 01:25:46,600
But in the end, it gives you perfect outcomes

1397
01:25:46,600 --> 01:25:50,600
and we have actually asked a German court,

1398
01:25:50,600 --> 01:25:53,600
legal court to evaluate whether it would take the output

1399
01:25:53,600 --> 01:26:00,600
of our algorithm in a lawsuit to represent the opinion

1400
01:26:00,600 --> 01:26:02,600
that usually is given by human expert.

1401
01:26:02,600 --> 01:26:04,600
And they said, the court said, yes, they would take this

1402
01:26:04,600 --> 01:26:07,600
because the quality is that we produce is higher

1403
01:26:07,600 --> 01:26:10,600
than the average human expert's quality.

1404
01:26:10,600 --> 01:26:12,600
So this is just to give you an example

1405
01:26:12,600 --> 01:26:15,600
that I'm a great fan of, I think a lot can be done,

1406
01:26:15,600 --> 01:26:18,600
but it doesn't all work out of itself.

1407
01:26:18,600 --> 01:26:21,600
Humans have to design such machines

1408
01:26:21,600 --> 01:26:26,600
like we've designed planes or cars or other machines.

1409
01:26:26,600 --> 01:26:29,600
And then they can work really well

1410
01:26:29,600 --> 01:26:34,600
and take a lot of hard and sweatshop type of work

1411
01:26:34,600 --> 01:26:37,600
of humans and create a lot of value.

1412
01:26:37,600 --> 01:26:39,600
And this is what I would like to show to you

1413
01:26:39,600 --> 01:26:41,600
and this compositional principle here

1414
01:26:41,600 --> 01:26:44,600
is I think what real AI is about.

1415
01:26:44,600 --> 01:26:48,600
And now it can be very cool to train neural networks

1416
01:26:48,600 --> 01:26:50,600
as one part of this,

1417
01:26:50,600 --> 01:26:53,600
but a neural network alone will usually not do

1418
01:26:53,600 --> 01:26:56,600
and that's why I'm showing an operator at the end

1419
01:26:56,600 --> 01:27:00,600
because this operator is usually a logical operator

1420
01:27:00,600 --> 01:27:03,600
and it makes sure that the result is reliable.

1421
01:27:03,600 --> 01:27:05,600
Because unlike stochastic models,

1422
01:27:05,600 --> 01:27:10,600
logical operators can auto-detect their mistakes.

1423
01:27:10,600 --> 01:27:15,600
And so that's why I'm a big fan of combining stochastic AI

1424
01:27:15,600 --> 01:27:18,600
with good old first-order logic AI,

1425
01:27:18,600 --> 01:27:22,600
which we both use in parallel in my company.

1426
01:27:22,600 --> 01:27:25,600
And so we have people who are specialized in neural networks,

1427
01:27:25,600 --> 01:27:29,600
but also people who are specialized in mathematical logic

1428
01:27:29,600 --> 01:27:32,600
so that we can obtain the precision that is needed

1429
01:27:32,600 --> 01:27:34,600
to automate human activity.

1430
01:27:37,600 --> 01:27:38,600
Very good.

1431
01:27:38,600 --> 01:27:39,600
That's it.

1432
01:27:39,600 --> 01:27:41,600
So thank you, Jost.

