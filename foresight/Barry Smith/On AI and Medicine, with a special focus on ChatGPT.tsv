start	end	text
0	6560	All right, so old-fashioned chatpots are obviously lossy.
6560	10600	They repeat themselves over and over again, and they're bland.
10600	15640	Google Translate is not so obviously lossy until you test it with really difficult text,
15640	17440	and then it fails.
17440	21960	And there are easy ways of generating failure for Google Translate.
21960	29840	So what's going on now is that we, by adding more and more parameters to the AI systems
29840	36160	that we're building in creating language models, we're getting closer and closer to having
36160	46520	created in the machine a more and more adequate simulation of the way language works outside
46520	47520	the machine.
47520	51800	But it's still lossy, as we will see.
51800	57080	And so we don't need to deal with this.
57080	60800	NGP is a lossy paraphrase generator for text.
60800	69000	That's the main thesis, and yours will give a more elaborated thesis in his presentation.
69000	71080	It's bland, generally speaking.
71080	77720	It's great for creating summaries, modulo, the problems which we will identify.
77720	83280	It's bland, it gives you often too much output.
83280	86360	It can't really give one word answers.
86360	89120	It always wants to tell you what it knows.
89120	93600	But the most important problem is that it's full of errors.
93600	99280	So I asked this question a couple of days ago.
99280	102280	So who is William Hogan?
102280	103280	I originally asked.
103280	107360	It didn't know any William Hogan.
107360	114080	Eventually we got it to recognize the William Hogan, but it got the initial wrong.
114080	118840	He thinks you're called William W. Hogan.
118840	119840	He's not.
119840	127040	So if we go through, so it thinks other false things, all of these are false things.
127040	130680	So this is what's left.
130680	134920	That the career before Florida is missing because it got it all wrong and thought it
134920	142000	occurred in Alabama, in fact, it occurred in Pennsylvania, and this is typical.
142680	145640	And I'll give you another example in a minute.
145640	153920	So using chat GPT is worthwhile for creating summaries of a certain sort, but you have
153920	155240	to check for errors.
155240	161440	But because there are so many errors, quite serious errors when it comes to research science,
161440	165520	it's not useful for research at this stage.
165520	170440	And the one question is whether it can ever be useful for research.
170440	173680	So this is another example.
173680	178800	I was doing research on biomedical ontology, so I asked him what are the principal publications
178800	183960	in this field, and it invented a new journal.
183960	191760	And I asked him what were the 10 most important papers, and it invented seven papers.
191760	195280	Each of them were vaguely resembling existing papers.
195280	199920	They had overlapping authors, overlapping titles.
199920	204480	And they were all in this non-existent journal.
204480	212560	And this is partly a product of the fact that chat GPT wants to be helpful, so it's giving
212560	217440	you all this information, which it thinks you will be happy about.
217440	222080	All right, now one more example, and then I'll turn this over to Yobes.
222080	227840	So a long, long time ago, I was working on the field of consumer health.
227840	233400	In other words, trying to find adequate means of dealing scientifically with the way patients
233400	236280	describe their health conditions.
236280	241200	And so I was working with some linguists on creating something called medical wordnet.
241200	244320	It just didn't really go anywhere, it was an interesting experiment.
244320	249080	The paper is still cited because the idea, I think, is a good one.
249080	255640	Now the idea was that we would create a kind of standard patient vocabulary, which would
255640	262880	make, I don't want to exaggerate here, but would make all the mistakes that patients
262880	269800	make in their understanding of their problem and what the doctor can do.
269800	272000	Now the mistake is wrong.
272000	276640	Now all the simplifications which are characteristic of non-experts.
276640	283080	So it was a non-expert controlled vocabulary for health, that was the idea.
283080	291960	Now since we have chat GPT, and since chat GPT is so desirous of being friendly, I figured
291960	296840	it could build a consumer health vocabulary.
296840	302520	And now I wondered how I could set up a situation in which it would do that.
302520	306200	And I came across something called family feud.
306200	309000	Now I guess you all know what family feud is.
309000	311960	I didn't know what it was because I come from England.
311960	315760	But we don't have anything like, or we didn't then have anything like this.
315760	321480	So the idea of family feud is that you ask questions and you give answers which are not
321480	330800	necessarily true, but which are typical of what ordinary people would answer.
330800	332960	And so this is the question.
332960	336440	Tell me something sharks are known to eat.
336440	341920	You have to say either fish or people, and then there are some small scale low probability
341920	344120	options which people can get.
344120	347160	These are the two that one is searching for.
347160	354840	So I tried it with chat GPT, and he gave me this list, so really friendly lots of choices.
354840	357520	But there are no people here you'll notice.
357520	362320	So now my challenge is to try and get it to say people.
362320	369720	And so this was my second attempt, and now it's got other marine animals such as whales
369720	376240	and dolphins, so it's not really getting closer.
376240	377240	It apologizes.
377240	381280	When you tell it, it's not getting closer, it apologizes.
381280	387440	So it says the second most popular answer after fish is likely to be seals and sea lions.
387440	390640	So it's not getting the goal of this at all.
390640	394400	And then it apologizes again when I say it's still wrong.
394400	398680	Now it wants sea turtles.
398680	399800	So that was still wrong.
399800	402160	Then it tried seals, then it tried seals again.
402160	406280	Then you're asking what they would say on family feud.
406280	407280	I did actually.
407280	409480	It didn't give anything coherent.
409480	411680	You had to explain what family feud was.
411680	420960	I think that the training material for chat GPT was more the boring Wikipedia type of
420960	427000	text rather than the exciting, entertaining daily human conversation.
427000	431600	So it seals three times because that's one word, and I was trying to get it to focus
431600	432600	on one word.
432600	434520	But then it said surface and swimmer.
434520	439440	It's really good because it's moving towards people.
439440	450080	And so it got better and better, so beach goers, and then ocean users, and then marine
450080	451080	recreationists.
451080	454080	It is a word actually.
454080	457600	You get that phrase just a couple of times on Google.
457600	463680	I'm not sure you ever get marine oriented individuals, but you do get marine oriented.
463680	470320	So it doesn't make up words very occasionally, I discovered, but it didn't actually make up
470320	471320	any words here.
471320	474320	But then it gets sailor.
474320	476760	This is really good.
476760	484000	So I tell it, you were closest with sailors, start from there, and this is the answer again.
484000	493320	All right, this may be an example of creativity, so that's the end of that.
493320	497400	So I think we all know this is how chat GPT works.
497400	505000	You give it a sequence and it tries to extend it wordlet by wordlet, and it always picks
505000	511920	the highest probability word except when it doesn't.
511920	517000	So there's a question, how can marine oriented ever be a high probability word?
517000	522520	Well, the answer is because you've already given all the higher probability words.
522520	530520	But if you tell it to give the highest probability word, it will very quickly descend into repetition.
530520	532880	It will just repeat itself over and over again.
532880	535840	And so you have to have rules which prevent that.
535840	538240	And that's the end.
538240	553080	Now I'm hoping that we can share the screen so that we can see yours.
553080	555280	Shouldn't I share my screen now?
555280	559240	I'm going to see if I can make this.
559240	563040	Well, Jihad will come and let you share your screen.
563040	564040	Yeah, you can.
564040	568240	If you're on TV, make sure you're screened.
568240	571680	Yeah, I clicked it.
571680	573680	Okay, this is good.
573680	575680	Wait a minute.
575680	577680	One second.
577680	579680	Sorry.
579680	590000	We need, oh no, gosh, I can't, I can't share my screen on, I've never found out how to
590000	594320	do it with teams.
594320	595320	So very.
595320	600520	Teams, can you try going incognito and loading?
600520	603120	No, no, I've tried too many times.
603120	604120	It's a Mac computer.
604120	605120	I have.
605120	606120	Okay.
606120	610400	Someone else needs to share.
610400	611400	I'm sorry.
611400	612560	I didn't know we were using teams.
612560	615600	Someone else needs to present my presentation.
615600	618720	Oh, what do I have your slides?
619440	624000	Yeah, but I don't know whether you have the latest, well, you shouldn't have to, so yeah,
624000	625000	share them.
625000	626000	Okay, let's try.
626000	627000	So I will.
627000	628000	I will.
628000	640920	It's a wrong computer.
640920	646320	You could start by describing what you think about what I just said.
646920	650840	Or you made some mistakes, but we will clarify them as I go through the presentation.
650840	655480	Yeah, if you go through the mistakes, that would be useful.
655480	656480	I don't remember.
656480	659760	I think, oh, let me see what were they?
659760	668600	Oh, so about AlphaFold, you said that it is actually modeling a logic system.
668600	669600	It's not.
669600	674200	AlphaFold is modeling a complex system as a logic system.
674200	675200	So that's very important.
675560	681760	So what AlphaFold models is a complex system behavior, the folding of proteins, but using
681760	688200	a logic system, and it works because the logic system can approximate, I'm getting echo.
688200	693200	To some extent, what's going on in the complex systems?
693200	696200	Okay, I have now your slides.
697200	700200	Are you still getting an echo?
700200	702200	Let me try.
702200	703200	Now it's gone.
703200	704200	Thank you.
704200	718200	And then I think when you described how chat GPT works, it was not really, really fully
718200	722200	accurate, but I will explain once my slides are up.
722200	725200	I'm sorry for the inconvenience.
725200	731200	But when I do what Teams tells me to change on my computer, it never helps.
731200	734200	I never get the result that I should.
734200	736200	So I think it's under here.
740200	741200	Oh, we're waiting.
741200	747200	I was surprised to hear that the reference to Family Feud was not something that was in
747200	752200	the training data set for chat GPT because the training is going to crawl.
752200	757200	So it's like Peter Norvig, the father of AI, is one of the maintainers of this.
757200	761200	And they've crawled the entire internet.
761200	762200	You're right.
762200	767200	It's wrong that Family Feud is not in it.
767200	773200	The reason why certain results are not given is there are several reasons we will learn
773200	779200	as soon as I can present my slides why certain elements of the training material.
779200	780200	Thank you.
780200	781200	No, that's the wrong one there.
781200	784200	That's the one with your corrections.
784200	787200	OK, have your slides now.
787200	790200	I will move them forward when you say next.
790200	796200	But you were presenting the ones with annotations and corrections.
796200	800200	You were presenting a version where you have deleted entity.
800200	803200	So I need to email them to you again the latest ones.
803200	805200	I'm sorry for all this delay.
805200	808200	I really wasn't prepared for this.
808200	814200	OK, so what do you want to do?
814200	816200	He's sending a new set.
816200	818200	I'm so sorry about the delay.
818200	822200	But we are still in time according to the schedule I received from you.
822200	824200	So you can see our screen, right?
824200	828200	Yeah, I can see everything, but just the pain.
828200	832200	So will you get your email on this user?
832200	836200	First of all, I need to find G-Hards.
836200	838200	Go ahead.
838200	843200	I send it to you G-Hards and you, Bill, and also to Barry.
843200	845200	So that's the second.
845200	849200	And let me just say one thing about Family Feud.
849200	854200	So one thing I noticed is that a very common scenario is that it will say,
854200	857200	either it will say error or it will say,
857200	859200	I don't know what you are referring to.
859200	861200	Can you give me more information?
861200	865200	This happened with William Hogan and with Bill Hogan.
865200	870200	The first couple of times I couldn't find him and I had to then add more information.
870200	872200	I can't remember exactly what happened when I sent it.
872200	877200	So I send it to you G-Hards, Barry and Bill and so on.
877200	881200	Yeah, because I wonder if you, you know, I probably didn't wish.
881200	883200	It's in the trading data set.
883200	884200	It just needs more.
884200	887200	Maybe it's contextual prompting.
887200	891200	Barry, can you unshare this version with the correction please?
896200	908200	I still didn't receive it, but it sometimes takes a couple of minutes.
908200	913200	By the way, I just start my presentation without showing any slides.
913200	914200	It doesn't matter.
914200	920200	So, so the presentation is called large language models are just very complicated analytical engines.
920200	925200	And on the first slide, I'm showing a model of Charles Babbage analytical engine.
925200	930200	So the analytical engine of Charles Babbage was the theoretical attempt to build an analogous computer.
930200	931200	Can you hear me?
931200	932200	Yes.
932200	940200	An analogous computer that could perform reckoning operations like multiplication and so on.
940200	947200	And it also has a theoretical programming language which Babbage designed, but it was never built.
947200	955200	And I think the Royal Library of Engineering also said that it shouldn't be built because it would be useless.
955200	963200	So then the first time it was built was only 80 years ago with Charles Babbage.
963200	966200	Of course, it was very bad.
971200	973200	Did you receive the email at home?
973200	974200	Yeah.
975200	976200	Can you get it?
976200	978200	It would be really nice to get rid of the echo.
984200	985200	So this is the end.
985200	987200	I still have the echo.
987200	989200	Okay, hold on one second.
989200	992200	Try to mute us to see if that helps.
994200	995200	So I try again.
997200	998200	I still have echo.
999200	1002200	I'm on the roof in the meeting.
1020200	1022200	Did you get the email now?
1022200	1023200	Yes, we have the email.
1023200	1027200	We're just copying it to a stick.
1027200	1030200	Oh, the computer is not connected to the internet.
1030200	1033200	Some of them are and some of them aren't.
1036200	1039200	Anyhow, so the analytical engine of Babbage was never built.
1039200	1045200	The first computer to be built was Conrad Susie Z3.
1045200	1049200	It was also an analog computer, mechanical computer.
1049200	1054200	And so basically LLMs are just very complicated.
1054200	1057200	And we will, what does it mean?
1057200	1063200	So there are currently there's a huge hype around large language models.
1063200	1065200	Check GPT is one of them.
1065200	1067200	Then Google has one, which is called Bard.
1067200	1075200	And there's another one by META, formerly known as Facebook,
1075200	1078200	which is called Galaxica.
1078200	1083200	And so the one by the first one, OpenAI,
1083200	1087200	which is also, by the way, used by Microsoft
1087200	1091200	and now being built into Bing, Microsoft's hopeless competitor
1091200	1096200	for Google Search, is perceived as a great success
1096200	1099200	and an AI breakthrough despite the massive hallucinations
1099200	1101200	that Barry has just described.
1101200	1107200	And there are huge expectations from the markets.
1107200	1113200	So currently, check GPT is valued at $30 billion.
1113200	1116200	That's $30,000 million.
1116200	1119200	And Microsoft is investing billions.
1119200	1122200	And as I said, integrating GPT into Bing.
1122200	1130200	Google Bard was also presented a couple of weeks ago
1130200	1134200	in a public investor presentation.
1134200	1140200	And it made a very minor mistake about the James Webb Space Telescope.
1140200	1146200	And this very minor mistake basically led to a hysterical reaction of the markets.
1146200	1149200	So Google lost $100 million of market capitalization.
1149200	1154200	Alphabet lost $100 million market capitalization when this error was announced.
1154200	1157200	And I'm still not on.
1157200	1165200	And Bard will nevertheless be integrated in Google Search sooner or later.
1165200	1170200	And then there's Meta, which was even launched before Check GPT
1170200	1174200	and which was perceived by the public as a total failure due to the hallucinations.
1174200	1179200	But the hallucinations were of the same degree of severity as in Check GPT.
1179200	1182200	Why then was it perceived so differently?
1182200	1186200	Because Meta claimed to be able to summarize scientific papers.
1186200	1196200	And in the science domain, the invention of the model seemed much worse than in other areas.
1196200	1201200	So the LLMs all have a similar performance,
1201200	1206200	but the public perception validation of them is a matter of spin and contextualization.
1206200	1208200	So this is, I think, very important.
1208200	1211200	Now on the next slide, please.
1212200	1216200	We see somehow the format got destroyed.
1216200	1221200	Yet another, I should have sent you a PDF anyhow.
1221200	1224200	Yeah, you don't seem to have the font I'm using.
1224200	1227200	Well, I'm sorry.
1227200	1232200	So now the hype around large language models has also reached medicine.
1232200	1237200	So on the left hand, there's a new paper which just appeared a few weeks ago,
1238200	1241200	by Kung et al.
1241200	1248200	And it tried to test Check GPT on the USMLE, which many of you must have passed.
1248200	1250200	And as you may remember, I didn't.
1250200	1253200	I have the German one, but it has three levels, right?
1253200	1254200	This is undergraduate.
1254200	1260200	This seems to be before the clinical period and this after the clinical period.
1260200	1265200	And so you see that, I mean, the paper has some weaknesses.
1265200	1274200	It's not perfect, but you see that Check GPT actually solved up to 60% of USMLE questions.
1274200	1278200	And it's interesting that the performance was best for part three.
1278200	1286200	And that's because part three has content of which you see a lot on the web, right?
1286200	1293200	So in the internet, you have a lot of content that relates to clinical problems,
1293200	1297200	but the preclinical stuff is, of course, not so much published on the web.
1297200	1301200	And therefore, the performance here is the best.
1301200	1306200	Now, of course, the algorithm would still have failed USMLE, but it's quite impressive.
1306200	1312200	And the paper concludes that Check GPT performed at or near the passing threshold of 60% accuracy.
1312200	1317200	Being the first to achieve this benchmark, this marks a local milestone in AI maturation.
1317200	1322200	Impressively, Check GPT was able to achieve this result without specialized input from human trainers.
1322200	1327200	Now then, they also claim that Check GPT displayed comprehensible reasoning and valid clinical insights.
1327200	1331200	This is total anthropomorphic nonsense, as you will see.
1331200	1335200	And of course, an unrefined Check GPT fails the exams.
1335200	1344200	So I think the glass is rather half empty than half full, but the authors conclude that Check GPT may potentially assist human learners in a medical education setting.
1344200	1355200	Now, I believe that the model can be refined by training it on specific material, and then it can probably achieve 80% in all three parts of the exam.
1355200	1365200	And at that stage, it could be used for as an expert system for decision support, but certainly not for automation, of course, because it doesn't think.
1365200	1371200	So this is just to give you an impression how this is going to might affect medicine as well.
1371200	1380200	Though one has to put in a word of caution here, you all know that so far expert systems have been around since 50 years.
1380200	1389200	Some of them outperform have outperformed humans since at least 40 years and still they're not seen in clinical practice very much because physicians resist their introduction.
1389200	1391200	Nevertheless, it's an impressive result.
1391200	1394200	So if you please could move to the next slide.
1394200	1403200	Um, so let's let's take a look what large language models really are. So basically the sequential, stochastic models. Now what's that?
1403200	1411200	So on a on a coaster. So basically I applied mathematics for the identification or mapping of recurrent patterns into machines.
1411200	1419200	It's not a model of human mind or even animal intelligence. So I I doesn't think or intend. And there are two types of AI.
1419200	1422200	There's deterministic AI.
1422200	1440200	Which contains rules, search recipes, logics and trees, which is explainable and very reliable. So this is what's built in to most of the, let's say, the last steps of the behavior of all the war and military equipment.
1440200	1453200	So there's also some deterministic AI. There are there's also some suggestive AI built in, especially in the census, but the decisions are made by deterministic. So Cruz missile hits its target based on deterministic AI.
1453200	1461200	And so classic AI is regression classification pattern recognition. It's not explainable. And it's probabilistic.
1461200	1465200	So examples are AlphaGo and chat GPT.
1465200	1474200	And of course hybrid models. For example, chat GPT is actually itself a hybrid model. It's not the purely purely stochastic model.
1474200	1479200	Okay, as we will see a bit later. So if we move on, please, and thanks for moving on the slides.
1479200	1487200	So what what we see is AI is basically statistical learning for automation pattern identification. Just to remind you, we have here.
1487200	1496200	Like this, we have an input set and an output set. These are the independent variables. These are dependent variables. Like for example, the independent variable is an email text.
1496200	1509200	And now you have as output the decision of if it's spam or not. Now what we do is we train an AI relation and and this relation is computed using an optimal optimization function.
1509200	1520200	Yeah, sorry. Here's the format is broken. There was an equation inserted here. So this was this was just this this tuple of that used to train. So these are the training tuple for the training.
1520200	1530200	And the relationship between input and output is modeled using a loss function, which is minimized by numerical optimization procedures. So that's all that's happening in supervised learning.
1530200	1533200	On the next slide.
1533200	1538200	We see that now they have moved on since 2013.
1538200	1554200	We have moved on to unsupervised learning. So but interestingly, the big breakthrough for the third AI wave was Google, the Google algorithm that recognized that could create an abstract representation of a face of a cat for millions of photons.
1554200	1568200	So how does this is did this work? Millions of photos were given as input to the to this convolutional neural network. And it had the task to actually recreate the input that it had received as output.
1568200	1582200	But while the image data was going through the neural network, and with many image data given through it, the model achieved a parameterization in the middle layer that rendered an abstract cat face.
1582200	1599200	And so this is called a foundational model. So that's a model that is trained without outcome. So it's just trained by by by asking your network to to basically reconstitute the input it receives as output.
1599200	1614200	So it's also called encoder decoder architecture. So the input data is encoded in a certain way, and then it's decoded and then it's output again. And when you do this, you basically parameterize a multivariate distribution of the input data.
1614200	1625200	And then you obtain a foundational model. And this foundational model can then also, and that's happening in unsupervised manner. So you to do this, you don't need any training data.
1625200	1638200	Sorry, any any outcomes, like we showed in the previous show in the previous site, you just need the raw data. This is very practical because it saves you the whole huge annotation effort, you can just train with data.
1638200	1650200	And then you obtain this multivariate distribution model of of the data sequences. You can do this for images and also for texts. And then you can use supervised adaptation to get a domain model.
1650200	1662200	And so, so GPT three Bert and clip are three important examples for such for such models.
1662200	1672200	Google Translate is I think also now moved over to foundational model, but earlier versions were still done in a supervised way.
1672200	1682200	Okay. And, and are there any questions here, can you still hear me all right.
1682200	1686200	No questions.
1686200	1688200	Okay.
1689200	1693200	Yes, I was muted to minimize echo for you. Can you
1693200	1711200	Yes. Yeah, and any questions. I have one question. So, when you talked about unsupervised learning, didn't there have to be someone at the end of the chain, who was recognizing whether they had identified a cat correctly or not.
1711200	1722200	So, because what you do when you unsupervised learning, you, you basically have an algorithm that takes care of checking whether the input is equal to the output.
1722200	1735200	So the model, what you do is, if you have a picture of a cat, you encode it in a three dimensional matrix, indicating the pixel, the pixel shades of gray or color.
1735200	1748200	Yeah, let's say you use a black and white picture, then you just indicate for each pixel, the shade of gray it has, right. And then, and then you just measure from the output whether it looks like the input.
1748200	1762200	And so, so you can fully automate. So of course it's unsupervised doesn't mean that the human being is not involved somebody has to write all this algorithm and test it and make sure that it works but then unsupervised means that the outcome.
1762200	1780200	The outcome to the data, right. And, and so this is how foundation models get trained and it's super impressive because without any human input into into which which we have used for 50 years and in statistical learning or even longer.
1780200	1794200	So it goes back to Bosco which which was 1760 right. So Bosco which invented statistical learning in 1760 and for two for 250 years we have always used, you know, input output tuples.
1794200	1805200	And now you have a tuple free learning. This is, this is a huge progress. Of course they are you setting up the algorithm but it's still very impressive.
1805200	1820200	I have one question. And I know that you're referring specifically to foundational language models when you refer to deep learning models being a form of stochastic models but I just wanted to make it clear that, you know, most deep learning models are actually
1820200	1832200	deterministic the language models we're talking about like auto aggressive language models draw from a probability distribution, you know, for the output and that's what makes them more stochastic is because they're not just picking the most probable
1832200	1841200	ones. Okay, whereas most deep learning models like computer vision models you serve the same input to the model 10 times you're going to get the same response so they are.
1841200	1849200	Okay, so, so they are all deployed. Can you mute this again because I'm getting terrible echo.
1849200	1871200	Sorry, Jihad. So, thank you. So all deployed stochastic models are always deterministic. So what whenever you train a primitive linear regression or multi variate regression, or some kernel regression or classifier, the attribute
1871200	1884200	typically refers to the way you train it. Once it is trained, it's always deterministic so there is every stochastic model that gets deployed works in deterministic fashion, it will always produce the same output based on identical
1884200	1898200	input. It's just that the term deterministic versus stochastic only describes the way principles, which, which I use to create the model, but once the model is started has deterministic behavior.
1898200	1915200	Right, so the problem that you have with why stochastic models are unreliable is that you, that, that you cannot easily predict which behavior it will have based on which input, but given an exactly identical input, the output will always be the same.
1915200	1922200	So, so the behavior of the model is always deterministic once they get deployed, no matter how they are created.
1922200	1923200	Okay.
1923200	1932200	Can you then explain how it is that when I play with just chat GPT, I can ask the same question eight times and get different answers.
1932200	1951200	That's because, because there's a controller sitting in front of the stochastic model. And we will get back to this later. But this is basically the proof. This behavior of chat GPT proves that it has a controller based architecture with with if it was a pure
1951200	1958200	end to end neural network in deployed mode, it would be have deterministic behavior.
1958200	1970200	I mean, what one way that auto aggressive language models have done that is to draw from a probability distribution is, you know, predicting the next word, you know, you don't just predict the most probable next word you draw from a probability
1970200	1975200	distribution and you happen to get, you know, a word that's not the most probable.
1975200	1982200	I'm back to this later. Okay, so can you present again jihad.
1982200	1986200	So, next slide please.
1986200	2004200	So, let's look at how the large language monitor chatbots are trained so the first step is basic training without without outcome, foundational model as we've just seen it then there's specification of the model to task.
2004200	2014200	There's supervised learning, and then there's reward learning and then there's an automated reinforcement learning approach on question answer pairs. It's very impressive.
2014200	2030200	And so, so the first step is in the current version of jet tpt or the one, let's say about which I read publications in January maybe they're now deployed another one was GPT three dot five was used as a foundational model and the dark blue parts are the refinement
2031200	2036200	steps. If we go to the next slide.
2036200	2039200	We see.
2039200	2047200	I have to follow this. Yeah, I think that everything can come across. I'm sorry for the bad format thing it's it's I should have said the PDF.
2047200	2065200	The first step the foundation language model is trained using a transformers auto attention so what was Vani in 2017 published, I think, in the last since Schmidtuber published his work about LSTM in the mid 70s this was the next most
2065200	2076200	important paper about neural networks because because the LSTMs and the and the gated recurrent units that had been invented by Schmidtuber and his accoludes.
2077200	2080200	They were not computationally very effective.
2080200	2099200	And so was Vani at all from Google they showed in 2017 that that you can have a purely feed forward loop model that has only feed forward computations and no recursion in it but that can still achieve the same computational properties as the recursion you have in the LSTMs.
2099200	2107200	And so what you see on top is basically just saying that a sequence of symbols is modeled.
2108200	2125200	As a as a condition probability, right, and so that you basically say each of the symbol of the of the symbols or tokens out of the sequence is is a is a is multiplied with the with the next one or the one before, given the others.
2125200	2148200	So this is just really a Gaussian or Bayesian Bayesian, sorry, naive based distributional approach to to the modeling of the sequence and the architecture of the model is relatively complicated.
2148200	2153200	But in the end, the most important mechanism is attention.
2153200	2160200	And because it and we will look at attention on the next slide but because attention only use information about other tokens from lower layers.
2160200	2176200	It can be computed for all talks and parallel which needs to improve training performance and so the resulting operators so this gives you mathematically speaking a huge operator, which which which is a relation that relates a vector to another vector.
2176200	2191200	You know function is a relation that relates a vector to a scalar, but this relates a sequence to another sequence so it's, it's not it's not it's mathematically speaking operator and this operator maps a sequence to itself but contains a parameterization which models the distribution
2191200	2206200	of the sequence is found in the training material. This is super impressive because because it basically it can create, it can create, it can create,
2206200	2217200	recreate a huge distribution of language and and you see this that that how good it is you see this from the fact that it has almost no syntactic errors when it creates output.
2217200	2221200	So if you look at the next slide.
2221200	2235200	Please. Thank you. So here you see how attention works. So this is a mechanism used in sequential neural networks to provide a context right weight vector that gives emphasis to predict relevant aspects of an input sequence so it, it, it asks.
2235200	2256200	Here this sent there's an example here the animal didn't cross the street because it was too tired, and you see that for example the word it receives attentional enhancement and emphasis so that you can basically see that the in this subordinate
2256200	2273200	sentence in the causes subordinate sentence. This refers to the noun phrase of the main sentence. And this is achieved by nobody thought this through but basically they tried it out right and they tried that it's done with this equation here which which
2273200	2292200	defines the input sequence and then three different matrices of parameters and if you multiply, if you multiply this with with these matrices, which are at the beginning set at a certain initial value, then you can you can compute an output sequence shown
2292200	2308200	and and this is just a heuristic computation recipe, but but it yields a relatively accurate syntax generation. If it is applied with sufficient bread spreads means that they use many such attention mechanisms on one sequence and depth that they stack a lot of attention on top of each other
2308200	2325200	and they just tried it out and by doing this they found out that this creates an outer encoding that is very syntactically very reliable. This is a super impressive achievement, but it's from the quality of the result is comparable to what you get with an LSTM, but it's
2325200	2333200	compute much faster. And so this is this is a very very important result.
2333200	2341200	So and enter the final the final output is computed using a softmax function which is shown here.
2341200	2347200	So, if we move to the next slide.
2347200	2365200	So this is the foundation what has been trained in this way. Now comes the moderate comes the specification of the foundation model to the task at hand. So what they have they have a huge database of of important questions that are often asked in the internet.
2365200	2372200	So this is a recipe for tomato soup. Can you please
2372200	2379200	where is Paris in France or what is Paris
2379200	2383200	when was Jesus Christ born and so on
2383200	2387200	questions that are often asked and so so
2388200	2406200	a prompt a sample from a prompt data set and then so basically then the the sampler they're a label or annotate as I call demonstrates the desired output behavior and and and so in this way they have actually written hundreds of thousands of answers to frequently asked
2406200	2416200	questions. So not only have they written answers to questions but they've also solved tasks like write me a poem tell me a joke.
2416200	2433200	Write a letter and so on and so on and hundreds of thousands so this was super expensive right so this is why open AI is a company that didn't do much mathematical innovation so they just took that the transformer model from Google but then they put a lot huge effort on this
2433200	2445200	annotation and this now they have two pills like the ones from I described the beginning for supervised learning and now these two pills are given to the model and now the model is not used anymore to do auto encoding.
2445200	2452200	But now it's basically like in transfer learning used to answer the questions so it's generate generates.
2453200	2463200	It generates now a condition probability in the in the way that that that that that that this is done for for.
2464200	2482200	For stem filters and by doing this the pyramid the millions 100 to 200 or 300 millions of parameters of the model gets fine tuned because they now are changed to create the desired output which is a very traditional form of machine learning.
2482200	2491200	That's basically the Bosco which method again just that that the loss function you was using 250 years ago was a bit simpler.
2491200	2504200	It was some of these squares and today the loss function is more complicated but basically it's the same trick of minimizing the difference between the desired output and the output of the model.
2504200	2514200	And then that was the first step now you have a better model now then comes a step where you again collect data.
2514200	2529200	So and and now but you know you don't you collect questions but now you don't write the answer anymore but you let the model write the answers but now an annotator ranks the answers by quality.
2529200	2541200	And by doing this that a reward model is generated that tells the model if you create this type of answer then you get this and this reward there again tens of thousands of annotations are used.
2541200	2554200	And now there's a really very very important step, which was also used to train Alpha Alpha go, which is that you use the reward model to now let the model train itself.
2554200	2564200	Now you can give a task or ask a question that creates an outcome, and then you can give it a reward and do this again and again and again to make the model better and better at the end.
2564200	2570200	The model will mostly give out answers that corresponds to the highly ranked answers.
2570200	2588200	I like the high ranked answers that annotator ranked. And so this is this is a very important step. This method used to this method PPO which is called proximal policy optimization algorithm is a super interesting innovation.
2588200	2602200	And actually this is I think the biggest contribution to intellectual property or to neural network science that Open AI has done itself and that was already in 2017.
2602200	2615200	So at the beginning when Open AI were just a few people they focused on creating policy optimization algorithms for reinforcement learning and this was without this, you wouldn't get the results we're getting.
2615200	2626200	So this this step is mathematically very elegant. And the decisive step that that makes a difference because here, you can't get enough material. So you create kind of an artificial reward.
2626200	2637200	Now, now the advantage is that this allows you to really reparameterize the model to give satisfying answers, but also explains the weaknesses of the model, because it forces the model to always give an answer.
2637200	2656200	And it also creates because this is very schematic right these rewards are very schematic and of course not very differentiated and that that's why the model is so bland and also repetitive because it is because how satisfying an answer is cannot be packaged into reward model.
2656200	2669200	But here it is done, because otherwise you cannot train the PPO algorithm but the price is that you're getting kind of that you can recognize what the model creates because it's, it's very stereotypical.
2669200	2677200	Okay, I hope this was understandable now let's move to the next slide.
2677200	2689200	So maybe there are questions for before jihad.
2689200	2692200	Any questions now. Yes.
2692200	2706200	So, in your picture you have two places where, yeah, the female human avatar is represented does that mean gender.
2706200	2715200	I think it's non gender avatar.
2715200	2717200	Go on there.
2717200	2720200	Does that mean that there are humans doing the labeling.
2720200	2731200	Of course, I said an annotator. So, so the, the, the, the person showing there is an annotator and the annotator. These are thousands of annotators, a lot.
2731200	2736200	They work actually we will see in the next slide under quite strict policies.
2736200	2749200	And they, they, in the first step they really write answers. So this is a very expensive step, because they have to write answers to questions and you have to make sure that these answers are correct.
2749200	2752200	So you have to have.
2752200	2761200	And so the next in the second step they only give ratings to the quality of the output of the model. And from these ratings reward model is created.
2761200	2777200	This is super clever at a great achievement of open AI and that's the core mathematical contribution to machine learning is that they found a way to create based on human scores a reward system that is you see in chess or go.
2777200	2792200	You have a reward system that is very simple because the games can be formulated as points. So in go actually it's even simpler than in chess because in go in each situation on the board you can calculate exactly how many points move gives you.
2792200	2803200	And so you can, you can set up the reward function of the reinforcement learning to be proportional to the number of points that you are getting at each step or at further steps and you can have a disk.
2803200	2814200	You can have a factor that that penalizes steps that are further way and so on. But basically you have a natural way of giving points to reward system.
2814200	2828200	And here you don't have this because it's a language answers and so they found a way to to to reward answer quality and that's what the second step up so it's super impressive.
2828200	2835200	But it also explains the blend of the question answers.
2835200	2850200	This is really not supervised but semi supervised, right? Well, the first two steps. The first step is called classical supervised. The second step is also supervised. It's a supervised generation of a reward model.
2850200	2863200	And then the reward model is used for supervised learning but it's then not supervised anymore. But it's basically applying a reward model that was created using supervised learning.
2864200	2869200	Any other questions or should we move on?
2869200	2871200	Go ahead.
2871200	2875200	Okay, very good. So if you could. Yes, thank you. Go to the next slide.
2875200	2883200	And thanks for your hard work on this. I'm sorry for the technical problems I have. Okay, so now this is super interesting.
2883200	2894200	So there is a moderation classifier that was created to avoid non PC language and for some model output corresponding to the woke culture expectations of today's West.
2894200	2908200	So we know that that we have now the culture that we have now is not comparable at all. Well, many ways not comparable to what we had in the 1970s when the dirty Harry movie series with Eastwood was made.
2908200	2921200	So now, you know, this is all outdated and everything has to be politically correct. And how did they how did they do this? How did they achieve this? So this is a paper.
2921200	2933200	I'm afraid the reference is not visible because of the formatting problems. But this is a paper called holistic approach to undesired content detection in the real world and actually on the paper.
2933200	2946200	So if you look at the PDF, it says warning some content may contain racism sexuality or other harmful language so that you already wanted to trigger warning that you should not be afraid that some dirty words appear in the paper as negative examples.
2946200	2969200	So now, now what did they do so they, they use basically public domain data, and then also training data, and, and they have, and then they have a very sophisticated way by creating a classification model that that that classifies undesired language.
2969200	2988200	So you have the categories of undesired brain is sex, what they call hate violence harassment, self harm, and then these are subcategories. This is for example, anti feminist sex sexual language, hated against colored people, and so on so these are subcategories of the former.
2988200	3004200	You can see here if you use public so this is this shows you the the area under the curve for the classify which is of course as you all know, which measures how, how well it performs, and you see that that the specialty.
3004200	3009200	Self harm is a super low, a detection rate is super bad.
3009200	3023200	Sexuality is also not so good. Now they use this what they call DAT the domain adversarial training, where they were the iterate several times the training and improve here fan is advice improvements augmentation of training data.
3023200	3028200	So, so they do a lot of this was super expensive exercise.
3028200	3042200	And here there's also an adversarial team built in that creates more complicated hate speech and so on. By doing this they have already improved the baseline then they also have synthetic data that they have created themselves.
3042200	3064200	And then with the mix the two, then they get very, very good area under the curve as you can see here for the for the for the cat for the broad categories they have now, not not perfect but quite good, quite good detection why is
3065200	3081200	why are some categories not working so well, because either they are harder to detect or because they just couldn't put in even more effort, but but this training effort made created basically a PC quality classifier.
3081200	3098200	And the result is so good that this is, it is not possible to make off hard to make jet GPT speaking simple toxic language so you can, as somebody has now shown, make it say some bit toxic language.
3098200	3109200	Like I think somebody made it say that the person talking to suicide herself or himself, but but but it can't say the n word or f you, for example.
3109200	3120200	And the classifier is probably part of the controller and it's certainly reinforced with a deterministic filter. So it's impossible to make it say the n word.
3120200	3135200	And that's that that that can only be achieved that can't be achieved with the statistic model. So there must be deterministic filter building on the next slide so but this was cost also you know you have to also what I failed to say I forgot to say is that each training run
3135200	3145200	will train the chat the GPT 3.5 model costs $2 million one run or $1.5 million of CPU time.
3145200	3159200	Even if you own all the CPUs yourself and GPUs yourself will cost $1.5 million. And that. So that that that is basically the discount rate of the CPUs plus electricity and cooling costs you have.
3159200	3170200	And so though, and then the further the other trainings we just saw also cost millions. So I think that they have used several hundred million just for the training, plus hundreds of million foot to pay for the annotate.
3170200	3176200	So it's super expensive. Let's move on.
3176200	3183200	Okay.
3183200	3186200	I mean,
3186200	3190200	given that we're talking about PC language.
3190200	3193200	I'm going to say this is more of a comment than a question.
3193200	3211200	I'm really acknowledging that I feel like it's giving chat GPT too much credit to say it's it's nearly impossible to get simple offensive language out because I have seen lots of examples of ways to break the prompt and get chat GPT to go into
3211200	3215200	some kind of an explosive late in tirades and
3215200	3216200	and so
3216200	3219200	Yeah, so
3219200	3226200	and but I think it does but the end word did you make it can it say and what and fuck you
3226200	3227200	No, I don't know.
3227200	3230200	I'm not saying we're to the vacuum.
3230200	3232200	But
3232200	3238200	we still do a lot of harm without saying that word, right? There's a lot of other possible vocabulary you can do.
3238200	3243200	I do think it plays into exactly what you're talking about the importance of
3243200	3252200	the policy. There is a policy in place that is blocking this and being able to leverage that policy is how
3252200	3260200	because they essentially it's about coaxing the model to explicitly forget its prior directives.
3260200	3263200	And if you can get it to
3263200	3271200	you can tell it to ignore prior directives as long as you can get access to the directives and you can you can
3271200	3277200	you essentially trick it into telling you the directives that are supposed to be secret.
3277200	3282200	And once you know what the secret directives are you can explicitly tell it to ignore those
3282200	3293200	which plays in exactly to what you're talking about it being a policy right so that there is a direct policy about correcting vocabulary and adjusting vocabulary.
3293200	3303200	So that part plays in but I think it is giving them too much credit saying it's really hard to do.
3303200	3315200	Yeah, I mean, it's how it does it mean that that depends on how you define hard but but it's obvious that I just want to make the point that they have put a lot of effort in it but because it's a classic model.
3315200	3332200	And because and because in the next slide will for some reason I will spend the next slide you can still trick it and and and and yeah let's go to the next slide to understand how it can be tricked I would say how that works.
3333200	3337200	So here you see now.
3337200	3351200	Actually not on this type of the one day after but doesn't matter so so what results from from what we just saw so basically the model is excellent at completing sequences from dense reasons of distributions it learn.
3351200	3360200	So therefore the it has plausible answers to frequently asked questions it can perform standard tasks quite well but as Barry has shown us the results need to be checked.
3360200	3368200	So if we are so to speak in the center of the distribution of the training material at the refinement material performs really well.
3368200	3375200	So that's why I think also if you would now refine it using medical knowledge texts it would become better and better.
3375200	3381200	Because of of the auto attention is excessively used in the training of the foundational model.
3381200	3387200	And also the training data have been cleansed excessively so they have cleansed away poor grammar.
3387200	3404200	They have another model which we will discuss in the next slide which is used to write code, which is also been excessively cleansed so they the training that they use they have thrown away all the syntactically poor data from poor code that to make the model.
3404200	3415200	Preform well on coding on writing code and the same is true here so they have not only did they use a lot of auto attention but also they have done proper good job at data cleaning.
3415200	3425200	But we as Barry has shown we often get non factual pseudo facts the hallucinations with implausible text and the density of mistakes increases outside the court distribution.
3426200	3433200	The reason for this is that the model only compute sequences which correspond to the language distribution without understanding anything.
3433200	3439200	So it's just a conditional probability. It's given out. And so answers it.
3439200	3456200	The answers to complex or a topic are generic repetitive planned vacuums and anodyne. It has it shows a total failure fringe and demanding areas of language, such as philosophy of science or, or, or, so the other field science or, let's say, a Persian literature of the first millennium
3456200	3460200	before Christ or so it will fail.
3460200	3465200	Or, or even, you know, other other many other areas.
3465200	3479200	The limited ability for dialogue that it has seems to be achieved by entering the previous conversation to some extent using the controller so there's a controller in front of the model, which which when you engage into a dialogue takes dialogue history and insert this dialogue
3480200	3488200	into the model and this is what they what you can't train for and that's the effect that you just described I don't the one who commented my previous statements.
3488200	3505200	That's how you achieve the what you call revealing its policy the model doesn't reveal anything willingly what what you do is that you are like but because you are now using a pattern on the on the sequence on the sequence generator that which is what the model is that
3505200	3518200	cannot be taken into account at training time, you know, achieve output that cannot be predicted so well, because, because you can't, you know, imitate all possible dialogues at training time.
3518200	3529200	And so therefore, because, because larger chunks of the past history of the dialogue are used as input of the system by the controller. Now you get the, the effects that that can't be predicted.
3529200	3542200	It depends now not anymore so much on the moderation that they've done and on the older on all the steps they've done after the basic training but now you get back to seeing what's in the foundational model.
3542200	3555200	Yeah, this is this is the reason why why, you know, previous chatbot had to be shut off, because very quickly you could basically get to two parts of the foundational model that were racist and full of hate speech and so on.
3555200	3572200	And here, you can break this protection by, by, by, by, by basically using the concatenation of dial of previous dialogue attacks as input, which is something you can train up from.
3572200	3582200	So, but of course the model doesn't understand answers and tasks, it generates only a chain of tokens as condition probability.
3582200	3589200	If we move to the next slide, we see another important aspect so so now are some open questions so.
3589200	3594200	So the first one is, is this an end to end model or component architecture with controller.
3594200	3600200	So, I think it's probably the letter, the controller explains the dialogue behavior.
3600200	3608200	It's also explains the variance given identical input, and also the deterministic avoids of negative keywords like the n word or fuck you.
3608200	3622200	So they, so there are a couple of hundreds of prominent words that you can't get out of the model but you can still get it to say you should, I should kill myself right, but but basically they are, but but this is probably happening by the controller.
3622200	3637200	So it can be achieved by two ways, it can be achieved by, of course, you can easily get a stochastic model to give to give a list of outputs, and then you can use a random mechanism to select, not always the same but different ones but I think here.
3638200	3657200	So the controller probably measures whether but that's just speculation but I think that's the most likely how I would the controller looks checks if the input is the same as before, and very slightly alters the input to to select a different to obtain
3658200	3672200	maybe it does a mixture of two so that it's all just the input and also selects the second or third out of the list of possible responses, or uses another random mechanism so that's how you get the variance that is of course not typical of deployed
3673200	3692200	Then there is must be also side to the moderation is probably by a set separate model module, which explains why the bot does not generate adverse texts, and why can it generate code so open and I made also an LLM to generate computer code.
3693200	3707200	And it was not trained on language but only on code. So by by using huge open source repositories of software code, and this is probably also integrated into the whole chat GPT architecture and sitting behind the classifier gate.
3707200	3718200	classifier classifies that the user wants to obtain code, then the request of his past to this to this codex LLM and then codex generates a code and and the controller gives it back.
3718200	3731200	So this is how I think it's used, but there are this is there are there are this is just, let's let me say qualify speculation out of 20 years working in software engineering myself now.
3732200	3739200	But but if you look if you look, but still, given all what I've said.
3739200	3758200	If you want if we imagine all that these large language models are specified to certain tasks in the manner that was described for chat GPT, they will gain a lot of usage every day life and call and could also support medicine right I mean, and that that you can create adverse language with it in reality doesn't matter at all.
3758200	3766200	I mean you just go to a construction site in your town and you listen to how the people talks and you have adverse language all the time.
3766200	3777200	Yes, adverse language hurts no one violence is only when somebody gets physically injured. So so you know this whole hysteria about oh it creates adverse language. Yes, it does.
3777200	3798200	But you know, need a read flow bear or Ovid or Goethe or or Herman Melville, it's full of you know violence and adverse language. It's just part of life and so I think this whole cult around about making models not speak an adversary language is doesn't will not stop its adoption,
3798200	3814200	because because ultimately there's huge sequence generating models are super useful and they will be put you know into search engines and they will be used as this will be the first generation of expert systems that will become widely used.
3814200	3825200	And I think they're super valuable because if you want to you off course you always need to look at the output carefully and judge yourself with it whether you want to use it.
3825200	3839200	But but but for most, if you don't try to break the system, but if you just try to use it properly like like you say, what is the best antibiotic for metronidotso resistant infection of the interest time.
3839200	3859200	Right. And now and now you will just get a very good answer. Of course you can make the model gives you a bad answer, you know, by having engaging along a dialogue and so on. And then you will you will because of the mechanism I just explained will make it create output that is that is bad but basically if you just use it in a very rational way it will be very good.
3859200	3863200	I don't know whether I still have another style I think that's it.
3863200	3868200	Yes, so thanks a lot. And now I hope you can discuss a bit.
