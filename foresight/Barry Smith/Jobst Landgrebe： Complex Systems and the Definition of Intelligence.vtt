WEBVTT

00:00.000 --> 00:02.960
Hello, everyone, and thanks for participating.

00:02.960 --> 00:04.920
My name is Jobs Danke, but I'm by training

00:04.920 --> 00:07.480
physician and mathematician, but I also

00:07.480 --> 00:10.040
study philosophy, I've come back to do

00:10.040 --> 00:12.520
philosophical research work as well.

00:12.520 --> 00:15.680
So today I'm going to talk about artificial intelligence,

00:15.680 --> 00:18.000
intelligent pseudo definitions.

00:18.000 --> 00:20.360
But before doing this, I need to introduce

00:20.360 --> 00:23.360
our view of complex systems, because we

00:23.360 --> 00:26.120
will need this later on in the talk.

00:26.120 --> 00:29.160
So I've been starting with two slides about complex systems,

00:29.160 --> 00:32.800
and then we'll move to the definitions of our intelligence.

00:32.800 --> 00:36.120
So what is a complex system?

00:36.120 --> 00:39.280
So maybe why do we need to understand a complex system?

00:39.280 --> 00:42.320
Because the animal and the human mind body

00:42.320 --> 00:48.560
continue, which produce intelligence, are complex systems.

00:48.560 --> 00:53.080
So even primal intelligence, which

00:53.080 --> 00:57.480
is the intelligence of a bird or a mammalian animal,

00:57.480 --> 01:00.480
the non-human, has primal intelligence

01:00.480 --> 01:03.320
that gets produced by a complex system.

01:03.320 --> 01:05.720
So what is a complex system?

01:05.720 --> 01:09.080
So let's start with the Newtonian system.

01:09.080 --> 01:13.240
So Newtonian systems are systems in which one

01:13.240 --> 01:16.240
can apply models of physics.

01:16.240 --> 01:21.200
And they have become extended quite a bit in the 19th century

01:21.200 --> 01:24.560
with thermodynamics and statistical mechanics.

01:24.560 --> 01:29.480
And then another bit by quantum theory and also

01:29.480 --> 01:31.560
general theory of relativity.

01:31.560 --> 01:33.800
But they still remain Newtonian systems.

01:33.800 --> 01:36.480
So what is a Newtonian system?

01:36.480 --> 01:38.200
But by the way, can you hear me all right?

01:38.200 --> 01:40.400
Does it work?

01:40.400 --> 01:42.960
OK, I assume yes.

01:42.960 --> 01:47.880
So Newtonian systems are made up by a small set of element

01:47.880 --> 01:48.880
types.

01:48.880 --> 01:51.320
For example, the solar system is made up

01:51.320 --> 01:56.200
by the sun and the planets, which are not many elements.

01:56.200 --> 02:00.800
The way the elements interact, they

02:00.800 --> 02:05.920
interact by the four basic forces or interaction types

02:05.920 --> 02:08.040
that are known in physics.

02:08.040 --> 02:12.120
And in this case of the solar system, it's gravitation.

02:12.120 --> 02:14.640
And actually, only gravitation.

02:14.640 --> 02:17.960
All the other forces don't matter for the solar system.

02:17.960 --> 02:19.680
At least not with regard to the way

02:19.680 --> 02:21.920
the planets move around the sun.

02:21.920 --> 02:26.320
And they interact in a uniform and isotropic way.

02:26.320 --> 02:35.440
So the force that is interacting here, the gravitation,

02:35.440 --> 02:41.200
has its effect in a symmetric way all around the sun.

02:41.200 --> 02:46.160
And it is the same everywhere.

02:46.160 --> 02:48.880
I mean, it gets weaker and weaker.

02:48.920 --> 02:52.920
But in a law-like fashion.

02:52.920 --> 02:54.680
Also, there is no force overlay.

02:54.680 --> 02:57.160
So there are other forces, like electromagnetic forces.

02:57.160 --> 03:00.440
There's, for example, light that comes out of the sun.

03:00.440 --> 03:04.720
But it doesn't interact significantly with gravitation.

03:04.720 --> 03:07.480
So if I model the way the planet moves around the sun,

03:07.480 --> 03:11.840
I don't need to take into account other forces than gravitation.

03:11.840 --> 03:19.640
The phase space in which the elements are placed or occur

03:19.640 --> 03:23.080
is deterministic and ergodic.

03:23.080 --> 03:26.480
So ergodicity is shown in a small inset here.

03:26.480 --> 03:34.680
So an ergodic phase space means that all accessible micro space

03:34.680 --> 03:39.640
states of the space are actually probable over a long time.

03:39.640 --> 03:45.640
So basically, in simple words, I can get to everywhere in this space

03:45.640 --> 03:50.360
with the same probability if I wait long enough.

03:50.360 --> 03:53.200
For example, if I have gas in a bottle,

03:53.200 --> 03:56.920
the molecules of the gas will distribute

03:56.920 --> 04:00.800
equi-probably over the volume of the bottle.

04:00.800 --> 04:05.680
And so it has to be such a phase space is ergodic,

04:05.680 --> 04:08.120
whereas there are also non-ergodic spaces to which

04:08.120 --> 04:11.000
we'll get back in a minute.

04:11.000 --> 04:13.800
Such Newtonian systems are non-driven.

04:13.800 --> 04:23.000
Drivenness means that there is no force flowing through the system.

04:23.000 --> 04:31.680
So for example, a steam engine has a driven aspect.

04:31.680 --> 04:36.480
Because all the time while it's driving or under energy,

04:36.480 --> 04:39.680
all the time energy is entering into the steam engine

04:39.680 --> 04:41.320
by the burning of the coals.

04:41.320 --> 04:43.200
And then it's been dissipated.

04:43.200 --> 04:47.160
And in Newtonian systems, that's not the case.

04:47.160 --> 04:49.880
Such systems have no evolution properties.

04:49.880 --> 04:52.840
So they don't obtain new element types.

04:52.840 --> 04:54.800
They have the element types they have.

04:54.800 --> 04:58.600
So yes, this solar system could get a new planet

04:58.600 --> 05:02.080
because there could be a big asteroid, could approach the sun,

05:02.080 --> 05:04.200
and could be attracted by the sun,

05:04.200 --> 05:10.920
and then start to orbit around the sun in the way the planets do.

05:10.920 --> 05:12.720
But that wouldn't be a new element type.

05:12.720 --> 05:15.160
It would just be a new element.

05:15.160 --> 05:17.080
And they have fixed boundary conditions.

05:17.080 --> 05:21.840
That is, if the solar system is four light-years away

05:21.840 --> 05:25.400
from the next solar system, which is alpha centauri,

05:25.400 --> 05:28.520
now if it would just be displaced by one or two light-years,

05:28.520 --> 05:31.960
or even three light-years, this wouldn't change anything.

05:31.960 --> 05:35.560
So basically, I can take the solar system out of its context

05:35.560 --> 05:38.800
and move it away, and it wouldn't change anything.

05:38.800 --> 05:42.840
Of course, if I would move it very, very close to alpha centauri,

05:42.840 --> 05:45.200
then the sun of alpha centauri and our sun

05:45.200 --> 05:47.520
would start to interact by gravitation,

05:47.520 --> 05:50.880
and then very terrible events could happen.

05:50.880 --> 05:55.320
But basically, Ceteris paribus,

05:55.320 --> 05:58.800
I can just take such a Newtonian system out of its context.

05:58.800 --> 06:01.400
Now, complex systems are completely different.

06:01.440 --> 06:04.800
They depend on multiple arbitrary element types.

06:04.800 --> 06:08.520
They have different interaction types between elements.

06:08.520 --> 06:09.640
They have force overlay.

06:09.640 --> 06:12.720
So that means that several forces act at the same time

06:12.720 --> 06:14.680
and also interact.

06:14.680 --> 06:19.200
The phase spaces that they have cannot be predicted

06:19.200 --> 06:22.120
from their system elements, and they are non-ergodic.

06:22.120 --> 06:26.440
So they behave in a way that the microstates

06:26.440 --> 06:29.840
are not accessed over a long time with the same probability.

06:29.840 --> 06:31.240
They're also driven.

06:31.240 --> 06:33.680
So they have inner or external drive.

06:33.680 --> 06:35.840
External drive is, for example, the steam engine

06:35.840 --> 06:37.920
that gets heated from coal.

06:37.920 --> 06:41.040
Internal drive is what humans or bacteria have.

06:41.040 --> 06:45.240
This is the drive to reproduce and also to survive.

06:45.240 --> 06:48.240
And drivenness means that there's a flow of energy

06:48.240 --> 06:50.320
flowing through the system all the time

06:50.320 --> 06:53.080
and that this energy is dissipating.

06:53.080 --> 06:55.160
And they lack an equilibrium state

06:55.160 --> 06:57.080
to which they would constantly be converging.

06:57.080 --> 07:00.320
So a driven system doesn't come to equilibrium.

07:00.320 --> 07:02.800
It's always it goes on.

07:02.800 --> 07:05.400
But when the system, when an organism dies,

07:05.400 --> 07:08.120
then it stops being driven and then it also

07:08.120 --> 07:11.400
converges towards an equilibrium state, which

07:11.400 --> 07:14.040
is, in this case, entropy.

07:14.040 --> 07:16.880
Also, complex systems have evolutionary properties.

07:16.880 --> 07:19.760
So they can evolve new element types.

07:19.760 --> 07:22.160
And they have non-fixable boundary conditions.

07:22.160 --> 07:24.760
So they are context dependent.

07:24.760 --> 07:29.200
You can read this comparison of complex and classically

07:29.200 --> 07:31.720
returning systems in turn at a very good book

07:31.720 --> 07:34.920
about complex systems.

07:34.920 --> 07:36.680
So let's look at some examples.

07:36.680 --> 07:40.040
We have those seven properties of complex systems.

07:40.040 --> 07:43.440
And the solar system has none of these properties

07:43.440 --> 07:45.680
because it's not a complex system.

07:45.680 --> 07:48.480
The steam engine has one property, it is driven.

07:48.480 --> 07:52.160
However, to reason about the steam engine,

07:52.160 --> 07:55.200
in many ways, you can abstract from its drivenness.

07:55.200 --> 07:59.080
So for example, the velocity of the steam engine,

07:59.080 --> 08:05.200
if it's used to drive a train, is

08:05.200 --> 08:08.240
proportional to the pressure that it builds up and so on.

08:08.240 --> 08:10.920
So this property, if it's the only driven property,

08:10.920 --> 08:12.920
you can abstract from it for many predictions

08:12.920 --> 08:15.440
you want to make about the behavior.

08:15.440 --> 08:22.760
Pryon is protein that can infect the brain

08:22.760 --> 08:24.600
and cause damage in the brain.

08:24.600 --> 08:27.480
You have heard of Jacob Kreuzfeld disease.

08:27.480 --> 08:32.480
And you may also heard of bovine spongiform encephalopathy,

08:32.480 --> 08:34.600
which is also a prior disease.

08:34.600 --> 08:39.360
And it has only two complex system properties, namely

08:39.360 --> 08:43.920
force overlay and a non-negotic phase space.

08:43.920 --> 08:47.080
And it has also a non-fixed boundary conditions,

08:47.080 --> 08:48.760
but it lacks all the others.

08:48.760 --> 08:52.440
But then as soon as I get to the virus,

08:52.440 --> 08:55.000
I almost have all the properties.

08:55.000 --> 08:58.120
Viruses, although not driven, because it cannot synthesize

08:58.120 --> 08:59.040
energy.

08:59.040 --> 09:01.440
And then with the most primitive organism,

09:01.440 --> 09:04.040
I have all the properties of a complex system.

09:04.040 --> 09:10.000
So it is important to realize that most systems in nature

09:10.000 --> 09:13.640
are complex or deterministically chaotic.

09:13.640 --> 09:18.240
And so basically, there's only a very little Newtonian systems

09:18.240 --> 09:19.440
out there.

09:19.440 --> 09:22.880
And most of the Newtonian systems that we master

09:23.440 --> 09:25.840
are technically devices that we have designed out there.

09:25.840 --> 09:27.080
All of them are Newtonian.

09:27.080 --> 09:29.600
All of them are built using equations

09:29.600 --> 09:31.400
that we have designed ourselves.

09:31.400 --> 09:33.360
And that's what we really understand and master.

09:33.360 --> 09:36.920
But nature is chaotic and complex.

09:36.920 --> 09:39.520
And humans react irrationally to it often.

09:39.520 --> 09:43.560
So you can, if you think of how, for example, we are now

09:43.560 --> 09:47.000
reacting to this virus.

09:47.000 --> 09:50.840
What it's causing is complex, but the reaction is irrational.

09:50.840 --> 09:54.640
And that's because we feel that we cannot control it.

09:54.640 --> 09:56.840
On the next page, you can see.

09:56.840 --> 09:59.280
So I will give you the opportunity

09:59.280 --> 10:01.440
to ask questions after this slide.

10:01.440 --> 10:04.880
So on the next slide, we see now the problem

10:04.880 --> 10:08.240
that complex systems pose to machine learning algorithms.

10:08.240 --> 10:10.120
So basically, machine learning algorithms

10:10.120 --> 10:14.640
cannot model complex systems because such algorithms

10:14.640 --> 10:18.200
are large auto-parameterized differential equations,

10:18.200 --> 10:20.400
partially auto-parameterized.

10:20.400 --> 10:23.200
And let's look at the problems of machine learning models.

10:23.200 --> 10:25.160
So on the left-hand side, you see the problems

10:25.160 --> 10:26.720
that everybody know.

10:26.720 --> 10:29.440
So that they optimize problem-specific loss

10:29.440 --> 10:32.000
functions that don't generalize well,

10:32.000 --> 10:35.280
that they narrowly depend on the selected training samples

10:35.280 --> 10:38.240
and the specific annotations of these samples,

10:38.240 --> 10:40.360
that they fail upon heterogeneous annotation

10:40.360 --> 10:41.400
of identical input.

10:41.400 --> 10:44.680
So identical input by different output,

10:44.680 --> 10:46.480
it will be very stressful, so to speak.

10:46.480 --> 10:49.520
I mean, it will not train well.

10:49.560 --> 10:53.200
They fail on sparsely-populated sample space parts,

10:53.200 --> 10:56.520
which is very often a very big problem

10:56.520 --> 11:00.600
and that explains why very often machine learning algorithms

11:00.600 --> 11:01.800
need so many samples.

11:01.800 --> 11:04.920
Because if you look at the Curse of Dimensionality,

11:04.920 --> 11:08.160
which you can, for example, read in Trevor Haste's

11:08.160 --> 11:11.480
wonderful book about statistical learning,

11:11.480 --> 11:13.600
you will see in chapter one or chapter two

11:13.600 --> 11:15.440
where he explains the Curse of Dimensionality

11:15.440 --> 11:18.920
that very quickly, you come from a topological perspective

11:18.960 --> 11:22.480
to a very sparsely-populated sample space areas.

11:22.480 --> 11:25.680
And in such areas, the machine doesn't learn anything.

11:25.680 --> 11:27.400
And that's very interesting because humans

11:27.400 --> 11:31.120
and also animals are very good at applying

11:31.120 --> 11:34.120
the intelligence to sparsely-populated sample space.

11:34.120 --> 11:36.600
It's basically, that's the key of intelligence.

11:36.600 --> 11:40.080
Intelligence means the ability to react to new situations.

11:40.080 --> 11:42.840
And machine learning models completely fail in new situations,

11:42.840 --> 11:45.920
which are such sparsely-populated sample space.

11:45.920 --> 11:47.920
They cannot be guaranteed to move

11:47.920 --> 11:49.800
to corrected outcomes.

11:49.800 --> 11:52.640
So if you have an error, erroneous behavior

11:52.640 --> 11:56.800
of such a system, and you try to correct the system

11:56.800 --> 11:59.760
once you've noted the error, it's very hard to guarantee

11:59.760 --> 12:01.480
that it moves to the corrected outcome.

12:01.480 --> 12:04.640
And while it's doing this, it may start to make new mistakes

12:04.640 --> 12:07.320
because remember that the model itself is nothing

12:07.320 --> 12:10.680
but a hyperplane in a k-dimensional space.

12:10.680 --> 12:14.720
And this hyperplane, of course, I mean,

12:15.400 --> 12:20.200
if you change its shape to get a certain effect,

12:20.200 --> 12:22.360
you may get other effects you don't want to.

12:23.400 --> 12:26.280
Such models cannot perceive their own failure, of course.

12:27.280 --> 12:29.240
So they cannot really raise exceptions.

12:29.240 --> 12:30.400
Well, they can raise an exception

12:30.400 --> 12:35.080
if the data don't match the input type, but not much more.

12:35.080 --> 12:37.880
They cannot model far-reaching relationships.

12:37.880 --> 12:40.360
They cannot model semantics of mental type,

12:40.360 --> 12:41.880
objectification semantics.

12:41.880 --> 12:43.640
Here I'm using the German word, I apologize.

12:43.640 --> 12:48.000
So it's object type, mental types they cannot use.

12:48.000 --> 12:49.360
And that's why they fail at image

12:49.360 --> 12:52.480
or language interpretation, they fail completely.

12:52.480 --> 12:57.320
And they, of course, have not a sufficient exactness

12:57.320 --> 12:58.760
for really critical settings.

13:00.360 --> 13:02.760
That's the general problems that have been known

13:02.760 --> 13:05.840
for machine learning, even before neural networks

13:05.840 --> 13:08.720
were invented already, I mean,

13:08.720 --> 13:10.760
before the deep neural networks were invented.

13:10.760 --> 13:14.120
Even in the 1970s, when there was just logistic regression

13:14.120 --> 13:17.240
and perceptron approach, it was already clear

13:17.240 --> 13:21.160
to every statistician that these are the problems

13:21.160 --> 13:22.400
of such models.

13:22.400 --> 13:24.840
On the other hand, there are also problems

13:24.840 --> 13:28.200
that are less well-known, that are described in our book.

13:28.200 --> 13:30.920
There will be no singularity that we expect to publish this year.

13:30.920 --> 13:34.520
It's with the, currently with the publisher for review,

13:34.520 --> 13:38.680
which are related to the fact that human behavior

13:38.720 --> 13:42.840
is emanating from a complex system, the human.

13:42.840 --> 13:46.400
And so if you look at what the problem is here,

13:46.400 --> 13:51.400
first of all, machine learning models require ergodicity

13:52.480 --> 13:55.480
because the samples must be assumed to be drawn

13:55.480 --> 13:57.360
from a representative distribution.

13:57.360 --> 14:00.960
So if you want to train a complex system,

14:00.960 --> 14:03.200
you need to have a representative distribution

14:03.200 --> 14:05.880
because next time you draw a sample,

14:05.880 --> 14:07.480
if it doesn't come from the distribution

14:07.480 --> 14:09.520
which you used to train, the model will fail

14:09.520 --> 14:11.560
because of reason number four.

14:11.560 --> 14:14.920
However, reason number one on the right-hand side

14:14.920 --> 14:18.680
gives you states that complex systems

14:18.680 --> 14:21.200
create non-ergodic distributions.

14:21.200 --> 14:23.880
And so when you draw from, when you make a photo,

14:23.880 --> 14:25.600
so to speak, from a non-complex,

14:25.600 --> 14:29.080
from complex system behavior-derived situation,

14:30.000 --> 14:33.560
this photograph, apparently in quote marks,

14:33.560 --> 14:35.480
is basically always skewed.

14:35.480 --> 14:36.640
That's the most important thing

14:36.640 --> 14:38.360
you have to learn today.

14:38.360 --> 14:40.760
So when you record human behavior,

14:40.760 --> 14:43.200
this behavior is never representative.

14:43.200 --> 14:44.880
Because, so in other words,

14:44.880 --> 14:47.280
there is no multivariate distribution to draw from

14:47.280 --> 14:49.320
because each situation is new.

14:49.320 --> 14:52.120
And the samples that you can draw are always outdated.

14:52.120 --> 14:56.000
So what you do is you train with this huge snap

14:56.000 --> 14:58.040
when you sample from a complex system.

14:58.040 --> 15:00.080
So the sample space is always sparse.

15:00.080 --> 15:03.280
I priori, that's really bad for complex systems

15:03.280 --> 15:05.960
and that's bad for ML algorithms.

15:05.960 --> 15:06.800
They cannot deal with this.

15:06.800 --> 15:08.240
They need repetitive pattern.

15:09.280 --> 15:11.960
Of course, then this is really the take-home message

15:11.960 --> 15:15.080
number one, that if you have a situation

15:15.080 --> 15:18.000
or an environment created by a complex system,

15:18.000 --> 15:22.080
then this is always a non-ergodic distribution

15:22.080 --> 15:24.520
deriving from a non-ergodic process.

15:24.520 --> 15:27.080
And such a distribution can never be represented.

15:28.280 --> 15:31.560
So also they don't have evolutionary property.

15:31.560 --> 15:33.120
They cannot model evolutionary properties

15:33.120 --> 15:36.720
because they model a fixed input-output relationship.

15:36.720 --> 15:38.720
But when you have evolutionary properties,

15:38.720 --> 15:41.200
input-output relationships change all the time.

15:42.880 --> 15:45.760
Because they are differentiable models,

15:45.760 --> 15:49.640
they cannot model non-continuous operators or functionals.

15:49.640 --> 15:50.880
And also they can, of course,

15:50.880 --> 15:52.840
not model non-isotropic forces.

15:54.760 --> 15:58.360
They cannot model elements specific interaction types.

15:58.360 --> 16:01.280
And also force overlay or interaction

16:01.280 --> 16:03.040
is very hard to model.

16:03.040 --> 16:07.680
So for them,

16:07.680 --> 16:09.760
and also they cannot model drivenness,

16:09.760 --> 16:11.960
which is energy dissipation.

16:11.960 --> 16:15.320
To give you a very simple example why this doesn't work,

16:15.320 --> 16:20.320
we haven't found any way to model the flow of water

16:20.440 --> 16:24.240
into, for example, water reservoirs.

16:24.240 --> 16:25.840
So turbulence.

16:25.840 --> 16:28.400
Turbulence is one of the most simplest

16:30.600 --> 16:32.400
natural phenomena of drivenness

16:32.400 --> 16:35.160
because the energy from the water flow dissipates

16:35.160 --> 16:38.600
in the reservoir to which the water is flowing,

16:38.600 --> 16:40.280
but we cannot model it.

16:40.280 --> 16:42.720
And so we cannot model how the energy

16:42.720 --> 16:45.480
that is the kinetic energy of the water distributes

16:46.440 --> 16:49.080
into the reservoir.

16:50.160 --> 16:55.160
And also complex systems cannot model contextuality,

16:55.320 --> 16:58.320
sorry, context-free machine learning models,

16:58.320 --> 17:00.640
cannot model contextuality.

17:00.640 --> 17:02.600
Because complex systems are always contextual,

17:02.600 --> 17:05.760
but machine learning models are always context-free.

17:05.760 --> 17:09.040
And so you have a discrepancy between the context-freeness

17:09.040 --> 17:10.080
of the machine learning model

17:10.080 --> 17:12.520
and the contextuality of the complex system.

17:12.520 --> 17:13.800
So this is a very,

17:13.800 --> 17:16.600
I don't know how long I spoke, maybe 15 minutes,

17:16.600 --> 17:19.120
in very short words.

17:19.120 --> 17:24.120
That's what complex systems,

17:24.120 --> 17:28.360
the problem is complex systems modeling in machine learning.

17:31.480 --> 17:33.320
Hello, are you still there?

17:33.320 --> 17:34.640
I'm still here.

17:34.640 --> 17:38.640
So now the students are required to challenge you.

17:42.040 --> 17:44.080
I have a question.

17:44.080 --> 17:44.920
Yes.

17:46.480 --> 17:51.480
So the issues raised here were the problems

17:52.320 --> 17:56.120
of modeling complex systems in the context

17:56.120 --> 17:58.080
of neural networks and deep learning.

17:58.520 --> 17:59.360
Machine learning.

17:59.360 --> 18:01.680
Machine learning, all right.

18:01.680 --> 18:04.760
But the thing is humans also haven't been able

18:04.760 --> 18:06.560
to model turbulence.

18:06.560 --> 18:08.840
And humans haven't been able to,

18:08.840 --> 18:11.440
so to an extent we've been able to model

18:11.440 --> 18:12.800
weather forecast systems.

18:12.800 --> 18:15.320
So we use certain sets of differential equations there.

18:17.320 --> 18:21.320
But if ML systems can, to a certain extent,

18:21.320 --> 18:23.120
as was mentioned in the slide,

18:23.120 --> 18:25.600
auto-parameterized differential equations model

18:26.600 --> 18:30.600
some extent of that, then why should it be considered

18:30.600 --> 18:33.600
a sign of no intelligence,

18:33.600 --> 18:35.600
even though humans have also not been able to do

18:35.600 --> 18:36.600
the same thing?

18:36.600 --> 18:41.600
So why this requirement from machine learning?

18:44.600 --> 18:48.600
So I'm not saying that it is a sign of intelligence.

18:48.600 --> 18:51.600
So now we are not talking about intelligence right now.

18:51.600 --> 18:54.600
We are only talking about what can be modeled with machine.

18:55.600 --> 18:58.600
And what I have not said is I've skipped

18:58.600 --> 18:59.600
a little bit of content.

18:59.600 --> 19:03.600
So what I've not said is that all the phenomena

19:03.600 --> 19:07.600
that are complex in nature can't be modeled using mathematics.

19:07.600 --> 19:10.600
So of course we cannot model complex systems mathematics.

19:10.600 --> 19:13.600
And so, but that's not because we are not intelligent,

19:13.600 --> 19:17.600
but because complex system modeling seems to be

19:17.600 --> 19:21.600
beyond the hardware that human intelligence possesses.

19:21.600 --> 19:23.600
So but that doesn't mean that humans are not intelligent.

19:23.600 --> 19:25.600
It just means that we can't model.

19:25.600 --> 19:27.600
There's no way to model complex systems,

19:27.600 --> 19:30.600
neither with paper and pen mathematics,

19:30.600 --> 19:33.600
nor with whatever kind of algorithm.

19:33.600 --> 19:36.600
And so there, but that doesn't mean that we are not intelligent.

19:36.600 --> 19:39.600
It just means that this is beyond our modeling capability.

19:39.600 --> 19:42.600
But this taken aside,

19:42.600 --> 19:46.600
when it comes to creating ML models,

19:46.600 --> 19:51.600
and we want to model our, or let's say animal intelligence,

19:51.600 --> 19:54.600
then we would have to model the output of a complex system,

19:54.600 --> 19:57.600
which not even humans can let alone machines.

19:59.600 --> 20:00.600
Okay.

20:00.600 --> 20:01.600
All right.

20:01.600 --> 20:04.600
But at least humans have been able to approximate certain

20:04.600 --> 20:06.600
complex systems like weather forecast systems.

20:07.600 --> 20:08.600
So yes.

20:08.600 --> 20:10.600
So that's, so go ahead.

20:11.600 --> 20:13.600
So my question then would be,

20:15.600 --> 20:19.600
what do you think of the potential for machine learning systems

20:19.600 --> 20:22.600
to approximate rather than create an exact model?

20:22.600 --> 20:24.600
So, so a very good question.

20:24.600 --> 20:28.600
So whether forecast models are approximative machine learning models,

20:28.600 --> 20:29.600
right?

20:29.600 --> 20:31.600
So many of them are built using machine learning.

20:31.600 --> 20:34.600
So of course it's possible to approximate certain

20:34.600 --> 20:36.600
complex phenomena using machine learning,

20:36.600 --> 20:38.600
but it's quite limited what you can achieve.

20:38.600 --> 20:40.600
Yeah, you can achieve some.

20:40.600 --> 20:42.600
The question is always how good is the approximation

20:42.600 --> 20:44.600
and what can you technically do with it?

20:44.600 --> 20:47.600
And so if you look at the tetanosphere we have,

20:47.600 --> 20:49.600
all the technical gadgets that surround us,

20:49.600 --> 20:52.600
that make our life so much easier and better,

20:52.600 --> 20:54.600
most of them are exact.

20:55.600 --> 20:58.600
So mobile phones are exact, bridges are exact, trains are exact,

20:58.600 --> 21:00.600
airplanes are exact, cars are exact.

21:00.600 --> 21:04.600
So we don't have so many approximative models in the

21:04.600 --> 21:05.600
tetanosphere.

21:05.600 --> 21:08.600
So one, so Google, Google advertising plays is

21:08.600 --> 21:11.600
approximative, but why can Google afford it?

21:11.600 --> 21:15.600
Because nobody gets killed if I get shown a female

21:16.600 --> 21:19.600
lipstick ad, right?

21:19.600 --> 21:22.600
So nobody gets killed if I get shown a lipstick ad,

21:22.600 --> 21:24.600
so they can afford to do it.

21:24.600 --> 21:28.600
But if they would use these algorithms in intensive care,

21:28.600 --> 21:31.600
unit machinery, they would kill people.

21:31.600 --> 21:32.600
So that's the point.

21:32.600 --> 21:35.600
The point is approximative modeling is fine,

21:35.600 --> 21:38.600
but you have to decide in practice for what purposes

21:38.600 --> 21:40.600
you can use it and where you can't use it.

21:40.600 --> 21:41.600
Perfect.

21:41.600 --> 21:42.600
Thank you.

21:42.600 --> 21:43.600
Thank you.

21:43.600 --> 21:44.600
Thank you very much.

21:46.600 --> 21:48.600
Any more questions?

21:54.600 --> 21:56.600
So I guess I'll try.

21:56.600 --> 22:01.600
So if you take a, we use this example already a couple of

22:01.600 --> 22:04.600
times, if you take a laptop and throw it into a river,

22:04.600 --> 22:09.600
then the laptop no longer behaves in such a way that

22:09.600 --> 22:11.600
it is a simple system.

22:11.600 --> 22:14.600
It starts to decay.

22:14.600 --> 22:17.600
And that's because the laptop plus the river is a

22:17.600 --> 22:19.600
complex system.

22:19.600 --> 22:22.600
Is that a correct account?

22:22.600 --> 22:27.600
Well, I mean the system as a whole, so when the laptop

22:27.600 --> 22:31.600
drops into the river or creek or whatever,

22:31.600 --> 22:34.600
the creek has mechanical energy in the water.

22:34.600 --> 22:39.600
And this mechanical energy will start to attack the

22:39.600 --> 22:40.600
structure of the laptop.

22:40.600 --> 22:44.600
And then of course chemical processes that are not in

22:44.600 --> 22:48.600
equilibrium because the water is driven is driven will

22:48.600 --> 22:50.600
also attack the laptop.

22:50.600 --> 22:54.600
So the laptop in a way will become part of a

22:54.600 --> 22:57.600
complex system, but on its own it will not be a

22:57.600 --> 22:58.600
complex system.

22:58.600 --> 23:01.600
So if you take it out and those forces stop working

23:01.600 --> 23:06.600
on it, then it will stay in this more or less in a

23:06.600 --> 23:08.600
certain form of decay.

23:08.600 --> 23:11.600
Although in the long term, even if you leave a laptop

23:11.600 --> 23:14.600
standing somewhere without water, it will also be

23:14.600 --> 23:16.600
part of a complex system because there will also be

23:16.600 --> 23:19.600
energy working on it, but much less so.

23:19.600 --> 23:22.600
So if it's in a building, to begin with it will take

23:22.600 --> 23:24.600
one or 200 years before the building is broken and

23:24.600 --> 23:27.600
then energy can really work on the laptop.

23:27.600 --> 23:32.600
Can you go back to slide two?

23:32.600 --> 23:34.600
Yes.

23:34.600 --> 23:37.600
So you give C elegans as an example of a complex

23:37.600 --> 23:38.600
system.

23:38.600 --> 23:41.600
I assume you would give a human being as an example of

23:41.600 --> 23:42.600
a complex system.

23:42.600 --> 23:43.600
Yeah, of course.

23:43.600 --> 23:44.600
Yes.

23:44.600 --> 23:46.600
Now, but can you tell me how a human being has

23:46.600 --> 23:48.600
evolutionary properties?

23:48.600 --> 23:49.600
Yes.

23:49.600 --> 23:55.600
So the evolutionary properties of living organisms

23:55.600 --> 24:03.600
constitute, come from the fact that new types of

24:03.600 --> 24:08.600
molecules can be created in adult organisms.

24:08.600 --> 24:12.600
So if you have an adult organism by its ability to

24:12.600 --> 24:15.600
react to the environment, it can create new types of

24:15.600 --> 24:19.600
macromolecules that it hasn't had before, like new

24:19.600 --> 24:23.600
memories and all new combinations of elements.

24:23.600 --> 24:26.600
And that's all it can actually change the way it

24:26.600 --> 24:29.600
methylates its DNA and therefore change the way it

24:29.600 --> 24:34.600
will, the inheritance will work if it becomes

24:34.600 --> 24:37.600
progenitor of new organisms.

24:37.600 --> 24:41.600
So this is the way that new elements and element

24:41.600 --> 24:46.600
interactions can arise in adult organisms.

24:46.600 --> 24:47.600
Good.

24:47.600 --> 24:52.600
Okay, any more questions from anybody?

24:52.600 --> 24:54.600
Yes, I will have one.

24:54.600 --> 24:57.600
I believe it's a very boring, usual philosophical

24:57.600 --> 25:02.600
question, but when we say that complex systems are

25:02.600 --> 25:03.600
not...

25:03.600 --> 25:07.600
We don't have a mathematical modeling for them.

25:07.600 --> 25:08.600
Is this correct?

25:08.600 --> 25:09.600
Yes.

25:09.600 --> 25:12.600
Is this intended to be one of our epistemic

25:12.600 --> 25:16.600
lacks or something which happens in nature

25:16.600 --> 25:21.600
unregarded of some very, very complex mathematical

25:21.600 --> 25:25.600
model, which at the moment we don't have, but...

25:25.600 --> 25:26.600
This is a very good question.

25:26.600 --> 25:30.600
So we don't know this, so we can't really give an answer

25:30.600 --> 25:31.600
to this question.

25:31.600 --> 25:32.600
It could be speculative.

25:32.600 --> 25:36.600
However, my view is that our ability for mathematical

25:36.600 --> 25:40.600
modeling is an evolutionary adaptation of humans.

25:40.600 --> 25:44.600
So very much like language is the way our hands work,

25:44.600 --> 25:46.600
our evolutionary adaptations.

25:46.600 --> 25:49.600
And it is a special evolutionary adaptation of our

25:49.600 --> 25:54.600
mind, the extent of which also strongly varies

25:54.600 --> 25:55.600
between individuals.

25:55.600 --> 25:58.600
So there are only out of a thousand, only one or two

25:58.600 --> 26:01.600
individuals are usually mathematically gifted.

26:01.600 --> 26:05.600
And so it has a high variance, but everybody with an IQ

26:05.600 --> 26:07.600
above 80 can count.

26:07.600 --> 26:11.600
And this skill is limited.

26:11.600 --> 26:15.600
And I think it's limited by the forces that shaped it

26:15.600 --> 26:16.600
during evolution.

26:16.600 --> 26:22.600
And so I think that certain aspects of nature are just

26:22.600 --> 26:26.600
too much, so to speak, for the structure of this skill.

26:26.600 --> 26:29.600
And this seems very plausible, given the history of

26:29.600 --> 26:32.600
mathematics so far.

26:32.600 --> 26:37.600
However, because in the end, all the mathematical objects

26:37.600 --> 26:42.600
that we know can be reduced to numbers and are very

26:42.600 --> 26:45.600
complicated combinations of them.

26:45.600 --> 26:47.600
So even, for example, if the invention of calculus by

26:47.600 --> 26:51.600
Newton and Leibniz seems like a very big step, the way

26:51.600 --> 26:53.600
they did it was quite geometrical.

26:53.600 --> 26:57.600
And great inventions are always great, but it's still

26:57.600 --> 27:00.600
linked, of course, to the history of mathematics.

27:00.600 --> 27:03.600
So I don't think that we will ever be able to model fully

27:03.600 --> 27:06.600
model complex systems, but we were able to, to some extent,

27:06.600 --> 27:12.600
of course, approximately model aspects of them.

27:12.600 --> 27:15.600
So in other words, I think the structural deficit.

27:15.600 --> 27:16.600
Yes, yes.

27:16.600 --> 27:19.600
Thank you.

27:19.600 --> 27:21.600
I also have another question.

27:21.600 --> 27:22.600
Yes.

27:22.600 --> 27:27.600
So if we consider the fact that how mathematics have evolved

27:27.600 --> 27:32.600
so far, and also the fact that mathematical modeling has

27:32.600 --> 27:38.600
up to a point tried to represent, in a way, the randomness

27:38.600 --> 27:45.600
that might be an attribute of these complex systems.

27:45.600 --> 27:50.600
So we have already made some, find out some mathematical

27:50.600 --> 27:53.600
ways through statistics and stochastic processes in order

27:53.600 --> 27:55.600
to study randomness.

27:55.600 --> 27:57.600
Yeah, of course.

27:57.600 --> 28:02.600
And so do you think that this is like a big step, a small

28:02.600 --> 28:06.600
step to accomplish something even bigger in the future?

28:06.600 --> 28:09.600
Because up until a point, we also thought that randomness

28:09.600 --> 28:12.600
was something that we couldn't even explain.

28:12.600 --> 28:15.600
And computationally.

28:15.600 --> 28:17.600
That's, that's not fully true.

28:17.600 --> 28:22.600
For example, there is no mathematical model for a true

28:22.600 --> 28:25.600
natural number generator, a random number generator.

28:25.600 --> 28:29.600
So a true random number generator, which produces true

28:29.600 --> 28:32.600
random events can only be constructed by using a physical

28:32.600 --> 28:36.600
device, namely normally one uses a Geiger counter, which is

28:36.600 --> 28:40.600
counting radioactive decay events, because they are truly

28:40.600 --> 28:41.600
random.

28:41.600 --> 28:43.600
So we cannot simulate randomness.

28:43.600 --> 28:46.600
Of course, we can model certain aspects of randomness.

28:46.600 --> 28:49.600
And this we can do since a couple of hundred years.

28:49.600 --> 28:53.600
And at the end of the 1980th century, we have started to

28:53.600 --> 28:57.600
understand how calculus can be applied to this.

28:57.600 --> 29:02.600
But the way that we model randomness is only applies to

29:02.600 --> 29:04.600
classical Newtonian systems.

29:04.600 --> 29:07.600
And when we get out of Newtonian systems, there are actually

29:07.600 --> 29:11.600
no examples to convincingly model a complex system behavior.

29:11.600 --> 29:15.600
So they are, of course, like one of your colleagues mentioned,

29:15.600 --> 29:19.600
approximative models of randomness, such as, or

29:19.600 --> 29:23.600
stochastic behavior, such as weather forecasting, but they

29:23.600 --> 29:26.600
have a very short-term forecast window.

29:26.600 --> 29:29.600
And they are not very, depending on in which landscape you are,

29:29.600 --> 29:31.600
they almost don't work at all.

29:31.600 --> 29:35.600
So near the sea or in the mountains, they almost do not work.

29:35.600 --> 29:39.600
And basically, they work for regular continental climate with

29:39.600 --> 29:42.600
strong determinants over very short durations.

29:42.600 --> 29:49.600
And so the fact that we have probability theory doesn't

29:49.600 --> 29:51.600
really help with complex systems.

29:51.600 --> 29:54.600
Turbulence, which I already mentioned, is a very good example

29:54.600 --> 29:59.600
because the mathematician who had found the best way so far of

29:59.600 --> 30:04.600
dealing with turbulence was actually probabilistic

30:04.600 --> 30:07.600
theoretician, was Kolmogorov, who invented the Kolmogorov-Smilnov

30:07.600 --> 30:10.600
theorem, one of the greatest mathematical geniuses of the

30:10.600 --> 30:11.600
20th century.

30:11.600 --> 30:14.600
In the 1940s, he tried to model turbulence.

30:14.600 --> 30:17.600
And while his model is, you can see it when our book will

30:17.600 --> 30:20.600
appear, it's explained in the book, while his model is very

30:20.600 --> 30:22.600
aesthetically highly valuable.

30:22.600 --> 30:24.600
It's very beautiful.

30:24.600 --> 30:27.600
It fails to model the reality of turbulence.

30:27.600 --> 30:32.600
And so there is no example for exact modeling or good

30:33.600 --> 30:35.600
approximative modeling of complex systems.

30:35.600 --> 30:36.600
And I don't know.

30:36.600 --> 30:44.600
It's actually, if you know how the theory of probability works,

30:44.600 --> 30:47.600
how the distribution are approximated, that it's already

30:47.600 --> 30:51.600
very, very hard actually to figure out or mathematically

30:51.600 --> 30:56.600
impossible to calculate mixed high-dimensional distributions.

30:56.600 --> 30:59.600
You will see that this is very, very far away.

30:59.600 --> 31:02.600
I don't know if it's possible.

31:02.600 --> 31:03.600
Okay.

31:03.600 --> 31:05.600
Thank you for your...

31:05.600 --> 31:06.600
Okay.

31:06.600 --> 31:09.600
I think we'll have yours to continue now for a bit.

31:09.600 --> 31:12.600
So now we look at intelligence.

31:12.600 --> 31:16.600
And you will see in the course of the second part why we needed

31:16.600 --> 31:18.600
to talk about complex systems.

31:18.600 --> 31:21.600
So this here, what you see is the standard definition of the

31:21.600 --> 31:25.600
artificial general intelligence community of intelligence.

31:25.600 --> 31:26.600
And this is...

31:26.600 --> 31:30.600
So they have a verbal definition which goes intelligence

31:30.600 --> 31:33.600
measures an agent's ability to achieve goals in a wide range

31:33.600 --> 31:34.600
of environments.

31:34.600 --> 31:39.600
That's the standard definition that everybody accepts.

31:39.600 --> 31:44.600
And so this ability to achieve goals is defined by a utility

31:44.600 --> 31:45.600
function.

31:45.600 --> 31:50.600
And this utility is here V of an agent pi depending on the

31:50.600 --> 31:52.600
environment of the agent's mu.

31:52.600 --> 31:56.600
And it's defined as the expectation of the sum of the

31:56.600 --> 32:00.600
rewards the agent is going to have, which is actually normed

32:00.600 --> 32:04.600
to be equal less to one.

32:04.600 --> 32:08.600
And so mu is a binary description of the environment of

32:08.600 --> 32:09.600
the agent.

32:09.600 --> 32:13.600
And this environment may be a fired abstract structure that

32:13.600 --> 32:15.600
is manipulated inside a computer.

32:15.600 --> 32:19.600
For example, if you have a theorem prover, then this would

32:19.600 --> 32:22.600
be one environment or it may relate to something in the

32:22.600 --> 32:23.600
physical world.

32:23.600 --> 32:28.600
For example, a nuclear power station that has broken down

32:28.600 --> 32:32.600
and where you now want to clean up the nuclear power station

32:32.600 --> 32:33.600
with a robot.

32:33.600 --> 32:35.600
Because it's not an environment where we would like to use

32:35.600 --> 32:36.600
humans.

32:36.600 --> 32:40.600
And by the way, there's a great documentation about

32:40.600 --> 32:41.600
Chernobyl.

32:41.600 --> 32:43.600
They tried to use robots.

32:43.600 --> 32:47.600
But the problem was that the transistors broke immediately

32:47.600 --> 32:49.600
because radiation was too strong.

32:49.600 --> 32:53.600
So the radiation destroyed the transistors inside the robot

32:53.600 --> 32:55.600
and then they stopped working.

32:55.600 --> 32:58.600
So they had to use humans after all.

32:58.600 --> 33:03.600
And in either case, no matter whether it's a physical

33:03.600 --> 33:09.600
scenario or artificial one, this vector takes a form of a

33:09.600 --> 33:10.600
binary string.

33:10.600 --> 33:17.600
And it's a description which plays a role in the HATTA

33:17.600 --> 33:18.600
model.

33:18.600 --> 33:21.600
And E is the expectation of the rewards.

33:21.600 --> 33:24.600
And what is a reward we will see it on the next slide.

33:24.600 --> 33:26.600
So this is a basic definition.

33:26.600 --> 33:31.600
Now, first of all, let's consider what's that this definition

33:31.600 --> 33:39.600
is actually mathematically broken, which I find kind of sad

33:39.600 --> 33:41.600
because they should at least get this right.

33:41.600 --> 33:44.600
But basically, this is really accepted in the AGI community.

33:44.600 --> 33:47.600
Now, one has to see that the AGI community is made up mainly by

33:47.600 --> 33:49.600
computer scientists and mathematicians.

33:49.600 --> 33:53.600
And computer scientists like physicists and engineers, they

33:53.600 --> 33:56.600
tend to not look so carefully at mathematical equations.

33:56.600 --> 34:02.600
But actually the definition of expectation is the sum of a

34:02.600 --> 34:06.600
variable, which is actually multiplied by the probability of

34:06.600 --> 34:09.600
this variable for each step.

34:09.600 --> 34:12.600
So every finite amount of steps, or here is actually even

34:12.600 --> 34:16.600
infinite, but it doesn't matter if you have some steps, you

34:16.600 --> 34:23.600
should calculate the reward of the variable by summing the

34:23.600 --> 34:26.600
product of the probability with the variable, which is the

34:26.600 --> 34:27.600
reward.

34:27.600 --> 34:29.600
And here they don't do this.

34:29.600 --> 34:32.600
They actually take the expectation of that is already

34:32.600 --> 34:33.600
summed up.

34:34.600 --> 34:38.600
So it's funny because if you wanted to implement this, it's

34:38.600 --> 34:41.600
actually not implementable because mathematically wrong, which

34:41.600 --> 34:44.600
I found quite interesting.

34:44.600 --> 34:48.600
But so in other words, the operator E of the expectation has

34:48.600 --> 34:51.600
to be applied not to the sum of the values of a random

34:51.600 --> 34:54.600
variable, but directly to the values of the variable, as shown

34:54.600 --> 34:56.600
here in this equation.

34:56.600 --> 34:59.600
And otherwise, you cannot take account of the norming

34:59.600 --> 35:00.600
denominator.

35:00.600 --> 35:04.600
So pi i is smaller than 1, and therefore it's a denominator.

35:04.600 --> 35:07.600
If you multiply by a number smaller than 1, it's the same

35:07.600 --> 35:09.600
as dividing by this number.

35:09.600 --> 35:13.600
And so usually you need a denominator.

35:13.600 --> 35:15.600
And if you don't have a denominator, you cannot get the

35:15.600 --> 35:19.600
sum below 1, of course.

35:19.600 --> 35:25.600
Because this is just mathematically silly.

35:25.600 --> 35:28.600
But they've published it, and the viewers have accepted it,

35:28.600 --> 35:31.600
and it gets cited hundreds and hundreds of times.

35:31.600 --> 35:32.600
I'm astonished.

35:32.600 --> 35:35.600
But anyhow, a lot of crap gets cited a lot.

35:35.600 --> 35:37.600
So it seems to be human nature.

35:37.600 --> 35:41.600
But it's remarkable that nobody has criticized this.

35:41.600 --> 35:46.600
Taking this aside and just ignoring the definition

35:46.600 --> 35:49.600
problems and imagining they had used the proper definition

35:49.600 --> 35:53.600
of expectation, which is actually you can read up in any

35:53.600 --> 35:56.600
very basic textbook of statistics.

35:57.600 --> 36:00.600
I mean, actually, it's cool when you learn to calculate

36:00.600 --> 36:04.600
the probability of throwing the number 7 with 2 dice.

36:04.600 --> 36:05.600
You learn this.

36:05.600 --> 36:07.600
So I was kind of astonished.

36:07.600 --> 36:11.600
Anyhow, in simplified terms, when we ignore this, the equation

36:11.600 --> 36:15.600
1 set in agent pi reacting to environment distribution mu

36:15.600 --> 36:18.600
obtains a finite reward, which corresponds to the expectation

36:18.600 --> 36:22.600
of reward it achieves over all the steps it undertakes.

36:22.600 --> 36:27.600
And actually, the higher this is, the better the utility.

36:27.600 --> 36:32.600
And so Barry asked me to insert something about reward.

36:32.600 --> 36:36.600
Of course, the reward here has no meaning for the machine.

36:36.600 --> 36:39.600
The machine is just a calculating machine.

36:39.600 --> 36:40.600
It's a Turing machine.

36:40.600 --> 36:49.600
It can just apply amounts of electricity to certain circuits.

36:49.600 --> 36:54.600
But by doing this, so reward has no meaning for the machine.

36:54.600 --> 36:58.600
That's just a mathematical concept to express an optimization

36:58.600 --> 36:59.600
problem.

36:59.600 --> 37:04.600
So this year, how can I maximize the utility?

37:04.600 --> 37:09.600
I can maximize this utility by doing something with this reward

37:09.600 --> 37:10.600
variable.

37:10.600 --> 37:12.600
And that's all it says.

37:12.600 --> 37:16.600
So reward doesn't mean what is reward for you so that I invite

37:16.600 --> 37:21.600
you to a good beer in the evening or anything or bring you

37:21.600 --> 37:24.600
some flowers or so, what humans experience as reward.

37:24.600 --> 37:27.600
It's just a way of formulating the mathematical optimization

37:27.600 --> 37:29.600
problem.

37:29.600 --> 37:30.600
OK.

37:30.600 --> 37:36.600
So on the next slide, what do they do with this utility?

37:36.600 --> 37:39.600
So here you can see this utility term again.

37:39.600 --> 37:42.600
But now it is in a bigger equation, which defines intelligence.

37:42.600 --> 37:49.600
And this uppercase epsilon, it's a mathematical uppercase epsilon

37:49.600 --> 37:55.600
as a function of the agent is defined as the sum of the utility

37:55.600 --> 37:59.600
multiplied with another factor.

37:59.600 --> 38:01.600
And what is this factor?

38:01.600 --> 38:07.600
So this factor contains k, which is a Kolmogorov complexity

38:07.600 --> 38:09.600
function.

38:09.600 --> 38:11.600
We already heard about Kolmogorov.

38:11.600 --> 38:16.600
He did not only work on probability distributions, but also on

38:16.600 --> 38:20.600
information theory and the complexity function.

38:20.600 --> 38:24.600
He invented it and it's indicating the complexity of the algorithm

38:24.600 --> 38:28.600
executed by the agent pi to represent the environment mu.

38:28.600 --> 38:32.600
So it's basically how many calculation steps are needed to

38:32.600 --> 38:33.600
represent mu.

38:33.600 --> 38:36.600
And as you can see, this is a negative exponent.

38:36.600 --> 38:38.600
So that means that this is a parallelizing factor.

38:38.600 --> 38:43.600
So the more steps I need, the more complex my representation

38:43.600 --> 38:47.600
algorithm is, the lower the intelligence will be.

38:47.600 --> 38:50.600
And that's pretty, I think, acceptable.

38:50.600 --> 38:55.600
Because so if you imagine a more intelligent individual will

38:55.600 --> 38:58.600
find it easier to achieve a goal than a stupid individual.

38:58.600 --> 39:01.600
So of course, you know this from everyday life.

39:01.600 --> 39:04.600
And so this says something similar.

39:04.600 --> 39:09.600
It says, so if you have a high utility for a given environment,

39:09.600 --> 39:13.600
but I needed a million steps to achieve it, then I'm less

39:13.600 --> 39:17.600
intelligent than somebody who achieved the same utility with a

39:17.600 --> 39:20.600
shorter number of steps.

39:20.600 --> 39:24.600
So further, a couple of more remarks.

39:24.600 --> 39:25.600
So mu is the environment.

39:25.600 --> 39:27.600
What is uppercase u?

39:27.600 --> 39:30.600
Uppercase u is a set of environment descriptions that the

39:30.600 --> 39:32.600
machine can process.

39:32.600 --> 39:37.600
So for example, if the machine is the AlphaGo machine, then all

39:37.600 --> 39:42.600
the situations are settings on the gold board and nothing more.

39:42.600 --> 39:46.600
But of course, an AGI agent would be able hopefully to process

39:46.600 --> 39:49.600
more different situations that at least what they hope.

39:49.600 --> 39:53.600
And so u is just a set of environments the agent could in

39:53.600 --> 39:55.600
theory be processing.

39:55.600 --> 39:58.600
Let me give you an example from the animal kingdom.

39:58.600 --> 40:04.600
So for example, a rat has quite a huge set u because a rat can

40:04.600 --> 40:06.600
adapt to very many environments.

40:06.600 --> 40:09.600
And therefore, it is also to be found all around the globe

40:09.600 --> 40:13.600
all mode, not in the Antarctic and not in the Sahara, I believe,

40:13.600 --> 40:15.600
but in many, many moderate environments.

40:15.600 --> 40:19.600
A rat is to be found, whereas other animals are very specialized

40:19.600 --> 40:21.600
and are only found in certain environments.

40:21.600 --> 40:25.600
And this is what this uppercase u is supposed to say.

40:26.600 --> 40:32.600
So basically the definition of intelligence shows to you the

40:32.600 --> 40:37.600
most efficient possible algorithm to achieve a certain utility.

40:37.600 --> 40:43.600
Also, the summation over all environments prevents a random hit.

40:43.600 --> 40:49.600
So because you have to perform this in many environments, you can

40:49.600 --> 40:53.600
guarantee that the equation doesn't give you a distorted outcome.

40:53.600 --> 40:58.600
So if the definition of the utility function would be

40:58.600 --> 41:02.600
mathematically sound, which you could easily do by replacing it

41:02.600 --> 41:06.600
with this, then the entire equation would mathematically

41:06.600 --> 41:08.600
make sense.

41:08.600 --> 41:13.600
So it is a proper definition of a weighted utility function.

41:13.600 --> 41:17.600
That's what it basically is.

41:17.600 --> 41:22.600
Before we take apart these two equations, now I've just

41:22.600 --> 41:29.600
criticized them from a mathematical point of view.

41:29.600 --> 41:33.600
Are there any questions regarding the two equations before I

41:33.600 --> 41:37.600
continue dissecting them?

41:37.600 --> 41:40.600
I have two questions when I was looking at this formula.

41:40.600 --> 41:45.600
So I was just looking at V as the author says,

41:45.600 --> 41:49.600
ability to achieve the value, the capital V.

41:49.600 --> 41:54.600
But then in the formula on the second slide, on this one.

41:54.600 --> 42:01.600
So why is Gogomolo complexity function as exponential?

42:01.600 --> 42:05.600
And the V is just like linear.

42:05.600 --> 42:09.600
Because it's just a negative exponent.

42:09.600 --> 42:13.600
So it means you just divide.

42:13.600 --> 42:16.600
This means 1 divided by 2 exponent k.

42:16.600 --> 42:21.600
So it's just used to actually penalize the utility function.

42:21.600 --> 42:27.600
But why didn't they just penalize with the inverse value?

42:27.600 --> 42:33.600
Why did they choose the exponential growth?

42:33.600 --> 42:36.600
To make it perfect, maybe.

42:36.600 --> 42:38.600
Yeah, maybe.

42:38.600 --> 42:41.600
Actually, I haven't thought about it.

42:41.600 --> 42:45.600
But it's basically the way that the Gogomolo complexity function

42:45.600 --> 42:49.600
always gets applied in theoretical informatics.

42:49.600 --> 42:54.600
So this form is when you read a textbook of theoretical informatics,

42:54.600 --> 42:57.600
you always see it represented like this.

42:57.600 --> 43:03.600
So that you get a smooth representation of the entire function.

43:03.600 --> 43:12.600
So I think it works pretty well to weigh whatever you want to weigh

43:12.600 --> 43:16.600
by the amount of work that you need to put into obtain it.

43:16.600 --> 43:20.600
Yeah, they argue that extensively in the paper anyway.

43:20.600 --> 43:26.600
But the second question is this capital U, the set of environment descriptions.

43:26.600 --> 43:28.600
How is this supposed to be interpreted?

43:28.600 --> 43:31.600
Because it's written as a sum.

43:31.600 --> 43:37.600
But I would take that in general, this is like a manifold in the best case.

43:37.600 --> 43:40.600
So it's a sum over the elements of a set.

43:40.600 --> 43:41.600
That's fine mathematically.

43:41.600 --> 43:47.600
So this is just an index of all the possible terms you can get.

43:47.600 --> 43:52.600
And let's say you would have three different environments.

43:52.600 --> 43:59.600
What if it has a ball or a sphere of different environments?

43:59.600 --> 44:01.600
So you just integrate?

44:01.600 --> 44:03.600
No, I don't think so.

44:03.600 --> 44:08.600
Because you basically can always, this is theoretical informatics.

44:08.600 --> 44:13.600
So that means that you can, no matter what from a functional analysis point of view,

44:13.600 --> 44:17.600
no matter what is the way the environments are,

44:17.600 --> 44:20.600
they are always compressed into binary vectors.

44:20.600 --> 44:26.600
And then you can just create a series of binary vectors or a set of binary vectors.

44:26.600 --> 44:28.600
And this is just one binary vector out of a set.

44:28.600 --> 44:32.600
So it's a countable set of binary vectors, no matter what.

44:32.600 --> 44:38.600
Maybe you should explain what a binary vector is.

44:38.600 --> 44:44.600
Yeah, so for the non-n estimaticians, it's just a vector of ones and zeros.

44:44.600 --> 44:50.600
So remember that a Turing machine can only deal with ones and zeros.

44:50.600 --> 44:56.600
It's like a big tape on which you can write ones and zeros and can change the ones and zeros.

44:56.600 --> 45:01.600
And this is basically, from the perspective of theoretical informatics,

45:01.600 --> 45:06.600
it's just a set of such binary vectors.

45:06.600 --> 45:12.600
Sorry, I think it would have been a bit more helpful if the input and output spaces were also defined.

45:12.600 --> 45:18.600
Because now it's clear to me what, so the inputs are basically strings, as far as I understand it.

45:18.600 --> 45:20.600
Is it?

45:20.600 --> 45:24.600
Yeah, so otherwise the mu is a bit hard to interpret.

45:24.600 --> 45:26.600
Yeah, the mu, yeah, you're right.

45:26.600 --> 45:34.600
So I have not defined, actually it's interesting why I've just copied the definition from their paper,

45:34.600 --> 45:42.600
but you're right, usually one would have to use a functional analysis type of definition of the input space at the output.

45:42.600 --> 45:44.600
Yeah, exactly.

45:44.600 --> 45:50.600
And I could easily have done this, but there are two reasons why they didn't do it for sloppiness,

45:50.600 --> 45:56.600
and I didn't do it because it's just too implicit for me because it's what I do all the time, but thank you.

45:56.600 --> 46:00.600
I think we should add this next time, it's a good point.

46:00.600 --> 46:01.600
Yeah.

46:01.600 --> 46:06.600
Okay, so anyhow, this is a definition.

46:06.600 --> 46:09.600
Now let's move on and look at the problem.

46:09.600 --> 46:14.600
So first of all, there is a verbose definition before the equation,

46:14.600 --> 46:19.600
which says intelligence measures an agent's ability to achieve goals in a wide range of environments.

46:19.600 --> 46:27.600
So first of all, the definition captures just one part of one of the standard definitions of primary intelligence.

46:27.600 --> 46:38.600
So primary intelligence says that you have to adapt to new environments suddenly and without being trained upfront.

46:38.600 --> 46:45.600
So you have to be spontaneously able to adapt to a new environment suddenly, quickly.

46:45.600 --> 46:52.600
And this definition just captures the adaptation.

46:52.600 --> 46:59.600
Now, what is more important is that the definition is very broad because it allows also this organism,

47:00.600 --> 47:08.600
a small worm that is one millimeter long and has a thousand cells and only 300 neurons to be intelligent,

47:08.600 --> 47:12.600
because what it can do, it can forage and reproduce in complex environments,

47:12.600 --> 47:18.600
so it can live in fruit, in vegetables, in mushrooms, in soil, and so on.

47:18.600 --> 47:21.600
It can use snails and stugs as migration vectors.

47:21.600 --> 47:28.600
So it can really live in many environments and it can also reproduce there.

47:28.600 --> 47:34.600
So I think it would be intelligent, according to the AGI definition.

47:34.600 --> 47:38.600
And even probably the amoeba here, this is a nice amoeba.

47:38.600 --> 47:43.600
It's called kaos kaolinensis because it never has the same shape.

47:43.600 --> 47:50.600
Here's just a drawing of it, but in the next second it will look different because it moves around by changing its shape.

47:50.600 --> 47:55.600
And it can also live in many, many environments and thrive wonderfully there.

47:55.600 --> 48:01.600
It can reproduce, it can find food, but it's just one cell.

48:01.600 --> 48:06.600
So it's one of the most primitive, well, it's the most primitive eukaryotes,

48:06.600 --> 48:10.600
but directly next to yeast comes already this amoeba.

48:10.600 --> 48:12.600
It would also be intelligent in this definition.

48:12.600 --> 48:15.600
So I think the definition of intelligence is too weak.

48:15.600 --> 48:16.600
Why is it so weak?

48:16.600 --> 48:21.600
Because in the book where this intelligence definition is used,

48:21.600 --> 48:28.600
that's the book by Götze and Pinache, which is called Artificial General Intelligence,

48:28.600 --> 48:31.600
they discuss many other definitions.

48:31.600 --> 48:37.600
But the problem is that if they use definition like the one very simple one of human intelligence,

48:37.600 --> 48:43.600
intelligence is the capability that enables us to speak, for example.

48:43.600 --> 48:47.600
That enables the language that humans can use.

48:47.600 --> 48:51.600
I mean, it's not my definition, but if you would, but they propose this definition,

48:51.600 --> 48:56.600
then they would automatically fail in generating artificial intelligence.

48:56.600 --> 49:00.600
So they've basically chosen a definition that doesn't yield intelligence,

49:00.600 --> 49:04.600
so that they claim now that they have an intelligence.

49:04.600 --> 49:09.600
But they've actually on purpose used a very stupid definition that is not intelligence.

49:09.600 --> 49:19.600
Now let's go and look at what time it is and look at other problems of the AGI definition.

49:19.600 --> 49:21.600
So let's first look at perception.

49:21.600 --> 49:27.600
So if you have this vector mu as a measure of complexity of environment,

49:27.600 --> 49:34.600
this vector mu presupposes that the environment can be represented by using a binary vector.

49:34.600 --> 49:38.600
In some artificial environments, such a binary representation may be adequate,

49:38.600 --> 49:43.600
but in natural environments, we have signals emanating from complex systems.

49:43.600 --> 49:53.600
And therefore, the signals need to be actively interpreted and reassessed all the time.

49:53.600 --> 49:57.600
And also the observation needs to be continuously adapted to the input

49:57.600 --> 50:02.600
as the agent takes account of the interpretation of each antecedent observation.

50:02.600 --> 50:12.600
And also an animal interpretation depends on previously experienced mental material,

50:12.600 --> 50:13.600
so memories.

50:13.600 --> 50:21.600
And for example, this tiger observing the prey, it does actually all the time update its observations.

50:21.600 --> 50:25.600
It does active perception or shields, I think a female tiger.

50:25.600 --> 50:29.600
And if there would be a puppet next to a young tiger,

50:29.600 --> 50:34.600
the young tiger would not be as good at observing those animals, those prey animals,

50:34.600 --> 50:36.600
because it has that experience.

50:36.600 --> 50:42.600
So the experience stored in the memory of the tiger also helps it to react better to what the animals are doing

50:42.600 --> 50:47.600
and to single out, for example, one animal to hunt it down.

50:47.600 --> 50:52.600
So the predator, if the predator is just sitting there and observing the prey,

50:52.600 --> 50:57.600
it's already acting to improve and adapt the perception of the prey.

50:57.600 --> 51:02.600
So perception is not static, but a dynamic process of constant iterative feedback loops

51:02.600 --> 51:08.600
between sensory and motor neuron circuits, and we cannot even know.

51:08.600 --> 51:14.600
So if we think of training an ML algorithm, we need to know the tuples that we used to train,

51:14.600 --> 51:18.600
but we don't even know when the cycle begins and ends.

51:18.600 --> 51:23.600
So the cycles can be very fast, and we don't know because we can only observe the overall behavior,

51:23.600 --> 51:31.600
but we don't know how to determine the tuples that constitute the perception process.

51:31.600 --> 51:37.600
So our JJ Gibson, a very important psychologist and philosopher says,

51:37.600 --> 51:41.600
normal activity of perception is to explore the world.

51:41.600 --> 51:44.600
So perception depends on more than just sensory stimulus.

51:44.600 --> 51:50.600
So the view that perception is just a result of sensory stimulus is completely outdated.

51:50.600 --> 51:55.600
So when we give input to computers, it's just sensory input, but that's not real.

51:55.600 --> 51:57.600
That's not perception.

51:57.600 --> 52:02.600
Perception requires purposeful activity, direct manipulation of the object,

52:02.600 --> 52:07.600
and innate or quiet knowledge of the expected patterns of reality.

52:07.600 --> 52:17.600
So I need categorical, predefined ability to deal with the environment and also quiet knowledge.

52:17.600 --> 52:22.600
And this manipulation of the object doesn't mean that I need to touch them,

52:22.600 --> 52:31.600
but the tiger can also manipulate these animals, prey animals in her imagination.

52:31.600 --> 52:36.600
So she can imagine that maybe this animal would now lean down to drink from water source

52:36.600 --> 52:41.600
and whether then maybe it would be a good moment to attack this animal and so on.

52:42.600 --> 52:48.600
So this is highly interactive and mu, this static vector mu,

52:48.600 --> 52:54.600
which basically models, for example, the input from a sensor doesn't capture any of this,

52:54.600 --> 52:57.600
does not capture any of what I've just said about perception,

52:57.600 --> 53:00.600
what we know about animal and human perception.

53:00.600 --> 53:06.600
Here's another example of perception, which is much more complex than this one

53:06.600 --> 53:10.600
because it involves dialogue, it involves observation of a dialogue,

53:10.600 --> 53:18.600
it involves probably, yeah, it has many, many interesting aspects

53:18.600 --> 53:22.600
that show how complicated perception is.

53:22.600 --> 53:31.600
And so, for example, what does Tony Curtis, who is here acting as a woman,

53:31.600 --> 53:33.600
think about Marilyn Monroe in this moment?

53:33.600 --> 53:36.600
I don't know, but it's certainly an interesting question.

53:36.600 --> 53:43.600
And anyhow, so perception is not modeled by this environment variable mu.

53:43.600 --> 53:48.600
On the next slide, we see the next problem, which is activity.

53:48.600 --> 53:55.600
So the steps of the utility function that you've seen here are results of,

53:55.600 --> 53:58.600
so each step is an activity which yields a reward.

53:58.600 --> 54:03.600
So in chess or in Go, you get a reward for making a certain move.

54:03.600 --> 54:07.600
Now, the steps of the utility function that are described by the Hutter definition

54:07.600 --> 54:14.600
of artificial intelligence, there are a linear sequence of discrete machine actions.

54:14.600 --> 54:21.600
But human motor acts are actually interactions of perception motor activity.

54:21.600 --> 54:25.600
They involve at every stage a dense synergy of multiple body systems

54:25.600 --> 54:27.600
at multiple levels of granularity.

54:27.600 --> 54:34.600
And that's, for example, if you think of human manufacturing activities

54:34.600 --> 54:39.600
where they have to use, like surgery, when you have to actually

54:39.600 --> 54:46.600
feel very exactly what you're doing or in certain steps in the construction

54:46.600 --> 54:50.600
of even of cars, fine motor, it's called in German.

54:50.600 --> 54:53.600
Barry, do you know the word in English?

54:54.600 --> 54:59.600
I don't. Sorry. Precision engineering would be...

54:59.600 --> 55:01.600
Yeah, probably.

55:01.600 --> 55:06.600
So we don't know how to make machines do this because we don't know

55:06.600 --> 55:13.600
how the circuitry between a perception and motor action work.

55:13.600 --> 55:16.600
So we know that there is some feedback loop, blah, blah, blah,

55:16.600 --> 55:18.600
but we don't know the details of it.

55:18.600 --> 55:23.600
And if you look at the most advanced textbooks or papers that are available

55:23.600 --> 55:32.600
about animal fine motor action, we have no clue how the animals

55:32.600 --> 55:35.600
do these fine motor actions or know how we do it.

55:35.600 --> 55:37.600
We have no models for it.

55:37.600 --> 55:42.600
And so because we have no models for it, we cannot do it in a computer.

55:42.600 --> 55:47.600
And so the activities that happen in real environments

55:47.600 --> 55:52.600
are much more complicated than those linear sequence of steps.

55:52.600 --> 55:57.600
And the interaction between sensory and motor activity

55:57.600 --> 56:01.600
could potentially scrub the linear sequence of effort, effort,

56:01.600 --> 56:05.600
and interest in neural signal events, but the coupling of such a sequence

56:05.600 --> 56:08.600
to any sort of reward is very indirect.

56:08.600 --> 56:13.600
So yes, in reality, of course, those things happen one after the other,

56:13.600 --> 56:16.600
but very, very quickly in a very complex session.

56:16.600 --> 56:19.600
And so we don't know how to cover this to reward.

56:19.600 --> 56:25.600
And so probably the sequence in reward terms that we need

56:25.600 --> 56:31.600
would probably not be possible to construct the correct reward sequence.

56:31.600 --> 56:34.600
Another aspect is that mammals can overcome

56:34.600 --> 56:36.600
massive negative rewards to achieve the goal.

56:36.600 --> 56:41.600
So here I have an example from animal psychology.

56:41.600 --> 56:44.600
So it's a cocaine self-administration experiment

56:44.600 --> 56:49.600
where the rats to get cocaine, they have to traverse a heated plate,

56:49.600 --> 56:51.600
which is burning their feet.

56:51.600 --> 56:54.600
So they have to damage themselves to get to the cocaine,

56:54.600 --> 57:00.600
but they still do it because they have primary intelligence.

57:00.600 --> 57:04.600
So they know that they must cross the heat plate to get to the cocaine,

57:04.600 --> 57:09.600
that they must press a button to get the administration of cocaine.

57:09.600 --> 57:12.600
And they get a short-term reward for this,

57:12.600 --> 57:15.600
but they don't have a net long-term reward.

57:15.600 --> 57:21.600
And so such a behavior is very, very hard to model with a reward function.

57:21.600 --> 57:26.600
And if you think many of you are maybe in the middle of their PhD thesis,

57:26.600 --> 57:32.600
so the PhD thesis process is not like the cocaine self-administration,

57:32.600 --> 57:34.600
but there's a lot of negative stuff you have to cope with

57:34.600 --> 57:38.600
over a long time before you get a reward, that's for sure.

57:38.600 --> 57:42.600
And I don't know how this can be modeled with such a reward model,

57:42.600 --> 57:44.600
at least being very hard.

57:44.600 --> 57:46.600
Speaking of reward,

57:46.600 --> 57:51.600
so what is even more important is that the reward pattern

57:51.600 --> 57:54.600
that we see is some of a reward.

57:54.600 --> 58:01.600
It is actually unable to model reward patterns that we encounter in real life,

58:01.600 --> 58:06.600
because first of all, the system that certifies the harder definition

58:06.600 --> 58:08.600
will always be situation-specific.

58:08.600 --> 58:12.600
So it will work only in the context where a human has already been at work

58:12.600 --> 58:15.600
in preparing appropriate rewards.

58:15.600 --> 58:17.600
There is no general or universal reward.

58:17.600 --> 58:22.600
So the reward, for example, that the machine receives for playing the game of Go,

58:22.600 --> 58:24.600
are points.

58:24.600 --> 58:27.600
And the algorithm is trying to find a functional or an operator

58:27.600 --> 58:29.600
that maximizes the number of points.

58:29.600 --> 58:34.600
So it's just a derivative of a very long equation that you have to find.

58:35.600 --> 58:40.600
And that has nothing to do with universal or general intelligence.

58:40.600 --> 58:45.600
The reason is that the mathematical definition that Hatter provides

58:45.600 --> 58:51.600
for the environment mu that must be matched by the reward.

58:51.600 --> 58:56.600
So it's not possible to find a reward that works for all environments,

58:56.600 --> 58:59.600
whereas humans have as a reward,

58:59.600 --> 59:04.600
the main rewards are to survive and to reproduce.

59:04.600 --> 59:13.600
And survival and reproduction can come very indirectly in highly evolved societies.

59:13.600 --> 59:19.600
So to sit in a room and do mathematical equations all day long

59:19.600 --> 59:23.600
for survival and reproduction, that's quite abstract

59:23.600 --> 59:27.600
or to do paintings of art or to compose music.

59:27.600 --> 59:36.600
And so there is a way of humans to delay the reward of reproduction and survival

59:36.600 --> 59:42.600
very far off and to do activities that seem to be non-connected to it.

59:42.600 --> 59:47.600
And that seems to be at least required for objectifying intelligence or human intelligence.

59:47.600 --> 59:52.600
And we don't know how we can model this with reward.

59:52.600 --> 01:00:02.600
Also further problem arises that the assumption is made that all rewards of one agent

01:00:02.600 --> 01:00:08.600
must be of the same type for every step under a given environment.

01:00:08.600 --> 01:00:13.600
So if we go back to the equation, r is only indexed by the step,

01:00:13.600 --> 01:00:18.600
but it cannot change its type, otherwise it would need the second index.

01:00:18.600 --> 01:00:25.600
So there's only one type. Yes, this type can obtain different values,

01:00:25.600 --> 01:00:30.600
but probably in animal and human behavior there are many different types of rewards.

01:00:30.600 --> 01:00:34.600
So we cannot model this.

01:00:34.600 --> 01:00:37.600
Then the question is, couldn't we create a sequence of rewards

01:00:37.600 --> 01:00:41.600
adequate for learning the behavior of a complex system?

01:00:41.600 --> 01:00:45.600
So the problem here would be that such a reward sequence,

01:00:45.600 --> 01:00:48.600
you would then imagine many, many rewards in a sequence

01:00:48.600 --> 01:00:52.600
and they would have to give situation-specific rewards.

01:00:52.600 --> 01:00:56.600
That's because each step on a complex system model trajectory

01:00:56.600 --> 01:01:01.600
would have to be able to deal with an unexpected situation

01:01:01.600 --> 01:01:05.600
because when the AI system interacts with its own environment,

01:01:05.600 --> 01:01:09.600
that will change the environment.

01:01:09.600 --> 01:01:14.600
And so this change of the environment will create an unexpected situation

01:01:14.600 --> 01:01:16.600
and may require different reward.

01:01:16.600 --> 01:01:20.600
So you cannot really find a reward part or trajectory

01:01:20.600 --> 01:01:24.600
because at each step a different reward would be needed to correspond

01:01:24.600 --> 01:01:28.600
to the emanations from the complex system that form the environment.

01:01:28.600 --> 01:01:33.600
So that has to do with the evolutionary character of complex systems.

01:01:33.600 --> 01:01:38.600
And such a temporal reward sequence would obviously not follow a Markovian pattern.

01:01:38.600 --> 01:01:43.600
And of course, speaking of probability theory, if we go back to this slide here,

01:01:43.600 --> 01:01:50.600
Markovian pattern is of course a pattern that applies only to classical Newtonian systems.

01:01:50.600 --> 01:01:53.600
Only Newtonian systems have the Markov property.

01:01:53.600 --> 01:01:58.600
Complex systems don't have the Markov property, but without the Markov property,

01:01:58.600 --> 01:02:03.600
you cannot achieve predictive modeling in stochastic equations.

01:02:03.600 --> 01:02:07.600
So stochastic differential equations or other stochastic process models,

01:02:07.600 --> 01:02:09.600
they always need a Markov pattern.

01:02:09.600 --> 01:02:20.600
And without the Markov pattern in which the reward would depend only on the previous or some previous steps.

01:02:20.600 --> 01:02:24.600
But if you don't have this pattern, but you have a dependency on many earlier steps

01:02:24.600 --> 01:02:28.600
and long-term dispositions of the organism and also short-term intentions,

01:02:28.600 --> 01:02:31.600
you can't find a reward sequence.

01:02:31.600 --> 01:02:44.600
And so, therefore, the reward sequence would need to correspond to complex emanations

01:02:44.600 --> 01:02:48.600
relating to situations varying as a successive test unfold.

01:02:48.600 --> 01:02:53.600
And therefore, the reward sequence itself has to be complex

01:02:53.600 --> 01:02:57.600
and thereby it would have all the properties of a complex system emanation.

01:02:57.600 --> 01:03:05.600
So that's the interesting thing, that to give rewards to an intelligent system in a complex setting,

01:03:05.600 --> 01:03:10.600
the reward sequence itself would have all the properties of a complex system emanation.

01:03:10.600 --> 01:03:16.600
So it would have the same properties that my stream of language that I'm currently giving to you has.

01:03:16.600 --> 01:03:21.600
And we have no mathematical models to create such sequences

01:03:21.600 --> 01:03:26.600
because we have no mathematical models for complex system emanations.

01:03:26.600 --> 01:03:33.600
And so we couldn't create the reward sequence that would be needed for the intelligent system

01:03:33.600 --> 01:03:36.600
to cope with a complex system situation.

01:03:36.600 --> 01:03:40.600
And that's a problem, I think, a very important problem to see this,

01:03:40.600 --> 01:03:45.600
that the reward approach, why reward is mathematically attractive

01:03:45.600 --> 01:03:54.600
because it allows to state the intelligence problem like an optimization problem for optimization theory.

01:03:54.600 --> 01:04:00.600
It doesn't create a realistic sequence of rewards that correspond in any way

01:04:00.600 --> 01:04:05.600
to what we experience or animals experience when they obtain rewards for their behavior.

01:04:05.600 --> 01:04:11.600
Actually, the reward that an animal and foraging animal receives, which is the food

01:04:11.600 --> 01:04:14.600
and for which the animal does very interesting things.

01:04:14.600 --> 01:04:18.600
So if you read modern books about foraging, it has been found out that parrots, for example,

01:04:18.600 --> 01:04:31.600
invent new patterns of shouts and vocal noises they make to describe sources for food.

01:04:31.600 --> 01:04:38.600
And while they search for those food sources, which they have to do every day,

01:04:38.600 --> 01:04:41.600
they create those sound patterns.

01:04:41.600 --> 01:04:45.600
And this process is highly complex and it has nothing to do.

01:04:45.600 --> 01:04:50.600
And the way they get to the reward has nothing to do at all with linear reward sequence

01:04:50.600 --> 01:04:53.600
that we see in those models.

01:04:53.600 --> 01:04:56.600
And so that's why I call these pseudo-definitions of intelligence

01:04:56.600 --> 01:05:06.600
because they just define something that can be actually put into a model optimization algorithm,

01:05:06.600 --> 01:05:14.600
a numerical model optimization algorithm, like a dual or something like this.

01:05:14.600 --> 01:05:20.600
But in reality, it has nothing to do with real intelligence, not even animal intelligence.

01:05:20.600 --> 01:05:26.600
Now, I think before I go to the last slide, I would like to give you the opportunity

01:05:26.600 --> 01:05:30.600
to ask one more round of questions.

01:05:30.600 --> 01:05:33.600
Yeah, I have one question, Professor.

01:05:34.600 --> 01:05:43.600
The thing is, so here the argument is that the reward sequence is extremely complex

01:05:43.600 --> 01:05:48.600
and since it is not Markovian, you cannot somehow model this.

01:05:48.600 --> 01:05:54.600
Now, if I were to take the example of a robot in a factory,

01:05:54.600 --> 01:05:57.600
let's say a robot that knows how to load boxes, unload boxes,

01:05:57.600 --> 01:06:01.600
get the boxes onto some other conveyor belt, et cetera, et cetera.

01:06:01.600 --> 01:06:06.600
Currently, there are reinforcement learning algorithms that are able to do this

01:06:06.600 --> 01:06:12.600
pretty effectively in these factories, just replacing entirely with robots that do this work.

01:06:12.600 --> 01:06:16.600
So there, of course, the reward sequence is very clear

01:06:16.600 --> 01:06:21.600
because you have just one mechanistic task that you keep doing throughout your life,

01:06:21.600 --> 01:06:24.600
well, throughout your robot life.

01:06:24.600 --> 01:06:34.600
My question here was, what if we could create a set of rewards for different tasks

01:06:34.600 --> 01:06:37.600
and model that together into a specific robot?

01:06:37.600 --> 01:06:40.600
So, for example, let's say loading and loading boxes, opening doors,

01:06:40.600 --> 01:06:43.600
walking, sitting down, different movements.

01:06:43.600 --> 01:06:47.600
Now, I'm not suggesting for a single second that humans learn this

01:06:47.600 --> 01:06:50.600
by this sort of discretized reward sequence.

01:06:50.600 --> 01:06:55.600
I'm not suggesting this at all because I don't know how humans learn it.

01:06:55.600 --> 01:06:58.600
But as far as teaching robots that is concerned,

01:06:58.600 --> 01:07:03.600
don't you think the reward sequence then would just be a set of different rewards

01:07:03.600 --> 01:07:07.600
for different tasks, but they would all be modeled into the same algorithm.

01:07:07.600 --> 01:07:11.600
So, in a sense, what I'm arguing is the whole is the sum of its parts.

01:07:11.600 --> 01:07:13.600
That's what I'm trying to say.

01:07:13.600 --> 01:07:19.600
So, you're actually already giving a small preview of the next slide.

01:07:19.600 --> 01:07:23.600
So, it's never what you're saying.

01:07:23.600 --> 01:07:29.600
So, basically, what you describe is, of course, that what is a factory?

01:07:29.600 --> 01:07:32.600
A factory is a Newtonian system.

01:07:32.600 --> 01:07:35.600
So, a factory has all those properties.

01:07:35.600 --> 01:07:41.600
And so, therefore, machine learning can be very, very efficient

01:07:41.600 --> 01:07:47.600
in implicitly modeling Newtonian equations, motion equations, also sensory equations.

01:07:47.600 --> 01:07:54.600
So, therefore, because in a factory, all processes have the Markov property

01:07:54.600 --> 01:07:59.600
and because there's no force overlay, the elements are well defined and so on,

01:07:59.600 --> 01:08:01.600
that will work pretty well.

01:08:01.600 --> 01:08:06.600
And that's why also machine learning is, I think, the best application of machine learning

01:08:06.600 --> 01:08:12.600
that is currently not visible very much is actually manufacturing and mining

01:08:12.600 --> 01:08:18.600
and other mechanical activities where humans still play a role,

01:08:18.600 --> 01:08:23.600
but where they will with the exception of this fine motor or sensoric behavior,

01:08:23.600 --> 01:08:28.600
which is very hard to model, they will be replaced by robots more and more.

01:08:28.600 --> 01:08:34.600
You're completely right, and that's because here, machine learning is used to model Newtonian systems.

01:08:34.600 --> 01:08:40.600
That's the first answer. The second answer is that it's essentially the way to build AI properly

01:08:40.600 --> 01:08:46.600
is to do it as you described, and I will come to this once the other questions are answered.

01:08:46.600 --> 01:08:52.600
Oh, okay, okay. Thank you so much.

01:08:52.600 --> 01:08:54.600
Is there any?

01:08:54.600 --> 01:08:58.600
I have one, but I'll give the students chance to butt in first.

01:09:07.600 --> 01:09:13.600
I would just remark maybe that the authors were aware of this argument that you are saying

01:09:13.600 --> 01:09:22.600
because they said that at no point they are trying to compare their definition to human intelligence.

01:09:22.600 --> 01:09:32.600
I think they perceive the machine intelligence just as a composite of different tasks

01:09:32.600 --> 01:09:36.600
as Ravidi was saying earlier.

01:09:36.600 --> 01:09:43.600
If you read Hutter's and Schmitt-Huber's papers about general intelligence, I disagree.

01:09:43.600 --> 01:09:48.600
They believe that they can create general intelligence, and many of them, I don't know whether Schmitt-Huber does,

01:09:48.600 --> 01:09:55.600
but many of them also believe in the singularity, which I think shows.

01:09:55.600 --> 01:10:01.600
I think that you should really get this AGI book, or I can send it to you, or Barry can send it to you,

01:10:01.600 --> 01:10:11.600
this AGI book by Gertse Penachin, which is one of the most important consensus readers

01:10:11.600 --> 01:10:14.600
where all the big shots of AGI have published papers.

01:10:14.600 --> 01:10:19.600
Yes, of course they say that it's not human intelligence, but they say it's a real intelligence.

01:10:19.600 --> 01:10:26.600
What I'm saying is no, what you're suggesting is not a real intelligence, but it's just a kind of amoeba intelligence,

01:10:26.600 --> 01:10:28.600
and probably not even that.

01:10:28.600 --> 01:10:36.600
I think that because on the next slide you will see how I define what one can do with AGI,

01:10:36.600 --> 01:10:41.600
and when I show this to people from this community, I get heavily attacked.

01:10:41.600 --> 01:10:46.600
Oh, that's old school, we can do much better, we can do general intelligence, and so on.

01:10:46.600 --> 01:10:57.600
If I understood correctly, they were also saying that, of course, the Chinese argument is completely right.

01:10:57.600 --> 01:11:07.600
I think where they were defending the critique, maybe that's at the end.

01:11:08.600 --> 01:11:12.600
But I don't even argue with the Chinese room argument here.

01:11:12.600 --> 01:11:20.600
I just say that even the most basic form of intelligence that we as humans perceive as intelligent

01:11:20.600 --> 01:11:29.600
would just see the behavior of a dog or another mammal, that this behavior cannot never be achieved by these equations.

01:11:29.600 --> 01:11:36.600
So what can be achieved by these equations is actually what neural networks already do,

01:11:36.600 --> 01:11:47.600
and that is because this here is a recipe to actually...

01:11:47.600 --> 01:11:55.600
Yeah, and so loss function minimizes loss, and here we maximize reward, but it's basically optimization, prescription,

01:11:55.600 --> 01:12:04.600
and we achieve this by applying these models to achieve what is shown on slide three, with all the pros and cons.

01:12:04.600 --> 01:12:09.600
Now, I'm saying that these ML models are very useful, but they have nothing to do with intelligence,

01:12:09.600 --> 01:12:14.600
and that's basically...

01:12:14.600 --> 01:12:19.600
I mean, when I discuss, you know that I have an AI company, when I discuss with clever customers,

01:12:19.600 --> 01:12:23.600
I mean their customers who want to buy intelligence, and I let them under the illusion,

01:12:23.600 --> 01:12:28.600
but others who are clever to say, but that's not really intelligence, I say, no, these are artificial instincts.

01:12:29.600 --> 01:12:38.600
And this is basically what we do, what we still do, even if we use such optimization procedures as those described by these models here,

01:12:38.600 --> 01:12:41.600
we still only obtain a narrow AI.

01:12:41.600 --> 01:12:45.600
And with this, Barry, you had a question before I go through the narrow AI.

01:12:45.600 --> 01:12:49.600
Yes, so I have a number of questions now, unfortunately.

01:12:49.600 --> 01:12:53.600
So what would leg Hutter say to the following objection?

01:12:53.600 --> 01:12:59.600
You define intelligence as the ability to achieve rewards in a wide range of environments,

01:12:59.600 --> 01:13:05.600
but AlphaGo can only achieve rewards in one kind of environment, which is the Go board.

01:13:05.600 --> 01:13:07.600
Yeah.

01:13:07.600 --> 01:13:10.600
So it's not a wide range at all.

01:13:10.600 --> 01:13:12.600
Yeah, yeah.

01:13:12.600 --> 01:13:16.600
Yeah, I believe that even actually, you're right.

01:13:16.600 --> 01:13:22.600
So I believe that even this definition, intelligence so will not be achievable.

01:13:22.600 --> 01:13:27.600
So the verbal definition is actually in conflict with the equations.

01:13:27.600 --> 01:13:29.600
Yeah, you're right.

01:13:29.600 --> 01:13:32.600
And now just a little correction, sorry.

01:13:32.600 --> 01:13:34.600
Sorry, just a little correction.

01:13:34.600 --> 01:13:39.600
AlphaZero actually can succeed in a variety of game environments.

01:13:39.600 --> 01:13:43.600
AlphaZero can play chess, it can play Go, it can play checker.

01:13:43.600 --> 01:13:45.600
I'm just talking about AlphaGo.

01:13:45.600 --> 01:13:47.600
You're right.

01:13:47.600 --> 01:13:50.600
Not AlphaGo, but AlphaZero, which was recently released.

01:13:50.600 --> 01:13:53.600
Yeah, AlphaZero can play different games.

01:13:53.600 --> 01:13:57.600
And actually, there was already in 2014 a precursor for AlphaZero,

01:13:57.600 --> 01:14:04.600
which could play all the Atari games, which was also trained with reward learning.

01:14:04.600 --> 01:14:08.600
It could play strategic games, but it could play Pong and all the ones

01:14:08.600 --> 01:14:10.600
that give you a series of points.

01:14:10.600 --> 01:14:14.600
But now OpenAI has come out with StarCraft games too.

01:14:14.600 --> 01:14:17.600
So they are getting better at these complex environments.

01:14:17.600 --> 01:14:21.600
Well, these are actually not complex environments.

01:14:21.600 --> 01:14:25.600
So these are still Newtonian environments.

01:14:25.600 --> 01:14:31.600
Because the reason this is very important, if you could hold on with your questions a minute.

01:14:31.600 --> 01:14:35.600
So the reason why these are Newtonian environments is that also,

01:14:35.600 --> 01:14:39.600
for example, games like strategy games, like civilization,

01:14:39.600 --> 01:14:42.600
they have also been beat by reinforcement learning.

01:14:42.600 --> 01:14:44.600
But the reason is, of course, what is civilization?

01:14:44.600 --> 01:14:51.600
Civilization is a set of rules and equations applied to a certain pattern.

01:14:51.600 --> 01:14:55.600
And even if this pattern is created by random, it's created as a multivariate

01:14:55.600 --> 01:14:58.600
random distribution, usually multivariate.

01:14:58.600 --> 01:15:01.600
And so it's multivariate Gaussian.

01:15:01.600 --> 01:15:05.600
And then, of course, you can take points from this distribution

01:15:05.600 --> 01:15:07.600
and put them into a set of rules.

01:15:07.600 --> 01:15:12.600
Then you create a very complex set of events, but it's still a Newtonian universe.

01:15:12.600 --> 01:15:16.600
And therefore, you can also train in the same place for the ego shooter games.

01:15:16.600 --> 01:15:20.600
And therefore, you can train AI that is beating this.

01:15:20.600 --> 01:15:25.600
But the point is, since 1950, we have heard predictions about free-moving robots.

01:15:25.600 --> 01:15:27.600
Why don't we have them?

01:15:27.600 --> 01:15:29.600
I mean, we have them, of course, in controlled environments.

01:15:29.600 --> 01:15:33.600
But why do we never encounter free-moving robots in our streets?

01:15:33.600 --> 01:15:36.600
Because these are real complex environments.

01:15:36.600 --> 01:15:38.600
Yeah, yeah, yeah.

01:15:38.600 --> 01:15:40.600
Thank you.

01:15:40.600 --> 01:15:43.600
I just have one quick question for anybody.

01:15:43.600 --> 01:15:52.600
So why do they use the word universal in the title of the Laguta paper?

01:15:52.600 --> 01:15:55.600
Because it sounds good.

01:15:55.600 --> 01:15:57.600
All right.

01:15:57.600 --> 01:16:02.600
Okay, that's what I guess the answer would be.

01:16:02.600 --> 01:16:03.600
I'm not sure.

01:16:03.600 --> 01:16:05.600
Jopster, am I wrong?

01:16:05.600 --> 01:16:06.600
Please, correct me.

01:16:06.600 --> 01:16:07.600
Probably.

01:16:07.600 --> 01:16:13.600
I mean, there's a wonderful paper by Johannes from 2005,

01:16:13.600 --> 01:16:17.600
which says why most of scientific research results are wrong.

01:16:17.600 --> 01:16:21.600
And the biggest reason is, of course, bias.

01:16:21.600 --> 01:16:23.600
And sounding good is also a form of bias.

01:16:23.600 --> 01:16:26.600
So anyhow, but I'm not a pessimist for AI.

01:16:26.600 --> 01:16:29.600
I remember that I made my little trunk for my life.

01:16:29.600 --> 01:16:33.600
Jopster, we have one student, Peter Bottaroni,

01:16:33.600 --> 01:16:35.600
who would like to ask a question.

01:16:35.600 --> 01:16:36.600
Yeah, please go ahead.

01:16:36.600 --> 01:16:37.600
Yes, thank you.

01:16:37.600 --> 01:16:39.600
Actually, there are two questions.

01:16:39.600 --> 01:16:44.600
The first is related with the discussion with the RID.

01:16:44.600 --> 01:16:49.600
In the sense, okay, then we say that a complex environment

01:16:49.600 --> 01:16:51.600
is an environment that is not...

01:16:51.600 --> 01:16:54.600
We cannot model mathematically, right?

01:16:54.600 --> 01:16:56.600
Yes, yes, basically, yes.

01:16:56.600 --> 01:16:59.600
Okay, then every complex environment cannot be something

01:16:59.600 --> 01:17:01.600
that we create with a software.

01:17:01.600 --> 01:17:06.600
Then a complex environment should be a real environment,

01:17:06.600 --> 01:17:09.600
in the sense that everything that is...

01:17:09.600 --> 01:17:12.600
Even the more complex game that we can play

01:17:12.600 --> 01:17:17.600
is not considered a complex environment, right?

01:17:17.600 --> 01:17:21.600
Unless what we sometimes have is that we have

01:17:21.600 --> 01:17:24.600
natural human behavior integrated as movies

01:17:24.600 --> 01:17:26.600
into an environment, into a game.

01:17:26.600 --> 01:17:29.600
And then the player in some strategy games,

01:17:29.600 --> 01:17:31.600
which I used to play 20 years ago,

01:17:31.600 --> 01:17:33.600
there was a movie sequence in the game embedded,

01:17:33.600 --> 01:17:36.600
and you had to interpret the movie scene correctly

01:17:36.600 --> 01:17:38.600
to continue to play the game.

01:17:38.600 --> 01:17:41.600
And of course, the behavior of humans during that scene,

01:17:41.600 --> 01:17:44.600
because this was a film of naturally behaving humans,

01:17:44.600 --> 01:17:46.600
where they were playing a role, but still,

01:17:46.600 --> 01:17:49.600
that was, of course, complex system behavior,

01:17:49.600 --> 01:17:51.600
but then the rest of the game was not,

01:17:51.600 --> 01:17:53.600
and that's still like it.

01:17:53.600 --> 01:17:56.600
Poker would be a complex game, would it not?

01:17:56.600 --> 01:18:01.600
Poker, or yes, poker played with humans is a complex game.

01:18:01.600 --> 01:18:03.600
Which one? Sorry?

01:18:03.600 --> 01:18:06.600
If you play a round of poker against human opponents...

01:18:06.600 --> 01:18:09.600
Ah, okay, okay, yeah, yeah, yeah, yeah, okay.

01:18:09.600 --> 01:18:13.600
But chess with human opponents is not a complex game.

01:18:13.600 --> 01:18:16.600
Correct. Yeah, good.

01:18:16.600 --> 01:18:21.600
And the second question is related with the reward definition,

01:18:21.600 --> 01:18:24.600
in the sense that, at least to me,

01:18:24.600 --> 01:18:29.600
it seems that we define the rewards with something that is given.

01:18:29.600 --> 01:18:32.600
We define the reward for the robot or for the machine.

01:18:32.600 --> 01:18:33.600
Yes.

01:18:33.600 --> 01:18:37.600
Actually, the human sometimes creates some reward or some goal

01:18:37.600 --> 01:18:41.600
by itself, maybe some sub-goal or sub-reward

01:18:41.600 --> 01:18:45.600
in order to achieve a bigger goal.

01:18:45.600 --> 01:18:46.600
Correct.

01:18:46.600 --> 01:18:51.600
Is it possible to model this creation of intermediate goal

01:18:51.600 --> 01:18:54.600
or final goal as a machine, according to you?

01:18:54.600 --> 01:18:56.600
Yes, so it has already been done.

01:18:56.600 --> 01:18:59.600
So you can, for example, you can have, as you know,

01:18:59.600 --> 01:19:01.600
you can have meta models in machine learning.

01:19:01.600 --> 01:19:05.600
So you can have, for example, several reinforcement learning models

01:19:05.600 --> 01:19:10.600
that use different reinforcement rewards from a choice of reward.

01:19:11.600 --> 01:19:15.600
And so you can optimize from a certain reward.

01:19:15.600 --> 01:19:18.600
You can try out several reward types in parallel

01:19:18.600 --> 01:19:22.600
and find the best model by varying the reward type.

01:19:22.600 --> 01:19:26.600
And then you can use adversarial learning, maybe,

01:19:26.600 --> 01:19:29.600
to drive the choice of the reward type, and so on.

01:19:29.600 --> 01:19:31.600
Yeah, so there are ways to do this,

01:19:31.600 --> 01:19:36.600
but they will always only solve problems in non-compact environment.

01:19:36.600 --> 01:19:37.600
Okay.

01:19:37.600 --> 01:19:39.600
Okay, thank you.

01:19:39.600 --> 01:19:40.600
Good.

01:19:40.600 --> 01:19:43.600
So now last slide.

01:19:43.600 --> 01:19:46.600
So you remember that I'm making money with AI

01:19:46.600 --> 01:19:47.600
and I'm a big fan of AI.

01:19:47.600 --> 01:19:51.600
It's not that I'm negative against it, yeah?

01:19:51.600 --> 01:19:54.600
So what can we do with the analytical engine?

01:19:54.600 --> 01:19:56.600
So the analytical engine is the first name

01:19:56.600 --> 01:19:58.600
that was given to a Turing machine.

01:19:58.600 --> 01:20:02.600
Charles Babbage was the one who built the first computer

01:20:02.600 --> 01:20:05.600
in the 19th century, around 1850,

01:20:05.600 --> 01:20:08.600
and it could already do computations.

01:20:09.600 --> 01:20:19.600
And Anna Lovelace said that an analytical engine

01:20:19.600 --> 01:20:21.600
has no pretensions to originate anything.

01:20:21.600 --> 01:20:26.600
It can only do whatever we know how to order it to perform.

01:20:26.600 --> 01:20:32.600
And so what is now, is that also true for the Turing machine?

01:20:32.600 --> 01:20:34.600
So Alan Turing says that, of course,

01:20:34.600 --> 01:20:37.600
an analytical engine is a Turing machine.

01:20:37.600 --> 01:20:39.600
And what can we do with it?

01:20:39.600 --> 01:20:43.600
And so I think we can do a lot.

01:20:43.600 --> 01:20:45.600
And we can do what one of you just said,

01:20:45.600 --> 01:20:47.600
namely we can engineer by composition.

01:20:47.600 --> 01:20:51.600
So that's an outcome that we want to achieve.

01:20:51.600 --> 01:20:56.600
And here you see many operators and functionals

01:20:56.600 --> 01:20:58.600
which are chained together.

01:20:59.600 --> 01:21:06.600
And these are an upper case data as an operator

01:21:06.600 --> 01:21:08.600
and a lower case data as a function.

01:21:08.600 --> 01:21:12.600
For the non-mathematicians, a function is a relation

01:21:12.600 --> 01:21:21.600
that takes an input vector and creates an output number.

01:21:21.600 --> 01:21:24.600
The operator can also create an output vector

01:21:24.600 --> 01:21:26.600
from an input vector.

01:21:26.600 --> 01:21:29.600
So they act together here in a chain

01:21:29.600 --> 01:21:34.600
to yield the final result, which is y or t hat.

01:21:34.600 --> 01:21:40.600
And the superscripts that you can see here,

01:21:40.600 --> 01:21:46.600
Teta, Kappa, Lambda, there are prior knowledge

01:21:46.600 --> 01:21:48.600
that can be configured into the functions

01:21:48.600 --> 01:21:51.600
and operated explicitly or via training tools.

01:21:51.600 --> 01:21:56.600
So for example, this could be just a set of rules

01:21:56.600 --> 01:21:58.600
and this could be the parallelization of the rules.

01:21:58.600 --> 01:22:00.600
This could be a function that has been trained

01:22:00.600 --> 01:22:04.600
like a spam filter and where the parameter indicates

01:22:04.600 --> 01:22:06.600
which training tools for you.

01:22:06.600 --> 01:22:09.600
And so I believe that it's very important

01:22:09.600 --> 01:22:11.600
that you make the prior knowledge

01:22:11.600 --> 01:22:13.600
that you gave to the machine explicit.

01:22:13.600 --> 01:22:17.600
So either you make it explicit if you have, for example,

01:22:17.600 --> 01:22:19.600
mechanical theory improving component,

01:22:19.600 --> 01:22:22.600
let's say this one here, or maybe this one

01:22:22.600 --> 01:22:24.600
would be mechanical theory improver,

01:22:24.600 --> 01:22:26.600
then you need some axioms for the mechanical theory improver.

01:22:26.600 --> 01:22:28.600
And those axioms are its configuration

01:22:28.600 --> 01:22:31.600
and you need to know which axioms you give to it

01:22:31.600 --> 01:22:33.600
so that you know how it will behave.

01:22:33.600 --> 01:22:36.600
Or this would be a functional, now the functional,

01:22:36.600 --> 01:22:38.600
of course, is trained by using training tools

01:22:38.600 --> 01:22:40.600
and other meta parameter.

01:22:40.600 --> 01:22:43.600
They are this and also those you have to know pretty well.

01:22:43.600 --> 01:22:46.600
So for example, we do customer correspondence automation,

01:22:46.600 --> 01:22:48.600
but we cannot use the way that,

01:22:48.600 --> 01:22:51.600
so if a company gives us an email that it received

01:22:51.600 --> 01:22:54.600
and also gives us a reaction to the email that they created,

01:22:54.600 --> 01:22:57.600
we cannot use these two bits for training

01:22:57.600 --> 01:23:00.600
because the outcomes that are created by the company are erratic.

01:23:00.600 --> 01:23:02.600
That has to do with many factors.

01:23:02.600 --> 01:23:04.600
It has to do with human error,

01:23:04.600 --> 01:23:07.600
but it has also to do with, for example,

01:23:07.600 --> 01:23:11.600
they have a period where they have understaffed.

01:23:11.600 --> 01:23:15.600
So they just give a stereotype answer to all the letters

01:23:15.600 --> 01:23:17.600
and later on, and they say,

01:23:17.600 --> 01:23:19.600
we will get back to you later,

01:23:19.600 --> 01:23:21.600
but we cannot react for the next three weeks.

01:23:21.600 --> 01:23:23.600
So you have a heterogeneous outcome

01:23:23.600 --> 01:23:27.600
which is not very well related to the input.

01:23:27.600 --> 01:23:30.600
And so therefore, you have to very carefully curate

01:23:30.600 --> 01:23:32.600
the training material that you use

01:23:32.600 --> 01:23:35.600
to train such complex chains of functions and operators.

01:23:35.600 --> 01:23:37.600
But if you do this,

01:23:37.600 --> 01:23:41.600
then you can achieve very impressive results.

01:23:41.600 --> 01:23:43.600
For example, we have automated

01:23:43.600 --> 01:23:45.600
in the insurance industry,

01:23:45.600 --> 01:23:48.600
there are builds that are, for example,

01:23:48.600 --> 01:23:51.600
created by when a car gets repaired,

01:23:51.600 --> 01:23:53.600
a bill is created.

01:23:53.600 --> 01:23:56.600
We have created an algorithm that can automatically evaluate

01:23:56.600 --> 01:23:59.600
whether the bill is correct.

01:23:59.600 --> 01:24:02.600
And that's very hard because it's usually done by a technician

01:24:02.600 --> 01:24:05.600
who looks at the bill and figures out

01:24:05.600 --> 01:24:08.600
whether the workshop repaired the car in the right way.

01:24:08.600 --> 01:24:10.600
And what we do is that we take the bill

01:24:10.600 --> 01:24:13.600
and transform the bill into mathematical logic.

01:24:13.600 --> 01:24:16.600
And then we have also, we get for this car,

01:24:16.600 --> 01:24:20.600
the car repair instructions by the manufacturer of the car,

01:24:20.600 --> 01:24:23.600
which we also transform into mathematical logic.

01:24:23.600 --> 01:24:26.600
And then we do mechanical theory improving

01:24:26.600 --> 01:24:28.600
to prove for each step

01:24:28.600 --> 01:24:30.600
what this step corresponds to a step

01:24:30.600 --> 01:24:33.600
that is also described in the repair instruction.

01:24:33.600 --> 01:24:36.600
This way we have a mechanical evaluation

01:24:36.600 --> 01:24:39.600
of the correctness of the repair of a car.

01:24:39.600 --> 01:24:41.600
And so this is quite impressive

01:24:41.600 --> 01:24:44.600
because it's a very demanding technical skill

01:24:44.600 --> 01:24:46.600
that I'm not able to perform.

01:24:46.600 --> 01:24:49.600
So I cannot perform what this computer program can

01:24:49.600 --> 01:24:53.600
because I don't know enough about car repair.

01:24:53.600 --> 01:24:56.600
Of course, the computer knows nothing about car repair,

01:24:56.600 --> 01:24:59.600
but it can formalize the repair bill from the workshop

01:24:59.600 --> 01:25:01.600
and it can formalize the repair instruction

01:25:01.600 --> 01:25:03.600
into mathematical logic.

01:25:03.600 --> 01:25:05.600
And then all it needs to do is to make a method

01:25:05.600 --> 01:25:08.600
to establish mathematical, logical equivalence

01:25:08.600 --> 01:25:13.600
between a repair step and an instruction step.

01:25:13.600 --> 01:25:17.600
And this is just an example for what something

01:25:17.600 --> 01:25:20.600
that can achieve with Turing machines.

01:25:20.600 --> 01:25:23.600
And there are many, many other impressive examples.

01:25:23.600 --> 01:25:28.600
But my opinion is that you need to do that intelligent behavior

01:25:28.600 --> 01:25:30.600
is very hard to reproduce with the machine

01:25:30.600 --> 01:25:32.600
that you have to make conscious decision.

01:25:32.600 --> 01:25:34.600
You have to carefully select the training material

01:25:34.600 --> 01:25:38.600
and you usually need a chain of algorithms that work together,

01:25:38.600 --> 01:25:40.600
which you orchestrate somehow.

01:25:40.600 --> 01:25:43.600
And then you can, that requires some work.

01:25:43.600 --> 01:25:46.600
But in the end, it gives you perfect outcomes

01:25:46.600 --> 01:25:50.600
and we have actually asked a German court,

01:25:50.600 --> 01:25:53.600
legal court to evaluate whether it would take the output

01:25:53.600 --> 01:26:00.600
of our algorithm in a lawsuit to represent the opinion

01:26:00.600 --> 01:26:02.600
that usually is given by human expert.

01:26:02.600 --> 01:26:04.600
And they said, the court said, yes, they would take this

01:26:04.600 --> 01:26:07.600
because the quality is that we produce is higher

01:26:07.600 --> 01:26:10.600
than the average human expert's quality.

01:26:10.600 --> 01:26:12.600
So this is just to give you an example

01:26:12.600 --> 01:26:15.600
that I'm a great fan of, I think a lot can be done,

01:26:15.600 --> 01:26:18.600
but it doesn't all work out of itself.

01:26:18.600 --> 01:26:21.600
Humans have to design such machines

01:26:21.600 --> 01:26:26.600
like we've designed planes or cars or other machines.

01:26:26.600 --> 01:26:29.600
And then they can work really well

01:26:29.600 --> 01:26:34.600
and take a lot of hard and sweatshop type of work

01:26:34.600 --> 01:26:37.600
of humans and create a lot of value.

01:26:37.600 --> 01:26:39.600
And this is what I would like to show to you

01:26:39.600 --> 01:26:41.600
and this compositional principle here

01:26:41.600 --> 01:26:44.600
is I think what real AI is about.

01:26:44.600 --> 01:26:48.600
And now it can be very cool to train neural networks

01:26:48.600 --> 01:26:50.600
as one part of this,

01:26:50.600 --> 01:26:53.600
but a neural network alone will usually not do

01:26:53.600 --> 01:26:56.600
and that's why I'm showing an operator at the end

01:26:56.600 --> 01:27:00.600
because this operator is usually a logical operator

01:27:00.600 --> 01:27:03.600
and it makes sure that the result is reliable.

01:27:03.600 --> 01:27:05.600
Because unlike stochastic models,

01:27:05.600 --> 01:27:10.600
logical operators can auto-detect their mistakes.

01:27:10.600 --> 01:27:15.600
And so that's why I'm a big fan of combining stochastic AI

01:27:15.600 --> 01:27:18.600
with good old first-order logic AI,

01:27:18.600 --> 01:27:22.600
which we both use in parallel in my company.

01:27:22.600 --> 01:27:25.600
And so we have people who are specialized in neural networks,

01:27:25.600 --> 01:27:29.600
but also people who are specialized in mathematical logic

01:27:29.600 --> 01:27:32.600
so that we can obtain the precision that is needed

01:27:32.600 --> 01:27:34.600
to automate human activity.

01:27:37.600 --> 01:27:38.600
Very good.

01:27:38.600 --> 01:27:39.600
That's it.

01:27:39.600 --> 01:27:41.600
So thank you, Jost.

