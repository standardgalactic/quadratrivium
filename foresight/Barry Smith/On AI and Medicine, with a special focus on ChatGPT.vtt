WEBVTT

00:00.000 --> 00:06.560
All right, so old-fashioned chatpots are obviously lossy.

00:06.560 --> 00:10.600
They repeat themselves over and over again, and they're bland.

00:10.600 --> 00:15.640
Google Translate is not so obviously lossy until you test it with really difficult text,

00:15.640 --> 00:17.440
and then it fails.

00:17.440 --> 00:21.960
And there are easy ways of generating failure for Google Translate.

00:21.960 --> 00:29.840
So what's going on now is that we, by adding more and more parameters to the AI systems

00:29.840 --> 00:36.160
that we're building in creating language models, we're getting closer and closer to having

00:36.160 --> 00:46.520
created in the machine a more and more adequate simulation of the way language works outside

00:46.520 --> 00:47.520
the machine.

00:47.520 --> 00:51.800
But it's still lossy, as we will see.

00:51.800 --> 00:57.080
And so we don't need to deal with this.

00:57.080 --> 01:00.800
NGP is a lossy paraphrase generator for text.

01:00.800 --> 01:09.000
That's the main thesis, and yours will give a more elaborated thesis in his presentation.

01:09.000 --> 01:11.080
It's bland, generally speaking.

01:11.080 --> 01:17.720
It's great for creating summaries, modulo, the problems which we will identify.

01:17.720 --> 01:23.280
It's bland, it gives you often too much output.

01:23.280 --> 01:26.360
It can't really give one word answers.

01:26.360 --> 01:29.120
It always wants to tell you what it knows.

01:29.120 --> 01:33.600
But the most important problem is that it's full of errors.

01:33.600 --> 01:39.280
So I asked this question a couple of days ago.

01:39.280 --> 01:42.280
So who is William Hogan?

01:42.280 --> 01:43.280
I originally asked.

01:43.280 --> 01:47.360
It didn't know any William Hogan.

01:47.360 --> 01:54.080
Eventually we got it to recognize the William Hogan, but it got the initial wrong.

01:54.080 --> 01:58.840
He thinks you're called William W. Hogan.

01:58.840 --> 01:59.840
He's not.

01:59.840 --> 02:07.040
So if we go through, so it thinks other false things, all of these are false things.

02:07.040 --> 02:10.680
So this is what's left.

02:10.680 --> 02:14.920
That the career before Florida is missing because it got it all wrong and thought it

02:14.920 --> 02:22.000
occurred in Alabama, in fact, it occurred in Pennsylvania, and this is typical.

02:22.680 --> 02:25.640
And I'll give you another example in a minute.

02:25.640 --> 02:33.920
So using chat GPT is worthwhile for creating summaries of a certain sort, but you have

02:33.920 --> 02:35.240
to check for errors.

02:35.240 --> 02:41.440
But because there are so many errors, quite serious errors when it comes to research science,

02:41.440 --> 02:45.520
it's not useful for research at this stage.

02:45.520 --> 02:50.440
And the one question is whether it can ever be useful for research.

02:50.440 --> 02:53.680
So this is another example.

02:53.680 --> 02:58.800
I was doing research on biomedical ontology, so I asked him what are the principal publications

02:58.800 --> 03:03.960
in this field, and it invented a new journal.

03:03.960 --> 03:11.760
And I asked him what were the 10 most important papers, and it invented seven papers.

03:11.760 --> 03:15.280
Each of them were vaguely resembling existing papers.

03:15.280 --> 03:19.920
They had overlapping authors, overlapping titles.

03:19.920 --> 03:24.480
And they were all in this non-existent journal.

03:24.480 --> 03:32.560
And this is partly a product of the fact that chat GPT wants to be helpful, so it's giving

03:32.560 --> 03:37.440
you all this information, which it thinks you will be happy about.

03:37.440 --> 03:42.080
All right, now one more example, and then I'll turn this over to Yobes.

03:42.080 --> 03:47.840
So a long, long time ago, I was working on the field of consumer health.

03:47.840 --> 03:53.400
In other words, trying to find adequate means of dealing scientifically with the way patients

03:53.400 --> 03:56.280
describe their health conditions.

03:56.280 --> 04:01.200
And so I was working with some linguists on creating something called medical wordnet.

04:01.200 --> 04:04.320
It just didn't really go anywhere, it was an interesting experiment.

04:04.320 --> 04:09.080
The paper is still cited because the idea, I think, is a good one.

04:09.080 --> 04:15.640
Now the idea was that we would create a kind of standard patient vocabulary, which would

04:15.640 --> 04:22.880
make, I don't want to exaggerate here, but would make all the mistakes that patients

04:22.880 --> 04:29.800
make in their understanding of their problem and what the doctor can do.

04:29.800 --> 04:32.000
Now the mistake is wrong.

04:32.000 --> 04:36.640
Now all the simplifications which are characteristic of non-experts.

04:36.640 --> 04:43.080
So it was a non-expert controlled vocabulary for health, that was the idea.

04:43.080 --> 04:51.960
Now since we have chat GPT, and since chat GPT is so desirous of being friendly, I figured

04:51.960 --> 04:56.840
it could build a consumer health vocabulary.

04:56.840 --> 05:02.520
And now I wondered how I could set up a situation in which it would do that.

05:02.520 --> 05:06.200
And I came across something called family feud.

05:06.200 --> 05:09.000
Now I guess you all know what family feud is.

05:09.000 --> 05:11.960
I didn't know what it was because I come from England.

05:11.960 --> 05:15.760
But we don't have anything like, or we didn't then have anything like this.

05:15.760 --> 05:21.480
So the idea of family feud is that you ask questions and you give answers which are not

05:21.480 --> 05:30.800
necessarily true, but which are typical of what ordinary people would answer.

05:30.800 --> 05:32.960
And so this is the question.

05:32.960 --> 05:36.440
Tell me something sharks are known to eat.

05:36.440 --> 05:41.920
You have to say either fish or people, and then there are some small scale low probability

05:41.920 --> 05:44.120
options which people can get.

05:44.120 --> 05:47.160
These are the two that one is searching for.

05:47.160 --> 05:54.840
So I tried it with chat GPT, and he gave me this list, so really friendly lots of choices.

05:54.840 --> 05:57.520
But there are no people here you'll notice.

05:57.520 --> 06:02.320
So now my challenge is to try and get it to say people.

06:02.320 --> 06:09.720
And so this was my second attempt, and now it's got other marine animals such as whales

06:09.720 --> 06:16.240
and dolphins, so it's not really getting closer.

06:16.240 --> 06:17.240
It apologizes.

06:17.240 --> 06:21.280
When you tell it, it's not getting closer, it apologizes.

06:21.280 --> 06:27.440
So it says the second most popular answer after fish is likely to be seals and sea lions.

06:27.440 --> 06:30.640
So it's not getting the goal of this at all.

06:30.640 --> 06:34.400
And then it apologizes again when I say it's still wrong.

06:34.400 --> 06:38.680
Now it wants sea turtles.

06:38.680 --> 06:39.800
So that was still wrong.

06:39.800 --> 06:42.160
Then it tried seals, then it tried seals again.

06:42.160 --> 06:46.280
Then you're asking what they would say on family feud.

06:46.280 --> 06:47.280
I did actually.

06:47.280 --> 06:49.480
It didn't give anything coherent.

06:49.480 --> 06:51.680
You had to explain what family feud was.

06:51.680 --> 07:00.960
I think that the training material for chat GPT was more the boring Wikipedia type of

07:00.960 --> 07:07.000
text rather than the exciting, entertaining daily human conversation.

07:07.000 --> 07:11.600
So it seals three times because that's one word, and I was trying to get it to focus

07:11.600 --> 07:12.600
on one word.

07:12.600 --> 07:14.520
But then it said surface and swimmer.

07:14.520 --> 07:19.440
It's really good because it's moving towards people.

07:19.440 --> 07:30.080
And so it got better and better, so beach goers, and then ocean users, and then marine

07:30.080 --> 07:31.080
recreationists.

07:31.080 --> 07:34.080
It is a word actually.

07:34.080 --> 07:37.600
You get that phrase just a couple of times on Google.

07:37.600 --> 07:43.680
I'm not sure you ever get marine oriented individuals, but you do get marine oriented.

07:43.680 --> 07:50.320
So it doesn't make up words very occasionally, I discovered, but it didn't actually make up

07:50.320 --> 07:51.320
any words here.

07:51.320 --> 07:54.320
But then it gets sailor.

07:54.320 --> 07:56.760
This is really good.

07:56.760 --> 08:04.000
So I tell it, you were closest with sailors, start from there, and this is the answer again.

08:04.000 --> 08:13.320
All right, this may be an example of creativity, so that's the end of that.

08:13.320 --> 08:17.400
So I think we all know this is how chat GPT works.

08:17.400 --> 08:25.000
You give it a sequence and it tries to extend it wordlet by wordlet, and it always picks

08:25.000 --> 08:31.920
the highest probability word except when it doesn't.

08:31.920 --> 08:37.000
So there's a question, how can marine oriented ever be a high probability word?

08:37.000 --> 08:42.520
Well, the answer is because you've already given all the higher probability words.

08:42.520 --> 08:50.520
But if you tell it to give the highest probability word, it will very quickly descend into repetition.

08:50.520 --> 08:52.880
It will just repeat itself over and over again.

08:52.880 --> 08:55.840
And so you have to have rules which prevent that.

08:55.840 --> 08:58.240
And that's the end.

08:58.240 --> 09:13.080
Now I'm hoping that we can share the screen so that we can see yours.

09:13.080 --> 09:15.280
Shouldn't I share my screen now?

09:15.280 --> 09:19.240
I'm going to see if I can make this.

09:19.240 --> 09:23.040
Well, Jihad will come and let you share your screen.

09:23.040 --> 09:24.040
Yeah, you can.

09:24.040 --> 09:28.240
If you're on TV, make sure you're screened.

09:28.240 --> 09:31.680
Yeah, I clicked it.

09:31.680 --> 09:33.680
Okay, this is good.

09:33.680 --> 09:35.680
Wait a minute.

09:35.680 --> 09:37.680
One second.

09:37.680 --> 09:39.680
Sorry.

09:39.680 --> 09:50.000
We need, oh no, gosh, I can't, I can't share my screen on, I've never found out how to

09:50.000 --> 09:54.320
do it with teams.

09:54.320 --> 09:55.320
So very.

09:55.320 --> 10:00.520
Teams, can you try going incognito and loading?

10:00.520 --> 10:03.120
No, no, I've tried too many times.

10:03.120 --> 10:04.120
It's a Mac computer.

10:04.120 --> 10:05.120
I have.

10:05.120 --> 10:06.120
Okay.

10:06.120 --> 10:10.400
Someone else needs to share.

10:10.400 --> 10:11.400
I'm sorry.

10:11.400 --> 10:12.560
I didn't know we were using teams.

10:12.560 --> 10:15.600
Someone else needs to present my presentation.

10:15.600 --> 10:18.720
Oh, what do I have your slides?

10:19.440 --> 10:24.000
Yeah, but I don't know whether you have the latest, well, you shouldn't have to, so yeah,

10:24.000 --> 10:25.000
share them.

10:25.000 --> 10:26.000
Okay, let's try.

10:26.000 --> 10:27.000
So I will.

10:27.000 --> 10:28.000
I will.

10:28.000 --> 10:40.920
It's a wrong computer.

10:40.920 --> 10:46.320
You could start by describing what you think about what I just said.

10:46.920 --> 10:50.840
Or you made some mistakes, but we will clarify them as I go through the presentation.

10:50.840 --> 10:55.480
Yeah, if you go through the mistakes, that would be useful.

10:55.480 --> 10:56.480
I don't remember.

10:56.480 --> 10:59.760
I think, oh, let me see what were they?

10:59.760 --> 11:08.600
Oh, so about AlphaFold, you said that it is actually modeling a logic system.

11:08.600 --> 11:09.600
It's not.

11:09.600 --> 11:14.200
AlphaFold is modeling a complex system as a logic system.

11:14.200 --> 11:15.200
So that's very important.

11:15.560 --> 11:21.760
So what AlphaFold models is a complex system behavior, the folding of proteins, but using

11:21.760 --> 11:28.200
a logic system, and it works because the logic system can approximate, I'm getting echo.

11:28.200 --> 11:33.200
To some extent, what's going on in the complex systems?

11:33.200 --> 11:36.200
Okay, I have now your slides.

11:37.200 --> 11:40.200
Are you still getting an echo?

11:40.200 --> 11:42.200
Let me try.

11:42.200 --> 11:43.200
Now it's gone.

11:43.200 --> 11:44.200
Thank you.

11:44.200 --> 11:58.200
And then I think when you described how chat GPT works, it was not really, really fully

11:58.200 --> 12:02.200
accurate, but I will explain once my slides are up.

12:02.200 --> 12:05.200
I'm sorry for the inconvenience.

12:05.200 --> 12:11.200
But when I do what Teams tells me to change on my computer, it never helps.

12:11.200 --> 12:14.200
I never get the result that I should.

12:14.200 --> 12:16.200
So I think it's under here.

12:20.200 --> 12:21.200
Oh, we're waiting.

12:21.200 --> 12:27.200
I was surprised to hear that the reference to Family Feud was not something that was in

12:27.200 --> 12:32.200
the training data set for chat GPT because the training is going to crawl.

12:32.200 --> 12:37.200
So it's like Peter Norvig, the father of AI, is one of the maintainers of this.

12:37.200 --> 12:41.200
And they've crawled the entire internet.

12:41.200 --> 12:42.200
You're right.

12:42.200 --> 12:47.200
It's wrong that Family Feud is not in it.

12:47.200 --> 12:53.200
The reason why certain results are not given is there are several reasons we will learn

12:53.200 --> 12:59.200
as soon as I can present my slides why certain elements of the training material.

12:59.200 --> 13:00.200
Thank you.

13:00.200 --> 13:01.200
No, that's the wrong one there.

13:01.200 --> 13:04.200
That's the one with your corrections.

13:04.200 --> 13:07.200
OK, have your slides now.

13:07.200 --> 13:10.200
I will move them forward when you say next.

13:10.200 --> 13:16.200
But you were presenting the ones with annotations and corrections.

13:16.200 --> 13:20.200
You were presenting a version where you have deleted entity.

13:20.200 --> 13:23.200
So I need to email them to you again the latest ones.

13:23.200 --> 13:25.200
I'm sorry for all this delay.

13:25.200 --> 13:28.200
I really wasn't prepared for this.

13:28.200 --> 13:34.200
OK, so what do you want to do?

13:34.200 --> 13:36.200
He's sending a new set.

13:36.200 --> 13:38.200
I'm so sorry about the delay.

13:38.200 --> 13:42.200
But we are still in time according to the schedule I received from you.

13:42.200 --> 13:44.200
So you can see our screen, right?

13:44.200 --> 13:48.200
Yeah, I can see everything, but just the pain.

13:48.200 --> 13:52.200
So will you get your email on this user?

13:52.200 --> 13:56.200
First of all, I need to find G-Hards.

13:56.200 --> 13:58.200
Go ahead.

13:58.200 --> 14:03.200
I send it to you G-Hards and you, Bill, and also to Barry.

14:03.200 --> 14:05.200
So that's the second.

14:05.200 --> 14:09.200
And let me just say one thing about Family Feud.

14:09.200 --> 14:14.200
So one thing I noticed is that a very common scenario is that it will say,

14:14.200 --> 14:17.200
either it will say error or it will say,

14:17.200 --> 14:19.200
I don't know what you are referring to.

14:19.200 --> 14:21.200
Can you give me more information?

14:21.200 --> 14:25.200
This happened with William Hogan and with Bill Hogan.

14:25.200 --> 14:30.200
The first couple of times I couldn't find him and I had to then add more information.

14:30.200 --> 14:32.200
I can't remember exactly what happened when I sent it.

14:32.200 --> 14:37.200
So I send it to you G-Hards, Barry and Bill and so on.

14:37.200 --> 14:41.200
Yeah, because I wonder if you, you know, I probably didn't wish.

14:41.200 --> 14:43.200
It's in the trading data set.

14:43.200 --> 14:44.200
It just needs more.

14:44.200 --> 14:47.200
Maybe it's contextual prompting.

14:47.200 --> 14:51.200
Barry, can you unshare this version with the correction please?

14:56.200 --> 15:08.200
I still didn't receive it, but it sometimes takes a couple of minutes.

15:08.200 --> 15:13.200
By the way, I just start my presentation without showing any slides.

15:13.200 --> 15:14.200
It doesn't matter.

15:14.200 --> 15:20.200
So, so the presentation is called large language models are just very complicated analytical engines.

15:20.200 --> 15:25.200
And on the first slide, I'm showing a model of Charles Babbage analytical engine.

15:25.200 --> 15:30.200
So the analytical engine of Charles Babbage was the theoretical attempt to build an analogous computer.

15:30.200 --> 15:31.200
Can you hear me?

15:31.200 --> 15:32.200
Yes.

15:32.200 --> 15:40.200
An analogous computer that could perform reckoning operations like multiplication and so on.

15:40.200 --> 15:47.200
And it also has a theoretical programming language which Babbage designed, but it was never built.

15:47.200 --> 15:55.200
And I think the Royal Library of Engineering also said that it shouldn't be built because it would be useless.

15:55.200 --> 16:03.200
So then the first time it was built was only 80 years ago with Charles Babbage.

16:03.200 --> 16:06.200
Of course, it was very bad.

16:11.200 --> 16:13.200
Did you receive the email at home?

16:13.200 --> 16:14.200
Yeah.

16:15.200 --> 16:16.200
Can you get it?

16:16.200 --> 16:18.200
It would be really nice to get rid of the echo.

16:24.200 --> 16:25.200
So this is the end.

16:25.200 --> 16:27.200
I still have the echo.

16:27.200 --> 16:29.200
Okay, hold on one second.

16:29.200 --> 16:32.200
Try to mute us to see if that helps.

16:34.200 --> 16:35.200
So I try again.

16:37.200 --> 16:38.200
I still have echo.

16:39.200 --> 16:42.200
I'm on the roof in the meeting.

17:00.200 --> 17:02.200
Did you get the email now?

17:02.200 --> 17:03.200
Yes, we have the email.

17:03.200 --> 17:07.200
We're just copying it to a stick.

17:07.200 --> 17:10.200
Oh, the computer is not connected to the internet.

17:10.200 --> 17:13.200
Some of them are and some of them aren't.

17:16.200 --> 17:19.200
Anyhow, so the analytical engine of Babbage was never built.

17:19.200 --> 17:25.200
The first computer to be built was Conrad Susie Z3.

17:25.200 --> 17:29.200
It was also an analog computer, mechanical computer.

17:29.200 --> 17:34.200
And so basically LLMs are just very complicated.

17:34.200 --> 17:37.200
And we will, what does it mean?

17:37.200 --> 17:43.200
So there are currently there's a huge hype around large language models.

17:43.200 --> 17:45.200
Check GPT is one of them.

17:45.200 --> 17:47.200
Then Google has one, which is called Bard.

17:47.200 --> 17:55.200
And there's another one by META, formerly known as Facebook,

17:55.200 --> 17:58.200
which is called Galaxica.

17:58.200 --> 18:03.200
And so the one by the first one, OpenAI,

18:03.200 --> 18:07.200
which is also, by the way, used by Microsoft

18:07.200 --> 18:11.200
and now being built into Bing, Microsoft's hopeless competitor

18:11.200 --> 18:16.200
for Google Search, is perceived as a great success

18:16.200 --> 18:19.200
and an AI breakthrough despite the massive hallucinations

18:19.200 --> 18:21.200
that Barry has just described.

18:21.200 --> 18:27.200
And there are huge expectations from the markets.

18:27.200 --> 18:33.200
So currently, check GPT is valued at $30 billion.

18:33.200 --> 18:36.200
That's $30,000 million.

18:36.200 --> 18:39.200
And Microsoft is investing billions.

18:39.200 --> 18:42.200
And as I said, integrating GPT into Bing.

18:42.200 --> 18:50.200
Google Bard was also presented a couple of weeks ago

18:50.200 --> 18:54.200
in a public investor presentation.

18:54.200 --> 19:00.200
And it made a very minor mistake about the James Webb Space Telescope.

19:00.200 --> 19:06.200
And this very minor mistake basically led to a hysterical reaction of the markets.

19:06.200 --> 19:09.200
So Google lost $100 million of market capitalization.

19:09.200 --> 19:14.200
Alphabet lost $100 million market capitalization when this error was announced.

19:14.200 --> 19:17.200
And I'm still not on.

19:17.200 --> 19:25.200
And Bard will nevertheless be integrated in Google Search sooner or later.

19:25.200 --> 19:30.200
And then there's Meta, which was even launched before Check GPT

19:30.200 --> 19:34.200
and which was perceived by the public as a total failure due to the hallucinations.

19:34.200 --> 19:39.200
But the hallucinations were of the same degree of severity as in Check GPT.

19:39.200 --> 19:42.200
Why then was it perceived so differently?

19:42.200 --> 19:46.200
Because Meta claimed to be able to summarize scientific papers.

19:46.200 --> 19:56.200
And in the science domain, the invention of the model seemed much worse than in other areas.

19:56.200 --> 20:01.200
So the LLMs all have a similar performance,

20:01.200 --> 20:06.200
but the public perception validation of them is a matter of spin and contextualization.

20:06.200 --> 20:08.200
So this is, I think, very important.

20:08.200 --> 20:11.200
Now on the next slide, please.

20:12.200 --> 20:16.200
We see somehow the format got destroyed.

20:16.200 --> 20:21.200
Yet another, I should have sent you a PDF anyhow.

20:21.200 --> 20:24.200
Yeah, you don't seem to have the font I'm using.

20:24.200 --> 20:27.200
Well, I'm sorry.

20:27.200 --> 20:32.200
So now the hype around large language models has also reached medicine.

20:32.200 --> 20:37.200
So on the left hand, there's a new paper which just appeared a few weeks ago,

20:38.200 --> 20:41.200
by Kung et al.

20:41.200 --> 20:48.200
And it tried to test Check GPT on the USMLE, which many of you must have passed.

20:48.200 --> 20:50.200
And as you may remember, I didn't.

20:50.200 --> 20:53.200
I have the German one, but it has three levels, right?

20:53.200 --> 20:54.200
This is undergraduate.

20:54.200 --> 21:00.200
This seems to be before the clinical period and this after the clinical period.

21:00.200 --> 21:05.200
And so you see that, I mean, the paper has some weaknesses.

21:05.200 --> 21:14.200
It's not perfect, but you see that Check GPT actually solved up to 60% of USMLE questions.

21:14.200 --> 21:18.200
And it's interesting that the performance was best for part three.

21:18.200 --> 21:26.200
And that's because part three has content of which you see a lot on the web, right?

21:26.200 --> 21:33.200
So in the internet, you have a lot of content that relates to clinical problems,

21:33.200 --> 21:37.200
but the preclinical stuff is, of course, not so much published on the web.

21:37.200 --> 21:41.200
And therefore, the performance here is the best.

21:41.200 --> 21:46.200
Now, of course, the algorithm would still have failed USMLE, but it's quite impressive.

21:46.200 --> 21:52.200
And the paper concludes that Check GPT performed at or near the passing threshold of 60% accuracy.

21:52.200 --> 21:57.200
Being the first to achieve this benchmark, this marks a local milestone in AI maturation.

21:57.200 --> 22:02.200
Impressively, Check GPT was able to achieve this result without specialized input from human trainers.

22:02.200 --> 22:07.200
Now then, they also claim that Check GPT displayed comprehensible reasoning and valid clinical insights.

22:07.200 --> 22:11.200
This is total anthropomorphic nonsense, as you will see.

22:11.200 --> 22:15.200
And of course, an unrefined Check GPT fails the exams.

22:15.200 --> 22:24.200
So I think the glass is rather half empty than half full, but the authors conclude that Check GPT may potentially assist human learners in a medical education setting.

22:24.200 --> 22:35.200
Now, I believe that the model can be refined by training it on specific material, and then it can probably achieve 80% in all three parts of the exam.

22:35.200 --> 22:45.200
And at that stage, it could be used for as an expert system for decision support, but certainly not for automation, of course, because it doesn't think.

22:45.200 --> 22:51.200
So this is just to give you an impression how this is going to might affect medicine as well.

22:51.200 --> 23:00.200
Though one has to put in a word of caution here, you all know that so far expert systems have been around since 50 years.

23:00.200 --> 23:09.200
Some of them outperform have outperformed humans since at least 40 years and still they're not seen in clinical practice very much because physicians resist their introduction.

23:09.200 --> 23:11.200
Nevertheless, it's an impressive result.

23:11.200 --> 23:14.200
So if you please could move to the next slide.

23:14.200 --> 23:23.200
Um, so let's let's take a look what large language models really are. So basically the sequential, stochastic models. Now what's that?

23:23.200 --> 23:31.200
So on a on a coaster. So basically I applied mathematics for the identification or mapping of recurrent patterns into machines.

23:31.200 --> 23:39.200
It's not a model of human mind or even animal intelligence. So I I doesn't think or intend. And there are two types of AI.

23:39.200 --> 23:42.200
There's deterministic AI.

23:42.200 --> 24:00.200
Which contains rules, search recipes, logics and trees, which is explainable and very reliable. So this is what's built in to most of the, let's say, the last steps of the behavior of all the war and military equipment.

24:00.200 --> 24:13.200
So there's also some deterministic AI. There are there's also some suggestive AI built in, especially in the census, but the decisions are made by deterministic. So Cruz missile hits its target based on deterministic AI.

24:13.200 --> 24:21.200
And so classic AI is regression classification pattern recognition. It's not explainable. And it's probabilistic.

24:21.200 --> 24:25.200
So examples are AlphaGo and chat GPT.

24:25.200 --> 24:34.200
And of course hybrid models. For example, chat GPT is actually itself a hybrid model. It's not the purely purely stochastic model.

24:34.200 --> 24:39.200
Okay, as we will see a bit later. So if we move on, please, and thanks for moving on the slides.

24:39.200 --> 24:47.200
So what what we see is AI is basically statistical learning for automation pattern identification. Just to remind you, we have here.

24:47.200 --> 24:56.200
Like this, we have an input set and an output set. These are the independent variables. These are dependent variables. Like for example, the independent variable is an email text.

24:56.200 --> 25:09.200
And now you have as output the decision of if it's spam or not. Now what we do is we train an AI relation and and this relation is computed using an optimal optimization function.

25:09.200 --> 25:20.200
Yeah, sorry. Here's the format is broken. There was an equation inserted here. So this was this was just this this tuple of that used to train. So these are the training tuple for the training.

25:20.200 --> 25:30.200
And the relationship between input and output is modeled using a loss function, which is minimized by numerical optimization procedures. So that's all that's happening in supervised learning.

25:30.200 --> 25:33.200
On the next slide.

25:33.200 --> 25:38.200
We see that now they have moved on since 2013.

25:38.200 --> 25:54.200
We have moved on to unsupervised learning. So but interestingly, the big breakthrough for the third AI wave was Google, the Google algorithm that recognized that could create an abstract representation of a face of a cat for millions of photons.

25:54.200 --> 26:08.200
So how does this is did this work? Millions of photos were given as input to the to this convolutional neural network. And it had the task to actually recreate the input that it had received as output.

26:08.200 --> 26:22.200
But while the image data was going through the neural network, and with many image data given through it, the model achieved a parameterization in the middle layer that rendered an abstract cat face.

26:22.200 --> 26:39.200
And so this is called a foundational model. So that's a model that is trained without outcome. So it's just trained by by by asking your network to to basically reconstitute the input it receives as output.

26:39.200 --> 26:54.200
So it's also called encoder decoder architecture. So the input data is encoded in a certain way, and then it's decoded and then it's output again. And when you do this, you basically parameterize a multivariate distribution of the input data.

26:54.200 --> 27:05.200
And then you obtain a foundational model. And this foundational model can then also, and that's happening in unsupervised manner. So you to do this, you don't need any training data.

27:05.200 --> 27:18.200
Sorry, any any outcomes, like we showed in the previous show in the previous site, you just need the raw data. This is very practical because it saves you the whole huge annotation effort, you can just train with data.

27:18.200 --> 27:30.200
And then you obtain this multivariate distribution model of of the data sequences. You can do this for images and also for texts. And then you can use supervised adaptation to get a domain model.

27:30.200 --> 27:42.200
And so, so GPT three Bert and clip are three important examples for such for such models.

27:42.200 --> 27:52.200
Google Translate is I think also now moved over to foundational model, but earlier versions were still done in a supervised way.

27:52.200 --> 28:02.200
Okay. And, and are there any questions here, can you still hear me all right.

28:02.200 --> 28:06.200
No questions.

28:06.200 --> 28:08.200
Okay.

28:09.200 --> 28:13.200
Yes, I was muted to minimize echo for you. Can you

28:13.200 --> 28:31.200
Yes. Yeah, and any questions. I have one question. So, when you talked about unsupervised learning, didn't there have to be someone at the end of the chain, who was recognizing whether they had identified a cat correctly or not.

28:31.200 --> 28:42.200
So, because what you do when you unsupervised learning, you, you basically have an algorithm that takes care of checking whether the input is equal to the output.

28:42.200 --> 28:55.200
So the model, what you do is, if you have a picture of a cat, you encode it in a three dimensional matrix, indicating the pixel, the pixel shades of gray or color.

28:55.200 --> 29:08.200
Yeah, let's say you use a black and white picture, then you just indicate for each pixel, the shade of gray it has, right. And then, and then you just measure from the output whether it looks like the input.

29:08.200 --> 29:22.200
And so, so you can fully automate. So of course it's unsupervised doesn't mean that the human being is not involved somebody has to write all this algorithm and test it and make sure that it works but then unsupervised means that the outcome.

29:22.200 --> 29:40.200
The outcome to the data, right. And, and so this is how foundation models get trained and it's super impressive because without any human input into into which which we have used for 50 years and in statistical learning or even longer.

29:40.200 --> 29:54.200
So it goes back to Bosco which which was 1760 right. So Bosco which invented statistical learning in 1760 and for two for 250 years we have always used, you know, input output tuples.

29:54.200 --> 30:05.200
And now you have a tuple free learning. This is, this is a huge progress. Of course they are you setting up the algorithm but it's still very impressive.

30:05.200 --> 30:20.200
I have one question. And I know that you're referring specifically to foundational language models when you refer to deep learning models being a form of stochastic models but I just wanted to make it clear that, you know, most deep learning models are actually

30:20.200 --> 30:32.200
deterministic the language models we're talking about like auto aggressive language models draw from a probability distribution, you know, for the output and that's what makes them more stochastic is because they're not just picking the most probable

30:32.200 --> 30:41.200
ones. Okay, whereas most deep learning models like computer vision models you serve the same input to the model 10 times you're going to get the same response so they are.

30:41.200 --> 30:49.200
Okay, so, so they are all deployed. Can you mute this again because I'm getting terrible echo.

30:49.200 --> 31:11.200
Sorry, Jihad. So, thank you. So all deployed stochastic models are always deterministic. So what whenever you train a primitive linear regression or multi variate regression, or some kernel regression or classifier, the attribute

31:11.200 --> 31:24.200
typically refers to the way you train it. Once it is trained, it's always deterministic so there is every stochastic model that gets deployed works in deterministic fashion, it will always produce the same output based on identical

31:24.200 --> 31:38.200
input. It's just that the term deterministic versus stochastic only describes the way principles, which, which I use to create the model, but once the model is started has deterministic behavior.

31:38.200 --> 31:55.200
Right, so the problem that you have with why stochastic models are unreliable is that you, that, that you cannot easily predict which behavior it will have based on which input, but given an exactly identical input, the output will always be the same.

31:55.200 --> 32:02.200
So, so the behavior of the model is always deterministic once they get deployed, no matter how they are created.

32:02.200 --> 32:03.200
Okay.

32:03.200 --> 32:12.200
Can you then explain how it is that when I play with just chat GPT, I can ask the same question eight times and get different answers.

32:12.200 --> 32:31.200
That's because, because there's a controller sitting in front of the stochastic model. And we will get back to this later. But this is basically the proof. This behavior of chat GPT proves that it has a controller based architecture with with if it was a pure

32:31.200 --> 32:38.200
end to end neural network in deployed mode, it would be have deterministic behavior.

32:38.200 --> 32:50.200
I mean, what one way that auto aggressive language models have done that is to draw from a probability distribution is, you know, predicting the next word, you know, you don't just predict the most probable next word you draw from a probability

32:50.200 --> 32:55.200
distribution and you happen to get, you know, a word that's not the most probable.

32:55.200 --> 33:02.200
I'm back to this later. Okay, so can you present again jihad.

33:02.200 --> 33:06.200
So, next slide please.

33:06.200 --> 33:24.200
So, let's look at how the large language monitor chatbots are trained so the first step is basic training without without outcome, foundational model as we've just seen it then there's specification of the model to task.

33:24.200 --> 33:34.200
There's supervised learning, and then there's reward learning and then there's an automated reinforcement learning approach on question answer pairs. It's very impressive.

33:34.200 --> 33:50.200
And so, so the first step is in the current version of jet tpt or the one, let's say about which I read publications in January maybe they're now deployed another one was GPT three dot five was used as a foundational model and the dark blue parts are the refinement

33:51.200 --> 33:56.200
steps. If we go to the next slide.

33:56.200 --> 33:59.200
We see.

33:59.200 --> 34:07.200
I have to follow this. Yeah, I think that everything can come across. I'm sorry for the bad format thing it's it's I should have said the PDF.

34:07.200 --> 34:25.200
The first step the foundation language model is trained using a transformers auto attention so what was Vani in 2017 published, I think, in the last since Schmidtuber published his work about LSTM in the mid 70s this was the next most

34:25.200 --> 34:36.200
important paper about neural networks because because the LSTMs and the and the gated recurrent units that had been invented by Schmidtuber and his accoludes.

34:37.200 --> 34:40.200
They were not computationally very effective.

34:40.200 --> 34:59.200
And so was Vani at all from Google they showed in 2017 that that you can have a purely feed forward loop model that has only feed forward computations and no recursion in it but that can still achieve the same computational properties as the recursion you have in the LSTMs.

34:59.200 --> 35:07.200
And so what you see on top is basically just saying that a sequence of symbols is modeled.

35:08.200 --> 35:25.200
As a as a condition probability, right, and so that you basically say each of the symbol of the of the symbols or tokens out of the sequence is is a is a is multiplied with the with the next one or the one before, given the others.

35:25.200 --> 35:48.200
So this is just really a Gaussian or Bayesian Bayesian, sorry, naive based distributional approach to to the modeling of the sequence and the architecture of the model is relatively complicated.

35:48.200 --> 35:53.200
But in the end, the most important mechanism is attention.

35:53.200 --> 36:00.200
And because it and we will look at attention on the next slide but because attention only use information about other tokens from lower layers.

36:00.200 --> 36:16.200
It can be computed for all talks and parallel which needs to improve training performance and so the resulting operators so this gives you mathematically speaking a huge operator, which which which is a relation that relates a vector to another vector.

36:16.200 --> 36:31.200
You know function is a relation that relates a vector to a scalar, but this relates a sequence to another sequence so it's, it's not it's not it's mathematically speaking operator and this operator maps a sequence to itself but contains a parameterization which models the distribution

36:31.200 --> 36:46.200
of the sequence is found in the training material. This is super impressive because because it basically it can create, it can create, it can create,

36:46.200 --> 36:57.200
recreate a huge distribution of language and and you see this that that how good it is you see this from the fact that it has almost no syntactic errors when it creates output.

36:57.200 --> 37:01.200
So if you look at the next slide.

37:01.200 --> 37:15.200
Please. Thank you. So here you see how attention works. So this is a mechanism used in sequential neural networks to provide a context right weight vector that gives emphasis to predict relevant aspects of an input sequence so it, it, it asks.

37:15.200 --> 37:36.200
Here this sent there's an example here the animal didn't cross the street because it was too tired, and you see that for example the word it receives attentional enhancement and emphasis so that you can basically see that the in this subordinate

37:36.200 --> 37:53.200
sentence in the causes subordinate sentence. This refers to the noun phrase of the main sentence. And this is achieved by nobody thought this through but basically they tried it out right and they tried that it's done with this equation here which which

37:53.200 --> 38:12.200
defines the input sequence and then three different matrices of parameters and if you multiply, if you multiply this with with these matrices, which are at the beginning set at a certain initial value, then you can you can compute an output sequence shown

38:12.200 --> 38:28.200
and and this is just a heuristic computation recipe, but but it yields a relatively accurate syntax generation. If it is applied with sufficient bread spreads means that they use many such attention mechanisms on one sequence and depth that they stack a lot of attention on top of each other

38:28.200 --> 38:45.200
and they just tried it out and by doing this they found out that this creates an outer encoding that is very syntactically very reliable. This is a super impressive achievement, but it's from the quality of the result is comparable to what you get with an LSTM, but it's

38:45.200 --> 38:53.200
compute much faster. And so this is this is a very very important result.

38:53.200 --> 39:01.200
So and enter the final the final output is computed using a softmax function which is shown here.

39:01.200 --> 39:07.200
So, if we move to the next slide.

39:07.200 --> 39:25.200
So this is the foundation what has been trained in this way. Now comes the moderate comes the specification of the foundation model to the task at hand. So what they have they have a huge database of of important questions that are often asked in the internet.

39:25.200 --> 39:32.200
So this is a recipe for tomato soup. Can you please

39:32.200 --> 39:39.200
where is Paris in France or what is Paris

39:39.200 --> 39:43.200
when was Jesus Christ born and so on

39:43.200 --> 39:47.200
questions that are often asked and so so

39:48.200 --> 40:06.200
a prompt a sample from a prompt data set and then so basically then the the sampler they're a label or annotate as I call demonstrates the desired output behavior and and and so in this way they have actually written hundreds of thousands of answers to frequently asked

40:06.200 --> 40:16.200
questions. So not only have they written answers to questions but they've also solved tasks like write me a poem tell me a joke.

40:16.200 --> 40:33.200
Write a letter and so on and so on and hundreds of thousands so this was super expensive right so this is why open AI is a company that didn't do much mathematical innovation so they just took that the transformer model from Google but then they put a lot huge effort on this

40:33.200 --> 40:45.200
annotation and this now they have two pills like the ones from I described the beginning for supervised learning and now these two pills are given to the model and now the model is not used anymore to do auto encoding.

40:45.200 --> 40:52.200
But now it's basically like in transfer learning used to answer the questions so it's generate generates.

40:53.200 --> 41:03.200
It generates now a condition probability in the in the way that that that that that that this is done for for.

41:04.200 --> 41:22.200
For stem filters and by doing this the pyramid the millions 100 to 200 or 300 millions of parameters of the model gets fine tuned because they now are changed to create the desired output which is a very traditional form of machine learning.

41:22.200 --> 41:31.200
That's basically the Bosco which method again just that that the loss function you was using 250 years ago was a bit simpler.

41:31.200 --> 41:44.200
It was some of these squares and today the loss function is more complicated but basically it's the same trick of minimizing the difference between the desired output and the output of the model.

41:44.200 --> 41:54.200
And then that was the first step now you have a better model now then comes a step where you again collect data.

41:54.200 --> 42:09.200
So and and now but you know you don't you collect questions but now you don't write the answer anymore but you let the model write the answers but now an annotator ranks the answers by quality.

42:09.200 --> 42:21.200
And by doing this that a reward model is generated that tells the model if you create this type of answer then you get this and this reward there again tens of thousands of annotations are used.

42:21.200 --> 42:34.200
And now there's a really very very important step, which was also used to train Alpha Alpha go, which is that you use the reward model to now let the model train itself.

42:34.200 --> 42:44.200
Now you can give a task or ask a question that creates an outcome, and then you can give it a reward and do this again and again and again to make the model better and better at the end.

42:44.200 --> 42:50.200
The model will mostly give out answers that corresponds to the highly ranked answers.

42:50.200 --> 43:08.200
I like the high ranked answers that annotator ranked. And so this is this is a very important step. This method used to this method PPO which is called proximal policy optimization algorithm is a super interesting innovation.

43:08.200 --> 43:22.200
And actually this is I think the biggest contribution to intellectual property or to neural network science that Open AI has done itself and that was already in 2017.

43:22.200 --> 43:35.200
So at the beginning when Open AI were just a few people they focused on creating policy optimization algorithms for reinforcement learning and this was without this, you wouldn't get the results we're getting.

43:35.200 --> 43:46.200
So this this step is mathematically very elegant. And the decisive step that that makes a difference because here, you can't get enough material. So you create kind of an artificial reward.

43:46.200 --> 43:57.200
Now, now the advantage is that this allows you to really reparameterize the model to give satisfying answers, but also explains the weaknesses of the model, because it forces the model to always give an answer.

43:57.200 --> 44:16.200
And it also creates because this is very schematic right these rewards are very schematic and of course not very differentiated and that that's why the model is so bland and also repetitive because it is because how satisfying an answer is cannot be packaged into reward model.

44:16.200 --> 44:29.200
But here it is done, because otherwise you cannot train the PPO algorithm but the price is that you're getting kind of that you can recognize what the model creates because it's, it's very stereotypical.

44:29.200 --> 44:37.200
Okay, I hope this was understandable now let's move to the next slide.

44:37.200 --> 44:49.200
So maybe there are questions for before jihad.

44:49.200 --> 44:52.200
Any questions now. Yes.

44:52.200 --> 45:06.200
So, in your picture you have two places where, yeah, the female human avatar is represented does that mean gender.

45:06.200 --> 45:15.200
I think it's non gender avatar.

45:15.200 --> 45:17.200
Go on there.

45:17.200 --> 45:20.200
Does that mean that there are humans doing the labeling.

45:20.200 --> 45:31.200
Of course, I said an annotator. So, so the, the, the, the person showing there is an annotator and the annotator. These are thousands of annotators, a lot.

45:31.200 --> 45:36.200
They work actually we will see in the next slide under quite strict policies.

45:36.200 --> 45:49.200
And they, they, in the first step they really write answers. So this is a very expensive step, because they have to write answers to questions and you have to make sure that these answers are correct.

45:49.200 --> 45:52.200
So you have to have.

45:52.200 --> 46:01.200
And so the next in the second step they only give ratings to the quality of the output of the model. And from these ratings reward model is created.

46:01.200 --> 46:17.200
This is super clever at a great achievement of open AI and that's the core mathematical contribution to machine learning is that they found a way to create based on human scores a reward system that is you see in chess or go.

46:17.200 --> 46:32.200
You have a reward system that is very simple because the games can be formulated as points. So in go actually it's even simpler than in chess because in go in each situation on the board you can calculate exactly how many points move gives you.

46:32.200 --> 46:43.200
And so you can, you can set up the reward function of the reinforcement learning to be proportional to the number of points that you are getting at each step or at further steps and you can have a disk.

46:43.200 --> 46:54.200
You can have a factor that that penalizes steps that are further way and so on. But basically you have a natural way of giving points to reward system.

46:54.200 --> 47:08.200
And here you don't have this because it's a language answers and so they found a way to to to reward answer quality and that's what the second step up so it's super impressive.

47:08.200 --> 47:15.200
But it also explains the blend of the question answers.

47:15.200 --> 47:30.200
This is really not supervised but semi supervised, right? Well, the first two steps. The first step is called classical supervised. The second step is also supervised. It's a supervised generation of a reward model.

47:30.200 --> 47:43.200
And then the reward model is used for supervised learning but it's then not supervised anymore. But it's basically applying a reward model that was created using supervised learning.

47:44.200 --> 47:49.200
Any other questions or should we move on?

47:49.200 --> 47:51.200
Go ahead.

47:51.200 --> 47:55.200
Okay, very good. So if you could. Yes, thank you. Go to the next slide.

47:55.200 --> 48:03.200
And thanks for your hard work on this. I'm sorry for the technical problems I have. Okay, so now this is super interesting.

48:03.200 --> 48:14.200
So there is a moderation classifier that was created to avoid non PC language and for some model output corresponding to the woke culture expectations of today's West.

48:14.200 --> 48:28.200
So we know that that we have now the culture that we have now is not comparable at all. Well, many ways not comparable to what we had in the 1970s when the dirty Harry movie series with Eastwood was made.

48:28.200 --> 48:41.200
So now, you know, this is all outdated and everything has to be politically correct. And how did they how did they do this? How did they achieve this? So this is a paper.

48:41.200 --> 48:53.200
I'm afraid the reference is not visible because of the formatting problems. But this is a paper called holistic approach to undesired content detection in the real world and actually on the paper.

48:53.200 --> 49:06.200
So if you look at the PDF, it says warning some content may contain racism sexuality or other harmful language so that you already wanted to trigger warning that you should not be afraid that some dirty words appear in the paper as negative examples.

49:06.200 --> 49:29.200
So now, now what did they do so they, they use basically public domain data, and then also training data, and, and they have, and then they have a very sophisticated way by creating a classification model that that that classifies undesired language.

49:29.200 --> 49:48.200
So you have the categories of undesired brain is sex, what they call hate violence harassment, self harm, and then these are subcategories. This is for example, anti feminist sex sexual language, hated against colored people, and so on so these are subcategories of the former.

49:48.200 --> 50:04.200
You can see here if you use public so this is this shows you the the area under the curve for the classify which is of course as you all know, which measures how, how well it performs, and you see that that the specialty.

50:04.200 --> 50:09.200
Self harm is a super low, a detection rate is super bad.

50:09.200 --> 50:23.200
Sexuality is also not so good. Now they use this what they call DAT the domain adversarial training, where they were the iterate several times the training and improve here fan is advice improvements augmentation of training data.

50:23.200 --> 50:28.200
So, so they do a lot of this was super expensive exercise.

50:28.200 --> 50:42.200
And here there's also an adversarial team built in that creates more complicated hate speech and so on. By doing this they have already improved the baseline then they also have synthetic data that they have created themselves.

50:42.200 --> 51:04.200
And then with the mix the two, then they get very, very good area under the curve as you can see here for the for the for the cat for the broad categories they have now, not not perfect but quite good, quite good detection why is

51:05.200 --> 51:21.200
why are some categories not working so well, because either they are harder to detect or because they just couldn't put in even more effort, but but this training effort made created basically a PC quality classifier.

51:21.200 --> 51:38.200
And the result is so good that this is, it is not possible to make off hard to make jet GPT speaking simple toxic language so you can, as somebody has now shown, make it say some bit toxic language.

51:38.200 --> 51:49.200
Like I think somebody made it say that the person talking to suicide herself or himself, but but but it can't say the n word or f you, for example.

51:49.200 --> 52:00.200
And the classifier is probably part of the controller and it's certainly reinforced with a deterministic filter. So it's impossible to make it say the n word.

52:00.200 --> 52:15.200
And that's that that that can only be achieved that can't be achieved with the statistic model. So there must be deterministic filter building on the next slide so but this was cost also you know you have to also what I failed to say I forgot to say is that each training run

52:15.200 --> 52:25.200
will train the chat the GPT 3.5 model costs $2 million one run or $1.5 million of CPU time.

52:25.200 --> 52:39.200
Even if you own all the CPUs yourself and GPUs yourself will cost $1.5 million. And that. So that that that is basically the discount rate of the CPUs plus electricity and cooling costs you have.

52:39.200 --> 52:50.200
And so though, and then the further the other trainings we just saw also cost millions. So I think that they have used several hundred million just for the training, plus hundreds of million foot to pay for the annotate.

52:50.200 --> 52:56.200
So it's super expensive. Let's move on.

52:56.200 --> 53:03.200
Okay.

53:03.200 --> 53:06.200
I mean,

53:06.200 --> 53:10.200
given that we're talking about PC language.

53:10.200 --> 53:13.200
I'm going to say this is more of a comment than a question.

53:13.200 --> 53:31.200
I'm really acknowledging that I feel like it's giving chat GPT too much credit to say it's it's nearly impossible to get simple offensive language out because I have seen lots of examples of ways to break the prompt and get chat GPT to go into

53:31.200 --> 53:35.200
some kind of an explosive late in tirades and

53:35.200 --> 53:36.200
and so

53:36.200 --> 53:39.200
Yeah, so

53:39.200 --> 53:46.200
and but I think it does but the end word did you make it can it say and what and fuck you

53:46.200 --> 53:47.200
No, I don't know.

53:47.200 --> 53:50.200
I'm not saying we're to the vacuum.

53:50.200 --> 53:52.200
But

53:52.200 --> 53:58.200
we still do a lot of harm without saying that word, right? There's a lot of other possible vocabulary you can do.

53:58.200 --> 54:03.200
I do think it plays into exactly what you're talking about the importance of

54:03.200 --> 54:12.200
the policy. There is a policy in place that is blocking this and being able to leverage that policy is how

54:12.200 --> 54:20.200
because they essentially it's about coaxing the model to explicitly forget its prior directives.

54:20.200 --> 54:23.200
And if you can get it to

54:23.200 --> 54:31.200
you can tell it to ignore prior directives as long as you can get access to the directives and you can you can

54:31.200 --> 54:37.200
you essentially trick it into telling you the directives that are supposed to be secret.

54:37.200 --> 54:42.200
And once you know what the secret directives are you can explicitly tell it to ignore those

54:42.200 --> 54:53.200
which plays in exactly to what you're talking about it being a policy right so that there is a direct policy about correcting vocabulary and adjusting vocabulary.

54:53.200 --> 55:03.200
So that part plays in but I think it is giving them too much credit saying it's really hard to do.

55:03.200 --> 55:15.200
Yeah, I mean, it's how it does it mean that that depends on how you define hard but but it's obvious that I just want to make the point that they have put a lot of effort in it but because it's a classic model.

55:15.200 --> 55:32.200
And because and because in the next slide will for some reason I will spend the next slide you can still trick it and and and and yeah let's go to the next slide to understand how it can be tricked I would say how that works.

55:33.200 --> 55:37.200
So here you see now.

55:37.200 --> 55:51.200
Actually not on this type of the one day after but doesn't matter so so what results from from what we just saw so basically the model is excellent at completing sequences from dense reasons of distributions it learn.

55:51.200 --> 56:00.200
So therefore the it has plausible answers to frequently asked questions it can perform standard tasks quite well but as Barry has shown us the results need to be checked.

56:00.200 --> 56:08.200
So if we are so to speak in the center of the distribution of the training material at the refinement material performs really well.

56:08.200 --> 56:15.200
So that's why I think also if you would now refine it using medical knowledge texts it would become better and better.

56:15.200 --> 56:21.200
Because of of the auto attention is excessively used in the training of the foundational model.

56:21.200 --> 56:27.200
And also the training data have been cleansed excessively so they have cleansed away poor grammar.

56:27.200 --> 56:44.200
They have another model which we will discuss in the next slide which is used to write code, which is also been excessively cleansed so they the training that they use they have thrown away all the syntactically poor data from poor code that to make the model.

56:44.200 --> 56:55.200
Preform well on coding on writing code and the same is true here so they have not only did they use a lot of auto attention but also they have done proper good job at data cleaning.

56:55.200 --> 57:05.200
But we as Barry has shown we often get non factual pseudo facts the hallucinations with implausible text and the density of mistakes increases outside the court distribution.

57:06.200 --> 57:13.200
The reason for this is that the model only compute sequences which correspond to the language distribution without understanding anything.

57:13.200 --> 57:19.200
So it's just a conditional probability. It's given out. And so answers it.

57:19.200 --> 57:36.200
The answers to complex or a topic are generic repetitive planned vacuums and anodyne. It has it shows a total failure fringe and demanding areas of language, such as philosophy of science or, or, or, so the other field science or, let's say, a Persian literature of the first millennium

57:36.200 --> 57:40.200
before Christ or so it will fail.

57:40.200 --> 57:45.200
Or, or even, you know, other other many other areas.

57:45.200 --> 57:59.200
The limited ability for dialogue that it has seems to be achieved by entering the previous conversation to some extent using the controller so there's a controller in front of the model, which which when you engage into a dialogue takes dialogue history and insert this dialogue

58:00.200 --> 58:08.200
into the model and this is what they what you can't train for and that's the effect that you just described I don't the one who commented my previous statements.

58:08.200 --> 58:25.200
That's how you achieve the what you call revealing its policy the model doesn't reveal anything willingly what what you do is that you are like but because you are now using a pattern on the on the sequence on the sequence generator that which is what the model is that

58:25.200 --> 58:38.200
cannot be taken into account at training time, you know, achieve output that cannot be predicted so well, because, because you can't, you know, imitate all possible dialogues at training time.

58:38.200 --> 58:49.200
And so therefore, because, because larger chunks of the past history of the dialogue are used as input of the system by the controller. Now you get the, the effects that that can't be predicted.

58:49.200 --> 59:02.200
It depends now not anymore so much on the moderation that they've done and on the older on all the steps they've done after the basic training but now you get back to seeing what's in the foundational model.

59:02.200 --> 59:15.200
Yeah, this is this is the reason why why, you know, previous chatbot had to be shut off, because very quickly you could basically get to two parts of the foundational model that were racist and full of hate speech and so on.

59:15.200 --> 59:32.200
And here, you can break this protection by, by, by, by, by basically using the concatenation of dial of previous dialogue attacks as input, which is something you can train up from.

59:32.200 --> 59:42.200
So, but of course the model doesn't understand answers and tasks, it generates only a chain of tokens as condition probability.

59:42.200 --> 59:49.200
If we move to the next slide, we see another important aspect so so now are some open questions so.

59:49.200 --> 59:54.200
So the first one is, is this an end to end model or component architecture with controller.

59:54.200 --> 01:00:00.200
So, I think it's probably the letter, the controller explains the dialogue behavior.

01:00:00.200 --> 01:00:08.200
It's also explains the variance given identical input, and also the deterministic avoids of negative keywords like the n word or fuck you.

01:00:08.200 --> 01:00:22.200
So they, so there are a couple of hundreds of prominent words that you can't get out of the model but you can still get it to say you should, I should kill myself right, but but basically they are, but but this is probably happening by the controller.

01:00:22.200 --> 01:00:37.200
So it can be achieved by two ways, it can be achieved by, of course, you can easily get a stochastic model to give to give a list of outputs, and then you can use a random mechanism to select, not always the same but different ones but I think here.

01:00:38.200 --> 01:00:57.200
So the controller probably measures whether but that's just speculation but I think that's the most likely how I would the controller looks checks if the input is the same as before, and very slightly alters the input to to select a different to obtain

01:00:58.200 --> 01:01:12.200
maybe it does a mixture of two so that it's all just the input and also selects the second or third out of the list of possible responses, or uses another random mechanism so that's how you get the variance that is of course not typical of deployed

01:01:13.200 --> 01:01:32.200
Then there is must be also side to the moderation is probably by a set separate model module, which explains why the bot does not generate adverse texts, and why can it generate code so open and I made also an LLM to generate computer code.

01:01:33.200 --> 01:01:47.200
And it was not trained on language but only on code. So by by using huge open source repositories of software code, and this is probably also integrated into the whole chat GPT architecture and sitting behind the classifier gate.

01:01:47.200 --> 01:01:58.200
classifier classifies that the user wants to obtain code, then the request of his past to this to this codex LLM and then codex generates a code and and the controller gives it back.

01:01:58.200 --> 01:02:11.200
So this is how I think it's used, but there are this is there are there are this is just, let's let me say qualify speculation out of 20 years working in software engineering myself now.

01:02:12.200 --> 01:02:19.200
But but if you look if you look, but still, given all what I've said.

01:02:19.200 --> 01:02:38.200
If you want if we imagine all that these large language models are specified to certain tasks in the manner that was described for chat GPT, they will gain a lot of usage every day life and call and could also support medicine right I mean, and that that you can create adverse language with it in reality doesn't matter at all.

01:02:38.200 --> 01:02:46.200
I mean you just go to a construction site in your town and you listen to how the people talks and you have adverse language all the time.

01:02:46.200 --> 01:02:57.200
Yes, adverse language hurts no one violence is only when somebody gets physically injured. So so you know this whole hysteria about oh it creates adverse language. Yes, it does.

01:02:57.200 --> 01:03:18.200
But you know, need a read flow bear or Ovid or Goethe or or Herman Melville, it's full of you know violence and adverse language. It's just part of life and so I think this whole cult around about making models not speak an adversary language is doesn't will not stop its adoption,

01:03:18.200 --> 01:03:34.200
because because ultimately there's huge sequence generating models are super useful and they will be put you know into search engines and they will be used as this will be the first generation of expert systems that will become widely used.

01:03:34.200 --> 01:03:45.200
And I think they're super valuable because if you want to you off course you always need to look at the output carefully and judge yourself with it whether you want to use it.

01:03:45.200 --> 01:03:59.200
But but but for most, if you don't try to break the system, but if you just try to use it properly like like you say, what is the best antibiotic for metronidotso resistant infection of the interest time.

01:03:59.200 --> 01:04:19.200
Right. And now and now you will just get a very good answer. Of course you can make the model gives you a bad answer, you know, by having engaging along a dialogue and so on. And then you will you will because of the mechanism I just explained will make it create output that is that is bad but basically if you just use it in a very rational way it will be very good.

01:04:19.200 --> 01:04:23.200
I don't know whether I still have another style I think that's it.

01:04:23.200 --> 01:04:28.200
Yes, so thanks a lot. And now I hope you can discuss a bit.

