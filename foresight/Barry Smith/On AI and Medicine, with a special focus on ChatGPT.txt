All right, so old-fashioned chatpots are obviously lossy.
They repeat themselves over and over again, and they're bland.
Google Translate is not so obviously lossy until you test it with really difficult text,
and then it fails.
And there are easy ways of generating failure for Google Translate.
So what's going on now is that we, by adding more and more parameters to the AI systems
that we're building in creating language models, we're getting closer and closer to having
created in the machine a more and more adequate simulation of the way language works outside
the machine.
But it's still lossy, as we will see.
And so we don't need to deal with this.
NGP is a lossy paraphrase generator for text.
That's the main thesis, and yours will give a more elaborated thesis in his presentation.
It's bland, generally speaking.
It's great for creating summaries, modulo, the problems which we will identify.
It's bland, it gives you often too much output.
It can't really give one word answers.
It always wants to tell you what it knows.
But the most important problem is that it's full of errors.
So I asked this question a couple of days ago.
So who is William Hogan?
I originally asked.
It didn't know any William Hogan.
Eventually we got it to recognize the William Hogan, but it got the initial wrong.
He thinks you're called William W. Hogan.
He's not.
So if we go through, so it thinks other false things, all of these are false things.
So this is what's left.
That the career before Florida is missing because it got it all wrong and thought it
occurred in Alabama, in fact, it occurred in Pennsylvania, and this is typical.
And I'll give you another example in a minute.
So using chat GPT is worthwhile for creating summaries of a certain sort, but you have
to check for errors.
But because there are so many errors, quite serious errors when it comes to research science,
it's not useful for research at this stage.
And the one question is whether it can ever be useful for research.
So this is another example.
I was doing research on biomedical ontology, so I asked him what are the principal publications
in this field, and it invented a new journal.
And I asked him what were the 10 most important papers, and it invented seven papers.
Each of them were vaguely resembling existing papers.
They had overlapping authors, overlapping titles.
And they were all in this non-existent journal.
And this is partly a product of the fact that chat GPT wants to be helpful, so it's giving
you all this information, which it thinks you will be happy about.
All right, now one more example, and then I'll turn this over to Yobes.
So a long, long time ago, I was working on the field of consumer health.
In other words, trying to find adequate means of dealing scientifically with the way patients
describe their health conditions.
And so I was working with some linguists on creating something called medical wordnet.
It just didn't really go anywhere, it was an interesting experiment.
The paper is still cited because the idea, I think, is a good one.
Now the idea was that we would create a kind of standard patient vocabulary, which would
make, I don't want to exaggerate here, but would make all the mistakes that patients
make in their understanding of their problem and what the doctor can do.
Now the mistake is wrong.
Now all the simplifications which are characteristic of non-experts.
So it was a non-expert controlled vocabulary for health, that was the idea.
Now since we have chat GPT, and since chat GPT is so desirous of being friendly, I figured
it could build a consumer health vocabulary.
And now I wondered how I could set up a situation in which it would do that.
And I came across something called family feud.
Now I guess you all know what family feud is.
I didn't know what it was because I come from England.
But we don't have anything like, or we didn't then have anything like this.
So the idea of family feud is that you ask questions and you give answers which are not
necessarily true, but which are typical of what ordinary people would answer.
And so this is the question.
Tell me something sharks are known to eat.
You have to say either fish or people, and then there are some small scale low probability
options which people can get.
These are the two that one is searching for.
So I tried it with chat GPT, and he gave me this list, so really friendly lots of choices.
But there are no people here you'll notice.
So now my challenge is to try and get it to say people.
And so this was my second attempt, and now it's got other marine animals such as whales
and dolphins, so it's not really getting closer.
It apologizes.
When you tell it, it's not getting closer, it apologizes.
So it says the second most popular answer after fish is likely to be seals and sea lions.
So it's not getting the goal of this at all.
And then it apologizes again when I say it's still wrong.
Now it wants sea turtles.
So that was still wrong.
Then it tried seals, then it tried seals again.
Then you're asking what they would say on family feud.
I did actually.
It didn't give anything coherent.
You had to explain what family feud was.
I think that the training material for chat GPT was more the boring Wikipedia type of
text rather than the exciting, entertaining daily human conversation.
So it seals three times because that's one word, and I was trying to get it to focus
on one word.
But then it said surface and swimmer.
It's really good because it's moving towards people.
And so it got better and better, so beach goers, and then ocean users, and then marine
recreationists.
It is a word actually.
You get that phrase just a couple of times on Google.
I'm not sure you ever get marine oriented individuals, but you do get marine oriented.
So it doesn't make up words very occasionally, I discovered, but it didn't actually make up
any words here.
But then it gets sailor.
This is really good.
So I tell it, you were closest with sailors, start from there, and this is the answer again.
All right, this may be an example of creativity, so that's the end of that.
So I think we all know this is how chat GPT works.
You give it a sequence and it tries to extend it wordlet by wordlet, and it always picks
the highest probability word except when it doesn't.
So there's a question, how can marine oriented ever be a high probability word?
Well, the answer is because you've already given all the higher probability words.
But if you tell it to give the highest probability word, it will very quickly descend into repetition.
It will just repeat itself over and over again.
And so you have to have rules which prevent that.
And that's the end.
Now I'm hoping that we can share the screen so that we can see yours.
Shouldn't I share my screen now?
I'm going to see if I can make this.
Well, Jihad will come and let you share your screen.
Yeah, you can.
If you're on TV, make sure you're screened.
Yeah, I clicked it.
Okay, this is good.
Wait a minute.
One second.
Sorry.
We need, oh no, gosh, I can't, I can't share my screen on, I've never found out how to
do it with teams.
So very.
Teams, can you try going incognito and loading?
No, no, I've tried too many times.
It's a Mac computer.
I have.
Okay.
Someone else needs to share.
I'm sorry.
I didn't know we were using teams.
Someone else needs to present my presentation.
Oh, what do I have your slides?
Yeah, but I don't know whether you have the latest, well, you shouldn't have to, so yeah,
share them.
Okay, let's try.
So I will.
I will.
It's a wrong computer.
You could start by describing what you think about what I just said.
Or you made some mistakes, but we will clarify them as I go through the presentation.
Yeah, if you go through the mistakes, that would be useful.
I don't remember.
I think, oh, let me see what were they?
Oh, so about AlphaFold, you said that it is actually modeling a logic system.
It's not.
AlphaFold is modeling a complex system as a logic system.
So that's very important.
So what AlphaFold models is a complex system behavior, the folding of proteins, but using
a logic system, and it works because the logic system can approximate, I'm getting echo.
To some extent, what's going on in the complex systems?
Okay, I have now your slides.
Are you still getting an echo?
Let me try.
Now it's gone.
Thank you.
And then I think when you described how chat GPT works, it was not really, really fully
accurate, but I will explain once my slides are up.
I'm sorry for the inconvenience.
But when I do what Teams tells me to change on my computer, it never helps.
I never get the result that I should.
So I think it's under here.
Oh, we're waiting.
I was surprised to hear that the reference to Family Feud was not something that was in
the training data set for chat GPT because the training is going to crawl.
So it's like Peter Norvig, the father of AI, is one of the maintainers of this.
And they've crawled the entire internet.
You're right.
It's wrong that Family Feud is not in it.
The reason why certain results are not given is there are several reasons we will learn
as soon as I can present my slides why certain elements of the training material.
Thank you.
No, that's the wrong one there.
That's the one with your corrections.
OK, have your slides now.
I will move them forward when you say next.
But you were presenting the ones with annotations and corrections.
You were presenting a version where you have deleted entity.
So I need to email them to you again the latest ones.
I'm sorry for all this delay.
I really wasn't prepared for this.
OK, so what do you want to do?
He's sending a new set.
I'm so sorry about the delay.
But we are still in time according to the schedule I received from you.
So you can see our screen, right?
Yeah, I can see everything, but just the pain.
So will you get your email on this user?
First of all, I need to find G-Hards.
Go ahead.
I send it to you G-Hards and you, Bill, and also to Barry.
So that's the second.
And let me just say one thing about Family Feud.
So one thing I noticed is that a very common scenario is that it will say,
either it will say error or it will say,
I don't know what you are referring to.
Can you give me more information?
This happened with William Hogan and with Bill Hogan.
The first couple of times I couldn't find him and I had to then add more information.
I can't remember exactly what happened when I sent it.
So I send it to you G-Hards, Barry and Bill and so on.
Yeah, because I wonder if you, you know, I probably didn't wish.
It's in the trading data set.
It just needs more.
Maybe it's contextual prompting.
Barry, can you unshare this version with the correction please?
I still didn't receive it, but it sometimes takes a couple of minutes.
By the way, I just start my presentation without showing any slides.
It doesn't matter.
So, so the presentation is called large language models are just very complicated analytical engines.
And on the first slide, I'm showing a model of Charles Babbage analytical engine.
So the analytical engine of Charles Babbage was the theoretical attempt to build an analogous computer.
Can you hear me?
Yes.
An analogous computer that could perform reckoning operations like multiplication and so on.
And it also has a theoretical programming language which Babbage designed, but it was never built.
And I think the Royal Library of Engineering also said that it shouldn't be built because it would be useless.
So then the first time it was built was only 80 years ago with Charles Babbage.
Of course, it was very bad.
Did you receive the email at home?
Yeah.
Can you get it?
It would be really nice to get rid of the echo.
So this is the end.
I still have the echo.
Okay, hold on one second.
Try to mute us to see if that helps.
So I try again.
I still have echo.
I'm on the roof in the meeting.
Did you get the email now?
Yes, we have the email.
We're just copying it to a stick.
Oh, the computer is not connected to the internet.
Some of them are and some of them aren't.
Anyhow, so the analytical engine of Babbage was never built.
The first computer to be built was Conrad Susie Z3.
It was also an analog computer, mechanical computer.
And so basically LLMs are just very complicated.
And we will, what does it mean?
So there are currently there's a huge hype around large language models.
Check GPT is one of them.
Then Google has one, which is called Bard.
And there's another one by META, formerly known as Facebook,
which is called Galaxica.
And so the one by the first one, OpenAI,
which is also, by the way, used by Microsoft
and now being built into Bing, Microsoft's hopeless competitor
for Google Search, is perceived as a great success
and an AI breakthrough despite the massive hallucinations
that Barry has just described.
And there are huge expectations from the markets.
So currently, check GPT is valued at $30 billion.
That's $30,000 million.
And Microsoft is investing billions.
And as I said, integrating GPT into Bing.
Google Bard was also presented a couple of weeks ago
in a public investor presentation.
And it made a very minor mistake about the James Webb Space Telescope.
And this very minor mistake basically led to a hysterical reaction of the markets.
So Google lost $100 million of market capitalization.
Alphabet lost $100 million market capitalization when this error was announced.
And I'm still not on.
And Bard will nevertheless be integrated in Google Search sooner or later.
And then there's Meta, which was even launched before Check GPT
and which was perceived by the public as a total failure due to the hallucinations.
But the hallucinations were of the same degree of severity as in Check GPT.
Why then was it perceived so differently?
Because Meta claimed to be able to summarize scientific papers.
And in the science domain, the invention of the model seemed much worse than in other areas.
So the LLMs all have a similar performance,
but the public perception validation of them is a matter of spin and contextualization.
So this is, I think, very important.
Now on the next slide, please.
We see somehow the format got destroyed.
Yet another, I should have sent you a PDF anyhow.
Yeah, you don't seem to have the font I'm using.
Well, I'm sorry.
So now the hype around large language models has also reached medicine.
So on the left hand, there's a new paper which just appeared a few weeks ago,
by Kung et al.
And it tried to test Check GPT on the USMLE, which many of you must have passed.
And as you may remember, I didn't.
I have the German one, but it has three levels, right?
This is undergraduate.
This seems to be before the clinical period and this after the clinical period.
And so you see that, I mean, the paper has some weaknesses.
It's not perfect, but you see that Check GPT actually solved up to 60% of USMLE questions.
And it's interesting that the performance was best for part three.
And that's because part three has content of which you see a lot on the web, right?
So in the internet, you have a lot of content that relates to clinical problems,
but the preclinical stuff is, of course, not so much published on the web.
And therefore, the performance here is the best.
Now, of course, the algorithm would still have failed USMLE, but it's quite impressive.
And the paper concludes that Check GPT performed at or near the passing threshold of 60% accuracy.
Being the first to achieve this benchmark, this marks a local milestone in AI maturation.
Impressively, Check GPT was able to achieve this result without specialized input from human trainers.
Now then, they also claim that Check GPT displayed comprehensible reasoning and valid clinical insights.
This is total anthropomorphic nonsense, as you will see.
And of course, an unrefined Check GPT fails the exams.
So I think the glass is rather half empty than half full, but the authors conclude that Check GPT may potentially assist human learners in a medical education setting.
Now, I believe that the model can be refined by training it on specific material, and then it can probably achieve 80% in all three parts of the exam.
And at that stage, it could be used for as an expert system for decision support, but certainly not for automation, of course, because it doesn't think.
So this is just to give you an impression how this is going to might affect medicine as well.
Though one has to put in a word of caution here, you all know that so far expert systems have been around since 50 years.
Some of them outperform have outperformed humans since at least 40 years and still they're not seen in clinical practice very much because physicians resist their introduction.
Nevertheless, it's an impressive result.
So if you please could move to the next slide.
Um, so let's let's take a look what large language models really are. So basically the sequential, stochastic models. Now what's that?
So on a on a coaster. So basically I applied mathematics for the identification or mapping of recurrent patterns into machines.
It's not a model of human mind or even animal intelligence. So I I doesn't think or intend. And there are two types of AI.
There's deterministic AI.
Which contains rules, search recipes, logics and trees, which is explainable and very reliable. So this is what's built in to most of the, let's say, the last steps of the behavior of all the war and military equipment.
So there's also some deterministic AI. There are there's also some suggestive AI built in, especially in the census, but the decisions are made by deterministic. So Cruz missile hits its target based on deterministic AI.
And so classic AI is regression classification pattern recognition. It's not explainable. And it's probabilistic.
So examples are AlphaGo and chat GPT.
And of course hybrid models. For example, chat GPT is actually itself a hybrid model. It's not the purely purely stochastic model.
Okay, as we will see a bit later. So if we move on, please, and thanks for moving on the slides.
So what what we see is AI is basically statistical learning for automation pattern identification. Just to remind you, we have here.
Like this, we have an input set and an output set. These are the independent variables. These are dependent variables. Like for example, the independent variable is an email text.
And now you have as output the decision of if it's spam or not. Now what we do is we train an AI relation and and this relation is computed using an optimal optimization function.
Yeah, sorry. Here's the format is broken. There was an equation inserted here. So this was this was just this this tuple of that used to train. So these are the training tuple for the training.
And the relationship between input and output is modeled using a loss function, which is minimized by numerical optimization procedures. So that's all that's happening in supervised learning.
On the next slide.
We see that now they have moved on since 2013.
We have moved on to unsupervised learning. So but interestingly, the big breakthrough for the third AI wave was Google, the Google algorithm that recognized that could create an abstract representation of a face of a cat for millions of photons.
So how does this is did this work? Millions of photos were given as input to the to this convolutional neural network. And it had the task to actually recreate the input that it had received as output.
But while the image data was going through the neural network, and with many image data given through it, the model achieved a parameterization in the middle layer that rendered an abstract cat face.
And so this is called a foundational model. So that's a model that is trained without outcome. So it's just trained by by by asking your network to to basically reconstitute the input it receives as output.
So it's also called encoder decoder architecture. So the input data is encoded in a certain way, and then it's decoded and then it's output again. And when you do this, you basically parameterize a multivariate distribution of the input data.
And then you obtain a foundational model. And this foundational model can then also, and that's happening in unsupervised manner. So you to do this, you don't need any training data.
Sorry, any any outcomes, like we showed in the previous show in the previous site, you just need the raw data. This is very practical because it saves you the whole huge annotation effort, you can just train with data.
And then you obtain this multivariate distribution model of of the data sequences. You can do this for images and also for texts. And then you can use supervised adaptation to get a domain model.
And so, so GPT three Bert and clip are three important examples for such for such models.
Google Translate is I think also now moved over to foundational model, but earlier versions were still done in a supervised way.
Okay. And, and are there any questions here, can you still hear me all right.
No questions.
Okay.
Yes, I was muted to minimize echo for you. Can you
Yes. Yeah, and any questions. I have one question. So, when you talked about unsupervised learning, didn't there have to be someone at the end of the chain, who was recognizing whether they had identified a cat correctly or not.
So, because what you do when you unsupervised learning, you, you basically have an algorithm that takes care of checking whether the input is equal to the output.
So the model, what you do is, if you have a picture of a cat, you encode it in a three dimensional matrix, indicating the pixel, the pixel shades of gray or color.
Yeah, let's say you use a black and white picture, then you just indicate for each pixel, the shade of gray it has, right. And then, and then you just measure from the output whether it looks like the input.
And so, so you can fully automate. So of course it's unsupervised doesn't mean that the human being is not involved somebody has to write all this algorithm and test it and make sure that it works but then unsupervised means that the outcome.
The outcome to the data, right. And, and so this is how foundation models get trained and it's super impressive because without any human input into into which which we have used for 50 years and in statistical learning or even longer.
So it goes back to Bosco which which was 1760 right. So Bosco which invented statistical learning in 1760 and for two for 250 years we have always used, you know, input output tuples.
And now you have a tuple free learning. This is, this is a huge progress. Of course they are you setting up the algorithm but it's still very impressive.
I have one question. And I know that you're referring specifically to foundational language models when you refer to deep learning models being a form of stochastic models but I just wanted to make it clear that, you know, most deep learning models are actually
deterministic the language models we're talking about like auto aggressive language models draw from a probability distribution, you know, for the output and that's what makes them more stochastic is because they're not just picking the most probable
ones. Okay, whereas most deep learning models like computer vision models you serve the same input to the model 10 times you're going to get the same response so they are.
Okay, so, so they are all deployed. Can you mute this again because I'm getting terrible echo.
Sorry, Jihad. So, thank you. So all deployed stochastic models are always deterministic. So what whenever you train a primitive linear regression or multi variate regression, or some kernel regression or classifier, the attribute
typically refers to the way you train it. Once it is trained, it's always deterministic so there is every stochastic model that gets deployed works in deterministic fashion, it will always produce the same output based on identical
input. It's just that the term deterministic versus stochastic only describes the way principles, which, which I use to create the model, but once the model is started has deterministic behavior.
Right, so the problem that you have with why stochastic models are unreliable is that you, that, that you cannot easily predict which behavior it will have based on which input, but given an exactly identical input, the output will always be the same.
So, so the behavior of the model is always deterministic once they get deployed, no matter how they are created.
Okay.
Can you then explain how it is that when I play with just chat GPT, I can ask the same question eight times and get different answers.
That's because, because there's a controller sitting in front of the stochastic model. And we will get back to this later. But this is basically the proof. This behavior of chat GPT proves that it has a controller based architecture with with if it was a pure
end to end neural network in deployed mode, it would be have deterministic behavior.
I mean, what one way that auto aggressive language models have done that is to draw from a probability distribution is, you know, predicting the next word, you know, you don't just predict the most probable next word you draw from a probability
distribution and you happen to get, you know, a word that's not the most probable.
I'm back to this later. Okay, so can you present again jihad.
So, next slide please.
So, let's look at how the large language monitor chatbots are trained so the first step is basic training without without outcome, foundational model as we've just seen it then there's specification of the model to task.
There's supervised learning, and then there's reward learning and then there's an automated reinforcement learning approach on question answer pairs. It's very impressive.
And so, so the first step is in the current version of jet tpt or the one, let's say about which I read publications in January maybe they're now deployed another one was GPT three dot five was used as a foundational model and the dark blue parts are the refinement
steps. If we go to the next slide.
We see.
I have to follow this. Yeah, I think that everything can come across. I'm sorry for the bad format thing it's it's I should have said the PDF.
The first step the foundation language model is trained using a transformers auto attention so what was Vani in 2017 published, I think, in the last since Schmidtuber published his work about LSTM in the mid 70s this was the next most
important paper about neural networks because because the LSTMs and the and the gated recurrent units that had been invented by Schmidtuber and his accoludes.
They were not computationally very effective.
And so was Vani at all from Google they showed in 2017 that that you can have a purely feed forward loop model that has only feed forward computations and no recursion in it but that can still achieve the same computational properties as the recursion you have in the LSTMs.
And so what you see on top is basically just saying that a sequence of symbols is modeled.
As a as a condition probability, right, and so that you basically say each of the symbol of the of the symbols or tokens out of the sequence is is a is a is multiplied with the with the next one or the one before, given the others.
So this is just really a Gaussian or Bayesian Bayesian, sorry, naive based distributional approach to to the modeling of the sequence and the architecture of the model is relatively complicated.
But in the end, the most important mechanism is attention.
And because it and we will look at attention on the next slide but because attention only use information about other tokens from lower layers.
It can be computed for all talks and parallel which needs to improve training performance and so the resulting operators so this gives you mathematically speaking a huge operator, which which which is a relation that relates a vector to another vector.
You know function is a relation that relates a vector to a scalar, but this relates a sequence to another sequence so it's, it's not it's not it's mathematically speaking operator and this operator maps a sequence to itself but contains a parameterization which models the distribution
of the sequence is found in the training material. This is super impressive because because it basically it can create, it can create, it can create,
recreate a huge distribution of language and and you see this that that how good it is you see this from the fact that it has almost no syntactic errors when it creates output.
So if you look at the next slide.
Please. Thank you. So here you see how attention works. So this is a mechanism used in sequential neural networks to provide a context right weight vector that gives emphasis to predict relevant aspects of an input sequence so it, it, it asks.
Here this sent there's an example here the animal didn't cross the street because it was too tired, and you see that for example the word it receives attentional enhancement and emphasis so that you can basically see that the in this subordinate
sentence in the causes subordinate sentence. This refers to the noun phrase of the main sentence. And this is achieved by nobody thought this through but basically they tried it out right and they tried that it's done with this equation here which which
defines the input sequence and then three different matrices of parameters and if you multiply, if you multiply this with with these matrices, which are at the beginning set at a certain initial value, then you can you can compute an output sequence shown
and and this is just a heuristic computation recipe, but but it yields a relatively accurate syntax generation. If it is applied with sufficient bread spreads means that they use many such attention mechanisms on one sequence and depth that they stack a lot of attention on top of each other
and they just tried it out and by doing this they found out that this creates an outer encoding that is very syntactically very reliable. This is a super impressive achievement, but it's from the quality of the result is comparable to what you get with an LSTM, but it's
compute much faster. And so this is this is a very very important result.
So and enter the final the final output is computed using a softmax function which is shown here.
So, if we move to the next slide.
So this is the foundation what has been trained in this way. Now comes the moderate comes the specification of the foundation model to the task at hand. So what they have they have a huge database of of important questions that are often asked in the internet.
So this is a recipe for tomato soup. Can you please
where is Paris in France or what is Paris
when was Jesus Christ born and so on
questions that are often asked and so so
a prompt a sample from a prompt data set and then so basically then the the sampler they're a label or annotate as I call demonstrates the desired output behavior and and and so in this way they have actually written hundreds of thousands of answers to frequently asked
questions. So not only have they written answers to questions but they've also solved tasks like write me a poem tell me a joke.
Write a letter and so on and so on and hundreds of thousands so this was super expensive right so this is why open AI is a company that didn't do much mathematical innovation so they just took that the transformer model from Google but then they put a lot huge effort on this
annotation and this now they have two pills like the ones from I described the beginning for supervised learning and now these two pills are given to the model and now the model is not used anymore to do auto encoding.
But now it's basically like in transfer learning used to answer the questions so it's generate generates.
It generates now a condition probability in the in the way that that that that that that this is done for for.
For stem filters and by doing this the pyramid the millions 100 to 200 or 300 millions of parameters of the model gets fine tuned because they now are changed to create the desired output which is a very traditional form of machine learning.
That's basically the Bosco which method again just that that the loss function you was using 250 years ago was a bit simpler.
It was some of these squares and today the loss function is more complicated but basically it's the same trick of minimizing the difference between the desired output and the output of the model.
And then that was the first step now you have a better model now then comes a step where you again collect data.
So and and now but you know you don't you collect questions but now you don't write the answer anymore but you let the model write the answers but now an annotator ranks the answers by quality.
And by doing this that a reward model is generated that tells the model if you create this type of answer then you get this and this reward there again tens of thousands of annotations are used.
And now there's a really very very important step, which was also used to train Alpha Alpha go, which is that you use the reward model to now let the model train itself.
Now you can give a task or ask a question that creates an outcome, and then you can give it a reward and do this again and again and again to make the model better and better at the end.
The model will mostly give out answers that corresponds to the highly ranked answers.
I like the high ranked answers that annotator ranked. And so this is this is a very important step. This method used to this method PPO which is called proximal policy optimization algorithm is a super interesting innovation.
And actually this is I think the biggest contribution to intellectual property or to neural network science that Open AI has done itself and that was already in 2017.
So at the beginning when Open AI were just a few people they focused on creating policy optimization algorithms for reinforcement learning and this was without this, you wouldn't get the results we're getting.
So this this step is mathematically very elegant. And the decisive step that that makes a difference because here, you can't get enough material. So you create kind of an artificial reward.
Now, now the advantage is that this allows you to really reparameterize the model to give satisfying answers, but also explains the weaknesses of the model, because it forces the model to always give an answer.
And it also creates because this is very schematic right these rewards are very schematic and of course not very differentiated and that that's why the model is so bland and also repetitive because it is because how satisfying an answer is cannot be packaged into reward model.
But here it is done, because otherwise you cannot train the PPO algorithm but the price is that you're getting kind of that you can recognize what the model creates because it's, it's very stereotypical.
Okay, I hope this was understandable now let's move to the next slide.
So maybe there are questions for before jihad.
Any questions now. Yes.
So, in your picture you have two places where, yeah, the female human avatar is represented does that mean gender.
I think it's non gender avatar.
Go on there.
Does that mean that there are humans doing the labeling.
Of course, I said an annotator. So, so the, the, the, the person showing there is an annotator and the annotator. These are thousands of annotators, a lot.
They work actually we will see in the next slide under quite strict policies.
And they, they, in the first step they really write answers. So this is a very expensive step, because they have to write answers to questions and you have to make sure that these answers are correct.
So you have to have.
And so the next in the second step they only give ratings to the quality of the output of the model. And from these ratings reward model is created.
This is super clever at a great achievement of open AI and that's the core mathematical contribution to machine learning is that they found a way to create based on human scores a reward system that is you see in chess or go.
You have a reward system that is very simple because the games can be formulated as points. So in go actually it's even simpler than in chess because in go in each situation on the board you can calculate exactly how many points move gives you.
And so you can, you can set up the reward function of the reinforcement learning to be proportional to the number of points that you are getting at each step or at further steps and you can have a disk.
You can have a factor that that penalizes steps that are further way and so on. But basically you have a natural way of giving points to reward system.
And here you don't have this because it's a language answers and so they found a way to to to reward answer quality and that's what the second step up so it's super impressive.
But it also explains the blend of the question answers.
This is really not supervised but semi supervised, right? Well, the first two steps. The first step is called classical supervised. The second step is also supervised. It's a supervised generation of a reward model.
And then the reward model is used for supervised learning but it's then not supervised anymore. But it's basically applying a reward model that was created using supervised learning.
Any other questions or should we move on?
Go ahead.
Okay, very good. So if you could. Yes, thank you. Go to the next slide.
And thanks for your hard work on this. I'm sorry for the technical problems I have. Okay, so now this is super interesting.
So there is a moderation classifier that was created to avoid non PC language and for some model output corresponding to the woke culture expectations of today's West.
So we know that that we have now the culture that we have now is not comparable at all. Well, many ways not comparable to what we had in the 1970s when the dirty Harry movie series with Eastwood was made.
So now, you know, this is all outdated and everything has to be politically correct. And how did they how did they do this? How did they achieve this? So this is a paper.
I'm afraid the reference is not visible because of the formatting problems. But this is a paper called holistic approach to undesired content detection in the real world and actually on the paper.
So if you look at the PDF, it says warning some content may contain racism sexuality or other harmful language so that you already wanted to trigger warning that you should not be afraid that some dirty words appear in the paper as negative examples.
So now, now what did they do so they, they use basically public domain data, and then also training data, and, and they have, and then they have a very sophisticated way by creating a classification model that that that classifies undesired language.
So you have the categories of undesired brain is sex, what they call hate violence harassment, self harm, and then these are subcategories. This is for example, anti feminist sex sexual language, hated against colored people, and so on so these are subcategories of the former.
You can see here if you use public so this is this shows you the the area under the curve for the classify which is of course as you all know, which measures how, how well it performs, and you see that that the specialty.
Self harm is a super low, a detection rate is super bad.
Sexuality is also not so good. Now they use this what they call DAT the domain adversarial training, where they were the iterate several times the training and improve here fan is advice improvements augmentation of training data.
So, so they do a lot of this was super expensive exercise.
And here there's also an adversarial team built in that creates more complicated hate speech and so on. By doing this they have already improved the baseline then they also have synthetic data that they have created themselves.
And then with the mix the two, then they get very, very good area under the curve as you can see here for the for the for the cat for the broad categories they have now, not not perfect but quite good, quite good detection why is
why are some categories not working so well, because either they are harder to detect or because they just couldn't put in even more effort, but but this training effort made created basically a PC quality classifier.
And the result is so good that this is, it is not possible to make off hard to make jet GPT speaking simple toxic language so you can, as somebody has now shown, make it say some bit toxic language.
Like I think somebody made it say that the person talking to suicide herself or himself, but but but it can't say the n word or f you, for example.
And the classifier is probably part of the controller and it's certainly reinforced with a deterministic filter. So it's impossible to make it say the n word.
And that's that that that can only be achieved that can't be achieved with the statistic model. So there must be deterministic filter building on the next slide so but this was cost also you know you have to also what I failed to say I forgot to say is that each training run
will train the chat the GPT 3.5 model costs $2 million one run or $1.5 million of CPU time.
Even if you own all the CPUs yourself and GPUs yourself will cost $1.5 million. And that. So that that that is basically the discount rate of the CPUs plus electricity and cooling costs you have.
And so though, and then the further the other trainings we just saw also cost millions. So I think that they have used several hundred million just for the training, plus hundreds of million foot to pay for the annotate.
So it's super expensive. Let's move on.
Okay.
I mean,
given that we're talking about PC language.
I'm going to say this is more of a comment than a question.
I'm really acknowledging that I feel like it's giving chat GPT too much credit to say it's it's nearly impossible to get simple offensive language out because I have seen lots of examples of ways to break the prompt and get chat GPT to go into
some kind of an explosive late in tirades and
and so
Yeah, so
and but I think it does but the end word did you make it can it say and what and fuck you
No, I don't know.
I'm not saying we're to the vacuum.
But
we still do a lot of harm without saying that word, right? There's a lot of other possible vocabulary you can do.
I do think it plays into exactly what you're talking about the importance of
the policy. There is a policy in place that is blocking this and being able to leverage that policy is how
because they essentially it's about coaxing the model to explicitly forget its prior directives.
And if you can get it to
you can tell it to ignore prior directives as long as you can get access to the directives and you can you can
you essentially trick it into telling you the directives that are supposed to be secret.
And once you know what the secret directives are you can explicitly tell it to ignore those
which plays in exactly to what you're talking about it being a policy right so that there is a direct policy about correcting vocabulary and adjusting vocabulary.
So that part plays in but I think it is giving them too much credit saying it's really hard to do.
Yeah, I mean, it's how it does it mean that that depends on how you define hard but but it's obvious that I just want to make the point that they have put a lot of effort in it but because it's a classic model.
And because and because in the next slide will for some reason I will spend the next slide you can still trick it and and and and yeah let's go to the next slide to understand how it can be tricked I would say how that works.
So here you see now.
Actually not on this type of the one day after but doesn't matter so so what results from from what we just saw so basically the model is excellent at completing sequences from dense reasons of distributions it learn.
So therefore the it has plausible answers to frequently asked questions it can perform standard tasks quite well but as Barry has shown us the results need to be checked.
So if we are so to speak in the center of the distribution of the training material at the refinement material performs really well.
So that's why I think also if you would now refine it using medical knowledge texts it would become better and better.
Because of of the auto attention is excessively used in the training of the foundational model.
And also the training data have been cleansed excessively so they have cleansed away poor grammar.
They have another model which we will discuss in the next slide which is used to write code, which is also been excessively cleansed so they the training that they use they have thrown away all the syntactically poor data from poor code that to make the model.
Preform well on coding on writing code and the same is true here so they have not only did they use a lot of auto attention but also they have done proper good job at data cleaning.
But we as Barry has shown we often get non factual pseudo facts the hallucinations with implausible text and the density of mistakes increases outside the court distribution.
The reason for this is that the model only compute sequences which correspond to the language distribution without understanding anything.
So it's just a conditional probability. It's given out. And so answers it.
The answers to complex or a topic are generic repetitive planned vacuums and anodyne. It has it shows a total failure fringe and demanding areas of language, such as philosophy of science or, or, or, so the other field science or, let's say, a Persian literature of the first millennium
before Christ or so it will fail.
Or, or even, you know, other other many other areas.
The limited ability for dialogue that it has seems to be achieved by entering the previous conversation to some extent using the controller so there's a controller in front of the model, which which when you engage into a dialogue takes dialogue history and insert this dialogue
into the model and this is what they what you can't train for and that's the effect that you just described I don't the one who commented my previous statements.
That's how you achieve the what you call revealing its policy the model doesn't reveal anything willingly what what you do is that you are like but because you are now using a pattern on the on the sequence on the sequence generator that which is what the model is that
cannot be taken into account at training time, you know, achieve output that cannot be predicted so well, because, because you can't, you know, imitate all possible dialogues at training time.
And so therefore, because, because larger chunks of the past history of the dialogue are used as input of the system by the controller. Now you get the, the effects that that can't be predicted.
It depends now not anymore so much on the moderation that they've done and on the older on all the steps they've done after the basic training but now you get back to seeing what's in the foundational model.
Yeah, this is this is the reason why why, you know, previous chatbot had to be shut off, because very quickly you could basically get to two parts of the foundational model that were racist and full of hate speech and so on.
And here, you can break this protection by, by, by, by, by basically using the concatenation of dial of previous dialogue attacks as input, which is something you can train up from.
So, but of course the model doesn't understand answers and tasks, it generates only a chain of tokens as condition probability.
If we move to the next slide, we see another important aspect so so now are some open questions so.
So the first one is, is this an end to end model or component architecture with controller.
So, I think it's probably the letter, the controller explains the dialogue behavior.
It's also explains the variance given identical input, and also the deterministic avoids of negative keywords like the n word or fuck you.
So they, so there are a couple of hundreds of prominent words that you can't get out of the model but you can still get it to say you should, I should kill myself right, but but basically they are, but but this is probably happening by the controller.
So it can be achieved by two ways, it can be achieved by, of course, you can easily get a stochastic model to give to give a list of outputs, and then you can use a random mechanism to select, not always the same but different ones but I think here.
So the controller probably measures whether but that's just speculation but I think that's the most likely how I would the controller looks checks if the input is the same as before, and very slightly alters the input to to select a different to obtain
maybe it does a mixture of two so that it's all just the input and also selects the second or third out of the list of possible responses, or uses another random mechanism so that's how you get the variance that is of course not typical of deployed
Then there is must be also side to the moderation is probably by a set separate model module, which explains why the bot does not generate adverse texts, and why can it generate code so open and I made also an LLM to generate computer code.
And it was not trained on language but only on code. So by by using huge open source repositories of software code, and this is probably also integrated into the whole chat GPT architecture and sitting behind the classifier gate.
classifier classifies that the user wants to obtain code, then the request of his past to this to this codex LLM and then codex generates a code and and the controller gives it back.
So this is how I think it's used, but there are this is there are there are this is just, let's let me say qualify speculation out of 20 years working in software engineering myself now.
But but if you look if you look, but still, given all what I've said.
If you want if we imagine all that these large language models are specified to certain tasks in the manner that was described for chat GPT, they will gain a lot of usage every day life and call and could also support medicine right I mean, and that that you can create adverse language with it in reality doesn't matter at all.
I mean you just go to a construction site in your town and you listen to how the people talks and you have adverse language all the time.
Yes, adverse language hurts no one violence is only when somebody gets physically injured. So so you know this whole hysteria about oh it creates adverse language. Yes, it does.
But you know, need a read flow bear or Ovid or Goethe or or Herman Melville, it's full of you know violence and adverse language. It's just part of life and so I think this whole cult around about making models not speak an adversary language is doesn't will not stop its adoption,
because because ultimately there's huge sequence generating models are super useful and they will be put you know into search engines and they will be used as this will be the first generation of expert systems that will become widely used.
And I think they're super valuable because if you want to you off course you always need to look at the output carefully and judge yourself with it whether you want to use it.
But but but for most, if you don't try to break the system, but if you just try to use it properly like like you say, what is the best antibiotic for metronidotso resistant infection of the interest time.
Right. And now and now you will just get a very good answer. Of course you can make the model gives you a bad answer, you know, by having engaging along a dialogue and so on. And then you will you will because of the mechanism I just explained will make it create output that is that is bad but basically if you just use it in a very rational way it will be very good.
I don't know whether I still have another style I think that's it.
Yes, so thanks a lot. And now I hope you can discuss a bit.
