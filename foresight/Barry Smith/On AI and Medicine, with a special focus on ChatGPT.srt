1
00:00:00,000 --> 00:00:06,560
All right, so old-fashioned chatpots are obviously lossy.

2
00:00:06,560 --> 00:00:10,600
They repeat themselves over and over again, and they're bland.

3
00:00:10,600 --> 00:00:15,640
Google Translate is not so obviously lossy until you test it with really difficult text,

4
00:00:15,640 --> 00:00:17,440
and then it fails.

5
00:00:17,440 --> 00:00:21,960
And there are easy ways of generating failure for Google Translate.

6
00:00:21,960 --> 00:00:29,840
So what's going on now is that we, by adding more and more parameters to the AI systems

7
00:00:29,840 --> 00:00:36,160
that we're building in creating language models, we're getting closer and closer to having

8
00:00:36,160 --> 00:00:46,520
created in the machine a more and more adequate simulation of the way language works outside

9
00:00:46,520 --> 00:00:47,520
the machine.

10
00:00:47,520 --> 00:00:51,800
But it's still lossy, as we will see.

11
00:00:51,800 --> 00:00:57,080
And so we don't need to deal with this.

12
00:00:57,080 --> 00:01:00,800
NGP is a lossy paraphrase generator for text.

13
00:01:00,800 --> 00:01:09,000
That's the main thesis, and yours will give a more elaborated thesis in his presentation.

14
00:01:09,000 --> 00:01:11,080
It's bland, generally speaking.

15
00:01:11,080 --> 00:01:17,720
It's great for creating summaries, modulo, the problems which we will identify.

16
00:01:17,720 --> 00:01:23,280
It's bland, it gives you often too much output.

17
00:01:23,280 --> 00:01:26,360
It can't really give one word answers.

18
00:01:26,360 --> 00:01:29,120
It always wants to tell you what it knows.

19
00:01:29,120 --> 00:01:33,600
But the most important problem is that it's full of errors.

20
00:01:33,600 --> 00:01:39,280
So I asked this question a couple of days ago.

21
00:01:39,280 --> 00:01:42,280
So who is William Hogan?

22
00:01:42,280 --> 00:01:43,280
I originally asked.

23
00:01:43,280 --> 00:01:47,360
It didn't know any William Hogan.

24
00:01:47,360 --> 00:01:54,080
Eventually we got it to recognize the William Hogan, but it got the initial wrong.

25
00:01:54,080 --> 00:01:58,840
He thinks you're called William W. Hogan.

26
00:01:58,840 --> 00:01:59,840
He's not.

27
00:01:59,840 --> 00:02:07,040
So if we go through, so it thinks other false things, all of these are false things.

28
00:02:07,040 --> 00:02:10,680
So this is what's left.

29
00:02:10,680 --> 00:02:14,920
That the career before Florida is missing because it got it all wrong and thought it

30
00:02:14,920 --> 00:02:22,000
occurred in Alabama, in fact, it occurred in Pennsylvania, and this is typical.

31
00:02:22,680 --> 00:02:25,640
And I'll give you another example in a minute.

32
00:02:25,640 --> 00:02:33,920
So using chat GPT is worthwhile for creating summaries of a certain sort, but you have

33
00:02:33,920 --> 00:02:35,240
to check for errors.

34
00:02:35,240 --> 00:02:41,440
But because there are so many errors, quite serious errors when it comes to research science,

35
00:02:41,440 --> 00:02:45,520
it's not useful for research at this stage.

36
00:02:45,520 --> 00:02:50,440
And the one question is whether it can ever be useful for research.

37
00:02:50,440 --> 00:02:53,680
So this is another example.

38
00:02:53,680 --> 00:02:58,800
I was doing research on biomedical ontology, so I asked him what are the principal publications

39
00:02:58,800 --> 00:03:03,960
in this field, and it invented a new journal.

40
00:03:03,960 --> 00:03:11,760
And I asked him what were the 10 most important papers, and it invented seven papers.

41
00:03:11,760 --> 00:03:15,280
Each of them were vaguely resembling existing papers.

42
00:03:15,280 --> 00:03:19,920
They had overlapping authors, overlapping titles.

43
00:03:19,920 --> 00:03:24,480
And they were all in this non-existent journal.

44
00:03:24,480 --> 00:03:32,560
And this is partly a product of the fact that chat GPT wants to be helpful, so it's giving

45
00:03:32,560 --> 00:03:37,440
you all this information, which it thinks you will be happy about.

46
00:03:37,440 --> 00:03:42,080
All right, now one more example, and then I'll turn this over to Yobes.

47
00:03:42,080 --> 00:03:47,840
So a long, long time ago, I was working on the field of consumer health.

48
00:03:47,840 --> 00:03:53,400
In other words, trying to find adequate means of dealing scientifically with the way patients

49
00:03:53,400 --> 00:03:56,280
describe their health conditions.

50
00:03:56,280 --> 00:04:01,200
And so I was working with some linguists on creating something called medical wordnet.

51
00:04:01,200 --> 00:04:04,320
It just didn't really go anywhere, it was an interesting experiment.

52
00:04:04,320 --> 00:04:09,080
The paper is still cited because the idea, I think, is a good one.

53
00:04:09,080 --> 00:04:15,640
Now the idea was that we would create a kind of standard patient vocabulary, which would

54
00:04:15,640 --> 00:04:22,880
make, I don't want to exaggerate here, but would make all the mistakes that patients

55
00:04:22,880 --> 00:04:29,800
make in their understanding of their problem and what the doctor can do.

56
00:04:29,800 --> 00:04:32,000
Now the mistake is wrong.

57
00:04:32,000 --> 00:04:36,640
Now all the simplifications which are characteristic of non-experts.

58
00:04:36,640 --> 00:04:43,080
So it was a non-expert controlled vocabulary for health, that was the idea.

59
00:04:43,080 --> 00:04:51,960
Now since we have chat GPT, and since chat GPT is so desirous of being friendly, I figured

60
00:04:51,960 --> 00:04:56,840
it could build a consumer health vocabulary.

61
00:04:56,840 --> 00:05:02,520
And now I wondered how I could set up a situation in which it would do that.

62
00:05:02,520 --> 00:05:06,200
And I came across something called family feud.

63
00:05:06,200 --> 00:05:09,000
Now I guess you all know what family feud is.

64
00:05:09,000 --> 00:05:11,960
I didn't know what it was because I come from England.

65
00:05:11,960 --> 00:05:15,760
But we don't have anything like, or we didn't then have anything like this.

66
00:05:15,760 --> 00:05:21,480
So the idea of family feud is that you ask questions and you give answers which are not

67
00:05:21,480 --> 00:05:30,800
necessarily true, but which are typical of what ordinary people would answer.

68
00:05:30,800 --> 00:05:32,960
And so this is the question.

69
00:05:32,960 --> 00:05:36,440
Tell me something sharks are known to eat.

70
00:05:36,440 --> 00:05:41,920
You have to say either fish or people, and then there are some small scale low probability

71
00:05:41,920 --> 00:05:44,120
options which people can get.

72
00:05:44,120 --> 00:05:47,160
These are the two that one is searching for.

73
00:05:47,160 --> 00:05:54,840
So I tried it with chat GPT, and he gave me this list, so really friendly lots of choices.

74
00:05:54,840 --> 00:05:57,520
But there are no people here you'll notice.

75
00:05:57,520 --> 00:06:02,320
So now my challenge is to try and get it to say people.

76
00:06:02,320 --> 00:06:09,720
And so this was my second attempt, and now it's got other marine animals such as whales

77
00:06:09,720 --> 00:06:16,240
and dolphins, so it's not really getting closer.

78
00:06:16,240 --> 00:06:17,240
It apologizes.

79
00:06:17,240 --> 00:06:21,280
When you tell it, it's not getting closer, it apologizes.

80
00:06:21,280 --> 00:06:27,440
So it says the second most popular answer after fish is likely to be seals and sea lions.

81
00:06:27,440 --> 00:06:30,640
So it's not getting the goal of this at all.

82
00:06:30,640 --> 00:06:34,400
And then it apologizes again when I say it's still wrong.

83
00:06:34,400 --> 00:06:38,680
Now it wants sea turtles.

84
00:06:38,680 --> 00:06:39,800
So that was still wrong.

85
00:06:39,800 --> 00:06:42,160
Then it tried seals, then it tried seals again.

86
00:06:42,160 --> 00:06:46,280
Then you're asking what they would say on family feud.

87
00:06:46,280 --> 00:06:47,280
I did actually.

88
00:06:47,280 --> 00:06:49,480
It didn't give anything coherent.

89
00:06:49,480 --> 00:06:51,680
You had to explain what family feud was.

90
00:06:51,680 --> 00:07:00,960
I think that the training material for chat GPT was more the boring Wikipedia type of

91
00:07:00,960 --> 00:07:07,000
text rather than the exciting, entertaining daily human conversation.

92
00:07:07,000 --> 00:07:11,600
So it seals three times because that's one word, and I was trying to get it to focus

93
00:07:11,600 --> 00:07:12,600
on one word.

94
00:07:12,600 --> 00:07:14,520
But then it said surface and swimmer.

95
00:07:14,520 --> 00:07:19,440
It's really good because it's moving towards people.

96
00:07:19,440 --> 00:07:30,080
And so it got better and better, so beach goers, and then ocean users, and then marine

97
00:07:30,080 --> 00:07:31,080
recreationists.

98
00:07:31,080 --> 00:07:34,080
It is a word actually.

99
00:07:34,080 --> 00:07:37,600
You get that phrase just a couple of times on Google.

100
00:07:37,600 --> 00:07:43,680
I'm not sure you ever get marine oriented individuals, but you do get marine oriented.

101
00:07:43,680 --> 00:07:50,320
So it doesn't make up words very occasionally, I discovered, but it didn't actually make up

102
00:07:50,320 --> 00:07:51,320
any words here.

103
00:07:51,320 --> 00:07:54,320
But then it gets sailor.

104
00:07:54,320 --> 00:07:56,760
This is really good.

105
00:07:56,760 --> 00:08:04,000
So I tell it, you were closest with sailors, start from there, and this is the answer again.

106
00:08:04,000 --> 00:08:13,320
All right, this may be an example of creativity, so that's the end of that.

107
00:08:13,320 --> 00:08:17,400
So I think we all know this is how chat GPT works.

108
00:08:17,400 --> 00:08:25,000
You give it a sequence and it tries to extend it wordlet by wordlet, and it always picks

109
00:08:25,000 --> 00:08:31,920
the highest probability word except when it doesn't.

110
00:08:31,920 --> 00:08:37,000
So there's a question, how can marine oriented ever be a high probability word?

111
00:08:37,000 --> 00:08:42,520
Well, the answer is because you've already given all the higher probability words.

112
00:08:42,520 --> 00:08:50,520
But if you tell it to give the highest probability word, it will very quickly descend into repetition.

113
00:08:50,520 --> 00:08:52,880
It will just repeat itself over and over again.

114
00:08:52,880 --> 00:08:55,840
And so you have to have rules which prevent that.

115
00:08:55,840 --> 00:08:58,240
And that's the end.

116
00:08:58,240 --> 00:09:13,080
Now I'm hoping that we can share the screen so that we can see yours.

117
00:09:13,080 --> 00:09:15,280
Shouldn't I share my screen now?

118
00:09:15,280 --> 00:09:19,240
I'm going to see if I can make this.

119
00:09:19,240 --> 00:09:23,040
Well, Jihad will come and let you share your screen.

120
00:09:23,040 --> 00:09:24,040
Yeah, you can.

121
00:09:24,040 --> 00:09:28,240
If you're on TV, make sure you're screened.

122
00:09:28,240 --> 00:09:31,680
Yeah, I clicked it.

123
00:09:31,680 --> 00:09:33,680
Okay, this is good.

124
00:09:33,680 --> 00:09:35,680
Wait a minute.

125
00:09:35,680 --> 00:09:37,680
One second.

126
00:09:37,680 --> 00:09:39,680
Sorry.

127
00:09:39,680 --> 00:09:50,000
We need, oh no, gosh, I can't, I can't share my screen on, I've never found out how to

128
00:09:50,000 --> 00:09:54,320
do it with teams.

129
00:09:54,320 --> 00:09:55,320
So very.

130
00:09:55,320 --> 00:10:00,520
Teams, can you try going incognito and loading?

131
00:10:00,520 --> 00:10:03,120
No, no, I've tried too many times.

132
00:10:03,120 --> 00:10:04,120
It's a Mac computer.

133
00:10:04,120 --> 00:10:05,120
I have.

134
00:10:05,120 --> 00:10:06,120
Okay.

135
00:10:06,120 --> 00:10:10,400
Someone else needs to share.

136
00:10:10,400 --> 00:10:11,400
I'm sorry.

137
00:10:11,400 --> 00:10:12,560
I didn't know we were using teams.

138
00:10:12,560 --> 00:10:15,600
Someone else needs to present my presentation.

139
00:10:15,600 --> 00:10:18,720
Oh, what do I have your slides?

140
00:10:19,440 --> 00:10:24,000
Yeah, but I don't know whether you have the latest, well, you shouldn't have to, so yeah,

141
00:10:24,000 --> 00:10:25,000
share them.

142
00:10:25,000 --> 00:10:26,000
Okay, let's try.

143
00:10:26,000 --> 00:10:27,000
So I will.

144
00:10:27,000 --> 00:10:28,000
I will.

145
00:10:28,000 --> 00:10:40,920
It's a wrong computer.

146
00:10:40,920 --> 00:10:46,320
You could start by describing what you think about what I just said.

147
00:10:46,920 --> 00:10:50,840
Or you made some mistakes, but we will clarify them as I go through the presentation.

148
00:10:50,840 --> 00:10:55,480
Yeah, if you go through the mistakes, that would be useful.

149
00:10:55,480 --> 00:10:56,480
I don't remember.

150
00:10:56,480 --> 00:10:59,760
I think, oh, let me see what were they?

151
00:10:59,760 --> 00:11:08,600
Oh, so about AlphaFold, you said that it is actually modeling a logic system.

152
00:11:08,600 --> 00:11:09,600
It's not.

153
00:11:09,600 --> 00:11:14,200
AlphaFold is modeling a complex system as a logic system.

154
00:11:14,200 --> 00:11:15,200
So that's very important.

155
00:11:15,560 --> 00:11:21,760
So what AlphaFold models is a complex system behavior, the folding of proteins, but using

156
00:11:21,760 --> 00:11:28,200
a logic system, and it works because the logic system can approximate, I'm getting echo.

157
00:11:28,200 --> 00:11:33,200
To some extent, what's going on in the complex systems?

158
00:11:33,200 --> 00:11:36,200
Okay, I have now your slides.

159
00:11:37,200 --> 00:11:40,200
Are you still getting an echo?

160
00:11:40,200 --> 00:11:42,200
Let me try.

161
00:11:42,200 --> 00:11:43,200
Now it's gone.

162
00:11:43,200 --> 00:11:44,200
Thank you.

163
00:11:44,200 --> 00:11:58,200
And then I think when you described how chat GPT works, it was not really, really fully

164
00:11:58,200 --> 00:12:02,200
accurate, but I will explain once my slides are up.

165
00:12:02,200 --> 00:12:05,200
I'm sorry for the inconvenience.

166
00:12:05,200 --> 00:12:11,200
But when I do what Teams tells me to change on my computer, it never helps.

167
00:12:11,200 --> 00:12:14,200
I never get the result that I should.

168
00:12:14,200 --> 00:12:16,200
So I think it's under here.

169
00:12:20,200 --> 00:12:21,200
Oh, we're waiting.

170
00:12:21,200 --> 00:12:27,200
I was surprised to hear that the reference to Family Feud was not something that was in

171
00:12:27,200 --> 00:12:32,200
the training data set for chat GPT because the training is going to crawl.

172
00:12:32,200 --> 00:12:37,200
So it's like Peter Norvig, the father of AI, is one of the maintainers of this.

173
00:12:37,200 --> 00:12:41,200
And they've crawled the entire internet.

174
00:12:41,200 --> 00:12:42,200
You're right.

175
00:12:42,200 --> 00:12:47,200
It's wrong that Family Feud is not in it.

176
00:12:47,200 --> 00:12:53,200
The reason why certain results are not given is there are several reasons we will learn

177
00:12:53,200 --> 00:12:59,200
as soon as I can present my slides why certain elements of the training material.

178
00:12:59,200 --> 00:13:00,200
Thank you.

179
00:13:00,200 --> 00:13:01,200
No, that's the wrong one there.

180
00:13:01,200 --> 00:13:04,200
That's the one with your corrections.

181
00:13:04,200 --> 00:13:07,200
OK, have your slides now.

182
00:13:07,200 --> 00:13:10,200
I will move them forward when you say next.

183
00:13:10,200 --> 00:13:16,200
But you were presenting the ones with annotations and corrections.

184
00:13:16,200 --> 00:13:20,200
You were presenting a version where you have deleted entity.

185
00:13:20,200 --> 00:13:23,200
So I need to email them to you again the latest ones.

186
00:13:23,200 --> 00:13:25,200
I'm sorry for all this delay.

187
00:13:25,200 --> 00:13:28,200
I really wasn't prepared for this.

188
00:13:28,200 --> 00:13:34,200
OK, so what do you want to do?

189
00:13:34,200 --> 00:13:36,200
He's sending a new set.

190
00:13:36,200 --> 00:13:38,200
I'm so sorry about the delay.

191
00:13:38,200 --> 00:13:42,200
But we are still in time according to the schedule I received from you.

192
00:13:42,200 --> 00:13:44,200
So you can see our screen, right?

193
00:13:44,200 --> 00:13:48,200
Yeah, I can see everything, but just the pain.

194
00:13:48,200 --> 00:13:52,200
So will you get your email on this user?

195
00:13:52,200 --> 00:13:56,200
First of all, I need to find G-Hards.

196
00:13:56,200 --> 00:13:58,200
Go ahead.

197
00:13:58,200 --> 00:14:03,200
I send it to you G-Hards and you, Bill, and also to Barry.

198
00:14:03,200 --> 00:14:05,200
So that's the second.

199
00:14:05,200 --> 00:14:09,200
And let me just say one thing about Family Feud.

200
00:14:09,200 --> 00:14:14,200
So one thing I noticed is that a very common scenario is that it will say,

201
00:14:14,200 --> 00:14:17,200
either it will say error or it will say,

202
00:14:17,200 --> 00:14:19,200
I don't know what you are referring to.

203
00:14:19,200 --> 00:14:21,200
Can you give me more information?

204
00:14:21,200 --> 00:14:25,200
This happened with William Hogan and with Bill Hogan.

205
00:14:25,200 --> 00:14:30,200
The first couple of times I couldn't find him and I had to then add more information.

206
00:14:30,200 --> 00:14:32,200
I can't remember exactly what happened when I sent it.

207
00:14:32,200 --> 00:14:37,200
So I send it to you G-Hards, Barry and Bill and so on.

208
00:14:37,200 --> 00:14:41,200
Yeah, because I wonder if you, you know, I probably didn't wish.

209
00:14:41,200 --> 00:14:43,200
It's in the trading data set.

210
00:14:43,200 --> 00:14:44,200
It just needs more.

211
00:14:44,200 --> 00:14:47,200
Maybe it's contextual prompting.

212
00:14:47,200 --> 00:14:51,200
Barry, can you unshare this version with the correction please?

213
00:14:56,200 --> 00:15:08,200
I still didn't receive it, but it sometimes takes a couple of minutes.

214
00:15:08,200 --> 00:15:13,200
By the way, I just start my presentation without showing any slides.

215
00:15:13,200 --> 00:15:14,200
It doesn't matter.

216
00:15:14,200 --> 00:15:20,200
So, so the presentation is called large language models are just very complicated analytical engines.

217
00:15:20,200 --> 00:15:25,200
And on the first slide, I'm showing a model of Charles Babbage analytical engine.

218
00:15:25,200 --> 00:15:30,200
So the analytical engine of Charles Babbage was the theoretical attempt to build an analogous computer.

219
00:15:30,200 --> 00:15:31,200
Can you hear me?

220
00:15:31,200 --> 00:15:32,200
Yes.

221
00:15:32,200 --> 00:15:40,200
An analogous computer that could perform reckoning operations like multiplication and so on.

222
00:15:40,200 --> 00:15:47,200
And it also has a theoretical programming language which Babbage designed, but it was never built.

223
00:15:47,200 --> 00:15:55,200
And I think the Royal Library of Engineering also said that it shouldn't be built because it would be useless.

224
00:15:55,200 --> 00:16:03,200
So then the first time it was built was only 80 years ago with Charles Babbage.

225
00:16:03,200 --> 00:16:06,200
Of course, it was very bad.

226
00:16:11,200 --> 00:16:13,200
Did you receive the email at home?

227
00:16:13,200 --> 00:16:14,200
Yeah.

228
00:16:15,200 --> 00:16:16,200
Can you get it?

229
00:16:16,200 --> 00:16:18,200
It would be really nice to get rid of the echo.

230
00:16:24,200 --> 00:16:25,200
So this is the end.

231
00:16:25,200 --> 00:16:27,200
I still have the echo.

232
00:16:27,200 --> 00:16:29,200
Okay, hold on one second.

233
00:16:29,200 --> 00:16:32,200
Try to mute us to see if that helps.

234
00:16:34,200 --> 00:16:35,200
So I try again.

235
00:16:37,200 --> 00:16:38,200
I still have echo.

236
00:16:39,200 --> 00:16:42,200
I'm on the roof in the meeting.

237
00:17:00,200 --> 00:17:02,200
Did you get the email now?

238
00:17:02,200 --> 00:17:03,200
Yes, we have the email.

239
00:17:03,200 --> 00:17:07,200
We're just copying it to a stick.

240
00:17:07,200 --> 00:17:10,200
Oh, the computer is not connected to the internet.

241
00:17:10,200 --> 00:17:13,200
Some of them are and some of them aren't.

242
00:17:16,200 --> 00:17:19,200
Anyhow, so the analytical engine of Babbage was never built.

243
00:17:19,200 --> 00:17:25,200
The first computer to be built was Conrad Susie Z3.

244
00:17:25,200 --> 00:17:29,200
It was also an analog computer, mechanical computer.

245
00:17:29,200 --> 00:17:34,200
And so basically LLMs are just very complicated.

246
00:17:34,200 --> 00:17:37,200
And we will, what does it mean?

247
00:17:37,200 --> 00:17:43,200
So there are currently there's a huge hype around large language models.

248
00:17:43,200 --> 00:17:45,200
Check GPT is one of them.

249
00:17:45,200 --> 00:17:47,200
Then Google has one, which is called Bard.

250
00:17:47,200 --> 00:17:55,200
And there's another one by META, formerly known as Facebook,

251
00:17:55,200 --> 00:17:58,200
which is called Galaxica.

252
00:17:58,200 --> 00:18:03,200
And so the one by the first one, OpenAI,

253
00:18:03,200 --> 00:18:07,200
which is also, by the way, used by Microsoft

254
00:18:07,200 --> 00:18:11,200
and now being built into Bing, Microsoft's hopeless competitor

255
00:18:11,200 --> 00:18:16,200
for Google Search, is perceived as a great success

256
00:18:16,200 --> 00:18:19,200
and an AI breakthrough despite the massive hallucinations

257
00:18:19,200 --> 00:18:21,200
that Barry has just described.

258
00:18:21,200 --> 00:18:27,200
And there are huge expectations from the markets.

259
00:18:27,200 --> 00:18:33,200
So currently, check GPT is valued at $30 billion.

260
00:18:33,200 --> 00:18:36,200
That's $30,000 million.

261
00:18:36,200 --> 00:18:39,200
And Microsoft is investing billions.

262
00:18:39,200 --> 00:18:42,200
And as I said, integrating GPT into Bing.

263
00:18:42,200 --> 00:18:50,200
Google Bard was also presented a couple of weeks ago

264
00:18:50,200 --> 00:18:54,200
in a public investor presentation.

265
00:18:54,200 --> 00:19:00,200
And it made a very minor mistake about the James Webb Space Telescope.

266
00:19:00,200 --> 00:19:06,200
And this very minor mistake basically led to a hysterical reaction of the markets.

267
00:19:06,200 --> 00:19:09,200
So Google lost $100 million of market capitalization.

268
00:19:09,200 --> 00:19:14,200
Alphabet lost $100 million market capitalization when this error was announced.

269
00:19:14,200 --> 00:19:17,200
And I'm still not on.

270
00:19:17,200 --> 00:19:25,200
And Bard will nevertheless be integrated in Google Search sooner or later.

271
00:19:25,200 --> 00:19:30,200
And then there's Meta, which was even launched before Check GPT

272
00:19:30,200 --> 00:19:34,200
and which was perceived by the public as a total failure due to the hallucinations.

273
00:19:34,200 --> 00:19:39,200
But the hallucinations were of the same degree of severity as in Check GPT.

274
00:19:39,200 --> 00:19:42,200
Why then was it perceived so differently?

275
00:19:42,200 --> 00:19:46,200
Because Meta claimed to be able to summarize scientific papers.

276
00:19:46,200 --> 00:19:56,200
And in the science domain, the invention of the model seemed much worse than in other areas.

277
00:19:56,200 --> 00:20:01,200
So the LLMs all have a similar performance,

278
00:20:01,200 --> 00:20:06,200
but the public perception validation of them is a matter of spin and contextualization.

279
00:20:06,200 --> 00:20:08,200
So this is, I think, very important.

280
00:20:08,200 --> 00:20:11,200
Now on the next slide, please.

281
00:20:12,200 --> 00:20:16,200
We see somehow the format got destroyed.

282
00:20:16,200 --> 00:20:21,200
Yet another, I should have sent you a PDF anyhow.

283
00:20:21,200 --> 00:20:24,200
Yeah, you don't seem to have the font I'm using.

284
00:20:24,200 --> 00:20:27,200
Well, I'm sorry.

285
00:20:27,200 --> 00:20:32,200
So now the hype around large language models has also reached medicine.

286
00:20:32,200 --> 00:20:37,200
So on the left hand, there's a new paper which just appeared a few weeks ago,

287
00:20:38,200 --> 00:20:41,200
by Kung et al.

288
00:20:41,200 --> 00:20:48,200
And it tried to test Check GPT on the USMLE, which many of you must have passed.

289
00:20:48,200 --> 00:20:50,200
And as you may remember, I didn't.

290
00:20:50,200 --> 00:20:53,200
I have the German one, but it has three levels, right?

291
00:20:53,200 --> 00:20:54,200
This is undergraduate.

292
00:20:54,200 --> 00:21:00,200
This seems to be before the clinical period and this after the clinical period.

293
00:21:00,200 --> 00:21:05,200
And so you see that, I mean, the paper has some weaknesses.

294
00:21:05,200 --> 00:21:14,200
It's not perfect, but you see that Check GPT actually solved up to 60% of USMLE questions.

295
00:21:14,200 --> 00:21:18,200
And it's interesting that the performance was best for part three.

296
00:21:18,200 --> 00:21:26,200
And that's because part three has content of which you see a lot on the web, right?

297
00:21:26,200 --> 00:21:33,200
So in the internet, you have a lot of content that relates to clinical problems,

298
00:21:33,200 --> 00:21:37,200
but the preclinical stuff is, of course, not so much published on the web.

299
00:21:37,200 --> 00:21:41,200
And therefore, the performance here is the best.

300
00:21:41,200 --> 00:21:46,200
Now, of course, the algorithm would still have failed USMLE, but it's quite impressive.

301
00:21:46,200 --> 00:21:52,200
And the paper concludes that Check GPT performed at or near the passing threshold of 60% accuracy.

302
00:21:52,200 --> 00:21:57,200
Being the first to achieve this benchmark, this marks a local milestone in AI maturation.

303
00:21:57,200 --> 00:22:02,200
Impressively, Check GPT was able to achieve this result without specialized input from human trainers.

304
00:22:02,200 --> 00:22:07,200
Now then, they also claim that Check GPT displayed comprehensible reasoning and valid clinical insights.

305
00:22:07,200 --> 00:22:11,200
This is total anthropomorphic nonsense, as you will see.

306
00:22:11,200 --> 00:22:15,200
And of course, an unrefined Check GPT fails the exams.

307
00:22:15,200 --> 00:22:24,200
So I think the glass is rather half empty than half full, but the authors conclude that Check GPT may potentially assist human learners in a medical education setting.

308
00:22:24,200 --> 00:22:35,200
Now, I believe that the model can be refined by training it on specific material, and then it can probably achieve 80% in all three parts of the exam.

309
00:22:35,200 --> 00:22:45,200
And at that stage, it could be used for as an expert system for decision support, but certainly not for automation, of course, because it doesn't think.

310
00:22:45,200 --> 00:22:51,200
So this is just to give you an impression how this is going to might affect medicine as well.

311
00:22:51,200 --> 00:23:00,200
Though one has to put in a word of caution here, you all know that so far expert systems have been around since 50 years.

312
00:23:00,200 --> 00:23:09,200
Some of them outperform have outperformed humans since at least 40 years and still they're not seen in clinical practice very much because physicians resist their introduction.

313
00:23:09,200 --> 00:23:11,200
Nevertheless, it's an impressive result.

314
00:23:11,200 --> 00:23:14,200
So if you please could move to the next slide.

315
00:23:14,200 --> 00:23:23,200
Um, so let's let's take a look what large language models really are. So basically the sequential, stochastic models. Now what's that?

316
00:23:23,200 --> 00:23:31,200
So on a on a coaster. So basically I applied mathematics for the identification or mapping of recurrent patterns into machines.

317
00:23:31,200 --> 00:23:39,200
It's not a model of human mind or even animal intelligence. So I I doesn't think or intend. And there are two types of AI.

318
00:23:39,200 --> 00:23:42,200
There's deterministic AI.

319
00:23:42,200 --> 00:24:00,200
Which contains rules, search recipes, logics and trees, which is explainable and very reliable. So this is what's built in to most of the, let's say, the last steps of the behavior of all the war and military equipment.

320
00:24:00,200 --> 00:24:13,200
So there's also some deterministic AI. There are there's also some suggestive AI built in, especially in the census, but the decisions are made by deterministic. So Cruz missile hits its target based on deterministic AI.

321
00:24:13,200 --> 00:24:21,200
And so classic AI is regression classification pattern recognition. It's not explainable. And it's probabilistic.

322
00:24:21,200 --> 00:24:25,200
So examples are AlphaGo and chat GPT.

323
00:24:25,200 --> 00:24:34,200
And of course hybrid models. For example, chat GPT is actually itself a hybrid model. It's not the purely purely stochastic model.

324
00:24:34,200 --> 00:24:39,200
Okay, as we will see a bit later. So if we move on, please, and thanks for moving on the slides.

325
00:24:39,200 --> 00:24:47,200
So what what we see is AI is basically statistical learning for automation pattern identification. Just to remind you, we have here.

326
00:24:47,200 --> 00:24:56,200
Like this, we have an input set and an output set. These are the independent variables. These are dependent variables. Like for example, the independent variable is an email text.

327
00:24:56,200 --> 00:25:09,200
And now you have as output the decision of if it's spam or not. Now what we do is we train an AI relation and and this relation is computed using an optimal optimization function.

328
00:25:09,200 --> 00:25:20,200
Yeah, sorry. Here's the format is broken. There was an equation inserted here. So this was this was just this this tuple of that used to train. So these are the training tuple for the training.

329
00:25:20,200 --> 00:25:30,200
And the relationship between input and output is modeled using a loss function, which is minimized by numerical optimization procedures. So that's all that's happening in supervised learning.

330
00:25:30,200 --> 00:25:33,200
On the next slide.

331
00:25:33,200 --> 00:25:38,200
We see that now they have moved on since 2013.

332
00:25:38,200 --> 00:25:54,200
We have moved on to unsupervised learning. So but interestingly, the big breakthrough for the third AI wave was Google, the Google algorithm that recognized that could create an abstract representation of a face of a cat for millions of photons.

333
00:25:54,200 --> 00:26:08,200
So how does this is did this work? Millions of photos were given as input to the to this convolutional neural network. And it had the task to actually recreate the input that it had received as output.

334
00:26:08,200 --> 00:26:22,200
But while the image data was going through the neural network, and with many image data given through it, the model achieved a parameterization in the middle layer that rendered an abstract cat face.

335
00:26:22,200 --> 00:26:39,200
And so this is called a foundational model. So that's a model that is trained without outcome. So it's just trained by by by asking your network to to basically reconstitute the input it receives as output.

336
00:26:39,200 --> 00:26:54,200
So it's also called encoder decoder architecture. So the input data is encoded in a certain way, and then it's decoded and then it's output again. And when you do this, you basically parameterize a multivariate distribution of the input data.

337
00:26:54,200 --> 00:27:05,200
And then you obtain a foundational model. And this foundational model can then also, and that's happening in unsupervised manner. So you to do this, you don't need any training data.

338
00:27:05,200 --> 00:27:18,200
Sorry, any any outcomes, like we showed in the previous show in the previous site, you just need the raw data. This is very practical because it saves you the whole huge annotation effort, you can just train with data.

339
00:27:18,200 --> 00:27:30,200
And then you obtain this multivariate distribution model of of the data sequences. You can do this for images and also for texts. And then you can use supervised adaptation to get a domain model.

340
00:27:30,200 --> 00:27:42,200
And so, so GPT three Bert and clip are three important examples for such for such models.

341
00:27:42,200 --> 00:27:52,200
Google Translate is I think also now moved over to foundational model, but earlier versions were still done in a supervised way.

342
00:27:52,200 --> 00:28:02,200
Okay. And, and are there any questions here, can you still hear me all right.

343
00:28:02,200 --> 00:28:06,200
No questions.

344
00:28:06,200 --> 00:28:08,200
Okay.

345
00:28:09,200 --> 00:28:13,200
Yes, I was muted to minimize echo for you. Can you

346
00:28:13,200 --> 00:28:31,200
Yes. Yeah, and any questions. I have one question. So, when you talked about unsupervised learning, didn't there have to be someone at the end of the chain, who was recognizing whether they had identified a cat correctly or not.

347
00:28:31,200 --> 00:28:42,200
So, because what you do when you unsupervised learning, you, you basically have an algorithm that takes care of checking whether the input is equal to the output.

348
00:28:42,200 --> 00:28:55,200
So the model, what you do is, if you have a picture of a cat, you encode it in a three dimensional matrix, indicating the pixel, the pixel shades of gray or color.

349
00:28:55,200 --> 00:29:08,200
Yeah, let's say you use a black and white picture, then you just indicate for each pixel, the shade of gray it has, right. And then, and then you just measure from the output whether it looks like the input.

350
00:29:08,200 --> 00:29:22,200
And so, so you can fully automate. So of course it's unsupervised doesn't mean that the human being is not involved somebody has to write all this algorithm and test it and make sure that it works but then unsupervised means that the outcome.

351
00:29:22,200 --> 00:29:40,200
The outcome to the data, right. And, and so this is how foundation models get trained and it's super impressive because without any human input into into which which we have used for 50 years and in statistical learning or even longer.

352
00:29:40,200 --> 00:29:54,200
So it goes back to Bosco which which was 1760 right. So Bosco which invented statistical learning in 1760 and for two for 250 years we have always used, you know, input output tuples.

353
00:29:54,200 --> 00:30:05,200
And now you have a tuple free learning. This is, this is a huge progress. Of course they are you setting up the algorithm but it's still very impressive.

354
00:30:05,200 --> 00:30:20,200
I have one question. And I know that you're referring specifically to foundational language models when you refer to deep learning models being a form of stochastic models but I just wanted to make it clear that, you know, most deep learning models are actually

355
00:30:20,200 --> 00:30:32,200
deterministic the language models we're talking about like auto aggressive language models draw from a probability distribution, you know, for the output and that's what makes them more stochastic is because they're not just picking the most probable

356
00:30:32,200 --> 00:30:41,200
ones. Okay, whereas most deep learning models like computer vision models you serve the same input to the model 10 times you're going to get the same response so they are.

357
00:30:41,200 --> 00:30:49,200
Okay, so, so they are all deployed. Can you mute this again because I'm getting terrible echo.

358
00:30:49,200 --> 00:31:11,200
Sorry, Jihad. So, thank you. So all deployed stochastic models are always deterministic. So what whenever you train a primitive linear regression or multi variate regression, or some kernel regression or classifier, the attribute

359
00:31:11,200 --> 00:31:24,200
typically refers to the way you train it. Once it is trained, it's always deterministic so there is every stochastic model that gets deployed works in deterministic fashion, it will always produce the same output based on identical

360
00:31:24,200 --> 00:31:38,200
input. It's just that the term deterministic versus stochastic only describes the way principles, which, which I use to create the model, but once the model is started has deterministic behavior.

361
00:31:38,200 --> 00:31:55,200
Right, so the problem that you have with why stochastic models are unreliable is that you, that, that you cannot easily predict which behavior it will have based on which input, but given an exactly identical input, the output will always be the same.

362
00:31:55,200 --> 00:32:02,200
So, so the behavior of the model is always deterministic once they get deployed, no matter how they are created.

363
00:32:02,200 --> 00:32:03,200
Okay.

364
00:32:03,200 --> 00:32:12,200
Can you then explain how it is that when I play with just chat GPT, I can ask the same question eight times and get different answers.

365
00:32:12,200 --> 00:32:31,200
That's because, because there's a controller sitting in front of the stochastic model. And we will get back to this later. But this is basically the proof. This behavior of chat GPT proves that it has a controller based architecture with with if it was a pure

366
00:32:31,200 --> 00:32:38,200
end to end neural network in deployed mode, it would be have deterministic behavior.

367
00:32:38,200 --> 00:32:50,200
I mean, what one way that auto aggressive language models have done that is to draw from a probability distribution is, you know, predicting the next word, you know, you don't just predict the most probable next word you draw from a probability

368
00:32:50,200 --> 00:32:55,200
distribution and you happen to get, you know, a word that's not the most probable.

369
00:32:55,200 --> 00:33:02,200
I'm back to this later. Okay, so can you present again jihad.

370
00:33:02,200 --> 00:33:06,200
So, next slide please.

371
00:33:06,200 --> 00:33:24,200
So, let's look at how the large language monitor chatbots are trained so the first step is basic training without without outcome, foundational model as we've just seen it then there's specification of the model to task.

372
00:33:24,200 --> 00:33:34,200
There's supervised learning, and then there's reward learning and then there's an automated reinforcement learning approach on question answer pairs. It's very impressive.

373
00:33:34,200 --> 00:33:50,200
And so, so the first step is in the current version of jet tpt or the one, let's say about which I read publications in January maybe they're now deployed another one was GPT three dot five was used as a foundational model and the dark blue parts are the refinement

374
00:33:51,200 --> 00:33:56,200
steps. If we go to the next slide.

375
00:33:56,200 --> 00:33:59,200
We see.

376
00:33:59,200 --> 00:34:07,200
I have to follow this. Yeah, I think that everything can come across. I'm sorry for the bad format thing it's it's I should have said the PDF.

377
00:34:07,200 --> 00:34:25,200
The first step the foundation language model is trained using a transformers auto attention so what was Vani in 2017 published, I think, in the last since Schmidtuber published his work about LSTM in the mid 70s this was the next most

378
00:34:25,200 --> 00:34:36,200
important paper about neural networks because because the LSTMs and the and the gated recurrent units that had been invented by Schmidtuber and his accoludes.

379
00:34:37,200 --> 00:34:40,200
They were not computationally very effective.

380
00:34:40,200 --> 00:34:59,200
And so was Vani at all from Google they showed in 2017 that that you can have a purely feed forward loop model that has only feed forward computations and no recursion in it but that can still achieve the same computational properties as the recursion you have in the LSTMs.

381
00:34:59,200 --> 00:35:07,200
And so what you see on top is basically just saying that a sequence of symbols is modeled.

382
00:35:08,200 --> 00:35:25,200
As a as a condition probability, right, and so that you basically say each of the symbol of the of the symbols or tokens out of the sequence is is a is a is multiplied with the with the next one or the one before, given the others.

383
00:35:25,200 --> 00:35:48,200
So this is just really a Gaussian or Bayesian Bayesian, sorry, naive based distributional approach to to the modeling of the sequence and the architecture of the model is relatively complicated.

384
00:35:48,200 --> 00:35:53,200
But in the end, the most important mechanism is attention.

385
00:35:53,200 --> 00:36:00,200
And because it and we will look at attention on the next slide but because attention only use information about other tokens from lower layers.

386
00:36:00,200 --> 00:36:16,200
It can be computed for all talks and parallel which needs to improve training performance and so the resulting operators so this gives you mathematically speaking a huge operator, which which which is a relation that relates a vector to another vector.

387
00:36:16,200 --> 00:36:31,200
You know function is a relation that relates a vector to a scalar, but this relates a sequence to another sequence so it's, it's not it's not it's mathematically speaking operator and this operator maps a sequence to itself but contains a parameterization which models the distribution

388
00:36:31,200 --> 00:36:46,200
of the sequence is found in the training material. This is super impressive because because it basically it can create, it can create, it can create,

389
00:36:46,200 --> 00:36:57,200
recreate a huge distribution of language and and you see this that that how good it is you see this from the fact that it has almost no syntactic errors when it creates output.

390
00:36:57,200 --> 00:37:01,200
So if you look at the next slide.

391
00:37:01,200 --> 00:37:15,200
Please. Thank you. So here you see how attention works. So this is a mechanism used in sequential neural networks to provide a context right weight vector that gives emphasis to predict relevant aspects of an input sequence so it, it, it asks.

392
00:37:15,200 --> 00:37:36,200
Here this sent there's an example here the animal didn't cross the street because it was too tired, and you see that for example the word it receives attentional enhancement and emphasis so that you can basically see that the in this subordinate

393
00:37:36,200 --> 00:37:53,200
sentence in the causes subordinate sentence. This refers to the noun phrase of the main sentence. And this is achieved by nobody thought this through but basically they tried it out right and they tried that it's done with this equation here which which

394
00:37:53,200 --> 00:38:12,200
defines the input sequence and then three different matrices of parameters and if you multiply, if you multiply this with with these matrices, which are at the beginning set at a certain initial value, then you can you can compute an output sequence shown

395
00:38:12,200 --> 00:38:28,200
and and this is just a heuristic computation recipe, but but it yields a relatively accurate syntax generation. If it is applied with sufficient bread spreads means that they use many such attention mechanisms on one sequence and depth that they stack a lot of attention on top of each other

396
00:38:28,200 --> 00:38:45,200
and they just tried it out and by doing this they found out that this creates an outer encoding that is very syntactically very reliable. This is a super impressive achievement, but it's from the quality of the result is comparable to what you get with an LSTM, but it's

397
00:38:45,200 --> 00:38:53,200
compute much faster. And so this is this is a very very important result.

398
00:38:53,200 --> 00:39:01,200
So and enter the final the final output is computed using a softmax function which is shown here.

399
00:39:01,200 --> 00:39:07,200
So, if we move to the next slide.

400
00:39:07,200 --> 00:39:25,200
So this is the foundation what has been trained in this way. Now comes the moderate comes the specification of the foundation model to the task at hand. So what they have they have a huge database of of important questions that are often asked in the internet.

401
00:39:25,200 --> 00:39:32,200
So this is a recipe for tomato soup. Can you please

402
00:39:32,200 --> 00:39:39,200
where is Paris in France or what is Paris

403
00:39:39,200 --> 00:39:43,200
when was Jesus Christ born and so on

404
00:39:43,200 --> 00:39:47,200
questions that are often asked and so so

405
00:39:48,200 --> 00:40:06,200
a prompt a sample from a prompt data set and then so basically then the the sampler they're a label or annotate as I call demonstrates the desired output behavior and and and so in this way they have actually written hundreds of thousands of answers to frequently asked

406
00:40:06,200 --> 00:40:16,200
questions. So not only have they written answers to questions but they've also solved tasks like write me a poem tell me a joke.

407
00:40:16,200 --> 00:40:33,200
Write a letter and so on and so on and hundreds of thousands so this was super expensive right so this is why open AI is a company that didn't do much mathematical innovation so they just took that the transformer model from Google but then they put a lot huge effort on this

408
00:40:33,200 --> 00:40:45,200
annotation and this now they have two pills like the ones from I described the beginning for supervised learning and now these two pills are given to the model and now the model is not used anymore to do auto encoding.

409
00:40:45,200 --> 00:40:52,200
But now it's basically like in transfer learning used to answer the questions so it's generate generates.

410
00:40:53,200 --> 00:41:03,200
It generates now a condition probability in the in the way that that that that that that this is done for for.

411
00:41:04,200 --> 00:41:22,200
For stem filters and by doing this the pyramid the millions 100 to 200 or 300 millions of parameters of the model gets fine tuned because they now are changed to create the desired output which is a very traditional form of machine learning.

412
00:41:22,200 --> 00:41:31,200
That's basically the Bosco which method again just that that the loss function you was using 250 years ago was a bit simpler.

413
00:41:31,200 --> 00:41:44,200
It was some of these squares and today the loss function is more complicated but basically it's the same trick of minimizing the difference between the desired output and the output of the model.

414
00:41:44,200 --> 00:41:54,200
And then that was the first step now you have a better model now then comes a step where you again collect data.

415
00:41:54,200 --> 00:42:09,200
So and and now but you know you don't you collect questions but now you don't write the answer anymore but you let the model write the answers but now an annotator ranks the answers by quality.

416
00:42:09,200 --> 00:42:21,200
And by doing this that a reward model is generated that tells the model if you create this type of answer then you get this and this reward there again tens of thousands of annotations are used.

417
00:42:21,200 --> 00:42:34,200
And now there's a really very very important step, which was also used to train Alpha Alpha go, which is that you use the reward model to now let the model train itself.

418
00:42:34,200 --> 00:42:44,200
Now you can give a task or ask a question that creates an outcome, and then you can give it a reward and do this again and again and again to make the model better and better at the end.

419
00:42:44,200 --> 00:42:50,200
The model will mostly give out answers that corresponds to the highly ranked answers.

420
00:42:50,200 --> 00:43:08,200
I like the high ranked answers that annotator ranked. And so this is this is a very important step. This method used to this method PPO which is called proximal policy optimization algorithm is a super interesting innovation.

421
00:43:08,200 --> 00:43:22,200
And actually this is I think the biggest contribution to intellectual property or to neural network science that Open AI has done itself and that was already in 2017.

422
00:43:22,200 --> 00:43:35,200
So at the beginning when Open AI were just a few people they focused on creating policy optimization algorithms for reinforcement learning and this was without this, you wouldn't get the results we're getting.

423
00:43:35,200 --> 00:43:46,200
So this this step is mathematically very elegant. And the decisive step that that makes a difference because here, you can't get enough material. So you create kind of an artificial reward.

424
00:43:46,200 --> 00:43:57,200
Now, now the advantage is that this allows you to really reparameterize the model to give satisfying answers, but also explains the weaknesses of the model, because it forces the model to always give an answer.

425
00:43:57,200 --> 00:44:16,200
And it also creates because this is very schematic right these rewards are very schematic and of course not very differentiated and that that's why the model is so bland and also repetitive because it is because how satisfying an answer is cannot be packaged into reward model.

426
00:44:16,200 --> 00:44:29,200
But here it is done, because otherwise you cannot train the PPO algorithm but the price is that you're getting kind of that you can recognize what the model creates because it's, it's very stereotypical.

427
00:44:29,200 --> 00:44:37,200
Okay, I hope this was understandable now let's move to the next slide.

428
00:44:37,200 --> 00:44:49,200
So maybe there are questions for before jihad.

429
00:44:49,200 --> 00:44:52,200
Any questions now. Yes.

430
00:44:52,200 --> 00:45:06,200
So, in your picture you have two places where, yeah, the female human avatar is represented does that mean gender.

431
00:45:06,200 --> 00:45:15,200
I think it's non gender avatar.

432
00:45:15,200 --> 00:45:17,200
Go on there.

433
00:45:17,200 --> 00:45:20,200
Does that mean that there are humans doing the labeling.

434
00:45:20,200 --> 00:45:31,200
Of course, I said an annotator. So, so the, the, the, the person showing there is an annotator and the annotator. These are thousands of annotators, a lot.

435
00:45:31,200 --> 00:45:36,200
They work actually we will see in the next slide under quite strict policies.

436
00:45:36,200 --> 00:45:49,200
And they, they, in the first step they really write answers. So this is a very expensive step, because they have to write answers to questions and you have to make sure that these answers are correct.

437
00:45:49,200 --> 00:45:52,200
So you have to have.

438
00:45:52,200 --> 00:46:01,200
And so the next in the second step they only give ratings to the quality of the output of the model. And from these ratings reward model is created.

439
00:46:01,200 --> 00:46:17,200
This is super clever at a great achievement of open AI and that's the core mathematical contribution to machine learning is that they found a way to create based on human scores a reward system that is you see in chess or go.

440
00:46:17,200 --> 00:46:32,200
You have a reward system that is very simple because the games can be formulated as points. So in go actually it's even simpler than in chess because in go in each situation on the board you can calculate exactly how many points move gives you.

441
00:46:32,200 --> 00:46:43,200
And so you can, you can set up the reward function of the reinforcement learning to be proportional to the number of points that you are getting at each step or at further steps and you can have a disk.

442
00:46:43,200 --> 00:46:54,200
You can have a factor that that penalizes steps that are further way and so on. But basically you have a natural way of giving points to reward system.

443
00:46:54,200 --> 00:47:08,200
And here you don't have this because it's a language answers and so they found a way to to to reward answer quality and that's what the second step up so it's super impressive.

444
00:47:08,200 --> 00:47:15,200
But it also explains the blend of the question answers.

445
00:47:15,200 --> 00:47:30,200
This is really not supervised but semi supervised, right? Well, the first two steps. The first step is called classical supervised. The second step is also supervised. It's a supervised generation of a reward model.

446
00:47:30,200 --> 00:47:43,200
And then the reward model is used for supervised learning but it's then not supervised anymore. But it's basically applying a reward model that was created using supervised learning.

447
00:47:44,200 --> 00:47:49,200
Any other questions or should we move on?

448
00:47:49,200 --> 00:47:51,200
Go ahead.

449
00:47:51,200 --> 00:47:55,200
Okay, very good. So if you could. Yes, thank you. Go to the next slide.

450
00:47:55,200 --> 00:48:03,200
And thanks for your hard work on this. I'm sorry for the technical problems I have. Okay, so now this is super interesting.

451
00:48:03,200 --> 00:48:14,200
So there is a moderation classifier that was created to avoid non PC language and for some model output corresponding to the woke culture expectations of today's West.

452
00:48:14,200 --> 00:48:28,200
So we know that that we have now the culture that we have now is not comparable at all. Well, many ways not comparable to what we had in the 1970s when the dirty Harry movie series with Eastwood was made.

453
00:48:28,200 --> 00:48:41,200
So now, you know, this is all outdated and everything has to be politically correct. And how did they how did they do this? How did they achieve this? So this is a paper.

454
00:48:41,200 --> 00:48:53,200
I'm afraid the reference is not visible because of the formatting problems. But this is a paper called holistic approach to undesired content detection in the real world and actually on the paper.

455
00:48:53,200 --> 00:49:06,200
So if you look at the PDF, it says warning some content may contain racism sexuality or other harmful language so that you already wanted to trigger warning that you should not be afraid that some dirty words appear in the paper as negative examples.

456
00:49:06,200 --> 00:49:29,200
So now, now what did they do so they, they use basically public domain data, and then also training data, and, and they have, and then they have a very sophisticated way by creating a classification model that that that classifies undesired language.

457
00:49:29,200 --> 00:49:48,200
So you have the categories of undesired brain is sex, what they call hate violence harassment, self harm, and then these are subcategories. This is for example, anti feminist sex sexual language, hated against colored people, and so on so these are subcategories of the former.

458
00:49:48,200 --> 00:50:04,200
You can see here if you use public so this is this shows you the the area under the curve for the classify which is of course as you all know, which measures how, how well it performs, and you see that that the specialty.

459
00:50:04,200 --> 00:50:09,200
Self harm is a super low, a detection rate is super bad.

460
00:50:09,200 --> 00:50:23,200
Sexuality is also not so good. Now they use this what they call DAT the domain adversarial training, where they were the iterate several times the training and improve here fan is advice improvements augmentation of training data.

461
00:50:23,200 --> 00:50:28,200
So, so they do a lot of this was super expensive exercise.

462
00:50:28,200 --> 00:50:42,200
And here there's also an adversarial team built in that creates more complicated hate speech and so on. By doing this they have already improved the baseline then they also have synthetic data that they have created themselves.

463
00:50:42,200 --> 00:51:04,200
And then with the mix the two, then they get very, very good area under the curve as you can see here for the for the for the cat for the broad categories they have now, not not perfect but quite good, quite good detection why is

464
00:51:05,200 --> 00:51:21,200
why are some categories not working so well, because either they are harder to detect or because they just couldn't put in even more effort, but but this training effort made created basically a PC quality classifier.

465
00:51:21,200 --> 00:51:38,200
And the result is so good that this is, it is not possible to make off hard to make jet GPT speaking simple toxic language so you can, as somebody has now shown, make it say some bit toxic language.

466
00:51:38,200 --> 00:51:49,200
Like I think somebody made it say that the person talking to suicide herself or himself, but but but it can't say the n word or f you, for example.

467
00:51:49,200 --> 00:52:00,200
And the classifier is probably part of the controller and it's certainly reinforced with a deterministic filter. So it's impossible to make it say the n word.

468
00:52:00,200 --> 00:52:15,200
And that's that that that can only be achieved that can't be achieved with the statistic model. So there must be deterministic filter building on the next slide so but this was cost also you know you have to also what I failed to say I forgot to say is that each training run

469
00:52:15,200 --> 00:52:25,200
will train the chat the GPT 3.5 model costs $2 million one run or $1.5 million of CPU time.

470
00:52:25,200 --> 00:52:39,200
Even if you own all the CPUs yourself and GPUs yourself will cost $1.5 million. And that. So that that that is basically the discount rate of the CPUs plus electricity and cooling costs you have.

471
00:52:39,200 --> 00:52:50,200
And so though, and then the further the other trainings we just saw also cost millions. So I think that they have used several hundred million just for the training, plus hundreds of million foot to pay for the annotate.

472
00:52:50,200 --> 00:52:56,200
So it's super expensive. Let's move on.

473
00:52:56,200 --> 00:53:03,200
Okay.

474
00:53:03,200 --> 00:53:06,200
I mean,

475
00:53:06,200 --> 00:53:10,200
given that we're talking about PC language.

476
00:53:10,200 --> 00:53:13,200
I'm going to say this is more of a comment than a question.

477
00:53:13,200 --> 00:53:31,200
I'm really acknowledging that I feel like it's giving chat GPT too much credit to say it's it's nearly impossible to get simple offensive language out because I have seen lots of examples of ways to break the prompt and get chat GPT to go into

478
00:53:31,200 --> 00:53:35,200
some kind of an explosive late in tirades and

479
00:53:35,200 --> 00:53:36,200
and so

480
00:53:36,200 --> 00:53:39,200
Yeah, so

481
00:53:39,200 --> 00:53:46,200
and but I think it does but the end word did you make it can it say and what and fuck you

482
00:53:46,200 --> 00:53:47,200
No, I don't know.

483
00:53:47,200 --> 00:53:50,200
I'm not saying we're to the vacuum.

484
00:53:50,200 --> 00:53:52,200
But

485
00:53:52,200 --> 00:53:58,200
we still do a lot of harm without saying that word, right? There's a lot of other possible vocabulary you can do.

486
00:53:58,200 --> 00:54:03,200
I do think it plays into exactly what you're talking about the importance of

487
00:54:03,200 --> 00:54:12,200
the policy. There is a policy in place that is blocking this and being able to leverage that policy is how

488
00:54:12,200 --> 00:54:20,200
because they essentially it's about coaxing the model to explicitly forget its prior directives.

489
00:54:20,200 --> 00:54:23,200
And if you can get it to

490
00:54:23,200 --> 00:54:31,200
you can tell it to ignore prior directives as long as you can get access to the directives and you can you can

491
00:54:31,200 --> 00:54:37,200
you essentially trick it into telling you the directives that are supposed to be secret.

492
00:54:37,200 --> 00:54:42,200
And once you know what the secret directives are you can explicitly tell it to ignore those

493
00:54:42,200 --> 00:54:53,200
which plays in exactly to what you're talking about it being a policy right so that there is a direct policy about correcting vocabulary and adjusting vocabulary.

494
00:54:53,200 --> 00:55:03,200
So that part plays in but I think it is giving them too much credit saying it's really hard to do.

495
00:55:03,200 --> 00:55:15,200
Yeah, I mean, it's how it does it mean that that depends on how you define hard but but it's obvious that I just want to make the point that they have put a lot of effort in it but because it's a classic model.

496
00:55:15,200 --> 00:55:32,200
And because and because in the next slide will for some reason I will spend the next slide you can still trick it and and and and yeah let's go to the next slide to understand how it can be tricked I would say how that works.

497
00:55:33,200 --> 00:55:37,200
So here you see now.

498
00:55:37,200 --> 00:55:51,200
Actually not on this type of the one day after but doesn't matter so so what results from from what we just saw so basically the model is excellent at completing sequences from dense reasons of distributions it learn.

499
00:55:51,200 --> 00:56:00,200
So therefore the it has plausible answers to frequently asked questions it can perform standard tasks quite well but as Barry has shown us the results need to be checked.

500
00:56:00,200 --> 00:56:08,200
So if we are so to speak in the center of the distribution of the training material at the refinement material performs really well.

501
00:56:08,200 --> 00:56:15,200
So that's why I think also if you would now refine it using medical knowledge texts it would become better and better.

502
00:56:15,200 --> 00:56:21,200
Because of of the auto attention is excessively used in the training of the foundational model.

503
00:56:21,200 --> 00:56:27,200
And also the training data have been cleansed excessively so they have cleansed away poor grammar.

504
00:56:27,200 --> 00:56:44,200
They have another model which we will discuss in the next slide which is used to write code, which is also been excessively cleansed so they the training that they use they have thrown away all the syntactically poor data from poor code that to make the model.

505
00:56:44,200 --> 00:56:55,200
Preform well on coding on writing code and the same is true here so they have not only did they use a lot of auto attention but also they have done proper good job at data cleaning.

506
00:56:55,200 --> 00:57:05,200
But we as Barry has shown we often get non factual pseudo facts the hallucinations with implausible text and the density of mistakes increases outside the court distribution.

507
00:57:06,200 --> 00:57:13,200
The reason for this is that the model only compute sequences which correspond to the language distribution without understanding anything.

508
00:57:13,200 --> 00:57:19,200
So it's just a conditional probability. It's given out. And so answers it.

509
00:57:19,200 --> 00:57:36,200
The answers to complex or a topic are generic repetitive planned vacuums and anodyne. It has it shows a total failure fringe and demanding areas of language, such as philosophy of science or, or, or, so the other field science or, let's say, a Persian literature of the first millennium

510
00:57:36,200 --> 00:57:40,200
before Christ or so it will fail.

511
00:57:40,200 --> 00:57:45,200
Or, or even, you know, other other many other areas.

512
00:57:45,200 --> 00:57:59,200
The limited ability for dialogue that it has seems to be achieved by entering the previous conversation to some extent using the controller so there's a controller in front of the model, which which when you engage into a dialogue takes dialogue history and insert this dialogue

513
00:58:00,200 --> 00:58:08,200
into the model and this is what they what you can't train for and that's the effect that you just described I don't the one who commented my previous statements.

514
00:58:08,200 --> 00:58:25,200
That's how you achieve the what you call revealing its policy the model doesn't reveal anything willingly what what you do is that you are like but because you are now using a pattern on the on the sequence on the sequence generator that which is what the model is that

515
00:58:25,200 --> 00:58:38,200
cannot be taken into account at training time, you know, achieve output that cannot be predicted so well, because, because you can't, you know, imitate all possible dialogues at training time.

516
00:58:38,200 --> 00:58:49,200
And so therefore, because, because larger chunks of the past history of the dialogue are used as input of the system by the controller. Now you get the, the effects that that can't be predicted.

517
00:58:49,200 --> 00:59:02,200
It depends now not anymore so much on the moderation that they've done and on the older on all the steps they've done after the basic training but now you get back to seeing what's in the foundational model.

518
00:59:02,200 --> 00:59:15,200
Yeah, this is this is the reason why why, you know, previous chatbot had to be shut off, because very quickly you could basically get to two parts of the foundational model that were racist and full of hate speech and so on.

519
00:59:15,200 --> 00:59:32,200
And here, you can break this protection by, by, by, by, by basically using the concatenation of dial of previous dialogue attacks as input, which is something you can train up from.

520
00:59:32,200 --> 00:59:42,200
So, but of course the model doesn't understand answers and tasks, it generates only a chain of tokens as condition probability.

521
00:59:42,200 --> 00:59:49,200
If we move to the next slide, we see another important aspect so so now are some open questions so.

522
00:59:49,200 --> 00:59:54,200
So the first one is, is this an end to end model or component architecture with controller.

523
00:59:54,200 --> 01:00:00,200
So, I think it's probably the letter, the controller explains the dialogue behavior.

524
01:00:00,200 --> 01:00:08,200
It's also explains the variance given identical input, and also the deterministic avoids of negative keywords like the n word or fuck you.

525
01:00:08,200 --> 01:00:22,200
So they, so there are a couple of hundreds of prominent words that you can't get out of the model but you can still get it to say you should, I should kill myself right, but but basically they are, but but this is probably happening by the controller.

526
01:00:22,200 --> 01:00:37,200
So it can be achieved by two ways, it can be achieved by, of course, you can easily get a stochastic model to give to give a list of outputs, and then you can use a random mechanism to select, not always the same but different ones but I think here.

527
01:00:38,200 --> 01:00:57,200
So the controller probably measures whether but that's just speculation but I think that's the most likely how I would the controller looks checks if the input is the same as before, and very slightly alters the input to to select a different to obtain

528
01:00:58,200 --> 01:01:12,200
maybe it does a mixture of two so that it's all just the input and also selects the second or third out of the list of possible responses, or uses another random mechanism so that's how you get the variance that is of course not typical of deployed

529
01:01:13,200 --> 01:01:32,200
Then there is must be also side to the moderation is probably by a set separate model module, which explains why the bot does not generate adverse texts, and why can it generate code so open and I made also an LLM to generate computer code.

530
01:01:33,200 --> 01:01:47,200
And it was not trained on language but only on code. So by by using huge open source repositories of software code, and this is probably also integrated into the whole chat GPT architecture and sitting behind the classifier gate.

531
01:01:47,200 --> 01:01:58,200
classifier classifies that the user wants to obtain code, then the request of his past to this to this codex LLM and then codex generates a code and and the controller gives it back.

532
01:01:58,200 --> 01:02:11,200
So this is how I think it's used, but there are this is there are there are this is just, let's let me say qualify speculation out of 20 years working in software engineering myself now.

533
01:02:12,200 --> 01:02:19,200
But but if you look if you look, but still, given all what I've said.

534
01:02:19,200 --> 01:02:38,200
If you want if we imagine all that these large language models are specified to certain tasks in the manner that was described for chat GPT, they will gain a lot of usage every day life and call and could also support medicine right I mean, and that that you can create adverse language with it in reality doesn't matter at all.

535
01:02:38,200 --> 01:02:46,200
I mean you just go to a construction site in your town and you listen to how the people talks and you have adverse language all the time.

536
01:02:46,200 --> 01:02:57,200
Yes, adverse language hurts no one violence is only when somebody gets physically injured. So so you know this whole hysteria about oh it creates adverse language. Yes, it does.

537
01:02:57,200 --> 01:03:18,200
But you know, need a read flow bear or Ovid or Goethe or or Herman Melville, it's full of you know violence and adverse language. It's just part of life and so I think this whole cult around about making models not speak an adversary language is doesn't will not stop its adoption,

538
01:03:18,200 --> 01:03:34,200
because because ultimately there's huge sequence generating models are super useful and they will be put you know into search engines and they will be used as this will be the first generation of expert systems that will become widely used.

539
01:03:34,200 --> 01:03:45,200
And I think they're super valuable because if you want to you off course you always need to look at the output carefully and judge yourself with it whether you want to use it.

540
01:03:45,200 --> 01:03:59,200
But but but for most, if you don't try to break the system, but if you just try to use it properly like like you say, what is the best antibiotic for metronidotso resistant infection of the interest time.

541
01:03:59,200 --> 01:04:19,200
Right. And now and now you will just get a very good answer. Of course you can make the model gives you a bad answer, you know, by having engaging along a dialogue and so on. And then you will you will because of the mechanism I just explained will make it create output that is that is bad but basically if you just use it in a very rational way it will be very good.

542
01:04:19,200 --> 01:04:23,200
I don't know whether I still have another style I think that's it.

543
01:04:23,200 --> 01:04:28,200
Yes, so thanks a lot. And now I hope you can discuss a bit.

