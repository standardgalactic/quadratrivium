start	end	text
0	7680	So I'm going to be talking about the book indeed, but I guess the most interesting part
7680	12800	of what I have to say is about chat GPT, and so I changed the title.
12800	14840	This is the book.
14840	18520	The subtitle is Artificial Intelligence Without Fear.
18520	23640	So we certainly don't need to be worried about the supposed fact that machines will one day
23640	25320	rule the world.
25720	33920	AI is a set of algorithms, of algorithms which belong to a certain kind of applied mathematics.
33920	36000	And these algorithms are very good.
36000	37520	They can do wonderful things.
37520	42800	And the fear that people have, and we are aiming in the book to set aside this fear,
42800	48800	is that one day there will be an algorithm of this sort which is able to provide an
48800	53480	intelligence which surpasses the intelligence of human beings.
53480	60520	And then once we have an AI algorithm like that, it would be able to write a new AI algorithm
60520	66000	which would be even more intelligent, and then we have an explosion of ever more intelligent
66000	72960	AIs, and eventually they would be able to use their intelligence to replace human beings
72960	77320	and to rule the world or the galaxy or the whole universe in principle.
77320	78720	This is nonsense.
78720	80200	This will never happen.
80280	85920	AI algorithms will be always much lower in intelligence than human beings.
85920	91560	Indeed, they will never have intelligence like the intelligence of human beings, because
91560	97600	they will always be what is called narrow AI, which means that they are intelligent only
97600	103200	in relation to one specific activity, for instance, playing the game of Go, and they
103200	108200	will never have the kind of general intelligence which we have and which would be needed to
108200	109760	take over the world.
109760	113240	So that's what we mean by artificial intelligence without fear.
113240	119800	There will never be the singularity when AI explodes and becomes more intelligent and
119800	122080	more powerful than we are.
122080	126440	So this is another way of formulating the main theses of the book, which rests upon
126440	129680	the mathematics of complex systems.
129680	135720	Complex systems, and that means all systems involving organisms, your brain, your digestive
135720	140360	system, you yourself, the system formed now by the people in this room.
140360	143480	All of these complex systems have evolutionary properties.
143480	145960	What that means is that they can change.
145960	152120	They can acquire new elements, new types of elements, new types of interactions.
152120	158480	And any model which can predict the behavior of a system breaks when you have new types
158480	161320	of phase space, they say, in physics.
161320	165000	We can't model complex systems mathematically.
165000	171680	Therefore, we can't emulate such systems inside a computer that follows trivially.
171680	173920	So this is the main thesis of the book.
173920	177240	The main chapter is about the mathematics of complex systems.
177240	180120	The rest of the book is about many things.
180120	183180	It's about intelligence, which I'm going to talk about next.
183180	185760	Human intelligence, what makes it special?
185760	191840	It's about attempts to emulate human intelligence by means of modeling in some sense biologically
191840	192840	the human brain.
192840	194960	I'm going to talk about that.
195680	201840	And then I'm going to focus my energies on chat GPT, which is, as I say, something glorious,
201840	205800	but it's also really, really, really bad.
205800	208640	And I'll try and prove that with some examples.
208640	215280	So an example of a system which is changing its phase space is the system of creating
215280	223120	spam, which is a system run by evil people whose life is devoted to creating these horrible
223160	225280	things called spam.
225280	227480	We can stop the spam using AI.
227480	231720	We can build spam filters, which are narrow AI in two senses.
231720	238320	One, they only filter out spam, but two, they only filter out spam of a sort and sort.
238320	243280	And as soon as new types of spam come down the pipeline, then the spam filters won't
243280	244280	work.
244280	249560	And this is what I mean by the impossibility of predicting the future, predicting future
249560	251240	behavior of a complex system.
251240	255880	Even a complex system as familiar as the system of spam creation.
255880	257080	All right.
257080	263240	So AI is always limited to simple systems in a technical sense.
263240	271000	So an AI algorithm like chat GPT is a huge mathematical polynomial function with billions
271000	272320	of parameters.
272320	280920	Google Translate is not based upon those complex systems which are human languages.
280920	288720	It's based on a frozen set of data, a corpus taken from the 96 or so human languages which
288720	291160	Google Translate translates.
291160	295720	And that corpus is then turned into a simple system.
295720	303040	And then Google Translate uses very large algorithms to create polynomial functions
303040	308240	which can take an input in German and yield an output in English.
308240	314200	And that's a mathematical application to binary vectors made up of zeros and ones which can
314200	319360	be translated as English sentences and binary vectors made up of zeros and ones which can
319360	322320	be translated as German sentences.
322320	324120	Google Translate is dumb.
324120	327480	It doesn't know anything about meaning or semantics.
327480	329560	It doesn't know what it's talking about.
329560	334080	It just performs a certain mathematical calculation.
334080	339200	Simply rather simple because it has to compute inside a Turing machine which is a relatively
339200	345040	simple kind of environment but incredibly long as an algorithm which explains why it's
345040	347520	able to perform such impressive feat.
347520	349960	So there is glory to Google Translate.
349960	354680	I think Google Translate is fantastic but it's not going to take over the world or anything
354680	355680	like that.
355680	356680	All right.
356680	357680	Now how does this work?
357680	362040	How does an algorithm like Google Translate work?
362040	367680	Well many people think that all you need is enough training data and then these things
367680	377280	called deep neural networks can be trained to use statistics in order to predict patterns
377280	380000	in those large bodies of data.
380000	385520	But this isn't quite right and even a lot of people in the AI world don't appreciate
385520	391920	this shortfall in the idea that all we need is mere quantity of data.
391920	398880	What we need is to be able to sample data which has a variance which is the same as
398880	400400	the target data.
400400	406000	So if we're going to take the sample data and use it to predict patterns in the target
406000	413040	data then the sample data has to be statistically like, it has to be a typical sample in other
413040	415400	words, like the target data.
415400	418880	It must be representative of the target data.
418880	423200	So what that means is that it has to have the same distribution of the target data and
423200	426600	this is the bell curve which is the simplest kind of distribution.
426600	432600	There are other kinds of distribution but the data you have has to have the same distribution
432600	436600	as the target data you're applying to and that's what Google Translate does.
436600	443200	It takes samples from all the world's languages and it is able to take them as representative
443200	448120	of the patterns in this frozen corpus that they use as a starting point.
448120	452360	Now there are target domains where there is no distribution and so there is no way in
452360	455160	which we can get representative sample data.
455160	461160	So this is true in an emergency room in a hospital in a big city.
461160	465320	You just can't predict how much blood will be needed or how many beds will be needed
465320	469360	or how many doctors will be needed even an hour ahead.
469360	471960	But it's true also of any conversation.
471960	476320	You can't predict what your conversation partner will say next.
476360	479200	Alright so this is an overview of what I'm going to talk about.
479200	484760	First of all I'm going to talk about human intelligence, actually animal and human intelligence.
484760	490520	Then I'm going to talk about the real reason why computers will never take over the world
490520	496560	which is the fact that they will never want to take over the world because algorithms can't
496560	497560	want.
497560	500040	They can only do what you tell them to do.
500040	506200	Then I'll talk a little bit about Nick Bostrom and his idea that we can build a super-intelligent
506200	511480	AI algorithm by emulating the whole brain of the human being.
511480	518320	And then finally I'll talk about the really funny story of chat GPT.
518320	526680	Alright so the big difference between organisms and simple systems is in one word it's thermodynamics.
526680	532760	So in other words it's a matter of physics which involves energy and we and all animals
532760	538200	survive because we have the drive to acquire energy from the environment.
538200	542600	Now we humans do this in a very complicated way involving things like supermarkets and
542600	548120	farms but every animal has a way of sucking energy out of the environment.
548120	550480	Every plant does this with the sun.
550480	552480	Even computers are driven in a certain sense.
552480	558080	They take energy from the environment but only because we give it to them and no one
558080	559520	gives us energy.
559520	562600	No one gives animals energy we have to go and find it ourselves.
562600	570120	If there is a surface of energy in our environment, in the ancestral environment of human beings
570120	576760	then we become obese because we like eating and so we keep eating and this eventually
576760	581240	will mean that we will eat so much that we use up all the energy in the environment and
581240	583240	then we die.
583240	591360	So gradually we moved out of the areas of the world where there was lots of food into
591360	597040	areas of the world which were cold and barren and so we had to find ways of surviving in
597040	599720	much harsher environments.
599720	608200	That's why through a long series of faltering steps we created civilization, police, armies
608200	613560	all the other things which make it possible for us to survive in a world where we are
613560	620880	competing with other groups for limited food supplies.
620880	626480	What civilization does, what social norms do is channel the excess dry which human beings
626480	627480	have.
627480	634320	In other words we become more rational and less instinctive.
634320	643200	But we are still always seeking for energy but now because we have found ways of solving
643200	647160	the energy problem through supermarkets and farms and so on.
647160	653920	We can do other things, we can build orchestras, we can go to talks about chat GPT, we can
653920	659360	play with chat GPT, we can watch the traffic through the window, we're always doing something.
659360	663880	We're doing one damn thing after another and that's the same with animals too.
663880	669760	We never stop, there's no tendency towards equilibrium.
669760	678640	As long as we're alive we are doing one damn thing after another so no convergence on equilibrium.
678640	685160	This is thermodynamically remarkable that there are entities on the planet which are
685160	693440	decreasing entropy by taking energy out of the environment and replacing it with cathedrals
693480	697800	or with airplanes or supermarkets.
697800	706880	Now so as we go through life not approaching any equilibrium we are constantly changing
706880	711480	our state, changing the phase space.
711480	716120	So if we're in an orchestra and we're under the command of the conductor we have one phase
716120	723400	space but then suddenly we have a pain in our arm and we run outside and go to the doctor
723400	728000	because we think there's something wrong with our arm and we're in another phase space.
728000	733560	Any kind of change like that and such changes happen all the time would break any kind of
733560	742400	predictive machine because predictive machines have to use mathematical equations of a mathematically
742400	748480	rather simple sort and they can't cope with multiple ever-changing phase spaces.
748480	754440	This is if you want to predict the behavior of an entity where you have a Cartesian coordinate
754440	760320	system telling you what its behavior is but then suddenly it changes the behavior so that
760320	764200	you need a further dimension and a different coordinate system.
764200	770600	Your predictive attempt would fail because you've changed the phase space.
770600	774440	Alright now there are in fact three kinds of drivenness.
774440	781040	There's animate drivenness which is organisms, animals and humans particularly.
781040	789200	There is inanimate drivenness so the tides take energy from the moon I guess and the
789200	792720	whole earth takes energy from the sun.
792720	798080	Machines get energy given to them so we give coal to the steam engine, we give electricity
798080	799920	to the computer.
799920	805760	This is external drivenness and external drivenness means that the external supplier of energy
805760	809840	which is typically a human being is in control of the machine.
809840	814320	That's another reason why machines will never rule the world.
814320	821360	Alright so we have natural drivenness and artificial drivenness and artificial drivenness
821360	825240	means steam engines, laptops, tanks and so on.
825240	827760	Ice drivenness depends on human drivenness.
827760	830960	We want to have the steam engine do something for us.
830960	836280	If it's not doing anything for us we're not going to feed it energy anymore and that's
836280	838400	what happens.
838400	845120	So somebody forgot to maintain this entity and so we don't need to supply it with energy
845120	848960	anymore.
848960	854120	And this is how Schrodinger expresses this matter.
854120	858840	Now of course eventually we do not escape the decay to equilibrium, there comes a point
858840	863480	where we go over the cliff and then we're dead jack.
863480	868360	But until then it's one damn thing after another.
868360	874440	Alright now so machines need energy from the environment and they create energy.
874440	880120	So a computer if it's switched on but not being used is a heater, it's giving off heat
880120	885360	and this is another reason why what we're talking about now is thermodynamics.
885360	892000	And this aspect of computers is often neglected but it's another factor in the question whether
892000	894400	computers would ever take over the world.
894400	901480	So we already know that the crypto coin industry is using significant amounts of energy, significant
901480	904800	fractions of the energy which humans need to live.
904800	910680	If we have computers of anything like the power that people conceive then there would
910680	917560	be an energy problem and that would mean that this power would be reduced one way or another.
917560	922480	But of course we'll never get even near there.
922480	926640	We will never see even the attempt to take over the world by machines because they cannot
926640	928480	want anything.
928480	935560	Alright so we produce energy storing molecules called ATP from the sun and from food and
935560	940880	so forth and then we use that energy to survive and reproduce and to do all the things that
940880	946160	we do such as wave our arms when we're speaking and things like that.
946160	948920	Now we come to intelligence.
948920	959040	So primal intelligence we find in both animals and humans.
959040	964560	And then there is a kind of intelligence that we call objectifying which is exclusive to
964560	970440	humans and which is the reason why we're able to build supermarkets and farms and airports
970440	977760	and all of those other things that enable us to do more than survive.
977760	984800	So primal intelligence is what animals do when they're in their ancestral environments
984800	987920	and they're acquiring food.
987920	990600	If there is food around then they just use it.
990600	995360	If they have to go chasing food, finding food because their available resources have been
995360	1001680	used up then they have to still use their primal intelligence but they have to use their
1001680	1006960	primal intelligence in order to find new food which means they need at least two aspects
1006960	1010280	of intelligence which plants don't have.
1010280	1016160	One is they need to have conscious perception because they need to be able to identify new
1016160	1023880	food as food rather than as something which looks like food but which is in fact poison
1023880	1028400	or are just an accident of similarity of shape.
1028400	1036920	And so they display ever more powerful versions of primal intelligence as they become more
1037920	1043720	complicated, more ambitious in their attempts to find new food and eventually they go hunting
1043720	1050600	in teams and then they develop a crude or language, a proto language to organize the
1050600	1054240	other members of the team so that they know what's going on when they're hunting large
1054240	1056680	animals for instance.
1056680	1062560	And so they become to some degree adaptive but always within the ancestral environment.
1062560	1067280	The adaptiveness is their ability to find new food and of course if they fail to find
1067280	1074960	new food then they're dead and this applies to all kinds of animals from parrots to humans
1074960	1080320	in the ancestral state.
1080320	1086240	So we don't learn primal intelligence, it's innate, it's instinct and the characteristic
1086240	1092400	of human beings is that they have abandoned, they've lost most of their instincts and instead
1092400	1097280	we have civilization, we have social norms, social control and so on.
1097280	1106360	And it's a marker for intelligence in the sense that it doesn't act by trial and error
1106360	1115240	or by, I don't know, some alternative to trial and error which would involve checking samples,
1115240	1116760	it's immediate.
1116760	1120600	As soon as they see something which looks like food immediately they know that that could
1120600	1126720	be food and the typical characteristic feature of something's being intelligent is that it's
1126720	1130640	a response which happens immediately.
1130640	1137640	Alright so these are the features of primal intelligence and so you can't train anything
1137640	1141880	to have it, either it has it or doesn't or it doesn't.
1141880	1147000	And non-human animals have just the goals of their ancestral environment to find food
1147000	1153640	in that environment, to survive when competitors try and steal the food so they have the ability
1153640	1160440	to fight or the ability to flee and they ignore everything which is not responding to their
1160440	1163120	biological needs.
1163120	1170880	Their world is just that which is relevant to eating, fighting, fleeing and so forth.
1170880	1175760	Now higher animals, as I've already said, can develop something like a proto-language
1175800	1181840	so birds have elaborate signalling systems for instance.
1181840	1186840	Many animals have developed elaborate tracking skills for seeking the food which they need
1186840	1195320	in order to survive and they've even developed something like wanting so they want to find
1195320	1197960	food when they're hungry.
1197960	1203640	But it's always within the ancestral environment so they don't build new kinds of buildings
1203640	1212600	because they don't build buildings really and that's the big difference of course when
1212600	1217560	we move to the case of humans.
1217560	1222360	So we had to survive in tough environments that meant that we had to go outside our ancestral
1222360	1227880	environment which means that we had to abandon practically all of the instincts that kept
1227880	1232920	us alive in the ancestral environment and work out new ways of living.
1232920	1238000	And that meant that we had to develop things like curiosity but we had to develop other
1238000	1249720	kinds of capabilities and one way of grouping these capabilities so everything I've said
1249720	1257400	so far is pretty standard but the term objectifying intelligence is a new term which we formulated
1257400	1263560	in response to Husserl's way, I'm switching suddenly to philosophy, of understanding the
1263560	1266560	way language and the mind works.
1266560	1273760	So he talks about objectifying acts and what he means by is acts directed towards objects
1273760	1278640	typically other people but it might also be things like tables chairs or it might be things
1278640	1285120	in the future or in the past things which are distant in each case we have this objectifying
1285120	1293640	intelligence and for humans this goes beyond any biological need, it can extend towards
1293640	1300480	the future, it can extend towards the opera, it can extend towards the planet Mars independently
1300480	1306760	of any biological need which is the reverse situation from what we find among animals.
1306760	1311560	So we're moving into new kinds of contexts all the time, we're able to keep track of
1311560	1316840	objects as we move from one context to another or we're able to switch targeting completely
1316840	1324440	to a new set of objects and a new set of norms and so forth and this happens sometimes in
1324440	1331600	a given in a single conversation so that reminds me of what we were talking about last Christmas
1331600	1337040	about the rotten cheese that had made me so sick just before the Covid panic started.
1337040	1343560	We jumped around in just one longish part sentence between multiple context you all
1343560	1349400	follow what I was referring to even though you've never heard this I'm not sure now
1349400	1353760	what would happen if I fed this into chat GPT.
1353760	1361400	So there's no Markov property here one of the reasons why computers are not able to predict
1361400	1366800	the future in a realm like human conversation is because human conversations don't have
1366800	1373440	the Markov property and our mathematical resources to model processes nearly always
1373440	1379120	rely on the Markov property that's missing.
1379120	1384720	Alright so how did objectifying intelligence evolve the answer is over millions of years
1384720	1393960	and certain parts of it I can talk about here so one important part I've already mentioned
1394320	1398520	because we have these proto languages and eventually have language in its fully formed
1398520	1405160	state we can engage in all kinds of shared agency so we can plan on going to the moon
1405160	1411800	or we can build a cathedral or we can well we started by building walls to keep us safe
1411800	1418040	against our enemies building a wall like that involved some considerable shared agency at
1418080	1424720	that time and this is one of the oldest five walls on earth on top by Google I guess I
1424720	1431040	could chat chat GPT to alright so these are some of the marks of objectifying intelligence
1431040	1437320	and I'll go through this quite quickly so as I say it doesn't depend upon our biological
1437320	1441520	state you can move in any cultural world you can move in the world of mathematics you can
1441520	1449480	move in the world of plant biology it's completely open and it involves categorical thinking
1449480	1456640	already from infancy so children can recognize categories they have an infant metaphysics
1456640	1462360	and we have a world model which is built out of these categories and the relations between
1462360	1468760	objects in different categories for instance the causal relations but then also the relations
1468760	1473240	having to do with ethics for instance that if you bump into a chair you don't need to
1473240	1478680	apologize to the chair but if you bump into a human being you probably need to apologize
1478680	1486640	and we have a theory of mind or inter subjectivity so you are all objects I am an object for you
1486640	1495120	I can also be an object for myself in being an object for me under the category of person I
1495360	1501360	appreciate automatically without reasoning about it that you have beliefs and desires and so
1501360	1510400	forth we can plan so objectifying intelligence allows us to plan for the future and we can
1510400	1521200	plan together to build an airport or a moon landing or whatever it might be and then finally a
1521360	1527840	feature of objectifying intelligence is that while we typically target objects that we believe to
1527840	1535120	exist we can cancel belief and we can imagine and we do that when we plan when we have ambitious
1535120	1539760	plans we plan going beyond the planet earth but we can also do it when we're writing fiction
1540560	1546400	and this is this is an ability way way beyond anything which animals have chat gpt has this
1546960	1553760	ability it but it doesn't need to suspend belief because it doesn't have any belief in the in
1553760	1562800	the beginning it can mimic writing imaginative texts and then we once we build these new environments
1562800	1570240	we can live in them culturally including in scientific environments so we can build an
1570240	1575600	environment to serve a certain purpose for instance studying disease or whatever it might be
1576640	1583120	all right now we come to the missing a i will and we'll talk a little bit about my hero nick
1583120	1590160	bostrom who wrote a book called superintelligence in this book he says all philosophers should
1590160	1600320	give up their job and work with him to prevent the singularity and this we this is a rational act
1600320	1607760	because once a i become superintelligent it will be able to do better philosophy than we can do now
1607760	1616320	anyway so preventing the intelligence well anyway you get the idea so now he thinks that
1616320	1624080	this singularity could exist and that there is a ticking time bomb which is the a i this chat gpt
1624080	1629920	plotting to take over the world it's already ticking and we don't know how far away we are from
1629920	1637840	the great cataclysmic events when a i will machines will join together and and take over
1637840	1645360	the universe but he worries a lot about it and the problem is as i say that computers can't want
1646080	1651760	and so they can't want to take over the world they do not have a will now bostrom talks quite a
1651760	1658160	bit about goals of machines in his book but he never explains how computers can have goals
1659040	1667840	what he does is refer to this man yudkowski who i understand does good work in a i ethics
1669280	1677120	and so and he apologizes he refers to yudkowski's work on the machine will but he wants to distance
1677120	1681680	himself because he appreciates that it's not really quite clear what yudkowski is trying to say
1682480	1688800	and you can decide for yourself so this is what he says it you will notice that he doesn't tell us
1688800	1695440	how a goal system will come into existence he just tells us about what a goal system is like
1696080	1700560	and he tells us only about the goal system goal system that he himself would like
1701520	1707040	not about the goal system which a machine if it could have goals which of course it can't
1707120	1713680	so it's a goal system containing only decisions super goals in and beliefs with all sub goal
1713680	1718960	content being identical with beliefs about which events are predicted to lead to other events
1718960	1726000	and all desirability being identical with with leads to supergoldness if you can understand that
1726000	1732640	then you're a better man or woman than i i have no idea what he's talking about and that's why
1732640	1738960	bostrom apologized because he didn't have any idea and he goes on like this so the content of this
1738960	1746800	goal system is our wish if we knew more thought faster were more people we wished we were had
1746800	1753600	drawn up further together where the extrapolation converges and so on it's complete i don't understand
1753680	1762560	what it is and it goes on so now why is a machine will and bostrom did not find a
1762560	1766560	count of a machine well i don't believe that there is a good account of how a machine could have a
1766560	1773600	will outside the cases i'm going to talk about in in talking about charting pt later on there is
1773600	1781840	something like a will that i will explain in a minute so without a will the machine could never
1781840	1788000	become an autonomous agent and if it can never become an autonomous agent then it can never
1788000	1794720	pursue goals and if it's not autonomous it can never be either moral or immoral you can only
1794720	1799600	be moral if you can take responsibility for your actions and you can only take responsibility if you
1799600	1805360	will them if we will them which is what we would do in writing the software they're not your goals
1805360	1813200	and you look you do not have a will you're just following our will so how do we understand the
1813200	1818880	human will now here i'm going to do some more philosophy this is a man called max sheila who
1818880	1825360	was a very influential philosopher the turn of the last century and one of his students with
1825360	1831920	edith stein who is one of the i wanted to say father figures but i guess i should say mother
1831920	1840080	figures of feminine of female philosophy who was also a saint so she died in auschwitz and
1841120	1848960	was canonized and he was a saint too oh he's a saint both of them were very
1848960	1855040	influent very much influenced by max sheila he whitey was habity tats jaunschlift is about
1855040	1861280	max sheila's word it's also about thomas equinas of course but it's about sheila primarily and
1861280	1867680	this is rather an amazing feat for a teacher to have two of his students become canonized
1869200	1875920	and but so but sheila is interesting for other reasons so this is his big book about ethics
1876560	1882160	and basically he distinguishes ethics into two categories first of all there's formal ethics
1882160	1888400	which is cant and the like where you have imperatives that you have to follow and they are
1888400	1894640	to be followed on the basis of rational arguments and then you have sheila's own version of ethics
1894640	1902000	which he calls material ethics which is based on feelings value feelings every normal person
1902000	1906000	experiences value feelings all the time even if it's just thirst
1906000	1919840	but there are some people psychopaths who are value blind and so sheila on this basis
1919840	1927280	tries to give an account of the will and his example is a rescue scenario where a man sees
1927280	1934080	a drowning child and jumps in to rescue the child so it's a perfectly general account of the will
1934080	1940000	and it could be applied also if you're playing chess the decision to move your night in a certain
1940000	1947360	direction would fit his schema for what the will is like and so this is the chess scenario
1948080	1953760	i'm going to talk about the jumping in scenario it consists of four stages but we're only going
1953760	1959040	to talk about two of them there's a fifth stage where you do actually jump in but this is what
1959040	1966880	is involved in the will to jump in and more precisely the act of will takes place at the end
1966880	1972560	here and that there is uh i'll give you a picture in a minute so you see the drowning child it's not
1972560	1978240	just perception you also begin to have value feelings you feel that there is something which
1978240	1986880	needs to be done here you might call that a moral affordance and then you draw the value consequence
1987600	1993360	in the sense that you you you watch the child you realize that she's going to drown and you
1993360	2000720	realize that this would be a bad thing and then you decide to act now this is this is a complex
2000720	2008720	phenomenon making a decision so you decide to jump in to save the child and this decision is
2008720	2015040	based on knowing that you can swim that you can swim well enough in the current to save the child
2015120	2019200	you have enough time to save the child so this part of the deciding is kind of
2019200	2028000	rational part combined with value feelings but there are other parts so 3a is forming an intention
2028640	2035520	to save the child and to view the child as worth saving something that ought to be preserved
2037040	2043040	and then part of the decision making process is delivering how to how to perform the rescue
2043680	2051520	but then the important part is resolving to take that course of action and here we're dealing with
2051520	2058880	something which is a physiological change in the brain and that physiological change in the brain
2059840	2065600	is it starts you off it starts you moving so it's an act of will which has a real consequence
2065600	2071440	or rather it's one side of an act of will because you have to have a physiological change also which
2071440	2083280	triggers the bodily movement so 3c the final very very tiny sliver of your deciding process when you
2083280	2090320	actually resolve to take the course of action in the full sense that your body starts moving
2090320	2096240	is practically just the other side of the coin from your body sending signals to your feet
2096240	2103280	that they need to start running and so we can see this roughly as taking this shape you have something
2103280	2112480	going on in the brain up here and you have something going on in the arm down here as your you move
2112480	2121600	out towards a swim I guess I should have taken feet here and that whole thing then is the is the
2121600	2129520	act of will it's a combination of a very very rapid triggering event in the brain and a very
2129520	2134240	very rapid signaling event to the relevant part of the body where the triggering event still has
2134240	2140720	something rational about it now we know very little about the brain and we can't predict any
2140720	2146320	practically speaking we can't predict any of this and so we can't emulate it in a machine or in an
2146320	2154480	algorithm and so we can't describe it mathematically and you can check by looking in textbooks of
2154480	2160640	neurophysiology there's very little in the way of mathematics all right now why is human well
2160640	2165360	so important well because of hunting and all of those important things which kept us alive during
2165360	2170240	the eight million years when we were involving ourselves evolving ourselves to a present to
2170240	2176480	present state now hunting involves tracking and tracking is really difficult and that's because
2176480	2183040	as you hunt the tiger the tiger is responding to you changing your environment as you change
2183040	2190320	his environment hiding behind trees performing tricks I don't know what tigers do but all the time
2190320	2197200	that you're moving around targeting the the tiger you're changing your face space and if you try to
2197200	2204320	do that with stationary sensors sending one-dimensional signals to a machine you'll get nowhere you will
2204320	2210640	never be able to hunt a lion a tiger and and we have a section of the book which describes
2210640	2216080	mathematically why something like tracking an animal or tracking a human being in a forest
2216080	2221280	or something is going to be way beyond the power of a computer so you have to spot the man with the
2221280	2231600	gun say he's well he's here and he has to spot the the bird that he's going to shoot and keep track
2231600	2237200	of the bird all right now the other reason why human well is so important there are many reasons
2237200	2243760	i'm just going to talk about two of them this is the second one conversation human conversation as
2243760	2252720	we saw is unpredictable how do we manage human conversation chat box created for bank telephone
2252720	2259280	conversation with customers after 50 years are still now i want to say crap but i wouldn't say
2259280	2267920	crap in it polite audience they're not not good 50 years why because conversation is really hard
2267920	2274800	it's harder than tracking a lion and the the the reason why it's hard is because conversations
2274800	2280080	rely on context so much and there are many different kinds of contexts including multiple
2280080	2287040	contexts in a single conversation as you talk about oh how bad it was in the covid era and so
2287040	2294080	you can shift the the context i just did now i've shifted the context to be about this particular
2294800	2301680	it's not really a conversation it's a one-sided harangue but um i'm now making what i'm saying
2302240	2310400	the context for what i'm saying and i just made the that context the context anyway um so our
2310400	2317360	goals will change but we always have goals it's the goals which keep the conversation alive my
2317360	2323280	goal is to convince you of certain things that's why i'm becoming so involved and that's some of
2323360	2329040	you may be becoming involved and we'll respond later i hope so that's what keeps conversation
2329040	2333520	alive everybody has goals their goals evolve through the conversation but without goals there
2333520	2340880	would be no conversation chat gpt has no goals well actually that's not quite true i will explain
2340880	2348240	in what sense chat gpt has a goal in a minute so how can you build a general intelligence a
2348240	2354880	machine intelligence that can do any of this um so the will will not arise by itself some people
2354880	2360240	claim that if you put all the computers together in a big internet system it will somehow evolve a
2360240	2367520	will that's just it's happy talk and you can't program a goal system not even you kowski can
2367520	2373680	program a goal system we can in some cases if you want to win at the game of go you can program a
2373680	2380160	goal system you can't program a goal system to win a conversation and if you don't believe me try it
2380160	2387760	with your spouse makes heaps core of each step in a conversation see who wins it will not work
2389280	2398400	all right so what are the proposed methods uh to i think i'm near 45 minutes is that correct
2398400	2405280	but that's fine just it's interesting okay um well that's good to hear all right so the old way of
2405280	2410880	doing ai was expert systems based on logic then came stochastic systems which are based on statistics
2410880	2416800	which we've been talking about that's chat gpt boss room have this idea of whole brain emulation
2416800	2424240	i think i'm going to skip that um because it's full of nonsense uh that is the the the the funny
2424240	2431360	chapter in the book and i'll give you just one joke um which is not me it's boston and he didn't
2431360	2436560	realize it was funny and then we will have we won't talk about artificial life at all we'll go
2436560	2441920	straight to chat gpt so this is the most boston's book and he thought that you could scan the brain
2443360	2447680	the problem with that is that to scan the brain you need to kill the patient and so that you're
2447680	2452480	scanning something which is static so you can never find the dynamic patterns in the brain and
2452480	2458400	that's just one of the problems so and um we don't know anything about the molecular
2458960	2468800	configuration of cells and um and some people think that we can do ai in in the general genuinely
2468800	2474080	intelligent sense if we use quantum computers but quantum computers are turing machines too
2474720	2479440	they're just a lot quicker and we we haven't built one yet practically speaking it's a dead end
2480080	2486240	maybe a dead end uh he also talks about biological enhancement of existing brains so you can maybe
2486240	2496320	make superintelligence by selective breeding uh you get i don't know um so you you get a lot of
2496320	2503120	people to breed and then you select only a small number of embryos that the clever ones so you have
2503120	2509840	a really clever way selecting intelligent embryos which i don't know about and then he says if we do
2509840	2520000	that we can raise the iq level by 24.3 iq points that is the silliest thing that was ever said
2520000	2527120	by anybody working in biology or in any anything near biology it's um anyway it's it's not good now
2528080	2539440	uh so that basically his whole thing doesn't work um so let's talk about chat gp t and um
2539440	2545440	i i really mean it when i say it's glorious and it's really a fantastic thing and i like ai
2545440	2552640	generally i just i'm aware that it's always going to be narrow ai now chat gp t is narrow ai too
2552640	2563120	can only do one thing um so let's talk about the misery and i imagine all of you have played
2563120	2569600	with chat gp t if not you should certainly play with chat gp t for a bit and you will find that it
2569600	2577760	does odd things so that it makes stuff up for instance and now this is an example where it
2577760	2583440	realizes that it's not really intelligent you can't do something which even a not very intelligent
2583440	2592320	human being can do so i asked it to send me five a list of five single authored papers on medical ai
2593840	2601120	and it said no he can't do that but then he gave me a list or sorry it gave me a whole paragraph
2601120	2606640	of stuff that i didn't want to know so telling me about ai applications in medicine and so on which
2606640	2613680	i knew anyway it it wants to be nice as it were gets anyway you'll see why it wants to be nice in
2613680	2618960	a minute and it couldn't give me an answer so to the question i wanted which is an easy question
2619520	2624880	so it gave me an answer to a different question but then i asked it again a few seconds later the
2624880	2632880	very same question and it gave me five single authored papers on medical ai sure here are five
2632880	2640240	single authored papers on medical ai ai so the first problem is that two of them have et al in the
2640240	2648480	author list now even an ignorant person who understands the request will know that this is a bad
2649600	2655120	first step in answering that request but it got three right out of five which is a good score
2656000	2663840	for these difficult questions so and as i say any human intelligence would find this
2663840	2668960	request is a trivial and it failed but the the next problem is that none of the five papers that
2668960	2678720	it requested exists it made them up so it can't even make up a single authored paper at random
2678720	2689360	it it failed on two of them and i'll try another one so i um in that this way i i this was a serious
2689360	2695440	question i wanted to know the answer uh so i have an iphone 11 and i thinking about buying an iphone
2695440	2703280	14 so i asked it and it said sorry the iphone 14 is not yet released this was on 17th of march
2703280	2710960	2023 and then it gave me all sorts of information that i didn't ask for about iphone 13 and so
2710960	2717840	but two minutes later i tell it but the iphone 14 was released four months ago
2718960	2724000	and so it says i apologize for the confusion you're right and so so that's not a good sign either
2724880	2733760	now i i've done a lot of work i know a lot about barry smith and so i can ask you all
2733760	2738640	such sorts of questions and work out the score of how often you get things right and it's it's
2738640	2746000	less than 50 so here we have the question who wrote that which i wrote i wrote this phd thesis so i
2746000	2752160	want the i want the answer barry smith so it gives me the answer kevin molligan who is a close friend
2752160	2759120	we've written things together but he did not write my phd dissertation so i tell it to try again
2759840	2766400	and then it says that my phd dissertation was written by a famous philosopher from the 1950s
2766400	2773760	1960s which was when i was a boy a little boy uh so he didn't write it and so i tell it to try again
2773760	2779120	it goes back to kevin molligan and i say are you sure yes i'm sure that kevin molligan wrote
2779680	2784880	are you sure you're sure and then he apologizes again and he says that it was actually written
2784880	2792960	by john michael croiss who i'd never heard of from that moment but it turns out that as
2792960	2797600	chat gpt says he was a philosopher which is correct he was a professor of philosophy at the
2797600	2803600	university of frankfurt which is incorrect he was born in 1943 which is correct in boston which is
2803600	2812240	incorrect and so so he gets a little bit of truth about the non-author of my phd dissertation which
2812240	2820000	is worth less than zero to me except that i can prove that there are things going on here that
2820000	2826480	shouldn't be going on now let's try this one all the swiss people in this room will know that there
2826480	2836000	is the orna loch and it's an old tongue basically an interesting old tongue so i asked it what is
2836000	2843600	the orna loch and it said i'm sorry but i'm not sure which specific orna loch you are referring to
2843600	2850880	as there may be different places or things with this name however one possible reference is to the
2850880	2859280	orna lochschaft so it changed the subject it says it's in switzerland in a district in the canton
2859280	2866480	of ori that encompasses the valleys of schekenthal and oeseren do those valleys exist anybody know
2868560	2875040	the valleys exist good the name orna lochschaft literally means the district of the ori valley
2875760	2883600	that is not true i'm assuming you can correct me here another reference is the orna loch cave
2883600	2890240	in austria there is no such cave now if you could provide more information on the specific orna
2890240	2896080	loch you are referring to i'd be happy to provide more information so i said could you provide me
2896080	2903280	with more information about the orna lochschaft in switzerland which doesn't exist and it gave me
2903280	2910960	two whole pages of tourism information other notable attractions include the historic town of
2910960	2916880	aldorf and the aldermat i have no idea whether any of these things exist either but the orna loch
2916880	2924240	shaft does not exist and you can check by asking google there is nothing there isn't a single entry
2924240	2929600	which is a kind of miracle for any strings that you might give to google it can usually think of
2929600	2937280	something but here there's nothing so is there an orna loch shaft i think no it made it up all
2937280	2945200	right now there is a a very nice um slide deck by yang luqun who is one of the real experts in
2947360	2953840	the sarcastic ai he's also one of the people who we cite in our book as also believing that
2953840	2958880	there is a lot of nonsense being talked about the singularity machines taking over the world
2960080	2968080	here he he gives a mathematical argument why these hallucinations they're called
2968080	2977920	non-nonsense that uh genomes that the chat gpt throws up um the the reason is a mathematical one
2978480	2985200	and it's so the mathematics we think is not quite right the formula needs to take account of
2985200	2991600	length of input and length of output because the likelihood of error goes up for longer inputs
2991600	2998320	and longer outputs which seems reasonable but this is a first step the probability of a of a
2998320	3007040	chat gpt output being correct is one minus e raised to the power n and that means it's this red
3007040	3015600	area here they are the correct answers and he thinks that this exponential divergence is not
3015600	3025280	fixable so chat gpt is dead jack because if they can't fix this nonsense no one will trust chat gpt
3025280	3029760	and it will be replaced by something quite different and no one knows what that is because the
3029760	3037920	four large language models which is what chat gpt is the google one the the bing one uh i've
3037920	3042800	forgotten the facebook one i guess they all use the same principles and they all have the same
3042800	3052320	error code they all generate stuff uh that they make up all right now that's the misery of chat
3052320	3059120	gpt and it should feel miserable now because i just declared it dead and i should really be investing
3059120	3066320	i should be shorting stock which relies on chat gpt being alive in say six months but i'm not doing
3066320	3072400	that all right so let's see how it works and why it is fantastic why it's a really a miracle which
3072400	3080560	surprised me so i'm not pleased with it at all but it it did something which is important so
3081360	3087360	how did we go the answer is through an ai method called reinforcement learning which is a method
3087360	3094560	which works well for games like go and the way it works is that for a game like you know you can
3094560	3100880	go you can define a reward system for each move and it can be a reward system which
3100880	3108560	whether rewards can be assigned by the computer now if you can do that you can play the game
3111440	3116320	over and over again billions of times inside the computer you don't need human beings so
3116320	3122560	they're still trying to crack the game of dota 2 which is apparently a leading esport game i've
3122560	3129040	no idea what esport means but dota 2 exists they still haven't cracked it but they're trying to
3129040	3137120	crack it with a software algorithm called open ai 5 which usually wins against humans and this can
3137120	3145920	play 180 years worth of dota 2 games in a single day if you can do that you can do you can perform
3146000	3157040	miracles in principles such as beating dota 2 so can you do it for conversation three months ago i
3157040	3164800	would have said no impossible and i just said it 10 minutes ago chat gpt showed how you can apply
3164800	3172800	reinforcement learning to what looks like conversations now how did it do that so what that
3172800	3179920	means is that we are doing a little bit like emulating human will because the alpha go has to
3179920	3185600	want to win the game of go in some sense of want it has to emulate the kind of want that you have
3185600	3193840	when you play a game and want to win so how does it work well you need a reward system and i i put
3193840	3200160	this in that's what i used to believe i still believe it really but um chat gp has unsettled my
3200160	3207920	conviction so chat gp found a chat gpt found a way to use reinforcement learning to emulate two
3207920	3214000	persons human conversation inside a computer and it says to here this is a big deal and i mean that
3214000	3223360	in a positive way now how does chat gpt work you give it any string and it will work out from its
3223360	3230560	really powerful knowledge of language what the next lightliest next syllable is
3231440	3236960	that's why it sometimes takes time when it's chatting as it were so if you say the best thing
3236960	3242320	about ai is its ability to then it will say learn because that's the next most probable
3244400	3249600	output and now it doesn't always take the most probable because if it did it would go around
3249680	3258000	circles so sometimes it has a random kick down the hierarchy all right now notice that it doesn't
3258000	3262960	understand anything it just has an incredibly powerful knowledge of the patterns of language
3262960	3268960	which enables it to know what the next syllable will be will be most likely after any given string
3270800	3276720	and so this is how it was built so the first part is creating this wonderful
3277440	3283200	patterned model of language not just english but other languages too and i won't talk about that
3283200	3289360	that's that that's the same kind of training that you find in google translate and and there are
3289360	3295360	then three steps which i'll go through one by one so we have prompts and these come from being
3296560	3300320	so they're being questions we don't know what they are and that's a little bit fishy
3301040	3306720	so some people would like to see the prompts because chat gpt4 is claiming that it can beat
3306720	3312720	humans in medical exams and some people think that the answers to the medical exam questions were in
3312720	3320400	the prompt database that was used to train chat gpt in which case the being able to beat humans
3320480	3330560	would be worth nothing um so and then you have people i some people say it was 40 contractors
3330560	3334640	but again that's a secret it may have been many more they were all in india so they didn't cost
3334640	3341120	as much as if they've been in theory for instance and they were hired to write responses to those
3341120	3352240	prompts now so the when was michael jackson born that kind of prompt and and many other prompts but
3352240	3357280	only a limited number and we don't know what they are but some people say 13 000 prompts
3358640	3365120	so that's the first step now the second step so you've you've you've got the prompts from being
3365120	3370080	and you've got the outputs from the people in india who are paid to respond to these prompts
3370160	3379840	producing text then the next stage is that you pay a labeler it's called but it means an evaluator
3380560	3389600	to rank the outputs created in the first stage now you can't rank outputs when you're talking to your
3389600	3397280	spouse in a conversation but you and that's why we can't create a reward system for conversation
3397280	3405280	because you can't rank outputs in conversations but the chat gpt found a way to rank outputs by
3405280	3412720	paying somebody to label them with a score of one two three or four points so four points means it's
3412720	3421920	a good output and one point means it's a bad output and i believe that the reason why chat gpt very
3421920	3429600	often says honestly i don't know the answer to your question so it didn't say i have no idea what
3429600	3436000	the orna lock is it told me to think about the orna lock shaft which has all kinds of rivers running
3436000	3441920	through and doesn't exist why did it go to that trouble of giving me all this tourist information
3441920	3451040	because the labelers poor things think that a response who says i don't know what you want
3452080	3459680	i'm sorry is less reward worthy than a long list of tourism information about a non-existing
3459680	3464000	village because it's a long list of tourist information that must be worth four points
3464000	3473200	and so chat gpt basically is being bribed by the labelers to reward long outputs which are
3474000	3480320	kind and gentle and so this is why it throws up so much rubbish the machine will always try to
3480320	3485280	have nice friendly output but the machine can play response prompt response prompt response
3485280	3490160	prompt response games with itself billions of times every day for weeks and that that is what
3490160	3495760	it did and it cost a lot of money to do all that training and then it can give answers that's how
3495760	3498720	it does it and so the lesson
