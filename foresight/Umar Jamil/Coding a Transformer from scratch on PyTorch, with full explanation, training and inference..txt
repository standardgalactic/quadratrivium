Hello guys, welcome to another episode about the transformer.
In this episode, we will be building the transformer from scratch using PyTorque, so coding it
from zero.
We will be building the model and we will also build the code for training it for inferencing
and for visualizing the attention scores.
Stick with me because it's going to be a long video, but I assure you that by the end of
the video you will have a deep knowledge of the transformer model, not only from a conceptual
point of view, but also from a practical point of view.
We will be building a translation model, which means that our model will be able to translate
from one language to another.
I chose a dataset that is called Opus Books and it's a synthesis taken from famous books.
I chose the English to Italian because I am Italian so I can understand and I can tell
that if the translation is good or not.
But I will show you which point you can change the language so you can test the same model
with the language of your choice.
Let's get started.
Let's open the IDE of our choice.
In my case I really love Visual Studio Code and let's create our first file which is
the model of the transformer.
Okay, let's go have a look at the transformer model first so we know which one part we are
going to build first and then we will build each part one by one.
The first part that we will be building is the input embeddings.
As you can see, the input embeddings take the input and convert into an embedding.
What is the input embedding?
As you remember from my previous video, the input embeddings allows to convert the original
sentence into a vector of 512 dimensions.
For example, in this sentence, your cat is a lovely cat.
First we convert the sentence into a list of input IDs, that is numbers that correspond
to the position of each word inside the vocabulary.
And then each of these numbers corresponds to an embedding which is a vector of size
512.
So let's build this layer first.
The first thing we need to do is to import Torch.
And then we need to create our class.
This is the constructor.
We will need to tell him what is the dimension of the model, so the dimension of the vector.
In the paper, this is called the model.
And we also need to tell him what is the vocabulary size.
So how many words there are in the vocabulary.
Save these two values.
And now we can create the actual embedding.
Okay, actually PyTorch already provides with a layer that does exactly what we want to
do.
That is taken, given a number, it will provide you with the same vector every time.
And this is exactly what embedding does, it's just a mapping between numbers and a vector
of size 512.
512 here in this other case is the D model.
So this is done by the embedding layer and end dot embedding and workup size and D model.
Let me check why my autocomplete is not working.
Okay, so now let's implement the forward method.
What we do in the embedding is that we just use the embedding layer provided by PyTorch
to do this mapping.
So return self dot embedding of X.
Now actually there is a little detail that is written on the paper.
That is, let's have a look at the paper actually.
Let's go here and if we check the embedding and softmax, we will see that in this sentence
in the embedding layer, we multiply the weights of the embedding by square root of D model.
So what the authors do, they take the embedding given by this embedding layer, which I remind
you is just a dictionary kind of layer that just maps numbers to the same vector every
time and this vector is learned by the model.
So we just multiply this by math dot sqrt of D model, you also need to import math.
Okay, now the embedding, the input embeddings are ready.
Let's go to the next module.
The next module we are going to build is the positional encoding.
Let's have also a look at what are the positional encoding very fast.
So we saw before that our original sentence gets mapped to a list of vectors by the embedding,
the embeddings layer.
And this is our embeddings.
Now we want to do, we want to convey to the model the information about the position
of each word inside the sentence and this is done by adding another vector of the same
size as the embedding, so of size 512, that includes some special values given by a formula
that I will show later, that tells the model that this particular word occupies this position
in the sentence.
So we will create these vectors called position embedding and we will add them to the embedding.
Okay, let's go do it.
Okay, let's define the class positional encoding.
And we define the constructor.
Okay, what we need to give to the constructor is for sure the demodel because this is the
size of the vector that the positional encoding should be.
And the sequence length, this is the maximum length of the sentence and because we need
to create one vector for each position.
And we also need to give the dropout.
Dropout is to make the model less overfit.
Okay, let's actually build the positional encoding.
First of all, the positional encoding is, we will build a matrix of shape sequence length
to demodel.
Why sequence length to demodel?
Because we need vectors of demodel size, so 512, but we need sequence length number
of them because the maximum length of the sentence is sequence length.
So let's do it.
Okay, before we create the matrix and we know how to create the matrix, let's have a look
at the formula used to create the positional encoding.
So let's go have a look at the formula used to create the positional encoding.
This is the slide from my previous video and let's have a look at how to build the vectors.
So as you remember, we have a sentence, let's say in this case we have three words.
We use these two formulas taken from the paper.
We create a vector of size 512 and one for each possible position, so up to sequence length.
And in the even positions, we apply the first formula in the odd positions of the vector,
we apply the second formula.
In this case, I will actually simplify the calculation because I saw online it has been
simplified also.
So we will do a slightly modified calculation using log space.
This is for numerical stability.
So when you apply the exponential and then the log of something inside the exponential,
the result is the same number, but it's more numerically stable.
So first we create a vector called the position that will represent the position of the word
inside the sentence.
And this vector can grow from 0 to sequence length minus 1.
So actually we are creating a tensor of shape sequence length to 1.
Okay, now we create the denominator of the formula.
And these are the two terms we see inside the formula.
Let's go back to the slide.
So the first tensor that we built that's called position is this pause here.
And the second tensor that we built is the denominator here, but we calculated it in
log space for numerical stability.
The value actually will be slightly different, but the result will be the same.
The model will learn this positional encoding.
Don't worry if you don't fully understand this part, it's just very special, let's say
functions that convey this positional information to the model.
And if you watch my previous video, you will also understand why.
Now we apply this to the denominator and denominator to the sign and the cosine.
As you remember, the sign is only used for the even positions and the cosine only for
the odd position, so we will apply it twice.
Let's do it.
So apply.
So every position will have the sign, but only, so every word will have the sign, but
only the even dimensions.
So starting from zero up to the end and going forward by two means every from zero, then
the number two, then the number four, etc., etc.
Position multiplied by tip term.
Then we do the same for the cosine.
In this case, we start from one and go forward by two.
It means one, three, five, etc.
And then we need to add the batch dimension to this tensor so that we can apply it to the
whole sentences, so to all the batch of sentences, because now the shape is sequence length to
demodel, but we will have a batch of sentences.
So what we do is we add a new dimension to this PE, and this is done using unsquiz and
in the first position.
So it will become a tensor of shape one to sequence length to demodel.
And finally, we can register this tensor in the buffer of this module.
So what is the buffer of the module?
Let's first do it.
So basically, when you have a tensor that you want to keep inside the module, not as
a parameter, a learned parameter, but you want it to be saved when you save the file
of the module, you should register it as a buffer.
This way, the tensor will be saved in the file along with the state of the module.
Then we do the forward method.
As you remember from before, we need to add this positional encoding to every word inside
the sentence.
So let's do it.
So we just do x is equal to x plus the positional encoding for this particular sentence.
And we also tell the module that we don't want to learn this positional encoding because
they are fixed.
They will always be the same.
They are not learned along the training process.
So we just do it requires grad false.
This will make this particular tensor not learned.
And then we apply the dropout.
And that's it.
This is the positional encoding.
Let's have a look at the next module.
Okay, we first we will build the encoder part of the transformer, which is this left side
here.
And we still have the multi hat attention to build the add and norm and the feed forward.
And actually, there is another layer which connects this skip connection to all these
sub layers.
So let's start with the easiest one.
Let's start with layer normalization, which is this add and norm.
As you remember from my previous video, let's have a look at the layer normalization, a
little briefing.
So later normalization basically means that if you have a batch of n items, in this case
only three, each item will have some features.
Let's say that these are actually sentences and each sentence is made up of many words
with its numbers.
So this is our three items.
And later normalization means that we for each item in this batch, we calculate a mean
and a variance independently from the other items of the batch.
And then we calculate the new values for each of them using their own mean and their own
variance.
In the layer normalization, usually we also introduce some parameters called the gamma
and the beta.
Some people call it alpha and beta, some people call it alpha and bias, okay, it doesn't matter.
One is multiplicative, so it's multiplied by each of these x and one is additive.
So it's added to each one of these x, y.
Because we want the model to have the possibility to amplify these values when he needs this
value to be amplified.
So the model will learn to multiply this gamma by these values in such a way to amplify
the values that it wants to be amplified.
Okay, let's go to build the code for this layer.
Let's define the layer normalization class and the constructor as usual.
In this case, we don't need any parameter except for one that I will show you now, which
is epsilon.
And usually EPS stands for epsilon, which is a very small number that you need to give
to the model.
And I will also show you why we need this number.
In this case, we use 10 to the power of minus six.
Let's save it.
Okay, this epsilon is needed because if we look at the slide, we have this epsilon here
in the denominator of this formula here.
So x with cap is equal to xj minus mu divided by the square root of sigma square plus epsilon.
Why we need this epsilon?
Because imagine this denominator, if sigma happens to be zero or very close to zero,
this x new will become very big, which is undesirable.
As we know that the CPU or the GPU can only represent numbers up to a certain position
and scale.
So we don't want very big numbers or very small numbers.
So usually for numerical stability, we use this epsilon also to avoid division by zero.
Let's go forward.
So now let's introduce the two parameters that we will use for the layer normalization.
One is called alpha, which will be multiplied and one is bias, which will be added.
Usually the additive is called bias.
It's always added.
And the alpha is the one that is multiplied.
In this case, we will use nn.parameter.
This makes the parameter learnable and we define also the bias.
This I want to remind you is multiplied and this is added.
Let's define the forward.
Okay, as you remember, we need to calculate the mean and the standard deviation or the
variance for both of these.
We will calculate the standard deviation of the last dimension.
So everything after the batch and we keep the dimension.
So this parameter keep dimension means that usually the mean cancels the dimension to
which it is applied, but we want to keep it.
And then we just apply the formula that we saw on the slide.
So alpha multiplied by what?
X minus its mean divided by the standard deviation plus self dot pps.
Everything added to bias.
And this is our layer normalization.
Okay, let's go have a look at the next layer we are going to build.
The next layer we are going to build is the feed forward.
You can see here and the feed forward is basically a fully connected layer.
That the model uses both in the encoder and in the decoder.
Let's first have a look at the paper to see what are the details of this feed forward
layer.
In the paper, the feed forward layer is basically two matrices, one w1, one w2 that are multiplied
by this X, one after another with a relu in between and with the bias.
We can do this in PyTorch using a linear layer in which we define the first one to be the
matrix with the w1 and the b1 and the second one to be the w2 and the b2 and in between
we apply a relu.
In the paper we can also see the dimensions of these matrices.
So the first one is basically D-model to DFF and the second one is from DFF to D-model.
So DFF is 2048 and D-model is 512.
Let's go build it, class feed forward block.
We also build in this case the constructor and in the constructor we need to define these
two values that we saw on the paper so D-model, DFF and also in this case drop out.
We define the first matrix so w1 and b1 to be the linear one and it's from D-model to
DFF and then we apply the drop out actually we define the drop out
and then we define the second matrix w2 and b2.
So let me write the comments here it's w1 and b1 of DFF to D-model and this is w2
and b2.
Why we have b2 because actually as you can see here bias is by default it's true so it's
already defining a bias matrix for us.
Okay let's define the forward method.
In this case what we are going to do is we have an input sentence which is batch it's
a tensor with dimension batch, sequence, length and D-model.
First we will convert it using linear one into another tensor of batch to sequence length
to DFF because if we apply this linear it will convert the D-model into DFF and then
we apply the linear to which will convert it back to D-model.
We apply the drop out in between and this is our feed forward block.
Let's go have a look at the next block.
Our next block is the most important and most interesting one and it's the multi-head attention.
We saw briefly in the last video how the multi-head attention works so I will open now the slide
again to show to rehearse how it actually works and then we will do it practically by
coding.
As you remember in the encoder we have the multi-head attention that takes the input
of the encoder and uses it three times.
One time it's called query, one time it's called key and one time it's called values.
You can also think it like a duplication of the input three times or you can just say
that it's the same input applied three times.
And the multi-head attention basically works like this.
We have our input sequence which is sequence length by D-model.
We transform into three matrices Q, K and V which are exactly the same as the input
in this case because we are talking about the encoder.
We will see that in the decoder it's slightly different.
And then we multiply this by matrices called W, Q, W, K and WV and this results in a new
matrix of dimension sequence by D-model.
We then split these matrices into H matrices, smaller matrices, why H?
Because it's the number of head we want for this multi-head attention.
And we split these matrices along the embedding dimension not along the sequence dimension
which means that each head we will have access to the full sentence but a different part
of the embedding of each word.
We apply the attention to each of these smaller matrices using this formula which will give
us smaller matrices as a result.
Then we combine them back so we concatenate them back just like the paper says so concatenation
of head 1 up to head H and finally we multiply it by WO to get the multi-head attention output
which again is a matrix that has the same dimension as the input matrix.
As you can see it's the output of the multi-head attention is also sequenced by D-model.
In this slide actually I didn't show the batch dimension because we are talking about
one sentence but when we code the transformer we don't work only with one sentence but with
multiple sentences so we need to think that we have another dimension here which is the
batch.
Okay, let's go to code this multi-head attention.
I will do it a little more slower so we can see in detail everything how it's done but
I really wanted you to have an overview again of how it works and why we are doing what
we are doing.
So let's go code it.
Class.
Also in this case we define the constructor and what we need to give to this multi-head
attention as parameter for sure the D-model of the model which is in our case 512.
The number of heads which we call H just like in the paper.
So H indicates the number of heads we want and then the dropout value.
We save these values.
As you can see we need to divide this embedding vector into H heads which means that this
D-model should be divisible by H otherwise we cannot divide equally the same dimension
vector representing the embedding into equal matrices for each head.
So we make sure that D-model is divisible by H basically.
And this will make the check.
If we watch again my slide we can see that the value D-model divided by H is called DK.
As we can see here if we divide the D-model by H heads we get a new value which is called
DK and to be aligned with what the paper with the nomenclature used in the paper we will
also call it DK.
So DK is D-model divided by H.
Okay let's also define the matrices by which we will multiply the query the key and the
values and also the output matrix W also.
This again is a linear so from D-model to D-model.
Why from D-model to D-model because as you can see from my slide this is D-model by D-model
so that the output will be sequenced by D-model.
So this is WQ, this is WK and this is WV.
Finally we also have the output matrix which is called WO here.
This WO is H by DV by D-model.
So H by DV is what DV is actually equal to DK because it's the D-model divided by H.
But why it's called DV here and DK here because this head is actually the result this head
comes from this multiplication and the last multiplication is by V and in the paper they
call this value DV but on a practical level it's equal to DK.
So our WO is also a matrix that is D-model by D-model because H by DV is equal to D-model.
And this is WO.
Finally we create the dropout.
Let's implement the forward method and let's see how the multi head attention works in
detail during the coding process.
We define the query, the key and the values and there is this mask.
So what is this mask?
The mask is basically if we want some words to not interact with other words we mask them.
And we saw in my previous video but now let's go back to those slides to see what is the
mask doing.
As you remember when we calculate the attention are using this formula so softmax of Q multiplied
by KT divided by square root of DK and then by V we get this head matrix.
But before we multiply by V so only this multiplication here with Q by K we get this
matrix which is each word with each other word.
It's a sequence by sequence matrix and if we don't want some words to interact with
other words we basically replace their value so their attention score with something that
is very small before we apply the softmax and when we apply the softmax these values
will become zero because as you remember the softmax on the numerator has e to the power
of x so if x goes to minus infinity so very small number e to the power of minus infinity
will become very small so very close to zero.
So basically we hide the attention for those two words so this is the job of the mask.
Just following my slide we do the multiplication one by one so as we remember we calculate
first the query are multiplied by the WQ so self dot WQ multiplied with the query gives
us a new matrix which is called the Q prime in my slides I just call it query here.
We do the same with the keys and the same with the values let me also write the dimensions
so we are going from batch sequence length to demodel with this multiplication we are
going to another matrix which is batch sequence length and demodel and you can see that from
the slides so when we do sequence by demodel multiplied by demodel by demodel we get
a new matrix which has the same dimension as the initial matrix so sequence by demodel
and it's the same for all three of them.
Now what we want to do is to we want to divide this query key and value into smaller matrices
so that we can give each small matrix to a different head so let's do it we will divide
into using the view method of PyTorch which means that we keep the batch dimension because
we don't want to split the sentence we want to split the embedding into H parts.
We also want to keep the second dimension which is the sequence because we don't want
to split it and the third dimension so the demodel we want to split it into two smaller
dimension which is H by DK so self dot H self dot DK as you remember DK is basically demodel
by divide by H so this multiplied by this gives you back gives you a demodel and then
we transpose one two why do we transpose because we prefer to have the H dimension instead
of being the third dimension we want it to be the second dimension and this way each
view H head we will see the all the sentence so we'll see this dimension so the sequence
length by DK let me also write the comment here so we are going from batch sequence length
demodel to batch sequence length H DK and then by using the transposition we are going
to batch H sequence length and DK this is really important because we will we want we
want each batch have we want each head to watch this stuff so the sequence length by DK which
means that each head we will see the full sentence so each word in the sentence but
only a smaller part of the embedding we do the same thing for the key and the value
okay now that we have the
so let me go back to the slide so I can show you where we are so we did this multiplication
we obtained query key and values we split into smaller matrices now we need to calculate
the attention using this formula here before we can calculate the attention let's create
a function to calculate the attention so if we create a new function that can be used
also later so self attention let's define it as a static method so static method means
basically that you can call this function without having an instance of this class you
can just say multi head attention block dot attention instead of having an instance of
this class we also give him the dropout layer okay what we do is we get the DK what is the
DK is the last dimension of the query key and the value and we will using this function
here let me first call it so that you can understand how we will use it and then we
redefine it so we want from this function we want to think the output and we want the
attention scores so the output of the softmax attention scores and we will call it like
this so we give it the query the key the value the mask and the dropout layer now let's go
back here so we have the DK now what we do is the first we apply the first part of the
formula that is the query multiplied by the transpose of the key divided by the square
root of DK so these are our attention scores query matrix multiplication so this add sign
means matrix multiplication in pytorch which transpose the last two dimensions minus two
minus one means transpose the last two dimensions so this will become the last dimension is a
sequence by sequence length by DK will become DK by sequence length and then we divide this
by math dot DK we before as we saw before before applying the softmax we need to apply
the mask so we want to hide some interaction between words we apply the mask and then we
apply the softmax so the softmax will take care of the values that we replaced how do
we apply the mask we just all the values that we want to mask we replace them with very very
small values so that the softmax will replace them with 0 so if a mask is defined apply it
basically replace all the values for which this statement is true with this value okay the mask
we will define in such a way that where this value this expression is true if we want it to
be replaced by this later we will see also how we will build the mask for now just take it for
granted that these are all the values that we don't want to have in the attention so we don't
want for example some word to watch future words for example when we will build a decoder or we
don't want the padding values to to interact with other values because they are just filler words
to reach the sequence length we will replace them with minus 1 to the power of minus 10 to the
power of 9 and which is a very big number in the negative range and which basically represents
minus infinity and then when we apply now the softmax it will be replaced by 0
we applied to this dimension okay let me write some comments so in this case we have a batch
by H so each head will and then sequence length and sequence length all right if we also have a
dropout so if dropout is not known we also apply the dropout and finally as we saw in the original
slide we multiply this the output of the softmax by the V matrix matrix multiplication so we return
attention scores multiplied by value and also the attention score itself so why are we returning
a tuple because we want this of course we need it because for the model because we need to give
it to the next layer but this will be used for visualization so the output of the the
self self attention so the multi head attention in this case is actually going to be here and
we will use it for visualizing so for visualizing what is the score given by the model for that
particular interaction let me also write some comments here so here we are doing like this batch
and let's go back here so now we have our multi head attention so the output of the multi
head attention what we do is finally we okay let's go back to the slide first where we are we
calculated this smaller matrices here so we applied the softmax q by kt divided by the square root
of dv and then we multiplied it also by v we can see it here which gives us this small matrix here
head 1 head 2 head 3 and 3 and 4 now we need to combine them together concat just like the formula
says from the paper and finally multiplied by wo so let's do it we transpose because before we
transform the matrix into sequence length by we had the sequence length as the third dimension
we wanted back in the first place to combine them because the resulting tensor we want the
sequence length to be in the second position so let me write it first what we want to do batch
we started from this one sequence length
first we do a transposition
and then what we want is this
so this transposition takes us here and then we we do a view but we cannot do it we need to use
contiguous this means basically that pytorch to to transform the shape of a tensor needs to
we put the memory to be contiguous so he can just do it in place
minus one and the self dot h multiplied by self dot dk which as you remember this is the
demodel because we defined dk to be here demodel by h divide by h
okay and finally we multiply this x by wo which is our output matrix
of x this will give us we go from batch
to
and this is and this is our multi had attention block we have I think all the ingredients now
to combine them all together we just miss one small layer let's go have a look at it first
there is one last layer we need to build which is the connection we can see here for example here
we have some output of this layer so add a norm that is taken here with this connection
and this one part is sent here then the output of this is sent to the add a norm and then combined
together by this layer so we need to create this the layer that manages this skip connection so
we take the input we give it to we skip it by one layer we take the output of the previous layer
so in this case the multi had attention we give it to this layer but also combining with this part
so let's build this layer we I will call it residual connection because it's basically a
skip connection okay let's build this residual connection as usual we define the constructor
and in this case we just need a dropout
as you remember the the skip connection is between the add and the norm and the previous layer so
we also need a norm which is our layer normalization which we defined before and then we define the
forward method so and the sublayer which is the previous layer
what we do is we take the x and we combine it with the output of the next layer which is in this
case is called sublayer and we apply the dropout
so this is the definition of add and a norm actually there is a slight difference that
we first apply the normalization and then we apply the sublayer in the case of the paper they
apply first the sublayer and then the normalization I saw many implementation and most of them actually
did it like this so we will also stick with this particular as you remember we have these blocks
are combined together by this bigger block here and we have n of them so this big block
we will call it encoder block and each of this encoder block is repeated n times where the output
of the previous is sent to the next one and the output of the last one is sent to the decoder
so we need to create this block which will contain one multi head attention to add and
the norm and one feed forward so let's do it
we will call this block the encoder block because the decoder has three blocks inside the encoder
has only two
and as I so before we have the self-attention block inside which is the multi head attention
we call it self-attention because in the case of the encoder it is applied to the same input
with three different roles the role of query of the key and the value
which is our feed forward and then we have a dropout which is a floating point and then we define
the two residual connections
we use the model list which is a way to organize a list of modules in this case we need two of them
okay let's define the forward method
I define the source mask what is the source mask is the mask that we want to apply to the input
of the encoder and why do we need a mask for the input of the encoder because they we want to hide
the interaction of the padding word with other words we don't want the padding word to interact
with other words so we apply the mask and let's do the first residual connection let's go back
to check the video actually to check the slide so we can understand what we are doing now so
the first skip connection is this x here is going to here but before it's added and with add a norm
we first need to apply the multi head attention so we take this x we send it to the multi head
attention and at the same time we also send it here and then we combine the two
so the first skip connection is between x and then the other x is coming from the self-attention
so this is the function so I will define the sublayer uh usually using a lambda so this basically
means first apply the self-attention self-attention in which we give the query key and the value is
our x so our input so this is why it's called self-attention because the role of the query key
and the value is x itself so the input itself so it's the sentence that is watching itself so each
word of one sentence is interacting with other words of the same sentence we will see that in
the decoder it's different because we have the cross attention so the keys coming from the decoder
are watching the sorry the query coming from the decoder are watching the key and the values coming
from the encoder we give it the source mask so what is this basically we are calling this function
the forward function of the multi head attention block so we give query key value and the mask
this will be combined with this by using the residual connection
then again we do the second one the second one is the feed forward
we don't need lambda here actually
and then we return x so this means combine the the feed forward and then the x itself
so the output of the previous layer which is this one and then apply the residual connection
and this defines our encoder block now we can define the encoder object so because the encoder
is made up of many encoder blocks we can have up to n of them according to the paper so let's
define the encoder how many layers we will have we will have n so we have many layers
and they are applied in one after another so this is a model list
and at the end we will apply a layer normalization
so we apply one layer after another
the output of the previous layer becomes the input for the next layer here I forgot something
and finally we apply the normalization and this concludes our journey around the encoder
let's go have a brief overview of what we have done we have taken the inputs send it to the
we didn't okay we didn't combine all the blocks together for now we just built this big block
here control called encoder which contains two smaller blocks that are the skip connection
the skip connection first one is between the multi head attention and this x that is sent here
the second one is between this field forward and this x that is sent here we have n of these
blocks one after another the output of the last will be sent to the decoder before but before
we apply the normalization now we will we built the the the decoder part now in the decoder the
output embeddings are the same as the input embeddings I mean the the the class that we
need to define is the same so we will just initialize it twice and the same goes for the
positional encodings we can use the same values that we use for the encoder also for the decoder
what we need to define is this big block here which is made of basket multi head attention
add a norm so one skip connection here another multi head attention with another skip connection
and the field forward with the skip connection here the way we defined the multi head attention
class actually already takes into consideration the mask so we don't need to reinvent the wheel
also for the decoder we can just define the decoder block which is this big block here made
of three sub layers and then we built the decoder using this n number of these decoder blocks so
let's do it let's define first the decoder block
in the decoder we have the self-attention which is let's go back this is a self-attention because
we have this input that is used three times in the basket multi head attention so this is called
self-attention because the same input plays the role of the query the key and the values
which means that the same sentence is each word in the sentence is matched with each other word
in the same sentence but in this part here we will have an attention calculated using the
query coming from the decoder while the key and the values will come from the encoder so this is
not a self-attention this is called cross-attention because we are crossing two kind of different
objects together and matching them somehow to calculate the relationship between them okay let's
define this is the cross-attention block which is basically the multi head attention but we will
give it the different parameters is our feed forward and then we have a dropout
okay we define also the residual connection in this case we have three of them
okay let's build the forward method which is very similar to the encoder with a slight difference
that i will highlight we need x what is x is the input of the decoder but we also need the
output of the encoder we need the source mask which is the mask applied to the encoder and the
target mask which is the mask applied to the decoder why they are called source mask and target mask
because in this particular case we are dealing with a translation task so we have a source language
in this case it's english and we have a target language which in our case is italian so you can
call it encoder mask or decoder mask but basically we have two masks one is the one coming from the
encoder one is the one coming from the decoder so in our case we will call it source so the source
mask is the one coming from the encoder so the source language and the target mask is the one
coming from the decoder so the target language and just like before we calculate the self-attention
first which is the first part of the decoder block in which the query the key and the values
are the same input but with the mask of the decoder because this is the self-attention block
of the decoder and then we need to combine the we need to calculate the cross-attention which
is our second residual connection we give him okay in this case we are giving the query coming
from the decoder so the x the key and the values coming from the encoder and the mask of the encoder
and finally the feed forward block just like before
and that's it we have all the ingredients actually to build the decoder now which is just n times this
block one after another just like we did for the encoder
also in this case we will provide with many layers so layers
it's just a model list and we will also have a normalization at the end
just like before we apply the the input to the to one layer and then we use they use the
output of the previous layer and give it as a input of the next layer so
the each layer is the decoder block so we need to give it x we need to give it the encoder output
than the source mask and the target mask so each of them is this we are calling the forward
method here so nothing different and finally we apply the normalization and this is our
decoder there is one last ingredient we need to to have what is a full transformer so let's have
a look at it the last ingredient we need is this layer here the linear layer so as you remember
from my slides the output of the multi head attention is something that is sequenced by
D model so here we expect to have the output to be sequenced by D model if we don't consider
the batch dimension however we want to map this word into the back into the vocabulary
so that's why we need this linear layer which will convert the embedding into a position of the
vocabulary i will call this layer called the projection layer because it's projecting the
embedding into the vocabulary let's go build it what we need for this layer is the D model
so the D model which is an integer and the vocabulary size
but this is basically a linear layer that is converting from D model to vocabulary size so
thought projection layer is
let's define the forward method
okay what we want to do let me write this little comment we want to batch
sequence land to D model converted into batch sequence land vocabulary size
and in this case we will also already apply the softmax and actually we will apply the log
softmax for numerical stability like i showed before to the last dimension
that's it this is our projection layer now we have all the ingredients we need for the
transformer so let's define our transformer block
in a transformer we have an encoder
which is our encoder we have a decoder which is our decoder we have a source embedding
why we need the source embedding at the target embedding because we are dealing with multiple
languages so we have one input embedding for the source language and one input embedding for the
target language and we have the target embedding then we have the source position and the target
position which will be the same actually and then we have the projection layer
we just serve this
now we define three methods one to encode one to the code and one to project
we will apply them in succession why why we don't just build one forward method because
as we will see during inferencing we can reuse the output of the encoder we don't need to
calculate it every time and also we prefer to keep the this output separate also for visualizing the
attention so for the encoder we have the source of the because we have the source language and the
source mess so the so what we do is we apply first the embedding
then we apply the positional encoding
and finally we apply the encoder
then we define the decode method
which takes the encoder output which is the tensor
the source mask which is the tensor the target and the target mask
and what we do is target we first apply the target embedding to the target sentence
then we apply the positional encoding to the target sentence
and finally with the code
this is basically the method the forward method of this decoder so we have the same order of
parameters yes finally we define the project method
in which we just apply the projection so we take from the embedding to the vocabulary size
okay this is also the this is the last block we had to build
but we didn't make a method to combine all these blocks together so we built many blocks
we need one that given the hyperparameters of the transformer builds for us one single
transformer in initializing all the encoder the encoder the embeddings etc so let's build
this function let's call it build transformer that given all the hyperparameters will build
the transformer for us and also initialize the parameters with some initial values
what we need to define a transformer for sure in this case we are talking about translation
okay this model that we are building we will be using for translation but you can use it for any
task so the naming I'm using are basically the ones used in the translation task later you can
change the naming but the structure is the same so you can use it for any other task for which the
transformer is applicable so the first thing we need is the vocabulary size of the source and the
target because we need to build the embedding the the embedding because the embedding need to convert
the from the the token of the vocabulary into a vector of size 512 so it needs to know how much
how big is the vocabulary so how many vectors it needs to create
then the target
which is also an integer then we need to tell him what is the source sequence length and the target
sequence length this is very important they could also be the same in our case it will be the same
but they can also be different for example in case you are using the transformer that is dealing
with the two very different languages for example for translation in which the tokens needed for the
source languages language are much higher or much lower than the other one so you don't need to keep
the same length you can use different lengths the next hyperparameter is the demodel
which we initialize with 512 because we want to keep the same values as the paper then we
define the hyperparameter n which is the number of layers so the number of encoder blocks and the
number of decoder blocks that we will be using is according to the paper is 6 then we define the
hyperparameter h which is the number of heads we want and according to the paper it is 8
the dropout is 0.1
and finally we have the hidden layer dff of the feedforward layer which is 2048 as we saw before
on the paper and this builds a transformer okay so first we do is we create the embedding layers
so source embedding
then the target embedding
then we create the positional encoding layers
we don't need to create two positional encoding layers because actually they
do the same job they and we they also don't add any parameter but because they have the dropout
and also because i want to make it verbal so you can understand each part without making any
optimization i think actually it's it's fine because this is for educational purpose so
i don't want to optimize the code i want to make it as much comprehensible as possible
so i do every part i need i don't take shortcuts
and then we created the encoder blocks we have n of them so let's define
let's create an empty array so
we have n of them so each encoder block has self-attention
which is a multi head attention block the multi head attention requires the demodel
the edge and the dropout value then we have a feed forward block
as you can see also the name i'm using are quite long mostly because i want to make it
as comprehensible as possible for everyone
so each encoder block is made of a self-attention
and a feed forward
and finally we tell him how much is the dropout
finally we add this encoder block
and then we can create the decoder blocks
we also have the cross attention for the decoder block
we also have the feed forward just like the encoder
then we define the decoder block itself which is the decoder block
cross attention and finally the feed forward
and the dropout
and finally we save it in its array
we now can create the encoder and the decoder
we give him all his blocks which are n and then also the decoder
and we create the projection layer
which will convert the model into vocabulary size which vocabulary of course the target because
we want to take from the source language to the target language so we want to project our
output into the target vocabulary and then we build the transformer
what does it need it needs an encoder a decoder source embedding
target embedding
then source positional encoding target positional encoding
and finally the projection layer
and that's it now we can just initialize the parameters
using the Xavier uniform this is a way to initialize the parameters to make the training
faster so they don't don't just start with random values
and there are many algorithms to do it I saw many implementations using Xavier so I think it's a
quite good start for the model to learn from
and finally return our beloved transformer and this is it this is how you build the model
and now that we have built the model we will go further to use it so we will create the
we will first have a look at the dataset then we will build the training loop after the training
loop we will also build the inferencing part and the code for visualizing the attention
so hold on and take some coffee take some tea because it's going to be a little long
but it's going to be worth it now that we have built the the code for the model our next step
is to build the training code but before we do that we first I let's recheck the code because
we may have some typos I actually already made this check and there are few mistakes in the code
I compare the old with the new one it is very minor problems so we wrote feed forward instead of
feed forward here and so the same problem is also present every in every reference to feed
forward and also here when we are building the decoder block and the other problem is that here
when we build the decoder block we just wrote nn.module instead it should be nn.module list
and then the feed forward should be also fixed here and here in the build the transformer method
now I can delete the old one so we don't need it anymore let me check the model
it's the correct one with feed forward yes okay our next step is to build the training code
but before we build the training code we have to look at the data what kind of data are we going
to work with so as I said before we are dealing with a translation task and I have chosen this
data set called opus books which we can find on hugging face and we will also use the library
from hugging face to download this data set for us and this is the only library we will be using
beside the pytorch because we of course we cannot reinvent the data set by ourselves so we will use
this data set and we will also use the hugging face tokenizer library to transform this text
into a vocabulary because our the our goal is to build the transformer so not to reinvent the
wheel about everything so we will be only focusing on building and training the transformer and in my
particular case I will be using the subset english to Italian but we will build the code in such a
way that you can choose the language and the code will act accordingly if we look at the data we
can see that each data item is a pair of sentences in english and in italian for example there was
no possibility of taking a walk that day which in italian means in quel giorno era impossibile
passeggiare so we will train our transformer to translate from the source language which is
english into the target language which is italian so let's do it we will do it step by step so first
we will make the code to download this data set and to create the tokenizer so what is the tokenizer
let's go back to the slides so just have a brief overview of what we are going to do with this data
the tokenizer is what comes before the input embeddings so we have an english sentence so for
example your cat is a lovely cat but this sentence will come from our data set the goal of the
tokenizer is to create this token so split this sentence into single words which has many strategies
as you can see here we have a sentence which is your cat is a lovely cat and the goal of the
tokenizer is to split this sentence into single words which can be done in many ways there is the
bpe tokenizer there is the word level tokenizer there is the sub word level word part tokenizer
there are many tokenizers the one we will be using is the simplest one called the word level
tokenizer so the word level tokenizer basically will split this sentence let's say by space so
each space defines the boundary of a word and so into the single words and each word will be mapped
to one number so this is the job of the tokenizer to build the vocabulary and of these numbers and
to map each word into a number the when we build the tokenizer we can also create special tokens
which we will use for the transformer for example the tokens called padding they call the token called
the start of sentence end of sentence which are necessary for training the transformer but we
will do it step by step so let's build first the code for the building the tokenizer and to
download the data set okay let's create a new file let's call it train dot pi okay let's import our
usual library so torch we will also import torch dot nm and we also because we we are using a library
from hugging face we also need to import the these two libraries we will using the we will be using
the data sets library which you can install using pip so data sets actually we will be using load
data set and we will also using we'll be using the tokenizer's library also from hugging face
which you can install with pip we also need the which tokenizer we need so
we are using we will use the word level tokenizer
and there is also the trainers so the the the tokenizer the the class that will train the
tokenizer so that will create the vocabulary given the list of sentences
and we will split the word according to the white space
i will build one method by one by one so i will build first the methods to create the tokenizer
and i will describe each parameter for now you will not have the bigger picture but later when
we combine all these methods together you will have the bigger picture so let's first make the
method that builds the tokenizer so we will call it get or build tokenizer
and this method takes the configuration which is the configuration of our model we will
define it later the data set and the language for which we are going to build the tokenizer
we define the tokenizer path so the file where we will be saved this tokenizer
and we do it path of config
okay let me define some things first of all this path is coming from the pathlib so from pathlib
this is a library that allows you to create absolute path given relative paths and we pretend
where that we have a configuration called the tokenizer file which is the path to the tokenizer
file and this path is formatable using the language so for example we can have something like this
for example
something like this and this will be
given the language it will create a tokenizer english or tokenizer italian for example
so if the tokenizer doesn't exist we create it
i took all this code actually from hugging phase there is it's nothing complicated i just
taken their quick tour of their tokenizers library and it's really easy to use it so
and saves you a lot of time because tokenizer to build the tokenizer it's really reinventing the wheel
and we will also introduce the unknown word unknown so what does it mean if our tokenizer
sees a word that it doesn't recognize in its vocabulary it will replace it with this word
unknown it will map it to the number corresponding to this word unknown
the pre tokenizer means basically that we split by white space
and then we train we build the trainer to train our tokenizer
okay this is the trainer what does it mean it means it will be a word level trainer so
it will split words using the white space and using the single words and it will also
have four special tokens one is unknown which means that if you cannot find that particular
word in the vocabulary just replace it with unknown it will also have the padding which we
will use to train the to train the transformer the start of sentence and the end of sentence
special tokens mean frequency means that a word for a word to appear in our vocabulary it has to
have a frequency of at least two now we can train the tokenizer
we use this method which means we build first a method that gives all the sentences from our
data set and we will build it later
okay so let's build also this method called get all sentence so that we can iterate through
the data set to get all the sentences corresponding to the part the particular language for which we
are creating the tokenizer as you remember each item in the data set it's a pair of sentences
one in English one in Italian we just want to extract one particular language
this is the item representing the pair and from this pair we extract only the one language that
we want and this is the code to build the tokenizer now let's write the code to load the data set
and then to build the tokenizer we will call this method get data set and which also takes
the configuration of the model which we will define later so let's load the data set we will call it
ds row okay hugging face allow us to download its data sets very easily we just need to tell him
what is the name of the data set and then tell him what is the subset we want we want the subset
that is English to Italian but we want to also make it configurable for you guys to change the
language very fast so let's build this subset dynamically we will have two parameters in the
configuration one is called language source and one is called language target later we can also
define what split we want of this data set in our case there is only the training split
in the original data set from hugging face but we will split by ourselves into the validation
and the training data so let's build the tokenizer
this is the row data set
and we also have the target
okay now because we only have the training split from hugging face we can split it by
by ourselves into a training and the validation we keep 90% of the data for training and 10% for
validation
the method random split allows it's a method from
by torch that allows to split a data set using the size that we give as input so in this case it
means split this data set into this two smaller data set one of this size and one of this size
but let's import the the the method from torch
let's also import the one that we will need later
and random split
now we need to create the data set the data set that our model will use to access the 10 source
directly because now we just created the tokenizer and we just loaded the data but we need to create
the 10 source that our model will use so let's create the data set let's call it um bilingual
data set and for that we create a new file also here we import torch and that's it
okay
we will call the data set we will call it bilingual data set
okay as usual we define the constructor
and in this constructor we need to give him the data set downloaded from hugging phase
the tokenizer of the source language the tokenizer of the target language the source language the name
of the source language the name of the target language and the sequence length that we will use
okay we save all these values
okay
we can also save the the tokens the particular tokens that we will use to create the
10 source for the model so we need the start of sentence end of sentence and the padding token
so how do we convert the token start of sentence
from into a number into the input ID there is a special method of the tokenizer to do that so
let's do it so this is the start of sentence token we want to build it into a tensor
this tensor will contain only one number which is given by we can use this tokenizer from the
source or the target it doesn't matter because they both contain these particular tokens
this is the method to convert into the token into a number
so start of sentence and the type of this token of this tensor is
we want it long because the vocabulary can be more than 32 bit long the vocabulary size so
usually use the long 64 bit and we do the same for the end of sentence and the padding token
we also need to define the length method of this dataset
which tells the length of the the dataset itself so
basically just the length of the dataset from hugging face and then we need to define the get item
method
okay first of all we will extract the original pair from the hugging face dataset
then we extract the source text and the target text
and finally we convert each text into token into tokens and then into input IDs
what does it mean we will first the tokenizer will first split the sentence into single words
and then we'll map each word into its corresponding number in the vocabulary and it will do it in one
pass only this is done by the encode method
.ids this gives us the input IDs so the numbers corresponding to each word in the original
sentence and it will be given as an array
we did the same for the decoder now as you remember we also need to pad the sentence
to reach the sequence length this is really important because we we want our model to always
always works with a fixed length sequence length but we don't have enough words in every sentence
so we use the padding token so this pad here as the padding token to fill the sentence until
it reaches the sequence length so we calculate how many padding tokens we need to add for the
encoder side and for the decoder side which is basically how many we need to reach the sequence
length minus two y minus two here so we already have this amount of tokens we need to reach this
one but we will add also the start of sentence token and the end of sentence token to this to
the encoder side so we also have minus two here and here only minus one if you remember my previous
video when we do the training we add only the start of sentence token to the decoder side and then
in the label we only add the end of sentence token so in this case we only need to add one token
special token to the sentence we also make sure that this sequence length that we have chosen
is enough to represent all the sentences in our dataset and if we chose a two small one we want
to throw a raise an exception so if so basically this number of padding tokens should never become
negative
okay now let's build the the two tensors for the encoder input and for the decoder input but also
for the label so one sentence will be sent to the input of the encoder one sentence will
be sent to the input of the decoder and one sentence is the one that we expect as the output
of the decoder and that output we will call label usually it's called target or label i call it label
we can cut the tensor of the start
okay we can cut three tensors first is this start of sentence token
then this the tokens of the source text
then the end of sentence token
and then enough padding tokens to reach the
sequence length we already calculated how many in padding tokens we need to add to this sentence
so let's just do it
okay
and this is the encoder input so let me write some comment here this is add s os and s to the source
text then we build the decoder input
which is also concatenation of tokens in this case we don't have the start of sentence we just have
the we don't have the end of sentence we just have the start of sentence
and finally we added enough padding tokens to reach the
sequence length we already calculated how many we need just use this value now and then we built
the label
in the label we only add the end of sentence token
let me copy it's faster
yeah because we need the same number of padding tokens as for the decoder input
and let's double just for debugging let's double check that we actually reach the sequence length
okay
okay now that we have made this check let me also write some comments here here we are only adding
eos now here s os to the decoder input and here is add eos to the label
what we expect as output from the decoder now we can return all these tensors so that our
training can use them we return a dictionary comprised of encoder input
what is the encoder input it's basically off-size sequence length then we have the the decoder input
which is also just a sequence length number of tokens
I forgot a comma here
and then we have the encoder mask so what is the encoder mask as you remember
over we are increasing the size of the encoder input sentence by adding padding tokens
but we don't want these padding tokens to participate in the self-attention
so what we need is to build a mask that says that we don't want these tokens to be seen by
the self-attention mechanism and so we build the mask for the encoder how do we build this mask we
just say that all the tokens that are not padding are okay all the tokens that are padding are not
okay we also on squeeze to add this sequence dimension and also to add the batch dimension later
and we convert into integers so this is one one sequence length
because this will be used in the self-attention mechanism
however for the decoder we need a special mask that is a causal mask which means that
each word can only look at the previous words and each word can only look at not non-padding
words so we don't want again we don't want the padding tokens to participate in the self-attention
we only want real words to participate in this and we also don't want each word to watch at
words that come after it but only that words come come before it so I will use a method here
called causal mask that will build it later we will build it also so now I just call it to show
you how it's used and then we will proceed to build it
also in this case we don't want the padding tokens
and we add the necessary dimensions and also we do a Boolean end with
causal mask which is a method that we will build right now and this causal mask needs
to build a matrix of size sequence length to sequence length what is sequence length is
basically the size of our decoder input
and this let me write a comment for you so this is one two sequence length
combine with so the end with one sequence length sequence and and this can be broadcasted
let's go define this method causal mask so what is causal mask
causal mask basically means that we want let's go back to the slides actually
as you remember from the slides we want each word in the decoder to only watch
words that come before it so what we want is to make all these values above this diagonal
that represents the multiplicate this matrix represents the multiplication of the
queries by the keys in the self-attention mechanism what we want is to hide all these
values so your cannot watch the word cat is a lovely cat it can only watch itself but this word
here for example this word lovely can watch everything that comes before it so from your
up to lovely itself but not the word cat that comes after it so what we do is we want all
these values here to be musket out so which also means that we want all the values above this
diagonal to be musket out and there is a very practical method in pytorch to do it so let's do
it let's go build let's go build this method so the mask is basically torch.triu which means
give me the every value that is above the diagonal that i am telling you so we want a matrix
which matrix matrix made of all ones
and this method will result will return every value above the diagonal and everything else
will become zero so we want diagonal one type we want it to be integer
and what we do is return mask is equal to zero so this will return all the values above the
diagonal and everything below the diagonal will become zero but we want actually the opposite
so we say okay everything that is zero should will become true with this expression and everything
that is not zero will become false so we apply it here to build this mask so this mask will be
one by sequence length by sequence length which is exactly what we want okay let's add also the
label the label is also i forgot a comma sequence length and then we have the source text just
for visualization we can send it source text and then the target text
and this is our data set now let's go back to our training method to continue writing the training
loop so now that we have the data set we can create it we can create two data set one for
training one for validation and then we send it to a data loader and finally to our training loop
we forgot to import the data set so let's import it here
we'll import the causal mask which we will need later
what is our source language it's in the configuration
what is our target language
and what is our sequence length is also in the configuration
we do the same for the validation
but the only difference is that we use this one now and the rest is same
we also just for choosing the max sequence length we also want to watch what is the maximum
length of each sentence in the source and the target for each of the two splits that we created
here so that if we choose a very small sequence length it we will know so
basically we do i load each sentence from each language from the source and the target language
i convert into ids using the tokenizer and i check the length if the length is
let's say 180 we can choose 200 as sequence length because it will cover all the possible
sentences that we have in this data set if it's let's say 500 we we can use 510 or something
like this because we also need to add the start of sentence and end of sentence sentence tokens to
these sentences
this is the source ids then let's create also the target ids and this is the language of
target and then we just say the source maximum length is the maximum of the
and the length of the current sentence the target is the target and the target ids
then we print these two values
we also do it for the target
and that's it now we can proceed to create the data loaders
we define the batch size according to our configuration which we still didn't define but
you can already guess what are its values
we want it to be shuffled
okay for the validation i will use a batch size of one because i want to process each sentence
one by one
and this method returns the the data loader of the training the data loader of the validation
the tokenizer of the source language and the tokenizer of the target language
now we can start building the model so let's define a new method called get model
which will according to our configuration our vocabulary size
build the model to transform our model so the model is
we didn't import the model so let's import it
build transformer
what is the first the source vocabulary size
and the target vocabulary size
and then we have the sequence length
and we have the sequence length of the source language and the sequence length of the target
language we will use the same for both and then we have the demodel
which is the size of the embedding we can keep all the rest the default as in the paper
if the model is too big for yours a GPU to be trained on you can try to reduce the number
of heads or the number of layers of course it will impact the performance of the model
but i think given the data set which is not so big and not so complicated it should not be a
big problem because we are not building a huge data set anyway okay now that we have the model
we can start building the training loop but before we try build the training loop let me
as define this configuration because it keeps coming and i think it's better to define the
the the structure now so let's create a new file called config.py in which we define two methods
one is called get config and one is to map to get the the the path where we will save the weights
of the model okay let's define the batch size
i choose 8 you can choose something bigger if your computer allows it the number of epochs
for which we will be training i would say 20 is enough the learning rate i am using 10 to the power
of minus 4 you can use other values i thought i thought this learning rate is reasonable
i think it's possible to change the learning rate during training
actually it's quite common to give a very high learning rate and then reduce it gradually
with every epoch we will not be using it because it will just complicate the code a little more
and this is not actually the goal of this video the goal of this video is to teach
how the transformer works
i have already checked the sequence length that we need for this particular dataset from
english to italian which is 350 is more than enough and the demodel that we will be using
is the default of 512 the language source is english so we are going from english the language
target is italian we are going to translate into italian we will save the model into the
folder called weights and the file name of which model will be t-model so transform a model
i also built the code to preload the model in case we want to restart the training
after maybe it's crash
and this is the tokenizer file so it will be saved like this so tokenizer n and tokenizer
it according to the language and this is the experiment name for tensorboard on which we will
save the losses
while training i think there is a comma here okay now let's define another method that allows to
find the path where we need to save the weights
why i'm creating such a complicated structure is because
we i will provide also notebooks to run this training on google colab so we just need to
change these parameters to make it work on google colab and save the weights directly
on your google drive i have already created actually this this code and it will be provided on
github and i will also provide the link in the video
okay the file is built according to
model page name then the epoch dot pt
let's import also here the path library
okay now let's go back to our training loop okay we can build the training loop now finally
so train model given the configuration okay first we need to define which device on which
we will put all the tensors so define the device
if i have to go down my computer so
okay then we also print
we make sure that the weights folder is created
okay
and then we load our data set we can just take these values here and say it's equal to
get ds of config
we create also the model
to get the vocabulary size there is method called get vocab size
and i think we don't have any other parameter
and finally we transfer the model to our device
we also start tensorboard tensorboard allows to visualize the loss the the graphics the charts
let's also import tensorboard
okay let's go back let's also create the optimizer i will be using the atom optimizer
okay since we also have the configuration that allow us to resume the training in case the model
crashes or something crashes let's implement that one and that will allow us to restore the
state of the model and the state of the optimizer
let's implement import this method we defined in the data set
okay
okay
we load the file
okay
here we have a typo okay the loss function we will be using is the cross entropy loss
we need to tell him what is the ignore index so we don't we want him to ignore the padding
token basically we don't want the loss to the padding token to contribute to the loss
okay
and we also will be using labels muting labels muting basically allows us our model to be less
confident about its decision so how to say imagine our model is telling us to choose
the word number three and with a very high probability so what we will do with labels
booting is take a little percentage of that probability and distribute to the other tokens
so that our model becomes less sure of its choices so kind of less over fit and this
actually improves the accuracy of the model so we will use the labels booting of 0.1 which means
from every highest probability token take 0.1 percent of score and give it to the others
okay let's build finally the training loop
we tell the model to train
I build a batch iterator for the for the data loader using tkodm which will show a very nice
progress bar
and we need to import tkodm
okay finally we get the tensors the encoder input
what is the size of this tensor it's batch to sequence length
the decoder input is batch of decoder input and we will also move it to our device
batch to sequence length we get the two masks also
this is the size and then the decoder mask
okay why these two masks are different because in the one case we are only telling him to
hide only the padding tokens in the other case we are also telling him to hide all these
subsequent words for each word to hide all the subsequent words to mask them out
okay now we run the let's make some run the tensors through the transformer
so first we calculate the output of the encoder
and we encode using what the encoder input and the mask of the encoder
then we calculate the decoder output
using the encoder output the source the mask of the encoder
then the decoder input and the decoder mask
okay as we know this the result of this so the output of the model dot encode will be a batch
sequence length d model
also the output of the decoder will be batch sequence length d model
but we want to map it back to the vocabulary so we need the projection so let's get the projection
output and this will produce a b so batch sequence length and target vocabulary size
okay now that we have the output of the model we want to compare it with our label so first
let's extract the label from the batch
and we also put it on our device so what is the label it's b so batch to sequence length
in which each position tell so the label is already for each b and sequence length so for each
dimension tells us what is the position in the vocabulary of that particular word
and then we want these two to be comparable so we first need to compute the loss
into this i show you now projection output view minus one
okay what does this do this basically transforms the i show you here
this size into this size b multiplied by sequence length and then target vocabulary size
okay because we want to compare it with this this is how the cross entropy wants the
tensors to be
and also the label
okay now we can we have calculated the loss we can update our progress bar this one with
the loss we have calculated
okay
and this is this will show the loss on our progress bar we can also log it on tensor board
okay
let's also flush it okay now we can back propagate the loss so loss dot backward
and finally we update the weights of the model so that is the job of the optimizer
and finally we can zero out the the grad
and remove the global step by one the global step is being used mostly for tensor board to
keep track of the loss we can save the model
every epoch okay model file name which we get from our special methods this one
I tell him the configuration we have and the name of the file which is the epoch but with zeros
in front and we save our model it is very good idea when we want to be able to resume the training
to also save not only the the state of the model but also the state of the optimizer because the
optimizer also keeps tracks of some statistics one for each weight to to understand how to move
each weight independently and usually actually I I saw that the the optimizer the dictionary is quite
big so even if it's big if you want your training to be resumable you need to save it otherwise the
optimizer will always start from zero and will have to figure out from zero even if you start from
a previous epoch how to move each weight so every time we save some snapshot I always include it
the state of the model
this is all the weights of the model we also want to stay save the optimizer
let's do also the global step
and we want to save all this into the file name so model file name
and that's it now let's build the code to run this so if name
I really find the warnings frustrating so I want to filter them out because I have some
a lot of libraries especially CUDA I already know what's the content and so I don't want to
visualize them every time but for sure for you guys I suggest watching them at least once to
understand if there is any big problem otherwise they're just complaining from CUDA
okay let's try to run this code and see if everything is working fine we should what we
expect is that the code should download the data set the first time then it should create the
tokenizer and save it into its file and it should also start training the model for 30 epochs of
course it will never finish but let's do it let me check again the configuration tokenizer okay let's
run it
okay it's building the tokenizer and we have some problem here sequence length
okay finally the model is training I show you recap you guys what I had mistaken first of all
the sequence length was written incorrectly there was a capital L here and also in the
data set I forgot to save it here and here I had it also written capital I so L was capital
and now the training is going on and as you can see the training is quite fast or at least on my
computer actually not so fast but because I choose a batch size of 8 I could try to increase it and
it's happening on CUDA the loss is decreasing and the weights will be saved here so if we reach the
end of the epoch it will create the first weight here so let's wait until the end of the epoch
and see if the weight is actually created before actually finishing the training of the model
let's do another thing we also would like to visualize the output of the model while we are
training and this is called validation so we want to check how our model is evolving while it is
getting trained so what we want to build is a validation loop which will allow us to evaluate
the model which also means that we want to inference from this model and check some sample
sentences and see if how they get translated so let's start building the validation loop
the first thing we do is we build a new method called run validation
and this method will accept some parameters that we will use for now I just write all of them and
later I explain how they will be used
okay the first thing we do to run the validation is we put our model into evaluation mode so we
do model.eval and this means that this tells PyTorch that we are going to evaluate our model
and then what we will do we will inference two sentences and see what is the output of the model
so with torch.nodegrad we are disabling the gradient calculation for this for every tensor that we
will run inside this width block and this is exactly what we want we just want to inference
from the model we don't want to train it during this loop so let's get a batch from the validation
dataset because we want to inference only two so we keep account of how many we have already
processed and we get the input from this current patch I want to remind you that for the validation
yes we only have a batch size of one
this is the encoder input and we can also get the encoder mask
let's just verify that the the size of the batch is actually one
and now let's go to the interesting part so as you remember when we
calculate the when we want to inference the model we need to calculate the encoder output
only once and reuse it for every token that we will the model will output from the decoder
so let's create another function that will run the greedy decoding on our model
and we'll use and we will see that it will run the encoder only once so let's call this function
greedy decode
okay let's create some tokens that we will need so the SOS token
which is the start of sentence we can get it from either a tokenizer it doesn't matter if it's the
target or the source they both have it
okay
okay and then we what we do is we pre compute the encoder output and reuse it
for every token we get from the decoder so
we just give the source and the source mask which is the encoder input and the encoder
mask we can also call it encoder input and encoder mask then we get the then we okay
how do we do the inferencing the first thing we do is we give to the decoder the start of
sentence token so that the decoder will output the first token of the sentence of the translated
sentence then at every iteration just like we saw in my slides at every iteration we add the
previous token to the to the decoder input and so that the decoder can output the next token
then we take the next token we put it again in front of the input to the decoder and we get the
successive token so let's build the decoder input for the first iteration which is only the start
of sentence token
we fill this one with the start of sentence token
and it has the same type as the encoder input okay now we will keep in asking the decoder to
output the next token until we reach either the end of sentence token or the max length we have
defined here so we can do a while true and then our first stopping condition is if we
the decoder output which is becomes the input of the next step becomes large larger than max
length or reaches max length here why do we have two dimensions one is for the batch and one is for
the tokens of the of the decoder input now we also need to create a mask for this
we can use our function causal mask to say that we don't want the input to watch future words
and we don't need the other mask because here we don't have any padding token as you can see
now we calculate the output
we reuse the output of the encoder for every iteration of the loop
we reuse the source mask so the input the mask of the encoder then we give the decoder input and
along with its mask the decoder mask and then we get the next token
so we get the probabilities of the next token using the projection layer
but we only want the projection of the last token so the next token after the last we have
given to the encoder now we can use the max
so we get the token with the maximum probability this is the greedy search
and then we get this word and we append it back to this one because it will become the input of
the next iteration and we concat
so we take the decoder input and we append the next token so we create another tensor for that
yeah it should be correct okay if the next token so if the next word or token is
equal equal to the end of sentence token then we also stop the loop and this is our greedy search
now we can just return the output so the output is basically the decoder input because every
time we are appending the next token to it and we remove the batch dimension so we squeeze it
and that's our greedy decoding now we can use it here in this function so in the validation
function so we can finally get the model output is equal to greedy decode in which we
give him all the parameters
and then we want to compare this model output with what we expected so with the label
so let's append all of this so what we give to the input we gave to the model what the model
output the output of the model so they predicted and what we expected as output we save all of this
in these lists and then at the end of the loop we will print them on the console
uh to get the text of the output of the model we need to use the tokenizer again
to convert the tokens back into text and we use of course the target tokenizer because this is the
target language
okay and now we save them all of this into their respective lists
and we can also print it on the console
while we are using why we are we using this function called print message
and why not just use the print of the python because we are using here in the main loop in
the training loop we are using here tkodm which is our really nice looking progress bar
but it is not suggested to print directly on the console when this progress bar is running
so to print on the console there is one method called print provided by tkodm and we will give
this method to this function so that the output does not interfere with the progress part printing
okay we print some bars
and then we print all the messages
and
and if we have already processed number of examples then we just break
so why we have created these lists actually we can also send all of this to
to a tensorboard so we can so for example if we have tensorboard enabled we can send
all of this to the tensorboard and to do that actually we need another library that allow
us to calculate some metrics i think we can skip this part but if you are really interested i
in my in the code i published on github you will find that i use this library called torch
matrix that allows to calculate the char error rate and the blue the blue metric which is really
useful for translation tasks and the word error rate so if you're really interested you can find
the code on the github but for our demonstration i think it's not necessary so and actually this
we can also remove it given that we are not doing this part okay so now that we have our
run validation method we can just call it okay what i usually do is i run the validation at every
few steps but because we want to see it as soon as possible the what we will do is we will first
run it at every iteration and we also put this model dot train inside of this loop so that every
time after we run the validation the model is back into its into its training mode so now we can
just run validation and we give it all the parameter that it needs to to to run the validation so give
it model model
okay for printing message are we printing any message we are so let's create a lambda
and we just do
and this is the message to write with the tkodm then we need to give the global step
and the writer which we will not use but okay now i think we can run the training again and
see if the validation works
all right looks like it is working so the model is okay it's running the validation at every step
which is not desirable at all but at least we know that the greedy search is working
and it's not at least looks like it it's working and the model is not predicting anything useful
actually it's just predicting a bunch of commands because it's not training at all but if we train
the model after a while we should see that after a few epochs the model should become better and
better and better so let's stop this training and let's put this one back to where it belongs
so at the end of every epoch here and this one we can keep it here no problem
yeah okay i will now skip fast forward to a model that has been pre-trained i
pre-trained it for a few hours so that we can inference it and we can visualize the attention
i have copied the pre-trained weights that i pre-calculated and i also created this
notebook reusing the functions that we have defined before in the train file the code is very
simple actually i just copy and pasted the code from the train file i just load the model and run
the validation the same method that we just wrote and then i ran the validation on the pre-trained
let's run it again for example and as you can see the model is inferencing 10 examples sentences
and the result is not bad i mean we can see that levin's mile levin's orisa levin's orisa it's
matching and most of them matching actually we could also say that it's nearly over fit
for this particular data but this is the power of the transformer i didn't train it for many days
i just trained it for a few hours if i remember correctly and the results are really really good
and now let's write let's make the notebook that we will use to visualize the attention
of this pre-trained model given the file that we built before so train.pi you can also train
your own model choosing the language of your choice which i highly recommend that you change the
language and try to see how the model is performing and try to diagnose why the model is performing
bad if it's performing bad or if it's performing well try to understand how can you improve it
further so let's try to visualize the attention so let's create a new notebook
let's call it let's say attention visualization
okay so the first thing we do we import all the libraries we will need
i will also be using this library called altire it's a visualization library for charts it's
nothing related to deep learning actually it's just a visualization function and in particular
the visualization function actually i found it online it's not written by me just like most of
the visualization functions you can find easily on the internet if you want to build a chart or
if you want to build a histogram etc so i am using this library mostly because i copied the code
from the internet to visualize it but all the rest is my own code so let's import it
okay let's import all of this
and of course you will have to install this particular library when you
run the code on your computer let's also define the device
you can just copy the code from here
and then we load the model which we can copy from here
like this okay let's paste it here and this one becomes
vocabulary source and vocabulary target
okay now let's make a function to load the batch
oops
I will convert the batch into tokens now using the tokenizer.
I will convert the batch into tokens now using the tokenizer.
And of course for the decoder we use the target vocabulary.
So let's just infer using our greedy decode algorithm.
So we acquired the model.
Okay now I will build the necessary functions to visualize the attention.
I will copy some functions from another file because actually what we are going to build is nothing interesting from a learning point of view with regards to the deep learning.
It's mostly functions to visualize the data.
So I will copy it because it's quite long to write and the salient part I will explain of course.
And this is the function.
Okay what does this function do?
Basically we have the attention that we will get from the encoder.
How to get the attention from the encoder?
For example the attention we have in three positions.
First is in the encoder, the second one is in the decoder at the beginning of the decoder so the self-attention of the decoder.
And then we have the cross-attention between the encoder and the decoder.
So we can visualize three types of attention.
How to get the information about the attention?
Well we load our model, we have the encoder, we choose which layer we want to get the attention from.
And then from each layer we can get the self-attention block and then its attention scores.
How do, where does this variable come from?
If you remember when we defined the attention calculation here,
when we calculate the attention we not only return the output to the next layer,
we also give this attention scores which is the output of the softmax.
And we save it here in this variable self.attention scores.
Now we can just retrieve it and visualize it.
So this function will, based on which attention we want to get,
from which layer and from which head will select the matrix, the correct matrix.
This function builds a data frame to visualize the information.
So the tokens and the score extracted from this matrix here.
So it will, this matrix we extract the row and the column.
And then we also build the chart.
The chart is built with Altair.
And what we will build actually is we will get the attention for all the,
I built this method to get the attention for all the heads and all the layers that we pass to this function as input.
So let me run this cell now.
Okay, let's create a new cell.
And then let's just run it.
Okay, first we want to visualize the sentence that we are dealing with.
So the batch, other input tokens.
So we load a batch and then we visualize what is the source and the target.
Then into the target.
And finally we calculate also the length.
What is the length?
Okay, it's basically all the characters that come before the padding character.
So the first occurrence of the padding character.
Because this is the batch taken from the dataset, which is already the tensor built for training.
So they already include the padding.
In our case, we just want to retrieve the number of actual characters in our sentence.
So this one we can, the number of actual words in our sentence.
So we can check the number of words that come before padding.
So let's run this one.
And there is some problem.
Here I forgot to, this function was wrong.
So now it should work.
Okay, this sentence is too small.
Let's get a longer one.
Okay, let me check the quality.
You cannot remain as your, especially you.
Okay, looks not bad.
Okay, let's print the attention for the layers.
Let's say zero, one and two, because we have six of them.
If you remember the parameter is n is equal to six.
So we will just visualize three layers and we will visualize all the heads.
We have eight of them for each layer.
So the head number zero, one, two, three, four, five, six, seven and seven.
Okay, let's first visualize the encoder self attention.
And we do get all attention maps, which one we want.
So the encoder one.
And we want these layers and these heads.
And what are the row tokens, the encoder input tokens.
What are the, what do we want in the column?
Because we are going to build a grid.
So as you know, the attention is a grid that correlates rows with columns.
In our case, we are talking about the self attention of the encoder.
So it's the same sentence that is attending itself.
So we need to provide the input sentence of the encoder on both the rows and the columns.
And what is the maximum number of length that we want to visualize?
Okay, let's say we want to visualize no more than 20.
So the minimum of 20 and sentence length.
Okay, this is our visualization.
We can see, and as we expected, actually, when we visualize the attention,
we expect the values along the diagonals to be high because it's the dot product of each token with itself.
And we can see also that there are other interesting relationships.
For example, we say that the start of sentence token and the end of sentence token,
at least for the head zero and the later zero, they are not related to other words like I would expect, actually.
And but other heads, they do learn some very small mapping.
We can, if we hover over each of the grid cells, we can see the actual value of the self attention.
So the score of the self attention.
For example, we can see the strong the attention is very strong here.
So the word especially and especially are related.
So it's the same word with itself, but also especially and now.
And we can visualize this kind of attention for all the layers.
So because each head will will watch different aspect of each word,
because we are distributing the word embedding among the heads equally.
So each head will see a different part of the embedding of the word.
We also hope that they learn different kind of mapping between the words.
And this is actually the case.
And between one layer and the next, we also have different WQWQ and WV metrics.
So they should also learn different relationships.
Now we can also want, we may also want to visualize the attention of the decoder.
So let's do it.
Let me just copy the code and just change the parameters.
Okay, here we want the decoder one.
We want the same layers, etc.
But the tokens that we will be on the rows and the columns are the decoder tokens.
So the coder input tokens and the coder input tokens.
Let's visualize.
And also we should see Italian language now because we are using the decoder self-attention.
And it is.
So here we see a different kind of attention on the decoder side.
And also here we have multiple heads that should learn different mapping
and also different layers should learn different mappings between words.
The one I find most interesting is the cross-attention.
So let's have a look at that.
Okay, let me just copy the code and run it again.
Okay, so if you remember the method, it's encoder, decoder, same layer.
So here on the rows, we will show the encoder input and on the columns,
we will show the decoder input tokens because it's a cross-attention
between the encoder and the decoder.
Okay, this is more or less how the interaction between the encoder
and the decoder works and how it happens.
So this is where we find the cross-attention calculated using the keys
and the values coming from the encoder while the query is coming from the decoder.
So this is actually where the translation task happens.
And this is how the model learns to relate these two sentences to each other
to actually calculate the translation.
So I invite you guys to run the code by yourself.
So the first suggestion I give you is to write the code along with me with the video.
You can pause the video, you can write the code by yourself.
Okay, let me give you some practical examples.
For example, when I'm writing the model code,
I suggest you watch me write the code for one particular layer
and then stop the video, write it by yourself, take some time.
Don't watch the solution right away, try to figure out what is going wrong.
And if you really cannot, after one, two minutes, you cannot really figure out what is the problem.
You can have a glimpse at the video, but try to do it by yourself.
Some things, of course, you cannot come up by yourself.
So for example, for the positional encoding and all this calculation,
it's basically just an application of formulas.
But the point is you should at least be able to come with a structure by yourself.
So how all the layers are interacting with each other.
This is my first recommendation.
And while about the training loop, the training part actually is quite standard.
So it's very similar to other training loops that you may have seen.
The interesting part is how we calculate the loss and how we use the transformer model.
And the last thing that is really important is how we inference the model, which is in this greedy decode.
So thank you everyone for watching the video and for staying with me for so long.
I can assure you that it was worth it.
And I hope in the next videos to make more examples of transformers and other models that I am familiar with.
And I also want to explore with you guys.
So let me know if there is something that you don't understand or you want me to explain better.
I will also for sure follow the comment section and please write me.
Thank you and have a nice day.
