start	end	text
0	6080	Okay. So we have a new announcement from OpenAI. This has just come out in the past 12 hours or so,
6080	11520	and basically they're announcing a couple of new things. The main one being this idea of
11520	17200	function calling, which I'll go through in a second in detail. But the other things that
17200	23920	they're announcing, which are kind of interesting, are more steerable API models, longer context and
23920	30400	lower prices. So what does more steerable API models mean? The thing that I would say this means
30400	38240	is that the GPT-4 model have been able to be influenced by the system prompt a lot more than
38240	46240	the GPT-3.5 turbo model. So perhaps in these new versions, we're actually going to see the GPT-3.5
46240	52160	turbo actually respond to the system prompt a lot more than it has been in the past. The other thing
52240	58240	that this also probably refers to is the actual sort of function idea here as well. So let's go down,
58240	64640	let's have a look at what's coming out. So we can see that the model has been updated. This is
64640	71280	one of the key things, along with this sort of announcing the deprecation of some of the previous
71280	78160	sort of checkpoints that they were making available for people to use. This basically is only an
78160	84160	issue if you were using a very specific checkpoint to make sure that it responded to a prompt in the
84160	91040	same way. So definitely we've seen as new versions of the GPT-3.5 turbo have come out that they
91040	95520	respond slightly different to the prompts as we go through them. As I mentioned earlier, we've got
95520	104000	more steerable versions of GPT-4 and 3.5 turbo. Another fantastic thing for the GPT-3.5 turbo
104000	112560	is the addition of this new 16k context version of this. So before this version, basically we only
112560	120000	had access to a 4k context window. And now we're going to basically have right up to 16k. So that
120000	125600	is a huge difference. That really makes the difference for what things like summarization,
125600	132720	you can probably go right out to an hour of YouTube. Now with this, things like archive
132720	138000	papers, you can do full papers in one shot. They should fit in the context window now.
138000	143280	A whole bunch of things that this suddenly going to become useful for as we go forward.
143280	150080	The other thing is that the cost of the standard GPT-3.5 turbo has been reduced another 25% as well.
150080	154640	So that's something that's interesting. And there are further down here to the cost of what these
154640	165440	new GPT-3.5 turbo 16k is going to be. So it's going to be 0.3 cents per 1000 tokens on the input
165440	171360	and 0.4 per 1000 tokens on the output here. As they mentioned here that you can now support
171360	179040	about 20 pages of text in a single response. This really is a huge factor going forward for this.
179040	185920	Now by the far the most interesting thing that they released in this is the idea of function
185920	191760	calling. So what actually is this idea of function calling? So if you've watched some of my previous
191760	198080	videos, I've talked about the idea of react and doing reasoning by putting in certain prompts to
198080	204720	basically get the model to tell us what tools to use out that kind of thing. What they've done here
204720	210000	is basically bait that into the model now. So they actually mentioned these models have been
210000	215920	fine-tuned to both detect when a function needs to be called and to respond with Jason the adheres
215920	223040	to the function signature. So basically now we can just pass in a definition of a tool or the
223040	230000	definition of something that we wanted to do. And it will give us back a structured reasoned output
230000	236640	based on the text that the user puts in here. So this basically changes a number of things. One,
236640	242480	we probably don't need to react as much anymore if you're using open AI. And also you're probably
242480	248240	not going to need as many of the output passes because you'll be able to define functions where
248240	253600	really you're just asking it to get the JSON data back in this. And that's something that you'll be
253600	258320	able to do. So that gives some examples here. They talk about creating chatbots that can use
258400	266240	external tools. And you would basically have a function where you give an example of email,
266240	271280	someone if they want to get coffee by next Friday, and that the function you'd pass in would be
271280	277600	something like a send email to and then a body for the email. And then the GPT model,
277600	283760	whether it's the GPT-4 or the GPT-3.5 turbo, don't forget both of these models have now been
283760	291120	tuned for doing this. It will actually insert the two string and also the body string in here.
291120	295440	I'm going to get another example they give is weather API. So what's the weather like in Boston?
296160	302480	The function will be get current weather. And that would basically pass in the location
302480	307760	and then the unit of measurement for the amount of degrees, etc. So this is one thing that you
307760	313600	can do with this. Another one is that you can convert natural language into API calls or database
313600	321920	queries. So this gives you a whole set of ways to basically now query databases just by peering
321920	328720	open AI, defining your functions, and then it can basically return back as a SQL query. It can
328720	336480	return back as a specific call to an API for this stuff. So the third use that they mentioned
336480	343040	is the idea of extracting structured data from text. So this is like the core package that
343040	348800	used open AI to basically extract data out of things. You can see here basically now you can
348800	354720	just define this as a function. Tell it that we want to extract people data and then we want to
354720	360640	extract the name, the birth date, the location, and it will be able to pass through and extract that
360640	367120	and return that information as JSON to you. So again, this is something that before would have
367120	372800	taken a lot of prompt manipulation, used up a lot more tokens. Now we're able to basically
372800	379760	do this with just providing a function definition to the model and it will be able to go through it
379760	384640	that way. So if we come down here and have a look at this, now in this video, I'm just going to go
384640	389680	through this here. I'm going to do another video. I've already started to put together a notebook
389680	395040	for this of getting it to work with leg chain. So I'll do this in another video straight after
395040	401680	this, but just showing you here what is actually going on here. So remember that we're talking about
402320	410720	GBT 3.5 model and the full model. So they use the sort of chat format of where you've got a system
410720	418800	token, you've got your human message, even AI message. So that's what's going on here. If we look
418800	424560	at like we're calling the function with a user input, and you'll see that, okay, we're basically
424560	429120	passing in the model, we've got our messages, and then we've got the role. So the first one
429120	435520	role is user, which is a language. This is called human. And then basically the content. Now the
435520	442480	other thing that we pass in is we need to pass in a function definition or technically we pass in
442480	449280	a list of function definitions. So here you can say this is the list that we've got here. And what
449280	454880	we're actually got here is basically only one function being passed in. The name of that is
454960	459280	getCurrentWither, which makes sense because the person who's asking what is the weather in Boston,
460000	466320	we then basically, you can see that the parameters of the function, and the main ones here are the
466320	473440	name of it, the description of it, the description of actually what the function does, and then the
473440	478880	actual properties that it needs to return, get a description as well. So you can see here we've
478880	485520	basically got location, going to be a string, and it's going to be a city and state,
486080	491280	G San Francisco, California. And then the second thing is going to be a unit. So this is going to
491280	498320	be either Celsius or Fahrenheit. So you can see this is an enum, so it's restricting just to these
498320	506400	two things. So we can't say, oh, tell me what the, or rather the model can't just say, make up some
506400	512720	other unit. It needs to be one of these two that gets returned in this. Now we can see that neither
512720	519200	of these, this particular unit is not required, but the location is definitely required. So we
519200	525280	can't just say, tell me what the weather is, because the GPT, unless it's got it already in the
525280	530800	history of it, of your conversation, it's not going to know that. But if it is in the history,
530800	534560	and we've been talking about a location, and I can say, tell me what the weather is like there now,
535040	540320	it will be able to do this. Now you can see here, this is basically just being quite simple. It's
540320	546880	just taking a location. You could imagine that you've got time, a time framing there, e.g. whether
546880	551920	it's now, whether it's tomorrow, whether it's next week, that kind of thing as well. So this is what's
551920	558800	getting passed into the model. We see that then we've got the response back from the model. So
559520	564640	it's basically just going to return back. And it's going to give you the details to basically call
565360	571200	this particular function based on this. So if you're just passed in, hello, how are you today,
571200	576640	it wouldn't use any of the function, it would just give an AI message back here. But you'll see when
576640	583520	I go through the notebook later on, that actually what's happening is it's returning back kind of
583600	589280	like a blank message, but with a particular function called variable that's in there,
589280	594800	that's going on in there. You can see that once it basically calls this API, it gets this stuff back.
594800	600240	So it's getting back the temperature, the unit Celsius to description sunny. And then finally,
600240	608160	it passes that back to open AI. So it passes that back to the GPT model. You can see here,
608160	615920	we're basically passing back the list of messages. And we've gone user assistant function. So this is
615920	621200	a very special type of message. It's not something that we've had before is a new kind of role that
621200	629200	we've got there. And this is basically passing back the response from that function. So you can see
629200	635200	the assistant passed out that it should use this function. So the assistant passed out that, okay,
635200	642640	you should use the name function, get current weather, and you should be using Boston messages.
642640	648080	So it's worked out the state here itself, right? So it's not like your user is going to have to
648080	654160	tell everything if it can work out certain things, you can work them out quite nicely in that way.
654160	660960	It gets the return back here. And then it's going to generate a response out. And the response that
661040	666480	it generates out is the weather in Boston is currently sunny with a temperature of 22 degrees
666480	674000	Celsius. And that's basically taken in these messages to then finally generate out this message.
674000	679440	So you can see we are using probably a decent amount more tokens for the actual functions
679440	685440	definitions that we're passing in there. But we're not needing to do any in context learning,
685440	691280	we're not needing to do any sort of examples or things like that. And it kind of manages the
691280	696880	scratch pad itself internally. So unlike things we've done before with React, where we had to
696880	702160	have a scratch pad, etc, opening eyes, taking care of that by putting it in these parts here.
702160	708320	In fact, you could think of this function role as the scratch pad of what came back from the
708320	714720	particular function call that we had there. So this gives you a rough idea of how these things
714720	719600	work functions. In the next video, I will go through a whole bunch of code, we'll look at
719600	724560	how to put it in a length chain, both manually and then turn it into an agent and use it as an
724560	729360	agent as well. As always, if you like the video, please click like and subscribe. If you've got
729360	734960	any questions, please put them in the comments below. I'll see you in the next video. Bye for now.
