WEBVTT

00:00.000 --> 00:06.080
Okay. So we have a new announcement from OpenAI. This has just come out in the past 12 hours or so,

00:06.080 --> 00:11.520
and basically they're announcing a couple of new things. The main one being this idea of

00:11.520 --> 00:17.200
function calling, which I'll go through in a second in detail. But the other things that

00:17.200 --> 00:23.920
they're announcing, which are kind of interesting, are more steerable API models, longer context and

00:23.920 --> 00:30.400
lower prices. So what does more steerable API models mean? The thing that I would say this means

00:30.400 --> 00:38.240
is that the GPT-4 model have been able to be influenced by the system prompt a lot more than

00:38.240 --> 00:46.240
the GPT-3.5 turbo model. So perhaps in these new versions, we're actually going to see the GPT-3.5

00:46.240 --> 00:52.160
turbo actually respond to the system prompt a lot more than it has been in the past. The other thing

00:52.240 --> 00:58.240
that this also probably refers to is the actual sort of function idea here as well. So let's go down,

00:58.240 --> 01:04.640
let's have a look at what's coming out. So we can see that the model has been updated. This is

01:04.640 --> 01:11.280
one of the key things, along with this sort of announcing the deprecation of some of the previous

01:11.280 --> 01:18.160
sort of checkpoints that they were making available for people to use. This basically is only an

01:18.160 --> 01:24.160
issue if you were using a very specific checkpoint to make sure that it responded to a prompt in the

01:24.160 --> 01:31.040
same way. So definitely we've seen as new versions of the GPT-3.5 turbo have come out that they

01:31.040 --> 01:35.520
respond slightly different to the prompts as we go through them. As I mentioned earlier, we've got

01:35.520 --> 01:44.000
more steerable versions of GPT-4 and 3.5 turbo. Another fantastic thing for the GPT-3.5 turbo

01:44.000 --> 01:52.560
is the addition of this new 16k context version of this. So before this version, basically we only

01:52.560 --> 02:00.000
had access to a 4k context window. And now we're going to basically have right up to 16k. So that

02:00.000 --> 02:05.600
is a huge difference. That really makes the difference for what things like summarization,

02:05.600 --> 02:12.720
you can probably go right out to an hour of YouTube. Now with this, things like archive

02:12.720 --> 02:18.000
papers, you can do full papers in one shot. They should fit in the context window now.

02:18.000 --> 02:23.280
A whole bunch of things that this suddenly going to become useful for as we go forward.

02:23.280 --> 02:30.080
The other thing is that the cost of the standard GPT-3.5 turbo has been reduced another 25% as well.

02:30.080 --> 02:34.640
So that's something that's interesting. And there are further down here to the cost of what these

02:34.640 --> 02:45.440
new GPT-3.5 turbo 16k is going to be. So it's going to be 0.3 cents per 1000 tokens on the input

02:45.440 --> 02:51.360
and 0.4 per 1000 tokens on the output here. As they mentioned here that you can now support

02:51.360 --> 02:59.040
about 20 pages of text in a single response. This really is a huge factor going forward for this.

02:59.040 --> 03:05.920
Now by the far the most interesting thing that they released in this is the idea of function

03:05.920 --> 03:11.760
calling. So what actually is this idea of function calling? So if you've watched some of my previous

03:11.760 --> 03:18.080
videos, I've talked about the idea of react and doing reasoning by putting in certain prompts to

03:18.080 --> 03:24.720
basically get the model to tell us what tools to use out that kind of thing. What they've done here

03:24.720 --> 03:30.000
is basically bait that into the model now. So they actually mentioned these models have been

03:30.000 --> 03:35.920
fine-tuned to both detect when a function needs to be called and to respond with Jason the adheres

03:35.920 --> 03:43.040
to the function signature. So basically now we can just pass in a definition of a tool or the

03:43.040 --> 03:50.000
definition of something that we wanted to do. And it will give us back a structured reasoned output

03:50.000 --> 03:56.640
based on the text that the user puts in here. So this basically changes a number of things. One,

03:56.640 --> 04:02.480
we probably don't need to react as much anymore if you're using open AI. And also you're probably

04:02.480 --> 04:08.240
not going to need as many of the output passes because you'll be able to define functions where

04:08.240 --> 04:13.600
really you're just asking it to get the JSON data back in this. And that's something that you'll be

04:13.600 --> 04:18.320
able to do. So that gives some examples here. They talk about creating chatbots that can use

04:18.400 --> 04:26.240
external tools. And you would basically have a function where you give an example of email,

04:26.240 --> 04:31.280
someone if they want to get coffee by next Friday, and that the function you'd pass in would be

04:31.280 --> 04:37.600
something like a send email to and then a body for the email. And then the GPT model,

04:37.600 --> 04:43.760
whether it's the GPT-4 or the GPT-3.5 turbo, don't forget both of these models have now been

04:43.760 --> 04:51.120
tuned for doing this. It will actually insert the two string and also the body string in here.

04:51.120 --> 04:55.440
I'm going to get another example they give is weather API. So what's the weather like in Boston?

04:56.160 --> 05:02.480
The function will be get current weather. And that would basically pass in the location

05:02.480 --> 05:07.760
and then the unit of measurement for the amount of degrees, etc. So this is one thing that you

05:07.760 --> 05:13.600
can do with this. Another one is that you can convert natural language into API calls or database

05:13.600 --> 05:21.920
queries. So this gives you a whole set of ways to basically now query databases just by peering

05:21.920 --> 05:28.720
open AI, defining your functions, and then it can basically return back as a SQL query. It can

05:28.720 --> 05:36.480
return back as a specific call to an API for this stuff. So the third use that they mentioned

05:36.480 --> 05:43.040
is the idea of extracting structured data from text. So this is like the core package that

05:43.040 --> 05:48.800
used open AI to basically extract data out of things. You can see here basically now you can

05:48.800 --> 05:54.720
just define this as a function. Tell it that we want to extract people data and then we want to

05:54.720 --> 06:00.640
extract the name, the birth date, the location, and it will be able to pass through and extract that

06:00.640 --> 06:07.120
and return that information as JSON to you. So again, this is something that before would have

06:07.120 --> 06:12.800
taken a lot of prompt manipulation, used up a lot more tokens. Now we're able to basically

06:12.800 --> 06:19.760
do this with just providing a function definition to the model and it will be able to go through it

06:19.760 --> 06:24.640
that way. So if we come down here and have a look at this, now in this video, I'm just going to go

06:24.640 --> 06:29.680
through this here. I'm going to do another video. I've already started to put together a notebook

06:29.680 --> 06:35.040
for this of getting it to work with leg chain. So I'll do this in another video straight after

06:35.040 --> 06:41.680
this, but just showing you here what is actually going on here. So remember that we're talking about

06:42.320 --> 06:50.720
GBT 3.5 model and the full model. So they use the sort of chat format of where you've got a system

06:50.720 --> 06:58.800
token, you've got your human message, even AI message. So that's what's going on here. If we look

06:58.800 --> 07:04.560
at like we're calling the function with a user input, and you'll see that, okay, we're basically

07:04.560 --> 07:09.120
passing in the model, we've got our messages, and then we've got the role. So the first one

07:09.120 --> 07:15.520
role is user, which is a language. This is called human. And then basically the content. Now the

07:15.520 --> 07:22.480
other thing that we pass in is we need to pass in a function definition or technically we pass in

07:22.480 --> 07:29.280
a list of function definitions. So here you can say this is the list that we've got here. And what

07:29.280 --> 07:34.880
we're actually got here is basically only one function being passed in. The name of that is

07:34.960 --> 07:39.280
getCurrentWither, which makes sense because the person who's asking what is the weather in Boston,

07:40.000 --> 07:46.320
we then basically, you can see that the parameters of the function, and the main ones here are the

07:46.320 --> 07:53.440
name of it, the description of it, the description of actually what the function does, and then the

07:53.440 --> 07:58.880
actual properties that it needs to return, get a description as well. So you can see here we've

07:58.880 --> 08:05.520
basically got location, going to be a string, and it's going to be a city and state,

08:06.080 --> 08:11.280
G San Francisco, California. And then the second thing is going to be a unit. So this is going to

08:11.280 --> 08:18.320
be either Celsius or Fahrenheit. So you can see this is an enum, so it's restricting just to these

08:18.320 --> 08:26.400
two things. So we can't say, oh, tell me what the, or rather the model can't just say, make up some

08:26.400 --> 08:32.720
other unit. It needs to be one of these two that gets returned in this. Now we can see that neither

08:32.720 --> 08:39.200
of these, this particular unit is not required, but the location is definitely required. So we

08:39.200 --> 08:45.280
can't just say, tell me what the weather is, because the GPT, unless it's got it already in the

08:45.280 --> 08:50.800
history of it, of your conversation, it's not going to know that. But if it is in the history,

08:50.800 --> 08:54.560
and we've been talking about a location, and I can say, tell me what the weather is like there now,

08:55.040 --> 09:00.320
it will be able to do this. Now you can see here, this is basically just being quite simple. It's

09:00.320 --> 09:06.880
just taking a location. You could imagine that you've got time, a time framing there, e.g. whether

09:06.880 --> 09:11.920
it's now, whether it's tomorrow, whether it's next week, that kind of thing as well. So this is what's

09:11.920 --> 09:18.800
getting passed into the model. We see that then we've got the response back from the model. So

09:19.520 --> 09:24.640
it's basically just going to return back. And it's going to give you the details to basically call

09:25.360 --> 09:31.200
this particular function based on this. So if you're just passed in, hello, how are you today,

09:31.200 --> 09:36.640
it wouldn't use any of the function, it would just give an AI message back here. But you'll see when

09:36.640 --> 09:43.520
I go through the notebook later on, that actually what's happening is it's returning back kind of

09:43.600 --> 09:49.280
like a blank message, but with a particular function called variable that's in there,

09:49.280 --> 09:54.800
that's going on in there. You can see that once it basically calls this API, it gets this stuff back.

09:54.800 --> 10:00.240
So it's getting back the temperature, the unit Celsius to description sunny. And then finally,

10:00.240 --> 10:08.160
it passes that back to open AI. So it passes that back to the GPT model. You can see here,

10:08.160 --> 10:15.920
we're basically passing back the list of messages. And we've gone user assistant function. So this is

10:15.920 --> 10:21.200
a very special type of message. It's not something that we've had before is a new kind of role that

10:21.200 --> 10:29.200
we've got there. And this is basically passing back the response from that function. So you can see

10:29.200 --> 10:35.200
the assistant passed out that it should use this function. So the assistant passed out that, okay,

10:35.200 --> 10:42.640
you should use the name function, get current weather, and you should be using Boston messages.

10:42.640 --> 10:48.080
So it's worked out the state here itself, right? So it's not like your user is going to have to

10:48.080 --> 10:54.160
tell everything if it can work out certain things, you can work them out quite nicely in that way.

10:54.160 --> 11:00.960
It gets the return back here. And then it's going to generate a response out. And the response that

11:01.040 --> 11:06.480
it generates out is the weather in Boston is currently sunny with a temperature of 22 degrees

11:06.480 --> 11:14.000
Celsius. And that's basically taken in these messages to then finally generate out this message.

11:14.000 --> 11:19.440
So you can see we are using probably a decent amount more tokens for the actual functions

11:19.440 --> 11:25.440
definitions that we're passing in there. But we're not needing to do any in context learning,

11:25.440 --> 11:31.280
we're not needing to do any sort of examples or things like that. And it kind of manages the

11:31.280 --> 11:36.880
scratch pad itself internally. So unlike things we've done before with React, where we had to

11:36.880 --> 11:42.160
have a scratch pad, etc, opening eyes, taking care of that by putting it in these parts here.

11:42.160 --> 11:48.320
In fact, you could think of this function role as the scratch pad of what came back from the

11:48.320 --> 11:54.720
particular function call that we had there. So this gives you a rough idea of how these things

11:54.720 --> 11:59.600
work functions. In the next video, I will go through a whole bunch of code, we'll look at

11:59.600 --> 12:04.560
how to put it in a length chain, both manually and then turn it into an agent and use it as an

12:04.560 --> 12:09.360
agent as well. As always, if you like the video, please click like and subscribe. If you've got

12:09.360 --> 12:14.960
any questions, please put them in the comments below. I'll see you in the next video. Bye for now.

