{"text": " Okay. So we have a new announcement from OpenAI. This has just come out in the past 12 hours or so, and basically they're announcing a couple of new things. The main one being this idea of function calling, which I'll go through in a second in detail. But the other things that they're announcing, which are kind of interesting, are more steerable API models, longer context and lower prices. So what does more steerable API models mean? The thing that I would say this means is that the GPT-4 model have been able to be influenced by the system prompt a lot more than the GPT-3.5 turbo model. So perhaps in these new versions, we're actually going to see the GPT-3.5 turbo actually respond to the system prompt a lot more than it has been in the past. The other thing that this also probably refers to is the actual sort of function idea here as well. So let's go down, let's have a look at what's coming out. So we can see that the model has been updated. This is one of the key things, along with this sort of announcing the deprecation of some of the previous sort of checkpoints that they were making available for people to use. This basically is only an issue if you were using a very specific checkpoint to make sure that it responded to a prompt in the same way. So definitely we've seen as new versions of the GPT-3.5 turbo have come out that they respond slightly different to the prompts as we go through them. As I mentioned earlier, we've got more steerable versions of GPT-4 and 3.5 turbo. Another fantastic thing for the GPT-3.5 turbo is the addition of this new 16k context version of this. So before this version, basically we only had access to a 4k context window. And now we're going to basically have right up to 16k. So that is a huge difference. That really makes the difference for what things like summarization, you can probably go right out to an hour of YouTube. Now with this, things like archive papers, you can do full papers in one shot. They should fit in the context window now. A whole bunch of things that this suddenly going to become useful for as we go forward. The other thing is that the cost of the standard GPT-3.5 turbo has been reduced another 25% as well. So that's something that's interesting. And there are further down here to the cost of what these new GPT-3.5 turbo 16k is going to be. So it's going to be 0.3 cents per 1000 tokens on the input and 0.4 per 1000 tokens on the output here. As they mentioned here that you can now support about 20 pages of text in a single response. This really is a huge factor going forward for this. Now by the far the most interesting thing that they released in this is the idea of function calling. So what actually is this idea of function calling? So if you've watched some of my previous videos, I've talked about the idea of react and doing reasoning by putting in certain prompts to basically get the model to tell us what tools to use out that kind of thing. What they've done here is basically bait that into the model now. So they actually mentioned these models have been fine-tuned to both detect when a function needs to be called and to respond with Jason the adheres to the function signature. So basically now we can just pass in a definition of a tool or the definition of something that we wanted to do. And it will give us back a structured reasoned output based on the text that the user puts in here. So this basically changes a number of things. One, we probably don't need to react as much anymore if you're using open AI. And also you're probably not going to need as many of the output passes because you'll be able to define functions where really you're just asking it to get the JSON data back in this. And that's something that you'll be able to do. So that gives some examples here. They talk about creating chatbots that can use external tools. And you would basically have a function where you give an example of email, someone if they want to get coffee by next Friday, and that the function you'd pass in would be something like a send email to and then a body for the email. And then the GPT model, whether it's the GPT-4 or the GPT-3.5 turbo, don't forget both of these models have now been tuned for doing this. It will actually insert the two string and also the body string in here. I'm going to get another example they give is weather API. So what's the weather like in Boston? The function will be get current weather. And that would basically pass in the location and then the unit of measurement for the amount of degrees, etc. So this is one thing that you can do with this. Another one is that you can convert natural language into API calls or database queries. So this gives you a whole set of ways to basically now query databases just by peering open AI, defining your functions, and then it can basically return back as a SQL query. It can return back as a specific call to an API for this stuff. So the third use that they mentioned is the idea of extracting structured data from text. So this is like the core package that used open AI to basically extract data out of things. You can see here basically now you can just define this as a function. Tell it that we want to extract people data and then we want to extract the name, the birth date, the location, and it will be able to pass through and extract that and return that information as JSON to you. So again, this is something that before would have taken a lot of prompt manipulation, used up a lot more tokens. Now we're able to basically do this with just providing a function definition to the model and it will be able to go through it that way. So if we come down here and have a look at this, now in this video, I'm just going to go through this here. I'm going to do another video. I've already started to put together a notebook for this of getting it to work with leg chain. So I'll do this in another video straight after this, but just showing you here what is actually going on here. So remember that we're talking about GBT 3.5 model and the full model. So they use the sort of chat format of where you've got a system token, you've got your human message, even AI message. So that's what's going on here. If we look at like we're calling the function with a user input, and you'll see that, okay, we're basically passing in the model, we've got our messages, and then we've got the role. So the first one role is user, which is a language. This is called human. And then basically the content. Now the other thing that we pass in is we need to pass in a function definition or technically we pass in a list of function definitions. So here you can say this is the list that we've got here. And what we're actually got here is basically only one function being passed in. The name of that is getCurrentWither, which makes sense because the person who's asking what is the weather in Boston, we then basically, you can see that the parameters of the function, and the main ones here are the name of it, the description of it, the description of actually what the function does, and then the actual properties that it needs to return, get a description as well. So you can see here we've basically got location, going to be a string, and it's going to be a city and state, G San Francisco, California. And then the second thing is going to be a unit. So this is going to be either Celsius or Fahrenheit. So you can see this is an enum, so it's restricting just to these two things. So we can't say, oh, tell me what the, or rather the model can't just say, make up some other unit. It needs to be one of these two that gets returned in this. Now we can see that neither of these, this particular unit is not required, but the location is definitely required. So we can't just say, tell me what the weather is, because the GPT, unless it's got it already in the history of it, of your conversation, it's not going to know that. But if it is in the history, and we've been talking about a location, and I can say, tell me what the weather is like there now, it will be able to do this. Now you can see here, this is basically just being quite simple. It's just taking a location. You could imagine that you've got time, a time framing there, e.g. whether it's now, whether it's tomorrow, whether it's next week, that kind of thing as well. So this is what's getting passed into the model. We see that then we've got the response back from the model. So it's basically just going to return back. And it's going to give you the details to basically call this particular function based on this. So if you're just passed in, hello, how are you today, it wouldn't use any of the function, it would just give an AI message back here. But you'll see when I go through the notebook later on, that actually what's happening is it's returning back kind of like a blank message, but with a particular function called variable that's in there, that's going on in there. You can see that once it basically calls this API, it gets this stuff back. So it's getting back the temperature, the unit Celsius to description sunny. And then finally, it passes that back to open AI. So it passes that back to the GPT model. You can see here, we're basically passing back the list of messages. And we've gone user assistant function. So this is a very special type of message. It's not something that we've had before is a new kind of role that we've got there. And this is basically passing back the response from that function. So you can see the assistant passed out that it should use this function. So the assistant passed out that, okay, you should use the name function, get current weather, and you should be using Boston messages. So it's worked out the state here itself, right? So it's not like your user is going to have to tell everything if it can work out certain things, you can work them out quite nicely in that way. It gets the return back here. And then it's going to generate a response out. And the response that it generates out is the weather in Boston is currently sunny with a temperature of 22 degrees Celsius. And that's basically taken in these messages to then finally generate out this message. So you can see we are using probably a decent amount more tokens for the actual functions definitions that we're passing in there. But we're not needing to do any in context learning, we're not needing to do any sort of examples or things like that. And it kind of manages the scratch pad itself internally. So unlike things we've done before with React, where we had to have a scratch pad, etc, opening eyes, taking care of that by putting it in these parts here. In fact, you could think of this function role as the scratch pad of what came back from the particular function call that we had there. So this gives you a rough idea of how these things work functions. In the next video, I will go through a whole bunch of code, we'll look at how to put it in a length chain, both manually and then turn it into an agent and use it as an agent as well. As always, if you like the video, please click like and subscribe. If you've got any questions, please put them in the comments below. I'll see you in the next video. Bye for now.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.08, "text": " Okay. So we have a new announcement from OpenAI. This has just come out in the past 12 hours or so,", "tokens": [50364, 1033, 13, 407, 321, 362, 257, 777, 12847, 490, 7238, 48698, 13, 639, 575, 445, 808, 484, 294, 264, 1791, 2272, 2496, 420, 370, 11, 50668], "temperature": 0.0, "avg_logprob": -0.16895007050555685, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.02712409384548664}, {"id": 1, "seek": 0, "start": 6.08, "end": 11.52, "text": " and basically they're announcing a couple of new things. The main one being this idea of", "tokens": [50668, 293, 1936, 436, 434, 28706, 257, 1916, 295, 777, 721, 13, 440, 2135, 472, 885, 341, 1558, 295, 50940], "temperature": 0.0, "avg_logprob": -0.16895007050555685, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.02712409384548664}, {"id": 2, "seek": 0, "start": 11.52, "end": 17.2, "text": " function calling, which I'll go through in a second in detail. But the other things that", "tokens": [50940, 2445, 5141, 11, 597, 286, 603, 352, 807, 294, 257, 1150, 294, 2607, 13, 583, 264, 661, 721, 300, 51224], "temperature": 0.0, "avg_logprob": -0.16895007050555685, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.02712409384548664}, {"id": 3, "seek": 0, "start": 17.2, "end": 23.92, "text": " they're announcing, which are kind of interesting, are more steerable API models, longer context and", "tokens": [51224, 436, 434, 28706, 11, 597, 366, 733, 295, 1880, 11, 366, 544, 30814, 712, 9362, 5245, 11, 2854, 4319, 293, 51560], "temperature": 0.0, "avg_logprob": -0.16895007050555685, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.02712409384548664}, {"id": 4, "seek": 2392, "start": 23.92, "end": 30.400000000000002, "text": " lower prices. So what does more steerable API models mean? The thing that I would say this means", "tokens": [50364, 3126, 7901, 13, 407, 437, 775, 544, 30814, 712, 9362, 5245, 914, 30, 440, 551, 300, 286, 576, 584, 341, 1355, 50688], "temperature": 0.0, "avg_logprob": -0.08710078092721793, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.3447036147117615}, {"id": 5, "seek": 2392, "start": 30.400000000000002, "end": 38.24, "text": " is that the GPT-4 model have been able to be influenced by the system prompt a lot more than", "tokens": [50688, 307, 300, 264, 26039, 51, 12, 19, 2316, 362, 668, 1075, 281, 312, 15269, 538, 264, 1185, 12391, 257, 688, 544, 813, 51080], "temperature": 0.0, "avg_logprob": -0.08710078092721793, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.3447036147117615}, {"id": 6, "seek": 2392, "start": 38.24, "end": 46.24, "text": " the GPT-3.5 turbo model. So perhaps in these new versions, we're actually going to see the GPT-3.5", "tokens": [51080, 264, 26039, 51, 12, 18, 13, 20, 20902, 2316, 13, 407, 4317, 294, 613, 777, 9606, 11, 321, 434, 767, 516, 281, 536, 264, 26039, 51, 12, 18, 13, 20, 51480], "temperature": 0.0, "avg_logprob": -0.08710078092721793, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.3447036147117615}, {"id": 7, "seek": 2392, "start": 46.24, "end": 52.160000000000004, "text": " turbo actually respond to the system prompt a lot more than it has been in the past. The other thing", "tokens": [51480, 20902, 767, 4196, 281, 264, 1185, 12391, 257, 688, 544, 813, 309, 575, 668, 294, 264, 1791, 13, 440, 661, 551, 51776], "temperature": 0.0, "avg_logprob": -0.08710078092721793, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.3447036147117615}, {"id": 8, "seek": 5216, "start": 52.239999999999995, "end": 58.239999999999995, "text": " that this also probably refers to is the actual sort of function idea here as well. So let's go down,", "tokens": [50368, 300, 341, 611, 1391, 14942, 281, 307, 264, 3539, 1333, 295, 2445, 1558, 510, 382, 731, 13, 407, 718, 311, 352, 760, 11, 50668], "temperature": 0.0, "avg_logprob": -0.10550390813768525, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.0030711651779711246}, {"id": 9, "seek": 5216, "start": 58.239999999999995, "end": 64.64, "text": " let's have a look at what's coming out. So we can see that the model has been updated. This is", "tokens": [50668, 718, 311, 362, 257, 574, 412, 437, 311, 1348, 484, 13, 407, 321, 393, 536, 300, 264, 2316, 575, 668, 10588, 13, 639, 307, 50988], "temperature": 0.0, "avg_logprob": -0.10550390813768525, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.0030711651779711246}, {"id": 10, "seek": 5216, "start": 64.64, "end": 71.28, "text": " one of the key things, along with this sort of announcing the deprecation of some of the previous", "tokens": [50988, 472, 295, 264, 2141, 721, 11, 2051, 365, 341, 1333, 295, 28706, 264, 1367, 13867, 399, 295, 512, 295, 264, 3894, 51320], "temperature": 0.0, "avg_logprob": -0.10550390813768525, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.0030711651779711246}, {"id": 11, "seek": 5216, "start": 71.28, "end": 78.16, "text": " sort of checkpoints that they were making available for people to use. This basically is only an", "tokens": [51320, 1333, 295, 1520, 20552, 300, 436, 645, 1455, 2435, 337, 561, 281, 764, 13, 639, 1936, 307, 787, 364, 51664], "temperature": 0.0, "avg_logprob": -0.10550390813768525, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.0030711651779711246}, {"id": 12, "seek": 7816, "start": 78.16, "end": 84.16, "text": " issue if you were using a very specific checkpoint to make sure that it responded to a prompt in the", "tokens": [50364, 2734, 498, 291, 645, 1228, 257, 588, 2685, 42269, 281, 652, 988, 300, 309, 15806, 281, 257, 12391, 294, 264, 50664], "temperature": 0.0, "avg_logprob": -0.0667121363621132, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.004536706954240799}, {"id": 13, "seek": 7816, "start": 84.16, "end": 91.03999999999999, "text": " same way. So definitely we've seen as new versions of the GPT-3.5 turbo have come out that they", "tokens": [50664, 912, 636, 13, 407, 2138, 321, 600, 1612, 382, 777, 9606, 295, 264, 26039, 51, 12, 18, 13, 20, 20902, 362, 808, 484, 300, 436, 51008], "temperature": 0.0, "avg_logprob": -0.0667121363621132, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.004536706954240799}, {"id": 14, "seek": 7816, "start": 91.03999999999999, "end": 95.52, "text": " respond slightly different to the prompts as we go through them. As I mentioned earlier, we've got", "tokens": [51008, 4196, 4748, 819, 281, 264, 41095, 382, 321, 352, 807, 552, 13, 1018, 286, 2835, 3071, 11, 321, 600, 658, 51232], "temperature": 0.0, "avg_logprob": -0.0667121363621132, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.004536706954240799}, {"id": 15, "seek": 7816, "start": 95.52, "end": 104.0, "text": " more steerable versions of GPT-4 and 3.5 turbo. Another fantastic thing for the GPT-3.5 turbo", "tokens": [51232, 544, 30814, 712, 9606, 295, 26039, 51, 12, 19, 293, 805, 13, 20, 20902, 13, 3996, 5456, 551, 337, 264, 26039, 51, 12, 18, 13, 20, 20902, 51656], "temperature": 0.0, "avg_logprob": -0.0667121363621132, "compression_ratio": 1.6208333333333333, "no_speech_prob": 0.004536706954240799}, {"id": 16, "seek": 10400, "start": 104.0, "end": 112.56, "text": " is the addition of this new 16k context version of this. So before this version, basically we only", "tokens": [50364, 307, 264, 4500, 295, 341, 777, 3165, 74, 4319, 3037, 295, 341, 13, 407, 949, 341, 3037, 11, 1936, 321, 787, 50792], "temperature": 0.0, "avg_logprob": -0.10952473723370096, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.017976870760321617}, {"id": 17, "seek": 10400, "start": 112.56, "end": 120.0, "text": " had access to a 4k context window. And now we're going to basically have right up to 16k. So that", "tokens": [50792, 632, 2105, 281, 257, 1017, 74, 4319, 4910, 13, 400, 586, 321, 434, 516, 281, 1936, 362, 558, 493, 281, 3165, 74, 13, 407, 300, 51164], "temperature": 0.0, "avg_logprob": -0.10952473723370096, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.017976870760321617}, {"id": 18, "seek": 10400, "start": 120.0, "end": 125.6, "text": " is a huge difference. That really makes the difference for what things like summarization,", "tokens": [51164, 307, 257, 2603, 2649, 13, 663, 534, 1669, 264, 2649, 337, 437, 721, 411, 14611, 2144, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10952473723370096, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.017976870760321617}, {"id": 19, "seek": 10400, "start": 125.6, "end": 132.72, "text": " you can probably go right out to an hour of YouTube. Now with this, things like archive", "tokens": [51444, 291, 393, 1391, 352, 558, 484, 281, 364, 1773, 295, 3088, 13, 823, 365, 341, 11, 721, 411, 23507, 51800], "temperature": 0.0, "avg_logprob": -0.10952473723370096, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.017976870760321617}, {"id": 20, "seek": 13272, "start": 132.72, "end": 138.0, "text": " papers, you can do full papers in one shot. They should fit in the context window now.", "tokens": [50364, 10577, 11, 291, 393, 360, 1577, 10577, 294, 472, 3347, 13, 814, 820, 3318, 294, 264, 4319, 4910, 586, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10827752377124543, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.008056269027292728}, {"id": 21, "seek": 13272, "start": 138.0, "end": 143.28, "text": " A whole bunch of things that this suddenly going to become useful for as we go forward.", "tokens": [50628, 316, 1379, 3840, 295, 721, 300, 341, 5800, 516, 281, 1813, 4420, 337, 382, 321, 352, 2128, 13, 50892], "temperature": 0.0, "avg_logprob": -0.10827752377124543, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.008056269027292728}, {"id": 22, "seek": 13272, "start": 143.28, "end": 150.07999999999998, "text": " The other thing is that the cost of the standard GPT-3.5 turbo has been reduced another 25% as well.", "tokens": [50892, 440, 661, 551, 307, 300, 264, 2063, 295, 264, 3832, 26039, 51, 12, 18, 13, 20, 20902, 575, 668, 9212, 1071, 3552, 4, 382, 731, 13, 51232], "temperature": 0.0, "avg_logprob": -0.10827752377124543, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.008056269027292728}, {"id": 23, "seek": 13272, "start": 150.07999999999998, "end": 154.64, "text": " So that's something that's interesting. And there are further down here to the cost of what these", "tokens": [51232, 407, 300, 311, 746, 300, 311, 1880, 13, 400, 456, 366, 3052, 760, 510, 281, 264, 2063, 295, 437, 613, 51460], "temperature": 0.0, "avg_logprob": -0.10827752377124543, "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.008056269027292728}, {"id": 24, "seek": 15464, "start": 154.64, "end": 165.44, "text": " new GPT-3.5 turbo 16k is going to be. So it's going to be 0.3 cents per 1000 tokens on the input", "tokens": [50364, 777, 26039, 51, 12, 18, 13, 20, 20902, 3165, 74, 307, 516, 281, 312, 13, 407, 309, 311, 516, 281, 312, 1958, 13, 18, 14941, 680, 9714, 22667, 322, 264, 4846, 50904], "temperature": 0.0, "avg_logprob": -0.08007581145675094, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.274946391582489}, {"id": 25, "seek": 15464, "start": 165.44, "end": 171.35999999999999, "text": " and 0.4 per 1000 tokens on the output here. As they mentioned here that you can now support", "tokens": [50904, 293, 1958, 13, 19, 680, 9714, 22667, 322, 264, 5598, 510, 13, 1018, 436, 2835, 510, 300, 291, 393, 586, 1406, 51200], "temperature": 0.0, "avg_logprob": -0.08007581145675094, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.274946391582489}, {"id": 26, "seek": 15464, "start": 171.35999999999999, "end": 179.04, "text": " about 20 pages of text in a single response. This really is a huge factor going forward for this.", "tokens": [51200, 466, 945, 7183, 295, 2487, 294, 257, 2167, 4134, 13, 639, 534, 307, 257, 2603, 5952, 516, 2128, 337, 341, 13, 51584], "temperature": 0.0, "avg_logprob": -0.08007581145675094, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.274946391582489}, {"id": 27, "seek": 17904, "start": 179.04, "end": 185.92, "text": " Now by the far the most interesting thing that they released in this is the idea of function", "tokens": [50364, 823, 538, 264, 1400, 264, 881, 1880, 551, 300, 436, 4736, 294, 341, 307, 264, 1558, 295, 2445, 50708], "temperature": 0.0, "avg_logprob": -0.10265803866916233, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.006902300752699375}, {"id": 28, "seek": 17904, "start": 185.92, "end": 191.76, "text": " calling. So what actually is this idea of function calling? So if you've watched some of my previous", "tokens": [50708, 5141, 13, 407, 437, 767, 307, 341, 1558, 295, 2445, 5141, 30, 407, 498, 291, 600, 6337, 512, 295, 452, 3894, 51000], "temperature": 0.0, "avg_logprob": -0.10265803866916233, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.006902300752699375}, {"id": 29, "seek": 17904, "start": 191.76, "end": 198.07999999999998, "text": " videos, I've talked about the idea of react and doing reasoning by putting in certain prompts to", "tokens": [51000, 2145, 11, 286, 600, 2825, 466, 264, 1558, 295, 4515, 293, 884, 21577, 538, 3372, 294, 1629, 41095, 281, 51316], "temperature": 0.0, "avg_logprob": -0.10265803866916233, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.006902300752699375}, {"id": 30, "seek": 17904, "start": 198.07999999999998, "end": 204.72, "text": " basically get the model to tell us what tools to use out that kind of thing. What they've done here", "tokens": [51316, 1936, 483, 264, 2316, 281, 980, 505, 437, 3873, 281, 764, 484, 300, 733, 295, 551, 13, 708, 436, 600, 1096, 510, 51648], "temperature": 0.0, "avg_logprob": -0.10265803866916233, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.006902300752699375}, {"id": 31, "seek": 20472, "start": 204.72, "end": 210.0, "text": " is basically bait that into the model now. So they actually mentioned these models have been", "tokens": [50364, 307, 1936, 16865, 300, 666, 264, 2316, 586, 13, 407, 436, 767, 2835, 613, 5245, 362, 668, 50628], "temperature": 0.0, "avg_logprob": -0.10005051902170932, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.008708697743713856}, {"id": 32, "seek": 20472, "start": 210.0, "end": 215.92, "text": " fine-tuned to both detect when a function needs to be called and to respond with Jason the adheres", "tokens": [50628, 2489, 12, 83, 43703, 281, 1293, 5531, 562, 257, 2445, 2203, 281, 312, 1219, 293, 281, 4196, 365, 11181, 264, 614, 19464, 50924], "temperature": 0.0, "avg_logprob": -0.10005051902170932, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.008708697743713856}, {"id": 33, "seek": 20472, "start": 215.92, "end": 223.04, "text": " to the function signature. So basically now we can just pass in a definition of a tool or the", "tokens": [50924, 281, 264, 2445, 13397, 13, 407, 1936, 586, 321, 393, 445, 1320, 294, 257, 7123, 295, 257, 2290, 420, 264, 51280], "temperature": 0.0, "avg_logprob": -0.10005051902170932, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.008708697743713856}, {"id": 34, "seek": 20472, "start": 223.04, "end": 230.0, "text": " definition of something that we wanted to do. And it will give us back a structured reasoned output", "tokens": [51280, 7123, 295, 746, 300, 321, 1415, 281, 360, 13, 400, 309, 486, 976, 505, 646, 257, 18519, 1778, 292, 5598, 51628], "temperature": 0.0, "avg_logprob": -0.10005051902170932, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.008708697743713856}, {"id": 35, "seek": 23000, "start": 230.0, "end": 236.64, "text": " based on the text that the user puts in here. So this basically changes a number of things. One,", "tokens": [50364, 2361, 322, 264, 2487, 300, 264, 4195, 8137, 294, 510, 13, 407, 341, 1936, 2962, 257, 1230, 295, 721, 13, 1485, 11, 50696], "temperature": 0.0, "avg_logprob": -0.11430096826633486, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.023316550999879837}, {"id": 36, "seek": 23000, "start": 236.64, "end": 242.48, "text": " we probably don't need to react as much anymore if you're using open AI. And also you're probably", "tokens": [50696, 321, 1391, 500, 380, 643, 281, 4515, 382, 709, 3602, 498, 291, 434, 1228, 1269, 7318, 13, 400, 611, 291, 434, 1391, 50988], "temperature": 0.0, "avg_logprob": -0.11430096826633486, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.023316550999879837}, {"id": 37, "seek": 23000, "start": 242.48, "end": 248.24, "text": " not going to need as many of the output passes because you'll be able to define functions where", "tokens": [50988, 406, 516, 281, 643, 382, 867, 295, 264, 5598, 11335, 570, 291, 603, 312, 1075, 281, 6964, 6828, 689, 51276], "temperature": 0.0, "avg_logprob": -0.11430096826633486, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.023316550999879837}, {"id": 38, "seek": 23000, "start": 248.24, "end": 253.6, "text": " really you're just asking it to get the JSON data back in this. And that's something that you'll be", "tokens": [51276, 534, 291, 434, 445, 3365, 309, 281, 483, 264, 31828, 1412, 646, 294, 341, 13, 400, 300, 311, 746, 300, 291, 603, 312, 51544], "temperature": 0.0, "avg_logprob": -0.11430096826633486, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.023316550999879837}, {"id": 39, "seek": 23000, "start": 253.6, "end": 258.32, "text": " able to do. So that gives some examples here. They talk about creating chatbots that can use", "tokens": [51544, 1075, 281, 360, 13, 407, 300, 2709, 512, 5110, 510, 13, 814, 751, 466, 4084, 5081, 65, 1971, 300, 393, 764, 51780], "temperature": 0.0, "avg_logprob": -0.11430096826633486, "compression_ratio": 1.7067137809187278, "no_speech_prob": 0.023316550999879837}, {"id": 40, "seek": 25832, "start": 258.4, "end": 266.24, "text": " external tools. And you would basically have a function where you give an example of email,", "tokens": [50368, 8320, 3873, 13, 400, 291, 576, 1936, 362, 257, 2445, 689, 291, 976, 364, 1365, 295, 3796, 11, 50760], "temperature": 0.0, "avg_logprob": -0.1307408469063895, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.004680203273892403}, {"id": 41, "seek": 25832, "start": 266.24, "end": 271.28, "text": " someone if they want to get coffee by next Friday, and that the function you'd pass in would be", "tokens": [50760, 1580, 498, 436, 528, 281, 483, 4982, 538, 958, 6984, 11, 293, 300, 264, 2445, 291, 1116, 1320, 294, 576, 312, 51012], "temperature": 0.0, "avg_logprob": -0.1307408469063895, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.004680203273892403}, {"id": 42, "seek": 25832, "start": 271.28, "end": 277.6, "text": " something like a send email to and then a body for the email. And then the GPT model,", "tokens": [51012, 746, 411, 257, 2845, 3796, 281, 293, 550, 257, 1772, 337, 264, 3796, 13, 400, 550, 264, 26039, 51, 2316, 11, 51328], "temperature": 0.0, "avg_logprob": -0.1307408469063895, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.004680203273892403}, {"id": 43, "seek": 25832, "start": 277.6, "end": 283.76, "text": " whether it's the GPT-4 or the GPT-3.5 turbo, don't forget both of these models have now been", "tokens": [51328, 1968, 309, 311, 264, 26039, 51, 12, 19, 420, 264, 26039, 51, 12, 18, 13, 20, 20902, 11, 500, 380, 2870, 1293, 295, 613, 5245, 362, 586, 668, 51636], "temperature": 0.0, "avg_logprob": -0.1307408469063895, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.004680203273892403}, {"id": 44, "seek": 28376, "start": 283.76, "end": 291.12, "text": " tuned for doing this. It will actually insert the two string and also the body string in here.", "tokens": [50364, 10870, 337, 884, 341, 13, 467, 486, 767, 8969, 264, 732, 6798, 293, 611, 264, 1772, 6798, 294, 510, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1265806111422452, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.019117407500743866}, {"id": 45, "seek": 28376, "start": 291.12, "end": 295.44, "text": " I'm going to get another example they give is weather API. So what's the weather like in Boston?", "tokens": [50732, 286, 478, 516, 281, 483, 1071, 1365, 436, 976, 307, 5503, 9362, 13, 407, 437, 311, 264, 5503, 411, 294, 12333, 30, 50948], "temperature": 0.0, "avg_logprob": -0.1265806111422452, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.019117407500743866}, {"id": 46, "seek": 28376, "start": 296.15999999999997, "end": 302.48, "text": " The function will be get current weather. And that would basically pass in the location", "tokens": [50984, 440, 2445, 486, 312, 483, 2190, 5503, 13, 400, 300, 576, 1936, 1320, 294, 264, 4914, 51300], "temperature": 0.0, "avg_logprob": -0.1265806111422452, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.019117407500743866}, {"id": 47, "seek": 28376, "start": 302.48, "end": 307.76, "text": " and then the unit of measurement for the amount of degrees, etc. So this is one thing that you", "tokens": [51300, 293, 550, 264, 4985, 295, 13160, 337, 264, 2372, 295, 5310, 11, 5183, 13, 407, 341, 307, 472, 551, 300, 291, 51564], "temperature": 0.0, "avg_logprob": -0.1265806111422452, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.019117407500743866}, {"id": 48, "seek": 28376, "start": 307.76, "end": 313.59999999999997, "text": " can do with this. Another one is that you can convert natural language into API calls or database", "tokens": [51564, 393, 360, 365, 341, 13, 3996, 472, 307, 300, 291, 393, 7620, 3303, 2856, 666, 9362, 5498, 420, 8149, 51856], "temperature": 0.0, "avg_logprob": -0.1265806111422452, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.019117407500743866}, {"id": 49, "seek": 31360, "start": 313.6, "end": 321.92, "text": " queries. So this gives you a whole set of ways to basically now query databases just by peering", "tokens": [50364, 24109, 13, 407, 341, 2709, 291, 257, 1379, 992, 295, 2098, 281, 1936, 586, 14581, 22380, 445, 538, 520, 1794, 50780], "temperature": 0.0, "avg_logprob": -0.12203691246804227, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0008423698018305004}, {"id": 50, "seek": 31360, "start": 321.92, "end": 328.72, "text": " open AI, defining your functions, and then it can basically return back as a SQL query. It can", "tokens": [50780, 1269, 7318, 11, 17827, 428, 6828, 11, 293, 550, 309, 393, 1936, 2736, 646, 382, 257, 19200, 14581, 13, 467, 393, 51120], "temperature": 0.0, "avg_logprob": -0.12203691246804227, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0008423698018305004}, {"id": 51, "seek": 31360, "start": 328.72, "end": 336.48, "text": " return back as a specific call to an API for this stuff. So the third use that they mentioned", "tokens": [51120, 2736, 646, 382, 257, 2685, 818, 281, 364, 9362, 337, 341, 1507, 13, 407, 264, 2636, 764, 300, 436, 2835, 51508], "temperature": 0.0, "avg_logprob": -0.12203691246804227, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0008423698018305004}, {"id": 52, "seek": 31360, "start": 336.48, "end": 343.04, "text": " is the idea of extracting structured data from text. So this is like the core package that", "tokens": [51508, 307, 264, 1558, 295, 49844, 18519, 1412, 490, 2487, 13, 407, 341, 307, 411, 264, 4965, 7372, 300, 51836], "temperature": 0.0, "avg_logprob": -0.12203691246804227, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0008423698018305004}, {"id": 53, "seek": 34304, "start": 343.04, "end": 348.8, "text": " used open AI to basically extract data out of things. You can see here basically now you can", "tokens": [50364, 1143, 1269, 7318, 281, 1936, 8947, 1412, 484, 295, 721, 13, 509, 393, 536, 510, 1936, 586, 291, 393, 50652], "temperature": 0.0, "avg_logprob": -0.08730104328256792, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.0035355885047465563}, {"id": 54, "seek": 34304, "start": 348.8, "end": 354.72, "text": " just define this as a function. Tell it that we want to extract people data and then we want to", "tokens": [50652, 445, 6964, 341, 382, 257, 2445, 13, 5115, 309, 300, 321, 528, 281, 8947, 561, 1412, 293, 550, 321, 528, 281, 50948], "temperature": 0.0, "avg_logprob": -0.08730104328256792, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.0035355885047465563}, {"id": 55, "seek": 34304, "start": 354.72, "end": 360.64000000000004, "text": " extract the name, the birth date, the location, and it will be able to pass through and extract that", "tokens": [50948, 8947, 264, 1315, 11, 264, 3965, 4002, 11, 264, 4914, 11, 293, 309, 486, 312, 1075, 281, 1320, 807, 293, 8947, 300, 51244], "temperature": 0.0, "avg_logprob": -0.08730104328256792, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.0035355885047465563}, {"id": 56, "seek": 34304, "start": 360.64000000000004, "end": 367.12, "text": " and return that information as JSON to you. So again, this is something that before would have", "tokens": [51244, 293, 2736, 300, 1589, 382, 31828, 281, 291, 13, 407, 797, 11, 341, 307, 746, 300, 949, 576, 362, 51568], "temperature": 0.0, "avg_logprob": -0.08730104328256792, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.0035355885047465563}, {"id": 57, "seek": 34304, "start": 367.12, "end": 372.8, "text": " taken a lot of prompt manipulation, used up a lot more tokens. Now we're able to basically", "tokens": [51568, 2726, 257, 688, 295, 12391, 26475, 11, 1143, 493, 257, 688, 544, 22667, 13, 823, 321, 434, 1075, 281, 1936, 51852], "temperature": 0.0, "avg_logprob": -0.08730104328256792, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.0035355885047465563}, {"id": 58, "seek": 37280, "start": 372.8, "end": 379.76, "text": " do this with just providing a function definition to the model and it will be able to go through it", "tokens": [50364, 360, 341, 365, 445, 6530, 257, 2445, 7123, 281, 264, 2316, 293, 309, 486, 312, 1075, 281, 352, 807, 309, 50712], "temperature": 0.0, "avg_logprob": -0.09506819678134605, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.003428039839491248}, {"id": 59, "seek": 37280, "start": 379.76, "end": 384.64, "text": " that way. So if we come down here and have a look at this, now in this video, I'm just going to go", "tokens": [50712, 300, 636, 13, 407, 498, 321, 808, 760, 510, 293, 362, 257, 574, 412, 341, 11, 586, 294, 341, 960, 11, 286, 478, 445, 516, 281, 352, 50956], "temperature": 0.0, "avg_logprob": -0.09506819678134605, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.003428039839491248}, {"id": 60, "seek": 37280, "start": 384.64, "end": 389.68, "text": " through this here. I'm going to do another video. I've already started to put together a notebook", "tokens": [50956, 807, 341, 510, 13, 286, 478, 516, 281, 360, 1071, 960, 13, 286, 600, 1217, 1409, 281, 829, 1214, 257, 21060, 51208], "temperature": 0.0, "avg_logprob": -0.09506819678134605, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.003428039839491248}, {"id": 61, "seek": 37280, "start": 389.68, "end": 395.04, "text": " for this of getting it to work with leg chain. So I'll do this in another video straight after", "tokens": [51208, 337, 341, 295, 1242, 309, 281, 589, 365, 1676, 5021, 13, 407, 286, 603, 360, 341, 294, 1071, 960, 2997, 934, 51476], "temperature": 0.0, "avg_logprob": -0.09506819678134605, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.003428039839491248}, {"id": 62, "seek": 37280, "start": 395.04, "end": 401.68, "text": " this, but just showing you here what is actually going on here. So remember that we're talking about", "tokens": [51476, 341, 11, 457, 445, 4099, 291, 510, 437, 307, 767, 516, 322, 510, 13, 407, 1604, 300, 321, 434, 1417, 466, 51808], "temperature": 0.0, "avg_logprob": -0.09506819678134605, "compression_ratio": 1.8154981549815499, "no_speech_prob": 0.003428039839491248}, {"id": 63, "seek": 40168, "start": 402.32, "end": 410.72, "text": " GBT 3.5 model and the full model. So they use the sort of chat format of where you've got a system", "tokens": [50396, 26809, 51, 805, 13, 20, 2316, 293, 264, 1577, 2316, 13, 407, 436, 764, 264, 1333, 295, 5081, 7877, 295, 689, 291, 600, 658, 257, 1185, 50816], "temperature": 0.0, "avg_logprob": -0.12535172756587235, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.004066914785653353}, {"id": 64, "seek": 40168, "start": 410.72, "end": 418.8, "text": " token, you've got your human message, even AI message. So that's what's going on here. If we look", "tokens": [50816, 14862, 11, 291, 600, 658, 428, 1952, 3636, 11, 754, 7318, 3636, 13, 407, 300, 311, 437, 311, 516, 322, 510, 13, 759, 321, 574, 51220], "temperature": 0.0, "avg_logprob": -0.12535172756587235, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.004066914785653353}, {"id": 65, "seek": 40168, "start": 418.8, "end": 424.56, "text": " at like we're calling the function with a user input, and you'll see that, okay, we're basically", "tokens": [51220, 412, 411, 321, 434, 5141, 264, 2445, 365, 257, 4195, 4846, 11, 293, 291, 603, 536, 300, 11, 1392, 11, 321, 434, 1936, 51508], "temperature": 0.0, "avg_logprob": -0.12535172756587235, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.004066914785653353}, {"id": 66, "seek": 40168, "start": 424.56, "end": 429.12, "text": " passing in the model, we've got our messages, and then we've got the role. So the first one", "tokens": [51508, 8437, 294, 264, 2316, 11, 321, 600, 658, 527, 7897, 11, 293, 550, 321, 600, 658, 264, 3090, 13, 407, 264, 700, 472, 51736], "temperature": 0.0, "avg_logprob": -0.12535172756587235, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.004066914785653353}, {"id": 67, "seek": 42912, "start": 429.12, "end": 435.52, "text": " role is user, which is a language. This is called human. And then basically the content. Now the", "tokens": [50364, 3090, 307, 4195, 11, 597, 307, 257, 2856, 13, 639, 307, 1219, 1952, 13, 400, 550, 1936, 264, 2701, 13, 823, 264, 50684], "temperature": 0.0, "avg_logprob": -0.15153351235896984, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.006485650781542063}, {"id": 68, "seek": 42912, "start": 435.52, "end": 442.48, "text": " other thing that we pass in is we need to pass in a function definition or technically we pass in", "tokens": [50684, 661, 551, 300, 321, 1320, 294, 307, 321, 643, 281, 1320, 294, 257, 2445, 7123, 420, 12120, 321, 1320, 294, 51032], "temperature": 0.0, "avg_logprob": -0.15153351235896984, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.006485650781542063}, {"id": 69, "seek": 42912, "start": 442.48, "end": 449.28000000000003, "text": " a list of function definitions. So here you can say this is the list that we've got here. And what", "tokens": [51032, 257, 1329, 295, 2445, 21988, 13, 407, 510, 291, 393, 584, 341, 307, 264, 1329, 300, 321, 600, 658, 510, 13, 400, 437, 51372], "temperature": 0.0, "avg_logprob": -0.15153351235896984, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.006485650781542063}, {"id": 70, "seek": 42912, "start": 449.28000000000003, "end": 454.88, "text": " we're actually got here is basically only one function being passed in. The name of that is", "tokens": [51372, 321, 434, 767, 658, 510, 307, 1936, 787, 472, 2445, 885, 4678, 294, 13, 440, 1315, 295, 300, 307, 51652], "temperature": 0.0, "avg_logprob": -0.15153351235896984, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.006485650781542063}, {"id": 71, "seek": 45488, "start": 454.96, "end": 459.28, "text": " getCurrentWither, which makes sense because the person who's asking what is the weather in Boston,", "tokens": [50368, 483, 34, 374, 1753, 54, 1839, 11, 597, 1669, 2020, 570, 264, 954, 567, 311, 3365, 437, 307, 264, 5503, 294, 12333, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1136522392431895, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.03206166252493858}, {"id": 72, "seek": 45488, "start": 460.0, "end": 466.32, "text": " we then basically, you can see that the parameters of the function, and the main ones here are the", "tokens": [50620, 321, 550, 1936, 11, 291, 393, 536, 300, 264, 9834, 295, 264, 2445, 11, 293, 264, 2135, 2306, 510, 366, 264, 50936], "temperature": 0.0, "avg_logprob": -0.1136522392431895, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.03206166252493858}, {"id": 73, "seek": 45488, "start": 466.32, "end": 473.44, "text": " name of it, the description of it, the description of actually what the function does, and then the", "tokens": [50936, 1315, 295, 309, 11, 264, 3855, 295, 309, 11, 264, 3855, 295, 767, 437, 264, 2445, 775, 11, 293, 550, 264, 51292], "temperature": 0.0, "avg_logprob": -0.1136522392431895, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.03206166252493858}, {"id": 74, "seek": 45488, "start": 473.44, "end": 478.88, "text": " actual properties that it needs to return, get a description as well. So you can see here we've", "tokens": [51292, 3539, 7221, 300, 309, 2203, 281, 2736, 11, 483, 257, 3855, 382, 731, 13, 407, 291, 393, 536, 510, 321, 600, 51564], "temperature": 0.0, "avg_logprob": -0.1136522392431895, "compression_ratio": 1.827906976744186, "no_speech_prob": 0.03206166252493858}, {"id": 75, "seek": 47888, "start": 478.88, "end": 485.52, "text": " basically got location, going to be a string, and it's going to be a city and state,", "tokens": [50364, 1936, 658, 4914, 11, 516, 281, 312, 257, 6798, 11, 293, 309, 311, 516, 281, 312, 257, 2307, 293, 1785, 11, 50696], "temperature": 0.0, "avg_logprob": -0.15721052997517135, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.06553995609283447}, {"id": 76, "seek": 47888, "start": 486.08, "end": 491.28, "text": " G San Francisco, California. And then the second thing is going to be a unit. So this is going to", "tokens": [50724, 460, 5271, 12279, 11, 5384, 13, 400, 550, 264, 1150, 551, 307, 516, 281, 312, 257, 4985, 13, 407, 341, 307, 516, 281, 50984], "temperature": 0.0, "avg_logprob": -0.15721052997517135, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.06553995609283447}, {"id": 77, "seek": 47888, "start": 491.28, "end": 498.32, "text": " be either Celsius or Fahrenheit. So you can see this is an enum, so it's restricting just to these", "tokens": [50984, 312, 2139, 22658, 420, 31199, 13, 407, 291, 393, 536, 341, 307, 364, 465, 449, 11, 370, 309, 311, 1472, 37714, 445, 281, 613, 51336], "temperature": 0.0, "avg_logprob": -0.15721052997517135, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.06553995609283447}, {"id": 78, "seek": 47888, "start": 498.32, "end": 506.4, "text": " two things. So we can't say, oh, tell me what the, or rather the model can't just say, make up some", "tokens": [51336, 732, 721, 13, 407, 321, 393, 380, 584, 11, 1954, 11, 980, 385, 437, 264, 11, 420, 2831, 264, 2316, 393, 380, 445, 584, 11, 652, 493, 512, 51740], "temperature": 0.0, "avg_logprob": -0.15721052997517135, "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.06553995609283447}, {"id": 79, "seek": 50640, "start": 506.4, "end": 512.72, "text": " other unit. It needs to be one of these two that gets returned in this. Now we can see that neither", "tokens": [50364, 661, 4985, 13, 467, 2203, 281, 312, 472, 295, 613, 732, 300, 2170, 8752, 294, 341, 13, 823, 321, 393, 536, 300, 9662, 50680], "temperature": 0.0, "avg_logprob": -0.08607689414437361, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.011327776126563549}, {"id": 80, "seek": 50640, "start": 512.72, "end": 519.1999999999999, "text": " of these, this particular unit is not required, but the location is definitely required. So we", "tokens": [50680, 295, 613, 11, 341, 1729, 4985, 307, 406, 4739, 11, 457, 264, 4914, 307, 2138, 4739, 13, 407, 321, 51004], "temperature": 0.0, "avg_logprob": -0.08607689414437361, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.011327776126563549}, {"id": 81, "seek": 50640, "start": 519.1999999999999, "end": 525.28, "text": " can't just say, tell me what the weather is, because the GPT, unless it's got it already in the", "tokens": [51004, 393, 380, 445, 584, 11, 980, 385, 437, 264, 5503, 307, 11, 570, 264, 26039, 51, 11, 5969, 309, 311, 658, 309, 1217, 294, 264, 51308], "temperature": 0.0, "avg_logprob": -0.08607689414437361, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.011327776126563549}, {"id": 82, "seek": 50640, "start": 525.28, "end": 530.8, "text": " history of it, of your conversation, it's not going to know that. But if it is in the history,", "tokens": [51308, 2503, 295, 309, 11, 295, 428, 3761, 11, 309, 311, 406, 516, 281, 458, 300, 13, 583, 498, 309, 307, 294, 264, 2503, 11, 51584], "temperature": 0.0, "avg_logprob": -0.08607689414437361, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.011327776126563549}, {"id": 83, "seek": 50640, "start": 530.8, "end": 534.56, "text": " and we've been talking about a location, and I can say, tell me what the weather is like there now,", "tokens": [51584, 293, 321, 600, 668, 1417, 466, 257, 4914, 11, 293, 286, 393, 584, 11, 980, 385, 437, 264, 5503, 307, 411, 456, 586, 11, 51772], "temperature": 0.0, "avg_logprob": -0.08607689414437361, "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.011327776126563549}, {"id": 84, "seek": 53456, "start": 535.04, "end": 540.3199999999999, "text": " it will be able to do this. Now you can see here, this is basically just being quite simple. It's", "tokens": [50388, 309, 486, 312, 1075, 281, 360, 341, 13, 823, 291, 393, 536, 510, 11, 341, 307, 1936, 445, 885, 1596, 2199, 13, 467, 311, 50652], "temperature": 0.0, "avg_logprob": -0.10217114214627247, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.010481606237590313}, {"id": 85, "seek": 53456, "start": 540.3199999999999, "end": 546.88, "text": " just taking a location. You could imagine that you've got time, a time framing there, e.g. whether", "tokens": [50652, 445, 1940, 257, 4914, 13, 509, 727, 3811, 300, 291, 600, 658, 565, 11, 257, 565, 28971, 456, 11, 308, 13, 70, 13, 1968, 50980], "temperature": 0.0, "avg_logprob": -0.10217114214627247, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.010481606237590313}, {"id": 86, "seek": 53456, "start": 546.88, "end": 551.92, "text": " it's now, whether it's tomorrow, whether it's next week, that kind of thing as well. So this is what's", "tokens": [50980, 309, 311, 586, 11, 1968, 309, 311, 4153, 11, 1968, 309, 311, 958, 1243, 11, 300, 733, 295, 551, 382, 731, 13, 407, 341, 307, 437, 311, 51232], "temperature": 0.0, "avg_logprob": -0.10217114214627247, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.010481606237590313}, {"id": 87, "seek": 53456, "start": 551.92, "end": 558.8, "text": " getting passed into the model. We see that then we've got the response back from the model. So", "tokens": [51232, 1242, 4678, 666, 264, 2316, 13, 492, 536, 300, 550, 321, 600, 658, 264, 4134, 646, 490, 264, 2316, 13, 407, 51576], "temperature": 0.0, "avg_logprob": -0.10217114214627247, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.010481606237590313}, {"id": 88, "seek": 55880, "start": 559.52, "end": 564.64, "text": " it's basically just going to return back. And it's going to give you the details to basically call", "tokens": [50400, 309, 311, 1936, 445, 516, 281, 2736, 646, 13, 400, 309, 311, 516, 281, 976, 291, 264, 4365, 281, 1936, 818, 50656], "temperature": 0.0, "avg_logprob": -0.11257870664301607, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.004537535831332207}, {"id": 89, "seek": 55880, "start": 565.3599999999999, "end": 571.1999999999999, "text": " this particular function based on this. So if you're just passed in, hello, how are you today,", "tokens": [50692, 341, 1729, 2445, 2361, 322, 341, 13, 407, 498, 291, 434, 445, 4678, 294, 11, 7751, 11, 577, 366, 291, 965, 11, 50984], "temperature": 0.0, "avg_logprob": -0.11257870664301607, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.004537535831332207}, {"id": 90, "seek": 55880, "start": 571.1999999999999, "end": 576.64, "text": " it wouldn't use any of the function, it would just give an AI message back here. But you'll see when", "tokens": [50984, 309, 2759, 380, 764, 604, 295, 264, 2445, 11, 309, 576, 445, 976, 364, 7318, 3636, 646, 510, 13, 583, 291, 603, 536, 562, 51256], "temperature": 0.0, "avg_logprob": -0.11257870664301607, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.004537535831332207}, {"id": 91, "seek": 55880, "start": 576.64, "end": 583.52, "text": " I go through the notebook later on, that actually what's happening is it's returning back kind of", "tokens": [51256, 286, 352, 807, 264, 21060, 1780, 322, 11, 300, 767, 437, 311, 2737, 307, 309, 311, 12678, 646, 733, 295, 51600], "temperature": 0.0, "avg_logprob": -0.11257870664301607, "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.004537535831332207}, {"id": 92, "seek": 58352, "start": 583.6, "end": 589.28, "text": " like a blank message, but with a particular function called variable that's in there,", "tokens": [50368, 411, 257, 8247, 3636, 11, 457, 365, 257, 1729, 2445, 1219, 7006, 300, 311, 294, 456, 11, 50652], "temperature": 0.0, "avg_logprob": -0.12215485471360227, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.03566398844122887}, {"id": 93, "seek": 58352, "start": 589.28, "end": 594.8, "text": " that's going on in there. You can see that once it basically calls this API, it gets this stuff back.", "tokens": [50652, 300, 311, 516, 322, 294, 456, 13, 509, 393, 536, 300, 1564, 309, 1936, 5498, 341, 9362, 11, 309, 2170, 341, 1507, 646, 13, 50928], "temperature": 0.0, "avg_logprob": -0.12215485471360227, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.03566398844122887}, {"id": 94, "seek": 58352, "start": 594.8, "end": 600.24, "text": " So it's getting back the temperature, the unit Celsius to description sunny. And then finally,", "tokens": [50928, 407, 309, 311, 1242, 646, 264, 4292, 11, 264, 4985, 22658, 281, 3855, 20412, 13, 400, 550, 2721, 11, 51200], "temperature": 0.0, "avg_logprob": -0.12215485471360227, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.03566398844122887}, {"id": 95, "seek": 58352, "start": 600.24, "end": 608.16, "text": " it passes that back to open AI. So it passes that back to the GPT model. You can see here,", "tokens": [51200, 309, 11335, 300, 646, 281, 1269, 7318, 13, 407, 309, 11335, 300, 646, 281, 264, 26039, 51, 2316, 13, 509, 393, 536, 510, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12215485471360227, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.03566398844122887}, {"id": 96, "seek": 60816, "start": 608.16, "end": 615.92, "text": " we're basically passing back the list of messages. And we've gone user assistant function. So this is", "tokens": [50364, 321, 434, 1936, 8437, 646, 264, 1329, 295, 7897, 13, 400, 321, 600, 2780, 4195, 10994, 2445, 13, 407, 341, 307, 50752], "temperature": 0.0, "avg_logprob": -0.09305402856124075, "compression_ratio": 1.9138755980861244, "no_speech_prob": 0.047376491129398346}, {"id": 97, "seek": 60816, "start": 615.92, "end": 621.1999999999999, "text": " a very special type of message. It's not something that we've had before is a new kind of role that", "tokens": [50752, 257, 588, 2121, 2010, 295, 3636, 13, 467, 311, 406, 746, 300, 321, 600, 632, 949, 307, 257, 777, 733, 295, 3090, 300, 51016], "temperature": 0.0, "avg_logprob": -0.09305402856124075, "compression_ratio": 1.9138755980861244, "no_speech_prob": 0.047376491129398346}, {"id": 98, "seek": 60816, "start": 621.1999999999999, "end": 629.1999999999999, "text": " we've got there. And this is basically passing back the response from that function. So you can see", "tokens": [51016, 321, 600, 658, 456, 13, 400, 341, 307, 1936, 8437, 646, 264, 4134, 490, 300, 2445, 13, 407, 291, 393, 536, 51416], "temperature": 0.0, "avg_logprob": -0.09305402856124075, "compression_ratio": 1.9138755980861244, "no_speech_prob": 0.047376491129398346}, {"id": 99, "seek": 60816, "start": 629.1999999999999, "end": 635.1999999999999, "text": " the assistant passed out that it should use this function. So the assistant passed out that, okay,", "tokens": [51416, 264, 10994, 4678, 484, 300, 309, 820, 764, 341, 2445, 13, 407, 264, 10994, 4678, 484, 300, 11, 1392, 11, 51716], "temperature": 0.0, "avg_logprob": -0.09305402856124075, "compression_ratio": 1.9138755980861244, "no_speech_prob": 0.047376491129398346}, {"id": 100, "seek": 63520, "start": 635.2, "end": 642.6400000000001, "text": " you should use the name function, get current weather, and you should be using Boston messages.", "tokens": [50364, 291, 820, 764, 264, 1315, 2445, 11, 483, 2190, 5503, 11, 293, 291, 820, 312, 1228, 12333, 7897, 13, 50736], "temperature": 0.0, "avg_logprob": -0.13765819867451987, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.00926340464502573}, {"id": 101, "seek": 63520, "start": 642.6400000000001, "end": 648.08, "text": " So it's worked out the state here itself, right? So it's not like your user is going to have to", "tokens": [50736, 407, 309, 311, 2732, 484, 264, 1785, 510, 2564, 11, 558, 30, 407, 309, 311, 406, 411, 428, 4195, 307, 516, 281, 362, 281, 51008], "temperature": 0.0, "avg_logprob": -0.13765819867451987, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.00926340464502573}, {"id": 102, "seek": 63520, "start": 648.08, "end": 654.1600000000001, "text": " tell everything if it can work out certain things, you can work them out quite nicely in that way.", "tokens": [51008, 980, 1203, 498, 309, 393, 589, 484, 1629, 721, 11, 291, 393, 589, 552, 484, 1596, 9594, 294, 300, 636, 13, 51312], "temperature": 0.0, "avg_logprob": -0.13765819867451987, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.00926340464502573}, {"id": 103, "seek": 63520, "start": 654.1600000000001, "end": 660.96, "text": " It gets the return back here. And then it's going to generate a response out. And the response that", "tokens": [51312, 467, 2170, 264, 2736, 646, 510, 13, 400, 550, 309, 311, 516, 281, 8460, 257, 4134, 484, 13, 400, 264, 4134, 300, 51652], "temperature": 0.0, "avg_logprob": -0.13765819867451987, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.00926340464502573}, {"id": 104, "seek": 66096, "start": 661.0400000000001, "end": 666.48, "text": " it generates out is the weather in Boston is currently sunny with a temperature of 22 degrees", "tokens": [50368, 309, 23815, 484, 307, 264, 5503, 294, 12333, 307, 4362, 20412, 365, 257, 4292, 295, 5853, 5310, 50640], "temperature": 0.0, "avg_logprob": -0.07549203895941013, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.008310419507324696}, {"id": 105, "seek": 66096, "start": 666.48, "end": 674.0, "text": " Celsius. And that's basically taken in these messages to then finally generate out this message.", "tokens": [50640, 22658, 13, 400, 300, 311, 1936, 2726, 294, 613, 7897, 281, 550, 2721, 8460, 484, 341, 3636, 13, 51016], "temperature": 0.0, "avg_logprob": -0.07549203895941013, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.008310419507324696}, {"id": 106, "seek": 66096, "start": 674.0, "end": 679.44, "text": " So you can see we are using probably a decent amount more tokens for the actual functions", "tokens": [51016, 407, 291, 393, 536, 321, 366, 1228, 1391, 257, 8681, 2372, 544, 22667, 337, 264, 3539, 6828, 51288], "temperature": 0.0, "avg_logprob": -0.07549203895941013, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.008310419507324696}, {"id": 107, "seek": 66096, "start": 679.44, "end": 685.44, "text": " definitions that we're passing in there. But we're not needing to do any in context learning,", "tokens": [51288, 21988, 300, 321, 434, 8437, 294, 456, 13, 583, 321, 434, 406, 18006, 281, 360, 604, 294, 4319, 2539, 11, 51588], "temperature": 0.0, "avg_logprob": -0.07549203895941013, "compression_ratio": 1.6051502145922747, "no_speech_prob": 0.008310419507324696}, {"id": 108, "seek": 68544, "start": 685.44, "end": 691.2800000000001, "text": " we're not needing to do any sort of examples or things like that. And it kind of manages the", "tokens": [50364, 321, 434, 406, 18006, 281, 360, 604, 1333, 295, 5110, 420, 721, 411, 300, 13, 400, 309, 733, 295, 22489, 264, 50656], "temperature": 0.0, "avg_logprob": -0.0934048796122053, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.11113324761390686}, {"id": 109, "seek": 68544, "start": 691.2800000000001, "end": 696.8800000000001, "text": " scratch pad itself internally. So unlike things we've done before with React, where we had to", "tokens": [50656, 8459, 6887, 2564, 19501, 13, 407, 8343, 721, 321, 600, 1096, 949, 365, 30644, 11, 689, 321, 632, 281, 50936], "temperature": 0.0, "avg_logprob": -0.0934048796122053, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.11113324761390686}, {"id": 110, "seek": 68544, "start": 696.8800000000001, "end": 702.1600000000001, "text": " have a scratch pad, etc, opening eyes, taking care of that by putting it in these parts here.", "tokens": [50936, 362, 257, 8459, 6887, 11, 5183, 11, 5193, 2575, 11, 1940, 1127, 295, 300, 538, 3372, 309, 294, 613, 3166, 510, 13, 51200], "temperature": 0.0, "avg_logprob": -0.0934048796122053, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.11113324761390686}, {"id": 111, "seek": 68544, "start": 702.1600000000001, "end": 708.32, "text": " In fact, you could think of this function role as the scratch pad of what came back from the", "tokens": [51200, 682, 1186, 11, 291, 727, 519, 295, 341, 2445, 3090, 382, 264, 8459, 6887, 295, 437, 1361, 646, 490, 264, 51508], "temperature": 0.0, "avg_logprob": -0.0934048796122053, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.11113324761390686}, {"id": 112, "seek": 68544, "start": 708.32, "end": 714.72, "text": " particular function call that we had there. So this gives you a rough idea of how these things", "tokens": [51508, 1729, 2445, 818, 300, 321, 632, 456, 13, 407, 341, 2709, 291, 257, 5903, 1558, 295, 577, 613, 721, 51828], "temperature": 0.0, "avg_logprob": -0.0934048796122053, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.11113324761390686}, {"id": 113, "seek": 71472, "start": 714.72, "end": 719.6, "text": " work functions. In the next video, I will go through a whole bunch of code, we'll look at", "tokens": [50364, 589, 6828, 13, 682, 264, 958, 960, 11, 286, 486, 352, 807, 257, 1379, 3840, 295, 3089, 11, 321, 603, 574, 412, 50608], "temperature": 0.0, "avg_logprob": -0.11987858951681911, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.02000165730714798}, {"id": 114, "seek": 71472, "start": 719.6, "end": 724.5600000000001, "text": " how to put it in a length chain, both manually and then turn it into an agent and use it as an", "tokens": [50608, 577, 281, 829, 309, 294, 257, 4641, 5021, 11, 1293, 16945, 293, 550, 1261, 309, 666, 364, 9461, 293, 764, 309, 382, 364, 50856], "temperature": 0.0, "avg_logprob": -0.11987858951681911, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.02000165730714798}, {"id": 115, "seek": 71472, "start": 724.5600000000001, "end": 729.36, "text": " agent as well. As always, if you like the video, please click like and subscribe. If you've got", "tokens": [50856, 9461, 382, 731, 13, 1018, 1009, 11, 498, 291, 411, 264, 960, 11, 1767, 2052, 411, 293, 3022, 13, 759, 291, 600, 658, 51096], "temperature": 0.0, "avg_logprob": -0.11987858951681911, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.02000165730714798}, {"id": 116, "seek": 71472, "start": 729.36, "end": 734.96, "text": " any questions, please put them in the comments below. I'll see you in the next video. Bye for now.", "tokens": [51096, 604, 1651, 11, 1767, 829, 552, 294, 264, 3053, 2507, 13, 286, 603, 536, 291, 294, 264, 958, 960, 13, 4621, 337, 586, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11987858951681911, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.02000165730714798}], "language": "en"}