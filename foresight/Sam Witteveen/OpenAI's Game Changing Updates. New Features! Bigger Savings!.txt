Okay. So we have a new announcement from OpenAI. This has just come out in the past 12 hours or so,
and basically they're announcing a couple of new things. The main one being this idea of
function calling, which I'll go through in a second in detail. But the other things that
they're announcing, which are kind of interesting, are more steerable API models, longer context and
lower prices. So what does more steerable API models mean? The thing that I would say this means
is that the GPT-4 model have been able to be influenced by the system prompt a lot more than
the GPT-3.5 turbo model. So perhaps in these new versions, we're actually going to see the GPT-3.5
turbo actually respond to the system prompt a lot more than it has been in the past. The other thing
that this also probably refers to is the actual sort of function idea here as well. So let's go down,
let's have a look at what's coming out. So we can see that the model has been updated. This is
one of the key things, along with this sort of announcing the deprecation of some of the previous
sort of checkpoints that they were making available for people to use. This basically is only an
issue if you were using a very specific checkpoint to make sure that it responded to a prompt in the
same way. So definitely we've seen as new versions of the GPT-3.5 turbo have come out that they
respond slightly different to the prompts as we go through them. As I mentioned earlier, we've got
more steerable versions of GPT-4 and 3.5 turbo. Another fantastic thing for the GPT-3.5 turbo
is the addition of this new 16k context version of this. So before this version, basically we only
had access to a 4k context window. And now we're going to basically have right up to 16k. So that
is a huge difference. That really makes the difference for what things like summarization,
you can probably go right out to an hour of YouTube. Now with this, things like archive
papers, you can do full papers in one shot. They should fit in the context window now.
A whole bunch of things that this suddenly going to become useful for as we go forward.
The other thing is that the cost of the standard GPT-3.5 turbo has been reduced another 25% as well.
So that's something that's interesting. And there are further down here to the cost of what these
new GPT-3.5 turbo 16k is going to be. So it's going to be 0.3 cents per 1000 tokens on the input
and 0.4 per 1000 tokens on the output here. As they mentioned here that you can now support
about 20 pages of text in a single response. This really is a huge factor going forward for this.
Now by the far the most interesting thing that they released in this is the idea of function
calling. So what actually is this idea of function calling? So if you've watched some of my previous
videos, I've talked about the idea of react and doing reasoning by putting in certain prompts to
basically get the model to tell us what tools to use out that kind of thing. What they've done here
is basically bait that into the model now. So they actually mentioned these models have been
fine-tuned to both detect when a function needs to be called and to respond with Jason the adheres
to the function signature. So basically now we can just pass in a definition of a tool or the
definition of something that we wanted to do. And it will give us back a structured reasoned output
based on the text that the user puts in here. So this basically changes a number of things. One,
we probably don't need to react as much anymore if you're using open AI. And also you're probably
not going to need as many of the output passes because you'll be able to define functions where
really you're just asking it to get the JSON data back in this. And that's something that you'll be
able to do. So that gives some examples here. They talk about creating chatbots that can use
external tools. And you would basically have a function where you give an example of email,
someone if they want to get coffee by next Friday, and that the function you'd pass in would be
something like a send email to and then a body for the email. And then the GPT model,
whether it's the GPT-4 or the GPT-3.5 turbo, don't forget both of these models have now been
tuned for doing this. It will actually insert the two string and also the body string in here.
I'm going to get another example they give is weather API. So what's the weather like in Boston?
The function will be get current weather. And that would basically pass in the location
and then the unit of measurement for the amount of degrees, etc. So this is one thing that you
can do with this. Another one is that you can convert natural language into API calls or database
queries. So this gives you a whole set of ways to basically now query databases just by peering
open AI, defining your functions, and then it can basically return back as a SQL query. It can
return back as a specific call to an API for this stuff. So the third use that they mentioned
is the idea of extracting structured data from text. So this is like the core package that
used open AI to basically extract data out of things. You can see here basically now you can
just define this as a function. Tell it that we want to extract people data and then we want to
extract the name, the birth date, the location, and it will be able to pass through and extract that
and return that information as JSON to you. So again, this is something that before would have
taken a lot of prompt manipulation, used up a lot more tokens. Now we're able to basically
do this with just providing a function definition to the model and it will be able to go through it
that way. So if we come down here and have a look at this, now in this video, I'm just going to go
through this here. I'm going to do another video. I've already started to put together a notebook
for this of getting it to work with leg chain. So I'll do this in another video straight after
this, but just showing you here what is actually going on here. So remember that we're talking about
GBT 3.5 model and the full model. So they use the sort of chat format of where you've got a system
token, you've got your human message, even AI message. So that's what's going on here. If we look
at like we're calling the function with a user input, and you'll see that, okay, we're basically
passing in the model, we've got our messages, and then we've got the role. So the first one
role is user, which is a language. This is called human. And then basically the content. Now the
other thing that we pass in is we need to pass in a function definition or technically we pass in
a list of function definitions. So here you can say this is the list that we've got here. And what
we're actually got here is basically only one function being passed in. The name of that is
getCurrentWither, which makes sense because the person who's asking what is the weather in Boston,
we then basically, you can see that the parameters of the function, and the main ones here are the
name of it, the description of it, the description of actually what the function does, and then the
actual properties that it needs to return, get a description as well. So you can see here we've
basically got location, going to be a string, and it's going to be a city and state,
G San Francisco, California. And then the second thing is going to be a unit. So this is going to
be either Celsius or Fahrenheit. So you can see this is an enum, so it's restricting just to these
two things. So we can't say, oh, tell me what the, or rather the model can't just say, make up some
other unit. It needs to be one of these two that gets returned in this. Now we can see that neither
of these, this particular unit is not required, but the location is definitely required. So we
can't just say, tell me what the weather is, because the GPT, unless it's got it already in the
history of it, of your conversation, it's not going to know that. But if it is in the history,
and we've been talking about a location, and I can say, tell me what the weather is like there now,
it will be able to do this. Now you can see here, this is basically just being quite simple. It's
just taking a location. You could imagine that you've got time, a time framing there, e.g. whether
it's now, whether it's tomorrow, whether it's next week, that kind of thing as well. So this is what's
getting passed into the model. We see that then we've got the response back from the model. So
it's basically just going to return back. And it's going to give you the details to basically call
this particular function based on this. So if you're just passed in, hello, how are you today,
it wouldn't use any of the function, it would just give an AI message back here. But you'll see when
I go through the notebook later on, that actually what's happening is it's returning back kind of
like a blank message, but with a particular function called variable that's in there,
that's going on in there. You can see that once it basically calls this API, it gets this stuff back.
So it's getting back the temperature, the unit Celsius to description sunny. And then finally,
it passes that back to open AI. So it passes that back to the GPT model. You can see here,
we're basically passing back the list of messages. And we've gone user assistant function. So this is
a very special type of message. It's not something that we've had before is a new kind of role that
we've got there. And this is basically passing back the response from that function. So you can see
the assistant passed out that it should use this function. So the assistant passed out that, okay,
you should use the name function, get current weather, and you should be using Boston messages.
So it's worked out the state here itself, right? So it's not like your user is going to have to
tell everything if it can work out certain things, you can work them out quite nicely in that way.
It gets the return back here. And then it's going to generate a response out. And the response that
it generates out is the weather in Boston is currently sunny with a temperature of 22 degrees
Celsius. And that's basically taken in these messages to then finally generate out this message.
So you can see we are using probably a decent amount more tokens for the actual functions
definitions that we're passing in there. But we're not needing to do any in context learning,
we're not needing to do any sort of examples or things like that. And it kind of manages the
scratch pad itself internally. So unlike things we've done before with React, where we had to
have a scratch pad, etc, opening eyes, taking care of that by putting it in these parts here.
In fact, you could think of this function role as the scratch pad of what came back from the
particular function call that we had there. So this gives you a rough idea of how these things
work functions. In the next video, I will go through a whole bunch of code, we'll look at
how to put it in a length chain, both manually and then turn it into an agent and use it as an
agent as well. As always, if you like the video, please click like and subscribe. If you've got
any questions, please put them in the comments below. I'll see you in the next video. Bye for now.
