1
00:00:00,000 --> 00:00:06,080
Okay. So we have a new announcement from OpenAI. This has just come out in the past 12 hours or so,

2
00:00:06,080 --> 00:00:11,520
and basically they're announcing a couple of new things. The main one being this idea of

3
00:00:11,520 --> 00:00:17,200
function calling, which I'll go through in a second in detail. But the other things that

4
00:00:17,200 --> 00:00:23,920
they're announcing, which are kind of interesting, are more steerable API models, longer context and

5
00:00:23,920 --> 00:00:30,400
lower prices. So what does more steerable API models mean? The thing that I would say this means

6
00:00:30,400 --> 00:00:38,240
is that the GPT-4 model have been able to be influenced by the system prompt a lot more than

7
00:00:38,240 --> 00:00:46,240
the GPT-3.5 turbo model. So perhaps in these new versions, we're actually going to see the GPT-3.5

8
00:00:46,240 --> 00:00:52,160
turbo actually respond to the system prompt a lot more than it has been in the past. The other thing

9
00:00:52,240 --> 00:00:58,240
that this also probably refers to is the actual sort of function idea here as well. So let's go down,

10
00:00:58,240 --> 00:01:04,640
let's have a look at what's coming out. So we can see that the model has been updated. This is

11
00:01:04,640 --> 00:01:11,280
one of the key things, along with this sort of announcing the deprecation of some of the previous

12
00:01:11,280 --> 00:01:18,160
sort of checkpoints that they were making available for people to use. This basically is only an

13
00:01:18,160 --> 00:01:24,160
issue if you were using a very specific checkpoint to make sure that it responded to a prompt in the

14
00:01:24,160 --> 00:01:31,040
same way. So definitely we've seen as new versions of the GPT-3.5 turbo have come out that they

15
00:01:31,040 --> 00:01:35,520
respond slightly different to the prompts as we go through them. As I mentioned earlier, we've got

16
00:01:35,520 --> 00:01:44,000
more steerable versions of GPT-4 and 3.5 turbo. Another fantastic thing for the GPT-3.5 turbo

17
00:01:44,000 --> 00:01:52,560
is the addition of this new 16k context version of this. So before this version, basically we only

18
00:01:52,560 --> 00:02:00,000
had access to a 4k context window. And now we're going to basically have right up to 16k. So that

19
00:02:00,000 --> 00:02:05,600
is a huge difference. That really makes the difference for what things like summarization,

20
00:02:05,600 --> 00:02:12,720
you can probably go right out to an hour of YouTube. Now with this, things like archive

21
00:02:12,720 --> 00:02:18,000
papers, you can do full papers in one shot. They should fit in the context window now.

22
00:02:18,000 --> 00:02:23,280
A whole bunch of things that this suddenly going to become useful for as we go forward.

23
00:02:23,280 --> 00:02:30,080
The other thing is that the cost of the standard GPT-3.5 turbo has been reduced another 25% as well.

24
00:02:30,080 --> 00:02:34,640
So that's something that's interesting. And there are further down here to the cost of what these

25
00:02:34,640 --> 00:02:45,440
new GPT-3.5 turbo 16k is going to be. So it's going to be 0.3 cents per 1000 tokens on the input

26
00:02:45,440 --> 00:02:51,360
and 0.4 per 1000 tokens on the output here. As they mentioned here that you can now support

27
00:02:51,360 --> 00:02:59,040
about 20 pages of text in a single response. This really is a huge factor going forward for this.

28
00:02:59,040 --> 00:03:05,920
Now by the far the most interesting thing that they released in this is the idea of function

29
00:03:05,920 --> 00:03:11,760
calling. So what actually is this idea of function calling? So if you've watched some of my previous

30
00:03:11,760 --> 00:03:18,080
videos, I've talked about the idea of react and doing reasoning by putting in certain prompts to

31
00:03:18,080 --> 00:03:24,720
basically get the model to tell us what tools to use out that kind of thing. What they've done here

32
00:03:24,720 --> 00:03:30,000
is basically bait that into the model now. So they actually mentioned these models have been

33
00:03:30,000 --> 00:03:35,920
fine-tuned to both detect when a function needs to be called and to respond with Jason the adheres

34
00:03:35,920 --> 00:03:43,040
to the function signature. So basically now we can just pass in a definition of a tool or the

35
00:03:43,040 --> 00:03:50,000
definition of something that we wanted to do. And it will give us back a structured reasoned output

36
00:03:50,000 --> 00:03:56,640
based on the text that the user puts in here. So this basically changes a number of things. One,

37
00:03:56,640 --> 00:04:02,480
we probably don't need to react as much anymore if you're using open AI. And also you're probably

38
00:04:02,480 --> 00:04:08,240
not going to need as many of the output passes because you'll be able to define functions where

39
00:04:08,240 --> 00:04:13,600
really you're just asking it to get the JSON data back in this. And that's something that you'll be

40
00:04:13,600 --> 00:04:18,320
able to do. So that gives some examples here. They talk about creating chatbots that can use

41
00:04:18,400 --> 00:04:26,240
external tools. And you would basically have a function where you give an example of email,

42
00:04:26,240 --> 00:04:31,280
someone if they want to get coffee by next Friday, and that the function you'd pass in would be

43
00:04:31,280 --> 00:04:37,600
something like a send email to and then a body for the email. And then the GPT model,

44
00:04:37,600 --> 00:04:43,760
whether it's the GPT-4 or the GPT-3.5 turbo, don't forget both of these models have now been

45
00:04:43,760 --> 00:04:51,120
tuned for doing this. It will actually insert the two string and also the body string in here.

46
00:04:51,120 --> 00:04:55,440
I'm going to get another example they give is weather API. So what's the weather like in Boston?

47
00:04:56,160 --> 00:05:02,480
The function will be get current weather. And that would basically pass in the location

48
00:05:02,480 --> 00:05:07,760
and then the unit of measurement for the amount of degrees, etc. So this is one thing that you

49
00:05:07,760 --> 00:05:13,600
can do with this. Another one is that you can convert natural language into API calls or database

50
00:05:13,600 --> 00:05:21,920
queries. So this gives you a whole set of ways to basically now query databases just by peering

51
00:05:21,920 --> 00:05:28,720
open AI, defining your functions, and then it can basically return back as a SQL query. It can

52
00:05:28,720 --> 00:05:36,480
return back as a specific call to an API for this stuff. So the third use that they mentioned

53
00:05:36,480 --> 00:05:43,040
is the idea of extracting structured data from text. So this is like the core package that

54
00:05:43,040 --> 00:05:48,800
used open AI to basically extract data out of things. You can see here basically now you can

55
00:05:48,800 --> 00:05:54,720
just define this as a function. Tell it that we want to extract people data and then we want to

56
00:05:54,720 --> 00:06:00,640
extract the name, the birth date, the location, and it will be able to pass through and extract that

57
00:06:00,640 --> 00:06:07,120
and return that information as JSON to you. So again, this is something that before would have

58
00:06:07,120 --> 00:06:12,800
taken a lot of prompt manipulation, used up a lot more tokens. Now we're able to basically

59
00:06:12,800 --> 00:06:19,760
do this with just providing a function definition to the model and it will be able to go through it

60
00:06:19,760 --> 00:06:24,640
that way. So if we come down here and have a look at this, now in this video, I'm just going to go

61
00:06:24,640 --> 00:06:29,680
through this here. I'm going to do another video. I've already started to put together a notebook

62
00:06:29,680 --> 00:06:35,040
for this of getting it to work with leg chain. So I'll do this in another video straight after

63
00:06:35,040 --> 00:06:41,680
this, but just showing you here what is actually going on here. So remember that we're talking about

64
00:06:42,320 --> 00:06:50,720
GBT 3.5 model and the full model. So they use the sort of chat format of where you've got a system

65
00:06:50,720 --> 00:06:58,800
token, you've got your human message, even AI message. So that's what's going on here. If we look

66
00:06:58,800 --> 00:07:04,560
at like we're calling the function with a user input, and you'll see that, okay, we're basically

67
00:07:04,560 --> 00:07:09,120
passing in the model, we've got our messages, and then we've got the role. So the first one

68
00:07:09,120 --> 00:07:15,520
role is user, which is a language. This is called human. And then basically the content. Now the

69
00:07:15,520 --> 00:07:22,480
other thing that we pass in is we need to pass in a function definition or technically we pass in

70
00:07:22,480 --> 00:07:29,280
a list of function definitions. So here you can say this is the list that we've got here. And what

71
00:07:29,280 --> 00:07:34,880
we're actually got here is basically only one function being passed in. The name of that is

72
00:07:34,960 --> 00:07:39,280
getCurrentWither, which makes sense because the person who's asking what is the weather in Boston,

73
00:07:40,000 --> 00:07:46,320
we then basically, you can see that the parameters of the function, and the main ones here are the

74
00:07:46,320 --> 00:07:53,440
name of it, the description of it, the description of actually what the function does, and then the

75
00:07:53,440 --> 00:07:58,880
actual properties that it needs to return, get a description as well. So you can see here we've

76
00:07:58,880 --> 00:08:05,520
basically got location, going to be a string, and it's going to be a city and state,

77
00:08:06,080 --> 00:08:11,280
G San Francisco, California. And then the second thing is going to be a unit. So this is going to

78
00:08:11,280 --> 00:08:18,320
be either Celsius or Fahrenheit. So you can see this is an enum, so it's restricting just to these

79
00:08:18,320 --> 00:08:26,400
two things. So we can't say, oh, tell me what the, or rather the model can't just say, make up some

80
00:08:26,400 --> 00:08:32,720
other unit. It needs to be one of these two that gets returned in this. Now we can see that neither

81
00:08:32,720 --> 00:08:39,200
of these, this particular unit is not required, but the location is definitely required. So we

82
00:08:39,200 --> 00:08:45,280
can't just say, tell me what the weather is, because the GPT, unless it's got it already in the

83
00:08:45,280 --> 00:08:50,800
history of it, of your conversation, it's not going to know that. But if it is in the history,

84
00:08:50,800 --> 00:08:54,560
and we've been talking about a location, and I can say, tell me what the weather is like there now,

85
00:08:55,040 --> 00:09:00,320
it will be able to do this. Now you can see here, this is basically just being quite simple. It's

86
00:09:00,320 --> 00:09:06,880
just taking a location. You could imagine that you've got time, a time framing there, e.g. whether

87
00:09:06,880 --> 00:09:11,920
it's now, whether it's tomorrow, whether it's next week, that kind of thing as well. So this is what's

88
00:09:11,920 --> 00:09:18,800
getting passed into the model. We see that then we've got the response back from the model. So

89
00:09:19,520 --> 00:09:24,640
it's basically just going to return back. And it's going to give you the details to basically call

90
00:09:25,360 --> 00:09:31,200
this particular function based on this. So if you're just passed in, hello, how are you today,

91
00:09:31,200 --> 00:09:36,640
it wouldn't use any of the function, it would just give an AI message back here. But you'll see when

92
00:09:36,640 --> 00:09:43,520
I go through the notebook later on, that actually what's happening is it's returning back kind of

93
00:09:43,600 --> 00:09:49,280
like a blank message, but with a particular function called variable that's in there,

94
00:09:49,280 --> 00:09:54,800
that's going on in there. You can see that once it basically calls this API, it gets this stuff back.

95
00:09:54,800 --> 00:10:00,240
So it's getting back the temperature, the unit Celsius to description sunny. And then finally,

96
00:10:00,240 --> 00:10:08,160
it passes that back to open AI. So it passes that back to the GPT model. You can see here,

97
00:10:08,160 --> 00:10:15,920
we're basically passing back the list of messages. And we've gone user assistant function. So this is

98
00:10:15,920 --> 00:10:21,200
a very special type of message. It's not something that we've had before is a new kind of role that

99
00:10:21,200 --> 00:10:29,200
we've got there. And this is basically passing back the response from that function. So you can see

100
00:10:29,200 --> 00:10:35,200
the assistant passed out that it should use this function. So the assistant passed out that, okay,

101
00:10:35,200 --> 00:10:42,640
you should use the name function, get current weather, and you should be using Boston messages.

102
00:10:42,640 --> 00:10:48,080
So it's worked out the state here itself, right? So it's not like your user is going to have to

103
00:10:48,080 --> 00:10:54,160
tell everything if it can work out certain things, you can work them out quite nicely in that way.

104
00:10:54,160 --> 00:11:00,960
It gets the return back here. And then it's going to generate a response out. And the response that

105
00:11:01,040 --> 00:11:06,480
it generates out is the weather in Boston is currently sunny with a temperature of 22 degrees

106
00:11:06,480 --> 00:11:14,000
Celsius. And that's basically taken in these messages to then finally generate out this message.

107
00:11:14,000 --> 00:11:19,440
So you can see we are using probably a decent amount more tokens for the actual functions

108
00:11:19,440 --> 00:11:25,440
definitions that we're passing in there. But we're not needing to do any in context learning,

109
00:11:25,440 --> 00:11:31,280
we're not needing to do any sort of examples or things like that. And it kind of manages the

110
00:11:31,280 --> 00:11:36,880
scratch pad itself internally. So unlike things we've done before with React, where we had to

111
00:11:36,880 --> 00:11:42,160
have a scratch pad, etc, opening eyes, taking care of that by putting it in these parts here.

112
00:11:42,160 --> 00:11:48,320
In fact, you could think of this function role as the scratch pad of what came back from the

113
00:11:48,320 --> 00:11:54,720
particular function call that we had there. So this gives you a rough idea of how these things

114
00:11:54,720 --> 00:11:59,600
work functions. In the next video, I will go through a whole bunch of code, we'll look at

115
00:11:59,600 --> 00:12:04,560
how to put it in a length chain, both manually and then turn it into an agent and use it as an

116
00:12:04,560 --> 00:12:09,360
agent as well. As always, if you like the video, please click like and subscribe. If you've got

117
00:12:09,360 --> 00:12:14,960
any questions, please put them in the comments below. I'll see you in the next video. Bye for now.

