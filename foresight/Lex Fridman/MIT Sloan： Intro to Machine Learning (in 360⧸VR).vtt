WEBVTT

00:00.000 --> 00:05.920
The video you're watching now is in 360. Resolution is not great but we wanted to

00:05.920 --> 00:10.280
try something different. So if you're on a desktop or laptop you can pan around

00:10.280 --> 00:13.800
with your mouse or if you're on a phone or tablet you should be able to just

00:13.800 --> 00:18.960
move your device to look around. Of course it's best viewed with a VR headset.

00:18.960 --> 00:23.320
The video that follows is a guest lecture on machine learning that I gave an

00:23.320 --> 00:28.560
MIT Sloan course on the business of artificial intelligence. The lecture is

00:28.560 --> 00:33.540
non-technical and intended to build intuition about these ideas amongst the

00:33.540 --> 00:38.400
business students in the audience. The room was a half circle so we thought why

00:38.400 --> 00:43.880
not film the lecture in 360. We recorded a screencast of the slides and pasted it

00:43.880 --> 00:48.480
into the video so that the slides are more crisp. Let me know what you think and

00:48.480 --> 00:56.120
remember it's an experiment. So this course is talking about the broad context

00:56.120 --> 00:59.880
the impact of artificial intelligence. The global. There's global which is the

00:59.880 --> 01:03.240
global impact of artificial intelligence. There's the business which is when you

01:03.240 --> 01:07.480
have to take these fun research ideas that I'll talk about today. A lot of them

01:07.480 --> 01:12.580
are cool on toy examples. When you bring them to reality you face real challenges

01:12.580 --> 01:18.040
which is what I would like to really highlight today. That's the business part.

01:18.040 --> 01:21.440
When you want to make real impact. When you're going to make these technologies

01:21.440 --> 01:26.920
a reality. So I'll talk about how amazing the technology is for a nerd like me but

01:26.920 --> 01:32.160
also talk about how when you take that into the real world what are the

01:32.160 --> 01:36.960
challenges you face. So machine learning which is the technology at the core of

01:36.960 --> 01:43.560
artificial intelligence. We'll talk about the promise. The excitement that I feel

01:43.560 --> 01:48.000
about it. The limitations. We'll bring it down a little bit. What are the real

01:48.040 --> 01:55.800
capabilities of technology. Where for the first time really as a civilization

01:55.800 --> 02:02.360
exploring the meaning of intelligence. It is if you pause for a second and just

02:02.360 --> 02:05.200
think you know maybe you have many of you want to make money out of this

02:05.200 --> 02:10.920
technology. Many of you want to save lives help people but also on the

02:10.920 --> 02:16.200
philosophical level we get to explore what makes us human. So while I'll talk

02:16.280 --> 02:21.320
about the low-level technologies also think about the incredible opportunity

02:21.320 --> 02:26.960
here. We get to almost psychoanalyze ourselves by trying to build versions of

02:26.960 --> 02:33.640
ourselves in the machine. All right so here's the open question. How powerful is

02:33.640 --> 02:38.480
artificial intelligence. How powerful is machine learning that lies at the core

02:38.480 --> 02:43.120
of artificial intelligence. Is it simply a helpful tool a special purpose tool to

02:43.120 --> 02:49.000
help you solve simple problems. Which is what it currently is. Currently

02:49.000 --> 02:54.760
machine learning artificial intelligence is a way if you can formally define the

02:54.760 --> 02:58.240
problem. You can formally define the tools you're working with. You can formally

02:58.240 --> 03:01.800
define the utility function where you want to achieve with those tools. As long

03:01.800 --> 03:05.720
as you can define those things we can come up with algorithms that can solve

03:05.720 --> 03:09.600
them. As long as you have the right kind of data which is what I'll talk about.

03:09.960 --> 03:20.200
Data is key. And the question is into the future can we break past this very

03:20.200 --> 03:22.920
narrow definition of what machine learning can give us which is solve

03:22.920 --> 03:29.440
specific problems. To something bigger. To where we approach the general

03:29.440 --> 03:33.000
intelligence that we exhibit as human beings. When we're born we know

03:33.080 --> 03:41.240
nothing and we learn quickly. From very little data. The right answer is we

03:41.240 --> 03:45.920
don't know. We don't know what are the limitations of the technology. What kind

03:45.920 --> 03:51.120
of machine learning are there. There are several flavors. The first two is what's

03:51.120 --> 03:55.720
really the first is what's achieved success today. Supervised learning. What

03:55.720 --> 04:01.760
I'm showing here on the left of the slide is the teachers. Is the data that is

04:01.840 --> 04:05.440
fed to the system. And on the right is the students which is the system itself

04:05.440 --> 04:09.280
from machine learning. So there's supervised learning. Whenever everybody

04:09.280 --> 04:13.960
talks about machine learning today. For the most part they're referring to

04:13.960 --> 04:17.600
supervised learning. Which means every single piece of data that is used to

04:17.600 --> 04:23.720
train the model is seen by human eyes. And those human eyes with an

04:23.720 --> 04:29.800
accompanying brain label that data in a way that makes it useful to the machine.

04:30.240 --> 04:35.080
This is critical. Because that's one. The blue box. The human. It's really

04:35.080 --> 04:40.760
costly. So whenever every single piece of data that's used to train the machine

04:40.760 --> 04:44.800
needs to be seen by a human. You need to pay for that human. And second you're

04:44.800 --> 04:51.120
limited to just the time. There's the amount of data necessary to label what

04:51.120 --> 04:57.760
it means to exist in this world is humongous. Augmented supervised learning

04:57.800 --> 05:01.240
is when you get machine to really to help you a little bit. There's a few

05:01.240 --> 05:04.520
tricks there. But still it's still only tricks. It's still the human is at the

05:04.520 --> 05:11.480
core of it. And the promise of future research that we're pursuing that I'm

05:11.480 --> 05:14.880
pursuing and perhaps in the applications if we get to discuss or some of the

05:14.880 --> 05:20.200
speakers here get to discuss. They're pursuing in semi-supervised and

05:20.200 --> 05:24.080
reinforcement learning. Where the human starts to play a smaller and smaller

05:24.080 --> 05:28.240
role in how much they get to annotate. They have to annotate the data. And the

05:28.240 --> 05:34.880
dream of the sort of wizards of the dark arts of deep learning are all excited

05:34.880 --> 05:40.640
about unsupervised learning. That has very few actual successes in application

05:40.640 --> 05:48.880
in the real world today. But it is the idea that you can build a machine that

05:48.880 --> 05:57.120
doesn't require a human teacher. A human being to teach you anything. It fills us

05:57.120 --> 06:05.320
artificial intelligence researchers with excitement. There's a theme here.

06:05.320 --> 06:12.600
Machine learning is really simple. The learning system in the middle. There's a

06:12.600 --> 06:17.920
training stage where you teach it something. All you need is some data.

06:17.960 --> 06:26.640
Input data. And you need to teach it the correct output for that input data. So

06:26.640 --> 06:32.080
you have to have a lot of pairs of input data and correct output. There'll be a

06:32.080 --> 06:35.280
theme of cats throughout this presentation. So if you want to teach a

06:35.280 --> 06:41.400
system difference between a cat and a dog, you need a lot of images of cats. And

06:41.400 --> 06:45.120
you need to tell it that this is a cat. This bounding box here in the image is a

06:45.160 --> 06:50.800
cat. You have to give it a lot of images of dogs and tell it, okay, in these

06:50.800 --> 06:56.520
pictures there are dogs. And then there's a spelling mistake on the second

06:56.520 --> 07:00.800
stage is the testing stage. When you actually give it new input data, it's

07:00.800 --> 07:06.160
never seen before. And you hope that it has given for cat versus dog enough data

07:06.160 --> 07:14.920
to guess is this new image that I've never seen before, a cat or a dog. Now

07:15.680 --> 07:24.160
one of the open questions you want to keep in mind is what in this world can we

07:24.160 --> 07:35.680
not model in this way? What activity? What task? What goal? I offer to you that

07:35.680 --> 07:48.360
there's nothing you can't model in this way. So let's think about what in terms

07:48.360 --> 07:54.360
of machine learning can be, so let's start small. What can be modeled in this

07:54.360 --> 07:59.280
way? First on the bottom of the slide left is one to one mapping where the

07:59.280 --> 08:04.320
input is an image of a cat and the output is a label that says cat or dog.

08:05.080 --> 08:10.760
You can also do one to many where the image, the input is an image of a cat and

08:10.760 --> 08:18.160
the output is a story about that cat, a captioning of the image. You can first of

08:18.160 --> 08:23.760
all, you can do the other way, many to one mapping where you give it a story

08:23.760 --> 08:28.320
about a cat and it generates an image. There's many to many. This is Google

08:28.320 --> 08:33.400
translate. We translate a sentence from one language to another and there's

08:33.640 --> 08:38.880
various flavors of that. Again, same theme here. Input data provided with

08:38.880 --> 08:47.440
correct output and then let it go into the wild where it runs on input data

08:47.440 --> 08:53.320
that hasn't seen before to provide guesses. And it's as simple as this.

08:53.320 --> 08:58.960
Whatever you can convert into one of the following four things. Numbers, vector of

08:59.000 --> 09:03.480
numbers, so a bunch of numbers, a sequence of numbers where the temporal

09:03.480 --> 09:09.640
dynamics matters, so like audio, video, where the sequence, the ordering matters,

09:09.640 --> 09:13.480
or a sequence of vector numbers, just a bunch of numbers. If you can convert it

09:13.480 --> 09:17.440
to numbers and I propose to you that there's nothing you can't convert it

09:17.440 --> 09:24.600
to numbers. If you can convert it to numbers, you can have a system learn to

09:24.600 --> 09:28.560
do it. And the same thing with the output. Generate numbers, vectors, and numbers

09:29.080 --> 09:31.840
sequence of numbers or sequence of vectors and numbers.

09:35.560 --> 09:37.400
First, is there any questions at this point?

09:39.960 --> 09:44.080
Well, we have a lot of fun slides to get through, but I'll pause every once in a

09:44.080 --> 09:48.520
while to make sure we're on the same page here. So what kind of input are we

09:48.520 --> 09:53.480
talking about? Just to fly through it. Images, so faces, or medical applications

09:53.480 --> 09:59.640
for looking at scans of different parts of the body to determine if, to

09:59.640 --> 10:05.200
diagnose any kind of medical conditions. Texts, so conversations, your texts,

10:05.200 --> 10:10.600
article, blog posts, for sentiment analysis, question and answering, so you

10:10.600 --> 10:14.760
ask it a question where the output you hope answers. Sounds of voice

10:14.760 --> 10:20.840
recognition, any kind of, anything you can tell from audio. Time series data, so

10:20.840 --> 10:26.240
financial data, stock market, you can use it to predict anything you want about

10:26.240 --> 10:30.040
the stock market, including whether to buy or sell. If you're curious, doesn't

10:30.040 --> 10:37.800
work quite well as a machine learning application. Physical world, so cars or

10:37.800 --> 10:42.800
any kind of object, any kind of robot that exists in this world. So location of

10:42.800 --> 10:47.520
where I am, location of where other things are, the actions of others, that

10:47.560 --> 10:51.600
could be all the input. All of it can be converted to numbers. And the correct

10:51.600 --> 10:56.120
output, same thing. Classification, a bunch of numbers. Classification is

10:56.120 --> 11:00.600
saying it's a cat or a dog. Regression is saying to what degree I turn the

11:00.600 --> 11:05.120
steering wheel. Sequence is generating audio, generating video, generating

11:05.120 --> 11:10.240
stories, captioning, text, images, generating anything you could think of as

11:10.240 --> 11:17.040
numbers. And at the core of it is a bunch of data agnostic machine learning

11:17.040 --> 11:20.840
algorithms. There's traditional ones, nearest neighbors, Naive Bay support

11:20.840 --> 11:30.480
machine, support vector machines. A lot of them are limited, and I'll describe

11:30.480 --> 11:36.200
how. And then there's neural networks. There's nothing special and new about

11:36.200 --> 11:43.200
neural networks. And I'll describe exactly the very subtle thing that is

11:43.880 --> 11:49.480
powerful. That's always been there all along. And certain things have now been

11:49.480 --> 11:53.640
able to unlock that power about neural networks. But it's still just the flavor

11:53.640 --> 11:57.960
of a machine learning algorithm. And the inspiration for neural networks, as

11:57.960 --> 12:03.600
Jonathan showed last time, is our human brain. It's perhaps why the media, perhaps

12:03.600 --> 12:09.560
why the hype is captivated by the idea of neural networks, is because you

12:09.600 --> 12:14.840
immediately jump to this feeling like, because there's this mysterious structure

12:14.840 --> 12:18.440
to them that scientists don't understand. Artificial neural networks, I'm

12:18.440 --> 12:23.200
referring to. And the biological ones. We don't understand them. And the

12:23.200 --> 12:29.640
similarity captivates our minds and we think, well, this approach is perhaps as

12:29.640 --> 12:36.640
limited, as limitless as our own human mind. But the comparison ends there. In

12:36.640 --> 12:43.400
fact, artificial neuron, their artificial neural networks are much simpler

12:43.400 --> 12:49.720
computational units. At the core of everything is this neuron. This is a

12:49.720 --> 12:58.760
computational unit that does two very simple operations. On the left side, it

12:58.760 --> 13:06.880
takes a set of numbers as inputs. It applies weights to those inputs, sums

13:06.880 --> 13:15.120
them together, applies a little bias, and provides an output somewhere between zero

13:15.120 --> 13:23.120
and one. So you can think of it as a computational entity that gets excited

13:23.280 --> 13:31.480
when it sees certain inputs and gets totally turned off when it gets other

13:31.480 --> 13:39.320
kinds of inputs. So maybe this neuron with a 0, with a 0.7, 0.6, 1.4 weights, it

13:39.320 --> 13:45.080
gets really excited when it sees pictures of cats and totally doesn't care about

13:45.080 --> 13:52.880
dogs. Some of us are like that. So that's the job of this neuron, is to detect

13:52.880 --> 14:01.000
cats. Now what, the way you build an artificial neural network, the way you

14:01.000 --> 14:06.040
release the power that I'll talk about in the following slides about the

14:06.040 --> 14:10.200
applications, what could be achieved, is just stacking a bunch of these together.

14:12.000 --> 14:18.160
Think about it. This is, this is a extremely simple computational unit.

14:19.120 --> 14:24.840
So you need to sort of pause whenever we talk about the following slides and

14:24.840 --> 14:31.920
think that there's a few slides that I'll show that say neural networks are

14:31.920 --> 14:37.920
amazing. I want you to think back to this slide that everything is built on top

14:37.920 --> 14:43.560
of these really simple addition operations with a simple nonlinear

14:43.560 --> 14:50.160
function applied at the end. Just a tiny math operation. We stack them together

14:50.160 --> 14:54.320
with it in a feedforward way so there's a bunch of layers and when people talk

14:54.320 --> 15:01.160
about deep neural networks it means there is a bunch of those layers and then

15:01.160 --> 15:05.320
there's recurrent neural networks that are also a special flavor that's able to

15:05.320 --> 15:12.000
have memory. So as opposed to just pushing input into output directly, it's

15:12.040 --> 15:16.280
also able to do stuff on the inside in a loop where it remembers things. This is

15:16.280 --> 15:19.680
useful for natural language processing, for audio processing, whenever the

15:19.680 --> 15:28.440
sequence is not, the length of the sequence is not defined. Okay, slide number

15:28.440 --> 15:35.200
one in terms of neural networks are amazing. This is, this is perhaps for the

15:35.240 --> 15:42.920
math nerds. But also I want you to use your imagination. There's a universality

15:42.920 --> 15:47.680
to neural networks. It means that the simple computational unit on the left is

15:47.680 --> 15:52.680
an input and the right is the output of this network with just a single hidden

15:52.680 --> 15:57.000
layer. It's called a hidden layer because it sits there in the middle of the

15:57.000 --> 16:03.640
input and the output layers. A single hidden layer with some number of nodes

16:04.560 --> 16:15.640
can represent any function. Any function. That means anything you want to build in

16:15.640 --> 16:22.880
this world. Everyone in this room can be represented with a neural network with a

16:22.880 --> 16:29.520
single hidden layer. So the power, and this is just one hidden layer, the power

16:29.560 --> 16:35.880
of these things is limitless. The problem, of course, is how do you find the

16:35.880 --> 16:43.560
network? So how do you build a network that is as clever as many of the people

16:43.560 --> 16:50.600
in this room? But the fact that you can build such a network is incredible. It's

16:50.600 --> 16:59.000
amazing. I want you to think about that. And the way you train a network, so it's

16:59.040 --> 17:05.160
born as a blank slate. Some random weights assigned to the edges. Again, a network

17:05.160 --> 17:08.080
is represented. The numbers at the core, the parameters at the core of this

17:08.080 --> 17:13.760
network are the numbers on each of those arrows, each of those edges. And you

17:13.760 --> 17:18.800
start knowing nothing. This is a baby network. And the way you teach it

17:18.800 --> 17:24.600
something, unfortunately, currently, as I said, in a supervised learning

17:24.960 --> 17:28.680
mechanism, you have to give it pairs of input and output. You have to give it

17:29.720 --> 17:37.560
pictures of cats and labels on those pictures saying that they're cats. And the

17:37.560 --> 17:47.760
basic fundamental operation of learning is when you compute the measure of an

17:47.760 --> 17:55.800
error and you back propagate it to the network. What I mean, everything's easier

17:55.800 --> 18:05.160
with cats. I apologize. I apologize. Too many cats. And so the input here is a cat.

18:05.160 --> 18:10.280
And the neural network we trained, it's just guessing. It doesn't know. So I don't

18:10.280 --> 18:16.920
know. It's guessing cat. Well, it happens to be right. So we have to, this is the

18:16.960 --> 18:21.520
measure of error. Yes. You got it right. And you have to back propagate that error.

18:22.640 --> 18:27.600
You have to reward the network for doing a good job. And all you do, what I mean

18:27.600 --> 18:33.400
by reward, there's weights on each of those edges. And so the node, the individual

18:33.400 --> 18:37.800
neurons that were responsible that back to that cat neuron, that cat neuron needs

18:37.800 --> 18:42.680
to be rewarded for seeing the cat. So you just increase the weights on the neurons

18:42.680 --> 18:46.800
that were associated with producing the correct answer. Now you give it a picture

18:46.840 --> 18:54.120
of a dog and the neural network says cat. Well, that's an incorrect answer. So no,

18:54.120 --> 18:57.840
there's a high error. It needs to be back propagated to the network. So the weights

18:57.840 --> 19:03.800
that were responsible with classifying this picture as a cat need to be punished.

19:03.800 --> 19:09.960
They need to be decreased. Simple. And you just repeat this process over and over.

19:09.960 --> 19:16.640
This is what we do as kids when we're first learning. For the most part,

19:16.680 --> 19:21.800
we're also supervised learning machines in the sense that we have our parents and

19:21.800 --> 19:28.440
we have the environment, the world, that teaches about what's correct and what's

19:28.440 --> 19:33.440
incorrect. And we back propagate this error and reward through our brain to learn.

19:34.440 --> 19:39.440
The problem is, as human beings, we don't need too many examples. And I'll talk

19:39.440 --> 19:43.200
about some of the drawbacks of these approaches. We don't need too many

19:43.240 --> 19:47.240
examples. You fall off your bike once or twice and you learn how to ride the bike.

19:47.240 --> 19:53.240
Unfortunately, neural networks need tens of thousands of times when they fall off

19:53.240 --> 19:58.240
the bike in order to learn how to not do it. That's one of the limitations.

19:58.240 --> 20:06.240
And one key thing I didn't mention here is when we refer to input data,

20:06.280 --> 20:14.280
we usually refer to sensory data, raw data. We have to represent that data in

20:14.280 --> 20:24.280
some clever way, in some deeply clever way, where we can reason about it,

20:24.280 --> 20:30.280
whether it's in our brains or in the neural network. And a very simple example

20:30.320 --> 20:37.320
here to illustrate why representation of data matters. So the way you represent

20:37.320 --> 20:43.320
the data can make the discrimination of one class from another, a cat versus dog,

20:43.320 --> 20:49.320
either incredibly difficult or incredibly simple. Here is a visualization of the

20:49.320 --> 20:54.320
same kind of data in Cartesian coordinates and polar coordinates. On the right,

20:54.360 --> 21:01.360
you can just draw a simple line to separate the two. What you want is a system that's

21:01.360 --> 21:07.360
able to learn the polar coordinate representation versus the Cartesian

21:07.360 --> 21:15.360
representation automatically. And this is where deep learning has stepped in and

21:15.360 --> 21:21.360
revealed the incredible power of this approach, which deep learning is the

21:21.400 --> 21:27.400
smallest circle there. It's a type of representational learning. Machine learning

21:27.400 --> 21:31.400
is the bigger second to the biggest. So this class is about the biggest circle,

21:31.400 --> 21:36.400
AI, includes robotics, includes all the fun things that are built on learning.

21:36.400 --> 21:40.400
And I'll discuss while machine learning I think will close this entire circle into

21:40.400 --> 21:46.400
one. But for now, AI is the biggest circle, then a subset of that is machine

21:46.400 --> 21:50.400
learning and a smaller subset of that is representation learning. So deep

21:50.440 --> 21:55.440
learning is not only able to say, given a few examples of cats and dogs that

21:55.440 --> 22:01.440
discriminate between a cat and a dog, it's able to represent what it means to be a

22:01.440 --> 22:09.440
cat. So it's able to automatically determine what are the fundamental

22:09.440 --> 22:15.440
units at the low level and the high level. We're talking about this very Plato.

22:15.480 --> 22:22.480
What it means to represent a cat from the whiskers to the high level shape of

22:22.480 --> 22:28.480
the head to the fuzziness and the deformable aspects of the cat. Not a cat

22:28.480 --> 22:32.480
expert, but I hear these are the features of a cat versus that are essential to

22:32.480 --> 22:37.480
discriminate between a cat and a dog. Learning those features as opposed to

22:37.480 --> 22:42.480
having to have experts, this is the drawback of systems that Jonathan talked

22:42.520 --> 22:46.520
about from the eighties and nineties where you have to bring in experts for any

22:46.520 --> 22:50.520
specific domain that you try to solve. You have to have them encode that

22:50.520 --> 22:58.520
information. Deep learning, this is simply the only big difference between deep

22:58.520 --> 23:02.520
learning and other methods is that it learns the representation for you. It

23:02.520 --> 23:06.520
learns what it means to be a cat. Nobody has to step in and help it figure out

23:06.560 --> 23:13.560
that cats have whiskers and dogs don't. What does this mean? The fact that it

23:13.560 --> 23:19.560
can learn these features, these whisker features, is as opposed to having five

23:19.560 --> 23:24.560
or ten or a hundred or five hundred features that are encoded by brilliant

23:24.560 --> 23:30.560
engineers with PhDs, it can find hundreds of thousands, millions of features

23:30.600 --> 23:38.600
automatically. Hundreds of millions of features. Stuff that can't be put into

23:38.600 --> 23:42.600
words or described, in fact it's one of the limitations in neural networks is they

23:42.600 --> 23:46.600
find so many fundamental things about what it means to be a cat that you can't

23:46.600 --> 23:52.600
visualize what it really knows. It just seems to know stuff. It finds that

23:52.600 --> 23:58.600
stuff automatically. What does this mean? The critical thing here is because it's

23:58.640 --> 24:03.640
able to automatically learn those hundreds of millions of features, it's able to

24:03.640 --> 24:12.640
utilize data. It doesn't start, the diminishing returns don't hit until well we

24:12.640 --> 24:15.640
don't know when they hit. The point is with the classical machine learning

24:15.640 --> 24:21.640
algorithms, you start hitting a wall when you have tens of thousands of images of

24:21.680 --> 24:30.680
cats. With deep learning, you get better and better with more data.

24:30.680 --> 24:36.680
Neural networks are amazing slide two. Here's a game, a simple arcade game where

24:36.680 --> 24:41.680
there's two paddles, they're bouncing a ball back and forth. Okay, great. You can

24:41.680 --> 24:45.680
figure out an artificial intelligence agent that can play this game. It can,

24:45.720 --> 24:51.720
not even that well, just kind of, it kind of learns to do alright and eventually

24:51.720 --> 25:01.720
win. Here's the fascinating thing. With deep learning as opposed to encoding the

25:01.720 --> 25:07.720
position of the paddles, the position of the ball, having an expert in this game,

25:07.720 --> 25:14.720
there's many, come in and encode the physics of this game. The input to the

25:14.760 --> 25:23.760
neural network is the raw pixels of the game. So, it's learning in the following

25:23.760 --> 25:29.760
way. You give it an evolution of the game. You give it a bunch of pixels.

25:29.760 --> 25:35.760
Pixels are, images are built up of pixels. They're just numbers from 0 to 256. So

25:35.760 --> 25:40.760
there's this array of numbers that represent each image and then you give it

25:40.800 --> 25:45.800
several tens of thousands of images that represent a game. So you have the stack

25:45.800 --> 25:51.800
of pixels and stack of images that represent a game. And the only thing you

25:51.800 --> 25:56.800
know, this giant stack of numbers, the only thing you know is at the end you

25:56.800 --> 26:04.800
won or lost. That's it. So based on that, you have to figure out how to play the

26:04.800 --> 26:09.760
game. You know nothing about games. You know nothing about colors or balls or

26:09.800 --> 26:17.800
paddles or winning or anything. That's it. So this is, why is this amazing? That

26:17.800 --> 26:22.800
it even works and it works, it wins. It's amazing because that's exactly what we do

26:22.800 --> 26:27.800
as human beings. This is general intelligence. So I need you to pause and

26:27.800 --> 26:31.800
think about this. We'll talk about special intelligence and the usefulness and

26:31.800 --> 26:37.800
okay, there's cool tricks here and there that we can do to get you an edge on your

26:37.840 --> 26:43.840
high frequency trading system. But this is general intelligence. General

26:43.840 --> 26:48.840
intelligence is the same intelligence we use as babies when we're born. What we

26:48.840 --> 26:53.840
get is an input, sensory input of image sensory input. Right now, all of us,

26:53.840 --> 27:00.840
most of us are seeing, hearing, feeling with touch and that's the only input we

27:00.880 --> 27:07.880
get. We know nothing and with that input, we have to learn something. Nobody is

27:07.880 --> 27:11.880
pre-teaching us stuff and this is an example of that. A trivial example, but

27:11.880 --> 27:16.880
one of the first examples where this is truly working. I'm sorry to linger on

27:16.880 --> 27:22.880
this, but it's a fundamental fact. The fact that we have systems that, and now

27:22.880 --> 27:28.880
outperform human beings in these simple arcade games is incredible. This is the

27:28.920 --> 27:36.920
research side of things. But let me step back, these again, the takeaways. That

27:36.920 --> 27:44.920
previous slide is why I think machine learning is limitless in the future.

27:44.920 --> 27:53.920
Currently, it's limited. Again, the representation of the data matters and if

27:53.960 --> 28:00.960
you want to have impact, we currently can only tackle the small problems. What are

28:00.960 --> 28:05.960
those problems? Image recognition. We can classify, given the entire image of a

28:05.960 --> 28:15.960
leopard, of a boat, of a mite, with pretty good accuracy of what's in that image.

28:15.960 --> 28:20.960
That's image classification. What else? We can find exactly where in that image

28:21.000 --> 28:27.000
each individual object is. That's called image segmentation. Again, the process

28:27.000 --> 28:36.000
is the same. The learning system in the middle, a neural network, as long as you

28:36.000 --> 28:42.000
give it a set of numbers as input and the correct set of labels as output, it

28:42.000 --> 28:47.000
learns to do that for data it hasn't seen in the past. Let me pause a second and

28:47.040 --> 28:52.040
maybe if you have any questions, does anyone have any questions about the

28:52.040 --> 28:55.040
techniques of neural networks? Yes.

28:55.080 --> 29:04.080
How do you represent data, as you mentioned, to us anyways, as well as

29:04.080 --> 29:12.080
coordinate the recent coordinates? How do you represent these two different

29:12.080 --> 29:15.080
kinds of data?

29:15.080 --> 29:23.080
That's a great question. In a couple of slides, I'll get to it exactly. The

29:23.120 --> 29:29.120
data representation, I'll elaborate in a little bit, but loosely, the data

29:29.120 --> 29:37.120
representation is for a neural network is in the weights of each of those arrows

29:37.120 --> 29:43.120
that connect the neurons. That's where the representation is. I'll show to

29:43.120 --> 29:51.120
really clarify that example of what that means. The Cartesian versus polar

29:51.160 --> 29:59.160
coordinates is just a very simple visualization of the concept. You want

29:59.160 --> 30:04.160
to be able to represent the data in an arbitrary way where there's no limits

30:04.160 --> 30:09.160
to the representation. It could be highly nonlinear, highly complex. Any other

30:09.160 --> 30:11.160
questions?

30:11.200 --> 30:16.200
We touched on it earlier. Generally speaking, in our current state, when we talk

30:16.200 --> 30:20.200
about machine learning or AI, it's simply statistical models that are able to

30:20.200 --> 30:25.200
recognize the trends or things of that nature where they're not necessarily

30:25.200 --> 30:32.200
thinking, but simply recognizing. I'm a little confused about how the current

30:32.200 --> 30:39.200
system differs from deep learning. Whether you think that there is a

30:39.240 --> 30:43.240
possibility of transition from recognizing to actually thinking.

30:43.240 --> 30:49.240
I have a couple of slides almost asking this question, because there's no

30:49.240 --> 30:54.240
good answers. One could argue, and I think somebody in the last class brought up

30:54.240 --> 31:03.240
that is machine learning just pattern recognition. It's possible that reasoning,

31:03.280 --> 31:14.280
thinking, is just pattern recognition. I'll describe sort of an intuition

31:14.280 --> 31:25.280
behind that. We tend to respect thinking a lot, because we've recently as human

31:25.280 --> 31:30.280
beings learned to do it. In our evolutionary time, we think that it's somehow

31:30.320 --> 31:35.320
special from, for example, perception. We've had visual perception for several

31:35.320 --> 31:41.320
orders of magnitude longer in our evolution as a living species. We've

31:41.320 --> 31:47.320
started to learn to reason, I think, about 100,000 years ago. We think it's

31:47.320 --> 31:52.320
somehow special from the same kind of mechanism we use for seeing things.

31:52.320 --> 31:57.320
Perhaps it's exactly the same thing. Perception is pattern recognition.

31:57.360 --> 32:03.360
Perhaps reasoning is just a few more layers of that. That's the hope.

32:03.360 --> 32:08.360
That's an open question.

32:08.400 --> 32:17.400
The concept of neural network itself is not very new. Is there any

32:17.400 --> 32:24.400
technical innovation breakthrough to expand the use of neural network?

32:24.400 --> 32:32.400
I think more it's just the increase of the result of computational

32:32.440 --> 32:38.440
innovation. Yes, that's a great question. There's been very few

32:38.440 --> 32:43.440
breakthroughs in neural networks, since through the AI winters that we've discussed,

32:43.440 --> 32:50.440
through a lot of excitement, in spurts, and even recently, there's been

32:50.440 --> 32:56.440
very few algorithmic innovations. The big gains came from compute,

32:56.480 --> 33:02.480
so improvements in GPU and better, faster computers. You can't underestimate

33:02.480 --> 33:08.480
the power of community, so the ability to share code and the internet,

33:08.480 --> 33:12.480
the ability to communicate together through the internet and work on code

33:12.480 --> 33:16.480
together, and then digitization of data, so the ability to have large data sets

33:16.480 --> 33:21.480
easily accessible and downloadable. All of those little things.

33:21.520 --> 33:25.520
I think in terms of the future of deep learning and machine learning,

33:25.520 --> 33:33.520
it all rides on compute, meaning continued, bigger, and faster computers.

33:33.520 --> 33:38.520
That doesn't necessarily mean Moore's law in making small and small chips.

33:38.520 --> 33:43.520
It means getting clever in different directions, massive parallelization,

33:43.520 --> 33:48.520
coming up with ways to do super-efficient, power-efficient

33:48.560 --> 33:53.560
implementations in neural networks, and so on.

33:53.560 --> 33:58.560
Let me just fly through a few examples of what we can do with machine learning

33:58.560 --> 34:02.560
just to give you a flavor, I think, in future lectures as possible.

34:02.560 --> 34:07.560
We'll discuss different speakers, different specific applications,

34:07.560 --> 34:12.560
really dig into those. As opposed to working with just images,

34:12.560 --> 34:17.560
you can work with videos and segment those, I mentioned, image segmentation.

34:17.600 --> 34:21.600
We can do video segmentation through video segment, the different parts

34:21.600 --> 34:25.600
of a scene that's useful to a particular application. Here in driving,

34:25.600 --> 34:29.600
you can segment the road from cars and vegetation

34:29.600 --> 34:33.600
and lane markings.

34:33.600 --> 34:37.600
You can also, this is a subtle but important point.

34:37.600 --> 34:42.600
Just go back to that one slide. How do they see the light?

34:42.640 --> 34:46.640
It's such a critical piece. The more I listen to you

34:46.640 --> 34:50.640
and read your stuff, it seems like this critical, these very small

34:50.640 --> 34:54.640
piece of information that we know are important, like there is a red light.

34:54.640 --> 34:58.640
I have to stop, I have to slow down. How does it filter that out

34:58.640 --> 35:02.640
and pick out that?

35:02.640 --> 35:06.640
It's got to be 100% reliable on that.

35:06.680 --> 35:10.680
Hello.

35:10.680 --> 35:14.680
Oof, hard question.

35:14.680 --> 35:18.680
The question was how do you detect the traffic light

35:18.680 --> 35:22.680
and lights.

35:22.680 --> 35:26.680
How do we do it as human beings? First of all, let's start there.

35:26.680 --> 35:30.680
The way we do it is by

35:30.680 --> 35:34.680
the knowledge we bring to the table. We know what it means

35:34.720 --> 35:38.720
to be on the road. There's a lot of the huge network of knowledge

35:38.720 --> 35:42.720
that you come with. And so that makes the perception problem much easier.

35:42.720 --> 35:46.720
This is pure perception. You take an image and you separate different parts

35:46.720 --> 35:50.720
based purely on tiny patterns of pixels.

35:50.720 --> 35:54.720
So first it finds all the edges and it learns that

35:54.720 --> 35:58.720
traffic lights have certain kinds of edges around them and then

35:58.720 --> 36:02.720
zoom out a little bit. They have

36:02.760 --> 36:06.760
certain collection of edges that make up this black

36:06.760 --> 36:10.760
rectangle type shape. So it's all about shapes. It kind of build up

36:10.760 --> 36:14.760
knowing this shape structure of things. But it's a purely

36:14.760 --> 36:18.760
perception problem and one of the things I argue is that if it's

36:18.760 --> 36:22.760
purely a perception approach and you bring no knowledge to the

36:22.760 --> 36:26.760
table about the physics of the world, the three-dimensional physics and the temporal dynamics,

36:26.760 --> 36:30.760
that you are not going to be able to successfully achieve

36:30.800 --> 36:34.800
near 100% accuracy on some of these systems.

36:34.800 --> 36:38.800
So that's exactly the right question is

36:38.800 --> 36:42.800
for all of these things, think about how you as a human being would solve

36:42.800 --> 36:46.800
these problems and what is lacking in the machine learning approach.

36:46.800 --> 36:50.800
What data is lacking in the machine learning approach in order to achieve

36:50.800 --> 36:54.800
the same kind of results, the same kind of reasoning required to

36:54.800 --> 36:58.800
that you would use as a human. So there is also image

36:58.840 --> 37:02.840
detection. Image detection, which means

37:02.840 --> 37:06.840
the subtle but important point. The stuff I mentioned before, image classification

37:06.840 --> 37:10.840
is given an image of a cat, you find the cat,

37:10.840 --> 37:14.840
sorry you don't find the cat, you say this image is of a cat or not, and then

37:14.840 --> 37:18.840
detection or localization is when you actually find where in the image that is.

37:18.840 --> 37:22.840
That problem is much harder but also doable

37:22.840 --> 37:26.840
with machine learning, with deep neural networks.

37:26.880 --> 37:30.880
Now as I said, inputs-outputs can be anything. The input can be

37:30.880 --> 37:34.880
video, the output can be video, and you can do anything you want with these videos,

37:34.880 --> 37:38.880
you can colorize the video. You can add, take an old black and white

37:38.880 --> 37:42.880
film and produce color images.

37:42.880 --> 37:46.880
Again, in terms of

37:46.880 --> 37:50.880
having an impact in the world using these applications,

37:50.880 --> 37:54.880
you have to think, this is a cool demonstration, but how well does it actually

37:54.920 --> 37:58.920
work in the real world? Translation,

37:58.920 --> 38:02.920
whether that's from text to text or image to image, you can translate

38:02.920 --> 38:06.920
here dark chocolate from one language to another.

38:06.920 --> 38:10.920
Class, global business of

38:10.920 --> 38:14.920
artificial intelligence, there's a reference below there, you can go and generate

38:14.920 --> 38:18.920
your own text. You can generate the writing of

38:18.920 --> 38:22.920
the act of generating handwriting. You can type in some text

38:22.960 --> 38:26.960
and given different styles that it learns from other handwriting

38:26.960 --> 38:30.960
samples, it can generate any kind of text using

38:30.960 --> 38:34.960
handwriting. Again, the input is language, the

38:34.960 --> 38:38.960
output is a sequence of writing

38:38.960 --> 38:42.960
of pen movements on the screen. You can complete sentences.

38:42.960 --> 38:46.960
This is kind of a fun one where if you start

38:46.960 --> 38:50.960
actually you can generate language, and you can generate

38:51.000 --> 38:55.000
language where you feed the system some input first. So in black

38:55.000 --> 38:59.000
there it says life is, and then have the neural network complete

38:59.000 --> 39:03.000
those sentences. Life is about kids.

39:03.000 --> 39:07.000
Life is about the weather, there's a lot of knowledge here

39:07.000 --> 39:11.000
I think being conveyed, and you can start the sentence with the meaning of life is.

39:11.000 --> 39:15.000
The meaning of life is literary recognition,

39:15.000 --> 39:19.000
true for us academics, or the meaning of life is the tradition of

39:19.040 --> 39:23.040
ancient human production. Also true.

39:23.040 --> 39:27.040
But these are all generated by a computer. You can also

39:27.040 --> 39:31.040
caption, this has become very popular recently, is

39:31.040 --> 39:35.040
caption generation. Given input as an image, the output is

39:35.040 --> 39:39.040
a set of text. The caption is the content of the image.

39:39.040 --> 39:43.040
You find the different objects in the image, that's a perception

39:43.040 --> 39:47.040
problem, and once you find the different objects, you stitch them together in a sentence

39:47.080 --> 39:51.080
that makes sense. You generate a bunch of sentences, and classify which sentences the most

39:51.080 --> 39:55.080
likely to fit this image.

39:55.080 --> 39:59.080
And you can, so certainly in the

39:59.080 --> 40:03.080
I try to avoid mentioning driving too much, because it is my field

40:03.080 --> 40:07.080
that is what I'm excited about, but then

40:07.080 --> 40:11.080
the moment I start talking about driving it'll all be about driving.

40:11.080 --> 40:15.080
But I should mention of course that deep learning is critical to driving

40:15.120 --> 40:19.120
applications, for both the perception and what is really exciting to us now is

40:19.120 --> 40:23.120
the end-to-end, the end-to-end approach.

40:23.120 --> 40:27.120
So whenever you say end-to-end in any application, what that means

40:27.120 --> 40:31.120
is you start from the very raw inputs that the

40:31.120 --> 40:35.120
system gets, and you produce the very final

40:35.120 --> 40:39.120
output that's expected of the system. So as opposed to in the cell driving car case

40:39.120 --> 40:43.120
as opposed to breaking a car down into each individual components of

40:43.160 --> 40:47.160
perception, localization, mapping, control, planning

40:47.160 --> 40:51.160
and just taking the whole stack and just ignoring

40:51.160 --> 40:55.160
all the super complex problems in the middle and just taking the external

40:55.160 --> 40:59.160
scene as input and as output produced steering and

40:59.160 --> 41:03.160
acceleration and braking commands. And so in this way, taking this input

41:03.160 --> 41:07.160
is the image of the external world, in this case in a Tesla

41:07.160 --> 41:11.160
we can generate steering commands for the car.

41:11.200 --> 41:15.200
And then input a bunch of numbers that's just images,

41:15.200 --> 41:19.200
output a single number that gives you the steering of

41:19.200 --> 41:23.200
the car.

41:23.200 --> 41:27.200
Okay, so let's step back

41:27.200 --> 41:31.200
for a second and think about what can't we do

41:31.200 --> 41:35.200
with machine learning. We talked about you can map numbers to numbers. Let's think about

41:35.200 --> 41:39.200
what we can't do. At the core of artificial intelligence in terms of

41:39.240 --> 41:43.240
how we can impact on this world is robotics. So what

41:43.240 --> 41:47.240
can't we solve in robotics and artificial intelligence with the machine learning approach?

41:47.240 --> 41:51.240
And let's break down what artificial intelligence means.

41:51.240 --> 41:55.240
Here's a stack starting at the very top is the environment, the world

41:55.240 --> 41:59.240
that you operate in. There's sensors that sense that world. There is feature

41:59.240 --> 42:03.240
extraction and learning from that data and there's some reasoning,

42:03.240 --> 42:07.240
planning and affectors are the ways you manipulate

42:07.280 --> 42:11.280
the world. What can't we learn in this way?

42:11.280 --> 42:15.280
So we've had a lot of success as Jonathan talked about

42:15.280 --> 42:19.280
in the history of AI with formal tasks, playing games,

42:19.280 --> 42:23.280
solving puzzles. Recently we're having a lot of breakthroughs with medical

42:23.280 --> 42:27.280
diagnosis. We're still

42:27.280 --> 42:31.280
struggling

42:31.280 --> 42:35.280
but are very excited about in the robotics space

42:35.320 --> 42:39.320
with more mundane tasks of walking, of

42:39.320 --> 42:43.320
basic perception, of natural language written and spoken.

42:43.320 --> 42:47.320
And then there is the human tasks which are

42:47.320 --> 42:51.320
perhaps completely out of reach of this pipeline

42:51.320 --> 42:55.320
at the moment is cognition,

42:55.320 --> 42:59.320
imagination, subjective

42:59.320 --> 43:03.320
experience. So high level reasoning, not just common sense,

43:03.360 --> 43:07.360
but high level human level reasoning.

43:07.360 --> 43:11.360
So let's fly through this pipeline. There's sensors,

43:11.360 --> 43:15.360
cameras, LiDAR, audio,

43:15.360 --> 43:19.360
there's communication that flies through the air

43:19.360 --> 43:23.360
or wired or wireless or wired. IMU measuring the movement

43:23.360 --> 43:27.360
of things. So that's the way you think about it.

43:27.360 --> 43:31.360
That's the way as human beings and as any kind of system that you design, you measure the world.

43:31.400 --> 43:35.400
You don't just get an API to the world.

43:35.400 --> 43:39.400
You need to somehow measure aspects of this world.

43:39.400 --> 43:43.400
So that's how you get the data. So that's how you convert the

43:43.400 --> 43:47.400
world into data you can play with. And once you have the data,

43:47.400 --> 43:51.400
this is the representation side. You have to convert that raw data, raw

43:51.400 --> 43:55.400
pixels, raw audio, raw LiDAR data. You have to convert that

43:55.400 --> 43:59.400
into data that's useful for the intelligence system,

43:59.440 --> 44:03.440
for the learning system to use to discriminate

44:03.440 --> 44:07.440
between one thing and another.

44:07.440 --> 44:11.440
For vision, that's finding edges, corners,

44:11.440 --> 44:15.440
object parts and entire objects. And there's the machine learning

44:15.440 --> 44:19.440
that I've talked about. There's different kinds of mapping

44:19.440 --> 44:23.440
of the representation that you've learned to an actual outputs.

44:23.440 --> 44:27.440
There is, once you have this, so you have this idea

44:27.480 --> 44:31.480
of, and this goes to maybe a little bit of Simon's question,

44:31.480 --> 44:35.480
is reasoning. This is something that's

44:35.480 --> 44:39.480
out of reach in machine learning at the moment. This is going to your question.

44:39.480 --> 44:43.480
Then we can build

44:43.480 --> 44:47.480
a world class machine learning system for taking an image

44:47.480 --> 44:51.480
and classifying that it's a duck. I wonder if this will work.

44:57.480 --> 45:01.480
Wake you up.

45:01.480 --> 45:05.480
So we could take, this is well studied,

45:05.480 --> 45:09.480
exceptionally well studied problem. We could take audio sample

45:09.480 --> 45:13.480
of a duck and tell that it's a duck. In fact, what species

45:13.480 --> 45:17.480
of bird? It's incredible how much research there is in bird species

45:17.480 --> 45:21.480
classification. And we can look at video and we can tell

45:21.480 --> 45:25.480
that we can do actual recognition that it's swimming. But we

45:25.520 --> 45:29.520
can't do with learning now, is reason. That if it

45:29.520 --> 45:33.520
looks like a duck, it swims like a duck and quacks like a duck,

45:33.520 --> 45:37.520
it's very likely to be a duck. This is the reasoning problem.

45:37.520 --> 45:41.520
This is the task that I personally am obsessed with

45:41.520 --> 45:45.520
and that I hope that machine learning can close.

45:45.520 --> 45:49.520
And then there is the planning action and the effectors.

45:55.520 --> 45:59.520
So this is another place

45:59.520 --> 46:03.520
where machine learning

46:03.520 --> 46:07.520
has not had many strides. There's mechanical issues here that are incredibly

46:07.520 --> 46:11.520
difficult. The degrees of freedom with all the actuators involved,

46:11.520 --> 46:15.520
with all the just the ability to

46:15.520 --> 46:19.520
localize every part of yourself in this dynamic

46:19.520 --> 46:23.520
space. Where things

46:23.560 --> 46:27.560
are constantly changing when there's degrees of uncertainty, when there's noise. Just that

46:27.560 --> 46:31.560
basic problem is exceptionally difficult.

46:35.560 --> 46:39.560
Let me just pose this question. We talked

46:39.560 --> 46:43.560
about what machine learning can do with the cats and the duck.

46:43.560 --> 46:47.560
We could do that. Given representation, it could predict what's in the image.

46:47.560 --> 46:51.560
But one of the open questions is, and deep

46:51.600 --> 46:55.600
learning has been able to do the feature extraction, the representation

46:55.600 --> 46:59.600
learning. This is the big breakthrough that everybody's excited about.

46:59.600 --> 47:03.600
But can it also reason? These are the open questions.

47:03.600 --> 47:07.600
Can it reason? Can it do the planning and action?

47:07.600 --> 47:11.600
And as human beings do, can it close the loop entirely

47:11.600 --> 47:15.600
from sensors to effectors? So learn not only

47:15.600 --> 47:19.600
the brain, but the way you sense the

47:19.640 --> 47:23.640
world, and the way you affect the world.

47:41.640 --> 47:45.640
So the question was about the pond game. Thank you.

47:45.680 --> 47:49.680
Let's get to talk to it for a little longer. It doesn't

47:49.680 --> 47:53.680
get punished when it doesn't detect the ball. This is the beautiful thing.

47:53.680 --> 47:57.680
It gets punished only at the very end of the game for losing the game, and gets

47:57.680 --> 48:01.680
rewarded for winning the game. So it knows nothing about that ball.

48:01.680 --> 48:05.680
And it learns about that ball. That's something you need to really sit and think

48:05.680 --> 48:09.680
about. Because like, as human

48:09.680 --> 48:13.680
beings, imagine if you're playing with a physical ball, how do you

48:13.720 --> 48:17.720
learn what a ball is? You get hurt by it,

48:17.720 --> 48:21.720
you like squeeze it, you throw it, you feel the dynamics of it,

48:21.720 --> 48:25.720
the physics of it, and

48:25.720 --> 48:29.720
nobody tells you about what a ball is. You're just using the raw

48:29.720 --> 48:33.720
sensory input. We take it for granted, and maybe this is what I can

48:33.720 --> 48:37.760
end on, is this is what something Jonathan

48:37.760 --> 48:41.760
brought up, is we take the simplicity of this task for granted.

48:41.840 --> 48:45.840
Because we've been, we've had eyes

48:45.840 --> 48:49.840
we broadly speaking as living

48:49.840 --> 48:53.840
species on planet Earth. These eyes have been evolved

48:53.840 --> 48:57.840
for 540 million years. So we have 540 million

48:57.840 --> 49:01.840
years of data. We've been walking for close to that

49:01.840 --> 49:05.840
by petal mammals. We have been

49:05.840 --> 49:09.840
thinking only very recently. So 100,000 years

49:09.840 --> 49:13.840
is 100 million years. And

49:13.840 --> 49:17.840
that's why we can't, some of these problems that we're trying to

49:17.840 --> 49:21.840
solve, you can't take for granted how actually difficult they are. So for

49:21.840 --> 49:25.840
example, this is the Marvex Paradox that Jonathan brought up,

49:25.840 --> 49:29.840
is that the easy problems are hard. The things we think are easy, actually really

49:29.840 --> 49:33.840
hard. This is the state-of-the-art robot on the right playing soccer,

49:33.840 --> 49:37.840
and that was the state-of-the-art human

49:37.840 --> 49:41.840
on the left playing soccer. And

49:41.840 --> 49:45.840
I'll give it a second.

49:53.840 --> 49:57.840
The question was, you know, there's a

49:57.840 --> 50:01.840
fundamental difference between the way we train neural networks and the way we've trained

50:01.840 --> 50:05.840
biological neural networks through evolution by discarding through natural selection a bunch of

50:05.840 --> 50:09.840
the neural networks that didn't work

50:09.840 --> 50:13.840
so well. So first of all, the process of

50:13.840 --> 50:17.840
evolution is, I think, not well understood.

50:17.840 --> 50:21.840
Meaning, sorry, the role,

50:21.840 --> 50:25.840
careful here. The role of

50:25.840 --> 50:29.840
evolution in the evolution of our cognition, of

50:29.840 --> 50:33.840
our intelligence. I don't know if that's, so this

50:33.840 --> 50:37.840
is an open question. So maybe clarify this point. Is neural networks

50:37.840 --> 50:41.840
artificial neural networks are fixed for the most part in size?

50:41.840 --> 50:45.840
This is exactly right. It's like a single human being that gets to

50:45.840 --> 50:49.840
learn. We don't have mechanisms of

50:49.840 --> 50:53.840
modifying or revolving those neural networks yet.

50:53.840 --> 50:57.840
Although you could think of researchers

50:57.840 --> 51:01.840
as doing exactly that. You have grad students working on different

51:01.840 --> 51:05.840
neural networks and the ones that don't do a good job don't get

51:05.840 --> 51:09.840
promoted and get a good job. There is a natural selection there, but other than that

51:09.840 --> 51:13.840
it's an open question. It's a fascinating one.

51:25.840 --> 51:29.840
Stay tuned and keep your head up because the future I believe

51:29.840 --> 51:33.840
is really promising. And the slides will be

51:33.840 --> 51:37.840
made available for sure.

51:37.840 --> 51:41.840
I think a lot of the explorations of

51:41.840 --> 51:45.840
what it means to build an intelligent machine has been in sci-fi movies. We're now beginning

51:45.840 --> 51:49.840
to actually make it a reality. This is Space Odyssey to keep with that theme

51:49.840 --> 51:53.840
in the previous lecture that we had. This is

51:53.840 --> 51:57.840
as opposed to the dream like monolith view

51:57.840 --> 52:01.840
when the astronaut is gazing out

52:01.840 --> 52:05.840
into the open sky at the stars. We're going to look at the practice of

52:05.840 --> 52:09.840
AI today and how we go, if you're familiar with the

52:09.840 --> 52:13.840
movie, when this new technology appeared before our eyes

52:13.840 --> 52:17.840
and we're full of excitement, how we transfer that into

52:17.840 --> 52:21.840
actual practical impact

52:21.840 --> 52:25.840
on our lives. To quickly review what we

52:25.840 --> 52:29.840
talked about last time, I presented the technology

52:29.840 --> 52:33.840
and asked the question of whether this technology merely serves a special purpose

52:33.840 --> 52:37.840
to answer specific tasks that can be formalized or whether it can

52:37.840 --> 52:41.840
be through the process of transferring

52:41.840 --> 52:45.840
the knowledge learned on one domain be generalizable

52:45.840 --> 52:49.840
to where an intelligent system that's trained in a small domain can be

52:49.840 --> 52:53.840
used to achieve general intelligent tasks like we do

52:53.840 --> 52:57.840
as human beings. This is kind of the stack of artificial intelligence

52:57.840 --> 53:01.840
going from all the way up to the top, the environment, the world

53:01.840 --> 53:05.840
the sensors, the sensor data, the intelligence system, the way

53:05.840 --> 53:09.840
it perceives this world. Then once you have this, you convert

53:09.840 --> 53:13.840
the world into some numbers, you're able to extract some representation of that

53:13.840 --> 53:17.840
world and this is where machine learning starts to come into play. And then there's

53:17.840 --> 53:21.840
the part where I will raise it again today is can machine learning

53:21.840 --> 53:25.840
be doing the following steps too that we can do very well as human beings is the

53:25.840 --> 53:29.840
reasoning step. You can tell the difference in a cat and a dog, but can you

53:29.840 --> 53:33.840
now start to reason about what it means to be alive, what it

53:33.840 --> 53:37.840
means to be a cat, a living creature, what it means to be this kind of physical object

53:37.840 --> 53:41.840
or this kind of physical object and take what's called common sense, things we take

53:41.840 --> 53:45.840
for granted, start to construct models of the world through

53:45.840 --> 53:49.840
reasoning. Descartes, I think therefore I am.

53:49.840 --> 53:53.840
We want our neural networks to come up with that on their own.

53:53.840 --> 53:57.840
And once you do that, action.

53:57.840 --> 54:01.840
You'll go right back into the world and you start acting in that world. So the question is

54:01.840 --> 54:05.840
can machine learning, can this be learned from data or do experts need to encode

54:05.840 --> 54:09.840
the knowledge of reasoning, the knowledge of actions, the set of actions.

54:09.840 --> 54:13.840
That's kind of the question, the open questions I raise. It continues throughout the talk

54:13.840 --> 54:17.840
today. And so as we start to think

54:17.840 --> 54:21.840
about how artificial intelligence, especially machine learning

54:21.840 --> 54:25.840
as it relies itself through robotics, gets to impact the world, we start thinking

54:25.840 --> 54:29.840
about what are the easy problems and what are the hard problems. And it seems

54:29.840 --> 54:33.840
to us that vision

54:33.840 --> 54:37.840
and movement, walking is easy because we've been doing it for millions

54:37.840 --> 54:41.840
of years, hundreds of millions of years, and thinking is hard.

54:41.840 --> 54:45.840
Reasoning is hard. I propose to you that it's perhaps because we've only

54:45.840 --> 54:49.840
been doing it for a short time and so think we're quite special

54:49.840 --> 54:53.840
because we're able to think. So we have to kind of question of what is easy

54:53.840 --> 54:57.840
and what is hard. Because when we start to develop some of these systems

54:57.840 --> 55:01.840
and you start to realize that all of these

55:01.840 --> 55:05.840
problems are equally hard. So the problem of walking that we take for granted,

55:05.840 --> 55:09.840
the actuation and the physical, the ability to recognize

55:09.840 --> 55:13.840
where you are in the physical space to sense the world

55:13.840 --> 55:17.840
around you, to deal with the

55:17.840 --> 55:21.840
uncertainty of the perception problem. And then

55:21.840 --> 55:25.840
so all of these robots, by the way, this is for the most recent DARPA Challenge

55:25.840 --> 55:29.840
which MIT was also part of.

55:29.840 --> 55:33.840
And so what are these robots doing? They

55:33.840 --> 55:37.840
don't have any, they only have sparse communication with human beings

55:37.840 --> 55:41.840
on the periphery. So most of the stuff they have to do autonomously

55:41.840 --> 55:45.840
like get inside a car, this is an MIT robot unfortunately,

55:45.840 --> 55:49.840
that they have to get in the car in the hardest tasks, they have to get out of the

55:49.840 --> 55:53.840
car. That's walking. So this kind of

55:53.840 --> 55:57.840
raises to you a very real

55:57.840 --> 56:01.840
aspect here. You want to build applications that actually work in the real world.

56:01.840 --> 56:05.840
And that's the first challenge and opportunity here.

56:05.840 --> 56:09.840
The many of the technologies we talked about currently crumble

56:09.840 --> 56:13.840
under the reality of

56:13.840 --> 56:17.840
our world. When we transfer them from a small

56:17.840 --> 56:21.840
data set in the lab to the real world. For the computer vision

56:21.840 --> 56:25.840
it's perhaps one of the best illustrations of this. Computer vision is the task

56:25.840 --> 56:29.840
as we talked about of interpreting images. And so when

56:29.840 --> 56:33.840
you, there's been a lot of great accomplishments on interpreting images.

56:33.840 --> 56:37.840
Cats versus dogs. Now when you try to create

56:37.840 --> 56:41.840
a system like the Tesla vehicle

56:41.840 --> 56:45.840
that I've often, that we work with and

56:45.840 --> 56:49.840
I always talk about is, it's a vision

56:49.840 --> 56:53.840
based robot, right? It has radar for basic obstacle avoidance

56:53.840 --> 56:57.840
but most of the understanding of the world comes from a single monocular camera.

56:57.840 --> 57:01.840
Now they've expanded the number of cameras but for the most time there's been 100,000

57:01.840 --> 57:05.840
vehicles driving on the roads today with a single, essentially

57:05.840 --> 57:09.840
a single webcam. So when you start to do

57:09.840 --> 57:13.840
that you have to perform all of these extraction of texture, color,

57:13.840 --> 57:17.840
optical flow. So the movement through time, temporal dynamics of the images

57:17.840 --> 57:21.840
you have to construct these patterns, construct the understanding of

57:21.840 --> 57:25.840
objects and entities and how they interact. And from that you have to act

57:25.840 --> 57:29.840
in this world. And that's all based on this computer vision system. So it's no longer

57:29.840 --> 57:33.840
cats versus dogs, it's

57:33.840 --> 57:37.840
detection of pedestrians. Where the wrong

57:37.840 --> 57:41.840
classification, the wrong detection

57:41.840 --> 57:45.840
is the difference between life and death. So let's look at

57:45.840 --> 57:49.840
cats, where things are a little more comfortable. So computer vision

57:49.840 --> 57:53.840
and I would like to illustrate to you why this is such a hard

57:53.840 --> 57:57.840
task. We talked about, we've been doing it for 500 million years

57:57.840 --> 58:01.840
so we think it's easy. Computer vision is actually incredible

58:01.840 --> 58:05.840
so all you're getting with your human eyes is you're getting essentially pixels

58:05.840 --> 58:09.840
in. There's light coming into your eyes and all you're getting is the reflection

58:09.840 --> 58:13.840
from the different surfaces in here of light and there's

58:13.840 --> 58:17.840
perception, there's sensors inside your eyes

58:17.840 --> 58:21.840
converting that into numbers. It's really very similar to this.

58:21.840 --> 58:25.840
Numbers in the case of what we use with computers, RGB images

58:25.840 --> 58:29.840
or the individual pixels that are numbers from

58:29.840 --> 58:33.840
255 to 256 possible numbers and there's just a bunch of them.

58:33.840 --> 58:37.840
And that's all we get. We get a collection of numbers where they're spatially

58:37.840 --> 58:41.840
connected. The ones that are close together are part of the same object so

58:41.840 --> 58:45.840
cat pixels are all connected together. That's the only thing we have to help

58:45.840 --> 58:49.840
us but the rest of it is just numbers, intensity numbers. And we have to use those numbers

58:49.840 --> 58:53.840
to classify what's in the image.

58:53.840 --> 58:57.840
And if you really think about it, this is a really difficult task. All you get

58:57.840 --> 59:01.840
is these numbers. How the heck are you supposed to form a model

59:01.840 --> 59:05.840
of the world with which you can detect

59:05.840 --> 59:09.840
pedestrians with really 99.99999% accuracy.

59:09.840 --> 59:13.840
Because these pedestrians or these cars

59:13.840 --> 59:17.840
are cyclists in the car context or any kind of applications that you're looking at.

59:17.840 --> 59:21.840
Even if your job is on the factory floor to detect

59:21.840 --> 59:25.840
the defective gummy bears that are flying past like 100 miles an hour

59:25.840 --> 59:29.840
your task is you don't want that bad gummy bear to get by

59:29.840 --> 59:33.840
that your product and the brand will be damaged. However

59:33.840 --> 59:37.840
serious or not serious your application is, what you have to be

59:37.840 --> 59:41.840
you have to have a computer vision system that deals with

59:41.840 --> 59:45.840
all of these aspects. Viewpoint variation, scale

59:45.840 --> 59:49.840
variation, no matter the size of the object is still the same object.

59:49.840 --> 59:53.840
No matter the viewpoint from which area you

59:53.840 --> 59:57.840
look at that object is still the same object. The lighting that moves

59:57.840 --> 01:00:01.840
we have lighting consistently here because we're indoors. But when you're outdoors

01:00:01.840 --> 01:00:05.840
or you're moving, the scene is moving, the lighting, the complexity

01:00:05.840 --> 01:00:09.840
of the lighting variations is incredible. From the illumination to just

01:00:09.840 --> 01:00:13.840
the movement of the different objects in the scene.

01:00:13.840 --> 01:00:17.840
Now that we've had these conversations, I think about this every time

01:00:17.840 --> 01:00:21.840
I drive. I think about you at this point and how hard it is to see these things.

01:00:21.840 --> 01:00:25.840
Particularly when I'm driving at night and particularly when it's twilight and the light is changing

01:00:25.840 --> 01:00:29.840
I think almost every time I drive

01:00:29.840 --> 01:00:33.840
there's one or two things that I see that I'm drawing

01:00:33.840 --> 01:00:37.840
like 200 million years in order to be able to figure out. It's a guy who's

01:00:37.840 --> 01:00:41.840
open his car door and I can't see him but I can just see the light doesn't look quite

01:00:41.840 --> 01:00:45.840
right on that side of the road and somehow I know in my

01:00:45.840 --> 01:00:49.840
mind it's a person. But it seems like an almost

01:00:49.840 --> 01:00:53.840
impossible problem for the machines to get right with sufficient accuracy.

01:00:53.840 --> 01:00:57.840
I will argue that the pure perception task is too hard.

01:00:57.840 --> 01:01:01.840
That you come to the table as human beings with all this

01:01:01.840 --> 01:01:05.840
huge amount of knowledge. That you're not actually

01:01:05.840 --> 01:01:09.840
interpreting all the complex lighting variations that you're seeing.

01:01:09.840 --> 01:01:13.840
You actually know enough about the world, enough about your commute home,

01:01:13.840 --> 01:01:17.840
enough about the way, the kinds of things you would see in this

01:01:17.840 --> 01:01:21.840
world, about Boston, about the way pedestrians move, the

01:01:21.840 --> 01:01:25.840
certain light of day. You bring all that to the table that makes the perception task

01:01:25.840 --> 01:01:29.840
doable. And that's one of the big missing pieces in the technology.

01:01:29.840 --> 01:01:33.840
As I'll talk about, that's the open problem of machine learning.

01:01:33.840 --> 01:01:37.840
It's how to bring all that knowledge, first of all build that knowledge and then bring that

01:01:37.840 --> 01:01:41.840
knowledge to the table as opposed to starting from scratch every time.

01:01:41.840 --> 01:01:45.840
And so, cats, I promise cats.

01:01:45.840 --> 01:01:49.840
Okay, so to me, occlusion, for most of the

01:01:49.840 --> 01:01:53.840
computer vision community, this is one of the biggest challenges. And it really highlights

01:01:53.840 --> 01:01:57.840
how far we are from

01:01:57.840 --> 01:02:01.840
being able to reason about this world. Occlusions are

01:02:01.840 --> 01:02:05.840
what an occlusion is, is when the objects you're trying to

01:02:05.840 --> 01:02:09.840
detect, something about, classify the object, detect the object,

01:02:09.840 --> 01:02:13.840
the object is blocked

01:02:13.840 --> 01:02:17.840
by another object in front of them. This is something

01:02:17.840 --> 01:02:21.840
that you think is trivial, perhaps. You don't even really think about it, because

01:02:21.840 --> 01:02:25.840
we reason in a three-dimensional way. But the occlusion

01:02:25.840 --> 01:02:29.840
aspect is, makes, makes perception

01:02:29.840 --> 01:02:33.840
incredibly difficult. So we have to design, think about this.

01:02:33.840 --> 01:02:37.840
So this image is converted into numbers. And we, for the task of

01:02:37.840 --> 01:02:41.840
detecting, is there a cat in this image? Yes or no? You have to be able to

01:02:41.840 --> 01:02:45.840
reason about this image with that object in the scene. Most

01:02:45.840 --> 01:02:49.840
of us are able to very easily detect that there's a cat in this image.

01:02:49.840 --> 01:02:53.840
We're able to detect that there's a cat in this image. Now think about this.

01:02:53.840 --> 01:02:57.840
There's a single eye and there's an ear.

01:02:57.840 --> 01:03:01.840
So you have to think about what is it part of our brain that allows

01:03:01.840 --> 01:03:05.840
us to understand, to, to suppose that with some

01:03:05.840 --> 01:03:09.840
high degree of accuracy that there's a cat here in this picture.

01:03:09.840 --> 01:03:13.840
I mean the degree of occlusion here is immense.

01:03:13.840 --> 01:03:17.840
So this is for most of you.

01:03:17.840 --> 01:03:21.840
Some of you will think this is in fact a monk

01:03:21.840 --> 01:03:25.840
eating a banana. But I would venture to say that most of us are

01:03:25.840 --> 01:03:29.840
able to tell it's never the last of cat.

01:03:29.840 --> 01:03:33.840
You'll watch this for hours.

01:03:33.840 --> 01:03:37.840
And so let me give you another, this is kind of a paper that's often cited

01:03:37.840 --> 01:03:41.840
or a set of papers

01:03:41.840 --> 01:03:45.840
to illustrate how difficult computer vision is.

01:03:45.840 --> 01:03:49.840
How thin the line that we're walking with

01:03:49.840 --> 01:03:53.840
all of these impressive results that we've been able to show recently in the machine

01:03:53.840 --> 01:03:57.840
learning community. In this case

01:03:57.840 --> 01:04:01.840
for deep neural networks are easily fooled paper.

01:04:01.840 --> 01:04:05.840
The seminal paper at this point shows that when you apply

01:04:05.840 --> 01:04:09.840
a network trained on ImageNet, so

01:04:09.840 --> 01:04:13.840
basically on detecting cats versus dogs or different categories inside images

01:04:13.840 --> 01:04:17.840
if you're, you can find

01:04:17.840 --> 01:04:21.840
an arbitrary number of images that look like noise up in the top row.

01:04:21.840 --> 01:04:25.840
Where the algorithm used

01:04:25.840 --> 01:04:29.840
to classify those images in ImageNet of cat versus dog

01:04:29.840 --> 01:04:33.840
is able to confidently say with 99.6% accuracy

01:04:33.840 --> 01:04:37.840
above that it's seeing a robin or a cheetah

01:04:37.840 --> 01:04:41.840
or an armadillo or a panda in that noise.

01:04:41.840 --> 01:04:45.840
So it's confidently saying given this noise that that's obviously a robin.

01:04:45.840 --> 01:04:49.840
So you have to realize that the kind

01:04:49.840 --> 01:04:53.840
of, this is patterns, the kind

01:04:53.840 --> 01:04:57.840
of processes it's using to understand what's contained in the image

01:04:57.840 --> 01:05:01.840
is purely a collection of patterns that it has been able to

01:05:01.840 --> 01:05:05.840
track from other images that has been human, annotated by humans.

01:05:05.840 --> 01:05:09.840
And that perhaps is very limiting

01:05:09.840 --> 01:05:13.840
to trying to create a system that's able to operate in the real world.

01:05:13.840 --> 01:05:17.840
This is a very, so this is a very clean illustration

01:05:17.840 --> 01:05:21.840
of that concept. In the same you can confidently predict

01:05:21.840 --> 01:05:25.840
in those images below where there's strong patterns, it's not even noise.

01:05:25.840 --> 01:05:29.840
Strong patterns that have nothing to do with the entities being detected.

01:05:29.840 --> 01:05:33.840
Again, confidently that same algorithm is able to see a penguin,

01:05:33.840 --> 01:05:37.840
a starfish, a baseball, and a guitar in that noise.

01:05:37.840 --> 01:05:41.840
And more serious for people

01:05:41.840 --> 01:05:45.840
designing robots like myself on the

01:05:45.840 --> 01:05:49.840
sensor side, you can flip that and say

01:05:49.840 --> 01:05:53.840
I can take a image

01:05:53.840 --> 01:05:57.840
and I can distort it with some very little amount of noise.

01:05:57.840 --> 01:06:01.840
And if that noise is applied to the image

01:06:01.840 --> 01:06:05.840
I can completely change the confident prediction about what's in that image.

01:06:05.840 --> 01:06:09.840
So to explain what's being shown. So on the left, the column in the left,

01:06:09.840 --> 01:06:13.840
and again here, what's

01:06:13.840 --> 01:06:17.840
the same kind of neural network is able to predict

01:06:17.840 --> 01:06:21.840
accurately, confidently, that there is a dog in that image.

01:06:21.840 --> 01:06:25.840
But if we apply just a little bit of noise to that image, to produce

01:06:25.840 --> 01:06:29.840
that image, imperceptible to our human eyes, the difference between those two,

01:06:29.840 --> 01:06:33.840
the same algorithm is saying that there's confidently an ostrich

01:06:33.840 --> 01:06:37.840
in that image. So another thing to really think about

01:06:37.840 --> 01:06:41.840
that noise can have such a significant impact on the

01:06:41.840 --> 01:06:45.840
prediction of these algorithms. This is really, really

01:06:45.840 --> 01:06:49.840
quite honestly out of all the things I'll say today and I'm aware of

01:06:49.840 --> 01:06:53.840
one of the biggest challenges of

01:06:53.840 --> 01:06:57.840
machine learning being applied in the real world is robustness.

01:06:57.840 --> 01:07:01.840
How much noise can you add into the system before

01:07:01.840 --> 01:07:05.840
everything falls apart? So how do you validate

01:07:05.840 --> 01:07:09.840
sensors? So say a car company has to produce a vehicle

01:07:09.840 --> 01:07:13.840
and it has sensors in that vehicle. How do you know that those sensors

01:07:13.840 --> 01:07:17.840
will not start generating slight noise due to interference of various

01:07:17.840 --> 01:07:21.840
kinds? And because of that noise, instead of seeing a pedestrian

01:07:21.840 --> 01:07:25.840
it will see nothing. Or the opposite, it will see pedestrians everywhere.

01:07:25.840 --> 01:07:29.840
So of course the most dangerous is when it will not see an object

01:07:29.840 --> 01:07:33.840
and collide with it, in the case of cars. There's also spoofing, which a lot of

01:07:33.840 --> 01:07:37.840
people as always with security, people are really concerned about.

01:07:37.840 --> 01:07:41.840
And perhaps people here are really concerned about this issue. I think this

01:07:41.840 --> 01:07:45.840
is a really important issue, but because you can apply noise and convince the system

01:07:45.840 --> 01:07:49.840
that you're seeing an ostrich when there's in fact no ostrich,

01:07:49.840 --> 01:07:53.840
you can do the same thing in an

01:07:53.840 --> 01:07:57.840
attacking way. So you can attack the sensors of a car and make

01:07:57.840 --> 01:08:01.840
it believe like with lidar spoofing. So spoof lidar, radar, ultrasonic

01:08:01.840 --> 01:08:05.840
sensors to believe that you're seeing pedestrians when they're not there

01:08:05.840 --> 01:08:09.840
and the opposite. To hide pedestrians, make pedestrians invisible

01:08:09.840 --> 01:08:13.840
to the sensor when they're in fact there.

01:08:13.840 --> 01:08:17.840
So whenever you have indulgent systems operating in this world

01:08:17.840 --> 01:08:21.840
they become susceptible to

01:08:21.840 --> 01:08:25.840
the fact that everything, so much of the work is done in software and based on

01:08:25.840 --> 01:08:29.840
sensors. So at any point in the chain if there's a failure

01:08:29.840 --> 01:08:33.840
you have to be able to detect that failure and right now we have no mechanisms for

01:08:33.840 --> 01:08:37.840
automatically detecting that failure. So on the data side

01:08:37.840 --> 01:08:41.840
so one challenge is that we're constantly

01:08:41.840 --> 01:08:45.840
dealing with is

01:08:45.840 --> 01:08:49.840
we, the algorithms in machine learning algorithms

01:08:49.840 --> 01:08:53.840
that we're using are need labeled

01:08:53.840 --> 01:08:57.840
data and we have very little labeled data.

01:08:57.840 --> 01:09:01.840
Labeled data again is when you have pairs of

01:09:01.840 --> 01:09:05.840
input data and the ground truth, the

01:09:05.840 --> 01:09:09.840
true label annotation class that

01:09:09.840 --> 01:09:13.840
that image belongs to or concept. And it doesn't have to

01:09:13.840 --> 01:09:17.840
be an image, it can be any source of data. It's a really costly process to

01:09:17.840 --> 01:09:21.840
do. So because it's so costly

01:09:21.840 --> 01:09:25.840
we rely

01:09:25.840 --> 01:09:29.840
every breakthrough we've had so far relies on that

01:09:29.840 --> 01:09:33.840
labeled data and because of its cost we don't

01:09:33.840 --> 01:09:37.840
have much of it. So all the problems that come from data can either

01:09:37.840 --> 01:09:41.840
be solved by having a lot more of this data which I believe is

01:09:41.840 --> 01:09:45.840
and most people believe is too challenging. It's too challenging to have

01:09:45.840 --> 01:09:49.840
human beings annotate huge amounts of data or we have to develop

01:09:49.840 --> 01:09:53.840
algorithms that are able to do something with the unlabeled data.

01:09:53.840 --> 01:09:57.840
It's the unsupervised, semi-supervised, sparsely supervised

01:09:57.840 --> 01:10:01.840
reinforcement learning. As we talked about last time I'll mention again

01:10:01.840 --> 01:10:05.840
here. So one way you understand

01:10:05.840 --> 01:10:09.840
something about data when you don't have labels is you reason

01:10:09.840 --> 01:10:13.840
about it. All you're given is a few facts. When you're a baby

01:10:13.840 --> 01:10:17.840
your parents give you a few facts and you go into this world with those facts

01:10:17.840 --> 01:10:21.840
and you grow your knowledge graph, your knowledge base, your understanding of the world from those

01:10:21.840 --> 01:10:25.840
few facts. We don't have a good method of doing that in an automated, unrestricted

01:10:25.840 --> 01:10:29.840
way. The inefficiency of our

01:10:29.840 --> 01:10:33.840
learners. The machine learning algorithms I've talked about in neural networks need

01:10:33.840 --> 01:10:37.840
a lot of examples of every single concept that they're given in order to learn anything

01:10:37.840 --> 01:10:41.840
about them. Thousands, tens of thousands of cats are needed to

01:10:41.840 --> 01:10:45.840
understand what the spatial patterns at every level

01:10:45.840 --> 01:10:49.840
the representation of a cat, the visual representation of a cat.

01:10:49.840 --> 01:10:53.840
We don't, we can't do anything with a single example. There's a few approaches

01:10:53.840 --> 01:10:57.840
but nothing quite robust

01:10:57.840 --> 01:11:01.840
yet. And we haven't come up with

01:11:01.840 --> 01:11:05.840
a way, this is also possible, to make

01:11:05.840 --> 01:11:09.840
annotation, this labeling process, somehow be

01:11:09.840 --> 01:11:13.840
very cheap. So leveraging, this is something been called

01:11:13.840 --> 01:11:17.840
human computation. That term is falling out of favor a little bit.

01:11:17.840 --> 01:11:21.840
One of my big passions is human computation is using

01:11:21.840 --> 01:11:25.840
something about our behavior, something about what we do in this world online

01:11:25.840 --> 01:11:29.840
or in the real world to annotate data automatically.

01:11:29.840 --> 01:11:33.840
So for example

01:11:33.840 --> 01:11:37.840
as you drive, which is what we do, everybody has to

01:11:37.840 --> 01:11:41.840
drive and we can collect data about you driving in order to train self-driving vehicles

01:11:41.840 --> 01:11:45.840
to drive. And that's a

01:11:45.840 --> 01:11:49.840
free annotation. So here are the annotated data sets

01:11:49.840 --> 01:11:53.840
we have. The supervised learning data sets.

01:11:53.840 --> 01:11:57.840
There's many, but these are some of the more famous ones.

01:11:57.840 --> 01:12:01.840
From the toy data sets of MNIST to the large, broad

01:12:01.840 --> 01:12:05.840
arbitrary categories of images data sets

01:12:05.840 --> 01:12:09.840
which is what ImageNet is. And there's

01:12:09.840 --> 01:12:13.840
in healthcare, there's in audio, there's in video, there's

01:12:13.840 --> 01:12:17.840
a huge number of data sets now, but each one of them is usually

01:12:17.840 --> 01:12:21.840
on the scale of hundreds of thousands, millions, tens of millions

01:12:21.840 --> 01:12:25.840
not billions or trillions, which is what we need to create

01:12:25.840 --> 01:12:29.840
systems that operate in the real world. And again

01:12:29.840 --> 01:12:33.840
these are the kinds of machine learning algorithms we have. There's five listed

01:12:33.840 --> 01:12:37.840
here. The teachers on the left

01:12:37.840 --> 01:12:41.840
is what is the input

01:12:41.840 --> 01:12:45.840
to the system that requires to train it. From the supervised

01:12:45.840 --> 01:12:49.840
learning at the very top is what we have all of our successes. And everything else is where the

01:12:49.840 --> 01:12:53.840
promise lies. The semi-supervised, the reinforcement

01:12:53.840 --> 01:12:57.840
or the fully unsupervised learning, where the input from the human is very

01:12:57.840 --> 01:13:01.840
minimal. And another way to think about this, so

01:13:01.840 --> 01:13:05.840
whenever you think about machine learning today, whenever somebody talks about machine

01:13:05.840 --> 01:13:09.840
learning, what they're talking about is systems that memorize.

01:13:09.840 --> 01:13:13.840
That memorize patterns. And so this is one of the big

01:13:13.840 --> 01:13:17.840
criticisms of the current machine learning approaches, where all they're doing is

01:13:17.840 --> 01:13:21.840
you're providing, they're only as good as the human annotated

01:13:21.840 --> 01:13:25.840
data that they're provided. We don't have mechanisms for actually

01:13:25.840 --> 01:13:29.840
understanding. You can pause and think about this. In order to

01:13:29.840 --> 01:13:33.840
create an intelligent system it shouldn't just memorize. It should understand

01:13:33.840 --> 01:13:37.840
the representations inside that data in order to operate in that

01:13:37.840 --> 01:13:41.840
world. And that's the open question.

01:13:41.840 --> 01:13:45.840
One of them. And one of the challenges and opportunities for machine learning

01:13:45.840 --> 01:13:49.840
researchers today is to extend machine learning

01:13:49.840 --> 01:13:53.840
from memorization to understanding. This is that

01:13:53.840 --> 01:13:57.840
duck. The reasoning

01:13:57.840 --> 01:14:01.840
if you get information from the perception systems that it

01:14:01.840 --> 01:14:05.840
looks like a duck, from the audio processing that it quacks like a duck,

01:14:05.840 --> 01:14:09.840
and then from video classification that the activity recognition that it

01:14:09.840 --> 01:14:13.840
swims like a duck, the reasoning step is how to connect those

01:14:13.840 --> 01:14:17.840
facts to then say that it is in fact a duck.

01:14:17.840 --> 01:14:21.840
Okay, so that's on the algorithm side and the

01:14:21.840 --> 01:14:25.840
data side. Now this is one of the reasons

01:14:25.840 --> 01:14:29.840
computational power, computational hardware that is at the core

01:14:29.840 --> 01:14:33.840
of the success of machine learning.

01:14:33.840 --> 01:14:37.840
So our algorithms have been the same since the 60's, since the

01:14:37.840 --> 01:14:41.840
80's, 90's depending on how you're counting. The big

01:14:41.840 --> 01:14:45.840
breakthroughs came in compute. So there's Moore's law.

01:14:45.840 --> 01:14:49.840
Most of you know the way the CPU

01:14:49.840 --> 01:14:53.840
side of our computers works for a single CPU is that it's

01:14:53.840 --> 01:14:57.840
for the most part executing a single action at a time

01:14:57.840 --> 01:15:01.840
in a sequence. So sequential. Very different from

01:15:01.840 --> 01:15:05.840
our brain which is a massively parallelized system.

01:15:05.840 --> 01:15:09.840
So because it's sequential the clock speed matters because that's how fast

01:15:09.840 --> 01:15:13.840
essentially those instructions are able to be executed. And so

01:15:13.840 --> 01:15:17.840
where we're leveling off, physics

01:15:17.840 --> 01:15:21.840
is stopping us from continuing Moore's law.

01:15:21.840 --> 01:15:25.840
Intel, AMD are aggressively pushing this Moore's law

01:15:25.840 --> 01:15:29.840
forward. But, and there's

01:15:29.840 --> 01:15:33.840
some promise that it will actually continue for another 10 or 15 years.

01:15:33.840 --> 01:15:37.840
Then there's another

01:15:37.840 --> 01:15:41.840
form of parallelism, massive parallelism. It's the GPU.

01:15:41.840 --> 01:15:45.840
This is essential for neural networks.

01:15:45.840 --> 01:15:49.840
This is essential to the success, recent success of neural networks is the

01:15:49.840 --> 01:15:53.840
ability to utilize these inherently parallel

01:15:53.840 --> 01:15:57.840
architectures of graphics processing

01:15:57.840 --> 01:16:01.840
units, GPUs. The same thing used for video games. This is the

01:16:01.840 --> 01:16:05.840
reason NVIDIA stock is doing

01:16:05.840 --> 01:16:09.840
extremely well, is GPUs. So it's

01:16:09.840 --> 01:16:13.840
parallelism of basic computational processes that make

01:16:13.840 --> 01:16:17.840
machine learning work on the GPU. One of the

01:16:17.840 --> 01:16:21.840
limitations of GPUs, one of the challenges is

01:16:21.840 --> 01:16:25.840
in bringing them to, in scaling and bringing them into real world applications is

01:16:25.840 --> 01:16:29.840
power usage, is power consumption. And so there is

01:16:29.840 --> 01:16:33.840
a lot of specialized chips specialized just

01:16:33.840 --> 01:16:37.840
from the neural network architectures coming out from Google

01:16:37.840 --> 01:16:41.840
with their tensor processing unit from IBM, Intel and so on.

01:16:41.840 --> 01:16:45.840
It's unclear how far this goes. So this is sort of the direction

01:16:45.840 --> 01:16:49.840
of trying to design an electronic brain, so it has the efficiency.

01:16:49.840 --> 01:16:53.840
Our human brain is exceptionally efficient at running the neural networks

01:16:53.840 --> 01:16:57.840
in our heads. Or does the magnitude more efficient

01:16:57.840 --> 01:17:01.840
than our computers are? And this is trying to design systems that are able to

01:17:01.840 --> 01:17:05.840
go towards that efficiency. Why do you care about

01:17:05.840 --> 01:17:09.840
efficiency? For several reasons. One, of course,

01:17:09.840 --> 01:17:13.840
I'm sure we'll talk about throughout this class is about the thing in our

01:17:13.840 --> 01:17:17.840
smartphones, battery usage. And this

01:17:17.840 --> 01:17:21.840
is the big one, community. I think

01:17:21.840 --> 01:17:25.840
it could be attributed to the big

01:17:25.840 --> 01:17:29.840
breakthroughs in machine learning recently in the last decade is

01:17:29.840 --> 01:17:33.840
the, you know, compute is important, algorithm development

01:17:33.840 --> 01:17:37.840
is important, but it's the community

01:17:37.840 --> 01:17:41.840
of nerds, global. This is global artificial intelligence

01:17:41.840 --> 01:17:45.840
and I will show in several ways why global is

01:17:45.840 --> 01:17:49.840
essential here is tens of

01:17:49.840 --> 01:17:53.840
hundreds of thousands, millions of programmers, mechanical

01:17:53.840 --> 01:17:57.840
engineers, building robots, building intelligence systems,

01:17:57.840 --> 01:18:01.840
building machine learning algorithms. The exciting nature

01:18:01.840 --> 01:18:05.840
of the growth of the community perhaps is the key

01:18:05.840 --> 01:18:09.840
to the future to unlocking the power of machine learning. So this is just

01:18:09.840 --> 01:18:13.840
one example of GitHub as a repository for code and this is showing on

01:18:13.840 --> 01:18:17.840
the y-axis at the bottom is 2008 when GitHub first opened and this is

01:18:17.840 --> 01:18:21.840
going up to 2012. Quick, near exponential

01:18:21.840 --> 01:18:25.840
growth of the number of users participating and the number of repositories. So these are

01:18:25.840 --> 01:18:29.840
standalone unique projects that are being hosted on GitHub.

01:18:29.840 --> 01:18:33.840
So this is one example I'll show you about this competition

01:18:33.840 --> 01:18:37.840
that we're recently running and then I'll challenge people here to participate in this competition

01:18:37.840 --> 01:18:41.840
if you dare. So this is a

01:18:41.840 --> 01:18:45.840
chance for you to build a neural network in your browser

01:18:45.840 --> 01:18:49.840
so you can do this on your phone later tonight of course.

01:18:49.840 --> 01:18:53.840
On your phone you can specify various

01:18:53.840 --> 01:18:57.840
parameters of the neural network, specify different numbers of layers and the depth

01:18:57.840 --> 01:19:01.840
of the network, the number of neurons in the network, the type of layers and it's pretty

01:19:01.840 --> 01:19:05.840
self-explanatory, super easy in terms of just

01:19:05.840 --> 01:19:09.840
tweaking little things and remember machine learning to a large

01:19:09.840 --> 01:19:13.840
part is an art at this point. It's

01:19:13.840 --> 01:19:17.840
more perhaps than even, you know, more than a well understood

01:19:17.840 --> 01:19:21.840
theoretically bounded science which is one of the challenges but it's also an opportunity.

01:19:21.840 --> 01:19:25.840
Deep traffic is a chance, so we've all been stuck

01:19:25.840 --> 01:19:29.840
in traffic. There you go, Americans spend 8 billion hours stuck in traffic every year.

01:19:29.840 --> 01:19:33.840
That's our pitch for this competition. So deep neural network can help

01:19:33.840 --> 01:19:37.840
and so you have a neural network that drives that little car with an

01:19:37.840 --> 01:19:41.840
MIT logo, red one, on this highway and tries to weave in and out of traffic to get

01:19:41.840 --> 01:19:45.840
to his destination and trying to achieve a speed of

01:19:45.840 --> 01:19:49.840
80 miles an hour which is the speed limit which is the physical

01:19:49.840 --> 01:19:53.840
speed limit of the car. Of course the actual speed limit of the road is 65 miles an hour

01:19:53.840 --> 01:19:57.840
but we don't care about that. We just want to get to work as quickly as possible at home.

01:19:57.840 --> 01:20:01.840
So what the basic

01:20:01.840 --> 01:20:05.840
structure of this game is and I want to explain this game a little bit and then tell

01:20:05.840 --> 01:20:09.840
you how incredibly popular it's gotten and how incredibly

01:20:09.840 --> 01:20:13.840
powerful the

01:20:13.840 --> 01:20:17.840
networks that people have built from all over the world. The community has built

01:20:17.840 --> 01:20:21.840
of this over a single month is incredible and this happens for

01:20:21.840 --> 01:20:25.840
thousands of projects out there. Now another challenging

01:20:25.840 --> 01:20:29.840
opportunity. Okay so you may have seen this, this is kind of ethics.

01:20:29.840 --> 01:20:33.840
Most engineers, I personally don't like, I

01:20:33.840 --> 01:20:37.840
love philosophy but this kind of construction of

01:20:37.840 --> 01:20:41.840
ethics that's often presented here is one that is not usually

01:20:41.840 --> 01:20:45.840
concerned to engineering. So what is this question? You know when you have a car

01:20:45.840 --> 01:20:49.840
and you have a bunch of pedestrians, do you hit the larger group of pedestrians

01:20:49.840 --> 01:20:53.840
or the smaller group of pedestrians? Do you avoid the group

01:20:53.840 --> 01:20:57.840
of pedestrians but put yourself into danger? These kinds of ethical

01:20:57.840 --> 01:21:01.840
questions of an intelligent system. It's a very interesting question.

01:21:01.840 --> 01:21:05.840
It's one that we can debate and there's really no good answer quite honestly

01:21:05.840 --> 01:21:09.840
but it's a problem that both humans and machines struggle with and so it's

01:21:09.840 --> 01:21:13.840
not interesting on the engineering side. We're interested with problems that we can

01:21:13.840 --> 01:21:17.840
solve on the engineering side. So the kind of problem that I'm obsessed with

01:21:17.840 --> 01:21:21.840
and very interested in is the real world problem of controlling a vehicle through this space.

01:21:21.840 --> 01:21:25.840
So it happens in a few seconds

01:21:25.840 --> 01:21:29.840
here. So this is a Manhattan-New York intersection, right?

01:21:29.840 --> 01:21:33.840
This is pedestrians walking perfectly

01:21:33.840 --> 01:21:37.840
legally. I think they have a green light. Of course there's a lot of jaywalking too as well.

01:21:37.840 --> 01:21:41.840
Well this car just, it's not part of the

01:21:41.840 --> 01:21:45.840
point but yes, exactly, there's an ambulance. And so there's another car that starts

01:21:45.840 --> 01:21:49.840
making a left turn in a little bit. Let me admit that hopefully not.

01:21:49.840 --> 01:21:53.840
But yeah, and then there's another car after that too that just illustrates

01:21:53.840 --> 01:21:57.840
when you design an algorithm that's supposed to move through this space

01:21:57.840 --> 01:22:01.840
like watch this car. The aggression it shows. Now this isn't

01:22:01.840 --> 01:22:05.840
a trivial example for those that try to build robots. This is the real question

01:22:05.840 --> 01:22:09.840
is how do you design a system

01:22:09.840 --> 01:22:13.840
that's able, so you have to think. You have to put

01:22:13.840 --> 01:22:17.840
reward functions, objective functions, utility functions under which

01:22:17.840 --> 01:22:21.840
it performs the planning. So a car like that has

01:22:21.840 --> 01:22:25.840
several thousand candidate trajectories you can take through that

01:22:25.840 --> 01:22:29.840
intersection. You can take a trajectory where it speeds up to 60 miles an hour and doesn't stop

01:22:29.840 --> 01:22:33.840
and just swerves and hits everything. Okay, that's a bad trajectory, right? Then

01:22:33.840 --> 01:22:37.840
there's a trajectory which most companies take, most

01:22:37.840 --> 01:22:41.840
Google self-driving car and every company that's concerned about PR is

01:22:41.840 --> 01:22:45.840
whenever there's any kind of obstacle, any kind of risk that's

01:22:45.840 --> 01:22:49.840
at all reasonable that you can maybe even touch an obstacle then you're not going

01:22:49.840 --> 01:22:53.840
to take that trajectory. So what that means is you're going to navigate through this intersection

01:22:53.840 --> 01:22:57.840
at 10 miles an hour and let people abuse you by walking in front of you

01:22:57.840 --> 01:23:01.840
because they know you're not going to stop. And so in the middle there is

01:23:01.840 --> 01:23:05.840
hundreds, thousands of trajectories that are ethically questionable

01:23:05.840 --> 01:23:09.840
in the sense that you're putting other human beings at risk in order to

01:23:09.840 --> 01:23:13.840
safely and successfully navigate through the intersection. And the design of those

01:23:13.840 --> 01:23:17.840
objective functions is the kind of question you have to ask

01:23:17.840 --> 01:23:21.840
for intelligence systems for cars. There's no

01:23:21.840 --> 01:23:25.840
grandma and a few children you have to choose who gets to

01:23:25.840 --> 01:23:29.840
die. Very difficult problems, of course. But

01:23:29.840 --> 01:23:33.840
the problem of what I'm very interested in in streets of Boston

01:23:33.840 --> 01:23:37.840
and streets of New York is how to gently nudge yourself

01:23:37.840 --> 01:23:41.840
through a crowd of pedestrians in the way we all actually do

01:23:41.840 --> 01:23:45.840
when we drive in New York in order to be able to safely

01:23:45.840 --> 01:23:49.840
navigate these environments. And these questions come up in healthcare, these questions come up

01:23:49.840 --> 01:23:53.840
in factory, in robots, in armed and humanoid robots

01:23:53.840 --> 01:23:57.840
that operate with other human beings.

01:23:57.840 --> 01:24:01.840
And that's one of the big challenges. Another sort of

01:24:01.840 --> 01:24:05.840
fun illustration that folks at OpenAI use often

01:24:05.840 --> 01:24:09.840
to illustrate, well let me just pause for a second, the gamified version

01:24:09.840 --> 01:24:13.840
of this. There's a game called Coast Runners and you're racing against other boats

01:24:13.840 --> 01:24:17.840
along this track. And your job is, there's your score here

01:24:17.840 --> 01:24:21.840
at the bottom left, number of laps, your time, and you're

01:24:21.840 --> 01:24:25.840
trying to get to the destination as quickly as possible while also collecting

01:24:25.840 --> 01:24:29.840
funky little things like these green

01:24:29.840 --> 01:24:33.840
these green little things along the way. Okay, so

01:24:33.840 --> 01:24:37.840
what they've done is a build intent system, the one, the general purpose one that we

01:24:37.840 --> 01:24:41.840
talked about last time that learns, oops, that learns

01:24:41.840 --> 01:24:45.840
how to navigate successfully through the space. So

01:24:45.840 --> 01:24:49.840
you're trying to maximize the reward. And what this boat

01:24:49.840 --> 01:24:53.840
learns to do is instead of finishing the race

01:24:53.840 --> 01:24:57.840
it learns to find a loop where

01:24:57.840 --> 01:25:01.840
it can keep going around and around, collecting those green dots

01:25:01.840 --> 01:25:05.840
and it learns the fact that they regenerate with time.

01:25:05.840 --> 01:25:09.840
So it learns to maximize this score

01:25:09.840 --> 01:25:13.840
by going around and around. Now these are the kinds of things

01:25:13.840 --> 01:25:17.840
this is the big challenge of reward functions, of designing systems

01:25:17.840 --> 01:25:21.840
of designing what you want your system to achieve is

01:25:21.840 --> 01:25:25.840
not only is it difficult to, the ethical questions are difficult

01:25:25.840 --> 01:25:29.840
but just avoiding the pitfalls of local optima

01:25:29.840 --> 01:25:33.840
of figuring out something really good that happens

01:25:33.840 --> 01:25:37.840
in the short term, the greedy, what are those, those psychology experiments of the kid

01:25:37.840 --> 01:25:41.840
eats the marshmallow and can't wait for, you know, can't

01:25:41.840 --> 01:25:45.840
delay gratification. This kind of, the idea of delay gratification

01:25:45.840 --> 01:25:49.840
in the case of designing intelligence systems is a huge actual serious problem

01:25:49.840 --> 01:25:53.840
and this is a good illustration of that.

01:25:53.840 --> 01:25:57.840
So we flew through a few concepts here

01:25:57.840 --> 01:26:01.840
is there any questions about

01:26:01.840 --> 01:26:05.840
the compute and the algorithm side we talked about today?

01:26:05.840 --> 01:26:09.840
So the question was, yeah you highlighted some of the limitations of

01:26:09.840 --> 01:26:13.840
machine, computer vision algorithms, machine learning algorithms, but

01:26:13.840 --> 01:26:17.840
you haven't highlighted some of the limitations of human beings and if you put those in a column

01:26:17.840 --> 01:26:21.840
and you compare those, are machines doing better

01:26:21.840 --> 01:26:25.840
overall or is there any kind of way to compare those? I mean there is actually

01:26:25.840 --> 01:26:29.840
interesting work on ImageNet, so ImageNet is this categorization

01:26:29.840 --> 01:26:33.840
task of where you have to classify images and you can ask the question

01:26:33.840 --> 01:26:37.840
when I present you images of cats and dogs, where are machines better than humans

01:26:37.840 --> 01:26:41.840
and when are they not? So you can compare when machines do better,

01:26:41.840 --> 01:26:45.840
what are the fail points and what are the fail points for humans and there's a lot of interesting

01:26:45.840 --> 01:26:49.840
visual perception questions there. But I think overall it's certainly

01:26:49.840 --> 01:26:53.840
true that machines fail differently than human beings, but

01:26:53.840 --> 01:26:57.840
in order to make an artificial intelligence system

01:26:57.840 --> 01:27:01.840
usable and could make you a lot of money

01:27:01.840 --> 01:27:05.840
and people would want to use, it has to be better for that particular task

01:27:05.840 --> 01:27:09.840
in every single way. In order

01:27:09.840 --> 01:27:13.840
for you to want to use a system, it has to be superior

01:27:13.840 --> 01:27:17.840
to human performance and usually far superior to human performance.

01:27:17.840 --> 01:27:21.840
So on the philosophical level it's an interesting thing to compare

01:27:21.840 --> 01:27:25.840
what are we good at, what are not, but if you're using

01:27:25.840 --> 01:27:29.840
Amazon Echo, your voice recognition

01:27:29.840 --> 01:27:33.840
or any kind of natural language, chatbots, or a car,

01:27:33.840 --> 01:27:37.840
you're not going to be, well this car is not so good with pedestrians, but I appreciate the fact

01:27:37.840 --> 01:27:41.840
that you can stay in the lane. Fortunately you have a very high standard

01:27:41.840 --> 01:27:45.840
for every single thing that you're good at and it has to be superior to that. I think

01:27:45.840 --> 01:27:49.840
maybe that's unfair to the robots.

01:27:49.840 --> 01:27:53.840
I'm more of the nerd that makes the technology happen

01:27:53.840 --> 01:27:57.840
but it's certainly on the self-driving car aspect

01:27:57.840 --> 01:28:01.840
policy is probably the biggest challenge and I don't think there's good answers

01:28:01.840 --> 01:28:05.840
there. Some of those ethical questions that

01:28:05.840 --> 01:28:09.840
come up, it feels like, so we work a lot with Tesla

01:28:09.840 --> 01:28:13.840
so I'm driving a Tesla around every day and we're playing around with it

01:28:13.840 --> 01:28:17.840
and studying human behavior inside Tesla and it seems like there's so much

01:28:17.840 --> 01:28:21.840
hunger amongst the media to jump on something and it feels like

01:28:21.840 --> 01:28:25.840
a very shaky PR terrain, a very shaky

01:28:25.840 --> 01:28:29.840
policy terrain we're all walking because we have no idea how

01:28:29.840 --> 01:28:33.840
we coexist with intelligent systems and then of course

01:28:33.840 --> 01:28:37.840
the government is nervous because how do we regulate this shaky terrain

01:28:37.840 --> 01:28:41.840
and everybody's nervous and excited.

01:28:41.840 --> 01:28:45.840
That's a perfect transition point if that's okay.

01:28:45.840 --> 01:28:49.840
That same kind of question to Jason in a moment. Thanks a lot Lex for another great session.

01:28:49.840 --> 01:28:53.840
Thank you.

