start	end	text
0	4960	The following is a conversation with Yosha Bach, his second time in the podcast.
4960	10680	Yosha is one of the most fascinating minds in the world exploring the nature of intelligence,
10680	14600	cognition, computation, and consciousness.
14600	21240	To support this podcast, please check out our sponsors, Coinbase, Codecademy, Linode,
21240	24120	Netsuite, and ExpressVPN.
24120	27000	Their links are in the description.
27000	33600	This is the Lex Friedman podcast, and here is my conversation with Yosha Bach.
33600	38920	Thank you for once again coming on to this particular Russian program and sticking to
38920	40920	the theme of a Russian program.
40920	45360	Let's start with the darkest of topics.
45360	48560	So this is inspired by one of your tweets.
48560	56640	You wrote that, quote, when life feels unbearable, I remind myself that I'm not a person.
56640	61660	I am a piece of software running on the brain for random ape for a few decades.
61660	64760	It's not the worst brain to run on.
64760	67840	Have you experienced low points in your life?
67840	69880	Have you experienced depression?
69880	75440	Of course, we all experience low points in our life, and we get appalled by the things,
75440	77120	by the ugliness of stuff around us.
77120	84840	We might get desperate about our lack of self-regulation, and sometimes life is hard.
84840	90640	And I suspect you don't get to your life nobody does to get to their life without low points
90640	93960	and without moments where they are despairing.
93960	100560	And I thought that let's capture this state and how to deal with that state.
100560	105000	And I found that very often you realize that when you stop taking things personally, when
105000	110800	you realize that this notion of a person is a fiction, similar as it is in Westworld where
110800	115040	the robots realize that their memories and desires are just stuff that keeps them in
115040	120160	the loop, and they don't have to act on those memories and desires, that our memories and
120160	122680	expectations is what make us unhappy.
122680	124520	And the present rarely does.
124520	128160	The day in which we are, for the most part, it's okay, right?
128160	133240	When we are sitting here, right here, right now, we can choose how we feel.
133240	138920	And the thing that affects us is the expectation that something is going to be different from
138920	144360	what we wanted to be or the memory that something was different from what we wanted it to be.
144360	149160	And once we basically zoom out from all this, what's left is not a person.
149160	153800	What's left is this state of being conscious, which is a software state.
153800	155840	And software doesn't have an identity.
155840	157960	It's a physical law.
157960	162440	And it's a law that acts in all of us, and it's embedded in a suitable substrate.
162440	163800	And we didn't pick that substrate, right?
163800	167080	We are mostly randomly instantiated on it.
167080	172160	And they're all these individuals, and everybody has to be one of them.
172160	176240	And eventually, you're stuck on one of them and have to deal with that.
176240	179400	So you're like a leaf floating down the river.
179400	185160	You just have to accept that there's a river, and you just float wherever there are.
185160	189720	The thing is that the illusion that you are an agent is a construct.
189720	193480	What part of that is actually under your control.
193560	198440	I think that our consciousness is largely a control model for our own attention.
198440	202680	So we notice where we are looking, and we can influence what we are looking,
202680	205880	how we are disambiguating things, how we put things together in our mind.
206600	210920	And the whole system that runs us is this big cybernetic motivational system.
210920	214840	So we're basically like a little monkey sitting on top of an elephant.
214840	219320	And we can prod this elephant here and there to go this way or that way.
219320	221960	And we might have the illusion that we are the elephant,
221960	223480	or that we are telling it what to do.
223480	227400	And sometimes we notice that it walks into a completely different direction.
227400	229000	And we didn't set this thing up.
229000	231800	It just is the situation that we find ourselves in.
232600	234840	How much prodding can we actually do of the elephant?
236360	242360	A lot. But I think that our consciousness cannot create the motive force.
243000	245320	Is the elephant consciousness in this metaphor?
245320	247880	No, the monkey is the consciousness.
247880	250360	The monkey is the attentional system that is observing things.
250360	254360	There is a large perceptual system combined with a motivational system
254360	258680	that is actually providing the interface to everything and our own consciousness,
258680	262440	I think, is the tool that directs the attention of that system,
262440	266920	which means it singles out features and performs conditional operations
266920	268840	for which it needs an index memory.
268840	272680	But this index memory is what we perceive as our stream of consciousness.
272680	275880	But the consciousness is not in charge. That's an illusion.
275880	281320	So everything outside of that consciousness is the elephant.
281320	285480	So it's the physics of the universe, but it's also society that's outside of your...
286040	288280	I would say the elephant is the agent.
288280	291320	So there is an environment in which the agent is stomping.
291320	294440	And you are influencing a little part of that agent.
295080	298280	So can you, is the agent a single human being?
299000	301480	What's, which object has agency?
302280	303800	That's an interesting question.
303800	309480	I think a way to think about an agent is that it's a controller with a set point generator.
310360	313800	The notion of a controller comes from cybernetics and control theory.
314360	320840	Control system consists out of a system that is regulating some value
320840	323880	and the deviation of that value from a set point.
323880	328280	And it has a sensor that measures the system's deviation from that set point
328840	332520	and an effector that can be parametrized by the controller.
332520	335400	So the controller tells the effector to do a certain thing.
335400	340760	And the goal is to reduce the distance between the set point and the current value of the system.
340760	343480	And there's an environment which disturbs the regulated system,
343480	345480	which brings it away from that set point.
345480	347720	So the simplest case is the thermostat.
347720	350120	And the thermostat is really simple because it doesn't have a model.
350120	354680	The thermostat is only trying to minimize the set point deviation in the next moment.
355720	359800	And if you want to minimize the set point deviation over a longer time span,
359800	363640	you need to integrate it. You need to model what is going to happen.
363640	367560	So for instance, when you think about that your set point is to be comfortable in life,
368200	370440	maybe you need to make yourself uncomfortable first.
371160	373960	Right? So you need to make a model of what's going to happen when.
373960	380520	And this is the task of the controller is to use its sensors to measure the state of the environment
380520	384120	and the system that is being regulated and figure out what to do.
384840	389640	And if the task is complex enough, the set points are complicated enough,
390040	394840	and if the controller has enough capacity and enough sensor feedback,
394840	399000	then the task of the controller is to make a model of the entire universe that it's in,
399000	401240	the conditions under which it exists and of itself.
402200	404920	And this is a very complex agent and we are in that category.
405640	409320	And an agent is not necessarily a thing in the universe.
409320	413480	It's a class of models that we use to interpret aspects of the universe.
414440	419720	And when we notice the environment around us, a lot of things only make sense at the level
419720	423160	that should be entangled with them, if we interpret them as control systems
423160	427160	that make models of the world and try to minimize their own set points.
427160	429400	So the models are the agents?
430360	434520	The agent is a class of model. And we notice that we are an agent ourselves.
434520	438680	We are the agent that is using our own control model to perform actions.
438680	443320	We notice we would use a change in the model and things in the world change.
443480	446680	And this is how we discover the idea that we have a body,
446680	449960	that we are a situated environment and that we have a first person perspective.
451080	458120	Still don't understand what's the best way to think of which object has agency with respect
458120	465880	to human beings. Is it the body? Is it the brain? Is it the contents of the brain that has agency?
465880	468120	Like what's the actuators that you're referring to?
469080	471960	What is the controller and where does it reside?
471960	476920	Or is it these impossible things? Because I keep trying to ground it to space time,
477560	480680	the three-dimensional space, and the one-dimension of time.
481480	483880	What's the agent in that for humans?
484440	488680	There is not just one. It depends on the way in which you're looking at this thing,
488680	493480	in which you're framing it. Imagine that you are, say, Angela Merkel,
493480	499240	and you are acting on behalf of Germany. And you could say that Germany is the agent.
499800	503480	And in the mind of Angela Merkel, she is Germany to some extent,
503480	507400	because in the way in which she acts, the destiny of Germany changes.
507960	513480	There are things that she can change that basically affect the behavior of that nation state.
513480	518680	Okay, so it's hierarchies to go to another one of your tweets with, I think, your
519640	524040	playfully mocking Jeff Hawkins with saying his brain's all the way down.
525720	531640	So it's agents all the way down. It's agents made up of agents made up of agents.
531640	536440	Like if Angela Merkel is Germany, and Germany's made up a bunch of people,
536440	540200	and the people are themselves agents in some kind of context.
540920	546520	And then people are made up of cells, each individual. So is it agents all the way down?
547080	552200	I suspect that has to be like this in a world where things are self-organizing.
552840	557640	Most of the complexity that we are looking at, everything in life is about self-organization.
558440	567400	So I think up from the level of life, you have agents. And below life, you rarely have agents,
567400	572680	because sometimes you have control systems that emerge randomly in nature and try to achieve
572680	577800	a set point. But they're not that interesting agents that make models. And because to make
577800	581720	an interesting model of the world, you typically need a system that is during complete.
582200	589400	Can I ask you a personal question? What's the line between life and non-life? It's personal
589400	596360	because you're a life form. So what do you think in this emerging complexity, at which point,
596360	598840	does the things that are being living and have agency?
599800	603560	Personally, I think that the simplest answer there is that life is cells, because...
604440	609640	Life is what? Cells, biological cells. So it's a particular kind of principle
609640	615160	that we have discovered to exist in nature. It's modular stuff that consists out of
616680	622600	basically this DNA tape with a redried head on top of it that is able to perform arbitrary
622600	627800	computations and state transitions within the cell. And it's combined with a membrane that
627800	635000	insulates the cell from its environment. And there are chemical reactions inside of the cell
635000	640360	that are in disequilibrium. And the cell is running in such a way that this disequilibrium
640360	645880	doesn't disappear. And the cell goes... If the cell goes into an equilibrium state, it dies.
646520	652440	And it requires some things like a neck entropy extractor to maintain this disequilibrium. So
652440	657240	it's able to harvest neck entropy from its environment and keep itself running.
658040	664280	Yeah, so there's information and there's a wall to protect, to maintain this disequilibrium.
664280	667960	But isn't this very earth-centric? Like what you're referring to...
669000	673560	I'm not making a normative notion. You could say that there are probably other things in the
673560	678760	universe that are cell-like and lifelike. And you could also call them life. But eventually,
678760	685240	it's just a willingness to find an agreement of how to use the terms. I like cells because
685240	689880	it's completely co-extensional. This is the way that we use the word even before we knew about
689880	695240	cells. So people were pointing at some stuff and saying, this is somehow animate. And this is very
695240	699480	different from the non-animate stuff. And what's the difference between the living and the dead
699480	705480	stuff? And it's mostly whether the cells are working or not. And also this boundary of life,
705480	709640	where we say that, for instance, a virus is basically an information packet that is subverting
709640	716360	the cell and not life by itself. That makes sense to me. And it's somewhat arbitrary. You could,
716360	721720	of course, say that systems that permanently maintain a disequilibrium and can self-replicate
721720	729320	are always life. And maybe that's a useful definition too. But this is eventually just
729320	732920	how you want to use the word. Is it so useful for conversation? But
734040	739960	is it somehow fundamental to the universe? Do you think there's an actual line to eventually
739960	745560	be drawn between life and non-life? Or is it all kind of continuum? I don't think it's a continuum,
745560	750520	but there's nothing magical that is happening. Living systems are a certain type of machine.
751160	756440	What about non-living systems? Is it also a machine? There are non-living machines. But
756440	762760	the question is at which point is a system able to perform arbitrary state transitions
762760	768360	in to make representations? And living things can do this. And of course, we can also build
768360	773880	non-living things that can do this. But we don't know anything in nature that is not a cell
774600	777240	and is not created by a stellar life that is able to do that.
777800	785000	Not only do we not know, I don't think we have the tools to see otherwise.
786280	793160	I always worry that we look at the world too narrowly. There could be life
793800	799800	of a very different kind right under our noses that we're just not seeing because we're not
800760	806280	either limitations of our cognitive capacity or we're just not open-minded enough.
807080	811000	Either with the tools of science or just the tools of our mind.
812120	817480	Yeah, that's possible. I find this thought very fascinating. And I suspect that many of us ask
817480	821960	ourselves since childhood, what are the things that we are missing? What kind of systems and
821960	832520	interconnections exist that are outside of our gaze? But we are looking for it and physics doesn't
833080	840280	have much room at the moment for opening up something that would not violate the conservation
840280	848280	of information as we know it. Yeah, but I wonder about timescale and spatial scale,
848280	856280	whether we just need to open up our idea of how life presents itself. It could be operating in a
856280	863640	much slower timescale, a much faster timescale. And it's almost sad to think that there's all this
863640	870040	life around us that we're not seeing because we're just not thinking in terms of the right
871320	877400	scale, both time and space. What is your definition of life? What do you understand as life?
877400	885880	Hmm. Entities of sufficiently high complexity that are full of surprises.
891800	896200	I don't know. I don't have a free will. So that just came out of my mouth. I'm not sure
896200	901480	that even makes sense. There's certain characteristics. So complexity seems to be a
901480	911640	necessary property of life. And I almost want to say it has ability to do something unexpected.
913240	916680	It seems to me that life is the main source of complexity on Earth.
918680	926360	Yes. And complexity is basically a bridgehead that order builds into chaos by modeling,
927080	932280	by processing information in such a way that you can perform reactions that would not be possible
932280	937240	for dump systems. And this means that you can harvest like entropy that dump systems cannot
937240	943160	harvest. And this is what complexity is mostly about. In some sense, the purpose of life is to
943160	952280	create complexity. Yeah. Increasing complexity. There seems to be some kind of universal drive
952280	958440	towards increasing pockets of complexity. I don't know what that is. That seems to be like a
958440	963960	fundamental, I don't know if it's a property of the universe, or it's just a consequence
963960	969960	of the way the universe works. But there seems to be this small pockets of emergent complexity
969960	975800	that builds on top of each other and starts having like greater and greater complexity by
975800	980600	having like a hierarchy of complexity, like little organisms building up a little society
980680	985640	that then operates almost as an individual organism itself. And all of a sudden, you have
985640	991320	Germany and Merkel. That's not obvious to me. Everything that goes up has to come down at some
991320	998520	point. So if you see this big exponential curve somewhere, it's usually the beginning of an S-curve,
999240	1003320	where something eventually reaches a saturation. And the S-curve is the beginning of some kind
1003320	1012120	of bump that goes down again. And there is just this thing that when you are inside of an evolution
1012120	1019880	of life, you are on top of a puddle of negentropy that is being sucked dry by life. And during
1019880	1024760	that happening, you see an increase in complexity, because life forms are competing with each other
1024760	1031000	to get more and more and finer and finer corner of that, like entropy extraction.
1031080	1034760	But that, I feel like that's a gradual, beautiful process, like that's almost,
1035720	1042200	follows a process akin to evolution. And the way it comes down is not the same way it came up.
1042840	1049800	The way it comes down is usually harshly and quickly. So usually there's some kind of catastrophic
1049800	1058040	event. The Roman Empire took a long time. Would that be, would you classify that as a
1058040	1063240	decrease in complexity, though? Yes. I think that this size of the cities that could be fed has
1063240	1068600	decreased dramatically. And you could see that the quality of the art decreased, and it did so
1068600	1075720	gradually. And maybe future generations, when they look at the history of the United States
1075720	1080120	in the 21st century, will also talk about the gradual decline, not something that suddenly
1080200	1089640	happens. Do you have a sense of where we are? Are we on the exponential rise?
1089640	1095720	Are we at the peak? Or are we at the down slope of the United States empire?
1095720	1101640	It's very hard to say from a single human perspective, but it seems to me that we are
1101640	1108840	probably at the peak. I think that's probably the definition of optimism and cynicism.
1109560	1112600	So my nature of optimism is, I think we're on the rise.
1117000	1121080	I think it's just all a matter of perspective that nobody knows. But I do think that
1121080	1126280	erring on the side of optimism, you need a sufficient number. You need a minimum
1126280	1132040	number of optimists in order to make that up thing actually work. And so I tend to be on
1132040	1136840	the side of the optimists. I think that we are basically a species of grasshoppers that have
1136840	1143480	turned into locusts. And when you are in that locust mode, you see an amazing rise of population
1143480	1150920	numbers and of the complexity of the interactions between the individuals. But ultimately, the
1150920	1157320	question is, is it sustainable? See, I think we're a bunch of lions and tigers that have become
1157320	1164200	domesticated cats to use a different metaphor. And so I'm not exactly sure we're so destructive,
1164200	1170040	we're just softer and nicer and lazier. But I think we have monkeys and not the cats. And if
1170040	1175880	you look at the monkeys, they are very busy. The ones that have a lot of sex, those monkeys?
1175880	1180040	Not just the bonobos. I think that all the monkeys are basically a discontent species that
1180040	1185720	always needs to meddle. Well, the gorillas seem to have a little bit more of a structure,
1185720	1194120	but it's a different part of the tree. Okay, you mentioned the elephant and the monkey riding
1194120	1203080	the elephant. And consciousness is the monkey. And there's some prodding that the monkey gets to do.
1203080	1209720	And sometimes the elephant listens. I heard you got into some content maybe you can correct me,
1209720	1215000	but I heard you got into some contentious free will discussions. Is this with Sam Harris or
1215000	1221800	something like that? Not that I know of. Some people in clubhouse told me you made a bunch of
1222360	1229640	big debate points about free will. Well, let me just then ask you where in terms of the monkey
1229640	1236360	and the elephant, do you think we land in terms of the illusion of free will? How much control does
1236360	1243880	the monkey have? We have to think about what the free will is in the first place. We are not the
1243880	1248600	machine. We are not the thing that is making the decisions. We are a model of that decision-making
1248600	1254920	process. And there is a difference between making your own decisions and predicting your
1254920	1263160	own decisions. And that difference is the first person perspective. And what basically makes
1263160	1269320	decision-making and the conditions of free will distinct from just automatically doing the best
1269320	1275400	thing is that we often don't know what the best thing is. We make decisions under uncertainty.
1275400	1279960	We make informed bets using a betting algorithm that we don't yet understand because we haven't
1279960	1284120	reverse engineered our own minds efficiently. We don't know the expected rewards. We don't
1284120	1288200	know the mechanism by which we estimate the rewards and so on. But there is an algorithm.
1288200	1295720	We observe ourselves performing, where we see that we weight facts and factors and the future.
1295720	1302360	And then some kind of possibility, some motive gets raised to an intention. And that's informed
1302360	1307640	bet that the system is making. And that making of the informed bet, the representation of that is
1307640	1313320	what we call free will. And it seems to be paradoxical because we think that the crucial
1313320	1319000	thing is that it's somehow indeterministic. And yet, if it was indeterministic, it would be random.
1320200	1324840	And it cannot be random because if it was random, if just dice were being thrown in the
1324840	1329640	universe randomly, forces you to do things, it would be meaningless. So the important part
1329640	1334920	of the decisions is always the deterministic stuff. But it appears to be indeterministic to
1334920	1340200	you because it's unpredictable. Because if it was predictable, you wouldn't experience it as a free
1340200	1346360	will decision. You would experience it as just doing the necessary right thing. And you see this
1346360	1352760	continuum between the free will and the execution of automatic behavior when you're observing other
1352760	1357560	people. So for instance, when you are observing your own children, if you don't understand them,
1357640	1364360	you will use this agent model where you have an agent with a set point generator. And the agent is
1364360	1368680	doing the best it can to minimize the difference to the set point. And it might be confused and
1369320	1374680	sometimes impulsive or whatever, but it's acting on its own free will. And when you understand
1374680	1380280	what happens in the mind of the child, you see that it's automatic. And you can outmodel the child,
1380280	1385160	you can build things around the child that will lead the child to making exactly the decision
1385160	1389640	that you are predicting. And under these circumstances, like when you are a stage
1389640	1396120	misvision, or somebody who is dealing with people that you sell a car to, and you completely
1396120	1400520	understand the psychology and the impulses and the space of thoughts that this individual can
1400520	1404280	have at that moment. Under these circumstances, it makes no sense to attribute free will.
1405960	1409560	Because it's no longer decision making under uncertainty. You are already certain. For them,
1409560	1411800	there's uncertainty, but you already know what they're doing.
1412520	1420440	But what about for you? So is this akin to like systems like cellular automata,
1421480	1426600	where it's deterministic? But when you squint your eyes a little bit,
1427800	1434440	it starts to look like there's agents making decisions at the higher sort of when you zoom
1434440	1440440	out and look at the entities that are composed by the individual cells, even though there's
1441240	1448760	underlying simple rules that make the system evolve in deterministic ways, it looks like
1448760	1456280	there's organisms making decisions. Is that where the illusion of free will emerges? That jump in scale.
1457320	1462520	It's a particular type of model, but this jump in scale is crucial. The jump in scale happens
1462520	1466360	whenever you have too many parts to count, and you cannot make a model at that level,
1466360	1471640	and you try to find some higher level regularity. And the higher level regularity is a pattern that
1471640	1477400	you project into the world to make sense of it. And agency is one of these patterns. You have all
1477400	1482680	these cells that interact with each other, and the cells in our body are set up in such a way
1482680	1488200	that they benefit if their behavior is coherent, which means that they act as if they were serving
1488200	1494200	a common goal. And that means that they will evolve regulation mechanisms that act as if
1494200	1499000	they were serving a common goal. And now you can make sense of all these cells by projecting the
1499000	1504920	common goal into them. Right. So for you, then, free will is an illusion? No, it's a model,
1504920	1509880	and it's a construct. It's basically a model that the system is making of its own behavior,
1509880	1514360	and it's the best model that it can come up with under the circumstances, and it can get replaced
1514360	1518520	by a different model, which is automatic behavior, when you fully understand the mechanism under
1518520	1525320	which you are acting. Yeah, but another word for model is what story. So it's the story you're
1525320	1531400	telling. I mean, do you actually have control? Is there such a thing as a you? And is there such a
1531400	1541880	thing as you having control? So like, are you manifesting your evolution as an entity?
1541880	1546920	In some sense, the you is the model of the system that is in control. It's a story that the system
1546920	1552600	tells itself about somebody who is in control. Yeah. And the contents of that model are being
1552600	1560360	used to inform the behavior of the system. Okay. So the system is completely mechanical,
1560360	1565960	and the system creates that story like a loom. And then it uses the contents of that story
1565960	1571080	to inform its actions and writes the results of that actions into the story.
1571080	1579000	So how's that non-illusion? The story is written then? Or a rather, we're not the writers of the
1579000	1586600	story? Yes, but we always knew that. No, we don't know that. When did we know that?
1586600	1592520	I think that's mostly a confusion about concepts. The conceptual illusion in our culture comes from
1592520	1597320	the idea that we live in physical reality, and that we experience physical reality,
1597320	1602040	and that you have ideas about it. And then you have this dualist interpretation where you have
1602760	1608840	two substances, res extensor, the world that you can touch, and that is made of extended things,
1608840	1613880	and res cogitans, which is the world of ideas. And in fact, both of them are mental representations.
1614520	1619240	One is the representations of the world as a game engine that your mind generates to make
1619240	1624040	sense of the perceptual data. And the other one, yes, that's what we perceive as the physical
1624040	1627880	world. But we already know that the physical world is nothing like that. Quantum mechanics is
1627880	1633480	very different from what you and me perceive as the world. The world you and me perceive is a game
1633480	1638280	engine. And there are no colors and sounds in the physical world that only exist in the game
1638280	1644680	engine generated by your brain. And then you have ideas that cannot be mapped onto extended regions.
1644680	1649960	So the objects that have a spatial extension in the game engine, res extensor, and the objects
1650040	1655800	that don't have a physical extension in the game engine are ideas. And they both interact in our
1655800	1664120	mind to produce models of the world. But when you play video games, I understand that what's
1664120	1673240	actually happening is zeros and ones inside of a computer, inside of a CPU and a GPU, but you're
1673320	1681800	still seeing like the rendering of that. And you're still making decisions whether to shoot,
1681800	1686520	to turn left or to turn right. If you're playing a shooter or every time you start thinking about
1686520	1691640	Skyrim and Elder Scrolls and walking around a beautiful nature and swinging a sword. But it
1691640	1696440	feels like you're making decisions inside that video game. So even though you don't have direct
1696440	1703640	access in terms of perception to the bits, to the zeros and ones, it still feels like you're
1703640	1708840	making decisions. And your decisions are actually feels like they're being applied
1710040	1714280	all the way down to the zeros and ones. So it feels like you have control even though you don't
1714280	1719720	direct access to reality. So there is basically a special character in the video game that is
1719720	1724360	being created by the video game engine. And this character is serving the aesthetics of the video
1724360	1731880	game. And that is you. Yes, but I feel like I have control inside the video game, like all those
1731880	1737960	like 12 year olds that kick my ass on the internet. So for when you play the video game, it doesn't
1737960	1741960	really matter that there's zeros and ones, right? You don't care about the vids of the bus, you don't
1741960	1746360	care about the nature of the CPU that it runs on. What you care about are the properties of the
1746360	1751640	game that you're playing. And you hope that the CPU is good enough. Yes. And a similar thing happens
1751640	1755880	when we interact with physics. The world that you and me are in is not the physical world.
1755880	1761400	The world that you and me are in is a dream world. How close is it to the real world, though?
1763400	1767480	We know that it's not very close, but we know that the dynamics of the dream world
1767480	1770920	match the dynamics of the physical world to a certain degree of resolution.
1770920	1773160	Right. The causal structure of the dream world is different.
1775160	1779080	So you see, for instance, waves crashing on your feet, right? But there are no waves in the
1779160	1783880	ocean. There's only water molecules that have tangents between the molecules that are
1785880	1789960	the result of electrons in the molecules interacting with each other.
1789960	1797800	Aren't they very consistent where you're seeing a very crude approximation? Isn't our dream world
1797800	1804200	very consistent to the point of being mapped directly one to one to the actual physical world,
1804200	1809080	as opposed to us being completely tricked? This is like where you have like Donald Hock.
1809080	1813560	It's not a trick. That's my point. It's not an illusion. It's a form of data compression.
1813560	1817640	It's an attempt to deal with the dynamics of too many parts to count at the level at which
1817640	1822600	we are entangled with the best model that you can find. Yeah. So we can act in that dream world
1822600	1828200	and our actions have impact in the real world, in the physical world, to which we don't have
1828200	1833080	access. Yes. But it's basically like accepting the fact that the software that we live in,
1833080	1837960	the dream that we live in is generated by something outside of this world that you and me are in.
1837960	1841400	So is the software deterministic and do we not have any control?
1843880	1852520	So free will is having a conscious being. Free will is the monkey being able to steer the elephant.
1855320	1861560	No. It's slightly different. Basically, in the same way as you are modeling the water molecules
1861560	1866360	in the ocean that engulf your feet when you are walking on the beach as waves and there are no
1866360	1871960	waves, but only the atoms on more complicated stuff underneath the atoms and so on. And you
1871960	1877880	know that, right? You would accept, yes, there is a certain abstraction that happens here. It's a
1877880	1883160	simplification of what happens and the simplification that is designed in such a way that your brain
1883160	1888280	can deal with it temporarily and spatially in terms of resources and tuned for the predictive
1888280	1893240	value. So you can predict with some accuracy whether your feet are going to get wet or not.
1893240	1898120	But it's a really good, it's a really good interface and approximation. Yes. It's like
1898120	1903720	e equals mc squared is a good, equations are good approximation for what they're much better
1903720	1910920	approximation. So to me waves is a really nice approximation of what's all the complexity
1910920	1914360	that's happening underneath. Basically, it's a machine learning model that is constantly tuned
1914360	1918920	to minimize surprises. So it basically tries to predict as well as it can what you're going to
1918920	1924680	perceive next. Are we talking about which is the machine learning our perception system or the dream
1924680	1931080	world? The dream world is the result of the machine learning process of the perception system.
1931080	1938040	That's doing the compression. Yes. And the model of you as an agent is not a different type of model
1938040	1945160	or it's a different type, but not different as in its model like nature from the model of the ocean,
1945160	1950680	right? Some things are oceans, some things are agents. And one of these agents is using your
1950680	1955320	own control model, the output of your model, the things that you perceive yourself as doing.
1956200	1961720	And that is you. What about the fact that is like when you're standing
1961720	1972040	with the water on your feet and you're looking out into the vast open water of the ocean and
1972040	1977960	then there's a beautiful sunset and the fact that it's beautiful and then maybe you have friends
1977960	1983000	or loved one with you and you feel love. What is that as the dream world? What is that? Yes,
1983000	1990360	it's all happening inside of the dream. Okay. But see the word dream makes it seem like it's not real.
1991240	1997560	Yeah, of course, it's not real. The physical universe is real, but the physical universe
1997560	2002120	is incomprehensible and it doesn't have any feeling of realness. The feeling of realness that
2002120	2006520	you experience gets attached to certain representations where your brain assesses,
2006520	2011000	this is the best model of reality that I have. So the only thing that's real to you is the
2011000	2017560	thing that's happening at the very base of reality. Yeah, for something to be real,
2017640	2024440	it needs to be implemented. So the model that you have of reality is real in as far as it is a
2024440	2029320	model, right? It's an appropriate description of the world to say that there are models that
2029320	2036200	are being experienced. But the world that you experience is not necessarily implemented.
2036760	2043560	There is a difference between a reality, a simulation and a simulacrum. The reality that
2043560	2048520	we're talking about is something that fully emerges over a causally close lowest layer.
2048520	2052680	And the idea of physicalism is that we are in that layer, that basically our world emerges
2052680	2057160	over that. Every alternative to physicalism is a simulation theory, which basically says
2057160	2061080	that we are in some kind of simulation universe and the real world needs to be an apparent
2061080	2066440	universe of that, where the actual causal structure is, right? And when you look at the ocean and
2066440	2070600	your own mind, you are looking at a simulation that explains what you're going to see next.
2071320	2075080	So we are living in a simulation. Yes, but a simulation generated by our own brains.
2076120	2080920	And this simulation is different from the physical reality because the causal structure
2080920	2084840	that is being produced, what you are seeing is different from the causal structure of physics.
2084840	2085640	But consistent.
2086840	2091480	Hopefully, if not, then you are going to end up in some kind of institution where people will
2091480	2096600	take care of you because your behavior will be inconsistent, right? Your behavior needs to work
2096600	2101400	in such a way that it's interacting with an accurately predictive model of reality. And
2101400	2106120	if your brain is unable to make your model of reality predictive, you will need help.
2106120	2112680	So what do you think about Donald Hoffman's argument that it doesn't have to be consistent,
2112680	2119400	the dream world, to what he calls like the interface to the actual physical reality,
2119400	2123960	where there could be evolution. I think he makes an evolutionary argument, which is like,
2123960	2130840	it could be an evolutionary advantage to have the dream world drift away from physical reality.
2130840	2134280	I think that only works if you have tenure. As long as you're still interacting with the
2134280	2137320	ground tools, your model needs to be somewhat predictive.
2139000	2143720	Well, in some sense, humans have achieved a kind of tenure in the animal kingdom.
2145080	2149080	Yeah. At some point, we became too big to fail. So we became postmodernist.
2149080	2152680	It all makes sense now.
2152680	2154040	It's a version of reality that we like.
2155000	2156760	Oh, man. Okay.
2157320	2162440	Yeah, but basically, you can do magic. You can change your assessment of reality,
2162440	2166760	but eventually, reality is going to come by to you in the ass if it's not predictive.
2166760	2175480	Do you have a sense of what is that base layer of physical reality? You have these attempts
2175560	2181880	at the theories of everything, the very, very small of like string theory, or what
2182680	2188440	Stephen Wolfram talks about with the hypergrass. These are these tiny, tiny, tiny, tiny objects,
2188440	2194840	and then there is more like quantum mechanics that's talking about objects that are much larger,
2194840	2200600	but still very, very, very tiny. Do you have a sense of where the tiniest thing is that is like
2201560	2205240	at the lowest level, the turtle at the very bottom? Do you have a sense?
2205880	2210680	I don't think that you can talk about where it is because space is emergent over the activity
2210680	2219560	of these things. So coordinates only exist in relation to other things. And so you could,
2219560	2224200	in some sense, abstract it into locations that can hold information and trajectories
2224200	2227880	that the information can take between the different locations. And this is how we
2227880	2234920	construct our notion of space. And physicists usually have a notion of space that is continuous.
2235640	2241880	And this is a point where I tend to agree with people like Stephen Wolfram, who are very skeptical
2241880	2247400	of the geometric notions. I think that geometry is the dynamics of too many parts to count. And
2249080	2254120	when there are no infinities, if there were two infinities, you would be running into contradictions,
2254120	2259080	which is in some sense what Gödel and Turing discovered in response to Hilbert's call.
2259720	2265240	There are no infinities. There are no infinities. There is unboundedness. But if you have a language
2265240	2268920	that talks about infinity, at some point, the language is going to contradict itself,
2269480	2273960	which means it's no longer valid. In order to deal with infinities and mathematics,
2273960	2279480	you have to postulate the existence initially. You cannot construct the infinities. And that's
2279480	2284680	an issue. You cannot build up an infinity from zero. But in practice, you never do this. When
2284680	2290360	you perform calculations, you only look at the dynamics of too many parts to count. And usually,
2290360	2296840	these numbers are not that large. They're not Googles or something. The infinities that we are
2296840	2304280	dealing with in our universe are, mathematically speaking, relatively small integers. And still,
2304760	2312120	what we're looking at is dynamics, where a trillion things behave similar to 100 trillion
2312120	2319960	things or something that is very, very large because they're converging. And these converging
2319960	2324120	dynamics, these operators, this is what we deal with when we are doing the geometry.
2324840	2330440	The geometry is stuff where we can pretend that it's continuous, because if we subdivide the
2330440	2337000	space sufficiently, fine-grained, these things approach a certain dynamic. And this approached
2337000	2342760	dynamic, that is what we mean by it. But I don't think that infinity would work, so to speak,
2342760	2347720	that you would know the last digit of pi and that you have a physical process that rests on knowing
2347720	2354040	the last digit of pi. Yeah, that could be just a peculiar quirk of human cognition that we like
2354040	2359720	discrete. Discrete makes sense to us. Infinity doesn't. So in terms of our intuitions.
2359720	2364840	No, the issue is that everything that we think about needs to be expressed in some kind of mental
2364840	2370040	language, not necessarily natural language, but some kind of mathematical language that your
2370040	2375880	neurons can speak, that refers to something in the world. And what we have discovered is that
2376760	2381160	we cannot construct a notion of infinity without running into contradictions, which means that
2381160	2386920	such a language is no longer valid. And I suspect this is what made Pythagoras so unhappy when
2386920	2391000	somebody came up with the notion of irrational numbers before it was time, right? There's this
2391000	2395160	myth that he had this person killed when he blabbed out the secret that not everything can be
2395160	2399960	expressed as a ratio between two numbers, but there are numbers between the ratios. The world
2399960	2405960	was not ready for this. And I think he was right that has confused mathematicians very seriously
2405960	2411480	because these numbers are not values, they are functions. And so you can calculate these functions
2411480	2416120	to a certain degree of approximation, but you cannot pretend that pi has actually a value.
2417000	2421400	Pi is a function that would approach this value to some degree,
2421400	2423560	but nothing in the world rests on knowing pi.
2426200	2429880	How important is this distinction between discrete and continuous
2430920	2433560	for you to get to the bottom? Because there's a, I mean,
2435000	2440360	in discussion of your favorite flavor of the theory of everything, there's a few on the table.
2441000	2448120	So there's string theory, there's a particular, there's a loop quantum gravity,
2448120	2456280	which focuses on one particular unification. There's just a bunch of favorite flavors of
2456280	2463880	different people trying to propose a theory of everything. Eric Weinstein and a bunch of people
2463880	2468840	throughout history. And then of course, Stephen Wolfram, who I think is one of the only people
2468840	2473800	doing a discrete. No, no, there's a bunch of physicists who do this right now. And
2476120	2483800	Topoli and Tomasello and the digital physics is something that is, I think, growing in popularity.
2484360	2494520	But the main reason why this is interesting is because it is important sometimes to settle
2494520	2498840	disagreements. I don't think that you need infinities at all and you never needed them.
2499720	2503320	You can always deal with very large numbers and you can deal with limits, right? You are
2503320	2507640	fine with doing that. You don't need any kind of affinity. You can build your computer algebra
2507640	2511000	systems just as well without believing in infinity in the first place.
2511000	2512120	So you're okay with limits?
2512680	2518920	Yeah. So basically a limit means that something is behaving pretty much the same if you make the
2518920	2523720	number larger, right? Because it's converging to a certain value and at some point the difference
2523720	2528760	becomes negligible and you can no longer measure it. And in this sense, you have things that
2529960	2534600	if you have an n-gon which has enough corners, then it's going to behave like a circle at some
2534600	2539800	point, right? And it's only going to be in some kind of esoteric thing that cannot exist in a
2539800	2545000	physical universe that you would be talking about this perfect circle. And now it turns out that
2545000	2549160	it also wouldn't work in mathematics because you cannot construct mathematics that has infinite
2549160	2555160	resolution without running into contradictions. So that is itself not that important because
2555160	2559720	we never did that, right? It's just a thing that some people thought we could. And this leads to
2559720	2565400	confusion. So for instance, Roger Penrose uses this as an argument to say that there are certain
2565400	2572280	things that mathematicians can do dealing with infinities and by extension our mind can do
2573160	2575080	that computers cannot do.
2575080	2582200	Yeah, he talks about that the human mind can do certain mathematical things that the computer
2582200	2588840	has defined by the universal Turing machine cannot. So that it has to do with infinity.
2588840	2594280	Yes, it's one of the things. So he is basically pointing at the fact that there are things that
2594280	2604120	are possible in the mathematical mind and in pure mathematics that are not possible in machines that
2604120	2609800	can be constructed in the physical universe. And because he is an honest guy, he thinks this
2609800	2614760	means that present physics cannot explain operations that happen in our mind.
2614760	2620680	Do you think he's right? So let's leave his discussion of consciousness aside for the moment.
2620680	2626280	Do you think he's right about just what he's basically referring to as intelligence? So
2627960	2633880	is the human mind fundamentally more capable as a thinking machine than a universal Turing machine?
2633960	2641240	No. So he's suggesting that, right? So our mind is actually less than a Turing machine. There can
2641240	2646840	be no Turing machine because it's defined as having an infinite tape and we always only have a finite
2646840	2651960	tape. But he's saying it's better. Or minds can only perform finitely many operations. He can do
2651960	2655960	the kind of computation that the Turing machine cannot. And that's because he thinks that our
2655960	2662440	minds can do operations that have infinite resolution in some sense. And I don't think
2662520	2666920	that's the case. Our minds are just able to discover these limit operators over too many
2666920	2677400	parts to count. What about his idea that consciousness is more than a computation?
2677400	2683720	So it's more than something that a Turing machine can do. So again, saying that there's
2683720	2687560	something special about our mind, they cannot be replicated in the machine.
2688520	2695240	The issue is that I don't even know how to construct a language to express this statement
2695240	2708120	correctly. Well, the basic statement is there's a human experience that includes intelligence,
2708120	2713960	that includes self-awareness, that includes the hard problem of consciousness. And the question is,
2713960	2720840	can that be fully simulated in the computer, in the mathematical model of the computer,
2720840	2730600	as we understand it today? Roger Prenos says no. So the universal Turing machine cannot
2730600	2736440	simulate the universe. So the interesting question is, and you have to ask him this, is why not?
2736440	2742200	What is this specific thing that cannot be modeled? And when I looked at his writings,
2742200	2748040	and I haven't read all of it, but when I read, for instance, the section that he writes in the
2748040	2754760	introduction and wrote to infinity, the thing that he specifically refers to is the way in which
2754760	2761800	human minds deal with infinities. And that itself can, I think, easily be deconstructed.
2762920	2767480	A lot of people feel that our experience cannot be explained in a mechanical way.
2768440	2774600	And therefore, it needs to be different. And I concur, our experience is not mechanical. Our
2774600	2779480	experience is simulated. It exists only in assimilation. The only assimilation can be
2779480	2783960	conscious. Physical systems cannot be conscious because they're only mechanical. Cells cannot
2783960	2788040	be conscious. Neurons cannot be conscious. Brains cannot be conscious. People cannot be
2788040	2793880	conscious as far as you, if you understand them as physical systems. What can be conscious is
2793960	2798600	the story of the system in the world where you write all these things into the story.
2799240	2803480	You have experiences for the same reason that a character and novel have experiences because
2803480	2809240	it's written into the story. And now the system is acting on that story. And it's not a story that
2809240	2813560	is written in a natural language. It's written in a perceptual language, in this multimedia
2813560	2819240	language of the game engine. And in there, you write in what kind of experience you have,
2819240	2822840	and what this means for the behavior of the system, for your behavior tendencies,
2822840	2826920	for your focus, for your attention, for your experience of valence and so on. And
2826920	2831480	this is being used to inform the behavior of the system in the next step. And then the
2832840	2837640	story updates with the reactions of the system and the changes in the world and so on.
2837640	2841240	And you live inside of that model. You don't live inside of the physical reality.
2843400	2850680	And I mean, just to linger on it, like, you see, okay, it's in the perceptual language,
2850680	2856840	the multimodal perceptual language, that's the experience. That's what consciousness is within
2856840	2865160	that, within that model, within that story. But do you have agency? When you play a video game,
2865160	2871000	you can turn left and you can turn right in that story. So in that dream world,
2871000	2876200	how much control do you have? Is there such a thing as you in that story?
2877160	2882680	Like, is it right to say the main character, you know, everybody's NPCs, and then there's the main
2882680	2888600	character, and you're controlling the main character? Or is that an illusion? Is there a main
2888600	2893320	character that you're controlling? I'm getting to the point of like, the free will point.
2893320	2899160	Imagine that you are building a robot that plays soccer. And you've been to MIT computer science,
2899160	2905080	you basically know how to do that. Right. And so you would say the robot is an agent that solves
2905640	2910680	the control problem. How to get the ball into the goal. And it needs to perceive the world and
2910680	2915000	the world is disturbing him in trying to do this. Right. So he has to control many variables to make
2915000	2920040	that happen and to project itself and the ball into the future and understand its position on
2920040	2926040	the field relative to the ball and so on in the position of its limbs or in the space around it
2926040	2930280	and so on. So it needs to have an adequate model that abstracting reality in a useful way.
2931240	2937320	And you could say that this robot does have agency over what it's doing in some sense.
2938280	2943320	And the model is going to be a control model. And inside of that control model, you can
2944280	2949320	possibly get to a point where this thing is sufficiently abstract to discover its own agency.
2949320	2953000	Our current robots don't do that. They don't have a unified model of the universe.
2953000	2957000	But there's not a reason why we shouldn't be getting there at some point in the
2957000	2963400	not too distant future. And once that happens, you will notice that the robot tells a story about
2963400	2970600	the robot playing soccer. So the robot will experience itself playing soccer in a simulation
2970600	2977320	of the world that it uses to construct a model of the locations of its legs and limbs in space on
2977320	2981400	the field with relationship to the ball. And it's not going to be at the level of the molecules.
2982120	2987320	It will be an abstraction that is exactly at the level that is most suitable for past planning
2987320	2992280	of the movements of the robot. It's going to be a high level abstraction, but a very useful one
2992280	2997400	that is as predictive as we can make it. And in that side of that story, there is a model of
2997400	3004680	the agency of that system. So this model can accurately predict that the contents of the model
3004680	3008760	are going to be driving the behavior of the robot in the immediate future.
3008760	3016120	But there's the hard problem of consciousness, which I would also, there's a subjective experience
3016120	3022520	of free will as well, that I'm not sure where the robot gets that where that little leap is.
3022520	3027080	Because for me right now, everything I imagine with that robot, as it gets more and more and
3027080	3035000	more sophisticated, the agency comes from the programmer of the robot still of what was programmed
3035000	3040200	in. You would probably do an end to end learning system. You maybe need to give it a few priors,
3040200	3044200	so you nudge the architecture in the right direction that it converges more quickly.
3044200	3049320	But ultimately, discovering the suitable hyperparameters of the architecture is also only a
3049320	3055240	search process, right? And as the search process was evolution, it has informed our brain architecture
3055240	3060200	so we can converge in a single lifetime on useful interaction with the world and the formation of
3060280	3066280	the problem is if we define hyperparameters broadly, so it's not just the parameters that
3066280	3070680	control this end to end learning system, but the entirety of the design of the robot.
3073320	3077240	You have to remove the human completely from the picture and then in order to build the robot,
3077240	3082520	you have to create an entire universe because you have to go, you can't just shortcut evolution,
3082520	3086760	you have to go from the very beginning in order for it to have, because I feel like there's always
3086760	3094280	a human pulling the strings and that makes it seem like the robot is cheating, it's getting a
3094280	3098520	shortcut to consciousness. And you are looking at the current Boston Dynamics robots, it doesn't
3098520	3102760	look as if there is somebody pulling the strings, it doesn't look like cheating anymore. Okay, so
3102760	3107000	let's go there because I got to talk to you about this. So obviously with the case of Boston Dynamics,
3107640	3114440	as you may or may not know, it's always either hard coded or remote controlled. There's no
3114440	3119000	intelligence. So I don't know how the current generation of Boston Dynamics robots works,
3119000	3124440	but what I've been told about the previous ones was that it's basically all cybernetic control,
3125080	3131000	which means you still have feedback mechanisms and so on, but it's not deep learning for the most
3131000	3136840	part as it's currently done. It's for the most part just identifying a control hierarchy
3136840	3141480	that is congruent to the limbs that exist and the parameters that need to be optimized for
3141480	3145560	the movement of these limbs. And then there is a convergence progress. So it's basically just
3145560	3149560	regression that you would need to control this. But again, I don't know whether that's true. That's
3149560	3154360	just what I've been told about how that works. We have to separate several levels of discussions
3154360	3160840	here. So the only thing they do is pretty sophisticated control with no machine learning
3160840	3168040	in order to maintain balance or to write itself. It's a control problem in terms of
3168040	3173560	using the actuators to when it's pushed or when it steps on a thing that's uneven,
3173560	3178040	how to always maintain balance. And there's a tricky heuristics around that, but
3179080	3184600	that's the only goal. Everything you see Boston Dynamics doing in terms of that to us humans
3184600	3191800	is compelling, which is any kind of higher order movement like turning, wiggling its butt,
3192520	3201640	like jumping back and it's two feet dancing. Dancing is even worse because dancing is hard
3201640	3209080	coded in. It's choreographed by humans. There's choreography software. So there is no, of all
3209080	3215800	that high level movement, there's no anything that you can call, certainly can't call AI.
3215880	3224360	There's no even like basic heuristics. It's all hard coded in. And yet we humans immediately
3224360	3232520	project agency onto them, which is fascinating. So the gap here is it doesn't necessarily have
3232520	3237320	agency. What it has is cybernetic control. And the cybernetic control means you have a hierarchy
3237320	3242600	of feedback loops that keep the behavior in certain boundaries so the robot doesn't fall over and it's
3242600	3247800	able to perform the movements. And the choreography cannot really happen with motion capture,
3247800	3251400	because the robot would fall over because the physics of the robot, the weight distribution
3251400	3257480	and so on is different from the weight distribution in the human body. So if you were using the
3257480	3261960	directly motion captured movements of a human body to project it into this robot, it wouldn't
3261960	3265960	work. You can do this with a computer animation that will look a little bit off, but who cares.
3265960	3272200	But if you want to correct for the physics, you need to basically tell the robot where it should
3272280	3278200	move, its limbs. And then the control algorithm is going to approximate a solution that makes
3278200	3283960	it possible within the physics of the robot. And you have to find the basic solution for
3283960	3289000	making that happen. And there's probably going to be some regression necessary to get the control
3289000	3294280	architecture to make these movements. But those two layers are separate. So the thing,
3294280	3299640	the higher level instruction of how you should move and where you should move is the higher
3299720	3304200	level. I expect that the control level of these robots at some level is dumb. This is just the
3305000	3309720	physical control movement, the motor architecture. But it's a relatively smart motor
3309720	3313640	architecture. It's just that there is no high level deliberation about what decisions to make
3313640	3319400	necessarily. But it doesn't feel like free will or consciousness. No, that was not where I was
3319400	3326200	trying to get to. I think that in our own body, we have that too. So we have a certain thing that
3326280	3330520	is basically just a cybernetic control architecture that is moving our limbs.
3331240	3336280	And deep learning can help in discovering such an architecture if you don't have it in the first
3336280	3341400	place. If you already know your hardware, you can maybe handcraft it. But if you don't know your
3341400	3346920	hardware, you can search for such an architecture. And this work already existed in the 80s and 90s,
3346920	3351160	people were starting to search for control architectures by motor babbling and so on and
3351160	3357480	just use reinforcement learning architectures to discover such a thing. And now imagine that you
3357480	3363640	have the cybernetic control architecture already inside of you. And you extend this a little bit.
3363640	3370760	So you are seeking output, for instance, or rest or and so on. And you get to have a baby at some
3370760	3377160	point. And now you add more and more control layers to this. And the system is reverse
3377160	3383000	engineering its own control architecture and builds a high level model to synchronize the
3383000	3388120	pursuit of very different conflicting goals. And this is how I think you get to purposes.
3388120	3392760	Purposes are models of your goals. The goals may be intrinsic as the result of the different
3392760	3398120	set point violations that you have, hunger and thirst for very different things and rest and
3398120	3403080	pain avoidance and so on. And you put all these things together and eventually you need to come
3403160	3409240	up with a strategy to synchronize them all. And you don't need just to do this alone by yourself
3409240	3414040	because we are state building organisms. We cannot function as an isolation the way that
3414040	3419800	Homo sapiens is set up. So our own behavior only makes sense when you zoom out very far
3419800	3426440	into a society or even into ecosystemic intelligence on the planet and our place in it.
3426440	3430760	So the individual behavior only makes sense in these larger contexts. And we have a number of
3430760	3436040	priors built into us. So we are behaving as if we are acting on these high level goals pretty
3436040	3441480	much right from the start. And eventually in the course of our life, we can reverse engineer the
3441480	3446840	goals that we are acting on what actually are our higher level purposes. And the more we understand
3446840	3452360	that the more our behavior makes sense. But this is all at this point, complex stories within stories
3452360	3459160	that are driving our behavior. Yeah, I just don't know how big of a leap it is to start
3460120	3462600	creating a system that's able to tell stories within stories.
3464200	3467560	Like how big of a leap that is from where currently Boston Dynamics is
3468120	3477000	or any robot that's operating in the physical space. And that leap might be big if it requires
3477000	3480840	to solve the hard problem of consciousness, which is telling a hell of a good story.
3481400	3486200	I suspect that consciousness itself is relatively simple. What's hard is perception
3487160	3489320	and the interface between perception and reasoning.
3490920	3496440	There's, for instance, the idea of the consciousness prior that would be built into such a system by
3497080	3504920	Yoshua Bengio. And what he describes, and I think that's accurate, is that our own
3506360	3510200	model of the world can be described through something like an energy function. The energy
3510200	3515080	function is modeling the contradictions that exist within the model at any given point. And you try
3515080	3520360	to minimize these contradictions, the tangents in the model. And to do this, you need to sometimes
3520360	3525560	test things. You need to conditionally disambiguate figure and ground. You need to distinguish
3525560	3529400	whether this is true or that is true. And so on. Eventually, you get to an interpretation,
3529400	3534520	but you will need to manually depress a few points in your model to let it snap into a state that
3534520	3538680	makes sense. And this function that tries to get the biggest dip in the energy function in your
3538680	3543480	model, according to Yoshua Bengio, is related to consciousness. It's a low dimensional
3543480	3548040	discrete function that tries to maximize this dip in the energy function.
3549640	3555480	Yeah, I think I would need to get into details because I think the way he uses the word consciousness
3555480	3561560	is more akin to self-awareness, like modeling yourself within the world, as opposed to the
3561560	3567160	subjective experience, the hard problem. No, it's not even the self within the world. The self is
3567160	3571560	the agent and you don't need to be aware of yourself in order to be conscious. The self is
3571640	3577080	just a particular content that you can have, but you don't have to have. But you can be conscious
3577080	3582200	in, for instance, a dream at night or during a meditation state where you don't have a self,
3583080	3587800	where you're just aware of the fact that you are aware. And what we mean by consciousness
3587800	3594920	and colloquial sense is largely this reflexive self-awareness, that we become aware of the
3594920	3599160	fact that we are paying attention, that we are the thing that pays attention.
3599160	3606600	We are the thing that pays attention. I don't see the awareness that we're aware.
3608360	3613960	The hard problem doesn't feel like it's solved. It's called a hard problem for a reason,
3615000	3618360	because it seems like there needs to be a major leap.
3619160	3624360	Yeah, I think the major leap is to understand how it is possible that a machine can dream,
3625160	3630920	that a physical system is able to create a representation, that a physical system is acting
3630920	3636600	on and that is spun force and so on. But once you accept the fact that you are not in physics,
3636600	3641560	but that you exist inside of the story, I think the mystery disappears. Everything is possible
3641560	3645720	in a story. You exist inside the story. Your consciousness is being written into the story.
3645720	3649720	The fact that you experience things is written to the story. You ask yourself,
3649720	3653240	is this real what I'm seeing? And your brain writes into the story, yes, it's real.
3653800	3661640	So what about the perception of consciousness? So to me, you look conscious. So the illusion
3661640	3667640	of consciousness, the demonstration of consciousness, I ask for the legged robot,
3667640	3673560	how do we make this legged robot conscious? So there's two things that maybe you can tell me
3673560	3680440	if they're neighboring ideas. One is actually making conscious and the other is make it appear
3680440	3688600	conscious to others. Are those related? Let's ask from the other direction, what would it take to
3688600	3696120	make you not conscious? So when you are thinking about how you perceive the world, can you decide
3696120	3703800	to switch from looking at qualia to looking at representational states? And it turns out you
3703800	3709480	can. There is a particular way in which you can look at the world and recognize its machine
3709480	3714760	nature, including your own. And in that state, you don't have that conscious experience in this way
3714760	3723000	anymore. It becomes apparent as a representation. Everything becomes opaque. And I think this
3723000	3726440	thing that you recognize everything as a representation, this is typically what we
3726440	3733240	mean with enlightenment states. And you can have a motivational level, but you can also do this on
3733240	3738280	the experiential level and the perceptual level. See, but then I can come back to a conscious state.
3740360	3749240	Okay, I particularly, I'm referring to the social aspect that the demonstration of consciousness
3750120	3757240	is a really nice thing at a party when you're trying to meet a new person. It's a nice thing to
3757240	3763320	know that they're conscious and they can, I don't know how fundamental consciousness is in human
3763320	3770200	interaction, but it seems like to be at least an important part. And I asked that in the same kind
3770200	3778440	of way for robots, you know, in order to create a rich, compelling human robot interaction, it
3778440	3781720	feels like there needs to be elements of consciousness within that interaction.
3782680	3788680	My cat is obviously conscious. And so my cat can do this party trick. She also knows that I am
3788680	3793640	conscious. We are able to have feedback about the fact that we are both acting on models of our
3793640	3801400	own awareness. The question is, how hard is it for the robot artificially created robot to achieve
3801400	3809000	cat level in party tricks? Yes. So the issue for me is currently not so much on how to build a system
3809000	3814760	that creates a story about a robot that lives in the world, but to make an adequate representation
3814760	3822440	of the world. And the model that you and me have is a unified one. It's one where you basically
3822440	3827000	make sense of everything that you can perceive every feature in the world that enters your
3827000	3833080	perception can be relationally mapped to a unified model of everything. And we don't have an AI that
3833080	3838760	is able to construct such a unified model yet. So you need that unified model to do the party trick?
3838760	3844200	Yes, I think that it doesn't make sense if this thing is conscious, but not in the same universe
3844680	3849640	because you could not relate to each other. So what's the process, would you say, of engineering
3849640	3857800	consciousness in a machine? What are the ideas here? So you probably want to have some kind of
3857800	3863320	perceptual system. This perceptual system is a processing agent that is able to track sensory
3863320	3869400	data and predict the next frame and the sensory data from the previous frames of the sensory
3869400	3874600	data in the current state of the system. So the current state of the system is in perception
3874600	3879720	instrumental to predicting what happens next. And this means you build lots and lots of functions
3879720	3884840	that take all the blips that you feel on your skin and that you see on your retina or that you hear
3885560	3891080	and puts them into a set of relationships that allows you to predict what kind of sensory data,
3891080	3895960	what kind of sensor of blips, your vector of blips, you're going to perceive in the next frame.
3896040	3900920	This is tuned and it's constantly tuned until it gets as accurate as it can.
3901880	3908440	You build a very accurate prediction mechanism that is step one of the perceptions. At first,
3908440	3911640	you predict, then you perceive and see the error in your prediction.
3911640	3915880	And you have to do two things to make that happen. One is you have to build a network of
3915880	3922200	relationships that are constraints, that take all the variants in the world, put each of the
3922200	3927880	variances into a variable that is connected with relationships to other variables.
3927880	3931720	And these relationships are computable functions that constrain each other. So when you see a
3931720	3936120	nose that points in a certain direction in space, you have a constraint that says there
3936120	3940280	should be a phase nearby that has the same direction. And if that is not the case,
3940280	3943560	you have some kind of contradiction that you need to resolve because it's probably not a
3943560	3948600	nose what you're looking at. It just looks like one. So you have to reinterpret the data and
3948680	3951720	until you get to a point where your model converges.
3952440	3957400	And this process of making the sensory data fit into your model structure is what
3957400	3963960	Piaget calls the assimilation. And echomodation is the change of the models
3963960	3967000	where you change your model such a way that you can assimilate everything.
3968120	3972280	So you're talking about building a hell of an awesome perception system
3972280	3975960	that's able to do prediction and perception correct and improving.
3976520	3982040	Wait, there's more. Yes, there's more. So the first thing that we wanted to do is we want to
3982040	3986920	minimize the contradictions in the model. And of course, it's very easy to make a model in which
3986920	3991320	you minimize the contradictions just by allowing that it can be in many, many possible states.
3991320	3996360	Right? So if you increase degrees of freedom, you will have fewer contradictions. But you also
3996360	4000600	want to reduce the degrees of freedom because degrees of freedom mean uncertainty. You want
4000680	4006600	your model to reduce uncertainty as much as possible. But reducing uncertainty is expensive.
4006600	4012440	So you have to have a trade-off between minimizing contradictions and reducing uncertainty.
4012440	4017640	And you have only a finite amount of compute and experimental time and effort available to
4017640	4023160	reduce uncertainty in the world. So you need to assign value to what you observe. So you need
4023160	4028200	some kind of motivational system that is estimating what you should be looking at and what you
4028200	4031960	should be thinking about it, how you should be applying your resources to model what that is.
4032680	4036760	Right? So you need to have something like convergence links that tell you how to get
4036760	4040680	from the present state of the model to the next one. You need to have these compatibility links
4040680	4047320	that tell you which constraints exist and which constraint violations exist. And you need to have
4047320	4051720	some kind of motivational system that tells you what to pay attention to. So now we have a second
4051720	4056280	agent next to the perceptual agent. We have a motivational agent. This is a cybernetic system
4056360	4061160	that is modeling what the system needs, what's important for the system, and that interacts
4061160	4066120	with the perceptual system to maximize the expected reward. And you're saying the motivational system
4066120	4072840	is some kind of, like, what is it, a high-level narrative over some lower level? No, it's just
4072840	4077320	your brainstem stuff, the limbic system stuff that tells you, okay, now you should get something
4077320	4080920	to eat because I've just measured your geo blood sugar. So you mean the motivational system,
4080920	4085400	like the lower level stuff, like Hungary? Yes. But there's basically a physiological
4085400	4088840	needs and some cognitive needs and some social needs, and they all interact. And they're all
4088840	4092920	implemented at different parts in your nervous system as the motivational system. But they're
4092920	4098520	basically cybernetic feedback loops. It's not that complicated. It's just a lot of code. And
4099800	4104520	so you now have a motivational agent that makes your robot go for the ball or that makes your warm
4104520	4110040	go to eat food and so on. And you have the perceptual system that lets it predicts the
4110040	4114760	environment so it's able to solve that control problem to some degree. And now what we learned
4114760	4118520	is that it's very hard to build a machine learning system that looks at all the data
4118520	4123800	simultaneously to see what kind of relationships could exist between them. So you need to
4123800	4128280	selectively model the world. You need to figure out, where can I make the biggest difference
4128280	4133000	if I would put the following things together? Sometimes you find a gradient for that, right?
4133000	4137000	When you have a gradient, you don't need to remember where you came from. You just follow
4137000	4141320	the gradient until it doesn't get any better. But if you have a world where the problems are
4141320	4147240	discontinuous and the search spaces are discontinuous, you need to retain memory of what you explored.
4147240	4153000	You need to construct a plan of what to explore next. And this thing that means that you have
4153000	4156840	next to this perceptual construction system and the motivational cybernetics,
4157640	4163320	an agent that is paying attention to what it should select at any given moment to maximize
4163320	4169000	reward. And this scanning system, this attention agent is required for consciousness. And
4169000	4176440	consciousness is its control model. So it's the index memories that this thing retains when it
4176440	4183080	manipulates the perceptual representations to maximize the value and minimize the conflicts and
4183080	4187960	it to increase coherence. So the purpose of consciousness is to create coherence in your
4187960	4193320	perceptual representations, remove conflicts, predict the future, construct counterfactual
4193320	4198680	representations so you can coordinate your actions and so on. And in order to do this,
4198680	4203240	it needs to form memories. These memories are partial binding states of the working memory
4203240	4208920	contents that are being revisited later on to backtrack, to undo certain states to look for
4208920	4214200	alternatives. And these index memories that you can recall, that is what you perceive as your
4214200	4219320	stream of consciousness. And being able to recall these memories, this is what makes you conscious.
4219320	4222440	If you could not remember what you paid attention to, you wouldn't be conscious.
4226040	4229480	So consciousness is the index in the memory database. Okay.
4231320	4238040	But let me sneak up to the questions of consciousness a little further. So we usually
4239640	4244040	relate suffering to consciousness. So the capacity to suffer.
4245000	4249640	I think to me, that's a really strong sign of consciousness, is a thing that can suffer.
4251640	4258440	How is that useful? Suffering. And like in your model, what you just described,
4258440	4263480	which is indexing of memories, and what is the coherence with the perception,
4264760	4270040	with this predictive thing that's going on in the perception, how does suffering relate to any of
4271000	4274760	that, the higher level of suffering that humans do?
4276440	4283320	Basically, pain is a reinforcement signal. Pain is a signal that one part of your brain
4283320	4288200	sends to another part of your brain, or an abstract sense, part of your mind sends to
4288200	4292520	another part of the mind to regulate its behavior, to tell it the behavior that you're
4292520	4298920	currently exhibiting should be improved. And this is a signal that I tell you to move away
4299000	4304920	from what you're currently doing and push into a different direction. So pain gives you a part
4304920	4310760	of you an impulse to do something differently. But sometimes this doesn't work, because the
4310760	4315800	training part of your brain is talking to the wrong region. Or because it has the wrong model
4315800	4319400	of the relationships in the world, maybe you're mismodeling yourself or you're mismodeling the
4319400	4323800	relationship of yourself to the world or you're mismodeling the dynamics of the world. So you're
4323800	4328520	trying to improve something that cannot be improved by generating more pain. But the system
4328520	4333880	doesn't have any alternative. So it doesn't get better. What do you do if something doesn't get
4333880	4338600	better and you want it to get better? You increase the strengths of the signal. And when the signal
4338600	4343320	becomes chronic, when it becomes permanent, without a change inside, this is what we call suffering.
4344280	4349640	And the purpose of consciousness is to deal with contradictions, with things that cannot be resolved.
4350200	4355640	The purpose of consciousness, I think, is similar to a conductor in an orchestra. When everything
4355640	4360680	works well, the orchestra doesn't need much of a conductor as long as it's coherent. But when
4360680	4366120	there is a lack of coherence or something is consistently producing disharmony and mismatches,
4366120	4371080	then the conductor becomes alert and interacts with it. So suffering attracts the activity
4371080	4376600	of our consciousness. And the purpose of that is ideally that we bring new layers online,
4376600	4383400	new layers of modeling that are able to create a model of the dysregulation so we can deal with it.
4384360	4389240	And this means that we typically get higher level consciousness, so to speak. We get some
4389240	4393640	consciousness above our pay grade, maybe, if we have some suffering early in our life. Most of the
4393640	4399880	interesting people had trauma early on in their childhood. And trauma means that you are suffering
4399880	4405320	an injury for which the system is not prepared, which it cannot deal with, which it cannot insulate
4405320	4410760	itself from. So something breaks. And this means that the behavior of the system is permanently
4411480	4418600	disturbed in a way that some mismatch exists now in a regulation that just by following your
4418600	4422920	impulse, by following the pain in the direction where it hurts, your situation doesn't improve
4422920	4429480	but get worse. And so what needs to happen is that you grow up. And that part that is grown up
4429480	4433160	is able to deal with the part that is stuck in this earlier phase.
4433160	4437720	Yeah, so it leads to growth, adding extra layers to your cognition.
4438040	4443560	Let me ask you then, because I gotta stick on suffering, the ethics of the whole thing.
4445400	4452360	So not our consciousness, but the consciousness of others. You've tweeted, one of my biggest fears
4453320	4458680	is that insects could be conscious. The amount of suffering on earth would be unthinkable.
4459640	4470520	So when we think of other conscious beings, is suffering a property of consciousness that we're
4470520	4482680	most concerned about? So I'm still thinking about robots, how to make sense of other non-human things
4483320	4492760	that appear to have the depth of experience that humans have. And to me, that means consciousness
4492760	4500680	and the darkest side of that, which is suffering, the capacity to suffer. And so I started thinking,
4500680	4506920	how much responsibility do we have for those other conscious beings? That's where the
4507640	4514600	the definition of consciousness becomes most urgent. Like having to come up with the definition of
4514600	4521160	consciousness becomes most urgent, is who should we and should we not be torturing?
4524600	4527960	There's no general answer to this. Was Chinggis Khan doing anything wrong?
4529000	4535080	It depends on how you look at it. Well, he drew a line somewhere
4536040	4544680	where this is us and that's them. It's the circle of empathy. You don't have to use the word consciousness,
4544680	4551240	but these are the things that matter to me if they suffer or not, and these are the things that
4551240	4556680	don't matter to me. Yeah, but when one of his commanders failed him, he broke his spine and
4556680	4563080	let him die in a horrible way. And so in some sense, I think he was indifferent to suffering.
4563720	4569240	Or he was not indifferent in the sense that he didn't see it as useful if he inflicted suffering.
4570280	4575320	But he did not see it as something that had to be avoided. That was not the goal.
4575320	4581160	The question was, how can I use suffering and the infliction of suffering to reach my goals
4581160	4588200	from his perspective? I see. So like different societies throughout history put different value
4588200	4594520	on the different individuals, different psychies, but also even the objective of avoiding suffering.
4595480	4602600	Some societies probably, I mean, this is where religious belief really helps that after life,
4603720	4607720	that doesn't matter that you suffer or die. What matters is you suffer honorably,
4608520	4614280	right? So that you enter the afterlife. It seems to be superstitious to me. Basically,
4614280	4622120	beliefs that assert things for which no evidence exists are incompatible with sound epistemology.
4622120	4626040	And I don't think that religion has to be superstitious. Otherwise, it should be condemned
4626040	4630200	in all cases. You're somebody who's saying, we live in a dream world, we have zero evidence for
4630200	4636760	anything. It's not the case. They are limits to what languages can be constructed. Mathematics
4636760	4643160	breaks solid evidence for its own structure. And once we have some idea of what languages exist
4643160	4648200	and how a system can learn and what learning itself is in the first place, and so on, we can
4649080	4655080	begin to realize that our intuitions, that we are able to learn about the regularities of the world
4655080	4660600	and minimize surprise and understand the nature of our own agency to some degree of abstraction,
4660600	4667000	that's not an illusion. So useful approximation. Just because we live in a dream world doesn't
4667000	4674840	mean mathematics can't give us a consistent glimpse of physical, of objective reality.
4674840	4681640	We can basically distinguish useful encodings from useless encodings. And when we apply our
4682120	4686680	truths seeking to the world, we know we usually cannot find out whether a certain thing is true.
4687320	4691240	What we typically do is we take the state vector of the universe, separate it into
4691240	4695800	separate objects that interact with each other, so interfaces. And this distinction that we are
4695800	4701960	making is not completely arbitrary. It's done to optimize the compression that we can apply
4701960	4707160	to our models of the universe. So we can predict what's happening with our limited resources.
4707160	4712360	In this sense, it's not arbitrary. But the separation of the world into objects that are
4712360	4717400	somehow discrete and interacting with each other is not the true reality. The boundaries
4717400	4722040	between the objects are projected into the world, not arbitrarily projected, but still,
4722680	4725080	it's only an approximation of what's actually the case.
4726360	4730200	And we sometimes notice that we run into contradictions when we try to understand
4730200	4735880	high-level things like economic aspects of the world and so on, or political aspects or
4735880	4740200	psychological aspects where we make simplifications. And the objects that we are using to separate
4740200	4746120	the world are just one of many possible projections of what's going on. And so it's not in this
4746120	4750200	postmodernist sense completely arbitrary, and you're free to pick what you want or dismiss
4750200	4755320	what you don't like because it's all stories. No, that's not true. You have to show for every model
4755320	4759320	of how well it predicts the world. So the confidence that you should have in the
4759320	4763080	entities of your models should correspond to the evidence that you have.
4764360	4773320	Can I ask you in a small tangent to talk about your favorite set of ideas and people,
4773320	4784440	which is postmodernism? What is postmodernism? How would you define it? And why, to you,
4784440	4791560	is it not a useful framework of thought? Postmodernism is something that I'm really
4791560	4799000	not an expert on. And postmodernism is a set of philosophical ideas that is difficult to lump
4799000	4806280	together, that is characterized by some useful thinkers, some of them poststructuralists and so
4806280	4811720	on. And I'm mostly not interested in it because I think that it's not leading me anywhere that I
4811720	4817880	find particularly useful. It's mostly, I think, born out of the insight that the ontologies
4817880	4823000	that we impose on the world are not literally true, and that we can often get to a different
4823000	4827400	interpretation by the world by using a different ontology that is different separation of the
4827400	4834280	world into interacting objects. But the idea that this makes the world a set of stories that are
4834280	4840600	arbitrary, I think, is wrong. And the people that are engaging in this type of philosophy
4840600	4845480	are working in an area that I largely don't find productive. There is nothing useful coming out of
4845480	4851080	this. So this idea that truth is relative is not something that has in some sense informed physics
4851080	4857000	or theory of relativity. And there is no feedback between those. There is no meaningful influence
4857000	4862920	of this type of philosophy on the sciences or on engineering or on politics. But there is a very
4862920	4871240	strong information of this on ideology, because it basically has become an ideology that is
4871240	4878120	justifying itself by the notion that truth is a relative concept. And it's not being used in such
4878120	4884600	a way that the philosophers or sociologists that take up these ideas say, oh, I should doubt my
4884600	4888760	own ideas, because maybe my separation of the world into objects is not completely valid. And I
4888760	4895320	should maybe use a different one and be open to a pluralism of ideas. But it mostly exists to dismiss
4895320	4902040	the ideas of other people. It becomes a political weapon of sorts to achieve power.
4902040	4910520	Basically, there's nothing wrong, I think, with developing a philosophy around this. But to develop
4910520	4916520	norms around the idea that truth is something that is completely negotiable is incompatible
4916520	4923720	with the scientific project. And I think if the academia has no defense against the ideological
4923720	4931400	parts of the postmodernist movement, it's doomed. Right. You have to acknowledge the ideological
4931400	4937160	part of any movement, actually, including postmodernism. Well, the question is what an ideology
4937160	4943960	is. And to me, an ideology is basically a viral memeplex that is changing your mind in such a way
4943960	4948760	that reality gets warped. It gets warped in such a way that you're being cut off from the rest of
4948760	4953960	human thought space. And you cannot consider things outside of the range of ideas of your own
4953960	4958520	ideology as possibly true. Right. So, I mean, there's certain properties to an ideology that
4958520	4966440	make it harmful. One of them is that dogmatism of just certainty, dogged certainty in that you're
4966760	4970600	you have the truth and nobody else does. Yeah. But what is creating this certainty? It's very
4970600	4975800	interesting to look at the type of model that is being produced. Is it basically just a strong
4975800	4980760	prior? And you tell people, oh, this idea that you consider to be very true, the evidence for this
4980760	4985320	is actually just much weaker than you thought and look here at some studies. No, this is not how it
4985320	4992760	works. It's usually normative, which means some thoughts are unthinkable because they would change
4992760	4999320	your identity into something that is no longer acceptable. And this cuts you off from considering
4999320	5005000	an alternative. And many de facto religions use this trick to lock people into a certain mode of
5005000	5009720	thought. This removes agency over your own thoughts. And it's very ugly to me. It's basically
5009720	5016200	not just a process of domestication, but it's actually an intellectual castration that happens.
5016200	5020600	It's an inability to think creatively and to bring forth new thoughts.
5023240	5029960	Can I ask you about substances, chemical substances that affect the video game,
5031720	5038760	the dream world? So psychedelics that increasing have been getting a lot of research done on them.
5038760	5045160	So in general psychedelics, psilocybin, MDMA, but also a really interesting one, the big one,
5045160	5051000	which is DMT. What and where are the places that these substances take
5052200	5058120	the mind that is operating in the dream world? Do you have an interesting sense how this
5058680	5065000	throws a wrinkle into the prediction model? Is it just some weird little quirk? Or is there
5066280	5068680	some fundamental expansion of the mind going on?
5069240	5077240	I suspect that a way to look at psychedelics is that they induce particular types of lucid dreaming
5077240	5084520	states. So it's a state in which certain connections are being severed in your mind and no longer
5084520	5089880	active. Your mind basically gets free to move in a certain direction because some inhibition,
5089880	5095240	some particular inhibition doesn't work anymore. And as a result, you might stop having yourself
5095240	5103800	or you might stop perceiving the world as three-dimensional. And you can explore that state.
5104440	5108840	And I suppose that for every state that can be induced with psychedelics, there are people that
5108840	5114200	are naturally in that state. So sometimes psychedelics that shift you through a range of
5114200	5119000	possible mental states. And they can also shift you out of the range of permissible mental states
5119000	5125000	that is where you can make predictive models of reality. And what I observe in people that
5125000	5131080	use psychedelics a lot is that they tend to be overfitting. Overfitting means that you are
5133320	5139560	using more bits for modeling the dynamics of a function than you should. And so you can fit
5139560	5144360	your curve to extremely detailed things in the past, but this model is no longer predictive
5144360	5151000	for the future. What is it about psychedelics that forces that? I thought it would be the opposite.
5151320	5162200	I thought it's a good mechanism for generalization, for regularization. So it feels like psychedelics
5162200	5169480	expansion of the mind, like forcing your model to be non-predictive is a good thing.
5171160	5177640	Meaning like it's almost like, okay, what I would say is psychedelics I akin to is traveling to a
5177640	5182760	totally different environment. Like going if you've never been to like India or something like that
5182760	5187720	from the United States. Very different set of people, different culture, different food, different
5188280	5194440	roads and values and all those kinds of things. So psychedelics can for instance teleport people
5194440	5201240	into a universe that is hyperbolic, which means that if you imagine a room that you are in,
5201240	5205240	you can turn around 360 degrees and you didn't go full circle. You need to go
5205720	5210760	20 degrees to go full circle. Exactly. So the things that people learn in that state
5210760	5215640	cannot be easily transferred in this universe that we are in. It could be that if they're able
5215640	5221000	to abstract and understand what happened to them, that they understand that some part of their spatial
5221000	5226200	cognition has been desynchronized and has found a different synchronization. And this different
5226200	5230280	synchronization happens to be a hyperbolic one, right? So you learn something interesting about
5230280	5234520	your brain. It's difficult to understand what exactly happened, but we get a pretty good idea
5234520	5239400	once we understand how the brain is representing geometry. Yeah, but doesn't give you a fresh
5239400	5249880	perspective on the physical reality? Who's making that sound? Is it inside my head or is it external?
5250840	5257560	Well, there is no sound outside of your mind, but it's making sense. Phenomenal physics.
5258440	5266200	Yeah, in the physical reality, there's sound waves traveling through air. Okay.
5266920	5270360	That's our model of what happened. That's our model of what happened. Right.
5275080	5278520	Don't second-hand, let's give you a fresh perspective on this physical reality.
5279080	5282600	Like, not this physical reality, but this more...
5285960	5289800	What do you call the dream world that's mapped directly to it?
5289800	5293240	The purpose of dreaming at night, I think, is data augmentation.
5294200	5297400	Exactly. So that's very different. That's very similar to second-hand.
5297400	5304040	Exchange parameters about the things that you have learned. And for instance, when you are young,
5304040	5308600	you have seen things from certain perspectives, but not from others. So your brain is generating
5308600	5313720	new perspectives of objects that you already know, which means they can learn to recognize them
5313720	5318280	later from different perspectives. And I suspect that's the reason why many of us remember to have
5318280	5321880	flying dreams as children, because it's just different perspectives of the world that you
5321880	5328440	already know, and that it starts to generate these different perspective changes, and then it
5328440	5332920	fluidly turns this into a flying dream to make sense of what's happening. So you fill in the
5332920	5338200	gaps, and suddenly you see yourself flying. And similar things can happen with semantic
5338200	5341400	relationships. So it's not just spatial relationships, but it can also be the
5342120	5347320	relationships between ideas that are being changed. And it seems that the mechanisms that make that
5347320	5356360	happen during dreaming are interacting with these same receptors that are being simulated by psychedelics.
5357160	5363160	So I suspect that there is a thing that I haven't read really about, the way in which dreams are
5363160	5368680	induced in the brain. It's not just that the activity of the brain gets tuned down because
5368680	5374200	you are somehow, your eyes are closed and you no longer get enough data from your eyes. But there
5374200	5380040	is a particular type of neurotransmitter that is saturating your brain during these phases,
5380040	5385880	during the RM phases, and you produce controlled hallucinations. And psychedelics
5385880	5392760	are linking into these mechanisms, I suspect. So isn't that another trickier form of data
5392760	5399720	augmentation? Yes. But it's also data augmentation that can happen outside of the specification
5399720	5403320	that your brain is tuned to. So basically, people are overclocking their brains,
5403320	5408440	and that produces states that are subjectively extremely interesting.
5409160	5412840	Yeah, I just... But from the outset, very suspicious.
5412840	5418200	So I think I'm over-applying the metaphor of a neural network in my own mind, which
5419560	5426040	I just think that doesn't lead to overfitting, right? But you were just sort of anecdotally
5426040	5430360	saying my experiences with people that have no psychedelics are that kind of quality.
5430360	5434360	I think it typically happens. So if you look at people like Timothy Leary,
5434360	5440120	and he has written beautiful manifestos about the effect of LSD on people,
5440120	5443880	he genuinely believed, he writes on these manifestos, that in the future,
5443880	5448120	science and art will only be done on psychedelics because it's so much more efficient and so much
5448120	5454440	better. And he gave LSD to children in this community of a few thousand people that he
5454440	5461000	had near San Francisco. And basically, he was losing touch with reality. He did not
5461000	5466520	understand the effects that the things that he was doing would have on the reception of psychedelics
5466520	5471000	by society because he was unable to think critically about what happened. What happened
5471000	5476520	was that he got in a euphoric state. That euphoric state happened because he was overfitting.
5476520	5482520	He was taking this sense of euphoria and translating it into a model of actual success
5482520	5488760	in the world. He was feeling better. Limitations had disappeared that he experienced to be
5488760	5492360	existing, but he didn't get superpowers. I understand what you mean by overfitting now.
5493800	5497400	There's a lot of interpretation to the term overfitting in this case,
5497400	5504280	but I got you. So he was getting positive rewards from a lot of actions that he shouldn't have.
5504280	5509480	But not just this. So if you take, for instance, John Lilly, who was studying dolphin languages
5509480	5514680	and aliens and so on, a lot of people that used psychedelics became very loopy.
5514680	5520840	And the typical thing that you notice when people are on psychedelics is that they are in a state
5520840	5525800	where they feel that everything can be explained now. Everything is clear. Everything is obvious.
5526600	5532520	And sometimes they have indeed discovered a useful connection, but not always. Very often
5532520	5539400	these connections are over interpretations. I wonder, there's a question of correlation
5539400	5545480	versus causation. And also, I wonder if it's the psychedelics or if it's more the social,
5545480	5552280	like being the outsider and having a strong community of outside and having a leadership
5552280	5558120	position in an outsider cult-like community, that could have a much stronger effect of overfitting
5558120	5563320	than do psychedelics themselves, the actual substances, because it's a counterculture thing.
5563320	5568680	So it could be that as opposed to the actual substance. If you're a boring person who wears
5568680	5574600	a suit and tie and works at a bank and takes psychedelics, that could be a very different
5574600	5580760	effect of psychedelics on your mind. I'm just sort of raising the point that the people you
5580760	5586200	referenced are already weirdos. I'm not sure exactly. No, not necessarily. A lot of the people that
5587080	5593960	tell me that they use psychedelics in a useful way started out as squares and were liberating
5593960	5598360	themselves because they were stuck. They were basically stuck in local optimum of their own
5598360	5603240	self-model, of their relationship to the world, and suddenly they had data augmentation. They
5603240	5608440	basically saw an experience, a space of possibilities. They experienced what it would be like to be
5608440	5613560	another person. And they took important lessons from that experience back home.
5614280	5624040	Yeah. I mean, I love the metaphor of data augmentation because that's been the primary
5624040	5630680	driver of self-supervised learning in the computer vision domain is data augmentation. So it's funny
5630680	5638280	to think of data augmentation, like chemically induced data augmentation in the human mind.
5639080	5645800	There's also a very interesting effect that I noticed. I know several people who
5646360	5654440	sphere to me that LSD has cured their migraines. So severe cluster headaches or migraines that
5654440	5659800	didn't respond to standard medication that disappeared after a single dose. And I don't
5659800	5665640	recommend anybody doing this, especially not in the US where it's illegal. And there are no studies
5665720	5671000	on this for that reason. But it seems that anecdotally that it basically
5672120	5677960	can reset the serotonergic system. So it's basically pushing them outside of their normal
5677960	5682680	boundaries. And as a result, it needs to find a new equilibrium and then some people that
5682680	5687720	equilibrium is better. But it also follows that in other people, it might be worse. So
5687720	5694040	if you have a brain that is already teetering on the boundary to psychosis, it can be permanently
5694040	5697720	pushed over that boundary. Well, that's why you have to do good science, which they're starting
5697720	5701480	to do on all these different substances of how it actually works for the different conditions
5701480	5709400	like MDMA seems to help with PTSD. Same with psilocybin. You need to do good science, meaning
5709400	5715240	large studies of large N. Yeah. So based on the existing studies with MDMA, it seems that
5716280	5720360	if you look at Rick Doblin's work and what he has published about this and talks about
5721320	5726200	MDMA seems to be a psychologically relatively safe drug, but it's physiologically not very
5726200	5732280	safe. That is, there is neurotoxicity if you would use a too large dose. And if you
5733000	5737960	combine this with alcohol, which a lot of kids do in party settings during raves and so on,
5737960	5744280	it's very hepatotoxic. So basically, you can kill your liver. And this means that it's probably
5744280	5749320	something that is best and most productively used in clinical setting by people who really know
5749320	5753880	what they're doing. And I suspect that's also true for the other psychedelics. That is,
5754680	5761080	while the other psychedelics are probably not as toxic as say alcohol, the effects on Nisaki
5761080	5767000	can be much more profound and lasting. Yeah. Well, as far as I know psilocybin, so mushrooms,
5767000	5773560	magic mushrooms, as far as I know in terms of the studies they're running, I think have no over,
5773560	5778920	like they're allowed to do what they're calling heroic doses. So that one does not have a toxicity.
5778920	5783480	So they can do like huge doses in a clinical setting when they're doing study on psilocybin,
5783480	5789160	which is kind of fun. Yeah, it seems that most of the psychedelics work in extremely small doses,
5789160	5795560	which means that the effect on the rest of the body is relatively low. And MDMA is probably the
5795560	5800440	exception. Maybe ketamine can be dangerous and larger doses because it can depress breathing and
5800440	5807160	so on. But the LSD and psilocybin work in very, very small doses, at least the active part of them
5807720	5814280	of psilocybin LSD is only the active part. And the, but the effect that it can have on
5814280	5819320	your mental wiring can be very dangerous, I think. Let's talk about AI a little bit.
5820600	5827080	What are your thoughts about GPT-3 and language models trained with self-supervised learning?
5829960	5833400	It came out quite a bit ago, but I wanted to get your thoughts on it. Yeah.
5833400	5840520	In the 90s, I was in New Zealand and I had an amazing professor, Ian Witten,
5841080	5847640	who realized I was bored in class and put me in his lab. And he gave me the task to discover
5847640	5853720	grammatical structure in an unknown language. And the unknown language that I picked was English
5853720	5859640	because it was the easiest one to find a corpus four construct one. And he gave me the largest
5859640	5864600	computer at the whole university. It had two gigabytes of RAM, which was amazing. And I wrote
5864600	5871000	everything in C with some in-memory compression to do statistics over the language. And I first
5871000	5876920	would create a dictionary of all the words, which basically tokenizes everything and compresses
5876920	5882920	things so that I don't need to store the whole word, but just a code for every word. And then I
5882920	5889480	was taking this all apart in sentences. And I was trying to find all the relationships between
5889480	5895080	all the words in the sentences and do statistics over them. And that proved to be impossible,
5895080	5900360	because the complexity is just too large. So if you want to discover the relationship
5900360	5904360	between an article and a noun, and there are three adjectives in between, you cannot do
5904360	5909240	n-gram statistics and look at all the possibilities that can exist, at least not with the resources
5909240	5914120	that we had back then. So I realized I need to make some statistics over what I need to make
5914200	5921080	statistics over. So I wrote something that was pretty much a hack that did this for at least
5921080	5925400	first-order relationships. And I came up with some kind of mutual information graph that was
5925400	5930440	indeed discovering something that looks exactly like the grammatical structure of the sentence,
5930440	5934840	just by trying to encode the sentence in such a way that the words would be written in the
5934840	5942520	optimal order inside of the model. And what I also found is that if we would be able to increase
5942600	5948840	the resolution of that and not just use this model to reproduce grammatically correct sentences,
5948840	5953480	we would also be able to correct stylistically correct sentences by just having more bits in
5953480	5957720	these relationships. And if we wanted to have meaning, we would have to go much higher order.
5958600	5963320	And I didn't know how to make higher order models back then without spending way more years in
5963320	5969800	research on how to make the statistics over what we need to make statistics over. And this thing
5969800	5974040	that we cannot look at the relationships between all the bits in your input is being solved in
5974040	5980440	different domains in different ways. So in computer graphics, the computer vision standard method
5980440	5984680	for many years now is convolutional neural networks. Convolutional neural networks are
5984680	5989880	hierarchies of filters that exploit the fact that neighboring pixels in images are usually
5989880	5994520	semantically related and distance pixels in images are usually not semantically related.
5995400	5999000	So you can just by grouping the pixels that are next to each other,
5999000	6004680	hierarchically together reconstruct the shape of objects. And this is an important prior that
6004680	6009880	we built into these models so they can converge quickly. But this doesn't work in language for
6009880	6015480	the reason that adjacent words are often but not always related and distant words are sometimes
6015480	6021720	related while the words in between are not. So how can you learn the topology of language?
6022600	6028840	And I think for this reason that this difficulty existed, the transformer was invented in
6029880	6035800	natural language processing not in vision. And what the transformer is doing, it's a hierarchy
6035800	6041480	of layers where every layer learns what to pay attention to in the given context in the previous
6041480	6050760	layer. So what to make the statistics over. And the context is significantly larger than the adjacent
6050760	6058440	word. Yes. So the context that GP3 has been using, the transformer itself is from 2017,
6058440	6064920	and it wasn't using that large of a context. OpenAI has basically scaled up this idea
6064920	6072760	as far as they could at the time. And the context is about 2048 symbols, tokens in the language.
6072760	6078120	These symbols are not characters, but they take the words and project them into a vector space.
6078600	6083880	Words that are statistically co-occurring a lot are neighbors already. So it's already a
6083880	6089160	simplification of the problem a little bit. And so every word is basically a set of coordinates
6089160	6094760	in a high dimensional space. And then they use some kind of trick to also encode the order
6094760	6101240	of the words in a sentence or in the not just sentence, but 2048 tokens is about couple pages
6101240	6106760	of text or two and a half pages of text. And so they managed to do pretty exhaustive statistics
6106760	6111880	over the potential relationships between two pages of text, which is tremendous, right? I was
6111880	6118840	just using a single sentence back then. And I was only looking for first order relationships. And
6118840	6124200	they were really looking for much, much higher level relationships. And what they discover
6124200	6128920	after they've fed this with an enormous amount of trim linear pretty much the written internet
6128920	6134520	or the subset of it that had some quality, but substantial portion of the common flaw
6135160	6140360	that they're not only able to reproduce style, but they're also able to reproduce some
6140360	6145400	pretty detailed semantics, like being able to add three digit numbers and multiply two
6145400	6149480	digit numbers or to translate between pro and languages and things like that.
6150200	6154120	So the results that GBT3 got, I think were amazing.
6154120	6159560	By the way, I actually didn't check carefully. It's funny you just mentioned
6160520	6166040	how you coupled semantics to the multiplication. Is it able to do some basic math and two digit
6166040	6172440	numbers? Yes. Okay, interesting. I thought, I thought there's a lot of failure cases.
6173000	6177720	Yeah, it basically fails if you take larger digit numbers. So four digit numbers and so on
6178440	6183320	makes carrying mistakes and so on. And if you take large, larger numbers, you don't get useful
6183320	6191560	results at all. And this could be an issue of the training set, but not many examples of success
6191560	6198040	for long form addition and standard human written text. And humans aren't very good at doing three
6198040	6203720	digit numbers either. Yeah, they're not, you're not writing a lot about it. And the other thing
6203720	6207960	is that the loss function that is being used is only minimizing surprises. So it's predicting what
6207960	6212920	comes next in a typical text. It's not trying to go for causal closure first as we do.
6212920	6222680	Yeah. But the fact that that kind of prediction works to generate text that's semantically rich
6222680	6227320	and consistent is interesting. Yeah. So yeah, so it's amazing that it's able to
6228600	6233000	generate semantically consistent text. It's not consistent. So the problem is that it loses
6233000	6239640	coherence at some point. But it's also, I think not correct to say that GPT-3 is unable to deal
6239640	6244840	with semantics at all, because you ask it to perform certain transformations in text and it
6244840	6250040	performs these transformations in text. And the kind of additions that it's able to perform are
6250600	6256280	transformations in text, right? And there are proper semantics involved. You can also do more.
6256280	6263240	There was a paper that was generating lots and lots of mathematically correct text
6264040	6269960	and was feeding this into a transformer. And as a result, it was able to learn how to do
6269960	6274920	differentiation integration in race that according to the authors, matematica could not.
6277240	6281320	To which some of the people in matematica responded that they were not using the
6281320	6284840	matematica in the right way and so on. I have not really followed the
6284840	6290040	resolution of this conflict. This part, as a small tangent, I really don't like
6290120	6298200	in machine learning papers, which they often do anecdotal evidence. They'll find like one example
6298200	6303080	in some kind of specific use of matematica and demonstrate, look, here's, they'll show successes
6303080	6309000	and failures, but they won't have a very clear representation of how many cases this actually
6309000	6314600	represents. Yes, but I think as a first paper, this is a pretty good start. And so the take-home
6314600	6321480	message, I think, is that the authors could get better results from this in their experiments
6321480	6325800	than they could get from the vein, which they were using computer algebra systems,
6325800	6333720	which means that was not nothing. And it's able to perform substantially better than GPTS-V can,
6333720	6338920	based on a much larger amount of training data, using the same underlying algorithm.
6339480	6348280	Let me ask, again, so I'm using your tweets as if this is like Play-Doh, as if this is
6348280	6355480	well thought out novels that you've written. You tweeted, GPT-4 is listening to us now.
6358680	6365320	This is one way of asking, what are the limitations of GPT-3 when it scales? So what do you think
6365320	6371720	will be the capabilities of GPT-4, GPT-5, and so on? What are the limits of this approach?
6371720	6376680	So obviously, when we are writing things right now, everything that we are writing now is going
6376680	6381080	to be training data for the next generation of machine learning models. So yes, of course,
6381080	6386760	GPT-4 is listening to us. And I think the tweet is already a little bit older. And we now have
6386760	6393400	Vudao and we have a number of other systems that basically are placeholders for GPT-4.
6393400	6395880	I don't know what open AIS plans are in this regard.
6395880	6402200	I read that tweet in several ways. So one is obviously everything you put on the
6402200	6410280	internet has used this training data. But in a second way, I read it is in a, we talked about
6410280	6417320	agency. I read it is almost like GPT-4 is intelligent enough to be choosing to listen.
6418120	6423560	So not only like did a programmer tell it to collect this data and use it for training,
6423560	6429000	I almost saw the humorous angle, which is like it has achieved AGI kind of thing.
6429000	6432920	Well, the thing is, could VB already be living in GPT-5?
6435080	6440680	So GPT-4 is listening and GPT-5 actually constructed the entirety of the reality?
6440680	6445800	Of course, in some sense, what everybody is trying to do right now in AI is to extend the
6445800	6452280	transformer to be able to deal with video. And there are very promising extensions, right?
6452280	6459640	There's a work by Google that is called Perceiver and that is overcoming some of the limitations
6459640	6464280	of the transformer by letting it learn the topology of the different modalities separately
6465320	6471960	and by training it to find better input features. So the basically feature abstractions that are
6471960	6479800	being used by this successor to GPT-3 are chosen such a way that it's able to deal with video input.
6480680	6487720	And there is more to be done. So one of the limitations of GPT-3 is that it's amnesiac.
6487720	6492280	So it forgets everything beyond the two pages that it currently reads also during generation,
6492280	6498520	not just during learning. Do you think that's fixable within the space of deep learning?
6498600	6501240	Can you just make a bigger, bigger, bigger input?
6501240	6506680	No, I don't think that our own working memory is infinitely large. It's probably also just a few
6506680	6512920	thousand bits. But what you can do is you can structure this working memory. So instead of just
6513480	6518280	force feeding this thing, a certain thing that it has to focus on and it's not allowed to focus
6518280	6524520	on anything else because it's network, you allow it to construct its own working memory as we do,
6524520	6529400	right? When we are reading a book, it's not that we are focusing our attention in such a way that
6529400	6535880	we can only remember the current page. We will also try to remember other pages and try to undo
6535880	6540280	what we learn from them or modify what we learned from them. We might get up and take another book
6540280	6545960	from the shelf. We might go out and ask somebody that we can edit our working memory in any way
6545960	6551240	that is useful to put a context together that allows us to draw the right inferences and to
6551240	6558600	learn the right things. This ability to perform experiments on the world based on an attempt
6558600	6563880	to become fully coherent and to achieve causal closure, to achieve a certain aesthetic of your
6563880	6570120	modeling, that is something that eventually needs to be done. At the moment, we are skirting this
6570120	6575000	in some sense by building systems that are larger and faster so they can use dramatically larger
6575000	6580360	resources and human beings can do much more training data to get to models that in some sense
6580360	6587880	are already very superhuman and in other ways are laughingly incoherent. Do you think making
6588600	6599800	the systems multi-resolutional, some of the language models are focused on two pages,
6599800	6607480	some are focused on two books, some are focused on two years of reading, some are focused on a
6607560	6614600	lifetime like stacks of GPT-3s all the way down? You want to have gaps in between them. It's not
6614600	6620840	necessarily two years, there's no gaps. It stinks out of two years or out of 20 years or 2000 years
6620840	6626680	or two billion years where you are just selecting those bits that are predicted to be the most useful
6626680	6632440	ones to understand what you're currently doing. This prediction itself requires a very complicated
6632440	6636440	model. That's the actual model that you need to be making. It's not just that you are trying to
6636440	6641000	understand the relationships between things but what you need to make relationships or discover
6641000	6647720	relationships over. I wonder what that thing looks like, what the architecture for the thing
6647720	6653080	that's able to have that kind of model. I think it needs more degrees of freedom than the current
6653080	6658840	models have. It starts out with the fact that you possibly don't just want to have a feed-forward
6658840	6665240	model but you want it to be fully recurrent. To make it fully recurrent, you probably need to
6665240	6671160	loop it back into itself and allow it to skip connections. Once you do this, you're predicting
6671160	6677480	the next frame and your internal next frame in every moment and you are able to skip connection,
6677480	6684280	it means that signals can travel from the output of the network into the middle of the network
6684280	6689400	faster than the inputs do. Do you think it can still be differentiable? Do you think it still
6689400	6695000	can be in your network? Sometimes it can and sometimes it cannot. It can still be in your
6695000	6700760	network but not the fully differentiable one. When you want to deal with non-differentiable ones,
6700760	6705480	you need to have an attention system that is discreet and no-dimensional and can perform
6705480	6710120	grammatical operations. You need to be able to perform program synthesis. You need to be able
6710120	6715240	to backtrack in these operations that you perform on this thing. This thing needs a model of what
6715240	6719640	it's currently doing. I think this is exactly the purpose of our own consciousness.
6720600	6727560	Yeah, the program things are tricking along with your networks. Let me ask you, it's not
6727560	6732680	quite program synthesis but the application of these language models to generation
6733880	6738360	to program synthesis but generation of programs. If you look at GitHub OpenPilot,
6739080	6743080	which is based on OpenAI's codecs, I don't know if you've got a chance to look at it, but it's
6743800	6749160	the system that's able to generate code once you prompt it with what is it,
6750120	6754600	like the header of a function with some comments. It seems to do an incredibly good job
6756040	6761880	or not a perfect job but it's very important but an incredibly good job of generating functions.
6763000	6769320	What do you make of that? Is this exciting or is this just a party trick, a demo? Or is this
6769400	6776360	revolutionary? I haven't worked with it yet so it's difficult for me to judge it but I would not
6776360	6781720	be surprised if it turns out to be revolutionary. That's because the majority of programming tasks
6781720	6787240	that are being done in the industry right now are not creative. People are writing code that other
6787240	6791560	people have written or they're putting things together from code fragments that others have had
6791560	6797400	and a lot of the work that programmers do in practice is to figure out how to overcome the
6797400	6802200	gaps in the current knowledge and the things that people have already done. How to copy and paste
6802200	6808440	from Stack Overflow, that's right. And so of course we can automate that. Yeah, to make it
6808440	6812760	much faster to copy and paste from Stack Overflow. Yes, but it's not just copying and pasting,
6812760	6818360	it's also basically learning which parts you need to modify to make them fit together. Yeah,
6819240	6822600	like literally sometimes as simple as just changing the variable names
6823320	6827400	so it fits into the rest of your code. Yes, but this requires that you understand the semantics
6827400	6832040	of what you're doing to some degree. Yeah, and you can automate some of those things. The thing
6832040	6839320	that makes people nervous of course is that a little bit wrong in a program can have a dramatic
6839320	6846360	effect on the actual final operation of that program. So that's one little error which in the
6846360	6852360	space of language doesn't really matter but in the space of programs can matter a lot. Yes,
6852360	6858840	but this is already what is happening when humans program code. Yeah, so we have a technology to
6858840	6865240	deal with this. Somehow it becomes scarier when you know that a program generated code that's
6865240	6873960	running a nuclear power plant. You know humans have errors too. Exactly. But it's scarier when
6873960	6888120	a program is doing it because why? There's a fear that a program may not be as good as humans to
6888120	6900120	know when stuff is important to not mess up. There's a misalignment of priorities, of values,
6900200	6905720	that's potential, that maybe that's the source of the worry. I mean, okay, if I give you code
6905720	6915960	generated by GitHub OpenPilot and code generated by a human and say, here, use one of these,
6915960	6921720	which how do you select today and in the next 10 years, which code do you use?
6921720	6923560	Wouldn't you still be comfortable with the human?
6924120	6931880	At the moment when you go to Stanford to get an MRI, they will write a bill to the insurance
6931880	6939320	over $20,000 and of this maybe half of that gets paid by the insurance and the quarter gets paid
6939320	6947080	by you and the MRI cost them $600 to make, maybe probably less. And what are the values of the
6947080	6953480	person that writes the software and deploys this process? It's very difficult for me to say
6953960	6958840	whether I trust people. I think that what happens there is a mixture of
6960040	6964120	proper Anglo-Saxon Protestant values where somebody is trying to serve an abstract
6964120	6974200	greater whole and organize crime. Well, that's a very harsh, I think that's a harsh view of
6974200	6980520	humanity. There's a lot of bad people, whether incompetent or just malevolent in this world,
6980520	6987880	yes. But it feels like the more malevolent, so the more damage you do to the world,
6989480	6994440	the more resistance you have in your own human heart.
6994440	6999080	But not explain with malevolence or stupidity what can be explained by just people acting on
6999080	7006600	their incentives. So what happens in Stanford is not that somebody is evil, it's just that they do
7006600	7013960	what they're being paid for. No, it's not evil. I see that as malevolence. I see,
7017080	7025000	even like being a good German, as I told you offline, it's not absolute malevolence,
7025000	7030280	but it's a small amount. It's cowardice. I mean, when you see there's something wrong with the world,
7031240	7037240	it's either incompetence that you're not able to see it, or it's cowardice that you're not able to
7037240	7045160	stand up, not necessarily in a big way, but in a small way. So I do think that is in a bit of
7045160	7047800	malevolence. I'm not sure the example you're describing is a good example.
7047800	7054200	So the question is, what is it that you are aiming for? And if you don't believe in the future,
7054760	7058760	if you, for instance, think that the dollar is going to crash by what you try to save dollars,
7059480	7063960	if you don't think that humanity will be around in 100 years from now, because
7064680	7069160	global warming will wipe out civilization, why would you need to act as if it were?
7070200	7076040	So the question is, is there an overarching aesthetics that is projecting you and the world
7076040	7080440	into the future, which I think is the basic idea of religion, that you understand the
7080440	7084920	interactions that we have with each other as some kind of civilization level agent that is
7084920	7092040	projecting itself into the future. If you don't have that shared purpose, what is there to be
7092040	7098760	ethical for? So I think when we talk about ethics and AI, we need to go beyond the insane bias
7098760	7104600	discussions and so on, where people are just measuring the distance between a statistic to their
7104600	7111080	preferred current world model. But the optimism, I was a little confused by the previous thing,
7111080	7122840	just to clarify, there is a kind of underlying morality to having an optimism that human
7122840	7128680	civilization will persist for longer than 100 years. I think a lot of people believe
7130040	7134280	that it's a good thing for us to keep living. Yeah, of course. And thriving.
7134280	7139560	This morality itself is not an end to itself. It's instrumental to people living in 100 years
7139560	7147240	from now, or 500 years from now. So it's only justifiable if you actually think that it will
7147240	7152440	lead to people or increase the probability of people being arrived in that time frame.
7152440	7155880	And a lot of people don't actually believe that, at least not actively.
7156840	7162280	But I believe what exactly? Most people don't believe that they can afford to act on such a
7162280	7167560	model. Basically, what happens in the US is I think that the healthcare system is for a lot of
7167560	7171480	people no longer sustainable, which means that if they need the help of the healthcare system,
7171480	7176840	they're often not able to afford it. And when they cannot help it, they are often going bankrupt.
7176840	7181560	It's I think the leading course of personal bankruptcy in the US is the healthcare system.
7182680	7187720	And that would not be necessary. It's not because people are consuming more and more
7187720	7192520	medical services and are achieving a much, much longer life as a result. That's not actually the
7192520	7196840	story that is happening, because you can compare it to other countries. And life expectancy in
7196840	7201000	the US is currently not increasing. And it's not as high as in all the other industrialized
7201000	7205560	countries. So some industrialized countries are doing better with a much cheaper healthcare system.
7206200	7212760	And what you can see is, for instance, administrative bloat, the healthcare system has
7212760	7219400	maybe to some degree deliberately set up as a job placement program to allow people to continue
7219400	7226600	living in middle class existence, despite not having a useful use case in productivity.
7226600	7231880	So they are being paid to push paper around. And the number of administrators in the healthcare
7231880	7236360	system has been increasing much faster than the number of practitioners. And this is something
7236360	7241640	that you have to pay for, right? And also the revenues that are being generated in the healthcare
7241640	7246280	system are relatively large, and somebody has to pay for them. And the result why they are so
7246280	7252120	large is because market mechanisms are not working. The FDA is largely not protecting
7252120	7258760	people from malpractice of healthcare providers. The FDA is protecting healthcare providers from
7258760	7264680	competition. So this is a thing that it has to do with values. And this is not because people
7264680	7270680	are malicious on all levels. It's because they are not incentivized to act on a greater whole on
7270680	7275320	this idea that you treat somebody who comes to you as a patient like you would treat a family
7275320	7280440	member. Yeah, but we're trying. I mean, you're highlighting a lot of the flaws of the different
7280520	7285080	institutions, the systems we're operating under. But I think there's a continued throughout history
7285880	7291720	mechanism design of trying to design incentives in such a way that these systems behave better and
7291720	7297320	better and better. I mean, it's a very difficult thing to operate a society of hundreds of millions
7297320	7304920	of people effectively with. Yes. Do we live in a society that is ever-correcting? Do we observe
7304920	7310280	that our models of what we are doing are predictive of the future? And when they are not,
7310280	7315880	we improve them. Our laws are adjudicated with clauses that you put into every law,
7315880	7319960	what is meant to be achieved by that law, and the law will be automatically repealed
7319960	7324200	if it's not achieving that. If you are optimizing your own laws, if you're writing your own source
7324200	7329320	code, you probably make an estimate of what is this thing that's currently wrong in my life?
7329320	7334200	What is it that I should change about my own policies? What is the expected outcome? And
7334200	7339240	if that outcome doesn't manifest, I will change the policy back, right? Or I would change it to
7339240	7343880	do something different. Are we doing this on a societal level? I think so. I think it's easy
7343880	7350360	to sort of highlight the... I think we're doing it in the way that I operate my current life. I
7350360	7356120	didn't sleep much last night. You would say that Lex, the way you need to operate your life is you
7356120	7361320	need to always get sleep. The fact that you didn't sleep last night is totally the wrong way to
7361320	7367640	operate in your life. Like, you should have gotten all your shit done in time and gotten to sleep
7367640	7372520	because sleep is very important for health. And you're highlighting, look, this person is not sleeping.
7372520	7379560	Look, the medical, the healthcare system is operating... But the point is we just... It seems
7379560	7384520	like this is the way, especially in the capitalist society, we operate... We keep running into trouble
7384520	7390120	and last minute, we try to get our way out through innovation. And it seems to work.
7391000	7395880	You have a lot of people that ultimately are trying to build a better world and get
7396680	7403400	urgency about them when it's... The problem becomes more and more imminent. And that's
7403400	7410360	the way this operates. But if you look at the history, the long arc of history, it seems like
7410360	7416920	that operating on deadlines produces progress and builds better and better systems.
7416920	7423960	You probably agree with me that the US should have engaged in mass production in January 2020
7424520	7430920	and that we should have shut down the airports early on and that we should have made it mandatory
7430920	7437880	that the people that work in nursery homes are living on campus rather than living at home
7437880	7443240	and then coming in and infecting people in the nursing homes that had no immune response to COVID.
7443880	7449400	And that is something that was, I think, visible back then. The correct decisions haven't been
7449400	7454520	made. We would have the same situation again. How do we know that these wrong decisions are not
7454520	7459240	being made again? Have the people that made the decisions to not protect the nursing homes been
7459240	7465720	punished? Have the people that made the wrong decisions with respect to testing that prevented
7465720	7470840	the development of testing by startup companies and the importing of tests from countries that
7470840	7475160	already had them? Have these people been held responsible? Yeah, but first of all, so what do
7475160	7481160	you want to put before the firing squad? I think they are. No, just make sure that this doesn't
7481160	7487480	happen again. No, but it's not that, yes, they're being held responsible by many voices by people
7487480	7493480	being frustrated. There's new leaders being born now. They're going to see rise to the top in 10
7493480	7501160	years. This moves slower than there's obviously a lot of older incompetence and bureaucracy
7501160	7508840	in these systems move slowly. They move like science one depth at a time. So yes, I think
7509560	7515480	the pain has been felt in the previous year is reverberating throughout the world.
7515480	7520120	Maybe I'm getting old. I suspect that every generation in the U.S. after the war has lost
7520120	7524600	the plot even more. I don't see this development. The war of World War II?
7524600	7530360	Yes, so basically there was a time when we were modernist and in this modernist time,
7531640	7536760	the U.S. felt actively threatened by the things that happened in the world. The U.S. was worried
7536760	7545320	about possibility of failure and this imminence of possible failure led to decisions. There was
7545320	7551160	a time when the government would listen to physicists about how to do things. The physicists
7551160	7555240	were actually concerned about what the government should be doing. They would be writing letters
7555240	7560120	to the government. For instance, the decision for the Manhattan Project was something that was
7560120	7565400	driven in a conversation between physicists and the government. I don't think such a discussion
7565400	7571240	would take place today. I disagree. I think if the virus was much deadlier, we would see a very
7571240	7576360	different response. I think the virus was not sufficiently deadly. Instead, because it wasn't
7576360	7583720	very deadly, what happened is the current system started to politicize it. This is what I realized
7583720	7590760	with masks early on. They were not very quickly became not as a solution, but they became a thing
7591320	7596280	that politicians used to divide the country. The same things happened with vaccines,
7596920	7602040	so people weren't talking about solutions to this problem because I don't think the problem
7602040	7609240	was bad enough. When you talk about the war, I think our lives are too comfortable. I think
7609240	7616520	in the developed world, things are too good and we have not faced severe dangers. The
7616520	7622200	severe dangers existential threats are faced. That's when we step up on a small scale and a large
7622280	7632360	scale. That's my argument here, but I did think the virus, I was hoping that it was actually
7632360	7639800	sufficiently dangerous for us to step up because especially in the early days, it was unclear. It
7639800	7651560	still is unclear because of mutations, how bad it might be. I thought we would step up. The mask's
7651640	7659400	point is a tricky one because to me, the manufacture of masks isn't even the problem. I'm still to
7659400	7664680	this day and I was involved with a bunch of this work, have not seen good signs done on whether
7664680	7671160	masks work or not. There still has not been a large-scale study. To me, there should be large
7671160	7676360	scale studies on every possible solution, aggressive. In the same way that the vaccine
7676360	7681480	development was aggressive, there should be masks which test what kind of tests work really well.
7683880	7688840	Even the question of how the virus spreads, there should be aggressive studies on that.
7688840	7694120	To understand, I'm still, as far as I know, there's still a lot of uncertainty about that.
7694120	7701400	Nobody wants to see this as an engineering problem that needs to be solved. That I was surprised
7701400	7706280	about. I find that our views are largely convergent but not completely. I agree with
7706360	7711880	the thing that because our society in some sense perceives itself as too big to fail.
7713400	7717720	The virus did not alert people to the fact that we are facing possible failure
7718760	7723240	that basically put us into the post-modernist mode. I don't mean in a philosophical sense,
7723240	7729240	but in a societal sense. The difference between a post-modern society and the modern society is
7729240	7733560	that the modernist society has to deal with the ground tools and the post-modernist society has
7733560	7739240	to deal with appearances. Politics becomes a performance. The performance is done for an
7739240	7745480	audience and the organized audience is the media. The media evaluates itself via other media.
7745480	7750360	We have an audience of critics that evaluate themselves. I don't think it's so much the
7750360	7755640	failure of the politicians because to get in power and to stay in power, you need to be able
7755640	7761000	to deal with the published opinion. I think it goes in cycles because what's going to happen
7761560	7767320	is all of the small business owners, all the people who truly are suffering and will suffer
7767320	7773640	more because the effects of the closure of the economy and the lack of solutions to the virus,
7774200	7780760	they're going to a prize. Hopefully, this is where charismatic leaders can get the
7781400	7789880	world in trouble. Hopefully, we'll elect great leaders that will break through this
7789880	7797240	post-modernist idea of the media and the perception and the drama on Twitter and all that kind of
7797240	7803880	stuff. You know this can go either way. When the Weimar Republic was unable to deal with the
7803880	7811000	economic crisis that Germany was facing, there was an option to go back. There were people
7811000	7816760	which thought, let's get back to a constitutional monarchy and let's get this to work because
7816760	7823240	democracy doesn't work. Eventually, there was no way back. People decided there was no way back.
7823240	7828840	They needed to go forward. The only options for going forward was to become Stalinist
7828840	7836520	communists, basically an option to completely expropriate the factories and so on and nationalize
7836520	7844120	them and to reorganize Germany in communist terms and ally itself with Stalin and fascism.
7844840	7851320	Both options were obviously very bad. The one that the Germans picked led to a catastrophe
7851320	7858520	that devastated Europe. I'm not sure if the US has an immune response against that. I think that
7858520	7862760	the far-right is currently very weak in the US, but this can easily change.
7865800	7871400	Do you think from a historical perspective Hitler could have been stopped from within
7871400	7879480	Germany or from outside? It depends on who you want to focus, whether you want to focus on Stalin
7879480	7884200	or Hitler, but it feels like Hitler was the one as a political movement that could have been stopped.
7885160	7891480	I think that the point was that a lot of people wanted Hitler, so he got support from a lot of
7891480	7896280	quarters. There were a number of industrialists who supported him because they thought that the
7896280	7902040	democracy is obviously not working in unstable and you need a strongman. He was willing to play
7902040	7908680	that part. There were also people in the US who thought that Hitler would stop Stalin and would
7908680	7915560	act as a bulwark against Bolshevism, which he probably would have done at which cost.
7917560	7923480	Many of the things that he was going to do, like the Holocaust, was something where people
7923480	7928520	thought this is rhetoric. He's not actually going to do this, especially many of the Jews
7928520	7933080	themselves, which were humanists. For them, this was outside of the scope that was thinkable.
7935000	7942920	I wonder if Hitler is uniquely, I want to carefully use this term, but uniquely evil.
7944600	7952040	Hitler was never born if somebody else would come in this place. Just thinking about the
7952120	7959400	progress of history, how important are those singular figures that lead to mass destruction
7959400	7969480	and cruelty? Because my sense is, Hitler was unique. It wasn't just about the environment and
7969480	7976680	the context that gave him. Another person would not come in his place to do as destructive of
7976680	7984440	the things that he did. There was a combination of charisma, of madness, of psychopathy,
7984440	7990840	of just ego, all those things, which are very unlikely to come together in one person in the
7990840	7996760	right time. It also depends on the context of the country that you're operating in. If you
7997480	8004680	tell the Germans that they have a historical destiny in this romantic country, the effect is
8004680	8012440	probably different than it is in other countries. Stalin has killed a few more people than
8012440	8020600	Hitler did. If you look at the probability that you're survived under Stalin, Hitler killed people
8022600	8028760	if he thought they were not worth living, or if they were harmful to his racist project.
8029480	8035080	Basically, he felt that the Jews would be too cosmopolitan and would not be willing to participate
8035080	8041240	in the racist redefinition of society and the value of society and an ethno-state in this way,
8041240	8048040	as he wanted it to have it. He saw them as a harmful danger, especially since they played
8048040	8057880	such an important role in the economy and culture of Germany. Basically, he had some radical,
8057880	8065320	but rational reason to murder them. Stalin just killed everyone. The Stalinist purges were such
8065320	8073320	a random thing where he said that there is a certain possibility that this particular part
8073320	8077320	of the population has a number of German collaborators or something, and we just killed
8077320	8083800	them all. Or if you look at what Mao did, the number of people that were killed in absolute
8083800	8090360	numbers were much higher than the Mao that they were under Stalin. It's super hard to say. The
8090360	8097400	other thing is that you look at Genghis Khan and so on, how many people he killed. You see
8097400	8102040	there are a number of things that happen in human history that actually put a substantial
8102040	8109480	dent in the existing population, or Napoleon. It's very difficult to eventually measure it,
8109480	8117320	because what's happening is basically evolution on a human scale where one monkey figures out a
8117320	8124520	way to become viral and is using this viral technology to change the patterns of society
8124520	8131240	at a very large scale. What we find so abhorrent about these changes is the complexity that is
8131240	8135800	being destroyed by this. It's basically like a big fire that burns out a lot of the existing
8135800	8141640	culture and structure that existed before. Yeah, and it all just starts with one monkey,
8142520	8147400	one charismatic ape, and there's a bunch of them throughout history. Yeah, but it's in a given
8147400	8152280	environment. It's basically similar to wildfires in California. The temperature is rising,
8153160	8158360	there is less rain falling, and then suddenly a single spark can have an effect that in other
8158360	8166600	times would be contained. Okay, speaking of which, I love how we went to Hitler and Stalin from
8166600	8175400	20, 30 minutes ago, GPT-3 generating, doing programs that this is. The argument was about
8175400	8186920	morality of AI versus human. Specifically in the context of writing programs, specifically in the
8186920	8192680	context of programs that can be destructive. So running nuclear power plants or autonomous
8192680	8200600	weapons systems, for example. And I think your inclination was to say that it's not so obvious
8200600	8207880	that AI would be less moral than humans or less effective at making a world that would make humans
8207880	8215720	happy. So I'm not talking about self-directed systems that are making their own goals at
8215720	8221080	the global scale. If you just talk about the deployment of technological systems that are able
8221080	8227480	to see order and patterns and use this as control models to act on the goals that we give them,
8228280	8233080	then if we have the correct incentives to set the correct incentives for these systems,
8233080	8238760	I'm quite optimistic. So humans versus AI, let me give you an example.
8239400	8247480	Autonomous Weapon Systems. Let's say there's a city somewhere in the Middle East that has
8248280	8254920	a number of terrorists. And the question is, what's currently done with drone technologies,
8254920	8260040	you have information about the location of a particular terrorist, and you have a targeted
8260040	8265960	attack, you have a bombing of that particular building. And that's all directed by humans at
8265960	8273160	the high level strategy and also the deployment of individual bombs and missiles. Everything is
8273160	8281240	done by human except the final targeting and the spot, similar thing, like control the flight.
8282680	8291480	What if you give AI control and saying, write a program that says, here's the best information
8291480	8297080	available about the location of these five terrorists, here's the city, make sure all the
8297080	8302680	bombing you do is constrained to the city, make sure it's precision based, but you take care of
8302680	8308440	it. So you do one level of abstraction out and saying, take care of the terrorists in the city.
8309640	8316120	Which are you more comfortable with? The humans or the JavaScript GPT-3 generated code that's doing
8316120	8326440	the deployment. This is the kind of question I'm asking, is the kind of bugs that we see in human
8326440	8332440	nature, are they better or worse than the kind of bugs we see in AI? They're different bugs.
8332440	8340760	There is an issue that if people are creating an imperfect automation of a process that normally
8340760	8347480	requires a moral judgment, and this moral judgment is the reason why it cannot be automated often,
8347480	8354840	it's not because the computation is too expensive, but because the model that you give the AI is not
8354840	8359720	an adequate model of the dynamics of the world, because the AI does not understand the context
8359720	8364680	that it's operating in the right way. And this is something that already happens with Excel,
8364680	8370280	where you don't need to have an AI system to do this. If you have an automated process in place,
8370280	8375960	where humans decide using automated criteria whom to kill when, and whom to target when,
8375960	8381480	which already happens. And you have no way to get off the kill list once that happens,
8381480	8385720	once you have been targeted according to some automatic criterion by people, right,
8385720	8391560	in a bureaucracy. That is the issue. The issue is not the AI, it's the automation.
8392280	8397960	So there's something about, right, it's automation, but there's something about the,
8398840	8403640	there's a certain level of abstraction where you give control to AI to do the automation.
8404440	8410760	There's a scale that could be achieved that it feels like the scale of bug and scale mistake
8410760	8417960	and scale of destruction that could be achieved of the kind that humans cannot achieve. So AI is
8417960	8423560	much more able to destroy an entire country accidentally, versus humans. It feels like
8423560	8430360	the more civilians die as a react or suffer as a consequence of your decisions,
8431000	8435240	the more weight there is on the human mind to make that decision.
8436360	8442680	And so like it becomes more and more unlikely to make that decision for humans. For AI, it feels
8442680	8449640	like it's harder to encode that kind of weight. In a way, the AI that we're currently building
8450120	8455160	is automating statistics, right? Intelligence is the ability to make models so you can act on them
8455160	8461560	and AI is a tool to make better models. So in principle, if you're using AI wisely,
8461560	8467880	you're able to prevent more harm. And I think that the main issue is not on the side of the AI,
8467880	8472280	it's on the side of the human command hierarchy that is using technology irresponsibly.
8472280	8479000	So the question is how hard is it to encode, to properly encode the right incentives into the AI?
8479080	8484440	So for instance, there's this idea of what happens if we let our airplanes being flown
8484440	8489800	with AI systems and the neural network is a black box and so on. And it turns out our neural
8489800	8495640	networks are actually not black boxes anymore. There are function approximators losing linear
8495640	8501800	algebra, and there are performing things that we can understand. But we can also, instead of
8501800	8506040	letting the neural network fly the airplane, use the neural network to generate approval,
8506040	8511080	be correct program with a degree of accuracy of the proof that a human could not achieve.
8511800	8516920	And so we can use our AI by combining different technologies to build systems that are much
8516920	8523480	more reliable than the systems that a human being could create. And so in this sense, I would say
8523480	8531320	that if you use an early stage of technology to save labor and don't employ competent people,
8531320	8535720	but just to hack something together because you can, that is very dangerous. And if people
8535800	8540600	are acting under these incentives that they get evasive, delivering shoddy work more
8540600	8545080	cheaply using AI is less human oversight than before. That's very dangerous.
8545080	8550280	The thing is though, AI is still going to be unreliable, perhaps less so than humans,
8550280	8557480	but it'll be unreliable in novel ways. And yeah, but this is an empirical question. And it's
8557480	8563080	something that we can figure out and work with. So the issue is, do we trust the systems,
8563080	8567960	the social systems that we have in place and the social systems that we can build and maintain,
8567960	8573960	that they're able to use AI responsibly? If they can, then AI is good news. If they cannot,
8573960	8576040	then it's going to make the existing problems worse.
8577080	8581160	Well, and also who creates the AI, who controls it, who makes money from it,
8581160	8586920	because it's ultimately humans. And then you start talking about how much you trust the humans.
8586920	8591080	So the question is, what does who mean? I don't think that we have identity per se.
8591080	8597240	I think that the story of a human being is somewhat random. What happens is more or less that
8597240	8601960	everybody is acting on their local incentives, what they perceive to be their incentives.
8601960	8607960	And the question is, what are the incentives that the one that is pressing the button is operating
8607960	8615480	under? Yeah, it's nice for those incentives to be transparent. So for example, I'll give you
8616040	8618920	there seems to be a significant distrust of
8621320	8626920	tech, like entrepreneurs in the tech space, or people that run, for example, social media
8626920	8633320	companies like Mark Zuckerberg. There's not a complete transparency of incentives under
8634120	8640600	which that particular human being operates. We can listen to the words he says,
8640680	8646200	or what the marketing team says for a company, but we don't know. And that's becomes a problem
8646200	8653560	when that the algorithms and the systems created by him and other people in that company
8654120	8660680	start having more and more impact on society. And that it starts, you know, if the incentives
8660680	8669080	were somehow the definition and the explainability of the incentives was decentralized such that
8669080	8676680	nobody can manipulate it. No propaganda type manipulation of like how these systems actually
8676680	8685560	operate could be done. Then yes, if you know, I think, I think AI could achieve much fair, much
8685560	8694680	more effective sort of like solutions to difficult ethical problems. But when there's like humans
8694680	8701560	in the loop, manipulating the dissemination, the communication of how the system actually
8701560	8707000	works, that feels like you can run to a lot of trouble. And that's why there's currently a lot
8707000	8712360	of distrust for people at the heads of companies that have increasingly powerful AI systems.
8713800	8719320	I suspect what happened traditionally in the US was that since our decision making is much more
8719880	8725080	decentralized than in an authoritarian state, people are making decisions autonomously at many,
8725080	8731400	many levels in the society. What happened that was we created coherence and cohesion in society
8731400	8737160	by controlling what people thought and what information they had. The media synchronized
8737160	8743160	public opinion. And social media have disrupted this. It's not, I think, so much Russian influence
8743160	8748680	or something. It's everybody's influence. It's that a random person can come up with a conspiracy
8748680	8755720	theory and disrupt what people think. And if that conspiracy theory is more compelling or more
8755720	8761080	attractive than the standardized public conspiracy theory that we give people as a default,
8761800	8765880	then it might get more traction, right? You suddenly have the situation that a single individual
8765880	8772920	somewhere on a farm in Texas has more listeners than CNN. Which particular farmer you're referring
8772920	8782280	to in Texas? Probably no. Yes, I had dinner with him a couple of times. Okay. Right. It's an
8782280	8787160	interesting situation because you cannot get to be an anchor in CNN if you don't go through a
8788440	8793640	complicated gatekeeping process. And suddenly you have random people without that gatekeeping
8793640	8799560	process just optimizing for attention. Not necessarily with a lot of responsibility for
8799640	8806120	the long-term effects of projecting these theories into the public. And now there is a push of making
8806120	8811400	social media more like traditional media, which means that the opinion that is being projected in
8811400	8817560	social media is more limited to an acceptable range with the goal of getting society into
8817560	8821960	safe waters and increase the stability and cohesion of society again, which I think is
8821960	8827480	a laudable goal. But of course, it also is an opportunity to cease the means of indoctrination.
8828280	8833320	And the incentives that people are under when they do this are in such a way that
8834120	8840520	the AI ethics that we would need becomes very often something like AI politics,
8840520	8846040	which is basically partisan and ideological. And this means that whatever one side says,
8846040	8851080	another side is going to be disagreeing with, right? In the same way as when you turn masks or
8851080	8856600	the vaccine into a political issue, if you say that it is politically virtuous to get vaccinated,
8856600	8861560	it will mean that the people that don't like you will not want to get vaccinated. And as soon
8861560	8867000	as you have this partisan discourse, it's going to be very hard to make the right decisions,
8867000	8871400	because the incentives get to be the wrong ones. AI ethics needs to be super boring. It needs to
8871400	8878120	be done by people who do statistics all the time and have extremely boring, long-winded discussions
8878120	8882360	that most people cannot follow because they are too complicated, but that are dead serious.
8882440	8886920	These people need to be able to be better at statistics than the leading machine learning
8886920	8893080	researchers. And at the moment, the AI ethics debate is the one where you don't have any barrier
8893080	8898680	to entry, right? Everybody who has a strong opinion and is able to signal that opinion in the right
8898680	8904280	way can enter it. Strong words to judge it back. And to me, that is a very frustrating thing,
8904280	8908440	because the field is so crucially important to our future. It's so crucially important, but
8908840	8913960	the only qualification you currently need is to be outraged by the injustice in the world.
8914680	8917640	It's more complicated, right? Everybody seems to be outraged.
8918920	8922520	So let's just say that the incentives are not always the right ones. So basically,
8923080	8929000	I suspect that a lot of people that enter this debate don't have a vision for what society
8929000	8934200	should be looking like in a way that is nonviolent, where we preserve liberal democracy, where we
8934200	8941240	make sure that we all get along and we are around in a few hundred years from now, preferably
8941240	8943960	with the comfortable technological civilization around us.
8944760	8952440	I generally have a very foggy view of that world, but I tend to try to follow and I think
8952440	8958040	society should in some degree follow the gradient of love, increasing the amount of love in the
8958040	8963880	world. And whenever I see different policies or algorithms or ideas that are not doing so,
8964520	8967240	obviously, that's the ones that kind of resist.
8967880	8974520	So the thing that terrifies me about this notion is I think that German fascism was driven by love.
8975560	8979000	It was just a very selective love. It was a love that basically-
8979000	8984760	Well, now you're just manipulating. I mean, that's, you have to be very careful.
8985720	8990520	You're talking to the wrong person in this way about love.
8990520	8995960	So let's talk about what love is. And I think that love is the discovery of shared purpose.
8995960	9001880	It's the recognition of the sacred and the other. And this enables non-transactual interactions.
9002840	9008920	But the size of the other that you include needs to be maximized.
9009720	9016280	So it's basically appreciation, like deep appreciation of
9018920	9025800	the world around you fully, including the people that are very different than you,
9025800	9030200	the people that disagree with you completely, including people, including living creatures
9030200	9036680	outside of just people, including ideas and appreciation of the full mess of it.
9036680	9043320	And also it has to do with empathy, which is coupled with a lack of confidence,
9043960	9051000	uncertainty about your own rightness. It's like a radical open-mindedness to the way forward.
9051000	9056440	I agree with every part of what you said. And now, if you scale it up, what you recognize is that
9057000	9062840	love is, in some sense, the service to a next level agency, to the highest level agency that
9062840	9069640	you can recognize. It could be, for instance, life on earth or beyond that, where you could say
9069640	9073400	intelligent complexity in the universe that you try to maximize in a certain way.
9073960	9080280	But when you think it through, it basically means a certain aesthetic. And there is not one possible
9080280	9085320	aesthetic. There are many possible aesthetics. And once you project an aesthetic into the future,
9085320	9090440	you can see that there are some which defect from it, which are in conflict with it,
9090760	9096280	that are corrupt, that are evil. You and me would probably agree that Hitler was evil,
9097000	9101480	because the aesthetic of the world that he wanted is in conflict with the aesthetic of
9101480	9109400	the world that you and me have in mind. And so the thing that he destroyed, we want to keep them
9109400	9118040	in the world. There's ways to deal. I mean, Hitler's an easier case, but perhaps he wasn't so easy
9118120	9121720	in the 30s, to understand who's Hitler and who's not.
9122280	9127400	No, it's just that there was no consensus that the aesthetics that he had in mind were unacceptable.
9127400	9137720	Yeah. I mean, it's difficult. Love is complicated, because you can't just be so open-minded that
9137720	9147800	you let evil walk into the door. But you can't be so self-assured that you can always identify
9147800	9154040	evil perfectly, because that's what leads to Nazi Germany, having a certainty of what is and
9154040	9163320	wasn't evil, like always drawing lines of good versus evil. There has to be a dance between
9166920	9174680	like hard stances extending up against what is wrong. And at the same time, empathy and open
9174680	9180760	mind towards not knowing what is right and wrong. And like a dance between those.
9181320	9186040	I found that when I watched the Miyazaki movies that there is nobody who captures my spirituality
9186040	9192280	as well as he does. It's very interesting and just vicious. There is something going on in
9192280	9198520	his movies that is very interesting. So, for instance, Mononoka is discussing not only an answer
9198520	9205320	to Disney's simplistic notion of Mowgli, the jungle boy who was raised by wolves. And as soon
9205320	9213240	as he sees people, realizes that he's one of them. And the way in which the moral life and nature
9213240	9218440	is simplified and romanticized and turned into kitsch is disgusting in the Disney movie. And he
9218440	9223880	answers to this. You see, he's replaced by Mononoka, this wolf girl who was raised by wolves and who
9223880	9229640	was fierce and dangerous and who cannot be socialized because he cannot be tamed. He cannot
9229640	9233560	be part of human society. And you see human society, it's something that is very, very
9233560	9239640	complicated. You see people extracting resources and destroying nature. But the purpose is not
9239640	9246440	to be evil, but to be able to have a life that is free from, for instance, oppression and violence
9247080	9254520	and to curb death and disease. And you basically see this conflict which cannot be resolved in a
9254520	9259640	certain way. You see this moment when nature is turned into a garden and it loses most of what
9259640	9265560	it actually is when humans no longer submitting to life and death and nature. And to these questions,
9265560	9270440	there is no easy answer. So, he just turns it into something that is being observed as a journey
9270440	9276280	that happens and that happens with a certain degree of inevitability. And the nice thing about
9276280	9280360	all his movies is there is a certain main character and it's the same in all movies.
9281160	9288520	It's this little girl that is basically Heidi and I suspect that happened because when he did
9289880	9295080	field work for working on the Heidi movies back then, the Heidi animations, before he did his
9295080	9302040	own movies, he traveled to Switzerland and south-eastern Europe and the Adriatic and so on
9302040	9306440	and got an idea about a certain aesthetic and a certain way of life that informed his
9306440	9312600	future thinking. And Heidi has a very interesting relationship to herself and to the world.
9313160	9318680	There's nothing that she takes for herself. She is in a way fearless because she is committed
9318680	9323960	to a service to a greater whole. Basically, she is completely committed to serving God.
9323960	9328680	And it's not an institutionalized God. It has nothing to do with the Roman Catholic Church or
9329640	9333880	something like this. But in some sense, Heidi is an embodiment of the spirit of European
9333880	9340680	Protestantism. It's this idea of a being that is completely perfect and pure. And it's not a
9340680	9350520	feminist vision because she is not a girl boss or something like this. She is the justification
9350520	9355080	for the men in the audience to protect her, to build the civilization around her that makes
9355080	9360920	her possible. So she is not just the sacrifice of Jesus who is innocent and therefore,
9360920	9366840	nailed to the cross. She is not being sacrificed. She is being protected by everybody around her
9366840	9373000	who recognizes that she is sacred and there are enough around her to see that. So that's a very
9373000	9377880	interesting perspective. There's a certain notion of innocence. And this notion of innocence is not
9377880	9384280	universal. It's not in all cultures. Hitler wasn't innocent. His idea of Germany was not
9384280	9388680	that there is innocence that is being protected. There was a predator that was going to triumph.
9389560	9393000	And it's also something that is not at the core of every religion. There are many religions
9393000	9399800	which don't care about innocence. They might care about increasing the status of something.
9400920	9407560	And that's a very interesting notion that is quite unique and not claiming it's the optimal one.
9407560	9413480	It's just a particular kind of aesthetic which I think makes Miyazaki into the most relevant
9413480	9420840	Protestant philosopher today. And you're saying in terms of all the ways that society can operate
9420840	9428840	perhaps the preservation of innocence might be one of the best? No, it's just my aesthetic.
9429880	9435320	Your aesthetic. It's a particular way in which I feel that I relate to the world that is natural
9435320	9440920	to my own civilization and maybe it's not an accident that I have cultural roots in Europe
9441160	9448040	in a particular world. And so maybe it's a natural convergence point and it's not something that you
9448040	9455720	will find in all other times in history. So I'd like to ask you both Solzhenitsyn and our individual
9455720	9462440	role as ants in this very large society. So he says that some version of the line between good
9462440	9467240	and evil runs to the heart of every man. Do you think all of us are capable of good and evil?
9467240	9477320	Like what's our role in this play in this game while playing? Is all of us capable to play any
9477320	9484840	role? Like is there an ultimate responsibility to you mentioned maintaining innocence or whatever
9484840	9491400	the whatever the highest ideal for society you want are all of us capable of living up to that?
9491400	9496520	And that's our responsibility or are there significant limitations to what we're able to
9496520	9504040	do in terms of good and evil? So there is a certain way if you are not parable,
9504040	9511080	if you are committed to some kind of civilizational agency, a next level agent that you are serving,
9511080	9516200	some kind of transcendent principle. In the eyes of that transcendental principle,
9516200	9519800	you are able to discern good from evil, otherwise you cannot, otherwise you have just
9519800	9526200	individual aesthetics. The cat that is torturing a mouse is not evil because the cat does not envision
9526280	9532760	or not part of the cat is envisioning a world where there is no violence and nobody is suffering.
9533640	9538840	If you have an aesthetic where you want to protect innocence, then torturing somebody
9538840	9545640	needlessly is evil, but only then. No, but within, I guess the question is within the aesthetic,
9545640	9554680	like within your sense of what is good and evil, it seems like we're still able to
9555480	9561160	commit evil. Yes. So basically, if you are committing to this next level agent, you are not
9561160	9566040	necessarily are this next level agent, right? You are part of it. You have a relationship to it,
9566040	9571320	like it sailed us to its organism, its hyperorganism, and it only exists to the degree
9571320	9577800	that it's being implemented by you and others. And that means that you're not completely fully
9577800	9582520	serving it. You have freedom in what you decide, whether you are acting on your impulses and local
9582520	9587160	incentives and your verbal impulses, so to speak, or whether you're committing to it.
9587160	9593640	And what you perceive then is a tension between what you would be doing with respect to the thing
9593640	9599800	that you recognize as the sacred, if you do, and what you're actually doing. And this is the line
9599800	9605640	between good and evil, right? Where you see, oh, I'm here acting on my local incentives or impulses,
9605640	9609720	and here I'm acting on what I consider to be sacred. And there's a tension between those.
9609720	9615080	And this is the line between good and evil that might run through your heart. And if you don't
9615080	9619320	have that, if you don't have this relationship to a transcendental agent, you could call this
9619320	9624280	relationship to the next level agent soul, right? It's not a thing. It's not an immortal thing that
9624280	9628760	is intrinsically valuable. It's a certain kind of relationship that you project to understand
9628760	9632440	what's happening. Somebody is serving this transcendental sacredness or they're not.
9633160	9639080	If you don't have a soul, you cannot be evil. You're just a complex natural phenomenon.
9639560	9644920	So if you look at life, like starting today, or starting tomorrow, when we leave here today,
9646040	9649720	there's a bunch of trajectories that you can take through life,
9651880	9659800	maybe countless. Do you think some of these trajectories in your own conception of yourself,
9659800	9668520	some of those trajectories are the ideal life, a life that if you were to be the hero of your life
9668520	9673720	story, you would want to be? Like is there some Joshua Bach that you're striving to be?
9674360	9680360	Like this is the question I ask myself as an individual trying to make a better world in the
9680360	9686440	best way that I could conceive of. What is my responsibility there? And how much am I responsible
9686440	9694840	for the failure to do so? Because I'm lazy and incompetent too often in my own perception.
9695640	9701720	In my own world view, I'm not very important. So I don't have place for me as a hero in my
9701720	9710360	own world. I'm trying to do the best that I can, which is often not very good. And so it's not
9710360	9717080	important for me to have status or to be seen in a particular way. It's helpful if others can see
9717080	9719640	me, or a few people can see me, that can be my friends.
9719720	9725480	No, sorry, I want to clarify. The hero didn't mean status or perception or
9728360	9732920	some kind of marketing thing, but more in private, in the quiet of your own mind.
9734040	9739800	Is there the kind of man you want to be and would consider it a failure if you don't become that?
9740360	9741880	That's what I meant by hero.
9741880	9745960	Yeah, not really. I don't perceive myself as having such an identity.
9746200	9758520	And it's also sometimes frustrating. But it's basically a lack of having this notion of
9758520	9760360	father that I need to be emulating.
9763960	9769720	It's interesting. I mean, it's the leaf floating down the river. I worry that...
9771800	9773320	Sometimes it's more like being the river.
9776680	9786360	I'm just a fat frog sitting on a leaf on a dirty muddy lake.
9791080	9795800	Waiting for a princess to kiss me. Or the other way. I forgot which way it goes.
9795800	9797000	Somebody kisses somebody.
9799000	9801880	Can I ask you, I don't know if you know who Michael Malis is, but
9802520	9808760	in terms of constructing systems of incentives, it's interesting to ask.
9809320	9815560	I don't think I've talked to you about this before. Malis espouses anarchism.
9815560	9823080	So he sees all government as fundamentally getting in the way or even being destructive to
9825480	9830360	collaborations between human beings thriving. What do you think?
9830360	9838920	What's the role of government in a society that thrives? Is anarchism at all compelling to you
9839640	9844040	as a system? Not just small government, but no government at all?
9845800	9847320	Yeah, I don't see how this would work.
9849720	9855080	The government is an agent that imposes an offset on your reward function, on your payout
9855080	9859320	metrics, so your behavior becomes compatible with the common good.
9860840	9867640	So the argument there is that you can have collectives, like governing organizations,
9867640	9873560	but not government, like where you're born in a particular set of land, and therefore you must
9873560	9883960	follow this rule or else. You're forced by what they call violence, because there's an implied
9883960	9893240	violence here. So the key aspect of government is it protects you from the rest of the world
9893240	9901560	with an army and with police. It has a monopoly on violence. It's the only one that's able to do
9901560	9906760	violence. There are many forms of government. Not all governments do that, but we find that
9906840	9915160	in successful countries, the government has a monopoly on violence. That means that you cannot
9915160	9919320	get ahead by starting your own army, because the government will come down on you and destroy you
9919320	9924360	if you try to do that. In countries where you can build your own army and get a way visit,
9924360	9929000	some people will do it. These countries is what we call failed countries in a way.
9931720	9936440	If you don't want to have violence, the point is not to appeal to the moral intentions of people,
9936840	9941720	because some people will use strategies if they get ahead with them that feel a particular kind
9941720	9948120	of ecological niche. So you need to destroy that ecological niche. And if an effective government
9948120	9953400	has a monopoly on violence, it can create a world where nobody is able to use violence
9953400	9958600	and get ahead. So you want to use that monopoly on violence not to exert violence,
9958600	9964440	but to make violence impossible, to raise the cost of violence. So people need to get ahead with
9964440	9970280	nonviolent means. So the idea is that you might be able to achieve that in an anarchist state
9970840	9977560	with companies. So with the forces of capitalism is create security companies
9978200	9983320	where the one that's most ethically sound rises to the top. Basically, it would be a much better
9983320	9992360	representative of the people, because there is less sort of stickiness to the big military force
9992440	9996280	sticking around, even though it's long, overlived, outlived.
9996280	10001640	So you have groups of militants that are hopefully officially organized, because otherwise they're
10001640	10006760	going to lose against the other groups of militants, and they are coordinating themselves with the
10006760	10012760	rest of society until they are having a monopoly on violence. How is that different from a government?
10013880	10016120	So it's basically converging to the same thing.
10016120	10021080	So I think it always, in my, as I was trying to argue with Malice, I feel like it always converges
10021080	10026360	towards government at scale. But I think the idea is you can have a lot of collectives that are,
10027400	10034120	you basically never let anything scale too big. So one of the problems with governments is it
10034120	10041240	gets too big in terms of the size of the group over which it has control.
10043880	10050600	My sense is that would happen anyway. So a successful company like Amazon or Facebook,
10051240	10057800	it starts forming a monopoly over the entire populations, not over just hundreds of millions,
10057800	10065000	but billions of people. So I don't know. But there is something about the abuses of power
10065000	10071640	the government can have when it has a monopoly on violence. And so that's attention there.
10072920	10077080	So the question is, how can you set the incentives for government correctly? And this
10077080	10082200	mostly applies at the highest levels of government. And we, because we haven't found a way to set
10082200	10087080	them correctly, we made the highest levels of government relatively weak. And this is,
10087080	10091320	I think, part of the reason why we had difficulty to coordinate the pandemic response.
10092120	10097720	And China didn't have that much difficulty. And there is, of course, a much higher risk of the
10097720	10104200	abuse of power that exists in China, because the power is largely unchecked. And that's
10104200	10108440	basically what happens in the next generation, for instance, imagine that we would agree that
10108440	10112920	the current government of China is largely correct and by never limit. And maybe we don't agree on
10112920	10119080	this. But if we did, how can we make sure that this stays like this? And if you don't have checks
10119080	10124520	and balances and division of power, it's hard to achieve. You don't have a solution for that problem.
10125240	10129720	But the abolishment of government basically would remove the control structure from a
10129720	10135720	cybernetic perspective. There is an optimal point in the system that the regulation should be
10135720	10140840	happening, right? Where you can measure the current incentives and the regulator would
10140840	10146120	properly incentivize to make the right decisions and change the payout metrics of everything below
10146120	10150840	it in such a way that the local prisoners' dilemmas get resolved. You cannot resolve the
10150840	10157080	prisoners' dilemma without some kind of eternal control that emulates an infinite game in a way.
10157800	10165800	Yeah. I mean, there's a sense in which it seems like the reason government, the parts of government
10165800	10175240	that don't work well currently is because there's not good mechanisms through which to interact
10175240	10180520	for the citizenry to interact with government is basically it hasn't caught up in terms of
10180520	10187000	technology. And I think once you integrate some of the digital revolution of being able to
10187080	10193080	have a lot of access to data, be able to vote on different ideas at a local level, at all levels,
10193080	10199480	at the optimal level, like you're saying, that can resolve the prisoner dilemmas and to integrate
10199480	10208600	AI to help you out, automate things that don't require the human ingenuity. I feel like that's
10208600	10214520	where government could operate that well and can also break apart the inefficient bureaucracies,
10214520	10220520	if needed. There'll be a strong incentive to be efficient and successful.
10220520	10225800	So out human history, we see an evolutionary competition of modes of government and of
10225800	10230520	individual governments as in these modes. And every nation state in some sense is some kind
10230520	10236280	of organism that has found different solutions for the problem of government. And you could look at
10236280	10241560	all these different models and the different scales at which it exists as empirical attempts to
10241560	10249080	validate the idea of how to build a better government. And I suspect that the idea of anarchism,
10249080	10256200	similar to the idea of communism, is the result of being disenchanted with the ugliness of the real
10256200	10263000	existing solutions and the attempt to get to an utopia. And I suspect that communism originally
10263000	10268520	was not a utopia. I think that in the same way as original Christianity, it had a particular kind
10268600	10275240	of vision. And this vision is a society, a mode of organization within the society,
10275240	10282920	in which humans can coexist at scale without coercion. The same way as we do in a healthy
10282920	10289480	family. In a good family, you don't terrorize each other into compliance, but you understand what
10289480	10293960	everybody needs and what everybody can is able to contribute and what the intended future of the
10293960	10299480	whole thing is. And everybody coordinates their behavior in the right way and informs each other
10299480	10304360	about how to do this and all the interactions that happen are instrumental to making that happen.
10305720	10310680	Could this happen at scale? And I think this is the idea of communism. Communism is opposed to the
10310680	10316280	idea that we need economic terror or other forms of terror to make that happen. But in practice,
10316280	10322120	what happened is that the proto-communist countries, the real existing socialism, replaced a part of
10322120	10327480	the economic terror with moral terror. So we were told to do the right thing for moral reasons.
10327480	10332200	And of course, it didn't really work and the economy eventually collapsed. And the moral
10332200	10337640	terror had actual real cost. People were imprisoned because they were morally non-compliant.
10341640	10345960	The other thing is that the idea of communism became a utopia. So it basically was projected
10345960	10352040	into the afterlife. We were told in my childhood that communism was a hypothetical society
10352040	10356280	to which we were in a permanent revolution that justified everything that was presently wrong
10356280	10361880	with society morally. But it was something that our grandchildren probably would not ever see
10361880	10366040	because it was too ideal and too far in the future to make it happen right now. And
10366040	10371720	people were just not there yet morally. And the same thing happened with Christianity. This notion
10371720	10377000	of heaven was mythologized and projected into an afterlife. And I think this was just the idea of
10377080	10381880	God's kingdom, of this world in which we instantiate the next level transcendental agent in the
10381880	10387240	perfect form. So everything goes smoothly and without violence and without conflict and without
10387240	10393160	this human messiness on this economic messiness and the terror and coercion that existed in the
10393160	10398840	present societies. And the idea of whether humans can exist at scale in a harmonious way
10398840	10404440	non-coercively is untested. A lot of people tested it but didn't get it to work so far.
10405080	10409560	And the utopia is a world in where you get all the good things without any of the bad things.
10410600	10415560	And you are, I think, very susceptible to believe in utopias when you are very young and don't
10415560	10420840	understand that everything has to happen in causal patterns, that there is always feedback loops that
10420840	10425800	ultimately are closed. There's nothing that just happens because it's good or bad. Good or bad
10425800	10429800	don't exist in isolation. They only exist with respect to larger systems.
10430600	10440360	So can you intuit why utopias fail as systems? So like having a utopia that's out there beyond
10440360	10448040	the horizon, is it because then, it's not only because it's impossible to achieve utopias,
10448120	10454920	but it's because what certain humans, certain small number of humans start to
10458360	10464360	sort of greedily attain power and money and control and influence
10465480	10474280	as they become, as they see the power in using this idea of a utopia.
10474520	10478760	It's a little like saying, why is my garden not perfect? It's because some evil weeds are
10478760	10484760	overgrowing it and they always do. But this is not how it works. A good garden is a system that
10484760	10491880	is in balance and requires minimal interactions by the gardener. And so you need to create a system
10491880	10496840	that is designed to self-stabilize. And the design of social systems requires not just the
10496840	10501800	implementation of the desired functionality but the next level design, also in biological systems.
10501800	10505320	You need to create a system that wants to converge to the intended function.
10506040	10510920	And so instead of just creating an institution like the FDA that is performing a particular kind
10510920	10517160	of role in society, you need to make sure that the FDA is actually driven by a system that wants to
10517160	10522680	do this optimally, that is incentivized to do it optimally and then makes the performance that is
10522680	10528600	actually enacted in every generation instrumental to that thing, that actual goal. And that is much
10528600	10534360	harder to design and to achieve. So you have to design a system where, and listen, communism also
10534360	10543960	was quote unquote incentivized to be a feedback loop system that achieves that utopia. It just,
10543960	10547960	it wasn't working given human nature. The incentives were not correct given human nature.
10547960	10553240	So how do you incentivize people when they are getting coal off the ground to work as hard as
10553240	10559240	possible? Because it's a terrible job and it's very bad for your health. And right, how do you do
10559240	10565320	this? And you can give them prices and metals and status to some degree, right? There's only so much
10565320	10570120	status to give for that. And most people will not fall for this, right? Or you can pay them.
10571240	10575800	And you probably have to pay them in an asymmetric way because if you pay everybody the same and
10577000	10581800	you nationalize the coal mines, eventually people will figure out that they can game the system.
10581880	10589080	Yes. So you're describing capitalism. Capitalism is the present solution to the system. And
10589080	10594280	what we also noticed that I think that Marx was correct in saying that capitalism is prone to
10594280	10600680	crisis, that capitalism is a system that in its dynamics is not convergent but divergent.
10600680	10608280	It's not a stable system. And that eventually it produces an enormous potential for productivity.
10608920	10613960	But it also is systematically misallocating resources. So a lot of people cannot participate
10614680	10619480	in the production and consumption anymore. And this is what we observe. We observe that
10619480	10625480	the middle class in the US is tiny. A lot of people think that they're middle class,
10625480	10628280	but if you are still flying economy, you're not middle class.
10629240	10634600	Right. Every class is a menu to a smaller than the previous class.
10640600	10643080	The thing about classes is really like airline classes.
10644360	10649240	It's the no-play class. A lot of people are economy class. Business class and very few
10649240	10655000	are first class and summer budget. I mean, I understand. I think there is, yeah,
10655640	10659640	maybe some people, probably I would push back against that definition of the middle class.
10659640	10664200	It does feel like the middle class is pretty large, but yes, there's a discrepancy in terms of wealth.
10665720	10671720	So think about the terms of the productivity that our society could have. There is no reason
10671720	10677000	for anybody to fly economy. We would be able to let everybody travel in style.
10677880	10683160	Well, but also some people like to be frugal even when they're billionaires. So let's take
10683160	10688440	that into account. Yes, but I mean, we probably don't need to be a traveling lavish, but you also
10688440	10693400	don't need to be tortured, right? There is a difference between frugal and subjecting yourself
10693400	10697880	to torture. Listen, I love economy. I don't understand why you're comparing a flying economy
10697880	10705480	to torture. Although the flight here, there's two crying babies next to me. But that has
10705480	10712200	nothing to do with crime babies. They're very cute though. I have two kids and sometimes I have
10712200	10721160	to go back to visit the grandparents and back means going from the west coast to Germany.
10721160	10727480	And that's a long flight. Is it true that when you're a father, you grow immune to the crying
10727480	10734120	and all that kind of stuff? Because like me just not having kids, other people's kids can be quite
10734120	10738520	annoying when they're crying and screaming and all that kind of stuff. When you have children
10738520	10743400	and you're wired up in a default natural way, you're lucky in this regard, you fall in love with
10743400	10749640	them. And this falling in love with them means that you basically start to see the world through
10749640	10753720	their eyes and you understand that in a given situation, they cannot do anything but
10755080	10761000	being expressing despair. And so it becomes more differentiated. I noticed that, for instance,
10761080	10769480	my son is typically acting on a pure experience of what things are like right now. And he has
10769480	10774920	to do this right now. And you have this small child that is, when he was a baby and so on,
10774920	10779080	where he was just immediately expressing what he felt. And if you cannot regulate this from the
10779080	10784360	outside, there's no point to be upset about it. It's like dealing with weather or something like
10784360	10790440	this. You all have to get through it. And it's not easy for him either. But if you also have
10791160	10794760	a daughter, maybe she is planning for that. Maybe she understands that
10796520	10800040	she's sitting in the car behind you and she's screaming at the top of her lungs and you're
10800040	10804920	almost doing an accident. And you really don't know what to do. What should I have done to
10804920	10807720	make you stop screaming? You could have given me candy.
10809960	10814600	I think that's like a cat versus dog discussion. I love it. Because you said the
10815240	10821080	like a fundamental aspect of that is love that makes it all worth it.
10822200	10829480	What in this monkey riding an elephant in a dream world, what role does love play
10829480	10835960	in the human condition? I think that love is the facilitator of non-transactual interaction.
10837880	10843480	When you are observing your own purposes, some of these purposes go beyond your ego. They go
10843480	10847400	beyond the particular organism that you are and your local interests.
10847400	10849240	That's what you mean by non-transactual.
10849240	10852760	Yes. So basically, when you are acting in a transactional way, it means that you are
10852760	10857400	respecting something in return for you from the one that you're interacting with.
10858520	10862360	You are interacting with a random stranger. You buy something from them on eBay. You expect
10862360	10867000	a fair value for the money that you send them and vice versa because you don't know that person.
10867000	10871080	You don't have any kind of relationship to them. But when you know this person a little bit better
10871080	10875320	and you know the situation that they're in, you understand what they try to achieve in their life
10875320	10880360	and you approve because you realize that they're in some sense serving the same
10880360	10885880	human sacredness as you are. They need to think that you have. Maybe you give it to them as a present.
10888360	10893880	The feeling itself of joy is a kind of benefit, is a kind of transaction.
10895080	10899800	Yes, but the joy is not the point. The joy is the signal that you get. It's the reinforcement
10899800	10904920	signal that your brain sends to you because you are acting on the incentives of the agent that
10904920	10910920	you're part of. We are meant to be part of something larger. That is the way in which we out-competed
10910920	10920920	other hominids. Take that, Neanderthals. Yes, and also other humans. There was a population
10920920	10927960	bottleneck for human society that leads to an extreme lack of genetic diversity among humans.
10927960	10933960	If you look at Bushmen in the Kalahari that basically tribes that are not that far
10933960	10939080	distant to each other have more genetic diversity than exist between Europeans and Chinese.
10940120	10945480	That's because basically the out-of-Africa population at some point had a bottleneck
10945480	10951960	of just a few thousand individuals. What probably happened is not that at any time the number of
10951960	10957800	people shrunk below a few hundred thousand. What probably happened is that there was a small group
10957800	10962360	that had a decisive mutation that produced an advantage. This group multiplied and killed
10962360	10970920	everybody else. We are descendants of that group. I wonder what the peculiar characteristics of
10970920	10981560	that group. We can only just listen to the echoes and the ripples that are still within us.
10981800	10988600	I suspect what eventually made a big difference was the ability to organize at scale, to program
10988600	10995000	each other with ideas. That we became programmable, that we were willing to work on lockstep, that we
10995000	11001240	went above the tribal level, that we no longer were groups of a few hundred individuals and
11001240	11007320	acted on direct reputation systems transactionally, but that we basically evolved an adaptation
11007320	11015640	to become state building. To form collectives outside of the direct collectives.
11015640	11021000	Yes, and that's basically a part of us became committed to serving something outside of what
11021000	11026120	we know. Then that's kind of what love is. It's terrifying because it meant that we
11026120	11032840	eradicated the others. It's a force. It's an adaptive force that gets us ahead in evolution,
11032840	11035160	which means we displace something else that doesn't have that.
11036680	11041480	Oh, so we had to murder a lot of people that warned about love. Love led to destruction.
11041480	11047320	They didn't have the same strong love as we did. That's why I mentioned this thing with fascism.
11047320	11055080	When you see these speeches, do you want total war? Everybody says yes. This is this big,
11056040	11060040	oh my God, be a part of something that is more important than me that gives meaning to my existence.
11062920	11072440	Fair enough. Do you have advice for young people today in high school, in college,
11073000	11079800	that are thinking about what to do with their career, with their life, so that at the end of
11079800	11086440	the whole thing, they could be proud of what they did? Don't cheat. Have integrity.
11087400	11091800	Integrity. So what does integrity look like when you're at the river, or at the leaf,
11091800	11099320	or the fat frog in a lake? It basically means that you try to figure out what the thing is that
11099320	11105880	is the most right. This doesn't mean that you have to look for what other people tell you
11105880	11112120	what's right, but you have to aim for moral autonomy. Things need to be right independently
11112120	11120840	of what other people say. I always felt that when people told me to listen to what others say,
11120840	11126520	like read the room, build your ideas of what's true based on the high status people of your
11126520	11131880	in-group, that does not protect me from fascism. The only way to protect yourself from fascism
11131880	11136680	is to decide it's the world that is being built here, the world that I want to be in.
11137400	11143480	Yeah. So in some sense, try to make your behavior sustainable. Act in such a way that you
11143480	11147880	would feel comfortable on all sides of the transaction. Realize that everybody is you
11147880	11152440	in a different timeline, but is seeing things differently and has reasons to do so.
11153880	11160600	Yeah. I've come to realize this recently. There is an inner voice that tells you what's right
11160600	11168360	and wrong. And speaking of reading the room, there's times what integrity looks like is there's
11168360	11174280	times when a lot of people are doing something wrong and what integrity looks like is not going
11174280	11180440	on Twitter and tweeting about it, but not participating quietly, not doing. So it's
11180440	11187000	not like signaling or not all this kind of stuff, but actually living your what you think is right,
11187880	11193320	like living it. That's also sometimes this expectation that others are like us. So imagine
11193320	11197880	the possibility that some of the people around you are space aliens that only look human.
11199160	11203400	Right. So they don't have the same prayers as you do. They don't have the same
11203400	11208920	impulses that what's right and wrong. There's a large diversity in these basic impulses that
11208920	11213640	people can have in a given situation. And now realize that you are a space alien.
11214360	11218760	Right. You are not actually human. You think that you're human, but you don't know what it means,
11218760	11223160	like what it's like to be human. You just make it up as you go along like everybody else.
11223960	11230280	And you have to figure that out what it means that you are a full human being, what it means to be
11230280	11236200	human in the world and how to connect with others on that. And there is also something, don't be
11236200	11242360	afraid in the sense that if you do this, you're not good enough. Because if you are acting on these
11242360	11247320	incentives of integrity, you become trustworthy. That's the way in which you can recognize each
11247320	11252360	other. There is a particular place where you can meet and you can figure out what that place is,
11252920	11258360	where you will give support to people because you realize that they act with integrity,
11258360	11262360	and they will also do that. So in some sense, you are safe if you do that.
11263720	11267800	You're not always protected. There are people which will abuse you and that might,
11268600	11272120	that are bad actors in a way that it's hard to imagine before you meet them.
11272680	11276840	But there is also people which will try to protect you.
11277720	11282600	Yeah, that's such a, thank you for saying that. There's such a hopeful message
11283800	11289880	that no matter what happens to you, there'll be a place, there's people you'll meet
11290520	11299160	that also have what you have. And you will find happiness there and safety there.
11299160	11305640	Yeah, but it doesn't need to end well. It can also all go wrong. So there's no guarantee in this life.
11305640	11311880	So you can do everything right and you still can fail. And horrible things happen to you that
11311880	11316680	traumatize you and mutilate you. And you have to be grateful if it doesn't happen.
11320200	11325240	And ultimately be grateful no matter what happens, because even just being alive is pretty damn nice.
11326760	11333400	Yeah, even that, you know. The gratefulness in some sense is also just generated by your brain
11333400	11341560	to keep you going. It's all the trick. Speaking of which, Camus said,
11342760	11349000	I see many people die because they judge that life is not worth living. I see others
11349000	11354040	paradoxically getting killed for the ideas or illusions that give them a reason for living.
11354840	11358200	What is called the reason for living is also an excellent reason for dying.
11359240	11363800	I therefore conclude that the meaning of life is the most urgent of questions.
11364600	11373640	So I have to ask what Josh Abok is the meaning of life. It is an urgent question according to Camus.
11373960	11381240	I don't think that there's a single answer to this. Nothing makes sense unless a mind makes it so.
11381240	11388120	So you basically have to project a purpose. And if you zoom out far enough, there's the heat test
11388120	11392280	of the universe and everything is meaningless. Everything is just a blip in between. And the
11392280	11398680	question is, do you find meaning in this blip in between? Do you find meaning in observing squirrels?
11399640	11404600	Do you find meaning in raising children and projecting a multi-generational organism into
11404600	11410600	the future? Do you find meaning in projecting an aesthetic of the world that you like to the future
11410600	11416280	and trying to serve that aesthetic? And if you do, then life has that meaning. And if you don't,
11416280	11425560	then it doesn't. I enjoy the idea that you just create the most vibrant, the most weird,
11425560	11431880	the most unique kind of blip you can, given your environment, given your set of skills.
11431880	11443560	Just be the most weird set of local pocket of complexity you can be. So that when people study
11443560	11450760	the universe, they'll pause and be like, that's weird. It looks like a useful strategy, but of
11450760	11457640	course it's still motivated reasoning. You're obviously acting on your incentives here.
11457640	11463800	It's still a story we tell ourselves within a dream that's hardly in touch with the reality.
11463800	11465880	It's definitely a good strategy if you are a podcaster.
11470040	11472760	And a human, which I'm still trying to figure out if I am.
11472760	11475000	Yeah, there's a mutual relationship somehow.
11475960	11482600	Josh, you're one of the most incredible people I know. I really love talking to you. I love
11482600	11487320	talking to you again. And it's really an honor that you spend your valuable time with me. I hope
11487320	11492600	we get to talk many times throughout our short and meaningless lives.
11493400	11494520	More meaningful.
11494520	11495800	More meaningful.
11495800	11497800	Thank you, Alex. I enjoyed this conversation very much.
11498920	11502760	Thanks for listening to this conversation with Yosha Bach. And thank you to
11502760	11509880	Coinbase, Codecademy, Linode, Netsuite, and ExpressVPN. Check them out in the description
11509880	11514680	to support this podcast. And now let me leave you with some words from Carl Jung.
11515720	11521000	People will do anything, no matter how absurd in order to avoid facing their own souls.
11521720	11527160	One does not become enlightened by imagining figures of light, but by making the darkness
11527720	11532600	conscious. Thank you for listening and hope to see you next time.
