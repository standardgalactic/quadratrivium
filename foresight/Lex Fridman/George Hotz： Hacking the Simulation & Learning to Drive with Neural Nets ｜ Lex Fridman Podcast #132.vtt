WEBVTT

00:00.000 --> 00:06.560
The following is a conversation with George Hotz, a.k.a. Geohot, his second time on the podcast.

00:06.560 --> 00:12.880
He's the founder of Kamma AI, an autonomous and semi-autonomous vehicle technology company

00:12.880 --> 00:21.360
that seeks to be, to Tesla autopilot, what Android is, to the iOS. They sell the Kamma 2 device for

00:21.360 --> 00:27.040
$1,000 that, when installed in many of their supported cars, can keep the vehicle centered

00:27.040 --> 00:33.520
in the lane even when there are no lane markings. It includes driver sensing that ensures that the

00:33.520 --> 00:39.440
driver's eyes are on the road. As you may know, I'm a big fan of driver sensing. I do believe Tesla

00:39.440 --> 00:45.520
Autopilot and others should definitely include it in their sensor suite. Also, I'm a fan of Android

00:45.520 --> 00:52.400
and a big fan of George, for many reasons, including his non-linear out-of-the-box brilliance and the

00:52.400 --> 00:58.480
fact that he's a superstar programmer of a very different style than myself. Styles make fights

00:59.040 --> 01:04.480
and styles make conversations, so I really enjoyed this chat and I'm sure we'll talk many more times

01:04.480 --> 01:10.160
on this podcast. Quick mention of a sponsor followed by some thoughts related to the episode.

01:10.160 --> 01:17.520
First is ForSigmatic, the maker of delicious mushroom coffee. Second is the coding digital,

01:17.520 --> 01:24.480
a podcast on tech and entrepreneurship that I listen to and enjoy. And finally, ExpressVPN,

01:24.480 --> 01:30.000
the VPN I've used for many years to protect my privacy on the internet. Please check out the

01:30.000 --> 01:35.760
sponsors in the description to get a discount and to support this podcast. As a side note,

01:35.760 --> 01:41.840
let me say that my work at MIT on autonomous and semi-autonomous vehicles led me to study the human

01:41.840 --> 01:47.680
side of autonomy enough to understand that it's a beautifully complicated and interesting problem

01:47.680 --> 01:54.080
space, much richer than what can be studied in the lab. In that sense, the data that comma AI,

01:54.080 --> 01:59.600
Tesla Autopilot, and perhaps others like Cadillac Supercruiser Collecting, gives us a chance to

01:59.600 --> 02:06.320
understand how we can design safe semi-autonomous vehicles for real human beings in real-world

02:06.320 --> 02:12.960
conditions. I think this requires bold innovation and a serious exploration of the first principles

02:12.960 --> 02:18.400
of the driving task itself. If you enjoyed this thing, subscribe on YouTube, review it with

02:18.400 --> 02:24.320
five stars and up a podcast, follow on Spotify, support on Patreon, or connect with me on Twitter

02:24.320 --> 02:32.800
at Lex Freedman. And now here's my conversation with George Hotz. So last time we started talking

02:32.800 --> 02:37.520
about the simulation, this time let me ask you, do you think there's intelligent life out there in

02:37.520 --> 02:44.000
the universe? I always maintained my answer to the Fermi paradox. I think there has been intelligent

02:44.000 --> 02:48.800
life elsewhere in the universe. So intelligent civilizations existed, but they've blown themselves

02:48.800 --> 02:55.760
up. So your general intuition is that intelligent civilizations quickly, like there's that parameter

02:55.760 --> 03:01.360
in the Drake equation, your sense is they don't last very long. Yeah. How are we doing on that?

03:01.840 --> 03:07.760
Have we lasted pretty good? Oh, no. How do we do? Oh, yeah. I mean, not quite yet.

03:09.040 --> 03:15.360
Well, it's only as Yukowski, IQ required to destroy the world falls by one point every year.

03:15.360 --> 03:20.160
Okay. So technology democratizes the destruction of the world.

03:20.960 --> 03:22.320
When can a meme destroy the world?

03:25.200 --> 03:27.200
It kind of is already, right?

03:27.200 --> 03:33.040
Somewhat. I don't think we've seen anywhere near the worst of it yet. World's going to get weird.

03:33.840 --> 03:39.360
Well, maybe a meme can save the world. We thought about that, the meme Lord Elon Musk fighting on

03:39.360 --> 03:46.880
the side of good versus the meme Lord of the darkness, which is not saying anything bad about

03:46.880 --> 03:52.960
Donald Trump, but he is the Lord of the meme on the dark side. He's a Darth Vader of memes.

03:53.680 --> 03:59.760
I think in every fairy tale, they always end it with, and they lived happily ever after,

03:59.760 --> 04:04.320
and I'm like, please tell me more about this happily ever after. I've heard 50% of marriages

04:04.320 --> 04:09.360
end in divorce. Why doesn't your marriage end up there? You can't just say happily ever after. So

04:10.880 --> 04:16.320
the thing about destruction is it's over after the destruction. We have to do everything right

04:16.320 --> 04:22.000
in order to avoid it. And one thing wrong, I mean, actually, this is what I really like about

04:22.000 --> 04:25.680
cryptography. Cryptography, it seems like we live in a world where the defense wins.

04:27.760 --> 04:33.280
Versus nuclear weapons, the opposite is true. It is much easier to build a warhead that splits

04:33.280 --> 04:39.440
into 100 little warheads than to build something that can take out 100 little warheads. The offense

04:39.440 --> 04:46.960
has the advantage there. So maybe our future is in crypto. So cryptography, right? The Goliath

04:46.960 --> 04:55.920
is the defense. And then all the different hackers are the Davids, and that equation is flipped for

04:55.920 --> 05:02.000
nuclear war. Because there's so many, like one nuclear weapon destroys everything, essentially.

05:02.000 --> 05:08.400
Yeah, and it is much easier to attack with a nuclear weapon than it is to like, the technology

05:08.400 --> 05:13.120
required to intercept and destroy a rocket is much more complicated than the technology required to

05:13.120 --> 05:21.040
just orbital trajectory send a rocket to somebody. Okay, your intuition that there were intelligent

05:21.040 --> 05:26.880
civilizations out there, but it's very possible that they're no longer there. It's kind of a sad

05:26.880 --> 05:34.240
picture. They enter some steady state. They all wirehead themselves. What's wirehead? Stimulate

05:34.240 --> 05:42.480
their platter centers and just live forever in this kind of stasis. They become, well, I mean,

05:42.480 --> 05:49.120
I think the reason I believe this is because where are they? If there's some reason they stopped

05:49.120 --> 05:53.280
expanding, because otherwise they would have taken over the universe. The universe isn't that big.

05:53.280 --> 05:57.440
Or at least, you know, let's just talk about the galaxy, right? 70,000 light years across.

05:58.480 --> 06:04.560
I took that number from Star Trek Voyager. I don't know how true it is. But yeah, that's not big.

06:04.560 --> 06:10.080
Right? 70,000 light years is nothing. For some possible technology that you can imagine that

06:10.240 --> 06:13.920
leverage like wormholes or something like that. You don't even need wormholes. Just a von Neumann

06:13.920 --> 06:19.120
probe is enough. A von Neumann probe and a million years of sub light travel, and you'd have taken

06:19.120 --> 06:25.040
over the whole universe. That clearly didn't happen. So something stopped it. So you mean a few,

06:25.040 --> 06:30.640
right, for like a few million years, if you sent out probes that travel close, what's sub light?

06:30.640 --> 06:35.600
You mean close to the speed of light? Let's say 0.1c. And it just spreads. Interesting. Actually,

06:35.600 --> 06:41.840
that's an interesting calculation. So what makes you think that we'd be able to communicate with

06:41.840 --> 06:48.880
them? Like, yeah, what's why do you think we would be able to be able to comprehend

06:48.880 --> 06:55.440
intelligent lies that are out there? Like, even if they were among us kind of thing? Like,

06:55.440 --> 07:02.880
or even just flying around? Well, I mean, that's possible. It's possible that there is some sort

07:02.880 --> 07:07.840
of prime directive that'd be a really cool universe to live in. And there's some reason

07:07.840 --> 07:14.000
they're not making themselves visible to us. But it makes sense that they would use the same,

07:15.040 --> 07:18.960
well, at least the same entropy. Well, you're implying the same laws of physics. I don't know

07:18.960 --> 07:23.440
what you mean by entropy in this case. Oh, yeah. I mean, if entropy is the scarce resource in the

07:23.440 --> 07:28.800
universe. So what do you think about like Steven Wolfram and everything is a computation? And

07:28.880 --> 07:33.680
then what if they are traveling through this world of computation? So if you think of the

07:33.680 --> 07:41.440
universe as just information processing, then what you're referring to with entropy, and then

07:41.440 --> 07:46.240
these pockets of interesting complex computation swimming around, how do we know they're not

07:46.240 --> 07:53.440
already here? How do we know that this, like all the different amazing things that are full of

07:53.440 --> 08:00.560
mystery on earth are just like little footprints of intelligence from light years away?

08:01.520 --> 08:06.880
Maybe. I mean, I tend to think that as civilizations expand, they use more and more energy.

08:07.680 --> 08:11.680
And you can never overcome the problem of waste heat. So where is their waste heat?

08:11.680 --> 08:15.520
So we'd be able to, with our crude methods, be able to see like, there's a whole lot of

08:17.200 --> 08:22.080
energy here. But it could be something we're not, I mean, we don't understand dark energy,

08:22.400 --> 08:27.360
dark matter. It could be just stuff we don't understand at all. Or they can have a fundamentally

08:27.360 --> 08:33.360
different physics, you know, like that we just don't even comprehend. Well, I think, okay,

08:33.360 --> 08:36.960
I mean, it depends how far out you want to go. I don't think physics is very different on the

08:36.960 --> 08:43.520
other side of the galaxy. I would suspect that they have, I mean, if they're in our universe,

08:43.520 --> 08:48.800
they have the same physics. Well, yeah, that's the assumption we have. But there could be like

08:48.800 --> 08:57.840
super trippy things like, like our cognition only gets to a slice, and all the possible

08:57.840 --> 09:01.760
instruments that we can design only get to a particular slice of the universe. And there's

09:01.760 --> 09:09.280
something much like weirder. Maybe we can try a thought experiment. Would people from the past

09:09.920 --> 09:16.400
be able to detect the remnants of our, we'll be able to detect our modern civilization?

09:16.400 --> 09:20.560
And I think the answer is obviously yes. You mean past from a hundred years ago?

09:20.560 --> 09:23.200
Well, let's even go back further. Let's go to a million years ago.

09:24.320 --> 09:28.400
The humans who were lying around in the desert probably didn't even have, maybe they just barely

09:28.400 --> 09:41.840
had fire. They would understand if a 747 flew overhead. In this vicinity, but not if a 747 flew

09:41.840 --> 09:46.320
on Mars. Like, because they wouldn't be able to see far, because we're not actually communicating

09:46.320 --> 09:52.640
that well with the rest of the universe. We're doing okay, just sending out random like 50s tracks

09:52.640 --> 09:59.120
of music. True. And yeah, I mean, they'd have to, you know, the, we've only been broadcasting radio

09:59.120 --> 10:07.920
waves for 150 years. And well, there's your light cone. So yeah, okay. What do you make about all

10:08.000 --> 10:15.760
the, I recently came across this, having talked to David's favor. I don't know if you caught

10:15.760 --> 10:22.800
what the videos that Pentagon released and the New York Times reporting of the UFO sightings.

10:23.440 --> 10:31.600
So I kind of looked into it, quote unquote. And there's actually been like hundreds of thousands

10:31.600 --> 10:37.040
of UFO sightings, right? And a lot of it, you can explain it way in different kinds of ways.

10:37.040 --> 10:43.120
So one is it could be interesting physical phenomena. Two, it could be people wanting to

10:43.120 --> 10:47.680
believe. And therefore they conjure up a lot of different things that just, you know, when you

10:47.680 --> 10:53.600
see different kinds of lights, some basic physics phenomena, and then you just conjure up ideas

10:53.600 --> 11:00.000
of possible out there mysterious worlds. But, you know, it's also possible like you have a case of

11:00.960 --> 11:08.880
David Fravor, who is a Navy pilot, who's, you know, as legit as it gets in terms of humans who

11:08.880 --> 11:16.160
are able to perceive things in the environment and make conclusions, whether those things are

11:16.160 --> 11:23.280
a threat or not. And he and several other pilots saw a thing, I don't know if you followed this,

11:23.280 --> 11:28.320
but they saw a thing that they've since then called TikTok that moved in all kinds of weird ways.

11:29.280 --> 11:36.480
They don't know what it is. It could be technology developed by the United States,

11:36.480 --> 11:40.560
and they're just not aware of it and the surface level from the Navy, right? It could be

11:40.560 --> 11:45.120
different kind of lighting technology or drone technology, all that kind of stuff. It could

11:45.120 --> 11:51.280
be the Russians and the Chinese, all that kind of stuff. And of course their mind, our mind,

11:51.280 --> 11:57.280
can also venture into the possibility that it's from another world. Have you looked into this at

11:57.280 --> 12:04.960
all? What do you think about it? I think all the news is a scythe. I think that the most plausible...

12:04.960 --> 12:13.120
Nothing is real. Yeah, I listened to the, I think it was Bob Lazar on Joe Rogan. And like,

12:13.120 --> 12:18.000
I believe everything this guy is saying. And then I think that it's probably just some like MK

12:18.000 --> 12:24.640
Ultra kind of thing, you know? What do you mean? Like, they, you know, they made some weird thing

12:24.640 --> 12:28.320
and they called it an alien spaceship. You know, maybe it was just to like stimulate young

12:28.320 --> 12:32.080
physicists' minds and tell them it's alien technology and we'll see what they come up with,

12:32.080 --> 12:37.840
right? Do you find any conspiracy theories compelling? Like, have you pulled at the

12:37.840 --> 12:42.960
string of the, of the rich complex world of conspiracy theories that's out there?

12:43.760 --> 12:48.960
I think that I've heard a conspiracy theory that conspiracy theories were invented by the CIA in

12:48.960 --> 12:58.480
the 60s to discredit true things. Yeah. So, you know, you can go to ridiculous conspiracy theories

12:58.480 --> 13:06.880
like Flat Earth and Pizza Gate and, you know, these things are almost to hide like conspiracy

13:06.880 --> 13:10.480
theories that like, you know, remember when the Chinese like locked up the doctors who discovered

13:10.480 --> 13:14.560
coronavirus? Like I tell people this and I'm like, no, no, no, that's not a conspiracy theory. That

13:14.560 --> 13:18.720
actually happened. Do you remember the time that the money used to be backed by gold and now it's

13:18.720 --> 13:24.480
backed by nothing? This is not a conspiracy theory. This actually happened. Well, that's one of my

13:24.480 --> 13:35.680
worries today with the idea of fake news is that when nothing is real, then like you dilute the

13:35.680 --> 13:41.200
possibility of anything being true by conjuring up all kinds of conspiracy theories. And then you

13:41.200 --> 13:47.680
don't know what to believe. And then like the idea of truth of objectivity is lost completely.

13:47.760 --> 13:53.440
Everybody has their own truth. So you used to control information by censoring it.

13:53.440 --> 13:58.240
Then the internet happened and governments are like, oh, shit, we can't censor things anymore.

13:58.240 --> 14:03.120
I know what we'll do. You know, it's the old story of the story of like,

14:04.080 --> 14:08.320
tying a flag with a leprechaun tells you as gold is buried and you tie one flag and you make the

14:08.320 --> 14:11.680
leprechaun swear to not remove the flag and you come back to the field later with a shovel and

14:11.680 --> 14:20.160
this flag's everywhere. That's one way to maintain privacy, right? In order to protect the contents

14:20.160 --> 14:25.920
of this conversation, for example, we could just generate like millions of deep fake conversations

14:25.920 --> 14:31.200
where you and I talk and say random things. So this is just one of them and nobody knows which

14:31.200 --> 14:35.920
one was the real one. This could be fake right now. Classic steganography technique.

14:36.000 --> 14:43.840
Okay, another absurd question about intelligent life because you're an incredible programmer

14:43.840 --> 14:52.480
outside of everything else we'll talk about just as a programmer. Do you think intelligent beings

14:52.480 --> 14:58.960
out there, the civilizations that were out there had computers and programming? Did they

14:58.960 --> 15:05.680
do naturally have to develop something where we engineer machines and are able to encode both

15:06.880 --> 15:12.240
knowledge into those machines and instructions that process that knowledge, process that

15:12.240 --> 15:17.680
information to make decisions and actions and so on? And would those programming languages,

15:18.240 --> 15:22.480
if you think they exist, be at all similar to anything we've developed?

15:24.080 --> 15:29.840
So I don't see that much of a difference between quote unquote natural languages and programming

15:29.840 --> 15:40.960
languages. I think there's so many similarities. So when asked the question what do alien languages

15:40.960 --> 15:48.720
look like, I imagine they're not all that dissimilar from ours. And I think translating in and out of

15:48.720 --> 15:58.160
them wouldn't be that crazy. It was difficult to compile like DNA to Python and then to see.

15:59.120 --> 16:02.560
There is a little bit of a gap in the kind of languages we use for

16:04.720 --> 16:10.880
touring machines and the kind of languages nature seems to use a little bit. Maybe that's just,

16:11.520 --> 16:16.240
we just haven't understood the kind of language that nature uses well yet.

16:16.240 --> 16:25.200
DNA is a CAD model. It's not quite a programming language. It has no sort of serial execution.

16:25.200 --> 16:32.400
It's not quite a CAD model. So I think in that sense, we actually completely understand it.

16:32.400 --> 16:39.360
The problem is simulating on these CAD models. I played with it a bit this year. It's super

16:39.360 --> 16:44.800
computationally intensive. If you want to go down to the molecular level where you need to go to see

16:44.800 --> 16:52.080
a lot of these phenomenon like protein folding. So yeah, it's not that we don't understand it.

16:52.080 --> 16:54.400
It just requires a whole lot of compute to kind of compile it.

16:55.040 --> 17:00.400
For human minds, it's inefficient both for the data representation and for the programming.

17:00.400 --> 17:04.960
Yeah, it runs well on raw nature. It runs well on raw nature. And when we try to build

17:04.960 --> 17:09.360
emulators or simulators for that, well, they're mad slow and I've tried it.

17:10.480 --> 17:15.840
It runs in that, yeah, you've commented elsewhere. I don't remember where that

17:16.160 --> 17:23.920
one of the problems is simulating nature is tough. And if you want to sort of deploy a prototype,

17:25.440 --> 17:30.240
I forgot how you put it, but it made me laugh. But animals or humans would need to be involved

17:31.680 --> 17:40.480
in order to try to run some prototype code. If we're talking about COVID and viruses and so on,

17:41.120 --> 17:45.760
if you were to try to engineer some kind of defense mechanisms like a vaccine

17:47.280 --> 17:52.480
against COVID or all that kind of stuff, that doing any kind of experimentation like you can

17:52.480 --> 17:59.520
with autonomous vehicles would be very technically and ethically costly.

17:59.520 --> 18:05.440
I'm not sure about that. I think you can do tons of crazy biology and test tubes. I think

18:05.440 --> 18:09.680
my bigger complaint is more, all the tools are so bad.

18:11.360 --> 18:16.480
Like literally, you mean like like libraries and I'm not pipetting shit. Like your hand

18:16.480 --> 18:25.200
of me, I gotta, no, no, no, no, there has to be some like automating stuff. And like the

18:25.920 --> 18:31.520
yeah, but human biology is messy. Like it seems like, look at those Duranos videos. They were

18:31.520 --> 18:35.760
joke. It's like a little gantry. It's like a little XY gantry high school science project

18:35.760 --> 18:40.240
with the pipet. I'm like, really? Gotta be something better. You can't build like nice

18:40.240 --> 18:46.400
microfluidics and I can program the computation to biointerface. I mean, this is going to happen.

18:47.040 --> 18:53.360
But like right now, if you are asking me to pipet 50 milliliters of solution, I'm out.

18:54.240 --> 19:01.200
This is so crude. Yeah. Okay. Let's get all the crazy out of the way. So a bunch of people

19:01.200 --> 19:06.800
asked me, since we talked about the simulation last time, we talked about hacking the simulation.

19:06.800 --> 19:11.520
Do you have any updates, any insights about how we might be able to go about

19:12.400 --> 19:15.920
hacking simulation if we indeed do live in a simulation?

19:17.040 --> 19:22.800
I think a lot of people misinterpreted the point of that South by talk. The point of the

19:22.800 --> 19:27.360
South by talk was not literally to hack the simulation. I think that this

19:28.240 --> 19:35.440
is an idea is literally just I think theoretical physics. I think that's the whole

19:38.880 --> 19:43.120
goal. You want your grand unified theory, but then, okay, build a grand unified theory,

19:43.120 --> 19:49.280
search for exploits. I think we're nowhere near actually there yet. My hope with that was just

19:49.280 --> 19:55.200
more to like, are your people kidding me with the things you spend time thinking about? Do you

19:55.200 --> 20:01.760
understand kind of how small you are? You are bites and God's computer, really?

20:02.400 --> 20:10.800
And the things that people get worked up about. So basically, it was more a message of we should

20:10.800 --> 20:22.320
humble ourselves. What are we humans in this byte code? Yeah. And not just humble ourselves,

20:22.320 --> 20:27.200
but I'm not trying to make people feel guilty or anything like that. I'm trying to say literally,

20:27.200 --> 20:31.600
look at what you are spending time on, right? What are you referring to? You're referring to

20:31.600 --> 20:35.920
the Kardashians? What are we talking about? I'm referring to, no, the Kardashians,

20:35.920 --> 20:43.840
everyone knows that's kind of fun. I'm referring more to the economy, this idea that

20:44.400 --> 20:54.720
we got to up our stock price. Or what is the goal function of humanity?

20:55.280 --> 20:59.360
You don't like the game of capitalism? You don't like the games we've constructed for

20:59.360 --> 21:04.720
ourselves as humans? I'm a big fan of capitalism. I don't think that's really the game we're playing

21:04.720 --> 21:08.320
right now. I think we're playing a different game where the rules are rigged.

21:09.040 --> 21:14.240
Okay, which games are interesting to you that we humans have constructed and which

21:14.240 --> 21:21.840
aren't? Which are productive and which are not? Actually, maybe that's the real point of the talk.

21:21.840 --> 21:27.680
It's like, stop playing these fake human games. There's a real game here. We can play the real

21:27.680 --> 21:34.400
game. The real game is nature wrote the rules. This is a real game. There still is a game to play.

21:35.040 --> 21:38.320
But if you look at, sorry to interrupt, I don't know if you've seen the Instagram account,

21:38.320 --> 21:46.240
Nature is Metal. The game that nature seems to be playing is a lot more cruel than we humans

21:46.240 --> 21:52.240
want to put up with. Or at least we see it as cruel. It's like the bigger thing eats the smaller

21:52.240 --> 21:59.680
thing and does it to impress another big thing so it can mate with that thing.

22:00.320 --> 22:03.200
And that's it. That seems to be the entirety of it.

22:04.000 --> 22:10.720
Well, there's no art. There's no music. There's no comma AI. There's no comma one,

22:10.720 --> 22:16.160
no comma two, no George Hott's with his brilliant talks at South by Southwest.

22:16.880 --> 22:21.520
I disagree though. I disagree that this is what nature is. I think nature just provided

22:22.320 --> 22:30.800
basically a open world MMORPG. And here it's open world. I mean, if that's the game you want to

22:30.800 --> 22:36.160
play, you can play that game. Isn't that beautiful? I know if you play Diablo, they used to have,

22:36.160 --> 22:43.760
I think, cow level where it's, so everybody will go just, they figured out this,

22:44.320 --> 22:50.720
like the best way to gain like experience points is to just slaughter cows over and over and over.

22:52.160 --> 22:57.600
And so they figured out this little sub game within the bigger game that this is the most

22:57.600 --> 23:02.560
efficient way to get experience points. And everybody somehow agreed that getting experience

23:02.560 --> 23:08.000
points in RPG context where you always want to be getting more stuff, more skills, more levels,

23:08.000 --> 23:14.960
keep advancing. That seems to be good. So might as well spend sacrifice, actual enjoyment of

23:15.040 --> 23:22.240
playing a game, exploring a world and spending like hundreds of hours of your time in cow level.

23:22.240 --> 23:28.320
I mean, the number of hours I spent in cow level, I'm not like the most impressive person because

23:28.320 --> 23:33.920
people have probably thousands of hours there, but it's ridiculous. So that's a little absurd game

23:33.920 --> 23:39.920
that brought me joints and weird dopamine drug kind of way. So you don't like those games.

23:40.640 --> 23:47.200
You don't think that's us humans failing the nature. I think so.

23:47.200 --> 23:51.280
And that was the point of the talk. Yeah. So how do we hack it then?

23:51.280 --> 23:55.680
Well, I want to live forever. And wait, I want to live forever.

23:55.680 --> 23:57.840
And this is the goal. Well, that's a game against nature.

23:59.120 --> 24:02.320
Yeah. Immortality is the good objective function to you.

24:03.360 --> 24:06.000
I mean, start there and then you can do whatever else you want because you got a long time.

24:06.960 --> 24:11.280
What if immortality makes the game just totally not fun? I mean,

24:11.280 --> 24:18.080
like why do you assume immortality is somehow a good objective function?

24:18.080 --> 24:22.480
It's not immortality that I want. A true immortality where I could not die.

24:22.480 --> 24:26.560
I would prefer what we have right now. But I want to choose my own death, of course.

24:27.920 --> 24:31.440
I don't want nature to decide when I die. I'm going to win. I'm going to be you.

24:31.840 --> 24:40.080
And then at some point, if you choose commit suicide, how long do you think you'd live?

24:41.440 --> 24:42.240
Until I get bored?

24:42.960 --> 24:49.200
See, I don't think people, brilliant people like you that really ponder

24:50.240 --> 24:57.680
living a long time are really considering how meaningless life becomes.

24:58.400 --> 25:00.320
Well, I want to know everything and then I'm ready to die.

25:02.320 --> 25:07.360
But why do you want, isn't it possible that you want to know everything because

25:08.080 --> 25:13.200
it's finite? Like the reason you want to know quote unquote everything is because you

25:13.200 --> 25:19.440
don't have enough time to know everything. And once you have unlimited time, then you realize

25:19.440 --> 25:23.760
like why do anything? Like why learn anything?

25:24.880 --> 25:26.960
I don't want to know everything and then I'm ready to die.

25:27.120 --> 25:33.760
It's a terminal value. It's not in service of anything else.

25:34.640 --> 25:39.440
I'm conscious of the possibility. This is not a certainty. But the possibility of

25:39.440 --> 25:49.680
that engine of curiosity that you're speaking to is actually a symptom of the finiteness of life.

25:49.680 --> 25:56.640
Like without that finiteness, your curiosity would vanish like a morning fog.

25:57.680 --> 25:59.200
Okosky talked about love like that.

26:00.080 --> 26:03.840
Let me solve immortality. Let me change the thing in my brain that reminds me of the fact

26:03.840 --> 26:07.600
that I'm immortal tells me that life is finite shit. Maybe I'll have it tell me that life ends

26:07.600 --> 26:14.320
next week. I'm okay with some self manipulation like that. I'm okay with deceiving myself.

26:14.320 --> 26:17.120
Oh, Rika, changing the code.

26:17.120 --> 26:21.680
If that's the problem, right? If the problem is that I will no longer have that curiosity,

26:21.680 --> 26:28.000
I'd like to have backup copies of myself which I check in with occasionally to make sure they're

26:28.000 --> 26:32.320
okay with the trajectory and they can kind of override it. Maybe a nice like I think of like

26:32.320 --> 26:35.040
those wave nets, those like logarithmic go back to the copies.

26:35.040 --> 26:40.880
But sometimes it's not reversible. I've done this with video games. Once you figure out the

26:40.880 --> 26:46.640
cheat code or like you look up how to cheat old school like single player, it ruins the game for

26:46.720 --> 26:51.760
you. Absolutely. I know that feeling. But again, that just means our brain manipulation

26:51.760 --> 26:54.560
technology is not good enough yet. Remove that cheat code from your brain.

26:55.600 --> 27:02.720
So it's also possible that if we figure out immortality that all of us will kill ourselves

27:03.360 --> 27:08.640
before we advance far enough to be able to revert the change.

27:08.640 --> 27:10.240
I'm not killing myself till I know everything.

27:11.760 --> 27:14.720
That's what you say now because your life is finite.

27:15.680 --> 27:21.360
You know, I think self modifying systems comes up with all these hairy complexities and

27:21.360 --> 27:25.440
can I promise that I'll do it perfectly? No, but I think I can put good safety structures in place.

27:27.120 --> 27:30.720
So that talk in your thinking here is not literally

27:33.120 --> 27:41.360
referring to a simulation in that our universe is a kind of computer program running in a computer.

27:42.080 --> 27:49.040
That's more of a thought experiment. Do you also think of the potential of the sort of

27:50.560 --> 27:59.520
Bostrom, Elon Musk, and others that talk about an actual program that simulates our universe?

27:59.520 --> 28:05.120
Oh, I don't doubt that we're in a simulation. I just think that it's not quite that important.

28:05.120 --> 28:08.880
I mean, I'm interested only in simulation theory as far as like it gives me power over nature.

28:09.680 --> 28:12.960
If it's totally unfalsifiable, then who cares?

28:12.960 --> 28:17.040
I mean, what do you think that experiment would look like? Like somebody on Twitter asks

28:18.160 --> 28:22.800
George what signs we would look for to know whether or not we're in the simulation,

28:22.800 --> 28:29.680
which is exactly what you're asking is like the step that precedes the step of knowing how to

28:29.680 --> 28:35.120
get more power from this knowledge is to get an indication that there's some power to be gained.

28:35.840 --> 28:42.000
Get an indication that you can discover and exploit cracks in the simulation,

28:42.000 --> 28:44.960
or it doesn't have to be in the physics of the universe.

28:46.640 --> 28:50.240
Show me. I mean, like a memory leak would be cool.

28:51.520 --> 28:55.200
Like some scrying technology. What kind of technology?

28:55.200 --> 29:01.840
Scrying. What's that? That's a weird. Scrying is the paranormal ability to

29:02.800 --> 29:06.720
like remote viewing, like being able to see somewhere where you're not.

29:08.160 --> 29:14.000
So, I don't think you can do it by chanting in a room, but if we could find, it's a memory leak,

29:14.000 --> 29:19.920
basically. It's a memory leak. Yeah, you're able to access parts you're not supposed to.

29:19.920 --> 29:22.000
Yeah, yeah, yeah. And thereby discover shortcut.

29:22.000 --> 29:25.280
Yeah, memory leak means the other thing as well. But I mean like, yeah,

29:25.280 --> 29:29.760
like an ability to read arbitrary memory. And that one's not that horrifying.

29:29.760 --> 29:34.720
The right ones start to be horrifying. Read it right. So, the reading is not the problem.

29:34.720 --> 29:39.680
Yeah, it's like Heartbleed for the universe. Oh boy, the writing is a big, big problem.

29:40.560 --> 29:46.080
It's a big problem. It's the moment you can write anything, even if it's just random noise.

29:47.600 --> 29:52.480
That's terrifying. I mean, even without, even without that, like even some of the, you know,

29:52.480 --> 29:58.800
the nanotech stuff that's coming, I think. I don't know if you're paying attention, but

29:58.880 --> 30:03.840
actually Eric Weinstein came out with the theory of everything. I mean, that came out. He's been

30:03.840 --> 30:08.800
working on a theory of everything in the physics world called geometric unity. And then for me,

30:08.800 --> 30:14.480
from computer science person, like you, Stephen Wolfram's theory of everything of like,

30:14.480 --> 30:19.760
hypergraphs is super interesting and beautiful. But not from a physics perspective, but from a

30:19.760 --> 30:22.720
computational perspective. I don't know, have you paid attention to any of that?

30:23.040 --> 30:28.640
So, again, like what would make me pay attention and like why like a hate string theory is,

30:29.360 --> 30:34.640
okay, make a testable prediction, right? I'm only interested in, I'm not interested in theories

30:34.640 --> 30:38.160
for their intrinsic beauty. I'm interested in theories that give me power over the universe.

30:39.760 --> 30:42.080
So, if these theories do, I'm very interested.

30:42.960 --> 30:48.080
Can I just say how beautiful that is? Because a lot of physicists say, I'm interested in

30:48.160 --> 30:54.320
experimental validation. And they skip out the part where they say, to give me more power in

30:54.320 --> 31:02.800
the universe. I just love the clarity of that. I want 100 gigahertz processors. I want transistors

31:02.800 --> 31:12.240
that are smaller than atoms. I want like power. That's true. And that's where people from aliens

31:12.320 --> 31:18.160
to this kind of technology where people are worried that governments, like who owns that power?

31:19.200 --> 31:25.760
Is it George Haatz? Is it thousands of distributed hackers across the world? Is it governments?

31:26.560 --> 31:33.840
You know, is it Mark Zuckerberg? There's a lot of people that, I don't know if anyone trusts

31:33.840 --> 31:36.560
anyone individual with power. So, they're always worried.

31:37.120 --> 31:38.400
It's the beauty of blockchains.

31:39.200 --> 31:43.840
That's the beauty of blockchains, which we'll talk about on Twitter.

31:43.840 --> 31:49.200
Somebody pointed me to a story, a bunch of people pointed me to a story a few months ago,

31:49.200 --> 31:53.040
where you went into a restaurant in New York, and you can correct me if I'm saying this is wrong,

31:53.680 --> 32:00.320
and ran into a bunch of folks from a company in a crypto company who are trying to scale up Ethereum.

32:01.760 --> 32:06.560
And they had a technical deadline related to a solidity to OVM compiler.

32:07.280 --> 32:13.200
So, these are all Ethereum technologies. So, you stepped in, they recognized you,

32:14.640 --> 32:18.320
pulled you aside, explained their problem, and you stepped in and helped them solve the problem,

32:19.520 --> 32:29.520
thereby creating legend status story. So, can you tell me the story a little more detail? It seems

32:29.520 --> 32:34.320
kind of incredible. Did this happen? Yeah, yeah, it's a true story. It's a true story. I mean,

32:34.400 --> 32:43.680
they wrote a very flattering account of it. So, the company is called Optimism,

32:43.680 --> 32:48.400
spin-off of Plasma. They're trying to build L2 solutions on Ethereum. So, right now,

32:50.800 --> 32:55.680
every Ethereum node has to run every transaction on the Ethereum network.

32:56.560 --> 33:00.640
And this kind of doesn't scale, right? Because if you have N computers, well,

33:00.640 --> 33:04.160
if that becomes two N computers, you actually still get the same amount of compute.

33:04.960 --> 33:11.440
Right? This is like O of one scaling because they all have to run it. Okay, fine. You get more

33:11.440 --> 33:16.320
blockchain security, but like, blockchain is already so secure. Can we trade some of that off

33:16.320 --> 33:22.080
for speed? So, that's kind of what these L2 solutions are. They built this thing, which kind of

33:23.200 --> 33:29.040
sandbox for Ethereum contracts. So, they can run it in this L2 world, and it can't do certain

33:29.040 --> 33:35.760
things in L1. Can I ask you for some definitions? What's L2? Oh, L2 is Layer 2. So, L1 is like the

33:35.760 --> 33:41.600
base Ethereum chain, and then Layer 2 is like a computational layer that runs

33:43.360 --> 33:49.600
elsewhere, but still is kind of secured by Layer 1. And I'm sure a lot of people know,

33:49.600 --> 33:54.080
but Ethereum is a cryptocurrency, probably one of the most popular cryptocurrency, second to

33:54.080 --> 34:01.200
Bitcoin, and a lot of interesting technological innovations there. Maybe you can also slip in

34:01.840 --> 34:06.240
whenever you talk about this, any things that are exciting to you in the Ethereum space.

34:06.240 --> 34:13.200
And why Ethereum? Well, I mean, Bitcoin is not turned complete. Ethereum is not technically

34:13.200 --> 34:18.000
turned complete with the gas limit, but close enough. With the gas limit? What's the gas limit?

34:18.960 --> 34:20.800
Yeah, I mean, no computer is actually turned complete.

34:22.960 --> 34:25.520
They're just fine at RAM. I can actually solve the whole problem.

34:25.520 --> 34:29.280
What's the word gas limit? You just have so many brilliant words. I'm not even gonna ask.

34:29.280 --> 34:31.600
Well, that's not my word. That's Ethereum's word.

34:32.560 --> 34:37.040
Ethereum, you have to spend gas per instruction. So, different op codes use different amounts of

34:37.040 --> 34:41.920
gas, and you buy gas with Ether to prevent people from basically de-dossing the network.

34:42.640 --> 34:46.960
So, Bitcoin is proof of work, and then what's Ethereum?

34:47.040 --> 34:50.960
It's also proof of work. They're working on some proof of stake Ethereum 2.0 stuff,

34:50.960 --> 34:54.640
but right now, it's proof of work. It uses a different hash function from Bitcoin

34:54.640 --> 34:56.560
that's more ASIC resistance because you need RAM.

34:57.280 --> 35:02.880
So, we're all talking about Ethereum 1.0. So, what were they trying to do to scale

35:02.880 --> 35:06.960
this whole process? So, they were like, well, if we could run contracts elsewhere,

35:07.920 --> 35:13.680
and then only save the results of that computation, well, we don't actually have

35:13.680 --> 35:16.800
to do the compute on the chain. We can do the compute off chain and just post what the results

35:16.800 --> 35:21.120
are. Now, the problem with that is, well, somebody could lie about what the results are.

35:21.120 --> 35:25.520
So, you need a resolution mechanism, and the resolution mechanism can be really expensive

35:26.400 --> 35:32.320
because you just have to make sure that the person who is saying, look, I swear that this

35:32.320 --> 35:39.120
is the real computation. I'm staking $10,000 on that fact, and if you prove it wrong, yeah,

35:39.120 --> 35:44.640
it might cost you $3,000 in gas fees to prove wrong, but you'll get the $10,000 bounty.

35:44.640 --> 35:50.480
So, you can secure using those kind of systems. So, it's effectively a sandbox,

35:50.480 --> 35:56.720
which runs contracts, and like just like any kind of normal sandbox, you have to like replace

35:56.720 --> 36:00.320
syscalls with, you know, calls into the hypervisor.

36:02.880 --> 36:05.920
Sandbox, syscalls, hypervisor, what do these things mean?

36:07.520 --> 36:09.040
As long as it's interesting to talk about.

36:09.040 --> 36:12.640
Yeah, I mean, you can take like the Chrome sandbox is maybe the one to think about, right?

36:12.720 --> 36:17.680
So, the Chrome process is doing a rendering. I can't, for example, read a file from the file

36:17.680 --> 36:23.920
system. If it tries to make an open syscall in Linux, you can't make an open syscall. No, no,

36:23.920 --> 36:31.040
no. You have to request from the kind of hypervisor process or like, I don't know what's

36:31.040 --> 36:37.440
called in Chrome, but the, hey, could you open this file for me? And then it does all these checks,

36:37.440 --> 36:41.600
and then it passes the file, handle back in if it's approved. So, that's yeah.

36:43.040 --> 36:47.600
What's the, in the context of Ethereum, what are the boundaries of the sandbox that we're talking

36:47.600 --> 36:54.160
about? Well, like one of the calls that you actually reading and writing any state to the

36:54.160 --> 37:01.200
Ethereum contract or to the Ethereum blockchain. Writing state is one of those calls that you're

37:01.200 --> 37:08.240
going to have to sandbox in layer two, because if you let layer two just arbitrarily write to the

37:08.240 --> 37:15.520
Ethereum blockchain. So, layer two is really sitting on top of layer one. So, you're going to

37:15.520 --> 37:20.320
have a lot of different kinds of ideas that you can play with. And they're all, they're not fundamentally

37:20.320 --> 37:28.800
changing the source code level of Ethereum. Well, you have to replace a bunch of calls

37:28.800 --> 37:35.360
with calls into the hypervisor. So, instead of doing the syscall directly, you replace it with a

37:35.360 --> 37:42.400
call to the hypervisor. So, originally they were doing this by first running the, so solidity is

37:42.400 --> 37:47.760
the language that most Ethereum contracts are written in, it compiles to a bytecode. And then

37:47.760 --> 37:52.800
they wrote this thing they called the transpiler. And the transpiler took the bytecode and it

37:52.800 --> 37:58.160
transpiled it into OVM safe bytecode, basically bytecode that didn't make any of those restricted

37:58.160 --> 38:04.240
syscalls and added the calls to the hypervisor. This transpiler was a 3000 line mess.

38:05.520 --> 38:09.440
And it's hard to do. It's hard to do if you're trying to do it like that because you have to

38:09.440 --> 38:15.760
kind of like deconstruct the bytecode, change things about it, and then reconstruct it. And

38:15.760 --> 38:18.960
I mean, as soon as I hear this, I'm like, well, why don't you just change the compiler?

38:19.760 --> 38:23.280
Right? Why not the first place you build the bytecode, just do it in the compiler?

38:25.520 --> 38:30.960
So, yeah, you know, I asked them how much they wanted it. Of course, we're measured in dollars

38:30.960 --> 38:37.520
and I'm like, well, okay. And yeah, you wrote the compiler. Yeah, I modified, I wrote a 300

38:37.520 --> 38:42.400
line diff to the compiler. It's open source, you can look at it. Yeah, I looked at the code last

38:42.400 --> 38:55.920
night. Yeah, exactly. Qt is a good word for it. And it's C++. C++, yeah. So when asked how you

38:55.920 --> 39:03.760
were able to do it, you said, you just got to think and then do it right. So can you break that

39:03.760 --> 39:08.800
apart a little bit? What's your process of one thinking and two doing it right?

39:09.760 --> 39:14.080
You know, the people who I was working for are amused that I said that it doesn't really mean

39:14.080 --> 39:22.560
anything. Okay. I mean, is there some deep profound insights to draw from like how you problem solve

39:22.560 --> 39:26.640
from that? This is always what I say. I'm like, do you want to be a good programmer? Do it for 20

39:26.640 --> 39:36.240
years? Yeah, there's no shortcuts. What are your thoughts on crypto in general? So what parts

39:36.240 --> 39:41.600
technically or philosophically defined, especially beautiful, maybe? Oh, I'm extremely bullish on

39:41.600 --> 39:50.560
crypto long term, not any specific crypto project, but this idea of, well, two ideas. One,

39:51.360 --> 39:58.480
the Nakamoto consensus algorithm is I think one of the greatest innovations of the 21st century.

39:58.480 --> 40:04.480
This idea that people can reach consensus, you can reach a group consensus using a relatively

40:04.480 --> 40:14.880
straightforward algorithm is wild. And like Satoshi Nakamoto, people always ask me who

40:14.880 --> 40:21.360
I look up to. It's like, whoever that is. Who do you think it is? Elon Musk? Is it you?

40:22.160 --> 40:30.240
It is definitely not me. And I do not think it's Elon Musk. But yeah, this idea of groups

40:30.240 --> 40:37.440
reaching consensus in a decentralized yet formulaic way is one extremely powerful idea from crypto.

40:38.080 --> 40:48.400
Maybe the second idea is this idea of smart contracts. When you write a contract between

40:48.400 --> 40:55.120
two parties, any contract, this contract, if there are disputes, it's interpreted by lawyers.

40:56.080 --> 41:02.560
Lawyers are just really shitty overpaid interpreters. Let's talk about them in terms of like,

41:02.560 --> 41:09.920
let's compare a lawyer to Python. Well, okay. That's brilliant. I never thought of it that way.

41:09.920 --> 41:17.040
It's hilarious. So Python, I'm paying even 10 cents an hour. I'll use the nice Azure machine.

41:17.040 --> 41:24.880
I can run Python for 10 cents an hour. Lawyers cost $1,000 an hour. So Python is 10,000x better

41:24.880 --> 41:32.080
on that axis. Lawyers don't always return the same answer. Python almost always does.

41:36.640 --> 41:42.720
Cost. Yeah. I mean, just cost, reliability, everything about Python is so much better than

41:42.720 --> 41:52.400
lawyers. So if you can make smart contracts, this whole concept of code is law. I would love to

41:52.400 --> 42:00.000
live in a world where everybody accepted that fact. So maybe you can talk about what smart contracts

42:00.000 --> 42:13.040
are. So let's say we have even something as simple as a safety deposit box. Safety deposit box that

42:13.040 --> 42:18.640
holds a million dollars. I have a contract with the bank that says two out of these three parties

42:19.600 --> 42:25.840
must be present to open the safety deposit box and get the money out. So that's a contract with

42:25.840 --> 42:32.640
the bank. And it's only as good as the bank and the lawyers. Let's say somebody dies and now,

42:32.640 --> 42:36.400
oh, we're going to go through a big legal dispute about whether, oh, was it in the will? Was it not

42:36.400 --> 42:45.120
in the will? It's just so messy. And the cost to determine truth is so expensive versus a smart

42:45.200 --> 42:48.880
contract, which just uses cryptography to check if two out of three keys are present.

42:50.000 --> 42:55.760
Well, I can look at that and I can have certainty in the answer that it's going to return.

42:55.760 --> 42:59.520
And that's what all businesses want is certainty. You know, they say businesses don't care,

42:59.520 --> 43:04.720
Viacom YouTube. YouTube's like, look, we don't care which way this lawsuit goes. Just please

43:04.720 --> 43:10.320
tell us so we can have certainty. I wonder how many agreements in this, because we're talking about

43:10.400 --> 43:16.160
financial transactions only in this case, correct? The smart contracts. Oh, you can go to,

43:16.160 --> 43:19.360
you can go to anything. You can go, you can put a prenup in the theorem blockchain.

43:21.840 --> 43:26.640
Married smart contract. Sorry, divorce lawyer. Sorry, you're going to be replaced by Python.

43:29.520 --> 43:37.120
Okay, so that's, so that's another beautiful idea. Do you think there's something that's

43:37.120 --> 43:43.760
appealing to you about any one specific implementation? So if you look 10, 20, 50 years

43:43.760 --> 43:50.080
down the line, do you see any like Bitcoin, Ethereum, any of the other hundreds of cryptocurrencies

43:50.080 --> 43:54.320
winning out? Is there like, what's your intuition about the space? Are you just sitting back and

43:54.320 --> 43:59.040
watching the chaos and look who cares what emerges? Oh, I don't, I don't speculate. I don't

43:59.040 --> 44:03.760
really care. I don't really care which one of these projects wins. I'm kind of in the Bitcoin

44:03.760 --> 44:09.680
as a meme coin camp. I mean, why does Bitcoin have value? It's technically kind of, you know,

44:11.600 --> 44:16.080
not great. Like the block size debate, or when I found out what the block size debate was, I'm

44:16.080 --> 44:22.640
like, are you guys kidding? What's the block size debate? You know what, it's really, it's too stupid

44:22.640 --> 44:28.960
to even talk. People can look it up, but I'm like, wow, you know, Ethereum seems, the governance of

44:28.960 --> 44:35.680
Ethereum seems much better. I've come around a bit on proof of stake ideas. You know, very

44:35.680 --> 44:41.200
smart people thinking about some things. Yeah, you know, governance is interesting. It does feel

44:41.200 --> 44:49.280
like Vitalik, it could just feel like an open, even in these distributed systems, leaders are

44:49.280 --> 44:56.960
helpful because they kind of help you drive the mission and the vision. And they put a face to

44:56.960 --> 45:03.600
a project. It's a weird thing about us humans. Geniuses are helpful, like Vitalik. Yeah, brilliant.

45:06.480 --> 45:13.440
Leaders are not necessarily, yeah. So you think the reason he's the face

45:14.560 --> 45:19.280
of Ethereum is because he's a genius. That's interesting. I mean, that was,

45:20.240 --> 45:28.800
it's interesting to think about that we need to create systems in which the quote unquote leaders

45:28.800 --> 45:35.200
that emerge are the geniuses in the system. I mean, that's arguably why the current state of

45:35.200 --> 45:40.240
democracy is broken is the people who are emerging as the leaders are not the most competent, are

45:40.240 --> 45:46.160
not the superstars of the system. And it seems like at least for now in the crypto world, oftentimes

45:47.120 --> 45:50.880
the leaders are the superstars. Imagine at the debate, they asked,

45:51.600 --> 45:55.440
what's the sixth amendment? What are the four fundamental forces in the universe?

45:56.080 --> 46:01.920
What's the integral of two to the X? Yeah, I'd love to see those questions asked. And that's

46:01.920 --> 46:09.680
what I want as our leader. It's a little bit. What's Bayes rule? Yeah, I mean, even, oh, wow,

46:09.760 --> 46:16.480
you're hurting my brain. My standard was even lower, but I would have loved to see

46:17.600 --> 46:23.040
just this basic brilliance, like I've talked to historians. There's just these,

46:23.040 --> 46:27.120
they're not even like, they don't have a PhD or even education history. They just

46:27.760 --> 46:34.320
like a Dan Carlin type character who just like, holy shit, how did all this information get

46:34.320 --> 46:40.240
into your head? They were able to just connect Genghis Khan to the entirety of the history of

46:40.240 --> 46:46.800
the 20th century. They know everything about every single battle that happened. And they know the

46:48.480 --> 46:55.040
like the game of thrones of the different power plays and all that happened there.

46:55.040 --> 47:00.960
And they know like the individuals and all the documents involved. And they integrate that into

47:01.040 --> 47:06.240
their regular life. It's not like they're ultra history nerds. They're just, they know this information.

47:06.240 --> 47:10.720
That's what competence looks like. Yeah. Because I've seen that with programmers too, right? That's

47:10.720 --> 47:16.640
what great programmers do. But yeah, it'll be, it's really unfortunate that those kinds of people

47:16.640 --> 47:22.480
aren't emerging as our leaders. But for now, at least in the crypto world, that seems to be the

47:22.480 --> 47:28.320
case. I don't know if that always, you could imagine that in 100 years, it's not the case,

47:28.800 --> 47:32.960
crypto world has one very powerful idea going for it. And that's the idea of forks.

47:34.880 --> 47:44.000
I mean, imagine we'll use a less controversial example. This was actually in my joke

47:46.080 --> 47:50.720
app in 2012. I was like, Barack Obama, Mitt Romney, let's let them both be president.

47:50.720 --> 47:54.720
All right. Like imagine we could fork America and just let them both be president. And then

47:54.720 --> 47:59.360
the Americas could compete and people could invest in one, pull their liquidity out of one,

47:59.360 --> 48:04.160
put it in the other. You have this in the crypto world. Ethereum forks into Ethereum and Ethereum

48:04.160 --> 48:09.760
Classic. And you can pull your liquidity out of one and put it in another. And people vote with

48:09.760 --> 48:17.520
their dollars, which forks, companies should be able to fork. I'd love to fork Nvidia.

48:18.000 --> 48:25.520
Yeah. Like different business strategies. And then try them out and see what works.

48:26.080 --> 48:36.560
Like even take, yeah, take common AI that closes its source and then take one that's open source

48:36.560 --> 48:43.440
and see what works. Take one that's purchased by GM and one that remains Android Renegade

48:43.440 --> 48:47.360
in all these different versions and see the beauty of common AI. Someone can actually do that.

48:47.840 --> 48:52.160
Please take common AI and fork it. That's right. That's the beauty of open source.

48:52.960 --> 48:59.440
So you're, I mean, we'll talk about autonomous vehicle space, but it does seem that you're

49:01.120 --> 49:05.440
really knowledgeable about a lot of different topics. So the natural question a bunch of people

49:05.440 --> 49:11.600
ask this, which is how do you keep learning new things? Do you have like practical advice

49:12.320 --> 49:19.200
if you were to introspect like taking notes, allocate time, or do you just mess around and

49:19.200 --> 49:24.400
just allow your curiosity to drive? I'll write these people a self-help book and I'll charge $67

49:24.400 --> 49:31.680
for it. And I will write on the cover of the self-help book. All of this advice is completely

49:31.680 --> 49:36.800
meaningless. You're going to be a sucker and buy this book anyway. And the one lesson that I hope

49:36.800 --> 49:41.520
they take away from the book is that I can't give you a meaningful answer to that.

49:42.400 --> 49:50.080
That's interesting. Let me translate that as you haven't really thought about what it is you do

49:51.520 --> 49:55.840
systematically because you could reduce it. And then some people, I mean, I've met brilliant

49:55.840 --> 50:04.160
people that this is really clear with athletes. Some are just the best in the world at something

50:05.040 --> 50:11.440
and they have zero interest in writing like a self-help book or how to master this game.

50:11.440 --> 50:17.200
And then there are some athletes who become great coaches and they love the analysis,

50:17.200 --> 50:21.680
perhaps the over-analysis. And you right now, at least at your age, which is an interesting,

50:21.680 --> 50:25.920
you're in the middle of the battle. You're like the warriors that have zero interest in writing

50:25.920 --> 50:32.320
books. So you're in the middle of the battle. So yeah. This is a fair point. I do think I have

50:32.320 --> 50:39.520
a certain aversion to this kind of deliberate, intentional way of living life.

50:40.640 --> 50:45.120
You eventually, the hilarity of this, especially since this is recorded,

50:47.120 --> 50:50.640
it will reveal beautifully the absurdity when you finally do publish this book.

50:51.200 --> 50:58.720
That guarantee you will. The story of comma AI, maybe it'll be a biography written about you.

50:59.200 --> 51:02.880
That'll be better, I guess. And you might be able to learn some cute lessons if you're starting

51:02.880 --> 51:07.520
a company like comma AI from that book. But if you're asking generic questions like,

51:07.520 --> 51:14.160
how do I be good at things? Dude, I don't know. Well, I mean, the interesting- Do them a lot.

51:14.160 --> 51:21.760
Do them a lot. But the interesting thing here is learning things outside of your current trajectory,

51:21.760 --> 51:28.560
which is what it feels like from an outsider's perspective. I don't know if there's

51:29.200 --> 51:34.640
advice on that, but it is an interesting curiosity. When you become really busy,

51:34.640 --> 51:43.920
you're running a company. Hard time. Yeah. But there's a natural inclination and

51:45.040 --> 51:50.960
trend. Just the momentum of life carries you into a particular direction of wanting to focus.

51:50.960 --> 51:58.000
And this kind of dispersion that curiosity can lead to gets harder and harder with time.

51:58.560 --> 52:03.680
You get really good at certain things, and it sucks trying things that you're not good at,

52:03.680 --> 52:08.880
like trying to figure them out. I mean, you do this with your live streams. You're on the fly

52:08.880 --> 52:15.920
figuring stuff out. You don't mind looking dumb. No. You just figured out pretty quickly.

52:16.560 --> 52:20.880
Sometimes I try things and I don't figure them out quickly. My chest rating is like a 1400,

52:20.880 --> 52:25.840
despite putting like a couple hundred hours in. It's pathetic. I mean, to be fair, I know that

52:25.840 --> 52:30.960
I could do it better. If I did it better, don't play five minute games, play 15 minute games at

52:30.960 --> 52:36.080
least. I know these things, but it just doesn't. It doesn't stick nicely in my knowledge stream.

52:37.120 --> 52:43.360
All right. Let's talk about Kama AI. What's the mission of the company? Let's look at the biggest

52:43.360 --> 52:50.640
picture. Oh, I have an exact statement. Solve self-driving cars while delivering shipable intermediaries.

52:51.440 --> 52:57.520
So long-term vision is have fully autonomous vehicles and make sure you're making money along

52:57.520 --> 53:02.560
the way. I think it doesn't really speak to money, but I can talk about what solve self-driving cars

53:02.560 --> 53:08.400
means. Solve self-driving cars, of course, means you're not building a new car. You're building

53:08.400 --> 53:13.840
a person replacement. That person can sit in the driver's seat and drive you anywhere a person can

53:13.840 --> 53:19.200
drive with a human or better level of safety, speed, quality, comfort.

53:21.280 --> 53:23.040
And what's the second part of that?

53:23.040 --> 53:27.920
Delivering shipable intermediaries is, well, it's a way to fund the company. That's true,

53:27.920 --> 53:35.040
but it's also a way to keep us honest. If you don't have that, it is very easy with

53:35.840 --> 53:41.280
this technology to think you're making progress when you're not. I've heard it best described on

53:41.280 --> 53:48.960
Hacker News as you can set any arbitrary milestone, meet that milestone, and still be infinitely

53:48.960 --> 53:54.080
far away from solving self-driving cars. So it's hard to have real deadlines when you're

53:55.120 --> 54:06.880
cruise or Waymo when you don't have revenue. Is revenue essentially the thing we're talking

54:06.880 --> 54:12.640
about here? Capitalism is based around consent. Capitalism, the way that you get

54:12.640 --> 54:17.200
revenue is real capitalism. Common is in the real capitalism camp. There's definitely scams out there,

54:17.200 --> 54:21.200
but real capitalism is based around consent. It's based around this idea that if we're getting

54:21.200 --> 54:25.600
revenue, it's because we're providing at least that much value to another person. When someone buys

54:25.600 --> 54:30.080
$1,000 comma two from us, we're providing them at least $1,000 of value, but they wouldn't buy it.

54:30.080 --> 54:34.560
Brilliant. So can you give a world-wind overview of the products that Kamii provides?

54:34.880 --> 54:40.720
Like throughout its history and today? I mean, yeah, the past ones aren't really that interesting.

54:40.720 --> 54:47.440
It's kind of just been refinement of the same idea. The real only product we sell today is the comma

54:47.440 --> 54:54.560
two, which is a piece of hardware with cameras. So the comma two, I mean, you can think about it

54:54.560 --> 54:59.360
kind of like a person. You know, in future hardware will probably be even more and more person-like.

55:00.160 --> 55:08.320
So it has eyes, ears, a mouth, a brain, and a way to interface with the car.

55:09.280 --> 55:12.560
Does it have consciousness? Just kidding. That was a trick question.

55:13.280 --> 55:16.240
I don't have consciousness either. Me and the comma two are the same.

55:16.240 --> 55:16.960
They are the same.

55:16.960 --> 55:20.880
I have a little more compute than it. It only has like the same compute as a B.

55:23.120 --> 55:26.320
You're more efficient energy-wise for the compute you're doing.

55:26.400 --> 55:30.480
Far more efficient energy-wise. 20 pay-to-flops, 20 wants, crazy.

55:30.480 --> 55:32.480
Do you lack consciousness? Sure.

55:32.480 --> 55:35.280
Do you fear death? You do. You want immortality.

55:35.280 --> 55:35.840
Of course I fear death.

55:35.840 --> 55:38.640
Does Kamii fear death? I don't think so.

55:39.360 --> 55:43.280
Of course it does. It very much fears, well, it fears negative loss. Oh, yeah.

55:45.600 --> 55:50.800
Okay. So comma two, when did that come out? That was a year ago? No, two.

55:51.760 --> 55:52.640
Early this year.

55:53.440 --> 56:00.560
Wow, time, it feels, yeah. 2020 feels like it's taken 10 years to get to the end.

56:00.560 --> 56:01.680
It's a long year.

56:01.680 --> 56:09.040
It's a long year. So what's the sexiest thing about comma two, feature-wise?

56:09.840 --> 56:15.600
So, I mean, maybe you can also link on like, what is it? Like, what's its purpose?

56:15.600 --> 56:20.560
Because there's a hardware, there's a software component. You've mentioned the sensors, but

56:20.640 --> 56:22.960
also like, what is it, its features and capabilities?

56:22.960 --> 56:26.960
I think our slogan summarizes it well. A comma slogan is make driving chill.

56:28.800 --> 56:35.280
I love it. Okay. Yeah, I mean, it is, you know, if you like cruise control, imagine cruise control,

56:35.280 --> 56:42.000
but much, much more. So it can do adaptive cruise control things, which is like, slow down for

56:42.000 --> 56:47.120
cars in front of it, maintain a certain speed. And it can also do lane keeping, so stay in the

56:47.120 --> 56:51.680
lane and do it better and better and better over time. It's very much machine learning based.

56:53.040 --> 56:56.320
So there's cameras, there's a driver facing camera too.

57:01.040 --> 57:06.560
What else is there? What am I thinking? So the hardware versus software, so open pilot versus

57:06.560 --> 57:11.440
the actual hardware, the device. What's, can you draw that distinction? What's one? What's the other?

57:11.440 --> 57:15.840
I mean, the hardware is pretty much a cell phone with a few additions, a cell phone with a cooling

57:15.840 --> 57:24.240
system and with a car interface connected to it. And by cell phone, you mean like Qualcomm Snapdragon?

57:25.520 --> 57:31.760
Yeah, the current hardware is a Snapdragon 821. It has Wi-Fi radio, it has an LTE radio, it has a screen.

57:33.840 --> 57:38.400
We use every part of the cell phone. And then the interface of the car is specific to the car,

57:38.400 --> 57:42.800
so you keep supporting more and more cars? Yeah, so the interface to the car, I mean,

57:42.800 --> 57:47.200
the device itself just has four CAN buses, has four CAN interfaces on it, they're connected

57:47.200 --> 57:54.400
through the USB port to the phone. And then, yeah, on those four CAN buses, you connect it to the car

57:54.400 --> 57:59.280
and there's a little part to do this. Cars are actually surprisingly similar. So CAN is the

57:59.280 --> 58:04.480
protocol about which cars communicate and then you're able to read stuff and write stuff to be

58:04.480 --> 58:09.120
able to control the car depending on the car. So what's the software side? What's open pilot?

58:09.920 --> 58:14.480
So, I mean, open pilot is, the hardware is pretty simple compared to open pilot, open pilot is...

58:18.000 --> 58:24.960
Well, so you have a machine learning model, which it's an open pilot, it's a blob, it's just a blob

58:24.960 --> 58:28.720
of weights. It's not like people are like, oh, it's closed source, I'm like, it's a blob of weights,

58:28.720 --> 58:32.800
what do you expect? So it's primarily neural network based?

58:33.680 --> 58:37.920
Well, open pilot is all the software kind of around that neural network, that if you have

58:37.920 --> 58:42.240
a neural network that says, here's where you want to send the car, open pilot actually goes and

58:42.240 --> 58:48.000
executes all of that. It cleans up the input to the neural network, it cleans up the output and

58:48.000 --> 58:53.200
executes on it. So it's the glue that connects everything together. Runs the sensors, does a

58:53.200 --> 58:58.480
bunch of calibration for the neural network, does, you know, deals with like, you know, if the car

58:58.480 --> 59:03.360
is on a banked road, you have to counter steer against that. And the neural network can't necessarily

59:03.360 --> 59:09.120
know that by looking at the picture. So you can do that with other sensors, infusion and localizer.

59:09.760 --> 59:15.840
Open pilot also is responsible for sending the data up to our servers, so we can learn from it,

59:16.640 --> 59:20.640
logging it, recording it, running the cameras, thermally managing the device,

59:21.360 --> 59:24.640
managing the disk space on the device, managing all the resources on the device.

59:24.640 --> 59:29.280
So what, since we last spoke, I don't remember when, maybe a year ago, maybe a little bit longer,

59:30.080 --> 59:32.960
how has open pilot improved?

59:32.960 --> 59:36.560
We did exactly what I promised you. I promised you that by the end of the year,

59:36.560 --> 59:46.000
we would be able to remove the lanes. The lateral policy is now almost completely end to end. You

59:46.000 --> 59:50.880
can turn the lanes off and it will drive slightly worse on the highway if you turn the lanes off,

59:50.880 --> 59:56.720
but you can turn the lanes off and it will drive well trained completely end to end on user data.

59:57.280 --> 59:59.920
And this year we hope to do the same for the longitudinal policy.

59:59.920 --> 01:00:04.400
So that's the interesting thing is you're not doing, you don't appear to be,

01:00:04.400 --> 01:00:10.000
maybe you can correct me, you don't appear to be doing lane detection or lane marking

01:00:10.000 --> 01:00:15.040
detection or kind of the segmentation task or any kind of object detection task,

01:00:15.040 --> 01:00:18.560
you're doing what's traditionally more called like end to end learning.

01:00:19.360 --> 01:00:25.840
So entrained on actual behavior of drivers when they're driving the car manually.

01:00:27.600 --> 01:00:31.120
And this is hard to do. It's not supervised learning.

01:00:32.080 --> 01:00:35.840
Yeah, but so the nice thing is there's a lot of data, so it's hard and easy.

01:00:37.680 --> 01:00:39.920
We have a lot of high quality data, yeah.

01:00:39.920 --> 01:00:41.600
Like more than you need in the sun.

01:00:41.600 --> 01:00:44.800
Well, we've way more than we do. We've way more data than we need.

01:00:44.800 --> 01:00:49.600
I mean, it's an interesting question, actually, because in terms of amount, you have more than

01:00:49.680 --> 01:00:54.160
you need. But the, you know, driving is full of edge cases.

01:00:54.160 --> 01:00:56.880
So how do you select the data you train on?

01:00:58.160 --> 01:01:00.480
I think this is an interesting open question.

01:01:00.480 --> 01:01:04.080
Like what's the cleverest way to select data?

01:01:04.080 --> 01:01:06.240
That's the question Tesla is probably working on.

01:01:07.600 --> 01:01:09.840
That's, I mean, the entirety of machine learning can be,

01:01:09.840 --> 01:01:12.160
they don't seem to really care. They just kind of select data.

01:01:12.160 --> 01:01:16.160
But I feel like that if you want to solve, if you want to create intelligence systems,

01:01:16.160 --> 01:01:18.800
you have to pick data well, right?

01:01:18.800 --> 01:01:22.800
And so do you have any hints, ideas of how to do it well?

01:01:22.800 --> 01:01:28.400
So in some ways, that is the definition I like of reinforcement learning versus supervised learning.

01:01:29.200 --> 01:01:32.880
In supervised learning, the weights depend on the data, right?

01:01:34.400 --> 01:01:39.360
And this is obviously true, but the, in reinforcement learning, the data depends on the weights.

01:01:40.320 --> 01:01:40.800
Yeah.

01:01:40.800 --> 01:01:42.400
Right. And actually both ways.

01:01:42.400 --> 01:01:43.920
That's poetry.

01:01:43.920 --> 01:01:44.800
That's brilliant.

01:01:44.800 --> 01:01:46.160
How does it know what data to train on?

01:01:46.160 --> 01:01:47.360
Well, let it pick.

01:01:47.360 --> 01:01:49.440
We're not there yet, but that's the eventual.

01:01:49.440 --> 01:01:52.560
So you're thinking this almost like a reinforcement learning framework.

01:01:53.120 --> 01:01:54.320
We're going to do RL on the world.

01:01:55.280 --> 01:01:58.000
Every time a car makes a mistake, user disengages.

01:01:58.000 --> 01:02:00.000
We train on that and do RL on the world.

01:02:00.000 --> 01:02:01.760
Ship out a new model. That's an epoch, right?

01:02:03.200 --> 01:02:09.600
And for now, you're not doing the Elon style promising that it's going to be fully autonomous.

01:02:09.600 --> 01:02:14.400
You really are sticking to level two and like it's supposed to be supervised.

01:02:15.440 --> 01:02:18.240
It is definitely supposed to be supervised and we enforce the fact that it's supervised.

01:02:19.680 --> 01:02:23.520
We look at our rate of improvement in disengagements.

01:02:23.520 --> 01:02:26.640
OpenPilot now has an unplanned disengagement about every 100 miles.

01:02:27.280 --> 01:02:36.400
This is up from 10 miles, like maybe, maybe a, maybe a year ago.

01:02:36.400 --> 01:02:41.600
Yeah. So maybe we've seen 10x improvement in a year, but 100 miles is still a far cry

01:02:41.600 --> 01:02:43.200
from the 100,000 you're going to need.

01:02:43.760 --> 01:02:48.240
So you're going to somehow need to get three more 10x's in there.

01:02:49.760 --> 01:02:51.280
And you're, what's your intuition?

01:02:52.240 --> 01:02:55.440
You're basically hoping that there's exponential improvement built into the,

01:02:55.440 --> 01:02:56.720
baked into the cake somewhere.

01:02:56.720 --> 01:03:00.400
Well, that's even, I mean, 10x improvement, that's already assuming exponential, right?

01:03:00.400 --> 01:03:02.480
There's definitely exponential improvement.

01:03:02.480 --> 01:03:05.200
And I think when Elon talks about exponential, like these things,

01:03:05.200 --> 01:03:07.760
these systems are going to exponentially improve.

01:03:07.760 --> 01:03:12.800
Just exponential doesn't mean you're getting 100 gigahertz processors tomorrow, right?

01:03:13.200 --> 01:03:16.080
Like it's going to still take a while because the gap

01:03:16.080 --> 01:03:19.520
between even our best system and humans is still large.

01:03:20.160 --> 01:03:22.240
So that's an interesting distinction to draw.

01:03:22.240 --> 01:03:24.800
So if you look at the way Tesla is approaching the problem,

01:03:26.000 --> 01:03:30.640
and the way you're approaching the problem, which is very different than the rest of the

01:03:30.640 --> 01:03:32.560
self-driving car world.

01:03:32.560 --> 01:03:33.760
So let's put them aside.

01:03:33.760 --> 01:03:37.440
Is you're treating most, the driving task as a machine learning problem.

01:03:37.440 --> 01:03:40.960
And the way Tesla is approaching it is with the multitask learning,

01:03:40.960 --> 01:03:45.280
where you break the task of driving into hundreds of different tasks.

01:03:45.280 --> 01:03:51.520
And you have this multi-headed neural network that's very good at performing each task.

01:03:51.520 --> 01:03:56.240
And there's presumably something on top that's stitching stuff together

01:03:56.240 --> 01:04:02.080
in order to make control decisions, policy decisions about how you move the car.

01:04:02.080 --> 01:04:05.520
But what that allows you, there's a brilliance to this because it allows you to

01:04:05.760 --> 01:04:14.560
master each task, like lane detection, stop sign detection, the traffic light detection,

01:04:15.440 --> 01:04:21.280
drivable area segmentation, vehicle bicycle pedestrian detection.

01:04:22.880 --> 01:04:24.800
There's some localization tasks in there.

01:04:25.440 --> 01:04:34.000
Also predicting how the entities in the scene are going to move.

01:04:34.240 --> 01:04:37.920
Everything is basically a machine learning task, where there's a classification,

01:04:37.920 --> 01:04:44.320
segmentation, prediction, and it's nice because you can have this entire engine,

01:04:44.320 --> 01:04:49.360
data engine that's mining for edge cases for each one of these tasks.

01:04:49.360 --> 01:04:53.600
And you can have people, like engineers that are basically masters of that task,

01:04:53.600 --> 01:04:59.600
they become the best person in the world at, as you talk about, the cone guy for Waymo.

01:04:59.600 --> 01:05:01.040
Yeah, we're a good old cone guy.

01:05:01.600 --> 01:05:06.000
Become the best person in the world at cone detection.

01:05:07.200 --> 01:05:10.640
So that's a compelling notion from a supervised learning perspective,

01:05:12.160 --> 01:05:17.360
automating much of the process of edge case discovery and retraining neural network for

01:05:17.360 --> 01:05:22.000
each of the individual perception tasks. And then you're looking at the machine learning

01:05:22.000 --> 01:05:28.960
in a more holistic way, basically doing end-to-end learning on the driving tasks, supervised,

01:05:29.760 --> 01:05:36.240
trained on the data of the actual driving of people they use comma AI,

01:05:36.240 --> 01:05:41.760
like actual human drivers, their manual control, plus the moments of disengagement

01:05:42.560 --> 01:05:47.120
that maybe with some labeling could indicate the failure of the system.

01:05:48.800 --> 01:05:53.920
You have a huge amount of data for positive control of the vehicle, like successful control

01:05:54.000 --> 01:05:59.920
of the vehicle, both maintaining the lane as I think you're also working on

01:05:59.920 --> 01:06:04.640
longitude, no control of the vehicle, and then failure cases where the vehicle does

01:06:04.640 --> 01:06:12.320
something wrong that needs disengagement. So why do you think you're right and Tesla is wrong

01:06:13.040 --> 01:06:19.840
on this? Do you think you'll come around the Tesla way? Do you think Tesla will come around to your way?

01:06:20.160 --> 01:06:25.120
If you were to start a chess engine company, would you hire a bishop guy?

01:06:26.000 --> 01:06:33.360
See, we have, this is Monday morning quarterbacking, is yes, probably.

01:06:35.840 --> 01:06:40.800
Oh, our Rook guy. Oh, we stole the Rook guy from that company. Oh, we're gonna have real good Rooks.

01:06:40.800 --> 01:06:48.640
Well, there's not many pieces, right? There's not many guys and gals to hire.

01:06:48.720 --> 01:06:52.560
You just have a few that work on the bishop, a few that work on the Rook.

01:06:52.560 --> 01:06:56.640
But is that not ludicrous today to think about in a world of AlphaZero?

01:06:57.440 --> 01:07:03.600
But AlphaZero is a chess game. So the fundamental question is how hard is driving compared to chess?

01:07:04.320 --> 01:07:12.080
Because so long term, end to end will be the right solution. The question is how many years

01:07:12.080 --> 01:07:15.680
away is that? End to end is going to be the only solution for level five.

01:07:15.680 --> 01:07:19.200
For the only way we get there. Of course. And of course, Tesla is going to come around to my

01:07:19.200 --> 01:07:25.360
way. And if you're a Rook guy out there, I'm sorry. The cone guy. I don't know.

01:07:25.360 --> 01:07:29.360
We're going to specialize each task. We're going to really understand Rook placement. Yeah.

01:07:30.400 --> 01:07:37.040
I understand the intuition you have. I mean, that is a very compelling notion that we can

01:07:37.040 --> 01:07:41.200
learn the task end to end, like the same compelling notion you might have for natural

01:07:41.200 --> 01:07:49.840
language conversation. But I'm not sure. Because one thing you sneaked in there is assertion that

01:07:50.640 --> 01:07:56.400
it's impossible to get to level five without this kind of approach. I don't know if that's

01:07:56.400 --> 01:08:02.400
obvious. I don't know if that's obvious either. I don't actually mean that. I think that it is much

01:08:02.400 --> 01:08:06.880
easier to get to level five with an end to end approach. I think that the other approach is

01:08:07.760 --> 01:08:12.880
doable, but the magnitude of the engineering challenge may exceed what humanity is capable of.

01:08:13.680 --> 01:08:20.720
But what do you think of the Tesla data engine approach, which to me is an active learning

01:08:20.720 --> 01:08:27.360
task is kind of fascinating, is breaking it down into these multiple tasks and mining their data

01:08:27.360 --> 01:08:32.080
constantly for like edge cases for these different tasks. But the tasks themselves are not being

01:08:32.080 --> 01:08:41.120
learned. This is feature engineering. I mean, it's a higher abstraction level of feature

01:08:41.120 --> 01:08:45.920
engineering for the different tasks. It's task engineering in a sense. It's slightly better

01:08:45.920 --> 01:08:50.560
feature engineering, but it's still fundamentally as feature engineering. And if anything about the

01:08:50.560 --> 01:08:55.680
history of AI has taught us anything, it's that feature engineering approaches will always be

01:08:55.680 --> 01:09:02.000
replaced and lose to end to end. Now, to be fair, I cannot really make promises on timelines,

01:09:02.000 --> 01:09:06.960
but I can say that when you look at the code for stock fish and the code for alpha zero,

01:09:06.960 --> 01:09:11.440
one is a lot shorter than the other. A lot more elegant required a lot less programmer hours to

01:09:11.440 --> 01:09:25.840
write. Yeah, but there was a lot more murder of bad agents on the alpha zero side. By murder, I mean

01:09:27.840 --> 01:09:34.240
agents that played a game and failed miserably. Yeah. Oh, in simulation, that failure is less

01:09:34.240 --> 01:09:39.680
costly. Yeah. In real world. Wait, do you mean in practice, like alpha zero has lost games

01:09:39.760 --> 01:09:46.960
miserably? No, I haven't seen that. No, but I know the requirement for alpha zero is

01:09:48.000 --> 01:09:53.440
to be able to like evolution, human evolution, not human evolution, biological evolution of life

01:09:53.440 --> 01:10:01.280
on earth from the origin of life has murdered trillions upon trillions of organisms on the path

01:10:01.280 --> 01:10:07.680
thus humans. So the question is, can we stitch together a human like object without having to

01:10:07.680 --> 01:10:11.840
go through the entirety process of evolution? Well, no, but do the evolution in simulation?

01:10:11.840 --> 01:10:15.680
Yeah, that's the question. Can we simulate? So do you ever sense that it's possible to simulate

01:10:15.680 --> 01:10:22.320
some aspect of it? Mu zero is exactly this. Mu zero is the solution to this. Mu zero, I think,

01:10:22.320 --> 01:10:26.720
is going to be looked back as the canonical paper. And I don't think deep learning is everything.

01:10:26.720 --> 01:10:30.320
I think that there's still a bunch of things missing to get there. But Mu zero, I think,

01:10:30.320 --> 01:10:36.960
is going to be looked back as the kind of cornerstone paper of this whole deep learning era.

01:10:36.960 --> 01:10:41.280
And Mu zero is the solution to self-driving cars. You have to make a few tweaks to it. But

01:10:41.280 --> 01:10:47.040
Mu zero does effectively that. It does those rollouts and those murdering in a learned

01:10:47.040 --> 01:10:51.440
simulator and a learned dynamics model. It's interesting. It doesn't get enough love.

01:10:51.440 --> 01:10:56.320
I was blown away when I was blown away when I read that paper. I'm like, okay, I've always

01:10:56.320 --> 01:10:59.360
said a comma. I'm going to sit and I'm going to wait for the solution to self-driving cars to come

01:10:59.360 --> 01:11:08.160
along. This year, I saw it. It's Mu zero. So sit back and let the winning roll in.

01:11:09.040 --> 01:11:15.040
So your sense just to elaborate a little bit to link on the topic. Your sense is neural networks

01:11:15.040 --> 01:11:18.160
will solve driving. Yes. Like we don't need anything else.

01:11:18.720 --> 01:11:23.200
I think the same way chess was maybe the chess and maybe Google are the pinnacle of like

01:11:24.000 --> 01:11:31.440
search algorithms and things that look kind of like a star. The pinnacle of this error is going

01:11:31.440 --> 01:11:39.440
to be self-driving cars. But on the path that you have to deliver products and it's possible that

01:11:39.440 --> 01:11:46.720
the path to full self-driving cars will take decades. I doubt it. How long would you put on it?

01:11:47.680 --> 01:11:54.720
Like what are we? You're chasing it. Tesla's chasing it. What are we talking about? 5 years,

01:11:54.720 --> 01:12:00.960
10 years, 50 years? Let's say in the 2020s. In the 2020s. The later part of the 2020s.

01:12:03.520 --> 01:12:08.240
With the neural network, that would be nice to see. And on the path to that, you're delivering

01:12:08.240 --> 01:12:12.960
products, which is a nice L2 system. That's what Tesla is doing, a nice L2 system.

01:12:12.960 --> 01:12:16.560
Just gets better every time. The only difference between L2 and the other levels

01:12:16.560 --> 01:12:20.240
is who takes liability. And I'm not a liability guy. I don't want to take liability. I'm going to

01:12:20.240 --> 01:12:28.800
level 2 forever. Now on that little transition, I mean, how do you make the transition work?

01:12:29.520 --> 01:12:35.840
Is this where driver sensing comes in? Like how do you make the, because you said 100 miles,

01:12:35.840 --> 01:12:42.080
like is there some sort of human factor psychology thing where people start to

01:12:42.080 --> 01:12:46.080
over-trust the system, all those kinds of effects. Once it gets better and better and

01:12:46.080 --> 01:12:50.880
better and better, they get lazier and lazier and lazier. Is that, like how do you get that

01:12:50.880 --> 01:12:55.520
transition right? First off, our monitoring is already adaptive. Our monitoring is already seen

01:12:55.520 --> 01:13:00.800
adaptive. Driver monitoring is just the camera that's looking at the driver. You have an infrared

01:13:00.800 --> 01:13:08.560
camera. Our policy for how we enforce the driver monitoring is seen adaptive. What's that mean?

01:13:09.120 --> 01:13:16.800
For example, in one of the extreme cases, if the car is not moving, we do not actively enforce

01:13:16.800 --> 01:13:24.480
driver monitoring. If you are going through like a 45-mile-an-hour road with lights

01:13:25.680 --> 01:13:30.720
and stop signs and potentially pedestrians, we enforce a very tight driver monitoring policy.

01:13:30.720 --> 01:13:35.520
If you are alone on a perfectly straight highway, and it's all machine learning,

01:13:35.520 --> 01:13:38.960
none of that is hand-coded. Actually, the stop is hand-coded, but...

01:13:38.960 --> 01:13:42.080
So there's some kind of machine learning estimation of risk.

01:13:43.520 --> 01:13:49.120
Yeah. I mean, I've always been a huge fan of that. That's a, it's difficult to do

01:13:50.640 --> 01:13:56.240
every step into that direction is a worthwhile stop to take. It might be difficult to do really

01:13:56.240 --> 01:14:01.360
well. Like us humans are able to estimate risk pretty damn well. Whatever the hell that is,

01:14:01.440 --> 01:14:08.880
that feels like one of the nice features of us humans. Because we humans are really good drivers

01:14:08.880 --> 01:14:14.000
when we're really tuned in. And we're good at estimating risk, like when are we supposed to

01:14:14.000 --> 01:14:20.400
be tuned in? Yeah. And people are like, oh, well, why would you ever make the driver monitoring policy

01:14:20.400 --> 01:14:24.560
less aggressive? Why would you always not keep it at its most aggressive? Because then people

01:14:24.560 --> 01:14:27.680
are just going to get fatigued from it. Yeah. Well, they get annoyed. You want them,

01:14:28.240 --> 01:14:32.400
you want the experience to be pleasant. Obviously, I want the experience to be pleasant,

01:14:32.400 --> 01:14:38.240
but even just from a straight up safety perspective, if you alert people when they look

01:14:38.240 --> 01:14:42.400
around and they're like, why is this thing alerting me? There's nothing I could possibly hit right

01:14:42.400 --> 01:14:47.040
now. People will just learn to tune it out. People will just learn to tune it out, to put

01:14:47.040 --> 01:14:52.160
weights on the steering wheel, to do whatever, to overcome it. And remember that you're always

01:14:52.160 --> 01:14:56.880
part of this adaptive system. So all I can really say about how this scales going forward is,

01:14:56.880 --> 01:15:01.680
yeah, something we have to monitor for. We don't know. This is a great psychology experiment at

01:15:01.680 --> 01:15:05.920
scale. We'll see. Yeah, it's fascinating. Track it. And making sure you have a good

01:15:06.560 --> 01:15:11.360
understanding of attention is a very key part of that psychology problem.

01:15:11.360 --> 01:15:15.600
Yeah. I think you and I probably have a different come to it differently, but to me,

01:15:16.560 --> 01:15:22.160
it's a fascinating psychology problem to explore something much deeper than just driving. It's

01:15:23.040 --> 01:15:30.080
it's such a nice way to explore human attention and human behavior, which is why, again,

01:15:30.080 --> 01:15:37.280
we've probably both criticized Mr. Elon Musk on this one topic from different avenues.

01:15:38.000 --> 01:15:41.280
So both offline and online, I had little chats with Elon and

01:15:44.000 --> 01:15:50.160
like, I love human beings as a computer vision problem, as an AI problem. It's fascinating.

01:15:51.040 --> 01:15:52.960
He wasn't so much interested in that problem.

01:15:54.400 --> 01:15:59.040
It's like, in order to solve driving, the whole point is you want to remove the human from the

01:15:59.040 --> 01:16:06.640
picture. And it seems like you can't do that quite yet. Eventually, yes, but you can't quite do that

01:16:06.640 --> 01:16:17.600
yet. So this is the moment where you can't yet say, I told you so to Tesla, but it's getting there

01:16:17.600 --> 01:16:21.760
because I don't know if you've seen this, there's some reporting that they're in fact starting to do

01:16:22.400 --> 01:16:24.320
driver monitoring. Yeah, they ship the model in shadow mode.

01:16:26.160 --> 01:16:33.120
We though, I believe only a visible light camera. It might even be fisheye. It's like a low resolution.

01:16:33.120 --> 01:16:37.040
Low resolution visible light. I mean, to be fair, that's what we have in the Eon as well,

01:16:37.040 --> 01:16:42.000
our last generation product. This is the one area where I can say our hardware is ahead of Tesla.

01:16:42.000 --> 01:16:45.120
The rest of our hardware way way behind, but our driver monitoring camera.

01:16:45.840 --> 01:16:52.320
So you think, I think on the third row Tesla podcast or somewhere else, I've heard you say that

01:16:53.680 --> 01:16:56.240
obviously eventually they're going to have driver monitoring.

01:16:57.040 --> 01:17:01.200
I think what I've said is Elon will definitely ship driver monitoring before he ships level

01:17:01.200 --> 01:17:05.840
five. And I'm willing to bet 10 grand on that. And you bet 10 grand on that.

01:17:06.880 --> 01:17:09.600
I mean, now I don't want to take the bet, but before, maybe someone would have thought I should

01:17:09.600 --> 01:17:18.480
have got my money. It's an interesting bet. I think you're right. I'm actually on a human level

01:17:19.040 --> 01:17:26.880
because he's made the decision. He said that driver monitoring is the wrong way to go.

01:17:27.520 --> 01:17:34.400
But you have to think of as a human as a CEO. I think that's the right thing to say when

01:17:34.400 --> 01:17:41.680
like sometimes you have to say things publicly that different than when you actually believe

01:17:41.680 --> 01:17:46.800
because when you're producing a large number of vehicles and the decision was made not to

01:17:46.800 --> 01:17:51.920
include the camera, like what are you supposed to say? Like our cars don't have the thing that I

01:17:51.920 --> 01:17:58.160
think is right to have. It's an interesting thing. But like on the other side as a CEO,

01:17:58.160 --> 01:18:03.520
I mean, something you could probably speak to as a leader, I think about me as a human

01:18:04.800 --> 01:18:09.440
to publicly change your mind on something. How hard is that? Well, especially when assholes

01:18:09.440 --> 01:18:15.760
like George Haas say, I told you so. All I will say is I am not a leader and I am happy to change

01:18:15.760 --> 01:18:25.280
my mind. You think Elon will? Yeah, I do. I think he'll come up with a good way to make it

01:18:25.280 --> 01:18:30.480
psychologically okay for him. Well, it's such an important thing, man, especially for a first

01:18:30.480 --> 01:18:35.680
principles thinker because he made a decision that driver monitoring is not the right way to go.

01:18:35.680 --> 01:18:40.320
And I could see that decision and I could even make that decision. Like I was on the fence

01:18:41.280 --> 01:18:48.640
too. Like I'm not a driver monitoring is such an obvious simple solution to the problem of

01:18:48.640 --> 01:18:54.320
attention. It's not obvious to me that just by putting a camera there, you solve things. You

01:18:54.320 --> 01:19:00.960
have to create an incredible compelling experience just like you're talking about.

01:19:00.960 --> 01:19:06.160
I don't know if it's easy to do that. It's not at all easy to do that, in fact, I think. So

01:19:07.120 --> 01:19:12.800
as a creator of a car that's trying to create a product that people love, which is what Tesla

01:19:12.800 --> 01:19:19.840
tries to do, right? It's not obvious to me that as a design decision, whether adding a camera

01:19:19.920 --> 01:19:25.280
is a good idea. From a safety perspective either, like in the human factors community,

01:19:25.280 --> 01:19:31.120
everybody says that you should obviously have driver sensing, driver monitoring. But like

01:19:32.480 --> 01:19:38.880
that's like saying it's obvious as parents you shouldn't let your kids go out at night.

01:19:39.920 --> 01:19:45.440
But okay. But like they're still going to find ways to do drugs.

01:19:46.000 --> 01:19:52.000
Yeah. You have to also be good parents. So it's much more complicated than just

01:19:52.720 --> 01:19:58.800
you need to have driver monitoring. I totally disagree on, okay, if you have a camera there

01:19:58.800 --> 01:20:02.880
and the camera is watching the person but never throws an alert, they'll never think about it.

01:20:03.600 --> 01:20:09.840
Right? The driver monitoring policy that you choose to, how you choose to communicate with

01:20:09.840 --> 01:20:20.320
the user is entirely separate from the data collection perspective. Right? So there's one

01:20:20.320 --> 01:20:26.640
thing to say, tell your teenager they can't do something. There's another thing to gather the

01:20:26.640 --> 01:20:30.240
data. So you can make informed decisions. That's really interesting. But you have to make that,

01:20:31.040 --> 01:20:36.960
that's the interesting thing about cars. But even true with Comm AI, you don't have to

01:20:36.960 --> 01:20:41.120
manufacture the thing into the car is you have to make a decision that anticipates

01:20:41.920 --> 01:20:47.360
the right strategy long term. So like you have to start collecting the data and start making

01:20:47.360 --> 01:20:52.080
decisions. Started it, started it three years ago. I believe that we have the best driver

01:20:52.080 --> 01:20:57.680
monitoring solution in the world. I think that when you compare it to Supercruise is the only

01:20:57.680 --> 01:21:04.880
other one that I really know that shipped and ours is better. What do you like and not like about

01:21:04.880 --> 01:21:11.920
Supercruise? I mean, I had a few Supercruise, the sun would be shining through the window,

01:21:11.920 --> 01:21:15.440
would blind the camera, and it would say I wasn't paying attention when I was looking completely

01:21:15.440 --> 01:21:19.760
straight. I couldn't reset the attention with a steering wheel touch and Supercruise would

01:21:19.760 --> 01:21:24.320
disengage. Like I was communicating to the car, I'm like, look, I am here, I am paying attention.

01:21:24.320 --> 01:21:30.400
Why are you really going to force me to disengage? And it did. So it's a constant

01:21:30.400 --> 01:21:34.640
conversation with the user. And yeah, there's no way to ship a system like this if you can OTA.

01:21:35.680 --> 01:21:40.960
We're shipping a new one every month. Sometimes we balance it with our users on Discord. Sometimes

01:21:40.960 --> 01:21:44.320
we make the driver monitoring a little more aggressive and people complain. Sometimes they

01:21:44.320 --> 01:21:48.560
don't. We want it to be as aggressive as possible where people don't complain and it doesn't feel

01:21:48.560 --> 01:21:52.640
intrusive. So being able to update the system over the air is an essential component. I mean,

01:21:52.640 --> 01:22:02.880
that's probably, to me, that is the biggest innovation of Tesla, that it made it. People

01:22:02.880 --> 01:22:10.000
realize that over the air updates is essential. Yeah. Was that not obvious from the iPhone?

01:22:10.000 --> 01:22:14.560
The iPhone was the first real product that OTAed, I think. Was it? Actually, that's brilliant.

01:22:14.560 --> 01:22:17.840
You're right. I mean, the game consoles used to not, right? The game consoles were maybe the

01:22:17.840 --> 01:22:22.160
second thing that did. Well, I didn't really think about it. One of the amazing features

01:22:22.160 --> 01:22:29.520
of a smartphone isn't just like the touchscreen isn't the thing. It's the ability to constantly

01:22:29.520 --> 01:22:40.720
update. Yeah. It gets better. It gets better. I love my iOS 14. Yeah. One thing that I probably

01:22:40.720 --> 01:22:47.760
disagree with you on driver monitoring is you said that it's easy. I mean, you tend to say

01:22:47.760 --> 01:22:55.520
stuff is easy. I guess you said it's easy relative to the external perception problem there.

01:22:55.760 --> 01:23:03.680
Can you elaborate why you think it's easy? Feature engineering works for driver monitoring. Feature

01:23:03.680 --> 01:23:10.640
engineering does not work for the external. So human faces are not, human faces and the movement

01:23:10.640 --> 01:23:16.080
of human faces and head and body is not as variable as the external environment? Yeah.

01:23:16.080 --> 01:23:19.360
Yeah. Because you're intuition. Yes. And there's another big difference as well.

01:23:20.080 --> 01:23:24.240
Your reliability of a driver monitoring system doesn't actually need to be that high.

01:23:24.320 --> 01:23:28.320
The uncertainty, if you have something that's detecting whether the human's paying attention

01:23:28.320 --> 01:23:32.640
and it only works 92% of the time, you're still getting almost all the benefit of that

01:23:32.640 --> 01:23:37.520
because the human, you're training the human. You're dealing with a system that's

01:23:37.520 --> 01:23:43.360
really helping you out. It's a conversation. It's not the external thing where guess what?

01:23:43.360 --> 01:23:48.240
If you swerve into a tree, you swerve into a tree. You get no margin for error there.

01:23:48.240 --> 01:23:54.160
Yeah. I think that's really well put. I think that's the right, exactly the place where

01:23:56.720 --> 01:24:01.440
comparing to the external perception and the control problem, driver monitoring is easier

01:24:01.440 --> 01:24:07.840
because the bar for success is much lower. Yeah. But I still think the human face

01:24:09.120 --> 01:24:13.280
is more complicated actually than the external environment. But for driving, you don't give a

01:24:13.280 --> 01:24:22.000
damn. I don't need something that complicated to have to communicate the idea to the human

01:24:22.000 --> 01:24:26.480
that I want to communicate, which is, yo, system might mess up here. You got to pay attention.

01:24:27.680 --> 01:24:33.840
Yeah. See, that's my love and fascination is the human face. And it feels like this is

01:24:34.800 --> 01:24:42.240
a nice place to create products that create an experience in the car. It feels like there should

01:24:42.240 --> 01:24:52.080
be more richer experiences in the car. That's an opportunity for something like AMAI or just any

01:24:52.080 --> 01:24:57.040
kind of system like a Tesla or any of the autonomous vehicle companies is because software,

01:24:57.600 --> 01:25:01.920
there's much more sensors and so much is running on software and you're doing machine learning anyway,

01:25:02.720 --> 01:25:08.080
there's an opportunity to create totally new experiences that we're not even anticipating.

01:25:08.160 --> 01:25:14.080
You don't think so? No. You think it's a box that gets you from A to B and you want to do it chill?

01:25:14.960 --> 01:25:19.200
Yeah. I mean, I think as soon as we get to level three on highways, okay, enjoy your candy crush,

01:25:19.200 --> 01:25:24.720
enjoy your Hulu, enjoy your, you know, whatever, whatever. Sure, you get this, you can look at

01:25:24.720 --> 01:25:29.360
screens basically versus right now, what do you have? Music and audiobooks. So level three is

01:25:29.360 --> 01:25:37.280
where you can kind of disengage and stretch this of time. Well, you think level three is possible?

01:25:37.280 --> 01:25:44.560
Like on the highway going 400 miles and you can just go to sleep? Oh yeah, sleep. So again,

01:25:44.560 --> 01:25:50.480
I think it's really all on a spectrum. I think that being able to use your phone while you're on

01:25:50.480 --> 01:25:55.600
the highway and like this all being okay and being aware that the car might alert you and you have

01:25:55.600 --> 01:25:59.120
five seconds to basically. So the five second thing is that you think it's possible? Yeah,

01:25:59.120 --> 01:26:04.320
I think it is. Oh yeah. Not in all scenarios. Right. Some scenarios it's not. It's the whole

01:26:04.320 --> 01:26:10.480
risk thing that you mentioned is nice is to be able to estimate like how risk is this situation.

01:26:10.480 --> 01:26:17.280
That's really important to understand. One other thing you mentioned comparing comma and autopilot

01:26:17.280 --> 01:26:25.760
is that something about the haptic feel of the way comma controls the car when things are uncertain,

01:26:25.760 --> 01:26:30.400
like it behaves a little bit more uncertain when things are uncertain. That's kind of an

01:26:30.400 --> 01:26:35.440
interesting point. And then autopilot is much more confident always, even when it's uncertain,

01:26:35.440 --> 01:26:42.160
until it runs into trouble. That's a funny thing. I actually mentioned that to Elon,

01:26:42.160 --> 01:26:47.920
I think, and then the first time we talked he was inviting is like communicating uncertainty.

01:26:48.720 --> 01:26:53.280
I guess comma doesn't really communicate uncertainty explicitly. It communicates it

01:26:53.280 --> 01:26:58.000
through haptic feel. Like what's the role of communicating uncertainty, do you think?

01:26:58.080 --> 01:27:01.680
We do some stuff explicitly. We do detect the lanes when you're on the highway,

01:27:01.680 --> 01:27:05.280
and we'll show you how many lanes we're using to drive with. You can look at where it thinks

01:27:05.280 --> 01:27:11.440
the lanes are. You can look at the path. We want to be better about this. We're actually hiring.

01:27:11.440 --> 01:27:16.080
We want to hire some new UI people. UI people. You mentioned this because it's such a UI problem

01:27:16.080 --> 01:27:21.120
too. We have a great designer now, but we need people who are just going to build this and

01:27:21.120 --> 01:27:26.320
debug these UIs, QT people. QT. Is that what the UI has done with this QT?

01:27:26.400 --> 01:27:30.240
Moving. The new UI is in QT. C++, QT?

01:27:31.840 --> 01:27:35.760
Tesla uses it too. We had some React stuff in there.

01:27:37.760 --> 01:27:41.040
React.js or just React. React has its own language, right?

01:27:41.040 --> 01:27:49.680
React Native. React is a JavaScript framework. It's all based on JavaScript, but I like C++.

01:27:50.160 --> 01:27:58.240
QT. What do you think about Dojo with Tesla and their foray into what appears to be

01:28:00.160 --> 01:28:07.280
specialized hardware for training on that? I guess it's something, maybe you can correct me,

01:28:07.280 --> 01:28:12.000
for my shallow looking at it. It seems like something that Google did with TPUs,

01:28:12.000 --> 01:28:17.120
but specialized for driving data. I don't think it's specialized for driving data.

01:28:17.120 --> 01:28:22.560
QT. It's just legit, just TPU. They want to go the Apple way. Basically,

01:28:22.560 --> 01:28:25.200
everything required in the chain is done in-house.

01:28:25.200 --> 01:28:32.720
QT. You have a problem right now. This is one of my concerns. I really would like to see

01:28:32.720 --> 01:28:36.800
somebody deal with this. If anyone out there is doing it, I'd like to help them if I can.

01:28:37.840 --> 01:28:43.520
You basically have two options right now to train. Your options are Nvidia or Google.

01:28:44.480 --> 01:28:52.240
QT. Google is not even an option. Their TPUs are only available in Google Cloud.

01:28:53.040 --> 01:28:59.120
Google has absolutely onerous terms of service restrictions. They may have changed it,

01:28:59.120 --> 01:29:02.400
but back in Google's terms of service, it said explicitly you are not allowed to use

01:29:02.400 --> 01:29:07.200
Google Cloud ML for training autonomous vehicles or for doing anything that competes with Google

01:29:07.200 --> 01:29:12.080
without Google's prior written permission. Google is not a platform company.

01:29:14.480 --> 01:29:18.320
I wouldn't touch TPUs with a 10-foot pole. That leaves you with the monopoly.

01:29:18.320 --> 01:29:19.600
QT. Nvidia?

01:29:19.600 --> 01:29:21.440
QT. Nvidia.

01:29:21.440 --> 01:29:23.120
QT. You're not a fan of?

01:29:23.120 --> 01:29:32.640
QT. Well, look, I was a huge fan of 2016 Nvidia. Jensen came sat in the car. Cool guy

01:29:32.640 --> 01:29:39.760
when the stock was $30 a share. Nvidia stock has skyrocketed. I witnessed a real change

01:29:39.840 --> 01:29:47.280
in who was in management over there in 2018. Now they are, let's exploit, let's take every

01:29:47.280 --> 01:29:52.080
dollar we possibly can out of this ecosystem. Let's charge $10,000 for A100s because we know

01:29:52.080 --> 01:29:58.800
we got the best share in the game. Let's charge $10,000 for an A100 when it's really not that

01:29:58.800 --> 01:30:06.080
different from $3080, which is $699. The margins that they are making off of those high-end chips

01:30:06.080 --> 01:30:11.120
are so high that I think they're shooting themselves in the foot just from a business

01:30:11.120 --> 01:30:15.760
perspective because there's a lot of people talking like me now who are like, somebody's

01:30:15.760 --> 01:30:22.000
got to take Nvidia down. Where they could dominate, Nvidia could be the new Intel.

01:30:22.000 --> 01:30:31.120
QT. Yeah, to be inside everything essentially. Yet the winners in certain spaces like in

01:30:31.200 --> 01:30:36.800
autonomous driving, the winners, only the people who are like desperately falling back and trying

01:30:36.800 --> 01:30:41.520
to catch up and have a ton of money like the big automakers are the ones interested in partnering

01:30:41.520 --> 01:30:46.240
with Nvidia. Jensen. Oh, and I think a lot of those things are going to fall through. If I were

01:30:46.240 --> 01:30:53.440
in Nvidia, sell chips. Sell chips at a reasonable markup. QT. To everybody. Jensen. To everybody.

01:30:53.440 --> 01:30:58.080
QT. Without any restrictions. Jensen. Without any restrictions. Intel did this. Look at Intel.

01:30:58.160 --> 01:31:02.960
They had a great long run. Nvidia is trying to turn their, they're like trying to productize

01:31:02.960 --> 01:31:09.360
their chips way too much. They're trying to extract way more value than they can sustainably.

01:31:09.360 --> 01:31:12.960
Sure. You can do it tomorrow. Is it going to up your share price? Sure. If you're one of those

01:31:12.960 --> 01:31:17.760
CEOs, it's like, how much can I strip mine in this company? And that's what's weird about it too.

01:31:17.760 --> 01:31:22.160
Like the CEO is the founder. It's the same guy. I mean, I still think Jensen's a great guy.

01:31:22.240 --> 01:31:28.560
It is great. Why do this? You have a choice. You have a choice right now. Are you trying to cash

01:31:28.560 --> 01:31:34.880
out? Are you trying to buy a yacht? If you are, fine. But if you're trying to be the next huge

01:31:34.880 --> 01:31:41.280
semiconductor company, sell chips. Well, the interesting thing about Jensen is he is a big

01:31:41.280 --> 01:31:50.400
vision guy. So he has a plan like for 50 years down the road. So it makes me wonder like...

01:31:50.400 --> 01:31:55.920
How does price gouging fit into it? Yeah. How does that fit? It doesn't seem to make sense of the

01:31:55.920 --> 01:32:02.400
plan. I worry that he's listening to the wrong people. Yeah. That's the sense I have too sometimes

01:32:02.400 --> 01:32:11.040
because I, despite everything, I think Nvidia is an incredible company. Well, one, I'm deeply

01:32:11.040 --> 01:32:15.120
grateful to Nvidia for the products they've created in the past. Me too. And so...

01:32:16.000 --> 01:32:18.880
The 1080 Ti was a great GPU. Still have a lot of them.

01:32:18.880 --> 01:32:24.080
Still is. Yeah. But at the same time, it just feels like...

01:32:26.720 --> 01:32:31.040
It feels like you don't want to put all your stock in Nvidia. And so the Elon is doing...

01:32:31.920 --> 01:32:38.320
What Tesla is doing with autopilot and Dojo is the Apple way. Because they're not going to share

01:32:38.320 --> 01:32:44.560
Dojo with George Hott's. I know. They should sell that chip. Oh, they should sell that.

01:32:44.640 --> 01:32:48.320
Even their accelerator. The accelerator that's in all the cars, the 30 watt one.

01:32:49.040 --> 01:32:55.120
Sell it. Why not? So open it up. Make me... Why does Tesla have to be a car company?

01:32:55.680 --> 01:33:00.480
Well, if you sell the chip, here's what you get. Yeah. Makes the money out of the chips. It doesn't

01:33:00.480 --> 01:33:06.160
take away from your chip. You're going to make some money, free money. And also the world is going

01:33:06.160 --> 01:33:11.040
to build an ecosystem of tooling for you. All right. You're not going to have to fix the bug

01:33:11.040 --> 01:33:16.640
in your 10H layer. Someone else already did. Well, the question... That's an interesting question.

01:33:16.640 --> 01:33:19.920
I mean, that's the question Steve Jobs asked. That's the question Elon Musk is

01:33:21.920 --> 01:33:30.960
perhaps asking is, do you want Tesla stuff inside other vehicles inside? Potentially inside like

01:33:30.960 --> 01:33:37.760
iRobot Vacuum Cleaner? Yeah. I think you should decide where your advantages are. I'm not saying

01:33:37.760 --> 01:33:41.680
Tesla should start selling battery packs to automakers because battery packs to automakers,

01:33:41.680 --> 01:33:45.360
they're straight up in competition with you. If I were Tesla, I'd keep the battery technology

01:33:45.360 --> 01:33:53.120
totally as far as we make batteries. But the thing about the Tesla TPU is anybody can build that.

01:33:53.120 --> 01:33:58.960
It's just a question of, are you willing to spend the money? It could be a huge source of revenue

01:33:58.960 --> 01:34:04.640
potentially. Are you willing to spend $100 million? Anyone can build it. And someone will.

01:34:04.640 --> 01:34:08.400
And a bunch of companies now are starting trying to build AI accelerators. Somebody's

01:34:08.400 --> 01:34:14.640
going to get the idea right. And hopefully they don't get greedy because they'll just lose to

01:34:14.640 --> 01:34:17.760
the next guy who finally... And then eventually the Chinese are going to make knockoff and video

01:34:17.760 --> 01:34:22.080
chips and that's... From your perspective, I don't know if you're also paying attention to

01:34:22.080 --> 01:34:30.320
Stan Tesla for a moment. Elon Musk has talked about a complete rewrite of the neural net that

01:34:30.400 --> 01:34:37.680
they're using that seems to... Again, I'm half paying attention, but it seems to involve basically

01:34:37.680 --> 01:34:45.920
a kind of integration of all the sensors to where it's a four-dimensional view. You have a 3D model

01:34:45.920 --> 01:34:52.720
of the world over time and then you can... I think it's done both for the... Actually,

01:34:53.200 --> 01:34:58.480
so the neural network is able to in a more holistic way deal with the world and make predictions

01:34:58.560 --> 01:35:07.680
and so on, but also to make the annotation task more easier. You can annotate the world in one

01:35:07.680 --> 01:35:11.840
place and they kind of distribute itself across the sensors and across the different...

01:35:12.880 --> 01:35:18.560
Like the hundreds of tests that are involved in the hydranet. What are your thoughts about this

01:35:18.560 --> 01:35:23.920
rewrite? Is it just like some details that are kind of obvious, there are steps that should be taken

01:35:23.920 --> 01:35:28.240
or is there something fundamental that could challenge your idea that end-to-end

01:35:28.960 --> 01:35:33.520
is the right solution? We're in the middle of a big review right now as well. We haven't shipped a

01:35:33.520 --> 01:35:39.280
new model in a bit. Of what kind? We're going from 2D to 3D. Right now, all our stuff like, for

01:35:39.280 --> 01:35:44.480
example, when the car pitches back, the lane lines also pitch back because we're assuming the flat

01:35:45.280 --> 01:35:49.840
world hypothesis, the new models do not do this. The new models output everything in 3D.

01:35:49.840 --> 01:35:55.920
But there's still no annotation, so the 3D is more about the output.

01:35:57.920 --> 01:36:06.480
We have Zs and everything. We've... Zs. We added Zs. We unified a lot of stuff as well.

01:36:06.480 --> 01:36:13.680
We switched from TensorFlow to PyTorch. My understanding of what Tesla's thing is,

01:36:13.680 --> 01:36:16.560
is that their annotator now annotates across the time dimension.

01:36:19.840 --> 01:36:26.400
I mean, cute. Why are you building an annotator? I find their entire pipeline.

01:36:28.160 --> 01:36:33.840
I find your vision, I mean, the vision of end-to-end very compelling. But I also like the

01:36:33.840 --> 01:36:38.800
engineering of the data engine that they've created. In terms of supervised learning

01:36:39.040 --> 01:36:46.240
pipelines, that thing is damn impressive. You're basically the idea is that you have

01:36:47.360 --> 01:36:52.240
hundreds of thousands of people that are doing data collection for you by doing their experience.

01:36:52.240 --> 01:37:00.160
So that's kind of similar to the Kama AI model. And you're able to mine that data based on the

01:37:00.160 --> 01:37:06.480
kind of education you need. I think it's harder to do in the end-to-end learning.

01:37:07.280 --> 01:37:13.360
The mining of the right edge cases. That's what feature engineering is actually really powerful

01:37:14.080 --> 01:37:21.040
because us humans are able to do this kind of mining a little better. But yeah, there's obvious,

01:37:21.040 --> 01:37:24.560
as we know, there's obvious constraints and limitations to that idea.

01:37:25.760 --> 01:37:31.120
Carpathian just tweeted, he's like, you get really interesting insights if you sort your

01:37:31.120 --> 01:37:41.280
validation set by loss and look at the highest loss examples. So yeah, we have a little data

01:37:41.280 --> 01:37:45.760
engine-like thing. We're training a Sagnat. Anyway, it's not fancy, it's just like, okay,

01:37:46.640 --> 01:37:51.440
train the new Sagnat, run it on 100,000 images, and now take the thousand with highest loss,

01:37:52.080 --> 01:37:57.120
select 100 of those by human, put those, get those ones labeled, retrain, do it again.

01:37:57.840 --> 01:38:03.520
So it's a much less well-written data engine. And yeah, you can take these things really far.

01:38:03.520 --> 01:38:09.760
And it is impressive engineering. And if you truly need supervised data for a problem,

01:38:09.760 --> 01:38:15.440
yeah, things like data engine are at the high end of what is attention? Is a human paying

01:38:15.440 --> 01:38:19.120
attention? I mean, we're going to probably build something that looks like data engine to push

01:38:19.120 --> 01:38:24.480
our driver monitoring further. But for driving itself, you have it all annotated beautifully

01:38:24.560 --> 01:38:29.200
by what the human does. Yeah, that's interesting. I mean, that applies to driver attention as well.

01:38:29.920 --> 01:38:33.440
Do you want to detect the eyes? Do you want to detect blinking and pupil movement?

01:38:33.440 --> 01:38:38.080
Do you want to detect all the like face alignments, the landmark detection and so on,

01:38:38.640 --> 01:38:43.200
and then doing kind of reasoning based on that? Or do you want to take the entirety of the face

01:38:43.200 --> 01:38:48.880
over time and do end to end? I mean, it's obvious that eventually you have to do end to end with

01:38:48.880 --> 01:38:55.680
some calibration, some fixes and so on. But it's like, I don't know when that's the right move.

01:38:55.680 --> 01:39:02.560
Even if it's end to end, there actually is, there is no kind of, you have to supervise that with

01:39:02.560 --> 01:39:06.880
humans. Whether a human is paying attention or not is a completely subjective judgment.

01:39:08.480 --> 01:39:12.320
Like you can try to like automatically do it with some stuff, but you don't have.

01:39:13.040 --> 01:39:18.320
If I record a video of a human, I don't have true annotations anywhere in that video.

01:39:18.320 --> 01:39:22.800
The only way to get them is with other humans labeling it really.

01:39:22.800 --> 01:39:30.880
Well, I don't know. If you think deeply about it, you might be able to, depending on the task,

01:39:30.880 --> 01:39:36.240
you might be able to discover self-anitating things like, you can look at like steering

01:39:36.240 --> 01:39:40.240
wheel reverse or something like that. You can discover little moments of lapse of attention.

01:39:43.040 --> 01:39:47.360
That's where psychology comes in. Is there an indicator? Because you have so much data to look

01:39:47.360 --> 01:39:54.240
at. So you might be able to find moments when there's just inattention that even with smartphone,

01:39:54.240 --> 01:39:59.360
if you want to take smartphone use, you can start to zoom in. I mean, that's the gold mine,

01:39:59.360 --> 01:40:03.840
sort of the comma AI. I mean, Tesla is doing this too, right? They're doing

01:40:05.440 --> 01:40:11.760
annotation based on like self-supervised learning too. It's just a small part of the

01:40:11.760 --> 01:40:18.880
entire picture. That's kind of the challenge of solving a problem in machine learning if you

01:40:18.880 --> 01:40:26.720
can discover self-anitating parts of the problem, right? Our driver monitoring team is half a person

01:40:26.720 --> 01:40:31.920
right now. Half a person. You know, once we have... Scale to a full... Once we have two,

01:40:31.920 --> 01:40:36.320
three people on that team, I definitely want to look at self-anitating stuff for attention.

01:40:36.880 --> 01:40:45.360
Let's go back for a sec to a comma. For people who are curious to try it out,

01:40:46.240 --> 01:40:52.720
how do you install a comma in say a 2020 Toyota Corolla? Or like, what are the cars that are

01:40:52.720 --> 01:40:58.800
supported? What are the cars that you recommend? And what does it take? You have a few videos out,

01:40:58.800 --> 01:41:02.800
but maybe through words, can you explain what's it take to actually install a thing?

01:41:02.800 --> 01:41:09.120
So we support... I think it's 91 cars. 91 makes models. We get to 100 this year.

01:41:10.080 --> 01:41:21.200
Nice. The 2020 Corolla, great choice. The 2020 Sonata, it's using the stock longitudinal. It's

01:41:21.200 --> 01:41:26.560
using just our lateral control, but it's a very refined car. Their longitudinal control is not

01:41:26.560 --> 01:41:34.160
bad at all. So yeah, Corolla, Sonata. Or if you're willing to get your hands a little dirty and

01:41:34.160 --> 01:41:38.480
look in the right places on the internet, the Honda Civic is great, but you're going to have to

01:41:38.480 --> 01:41:42.880
install a modified EPS firmware in order to get a little bit more torque. And I can't help you

01:41:42.880 --> 01:41:48.480
with that. Comma does not efficiently endorse that, but we have been doing it. We didn't ever release

01:41:48.480 --> 01:41:54.400
it. We waited for someone else to discover it. And then, you know... And you have a Discord server

01:41:54.480 --> 01:42:02.320
where people... There's a very active developer community, I suppose. So depending on the level

01:42:02.320 --> 01:42:09.440
of experimentation you're willing to do, that's a community. If you just want to buy it and you

01:42:09.440 --> 01:42:16.960
have a supported car, it's 10 minutes to install. There's YouTube videos. It's IKEA furniture level.

01:42:16.960 --> 01:42:21.200
If you can set up a table from IKEA, you can install a Comma 2 in your supported car,

01:42:21.200 --> 01:42:25.680
and it will just work. Now you're like, oh, but I want this high-end feature or I want to fix this

01:42:25.680 --> 01:42:31.600
bug. Okay, well, welcome to the developer community. So what... If I wanted to... This is something I

01:42:31.600 --> 01:42:42.080
asked you offline, like a few months ago. If I wanted to run my own code to... So use Comma as a

01:42:42.640 --> 01:42:46.960
platform and try to run something like OpenPilot, what does it take to do that?

01:42:47.760 --> 01:42:54.000
So there's a toggle in the settings called enable SSH. And if you toggle that, you can SSH into

01:42:54.000 --> 01:42:58.560
your device. You can modify the code. You can upload whatever code you want to it. There's a whole

01:42:58.560 --> 01:43:04.800
lot of people. So about 60% of people are running stock Comma. About 40% of people are running Forks.

01:43:05.360 --> 01:43:10.800
And there's a community of... There's a bunch of people who maintain these Forks. And these Forks

01:43:10.800 --> 01:43:17.200
support different cars or they have different toggles. We try to keep away from the toggles

01:43:17.200 --> 01:43:22.320
that are disabled driver monitoring. But some people might want that kind of thing. And yeah,

01:43:22.320 --> 01:43:31.200
you can... It's your car. It's your... I'm not here to tell you. We have some... We ban... If you're

01:43:31.200 --> 01:43:34.480
trying to subvert safety features, you're banned from our Discord. I don't want anything to do with

01:43:34.480 --> 01:43:43.360
you. But there's some Forks doing that. Got it. So you encourage responsible Forking. Yeah. Yeah,

01:43:43.360 --> 01:43:48.560
we encourage... Some people... Yeah, some people... Like there's Forks that will do... Some people just

01:43:48.560 --> 01:43:54.160
like having a lot of readouts on the UI. Like a lot of flashing numbers. So there's Forks that do

01:43:54.160 --> 01:43:58.400
that. Some people don't like the fact that it disengages when you press the gas pedal. There's

01:43:58.400 --> 01:44:05.760
Forks that disable that. Got it. Now, the stock experience is what like... So it does both lane

01:44:05.760 --> 01:44:11.120
keeping and longitudinal control altogether. So it's not separate like it is an autopilot.

01:44:11.120 --> 01:44:15.760
No. So, okay. Some cars, we use the stock longitudinal control. We don't do the longitudinal

01:44:15.760 --> 01:44:19.920
control in all the cars. Some cars, the ACCs are pretty good in the cars. It's the lane

01:44:19.920 --> 01:44:25.440
keep that's atrocious in anything except for autopilot and supercruise. But you just turn it on

01:44:26.000 --> 01:44:31.680
and it works. What does disengagement look like? Yeah. So we have... I mean, I'm very concerned

01:44:31.680 --> 01:44:38.800
about mode confusion. I've experienced it on supercruise and autopilot where like autopilot

01:44:38.800 --> 01:44:44.880
disengages. I don't realize that the ACC is still on. The lead car moves slightly over and then the

01:44:44.880 --> 01:44:49.520
Tesla accelerates to like whatever my set speed is super fast and like what's going on here.

01:44:50.480 --> 01:44:56.320
We have engaged and disengaged. And this is similar to my understanding. I'm not a pilot,

01:44:56.320 --> 01:45:01.520
but my understanding is either the pilot is in control or the copilot is in control.

01:45:02.080 --> 01:45:07.440
And we have the same kind of transition system. Either open pilot is engaged or open pilot is

01:45:07.440 --> 01:45:12.400
disengaged. Engage with cruise control, disengage with either gas break or cancel.

01:45:13.200 --> 01:45:17.280
Let's talk about money. What's the business strategy for karma?

01:45:17.360 --> 01:45:26.080
Profitable. Well, you did it. So congratulations. What... So it's basically selling, we should

01:45:26.080 --> 01:45:31.520
say, karma costs a thousand bucks, comma two. Two hundred for the interface to the car as well.

01:45:31.520 --> 01:45:38.480
It's 1200. I'll send that. Nobody's usually upfront like this. You gotta add the tack on, right?

01:45:38.480 --> 01:45:43.760
I love it. I'm not gonna lie to you. Trust me, it will add $1,200 of value to your life.

01:45:43.760 --> 01:45:47.840
Yes, it's still super cheap. 30 days, no questions asked, money back guarantee,

01:45:47.840 --> 01:45:52.880
and prices are only going up. If there ever is future hardware, it could cost a lot more than

01:45:52.880 --> 01:45:59.760
$1,200. So comma three is in the works. It could be. All I will say is future hardware is going

01:45:59.760 --> 01:46:05.760
to cost a lot more than the current hardware. Yeah. The people that use... The people I've

01:46:05.760 --> 01:46:11.440
spoken with that use comma, they use open pilot, they... At first, they use it a lot.

01:46:12.080 --> 01:46:16.000
So people that use it, they fall in love with it. Oh, our retention rate is insane.

01:46:16.640 --> 01:46:23.680
It's a good sign. Yeah. It's a really good sign. 70% of comma two buyers are daily active users.

01:46:23.680 --> 01:46:30.000
Yeah, it's amazing. Oh, also, we don't plan on stopping selling the comma two.

01:46:31.840 --> 01:46:40.160
So whatever you create that's beyond comma two, it would be potentially a phase shift.

01:46:41.120 --> 01:46:45.520
It's so much better that... You could use comma two and you can use comma whatever.

01:46:45.520 --> 01:46:51.920
Depends what you want. It's 3.41, 42. Yeah. Auto pilot, hardware one versus hardware two.

01:46:51.920 --> 01:46:55.280
The comma two is kind of like hardware one. Got it. You can still use both. Got it.

01:46:56.160 --> 01:47:00.960
I think I heard you talk about retention rate with the BR headsets that the average is just once.

01:47:02.240 --> 01:47:06.640
I mean, it's such a fascinating way to think about technology. And this is a really,

01:47:06.640 --> 01:47:09.760
really good sign. And the other thing that people say about comma is they can't believe

01:47:09.760 --> 01:47:19.040
they're getting this 4,000 bucks. It seems like some kind of steal. But in terms of long-term

01:47:19.040 --> 01:47:33.040
business strategies, it's currently in 1,000 plus cars. 1,200. More. So yeah,

01:47:33.520 --> 01:47:38.800
uh, daily is about 2,000. Weekly is about 2,500. Monthly is over 3,000.

01:47:38.800 --> 01:47:42.080
Wow. We've grown a lot since we last talked.

01:47:42.080 --> 01:47:47.360
Is the goal... Can we talk crazy for a second? I mean, what's the goal to overtake Tesla?

01:47:48.560 --> 01:47:51.440
Let's talk... Okay, so... I mean, Android did overtake iOS.

01:47:51.440 --> 01:47:54.640
That's exactly it, right? So... Yeah.

01:47:54.640 --> 01:48:00.400
They did it. I actually don't know the timeline of that one. But let's talk...

01:48:00.400 --> 01:48:04.640
Because everything is in alpha now. The autopilot, you could argue, is in alpha in terms of

01:48:04.640 --> 01:48:09.760
towards the big mission of autonomous driving, right? And so what... Yeah,

01:48:09.760 --> 01:48:16.080
it's your goal to overtake millions of cars, essentially. Of course. Where would it stop?

01:48:16.640 --> 01:48:20.320
Like, it's open-source software. It might not be millions of cars with a piece of comma hardware.

01:48:20.320 --> 01:48:26.800
But yeah, I think OpenPilot at some point will cross over autopilot in users,

01:48:26.880 --> 01:48:31.040
just like Android crossed over iOS. How does Google make money from Android?

01:48:34.080 --> 01:48:36.160
It's complicated. Their own devices make money.

01:48:37.280 --> 01:48:41.040
Google... Google makes money by just kind of having you on the internet.

01:48:42.080 --> 01:48:45.440
Yes. Google Search is built in. Gmail is built in.

01:48:45.440 --> 01:48:48.080
Android is just a shield for the rest of Google's ecosystem kind.

01:48:48.080 --> 01:48:52.640
Yeah, but the problem is, Android is not... It's a brilliant thing. I mean,

01:48:53.360 --> 01:49:00.720
Android arguably changed the world. So there you go. You can feel good ethically speaking.

01:49:00.720 --> 01:49:04.160
But as a business strategy, it's questionable.

01:49:04.160 --> 01:49:04.880
It's so hardware.

01:49:05.680 --> 01:49:08.000
So hardware. I mean, it took Google a long time to come around to it,

01:49:08.000 --> 01:49:09.360
but they are now making money on the Pixel.

01:49:10.080 --> 01:49:13.120
You're not about money. You're more about winning.

01:49:13.120 --> 01:49:14.000
Yeah, of course.

01:49:14.000 --> 01:49:19.840
But if only 10% of OpenPilot devices come from comma AI...

01:49:19.840 --> 01:49:20.720
They still make a lot.

01:49:20.720 --> 01:49:22.720
That is still yes. That is a ton of money for our company.

01:49:22.720 --> 01:49:26.400
But can't somebody create a better comma using OpenPilot?

01:49:26.960 --> 01:49:28.640
Or are you basically saying we'll out-compete them?

01:49:28.640 --> 01:49:29.360
We'll out-compete you.

01:49:29.360 --> 01:49:32.160
Is... Can you create a better Android phone than the Google Pixel?

01:49:32.160 --> 01:49:32.560
Right.

01:49:32.560 --> 01:49:34.560
I mean, you can, but like, you know...

01:49:34.560 --> 01:49:38.240
I love that. So you're confident, like, you know what the hell you're doing.

01:49:38.240 --> 01:49:38.480
Yeah.

01:49:39.920 --> 01:49:43.280
It's competence and merit.

01:49:43.280 --> 01:49:46.080
I mean, our money... Yeah, our money comes from... We're a consumer electronics company.

01:49:46.080 --> 01:49:46.400
Yeah.

01:49:46.400 --> 01:49:49.840
And put it this way. So we sold... We sold like 3,000 comma twos.

01:49:51.520 --> 01:49:52.320
2,500 right now.

01:49:58.640 --> 01:50:01.040
Okay. We're probably going to sell 10,000 units next year.

01:50:01.840 --> 01:50:08.160
10,000 units. Even just $1,000 a unit. Okay. We're at $10 million in revenue.

01:50:09.280 --> 01:50:12.000
Get that up to $100,000. Maybe double the price of the unit.

01:50:12.000 --> 01:50:13.440
Now we're talking like $200 million revenue.

01:50:13.440 --> 01:50:13.840
We're talking like a series.

01:50:13.840 --> 01:50:14.800
Yeah, actually making money.

01:50:15.760 --> 01:50:20.480
One of the rare semi-autonomous or autonomous vehicle companies that are actually making

01:50:20.480 --> 01:50:21.280
money. Yeah.

01:50:22.480 --> 01:50:25.200
You know, if you have... If you look at a model,

01:50:25.200 --> 01:50:28.240
when we were just talking about this yesterday, if you look at a model and like you're testing,

01:50:28.240 --> 01:50:32.240
like you're A-B testing your model, and if you're one branch of the A-B test,

01:50:32.240 --> 01:50:35.200
the losses go down very fast in the first five epochs.

01:50:35.200 --> 01:50:39.200
That model is probably going to converge to something considerably better than the one

01:50:39.200 --> 01:50:42.880
where the losses are going down slower. Why do people think this is going to stop?

01:50:42.880 --> 01:50:45.520
Why do people think one day there's going to be a great like,

01:50:45.520 --> 01:50:48.000
well, Waymo's eventually going to surpass you guys?

01:50:48.240 --> 01:50:50.080
Well, they're not.

01:50:51.840 --> 01:50:57.760
Do you see like a world where like a Tesla or a car like a Tesla would be able to basically

01:50:57.760 --> 01:51:03.680
press a button and you like switch to open pilot, you know, you load in?

01:51:04.480 --> 01:51:10.000
No. So I think, so first off, I think that we may surpass Tesla in terms of users.

01:51:10.560 --> 01:51:13.360
I do not think we're going to surpass Tesla ever in terms of revenue.

01:51:13.360 --> 01:51:17.200
I think Tesla can capture a lot more revenue per user than we can.

01:51:17.200 --> 01:51:20.400
But this mimics the Android iOS model exactly.

01:51:20.400 --> 01:51:22.400
There may be more Android devices, but you know,

01:51:22.400 --> 01:51:24.160
there's a lot more iPhones than Google pixels.

01:51:24.160 --> 01:51:27.600
So I think there'll be a lot more Tesla cars sold than pieces of comma hardware.

01:51:30.240 --> 01:51:35.600
And then as far as a Tesla owner being able to switch to open pilot,

01:51:36.560 --> 01:51:39.120
does iOS, does iPhones run Android?

01:51:40.960 --> 01:51:44.320
No, but you can if you really want to do it, but it doesn't really make sense.

01:51:44.320 --> 01:51:45.200
Like it's not.

01:51:45.200 --> 01:51:46.160
It doesn't make sense.

01:51:46.160 --> 01:51:46.720
Who cares?

01:51:46.720 --> 01:51:53.280
What about if a large company like automakers for GM Toyota came to George

01:51:53.280 --> 01:51:59.600
Hotz or on the tech space, Amazon, Facebook, Google came with a large pile of cash?

01:52:02.320 --> 01:52:05.120
Would you consider being purchased?

01:52:07.840 --> 01:52:09.360
Do you see that as a one possible?

01:52:10.480 --> 01:52:11.440
Not seriously, no.

01:52:12.240 --> 01:52:18.640
I would probably see how much shit they'll entertain for me.

01:52:19.520 --> 01:52:22.800
And if they're willing to jump through a bunch of my hoops, then maybe.

01:52:22.800 --> 01:52:25.040
But no, not the way that M&A works today.

01:52:25.040 --> 01:52:26.480
I mean, we've been approached.

01:52:26.480 --> 01:52:27.840
And I laugh in these people's faces.

01:52:27.840 --> 01:52:28.880
I'm like, are you kidding?

01:52:30.880 --> 01:52:33.520
Yeah, because it's so demeaning.

01:52:33.520 --> 01:52:36.880
The M&A people are so demeaning to companies.

01:52:36.880 --> 01:52:41.200
They treat the startup world as their innovation ecosystem.

01:52:41.200 --> 01:52:44.960
And they think that I'm cool with going along with that so I can have some of their scam fake

01:52:44.960 --> 01:52:46.800
Fed dollars, Fed coin.

01:52:47.440 --> 01:52:51.200
What am I going to do with more Fed coin, Fed coin, man?

01:52:51.200 --> 01:52:52.000
I love that.

01:52:52.000 --> 01:52:56.080
So that's the cool thing about podcasting actually is people criticize.

01:52:56.080 --> 01:53:01.040
I don't know if you're familiar with Spotify giving Joe Rogan 100 million.

01:53:01.760 --> 01:53:02.640
I'd talk about that.

01:53:03.680 --> 01:53:09.440
And they respect, despite all the shit that people are talking about Spotify,

01:53:11.360 --> 01:53:16.320
people understand that podcasters like Joe Rogan know what the hell they're doing.

01:53:17.040 --> 01:53:20.160
So they give them money and say, just do what you do.

01:53:21.040 --> 01:53:28.320
And the equivalent for you would be like, George, do what the hell you do because you're good at it.

01:53:28.320 --> 01:53:30.080
Try not to murder too many people.

01:53:31.280 --> 01:53:37.280
There's some kind of common sense things like just don't go on a weird rampage of...

01:53:37.280 --> 01:53:40.240
Yeah, it comes down to what companies I could respect.

01:53:42.160 --> 01:53:44.640
You know, could I respect GM? Never.

01:53:46.480 --> 01:53:47.280
No, I couldn't.

01:53:47.280 --> 01:53:50.160
I mean, could I respect like a Hyundai?

01:53:50.800 --> 01:53:51.440
More so.

01:53:52.080 --> 01:53:52.400
Right?

01:53:52.400 --> 01:53:53.360
That's a lot closer.

01:53:53.360 --> 01:53:53.920
Toyota?

01:53:54.560 --> 01:53:54.960
What's your...

01:53:55.760 --> 01:53:58.480
Nah, no, Korean is the way.

01:53:59.200 --> 01:54:03.200
I think that the Japanese, the Germans, the US, they're all too...

01:54:04.080 --> 01:54:05.600
They all think they're too great to be honest.

01:54:05.600 --> 01:54:06.800
What about the tech companies?

01:54:07.440 --> 01:54:07.840
Apple?

01:54:08.720 --> 01:54:10.880
Apple is of the tech companies that I could respect.

01:54:10.880 --> 01:54:12.080
Apple is the closest.

01:54:12.080 --> 01:54:12.400
Yeah.

01:54:12.400 --> 01:54:13.200
I mean, I could never...

01:54:13.200 --> 01:54:14.240
It would be ironic.

01:54:14.240 --> 01:54:19.680
It would be ironic if Kama AI is acquired by Apple.

01:54:19.680 --> 01:54:20.320
I mean, Facebook.

01:54:20.320 --> 01:54:23.200
Look, I quit Facebook 10 years ago because I didn't respect the business model.

01:54:24.240 --> 01:54:27.760
Google has declined so fast in the last five years.

01:54:28.400 --> 01:54:32.800
What are your thoughts about Waymo and its present and its future?

01:54:32.800 --> 01:54:36.640
Let me start by saying something nice,

01:54:37.280 --> 01:54:44.400
which is I've visited them a few times and I've written in their cars,

01:54:45.280 --> 01:54:51.600
and the engineering that they're doing, both the research and the actual development,

01:54:51.600 --> 01:54:55.040
and the engineering they're doing and the scale they're actually achieving

01:54:55.040 --> 01:54:57.680
by doing it all themselves, is really impressive.

01:54:58.320 --> 01:55:00.800
And the balance of safety and innovation.

01:55:01.440 --> 01:55:07.120
And the cars work really well for the routes they drive.

01:55:07.120 --> 01:55:10.560
Like, they drive fast, which was very surprising to me.

01:55:10.560 --> 01:55:15.360
Like, it drives the speed limit, or faster than the speed limit, it goes.

01:55:16.080 --> 01:55:19.040
And it works really damn well, and the interface is nice.

01:55:19.040 --> 01:55:20.240
And channel Arizona, yeah.

01:55:20.240 --> 01:55:22.320
Yeah, and channel Arizona is in a very specific environment.

01:55:24.480 --> 01:55:28.480
It gives me enough material in my mind to push back

01:55:28.560 --> 01:55:32.240
against the madmen of the world, like George Hutz, to be like...

01:55:34.560 --> 01:55:37.920
Because you kind of imply there's zero probability they're going to win.

01:55:37.920 --> 01:55:38.800
Yeah.

01:55:38.800 --> 01:55:44.400
And after I've written in it, to me it's not zero.

01:55:44.400 --> 01:55:45.920
Oh, it's not for technology reasons.

01:55:46.640 --> 01:55:48.080
Bureaucracy?

01:55:48.080 --> 01:55:49.440
No, it's worse than that.

01:55:49.440 --> 01:55:51.040
It's actually for product reasons, I think.

01:55:51.920 --> 01:55:54.480
Oh, you think they're just not capable of creating an amazing product?

01:55:55.440 --> 01:55:58.960
No, I think that the product that they're building doesn't make sense.

01:56:00.960 --> 01:56:02.080
So, a few things.

01:56:03.200 --> 01:56:04.400
You say the Waymo's are fast.

01:56:05.760 --> 01:56:08.400
Benchmark a Waymo against a competent Uber driver.

01:56:08.960 --> 01:56:09.520
Right.

01:56:09.520 --> 01:56:09.840
Right.

01:56:09.840 --> 01:56:10.960
The Uber driver's faster.

01:56:10.960 --> 01:56:12.080
It's not even about speed.

01:56:12.080 --> 01:56:13.040
It's the thing you said.

01:56:13.040 --> 01:56:16.160
It's about the experience of being stuck at a stop sign.

01:56:16.160 --> 01:56:18.560
Because pedestrians are crossing non-stop.

01:56:20.000 --> 01:56:23.760
I like when my Uber driver doesn't come to a full stop at the stop sign, you know?

01:56:24.320 --> 01:56:31.440
And so, let's say the Waymo's are 20% slower than an Uber, right?

01:56:32.800 --> 01:56:34.240
You can argue that they're going to be cheaper.

01:56:34.880 --> 01:56:39.200
And I argue that users already have the choice to trade off money for speed.

01:56:39.200 --> 01:56:40.080
It's called Uber Pool.

01:56:42.080 --> 01:56:44.480
I think it's like 15% of rides at Uber pools.

01:56:45.360 --> 01:56:45.840
Right.

01:56:45.840 --> 01:56:48.640
Users are not willing to trade off money for speed.

01:56:49.280 --> 01:56:53.840
So, the whole product that they're building is not going to be competitive

01:56:54.640 --> 01:56:56.640
with traditional ride-sharing networks.

01:56:56.640 --> 01:56:57.120
Right.

01:56:59.200 --> 01:57:07.280
Like, and also, whether there's profit to be made depends entirely on one company having a monopoly.

01:57:07.280 --> 01:57:13.360
I think that the level for autonomous ride-sharing vehicles market is going to look a lot like

01:57:13.360 --> 01:57:17.600
the scooter market if even the technology does come to exist, which I question.

01:57:18.640 --> 01:57:20.080
Who's doing well in that market?

01:57:20.080 --> 01:57:20.320
Yeah.

01:57:20.320 --> 01:57:22.240
It's a race to the bottom, you know.

01:57:22.240 --> 01:57:27.440
Well, it could be closer like an Uber and a Lyft where it's just a one or two players.

01:57:28.000 --> 01:57:34.800
Well, the scooter people have given up trying to market scooters as a practical means of

01:57:34.800 --> 01:57:37.680
transportation and they're just like, they're super fun to ride.

01:57:37.680 --> 01:57:38.320
Look at wheels.

01:57:38.320 --> 01:57:40.080
I love those things and they're great on that front.

01:57:40.640 --> 01:57:40.960
Yeah.

01:57:40.960 --> 01:57:46.160
But from an actual transportation product perspective, I do not think scooters are viable

01:57:46.160 --> 01:57:48.240
and I do not think level four autonomous cars are viable.

01:57:49.120 --> 01:57:51.520
If you, let's play a fun experiment.

01:57:51.520 --> 01:57:56.240
If you ran, let's do Tesla and let's do Waymo.

01:57:56.240 --> 01:57:56.880
All right.

01:57:56.880 --> 01:58:02.240
If Elon Musk took a vacation for a year, he just said, screw it.

01:58:02.240 --> 01:58:05.280
I'm going to go live on an island, no electronics.

01:58:05.280 --> 01:58:08.320
And the board decides that we need to find somebody to run the company

01:58:09.040 --> 01:58:12.160
and they decide that you should run the company for a year.

01:58:12.160 --> 01:58:13.840
How do you run Tesla differently?

01:58:14.720 --> 01:58:15.600
I wouldn't change much.

01:58:16.400 --> 01:58:17.840
Do you think they're on the right track?

01:58:17.840 --> 01:58:18.640
I wouldn't change.

01:58:18.640 --> 01:58:21.600
I mean, I'd have some minor changes.

01:58:21.600 --> 01:58:29.040
But even my debate with Tesla about end-to-end versus segnets, that's just software.

01:58:29.040 --> 01:58:29.520
Who cares?

01:58:31.920 --> 01:58:34.320
It's not like you're doing something terrible with segnets.

01:58:34.320 --> 01:58:37.200
You're probably building something that's at least going to help you debug the end-to-end

01:58:37.200 --> 01:58:37.840
system a lot.

01:58:39.360 --> 01:58:44.240
It's very easy to transition from what they have to an end-to-end kind of thing.

01:58:45.200 --> 01:58:45.840
Right.

01:58:45.840 --> 01:58:51.520
And then I presume you would, in the Model Y or maybe in the Model 3,

01:58:51.520 --> 01:58:53.520
start adding driver sensing with infrared.

01:58:53.520 --> 01:58:59.520
Yes, I would add infrared lights right away to those cars.

01:59:02.160 --> 01:59:04.960
And start collecting that data and do all that kind of stuff.

01:59:04.960 --> 01:59:05.520
Yeah.

01:59:05.520 --> 01:59:06.080
Very much.

01:59:06.080 --> 01:59:07.360
I think they're already kind of doing it.

01:59:07.360 --> 01:59:09.280
It's an incredibly minor change.

01:59:09.280 --> 01:59:12.320
If I actually were CEO of Tesla first off, I'd be horrified that I wouldn't be able

01:59:12.320 --> 01:59:14.000
to do a better job as Elon.

01:59:14.000 --> 01:59:17.600
And then I would try to understand the way he's done things before.

01:59:17.600 --> 01:59:19.120
You would also have to take over as Twitter.

01:59:20.160 --> 01:59:21.360
God, I don't tweet.

01:59:22.080 --> 01:59:23.520
Yeah, what's your Twitter situation?

01:59:23.520 --> 01:59:25.440
Why are you so quiet on Twitter?

01:59:25.440 --> 01:59:25.920
Oh, I mean.

01:59:25.920 --> 01:59:30.240
I says Dukama is like, what's your social network presence like?

01:59:30.240 --> 01:59:34.160
Because on Instagram, you do live streams.

01:59:36.720 --> 01:59:41.520
You understand the music of the internet, but you don't always fully engage into it.

01:59:42.000 --> 01:59:42.640
Part-time.

01:59:42.640 --> 01:59:43.600
Why do you still have a Twitter?

01:59:44.160 --> 01:59:47.600
Yeah, I mean, Instagram is a pretty place.

01:59:47.600 --> 01:59:48.960
Instagram is a beautiful place.

01:59:48.960 --> 01:59:49.840
It glorifies beauty.

01:59:49.840 --> 01:59:52.560
I like Instagram's values as a network.

01:59:53.360 --> 02:00:01.280
Twitter glorifies conflict, glorifies like taking shots of people.

02:00:01.280 --> 02:00:08.320
And it's like, Twitter and Donald Trump are perfect for each other.

02:00:08.400 --> 02:00:11.600
So Tesla is on the right track in your view.

02:00:12.640 --> 02:00:16.480
Okay, so let's really try this experiment.

02:00:16.480 --> 02:00:20.400
If you ran Waymo, let's say they're, I don't know if you agree,

02:00:20.400 --> 02:00:23.280
but they seem to be at the head of the pack of the kind of...

02:00:25.520 --> 02:00:26.800
What would you call that approach?

02:00:27.440 --> 02:00:30.080
It's not necessarily lighter-based because it's not about lighter.

02:00:30.080 --> 02:00:31.680
Level four robotaxi.

02:00:31.680 --> 02:00:35.520
Level four robotaxi all in before making any revenue.

02:00:36.400 --> 02:00:38.560
So they're probably at the head of the pack.

02:00:38.560 --> 02:00:44.480
If you said, hey, George, can you please run this company for a year?

02:00:44.480 --> 02:00:45.920
How would you change it?

02:00:47.120 --> 02:00:49.760
I would go, I would get Anthony Lewandowski out of jail,

02:00:49.760 --> 02:00:51.200
and I would put him in charge of the company.

02:00:56.720 --> 02:00:58.080
Let's try to break that apart.

02:00:59.760 --> 02:01:01.520
You want to destroy the company by doing that?

02:01:01.840 --> 02:01:08.560
Or do you mean you like renegade style thinking that pushes,

02:01:09.360 --> 02:01:12.480
that throws away bureaucracy and goes to first principle thinking?

02:01:12.480 --> 02:01:14.000
What do you mean by that?

02:01:14.000 --> 02:01:16.000
I think Anthony Lewandowski is a genius,

02:01:16.000 --> 02:01:19.200
and I think he would come up with a much better idea

02:01:19.200 --> 02:01:20.720
of what to do with Waymo than me.

02:01:22.240 --> 02:01:23.760
So you mean that unironically.

02:01:23.760 --> 02:01:24.720
He is a genius.

02:01:24.720 --> 02:01:26.000
Oh, yes. Oh, absolutely.

02:01:26.560 --> 02:01:27.600
Without a doubt.

02:01:27.600 --> 02:01:30.320
I mean, I'm not saying there's no shortcomings,

02:01:30.960 --> 02:01:33.760
but in the interactions I've had with him, yeah.

02:01:35.680 --> 02:01:37.440
He's also willing to take, like,

02:01:37.440 --> 02:01:39.280
who knows what he would do with Waymo?

02:01:39.280 --> 02:01:41.680
I mean, he's also out there, like far more out there than I am.

02:01:41.680 --> 02:01:43.360
Yeah, his big risks.

02:01:43.360 --> 02:01:44.240
What do you make of him?

02:01:44.240 --> 02:01:46.880
I was going to talk to him in his pockets,

02:01:46.880 --> 02:01:48.080
and I was going back and forth.

02:01:48.880 --> 02:01:53.120
I'm such a gullible naive human, like I see the best in people.

02:01:53.840 --> 02:01:58.080
And I slowly started to realize that there might be some people out there

02:01:59.040 --> 02:02:04.320
that, like, have multiple faces to the world.

02:02:05.120 --> 02:02:08.000
They're, like, deceiving and dishonest.

02:02:08.000 --> 02:02:12.960
I still refuse to, like, I just, I trust people,

02:02:12.960 --> 02:02:14.480
and I don't care if I get hurt by it,

02:02:14.480 --> 02:02:17.680
but, like, you know, sometimes you have to be a little bit careful,

02:02:17.680 --> 02:02:20.400
especially platform-wise and podcast-wise.

02:02:21.440 --> 02:02:22.480
What am I supposed to think?

02:02:23.040 --> 02:02:25.040
So you think, you think he's a good person?

02:02:26.000 --> 02:02:27.600
Oh, I don't know.

02:02:27.600 --> 02:02:29.040
I don't really make moral judgments.

02:02:29.680 --> 02:02:30.640
And it's difficult to...

02:02:30.640 --> 02:02:32.400
Oh, I mean this about the Waymo.

02:02:32.400 --> 02:02:34.800
Actually, I mean that whole idea, very non-ironically,

02:02:34.800 --> 02:02:36.000
about what I would do.

02:02:36.000 --> 02:02:37.920
The problem with putting me in charge of Waymo

02:02:37.920 --> 02:02:41.520
is Waymo is already $10 billion in the haul, right?

02:02:41.520 --> 02:02:43.600
Whatever idea Waymo does, look,

02:02:43.600 --> 02:02:45.840
comma's profitable, comma's raised $8.1 million.

02:02:46.480 --> 02:02:48.000
That's small, you know, that's small money.

02:02:48.000 --> 02:02:50.720
Like, I can build a reasonable consumer electronics company

02:02:50.720 --> 02:02:52.640
and succeed wildly at that

02:02:52.720 --> 02:02:55.120
and still never be able to pay back Waymo's $10 billion.

02:02:55.680 --> 02:02:58.400
So I think the basic idea with Waymo,

02:02:58.400 --> 02:03:00.800
well, forget the $10 billion because they have some backing,

02:03:00.800 --> 02:03:03.200
but your basic thing is, like,

02:03:03.200 --> 02:03:05.600
what can we do to start making some money?

02:03:05.600 --> 02:03:07.840
Well, no, I mean, my bigger idea is, like,

02:03:07.840 --> 02:03:10.160
whatever the idea is that's going to save Waymo,

02:03:10.160 --> 02:03:11.280
I don't have it.

02:03:11.280 --> 02:03:13.360
It's going to have to be a big risk idea,

02:03:13.360 --> 02:03:15.120
and I cannot think of a better person

02:03:15.120 --> 02:03:16.480
than Anthony Lewandowski to do it.

02:03:17.680 --> 02:03:20.080
So that is completely what I would do with CEO of Waymo.

02:03:20.080 --> 02:03:22.560
I've called myself a transitionary CEO.

02:03:22.560 --> 02:03:24.640
Do everything I can to fix that situation up.

02:03:24.640 --> 02:03:25.600
Transitionary CEO, yeah.

02:03:27.920 --> 02:03:28.320
Yeah.

02:03:28.320 --> 02:03:29.600
Because I can't do it, right?

02:03:29.600 --> 02:03:32.080
Like, I can't, I can't, I mean,

02:03:32.080 --> 02:03:34.240
I can talk about how what I really want to do

02:03:34.240 --> 02:03:37.280
is just apologize for all those corny, you know,

02:03:37.280 --> 02:03:38.240
ad campaigns and be like,

02:03:38.240 --> 02:03:39.760
here's the real estate of the technology.

02:03:39.760 --> 02:03:42.160
Yeah, that's, like, I have several criticism.

02:03:42.160 --> 02:03:46.000
I'm a little bit more bullish on Waymo than you seem to be,

02:03:46.000 --> 02:03:50.720
but one criticism I have is it went into corny mode too early.

02:03:50.720 --> 02:03:52.080
Like, it's still a startup.

02:03:52.080 --> 02:03:53.600
It hasn't delivered on anything.

02:03:53.600 --> 02:03:55.680
So it should be, like, more renegade

02:03:56.320 --> 02:03:59.200
and show off the engineering that they're doing,

02:03:59.200 --> 02:04:00.480
which just can be impressive,

02:04:00.480 --> 02:04:02.640
as opposed to doing these weird commercials of, like,

02:04:03.600 --> 02:04:06.880
your friendly car company.

02:04:06.880 --> 02:04:07.840
I mean, that's my biggest,

02:04:07.840 --> 02:04:09.920
my biggest snipe at Waymo was always,

02:04:09.920 --> 02:04:11.280
that guy's a paid actor.

02:04:11.280 --> 02:04:12.720
That guy's not a Waymo user.

02:04:12.720 --> 02:04:13.520
He's a paid actor.

02:04:13.520 --> 02:04:15.280
Look here, I found his call sheet.

02:04:15.280 --> 02:04:18.720
Do kind of like what SpaceX is doing with the rocket launch

02:04:18.720 --> 02:04:20.880
is just get, put the nerds up front,

02:04:20.880 --> 02:04:22.080
put the engineers up front,

02:04:22.080 --> 02:04:25.600
and just, like, show failures too, just...

02:04:25.600 --> 02:04:27.760
I love SpaceX's, yeah.

02:04:27.760 --> 02:04:29.760
Yeah, the thing that they're doing is right,

02:04:29.760 --> 02:04:31.600
and it just feels like the right...

02:04:31.600 --> 02:04:34.320
But we're all so excited to see them succeed.

02:04:34.320 --> 02:04:34.640
Yeah.

02:04:34.640 --> 02:04:36.640
I can't wait to see Waymo fail, you know?

02:04:37.200 --> 02:04:38.080
Like, you lie to me.

02:04:38.080 --> 02:04:39.280
I want you to fail.

02:04:39.280 --> 02:04:40.240
You tell me the truth.

02:04:40.240 --> 02:04:41.040
You be honest with me.

02:04:41.040 --> 02:04:42.000
I want you to succeed.

02:04:42.000 --> 02:04:42.320
Yeah.

02:04:44.880 --> 02:04:48.720
Yeah, and that requires the renegade CEO, right?

02:04:49.680 --> 02:04:50.880
I'm with you.

02:04:50.880 --> 02:04:51.360
I'm with you.

02:04:51.360 --> 02:04:52.960
I still have a little bit of faith in Waymo

02:04:53.920 --> 02:04:56.160
for the renegade CEO to step forward, but...

02:04:57.760 --> 02:04:59.440
It's not, it's not John Kraft.

02:05:00.560 --> 02:05:01.280
Yeah.

02:05:01.280 --> 02:05:02.800
It's, you can't...

02:05:02.800 --> 02:05:03.760
It's not Chris Omston.

02:05:04.800 --> 02:05:07.600
And those people may be very good at certain things.

02:05:07.600 --> 02:05:08.160
Yeah.

02:05:08.160 --> 02:05:09.280
But they're not renegades.

02:05:10.240 --> 02:05:11.920
Yeah, because these companies are fundamentally,

02:05:11.920 --> 02:05:14.240
even though we're talking about billion dollars,

02:05:14.240 --> 02:05:15.600
all these crazy numbers,

02:05:15.600 --> 02:05:18.400
they're still, like, early-stage startups.

02:05:19.120 --> 02:05:21.760
I mean, I just, if you are pre-revenue

02:05:21.760 --> 02:05:24.160
and you've raised $10 billion, I have no idea.

02:05:24.160 --> 02:05:25.760
Like, this just doesn't work.

02:05:26.320 --> 02:05:27.840
You know, it's against everything Silicon Valley.

02:05:27.840 --> 02:05:29.120
Where's your minimum viable product?

02:05:29.680 --> 02:05:31.360
You know, where's your users?

02:05:31.360 --> 02:05:32.320
Where's your growth numbers?

02:05:33.200 --> 02:05:35.520
This is traditional Silicon Valley.

02:05:36.160 --> 02:05:37.920
Why do you not apply it to what you think

02:05:37.920 --> 02:05:39.280
you're too big to fail already?

02:05:39.280 --> 02:05:39.520
Like...

02:05:41.600 --> 02:05:45.680
How do you think autonomous driving will change society?

02:05:45.680 --> 02:05:50.800
So the mission is, for comma, to solve self-driving.

02:05:52.560 --> 02:05:55.520
Do you have, like, a vision of the world of how it'll be different?

02:05:57.840 --> 02:06:00.000
Is it as simple as A to B transportation?

02:06:00.000 --> 02:06:01.040
Or is there, like...

02:06:01.040 --> 02:06:02.160
Because these are robots.

02:06:03.520 --> 02:06:05.760
It's not about autonomous driving in and of itself.

02:06:05.760 --> 02:06:06.960
It's what the technology enables.

02:06:09.600 --> 02:06:12.080
It's, I think it's the coolest applied AI problem.

02:06:12.080 --> 02:06:14.560
I like it because it has a clear path

02:06:15.040 --> 02:06:15.840
to monetary value.

02:06:17.600 --> 02:06:20.560
But as far as that being the thing that changes the world...

02:06:21.280 --> 02:06:23.520
I mean, no.

02:06:23.520 --> 02:06:25.360
Like, there's cute things we're doing in comma.

02:06:25.360 --> 02:06:26.800
Like, who'd have thought you could stick a phone

02:06:26.800 --> 02:06:28.080
on the windshield and it'll drive.

02:06:28.880 --> 02:06:31.040
But, like, really the product that you're building

02:06:31.040 --> 02:06:33.760
is not something that people were not capable

02:06:33.760 --> 02:06:35.040
of imagining 50 years ago.

02:06:35.600 --> 02:06:37.040
So, no, it doesn't change the world on that front.

02:06:37.600 --> 02:06:39.360
Could people have imagined the Internet 50 years ago?

02:06:39.360 --> 02:06:41.760
Only true genius visionaries.

02:06:42.320 --> 02:06:44.880
Everyone could have imagined autonomous cars 50 years ago.

02:06:44.880 --> 02:06:46.960
It's like a car, but I don't drive it.

02:06:46.960 --> 02:06:49.360
See, I have the sense, and I told you, like,

02:06:49.360 --> 02:06:57.200
I'm my long-term dream is robots with whom you have deep connections.

02:06:59.200 --> 02:07:02.240
And there's different trajectories towards that.

02:07:03.440 --> 02:07:06.720
And I've been thinking of launching a startup.

02:07:08.240 --> 02:07:11.280
I see autonomous vehicles as a potential trajectory to that.

02:07:12.240 --> 02:07:16.480
That's not where the direction I would like to go.

02:07:16.480 --> 02:07:19.840
But I also see Tesla or even Kamii like pivoting

02:07:19.840 --> 02:07:25.840
into robotics broadly defined at some stage.

02:07:25.840 --> 02:07:28.240
In the way, like you're mentioning, the Internet didn't expect.

02:07:29.520 --> 02:07:32.400
Let's solve, you know, when I say a comma about this.

02:07:32.400 --> 02:07:34.560
We could talk about this, but let's solve self-driving cars first.

02:07:35.520 --> 02:07:36.800
Gotta stay focused on the mission.

02:07:37.920 --> 02:07:39.120
You're not too big to fail.

02:07:39.120 --> 02:07:42.720
For however much I think comm is winning, like, no, no, no, no.

02:07:42.720 --> 02:07:44.880
You're winning when you solve level five self-driving cars.

02:07:44.880 --> 02:07:46.720
And until then, you haven't won and won.

02:07:46.720 --> 02:07:49.920
And, you know, again, you want to be arrogant in the face of other people?

02:07:49.920 --> 02:07:51.760
Great. You want to be arrogant in the face of nature?

02:07:51.760 --> 02:07:52.400
You're an idiot.

02:07:52.400 --> 02:07:55.360
Right. Stay mission focused, brilliantly put.

02:07:56.320 --> 02:07:58.480
Like I mentioned, thinking of launching a startup,

02:07:58.480 --> 02:08:01.120
I've been considering, actually, before COVID,

02:08:01.120 --> 02:08:03.200
I've been thinking of moving to San Francisco.

02:08:03.200 --> 02:08:04.880
Oh, I wouldn't go there.

02:08:05.840 --> 02:08:10.240
So why is, well, and now I'm thinking about potentially Austin.

02:08:12.000 --> 02:08:13.440
And we're in San Diego now.

02:08:13.440 --> 02:08:14.800
San Diego, come here.

02:08:14.800 --> 02:08:20.400
So why, what, I mean, you're such an interesting human.

02:08:20.400 --> 02:08:22.160
You've launched so many successful things.

02:08:23.040 --> 02:08:26.080
What, why San Diego?

02:08:26.080 --> 02:08:26.880
What do you recommend?

02:08:26.880 --> 02:08:28.160
Why not San Francisco?

02:08:29.120 --> 02:08:31.760
Have you thought, so in your case,

02:08:31.760 --> 02:08:36.080
San Diego with Qualcomm and Staff Dragon, I mean, that's an amazing combination.

02:08:36.880 --> 02:08:37.920
But that wasn't really why.

02:08:38.480 --> 02:08:39.440
That wasn't the why?

02:08:39.440 --> 02:08:41.200
No. I mean, Qualcomm was an afterthought.

02:08:41.200 --> 02:08:42.720
Qualcomm was, it was a nice thing to think about.

02:08:42.720 --> 02:08:45.600
It's like you can have a tech company here and a good one.

02:08:45.600 --> 02:08:48.320
I mean, you know, I like Qualcomm, but no.

02:08:48.320 --> 02:08:50.400
Well, so why is San Diego better than San Francisco?

02:08:50.400 --> 02:08:51.760
Why does San Francisco suck?

02:08:51.760 --> 02:08:52.240
Well, so, okay.

02:08:52.240 --> 02:08:55.200
So first off, we all kind of said, like, we want to stay in California,

02:08:55.200 --> 02:08:59.840
people like the ocean, you know, California for its flaws.

02:08:59.920 --> 02:09:03.760
It's like a lot of the flaws of California are not necessarily California as a whole,

02:09:03.760 --> 02:09:05.440
and they're much more San Francisco specific.

02:09:06.560 --> 02:09:11.200
San Francisco, so I think first-tier cities in general have stopped wanting growth.

02:09:13.200 --> 02:09:17.040
Well, you have like in San Francisco, you know, the voting class always

02:09:17.040 --> 02:09:20.960
votes to not build more houses because they own all the houses and they're like, well,

02:09:20.960 --> 02:09:25.040
you know, once people have figured out how to vote themselves more money, they're going to do it.

02:09:25.040 --> 02:09:26.800
It is so insanely corrupt.

02:09:27.760 --> 02:09:31.440
It is not balanced at all, like political party-wise.

02:09:31.440 --> 02:09:33.680
You know, it's a one-party city.

02:09:33.680 --> 02:09:42.000
And for all the discussion of diversity, it stops lacking real diversity of thought,

02:09:42.000 --> 02:09:47.280
of background, of approaches, of strategies, of ideas.

02:09:48.880 --> 02:09:53.760
It's kind of a strange place that it's the loudest people about diversity

02:09:54.320 --> 02:09:56.400
and the biggest lack of diversity.

02:09:56.400 --> 02:09:58.560
I mean, that's what they say, right?

02:09:58.560 --> 02:09:59.520
It's the projection.

02:10:00.240 --> 02:10:01.040
Projection, yeah.

02:10:02.000 --> 02:10:02.800
Yeah, it's interesting.

02:10:02.800 --> 02:10:07.200
And even people in Silicon Valley telling me that's like high up people,

02:10:07.920 --> 02:10:09.920
everybody is like, this is a terrible place.

02:10:09.920 --> 02:10:10.480
It doesn't make sense.

02:10:10.480 --> 02:10:13.120
I mean, and coronavirus is really what killed it.

02:10:13.120 --> 02:10:18.000
San Francisco was the number one Exodus during coronavirus.

02:10:18.800 --> 02:10:21.440
We still think San Diego is a good place to be.

02:10:23.360 --> 02:10:24.560
Yeah, I mean, we'll see.

02:10:24.640 --> 02:10:29.040
We'll see what happens with California a bit longer term.

02:10:29.760 --> 02:10:31.920
Like, Austin's an interesting choice.

02:10:31.920 --> 02:10:34.480
I wouldn't, I wouldn't, I don't have really anything bad to say about Austin

02:10:35.280 --> 02:10:38.400
either except for the extreme heat in the summer, which, you know,

02:10:38.400 --> 02:10:40.080
but that's like very on the surface, right?

02:10:40.080 --> 02:10:43.600
I think as far as like an ecosystem goes, it's cool.

02:10:43.600 --> 02:10:45.120
I personally love Colorado.

02:10:45.120 --> 02:10:46.080
Colorado is great.

02:10:46.960 --> 02:10:51.040
Yeah, I mean, you have these states that are, you know, like just way better run.

02:10:51.920 --> 02:10:55.200
Um, California is, you know, it's especially San Francisco.

02:10:55.200 --> 02:10:58.320
It's on its high horse and like, yeah.

02:10:58.320 --> 02:11:02.880
Can I ask you for advice to me and to others about

02:11:04.560 --> 02:11:06.720
what's the take to build a successful startup?

02:11:07.360 --> 02:11:07.840
Oh, I don't know.

02:11:07.840 --> 02:11:08.480
I haven't done that.

02:11:09.040 --> 02:11:10.000
Talk to someone who did that.

02:11:10.640 --> 02:11:18.480
Well, you know, this is like another book of yours that I'll buy for $67, I suppose.

02:11:19.200 --> 02:11:20.400
Uh, so there's, um,

02:11:22.640 --> 02:11:24.000
one of these days I'll sell out.

02:11:24.000 --> 02:11:24.640
Yeah, that's right.

02:11:24.640 --> 02:11:27.200
Jail breaks are going to be a dollar and books are going to be 67.

02:11:27.840 --> 02:11:31.920
How I, uh, how I, you know, broke the iPhone by George Hots.

02:11:31.920 --> 02:11:32.720
That's right.

02:11:32.720 --> 02:11:35.680
How I jail broke the iPhone and you can do it.

02:11:35.680 --> 02:11:38.800
You can't do it in 21 days.

02:11:38.800 --> 02:11:39.360
That's right.

02:11:39.360 --> 02:11:40.240
That's right.

02:11:40.240 --> 02:11:41.040
Oh God.

02:11:41.040 --> 02:11:41.440
Okay.

02:11:41.440 --> 02:11:45.680
I can't wait, but quite, so you have an introspective, you have built

02:11:46.640 --> 02:11:49.360
a very unique company.

02:11:49.360 --> 02:11:51.840
I mean, not, not you, but you and others.

02:11:53.040 --> 02:11:56.880
But I don't know, um, there's no, there's nothing.

02:11:56.880 --> 02:12:00.480
You have an interest, but you haven't really sat down and thought about like,

02:12:01.600 --> 02:12:05.200
well, like if you and I were having a bunch of, we're having some beers

02:12:06.080 --> 02:12:09.120
and you're seeing that I'm depressed and whatever I'm struggling.

02:12:09.760 --> 02:12:10.960
There's no advice you can give.

02:12:11.520 --> 02:12:13.600
Oh, I mean, more beer.

02:12:14.400 --> 02:12:14.960
More beer.

02:12:16.240 --> 02:12:18.720
Uh, uh, yeah.

02:12:18.720 --> 02:12:22.160
I think it's all very like situation dependent.

02:12:22.720 --> 02:12:24.160
Um, here's, okay.

02:12:24.160 --> 02:12:27.120
If I can give a generic piece of advice, it's the technology always wins.

02:12:28.080 --> 02:12:33.040
The better technology always wins and lying always loses.

02:12:35.120 --> 02:12:36.960
Build technology and don't lie.

02:12:38.480 --> 02:12:39.120
I'm with you.

02:12:39.120 --> 02:12:40.400
I agree very much.

02:12:40.400 --> 02:12:40.880
Long run.

02:12:40.880 --> 02:12:41.360
Long run.

02:12:41.360 --> 02:12:41.440
Sure.

02:12:41.440 --> 02:12:42.240
That's the long run.

02:12:42.240 --> 02:12:42.880
And you know what?

02:12:42.880 --> 02:12:45.840
The market can remain irrational longer than you can remain solvent.

02:12:46.400 --> 02:12:47.120
True fact.

02:12:47.120 --> 02:12:52.800
Well, this is, this is an interesting point because I ethically and just as a human believe that, um,

02:12:54.320 --> 02:13:01.840
like, like hype and smoke and mirrors is not at any stage of the company is a good strategy.

02:13:02.560 --> 02:13:08.080
I mean, there's some like, you know, PR magic kind of like, you know, you want a new product,

02:13:08.080 --> 02:13:08.400
right?

02:13:08.400 --> 02:13:12.560
If there's a call to action, if there's like a call to action like buy my new GPU,

02:13:12.560 --> 02:13:12.960
look at it.

02:13:12.960 --> 02:13:14.880
It takes up three slots and it's this big.

02:13:14.880 --> 02:13:16.160
It's huge buy my GPU.

02:13:16.160 --> 02:13:16.960
Yeah, that's great.

02:13:16.960 --> 02:13:22.000
If you look at, you know, especially in that, in AI space broadly, but autonomous vehicles,

02:13:22.640 --> 02:13:25.680
like you can raise a huge amount of money on nothing.

02:13:26.480 --> 02:13:28.880
And the question to me is like, I'm against that.

02:13:29.920 --> 02:13:31.440
I'll never be part of that.

02:13:31.440 --> 02:13:35.520
I don't think I hope not willingly not.

02:13:35.680 --> 02:13:44.880
But like, is there something to be said to, uh, essentially lying to raise money,

02:13:44.880 --> 02:13:46.640
like fake it till you make it kind of thing?

02:13:47.760 --> 02:13:50.080
I mean, this is Billy McFarlane the fire festival.

02:13:50.080 --> 02:13:54.240
Like we all, we all experienced, uh, you know, what happens with that?

02:13:54.240 --> 02:13:57.440
No, no, don't fake it till you make it.

02:13:57.440 --> 02:14:00.480
Be honest and hope you make it the whole way.

02:14:00.480 --> 02:14:01.840
The technology wins.

02:14:01.840 --> 02:14:02.160
Right.

02:14:02.160 --> 02:14:02.960
The technology wins.

02:14:02.960 --> 02:14:07.600
And like there is, I'm not used to like the anti-hype, you know, that's, that's

02:14:07.600 --> 02:14:13.120
Slava KPSS reference, but, um, hype isn't necessarily bad.

02:14:13.120 --> 02:14:15.440
I loved camping out for the iPhones.

02:14:16.960 --> 02:14:21.760
You know, and as long as the hype is backed by like substance, as long as it's backed by

02:14:21.760 --> 02:14:27.840
something I can actually buy and like it's real, then hype is great and it's a great feeling.

02:14:28.560 --> 02:14:31.920
It's when the hype is backed by lies that it's a bad feeling.

02:14:31.920 --> 02:14:34.400
I mean, a lot of people call Elon Musk a fraud.

02:14:34.400 --> 02:14:35.520
How could he be a fraud?

02:14:35.520 --> 02:14:41.280
I've noticed this, this kind of interesting effect, which is he does tend to over promise

02:14:42.320 --> 02:14:45.680
and deliver, what's, what's the better way to phrase it?

02:14:45.680 --> 02:14:50.800
Promise a timeline that he doesn't deliver on, he delivers much later on.

02:14:51.600 --> 02:14:52.640
What do you think about that?

02:14:52.640 --> 02:14:53.520
Because I do that.

02:14:53.520 --> 02:14:55.280
I think that's a programmer thing too.

02:14:56.080 --> 02:14:57.600
I do that as well.

02:14:57.600 --> 02:15:01.040
You think that's a really bad thing to do or is that okay?

02:15:01.040 --> 02:15:06.400
Oh, I think that's again, as long as like you're working toward it and you're going to deliver

02:15:06.400 --> 02:15:09.520
on it and it's not too far off, right?

02:15:09.520 --> 02:15:10.160
Yeah.

02:15:10.160 --> 02:15:10.720
Right?

02:15:10.720 --> 02:15:15.200
Like, like, you know, the whole, the whole autonomous vehicle thing, it's like,

02:15:16.000 --> 02:15:19.680
I mean, I still think Tesla's on track to beat us.

02:15:19.680 --> 02:15:24.000
I still think even with their, even with their missteps, they have advantages we don't have.

02:15:24.880 --> 02:15:32.160
You know, Elon is better than me at like marshalling massive amounts of resources.

02:15:33.040 --> 02:15:38.160
So, you know, I still think given the fact they're maybe making some wrong decisions,

02:15:38.160 --> 02:15:39.360
they'll end up winning.

02:15:39.360 --> 02:15:45.120
And like, it's fine to hype it if you're actually going to win, right?

02:15:45.120 --> 02:15:49.360
If Elon says, look, we're going to be landing rockets back on Earth in a year and it takes four,

02:15:49.360 --> 02:15:55.360
like, you know, he landed a rocket back on Earth and he was working toward it the whole time.

02:15:55.360 --> 02:15:59.440
I think there's some amount of like, I think what it becomes wrong is if you know you're not

02:15:59.440 --> 02:16:00.400
going to meet that deadline.

02:16:00.400 --> 02:16:01.520
If you're lying.

02:16:01.520 --> 02:16:02.080
Yeah.

02:16:02.080 --> 02:16:03.200
That's brilliantly put.

02:16:03.200 --> 02:16:06.800
Like, this is what people don't understand, I think.

02:16:06.800 --> 02:16:09.120
Like, Elon believes everything he says.

02:16:09.120 --> 02:16:10.320
He does.

02:16:10.320 --> 02:16:12.080
As far as I can tell, he does.

02:16:12.080 --> 02:16:14.720
And I detected that in myself too.

02:16:14.720 --> 02:16:21.280
Like, if I, it's only bullshit if you're like conscious of yourself lying.

02:16:21.280 --> 02:16:22.480
Yeah, I think so.

02:16:22.480 --> 02:16:23.280
Yeah.

02:16:23.280 --> 02:16:25.600
No, you can't take that to such an extreme, right?

02:16:25.600 --> 02:16:28.960
Like, in a way, I think maybe Billy McFarland believed everything he said too.

02:16:29.920 --> 02:16:30.320
Right.

02:16:30.320 --> 02:16:33.680
That's how you start a cult and then everybody kills themselves.

02:16:33.680 --> 02:16:34.240
Yeah.

02:16:34.240 --> 02:16:34.640
Yeah.

02:16:34.640 --> 02:16:39.200
Like, it's you need, you need, if there's like some factor on it, it's fine.

02:16:39.200 --> 02:16:42.800
And you need some people to like, you know, keep you in check.

02:16:42.800 --> 02:16:48.720
But like, if you deliver on most of the things you say and just the timelines are off, man.

02:16:48.720 --> 02:16:50.240
It does piss people off though.

02:16:50.240 --> 02:16:56.800
I wonder, but who cares in a long arc of history that people, everybody gets pissed off at the

02:16:56.800 --> 02:17:03.280
people who succeed, which is one of the things that frustrates me about this world is they

02:17:03.280 --> 02:17:06.480
don't celebrate the success of others.

02:17:07.600 --> 02:17:11.760
Like, there's so many people that want Elon to fail.

02:17:12.880 --> 02:17:14.800
It's so fascinating to me.

02:17:14.800 --> 02:17:16.960
Like, what is wrong with you?

02:17:18.080 --> 02:17:23.440
Like, so Elon Musk talks about like people at short, like they talk about financial,

02:17:23.440 --> 02:17:25.280
but I think it's much bigger than the financials.

02:17:25.280 --> 02:17:30.480
I've seen like the human factors community, they want, they want other people to fail.

02:17:31.520 --> 02:17:31.840
What?

02:17:31.840 --> 02:17:32.240
What?

02:17:32.240 --> 02:17:32.800
What?

02:17:32.800 --> 02:17:38.480
Like, even people, the harshest thing is like, you know, even people that like seem to really

02:17:38.480 --> 02:17:41.280
hate Donald Trump, they want him to fail.

02:17:41.280 --> 02:17:41.840
Yeah, I know.

02:17:41.840 --> 02:17:45.040
Or like the other president, or they want Barack Obama to fail.

02:17:45.760 --> 02:17:46.080
It's like.

02:17:46.960 --> 02:17:48.400
We're all on the same boat, man.

02:17:49.600 --> 02:17:54.400
It's weird, but I want that, I would love to inspire that part of the world to change because,

02:17:55.520 --> 02:18:00.400
well, damn it, if the human species is going to survive, we can celebrate success.

02:18:00.400 --> 02:18:05.040
Like, it seems like the efficient thing to do in this objective function that like we're all

02:18:05.040 --> 02:18:10.400
striving for is to celebrate the ones that like figure out how to like do better at that

02:18:10.480 --> 02:18:15.840
objective function as opposed to like dragging them down back into them, into the mud.

02:18:16.560 --> 02:18:20.720
I think there is, this is the speech I always give about the commenters on hacker news.

02:18:21.600 --> 02:18:24.720
So first off, something to remember about the internet in general,

02:18:24.720 --> 02:18:28.560
is commenters are not representative of the population.

02:18:29.200 --> 02:18:30.560
I don't comment on anything.

02:18:31.440 --> 02:18:36.080
You know, commenters are representative of a certain sliver of the population.

02:18:36.640 --> 02:18:40.960
And on hacker news, a common thing I'll say is when you'll see something that's like,

02:18:42.000 --> 02:18:46.640
you know, promises to be wild out there and innovative.

02:18:47.440 --> 02:18:52.240
There is some amount of, you know, checking them back to earth, but there's also some amount of,

02:18:52.240 --> 02:19:00.000
if this thing succeeds, well, I'm 36 and I've worked at large tech companies my whole life.

02:19:00.000 --> 02:19:06.880
They can't succeed because if they succeed, that would mean that I could have done something

02:19:06.880 --> 02:19:10.160
different with my life, but we know that I could have, we know that I couldn't have,

02:19:10.160 --> 02:19:11.840
and that's why they're going to fail.

02:19:11.840 --> 02:19:15.040
And they have to root for them to fail to kind of maintain their world image.

02:19:17.440 --> 02:19:17.840
Tune it out.

02:19:17.840 --> 02:19:19.600
And they comment, well, it's hard.

02:19:20.480 --> 02:19:27.520
So one of the things, one of the things I'm considering startup wise is to change that

02:19:27.520 --> 02:19:31.680
because I think the, I think it's also a technology problem.

02:19:31.680 --> 02:19:32.960
It's a platform problem.

02:19:32.960 --> 02:19:33.520
I agree.

02:19:33.520 --> 02:19:37.040
It's like, because the thing you said most people don't comment,

02:19:39.600 --> 02:19:41.440
I think most people want to comment.

02:19:42.880 --> 02:19:45.840
They just don't because it's all the assholes for commenting.

02:19:45.840 --> 02:19:46.240
Exactly.

02:19:46.240 --> 02:19:47.920
I don't want to be grouped in with them on that.

02:19:47.920 --> 02:19:50.720
You don't want to be at a party where everyone's an asshole.

02:19:50.720 --> 02:19:54.400
And so they, but that's a platform problem that's.

02:19:54.400 --> 02:19:56.000
I can't believe what Reddit's become.

02:19:56.000 --> 02:19:58.960
I can't believe the group think in Reddit comments.

02:20:00.480 --> 02:20:04.240
There's a, Reddit is interesting one because they're subreddits.

02:20:05.040 --> 02:20:10.000
And so you can still see, especially small subreddits that like,

02:20:10.000 --> 02:20:17.040
that are little like havens of like joy and positivity and like deep, even disagreement,

02:20:17.040 --> 02:20:20.800
but like nuanced discussion, but it's only like small little pockets.

02:20:21.600 --> 02:20:23.360
But that's, that's emergent.

02:20:23.360 --> 02:20:26.080
The platform is not helping that or herring that.

02:20:26.960 --> 02:20:30.240
So I guess naturally something about the internet,

02:20:31.040 --> 02:20:36.560
if you don't put in a lot of effort to encourage nuance and positive, good vibes,

02:20:37.120 --> 02:20:41.120
it's naturally going to decline into chaos.

02:20:41.120 --> 02:20:42.800
I would love to see someone do this well.

02:20:42.800 --> 02:20:43.040
Yeah.

02:20:43.680 --> 02:20:45.440
I think it's, yeah, very doable.

02:20:46.000 --> 02:20:51.280
I think actually, so I feel like Twitter could be overthrown.

02:20:52.080 --> 02:20:57.360
Yasho Bach talked about how like, if you have like and retweet,

02:20:57.920 --> 02:21:02.000
like that's only positive wiring, right?

02:21:02.000 --> 02:21:07.520
The only way to do anything like negative there is with a comment.

02:21:08.240 --> 02:21:12.880
And that's like that asymmetry is what gives, you know,

02:21:12.880 --> 02:21:14.880
Twitter its particular toxicness.

02:21:15.440 --> 02:21:19.040
Whereas I find YouTube comments to be much better because YouTube comments have a,

02:21:19.040 --> 02:21:22.560
have a, have an up and a down and they don't show the downloads.

02:21:23.680 --> 02:21:26.720
Without getting into depth of this particular discussion,

02:21:26.720 --> 02:21:30.800
the point is to explore possibilities and get a lot of data on it.

02:21:30.800 --> 02:21:34.000
Because I mean, I could disagree with what you just said.

02:21:34.000 --> 02:21:35.920
It's, it's on the point is it's unclear.

02:21:35.920 --> 02:21:38.800
It's a, it hasn't been explored in a really rich way.

02:21:39.760 --> 02:21:45.760
Like the, these questions of how to create platforms that encourage positivity.

02:21:46.960 --> 02:21:47.280
Yeah.

02:21:47.280 --> 02:21:49.440
I think it's a, it's a technology problem.

02:21:49.440 --> 02:21:51.840
And I think we'll look back at Twitter as it is now.

02:21:51.840 --> 02:21:56.640
Maybe it'll happen within Twitter, but most likely somebody overthrows them is,

02:21:57.680 --> 02:22:03.040
we'll look back at Twitter and say, we can't believe we put up with this level of toxicity.

02:22:03.040 --> 02:22:04.320
You need a different business model too.

02:22:04.960 --> 02:22:08.560
Any, any social network that fundamentally has advertising as a business model,

02:22:08.560 --> 02:22:11.600
this was in the social dilemma, which I didn't watch, but I liked it.

02:22:11.600 --> 02:22:14.160
It's like, you know, there's always the, you know, you're the product, you're not the,

02:22:15.280 --> 02:22:17.840
but they had a nuanced take on it that I really liked.

02:22:17.840 --> 02:22:22.960
And it said, the product being sold is influence over you.

02:22:24.480 --> 02:22:28.480
The product being sold is literally your, you know, influence on you.

02:22:30.640 --> 02:22:33.600
That can't be, if that's your idea, okay.

02:22:33.600 --> 02:22:35.440
Well, you know, guess what?

02:22:35.440 --> 02:22:36.960
It can't not be toxic.

02:22:36.960 --> 02:22:37.520
Yeah.

02:22:37.520 --> 02:22:42.400
Maybe there's ways to spin it, like with, with giving a lot more control to the user

02:22:42.400 --> 02:22:47.200
and transparency to see what is happening to them as opposed to in the shadows as possible.

02:22:47.200 --> 02:22:49.200
But that can't be the primary source of,

02:22:49.200 --> 02:22:50.720
But the users aren't, no one's going to use that.

02:22:51.840 --> 02:22:52.480
It depends.

02:22:52.480 --> 02:22:53.040
It depends.

02:22:53.040 --> 02:22:53.840
It depends.

02:22:53.840 --> 02:23:00.160
I think, I think that the, your, your not going to, you can't depend on self-awareness of the users.

02:23:00.160 --> 02:23:04.560
It's a, it's another, it's a longer discussion because you can't depend on it, but

02:23:05.040 --> 02:23:08.240
you can reward self-awareness.

02:23:08.240 --> 02:23:12.480
Like if, for the ones who are willing to put in the work of self-awareness,

02:23:12.480 --> 02:23:17.680
you can reward them and incentivize and perhaps be pleasantly surprised how many people

02:23:18.960 --> 02:23:21.840
are willing to be self-aware on the internet.

02:23:21.840 --> 02:23:23.200
Like we are in real life.

02:23:23.200 --> 02:23:27.120
Like I'm putting in a lot of effort with you right now being self-aware about,

02:23:27.120 --> 02:23:31.440
if I say something stupid or mean, I'll like look at your, like body language.

02:23:31.440 --> 02:23:34.240
Like I'm putting in that effort, it's costly.

02:23:34.240 --> 02:23:36.240
For an introvert, it's a great costly thing.

02:23:36.240 --> 02:23:38.480
But on the internet, fuck it.

02:23:39.440 --> 02:23:43.360
Like most people are like, I don't care if this hurts somebody.

02:23:43.360 --> 02:23:48.560
I don't care if this is not interesting or if this is, yeah, the mean or whatever.

02:23:48.560 --> 02:23:52.320
I think so much of the engagement today on the internet is so disingenuine too.

02:23:52.880 --> 02:23:55.360
You're not doing this out of a genuine, this is what you think.

02:23:55.360 --> 02:23:57.520
You're doing this just straight up to manipulate others.

02:23:57.520 --> 02:23:59.200
Whether you're in, you just became an ad.

02:23:59.760 --> 02:24:03.600
Yeah. Okay, let's talk about a fun topic, which is programming.

02:24:04.320 --> 02:24:05.680
Here's another book idea for you.

02:24:05.680 --> 02:24:06.320
Let me pitch.

02:24:07.200 --> 02:24:09.600
What's your perfect programming setup?

02:24:09.600 --> 02:24:12.640
So like, this by George Hott's.

02:24:12.640 --> 02:24:16.960
So like what, listen, you're-

02:24:16.960 --> 02:24:21.040
Give me a MacBook Air, sit me in a corner of a hotel room and you know I'll still have food.

02:24:21.040 --> 02:24:21.920
So you really don't care.

02:24:21.920 --> 02:24:25.920
You don't fetishize like multiple monitors, keyboard.

02:24:26.880 --> 02:24:30.160
Those things are nice and I'm not going to say no to them,

02:24:30.160 --> 02:24:33.120
but do they automatically unlock tons of productivity?

02:24:33.120 --> 02:24:33.920
No, not at all.

02:24:33.920 --> 02:24:37.360
I have definitely been more productive on a MacBook Air in a corner of a hotel room.

02:24:38.160 --> 02:24:40.640
What about IDE?

02:24:41.520 --> 02:24:45.120
So which operating system do you love?

02:24:45.920 --> 02:24:48.880
What text editor do you use, IDE?

02:24:48.880 --> 02:24:53.760
What, is there something that is like the perfect, if you could just say

02:24:54.560 --> 02:24:57.600
the perfect productivity setup for George Hott's?

02:24:57.600 --> 02:24:58.160
Doesn't matter.

02:24:58.160 --> 02:24:58.960
Doesn't matter.

02:24:58.960 --> 02:25:00.240
Literally doesn't matter.

02:25:00.240 --> 02:25:02.960
You know, I guess I code most of the time in VIM.

02:25:02.960 --> 02:25:07.120
Like literally I'm using an editor from the 70s, you know, you didn't make anything better.

02:25:07.120 --> 02:25:08.880
You know, okay, VS Code is nice for reading code.

02:25:08.880 --> 02:25:10.160
There's a few things that are nice about it.

02:25:10.960 --> 02:25:13.360
I think that there, you can build much better tools.

02:25:13.360 --> 02:25:16.960
How like, Ida's X refs work way better than VS codes, why?

02:25:18.480 --> 02:25:20.080
Yeah, actually, that's a good question.

02:25:20.080 --> 02:25:20.640
Like why?

02:25:20.640 --> 02:25:27.760
I still use, sorry, Emacs for most, I've actually never, I have to confess something dark.

02:25:28.720 --> 02:25:30.720
So I've never used VIM.

02:25:32.480 --> 02:25:39.040
I think maybe I'm just afraid that my life has been like a waste.

02:25:40.560 --> 02:25:43.360
I'm so, I'm not, I'm not even gelical about Emacs.

02:25:44.480 --> 02:25:47.040
This is how I feel about TensorFlow versus PyTorch.

02:25:47.040 --> 02:25:47.680
Yeah.

02:25:47.680 --> 02:25:50.000
Having just like, we've switched everything to PyTorch.

02:25:50.000 --> 02:25:51.760
Now put months into the switch.

02:25:51.760 --> 02:25:54.400
I have felt like I've wasted years on TensorFlow.

02:25:54.400 --> 02:25:55.360
I can't believe it.

02:25:56.160 --> 02:25:58.240
I can't believe how much better PyTorch is.

02:25:58.240 --> 02:25:58.560
Yeah.

02:25:59.440 --> 02:26:00.720
I've used Emacs in VIM.

02:26:00.720 --> 02:26:01.360
Doesn't matter.

02:26:01.360 --> 02:26:03.440
Yeah, still just my heart somehow.

02:26:03.440 --> 02:26:04.560
I fell in love with Lisp.

02:26:04.560 --> 02:26:05.200
I don't know why.

02:26:05.200 --> 02:26:07.920
You can't, the heart wants what the heart wants.

02:26:07.920 --> 02:26:10.320
I don't, I don't understand it, but it just connected with me.

02:26:10.320 --> 02:26:13.120
Maybe it's the functional language at first I connected with.

02:26:13.120 --> 02:26:17.360
Maybe it's because so many of the AI courses before the deep learning revolution were

02:26:17.360 --> 02:26:18.640
taught with Lisp in mind.

02:26:19.280 --> 02:26:19.840
I don't know.

02:26:19.840 --> 02:26:22.240
I don't know what it is, but I'm stuck with it.

02:26:22.240 --> 02:26:26.240
But at the same time, like why am I not using a modern ID for some of these programming?

02:26:26.240 --> 02:26:27.120
I don't know.

02:26:27.120 --> 02:26:28.320
They're not that much better.

02:26:28.320 --> 02:26:29.440
I've used modern IDs there.

02:26:30.000 --> 02:26:33.440
But at the same time, so like to just, we're not to disagree with you, but

02:26:33.440 --> 02:26:34.960
like I like multiple monitors.

02:26:35.680 --> 02:26:41.360
Like I have to do work on a laptop and it's a pain in the ass.

02:26:41.360 --> 02:26:44.640
And also I'm addicted to the Kinesis weird keyboard.

02:26:45.440 --> 02:26:46.480
You could see there.

02:26:47.360 --> 02:26:49.920
Yeah, so you don't have any of that.

02:26:49.920 --> 02:26:51.600
You can just be on a MacBook.

02:26:51.600 --> 02:26:52.800
I mean, look at work.

02:26:52.800 --> 02:26:55.040
I have three 24 inch monitors.

02:26:55.040 --> 02:26:56.400
I have a happy hacking keyboard.

02:26:56.400 --> 02:26:58.560
I have a razor death header mouse.

02:26:59.680 --> 02:27:01.120
But it's not essential for you.

02:27:01.120 --> 02:27:01.840
No.

02:27:01.840 --> 02:27:04.560
Let's go to a day in the life of George Hots.

02:27:04.560 --> 02:27:08.560
What is the perfect day productivity wise?

02:27:08.560 --> 02:27:16.160
So we're not talking about like Hunter S. Thompson drugs and let's look at productivity.

02:27:16.800 --> 02:27:19.680
What's the day look like on like hour by hour?

02:27:19.680 --> 02:27:25.200
Is there any regularities that create a magical George Hots experience?

02:27:25.760 --> 02:27:33.040
I can remember three days in my life and I remember these days vividly when I've gone

02:27:33.040 --> 02:27:37.760
through kind of radical transformations to the way I think.

02:27:37.760 --> 02:27:40.000
And what I would give, I would pay $100,000.

02:27:40.000 --> 02:27:44.800
If I could have one of these days tomorrow, the days have been so impactful.

02:27:44.800 --> 02:27:50.640
And one was first discovering LEIs of Yudkowsky on the Singularity and reading that stuff.

02:27:50.640 --> 02:27:52.560
And like my mind was blown.

02:27:54.160 --> 02:27:59.200
And the next was discovering the Hunter price and that AI is just compression.

02:27:59.840 --> 02:28:03.680
Like finally understanding AIXI and what all of that was.

02:28:03.680 --> 02:28:06.320
You know, I like read about it when I was 18, 19, I didn't understand it.

02:28:06.320 --> 02:28:09.920
And then the fact that like lossless compression implies intelligence.

02:28:09.920 --> 02:28:11.440
The day that I was shown that.

02:28:12.320 --> 02:28:14.000
And then the third one is controversial.

02:28:14.000 --> 02:28:19.120
The day I found a blog called Unqualified Reservations and read that.

02:28:19.120 --> 02:28:21.360
And I was like, wait, which one is that?

02:28:21.360 --> 02:28:22.880
That's what's the guy's name?

02:28:22.880 --> 02:28:24.080
Curtis Garvin.

02:28:24.080 --> 02:28:24.320
Yeah.

02:28:25.120 --> 02:28:27.600
So many people tell me I'm supposed to talk to him.

02:28:27.600 --> 02:28:32.480
Yeah, the day he sounds insane or brilliant, but insane or both.

02:28:32.480 --> 02:28:33.040
I don't know.

02:28:33.040 --> 02:28:36.480
The day I found that blog was another like this was during like like

02:28:36.480 --> 02:28:38.960
Gamergate and kind of the run up to the 2016 election.

02:28:38.960 --> 02:28:42.800
And I'm like, wow, okay, the world makes sense now.

02:28:42.800 --> 02:28:45.520
This is like, I had a framework now to interpret this,

02:28:45.520 --> 02:28:49.360
just like I got the framework for AI and a framework to interpret technological progress.

02:28:49.360 --> 02:28:52.720
Like those days when I discovered these new frameworks or.

02:28:52.720 --> 02:28:53.440
Oh, interesting.

02:28:53.440 --> 02:28:57.040
So it's not about, but what was special about those days?

02:28:57.040 --> 02:28:58.720
How did those days come to be?

02:28:58.720 --> 02:28:59.920
Is it just you got lucky?

02:28:59.920 --> 02:29:00.480
Like, sure.

02:29:01.360 --> 02:29:06.960
I like, you just encountered a hunter prize on on hacking news or something like that.

02:29:07.680 --> 02:29:08.800
Um, like what?

02:29:09.680 --> 02:29:13.200
But you see, I don't think it's just, see, I don't think it's just that like,

02:29:13.200 --> 02:29:14.720
I could have gotten lucky at any point.

02:29:14.720 --> 02:29:16.160
I think that in a way.

02:29:16.160 --> 02:29:17.760
You were ready at that moment.

02:29:17.760 --> 02:29:18.480
Yeah, exactly.

02:29:18.480 --> 02:29:19.600
To receive the information.

02:29:21.440 --> 02:29:24.720
But is there some magic to the day today of like,

02:29:25.520 --> 02:29:27.040
like eating breakfast?

02:29:27.040 --> 02:29:28.400
And it's the mundane things.

02:29:29.040 --> 02:29:29.600
Nah.

02:29:29.600 --> 02:29:30.160
Nothing.

02:29:30.160 --> 02:29:32.160
No, I drift through, I drift through life.

02:29:32.800 --> 02:29:34.080
Without structure.

02:29:34.080 --> 02:29:38.480
I drift through life hoping and praying that I will get another day like those days.

02:29:38.480 --> 02:29:42.080
And there's nothing in particular you do to, uh, to be a receptacle

02:29:42.960 --> 02:29:44.480
for another for day number four.

02:29:46.000 --> 02:29:48.080
No, I didn't do anything to get the other ones.

02:29:48.080 --> 02:29:51.040
So I don't think I have to really do anything now.

02:29:51.040 --> 02:29:53.520
I took a month long trip to New York and

02:29:54.560 --> 02:29:57.920
I mean, the Ethereum thing was the highlight of it, but the rest of it was pretty terrible.

02:29:57.920 --> 02:30:01.680
I did a two week road trip and I got, I had to turn around.

02:30:01.680 --> 02:30:06.560
I had to turn around, I'm driving in, uh, in Gunnison, Colorado.

02:30:06.560 --> 02:30:10.480
I passed through Gunnison and, uh, the snow starts coming down.

02:30:10.480 --> 02:30:12.960
There's a pass up there called Monarch Pass in order to get through to Denver.

02:30:12.960 --> 02:30:13.920
You got to get over the Rockies.

02:30:14.640 --> 02:30:16.160
And I had to turn my car around.

02:30:16.720 --> 02:30:20.080
I couldn't, I watched, uh, I watched a F-150 go off the road.

02:30:20.080 --> 02:30:21.600
I'm like, I got to go back.

02:30:21.600 --> 02:30:25.920
And like that day was meaningful because like, like it was real.

02:30:25.920 --> 02:30:27.920
Like I actually had to turn my car around.

02:30:28.800 --> 02:30:31.200
It's rare that anything even real happens in my life.

02:30:31.200 --> 02:30:35.040
Even as, you know, mundane is the fact that, yeah, there was snow.

02:30:35.040 --> 02:30:37.600
I had to turn around, stay in Gunnison and leave the next day.

02:30:37.600 --> 02:30:39.040
Something about that moment felt real.

02:30:40.160 --> 02:30:45.280
Okay. So it's interesting to break apart the three moments you mentioned if it's okay.

02:30:45.280 --> 02:30:50.320
So it'll, uh, I always have trouble pronouncing his name, but allows a Yurkowski.

02:30:53.200 --> 02:31:00.240
So what, how did your worldview change in starting to consider the

02:31:01.440 --> 02:31:05.520
exponential growth of AI and AGI that he thinks about and the, uh,

02:31:05.520 --> 02:31:08.720
the threats of artificial intelligence and all that kind of ideas?

02:31:08.720 --> 02:31:12.400
Like, can you, is it just like, can you maybe, uh, break apart?

02:31:12.400 --> 02:31:17.280
Like what exactly was so magical to you as a transformation experience?

02:31:17.280 --> 02:31:19.600
Today, everyone knows him for threats and AI safety.

02:31:20.240 --> 02:31:22.080
This was pre that stuff.

02:31:22.080 --> 02:31:24.240
There was, I don't think a mention of AI safety on the page.

02:31:25.840 --> 02:31:27.760
This is, this is old Yurkowski stuff.

02:31:27.760 --> 02:31:28.960
He'd probably denounce it all now.

02:31:28.960 --> 02:31:31.600
He'd probably be like, that's exactly what I didn't want to happen.

02:31:31.600 --> 02:31:32.560
Oh, sorry, man.

02:31:35.200 --> 02:31:38.640
Is there something specific you can take from his work that you can remember?

02:31:38.640 --> 02:31:45.920
Yeah. Uh, it was this realization that, uh, computers double in power every 18 months

02:31:45.920 --> 02:31:47.040
and humans do not.

02:31:47.680 --> 02:31:52.640
And they haven't crossed yet, but if you have one thing that's doubling every 18 months

02:31:52.640 --> 02:31:56.000
and one thing that's staying like this, you know, here's your log graph.

02:31:56.880 --> 02:31:59.600
Here's your line, you know, you calculate that.

02:32:01.920 --> 02:32:05.120
And then did that open the door to the exponential thinking?

02:32:05.120 --> 02:32:10.880
Like thinking that, like, you know what, with technology, we can actually transform the world.

02:32:11.440 --> 02:32:13.040
It opened the door to human obsolescence.

02:32:13.600 --> 02:32:19.360
It, it opened the door to realize that in my lifetime, humans are going to be replaced.

02:32:20.560 --> 02:32:24.480
And then the matching idea to that of artificial intelligence with the Hutter Prize.

02:32:26.960 --> 02:32:27.840
You know, I'm torn.

02:32:27.840 --> 02:32:30.080
I go back and forth on what I think about it.

02:32:30.080 --> 02:32:30.400
Yeah.

02:32:31.360 --> 02:32:34.320
But the, the, the basic thesis is it's nice.

02:32:34.320 --> 02:32:39.600
It's a nice compelling notion that we can reduce the task of creating an intelligent system,

02:32:39.600 --> 02:32:42.720
a generally intelligent system, into the task of compression.

02:32:43.840 --> 02:32:46.240
So you, you can think of all of intelligence in the universe.

02:32:46.240 --> 02:32:48.160
In fact, this is a kind of compression.

02:32:50.160 --> 02:32:53.840
Do you find that a, was that just at the time you found that as a compelling idea?

02:32:53.840 --> 02:32:56.080
Or do you still find that a compelling idea?

02:32:56.880 --> 02:32:58.240
I still find that a compelling idea.

02:32:59.040 --> 02:33:06.160
I think that it's not that useful day-to-day, but actually one of maybe my quests before that

02:33:06.160 --> 02:33:08.560
was a search for the definition of the word intelligence.

02:33:09.120 --> 02:33:10.080
And I never had one.

02:33:10.800 --> 02:33:13.680
And I definitely have a definition of the word compression.

02:33:14.560 --> 02:33:17.600
It's a very simple, straightforward one.

02:33:18.320 --> 02:33:19.600
And you know what compression is.

02:33:19.600 --> 02:33:22.320
You know what lossless, is lossless compression, not lossy, lossless compression.

02:33:22.880 --> 02:33:26.480
And that that is equivalent to intelligence, which I believe,

02:33:26.480 --> 02:33:28.800
I'm not sure how useful that definition is day-to-day,

02:33:28.800 --> 02:33:31.280
but like I now have a framework to understand what it is.

02:33:32.160 --> 02:33:37.520
And he just 10x'd the prize for that competition, like recently a few months ago.

02:33:37.520 --> 02:33:39.360
You ever thought of taking a crack at that?

02:33:39.360 --> 02:33:40.960
Oh, I did.

02:33:40.960 --> 02:33:41.440
Oh, I did.

02:33:41.440 --> 02:33:46.960
I spent, I spent the next, after I found the prize, I spent the next six months of my life

02:33:46.960 --> 02:33:47.760
trying it.

02:33:47.760 --> 02:33:51.200
And well, that's when I started learning everything about AI.

02:33:51.200 --> 02:33:53.200
And then I worked at Vicarious for a bit.

02:33:53.200 --> 02:33:55.200
And then I learned, read all the deep learning stuff.

02:33:55.200 --> 02:33:57.680
And I'm like, okay, now I like, I'm caught up to modern AI.

02:33:58.720 --> 02:34:02.400
And I had, I had a really good framework to put it all in from the compression stuff.

02:34:04.000 --> 02:34:04.320
Right.

02:34:04.320 --> 02:34:07.440
Like some of the first, some of the first deep learning models I played with were,

02:34:08.240 --> 02:34:10.720
were GTT, GPT basically.

02:34:10.720 --> 02:34:17.840
But before Transformers, before it was still RNNs to do character prediction.

02:34:17.840 --> 02:34:22.800
But by the way, on the compression side, I mean, the, especially with neural networks,

02:34:22.800 --> 02:34:26.480
what do you make of the lossless requirement with the harder prize?

02:34:26.480 --> 02:34:32.960
So, you know, human intelligence and neural networks can probably compress stuff pretty

02:34:32.960 --> 02:34:34.480
well, but there will be lossy.

02:34:35.120 --> 02:34:35.920
It's imperfect.

02:34:36.560 --> 02:34:40.240
You can turn a lossy compression into a lossless compressor pretty easily using an arithmetic

02:34:40.240 --> 02:34:40.960
encoder, right?

02:34:40.960 --> 02:34:45.920
You can take an arithmetic encoder and you can just encode the noise with maximum efficiency.

02:34:45.920 --> 02:34:49.840
Right. So even if you can't predict exactly what the next character is,

02:34:50.400 --> 02:34:54.160
the better a probability distribution you can put over the next character,

02:34:54.160 --> 02:34:57.200
you can then use an arithmetic encoder to, right?

02:34:57.200 --> 02:34:59.120
You don't have to know whether it's an E or an I.

02:34:59.120 --> 02:35:02.880
You just have to put good probabilities on them and then, you know, code those.

02:35:02.880 --> 02:35:05.600
And if you have, it's a bit of entropy thing, right?

02:35:06.320 --> 02:35:10.160
So let me, on that topic, it could be interesting as a little side tour.

02:35:10.160 --> 02:35:14.960
What are your thoughts in this year about GPT3 and these language models and these

02:35:14.960 --> 02:35:16.080
transformers?

02:35:16.080 --> 02:35:19.920
Is there something interesting to you as an AI researcher?

02:35:20.560 --> 02:35:24.160
Or is there something interesting to you as an autonomous vehicle developer?

02:35:24.720 --> 02:35:27.280
Nah, I think, uh, I think it's all right.

02:35:27.280 --> 02:35:28.960
I mean, it's not like it's cool.

02:35:28.960 --> 02:35:30.080
It's cool for what it is.

02:35:30.080 --> 02:35:34.480
But no, we're not just going to be able to scale up to GPT12 and get general purpose

02:35:34.480 --> 02:35:35.280
intelligence.

02:35:35.280 --> 02:35:40.480
Like your loss function is literally just, you know, you know, cross entropy loss on the

02:35:40.480 --> 02:35:41.280
character, right?

02:35:41.280 --> 02:35:43.360
Like that's not the loss function of general intelligence.

02:35:43.600 --> 02:35:44.800
Is that obvious to you?

02:35:44.800 --> 02:35:45.280
Yes.

02:35:46.160 --> 02:35:54.080
Can you imagine that, like to play devil's advocate on yourself, is it possible that

02:35:54.080 --> 02:35:59.440
you can, the GPT12 will achieve general intelligence with something as dumb as this

02:35:59.440 --> 02:36:00.720
kind of loss function?

02:36:00.720 --> 02:36:03.680
I guess it depends what you mean by general intelligence.

02:36:03.680 --> 02:36:09.920
So there's another problem with the GPTs and that's that they don't have a, uh, they

02:36:09.920 --> 02:36:10.880
don't have long-term memory.

02:36:11.680 --> 02:36:12.160
Right.

02:36:12.880 --> 02:36:13.440
Right.

02:36:13.440 --> 02:36:13.840
All right.

02:36:13.840 --> 02:36:24.960
So like just GPT12, a scaled up version of GPT2 or 3, I find it hard to believe.

02:36:25.920 --> 02:36:27.040
Well, you can scale it in.

02:36:28.080 --> 02:36:33.840
It's, uh, so it's a hard-coded, hard-coded length, but you can make it wider and wider

02:36:33.840 --> 02:36:34.240
and wider.

02:36:34.240 --> 02:36:39.920
Yeah, you're going to get, you're going to get cool things from those systems.

02:36:40.880 --> 02:36:46.080
But I don't think you're ever going to get something that can like, you know,

02:36:46.080 --> 02:36:47.520
build me a rocket ship.

02:36:47.520 --> 02:36:48.880
What about solve driving?

02:36:49.440 --> 02:36:53.680
So, you know, you can use transformer with video, for example.

02:36:54.640 --> 02:36:57.200
You think, is there something in there?

02:36:57.200 --> 02:37:00.560
No, because look, we use, we use a grew.

02:37:01.120 --> 02:37:01.760
We use a grew.

02:37:01.760 --> 02:37:03.520
We could change that grew out to a transformer.

02:37:04.880 --> 02:37:08.560
Um, I think driving is much more Markovian than language.

02:37:09.360 --> 02:37:13.600
So Markovian, you mean like the memory, which, which aspect of Markovian?

02:37:13.600 --> 02:37:18.640
I mean that like most of the information in the state at T minus one is also in the,

02:37:18.640 --> 02:37:20.240
is in state T, right?

02:37:20.240 --> 02:37:23.840
And it kind of like drops off nicely like this, whereas sometime with language,

02:37:23.840 --> 02:37:26.640
you have to refer back to the third paragraph on the second page.

02:37:27.200 --> 02:37:30.800
I feel like there's not many, like, like you can say like speed limit signs,

02:37:30.800 --> 02:37:33.920
but there's really not many things in autonomous driving that look like that.

02:37:33.920 --> 02:37:38.880
But if you look at, uh, to play devil's advocate is, uh, the risk estimation thing

02:37:38.880 --> 02:37:40.560
that you've talked about is kind of interesting.

02:37:41.200 --> 02:37:47.280
Is, uh, it feels like there might be some longer term, uh, aggregation of context

02:37:47.280 --> 02:37:50.960
necessary to be able to figure out like the context.

02:37:52.080 --> 02:37:53.200
Yeah, I'm not even sure.

02:37:53.200 --> 02:37:55.360
I'm, I'm believing my, my own devil's.

02:37:55.360 --> 02:37:59.200
We have a nice, we have a nice like vision model, which outputs like a,

02:37:59.200 --> 02:38:01.760
a one to four dimensional perception space.

02:38:02.240 --> 02:38:04.720
Um, can I try transformers on it?

02:38:04.720 --> 02:38:05.040
Sure.

02:38:05.040 --> 02:38:05.680
I probably will.

02:38:06.480 --> 02:38:08.800
At some point we'll try transformers and then we'll just see.

02:38:08.800 --> 02:38:09.600
Do they do better?

02:38:09.600 --> 02:38:09.840
Sure.

02:38:09.840 --> 02:38:10.320
I'm, I'm,

02:38:10.320 --> 02:38:11.760
but it might not be a game changer.

02:38:11.760 --> 02:38:16.080
No, well, I'm not like, like might transformers work better than grooves for autonomous driving?

02:38:16.080 --> 02:38:16.640
Sure.

02:38:16.640 --> 02:38:17.520
Might we switch?

02:38:17.520 --> 02:38:17.840
Sure.

02:38:17.840 --> 02:38:19.200
Is this some radical change?

02:38:19.200 --> 02:38:19.920
No.

02:38:19.920 --> 02:38:20.240
Okay.

02:38:20.240 --> 02:38:23.360
We used a slightly different, you know, we switched from RNNs to grooves like,

02:38:23.360 --> 02:38:25.600
okay, maybe it's grooves to transformers, but no, it's not.

02:38:27.680 --> 02:38:31.200
Well, on the, on the topic of general intelligence, I don't know how much

02:38:31.200 --> 02:38:32.080
I've talked to you about it.

02:38:32.080 --> 02:38:37.200
Like what, um, do you think we'll actually build an AGI?

02:38:37.920 --> 02:38:41.680
Like if, if you look at Ray Kurzweil with a singularity, do you, do you have like an

02:38:41.680 --> 02:38:45.120
intuition about, you're kind of saying driving is easy.

02:38:45.120 --> 02:38:45.520
Yeah.

02:38:46.160 --> 02:38:54.880
And I, I tend to personally believe that solving driving will have really deep,

02:38:54.880 --> 02:38:59.200
important impacts on our ability to solve general intelligence.

02:38:59.200 --> 02:39:04.320
Like I think driving doesn't require general intelligence, but I think they're going to

02:39:04.320 --> 02:39:10.960
be neighbors in a way that it's like deeply tied because it's so, like driving is so deeply

02:39:10.960 --> 02:39:17.280
connected to the human experience that I think solving one will help solve the other.

02:39:17.280 --> 02:39:22.240
But, but so I don't see, I don't see driving as like easy and almost like separate than

02:39:22.240 --> 02:39:26.560
general intelligence, but like what's your vision of a future with a singularity?

02:39:26.560 --> 02:39:30.480
Do you see there'll be a single moment, like a singularity where it'll be a face shift?

02:39:30.480 --> 02:39:32.240
Are we in the singularity now?

02:39:32.240 --> 02:39:35.520
Like what, do you have crazy ideas about the future in terms of AGI?

02:39:35.520 --> 02:39:37.120
We're definitely in the singularity now.

02:39:37.920 --> 02:39:38.320
We are.

02:39:38.320 --> 02:39:40.000
Of course, of course.

02:39:40.000 --> 02:39:41.360
Look at the bandwidth between people.

02:39:41.360 --> 02:39:42.640
The bandwidth between people goes up.

02:39:43.200 --> 02:39:43.600
All right.

02:39:44.640 --> 02:39:47.120
The singularity is just, you know, when the bandwidth, but

02:39:47.120 --> 02:39:48.560
What do you mean by the bandwidth of people?

02:39:48.560 --> 02:39:51.200
Communications, tools, the whole world is networked.

02:39:51.200 --> 02:39:53.760
The whole world is networked and we raise the speed of that network, right?

02:39:54.560 --> 02:40:00.000
Oh, so you think the communication of information in a distributed way is a

02:40:00.000 --> 02:40:02.160
empowering thing for collective intelligence?

02:40:02.160 --> 02:40:04.400
Oh, I didn't say it's necessarily a good thing, but I think that's like

02:40:04.400 --> 02:40:07.360
when I think of the definition of the singularity, yeah, it seems kind of right.

02:40:08.080 --> 02:40:12.640
I see, like it's a change in the world beyond which

02:40:13.440 --> 02:40:16.560
like the world be transformed in ways that we can't possibly imagine.

02:40:16.560 --> 02:40:19.600
No, I mean, I think we're in the singularity now in the sense that there's like, you know,

02:40:19.600 --> 02:40:22.320
one world in a monoculture and it's also linked.

02:40:22.320 --> 02:40:28.080
Yeah, I mean, I kind of shared the intuition that the singularity will originate from the

02:40:28.080 --> 02:40:35.200
collective intelligence of us ants versus the like some single system AGI type thing.

02:40:35.200 --> 02:40:36.160
Oh, I totally agree with that.

02:40:36.880 --> 02:40:40.960
Yeah, I don't, I don't really believe in like like a hard takeoff AGI kind of thing.

02:40:45.120 --> 02:40:49.440
Yeah, I don't think, I don't even think AI is all that different in kind

02:40:49.440 --> 02:40:50.960
from what we've already been building.

02:40:51.920 --> 02:40:56.160
With respect to driving, I think driving is a subset of general intelligence,

02:40:56.160 --> 02:40:57.920
and I think it's a pretty complete subset.

02:40:57.920 --> 02:41:02.160
I think the tools we develop at comma will also be extremely helpful

02:41:02.160 --> 02:41:03.920
to solving general intelligence.

02:41:03.920 --> 02:41:06.400
And that's, I think the real reason why I'm doing it.

02:41:06.400 --> 02:41:07.680
I don't care about self-driving cars.

02:41:08.240 --> 02:41:09.600
It's a cool problem to beat people at.

02:41:10.800 --> 02:41:14.400
But yeah, I mean, yeah, you're kind of, you're of two minds.

02:41:14.400 --> 02:41:18.400
So one, you do have to have a mission and you want to focus and make sure you get,

02:41:18.400 --> 02:41:18.960
you get there.

02:41:18.960 --> 02:41:20.560
You can't forget that.

02:41:20.560 --> 02:41:26.960
But at the same time, there is a thread that's much bigger than the

02:41:26.960 --> 02:41:29.840
capacity entirety of your effort that's much bigger than just driving.

02:41:31.120 --> 02:41:35.120
With AI and with general intelligence, it is so easy to delude yourself

02:41:35.120 --> 02:41:37.280
into thinking you've figured something out when you haven't.

02:41:37.280 --> 02:41:42.560
If we build a level five self-driving car, we have indisputably built something.

02:41:42.560 --> 02:41:43.200
Yeah.

02:41:43.200 --> 02:41:44.640
Is it general intelligence?

02:41:44.640 --> 02:41:45.840
I'm not going to debate that.

02:41:45.840 --> 02:41:49.600
I will say we've built something that provides huge financial value.

02:41:49.600 --> 02:41:50.480
Yeah, beautifully put.

02:41:50.480 --> 02:41:53.520
That's the engineer and credo, just build the thing.

02:41:53.520 --> 02:41:58.640
It's like, that's why I'm with Elon on Go to Mars.

02:41:58.640 --> 02:41:59.680
Yeah, that's a great one.

02:41:59.680 --> 02:42:02.960
You can argue who the hell cares about Go to Mars.

02:42:03.520 --> 02:42:09.040
But the reality is, set that as a mission, get it done, and then you're going to crack some

02:42:09.040 --> 02:42:12.640
problem that you've never even expected in the process of doing that.

02:42:12.640 --> 02:42:12.960
Yeah.

02:42:13.840 --> 02:42:17.680
Yeah, I mean, I think if I had a choice between humanity going to Mars and solving

02:42:17.680 --> 02:42:21.120
self-driving cars, I think going to Mars is better.

02:42:21.120 --> 02:42:23.440
But I don't know, I'm more suited for self-driving cars.

02:42:23.440 --> 02:42:24.240
I'm an information guy.

02:42:24.240 --> 02:42:24.880
I'm not a modernist.

02:42:24.880 --> 02:42:25.600
I'm a post-modernist.

02:42:26.480 --> 02:42:28.240
Post-modernist, all right.

02:42:28.240 --> 02:42:28.880
Beautifully put.

02:42:29.840 --> 02:42:32.240
Let me drag you back to programming for a sec.

02:42:32.240 --> 02:42:36.480
What three, maybe three to five programming languages should people learn, do you think?

02:42:36.480 --> 02:42:41.600
Like if you look at yourself, what did you get the most out of from learning?

02:42:42.560 --> 02:42:45.840
Well, so everybody should learn C and assembly.

02:42:45.840 --> 02:42:47.280
We'll start with those two, right?

02:42:47.280 --> 02:42:48.000
Assembly.

02:42:48.000 --> 02:42:48.480
Yeah.

02:42:48.480 --> 02:42:51.520
If you can't code in assembly, you don't know what the computer's doing.

02:42:51.520 --> 02:42:56.080
You don't understand, you don't have to be great in assembly, but you have to code in it.

02:42:56.640 --> 02:43:01.200
And then you have to appreciate assembly in order to appreciate all the great things C gets you.

02:43:01.920 --> 02:43:05.440
And then you have to code in C in order to appreciate all the great things Python gets you.

02:43:06.160 --> 02:43:08.640
So I'll just say assembly C and Python, we'll start with those three.

02:43:09.600 --> 02:43:16.640
The memory allocation of C and the fact that assembly is to give you a sense of just how

02:43:16.640 --> 02:43:20.560
many levels of abstraction you get to work on in modern-day programming.

02:43:20.560 --> 02:43:21.040
Yeah, yeah.

02:43:21.040 --> 02:43:24.400
Graph coloring for assignment, register assignment, and compilers.

02:43:26.080 --> 02:43:28.400
The computer only has a certain number of registers.

02:43:28.400 --> 02:43:30.320
You can have all the variables you want in a C function.

02:43:31.040 --> 02:43:35.600
So you get to start to build intuition about compilation, like what a compiler gets you.

02:43:37.280 --> 02:43:37.680
What else?

02:43:38.400 --> 02:43:43.840
Well, then there's kind of... So those are all very imperative programming languages.

02:43:45.680 --> 02:43:49.120
Then there's two other paradigms for programming that everybody should be familiar with.

02:43:49.120 --> 02:43:50.080
One of them is functional.

02:43:51.120 --> 02:43:55.120
You should learn Haskell and take that all the way through, learn a language with dependent

02:43:55.120 --> 02:44:01.600
types like Coq. Learn that whole space, like the very PL theory heavy languages.

02:44:02.560 --> 02:44:06.400
And Haskell is your favorite functional? Is that the go-to, you'd say?

02:44:06.400 --> 02:44:10.400
Yeah, I'm not a great Haskell programmer. I wrote a compiler in Haskell once.

02:44:10.400 --> 02:44:13.600
There's another paradigm, and actually there's one more paradigm that I'll even talk about

02:44:13.600 --> 02:44:15.760
after that that I never used to talk about when I would think about this.

02:44:15.760 --> 02:44:18.160
But the next paradigm is learn Verilog or HDL.

02:44:20.160 --> 02:44:22.880
Understand this idea of all of the instructions executed once.

02:44:25.360 --> 02:44:29.760
If I have a block in Verilog and I write stuff in it, it's not sequential.

02:44:29.760 --> 02:44:30.960
They all execute it once.

02:44:33.600 --> 02:44:35.440
And then think like that. That's how hard it works.

02:44:36.480 --> 02:44:39.840
So I guess assembly doesn't quite get you that.

02:44:39.840 --> 02:44:44.080
Assembly is more about compilation, and Verilog is more about the hardware,

02:44:44.080 --> 02:44:47.680
like giving a sense of what actually the hardware is doing.

02:44:48.400 --> 02:44:52.320
Assembly, C, Python are straight like they sit right on top of each other.

02:44:52.320 --> 02:44:55.520
In fact, C is kind of coded in C.

02:44:55.520 --> 02:44:59.040
But you could imagine the first C was coded in assembly, and Python is actually coded in C.

02:44:59.920 --> 02:45:02.240
So you can straight up go on that.

02:45:03.520 --> 02:45:05.520
Got it. And then Verilog gives you that.

02:45:05.600 --> 02:45:07.440
That's brilliant. Okay.

02:45:07.440 --> 02:45:09.760
And then I think there's another one now.

02:45:09.760 --> 02:45:14.480
Everyone's Carpathian calls it programming 2.0, which is learn a...

02:45:15.280 --> 02:45:18.480
I'm not even gonna... Don't learn TensorFlow. Learn PyTorch.

02:45:18.480 --> 02:45:19.440
So machine learning.

02:45:20.080 --> 02:45:22.800
We've got to come up with a better term than programming 2.0 or...

02:45:25.360 --> 02:45:26.000
But yeah.

02:45:26.000 --> 02:45:26.880
It's a programming language.

02:45:29.760 --> 02:45:32.560
I wonder if it can be formalized a little bit better.

02:45:32.560 --> 02:45:35.920
We feel like we're in the early days of what that actually entails.

02:45:36.960 --> 02:45:38.160
Data-driven programming?

02:45:39.040 --> 02:45:40.640
Data-driven programming, yeah.

02:45:41.600 --> 02:45:44.480
But it's so fundamentally different as a paradigm than the others.

02:45:45.840 --> 02:45:47.920
Like it almost requires a different skill set.

02:45:50.240 --> 02:45:51.440
But you think it's still... Yeah.

02:45:53.600 --> 02:45:55.680
And PyTorch versus TensorFlow, PyTorch wins.

02:45:56.480 --> 02:45:57.360
It's the fourth paradigm.

02:45:57.360 --> 02:45:59.280
It's the fourth paradigm that I've kind of seen.

02:45:59.280 --> 02:46:04.720
There's like this imperative functional hardware.

02:46:04.720 --> 02:46:06.240
I don't know a better word for it.

02:46:06.240 --> 02:46:07.120
And then ML.

02:46:08.160 --> 02:46:15.120
Do you have advice for people that want to get into programming

02:46:15.120 --> 02:46:16.080
and want to learn programming?

02:46:16.080 --> 02:46:17.760
You have a video.

02:46:19.920 --> 02:46:20.880
What is programming?

02:46:20.880 --> 02:46:22.800
Newb lessons, exclamation point.

02:46:22.800 --> 02:46:24.640
And I think the top comment is like,

02:46:24.640 --> 02:46:26.240
warning, this is not for noobs.

02:46:27.200 --> 02:46:32.480
Do you have a newb, like TLDW for that video,

02:46:32.480 --> 02:46:39.520
but also a newb friendly advice on how to get into programming?

02:46:39.520 --> 02:46:42.880
We're never going to learn programming by watching a video

02:46:42.880 --> 02:46:44.560
called Learn Programming.

02:46:44.560 --> 02:46:46.560
The only way to learn programming, I think,

02:46:46.560 --> 02:46:49.040
and the only one is the only way everyone I've ever met

02:46:49.040 --> 02:46:51.840
who can program well learned it all in the same way.

02:46:51.840 --> 02:46:53.360
They had something they wanted to do,

02:46:53.680 --> 02:46:55.360
and then they tried to do it.

02:46:55.360 --> 02:46:59.600
And then they were like, oh, well, okay, this is kind of,

02:46:59.600 --> 02:47:01.600
you know, it'd be nice if the computer could kind of do this.

02:47:01.600 --> 02:47:03.360
And then, you know, that's how you learn.

02:47:03.360 --> 02:47:05.360
You just keep pushing on a project.

02:47:07.360 --> 02:47:10.720
So the only advice I have for learning programming is go program.

02:47:10.720 --> 02:47:13.120
Somebody wrote to me a question like,

02:47:13.120 --> 02:47:15.840
we don't really, they're looking to learn

02:47:15.840 --> 02:47:17.440
about recurring neural networks.

02:47:17.440 --> 02:47:19.760
And it's saying like, my company's thinking of doing,

02:47:19.760 --> 02:47:22.560
using recurring neural networks for time series data.

02:47:22.880 --> 02:47:26.000
But we don't really have an idea of where to use it yet.

02:47:26.000 --> 02:47:28.480
We just want to, like, do you have any advice on how to learn about,

02:47:28.480 --> 02:47:31.840
these are these kind of general machine learning questions.

02:47:31.840 --> 02:47:36.480
And I think the answer is like, actually have a problem

02:47:36.480 --> 02:47:38.480
that you're trying to solve and just.

02:47:38.480 --> 02:47:39.440
I see that stuff.

02:47:39.440 --> 02:47:41.600
Oh my God, when people talk like that, they're like,

02:47:41.600 --> 02:47:44.000
I heard machine learning is important.

02:47:44.000 --> 02:47:46.000
Could you help us integrate machine learning

02:47:46.000 --> 02:47:48.000
with macaroni and cheese production?

02:47:48.720 --> 02:47:53.840
You just, I don't even, you can't help these people.

02:47:53.840 --> 02:47:55.840
Like who lets you run anything?

02:47:55.840 --> 02:47:57.680
Who lets that kind of person run anything?

02:47:58.240 --> 02:48:02.560
I think we're all, we're all beginners at some point.

02:48:02.560 --> 02:48:03.040
So.

02:48:03.040 --> 02:48:04.800
It's not like they're a beginner.

02:48:04.800 --> 02:48:08.480
It's like, my problem is not that they don't know about machine learning.

02:48:08.480 --> 02:48:11.600
My problem is that they think that machine learning has something to say

02:48:11.600 --> 02:48:13.200
about macaroni and cheese production.

02:48:14.480 --> 02:48:17.040
Or like, I heard about this new technology.

02:48:17.040 --> 02:48:18.560
How can I use it for why?

02:48:19.760 --> 02:48:22.640
Like, I don't know what it is, but how can I use it for why?

02:48:23.440 --> 02:48:24.080
That's true.

02:48:24.080 --> 02:48:26.000
And you have to build up an intuition of how,

02:48:26.000 --> 02:48:27.520
because you might be able to figure out a way,

02:48:27.520 --> 02:48:32.160
but like the prerequisites is you should have a macaroni and cheese problem to solve first.

02:48:32.160 --> 02:48:32.640
Exactly.

02:48:33.360 --> 02:48:36.800
And then two, you should have more traditional,

02:48:36.800 --> 02:48:41.920
like the learning process involved more traditionally applicable problems

02:48:41.920 --> 02:48:44.480
in the space of whatever that is of machine learning.

02:48:44.480 --> 02:48:46.960
And then see if it can be applied to macrophages.

02:48:46.960 --> 02:48:49.040
At least start with, tell me about a problem.

02:48:49.040 --> 02:48:50.560
Like, if you have a problem, you're like,

02:48:50.560 --> 02:48:53.600
you know, some of my boxes aren't getting enough macaroni in them.

02:48:54.400 --> 02:48:56.640
Can we use machine learning to solve this problem?

02:48:56.640 --> 02:48:58.400
That's much, much better than,

02:48:58.400 --> 02:49:01.520
how do I apply machine learning to macaroni and cheese?

02:49:01.520 --> 02:49:06.320
One big thing, maybe this is me talking to the audience a little bit,

02:49:06.320 --> 02:49:09.840
because I get these days so many messages,

02:49:09.840 --> 02:49:13.360
advice on how to like, learn stuff.

02:49:13.360 --> 02:49:18.080
Okay. My, this is not me being mean.

02:49:18.080 --> 02:49:22.080
I think this is quite profound actually, is you should Google it.

02:49:22.720 --> 02:49:23.200
Oh yeah.

02:49:23.760 --> 02:49:30.480
Like one of the like skills that you should really acquire as an engineer,

02:49:31.040 --> 02:49:34.000
as a researcher, as a thinker, like one,

02:49:34.000 --> 02:49:36.560
there's two complementary skills.

02:49:36.560 --> 02:49:40.720
Like one is with a blank sheet of paper with no internet to think deeply.

02:49:41.520 --> 02:49:45.840
And then the other is to Google the crap out of the questions you have.

02:49:45.840 --> 02:49:47.040
Like that's actually a skill.

02:49:47.840 --> 02:49:50.560
People often talk about, but like doing research,

02:49:50.560 --> 02:49:53.280
like pulling at the thread and like looking up different words,

02:49:53.840 --> 02:49:57.920
going into like GitHub repositories with two stars,

02:49:57.920 --> 02:49:59.600
and like looking how they did stuff,

02:49:59.600 --> 02:50:03.360
like looking at the code or going on Twitter,

02:50:03.360 --> 02:50:07.440
seeing like there's little pockets of brilliant people that are like having discussions.

02:50:07.520 --> 02:50:11.520
Like if you're a neuroscientist, go into signal processing community.

02:50:11.520 --> 02:50:15.760
If you're an AI person going into the psychology community,

02:50:15.760 --> 02:50:19.840
like the switch communities that keep searching, searching, searching,

02:50:19.840 --> 02:50:25.680
because it's so much better to invest in like finding somebody else

02:50:25.680 --> 02:50:30.240
who already solved your problem than is to try to solve the problem.

02:50:30.800 --> 02:50:34.240
And because they've often invested years of their life,

02:50:34.320 --> 02:50:39.120
like entire communities are probably already out there who have tried to solve your problem.

02:50:39.120 --> 02:50:40.240
I think they're the same thing.

02:50:40.880 --> 02:50:44.080
I think you go try to solve the problem.

02:50:44.080 --> 02:50:46.080
And then in trying to solve the problem,

02:50:46.080 --> 02:50:47.680
if you're good at solving problems,

02:50:47.680 --> 02:50:49.760
you'll stumble upon the person who solved it already.

02:50:49.760 --> 02:50:52.240
Yeah, but the stumbling is really important.

02:50:52.240 --> 02:50:54.160
I think that's a skill that people should really put,

02:50:54.160 --> 02:50:56.880
especially in undergrad, like search.

02:50:57.600 --> 02:51:00.320
If you ask me a question, how should I get started in deep learning?

02:51:01.200 --> 02:51:07.120
Especially that is just so Google-able.

02:51:08.000 --> 02:51:11.520
The whole point is you Google that and you get a million pages

02:51:11.520 --> 02:51:12.960
and just start looking at them.

02:51:13.840 --> 02:51:14.400
Yeah.

02:51:14.400 --> 02:51:17.200
Start pulling at the thread, start exploring, start taking notes,

02:51:17.200 --> 02:51:22.720
start getting advice from a million people that already spent their life

02:51:22.720 --> 02:51:24.160
answering that question, actually.

02:51:24.880 --> 02:51:25.280
Oh, well, yeah.

02:51:25.280 --> 02:51:27.680
I mean, that's definitely also when people ask me things like that,

02:51:27.760 --> 02:51:30.320
I'm like, trust me, the top answer on Google is much, much better

02:51:30.320 --> 02:51:31.440
than anything I'm going to tell you.

02:51:31.440 --> 02:51:32.000
Anything you want to say.

02:51:32.000 --> 02:51:32.800
Right?

02:51:32.800 --> 02:51:33.120
Yeah.

02:51:34.400 --> 02:51:35.120
People ask.

02:51:35.920 --> 02:51:37.040
It's an interesting question.

02:51:38.000 --> 02:51:39.840
Let me know if you have any recommendations.

02:51:39.840 --> 02:51:43.680
What three books, technical or fiction or philosophical,

02:51:43.680 --> 02:51:47.440
had an impact on your life or you wouldn't recommend, perhaps?

02:51:49.040 --> 02:51:51.040
Maybe we'll start with the least controversial.

02:51:51.040 --> 02:51:51.760
Infinite Jest.

02:51:54.080 --> 02:51:55.920
Infinite Jest is a...

02:51:56.880 --> 02:51:58.800
David Foster Wallace.

02:51:58.800 --> 02:52:00.640
Yeah, it's a book about wireheading, really.

02:52:03.760 --> 02:52:06.880
Very enjoyable to read, very well-written.

02:52:08.320 --> 02:52:10.000
You will grow as a person reading this book.

02:52:11.040 --> 02:52:11.600
It's effort.

02:52:12.960 --> 02:52:16.000
And I'll set that up for the second book, which is pornography.

02:52:16.000 --> 02:52:20.240
That's called Atlas Shrugged, which...

02:52:21.040 --> 02:52:22.480
Atlas Shrugged is pornography.

02:52:22.480 --> 02:52:23.760
Yeah, I mean, it is.

02:52:23.760 --> 02:52:25.600
I will not defend the...

02:52:25.600 --> 02:52:27.840
I will not say Atlas Shrugged is a well-written book.

02:52:28.480 --> 02:52:31.440
It is entertaining to read, certainly, just like pornography.

02:52:31.440 --> 02:52:33.040
The production value isn't great.

02:52:34.080 --> 02:52:38.800
There's a 60-page monologue in there that Anne Rand's editor really wanted to take out.

02:52:38.800 --> 02:52:44.880
And she paid out of her pocket to keep that 60-page monologue in the book.

02:52:45.840 --> 02:52:54.560
But it is a great book for a kind of framework of human relations.

02:52:54.560 --> 02:52:57.360
And I know a lot of people are like, yeah, but it's a terrible framework.

02:52:57.920 --> 02:52:59.280
Yeah, but it's a framework.

02:53:00.320 --> 02:53:06.240
Just for context, in a couple of days, I'm speaking for probably four plus hours with

02:53:06.240 --> 02:53:11.440
Yaron Brooke, who's the main living, remaining objectivist.

02:53:12.160 --> 02:53:13.200
Objectivist.

02:53:13.200 --> 02:53:13.680
Interesting.

02:53:14.560 --> 02:53:20.320
So I've always found this philosophy quite interesting on many levels.

02:53:20.320 --> 02:53:28.880
One of how repulsive some large percent of the population find it, which is always funny to

02:53:28.880 --> 02:53:35.280
me when people are unable to even read a philosophy because of some...

02:53:35.280 --> 02:53:40.080
I think that says more about their psychological perspective on it.

02:53:40.960 --> 02:53:48.640
But there is something about objectivism and Anne Rand's philosophy that's deeply connected

02:53:48.640 --> 02:53:54.320
to this idea of capitalism, of the ethical life is the productive life,

02:53:56.480 --> 02:53:59.920
that was always compelling to me.

02:54:01.760 --> 02:54:05.440
I didn't seem to interpret it in the negative sense that some people do.

02:54:05.440 --> 02:54:07.200
To be fair, I read the book when I was 19.

02:54:07.840 --> 02:54:09.120
So you had an impact at that point.

02:54:09.520 --> 02:54:15.200
Yeah, and the bad guys in the book have this slogan, from each according to their ability

02:54:15.200 --> 02:54:16.480
to each according to their need.

02:54:18.480 --> 02:54:20.240
I'm looking at this and I'm like, these are the most...

02:54:20.240 --> 02:54:22.800
This is team rocket level cartoonishness, right?

02:54:22.800 --> 02:54:23.680
No, bad guy.

02:54:23.680 --> 02:54:28.160
And then when I realized that was actually the slogan of the Communist Party, I'm like,

02:54:28.160 --> 02:54:31.520
wait a second, wait, no, no, no, no, no.

02:54:31.520 --> 02:54:32.960
You're telling me this really happened?

02:54:33.920 --> 02:54:34.720
Yeah, it's interesting.

02:54:34.960 --> 02:54:39.040
One of the criticisms of her work is she has a cartoonish view of good and evil.

02:54:42.080 --> 02:54:47.520
The reality, as Jordan Peterson says, is that each of us have the capacity for good and evil in

02:54:47.520 --> 02:54:52.080
us as opposed to there's some characters who are purely evil and some characters are purely good.

02:54:52.080 --> 02:54:53.920
And that's in a way why it's pornographic.

02:54:55.040 --> 02:54:56.880
The production value, I love it.

02:54:56.880 --> 02:55:01.920
Well, evil is punished and they're very clearly like, you know, there's no, you know,

02:55:02.640 --> 02:55:07.360
you know, just like porn doesn't have, you know, like character growth, you know,

02:55:07.360 --> 02:55:08.880
neither does Alice Rodney to like.

02:55:09.440 --> 02:55:10.800
Brilliant, well put.

02:55:10.800 --> 02:55:14.080
But at 19 year old George Hots, it was good enough.

02:55:14.080 --> 02:55:15.280
Yeah, yeah, yeah, yeah.

02:55:15.280 --> 02:55:16.800
What's the third?

02:55:16.800 --> 02:55:17.440
You have something?

02:55:19.040 --> 02:55:21.360
I could give these two I'll just throw out.

02:55:21.360 --> 02:55:23.360
They're sci-fi, Perbutation City.

02:55:24.160 --> 02:55:26.560
Great thing, just try thinking about copies yourself.

02:55:26.560 --> 02:55:27.360
And then the minute...

02:55:27.360 --> 02:55:27.840
Isn't that by?

02:55:27.840 --> 02:55:28.560
Sorry to interrupt.

02:55:28.560 --> 02:55:30.720
That is Greg Egan.

02:55:32.320 --> 02:55:33.440
That might not be his real name.

02:55:33.440 --> 02:55:34.320
Some Australian guy.

02:55:34.320 --> 02:55:35.600
It might not be Australian.

02:55:35.600 --> 02:55:36.080
I don't know.

02:55:36.960 --> 02:55:38.560
And then this one's online.

02:55:38.560 --> 02:55:40.720
It's called the Metamorphosis of Prime Intellect.

02:55:42.960 --> 02:55:45.280
It's a story set in a post-singularity world.

02:55:45.280 --> 02:55:45.760
It's interesting.

02:55:46.640 --> 02:55:50.160
Is there, can you, in either of the worlds, do you find something

02:55:50.160 --> 02:55:52.240
philosophical interesting in them that you can comment on?

02:55:53.520 --> 02:55:55.520
I mean, it is clear to me that

02:55:57.920 --> 02:56:01.200
Metamorphosis of Prime Intellect is like written by an engineer.

02:56:02.720 --> 02:56:11.040
Which is, it's very, it's very almost a pragmatic take on a utopia, in a way.

02:56:12.480 --> 02:56:13.440
Positive or negative?

02:56:15.120 --> 02:56:17.840
That's up to you to decide reading the book.

02:56:17.840 --> 02:56:21.440
And the ending of it is very interesting as well.

02:56:21.440 --> 02:56:23.520
And I didn't realize what it was.

02:56:23.520 --> 02:56:25.120
I first read that when I was 15.

02:56:25.120 --> 02:56:26.800
I've reread that book several times in my life.

02:56:27.360 --> 02:56:29.040
And it's short, it's 50 pages.

02:56:29.040 --> 02:56:29.920
I want you to go read it.

02:56:30.720 --> 02:56:33.040
What's, sorry, it's a little tangent.

02:56:33.040 --> 02:56:34.560
I've been working through the foundation.

02:56:34.560 --> 02:56:36.960
I've been, I haven't read much sci-fi in my whole life.

02:56:36.960 --> 02:56:40.080
And I'm trying to fix that in the last few months.

02:56:40.080 --> 02:56:41.520
That's been a little side project.

02:56:42.080 --> 02:56:47.520
What's to use the greatest sci-fi novel that people should read?

02:56:47.520 --> 02:56:49.200
Or is it, or?

02:56:49.200 --> 02:56:51.040
I mean, I would, yeah, I would, I would say like, yeah,

02:56:51.040 --> 02:56:52.800
Permutation City, Metamorphosis of Prime Intellect.

02:56:52.800 --> 02:56:53.680
Got it.

02:56:53.680 --> 02:56:54.320
I don't know.

02:56:54.320 --> 02:56:55.520
I didn't like Foundation.

02:56:56.320 --> 02:56:57.680
I thought it was way too modernist.

02:56:58.560 --> 02:57:00.640
Be like Dune and like all of those.

02:57:00.640 --> 02:57:01.760
I've never read Dune.

02:57:01.760 --> 02:57:02.800
I've never read Dune.

02:57:02.800 --> 02:57:03.760
I have to read it.

02:57:04.480 --> 02:57:06.720
Fire Upon the Deep is interesting.

02:57:08.960 --> 02:57:10.400
Okay, I mean, look, everyone should read,

02:57:10.400 --> 02:57:11.280
everyone should read Neuromancer.

02:57:11.280 --> 02:57:12.720
Everyone should read Snow Crash.

02:57:12.720 --> 02:57:14.400
If you haven't read those, like, start there.

02:57:15.360 --> 02:57:16.320
Yeah, I haven't read Snow Crash.

02:57:16.320 --> 02:57:17.280
I haven't read Snow Crash.

02:57:17.280 --> 02:57:18.240
Oh, it's, I mean, it's very entertaining.

02:57:18.240 --> 02:57:18.720
It's surprising.

02:57:19.840 --> 02:57:20.640
Go to Lesher Bach.

02:57:20.640 --> 02:57:23.040
And if you want the controversial one, Bronze Age Mindset.

02:57:25.200 --> 02:57:26.560
All right, I'll look into that one.

02:57:27.520 --> 02:57:30.080
Those aren't sci-fi, but just to round out books.

02:57:31.600 --> 02:57:35.200
So a bunch of people asked me on Twitter and Reddit and so on

02:57:35.840 --> 02:57:36.800
for advice.

02:57:36.800 --> 02:57:40.080
So what advice would you give a young person today about life?

02:57:44.720 --> 02:57:47.440
What, yeah, I mean, looking back,

02:57:47.440 --> 02:57:49.760
especially when you're a young, younger, you did,

02:57:50.400 --> 02:57:54.800
and you continued it, you've accomplished a lot of interesting things.

02:57:54.800 --> 02:57:57.120
Is there some advice from those,

02:57:59.760 --> 02:58:01.760
from that life of yours that you can pass on?

02:58:01.760 --> 02:58:06.480
If college ever opens again, I would love to give a graduation speech.

02:58:07.520 --> 02:58:12.160
At that point, I will put a lot of somewhat satirical effort into this question.

02:58:12.160 --> 02:58:15.280
Yeah, at this, you haven't written anything at this point.

02:58:16.240 --> 02:58:18.080
You know what, always wear sunscreen.

02:58:18.080 --> 02:58:19.680
This is water, like.

02:58:19.680 --> 02:58:21.040
I think you're plagiarizing.

02:58:21.040 --> 02:58:24.720
I mean, you know, but that's the, that's the, like,

02:58:24.880 --> 02:58:26.080
put clean your room.

02:58:26.080 --> 02:58:28.480
You know, yeah, you can plagiarize them from all of this stuff.

02:58:28.480 --> 02:58:29.040
And it's, it's,

02:58:33.200 --> 02:58:33.920
there is no,

02:58:35.840 --> 02:58:37.520
self-help books aren't designed to help you.

02:58:37.520 --> 02:58:38.880
They're designed to make you feel good.

02:58:39.920 --> 02:58:44.000
Like, whatever advice I could give, you already know.

02:58:44.000 --> 02:58:45.120
Everyone already knows.

02:58:45.760 --> 02:58:47.200
Sorry, it doesn't feel good.

02:58:50.080 --> 02:58:50.480
Right?

02:58:50.480 --> 02:58:52.000
Like, you know, you know.

02:58:52.960 --> 02:58:55.760
If I tell you that you should, you know,

02:58:56.800 --> 02:59:01.120
eat well and read more and it's not going to do anything.

02:59:01.760 --> 02:59:06.160
I think the whole, like, genre of those kind of questions is meaningless.

02:59:07.360 --> 02:59:07.840
I don't know.

02:59:07.840 --> 02:59:10.480
If anything, it's don't worry so much about that stuff.

02:59:10.480 --> 02:59:11.760
Don't be so caught up in your head.

02:59:12.320 --> 02:59:12.960
Right.

02:59:12.960 --> 02:59:16.400
I mean, you're, yeah, in the sense that your whole life,

02:59:16.400 --> 02:59:19.600
your whole existence is like moving version of that advice.

02:59:20.560 --> 02:59:21.040
I don't know.

02:59:23.760 --> 02:59:25.360
There's, there's something, I mean,

02:59:25.360 --> 02:59:28.160
there's something in you that resists that kind of thinking in that,

02:59:28.160 --> 02:59:34.400
in itself is, it's just illustrative of who you are.

02:59:34.400 --> 02:59:35.760
And there's something to learn from that.

02:59:36.640 --> 02:59:39.440
I think you're, you're clearly not overthinking stuff.

02:59:41.280 --> 02:59:41.600
Yeah.

02:59:41.600 --> 02:59:42.000
And you know what?

02:59:42.000 --> 02:59:43.040
There's a gut thing.

02:59:43.040 --> 02:59:45.120
I, even when I talk about my advice, I'm like,

02:59:45.120 --> 02:59:46.720
my advice is only relevant to me.

02:59:47.280 --> 02:59:48.640
It's not relevant to anybody else.

02:59:48.640 --> 02:59:49.840
I'm not saying you should go out.

02:59:49.840 --> 02:59:51.520
If you're the kind of person who overthinks things,

02:59:51.520 --> 02:59:53.280
to stop overthinking things, it's not bad.

02:59:53.920 --> 02:59:54.800
It doesn't work for me.

02:59:54.800 --> 02:59:55.600
Maybe it works for you.

02:59:55.600 --> 02:59:56.880
I, you know, I don't know.

02:59:57.840 --> 02:59:59.200
Let me ask you about love.

02:59:59.200 --> 02:59:59.520
Yeah.

03:00:01.840 --> 03:00:05.040
So I think last time we talked about the meaning of life,

03:00:05.040 --> 03:00:08.480
and it was, it was kind of about winning.

03:00:08.480 --> 03:00:08.800
Of course.

03:00:10.720 --> 03:00:12.960
I don't think I've talked to you about love much,

03:00:12.960 --> 03:00:17.280
whether romantic or just love for the common humanity amongst us all.

03:00:18.000 --> 03:00:21.280
What role has love played in your life?

03:00:21.280 --> 03:00:25.600
In this, in this quest for winning, where does love fit in?

03:00:26.160 --> 03:00:29.760
Well, where love, I think means several different things.

03:00:29.760 --> 03:00:32.800
There's love in the sense of, maybe I could just say,

03:00:32.800 --> 03:00:37.840
there's like love in the sense of opiates and love in the sense of oxytocin,

03:00:37.840 --> 03:00:44.320
and then love in the sense of maybe like a love for math.

03:00:44.320 --> 03:00:46.720
I don't think fits into either of those first two paradigms.

03:00:47.840 --> 03:00:56.000
So each of those, have they, have they, have they given something to you in your life?

03:00:56.720 --> 03:00:58.640
I'm not that big of a fan of the first two.

03:01:00.640 --> 03:01:00.880
Why?

03:01:03.440 --> 03:01:06.160
The same reason I'm not a fan of, you know,

03:01:06.160 --> 03:01:09.600
the same reason I don't do opiates and don't take ecstasy.

03:01:11.280 --> 03:01:12.960
And there were times, look, I've tried both.

03:01:13.920 --> 03:01:17.040
I like opiates way more than ecstasy.

03:01:18.240 --> 03:01:23.200
But they're not, the ethical life is the productive life.

03:01:24.320 --> 03:01:26.880
So maybe that's my problem with those.

03:01:26.880 --> 03:01:31.200
And then like, yeah, a sense of, I don't know, like abstract love for humanity.

03:01:32.000 --> 03:01:36.080
I mean, the abstract love for humanity, I'm like, yeah, I've always felt that.

03:01:36.080 --> 03:01:40.320
And I guess it's hard for me to imagine not feeling it.

03:01:40.320 --> 03:01:42.560
And maybe there's people who don't and I don't know.

03:01:43.440 --> 03:01:43.680
Yeah.

03:01:43.680 --> 03:01:45.680
That's just like a background thing that's there.

03:01:46.320 --> 03:01:49.200
I mean, since we brought up drugs, let me ask you,

03:01:51.600 --> 03:01:55.360
this is becoming more and more a part of my life because I'm talking to a few researchers

03:01:55.360 --> 03:01:56.640
that are working on psychedelics.

03:01:57.600 --> 03:02:00.240
I've eaten shrooms a couple of times.

03:02:00.240 --> 03:02:04.800
And it was fascinating to me that like the mind can go to like,

03:02:06.080 --> 03:02:09.280
just fascinating, the mind can go to places I didn't imagine it could go.

03:02:09.360 --> 03:02:12.640
And I was very friendly and positive and exciting.

03:02:12.640 --> 03:02:16.080
And everything was kind of hilarious in the place.

03:02:16.080 --> 03:02:18.160
Wherever my mind went, that's where I went.

03:02:18.160 --> 03:02:21.040
Is what do you think about psychedelics?

03:02:21.040 --> 03:02:24.560
Do you think they have, where do you think the mind goes?

03:02:24.560 --> 03:02:26.080
Have you done psychedelics?

03:02:26.080 --> 03:02:27.360
Where do you think the mind goes?

03:02:28.560 --> 03:02:31.040
Is there something useful to learn about the places it goes?

03:02:32.240 --> 03:02:33.120
Once you come back.

03:02:33.760 --> 03:02:40.400
I find it interesting that this idea that psychedelics have something to teach

03:02:40.400 --> 03:02:42.400
is almost unique to psychedelics.

03:02:43.760 --> 03:02:45.600
People don't argue this about amphetamines.

03:02:48.400 --> 03:02:50.160
And I'm not really sure why.

03:02:50.160 --> 03:02:52.960
I think all of the drugs have lessons to teach.

03:02:53.680 --> 03:02:55.040
I think there's things to learn from opiates.

03:02:55.040 --> 03:02:56.480
I think there's things to learn from amphetamines.

03:02:56.480 --> 03:02:58.080
I think there's things to learn from psychedelics,

03:02:58.080 --> 03:02:59.120
things to learn from marijuana.

03:03:00.000 --> 03:03:06.640
But also at the same time, recognize that I don't think you're learning things

03:03:06.640 --> 03:03:07.280
about the world.

03:03:07.280 --> 03:03:08.800
I think you're learning things about yourself.

03:03:10.640 --> 03:03:14.160
And what's the, it might have even been.

03:03:15.600 --> 03:03:17.280
Might have even been a Timothy Leary quote.

03:03:17.280 --> 03:03:18.080
I don't know his quote.

03:03:18.080 --> 03:03:21.440
But the idea is basically like, everybody should look behind the door.

03:03:21.440 --> 03:03:23.680
But then once you've seen behind the door, you don't need to keep going back.

03:03:24.400 --> 03:03:29.040
So, I mean, and that's my thoughts on all real drug use too.

03:03:29.760 --> 03:03:30.800
So maybe for caffeine.

03:03:32.640 --> 03:03:36.160
It's a little experience that it's good to have, but.

03:03:37.040 --> 03:03:37.520
Oh yeah.

03:03:37.520 --> 03:03:40.400
No, I mean, yeah, I guess, yeah, psychedelics are definitely.

03:03:41.760 --> 03:03:43.840
So you're a fan of new experiences, I suppose.

03:03:43.840 --> 03:03:44.160
Yes.

03:03:44.160 --> 03:03:46.880
Because they all contain a little, especially the first few times,

03:03:46.880 --> 03:03:48.640
it contains some lessons that can be picked up.

03:03:49.600 --> 03:03:50.000
Yeah.

03:03:50.000 --> 03:03:53.760
And I'll, I'll revisit psychedelics maybe once a year.

03:03:55.600 --> 03:03:57.200
Usually small, smaller doses.

03:03:58.640 --> 03:04:00.160
Maybe they turn up the learning rate of your brain.

03:04:01.280 --> 03:04:03.120
I've heard that, I like that.

03:04:03.120 --> 03:04:04.160
Yeah, that's cool.

03:04:04.160 --> 03:04:05.760
Big learning rates have frozen comms.

03:04:07.360 --> 03:04:09.360
Last question, and this is a little weird one,

03:04:09.360 --> 03:04:11.520
but you've called yourself crazy in the past.

03:04:14.000 --> 03:04:17.840
First of all, on a scale of one to 10, how crazy would you say, are you?

03:04:17.840 --> 03:04:20.240
Oh, I mean, it depends how you, you know, when you compare me to

03:04:20.240 --> 03:04:22.320
Elon Musk and I say Levin Dalsky, not so crazy.

03:04:23.360 --> 03:04:24.560
So like, like a seven?

03:04:25.760 --> 03:04:27.040
Let's go with six.

03:04:27.040 --> 03:04:28.560
Six, six, six.

03:04:29.360 --> 03:04:30.080
What, uh.

03:04:31.520 --> 03:04:33.200
I like seven, seven's a good number.

03:04:33.200 --> 03:04:34.160
Seven, sorry.

03:04:34.160 --> 03:04:36.480
Well, yeah, I'm sure day by day changes, right?

03:04:36.480 --> 03:04:39.840
So, but you're in that, in that area.

03:04:39.840 --> 03:04:45.120
What, uh, in thinking about that, what do you think is the role of madness?

03:04:45.680 --> 03:04:50.320
Is that a feature or a bug if you were to dissect your brain?

03:04:51.680 --> 03:04:56.960
So, okay, from like a, like mental health lens on crazy,

03:04:56.960 --> 03:04:58.880
I'm not sure I really believe in that.

03:04:58.880 --> 03:05:02.960
I'm not sure I really believe in like a lot of that stuff, right?

03:05:02.960 --> 03:05:05.840
This concept of, okay, you know, when you get over to like,

03:05:06.640 --> 03:05:09.520
like, like, like hardcore bipolar and schizophrenia,

03:05:09.520 --> 03:05:13.120
these things are clearly real, somewhat biological.

03:05:13.120 --> 03:05:18.320
And then over here on the spectrum, you have like ADD and oppositional defiance disorder

03:05:18.320 --> 03:05:22.800
and these things that are like, wait, this is normal spectrum human behavior.

03:05:22.800 --> 03:05:28.560
Like this isn't, you know, where's the, the line here?

03:05:28.560 --> 03:05:31.200
And why is this like a problem?

03:05:31.200 --> 03:05:35.760
So there's this whole, you know, the neurodiversity of humanity is huge.

03:05:35.760 --> 03:05:37.520
Like people think I'm always on drugs.

03:05:37.520 --> 03:05:39.360
People are always saying this to me on my streams and like guys,

03:05:39.360 --> 03:05:41.280
you know, like I'm real open with my drug use.

03:05:41.280 --> 03:05:45.520
I would tell you if I was on drugs and I mean, I had like a cup of coffee this morning,

03:05:45.520 --> 03:05:47.120
but other than that, this is just me.

03:05:47.120 --> 03:05:49.520
You're witnessing my brain in action.

03:05:51.440 --> 03:05:55.520
So, so the word madness doesn't even make sense.

03:05:55.520 --> 03:05:59.360
And then you're in the rich neurodiversity of humans.

03:06:00.880 --> 03:06:06.960
I think it makes sense, but only for like some insane extremes.

03:06:06.960 --> 03:06:14.880
Like if you are actually like visibly hallucinating, you know, that's okay.

03:06:14.880 --> 03:06:17.360
But there is the kind of spectrum on which you stand out.

03:06:17.360 --> 03:06:23.680
Like that, that's like, if I were to look, you know, at decorations on a Christmas tree

03:06:23.680 --> 03:06:28.720
or something like that, like if you were a decoration that would catch my eye.

03:06:28.720 --> 03:06:30.560
Like that thing is sparkly.

03:06:32.720 --> 03:06:36.480
Whatever the hell that thing is, there's something to that.

03:06:37.280 --> 03:06:44.240
Just like refusing to be boring or maybe boring is the wrong word, but to,

03:06:46.640 --> 03:06:50.240
yeah, I mean, be willing to sparkle.

03:06:51.200 --> 03:06:54.080
You know, it's, it's like somewhat constructed.

03:06:54.080 --> 03:06:56.400
I mean, I am who I choose to be.

03:06:58.400 --> 03:07:00.800
I'm going to say things as true as I can see them.

03:07:00.800 --> 03:07:04.240
I'm not going to, I'm not going to lie.

03:07:04.240 --> 03:07:06.480
And but that's a really important feature in itself.

03:07:06.480 --> 03:07:10.240
So like whatever the neurodiversity of your, whatever your brain is,

03:07:11.040 --> 03:07:17.760
not putting constraints on it that force it to fit into the mold of what society is,

03:07:17.760 --> 03:07:20.560
like defines what you're supposed to be.

03:07:20.560 --> 03:07:26.160
So you're one of the specimens that, that doesn't mind being yourself.

03:07:27.760 --> 03:07:33.440
Being right is super important, except at the expense of being wrong.

03:07:37.120 --> 03:07:38.320
Without breaking that apart.

03:07:38.320 --> 03:07:40.240
I think it's a beautiful way to end it.

03:07:40.240 --> 03:07:42.960
George, you're, you're one of the most special humans I know.

03:07:42.960 --> 03:07:44.480
It's truly an honor to talk to you.

03:07:44.480 --> 03:07:45.600
Thanks so much for doing it.

03:07:45.600 --> 03:07:47.600
Thank you for having me.

03:07:47.600 --> 03:07:50.240
Thanks for listening to this conversation with George Hots.

03:07:50.240 --> 03:07:56.400
And thank you to our sponsors for Sigmatic, which is the maker of delicious mushroom coffee,

03:07:57.120 --> 03:08:01.360
Decoding Digital, which is a tech podcast that I listen to and enjoy,

03:08:02.080 --> 03:08:06.400
and ExpressVPN, which is the VPN I've used for many years.

03:08:06.960 --> 03:08:10.320
Please check out these sponsors in the description to get a discount

03:08:10.320 --> 03:08:12.320
and to support this podcast.

03:08:12.960 --> 03:08:17.760
If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple podcast,

03:08:17.760 --> 03:08:23.680
follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.

03:08:24.320 --> 03:08:29.920
And now let me leave you with some words from the great and powerful Linus Torvald.

03:08:30.640 --> 03:08:32.000
Talk is cheap.

03:08:32.000 --> 03:08:34.160
Show me the code.

03:08:34.160 --> 03:08:42.160
Thank you for listening and hope to see you next time.

