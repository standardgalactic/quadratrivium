{"text": " The following is a conversation with Ilya Satskeva, co-founder and chief scientist of Open AI, one of the most cited computer scientists in history with over 165,000 citations, and to me, one of the most brilliant and insightful minds ever in the field of deep learning. There are very few people in this world who I would rather talk to and brainstorm with about deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor and a pleasure. This conversation was recorded before the outbreak of the pandemic, for everyone feeling the medical, psychological, and financial burden of this crisis, I'm sending love your way. Stay strong, we're in this together, we'll beat this thing. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, review it with Five Stars and Apple Podcasts, support on Patreon, or simply connect with me on Twitter at Lex Freedman's belt F-R-I-D-M-A-N. As usual, I'll do a few minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LexPodcast. Cash App lets you send money to friends, buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating. I recommend Ascent of Money as a great book on this history. Both the book and audiobook are great. Debits and credits on ledgers started around 30,000 years ago. The US Dollar created over 200 years ago, and Bitcoin, the first decentralized cryptocurrency released just over 10 years ago. Given that history, cryptocurrency is still very much in its early days of development, but is still aiming to and just might redefine the nature of money. Again, if you get Cash App from the App Store or Google Play and use the code LexPodcast, you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping advance robotics and STEM education for young people around the world. And now, here's my conversation with Ilya Satskeva. You were one of the three authors with Alex Koshchevsky, Jeff Hinton, of the famed AlexNet paper that is arguably the paper that marked the big catalytic moment that launched the deep learning revolution. At that time, take us back to that time. What was your intuition about neural networks, about the representation of power of neural networks? And maybe you could mention how did that evolve over the next few years, up to today, over the 10 years? Yeah, I can answer that question. At some point in about 2010 or 2011, I connected two facts in my mind. Basically, the realization was this. At some point, we realized that we can train very large, I shouldn't say very tiny by today's standards, but large and deep neural networks end to end with back propagation. At some point, different people obtained this result. I obtained this result. The first moment in which I realized that deep neural networks are powerful was when James Martens invented the Hessian Free Optimizer in 2010. And he trained a 10 layer neural network end to end without pre-training from scratch. And when that happened, I thought this is it. Because if you can train a big neural network, a big neural network can represent very complicated function. Because if you have a neural network with 10 layers, it's as though you allow the human brain to run for some number of milliseconds, neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times. So it's also kind of like 10 layers. And in 100 milliseconds, you can perfectly recognize any object. So I thought, so I already had the idea then that we need to train a very big neural network on lots of supervised data. And then it must succeed, because we can find the best neural network. And then there's also theory that if you have more data than parameters, you won't overfit. Today, we know that actually, this theory is very incomplete, and you won't overfit even you have less data than parameters. But definitely, if you have more data than parameters, you won't overfit. So the fact that neural networks were heavily overparameterized, wasn't discouraging to you. So you were thinking about the theory that the number of parameters, the fact there's a huge number of parameters is okay, it's going to be okay. I mean, there was some evidence before that it was okay, but the theory was most the theory was that if you had a big data set and a big neural net, it was going to work. The overparameterization just didn't really figure much as a problem. I thought, well, with images, you just go and add some data augmentation, and it's going to be okay. So where was any doubt coming from? The main doubt was can we train a bigger, really have enough compute to train a big enough neural net with back propagation, back propagation, I thought was would work. This image wasn't clear would was whether there would be enough compute to get a very convincing result. And then at some point, Alex Krzewski wrote these insanely fast CUDA kernels for training convolutional neural nets. Net was bam, let's do this. Let's get image net and it's going to be the greatest thing. Was your intuition, most of your intuition from empirical results by you and by others? So like just actually demonstrating that a piece of program can train a 10 layer neural network? Or was there some pen and paper or marker and white board thinking intuition? Because you just connected a 10 layer large neural network to the brain. So you just mentioned the brain. So in your intuition about neural networks, does the human brain come into play as a intuition builder? Definitely. I mean, you know, you got to be precise with these analogies between neural artificial neural networks in the brain. But there's no question that the brain is a huge source of intuition and inspiration for deep learning researchers since all the way from Rosenblatt in the 60s. Like, if you look at the whole idea of a neural network is directly inspired by the brain. You had people like McCallum and Pitts who were saying, hey, you got these neurons in the brain. And hey, we recently learned about the computer and automata. Can we use some ideas from the computer and automata to design some kind of computational object that's going to be simple, computational and kind of like the brain and invented the neuron. So they were inspired by it back then. Then you had the convolutional neural network from Fukushima. And then later young Lacan, who said, Hey, if you limit the receptive fields of a neural network, it's going to be especially suitable for images, as it turned out to be true. So there was a very small number of examples where analogies to the brain were successful. And I thought, well, probably an artificial neuron is not that different from the brain if it's cleaned hard enough. So let's just assume it is and roll with it. So we're now at a time where deep learning is very successful. So let us squint less and say, let's open our eyes and say, what do you use an interesting difference between the human brain? Now, I know you're probably not an expert, neither in your scientists and your biologists, but loosely speaking, what's the difference between the human brain and artificial neural networks? That's interesting to you for the next decade or two. That's a good question to ask. What is an interesting difference between the neural between the brain and our artificial neural networks? So I feel like today, artificial neural networks, so we all agree that there are certain dimensions in which the human brain vastly outperforms our models. But I also think that there are some ways in which artificial neural networks have a number of very important advantages over the brain. Looking at the advantages versus disadvantages is a good way to figure out what is the important difference. So the brain uses spikes, which may or may not be important. Yes, that's a really interesting question. Do you think it's important or not? That's one big architectural difference between artificial neural networks. It's hard to tell, but my prior is not very high. And I can say why. There are people who are interested in spiking neural networks. And basically, what they figured out is that they need to simulate the non-spiking neural networks in spikes. And that's how they're going to make them work. If you don't simulate the non-spiking neural networks in spikes, it's not going to work because the question is, why should it work? And that connects to questions around back propagation and questions around deep learning. You've got this giant neural network. Why should it work at all? Why should that learning rule work at all? It's not a self-evident question, especially if you, let's say, if you were just starting in the field and you read the very early papers, you can say, hey, people are saying, let's build neural networks. That's a great idea because the brain is a neural network, so it would be useful to build neural networks. Now, let's figure out how to train them. It should be possible to train them probably, but how? And so the big idea is the cost function. That's the big idea. The cost function is a way of measuring the performance of the system according to some measure. By the way, that is a big, actually, let me think. Is that one, a difficult idea to arrive at? And how big of an idea is that? That there's a single cost function? Sorry, let me take a pause. Is supervised learning a difficult concept to come to? I don't know. All concepts are very easy in retrospect. Yeah, that's what it seems trivial now. Because the reason I asked that, and we'll talk about it, is there other things? Is there things that don't necessarily have a cost function, maybe have many cost functions, or maybe have dynamic cost functions, or maybe a totally different kind of architectures? Because we have to think like that in order to arrive at something new, right? So the good examples of things which don't have clear cost functions are GANs. And again, you have a game. So instead of thinking of a cost function, where you want to optimize, where you know that you have an algorithm gradient descent, which will optimize the cost function. And then you can reason about the behavior of your system in terms of what it optimizes. With GAN, you say, I have a game, and I'll reason about the behavior of the system in terms of the equilibrium of the game. But it's all about coming up with these mathematical objects that help us reason about the behavior of our system. Right, that's really interesting. Yeah, so GAN is the only one. It's kind of a, the cost function is emergent from the comparison. I don't know if it has a cost function. I don't know if it's meaningful to talk about the cost function of a GAN. It's kind of like the cost function of biological evolution or the cost function of the economy. It's, you can talk about regions to which it will go towards, but I don't think, I don't think the cost function analogy is the most useful. So evolution doesn't, that's really interesting. So if evolution doesn't really have a cost function, like a cost function based on it's something akin to our mathematical conception of a cost function, then do you think cost functions in deep learning are holding us back? Yeah, I, so you just kind of mentioned that cost function is a, is a nice first profound idea. Do you think that's a good idea? Do you think it's an idea will go past? So self play starts to touch on that a little bit in reinforcement learning systems. That's right. Self play and also ideas around exploration where you're trying to take action. That's, that's surprise a predictor. I'm a big fan of cost functions. I think cost functions are great and they serve us really well. And I think that whenever we can do things with cost functions, we should. And you know, maybe there is a chance that we will come up with some yet another profound way of looking at things that will involve cost functions in a less central way. But I don't know, I think cost functions are, I mean, I would not bet against against cost functions. Is there other things about the brain that pop into your mind that might be different and interesting for us to consider in designing artificial neural networks? So we talked about spiking a little bit. I mean, one, one thing which may potentially be useful, I think people, neuroscientists figured out something about the learning rule of the brain or talking about spike time independent plasticity. And it would be nice if some people were to study that in simulation. Wait, sorry, spike time independent plasticity. Yeah, that's STD. It's a particular learning rule that uses spike timing to figure out how to determine how to update the synapses. So it's kind of like, if a synapse fires into the neuron before the neuron fires, then it's strengthened the synapse. And if the synapse fires into the neurons shortly after the neuron fired, then it becomes the synapse something along this line. I'm 90% sure it's right. So if I said something wrong here, don't don't get too angry. But you sounded brilliant while saying it. But the timing, that's one thing that's missing. The temporal dynamics is not captured. I think that's like a fundamental property of the brain is the timing of the signals. Well, you're recording your networks. But you think of that as, I mean, that's a very crude simplified, what's that called? There's a clock, I guess, to recurrent neural networks. This seems like the brain is the general, the continuous version of that, the generalization where all possible timings are possible. And then within those timings, this contains some information. You think recurrent neural networks, the recurrence in recurrent neural networks can capture the same kind of phenomena as the timing that seems to be important for the brain in the firing of neurons in the brain? I mean, I think recurrent neural networks are amazing and they can do, I think they can do anything we'd want them to, we'd want a system to do. Right now, recurrent neural networks have been superseded by transformers, but maybe one day they'll make a comeback, maybe they'll be back, we'll see. Let me in a small tangent say, do you think they'll be back? So so much of the breakthroughs recently that we'll talk about on natural language processing and language modeling has been with transformers that don't emphasize recurrence. Do you think recurrence will make a comeback? Well, some kind of recurrence, I think very likely. Recurrent neural networks, as they're typically thought of for processing sequences, I think it's also possible. What is, to you, a recurrent neural network? In general speaking, I guess, what is a recurrent neural network? You have a neural network which maintains a high dimensional hidden state. And then when an observation arrives, it updates its high dimensional hidden state through its connections in some way. So do you think, you know, that's what like expert systems did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a hidden state, which is its knowledge base and is growing it by sequential processing. Do you think of it more generally in that way? Or is it simply, is it the more constrained form of a hidden state with certain kind of gating units that we think of as today with LSTMs and that? I mean, the hidden state is technically what you described there, the hidden state that goes inside the LSTM or there are an N or something like this. But then what should be contained, you know, if you want to make the expert system and analogy, I'm not, I mean, you could say that the knowledge is stored in the connections and then the short term processing is done in the hidden state. Yes. Could you say that? So sort of, do you think there's a future of building large scale knowledge bases within the neural networks? Definitely. So we're going to pause on that confidence because I want to explore that. Well, let me zoom back out and ask back to the history of ImageNet. Neural networks have been around for many decades as you mentioned. What do you think were the key ideas that led to their success, that ImageNet moment and beyond the success in the past 10 years? Okay. So the question is to make sure I didn't miss anything, the key ideas that led to the success of deep learning over the past 10 years. Exactly. Even though the fundamental thing behind deep learning has been around for much longer. So the key idea about deep learning, or rather the key fact about deep learning before deep learning started to be successful is that it was underestimated. People who worked in machine learning simply didn't think that neural networks could do much. People didn't believe that large neural networks could be trained. People thought that, well, there was lots of, there was a lot of debate going on in machine learning about what are the right methods and so on. And people were arguing because there were no, there were no, there was no way to get hard facts. And by that, I mean, there were no benchmarks which were truly hard, that if you do really well on them, then you can say, look, here's my system. That's when you switch from, that's when this field becomes a little bit more of an engineering field. So in terms of deep learning to answer the question directly, the ideas were all there. The thing that was missing was a lot of supervised data and a lot of compute. Once you have a lot of supervised data and a lot of compute, then there is a third thing which is needed as well. And that is conviction, conviction that if you take the right stuff, which already exists, and apply and mixed with a lot of data and a lot of compute, that it will in fact work. And so that was the missing piece. It was you had the, you needed the data, you needed the compute which showed up in terms of GPUs, and you needed the conviction to realize that you need to mix them together. So that's really interesting. So I guess the presence of compute and the presence supervised data allowed the empirical evidence to do the convincing of the majority of the computer science community. So I guess there's a key moment with Jitendra Malik and Alex, Alyosha Efros, who were very skeptical, right? And then there's a Jeffrey Hinton that was the opposite of skeptical. And there was a convincing moment. And I think ImageNet served as that moment. And that represented this kind of, or the big pillars of computer vision community, kind of the wizards got together. And then all of a sudden there was a shift. And it's not enough for the ideas to all be there and the computer to be there. It's for it to convince the cynicism that existed that that's interesting that people just didn't believe for a couple of decades. Yeah, well, but it's more than that. It's kind of when put this way, it sounds like, well, you know, those silly people who didn't believe what were they missing. But in reality, things were confusing because neural networks really did not work on anything. And they were not the best method on pretty much anything as well. And it was pretty rational to say, yeah, this stuff doesn't have any traction. And that's why you need to have these very hard tasks which are which produce undeniable evidence. And that's how we make progress. And that's why the field is making progress today, because we have these hard benchmarks, which represent true progress. And so, and this is why we were able to avoid endless debate. So incredibly, you've contributed some of the biggest recent ideas in AI in computer vision, language, natural language processing, reinforcement learning, sort of everything in between, maybe not GANs. Is there, there may not be a topic you haven't touched. And of course, the fundamental science of deep learning. What is the difference to you between vision, language, and as in reinforcement learning action, as learning problems? And what are the commonalities? Do you see them as all interconnected? Are they fundamentally different domains that require different approaches? Okay, that's a good question. Machine learning is a field with a lot of unity, a huge amount of unity. In fact, what do you mean by unity, like overlap of ideas? overlap of ideas overlap of principles. In fact, there's only one or two or three principles, which are very, very simple. And then they apply in almost the same way, in almost the same way to the different modalities through the different problems. And that's why today, when someone writes a paper on improving optimization of deep learning and vision, it improves the different NLP applications, and it improves the different reinforcement learning applications. Reinforcement learning. So I would say that computer vision and NLP are very similar to each other. Today, they differ in that they have slightly different architectures. We use transformers in NLP, and we use convolutional neural networks in vision. But it's also possible that one day this will change and everything will be unified with a single architecture. Because if you go back a few years ago in natural language processing, there were a huge, huge number of architectures for every different tiny problem had its own architecture. Today, there's just one transformer for all those different tasks. And if you go back in time even more, you had even more and more fragmentation and every little problem in AI had its own little subspecialization and sub, you know, little set of collection of skills, people who would know how to engineer the features. Now it's all been subsumed by deep learning. We have this unification. And so I expect vision to become unified with natural language as well. Or rather, I shouldn't say expect, I think it's possible. I don't want to be too sure, because I think on the commercial neural network, it is very computationally efficient. Arell is different. Arell doesn't require slightly different techniques, because you really do need to take action. You really need to do something about exploration, your variance is much higher. But I think there is a lot of unity even there. And I would expect, for example, that at some point, there will be some broader unification between Arell and supervised learning, where somehow the Arell will be making decisions to make the supervised learning go better. And it will be, I imagine one big black box and you just throw every, you know, you shovel, shovel things into it. And it just figures out what to do with whatever you shovel in it. I mean, reinforcement learning has some aspects of language and vision combined, almost, there's elements of a long term memory that you should be utilizing, and there's elements of a really rich sensory space. So it seems like the, it's like the union of the two or something like that. I'd say something slightly different. I'd say that reinforcement learning is neither, but it naturally interfaces and integrates with the two of them. Do you think action is fundamentally different? So yeah, what is interesting about, what is unique about policy of learning to act? Well, so one example, for instance, is that when you learn to act, you are fundamentally in a non stationary world. Because as your actions change, the things you see start changing. You, you experience the world in a different way. And this is not the case for the more traditional static problem where you have a some distribution and you just apply a model to that distribution. You think it's a fundamentally different problem or is it just a more difficult general, it's a generalization of the problem of understanding. I mean, it's, it's, it's a question of definitions almost. There is a huge amount of commonality for sure. You take gradients, you try, you take gradients, we try to approximate gradients in both cases. In some case, in the case of reinforcement learning, you have some tools to reduce the variance of the gradients. You do that. There's lots of commonality, use the same neural net in both cases. You compute the gradient, you apply atom in both cases. So, I mean, there's lots in common for sure, but there are some small differences which are not completely insignificant. It's really just a matter of your, of your point of view, what frame of reference you, what, how much do you want to zoom in or out as you look at these problems? Which problem do you think is harder? So people like no Chomsky believe that language is fundamental to everything. So it underlies everything. Do you think language understanding is harder than visual scene understanding or vice versa? I think that asking if a problem is hard is slightly wrong. I think the question is a little bit wrong, and I want to explain why. So what does it mean for a problem to be hard? Okay, the non-interesting, dumb answer to that is there's a, there's a benchmark, and there's a human level performance on that benchmark. And how is the effort required to reach the human level benchmark? So from the perspective of how much until we get to human level on a very good benchmark? Yeah, like some, I understand what you mean by that. So what I was going to say that a lot of it depends on, you know, once you solve a problem, it stops being hard. And that's, that's always true. And so, but if something is hard or not, depends on what our tools can do today. So you know, you say today, through human level, language understanding and visual perception are hard in the sense that there is no way of solving the problem completely in the next three months. Right. So I agree with that statement. Beyond that, I'm just, I'll be my, my guess would be as good as yours. I don't know. Oh, okay. So you don't have a fundamental intuition about how hard language understanding is. I think I, I know I changed my mind. I'd say language is probably going to be harder. I mean, it depends on how you define it. Like if you mean absolute top notch 100% language understanding, I'll go with language. So, but then if I show you a piece of paper with letters on it, is that you see what I mean? So you have a vision system, you say it's the best human level vision system. I show you, I open a book and I show you letters. Will it understand how these letters form into word and sentences and meaning is this part of the vision problem? Where does vision end and language begin? Yeah. So Chomsky would say it starts at language. So vision is just a little example of the kind of structure and, you know, fundamental hierarchy of ideas that's already represented in our brain. Somehow that's represented through language. But where does vision stop and language begin? That's a really interesting question. So one possibility is that it's impossible to achieve really deep understanding in either images or language without basically using the same kind of system. So you're going to get the other for free. I think, I think it's pretty likely that yes, if we can get one, our machine learning is probably that good that we can get the other. But it's not 100. I'm not 100% sure. And also, I think a lot of it really does depend on your definitions. Definitions of like perfect vision. Because reading is vision, but should it count? Yeah, to me, my definition is if a system looked at an image and then a system looked at a piece of text, and then told me something about that. And I was really impressed. That's relative. You'll be impressed for half an hour. And then you're going to say, well, I mean, all the systems do that. But here's the thing they don't do. Yeah, but I don't have that with humans. Humans continue to impress me. Is that true? Well, the ones, okay, so I'm a fan of monogamy. So I like the idea of marrying somebody being with them for several decades. So I believe in the fact that, yes, it's possible to have somebody continuously giving you pleasurable, interesting, witty, new ideas, friends. Yeah, I think so. They continue to surprise you. The surprise, it's that injection of randomness seems to be a nice source of continued inspiration, like the wit, the humor. I think, yeah, that would be, it's a very subjective test, but I think if you have enough humans in the room. Yeah, I understand what you mean. Yeah, I feel like I misunderstood what you meant by impressing you. I thought you meant to impress you with its intelligence, with how valid understands an image. I thought you meant something like, I'm going to show you a really complicated image and it's going to get it right, and you're going to say, wow, that's really cool, the systems of January 2020 have not been doing that. Yeah, I think it all boils down to the reason people click like on stuff on the internet, which is like it makes them laugh. So it's like humor or wit or insight. I'm sure we'll get that as well. So forgive the romanticized question, but looking back to you, what is the most beautiful or surprising idea in deep learning or AI in general you've come across? So I think the most beautiful thing about deep learning is that it actually works. And I mean it because you got these ideas, you got a little neural network, you got the back propagation algorithm. And then you got some theories as to, you know, this is kind of like the brain. So maybe if you make it large, if you make the neural network large and you train a lot of data, then it will do the same function that the brain does. And it turns out to be true. That's crazy. And now we just train these neural networks and you make them larger and they keep getting better. And I find it unbelievable. I find it unbelievable that this whole AI stuff with neural networks works. Have you built up an intuition of why are there a little bits and pieces of intuitions of insights of why this whole thing works? I mean, some definitely. Well, we know that optimization, we now have good, you know, we've had lots of empirical, you know, huge amounts of empirical reasons to believe that optimization should work on all most problems we care about. Do you have insights of what, so you just said empirical evidence. Is most of your sort of empirical evidence kind of convinces you, it's like evolution is empirical, it shows you that look, this evolutionary process seems to be a good way to design organisms that survive in their environment. But it doesn't really get you to the insights of how the whole thing works. I think it's a good analogy is physics. You know how you say, hey, let's do some physics calculation and come up with some new physics theory and make some prediction. But then you got around the experiment. You know, you got around the experiment, it's important. So it's a bit the same here, except that maybe sometimes the experiment came before the theory. But it still is the case, you know, you have some data and you come up with some prediction, you say, yeah, let's make a big neural network, let's train it, and it's going to work much better than anything before it. And it will in fact continue to get better as you make it larger. And it turns out to be true. That's, that's amazing when a theory is validated like this, you know, it's not a mathematical theory, it's more of a biological theory almost. So I think there are not terrible analogies between deep learning and biology. I would say it's like the geometric mean of biology and physics, that's deep learning. The geometric mean of biology and physics. I think I'm going to need a few hours to wrap my head around that. Because just to find the geometric, just to find the set of what biology represents. Well, biology, in biology, things are really complicated. The theories are really, really, it's really hard to have good predictive theory. And if in physics, the theories are too good. In theory, in physics, people make these super precise theories, which make these amazing predictions. And in machine learning, we're kind of in between. Kind of in between. But it'd be nice if machine learning somehow helped us discover the unification of the two as opposed to server the in between. But you're right, that's, you're kind of trying to juggle both. So do you think there are still beautiful and mysterious properties in your networks that are yet to be discovered? Definitely. I think that we are still massively underestimating deep learning. What do you think it will look like? Like what if I knew I would have done it? So, but if you look at all the progress from the past 10 years, I would say most of it, I would say there have been a few cases where some were things that felt like really new ideas showed up. But by and large, it was every year, we thought, okay, deep learning goes this far. Nope, it actually goes further. And then the next year, okay, now you know, this is this is big deep learning, we are really done. Nope, it goes further, it just keeps going further each year. So that means that we keep underestimating, we keep not understanding it as surprising properties all the time. You think it's getting harder and harder to make progress, need to make progress? It depends on what we mean. I think the field will continue to make very robust progress for quite a while. I think for individual researchers, especially people who are doing research, it can be harder because there is a very large number of researchers right now. I think that if you have a lot of compute, then you can make a lot of very interesting discoveries, but then you have to deal with the challenge of managing a huge computer, a huge class, a huge computer cluster to run your experiments. It's a little bit harder. So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest people I know, so I'm going to keep asking. So let's imagine all the breakthroughs that happen in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by one person with one computer? In the space of breakthroughs, do you think compute and large efforts will be necessary? I mean, I can't be sure. When you say one computer, you mean how large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely. I think it's pretty unlikely. I think that the stack of deep learning is starting to be quite deep. If you look at it, you've got all the way from the ideas, the systems to build the datasets, the distributed programming, the building the actual cluster, the GPU programming, putting it all together. So the stack is getting really deep, and I think it can be quite hard for a single person to become to be world-class in every single layer of the stack. What about Vladimir Vapnik really insists on is taking MNIST and trying to learn from very few examples, so being able to learn more efficiently. Do you think there'll be breakthroughs in that space that may not need a huge compute? I think there will be a large number of breakthroughs in general that will not need a huge amount of compute. So maybe I should clarify that. I think that some breakthroughs will require a lot of compute, and I think building systems which actually do things will require a huge amount of compute. That one is pretty obvious. If you want to do X, and X requires a huge neural net, you've got to get a huge neural net, but I think there will be lots of, I think there is lots of room for very important work being done by small groups and individuals. Can you maybe sort of on the topic of the science of deep learning, talk about one of the recent papers that you've released, the Deep Double Descent, where bigger models and more data hurt. I think it's a really interesting paper. Can you describe the main idea? Yeah, definitely. So what happened is that some over the years, some small number of researchers noticed that it is kind of weird that when you make the neural net work larger, it works better, and it seems to go in contradiction with statistical ideas. And then some people made an analysis showing that actually you got this double descent bump. And what we've done was to show that double descent occurs for pretty much all practical deep learning systems. And that it'll be also, so can you step back? What's the X axis and the Y axis of a double descent plot? Okay, great. So you can look, you can do things like you can take your neural network, and you can start increasing its size slowly, while keeping your data set fixed. So if you increase the size of the neural network slowly, and if you don't do early stopping, that's a pretty important detail. Then when the neural network is really small, you make it larger, you get a very rapid increase in performance. Then you continue to make it larger. And at some point performance, you'll get worse. And it gets and it gets the worst exactly at the point at which it achieves zero training error, precisely zero training loss. And then as you make it larger, it starts to get better again. And it's kind of counterintuitive because you'd expect deep learning phenomena to be monotonic. And it's hard to be sure what it means, but it also occurs in the case of linear classifiers. And the intuition basically boils down to the following. When you, when you have a lot, when you have a large data set, and a small model, then small, tiny random. So basically, what is overfitting? Overfitting is when your model is somehow very sensitive to the small random, unimportant stuff in your data set in the training data in the training data set precisely. So if you have a small model, and you have a big data set, and there may be some random thing, you know, some training cases are randomly in the data set, and others may not be there. But the small model, but the small model is kind of insensitive to this randomness, because it's the same, you there is pretty much no uncertainty about the model when the data set is large. So okay, so at the very basic level, to me, it is the most surprising thing that neural networks don't overfit every time very quickly. Before ever being able to learn anything, the huge number of parameters. So here is so there is one way Okay, so maybe so let me try to give the explanation and maybe that will be that will work. So you got a huge neural network, let's suppose you got a your you have a huge neural network, you have a huge number of parameters. And now let's pretend everything is linear, which is not let's just pretend. Then there is this big subspace, where a neural network achieves zero error. And SGT is going to find approximately that's right, approximately the point with the smallest norm in that subspace. And that can also be proven to be insensitive to the small randomness in the data, when the dimensionality is high. But when the dimensionality of the data is equal to the dimensionality of the model, then there is a one to one correspondence between all the data sets and the models. So small changes in the data set actually lead to large changes in the model and that's why performance gets worse. So this is the best explanation more or less. So then it would be good for the model to have more parameters sort of to be bigger than the data. That's right, but only if you don't early stop. If you introduce early stop in your regularization, you can make a double as a descent pump almost completely disappear. What is early stop early stopping is when you train your model, and you monitor your validation performance. And then if at some point validation performance starts to get worse, you say, okay, let's stop training. We are good. We are good. We are good enough. So the magic happens after that moment. So you don't want to do the early stopping. Well, if you don't do the early stopping, you get this very, you get the very pronounced double descent. Do you have any intuition why this happens? Double descent or sorry, are you stopping? No, the double descent. So the well, yeah, so I try it. Let's see the intuition is basically is this that when the data set has as many degrees of freedom as the model, then there is a one to one correspondence between them. And so small changes to the data set lead to noticeable changes in the model. So your model is very sensitive to all the randomness. It is unable to discard it. Whereas it turns out that when you have a lot more data than parameters, or a lot more parameters than data, the resulting solution will be insensitive to small changes in the data set. So it's able to nicely put discard the small changes, the randomness. Exactly. The spurious correlations which you don't want. Jeff Hinton suggested we need to throw back propagation. We already kind of talked about this a little bit, but he suggested we need to throw away back propagation and start over. I mean, of course, some of that is a little bit wit and humor. But what do you think? What could be an alternative method of training neural networks? Well, the thing that he said precisely is that to the extent that you can't find back propagation in the brain, it's worth seeing if we can learn something from how the brain learns. But back propagation is very useful and we should keep using it. Oh, you're saying that once we discover the mechanism of learning in the brain or any aspects of that mechanism, we should also try to implement that in your networks. If it turns out that you can't find back propagation in the brain. If we can't find back propagation in the brain. Well, so I guess your answer to that is back propagation is pretty damn useful. So why are we complaining? I mean, I personally am a big fan of back propagation. I think it's a great algorithm because it solves an extremely fundamental problem which is finding a neural circuit subject to some constraints. I don't see that problem going away. So that's why I really, I think it's pretty unlikely that we'll have anything which is going to be dramatically different. It could happen. But I wouldn't bet on it right now. So let me ask a sort of big picture question. Do you think neural networks can be made to reason? Why not? Well, if you look, for example, at AlphaGo or AlphaZero, the neural network of AlphaZero plays Go, which we all agree is a game that requires reasoning, better than 99.9% of all humans. Just the neural network without the search, just the neural network itself. Doesn't that give us an existence proof that neural networks can reason? To push back and disagree a little bit, we all agree that Go is reasoning. I think I agree. I don't think it's a trivial. So obviously, reasoning like intelligence is a loose gray area term a little bit. Maybe you disagree with that. But yes, I think it has some of the same elements of reasoning. Reasoning is almost akin to search. There's a sequential element of stepwise consideration of possibilities and sort of building on top of those possibilities in a sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of like that. And when you have a single neural network doing that without search, that's kind of like that. So there's an existence proof in a particular constrained environment that a process akin to what many people call reasoning exists. But more general kind of reasoning. So off the board. There is one other existence proof. Oh boy, which one? Us humans? Yes. Okay. All right. So do you think the architecture that will allow neural networks to reason will look similar to the neural network architectures we have today? I think it will. I think, well, I don't want to make two overly definitive statements. I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs of the future will be very similar to the architecture that exists today, maybe a little bit more recurrent, maybe a little bit deeper. But these neural nets are so insanely powerful. Why wouldn't they be able to learn to reason? Humans can reason. So why can't neural networks? So do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning? So it's not a fundamentally different process. Again, this is stuff we don't nobody knows the answer to. So when it comes to our neural networks, I would think which I would say is that neural networks are capable of reasoning. But if you train a neural network on a task which doesn't require reasoning, it's not going to reason. This is a well-known effect where the neural network will solve exactly the problem that you pose in front of it in the easiest way possible. Right. That takes us to one of the brilliant ways you describe neural networks, which is you've referred to neural networks as the search for small circuits and maybe general intelligence as the search for small programs, which I found is a metaphor very compelling. Can you elaborate on that difference? Yeah. So the thing which I said precisely was that if you can find the shortest program that outputs the data in your disposal, then you will be able to use it to make the best prediction possible. And that's a theoretical statement which can be proved mathematically. Now, you can also prove mathematically that it is that finding the shortest program which generates some data is not a computable operation. No finite amount of compute can do this. So then with neural networks, neural networks are the next best thing that actually works in practice. We are not able to find the best, the shortest program which generates our data, but we are able to find a small, but now that statement should be amended, even a large circuit which fits our data in some way. Well, I think what you meant by the small circuit is the smallest needed circuit. Well, the thing which I would change now, back then I really haven't fully internalized the overparameterized results. The things we know about overparameterized neural nets, now I would phrase it as a large circuit whose weights contain a small amount of information, which I think is what's going on. If you imagine the training process of a neural network as you slowly transmit entropy from the data set to the parameters, then somehow the amount of information in the weights ends up being not very large, which would explain whether generalized so well. So that's the large circuit might be one that's helpful for the generalization. Yeah, something like this. But do you see it important to be able to try to learn something like programs? I mean, if we can, definitely. I think the answer is kind of yes, if we can do it. We should do things that we can do it. It's the reason we are pushing on deep learning. The fundamental reason, the root cause is that we are able to train them. So in other words, training comes first. We've got our pillar, which is the training pillar. And now we are trying to contort our neural networks around the training pillar. We got to stay trainable. This is an invariant we cannot violate. And so being trainable means starting from scratch, knowing nothing, you can actually pretty quickly converge towards knowing a lot or even slowly. But it means that given the resources at your disposal, you can train the neural net and get it to achieve useful performance. Yeah, that's a pillar we can't move away from. That's right. Because if you can, whereas if you say, Hey, let's find the shortest program, we can't do that. So it doesn't matter how useful that would be. We can do it. So we want. So do you think you kind of mentioned that the neural networks are good at finding small circuits or large circuits? Do you think then the matter of finding small programs is just the data? No. So the, sorry, not the size or the quality, the type of data sort of ask giving it programs. Well, I think the thing is that right now, finding there are no good precedents of people successfully finding programs really well. And so the way you'd find programs is you'd train a deep neural network to do it basically. But which is the right way to go about it. But there's not good illustrations that hasn't been done yet. But in principle, it should be possible. Can you elaborate a little bit? What's your insight in principle? And put another way, you don't see why it's not possible. Well, it's kind of like more, it's more a statement of, I think that it's, I think that it's unwise to bet against deep learning. And if it's a fun, if it's a cognitive function that humans seem to be able to do, then it doesn't take too long for some deep neural net to pop up that can do it too. Yeah, I'm there with you. I can, I've stopped betting against neural networks at this point because they continue to surprise us. What about long term memory? Can neural networks have long term memory or something like knowledge basis? So being able to aggregate important information over long periods of time, that would then serve as useful sort of representations of state that you can make decisions by so have a long term context based on what you make in the decision. So in some sense, the parameters already do that. The parameters are an aggregation of the day of the neural of the entirety of the neural net experience. And so they count as the long form, long term knowledge. And people have trained various neural nets to act as knowledge basis and, you know, investigated with invest, people have investigated language models as knowledge basis. So there is work, there is work there. Yeah, but in some sense, do you think in every sense, do you think there's a, it's all just a matter of coming up with a better mechanism of forgetting the useless stuff and remembering the useful stuff? Because right now, I mean, there's not been mechanisms that do remember really long term information. What do you mean by that precisely? Precisely. I like the word precisely. So I'm thinking of the kind of compression of information the knowledge basis represent. Sort of creating a, now I apologize for my sort of human centric thinking about what knowledge is because neural networks aren't interpretable necessarily with the kind of knowledge they have discovered. But a good example for me is knowledge basis being able to build up over time something like the knowledge that Wikipedia represents. It's a really compressed, structured knowledge base. Obviously, not the actual Wikipedia or the language, but like a semantic web, the dream that semantic web represented. So it's a really nice compressed knowledge base or something akin to that in the noninterpretable sense as neural networks would have. Well, the neural networks would be noninterpretable if you look at their rates, but their outputs should be very interpretable. Okay, so yeah, how do you, how do you make very smart neural networks like language models interpretable? Well, you ask them to generate some text and the text will generally be interpretable. Do you find that the epitome of interpretability, like can you do better? Like can you, because you can't, okay, I'd like to know what does it know and what doesn't know. I would like the neural network to come up with examples where it's completely dumb and examples where it's completely brilliant. And the only way I know how to do that now is to generate a lot of examples and use my human judgment. But it would be nice if the neural network had some self-awareness about 100%. I'm a big believer in self-awareness. And I think that I think, I think neural net self-awareness will allow for things like the capabilities, like the ones you describe, like for them to know what they know and what they don't know. And for them to know where to invest, to increase their skills most optimally. And to your question of interpretability, there are actually two answers to that question. One answer is, you know, we have the neural net, so we can analyze the neurons and we can try to understand what the different neurons and different layers mean. And you can actually do that. And OpenAI has done some work on that. But there is a different answer, which is that I would say that's the human centric answer where you say, you know, you look at a human being, you can't read. How do you know what a human being is thinking? You ask them, you say, Hey, what do you think about this? What do you think about that? And you get some answers. The answers you get are sticky. In the sense, you already have a mental model. You already have a mental model of that human being. You already have an understanding of like a big conception of what it of that human being, how they think, what they know, how they see the world, and then everything you ask, you're adding on to that. And that stickiness seems to be, that's one of the really interesting qualities of the human being is that information is sticky. You don't, you seem to remember the useful stuff, aggregate it well, and forget most of the information that's not useful, that process. But that's also pretty similar to the process that neural networks do is just that neural networks are much crappier at this time. It doesn't seem to be fundamentally that different. But just stick on reasoning for a little longer. He said, why not? Why can't I reason? What's a good impressive feat benchmark to you of reasoning that you'll be impressed by if neural networks were able to do? Is that something you already have in mind? Well, I think writing, writing really good code. I think proving really hard theorems, solving open ended problems without of the box solutions. And sort of theorem type mathematical problems. Yeah, I think those ones are a very natural example as well. You know, if you can prove an unproven theorem, then it's hard to argue don't reason. And so by the way, and this comes back to the point about the hard results, you know, if you got a hard, if you have machine learning, deep learning as a field is very fortunate, because we have the ability to sometimes produce these unambiguous results. And when they happen, the debate changes, the conversation changes, it's a converse, you have the ability to produce conversation changing results conversation. And then of course, just like you said, people kind of take that for granted, say that wasn't actually a hard problem. Well, I mean, at some point, we'll probably run out of hard problems. Yeah, that whole mortality thing is kind of kind of a sticky problem that we haven't quite figured out. Maybe we'll solve that one. I think one of the fascinating things in your entire body of work, but also the work at OpenAI recently, one of the conversation changers has been in the world of language models. Can you briefly kind of try to describe the recent history of using neural networks in the domain of language and text? Well, there's been lots of history. I think I think the Elman network was was a small, tiny recurrent neural network applied to language back in the 80s. So the history is really, you know, fairly long, at least. And the thing that started the thing that changed the trajectory of neural networks and language is the thing that changed the trajectory of all deep learning and that's data and compute. So suddenly you move from small language models, which learn a little bit. And with language models, in particular, you can, there's a very clear explanation for why they need to be large to be good, because they're trying to predict the next word. So when you don't know anything, you'll notice very, very broad strokes, surface level patterns, like sometimes there are characters and there is space between those characters, you'll notice this pattern. And you'll notice that sometimes there is a comma and then the next character is a capital letter, you'll notice that pattern. Eventually, you may start to notice that there are certain words occur often, you may notice that spellings are a thing, you may notice syntax. And when you get really good at all these, you start to notice the semantics, you start to notice the facts. But for that to happen, the language model needs to be larger. So that's, let's linger on that, because that's where you and Noam Chomsky disagree. So you think we're actually taking incremental steps, a sort of larger network, larger compute will be able to get to the semantics, be able to understand language without what Noam likes to sort of think of as a fundamental understandings of the structure of language, like imposing your theory of language onto the learning mechanism. So you're saying the learning, you can learn from raw data, the mechanism that underlies language? Well, I think it's pretty likely. But I also want to say that I don't really know precisely what Chomsky means when he talks about him. You said something about imposing your structure on language. I'm not 100% sure what he means. But empirically, it seems that when you inspect those larger language models, they exhibit signs of understanding the semantics, whereas the smaller language models do not. We've seen that a few years ago when we did work on the sentiment neuron, we trained a small, you know, smallish LSTM to predict the next character in Amazon reviews. And we noticed that when you increase the size of the LSTM from 500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent the sentiment of the article, of sorry, of their view. Now, why is that sentiment is a pretty semantic attribute? It's not a syntactic attribute. And for people who might not know, I don't know if that's a standard term, but sentiment is whether it's a positive or negative review. That's right. Like, is the person happy with something or is the person unhappy with something? And so here we had very clear evidence that a small neural net does not capture sentiment while a large neural net does. And why is that? Well, our theory is that at some point, you run out of syntax to models, you start to gotta focus on something else. And with size, you quickly run out of syntax to model, and then you really start to focus on the semantics is would be the idea. That's right. And so I don't want to imply that our models have complete semantic understanding, because that's not true. But they definitely are showing signs of semantic understanding, partial semantic understanding, but the smaller models do not show that those signs. Can you take a step back and say, what is GPT2, which is one of the big language models that was the conversation changer in the past couple of years? Yes. So GPT2 is a transformer with one and a half billion parameters that was trained on up on about 40 billion tokens of text, which were obtained from web pages that were linked to from Reddit articles with more than three uploads. And what's the transformer? The transformer, it's the most important advance in neural network architectures in recent history. What is attention maybe to because I think that's an interesting idea, not necessarily sort of technically speaking, but the idea of attention versus maybe what recurrent neural networks represent. Yeah. So the thing is, the transformer is a combination of multiple ideas simultaneously of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key. The transformer is successful because it is the simultaneous combination of multiple ideas. And if you were to remove either idea, it would be much less successful. So the transformer uses a lot of attention, but attention exists for a few years. So that can't be the main innovation. The transformer is designed in such a way that it runs really fast on the GPU. And that makes a huge amount of difference. This is one thing. The second thing is that transformer is not recurrent. And that is really important too, because it is more shallow and therefore much easier to optimize. So in other words, uses attention. It is, it is a really great fit to the GPU. And it is not recurrent. So therefore, less deep and easier to optimize. And the combination of those factors make it successful. So now it makes it makes great use of your GPU. It allows you to achieve better results for the same amount of compute. And that's why it's successful. Were you surprised how well transformers worked? And GPT2 worked? So you worked on language. You've had a lot of great ideas before transformers came about in language. So you got to see the whole set of revolutions before and after. Were you surprised? Yeah, a little. A little? Yeah. I mean, it's hard. It's hard to remember because you adapt really quickly. But it definitely was surprising. It definitely was. In fact, I'll, you know what, I'll, I'll retract my statement. It was, it was pretty amazing. It was just amazing to see, generate this text of this. And you know, you got to keep in mind that we've seen, at that time, we've seen all this progress in GANs, in improving the, you know, the samples produced by GANs were just amazing. You have these realistic faces, but text hasn't really moved that much. And suddenly we moved from, you know, whatever GANs were in 2015, to the best, most amazing GANs in one step. And I was really stunning. Even though theory predicted, yeah, you train a big language model, of course, you should get this, but then to see it with your own eyes. It's something else. And yet we adapt really quickly. And now there's a sort of some cognitive scientist, right? Articles saying that GPT2 models don't truly understand language. So we adapt quickly to how amazing the fact that they're able to model the language so well is. So what do you think is the bar? For what? For impressing us that it. I don't know. Do you think that bar will continuously be moved? Definitely. I think when you start to see really dramatic economic impact, that's when I think that's in some sense the next barrier. Because right now, if you think about the work in AI, it's really confusing. It's really hard to know what to make of all these advances. It's kind of like, okay, you got an advance and now you can do more things and you got another improvement and you got another cool demo. At some point, I think people who are outside of AI, they can no longer distinguish this progress anymore. So we were talking offline about translating Russian to English and how there's a lot of brilliant work in Russian that the rest of the world doesn't know about. That's true for Chinese. That's true for a lot of scientists and just artistic work in general. Do you think translation is the place where we're going to see sort of economic big impact? I don't know. I think there is a huge number of applications. First of all, I want to point out that translation already today is huge. I think billions of people interact with big chunks of the internet primarily through translation. So translation is already huge and it's hugely positive too. I think self-driving is going to be hugely impactful. And it's unknown exactly when it happens, but again, I would not bet against deep learning. So there's deep learning in general, but you think deep learning for self-driving? Yes, deep learning for self-driving. But I was talking about sort of language models. Be your duff a little bit. Just to check. You're not seeing a connection between driving and language. No, no. Okay. I'd rather both use neural nets. That would be a poetic connection. I think there might be some, like you said, there might be some kind of unification towards a kind of multitask transformers that can take on both language and vision tasks. That'd be an interesting unification. Let's see. What can I ask about GPT two more? It's simple. It's not much to ask. You take a transform, you make it bigger, give it more data, and suddenly it does all those amazing things. Yeah. One of the beautiful things is that GPT, the transformers are fundamentally simple to explain, to train. Do you think bigger will continue to show better results in language? Probably. Sort of like what are the next steps with GPT two? Do you think? I mean, I think for sure seeing what larger versions can do is one direction. Also, I mean, there are many questions. There's one question which I'm curious about, and that's the following. Right now, GPT two, so we feed it all this data from the internet, which means that it needs to memorize all those random facts about everything in the internet. It would be nice if the model could somehow use its own intelligence to decide what data it wants to start, accept, and what data it wants to reject, just like people. People don't learn all data indiscriminately. We are super selective about what we learn, and I think this kind of active learning I think would be very nice to have. Yeah. Listen, I love active learning. So let me ask, does the selection of data, can you just elaborate that a little bit more? Do you think the selection of data is, like I have this kind of sense that the optimization of how you select data, so the active learning process is going to be a place for a lot of breakthroughs even in the near future, because there hasn't been many breakthroughs there that are public. I feel like there might be private breakthroughs that companies keep to themselves, because the fundamental problem has to be solved if you want to solve self-driving, if you want to solve a particular task. What do you think about the space in general? Yeah, so I think that for something like active learning, or in fact for any kind of capability, like active learning, the thing that it really needs is a problem. It needs a problem that requires it. It's very hard to do research about the capability if you don't have a task, because then what's going to happen is you will come up with an artificial task, get good results, but not really convince anyone. Right. We're now past the stage where getting a result on MNIST, some clever formulation of MNIST will convince people. That's right. In fact, you could quite easily come up with a simple active learning scheme on MNIST and get a 10x speed up, but then so what? I think that active learning will naturally arise as problems that require it to pop up. That's my take on it. There's another interesting thing that OpenAS brought up with GPT2, which is when you create a powerful artificial intelligence system, and it was unclear what kind of detrimental, once you release GPT2, what kind of detrimental effect it'll have. Because if you have a model that can generate pretty realistic text, you can start to imagine that it would be used by bots in some way that we can't even imagine. There's this nervousness about what it's possible to do. You did a really brave and I think profound thing, which just started a conversation about this. How do we release powerful artificial intelligence models to the public? If we do it all, how do we privately discuss with other even competitors about how we manage the use of the systems and so on? From this whole experience, you've released a report on it, but in general, are there any insights that you've gathered from just thinking about this, about how you release models like this? I think that my take on this is that the field of AI has been in a state of childhood, and now it's exiting that state and it's entering a state of maturity. What that means is that AI is very successful and also very impactful, and its impact is not only large, but it's also growing. For that reason, it seems wise to start thinking about the impact of our systems before releasing them maybe a little bit too soon rather than a little bit too late. With the case of GPT-2, like I mentioned earlier, the results really were stunning, and it seemed plausible. It didn't seem certain. It seemed plausible that something like GPT-2 could easily use to reduce the cost of disinformation. There was a question of what's the best way to release it, and a staged release seemed logical. A small model was released, and there was time to see the... Many people use these models in lots of cool ways. There've been lots of really cool applications. There haven't been any negative applications we know of, and so eventually it was released, but also other people replicated similar models. That's an interesting question though that we know of. In your view, staged release is at least part of the answer to the question of how do we... What do we do once we create a system like this? It's part of the answer, yes. Is there any other insights? Say you don't want to release the model at all because it's useful to you for whatever the business is. Well, there are plenty of people who don't release models already. Right, of course, but is there some moral ethical responsibility when you have a very powerful model to communicate? Just as you said, when you had GPT-2, it was unclear how much it could be used for misinformation. It's an open question, and getting an answer to that might require that you talk to other really smart people that are outside of your particular group. Have you... Please tell me there's some optimistic pathway for people across the world to collaborate on these kinds of cases, or is it still really difficult from one company to talk to another company? So it's definitely possible. It's definitely possible to discuss these kind of models with colleagues elsewhere and to get their take on what to do. How hard is it though? I mean... Do you see that happening? I think that's a place where it's important to gradually build trust between companies, because ultimately, all the AI developers are building technology, which is going to be increasingly more powerful, and so it's... The way to think about it is that ultimately we're only together. Yeah, it's... I tend to believe in the better angels of our nature, but I do hope that when you build a really powerful AI system in a particular domain, that you also think about the potential negative consequences of... Yeah. It's an interesting and scary possibility that there will be a race for AI development that would push people to close that development and not share ideas with others. I don't love this. I've been in a pure academic for 10 years. I really like sharing ideas and it's fun, it's exciting. What do you think it takes to... Let's talk about AGI a little bit. What do you think it takes to build a system of human level intelligence? We talked about reasoning, we talked about long-term memory, but in general, what does it take, do you think? Well, I can't be sure, but I think the deep learning plus maybe another small idea. Do you think self-play will be involved? Sort of like you've spoken about the powerful mechanism of self-play where systems learn by sort of exploring the world in a competitive setting against other entities that are similarly skilled as them and so incrementally improving this way? Do you think self-play will be a component of building an AGI system? Yeah, so what I would say to build AGI, I think it's going to be deep learning plus some ideas and I think self-play will be one of those ideas. I think that that is a very... Self-play has this amazing property that it can surprise us in truly novel ways. For example, like we, I mean, pretty much every self-play system, both are Dota bot. I don't know if OpenAI had a release about multi-agents where you had two little agents who were playing hide and seek and of course also AlphaZero. They all produce surprising behaviors. They all produce behaviors that we didn't expect. They are creative solutions to problems and that seems like an important part of AGI that our systems don't exhibit routinely right now. And so that's why I like this area, I like this direction because of its ability to surprises. To surprises and an AGI system would surprise us fundamentally. Yes, but and to be precise, not just a random surprise, but to find a surprising solution to a problem that's also useful. Now, a lot of the self-play mechanisms have been used in the game context or at least in the simulation context. How far along the path to AGI do you think will be done in simulation? How much faith promise do you have in simulation versus having to have a system that operates in the real world, whether it's the real world of digital real-world data or real-world like actual physical world with robotics? I don't think it's in either war. I think simulation is a tool and it helps. It has certain strengths and certain weaknesses and we should use it. Yeah, but okay, I understand that that's true. But one of the criticisms of self-play, one of the criticisms of reinforcement learning is one of the its current power, its current results while amazing have been demonstrated in a simulated environments or very constrained physical environments. Do you think it's possible to escape them? Escape the simulated environments and be able to learn in non-simulated environments? Or do you think it's possible to also just simulate in a photorealistic and physics realistic way the real world in a way that we can solve real problems with self-play in simulation? I think that transfer from simulation to the real world is definitely possible and has been exhibited many times by many different groups. It's been especially successful in vision. Also OpenAI in the summer has demonstrated a robot hand which was trained entirely in simulation in a certain way that allowed for sim to real transfer to occur. Is this for the Rubik's Cube? Yes, that's right. I wasn't aware that was trained in simulation. It was trained in simulation entirely. Really? So it wasn't in the physical that the hand wasn't trained? No, 100% of the training was done in simulation and the policy that was learned in simulation was trained to be very adaptive. So adaptive that when you transfer it, it could very quickly adapt to the physical world. So the kind of perturbations with the giraffe or whatever the heck it was, were those part of the simulation? Well, the simulation was generally, so the simulation was trained to be robust to many different things, but not the kind of perturbations we've had in the video. So it's never been trained with a glove. It's never been trained with a stuffed giraffe. So in theory, these are novel perturbations? Correct. It's not in theory in practice. That those are novel perturbations? Well, that's okay. That's a clean, small-scale, but clean example of a transfer from the simulated world to the physical world. Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in general and the better the transfer capabilities are, the more useful simulation will become because then you could take, you could experience something in simulation and then learn a moral of the story which you could then carry with you to the real world, right? As humans do all the time and they play computer games. So let me ask sort of an embodied question, staying on AGI for a sec. Do you think AGI system, we need to have a body? We need to have some of those human elements of self-awareness, consciousness, sort of fear of mortality, sort of self-preservation in the physical space which comes with having a body? I think having a body will be useful. I don't think it's necessary, but I think it's very useful to have a body for sure because you can learn a whole new, you can learn things which cannot be learned without a body, but at the same time, I think that you can, if you don't have a body, you could compensate for it and still succeed. You think so? Yes. Well, there is evidence for this. For example, there are many people who were born deaf and blind and they were able to compensate for the lack of modalities. I'm thinking about Helen Keller specifically. So even if you're not able to physically interact with the world and if you're not able to, I actually was getting at, maybe let me ask on the more particular, I'm not sure if it's connected to having a body or not, but the idea of consciousness and a more constrained version of that is self-awareness. Do you think an AGI system should have consciousness? We can't define whatever the heck you think consciousness is. Yeah. Hard question to answer, given how hard it is to define it. Do you think it's useful to think about? I mean, it's definitely interesting, it's fascinating. I think it's definitely possible that our systems will be conscious. Do you think that's an emergent thing that just comes from, do you think consciousness could emerge from the representation that's stored within your networks? So like that it naturally just emerges when you become more and more, you're able to represent more and more over the world? Well, I'd say I'd make the following argument, which is humans are conscious, and if you believe that artificial neural nets are sufficiently similar to the brain, then there should at least exist artificial neural nets you should be conscious to. You're leaning on that existence proof pretty heavily. Okay. But that's the best answer I can give. No, I know. I know. There's still an open question if there's not some magic in the brain that we're not. I mean, I don't mean a non-materialistic magic, but that the brain might be a lot more complicated and interesting than we give it credit for. If that's the case, then it should show up and at some point, we will find out that we can't continue to make progress. But I think it's unlikely. So we talk about consciousness, but let me talk about another poorly defined concept of intelligence. Again, we've talked about reasoning. We've talked about memory. What do you think is a good test of intelligence for you? Are you impressed by the test that Alan Turing formulated with the imitation game with natural language? Is there something in your mind that you would be deeply impressed by if a system was able to do? I mean, lots of things. There's a certain frontier of capabilities today. And there exist things outside of that frontier, and I would be impressed by any such thing. For example, I would be impressed by a deep learning system which solves a very pedestrian task like machine translation or computer vision task or something which never makes a mistake a human wouldn't make under any circumstances. I think that is something which have not yet been demonstrated and I would find it very impressive. Yes, so right now, they make mistakes in different, they might be more accurate than human beings, but they still make a different set of mistakes. So I would guess that a lot of the skepticism that some people have about deep learning is when they look at their mistakes and they say, well, those mistakes, they make no sense. If you understood the concept, you wouldn't make that mistake. And I think that changing that would inspire me. That would be, yes, this is progress. Yeah, that's a really nice way to put it. But I also just don't like that human instinct to criticize a model is not intelligent. That's the same instinct as we do when we criticize any group of creatures as the other. Because it's very possible that GPT2 is much smarter than human beings at many things. That's definitely true. It has a lot more breadth of knowledge. Yes, breadth of knowledge and even perhaps depth on certain topics. It's kind of hard to judge what depth means, but there's definitely a sense in which humans don't make mistakes that these models do. Yes, the same is applied to autonomous vehicles. The same is probably going to continue being applied to a lot of artificial intelligence systems. In the 21st century, the process of analyzing the progress of AI is the search for one case where the system fails in a big way where humans would not. And then many people writing articles about it. And then broadly, the public generally gets convinced that the system is not intelligent. And we pacify ourselves by thinking it's not intelligent because of this one anecdotal case. And this seems to continue happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also extremely impressed by the systems that exist today. But I think this connects to the earlier point we discussed that it's just confusing to judge progress in AI. And you have a new robot demonstrating something. How impressed should you be? And I think that people will start to be impressed once AI starts to really move the needle on the GDP. So you're one of the people that might be able to create an AI system here, not you, but you and open AI. If you do create an AI system, and you get to spend sort of the evening with it, him, her, what would you talk about, do you think? The very first time? First time? Well, the first time I would just would just ask all kinds of questions and try to make it to get it to make a mistake. And that would be amazed that it doesn't make mistakes and just keep keep asking broad. Okay, what kind of questions do you think? Would they be factual or would they be personal, emotional, psychological? What do you think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself talking to a system like this? Now, again, let me emphasize the fact that you truly are one of the people that might be in the room where this happens. So let me ask a sort of a profound question about, I've just talked to a Stalin historian. I've been talking to a lot of people who are studying power. Abraham Lincoln said, nearly all men can stand adversity. But if you want to test a man's character, give him power. I would say the power of the 21st century, maybe the 22nd, but hopefully the 21st would be the creation of an AGI system and the people who have control, direct possession and control the AGI system. So what do you think after spending that evening having a discussion with the AGI system? What do you think you would do? Well, the ideal world I'd like to imagine is one where humanity are like the board, the board members of a company where the AGI is the CEO. So it would be, I would like the picture of which I would imagine is you have some kind of different entities, different countries or cities, and the people that leave their vote for what the AGI that represents them should do and the AGI that represents them goes and does it. I think a picture like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city, for a country, and it would be trying to in effect take the democratic process to the next level. And the board can almost fire the CEO? Essentially, press the reset button, say, re-randomize the parameters. But let me sort of, that's actually, okay, that's a beautiful vision, I think, as long as it's possible to press the reset button. Do you think it will always be possible to press the reset button? So I think that it's definitely really possible to build. So you're talking, so the question that I really understand from you is, will humans or humans people have control over the AI systems that they build? Yes. And my answer is, it's definitely possible to build AI systems which will want to be controlled by their humans. Wow, that's part of their, so it's not that just they can't help but be controlled, but that's the one of the objectives of their existence is to be controlled. In the same way that human parents generally want to help their children, they want their children to succeed. It's not a burden for them. They are excited to help the children to feed them and to dress them and to take care of them. And I believe with high conviction that the same will be possible for an AGI. It will be possible to program an AGI to design it in such a way that it will have a similar deep drive that it will be delighted to fulfill and the drive will be to help humans flourish. But let me take a step back to that moment where you create the AGI system. I think this is a really crucial moment. And between that moment and the Democratic Board members with the AGI at the head, there has to be a relinquishing of power. So as George Washington, despite all the bad things he did, one of the big things he did is he relinquished power. He first of all didn't want to be president and even when he became president, he didn't keep just serving as most dictators do for indefinitely. Do you see yourself being able to relinquish control over an AGI system, given how much power you can have over the world, at first financial, just make a lot of money and then control by having possession as a AGI system? I'd find it trivial to do that. I'd find it trivial to relinquish this kind of power. I mean, you know, the kind of scenario you are describing sounds terrifying to me. That's all. I would absolutely not want to be in that position. Do you think you represent the majority or the minority of people in the AGI community? Well, I mean, it's an open question, an important one. Are most people good is another way to ask it. So I don't know if most people are good. But I think that when it really counts, people can be better than we think. That's beautifully put. Yeah. Are there specific mechanisms you can think of of aligning AGI in values to human values? Is that do you think about these problems of continued alignment as we develop the systems? Yeah, definitely. In some sense, the kind of question which you are asking is so if you have to translate the question to today's terms, it would be a question about how to get an RL agent that's optimizing a value function which itself is learned. And if you look at humans, humans are like that because the reward function, the value function of humans is not external, it is internal. That's right. And and there are definite ideas of how to train a value function, basically an objective, an as objective as possible perception system that will be trained separately to recognize, to internalize human judgments on different situations. And then that component wouldn't be integrated as the value as the base value function for some more capable RL system. You could imagine a process like this. I'm not saying this is the process. I'm saying this is an example of the kind of thing you could do. So on that topic of the objective functions of human existence, what do you think is the objective function that's implicit in human existence? What's the meaning of life? Oh, I think the question is wrong in some way. I think that the question implies that there is an objective answer which is an external answer, you know, your meaning of life is X. I think what's going on is that we exist and that's amazing. And we should try to make the most of it and try to maximize our own value and enjoyment of a very short time while we do exist. It's funny because action does require an objective function. It's definitely theirs in some form, but it's difficult to make it explicit and maybe impossible to make it explicit. I guess is what you're getting at. And that's an interesting fact of an RL environment. Well, what I was making a slightly different point is that humans want things and their wants create the drives that cause them to, you know, our wants are our objective functions, our individual objective functions. We can later decide that we want to change, that what we wanted before is no longer good and we want something else. Yeah, but they're so dynamic, there's got to be some underlying sort of Freud, there's things like sexual stuff, there's people who think it's the fear of death and there's also the desire for knowledge and, you know, all these kinds of things, procreation, the sort of all the evolutionary arguments, it seems to be, there might be some kind of fundamental objective function from which everything else emerges. But it seems like it's very difficult to make it explicit. I think that probably is an evolutionary objective function, which is to survive and procreate and make your students succeed. That would be my guess, but it doesn't give an answer to the question of what's the meaning of life. I think you can see how humans are part of this big process, this ancient process we are, we exist on a small planet. And that's it. So given that we exist, try to make the most of it and try to enjoy more and suffer less as much as we can. Let me ask two silly questions about life. One, do you have regrets? Moments that if you went back, you would do differently. And two, are there moments that you're especially proud of that made you truly happy? So I can answer both questions. Of course, there's a huge number of choices and decisions that have made that with the benefit of hindsight, I wouldn't have made them. And I do experience some regret, but I tried to take solace in the knowledge that at the time I did the best I could. And in terms of things that I'm proud of, I'm very fortunate to have done things I'm proud of and they made me happy for some time, but I don't think that that is the source of happiness. So your academic accomplishments, all the papers, you're one of the most cited people in the world. All the breakthroughs I mentioned in computer vision and language and so on is what is the source of happiness and pride for you? I mean, all those things are a source of pride for sure. I'm very grateful for having done all those things. And it was very fun to do them. But happiness comes, you know, you can, happiness, well, my current view is that happiness comes from our to a very large degree from the way we look at things. You know, you can have a simple meal and be quite happy as a result or you can talk to someone and be happy as a result as well. Or conversely, you can have a meal and be disappointed that the meal wasn't a better meal. So I think a lot of happiness comes from that. But I'm not sure. I don't want to be too confident. Being humble in the face of the uncertainty seems to be also a part of this whole happiness thing. Well, I don't think there's a better way to end it than meaning of life and discussions of happiness. So Ilya, thank you so much. You've given me a few incredible ideas. You've given the world many incredible ideas. I really appreciate it. And thanks for talking today. Yeah, thanks for stopping by. I really enjoyed it. Thanks for listening to this conversation with Ilya Setskever. And thank you to our presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App and using the code Lex podcast. If you enjoy this podcast, subscribe on YouTube, review it with five stars and Apple podcast, support on Patreon or simply connect with me on Twitter at Lex Friedman. And now let me leave you with some words from Alan Turing on machine learning. Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child? If this were then subjected to an appropriate course of education, one would obtain the adult brain. Thank you for listening and hope to see you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.36, "text": " The following is a conversation with Ilya Satskeva, co-founder and chief scientist of Open AI,", "tokens": [50364, 440, 3480, 307, 257, 3761, 365, 286, 45106, 318, 1720, 330, 2757, 11, 598, 12, 33348, 293, 9588, 12662, 295, 7238, 7318, 11, 50632], "temperature": 0.0, "avg_logprob": -0.13198044594753994, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0215502567589283}, {"id": 1, "seek": 0, "start": 6.08, "end": 12.8, "text": " one of the most cited computer scientists in history with over 165,000 citations,", "tokens": [50668, 472, 295, 264, 881, 30134, 3820, 7708, 294, 2503, 365, 670, 3165, 20, 11, 1360, 4814, 763, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13198044594753994, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0215502567589283}, {"id": 2, "seek": 0, "start": 13.44, "end": 19.04, "text": " and to me, one of the most brilliant and insightful minds ever in the field of deep learning.", "tokens": [51036, 293, 281, 385, 11, 472, 295, 264, 881, 10248, 293, 46401, 9634, 1562, 294, 264, 2519, 295, 2452, 2539, 13, 51316], "temperature": 0.0, "avg_logprob": -0.13198044594753994, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0215502567589283}, {"id": 3, "seek": 0, "start": 19.92, "end": 24.240000000000002, "text": " There are very few people in this world who I would rather talk to and brainstorm with about", "tokens": [51360, 821, 366, 588, 1326, 561, 294, 341, 1002, 567, 286, 576, 2831, 751, 281, 293, 35245, 365, 466, 51576], "temperature": 0.0, "avg_logprob": -0.13198044594753994, "compression_ratio": 1.5512820512820513, "no_speech_prob": 0.0215502567589283}, {"id": 4, "seek": 2424, "start": 24.24, "end": 31.919999999999998, "text": " deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor", "tokens": [50364, 2452, 2539, 11, 7599, 11, 293, 993, 294, 2674, 813, 286, 45106, 11, 322, 293, 766, 264, 3123, 13, 639, 390, 364, 5968, 50748], "temperature": 0.0, "avg_logprob": -0.08653775545266959, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.28391218185424805}, {"id": 5, "seek": 2424, "start": 31.919999999999998, "end": 37.12, "text": " and a pleasure. This conversation was recorded before the outbreak of the pandemic,", "tokens": [50748, 293, 257, 6834, 13, 639, 3761, 390, 8287, 949, 264, 20963, 295, 264, 5388, 11, 51008], "temperature": 0.0, "avg_logprob": -0.08653775545266959, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.28391218185424805}, {"id": 6, "seek": 2424, "start": 37.12, "end": 41.36, "text": " for everyone feeling the medical, psychological, and financial burden of this crisis,", "tokens": [51008, 337, 1518, 2633, 264, 4625, 11, 14346, 11, 293, 4669, 12578, 295, 341, 5869, 11, 51220], "temperature": 0.0, "avg_logprob": -0.08653775545266959, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.28391218185424805}, {"id": 7, "seek": 2424, "start": 41.36, "end": 46.16, "text": " I'm sending love your way. Stay strong, we're in this together, we'll beat this thing.", "tokens": [51220, 286, 478, 7750, 959, 428, 636, 13, 8691, 2068, 11, 321, 434, 294, 341, 1214, 11, 321, 603, 4224, 341, 551, 13, 51460], "temperature": 0.0, "avg_logprob": -0.08653775545266959, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.28391218185424805}, {"id": 8, "seek": 2424, "start": 47.12, "end": 51.68, "text": " This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,", "tokens": [51508, 639, 307, 264, 5735, 10371, 27274, 29972, 13, 759, 291, 2103, 309, 11, 3022, 322, 3088, 11, 51736], "temperature": 0.0, "avg_logprob": -0.08653775545266959, "compression_ratio": 1.6343283582089552, "no_speech_prob": 0.28391218185424805}, {"id": 9, "seek": 5168, "start": 51.68, "end": 56.24, "text": " review it with Five Stars and Apple Podcasts, support on Patreon, or simply connect with me on", "tokens": [50364, 3131, 309, 365, 9436, 20957, 293, 6373, 29972, 82, 11, 1406, 322, 15692, 11, 420, 2935, 1745, 365, 385, 322, 50592], "temperature": 0.0, "avg_logprob": -0.14291836233700023, "compression_ratio": 1.5573248407643312, "no_speech_prob": 0.017164628952741623}, {"id": 10, "seek": 5168, "start": 56.24, "end": 62.88, "text": " Twitter at Lex Freedman's belt F-R-I-D-M-A-N. As usual, I'll do a few minutes of ads now", "tokens": [50592, 5794, 412, 24086, 6142, 292, 1601, 311, 10750, 479, 12, 49, 12, 40, 12, 35, 12, 44, 12, 32, 12, 45, 13, 1018, 7713, 11, 286, 603, 360, 257, 1326, 2077, 295, 10342, 586, 50924], "temperature": 0.0, "avg_logprob": -0.14291836233700023, "compression_ratio": 1.5573248407643312, "no_speech_prob": 0.017164628952741623}, {"id": 11, "seek": 5168, "start": 62.88, "end": 66.48, "text": " and never any ads in the middle that can break the flow of the conversation.", "tokens": [50924, 293, 1128, 604, 10342, 294, 264, 2808, 300, 393, 1821, 264, 3095, 295, 264, 3761, 13, 51104], "temperature": 0.0, "avg_logprob": -0.14291836233700023, "compression_ratio": 1.5573248407643312, "no_speech_prob": 0.017164628952741623}, {"id": 12, "seek": 5168, "start": 66.48, "end": 69.84, "text": " I hope that works for you and doesn't hurt the listening experience.", "tokens": [51104, 286, 1454, 300, 1985, 337, 291, 293, 1177, 380, 4607, 264, 4764, 1752, 13, 51272], "temperature": 0.0, "avg_logprob": -0.14291836233700023, "compression_ratio": 1.5573248407643312, "no_speech_prob": 0.017164628952741623}, {"id": 13, "seek": 5168, "start": 70.88, "end": 75.6, "text": " This show is presented by Cash App, the number one finance app in the App Store.", "tokens": [51324, 639, 855, 307, 8212, 538, 27016, 3132, 11, 264, 1230, 472, 10719, 724, 294, 264, 3132, 17242, 13, 51560], "temperature": 0.0, "avg_logprob": -0.14291836233700023, "compression_ratio": 1.5573248407643312, "no_speech_prob": 0.017164628952741623}, {"id": 14, "seek": 5168, "start": 75.6, "end": 80.88, "text": " When you get it, use code LexPodcast. Cash App lets you send money to friends,", "tokens": [51560, 1133, 291, 483, 309, 11, 764, 3089, 24086, 40742, 3734, 13, 27016, 3132, 6653, 291, 2845, 1460, 281, 1855, 11, 51824], "temperature": 0.0, "avg_logprob": -0.14291836233700023, "compression_ratio": 1.5573248407643312, "no_speech_prob": 0.017164628952741623}, {"id": 15, "seek": 8088, "start": 80.88, "end": 86.72, "text": " buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you to buy", "tokens": [50364, 2256, 11414, 11, 1963, 294, 264, 4127, 2142, 365, 382, 707, 382, 1848, 16, 13, 4162, 27016, 3132, 4045, 291, 281, 2256, 50656], "temperature": 0.0, "avg_logprob": -0.10692410285656269, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.0025504613295197487}, {"id": 16, "seek": 8088, "start": 86.72, "end": 92.39999999999999, "text": " Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating.", "tokens": [50656, 11414, 11, 718, 385, 2152, 300, 28809, 294, 264, 4319, 295, 264, 2503, 295, 1460, 307, 10343, 13, 50940], "temperature": 0.0, "avg_logprob": -0.10692410285656269, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.0025504613295197487}, {"id": 17, "seek": 8088, "start": 92.96, "end": 96.08, "text": " I recommend Ascent of Money as a great book on this history.", "tokens": [50968, 286, 2748, 1018, 2207, 295, 16631, 382, 257, 869, 1446, 322, 341, 2503, 13, 51124], "temperature": 0.0, "avg_logprob": -0.10692410285656269, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.0025504613295197487}, {"id": 18, "seek": 8088, "start": 96.72, "end": 102.0, "text": " Both the book and audiobook are great. Debits and credits on ledgers started around", "tokens": [51156, 6767, 264, 1446, 293, 40031, 366, 869, 13, 27347, 1208, 293, 16816, 322, 4684, 9458, 1409, 926, 51420], "temperature": 0.0, "avg_logprob": -0.10692410285656269, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.0025504613295197487}, {"id": 19, "seek": 8088, "start": 102.0, "end": 108.88, "text": " 30,000 years ago. The US Dollar created over 200 years ago, and Bitcoin, the first decentralized", "tokens": [51420, 2217, 11, 1360, 924, 2057, 13, 440, 2546, 32370, 2942, 670, 2331, 924, 2057, 11, 293, 11414, 11, 264, 700, 32870, 51764], "temperature": 0.0, "avg_logprob": -0.10692410285656269, "compression_ratio": 1.599264705882353, "no_speech_prob": 0.0025504613295197487}, {"id": 20, "seek": 10888, "start": 108.88, "end": 114.88, "text": " cryptocurrency released just over 10 years ago. Given that history, cryptocurrency is still very", "tokens": [50364, 28809, 4736, 445, 670, 1266, 924, 2057, 13, 18600, 300, 2503, 11, 28809, 307, 920, 588, 50664], "temperature": 0.0, "avg_logprob": -0.1250387212281586, "compression_ratio": 1.556, "no_speech_prob": 0.009266156703233719}, {"id": 21, "seek": 10888, "start": 114.88, "end": 120.72, "text": " much in its early days of development, but is still aiming to and just might redefine the nature of", "tokens": [50664, 709, 294, 1080, 2440, 1708, 295, 3250, 11, 457, 307, 920, 20253, 281, 293, 445, 1062, 38818, 533, 264, 3687, 295, 50956], "temperature": 0.0, "avg_logprob": -0.1250387212281586, "compression_ratio": 1.556, "no_speech_prob": 0.009266156703233719}, {"id": 22, "seek": 10888, "start": 120.72, "end": 127.44, "text": " money. Again, if you get Cash App from the App Store or Google Play and use the code LexPodcast,", "tokens": [50956, 1460, 13, 3764, 11, 498, 291, 483, 27016, 3132, 490, 264, 3132, 17242, 420, 3329, 5506, 293, 764, 264, 3089, 24086, 40742, 3734, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1250387212281586, "compression_ratio": 1.556, "no_speech_prob": 0.009266156703233719}, {"id": 23, "seek": 10888, "start": 128.0, "end": 134.16, "text": " you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping advance", "tokens": [51320, 291, 483, 1848, 3279, 293, 27016, 3132, 486, 611, 17751, 1848, 3279, 281, 41538, 6840, 11, 364, 4475, 300, 307, 4315, 7295, 51628], "temperature": 0.0, "avg_logprob": -0.1250387212281586, "compression_ratio": 1.556, "no_speech_prob": 0.009266156703233719}, {"id": 24, "seek": 13416, "start": 134.16, "end": 140.56, "text": " robotics and STEM education for young people around the world. And now, here's my conversation", "tokens": [50364, 34145, 293, 25043, 3309, 337, 2037, 561, 926, 264, 1002, 13, 400, 586, 11, 510, 311, 452, 3761, 50684], "temperature": 0.0, "avg_logprob": -0.1747055455258018, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.12074924260377884}, {"id": 25, "seek": 13416, "start": 140.56, "end": 147.68, "text": " with Ilya Satskeva. You were one of the three authors with Alex Koshchevsky, Jeff Hinton,", "tokens": [50684, 365, 286, 45106, 318, 1720, 330, 2757, 13, 509, 645, 472, 295, 264, 1045, 16552, 365, 5202, 591, 3019, 1876, 85, 25810, 11, 7506, 389, 12442, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1747055455258018, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.12074924260377884}, {"id": 26, "seek": 13416, "start": 147.68, "end": 155.2, "text": " of the famed AlexNet paper that is arguably the paper that marked the big catalytic moment that", "tokens": [51040, 295, 264, 1087, 292, 5202, 31890, 3035, 300, 307, 26771, 264, 3035, 300, 12658, 264, 955, 13192, 43658, 1623, 300, 51416], "temperature": 0.0, "avg_logprob": -0.1747055455258018, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.12074924260377884}, {"id": 27, "seek": 13416, "start": 155.2, "end": 159.92, "text": " launched the deep learning revolution. At that time, take us back to that time. What was your", "tokens": [51416, 8730, 264, 2452, 2539, 8894, 13, 1711, 300, 565, 11, 747, 505, 646, 281, 300, 565, 13, 708, 390, 428, 51652], "temperature": 0.0, "avg_logprob": -0.1747055455258018, "compression_ratio": 1.526530612244898, "no_speech_prob": 0.12074924260377884}, {"id": 28, "seek": 15992, "start": 159.92, "end": 164.88, "text": " intuition about neural networks, about the representation of power of neural networks?", "tokens": [50364, 24002, 466, 18161, 9590, 11, 466, 264, 10290, 295, 1347, 295, 18161, 9590, 30, 50612], "temperature": 0.0, "avg_logprob": -0.10927963256835938, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.09896181523799896}, {"id": 29, "seek": 15992, "start": 165.92, "end": 172.0, "text": " And maybe you could mention how did that evolve over the next few years, up to today, over the", "tokens": [50664, 400, 1310, 291, 727, 2152, 577, 630, 300, 16693, 670, 264, 958, 1326, 924, 11, 493, 281, 965, 11, 670, 264, 50968], "temperature": 0.0, "avg_logprob": -0.10927963256835938, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.09896181523799896}, {"id": 30, "seek": 15992, "start": 172.0, "end": 178.23999999999998, "text": " 10 years? Yeah, I can answer that question. At some point in about 2010 or 2011,", "tokens": [50968, 1266, 924, 30, 865, 11, 286, 393, 1867, 300, 1168, 13, 1711, 512, 935, 294, 466, 9657, 420, 10154, 11, 51280], "temperature": 0.0, "avg_logprob": -0.10927963256835938, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.09896181523799896}, {"id": 31, "seek": 15992, "start": 179.92, "end": 188.07999999999998, "text": " I connected two facts in my mind. Basically, the realization was this. At some point,", "tokens": [51364, 286, 4582, 732, 9130, 294, 452, 1575, 13, 8537, 11, 264, 25138, 390, 341, 13, 1711, 512, 935, 11, 51772], "temperature": 0.0, "avg_logprob": -0.10927963256835938, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.09896181523799896}, {"id": 32, "seek": 18808, "start": 188.08, "end": 193.52, "text": " we realized that we can train very large, I shouldn't say very tiny by today's standards, but", "tokens": [50364, 321, 5334, 300, 321, 393, 3847, 588, 2416, 11, 286, 4659, 380, 584, 588, 5870, 538, 965, 311, 7787, 11, 457, 50636], "temperature": 0.0, "avg_logprob": -0.1363898587514119, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0572669543325901}, {"id": 33, "seek": 18808, "start": 194.32000000000002, "end": 199.20000000000002, "text": " large and deep neural networks end to end with back propagation. At some point,", "tokens": [50676, 2416, 293, 2452, 18161, 9590, 917, 281, 917, 365, 646, 38377, 13, 1711, 512, 935, 11, 50920], "temperature": 0.0, "avg_logprob": -0.1363898587514119, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0572669543325901}, {"id": 34, "seek": 18808, "start": 200.64000000000001, "end": 206.32000000000002, "text": " different people obtained this result. I obtained this result. The first moment in which I realized", "tokens": [50992, 819, 561, 14879, 341, 1874, 13, 286, 14879, 341, 1874, 13, 440, 700, 1623, 294, 597, 286, 5334, 51276], "temperature": 0.0, "avg_logprob": -0.1363898587514119, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0572669543325901}, {"id": 35, "seek": 18808, "start": 206.32000000000002, "end": 212.16000000000003, "text": " that deep neural networks are powerful was when James Martens invented the Hessian Free Optimizer", "tokens": [51276, 300, 2452, 18161, 9590, 366, 4005, 390, 562, 5678, 5807, 694, 14479, 264, 35960, 952, 11551, 35013, 6545, 51568], "temperature": 0.0, "avg_logprob": -0.1363898587514119, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0572669543325901}, {"id": 36, "seek": 21216, "start": 212.16, "end": 218.07999999999998, "text": " in 2010. And he trained a 10 layer neural network end to end without pre-training", "tokens": [50364, 294, 9657, 13, 400, 415, 8895, 257, 1266, 4583, 18161, 3209, 917, 281, 917, 1553, 659, 12, 17227, 1760, 50660], "temperature": 0.0, "avg_logprob": -0.11121816949529963, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.019705582410097122}, {"id": 37, "seek": 21216, "start": 219.6, "end": 225.12, "text": " from scratch. And when that happened, I thought this is it. Because if you can train a big neural", "tokens": [50736, 490, 8459, 13, 400, 562, 300, 2011, 11, 286, 1194, 341, 307, 309, 13, 1436, 498, 291, 393, 3847, 257, 955, 18161, 51012], "temperature": 0.0, "avg_logprob": -0.11121816949529963, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.019705582410097122}, {"id": 38, "seek": 21216, "start": 225.12, "end": 230.16, "text": " network, a big neural network can represent very complicated function. Because if you have a neural", "tokens": [51012, 3209, 11, 257, 955, 18161, 3209, 393, 2906, 588, 6179, 2445, 13, 1436, 498, 291, 362, 257, 18161, 51264], "temperature": 0.0, "avg_logprob": -0.11121816949529963, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.019705582410097122}, {"id": 39, "seek": 21216, "start": 230.16, "end": 238.24, "text": " network with 10 layers, it's as though you allow the human brain to run for some number of milliseconds,", "tokens": [51264, 3209, 365, 1266, 7914, 11, 309, 311, 382, 1673, 291, 2089, 264, 1952, 3567, 281, 1190, 337, 512, 1230, 295, 34184, 11, 51668], "temperature": 0.0, "avg_logprob": -0.11121816949529963, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.019705582410097122}, {"id": 40, "seek": 23824, "start": 238.32000000000002, "end": 244.64000000000001, "text": " neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times.", "tokens": [50368, 34090, 12159, 1109, 366, 2964, 13, 400, 370, 294, 1310, 2319, 34184, 11, 428, 22027, 787, 2610, 1266, 1413, 13, 50684], "temperature": 0.0, "avg_logprob": -0.07745296913280822, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002017747610807419}, {"id": 41, "seek": 23824, "start": 244.64000000000001, "end": 249.44, "text": " So it's also kind of like 10 layers. And in 100 milliseconds, you can perfectly recognize any", "tokens": [50684, 407, 309, 311, 611, 733, 295, 411, 1266, 7914, 13, 400, 294, 2319, 34184, 11, 291, 393, 6239, 5521, 604, 50924], "temperature": 0.0, "avg_logprob": -0.07745296913280822, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002017747610807419}, {"id": 42, "seek": 23824, "start": 249.44, "end": 254.8, "text": " object. So I thought, so I already had the idea then that we need to train a very big neural network", "tokens": [50924, 2657, 13, 407, 286, 1194, 11, 370, 286, 1217, 632, 264, 1558, 550, 300, 321, 643, 281, 3847, 257, 588, 955, 18161, 3209, 51192], "temperature": 0.0, "avg_logprob": -0.07745296913280822, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002017747610807419}, {"id": 43, "seek": 23824, "start": 256.08, "end": 260.56, "text": " on lots of supervised data. And then it must succeed, because we can find the best neural", "tokens": [51256, 322, 3195, 295, 46533, 1412, 13, 400, 550, 309, 1633, 7754, 11, 570, 321, 393, 915, 264, 1151, 18161, 51480], "temperature": 0.0, "avg_logprob": -0.07745296913280822, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002017747610807419}, {"id": 44, "seek": 23824, "start": 260.56, "end": 264.8, "text": " network. And then there's also theory that if you have more data than parameters, you won't", "tokens": [51480, 3209, 13, 400, 550, 456, 311, 611, 5261, 300, 498, 291, 362, 544, 1412, 813, 9834, 11, 291, 1582, 380, 51692], "temperature": 0.0, "avg_logprob": -0.07745296913280822, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.002017747610807419}, {"id": 45, "seek": 26480, "start": 264.8, "end": 268.88, "text": " overfit. Today, we know that actually, this theory is very incomplete, and you won't overfit even", "tokens": [50364, 670, 6845, 13, 2692, 11, 321, 458, 300, 767, 11, 341, 5261, 307, 588, 31709, 11, 293, 291, 1582, 380, 670, 6845, 754, 50568], "temperature": 0.0, "avg_logprob": -0.12767782603224662, "compression_ratio": 2.017301038062284, "no_speech_prob": 0.012428098358213902}, {"id": 46, "seek": 26480, "start": 268.88, "end": 272.64, "text": " you have less data than parameters. But definitely, if you have more data than parameters, you won't", "tokens": [50568, 291, 362, 1570, 1412, 813, 9834, 13, 583, 2138, 11, 498, 291, 362, 544, 1412, 813, 9834, 11, 291, 1582, 380, 50756], "temperature": 0.0, "avg_logprob": -0.12767782603224662, "compression_ratio": 2.017301038062284, "no_speech_prob": 0.012428098358213902}, {"id": 47, "seek": 26480, "start": 272.64, "end": 278.64, "text": " overfit. So the fact that neural networks were heavily overparameterized, wasn't discouraging to", "tokens": [50756, 670, 6845, 13, 407, 264, 1186, 300, 18161, 9590, 645, 10950, 670, 2181, 335, 2398, 1602, 11, 2067, 380, 21497, 3568, 281, 51056], "temperature": 0.0, "avg_logprob": -0.12767782603224662, "compression_ratio": 2.017301038062284, "no_speech_prob": 0.012428098358213902}, {"id": 48, "seek": 26480, "start": 278.64, "end": 283.92, "text": " you. So you were thinking about the theory that the number of parameters, the fact there's a huge", "tokens": [51056, 291, 13, 407, 291, 645, 1953, 466, 264, 5261, 300, 264, 1230, 295, 9834, 11, 264, 1186, 456, 311, 257, 2603, 51320], "temperature": 0.0, "avg_logprob": -0.12767782603224662, "compression_ratio": 2.017301038062284, "no_speech_prob": 0.012428098358213902}, {"id": 49, "seek": 26480, "start": 283.92, "end": 287.36, "text": " number of parameters is okay, it's going to be okay. I mean, there was some evidence before that", "tokens": [51320, 1230, 295, 9834, 307, 1392, 11, 309, 311, 516, 281, 312, 1392, 13, 286, 914, 11, 456, 390, 512, 4467, 949, 300, 51492], "temperature": 0.0, "avg_logprob": -0.12767782603224662, "compression_ratio": 2.017301038062284, "no_speech_prob": 0.012428098358213902}, {"id": 50, "seek": 26480, "start": 287.36, "end": 291.68, "text": " it was okay, but the theory was most the theory was that if you had a big data set and a big", "tokens": [51492, 309, 390, 1392, 11, 457, 264, 5261, 390, 881, 264, 5261, 390, 300, 498, 291, 632, 257, 955, 1412, 992, 293, 257, 955, 51708], "temperature": 0.0, "avg_logprob": -0.12767782603224662, "compression_ratio": 2.017301038062284, "no_speech_prob": 0.012428098358213902}, {"id": 51, "seek": 29168, "start": 291.68, "end": 296.64, "text": " neural net, it was going to work. The overparameterization just didn't really figure much as a", "tokens": [50364, 18161, 2533, 11, 309, 390, 516, 281, 589, 13, 440, 670, 2181, 335, 2398, 2144, 445, 994, 380, 534, 2573, 709, 382, 257, 50612], "temperature": 0.0, "avg_logprob": -0.16825612833802128, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.004980678204447031}, {"id": 52, "seek": 29168, "start": 296.64, "end": 299.44, "text": " problem. I thought, well, with images, you just go and add some data augmentation, and it's going", "tokens": [50612, 1154, 13, 286, 1194, 11, 731, 11, 365, 5267, 11, 291, 445, 352, 293, 909, 512, 1412, 14501, 19631, 11, 293, 309, 311, 516, 50752], "temperature": 0.0, "avg_logprob": -0.16825612833802128, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.004980678204447031}, {"id": 53, "seek": 29168, "start": 299.44, "end": 304.56, "text": " to be okay. So where was any doubt coming from? The main doubt was can we train a bigger, really", "tokens": [50752, 281, 312, 1392, 13, 407, 689, 390, 604, 6385, 1348, 490, 30, 440, 2135, 6385, 390, 393, 321, 3847, 257, 3801, 11, 534, 51008], "temperature": 0.0, "avg_logprob": -0.16825612833802128, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.004980678204447031}, {"id": 54, "seek": 29168, "start": 304.56, "end": 308.16, "text": " have enough compute to train a big enough neural net with back propagation, back propagation,", "tokens": [51008, 362, 1547, 14722, 281, 3847, 257, 955, 1547, 18161, 2533, 365, 646, 38377, 11, 646, 38377, 11, 51188], "temperature": 0.0, "avg_logprob": -0.16825612833802128, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.004980678204447031}, {"id": 55, "seek": 29168, "start": 308.16, "end": 312.32, "text": " I thought was would work. This image wasn't clear would was whether there would be enough compute", "tokens": [51188, 286, 1194, 390, 576, 589, 13, 639, 3256, 2067, 380, 1850, 576, 390, 1968, 456, 576, 312, 1547, 14722, 51396], "temperature": 0.0, "avg_logprob": -0.16825612833802128, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.004980678204447031}, {"id": 56, "seek": 29168, "start": 312.32, "end": 316.72, "text": " to get a very convincing result. And then at some point, Alex Krzewski wrote these insanely fast", "tokens": [51396, 281, 483, 257, 588, 24823, 1874, 13, 400, 550, 412, 512, 935, 11, 5202, 6332, 89, 1023, 18020, 4114, 613, 40965, 2370, 51616], "temperature": 0.0, "avg_logprob": -0.16825612833802128, "compression_ratio": 1.811912225705329, "no_speech_prob": 0.004980678204447031}, {"id": 57, "seek": 31672, "start": 316.8, "end": 321.12, "text": " CUDA kernels for training convolutional neural nets. Net was bam, let's do this. Let's get", "tokens": [50368, 29777, 7509, 23434, 1625, 337, 3097, 45216, 304, 18161, 36170, 13, 6188, 390, 18132, 11, 718, 311, 360, 341, 13, 961, 311, 483, 50584], "temperature": 0.0, "avg_logprob": -0.15597518714698586, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.03786281868815422}, {"id": 58, "seek": 31672, "start": 321.12, "end": 326.16, "text": " image net and it's going to be the greatest thing. Was your intuition, most of your intuition from", "tokens": [50584, 3256, 2533, 293, 309, 311, 516, 281, 312, 264, 6636, 551, 13, 3027, 428, 24002, 11, 881, 295, 428, 24002, 490, 50836], "temperature": 0.0, "avg_logprob": -0.15597518714698586, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.03786281868815422}, {"id": 59, "seek": 31672, "start": 326.16, "end": 332.56, "text": " empirical results by you and by others? So like just actually demonstrating that a piece of program", "tokens": [50836, 31886, 3542, 538, 291, 293, 538, 2357, 30, 407, 411, 445, 767, 29889, 300, 257, 2522, 295, 1461, 51156], "temperature": 0.0, "avg_logprob": -0.15597518714698586, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.03786281868815422}, {"id": 60, "seek": 31672, "start": 332.56, "end": 339.20000000000005, "text": " can train a 10 layer neural network? Or was there some pen and paper or marker and white board", "tokens": [51156, 393, 3847, 257, 1266, 4583, 18161, 3209, 30, 1610, 390, 456, 512, 3435, 293, 3035, 420, 15247, 293, 2418, 3150, 51488], "temperature": 0.0, "avg_logprob": -0.15597518714698586, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.03786281868815422}, {"id": 61, "seek": 31672, "start": 339.20000000000005, "end": 345.36, "text": " thinking intuition? Because you just connected a 10 layer large neural network to the brain.", "tokens": [51488, 1953, 24002, 30, 1436, 291, 445, 4582, 257, 1266, 4583, 2416, 18161, 3209, 281, 264, 3567, 13, 51796], "temperature": 0.0, "avg_logprob": -0.15597518714698586, "compression_ratio": 1.685512367491166, "no_speech_prob": 0.03786281868815422}, {"id": 62, "seek": 34536, "start": 345.36, "end": 349.76, "text": " So you just mentioned the brain. So in your intuition about neural networks, does the human", "tokens": [50364, 407, 291, 445, 2835, 264, 3567, 13, 407, 294, 428, 24002, 466, 18161, 9590, 11, 775, 264, 1952, 50584], "temperature": 0.0, "avg_logprob": -0.09893183935256232, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.0023203014861792326}, {"id": 63, "seek": 34536, "start": 349.76, "end": 356.08000000000004, "text": " brain come into play as a intuition builder? Definitely. I mean, you know, you got to be", "tokens": [50584, 3567, 808, 666, 862, 382, 257, 24002, 27377, 30, 12151, 13, 286, 914, 11, 291, 458, 11, 291, 658, 281, 312, 50900], "temperature": 0.0, "avg_logprob": -0.09893183935256232, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.0023203014861792326}, {"id": 64, "seek": 34536, "start": 356.08000000000004, "end": 360.48, "text": " precise with these analogies between neural artificial neural networks in the brain. But", "tokens": [50900, 13600, 365, 613, 16660, 530, 1296, 18161, 11677, 18161, 9590, 294, 264, 3567, 13, 583, 51120], "temperature": 0.0, "avg_logprob": -0.09893183935256232, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.0023203014861792326}, {"id": 65, "seek": 34536, "start": 361.12, "end": 366.48, "text": " there's no question that the brain is a huge source of intuition and inspiration for deep", "tokens": [51152, 456, 311, 572, 1168, 300, 264, 3567, 307, 257, 2603, 4009, 295, 24002, 293, 10249, 337, 2452, 51420], "temperature": 0.0, "avg_logprob": -0.09893183935256232, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.0023203014861792326}, {"id": 66, "seek": 34536, "start": 366.48, "end": 372.32, "text": " learning researchers since all the way from Rosenblatt in the 60s. Like, if you look at the", "tokens": [51420, 2539, 10309, 1670, 439, 264, 636, 490, 33630, 5199, 1591, 294, 264, 4060, 82, 13, 1743, 11, 498, 291, 574, 412, 264, 51712], "temperature": 0.0, "avg_logprob": -0.09893183935256232, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.0023203014861792326}, {"id": 67, "seek": 37232, "start": 372.32, "end": 376.64, "text": " whole idea of a neural network is directly inspired by the brain. You had people like", "tokens": [50364, 1379, 1558, 295, 257, 18161, 3209, 307, 3838, 7547, 538, 264, 3567, 13, 509, 632, 561, 411, 50580], "temperature": 0.0, "avg_logprob": -0.1278273547137225, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.000882425403688103}, {"id": 68, "seek": 37232, "start": 376.64, "end": 382.8, "text": " McCallum and Pitts who were saying, hey, you got these neurons in the brain. And hey, we recently", "tokens": [50580, 12061, 336, 449, 293, 29478, 567, 645, 1566, 11, 4177, 11, 291, 658, 613, 22027, 294, 264, 3567, 13, 400, 4177, 11, 321, 3938, 50888], "temperature": 0.0, "avg_logprob": -0.1278273547137225, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.000882425403688103}, {"id": 69, "seek": 37232, "start": 382.8, "end": 386.48, "text": " learned about the computer and automata. Can we use some ideas from the computer and automata to", "tokens": [50888, 3264, 466, 264, 3820, 293, 3553, 3274, 13, 1664, 321, 764, 512, 3487, 490, 264, 3820, 293, 3553, 3274, 281, 51072], "temperature": 0.0, "avg_logprob": -0.1278273547137225, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.000882425403688103}, {"id": 70, "seek": 37232, "start": 386.48, "end": 391.92, "text": " design some kind of computational object that's going to be simple, computational and kind of", "tokens": [51072, 1715, 512, 733, 295, 28270, 2657, 300, 311, 516, 281, 312, 2199, 11, 28270, 293, 733, 295, 51344], "temperature": 0.0, "avg_logprob": -0.1278273547137225, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.000882425403688103}, {"id": 71, "seek": 37232, "start": 391.92, "end": 396.32, "text": " like the brain and invented the neuron. So they were inspired by it back then. Then you had the", "tokens": [51344, 411, 264, 3567, 293, 14479, 264, 34090, 13, 407, 436, 645, 7547, 538, 309, 646, 550, 13, 1396, 291, 632, 264, 51564], "temperature": 0.0, "avg_logprob": -0.1278273547137225, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.000882425403688103}, {"id": 72, "seek": 37232, "start": 396.32, "end": 400.96, "text": " convolutional neural network from Fukushima. And then later young Lacan, who said, Hey, if you", "tokens": [51564, 45216, 304, 18161, 3209, 490, 33043, 49754, 13, 400, 550, 1780, 2037, 40113, 282, 11, 567, 848, 11, 1911, 11, 498, 291, 51796], "temperature": 0.0, "avg_logprob": -0.1278273547137225, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.000882425403688103}, {"id": 73, "seek": 40096, "start": 400.96, "end": 405.28, "text": " limit the receptive fields of a neural network, it's going to be especially suitable for images,", "tokens": [50364, 4948, 264, 45838, 7909, 295, 257, 18161, 3209, 11, 309, 311, 516, 281, 312, 2318, 12873, 337, 5267, 11, 50580], "temperature": 0.0, "avg_logprob": -0.11372238794962565, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.0009545929497107863}, {"id": 74, "seek": 40096, "start": 405.28, "end": 411.03999999999996, "text": " as it turned out to be true. So there was a very small number of examples where analogies to the", "tokens": [50580, 382, 309, 3574, 484, 281, 312, 2074, 13, 407, 456, 390, 257, 588, 1359, 1230, 295, 5110, 689, 16660, 530, 281, 264, 50868], "temperature": 0.0, "avg_logprob": -0.11372238794962565, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.0009545929497107863}, {"id": 75, "seek": 40096, "start": 411.03999999999996, "end": 416.24, "text": " brain were successful. And I thought, well, probably an artificial neuron is not that different from", "tokens": [50868, 3567, 645, 4406, 13, 400, 286, 1194, 11, 731, 11, 1391, 364, 11677, 34090, 307, 406, 300, 819, 490, 51128], "temperature": 0.0, "avg_logprob": -0.11372238794962565, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.0009545929497107863}, {"id": 76, "seek": 40096, "start": 416.24, "end": 421.59999999999997, "text": " the brain if it's cleaned hard enough. So let's just assume it is and roll with it. So we're now", "tokens": [51128, 264, 3567, 498, 309, 311, 16146, 1152, 1547, 13, 407, 718, 311, 445, 6552, 309, 307, 293, 3373, 365, 309, 13, 407, 321, 434, 586, 51396], "temperature": 0.0, "avg_logprob": -0.11372238794962565, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.0009545929497107863}, {"id": 77, "seek": 40096, "start": 421.59999999999997, "end": 428.47999999999996, "text": " at a time where deep learning is very successful. So let us squint less and say, let's open our", "tokens": [51396, 412, 257, 565, 689, 2452, 2539, 307, 588, 4406, 13, 407, 718, 505, 2339, 686, 1570, 293, 584, 11, 718, 311, 1269, 527, 51740], "temperature": 0.0, "avg_logprob": -0.11372238794962565, "compression_ratio": 1.6909722222222223, "no_speech_prob": 0.0009545929497107863}, {"id": 78, "seek": 42848, "start": 428.56, "end": 434.08000000000004, "text": " eyes and say, what do you use an interesting difference between the human brain? Now, I know", "tokens": [50368, 2575, 293, 584, 11, 437, 360, 291, 764, 364, 1880, 2649, 1296, 264, 1952, 3567, 30, 823, 11, 286, 458, 50644], "temperature": 0.0, "avg_logprob": -0.11915397644042969, "compression_ratio": 1.9877049180327868, "no_speech_prob": 0.004980744794011116}, {"id": 79, "seek": 42848, "start": 434.08000000000004, "end": 439.6, "text": " you're probably not an expert, neither in your scientists and your biologists, but loosely speaking,", "tokens": [50644, 291, 434, 1391, 406, 364, 5844, 11, 9662, 294, 428, 7708, 293, 428, 3228, 12256, 11, 457, 37966, 4124, 11, 50920], "temperature": 0.0, "avg_logprob": -0.11915397644042969, "compression_ratio": 1.9877049180327868, "no_speech_prob": 0.004980744794011116}, {"id": 80, "seek": 42848, "start": 439.6, "end": 443.12, "text": " what's the difference between the human brain and artificial neural networks? That's interesting", "tokens": [50920, 437, 311, 264, 2649, 1296, 264, 1952, 3567, 293, 11677, 18161, 9590, 30, 663, 311, 1880, 51096], "temperature": 0.0, "avg_logprob": -0.11915397644042969, "compression_ratio": 1.9877049180327868, "no_speech_prob": 0.004980744794011116}, {"id": 81, "seek": 42848, "start": 443.12, "end": 448.72, "text": " to you for the next decade or two. That's a good question to ask. What is an interesting difference", "tokens": [51096, 281, 291, 337, 264, 958, 10378, 420, 732, 13, 663, 311, 257, 665, 1168, 281, 1029, 13, 708, 307, 364, 1880, 2649, 51376], "temperature": 0.0, "avg_logprob": -0.11915397644042969, "compression_ratio": 1.9877049180327868, "no_speech_prob": 0.004980744794011116}, {"id": 82, "seek": 42848, "start": 448.72, "end": 454.0, "text": " between the neural between the brain and our artificial neural networks? So I feel like today,", "tokens": [51376, 1296, 264, 18161, 1296, 264, 3567, 293, 527, 11677, 18161, 9590, 30, 407, 286, 841, 411, 965, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11915397644042969, "compression_ratio": 1.9877049180327868, "no_speech_prob": 0.004980744794011116}, {"id": 83, "seek": 45400, "start": 454.88, "end": 460.0, "text": " artificial neural networks, so we all agree that there are certain dimensions in which the human", "tokens": [50408, 11677, 18161, 9590, 11, 370, 321, 439, 3986, 300, 456, 366, 1629, 12819, 294, 597, 264, 1952, 50664], "temperature": 0.0, "avg_logprob": -0.08773420441825434, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.004902475979179144}, {"id": 84, "seek": 45400, "start": 460.0, "end": 465.28, "text": " brain vastly outperforms our models. But I also think that there are some ways in which artificial", "tokens": [50664, 3567, 41426, 484, 26765, 82, 527, 5245, 13, 583, 286, 611, 519, 300, 456, 366, 512, 2098, 294, 597, 11677, 50928], "temperature": 0.0, "avg_logprob": -0.08773420441825434, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.004902475979179144}, {"id": 85, "seek": 45400, "start": 465.28, "end": 471.44, "text": " neural networks have a number of very important advantages over the brain. Looking at the advantages", "tokens": [50928, 18161, 9590, 362, 257, 1230, 295, 588, 1021, 14906, 670, 264, 3567, 13, 11053, 412, 264, 14906, 51236], "temperature": 0.0, "avg_logprob": -0.08773420441825434, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.004902475979179144}, {"id": 86, "seek": 45400, "start": 471.44, "end": 476.72, "text": " versus disadvantages is a good way to figure out what is the important difference. So the brain", "tokens": [51236, 5717, 37431, 307, 257, 665, 636, 281, 2573, 484, 437, 307, 264, 1021, 2649, 13, 407, 264, 3567, 51500], "temperature": 0.0, "avg_logprob": -0.08773420441825434, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.004902475979179144}, {"id": 87, "seek": 45400, "start": 477.36, "end": 481.76, "text": " uses spikes, which may or may not be important. Yes, that's a really interesting question. Do you", "tokens": [51532, 4960, 28997, 11, 597, 815, 420, 815, 406, 312, 1021, 13, 1079, 11, 300, 311, 257, 534, 1880, 1168, 13, 1144, 291, 51752], "temperature": 0.0, "avg_logprob": -0.08773420441825434, "compression_ratio": 1.8081180811808117, "no_speech_prob": 0.004902475979179144}, {"id": 88, "seek": 48176, "start": 481.76, "end": 487.2, "text": " think it's important or not? That's one big architectural difference between artificial", "tokens": [50364, 519, 309, 311, 1021, 420, 406, 30, 663, 311, 472, 955, 26621, 2649, 1296, 11677, 50636], "temperature": 0.0, "avg_logprob": -0.08504248366636388, "compression_ratio": 1.9383561643835616, "no_speech_prob": 0.004678601399064064}, {"id": 89, "seek": 48176, "start": 487.2, "end": 493.59999999999997, "text": " neural networks. It's hard to tell, but my prior is not very high. And I can say why. There are", "tokens": [50636, 18161, 9590, 13, 467, 311, 1152, 281, 980, 11, 457, 452, 4059, 307, 406, 588, 1090, 13, 400, 286, 393, 584, 983, 13, 821, 366, 50956], "temperature": 0.0, "avg_logprob": -0.08504248366636388, "compression_ratio": 1.9383561643835616, "no_speech_prob": 0.004678601399064064}, {"id": 90, "seek": 48176, "start": 493.59999999999997, "end": 497.92, "text": " people who are interested in spiking neural networks. And basically, what they figured out is", "tokens": [50956, 561, 567, 366, 3102, 294, 637, 13085, 18161, 9590, 13, 400, 1936, 11, 437, 436, 8932, 484, 307, 51172], "temperature": 0.0, "avg_logprob": -0.08504248366636388, "compression_ratio": 1.9383561643835616, "no_speech_prob": 0.004678601399064064}, {"id": 91, "seek": 48176, "start": 497.92, "end": 503.28, "text": " that they need to simulate the non-spiking neural networks in spikes. And that's how they're going", "tokens": [51172, 300, 436, 643, 281, 27817, 264, 2107, 12, 4952, 13085, 18161, 9590, 294, 28997, 13, 400, 300, 311, 577, 436, 434, 516, 51440], "temperature": 0.0, "avg_logprob": -0.08504248366636388, "compression_ratio": 1.9383561643835616, "no_speech_prob": 0.004678601399064064}, {"id": 92, "seek": 48176, "start": 503.28, "end": 507.2, "text": " to make them work. If you don't simulate the non-spiking neural networks in spikes, it's not", "tokens": [51440, 281, 652, 552, 589, 13, 759, 291, 500, 380, 27817, 264, 2107, 12, 4952, 13085, 18161, 9590, 294, 28997, 11, 309, 311, 406, 51636], "temperature": 0.0, "avg_logprob": -0.08504248366636388, "compression_ratio": 1.9383561643835616, "no_speech_prob": 0.004678601399064064}, {"id": 93, "seek": 48176, "start": 507.2, "end": 510.56, "text": " going to work because the question is, why should it work? And that connects to questions around", "tokens": [51636, 516, 281, 589, 570, 264, 1168, 307, 11, 983, 820, 309, 589, 30, 400, 300, 16967, 281, 1651, 926, 51804], "temperature": 0.0, "avg_logprob": -0.08504248366636388, "compression_ratio": 1.9383561643835616, "no_speech_prob": 0.004678601399064064}, {"id": 94, "seek": 51056, "start": 510.56, "end": 516.8, "text": " back propagation and questions around deep learning. You've got this giant neural network.", "tokens": [50364, 646, 38377, 293, 1651, 926, 2452, 2539, 13, 509, 600, 658, 341, 7410, 18161, 3209, 13, 50676], "temperature": 0.0, "avg_logprob": -0.07872652374537645, "compression_ratio": 1.75390625, "no_speech_prob": 0.0022156969644129276}, {"id": 95, "seek": 51056, "start": 516.8, "end": 519.84, "text": " Why should it work at all? Why should that learning rule work at all?", "tokens": [50676, 1545, 820, 309, 589, 412, 439, 30, 1545, 820, 300, 2539, 4978, 589, 412, 439, 30, 50828], "temperature": 0.0, "avg_logprob": -0.07872652374537645, "compression_ratio": 1.75390625, "no_speech_prob": 0.0022156969644129276}, {"id": 96, "seek": 51056, "start": 523.12, "end": 527.2, "text": " It's not a self-evident question, especially if you, let's say, if you were just starting in the", "tokens": [50992, 467, 311, 406, 257, 2698, 12, 13379, 1078, 1168, 11, 2318, 498, 291, 11, 718, 311, 584, 11, 498, 291, 645, 445, 2891, 294, 264, 51196], "temperature": 0.0, "avg_logprob": -0.07872652374537645, "compression_ratio": 1.75390625, "no_speech_prob": 0.0022156969644129276}, {"id": 97, "seek": 51056, "start": 527.2, "end": 532.64, "text": " field and you read the very early papers, you can say, hey, people are saying, let's build neural", "tokens": [51196, 2519, 293, 291, 1401, 264, 588, 2440, 10577, 11, 291, 393, 584, 11, 4177, 11, 561, 366, 1566, 11, 718, 311, 1322, 18161, 51468], "temperature": 0.0, "avg_logprob": -0.07872652374537645, "compression_ratio": 1.75390625, "no_speech_prob": 0.0022156969644129276}, {"id": 98, "seek": 51056, "start": 532.64, "end": 536.64, "text": " networks. That's a great idea because the brain is a neural network, so it would be useful to", "tokens": [51468, 9590, 13, 663, 311, 257, 869, 1558, 570, 264, 3567, 307, 257, 18161, 3209, 11, 370, 309, 576, 312, 4420, 281, 51668], "temperature": 0.0, "avg_logprob": -0.07872652374537645, "compression_ratio": 1.75390625, "no_speech_prob": 0.0022156969644129276}, {"id": 99, "seek": 53664, "start": 536.64, "end": 541.36, "text": " build neural networks. Now, let's figure out how to train them. It should be possible to train", "tokens": [50364, 1322, 18161, 9590, 13, 823, 11, 718, 311, 2573, 484, 577, 281, 3847, 552, 13, 467, 820, 312, 1944, 281, 3847, 50600], "temperature": 0.0, "avg_logprob": -0.1074539495974171, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.007569967769086361}, {"id": 100, "seek": 53664, "start": 541.36, "end": 549.1999999999999, "text": " them probably, but how? And so the big idea is the cost function. That's the big idea. The cost", "tokens": [50600, 552, 1391, 11, 457, 577, 30, 400, 370, 264, 955, 1558, 307, 264, 2063, 2445, 13, 663, 311, 264, 955, 1558, 13, 440, 2063, 50992], "temperature": 0.0, "avg_logprob": -0.1074539495974171, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.007569967769086361}, {"id": 101, "seek": 53664, "start": 549.1999999999999, "end": 554.72, "text": " function is a way of measuring the performance of the system according to some measure.", "tokens": [50992, 2445, 307, 257, 636, 295, 13389, 264, 3389, 295, 264, 1185, 4650, 281, 512, 3481, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1074539495974171, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.007569967769086361}, {"id": 102, "seek": 53664, "start": 554.72, "end": 561.28, "text": " By the way, that is a big, actually, let me think. Is that one, a difficult idea to arrive at? And", "tokens": [51268, 3146, 264, 636, 11, 300, 307, 257, 955, 11, 767, 11, 718, 385, 519, 13, 1119, 300, 472, 11, 257, 2252, 1558, 281, 8881, 412, 30, 400, 51596], "temperature": 0.0, "avg_logprob": -0.1074539495974171, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.007569967769086361}, {"id": 103, "seek": 56128, "start": 561.28, "end": 569.04, "text": " how big of an idea is that? That there's a single cost function? Sorry, let me take a pause. Is", "tokens": [50364, 577, 955, 295, 364, 1558, 307, 300, 30, 663, 456, 311, 257, 2167, 2063, 2445, 30, 4919, 11, 718, 385, 747, 257, 10465, 13, 1119, 50752], "temperature": 0.0, "avg_logprob": -0.09666432166586117, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.019413547590374947}, {"id": 104, "seek": 56128, "start": 569.04, "end": 575.76, "text": " supervised learning a difficult concept to come to? I don't know. All concepts are very easy in", "tokens": [50752, 46533, 2539, 257, 2252, 3410, 281, 808, 281, 30, 286, 500, 380, 458, 13, 1057, 10392, 366, 588, 1858, 294, 51088], "temperature": 0.0, "avg_logprob": -0.09666432166586117, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.019413547590374947}, {"id": 105, "seek": 56128, "start": 575.76, "end": 580.72, "text": " retrospect. Yeah, that's what it seems trivial now. Because the reason I asked that, and we'll", "tokens": [51088, 34997, 13, 865, 11, 300, 311, 437, 309, 2544, 26703, 586, 13, 1436, 264, 1778, 286, 2351, 300, 11, 293, 321, 603, 51336], "temperature": 0.0, "avg_logprob": -0.09666432166586117, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.019413547590374947}, {"id": 106, "seek": 56128, "start": 580.72, "end": 587.04, "text": " talk about it, is there other things? Is there things that don't necessarily have a cost function,", "tokens": [51336, 751, 466, 309, 11, 307, 456, 661, 721, 30, 1119, 456, 721, 300, 500, 380, 4725, 362, 257, 2063, 2445, 11, 51652], "temperature": 0.0, "avg_logprob": -0.09666432166586117, "compression_ratio": 1.5843621399176955, "no_speech_prob": 0.019413547590374947}, {"id": 107, "seek": 58704, "start": 587.12, "end": 591.68, "text": " maybe have many cost functions, or maybe have dynamic cost functions, or maybe", "tokens": [50368, 1310, 362, 867, 2063, 6828, 11, 420, 1310, 362, 8546, 2063, 6828, 11, 420, 1310, 50596], "temperature": 0.0, "avg_logprob": -0.09458261151467601, "compression_ratio": 1.838487972508591, "no_speech_prob": 0.005999772809445858}, {"id": 108, "seek": 58704, "start": 591.68, "end": 595.8399999999999, "text": " a totally different kind of architectures? Because we have to think like that in order to", "tokens": [50596, 257, 3879, 819, 733, 295, 6331, 1303, 30, 1436, 321, 362, 281, 519, 411, 300, 294, 1668, 281, 50804], "temperature": 0.0, "avg_logprob": -0.09458261151467601, "compression_ratio": 1.838487972508591, "no_speech_prob": 0.005999772809445858}, {"id": 109, "seek": 58704, "start": 595.8399999999999, "end": 601.04, "text": " arrive at something new, right? So the good examples of things which don't have clear cost", "tokens": [50804, 8881, 412, 746, 777, 11, 558, 30, 407, 264, 665, 5110, 295, 721, 597, 500, 380, 362, 1850, 2063, 51064], "temperature": 0.0, "avg_logprob": -0.09458261151467601, "compression_ratio": 1.838487972508591, "no_speech_prob": 0.005999772809445858}, {"id": 110, "seek": 58704, "start": 601.04, "end": 607.1999999999999, "text": " functions are GANs. And again, you have a game. So instead of thinking of a cost function,", "tokens": [51064, 6828, 366, 460, 1770, 82, 13, 400, 797, 11, 291, 362, 257, 1216, 13, 407, 2602, 295, 1953, 295, 257, 2063, 2445, 11, 51372], "temperature": 0.0, "avg_logprob": -0.09458261151467601, "compression_ratio": 1.838487972508591, "no_speech_prob": 0.005999772809445858}, {"id": 111, "seek": 58704, "start": 608.0799999999999, "end": 612.0, "text": " where you want to optimize, where you know that you have an algorithm gradient descent,", "tokens": [51416, 689, 291, 528, 281, 19719, 11, 689, 291, 458, 300, 291, 362, 364, 9284, 16235, 23475, 11, 51612], "temperature": 0.0, "avg_logprob": -0.09458261151467601, "compression_ratio": 1.838487972508591, "no_speech_prob": 0.005999772809445858}, {"id": 112, "seek": 58704, "start": 612.0, "end": 616.24, "text": " which will optimize the cost function. And then you can reason about the behavior of your system", "tokens": [51612, 597, 486, 19719, 264, 2063, 2445, 13, 400, 550, 291, 393, 1778, 466, 264, 5223, 295, 428, 1185, 51824], "temperature": 0.0, "avg_logprob": -0.09458261151467601, "compression_ratio": 1.838487972508591, "no_speech_prob": 0.005999772809445858}, {"id": 113, "seek": 61624, "start": 616.24, "end": 621.6800000000001, "text": " in terms of what it optimizes. With GAN, you say, I have a game, and I'll reason about the behavior", "tokens": [50364, 294, 2115, 295, 437, 309, 5028, 5660, 13, 2022, 460, 1770, 11, 291, 584, 11, 286, 362, 257, 1216, 11, 293, 286, 603, 1778, 466, 264, 5223, 50636], "temperature": 0.0, "avg_logprob": -0.08868644739452161, "compression_ratio": 1.9150326797385622, "no_speech_prob": 0.002357715042307973}, {"id": 114, "seek": 61624, "start": 621.6800000000001, "end": 625.6, "text": " of the system in terms of the equilibrium of the game. But it's all about coming up with these", "tokens": [50636, 295, 264, 1185, 294, 2115, 295, 264, 15625, 295, 264, 1216, 13, 583, 309, 311, 439, 466, 1348, 493, 365, 613, 50832], "temperature": 0.0, "avg_logprob": -0.08868644739452161, "compression_ratio": 1.9150326797385622, "no_speech_prob": 0.002357715042307973}, {"id": 115, "seek": 61624, "start": 625.6, "end": 630.64, "text": " mathematical objects that help us reason about the behavior of our system. Right, that's really", "tokens": [50832, 18894, 6565, 300, 854, 505, 1778, 466, 264, 5223, 295, 527, 1185, 13, 1779, 11, 300, 311, 534, 51084], "temperature": 0.0, "avg_logprob": -0.08868644739452161, "compression_ratio": 1.9150326797385622, "no_speech_prob": 0.002357715042307973}, {"id": 116, "seek": 61624, "start": 630.64, "end": 635.6, "text": " interesting. Yeah, so GAN is the only one. It's kind of a, the cost function is emergent from the", "tokens": [51084, 1880, 13, 865, 11, 370, 460, 1770, 307, 264, 787, 472, 13, 467, 311, 733, 295, 257, 11, 264, 2063, 2445, 307, 4345, 6930, 490, 264, 51332], "temperature": 0.0, "avg_logprob": -0.08868644739452161, "compression_ratio": 1.9150326797385622, "no_speech_prob": 0.002357715042307973}, {"id": 117, "seek": 61624, "start": 635.6, "end": 640.16, "text": " comparison. I don't know if it has a cost function. I don't know if it's meaningful to talk about the", "tokens": [51332, 9660, 13, 286, 500, 380, 458, 498, 309, 575, 257, 2063, 2445, 13, 286, 500, 380, 458, 498, 309, 311, 10995, 281, 751, 466, 264, 51560], "temperature": 0.0, "avg_logprob": -0.08868644739452161, "compression_ratio": 1.9150326797385622, "no_speech_prob": 0.002357715042307973}, {"id": 118, "seek": 61624, "start": 640.16, "end": 644.24, "text": " cost function of a GAN. It's kind of like the cost function of biological evolution or the cost", "tokens": [51560, 2063, 2445, 295, 257, 460, 1770, 13, 467, 311, 733, 295, 411, 264, 2063, 2445, 295, 13910, 9303, 420, 264, 2063, 51764], "temperature": 0.0, "avg_logprob": -0.08868644739452161, "compression_ratio": 1.9150326797385622, "no_speech_prob": 0.002357715042307973}, {"id": 119, "seek": 64424, "start": 644.24, "end": 651.92, "text": " function of the economy. It's, you can talk about regions to which it will go towards, but I don't", "tokens": [50364, 2445, 295, 264, 5010, 13, 467, 311, 11, 291, 393, 751, 466, 10682, 281, 597, 309, 486, 352, 3030, 11, 457, 286, 500, 380, 50748], "temperature": 0.0, "avg_logprob": -0.06467030638007708, "compression_ratio": 1.8468899521531101, "no_speech_prob": 0.004754023626446724}, {"id": 120, "seek": 64424, "start": 651.92, "end": 659.44, "text": " think, I don't think the cost function analogy is the most useful. So evolution doesn't, that's", "tokens": [50748, 519, 11, 286, 500, 380, 519, 264, 2063, 2445, 21663, 307, 264, 881, 4420, 13, 407, 9303, 1177, 380, 11, 300, 311, 51124], "temperature": 0.0, "avg_logprob": -0.06467030638007708, "compression_ratio": 1.8468899521531101, "no_speech_prob": 0.004754023626446724}, {"id": 121, "seek": 64424, "start": 659.44, "end": 664.24, "text": " really interesting. So if evolution doesn't really have a cost function, like a cost function based", "tokens": [51124, 534, 1880, 13, 407, 498, 9303, 1177, 380, 534, 362, 257, 2063, 2445, 11, 411, 257, 2063, 2445, 2361, 51364], "temperature": 0.0, "avg_logprob": -0.06467030638007708, "compression_ratio": 1.8468899521531101, "no_speech_prob": 0.004754023626446724}, {"id": 122, "seek": 64424, "start": 664.24, "end": 671.92, "text": " on it's something akin to our mathematical conception of a cost function, then do you think", "tokens": [51364, 322, 309, 311, 746, 47540, 281, 527, 18894, 30698, 295, 257, 2063, 2445, 11, 550, 360, 291, 519, 51748], "temperature": 0.0, "avg_logprob": -0.06467030638007708, "compression_ratio": 1.8468899521531101, "no_speech_prob": 0.004754023626446724}, {"id": 123, "seek": 67192, "start": 671.92, "end": 677.4399999999999, "text": " cost functions in deep learning are holding us back? Yeah, I, so you just kind of mentioned", "tokens": [50364, 2063, 6828, 294, 2452, 2539, 366, 5061, 505, 646, 30, 865, 11, 286, 11, 370, 291, 445, 733, 295, 2835, 50640], "temperature": 0.0, "avg_logprob": -0.09969263389462331, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.009122524410486221}, {"id": 124, "seek": 67192, "start": 677.4399999999999, "end": 683.4399999999999, "text": " that cost function is a, is a nice first profound idea. Do you think that's a good idea? Do you", "tokens": [50640, 300, 2063, 2445, 307, 257, 11, 307, 257, 1481, 700, 14382, 1558, 13, 1144, 291, 519, 300, 311, 257, 665, 1558, 30, 1144, 291, 50940], "temperature": 0.0, "avg_logprob": -0.09969263389462331, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.009122524410486221}, {"id": 125, "seek": 67192, "start": 683.4399999999999, "end": 690.24, "text": " think it's an idea will go past? So self play starts to touch on that a little bit in reinforcement", "tokens": [50940, 519, 309, 311, 364, 1558, 486, 352, 1791, 30, 407, 2698, 862, 3719, 281, 2557, 322, 300, 257, 707, 857, 294, 29280, 51280], "temperature": 0.0, "avg_logprob": -0.09969263389462331, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.009122524410486221}, {"id": 126, "seek": 67192, "start": 690.24, "end": 695.8399999999999, "text": " learning systems. That's right. Self play and also ideas around exploration where you're trying to", "tokens": [51280, 2539, 3652, 13, 663, 311, 558, 13, 16348, 862, 293, 611, 3487, 926, 16197, 689, 291, 434, 1382, 281, 51560], "temperature": 0.0, "avg_logprob": -0.09969263389462331, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.009122524410486221}, {"id": 127, "seek": 67192, "start": 695.8399999999999, "end": 700.8, "text": " take action. That's, that's surprise a predictor. I'm a big fan of cost functions. I think cost", "tokens": [51560, 747, 3069, 13, 663, 311, 11, 300, 311, 6365, 257, 6069, 284, 13, 286, 478, 257, 955, 3429, 295, 2063, 6828, 13, 286, 519, 2063, 51808], "temperature": 0.0, "avg_logprob": -0.09969263389462331, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.009122524410486221}, {"id": 128, "seek": 70080, "start": 700.8, "end": 704.56, "text": " functions are great and they serve us really well. And I think that whenever we can do things with", "tokens": [50364, 6828, 366, 869, 293, 436, 4596, 505, 534, 731, 13, 400, 286, 519, 300, 5699, 321, 393, 360, 721, 365, 50552], "temperature": 0.0, "avg_logprob": -0.10684829491835374, "compression_ratio": 1.8242677824267783, "no_speech_prob": 0.007459022104740143}, {"id": 129, "seek": 70080, "start": 704.56, "end": 710.24, "text": " cost functions, we should. And you know, maybe there is a chance that we will come up with some", "tokens": [50552, 2063, 6828, 11, 321, 820, 13, 400, 291, 458, 11, 1310, 456, 307, 257, 2931, 300, 321, 486, 808, 493, 365, 512, 50836], "temperature": 0.0, "avg_logprob": -0.10684829491835374, "compression_ratio": 1.8242677824267783, "no_speech_prob": 0.007459022104740143}, {"id": 130, "seek": 70080, "start": 710.24, "end": 715.4399999999999, "text": " yet another profound way of looking at things that will involve cost functions in a less central way.", "tokens": [50836, 1939, 1071, 14382, 636, 295, 1237, 412, 721, 300, 486, 9494, 2063, 6828, 294, 257, 1570, 5777, 636, 13, 51096], "temperature": 0.0, "avg_logprob": -0.10684829491835374, "compression_ratio": 1.8242677824267783, "no_speech_prob": 0.007459022104740143}, {"id": 131, "seek": 70080, "start": 715.4399999999999, "end": 717.28, "text": " But I don't know, I think cost functions are, I mean,", "tokens": [51096, 583, 286, 500, 380, 458, 11, 286, 519, 2063, 6828, 366, 11, 286, 914, 11, 51188], "temperature": 0.0, "avg_logprob": -0.10684829491835374, "compression_ratio": 1.8242677824267783, "no_speech_prob": 0.007459022104740143}, {"id": 132, "seek": 70080, "start": 719.8399999999999, "end": 724.64, "text": " I would not bet against against cost functions. Is there other things about the brain", "tokens": [51316, 286, 576, 406, 778, 1970, 1970, 2063, 6828, 13, 1119, 456, 661, 721, 466, 264, 3567, 51556], "temperature": 0.0, "avg_logprob": -0.10684829491835374, "compression_ratio": 1.8242677824267783, "no_speech_prob": 0.007459022104740143}, {"id": 133, "seek": 72464, "start": 725.36, "end": 730.24, "text": " that pop into your mind that might be different and interesting for us to consider", "tokens": [50400, 300, 1665, 666, 428, 1575, 300, 1062, 312, 819, 293, 1880, 337, 505, 281, 1949, 50644], "temperature": 0.0, "avg_logprob": -0.15431177932604223, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0900537520647049}, {"id": 134, "seek": 72464, "start": 730.96, "end": 734.88, "text": " in designing artificial neural networks? So we talked about spiking a little bit.", "tokens": [50680, 294, 14685, 11677, 18161, 9590, 30, 407, 321, 2825, 466, 637, 13085, 257, 707, 857, 13, 50876], "temperature": 0.0, "avg_logprob": -0.15431177932604223, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0900537520647049}, {"id": 135, "seek": 72464, "start": 736.08, "end": 739.28, "text": " I mean, one, one thing which may potentially be useful, I think people,", "tokens": [50936, 286, 914, 11, 472, 11, 472, 551, 597, 815, 7263, 312, 4420, 11, 286, 519, 561, 11, 51096], "temperature": 0.0, "avg_logprob": -0.15431177932604223, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0900537520647049}, {"id": 136, "seek": 72464, "start": 739.28, "end": 742.3199999999999, "text": " neuroscientists figured out something about the learning rule of the brain or", "tokens": [51096, 28813, 5412, 1751, 8932, 484, 746, 466, 264, 2539, 4978, 295, 264, 3567, 420, 51248], "temperature": 0.0, "avg_logprob": -0.15431177932604223, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0900537520647049}, {"id": 137, "seek": 72464, "start": 742.88, "end": 746.48, "text": " talking about spike time independent plasticity. And it would be nice if some people were to", "tokens": [51276, 1417, 466, 21053, 565, 6695, 5900, 507, 13, 400, 309, 576, 312, 1481, 498, 512, 561, 645, 281, 51456], "temperature": 0.0, "avg_logprob": -0.15431177932604223, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0900537520647049}, {"id": 138, "seek": 72464, "start": 746.48, "end": 751.2, "text": " study that in simulation. Wait, sorry, spike time independent plasticity. Yeah, that's", "tokens": [51456, 2979, 300, 294, 16575, 13, 3802, 11, 2597, 11, 21053, 565, 6695, 5900, 507, 13, 865, 11, 300, 311, 51692], "temperature": 0.0, "avg_logprob": -0.15431177932604223, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0900537520647049}, {"id": 139, "seek": 75120, "start": 751.6800000000001, "end": 757.36, "text": " STD. It's a particular learning rule that uses spike timing to figure out how to determine how", "tokens": [50388, 4904, 35, 13, 467, 311, 257, 1729, 2539, 4978, 300, 4960, 21053, 10822, 281, 2573, 484, 577, 281, 6997, 577, 50672], "temperature": 0.0, "avg_logprob": -0.10464610891827082, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.01300407387316227}, {"id": 140, "seek": 75120, "start": 757.36, "end": 763.2800000000001, "text": " to update the synapses. So it's kind of like, if a synapse fires into the neuron before the neuron", "tokens": [50672, 281, 5623, 264, 5451, 2382, 279, 13, 407, 309, 311, 733, 295, 411, 11, 498, 257, 5451, 11145, 15044, 666, 264, 34090, 949, 264, 34090, 50968], "temperature": 0.0, "avg_logprob": -0.10464610891827082, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.01300407387316227}, {"id": 141, "seek": 75120, "start": 763.2800000000001, "end": 768.32, "text": " fires, then it's strengthened the synapse. And if the synapse fires into the neurons shortly", "tokens": [50968, 15044, 11, 550, 309, 311, 38942, 264, 5451, 11145, 13, 400, 498, 264, 5451, 11145, 15044, 666, 264, 22027, 13392, 51220], "temperature": 0.0, "avg_logprob": -0.10464610891827082, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.01300407387316227}, {"id": 142, "seek": 75120, "start": 768.32, "end": 772.96, "text": " after the neuron fired, then it becomes the synapse something along this line. I'm 90%", "tokens": [51220, 934, 264, 34090, 11777, 11, 550, 309, 3643, 264, 5451, 11145, 746, 2051, 341, 1622, 13, 286, 478, 4289, 4, 51452], "temperature": 0.0, "avg_logprob": -0.10464610891827082, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.01300407387316227}, {"id": 143, "seek": 75120, "start": 772.96, "end": 778.4000000000001, "text": " sure it's right. So if I said something wrong here, don't don't get too angry.", "tokens": [51452, 988, 309, 311, 558, 13, 407, 498, 286, 848, 746, 2085, 510, 11, 500, 380, 500, 380, 483, 886, 6884, 13, 51724], "temperature": 0.0, "avg_logprob": -0.10464610891827082, "compression_ratio": 1.8299595141700404, "no_speech_prob": 0.01300407387316227}, {"id": 144, "seek": 77840, "start": 779.28, "end": 783.4399999999999, "text": " But you sounded brilliant while saying it. But the timing, that's one thing that's missing.", "tokens": [50408, 583, 291, 17714, 10248, 1339, 1566, 309, 13, 583, 264, 10822, 11, 300, 311, 472, 551, 300, 311, 5361, 13, 50616], "temperature": 0.0, "avg_logprob": -0.17656477759866154, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0037052698899060488}, {"id": 145, "seek": 77840, "start": 784.16, "end": 790.0, "text": " The temporal dynamics is not captured. I think that's like a fundamental property of the brain", "tokens": [50652, 440, 30881, 15679, 307, 406, 11828, 13, 286, 519, 300, 311, 411, 257, 8088, 4707, 295, 264, 3567, 50944], "temperature": 0.0, "avg_logprob": -0.17656477759866154, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0037052698899060488}, {"id": 146, "seek": 77840, "start": 790.0, "end": 794.48, "text": " is the timing of the signals. Well, you're recording your networks.", "tokens": [50944, 307, 264, 10822, 295, 264, 12354, 13, 1042, 11, 291, 434, 6613, 428, 9590, 13, 51168], "temperature": 0.0, "avg_logprob": -0.17656477759866154, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0037052698899060488}, {"id": 147, "seek": 77840, "start": 795.36, "end": 801.92, "text": " But you think of that as, I mean, that's a very crude simplified, what's that called?", "tokens": [51212, 583, 291, 519, 295, 300, 382, 11, 286, 914, 11, 300, 311, 257, 588, 30796, 26335, 11, 437, 311, 300, 1219, 30, 51540], "temperature": 0.0, "avg_logprob": -0.17656477759866154, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0037052698899060488}, {"id": 148, "seek": 80192, "start": 802.64, "end": 809.5999999999999, "text": " There's a clock, I guess, to recurrent neural networks. This seems like the brain is the", "tokens": [50400, 821, 311, 257, 7830, 11, 286, 2041, 11, 281, 18680, 1753, 18161, 9590, 13, 639, 2544, 411, 264, 3567, 307, 264, 50748], "temperature": 0.0, "avg_logprob": -0.14175047357398343, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.021277954801917076}, {"id": 149, "seek": 80192, "start": 809.5999999999999, "end": 815.4399999999999, "text": " general, the continuous version of that, the generalization where all possible timings are", "tokens": [50748, 2674, 11, 264, 10957, 3037, 295, 300, 11, 264, 2674, 2144, 689, 439, 1944, 524, 1109, 366, 51040], "temperature": 0.0, "avg_logprob": -0.14175047357398343, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.021277954801917076}, {"id": 150, "seek": 80192, "start": 815.4399999999999, "end": 821.1999999999999, "text": " possible. And then within those timings, this contains some information. You think recurrent", "tokens": [51040, 1944, 13, 400, 550, 1951, 729, 524, 1109, 11, 341, 8306, 512, 1589, 13, 509, 519, 18680, 1753, 51328], "temperature": 0.0, "avg_logprob": -0.14175047357398343, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.021277954801917076}, {"id": 151, "seek": 80192, "start": 821.1999999999999, "end": 827.52, "text": " neural networks, the recurrence in recurrent neural networks can capture the same kind of", "tokens": [51328, 18161, 9590, 11, 264, 18680, 10760, 294, 18680, 1753, 18161, 9590, 393, 7983, 264, 912, 733, 295, 51644], "temperature": 0.0, "avg_logprob": -0.14175047357398343, "compression_ratio": 1.7745098039215685, "no_speech_prob": 0.021277954801917076}, {"id": 152, "seek": 82752, "start": 827.6, "end": 835.36, "text": " phenomena as the timing that seems to be important for the brain in the firing of neurons in the", "tokens": [50368, 22004, 382, 264, 10822, 300, 2544, 281, 312, 1021, 337, 264, 3567, 294, 264, 16045, 295, 22027, 294, 264, 50756], "temperature": 0.0, "avg_logprob": -0.12507602605926857, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.002115146489813924}, {"id": 153, "seek": 82752, "start": 835.36, "end": 841.92, "text": " brain? I mean, I think recurrent neural networks are amazing and they can do,", "tokens": [50756, 3567, 30, 286, 914, 11, 286, 519, 18680, 1753, 18161, 9590, 366, 2243, 293, 436, 393, 360, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12507602605926857, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.002115146489813924}, {"id": 154, "seek": 82752, "start": 842.56, "end": 848.0, "text": " I think they can do anything we'd want them to, we'd want a system to do. Right now,", "tokens": [51116, 286, 519, 436, 393, 360, 1340, 321, 1116, 528, 552, 281, 11, 321, 1116, 528, 257, 1185, 281, 360, 13, 1779, 586, 11, 51388], "temperature": 0.0, "avg_logprob": -0.12507602605926857, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.002115146489813924}, {"id": 155, "seek": 82752, "start": 848.0, "end": 852.0, "text": " recurrent neural networks have been superseded by transformers, but maybe one day they'll make", "tokens": [51388, 18680, 1753, 18161, 9590, 362, 668, 37906, 37679, 538, 4088, 433, 11, 457, 1310, 472, 786, 436, 603, 652, 51588], "temperature": 0.0, "avg_logprob": -0.12507602605926857, "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.002115146489813924}, {"id": 156, "seek": 85200, "start": 852.0, "end": 858.24, "text": " a comeback, maybe they'll be back, we'll see. Let me in a small tangent say, do you think they'll", "tokens": [50364, 257, 23464, 11, 1310, 436, 603, 312, 646, 11, 321, 603, 536, 13, 961, 385, 294, 257, 1359, 27747, 584, 11, 360, 291, 519, 436, 603, 50676], "temperature": 0.0, "avg_logprob": -0.09820473447759101, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.013015154749155045}, {"id": 157, "seek": 85200, "start": 858.24, "end": 864.24, "text": " be back? So so much of the breakthroughs recently that we'll talk about on natural language processing", "tokens": [50676, 312, 646, 30, 407, 370, 709, 295, 264, 22397, 82, 3938, 300, 321, 603, 751, 466, 322, 3303, 2856, 9007, 50976], "temperature": 0.0, "avg_logprob": -0.09820473447759101, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.013015154749155045}, {"id": 158, "seek": 85200, "start": 864.24, "end": 871.12, "text": " and language modeling has been with transformers that don't emphasize recurrence. Do you think", "tokens": [50976, 293, 2856, 15983, 575, 668, 365, 4088, 433, 300, 500, 380, 16078, 18680, 10760, 13, 1144, 291, 519, 51320], "temperature": 0.0, "avg_logprob": -0.09820473447759101, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.013015154749155045}, {"id": 159, "seek": 85200, "start": 871.12, "end": 875.92, "text": " recurrence will make a comeback? Well, some kind of recurrence, I think very likely.", "tokens": [51320, 18680, 10760, 486, 652, 257, 23464, 30, 1042, 11, 512, 733, 295, 18680, 10760, 11, 286, 519, 588, 3700, 13, 51560], "temperature": 0.0, "avg_logprob": -0.09820473447759101, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.013015154749155045}, {"id": 160, "seek": 87592, "start": 876.8, "end": 882.9599999999999, "text": " Recurrent neural networks, as they're typically thought of for processing sequences, I think it's", "tokens": [50408, 9647, 374, 1753, 18161, 9590, 11, 382, 436, 434, 5850, 1194, 295, 337, 9007, 22978, 11, 286, 519, 309, 311, 50716], "temperature": 0.0, "avg_logprob": -0.11878623085460444, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.03355056419968605}, {"id": 161, "seek": 87592, "start": 882.9599999999999, "end": 889.12, "text": " also possible. What is, to you, a recurrent neural network? In general speaking, I guess,", "tokens": [50716, 611, 1944, 13, 708, 307, 11, 281, 291, 11, 257, 18680, 1753, 18161, 3209, 30, 682, 2674, 4124, 11, 286, 2041, 11, 51024], "temperature": 0.0, "avg_logprob": -0.11878623085460444, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.03355056419968605}, {"id": 162, "seek": 87592, "start": 889.12, "end": 893.52, "text": " what is a recurrent neural network? You have a neural network which maintains a high dimensional", "tokens": [51024, 437, 307, 257, 18680, 1753, 18161, 3209, 30, 509, 362, 257, 18161, 3209, 597, 33385, 257, 1090, 18795, 51244], "temperature": 0.0, "avg_logprob": -0.11878623085460444, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.03355056419968605}, {"id": 163, "seek": 87592, "start": 893.52, "end": 899.1999999999999, "text": " hidden state. And then when an observation arrives, it updates its high dimensional hidden state", "tokens": [51244, 7633, 1785, 13, 400, 550, 562, 364, 14816, 20116, 11, 309, 9205, 1080, 1090, 18795, 7633, 1785, 51528], "temperature": 0.0, "avg_logprob": -0.11878623085460444, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.03355056419968605}, {"id": 164, "seek": 89920, "start": 899.76, "end": 906.96, "text": " through its connections in some way. So do you think, you know, that's what like expert systems", "tokens": [50392, 807, 1080, 9271, 294, 512, 636, 13, 407, 360, 291, 519, 11, 291, 458, 11, 300, 311, 437, 411, 5844, 3652, 50752], "temperature": 0.0, "avg_logprob": -0.15165484791070644, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.010811973363161087}, {"id": 165, "seek": 89920, "start": 906.96, "end": 917.0400000000001, "text": " did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a hidden state,", "tokens": [50752, 630, 11, 558, 30, 3902, 5612, 299, 7318, 11, 264, 3601, 2361, 11, 4194, 257, 3601, 3096, 307, 14916, 257, 7633, 1785, 11, 51256], "temperature": 0.0, "avg_logprob": -0.15165484791070644, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.010811973363161087}, {"id": 166, "seek": 89920, "start": 917.0400000000001, "end": 921.0400000000001, "text": " which is its knowledge base and is growing it by sequential processing. Do you think of it more", "tokens": [51256, 597, 307, 1080, 3601, 3096, 293, 307, 4194, 309, 538, 42881, 9007, 13, 1144, 291, 519, 295, 309, 544, 51456], "temperature": 0.0, "avg_logprob": -0.15165484791070644, "compression_ratio": 1.5668449197860963, "no_speech_prob": 0.010811973363161087}, {"id": 167, "seek": 92104, "start": 921.12, "end": 929.92, "text": " generally in that way? Or is it simply, is it the more constrained form of a hidden state with", "tokens": [50368, 5101, 294, 300, 636, 30, 1610, 307, 309, 2935, 11, 307, 309, 264, 544, 38901, 1254, 295, 257, 7633, 1785, 365, 50808], "temperature": 0.0, "avg_logprob": -0.12312245171917371, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.07576832175254822}, {"id": 168, "seek": 92104, "start": 929.92, "end": 934.9599999999999, "text": " certain kind of gating units that we think of as today with LSTMs and that? I mean, the hidden", "tokens": [50808, 1629, 733, 295, 290, 990, 6815, 300, 321, 519, 295, 382, 965, 365, 441, 6840, 26386, 293, 300, 30, 286, 914, 11, 264, 7633, 51060], "temperature": 0.0, "avg_logprob": -0.12312245171917371, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.07576832175254822}, {"id": 169, "seek": 92104, "start": 934.9599999999999, "end": 939.28, "text": " state is technically what you described there, the hidden state that goes inside the LSTM or", "tokens": [51060, 1785, 307, 12120, 437, 291, 7619, 456, 11, 264, 7633, 1785, 300, 1709, 1854, 264, 441, 6840, 44, 420, 51276], "temperature": 0.0, "avg_logprob": -0.12312245171917371, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.07576832175254822}, {"id": 170, "seek": 92104, "start": 939.28, "end": 943.5999999999999, "text": " there are an N or something like this. But then what should be contained, you know, if you want to", "tokens": [51276, 456, 366, 364, 426, 420, 746, 411, 341, 13, 583, 550, 437, 820, 312, 16212, 11, 291, 458, 11, 498, 291, 528, 281, 51492], "temperature": 0.0, "avg_logprob": -0.12312245171917371, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.07576832175254822}, {"id": 171, "seek": 92104, "start": 943.5999999999999, "end": 950.0, "text": " make the expert system and analogy, I'm not, I mean, you could say that the knowledge is stored", "tokens": [51492, 652, 264, 5844, 1185, 293, 21663, 11, 286, 478, 406, 11, 286, 914, 11, 291, 727, 584, 300, 264, 3601, 307, 12187, 51812], "temperature": 0.0, "avg_logprob": -0.12312245171917371, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.07576832175254822}, {"id": 172, "seek": 95000, "start": 950.08, "end": 955.12, "text": " in the connections and then the short term processing is done in the hidden state.", "tokens": [50368, 294, 264, 9271, 293, 550, 264, 2099, 1433, 9007, 307, 1096, 294, 264, 7633, 1785, 13, 50620], "temperature": 0.0, "avg_logprob": -0.11598644256591797, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.0028000047896057367}, {"id": 173, "seek": 95000, "start": 956.24, "end": 962.8, "text": " Yes. Could you say that? So sort of, do you think there's a future of building large scale", "tokens": [50676, 1079, 13, 7497, 291, 584, 300, 30, 407, 1333, 295, 11, 360, 291, 519, 456, 311, 257, 2027, 295, 2390, 2416, 4373, 51004], "temperature": 0.0, "avg_logprob": -0.11598644256591797, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.0028000047896057367}, {"id": 174, "seek": 95000, "start": 963.44, "end": 966.08, "text": " knowledge bases within the neural networks? Definitely.", "tokens": [51036, 3601, 17949, 1951, 264, 18161, 9590, 30, 12151, 13, 51168], "temperature": 0.0, "avg_logprob": -0.11598644256591797, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.0028000047896057367}, {"id": 175, "seek": 95000, "start": 968.96, "end": 974.16, "text": " So we're going to pause on that confidence because I want to explore that. Well, let me zoom back out", "tokens": [51312, 407, 321, 434, 516, 281, 10465, 322, 300, 6687, 570, 286, 528, 281, 6839, 300, 13, 1042, 11, 718, 385, 8863, 646, 484, 51572], "temperature": 0.0, "avg_logprob": -0.11598644256591797, "compression_ratio": 1.497737556561086, "no_speech_prob": 0.0028000047896057367}, {"id": 176, "seek": 97416, "start": 974.16, "end": 981.36, "text": " and ask back to the history of ImageNet. Neural networks have been around for many decades as", "tokens": [50364, 293, 1029, 646, 281, 264, 2503, 295, 29903, 31890, 13, 1734, 1807, 9590, 362, 668, 926, 337, 867, 7878, 382, 50724], "temperature": 0.0, "avg_logprob": -0.0903269757506668, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.014724355190992355}, {"id": 177, "seek": 97416, "start": 981.36, "end": 987.28, "text": " you mentioned. What do you think were the key ideas that led to their success, that ImageNet moment", "tokens": [50724, 291, 2835, 13, 708, 360, 291, 519, 645, 264, 2141, 3487, 300, 4684, 281, 641, 2245, 11, 300, 29903, 31890, 1623, 51020], "temperature": 0.0, "avg_logprob": -0.0903269757506668, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.014724355190992355}, {"id": 178, "seek": 97416, "start": 987.28, "end": 994.24, "text": " and beyond the success in the past 10 years? Okay. So the question is to make sure I didn't", "tokens": [51020, 293, 4399, 264, 2245, 294, 264, 1791, 1266, 924, 30, 1033, 13, 407, 264, 1168, 307, 281, 652, 988, 286, 994, 380, 51368], "temperature": 0.0, "avg_logprob": -0.0903269757506668, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.014724355190992355}, {"id": 179, "seek": 97416, "start": 994.24, "end": 999.28, "text": " miss anything, the key ideas that led to the success of deep learning over the past 10 years.", "tokens": [51368, 1713, 1340, 11, 264, 2141, 3487, 300, 4684, 281, 264, 2245, 295, 2452, 2539, 670, 264, 1791, 1266, 924, 13, 51620], "temperature": 0.0, "avg_logprob": -0.0903269757506668, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.014724355190992355}, {"id": 180, "seek": 99928, "start": 999.36, "end": 1004.8, "text": " Exactly. Even though the fundamental thing behind deep learning has been around for much longer.", "tokens": [50368, 7587, 13, 2754, 1673, 264, 8088, 551, 2261, 2452, 2539, 575, 668, 926, 337, 709, 2854, 13, 50640], "temperature": 0.0, "avg_logprob": -0.10119293928146363, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0017249626107513905}, {"id": 181, "seek": 99928, "start": 1005.36, "end": 1015.6, "text": " So the key idea about deep learning, or rather the key fact about deep learning before deep", "tokens": [50668, 407, 264, 2141, 1558, 466, 2452, 2539, 11, 420, 2831, 264, 2141, 1186, 466, 2452, 2539, 949, 2452, 51180], "temperature": 0.0, "avg_logprob": -0.10119293928146363, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0017249626107513905}, {"id": 182, "seek": 99928, "start": 1015.6, "end": 1022.48, "text": " learning started to be successful is that it was underestimated. People who worked in machine", "tokens": [51180, 2539, 1409, 281, 312, 4406, 307, 300, 309, 390, 24612, 33008, 13, 3432, 567, 2732, 294, 3479, 51524], "temperature": 0.0, "avg_logprob": -0.10119293928146363, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0017249626107513905}, {"id": 183, "seek": 99928, "start": 1022.48, "end": 1028.08, "text": " learning simply didn't think that neural networks could do much. People didn't believe that large", "tokens": [51524, 2539, 2935, 994, 380, 519, 300, 18161, 9590, 727, 360, 709, 13, 3432, 994, 380, 1697, 300, 2416, 51804], "temperature": 0.0, "avg_logprob": -0.10119293928146363, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0017249626107513905}, {"id": 184, "seek": 102808, "start": 1028.08, "end": 1033.84, "text": " neural networks could be trained. People thought that, well, there was lots of, there was a lot of", "tokens": [50364, 18161, 9590, 727, 312, 8895, 13, 3432, 1194, 300, 11, 731, 11, 456, 390, 3195, 295, 11, 456, 390, 257, 688, 295, 50652], "temperature": 0.0, "avg_logprob": -0.108300154326392, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0019248576136305928}, {"id": 185, "seek": 102808, "start": 1033.84, "end": 1038.72, "text": " debate going on in machine learning about what are the right methods and so on. And people were", "tokens": [50652, 7958, 516, 322, 294, 3479, 2539, 466, 437, 366, 264, 558, 7150, 293, 370, 322, 13, 400, 561, 645, 50896], "temperature": 0.0, "avg_logprob": -0.108300154326392, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0019248576136305928}, {"id": 186, "seek": 102808, "start": 1038.72, "end": 1043.76, "text": " arguing because there were no, there were no, there was no way to get hard facts. And by that,", "tokens": [50896, 19697, 570, 456, 645, 572, 11, 456, 645, 572, 11, 456, 390, 572, 636, 281, 483, 1152, 9130, 13, 400, 538, 300, 11, 51148], "temperature": 0.0, "avg_logprob": -0.108300154326392, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0019248576136305928}, {"id": 187, "seek": 102808, "start": 1043.76, "end": 1048.48, "text": " I mean, there were no benchmarks which were truly hard, that if you do really well on them, then", "tokens": [51148, 286, 914, 11, 456, 645, 572, 43751, 597, 645, 4908, 1152, 11, 300, 498, 291, 360, 534, 731, 322, 552, 11, 550, 51384], "temperature": 0.0, "avg_logprob": -0.108300154326392, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0019248576136305928}, {"id": 188, "seek": 102808, "start": 1048.48, "end": 1056.6399999999999, "text": " you can say, look, here's my system. That's when you switch from, that's when this field becomes", "tokens": [51384, 291, 393, 584, 11, 574, 11, 510, 311, 452, 1185, 13, 663, 311, 562, 291, 3679, 490, 11, 300, 311, 562, 341, 2519, 3643, 51792], "temperature": 0.0, "avg_logprob": -0.108300154326392, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0019248576136305928}, {"id": 189, "seek": 105664, "start": 1056.64, "end": 1060.48, "text": " a little bit more of an engineering field. So in terms of deep learning to answer the question", "tokens": [50364, 257, 707, 857, 544, 295, 364, 7043, 2519, 13, 407, 294, 2115, 295, 2452, 2539, 281, 1867, 264, 1168, 50556], "temperature": 0.0, "avg_logprob": -0.08055356067159901, "compression_ratio": 1.9346938775510205, "no_speech_prob": 0.0023223981261253357}, {"id": 190, "seek": 105664, "start": 1060.48, "end": 1067.0400000000002, "text": " directly, the ideas were all there. The thing that was missing was a lot of supervised data and a lot", "tokens": [50556, 3838, 11, 264, 3487, 645, 439, 456, 13, 440, 551, 300, 390, 5361, 390, 257, 688, 295, 46533, 1412, 293, 257, 688, 50884], "temperature": 0.0, "avg_logprob": -0.08055356067159901, "compression_ratio": 1.9346938775510205, "no_speech_prob": 0.0023223981261253357}, {"id": 191, "seek": 105664, "start": 1067.0400000000002, "end": 1073.2, "text": " of compute. Once you have a lot of supervised data and a lot of compute, then there is a third", "tokens": [50884, 295, 14722, 13, 3443, 291, 362, 257, 688, 295, 46533, 1412, 293, 257, 688, 295, 14722, 11, 550, 456, 307, 257, 2636, 51192], "temperature": 0.0, "avg_logprob": -0.08055356067159901, "compression_ratio": 1.9346938775510205, "no_speech_prob": 0.0023223981261253357}, {"id": 192, "seek": 105664, "start": 1073.2, "end": 1077.68, "text": " thing which is needed as well. And that is conviction, conviction that if you take", "tokens": [51192, 551, 597, 307, 2978, 382, 731, 13, 400, 300, 307, 24837, 11, 24837, 300, 498, 291, 747, 51416], "temperature": 0.0, "avg_logprob": -0.08055356067159901, "compression_ratio": 1.9346938775510205, "no_speech_prob": 0.0023223981261253357}, {"id": 193, "seek": 105664, "start": 1078.3200000000002, "end": 1083.3600000000001, "text": " the right stuff, which already exists, and apply and mixed with a lot of data and a lot of compute,", "tokens": [51448, 264, 558, 1507, 11, 597, 1217, 8198, 11, 293, 3079, 293, 7467, 365, 257, 688, 295, 1412, 293, 257, 688, 295, 14722, 11, 51700], "temperature": 0.0, "avg_logprob": -0.08055356067159901, "compression_ratio": 1.9346938775510205, "no_speech_prob": 0.0023223981261253357}, {"id": 194, "seek": 108336, "start": 1083.4399999999998, "end": 1089.4399999999998, "text": " that it will in fact work. And so that was the missing piece. It was you had the, you needed", "tokens": [50368, 300, 309, 486, 294, 1186, 589, 13, 400, 370, 300, 390, 264, 5361, 2522, 13, 467, 390, 291, 632, 264, 11, 291, 2978, 50668], "temperature": 0.0, "avg_logprob": -0.09100106057156337, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00036245217779651284}, {"id": 195, "seek": 108336, "start": 1089.4399999999998, "end": 1095.04, "text": " the data, you needed the compute which showed up in terms of GPUs, and you needed the conviction", "tokens": [50668, 264, 1412, 11, 291, 2978, 264, 14722, 597, 4712, 493, 294, 2115, 295, 18407, 82, 11, 293, 291, 2978, 264, 24837, 50948], "temperature": 0.0, "avg_logprob": -0.09100106057156337, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00036245217779651284}, {"id": 196, "seek": 108336, "start": 1095.04, "end": 1101.9199999999998, "text": " to realize that you need to mix them together. So that's really interesting. So I guess the", "tokens": [50948, 281, 4325, 300, 291, 643, 281, 2890, 552, 1214, 13, 407, 300, 311, 534, 1880, 13, 407, 286, 2041, 264, 51292], "temperature": 0.0, "avg_logprob": -0.09100106057156337, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00036245217779651284}, {"id": 197, "seek": 108336, "start": 1101.9199999999998, "end": 1108.8799999999999, "text": " presence of compute and the presence supervised data allowed the empirical evidence to do the", "tokens": [51292, 6814, 295, 14722, 293, 264, 6814, 46533, 1412, 4350, 264, 31886, 4467, 281, 360, 264, 51640], "temperature": 0.0, "avg_logprob": -0.09100106057156337, "compression_ratio": 1.744186046511628, "no_speech_prob": 0.00036245217779651284}, {"id": 198, "seek": 110888, "start": 1108.96, "end": 1113.1200000000001, "text": " convincing of the majority of the computer science community. So I guess there's a", "tokens": [50368, 24823, 295, 264, 6286, 295, 264, 3820, 3497, 1768, 13, 407, 286, 2041, 456, 311, 257, 50576], "temperature": 0.0, "avg_logprob": -0.1766676902770996, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.02330232411623001}, {"id": 199, "seek": 110888, "start": 1113.1200000000001, "end": 1122.72, "text": " key moment with Jitendra Malik and Alex, Alyosha Efros, who were very skeptical, right? And then", "tokens": [50576, 2141, 1623, 365, 508, 270, 27332, 5746, 1035, 293, 5202, 11, 27008, 329, 1641, 31840, 2635, 11, 567, 645, 588, 28601, 11, 558, 30, 400, 550, 51056], "temperature": 0.0, "avg_logprob": -0.1766676902770996, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.02330232411623001}, {"id": 200, "seek": 110888, "start": 1122.72, "end": 1128.16, "text": " there's a Jeffrey Hinton that was the opposite of skeptical. And there was a convincing moment.", "tokens": [51056, 456, 311, 257, 28721, 389, 12442, 300, 390, 264, 6182, 295, 28601, 13, 400, 456, 390, 257, 24823, 1623, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1766676902770996, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.02330232411623001}, {"id": 201, "seek": 110888, "start": 1128.16, "end": 1133.8400000000001, "text": " And I think ImageNet served as that moment. And that represented this kind of, or the big", "tokens": [51328, 400, 286, 519, 29903, 31890, 7584, 382, 300, 1623, 13, 400, 300, 10379, 341, 733, 295, 11, 420, 264, 955, 51612], "temperature": 0.0, "avg_logprob": -0.1766676902770996, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.02330232411623001}, {"id": 202, "seek": 113384, "start": 1133.84, "end": 1140.48, "text": " pillars of computer vision community, kind of the wizards got together. And then all of a sudden", "tokens": [50364, 26729, 295, 3820, 5201, 1768, 11, 733, 295, 264, 40808, 2287, 658, 1214, 13, 400, 550, 439, 295, 257, 3990, 50696], "temperature": 0.0, "avg_logprob": -0.13537271095044684, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.013416152447462082}, {"id": 203, "seek": 113384, "start": 1140.48, "end": 1146.24, "text": " there was a shift. And it's not enough for the ideas to all be there and the computer to be there.", "tokens": [50696, 456, 390, 257, 5513, 13, 400, 309, 311, 406, 1547, 337, 264, 3487, 281, 439, 312, 456, 293, 264, 3820, 281, 312, 456, 13, 50984], "temperature": 0.0, "avg_logprob": -0.13537271095044684, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.013416152447462082}, {"id": 204, "seek": 113384, "start": 1146.24, "end": 1153.04, "text": " It's for it to convince the cynicism that existed that that's interesting that people just didn't", "tokens": [50984, 467, 311, 337, 309, 281, 13447, 264, 28365, 26356, 300, 13135, 300, 300, 311, 1880, 300, 561, 445, 994, 380, 51324], "temperature": 0.0, "avg_logprob": -0.13537271095044684, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.013416152447462082}, {"id": 205, "seek": 113384, "start": 1153.04, "end": 1160.56, "text": " believe for a couple of decades. Yeah, well, but it's more than that. It's kind of when put this", "tokens": [51324, 1697, 337, 257, 1916, 295, 7878, 13, 865, 11, 731, 11, 457, 309, 311, 544, 813, 300, 13, 467, 311, 733, 295, 562, 829, 341, 51700], "temperature": 0.0, "avg_logprob": -0.13537271095044684, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.013416152447462082}, {"id": 206, "seek": 116056, "start": 1160.56, "end": 1165.44, "text": " way, it sounds like, well, you know, those silly people who didn't believe what were they missing.", "tokens": [50364, 636, 11, 309, 3263, 411, 11, 731, 11, 291, 458, 11, 729, 11774, 561, 567, 994, 380, 1697, 437, 645, 436, 5361, 13, 50608], "temperature": 0.0, "avg_logprob": -0.09072222261347322, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.007685129996389151}, {"id": 207, "seek": 116056, "start": 1165.44, "end": 1170.08, "text": " But in reality, things were confusing because neural networks really did not work on anything.", "tokens": [50608, 583, 294, 4103, 11, 721, 645, 13181, 570, 18161, 9590, 534, 630, 406, 589, 322, 1340, 13, 50840], "temperature": 0.0, "avg_logprob": -0.09072222261347322, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.007685129996389151}, {"id": 208, "seek": 116056, "start": 1170.08, "end": 1175.52, "text": " And they were not the best method on pretty much anything as well. And it was pretty rational to", "tokens": [50840, 400, 436, 645, 406, 264, 1151, 3170, 322, 1238, 709, 1340, 382, 731, 13, 400, 309, 390, 1238, 15090, 281, 51112], "temperature": 0.0, "avg_logprob": -0.09072222261347322, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.007685129996389151}, {"id": 209, "seek": 116056, "start": 1175.52, "end": 1181.6799999999998, "text": " say, yeah, this stuff doesn't have any traction. And that's why you need to have these very hard", "tokens": [51112, 584, 11, 1338, 11, 341, 1507, 1177, 380, 362, 604, 23558, 13, 400, 300, 311, 983, 291, 643, 281, 362, 613, 588, 1152, 51420], "temperature": 0.0, "avg_logprob": -0.09072222261347322, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.007685129996389151}, {"id": 210, "seek": 116056, "start": 1181.6799999999998, "end": 1187.28, "text": " tasks which are which produce undeniable evidence. And that's how we make progress. And that's why", "tokens": [51420, 9608, 597, 366, 597, 5258, 674, 268, 9364, 4467, 13, 400, 300, 311, 577, 321, 652, 4205, 13, 400, 300, 311, 983, 51700], "temperature": 0.0, "avg_logprob": -0.09072222261347322, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.007685129996389151}, {"id": 211, "seek": 118728, "start": 1187.28, "end": 1191.6, "text": " the field is making progress today, because we have these hard benchmarks, which represent true", "tokens": [50364, 264, 2519, 307, 1455, 4205, 965, 11, 570, 321, 362, 613, 1152, 43751, 11, 597, 2906, 2074, 50580], "temperature": 0.0, "avg_logprob": -0.11325664979865752, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.000687664607539773}, {"id": 212, "seek": 118728, "start": 1191.6, "end": 1199.84, "text": " progress. And so, and this is why we were able to avoid endless debate. So incredibly, you've", "tokens": [50580, 4205, 13, 400, 370, 11, 293, 341, 307, 983, 321, 645, 1075, 281, 5042, 16144, 7958, 13, 407, 6252, 11, 291, 600, 50992], "temperature": 0.0, "avg_logprob": -0.11325664979865752, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.000687664607539773}, {"id": 213, "seek": 118728, "start": 1199.84, "end": 1205.92, "text": " contributed some of the biggest recent ideas in AI in computer vision, language, natural", "tokens": [50992, 18434, 512, 295, 264, 3880, 5162, 3487, 294, 7318, 294, 3820, 5201, 11, 2856, 11, 3303, 51296], "temperature": 0.0, "avg_logprob": -0.11325664979865752, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.000687664607539773}, {"id": 214, "seek": 118728, "start": 1205.92, "end": 1212.08, "text": " language processing, reinforcement learning, sort of everything in between, maybe not GANs.", "tokens": [51296, 2856, 9007, 11, 29280, 2539, 11, 1333, 295, 1203, 294, 1296, 11, 1310, 406, 460, 1770, 82, 13, 51604], "temperature": 0.0, "avg_logprob": -0.11325664979865752, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.000687664607539773}, {"id": 215, "seek": 121208, "start": 1212.8, "end": 1217.84, "text": " Is there, there may not be a topic you haven't touched. And of course, the fundamental science", "tokens": [50400, 1119, 456, 11, 456, 815, 406, 312, 257, 4829, 291, 2378, 380, 9828, 13, 400, 295, 1164, 11, 264, 8088, 3497, 50652], "temperature": 0.0, "avg_logprob": -0.1443014144897461, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.03844539821147919}, {"id": 216, "seek": 121208, "start": 1217.84, "end": 1225.6, "text": " of deep learning. What is the difference to you between vision, language, and as in reinforcement", "tokens": [50652, 295, 2452, 2539, 13, 708, 307, 264, 2649, 281, 291, 1296, 5201, 11, 2856, 11, 293, 382, 294, 29280, 51040], "temperature": 0.0, "avg_logprob": -0.1443014144897461, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.03844539821147919}, {"id": 217, "seek": 121208, "start": 1225.6, "end": 1230.3999999999999, "text": " learning action, as learning problems? And what are the commonalities? Do you see them as all", "tokens": [51040, 2539, 3069, 11, 382, 2539, 2740, 30, 400, 437, 366, 264, 2689, 16110, 30, 1144, 291, 536, 552, 382, 439, 51280], "temperature": 0.0, "avg_logprob": -0.1443014144897461, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.03844539821147919}, {"id": 218, "seek": 121208, "start": 1230.3999999999999, "end": 1236.32, "text": " interconnected? Are they fundamentally different domains that require different approaches?", "tokens": [51280, 36611, 30, 2014, 436, 17879, 819, 25514, 300, 3651, 819, 11587, 30, 51576], "temperature": 0.0, "avg_logprob": -0.1443014144897461, "compression_ratio": 1.6651982378854626, "no_speech_prob": 0.03844539821147919}, {"id": 219, "seek": 123632, "start": 1236.8799999999999, "end": 1242.6399999999999, "text": " Okay, that's a good question. Machine learning is a field with a lot of unity, a huge amount of", "tokens": [50392, 1033, 11, 300, 311, 257, 665, 1168, 13, 22155, 2539, 307, 257, 2519, 365, 257, 688, 295, 18205, 11, 257, 2603, 2372, 295, 50680], "temperature": 0.0, "avg_logprob": -0.11783119755932409, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.011325421743094921}, {"id": 220, "seek": 123632, "start": 1242.6399999999999, "end": 1249.6799999999998, "text": " unity. In fact, what do you mean by unity, like overlap of ideas? overlap of ideas overlap of", "tokens": [50680, 18205, 13, 682, 1186, 11, 437, 360, 291, 914, 538, 18205, 11, 411, 19959, 295, 3487, 30, 19959, 295, 3487, 19959, 295, 51032], "temperature": 0.0, "avg_logprob": -0.11783119755932409, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.011325421743094921}, {"id": 221, "seek": 123632, "start": 1249.6799999999998, "end": 1254.32, "text": " principles. In fact, there's only one or two or three principles, which are very, very simple.", "tokens": [51032, 9156, 13, 682, 1186, 11, 456, 311, 787, 472, 420, 732, 420, 1045, 9156, 11, 597, 366, 588, 11, 588, 2199, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11783119755932409, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.011325421743094921}, {"id": 222, "seek": 123632, "start": 1254.32, "end": 1259.84, "text": " And then they apply in almost the same way, in almost the same way to the different modalities", "tokens": [51264, 400, 550, 436, 3079, 294, 1920, 264, 912, 636, 11, 294, 1920, 264, 912, 636, 281, 264, 819, 1072, 16110, 51540], "temperature": 0.0, "avg_logprob": -0.11783119755932409, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.011325421743094921}, {"id": 223, "seek": 123632, "start": 1259.84, "end": 1264.72, "text": " through the different problems. And that's why today, when someone writes a paper on improving", "tokens": [51540, 807, 264, 819, 2740, 13, 400, 300, 311, 983, 965, 11, 562, 1580, 13657, 257, 3035, 322, 11470, 51784], "temperature": 0.0, "avg_logprob": -0.11783119755932409, "compression_ratio": 1.823076923076923, "no_speech_prob": 0.011325421743094921}, {"id": 224, "seek": 126472, "start": 1264.72, "end": 1269.2, "text": " optimization of deep learning and vision, it improves the different NLP applications,", "tokens": [50364, 19618, 295, 2452, 2539, 293, 5201, 11, 309, 24771, 264, 819, 426, 45196, 5821, 11, 50588], "temperature": 0.0, "avg_logprob": -0.10270001277450688, "compression_ratio": 1.8202614379084967, "no_speech_prob": 0.001303515280596912}, {"id": 225, "seek": 126472, "start": 1269.2, "end": 1273.2, "text": " and it improves the different reinforcement learning applications. Reinforcement learning.", "tokens": [50588, 293, 309, 24771, 264, 819, 29280, 2539, 5821, 13, 42116, 9382, 2539, 13, 50788], "temperature": 0.0, "avg_logprob": -0.10270001277450688, "compression_ratio": 1.8202614379084967, "no_speech_prob": 0.001303515280596912}, {"id": 226, "seek": 126472, "start": 1273.2, "end": 1280.0, "text": " So I would say that computer vision and NLP are very similar to each other. Today, they differ in", "tokens": [50788, 407, 286, 576, 584, 300, 3820, 5201, 293, 426, 45196, 366, 588, 2531, 281, 1184, 661, 13, 2692, 11, 436, 743, 294, 51128], "temperature": 0.0, "avg_logprob": -0.10270001277450688, "compression_ratio": 1.8202614379084967, "no_speech_prob": 0.001303515280596912}, {"id": 227, "seek": 126472, "start": 1280.0, "end": 1284.64, "text": " that they have slightly different architectures. We use transformers in NLP, and we use convolutional", "tokens": [51128, 300, 436, 362, 4748, 819, 6331, 1303, 13, 492, 764, 4088, 433, 294, 426, 45196, 11, 293, 321, 764, 45216, 304, 51360], "temperature": 0.0, "avg_logprob": -0.10270001277450688, "compression_ratio": 1.8202614379084967, "no_speech_prob": 0.001303515280596912}, {"id": 228, "seek": 126472, "start": 1284.64, "end": 1289.76, "text": " neural networks in vision. But it's also possible that one day this will change and everything", "tokens": [51360, 18161, 9590, 294, 5201, 13, 583, 309, 311, 611, 1944, 300, 472, 786, 341, 486, 1319, 293, 1203, 51616], "temperature": 0.0, "avg_logprob": -0.10270001277450688, "compression_ratio": 1.8202614379084967, "no_speech_prob": 0.001303515280596912}, {"id": 229, "seek": 126472, "start": 1289.76, "end": 1293.84, "text": " will be unified with a single architecture. Because if you go back a few years ago in", "tokens": [51616, 486, 312, 26787, 365, 257, 2167, 9482, 13, 1436, 498, 291, 352, 646, 257, 1326, 924, 2057, 294, 51820], "temperature": 0.0, "avg_logprob": -0.10270001277450688, "compression_ratio": 1.8202614379084967, "no_speech_prob": 0.001303515280596912}, {"id": 230, "seek": 129384, "start": 1293.84, "end": 1299.76, "text": " natural language processing, there were a huge, huge number of architectures for every", "tokens": [50364, 3303, 2856, 9007, 11, 456, 645, 257, 2603, 11, 2603, 1230, 295, 6331, 1303, 337, 633, 50660], "temperature": 0.0, "avg_logprob": -0.07635604876738328, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.003585885977372527}, {"id": 231, "seek": 129384, "start": 1299.76, "end": 1306.32, "text": " different tiny problem had its own architecture. Today, there's just one transformer for all", "tokens": [50660, 819, 5870, 1154, 632, 1080, 1065, 9482, 13, 2692, 11, 456, 311, 445, 472, 31782, 337, 439, 50988], "temperature": 0.0, "avg_logprob": -0.07635604876738328, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.003585885977372527}, {"id": 232, "seek": 129384, "start": 1306.32, "end": 1310.56, "text": " those different tasks. And if you go back in time even more, you had even more and more", "tokens": [50988, 729, 819, 9608, 13, 400, 498, 291, 352, 646, 294, 565, 754, 544, 11, 291, 632, 754, 544, 293, 544, 51200], "temperature": 0.0, "avg_logprob": -0.07635604876738328, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.003585885977372527}, {"id": 233, "seek": 129384, "start": 1310.56, "end": 1316.32, "text": " fragmentation and every little problem in AI had its own little subspecialization and sub,", "tokens": [51200, 9241, 19631, 293, 633, 707, 1154, 294, 7318, 632, 1080, 1065, 707, 2090, 494, 1013, 2144, 293, 1422, 11, 51488], "temperature": 0.0, "avg_logprob": -0.07635604876738328, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.003585885977372527}, {"id": 234, "seek": 129384, "start": 1316.32, "end": 1320.8, "text": " you know, little set of collection of skills, people who would know how to engineer the features.", "tokens": [51488, 291, 458, 11, 707, 992, 295, 5765, 295, 3942, 11, 561, 567, 576, 458, 577, 281, 11403, 264, 4122, 13, 51712], "temperature": 0.0, "avg_logprob": -0.07635604876738328, "compression_ratio": 1.7538461538461538, "no_speech_prob": 0.003585885977372527}, {"id": 235, "seek": 132080, "start": 1320.8799999999999, "end": 1325.04, "text": " Now it's all been subsumed by deep learning. We have this unification. And so I expect", "tokens": [50368, 823, 309, 311, 439, 668, 2090, 28189, 538, 2452, 2539, 13, 492, 362, 341, 517, 3774, 13, 400, 370, 286, 2066, 50576], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 236, "seek": 132080, "start": 1325.68, "end": 1329.52, "text": " vision to become unified with natural language as well. Or rather, I shouldn't say expect,", "tokens": [50608, 5201, 281, 1813, 26787, 365, 3303, 2856, 382, 731, 13, 1610, 2831, 11, 286, 4659, 380, 584, 2066, 11, 50800], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 237, "seek": 132080, "start": 1329.52, "end": 1333.36, "text": " I think it's possible. I don't want to be too sure, because I think on the commercial", "tokens": [50800, 286, 519, 309, 311, 1944, 13, 286, 500, 380, 528, 281, 312, 886, 988, 11, 570, 286, 519, 322, 264, 6841, 50992], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 238, "seek": 132080, "start": 1333.36, "end": 1337.28, "text": " neural network, it is very computationally efficient. Arell is different. Arell doesn't", "tokens": [50992, 18161, 3209, 11, 309, 307, 588, 24903, 379, 7148, 13, 2014, 285, 307, 819, 13, 2014, 285, 1177, 380, 51188], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 239, "seek": 132080, "start": 1337.28, "end": 1341.36, "text": " require slightly different techniques, because you really do need to take action. You really", "tokens": [51188, 3651, 4748, 819, 7512, 11, 570, 291, 534, 360, 643, 281, 747, 3069, 13, 509, 534, 51392], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 240, "seek": 132080, "start": 1341.36, "end": 1346.3999999999999, "text": " need to do something about exploration, your variance is much higher. But I think there", "tokens": [51392, 643, 281, 360, 746, 466, 16197, 11, 428, 21977, 307, 709, 2946, 13, 583, 286, 519, 456, 51644], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 241, "seek": 132080, "start": 1346.3999999999999, "end": 1350.24, "text": " is a lot of unity even there. And I would expect, for example, that at some point, there will be", "tokens": [51644, 307, 257, 688, 295, 18205, 754, 456, 13, 400, 286, 576, 2066, 11, 337, 1365, 11, 300, 412, 512, 935, 11, 456, 486, 312, 51836], "temperature": 0.0, "avg_logprob": -0.15502469679888556, "compression_ratio": 1.742382271468144, "no_speech_prob": 0.002755386522039771}, {"id": 242, "seek": 135024, "start": 1350.24, "end": 1356.08, "text": " some broader unification between Arell and supervised learning, where somehow the Arell", "tokens": [50364, 512, 13227, 517, 3774, 1296, 2014, 285, 293, 46533, 2539, 11, 689, 6063, 264, 2014, 285, 50656], "temperature": 0.0, "avg_logprob": -0.1396160482246185, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0007666054298169911}, {"id": 243, "seek": 135024, "start": 1356.08, "end": 1360.64, "text": " will be making decisions to make the supervised learning go better. And it will be, I imagine", "tokens": [50656, 486, 312, 1455, 5327, 281, 652, 264, 46533, 2539, 352, 1101, 13, 400, 309, 486, 312, 11, 286, 3811, 50884], "temperature": 0.0, "avg_logprob": -0.1396160482246185, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0007666054298169911}, {"id": 244, "seek": 135024, "start": 1360.64, "end": 1365.1200000000001, "text": " one big black box and you just throw every, you know, you shovel, shovel things into it. And it", "tokens": [50884, 472, 955, 2211, 2424, 293, 291, 445, 3507, 633, 11, 291, 458, 11, 291, 29789, 11, 29789, 721, 666, 309, 13, 400, 309, 51108], "temperature": 0.0, "avg_logprob": -0.1396160482246185, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0007666054298169911}, {"id": 245, "seek": 135024, "start": 1365.1200000000001, "end": 1369.84, "text": " just figures out what to do with whatever you shovel in it. I mean, reinforcement learning has", "tokens": [51108, 445, 9624, 484, 437, 281, 360, 365, 2035, 291, 29789, 294, 309, 13, 286, 914, 11, 29280, 2539, 575, 51344], "temperature": 0.0, "avg_logprob": -0.1396160482246185, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0007666054298169911}, {"id": 246, "seek": 135024, "start": 1369.84, "end": 1377.6, "text": " some aspects of language and vision combined, almost, there's elements of a long term memory", "tokens": [51344, 512, 7270, 295, 2856, 293, 5201, 9354, 11, 1920, 11, 456, 311, 4959, 295, 257, 938, 1433, 4675, 51732], "temperature": 0.0, "avg_logprob": -0.1396160482246185, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0007666054298169911}, {"id": 247, "seek": 137760, "start": 1377.6, "end": 1382.24, "text": " that you should be utilizing, and there's elements of a really rich sensory space.", "tokens": [50364, 300, 291, 820, 312, 26775, 11, 293, 456, 311, 4959, 295, 257, 534, 4593, 27233, 1901, 13, 50596], "temperature": 0.0, "avg_logprob": -0.1228170569883574, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.008842370472848415}, {"id": 248, "seek": 137760, "start": 1382.9599999999998, "end": 1389.04, "text": " So it seems like the, it's like the union of the two or something like that. I'd say something", "tokens": [50632, 407, 309, 2544, 411, 264, 11, 309, 311, 411, 264, 11671, 295, 264, 732, 420, 746, 411, 300, 13, 286, 1116, 584, 746, 50936], "temperature": 0.0, "avg_logprob": -0.1228170569883574, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.008842370472848415}, {"id": 249, "seek": 137760, "start": 1389.04, "end": 1394.7199999999998, "text": " slightly different. I'd say that reinforcement learning is neither, but it naturally interfaces", "tokens": [50936, 4748, 819, 13, 286, 1116, 584, 300, 29280, 2539, 307, 9662, 11, 457, 309, 8195, 28416, 51220], "temperature": 0.0, "avg_logprob": -0.1228170569883574, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.008842370472848415}, {"id": 250, "seek": 137760, "start": 1394.7199999999998, "end": 1399.6, "text": " and integrates with the two of them. Do you think action is fundamentally different? So yeah,", "tokens": [51220, 293, 3572, 1024, 365, 264, 732, 295, 552, 13, 1144, 291, 519, 3069, 307, 17879, 819, 30, 407, 1338, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1228170569883574, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.008842370472848415}, {"id": 251, "seek": 137760, "start": 1399.6, "end": 1406.8, "text": " what is interesting about, what is unique about policy of learning to act? Well, so one example,", "tokens": [51464, 437, 307, 1880, 466, 11, 437, 307, 3845, 466, 3897, 295, 2539, 281, 605, 30, 1042, 11, 370, 472, 1365, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1228170569883574, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.008842370472848415}, {"id": 252, "seek": 140680, "start": 1406.8, "end": 1412.48, "text": " for instance, is that when you learn to act, you are fundamentally in a non stationary world.", "tokens": [50364, 337, 5197, 11, 307, 300, 562, 291, 1466, 281, 605, 11, 291, 366, 17879, 294, 257, 2107, 30452, 1002, 13, 50648], "temperature": 0.0, "avg_logprob": -0.14344567115153742, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.002047778805717826}, {"id": 253, "seek": 140680, "start": 1413.12, "end": 1420.48, "text": " Because as your actions change, the things you see start changing. You, you experience the world", "tokens": [50680, 1436, 382, 428, 5909, 1319, 11, 264, 721, 291, 536, 722, 4473, 13, 509, 11, 291, 1752, 264, 1002, 51048], "temperature": 0.0, "avg_logprob": -0.14344567115153742, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.002047778805717826}, {"id": 254, "seek": 140680, "start": 1420.48, "end": 1425.04, "text": " in a different way. And this is not the case for the more traditional static problem where you have", "tokens": [51048, 294, 257, 819, 636, 13, 400, 341, 307, 406, 264, 1389, 337, 264, 544, 5164, 13437, 1154, 689, 291, 362, 51276], "temperature": 0.0, "avg_logprob": -0.14344567115153742, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.002047778805717826}, {"id": 255, "seek": 140680, "start": 1425.04, "end": 1430.32, "text": " a some distribution and you just apply a model to that distribution. You think it's a fundamentally", "tokens": [51276, 257, 512, 7316, 293, 291, 445, 3079, 257, 2316, 281, 300, 7316, 13, 509, 519, 309, 311, 257, 17879, 51540], "temperature": 0.0, "avg_logprob": -0.14344567115153742, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.002047778805717826}, {"id": 256, "seek": 140680, "start": 1430.32, "end": 1435.52, "text": " different problem or is it just a more difficult general, it's a generalization of the problem", "tokens": [51540, 819, 1154, 420, 307, 309, 445, 257, 544, 2252, 2674, 11, 309, 311, 257, 2674, 2144, 295, 264, 1154, 51800], "temperature": 0.0, "avg_logprob": -0.14344567115153742, "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.002047778805717826}, {"id": 257, "seek": 143552, "start": 1435.52, "end": 1440.16, "text": " of understanding. I mean, it's, it's, it's a question of definitions almost. There is a huge", "tokens": [50364, 295, 3701, 13, 286, 914, 11, 309, 311, 11, 309, 311, 11, 309, 311, 257, 1168, 295, 21988, 1920, 13, 821, 307, 257, 2603, 50596], "temperature": 0.0, "avg_logprob": -0.14563915377757589, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.007115171290934086}, {"id": 258, "seek": 143552, "start": 1440.16, "end": 1444.8, "text": " amount of commonality for sure. You take gradients, you try, you take gradients, we try to approximate", "tokens": [50596, 2372, 295, 2689, 1860, 337, 988, 13, 509, 747, 2771, 2448, 11, 291, 853, 11, 291, 747, 2771, 2448, 11, 321, 853, 281, 30874, 50828], "temperature": 0.0, "avg_logprob": -0.14563915377757589, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.007115171290934086}, {"id": 259, "seek": 143552, "start": 1444.8, "end": 1448.56, "text": " gradients in both cases. In some case, in the case of reinforcement learning, you have", "tokens": [50828, 2771, 2448, 294, 1293, 3331, 13, 682, 512, 1389, 11, 294, 264, 1389, 295, 29280, 2539, 11, 291, 362, 51016], "temperature": 0.0, "avg_logprob": -0.14563915377757589, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.007115171290934086}, {"id": 260, "seek": 143552, "start": 1448.56, "end": 1453.76, "text": " some tools to reduce the variance of the gradients. You do that. There's lots of commonality,", "tokens": [51016, 512, 3873, 281, 5407, 264, 21977, 295, 264, 2771, 2448, 13, 509, 360, 300, 13, 821, 311, 3195, 295, 2689, 1860, 11, 51276], "temperature": 0.0, "avg_logprob": -0.14563915377757589, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.007115171290934086}, {"id": 261, "seek": 143552, "start": 1453.76, "end": 1458.32, "text": " use the same neural net in both cases. You compute the gradient, you apply atom in both cases.", "tokens": [51276, 764, 264, 912, 18161, 2533, 294, 1293, 3331, 13, 509, 14722, 264, 16235, 11, 291, 3079, 12018, 294, 1293, 3331, 13, 51504], "temperature": 0.0, "avg_logprob": -0.14563915377757589, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.007115171290934086}, {"id": 262, "seek": 145832, "start": 1458.3999999999999, "end": 1467.52, "text": " So, I mean, there's lots in common for sure, but there are some small differences which are not", "tokens": [50368, 407, 11, 286, 914, 11, 456, 311, 3195, 294, 2689, 337, 988, 11, 457, 456, 366, 512, 1359, 7300, 597, 366, 406, 50824], "temperature": 0.0, "avg_logprob": -0.13916366978695519, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.004329386632889509}, {"id": 263, "seek": 145832, "start": 1467.52, "end": 1471.36, "text": " completely insignificant. It's really just a matter of your, of your point of view, what", "tokens": [50824, 2584, 43685, 13, 467, 311, 534, 445, 257, 1871, 295, 428, 11, 295, 428, 935, 295, 1910, 11, 437, 51016], "temperature": 0.0, "avg_logprob": -0.13916366978695519, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.004329386632889509}, {"id": 264, "seek": 145832, "start": 1471.36, "end": 1477.12, "text": " frame of reference you, what, how much do you want to zoom in or out as you look at these problems?", "tokens": [51016, 3920, 295, 6408, 291, 11, 437, 11, 577, 709, 360, 291, 528, 281, 8863, 294, 420, 484, 382, 291, 574, 412, 613, 2740, 30, 51304], "temperature": 0.0, "avg_logprob": -0.13916366978695519, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.004329386632889509}, {"id": 265, "seek": 145832, "start": 1477.12, "end": 1482.72, "text": " Which problem do you think is harder? So people like no Chomsky believe that language is fundamental", "tokens": [51304, 3013, 1154, 360, 291, 519, 307, 6081, 30, 407, 561, 411, 572, 761, 4785, 4133, 1697, 300, 2856, 307, 8088, 51584], "temperature": 0.0, "avg_logprob": -0.13916366978695519, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.004329386632889509}, {"id": 266, "seek": 148272, "start": 1482.8, "end": 1489.2, "text": " to everything. So it underlies everything. Do you think language understanding is harder than", "tokens": [50368, 281, 1203, 13, 407, 309, 833, 24119, 1203, 13, 1144, 291, 519, 2856, 3701, 307, 6081, 813, 50688], "temperature": 0.0, "avg_logprob": -0.09966965675354005, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.049553852528333664}, {"id": 267, "seek": 148272, "start": 1489.2, "end": 1494.64, "text": " visual scene understanding or vice versa? I think that asking if a problem is hard is", "tokens": [50688, 5056, 4145, 3701, 420, 11964, 25650, 30, 286, 519, 300, 3365, 498, 257, 1154, 307, 1152, 307, 50960], "temperature": 0.0, "avg_logprob": -0.09966965675354005, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.049553852528333664}, {"id": 268, "seek": 148272, "start": 1494.64, "end": 1498.48, "text": " slightly wrong. I think the question is a little bit wrong, and I want to explain why.", "tokens": [50960, 4748, 2085, 13, 286, 519, 264, 1168, 307, 257, 707, 857, 2085, 11, 293, 286, 528, 281, 2903, 983, 13, 51152], "temperature": 0.0, "avg_logprob": -0.09966965675354005, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.049553852528333664}, {"id": 269, "seek": 148272, "start": 1499.44, "end": 1502.16, "text": " So what does it mean for a problem to be hard?", "tokens": [51200, 407, 437, 775, 309, 914, 337, 257, 1154, 281, 312, 1152, 30, 51336], "temperature": 0.0, "avg_logprob": -0.09966965675354005, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.049553852528333664}, {"id": 270, "seek": 148272, "start": 1504.24, "end": 1510.56, "text": " Okay, the non-interesting, dumb answer to that is there's a, there's a benchmark,", "tokens": [51440, 1033, 11, 264, 2107, 12, 5106, 8714, 11, 10316, 1867, 281, 300, 307, 456, 311, 257, 11, 456, 311, 257, 18927, 11, 51756], "temperature": 0.0, "avg_logprob": -0.09966965675354005, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.049553852528333664}, {"id": 271, "seek": 151056, "start": 1510.56, "end": 1516.72, "text": " and there's a human level performance on that benchmark. And how is the effort required to", "tokens": [50364, 293, 456, 311, 257, 1952, 1496, 3389, 322, 300, 18927, 13, 400, 577, 307, 264, 4630, 4739, 281, 50672], "temperature": 0.0, "avg_logprob": -0.1258170951125968, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0026690412778407335}, {"id": 272, "seek": 151056, "start": 1516.72, "end": 1522.0, "text": " reach the human level benchmark? So from the perspective of how much until we get to human", "tokens": [50672, 2524, 264, 1952, 1496, 18927, 30, 407, 490, 264, 4585, 295, 577, 709, 1826, 321, 483, 281, 1952, 50936], "temperature": 0.0, "avg_logprob": -0.1258170951125968, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0026690412778407335}, {"id": 273, "seek": 151056, "start": 1522.0, "end": 1528.72, "text": " level on a very good benchmark? Yeah, like some, I understand what you mean by that.", "tokens": [50936, 1496, 322, 257, 588, 665, 18927, 30, 865, 11, 411, 512, 11, 286, 1223, 437, 291, 914, 538, 300, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1258170951125968, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0026690412778407335}, {"id": 274, "seek": 151056, "start": 1528.72, "end": 1533.04, "text": " So what I was going to say that a lot of it depends on, you know, once you solve a problem,", "tokens": [51272, 407, 437, 286, 390, 516, 281, 584, 300, 257, 688, 295, 309, 5946, 322, 11, 291, 458, 11, 1564, 291, 5039, 257, 1154, 11, 51488], "temperature": 0.0, "avg_logprob": -0.1258170951125968, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0026690412778407335}, {"id": 275, "seek": 151056, "start": 1533.04, "end": 1537.6799999999998, "text": " it stops being hard. And that's, that's always true. And so, but if something is hard or not,", "tokens": [51488, 309, 10094, 885, 1152, 13, 400, 300, 311, 11, 300, 311, 1009, 2074, 13, 400, 370, 11, 457, 498, 746, 307, 1152, 420, 406, 11, 51720], "temperature": 0.0, "avg_logprob": -0.1258170951125968, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.0026690412778407335}, {"id": 276, "seek": 153768, "start": 1537.68, "end": 1542.8, "text": " depends on what our tools can do today. So you know, you say today, through human level,", "tokens": [50364, 5946, 322, 437, 527, 3873, 393, 360, 965, 13, 407, 291, 458, 11, 291, 584, 965, 11, 807, 1952, 1496, 11, 50620], "temperature": 0.0, "avg_logprob": -0.16175381796700614, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.002630471019074321}, {"id": 277, "seek": 153768, "start": 1543.6000000000001, "end": 1549.04, "text": " language understanding and visual perception are hard in the sense that there is no way of", "tokens": [50660, 2856, 3701, 293, 5056, 12860, 366, 1152, 294, 264, 2020, 300, 456, 307, 572, 636, 295, 50932], "temperature": 0.0, "avg_logprob": -0.16175381796700614, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.002630471019074321}, {"id": 278, "seek": 153768, "start": 1549.04, "end": 1553.04, "text": " solving the problem completely in the next three months. Right. So I agree with that statement.", "tokens": [50932, 12606, 264, 1154, 2584, 294, 264, 958, 1045, 2493, 13, 1779, 13, 407, 286, 3986, 365, 300, 5629, 13, 51132], "temperature": 0.0, "avg_logprob": -0.16175381796700614, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.002630471019074321}, {"id": 279, "seek": 153768, "start": 1553.8400000000001, "end": 1557.44, "text": " Beyond that, I'm just, I'll be my, my guess would be as good as yours. I don't know.", "tokens": [51172, 19707, 300, 11, 286, 478, 445, 11, 286, 603, 312, 452, 11, 452, 2041, 576, 312, 382, 665, 382, 6342, 13, 286, 500, 380, 458, 13, 51352], "temperature": 0.0, "avg_logprob": -0.16175381796700614, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.002630471019074321}, {"id": 280, "seek": 153768, "start": 1557.44, "end": 1562.72, "text": " Oh, okay. So you don't have a fundamental intuition about how hard language understanding is.", "tokens": [51352, 876, 11, 1392, 13, 407, 291, 500, 380, 362, 257, 8088, 24002, 466, 577, 1152, 2856, 3701, 307, 13, 51616], "temperature": 0.0, "avg_logprob": -0.16175381796700614, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.002630471019074321}, {"id": 281, "seek": 153768, "start": 1562.72, "end": 1567.04, "text": " I think I, I know I changed my mind. I'd say language is probably going to be harder. I mean,", "tokens": [51616, 286, 519, 286, 11, 286, 458, 286, 3105, 452, 1575, 13, 286, 1116, 584, 2856, 307, 1391, 516, 281, 312, 6081, 13, 286, 914, 11, 51832], "temperature": 0.0, "avg_logprob": -0.16175381796700614, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.002630471019074321}, {"id": 282, "seek": 156704, "start": 1567.04, "end": 1572.8799999999999, "text": " it depends on how you define it. Like if you mean absolute top notch 100% language understanding,", "tokens": [50364, 309, 5946, 322, 577, 291, 6964, 309, 13, 1743, 498, 291, 914, 8236, 1192, 26109, 2319, 4, 2856, 3701, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11225984904391706, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.0024323142133653164}, {"id": 283, "seek": 156704, "start": 1572.8799999999999, "end": 1579.2, "text": " I'll go with language. So, but then if I show you a piece of paper with letters on it, is that", "tokens": [50656, 286, 603, 352, 365, 2856, 13, 407, 11, 457, 550, 498, 286, 855, 291, 257, 2522, 295, 3035, 365, 7825, 322, 309, 11, 307, 300, 50972], "temperature": 0.0, "avg_logprob": -0.11225984904391706, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.0024323142133653164}, {"id": 284, "seek": 156704, "start": 1579.92, "end": 1584.96, "text": " you see what I mean? So you have a vision system, you say it's the best human level vision system.", "tokens": [51008, 291, 536, 437, 286, 914, 30, 407, 291, 362, 257, 5201, 1185, 11, 291, 584, 309, 311, 264, 1151, 1952, 1496, 5201, 1185, 13, 51260], "temperature": 0.0, "avg_logprob": -0.11225984904391706, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.0024323142133653164}, {"id": 285, "seek": 156704, "start": 1584.96, "end": 1590.56, "text": " I show you, I open a book and I show you letters. Will it understand how these letters form into", "tokens": [51260, 286, 855, 291, 11, 286, 1269, 257, 1446, 293, 286, 855, 291, 7825, 13, 3099, 309, 1223, 577, 613, 7825, 1254, 666, 51540], "temperature": 0.0, "avg_logprob": -0.11225984904391706, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.0024323142133653164}, {"id": 286, "seek": 156704, "start": 1590.56, "end": 1594.56, "text": " word and sentences and meaning is this part of the vision problem? Where does vision end and", "tokens": [51540, 1349, 293, 16579, 293, 3620, 307, 341, 644, 295, 264, 5201, 1154, 30, 2305, 775, 5201, 917, 293, 51740], "temperature": 0.0, "avg_logprob": -0.11225984904391706, "compression_ratio": 1.7427536231884058, "no_speech_prob": 0.0024323142133653164}, {"id": 287, "seek": 159456, "start": 1594.56, "end": 1599.28, "text": " language begin? Yeah. So Chomsky would say it starts at language. So vision is just a little", "tokens": [50364, 2856, 1841, 30, 865, 13, 407, 761, 4785, 4133, 576, 584, 309, 3719, 412, 2856, 13, 407, 5201, 307, 445, 257, 707, 50600], "temperature": 0.0, "avg_logprob": -0.1004518884601015, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0008425042033195496}, {"id": 288, "seek": 159456, "start": 1599.28, "end": 1606.8799999999999, "text": " example of the kind of structure and, you know, fundamental hierarchy of ideas that's already", "tokens": [50600, 1365, 295, 264, 733, 295, 3877, 293, 11, 291, 458, 11, 8088, 22333, 295, 3487, 300, 311, 1217, 50980], "temperature": 0.0, "avg_logprob": -0.1004518884601015, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0008425042033195496}, {"id": 289, "seek": 159456, "start": 1606.8799999999999, "end": 1615.36, "text": " represented in our brain. Somehow that's represented through language. But where does vision stop and", "tokens": [50980, 10379, 294, 527, 3567, 13, 28357, 300, 311, 10379, 807, 2856, 13, 583, 689, 775, 5201, 1590, 293, 51404], "temperature": 0.0, "avg_logprob": -0.1004518884601015, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.0008425042033195496}, {"id": 290, "seek": 161536, "start": 1615.36, "end": 1629.28, "text": " language begin? That's a really interesting question. So one possibility is that it's", "tokens": [50364, 2856, 1841, 30, 663, 311, 257, 534, 1880, 1168, 13, 407, 472, 7959, 307, 300, 309, 311, 51060], "temperature": 0.0, "avg_logprob": -0.0674703007652646, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.1022421345114708}, {"id": 291, "seek": 161536, "start": 1629.28, "end": 1636.24, "text": " impossible to achieve really deep understanding in either images or language without basically", "tokens": [51060, 6243, 281, 4584, 534, 2452, 3701, 294, 2139, 5267, 420, 2856, 1553, 1936, 51408], "temperature": 0.0, "avg_logprob": -0.0674703007652646, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.1022421345114708}, {"id": 292, "seek": 161536, "start": 1636.24, "end": 1642.0, "text": " using the same kind of system. So you're going to get the other for free. I think, I think it's", "tokens": [51408, 1228, 264, 912, 733, 295, 1185, 13, 407, 291, 434, 516, 281, 483, 264, 661, 337, 1737, 13, 286, 519, 11, 286, 519, 309, 311, 51696], "temperature": 0.0, "avg_logprob": -0.0674703007652646, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.1022421345114708}, {"id": 293, "seek": 164200, "start": 1642.0, "end": 1646.56, "text": " pretty likely that yes, if we can get one, our machine learning is probably that good that we", "tokens": [50364, 1238, 3700, 300, 2086, 11, 498, 321, 393, 483, 472, 11, 527, 3479, 2539, 307, 1391, 300, 665, 300, 321, 50592], "temperature": 0.0, "avg_logprob": -0.15734652519226075, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.018529264256358147}, {"id": 294, "seek": 164200, "start": 1646.56, "end": 1654.08, "text": " can get the other. But it's not 100. I'm not 100% sure. And also, I think a lot of it really does", "tokens": [50592, 393, 483, 264, 661, 13, 583, 309, 311, 406, 2319, 13, 286, 478, 406, 2319, 4, 988, 13, 400, 611, 11, 286, 519, 257, 688, 295, 309, 534, 775, 50968], "temperature": 0.0, "avg_logprob": -0.15734652519226075, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.018529264256358147}, {"id": 295, "seek": 164200, "start": 1654.08, "end": 1662.4, "text": " depend on your definitions. Definitions of like perfect vision. Because reading is vision, but", "tokens": [50968, 5672, 322, 428, 21988, 13, 46245, 2451, 295, 411, 2176, 5201, 13, 1436, 3760, 307, 5201, 11, 457, 51384], "temperature": 0.0, "avg_logprob": -0.15734652519226075, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.018529264256358147}, {"id": 296, "seek": 164200, "start": 1662.4, "end": 1669.52, "text": " should it count? Yeah, to me, my definition is if a system looked at an image and then a system", "tokens": [51384, 820, 309, 1207, 30, 865, 11, 281, 385, 11, 452, 7123, 307, 498, 257, 1185, 2956, 412, 364, 3256, 293, 550, 257, 1185, 51740], "temperature": 0.0, "avg_logprob": -0.15734652519226075, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.018529264256358147}, {"id": 297, "seek": 166952, "start": 1670.48, "end": 1677.04, "text": " looked at a piece of text, and then told me something about that. And I was really impressed.", "tokens": [50412, 2956, 412, 257, 2522, 295, 2487, 11, 293, 550, 1907, 385, 746, 466, 300, 13, 400, 286, 390, 534, 11679, 13, 50740], "temperature": 0.0, "avg_logprob": -0.09183738795855573, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.013633367605507374}, {"id": 298, "seek": 166952, "start": 1678.24, "end": 1682.08, "text": " That's relative. You'll be impressed for half an hour. And then you're going to say, well,", "tokens": [50800, 663, 311, 4972, 13, 509, 603, 312, 11679, 337, 1922, 364, 1773, 13, 400, 550, 291, 434, 516, 281, 584, 11, 731, 11, 50992], "temperature": 0.0, "avg_logprob": -0.09183738795855573, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.013633367605507374}, {"id": 299, "seek": 166952, "start": 1682.08, "end": 1686.16, "text": " I mean, all the systems do that. But here's the thing they don't do. Yeah, but I don't have that", "tokens": [50992, 286, 914, 11, 439, 264, 3652, 360, 300, 13, 583, 510, 311, 264, 551, 436, 500, 380, 360, 13, 865, 11, 457, 286, 500, 380, 362, 300, 51196], "temperature": 0.0, "avg_logprob": -0.09183738795855573, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.013633367605507374}, {"id": 300, "seek": 166952, "start": 1686.16, "end": 1693.2, "text": " with humans. Humans continue to impress me. Is that true? Well, the ones, okay, so I'm a fan of", "tokens": [51196, 365, 6255, 13, 35809, 2354, 281, 6729, 385, 13, 1119, 300, 2074, 30, 1042, 11, 264, 2306, 11, 1392, 11, 370, 286, 478, 257, 3429, 295, 51548], "temperature": 0.0, "avg_logprob": -0.09183738795855573, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.013633367605507374}, {"id": 301, "seek": 166952, "start": 1693.2, "end": 1698.96, "text": " monogamy. So I like the idea of marrying somebody being with them for several decades. So I believe", "tokens": [51548, 1108, 664, 7804, 13, 407, 286, 411, 264, 1558, 295, 36376, 2618, 885, 365, 552, 337, 2940, 7878, 13, 407, 286, 1697, 51836], "temperature": 0.0, "avg_logprob": -0.09183738795855573, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.013633367605507374}, {"id": 302, "seek": 169896, "start": 1698.96, "end": 1705.04, "text": " in the fact that, yes, it's possible to have somebody continuously giving you pleasurable,", "tokens": [50364, 294, 264, 1186, 300, 11, 2086, 11, 309, 311, 1944, 281, 362, 2618, 15684, 2902, 291, 35122, 25863, 11, 50668], "temperature": 0.0, "avg_logprob": -0.14058571431174208, "compression_ratio": 1.52, "no_speech_prob": 0.0032722540199756622}, {"id": 303, "seek": 169896, "start": 1705.76, "end": 1711.1200000000001, "text": " interesting, witty, new ideas, friends. Yeah, I think so. They continue to surprise you.", "tokens": [50704, 1880, 11, 261, 10016, 11, 777, 3487, 11, 1855, 13, 865, 11, 286, 519, 370, 13, 814, 2354, 281, 6365, 291, 13, 50972], "temperature": 0.0, "avg_logprob": -0.14058571431174208, "compression_ratio": 1.52, "no_speech_prob": 0.0032722540199756622}, {"id": 304, "seek": 169896, "start": 1712.0, "end": 1724.48, "text": " The surprise, it's that injection of randomness seems to be a nice source of continued", "tokens": [51016, 440, 6365, 11, 309, 311, 300, 22873, 295, 4974, 1287, 2544, 281, 312, 257, 1481, 4009, 295, 7014, 51640], "temperature": 0.0, "avg_logprob": -0.14058571431174208, "compression_ratio": 1.52, "no_speech_prob": 0.0032722540199756622}, {"id": 305, "seek": 172448, "start": 1725.1200000000001, "end": 1734.8, "text": " inspiration, like the wit, the humor. I think, yeah, that would be, it's a very subjective test,", "tokens": [50396, 10249, 11, 411, 264, 32161, 11, 264, 14318, 13, 286, 519, 11, 1338, 11, 300, 576, 312, 11, 309, 311, 257, 588, 25972, 1500, 11, 50880], "temperature": 0.0, "avg_logprob": -0.14851529022742962, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.011842994950711727}, {"id": 306, "seek": 172448, "start": 1734.8, "end": 1740.64, "text": " but I think if you have enough humans in the room. Yeah, I understand what you mean. Yeah,", "tokens": [50880, 457, 286, 519, 498, 291, 362, 1547, 6255, 294, 264, 1808, 13, 865, 11, 286, 1223, 437, 291, 914, 13, 865, 11, 51172], "temperature": 0.0, "avg_logprob": -0.14851529022742962, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.011842994950711727}, {"id": 307, "seek": 172448, "start": 1740.64, "end": 1743.84, "text": " I feel like I misunderstood what you meant by impressing you. I thought you meant to", "tokens": [51172, 286, 841, 411, 286, 33870, 437, 291, 4140, 538, 6729, 278, 291, 13, 286, 1194, 291, 4140, 281, 51332], "temperature": 0.0, "avg_logprob": -0.14851529022742962, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.011842994950711727}, {"id": 308, "seek": 172448, "start": 1743.84, "end": 1750.8, "text": " impress you with its intelligence, with how valid understands an image. I thought you meant", "tokens": [51332, 6729, 291, 365, 1080, 7599, 11, 365, 577, 7363, 15146, 364, 3256, 13, 286, 1194, 291, 4140, 51680], "temperature": 0.0, "avg_logprob": -0.14851529022742962, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.011842994950711727}, {"id": 309, "seek": 172448, "start": 1750.8, "end": 1753.84, "text": " something like, I'm going to show you a really complicated image and it's going to get it right,", "tokens": [51680, 746, 411, 11, 286, 478, 516, 281, 855, 291, 257, 534, 6179, 3256, 293, 309, 311, 516, 281, 483, 309, 558, 11, 51832], "temperature": 0.0, "avg_logprob": -0.14851529022742962, "compression_ratio": 1.8514056224899598, "no_speech_prob": 0.011842994950711727}, {"id": 310, "seek": 175384, "start": 1753.9199999999998, "end": 1759.4399999999998, "text": " and you're going to say, wow, that's really cool, the systems of January 2020 have not been doing", "tokens": [50368, 293, 291, 434, 516, 281, 584, 11, 6076, 11, 300, 311, 534, 1627, 11, 264, 3652, 295, 7061, 4808, 362, 406, 668, 884, 50644], "temperature": 0.0, "avg_logprob": -0.09840808528484685, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00818099919706583}, {"id": 311, "seek": 175384, "start": 1759.4399999999998, "end": 1766.0, "text": " that. Yeah, I think it all boils down to the reason people click like on stuff on the internet,", "tokens": [50644, 300, 13, 865, 11, 286, 519, 309, 439, 35049, 760, 281, 264, 1778, 561, 2052, 411, 322, 1507, 322, 264, 4705, 11, 50972], "temperature": 0.0, "avg_logprob": -0.09840808528484685, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00818099919706583}, {"id": 312, "seek": 175384, "start": 1766.0, "end": 1774.32, "text": " which is like it makes them laugh. So it's like humor or wit or insight. I'm sure we'll get that", "tokens": [50972, 597, 307, 411, 309, 1669, 552, 5801, 13, 407, 309, 311, 411, 14318, 420, 32161, 420, 11269, 13, 286, 478, 988, 321, 603, 483, 300, 51388], "temperature": 0.0, "avg_logprob": -0.09840808528484685, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00818099919706583}, {"id": 313, "seek": 175384, "start": 1774.32, "end": 1781.9199999999998, "text": " as well. So forgive the romanticized question, but looking back to you, what is the most beautiful", "tokens": [51388, 382, 731, 13, 407, 10718, 264, 13590, 1602, 1168, 11, 457, 1237, 646, 281, 291, 11, 437, 307, 264, 881, 2238, 51768], "temperature": 0.0, "avg_logprob": -0.09840808528484685, "compression_ratio": 1.531496062992126, "no_speech_prob": 0.00818099919706583}, {"id": 314, "seek": 178192, "start": 1781.92, "end": 1787.6000000000001, "text": " or surprising idea in deep learning or AI in general you've come across? So I think the most", "tokens": [50364, 420, 8830, 1558, 294, 2452, 2539, 420, 7318, 294, 2674, 291, 600, 808, 2108, 30, 407, 286, 519, 264, 881, 50648], "temperature": 0.0, "avg_logprob": -0.10873765592221861, "compression_ratio": 1.82, "no_speech_prob": 0.0009398055262863636}, {"id": 315, "seek": 178192, "start": 1787.6000000000001, "end": 1792.5600000000002, "text": " beautiful thing about deep learning is that it actually works. And I mean it because you got", "tokens": [50648, 2238, 551, 466, 2452, 2539, 307, 300, 309, 767, 1985, 13, 400, 286, 914, 309, 570, 291, 658, 50896], "temperature": 0.0, "avg_logprob": -0.10873765592221861, "compression_ratio": 1.82, "no_speech_prob": 0.0009398055262863636}, {"id": 316, "seek": 178192, "start": 1792.5600000000002, "end": 1796.24, "text": " these ideas, you got a little neural network, you got the back propagation algorithm.", "tokens": [50896, 613, 3487, 11, 291, 658, 257, 707, 18161, 3209, 11, 291, 658, 264, 646, 38377, 9284, 13, 51080], "temperature": 0.0, "avg_logprob": -0.10873765592221861, "compression_ratio": 1.82, "no_speech_prob": 0.0009398055262863636}, {"id": 317, "seek": 178192, "start": 1798.8000000000002, "end": 1802.48, "text": " And then you got some theories as to, you know, this is kind of like the brain. So maybe if you", "tokens": [51208, 400, 550, 291, 658, 512, 13667, 382, 281, 11, 291, 458, 11, 341, 307, 733, 295, 411, 264, 3567, 13, 407, 1310, 498, 291, 51392], "temperature": 0.0, "avg_logprob": -0.10873765592221861, "compression_ratio": 1.82, "no_speech_prob": 0.0009398055262863636}, {"id": 318, "seek": 178192, "start": 1802.48, "end": 1806.3200000000002, "text": " make it large, if you make the neural network large and you train a lot of data, then it will", "tokens": [51392, 652, 309, 2416, 11, 498, 291, 652, 264, 18161, 3209, 2416, 293, 291, 3847, 257, 688, 295, 1412, 11, 550, 309, 486, 51584], "temperature": 0.0, "avg_logprob": -0.10873765592221861, "compression_ratio": 1.82, "no_speech_prob": 0.0009398055262863636}, {"id": 319, "seek": 178192, "start": 1807.8400000000001, "end": 1811.28, "text": " do the same function that the brain does. And it turns out to be true. That's crazy.", "tokens": [51660, 360, 264, 912, 2445, 300, 264, 3567, 775, 13, 400, 309, 4523, 484, 281, 312, 2074, 13, 663, 311, 3219, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10873765592221861, "compression_ratio": 1.82, "no_speech_prob": 0.0009398055262863636}, {"id": 320, "seek": 181192, "start": 1812.4, "end": 1815.92, "text": " And now we just train these neural networks and you make them larger and they keep getting better.", "tokens": [50388, 400, 586, 321, 445, 3847, 613, 18161, 9590, 293, 291, 652, 552, 4833, 293, 436, 1066, 1242, 1101, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1273767975555069, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0031688872259110212}, {"id": 321, "seek": 181192, "start": 1816.5600000000002, "end": 1821.28, "text": " And I find it unbelievable. I find it unbelievable that this whole AI stuff with neural networks", "tokens": [50596, 400, 286, 915, 309, 16605, 13, 286, 915, 309, 16605, 300, 341, 1379, 7318, 1507, 365, 18161, 9590, 50832], "temperature": 0.0, "avg_logprob": -0.1273767975555069, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0031688872259110212}, {"id": 322, "seek": 181192, "start": 1821.28, "end": 1827.8400000000001, "text": " works. Have you built up an intuition of why are there a little bits and pieces of intuitions", "tokens": [50832, 1985, 13, 3560, 291, 3094, 493, 364, 24002, 295, 983, 366, 456, 257, 707, 9239, 293, 3755, 295, 16224, 626, 51160], "temperature": 0.0, "avg_logprob": -0.1273767975555069, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0031688872259110212}, {"id": 323, "seek": 181192, "start": 1827.8400000000001, "end": 1833.76, "text": " of insights of why this whole thing works? I mean, some definitely. Well, we know that", "tokens": [51160, 295, 14310, 295, 983, 341, 1379, 551, 1985, 30, 286, 914, 11, 512, 2138, 13, 1042, 11, 321, 458, 300, 51456], "temperature": 0.0, "avg_logprob": -0.1273767975555069, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0031688872259110212}, {"id": 324, "seek": 183376, "start": 1833.84, "end": 1840.8, "text": " optimization, we now have good, you know, we've had lots of empirical, you know,", "tokens": [50368, 19618, 11, 321, 586, 362, 665, 11, 291, 458, 11, 321, 600, 632, 3195, 295, 31886, 11, 291, 458, 11, 50716], "temperature": 0.0, "avg_logprob": -0.15667817706153506, "compression_ratio": 1.7810945273631842, "no_speech_prob": 0.04944544658064842}, {"id": 325, "seek": 183376, "start": 1840.8, "end": 1844.96, "text": " huge amounts of empirical reasons to believe that optimization should work on all most", "tokens": [50716, 2603, 11663, 295, 31886, 4112, 281, 1697, 300, 19618, 820, 589, 322, 439, 881, 50924], "temperature": 0.0, "avg_logprob": -0.15667817706153506, "compression_ratio": 1.7810945273631842, "no_speech_prob": 0.04944544658064842}, {"id": 326, "seek": 183376, "start": 1844.96, "end": 1850.72, "text": " problems we care about. Do you have insights of what, so you just said empirical evidence.", "tokens": [50924, 2740, 321, 1127, 466, 13, 1144, 291, 362, 14310, 295, 437, 11, 370, 291, 445, 848, 31886, 4467, 13, 51212], "temperature": 0.0, "avg_logprob": -0.15667817706153506, "compression_ratio": 1.7810945273631842, "no_speech_prob": 0.04944544658064842}, {"id": 327, "seek": 183376, "start": 1850.72, "end": 1860.32, "text": " Is most of your sort of empirical evidence kind of convinces you, it's like evolution is empirical,", "tokens": [51212, 1119, 881, 295, 428, 1333, 295, 31886, 4467, 733, 295, 9854, 887, 291, 11, 309, 311, 411, 9303, 307, 31886, 11, 51692], "temperature": 0.0, "avg_logprob": -0.15667817706153506, "compression_ratio": 1.7810945273631842, "no_speech_prob": 0.04944544658064842}, {"id": 328, "seek": 186032, "start": 1860.32, "end": 1864.48, "text": " it shows you that look, this evolutionary process seems to be a good way to design", "tokens": [50364, 309, 3110, 291, 300, 574, 11, 341, 27567, 1399, 2544, 281, 312, 257, 665, 636, 281, 1715, 50572], "temperature": 0.0, "avg_logprob": -0.09849105981680063, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.006096091121435165}, {"id": 329, "seek": 186032, "start": 1865.76, "end": 1871.76, "text": " organisms that survive in their environment. But it doesn't really get you to the insights of how", "tokens": [50636, 22110, 300, 7867, 294, 641, 2823, 13, 583, 309, 1177, 380, 534, 483, 291, 281, 264, 14310, 295, 577, 50936], "temperature": 0.0, "avg_logprob": -0.09849105981680063, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.006096091121435165}, {"id": 330, "seek": 186032, "start": 1872.6399999999999, "end": 1877.52, "text": " the whole thing works. I think it's a good analogy is physics. You know how you say, hey,", "tokens": [50980, 264, 1379, 551, 1985, 13, 286, 519, 309, 311, 257, 665, 21663, 307, 10649, 13, 509, 458, 577, 291, 584, 11, 4177, 11, 51224], "temperature": 0.0, "avg_logprob": -0.09849105981680063, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.006096091121435165}, {"id": 331, "seek": 186032, "start": 1877.52, "end": 1881.6, "text": " let's do some physics calculation and come up with some new physics theory and make some prediction.", "tokens": [51224, 718, 311, 360, 512, 10649, 17108, 293, 808, 493, 365, 512, 777, 10649, 5261, 293, 652, 512, 17630, 13, 51428], "temperature": 0.0, "avg_logprob": -0.09849105981680063, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.006096091121435165}, {"id": 332, "seek": 186032, "start": 1881.6, "end": 1885.9199999999998, "text": " But then you got around the experiment. You know, you got around the experiment, it's important.", "tokens": [51428, 583, 550, 291, 658, 926, 264, 5120, 13, 509, 458, 11, 291, 658, 926, 264, 5120, 11, 309, 311, 1021, 13, 51644], "temperature": 0.0, "avg_logprob": -0.09849105981680063, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.006096091121435165}, {"id": 333, "seek": 186032, "start": 1885.9199999999998, "end": 1890.08, "text": " So it's a bit the same here, except that maybe sometimes the experiment came before", "tokens": [51644, 407, 309, 311, 257, 857, 264, 912, 510, 11, 3993, 300, 1310, 2171, 264, 5120, 1361, 949, 51852], "temperature": 0.0, "avg_logprob": -0.09849105981680063, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.006096091121435165}, {"id": 334, "seek": 189008, "start": 1890.08, "end": 1894.48, "text": " the theory. But it still is the case, you know, you have some data and you come up with some", "tokens": [50364, 264, 5261, 13, 583, 309, 920, 307, 264, 1389, 11, 291, 458, 11, 291, 362, 512, 1412, 293, 291, 808, 493, 365, 512, 50584], "temperature": 0.0, "avg_logprob": -0.11395283540089925, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.001837096526287496}, {"id": 335, "seek": 189008, "start": 1894.48, "end": 1897.84, "text": " prediction, you say, yeah, let's make a big neural network, let's train it, and it's going to work", "tokens": [50584, 17630, 11, 291, 584, 11, 1338, 11, 718, 311, 652, 257, 955, 18161, 3209, 11, 718, 311, 3847, 309, 11, 293, 309, 311, 516, 281, 589, 50752], "temperature": 0.0, "avg_logprob": -0.11395283540089925, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.001837096526287496}, {"id": 336, "seek": 189008, "start": 1898.3999999999999, "end": 1901.84, "text": " much better than anything before it. And it will in fact continue to get better as you make it", "tokens": [50780, 709, 1101, 813, 1340, 949, 309, 13, 400, 309, 486, 294, 1186, 2354, 281, 483, 1101, 382, 291, 652, 309, 50952], "temperature": 0.0, "avg_logprob": -0.11395283540089925, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.001837096526287496}, {"id": 337, "seek": 189008, "start": 1901.84, "end": 1906.8799999999999, "text": " larger. And it turns out to be true. That's, that's amazing when a theory is validated like this,", "tokens": [50952, 4833, 13, 400, 309, 4523, 484, 281, 312, 2074, 13, 663, 311, 11, 300, 311, 2243, 562, 257, 5261, 307, 40693, 411, 341, 11, 51204], "temperature": 0.0, "avg_logprob": -0.11395283540089925, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.001837096526287496}, {"id": 338, "seek": 189008, "start": 1906.8799999999999, "end": 1910.56, "text": " you know, it's not a mathematical theory, it's more of a biological theory almost.", "tokens": [51204, 291, 458, 11, 309, 311, 406, 257, 18894, 5261, 11, 309, 311, 544, 295, 257, 13910, 5261, 1920, 13, 51388], "temperature": 0.0, "avg_logprob": -0.11395283540089925, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.001837096526287496}, {"id": 339, "seek": 189008, "start": 1911.6, "end": 1915.84, "text": " So I think there are not terrible analogies between deep learning and biology. I would say", "tokens": [51440, 407, 286, 519, 456, 366, 406, 6237, 16660, 530, 1296, 2452, 2539, 293, 14956, 13, 286, 576, 584, 51652], "temperature": 0.0, "avg_logprob": -0.11395283540089925, "compression_ratio": 1.7658227848101267, "no_speech_prob": 0.001837096526287496}, {"id": 340, "seek": 191584, "start": 1915.84, "end": 1919.36, "text": " it's like the geometric mean of biology and physics, that's deep learning.", "tokens": [50364, 309, 311, 411, 264, 33246, 914, 295, 14956, 293, 10649, 11, 300, 311, 2452, 2539, 13, 50540], "temperature": 0.0, "avg_logprob": -0.15220338326913338, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.004900888539850712}, {"id": 341, "seek": 191584, "start": 1920.08, "end": 1925.36, "text": " The geometric mean of biology and physics. I think I'm going to need a few hours to wrap", "tokens": [50576, 440, 33246, 914, 295, 14956, 293, 10649, 13, 286, 519, 286, 478, 516, 281, 643, 257, 1326, 2496, 281, 7019, 50840], "temperature": 0.0, "avg_logprob": -0.15220338326913338, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.004900888539850712}, {"id": 342, "seek": 191584, "start": 1925.36, "end": 1935.36, "text": " my head around that. Because just to find the geometric, just to find the set of what biology", "tokens": [50840, 452, 1378, 926, 300, 13, 1436, 445, 281, 915, 264, 33246, 11, 445, 281, 915, 264, 992, 295, 437, 14956, 51340], "temperature": 0.0, "avg_logprob": -0.15220338326913338, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.004900888539850712}, {"id": 343, "seek": 191584, "start": 1935.36, "end": 1940.24, "text": " represents. Well, biology, in biology, things are really complicated. The theories are really,", "tokens": [51340, 8855, 13, 1042, 11, 14956, 11, 294, 14956, 11, 721, 366, 534, 6179, 13, 440, 13667, 366, 534, 11, 51584], "temperature": 0.0, "avg_logprob": -0.15220338326913338, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.004900888539850712}, {"id": 344, "seek": 191584, "start": 1940.24, "end": 1944.32, "text": " really, it's really hard to have good predictive theory. And if in physics, the theories are too", "tokens": [51584, 534, 11, 309, 311, 534, 1152, 281, 362, 665, 35521, 5261, 13, 400, 498, 294, 10649, 11, 264, 13667, 366, 886, 51788], "temperature": 0.0, "avg_logprob": -0.15220338326913338, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.004900888539850712}, {"id": 345, "seek": 194432, "start": 1944.32, "end": 1948.56, "text": " good. In theory, in physics, people make these super precise theories, which make these amazing", "tokens": [50364, 665, 13, 682, 5261, 11, 294, 10649, 11, 561, 652, 613, 1687, 13600, 13667, 11, 597, 652, 613, 2243, 50576], "temperature": 0.0, "avg_logprob": -0.1411833872321908, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.003218850353732705}, {"id": 346, "seek": 194432, "start": 1948.56, "end": 1953.4399999999998, "text": " predictions. And in machine learning, we're kind of in between. Kind of in between. But it'd be", "tokens": [50576, 21264, 13, 400, 294, 3479, 2539, 11, 321, 434, 733, 295, 294, 1296, 13, 9242, 295, 294, 1296, 13, 583, 309, 1116, 312, 50820], "temperature": 0.0, "avg_logprob": -0.1411833872321908, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.003218850353732705}, {"id": 347, "seek": 194432, "start": 1953.4399999999998, "end": 1958.3999999999999, "text": " nice if machine learning somehow helped us discover the unification of the two as opposed to server", "tokens": [50820, 1481, 498, 3479, 2539, 6063, 4254, 505, 4411, 264, 517, 3774, 295, 264, 732, 382, 8851, 281, 7154, 51068], "temperature": 0.0, "avg_logprob": -0.1411833872321908, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.003218850353732705}, {"id": 348, "seek": 194432, "start": 1958.3999999999999, "end": 1965.6799999999998, "text": " the in between. But you're right, that's, you're kind of trying to juggle both. So do you think", "tokens": [51068, 264, 294, 1296, 13, 583, 291, 434, 558, 11, 300, 311, 11, 291, 434, 733, 295, 1382, 281, 361, 31726, 1293, 13, 407, 360, 291, 519, 51432], "temperature": 0.0, "avg_logprob": -0.1411833872321908, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.003218850353732705}, {"id": 349, "seek": 194432, "start": 1965.6799999999998, "end": 1969.4399999999998, "text": " there are still beautiful and mysterious properties in your networks that are yet to be", "tokens": [51432, 456, 366, 920, 2238, 293, 13831, 7221, 294, 428, 9590, 300, 366, 1939, 281, 312, 51620], "temperature": 0.0, "avg_logprob": -0.1411833872321908, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.003218850353732705}, {"id": 350, "seek": 194432, "start": 1969.4399999999998, "end": 1974.0, "text": " discovered? Definitely. I think that we are still massively underestimating deep learning.", "tokens": [51620, 6941, 30, 12151, 13, 286, 519, 300, 321, 366, 920, 29379, 24612, 332, 990, 2452, 2539, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1411833872321908, "compression_ratio": 1.802547770700637, "no_speech_prob": 0.003218850353732705}, {"id": 351, "seek": 197432, "start": 1975.28, "end": 1981.36, "text": " What do you think it will look like? Like what if I knew I would have done it? So,", "tokens": [50412, 708, 360, 291, 519, 309, 486, 574, 411, 30, 1743, 437, 498, 286, 2586, 286, 576, 362, 1096, 309, 30, 407, 11, 50716], "temperature": 0.0, "avg_logprob": -0.13149434081779038, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0010808446677401662}, {"id": 352, "seek": 197432, "start": 1982.56, "end": 1986.24, "text": " but if you look at all the progress from the past 10 years, I would say most of it,", "tokens": [50776, 457, 498, 291, 574, 412, 439, 264, 4205, 490, 264, 1791, 1266, 924, 11, 286, 576, 584, 881, 295, 309, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13149434081779038, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0010808446677401662}, {"id": 353, "seek": 197432, "start": 1986.96, "end": 1992.08, "text": " I would say there have been a few cases where some were things that felt like really new ideas", "tokens": [50996, 286, 576, 584, 456, 362, 668, 257, 1326, 3331, 689, 512, 645, 721, 300, 2762, 411, 534, 777, 3487, 51252], "temperature": 0.0, "avg_logprob": -0.13149434081779038, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0010808446677401662}, {"id": 354, "seek": 197432, "start": 1992.08, "end": 1997.12, "text": " showed up. But by and large, it was every year, we thought, okay, deep learning goes this far.", "tokens": [51252, 4712, 493, 13, 583, 538, 293, 2416, 11, 309, 390, 633, 1064, 11, 321, 1194, 11, 1392, 11, 2452, 2539, 1709, 341, 1400, 13, 51504], "temperature": 0.0, "avg_logprob": -0.13149434081779038, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0010808446677401662}, {"id": 355, "seek": 197432, "start": 1997.12, "end": 2001.6799999999998, "text": " Nope, it actually goes further. And then the next year, okay, now you know, this is this is", "tokens": [51504, 12172, 11, 309, 767, 1709, 3052, 13, 400, 550, 264, 958, 1064, 11, 1392, 11, 586, 291, 458, 11, 341, 307, 341, 307, 51732], "temperature": 0.0, "avg_logprob": -0.13149434081779038, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0010808446677401662}, {"id": 356, "seek": 200168, "start": 2001.68, "end": 2005.44, "text": " big deep learning, we are really done. Nope, it goes further, it just keeps going further each", "tokens": [50364, 955, 2452, 2539, 11, 321, 366, 534, 1096, 13, 12172, 11, 309, 1709, 3052, 11, 309, 445, 5965, 516, 3052, 1184, 50552], "temperature": 0.0, "avg_logprob": -0.12357825040817261, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.00235898420214653}, {"id": 357, "seek": 200168, "start": 2005.44, "end": 2010.24, "text": " year. So that means that we keep underestimating, we keep not understanding it as surprising properties", "tokens": [50552, 1064, 13, 407, 300, 1355, 300, 321, 1066, 24612, 332, 990, 11, 321, 1066, 406, 3701, 309, 382, 8830, 7221, 50792], "temperature": 0.0, "avg_logprob": -0.12357825040817261, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.00235898420214653}, {"id": 358, "seek": 200168, "start": 2010.24, "end": 2015.1200000000001, "text": " all the time. You think it's getting harder and harder to make progress, need to make progress?", "tokens": [50792, 439, 264, 565, 13, 509, 519, 309, 311, 1242, 6081, 293, 6081, 281, 652, 4205, 11, 643, 281, 652, 4205, 30, 51036], "temperature": 0.0, "avg_logprob": -0.12357825040817261, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.00235898420214653}, {"id": 359, "seek": 200168, "start": 2015.8400000000001, "end": 2019.8400000000001, "text": " It depends on what we mean. I think the field will continue to make very robust progress", "tokens": [51072, 467, 5946, 322, 437, 321, 914, 13, 286, 519, 264, 2519, 486, 2354, 281, 652, 588, 13956, 4205, 51272], "temperature": 0.0, "avg_logprob": -0.12357825040817261, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.00235898420214653}, {"id": 360, "seek": 200168, "start": 2019.8400000000001, "end": 2024.3200000000002, "text": " for quite a while. I think for individual researchers, especially people who are doing", "tokens": [51272, 337, 1596, 257, 1339, 13, 286, 519, 337, 2609, 10309, 11, 2318, 561, 567, 366, 884, 51496], "temperature": 0.0, "avg_logprob": -0.12357825040817261, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.00235898420214653}, {"id": 361, "seek": 200168, "start": 2025.04, "end": 2029.1200000000001, "text": " research, it can be harder because there is a very large number of researchers right now.", "tokens": [51532, 2132, 11, 309, 393, 312, 6081, 570, 456, 307, 257, 588, 2416, 1230, 295, 10309, 558, 586, 13, 51736], "temperature": 0.0, "avg_logprob": -0.12357825040817261, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.00235898420214653}, {"id": 362, "seek": 202912, "start": 2030.0, "end": 2034.08, "text": " I think that if you have a lot of compute, then you can make a lot of very interesting", "tokens": [50408, 286, 519, 300, 498, 291, 362, 257, 688, 295, 14722, 11, 550, 291, 393, 652, 257, 688, 295, 588, 1880, 50612], "temperature": 0.0, "avg_logprob": -0.12351276256419995, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.0006562944035977125}, {"id": 363, "seek": 202912, "start": 2034.08, "end": 2040.1599999999999, "text": " discoveries, but then you have to deal with the challenge of managing a huge computer,", "tokens": [50612, 28400, 11, 457, 550, 291, 362, 281, 2028, 365, 264, 3430, 295, 11642, 257, 2603, 3820, 11, 50916], "temperature": 0.0, "avg_logprob": -0.12351276256419995, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.0006562944035977125}, {"id": 364, "seek": 202912, "start": 2040.1599999999999, "end": 2043.1999999999998, "text": " a huge class, a huge computer cluster to run your experiments. It's a little bit harder.", "tokens": [50916, 257, 2603, 1508, 11, 257, 2603, 3820, 13630, 281, 1190, 428, 12050, 13, 467, 311, 257, 707, 857, 6081, 13, 51068], "temperature": 0.0, "avg_logprob": -0.12351276256419995, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.0006562944035977125}, {"id": 365, "seek": 202912, "start": 2043.1999999999998, "end": 2047.76, "text": " So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest", "tokens": [51068, 407, 286, 478, 3365, 439, 613, 1651, 300, 5079, 3255, 264, 1867, 281, 11, 457, 291, 434, 472, 295, 264, 41491, 51296], "temperature": 0.0, "avg_logprob": -0.12351276256419995, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.0006562944035977125}, {"id": 366, "seek": 202912, "start": 2047.76, "end": 2052.7999999999997, "text": " people I know, so I'm going to keep asking. So let's imagine all the breakthroughs that happen", "tokens": [51296, 561, 286, 458, 11, 370, 286, 478, 516, 281, 1066, 3365, 13, 407, 718, 311, 3811, 439, 264, 22397, 82, 300, 1051, 51548], "temperature": 0.0, "avg_logprob": -0.12351276256419995, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.0006562944035977125}, {"id": 367, "seek": 202912, "start": 2052.7999999999997, "end": 2058.0, "text": " in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by", "tokens": [51548, 294, 264, 958, 2217, 924, 294, 2452, 2539, 13, 1144, 291, 519, 881, 295, 729, 22397, 82, 393, 312, 1096, 538, 51808], "temperature": 0.0, "avg_logprob": -0.12351276256419995, "compression_ratio": 1.8092105263157894, "no_speech_prob": 0.0006562944035977125}, {"id": 368, "seek": 205800, "start": 2058.0, "end": 2068.32, "text": " one person with one computer? In the space of breakthroughs, do you think compute and", "tokens": [50364, 472, 954, 365, 472, 3820, 30, 682, 264, 1901, 295, 22397, 82, 11, 360, 291, 519, 14722, 293, 50880], "temperature": 0.0, "avg_logprob": -0.11487483978271484, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.007338008843362331}, {"id": 369, "seek": 205800, "start": 2068.32, "end": 2075.76, "text": " large efforts will be necessary? I mean, I can't be sure. When you say one computer, you mean how", "tokens": [50880, 2416, 6484, 486, 312, 4818, 30, 286, 914, 11, 286, 393, 380, 312, 988, 13, 1133, 291, 584, 472, 3820, 11, 291, 914, 577, 51252], "temperature": 0.0, "avg_logprob": -0.11487483978271484, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.007338008843362331}, {"id": 370, "seek": 205800, "start": 2075.76, "end": 2085.84, "text": " large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely.", "tokens": [51252, 2416, 30, 509, 434, 13494, 13, 286, 914, 11, 472, 18407, 13, 286, 536, 13, 286, 519, 309, 311, 1238, 17518, 13, 51756], "temperature": 0.0, "avg_logprob": -0.11487483978271484, "compression_ratio": 1.463276836158192, "no_speech_prob": 0.007338008843362331}, {"id": 371, "seek": 208584, "start": 2086.8, "end": 2093.04, "text": " I think it's pretty unlikely. I think that the stack of deep learning is starting to be quite", "tokens": [50412, 286, 519, 309, 311, 1238, 17518, 13, 286, 519, 300, 264, 8630, 295, 2452, 2539, 307, 2891, 281, 312, 1596, 50724], "temperature": 0.0, "avg_logprob": -0.14863900078667536, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0022864777129143476}, {"id": 372, "seek": 208584, "start": 2093.04, "end": 2101.44, "text": " deep. If you look at it, you've got all the way from the ideas, the systems to build the datasets,", "tokens": [50724, 2452, 13, 759, 291, 574, 412, 309, 11, 291, 600, 658, 439, 264, 636, 490, 264, 3487, 11, 264, 3652, 281, 1322, 264, 42856, 11, 51144], "temperature": 0.0, "avg_logprob": -0.14863900078667536, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0022864777129143476}, {"id": 373, "seek": 208584, "start": 2102.0, "end": 2107.36, "text": " the distributed programming, the building the actual cluster, the GPU programming,", "tokens": [51172, 264, 12631, 9410, 11, 264, 2390, 264, 3539, 13630, 11, 264, 18407, 9410, 11, 51440], "temperature": 0.0, "avg_logprob": -0.14863900078667536, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0022864777129143476}, {"id": 374, "seek": 208584, "start": 2108.08, "end": 2113.1200000000003, "text": " putting it all together. So the stack is getting really deep, and I think it can be quite hard", "tokens": [51476, 3372, 309, 439, 1214, 13, 407, 264, 8630, 307, 1242, 534, 2452, 11, 293, 286, 519, 309, 393, 312, 1596, 1152, 51728], "temperature": 0.0, "avg_logprob": -0.14863900078667536, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.0022864777129143476}, {"id": 375, "seek": 211312, "start": 2113.12, "end": 2118.3199999999997, "text": " for a single person to become to be world-class in every single layer of the stack. What about", "tokens": [50364, 337, 257, 2167, 954, 281, 1813, 281, 312, 1002, 12, 11665, 294, 633, 2167, 4583, 295, 264, 8630, 13, 708, 466, 50624], "temperature": 0.0, "avg_logprob": -0.12215374863666037, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.01770123280584812}, {"id": 376, "seek": 211312, "start": 2119.92, "end": 2125.8399999999997, "text": " Vladimir Vapnik really insists on is taking MNIST and trying to learn from very few examples,", "tokens": [50704, 31669, 691, 569, 13123, 534, 50137, 322, 307, 1940, 376, 45, 19756, 293, 1382, 281, 1466, 490, 588, 1326, 5110, 11, 51000], "temperature": 0.0, "avg_logprob": -0.12215374863666037, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.01770123280584812}, {"id": 377, "seek": 211312, "start": 2125.8399999999997, "end": 2132.0, "text": " so being able to learn more efficiently. Do you think there'll be breakthroughs in that space", "tokens": [51000, 370, 885, 1075, 281, 1466, 544, 19621, 13, 1144, 291, 519, 456, 603, 312, 22397, 82, 294, 300, 1901, 51308], "temperature": 0.0, "avg_logprob": -0.12215374863666037, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.01770123280584812}, {"id": 378, "seek": 211312, "start": 2132.0, "end": 2138.3199999999997, "text": " that may not need a huge compute? I think there will be a large number of breakthroughs in general", "tokens": [51308, 300, 815, 406, 643, 257, 2603, 14722, 30, 286, 519, 456, 486, 312, 257, 2416, 1230, 295, 22397, 82, 294, 2674, 51624], "temperature": 0.0, "avg_logprob": -0.12215374863666037, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.01770123280584812}, {"id": 379, "seek": 213832, "start": 2138.32, "end": 2142.96, "text": " that will not need a huge amount of compute. So maybe I should clarify that. I think that", "tokens": [50364, 300, 486, 406, 643, 257, 2603, 2372, 295, 14722, 13, 407, 1310, 286, 820, 17594, 300, 13, 286, 519, 300, 50596], "temperature": 0.0, "avg_logprob": -0.11438756153501313, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.04182131215929985}, {"id": 380, "seek": 213832, "start": 2142.96, "end": 2148.2400000000002, "text": " some breakthroughs will require a lot of compute, and I think building systems which actually do", "tokens": [50596, 512, 22397, 82, 486, 3651, 257, 688, 295, 14722, 11, 293, 286, 519, 2390, 3652, 597, 767, 360, 50860], "temperature": 0.0, "avg_logprob": -0.11438756153501313, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.04182131215929985}, {"id": 381, "seek": 213832, "start": 2148.2400000000002, "end": 2153.44, "text": " things will require a huge amount of compute. That one is pretty obvious. If you want to do X,", "tokens": [50860, 721, 486, 3651, 257, 2603, 2372, 295, 14722, 13, 663, 472, 307, 1238, 6322, 13, 759, 291, 528, 281, 360, 1783, 11, 51120], "temperature": 0.0, "avg_logprob": -0.11438756153501313, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.04182131215929985}, {"id": 382, "seek": 213832, "start": 2153.44, "end": 2158.2400000000002, "text": " and X requires a huge neural net, you've got to get a huge neural net, but I think there will be", "tokens": [51120, 293, 1783, 7029, 257, 2603, 18161, 2533, 11, 291, 600, 658, 281, 483, 257, 2603, 18161, 2533, 11, 457, 286, 519, 456, 486, 312, 51360], "temperature": 0.0, "avg_logprob": -0.11438756153501313, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.04182131215929985}, {"id": 383, "seek": 213832, "start": 2158.2400000000002, "end": 2163.76, "text": " lots of, I think there is lots of room for very important work being done by small groups and", "tokens": [51360, 3195, 295, 11, 286, 519, 456, 307, 3195, 295, 1808, 337, 588, 1021, 589, 885, 1096, 538, 1359, 3935, 293, 51636], "temperature": 0.0, "avg_logprob": -0.11438756153501313, "compression_ratio": 1.873015873015873, "no_speech_prob": 0.04182131215929985}, {"id": 384, "seek": 216376, "start": 2163.76, "end": 2170.48, "text": " individuals. Can you maybe sort of on the topic of the science of deep learning, talk about one", "tokens": [50364, 5346, 13, 1664, 291, 1310, 1333, 295, 322, 264, 4829, 295, 264, 3497, 295, 2452, 2539, 11, 751, 466, 472, 50700], "temperature": 0.0, "avg_logprob": -0.14398081667788393, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.030628567561507225}, {"id": 385, "seek": 216376, "start": 2170.48, "end": 2177.2000000000003, "text": " of the recent papers that you've released, the Deep Double Descent, where bigger models and more", "tokens": [50700, 295, 264, 5162, 10577, 300, 291, 600, 4736, 11, 264, 14895, 16633, 3885, 2207, 11, 689, 3801, 5245, 293, 544, 51036], "temperature": 0.0, "avg_logprob": -0.14398081667788393, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.030628567561507225}, {"id": 386, "seek": 216376, "start": 2177.2000000000003, "end": 2182.7200000000003, "text": " data hurt. I think it's a really interesting paper. Can you describe the main idea? Yeah,", "tokens": [51036, 1412, 4607, 13, 286, 519, 309, 311, 257, 534, 1880, 3035, 13, 1664, 291, 6786, 264, 2135, 1558, 30, 865, 11, 51312], "temperature": 0.0, "avg_logprob": -0.14398081667788393, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.030628567561507225}, {"id": 387, "seek": 216376, "start": 2182.7200000000003, "end": 2188.4, "text": " definitely. So what happened is that some over the years, some small number of researchers", "tokens": [51312, 2138, 13, 407, 437, 2011, 307, 300, 512, 670, 264, 924, 11, 512, 1359, 1230, 295, 10309, 51596], "temperature": 0.0, "avg_logprob": -0.14398081667788393, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.030628567561507225}, {"id": 388, "seek": 216376, "start": 2188.4, "end": 2192.0800000000004, "text": " noticed that it is kind of weird that when you make the neural net work larger, it works better,", "tokens": [51596, 5694, 300, 309, 307, 733, 295, 3657, 300, 562, 291, 652, 264, 18161, 2533, 589, 4833, 11, 309, 1985, 1101, 11, 51780], "temperature": 0.0, "avg_logprob": -0.14398081667788393, "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.030628567561507225}, {"id": 389, "seek": 219208, "start": 2192.08, "end": 2195.7599999999998, "text": " and it seems to go in contradiction with statistical ideas. And then some people made", "tokens": [50364, 293, 309, 2544, 281, 352, 294, 34937, 365, 22820, 3487, 13, 400, 550, 512, 561, 1027, 50548], "temperature": 0.0, "avg_logprob": -0.10241426740373884, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0010480309138074517}, {"id": 390, "seek": 219208, "start": 2195.7599999999998, "end": 2200.08, "text": " an analysis showing that actually you got this double descent bump. And what we've done was to", "tokens": [50548, 364, 5215, 4099, 300, 767, 291, 658, 341, 3834, 23475, 9961, 13, 400, 437, 321, 600, 1096, 390, 281, 50764], "temperature": 0.0, "avg_logprob": -0.10241426740373884, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0010480309138074517}, {"id": 391, "seek": 219208, "start": 2200.08, "end": 2205.6, "text": " show that double descent occurs for pretty much all practical deep learning systems.", "tokens": [50764, 855, 300, 3834, 23475, 11843, 337, 1238, 709, 439, 8496, 2452, 2539, 3652, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10241426740373884, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0010480309138074517}, {"id": 392, "seek": 219208, "start": 2206.3199999999997, "end": 2214.48, "text": " And that it'll be also, so can you step back? What's the X axis and the Y axis of a double", "tokens": [51076, 400, 300, 309, 603, 312, 611, 11, 370, 393, 291, 1823, 646, 30, 708, 311, 264, 1783, 10298, 293, 264, 398, 10298, 295, 257, 3834, 51484], "temperature": 0.0, "avg_logprob": -0.10241426740373884, "compression_ratio": 1.5964125560538116, "no_speech_prob": 0.0010480309138074517}, {"id": 393, "seek": 221448, "start": 2214.48, "end": 2223.28, "text": " descent plot? Okay, great. So you can look, you can do things like you can take your neural", "tokens": [50364, 23475, 7542, 30, 1033, 11, 869, 13, 407, 291, 393, 574, 11, 291, 393, 360, 721, 411, 291, 393, 747, 428, 18161, 50804], "temperature": 0.0, "avg_logprob": -0.0949827588122824, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.0111541161313653}, {"id": 394, "seek": 221448, "start": 2223.28, "end": 2228.72, "text": " network, and you can start increasing its size slowly, while keeping your data set fixed.", "tokens": [50804, 3209, 11, 293, 291, 393, 722, 5662, 1080, 2744, 5692, 11, 1339, 5145, 428, 1412, 992, 6806, 13, 51076], "temperature": 0.0, "avg_logprob": -0.0949827588122824, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.0111541161313653}, {"id": 395, "seek": 221448, "start": 2230.0, "end": 2236.8, "text": " So if you increase the size of the neural network slowly, and if you don't do early stopping,", "tokens": [51140, 407, 498, 291, 3488, 264, 2744, 295, 264, 18161, 3209, 5692, 11, 293, 498, 291, 500, 380, 360, 2440, 12767, 11, 51480], "temperature": 0.0, "avg_logprob": -0.0949827588122824, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.0111541161313653}, {"id": 396, "seek": 221448, "start": 2236.8, "end": 2243.36, "text": " that's a pretty important detail. Then when the neural network is really small, you make it larger,", "tokens": [51480, 300, 311, 257, 1238, 1021, 2607, 13, 1396, 562, 264, 18161, 3209, 307, 534, 1359, 11, 291, 652, 309, 4833, 11, 51808], "temperature": 0.0, "avg_logprob": -0.0949827588122824, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.0111541161313653}, {"id": 397, "seek": 224336, "start": 2243.44, "end": 2247.52, "text": " you get a very rapid increase in performance. Then you continue to make it larger. And at some", "tokens": [50368, 291, 483, 257, 588, 7558, 3488, 294, 3389, 13, 1396, 291, 2354, 281, 652, 309, 4833, 13, 400, 412, 512, 50572], "temperature": 0.0, "avg_logprob": -0.10439558925791684, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0004107286222279072}, {"id": 398, "seek": 224336, "start": 2247.52, "end": 2254.0, "text": " point performance, you'll get worse. And it gets and it gets the worst exactly at the point at", "tokens": [50572, 935, 3389, 11, 291, 603, 483, 5324, 13, 400, 309, 2170, 293, 309, 2170, 264, 5855, 2293, 412, 264, 935, 412, 50896], "temperature": 0.0, "avg_logprob": -0.10439558925791684, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0004107286222279072}, {"id": 399, "seek": 224336, "start": 2254.0, "end": 2259.28, "text": " which it achieves zero training error, precisely zero training loss. And then as you make it", "tokens": [50896, 597, 309, 3538, 977, 4018, 3097, 6713, 11, 13402, 4018, 3097, 4470, 13, 400, 550, 382, 291, 652, 309, 51160], "temperature": 0.0, "avg_logprob": -0.10439558925791684, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0004107286222279072}, {"id": 400, "seek": 224336, "start": 2259.28, "end": 2263.6, "text": " larger, it starts to get better again. And it's kind of counterintuitive because you'd expect", "tokens": [51160, 4833, 11, 309, 3719, 281, 483, 1101, 797, 13, 400, 309, 311, 733, 295, 5682, 686, 48314, 570, 291, 1116, 2066, 51376], "temperature": 0.0, "avg_logprob": -0.10439558925791684, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0004107286222279072}, {"id": 401, "seek": 224336, "start": 2263.6, "end": 2271.28, "text": " deep learning phenomena to be monotonic. And it's hard to be sure what it means, but it also occurs", "tokens": [51376, 2452, 2539, 22004, 281, 312, 1108, 310, 11630, 13, 400, 309, 311, 1152, 281, 312, 988, 437, 309, 1355, 11, 457, 309, 611, 11843, 51760], "temperature": 0.0, "avg_logprob": -0.10439558925791684, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0004107286222279072}, {"id": 402, "seek": 227128, "start": 2271.36, "end": 2275.36, "text": " in the case of linear classifiers. And the intuition basically boils down to the following.", "tokens": [50368, 294, 264, 1389, 295, 8213, 1508, 23463, 13, 400, 264, 24002, 1936, 35049, 760, 281, 264, 3480, 13, 50568], "temperature": 0.0, "avg_logprob": -0.14777005986964448, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.006793129723519087}, {"id": 403, "seek": 227128, "start": 2276.96, "end": 2284.0, "text": " When you, when you have a lot, when you have a large data set, and a small model, then small,", "tokens": [50648, 1133, 291, 11, 562, 291, 362, 257, 688, 11, 562, 291, 362, 257, 2416, 1412, 992, 11, 293, 257, 1359, 2316, 11, 550, 1359, 11, 51000], "temperature": 0.0, "avg_logprob": -0.14777005986964448, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.006793129723519087}, {"id": 404, "seek": 227128, "start": 2284.0, "end": 2291.44, "text": " tiny random. So basically, what is overfitting? Overfitting is when your model is somehow very", "tokens": [51000, 5870, 4974, 13, 407, 1936, 11, 437, 307, 670, 69, 2414, 30, 4886, 69, 2414, 307, 562, 428, 2316, 307, 6063, 588, 51372], "temperature": 0.0, "avg_logprob": -0.14777005986964448, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.006793129723519087}, {"id": 405, "seek": 227128, "start": 2291.44, "end": 2297.2000000000003, "text": " sensitive to the small random, unimportant stuff in your data set in the training data in the", "tokens": [51372, 9477, 281, 264, 1359, 4974, 11, 517, 41654, 1507, 294, 428, 1412, 992, 294, 264, 3097, 1412, 294, 264, 51660], "temperature": 0.0, "avg_logprob": -0.14777005986964448, "compression_ratio": 1.755868544600939, "no_speech_prob": 0.006793129723519087}, {"id": 406, "seek": 229720, "start": 2297.2, "end": 2302.3999999999996, "text": " training data set precisely. So if you have a small model, and you have a big data set,", "tokens": [50364, 3097, 1412, 992, 13402, 13, 407, 498, 291, 362, 257, 1359, 2316, 11, 293, 291, 362, 257, 955, 1412, 992, 11, 50624], "temperature": 0.0, "avg_logprob": -0.09291342270275778, "compression_ratio": 1.962655601659751, "no_speech_prob": 0.0018098861910402775}, {"id": 407, "seek": 229720, "start": 2303.2799999999997, "end": 2307.3599999999997, "text": " and there may be some random thing, you know, some training cases are randomly in the data set,", "tokens": [50668, 293, 456, 815, 312, 512, 4974, 551, 11, 291, 458, 11, 512, 3097, 3331, 366, 16979, 294, 264, 1412, 992, 11, 50872], "temperature": 0.0, "avg_logprob": -0.09291342270275778, "compression_ratio": 1.962655601659751, "no_speech_prob": 0.0018098861910402775}, {"id": 408, "seek": 229720, "start": 2307.3599999999997, "end": 2311.68, "text": " and others may not be there. But the small model, but the small model is kind of insensitive to", "tokens": [50872, 293, 2357, 815, 406, 312, 456, 13, 583, 264, 1359, 2316, 11, 457, 264, 1359, 2316, 307, 733, 295, 1028, 34465, 281, 51088], "temperature": 0.0, "avg_logprob": -0.09291342270275778, "compression_ratio": 1.962655601659751, "no_speech_prob": 0.0018098861910402775}, {"id": 409, "seek": 229720, "start": 2311.68, "end": 2316.96, "text": " this randomness, because it's the same, you there is pretty much no uncertainty about the model", "tokens": [51088, 341, 4974, 1287, 11, 570, 309, 311, 264, 912, 11, 291, 456, 307, 1238, 709, 572, 15697, 466, 264, 2316, 51352], "temperature": 0.0, "avg_logprob": -0.09291342270275778, "compression_ratio": 1.962655601659751, "no_speech_prob": 0.0018098861910402775}, {"id": 410, "seek": 229720, "start": 2316.96, "end": 2322.3999999999996, "text": " when the data set is large. So okay, so at the very basic level, to me, it is the most surprising", "tokens": [51352, 562, 264, 1412, 992, 307, 2416, 13, 407, 1392, 11, 370, 412, 264, 588, 3875, 1496, 11, 281, 385, 11, 309, 307, 264, 881, 8830, 51624], "temperature": 0.0, "avg_logprob": -0.09291342270275778, "compression_ratio": 1.962655601659751, "no_speech_prob": 0.0018098861910402775}, {"id": 411, "seek": 232240, "start": 2322.4, "end": 2333.28, "text": " thing that neural networks don't overfit every time very quickly. Before ever being able to learn", "tokens": [50364, 551, 300, 18161, 9590, 500, 380, 670, 6845, 633, 565, 588, 2661, 13, 4546, 1562, 885, 1075, 281, 1466, 50908], "temperature": 0.0, "avg_logprob": -0.1658226946989695, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.0263378769159317}, {"id": 412, "seek": 232240, "start": 2333.28, "end": 2339.28, "text": " anything, the huge number of parameters. So here is so there is one way Okay, so maybe so let me try", "tokens": [50908, 1340, 11, 264, 2603, 1230, 295, 9834, 13, 407, 510, 307, 370, 456, 307, 472, 636, 1033, 11, 370, 1310, 370, 718, 385, 853, 51208], "temperature": 0.0, "avg_logprob": -0.1658226946989695, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.0263378769159317}, {"id": 413, "seek": 232240, "start": 2339.28, "end": 2343.52, "text": " to give the explanation and maybe that will be that will work. So you got a huge neural network,", "tokens": [51208, 281, 976, 264, 10835, 293, 1310, 300, 486, 312, 300, 486, 589, 13, 407, 291, 658, 257, 2603, 18161, 3209, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1658226946989695, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.0263378769159317}, {"id": 414, "seek": 232240, "start": 2343.52, "end": 2348.64, "text": " let's suppose you got a your you have a huge neural network, you have a huge number of parameters.", "tokens": [51420, 718, 311, 7297, 291, 658, 257, 428, 291, 362, 257, 2603, 18161, 3209, 11, 291, 362, 257, 2603, 1230, 295, 9834, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1658226946989695, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.0263378769159317}, {"id": 415, "seek": 234864, "start": 2349.6, "end": 2353.8399999999997, "text": " And now let's pretend everything is linear, which is not let's just pretend. Then there is this big", "tokens": [50412, 400, 586, 718, 311, 11865, 1203, 307, 8213, 11, 597, 307, 406, 718, 311, 445, 11865, 13, 1396, 456, 307, 341, 955, 50624], "temperature": 0.0, "avg_logprob": -0.1489443907866607, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.0020177059341222048}, {"id": 416, "seek": 234864, "start": 2353.8399999999997, "end": 2361.04, "text": " subspace, where a neural network achieves zero error. And SGT is going to find approximately", "tokens": [50624, 2090, 17940, 11, 689, 257, 18161, 3209, 3538, 977, 4018, 6713, 13, 400, 34520, 51, 307, 516, 281, 915, 10447, 50984], "temperature": 0.0, "avg_logprob": -0.1489443907866607, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.0020177059341222048}, {"id": 417, "seek": 234864, "start": 2361.04, "end": 2365.12, "text": " that's right, approximately the point with the smallest norm in that subspace.", "tokens": [50984, 300, 311, 558, 11, 10447, 264, 935, 365, 264, 16998, 2026, 294, 300, 2090, 17940, 13, 51188], "temperature": 0.0, "avg_logprob": -0.1489443907866607, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.0020177059341222048}, {"id": 418, "seek": 234864, "start": 2367.04, "end": 2373.6, "text": " And that can also be proven to be insensitive to the small randomness in the data, when the", "tokens": [51284, 400, 300, 393, 611, 312, 12785, 281, 312, 1028, 34465, 281, 264, 1359, 4974, 1287, 294, 264, 1412, 11, 562, 264, 51612], "temperature": 0.0, "avg_logprob": -0.1489443907866607, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.0020177059341222048}, {"id": 419, "seek": 234864, "start": 2373.6, "end": 2378.48, "text": " dimensionality is high. But when the dimensionality of the data is equal to the dimensionality of", "tokens": [51612, 10139, 1860, 307, 1090, 13, 583, 562, 264, 10139, 1860, 295, 264, 1412, 307, 2681, 281, 264, 10139, 1860, 295, 51856], "temperature": 0.0, "avg_logprob": -0.1489443907866607, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.0020177059341222048}, {"id": 420, "seek": 237848, "start": 2378.48, "end": 2384.4, "text": " the model, then there is a one to one correspondence between all the data sets and the models. So", "tokens": [50364, 264, 2316, 11, 550, 456, 307, 257, 472, 281, 472, 38135, 1296, 439, 264, 1412, 6352, 293, 264, 5245, 13, 407, 50660], "temperature": 0.0, "avg_logprob": -0.1212608264042781, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.001169311930425465}, {"id": 421, "seek": 237848, "start": 2384.4, "end": 2387.6, "text": " small changes in the data set actually lead to large changes in the model and that's why", "tokens": [50660, 1359, 2962, 294, 264, 1412, 992, 767, 1477, 281, 2416, 2962, 294, 264, 2316, 293, 300, 311, 983, 50820], "temperature": 0.0, "avg_logprob": -0.1212608264042781, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.001169311930425465}, {"id": 422, "seek": 237848, "start": 2387.6, "end": 2392.88, "text": " performance gets worse. So this is the best explanation more or less. So then it would be", "tokens": [50820, 3389, 2170, 5324, 13, 407, 341, 307, 264, 1151, 10835, 544, 420, 1570, 13, 407, 550, 309, 576, 312, 51084], "temperature": 0.0, "avg_logprob": -0.1212608264042781, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.001169311930425465}, {"id": 423, "seek": 237848, "start": 2392.88, "end": 2398.96, "text": " good for the model to have more parameters sort of to be bigger than the data. That's right,", "tokens": [51084, 665, 337, 264, 2316, 281, 362, 544, 9834, 1333, 295, 281, 312, 3801, 813, 264, 1412, 13, 663, 311, 558, 11, 51388], "temperature": 0.0, "avg_logprob": -0.1212608264042781, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.001169311930425465}, {"id": 424, "seek": 237848, "start": 2398.96, "end": 2402.72, "text": " but only if you don't early stop. If you introduce early stop in your regularization,", "tokens": [51388, 457, 787, 498, 291, 500, 380, 2440, 1590, 13, 759, 291, 5366, 2440, 1590, 294, 428, 3890, 2144, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1212608264042781, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.001169311930425465}, {"id": 425, "seek": 237848, "start": 2402.72, "end": 2407.28, "text": " you can make a double as a descent pump almost completely disappear. What is early stop early", "tokens": [51576, 291, 393, 652, 257, 3834, 382, 257, 23475, 5889, 1920, 2584, 11596, 13, 708, 307, 2440, 1590, 2440, 51804], "temperature": 0.0, "avg_logprob": -0.1212608264042781, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.001169311930425465}, {"id": 426, "seek": 240728, "start": 2407.28, "end": 2412.48, "text": " stopping is when you train your model, and you monitor your validation performance.", "tokens": [50364, 12767, 307, 562, 291, 3847, 428, 2316, 11, 293, 291, 6002, 428, 24071, 3389, 13, 50624], "temperature": 0.0, "avg_logprob": -0.15094695772443498, "compression_ratio": 1.868312757201646, "no_speech_prob": 0.007692999206483364}, {"id": 427, "seek": 240728, "start": 2413.44, "end": 2416.5600000000004, "text": " And then if at some point validation performance starts to get worse, you say, okay, let's stop", "tokens": [50672, 400, 550, 498, 412, 512, 935, 24071, 3389, 3719, 281, 483, 5324, 11, 291, 584, 11, 1392, 11, 718, 311, 1590, 50828], "temperature": 0.0, "avg_logprob": -0.15094695772443498, "compression_ratio": 1.868312757201646, "no_speech_prob": 0.007692999206483364}, {"id": 428, "seek": 240728, "start": 2416.5600000000004, "end": 2423.0400000000004, "text": " training. We are good. We are good. We are good enough. So the magic happens after that moment.", "tokens": [50828, 3097, 13, 492, 366, 665, 13, 492, 366, 665, 13, 492, 366, 665, 1547, 13, 407, 264, 5585, 2314, 934, 300, 1623, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15094695772443498, "compression_ratio": 1.868312757201646, "no_speech_prob": 0.007692999206483364}, {"id": 429, "seek": 240728, "start": 2423.0400000000004, "end": 2426.5600000000004, "text": " So you don't want to do the early stopping. Well, if you don't do the early stopping,", "tokens": [51152, 407, 291, 500, 380, 528, 281, 360, 264, 2440, 12767, 13, 1042, 11, 498, 291, 500, 380, 360, 264, 2440, 12767, 11, 51328], "temperature": 0.0, "avg_logprob": -0.15094695772443498, "compression_ratio": 1.868312757201646, "no_speech_prob": 0.007692999206483364}, {"id": 430, "seek": 240728, "start": 2426.5600000000004, "end": 2431.92, "text": " you get this very, you get the very pronounced double descent. Do you have any intuition why", "tokens": [51328, 291, 483, 341, 588, 11, 291, 483, 264, 588, 23155, 3834, 23475, 13, 1144, 291, 362, 604, 24002, 983, 51596], "temperature": 0.0, "avg_logprob": -0.15094695772443498, "compression_ratio": 1.868312757201646, "no_speech_prob": 0.007692999206483364}, {"id": 431, "seek": 243192, "start": 2431.92, "end": 2436.96, "text": " this happens? Double descent or sorry, are you stopping? No, the double descent. So the", "tokens": [50364, 341, 2314, 30, 16633, 23475, 420, 2597, 11, 366, 291, 12767, 30, 883, 11, 264, 3834, 23475, 13, 407, 264, 50616], "temperature": 0.0, "avg_logprob": -0.1462067406753014, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.008969743736088276}, {"id": 432, "seek": 243192, "start": 2436.96, "end": 2443.6800000000003, "text": " well, yeah, so I try it. Let's see the intuition is basically is this that when the data set has", "tokens": [50616, 731, 11, 1338, 11, 370, 286, 853, 309, 13, 961, 311, 536, 264, 24002, 307, 1936, 307, 341, 300, 562, 264, 1412, 992, 575, 50952], "temperature": 0.0, "avg_logprob": -0.1462067406753014, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.008969743736088276}, {"id": 433, "seek": 243192, "start": 2443.6800000000003, "end": 2449.6800000000003, "text": " as many degrees of freedom as the model, then there is a one to one correspondence between them.", "tokens": [50952, 382, 867, 5310, 295, 5645, 382, 264, 2316, 11, 550, 456, 307, 257, 472, 281, 472, 38135, 1296, 552, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1462067406753014, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.008969743736088276}, {"id": 434, "seek": 243192, "start": 2449.6800000000003, "end": 2455.52, "text": " And so small changes to the data set lead to noticeable changes in the model. So your model", "tokens": [51252, 400, 370, 1359, 2962, 281, 264, 1412, 992, 1477, 281, 26041, 2962, 294, 264, 2316, 13, 407, 428, 2316, 51544], "temperature": 0.0, "avg_logprob": -0.1462067406753014, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.008969743736088276}, {"id": 435, "seek": 243192, "start": 2455.52, "end": 2461.36, "text": " is very sensitive to all the randomness. It is unable to discard it. Whereas it turns out that", "tokens": [51544, 307, 588, 9477, 281, 439, 264, 4974, 1287, 13, 467, 307, 11299, 281, 31597, 309, 13, 13813, 309, 4523, 484, 300, 51836], "temperature": 0.0, "avg_logprob": -0.1462067406753014, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.008969743736088276}, {"id": 436, "seek": 246136, "start": 2461.36, "end": 2467.1200000000003, "text": " when you have a lot more data than parameters, or a lot more parameters than data, the resulting", "tokens": [50364, 562, 291, 362, 257, 688, 544, 1412, 813, 9834, 11, 420, 257, 688, 544, 9834, 813, 1412, 11, 264, 16505, 50652], "temperature": 0.0, "avg_logprob": -0.14755299308083275, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0028858082368969917}, {"id": 437, "seek": 246136, "start": 2467.1200000000003, "end": 2474.1600000000003, "text": " solution will be insensitive to small changes in the data set. So it's able to nicely put discard", "tokens": [50652, 3827, 486, 312, 1028, 34465, 281, 1359, 2962, 294, 264, 1412, 992, 13, 407, 309, 311, 1075, 281, 9594, 829, 31597, 51004], "temperature": 0.0, "avg_logprob": -0.14755299308083275, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0028858082368969917}, {"id": 438, "seek": 246136, "start": 2474.1600000000003, "end": 2479.84, "text": " the small changes, the randomness. Exactly. The spurious correlations which you don't want.", "tokens": [51004, 264, 1359, 2962, 11, 264, 4974, 1287, 13, 7587, 13, 440, 637, 24274, 13983, 763, 597, 291, 500, 380, 528, 13, 51288], "temperature": 0.0, "avg_logprob": -0.14755299308083275, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0028858082368969917}, {"id": 439, "seek": 246136, "start": 2480.48, "end": 2484.48, "text": " Jeff Hinton suggested we need to throw back propagation. We already kind of talked about", "tokens": [51320, 7506, 389, 12442, 10945, 321, 643, 281, 3507, 646, 38377, 13, 492, 1217, 733, 295, 2825, 466, 51520], "temperature": 0.0, "avg_logprob": -0.14755299308083275, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0028858082368969917}, {"id": 440, "seek": 246136, "start": 2484.48, "end": 2488.88, "text": " this a little bit, but he suggested we need to throw away back propagation and start over.", "tokens": [51520, 341, 257, 707, 857, 11, 457, 415, 10945, 321, 643, 281, 3507, 1314, 646, 38377, 293, 722, 670, 13, 51740], "temperature": 0.0, "avg_logprob": -0.14755299308083275, "compression_ratio": 1.7453183520599251, "no_speech_prob": 0.0028858082368969917}, {"id": 441, "seek": 248888, "start": 2489.6800000000003, "end": 2491.92, "text": " I mean, of course, some of that is a little bit", "tokens": [50404, 286, 914, 11, 295, 1164, 11, 512, 295, 300, 307, 257, 707, 857, 50516], "temperature": 0.0, "avg_logprob": -0.09249265407159077, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.006387125235050917}, {"id": 442, "seek": 248888, "start": 2493.76, "end": 2498.6400000000003, "text": " wit and humor. But what do you think? What could be an alternative method of training neural", "tokens": [50608, 32161, 293, 14318, 13, 583, 437, 360, 291, 519, 30, 708, 727, 312, 364, 8535, 3170, 295, 3097, 18161, 50852], "temperature": 0.0, "avg_logprob": -0.09249265407159077, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.006387125235050917}, {"id": 443, "seek": 248888, "start": 2498.6400000000003, "end": 2503.52, "text": " networks? Well, the thing that he said precisely is that to the extent that you can't find back", "tokens": [50852, 9590, 30, 1042, 11, 264, 551, 300, 415, 848, 13402, 307, 300, 281, 264, 8396, 300, 291, 393, 380, 915, 646, 51096], "temperature": 0.0, "avg_logprob": -0.09249265407159077, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.006387125235050917}, {"id": 444, "seek": 248888, "start": 2503.52, "end": 2508.96, "text": " propagation in the brain, it's worth seeing if we can learn something from how the brain learns.", "tokens": [51096, 38377, 294, 264, 3567, 11, 309, 311, 3163, 2577, 498, 321, 393, 1466, 746, 490, 577, 264, 3567, 27152, 13, 51368], "temperature": 0.0, "avg_logprob": -0.09249265407159077, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.006387125235050917}, {"id": 445, "seek": 248888, "start": 2508.96, "end": 2513.44, "text": " But back propagation is very useful and we should keep using it. Oh, you're saying that", "tokens": [51368, 583, 646, 38377, 307, 588, 4420, 293, 321, 820, 1066, 1228, 309, 13, 876, 11, 291, 434, 1566, 300, 51592], "temperature": 0.0, "avg_logprob": -0.09249265407159077, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.006387125235050917}, {"id": 446, "seek": 248888, "start": 2513.44, "end": 2518.0, "text": " once we discover the mechanism of learning in the brain or any aspects of that mechanism,", "tokens": [51592, 1564, 321, 4411, 264, 7513, 295, 2539, 294, 264, 3567, 420, 604, 7270, 295, 300, 7513, 11, 51820], "temperature": 0.0, "avg_logprob": -0.09249265407159077, "compression_ratio": 1.7743055555555556, "no_speech_prob": 0.006387125235050917}, {"id": 447, "seek": 251800, "start": 2518.0, "end": 2522.16, "text": " we should also try to implement that in your networks. If it turns out that you can't find", "tokens": [50364, 321, 820, 611, 853, 281, 4445, 300, 294, 428, 9590, 13, 759, 309, 4523, 484, 300, 291, 393, 380, 915, 50572], "temperature": 0.0, "avg_logprob": -0.09296135719005878, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.0005357532063499093}, {"id": 448, "seek": 251800, "start": 2522.16, "end": 2525.92, "text": " back propagation in the brain. If we can't find back propagation in the brain.", "tokens": [50572, 646, 38377, 294, 264, 3567, 13, 759, 321, 393, 380, 915, 646, 38377, 294, 264, 3567, 13, 50760], "temperature": 0.0, "avg_logprob": -0.09296135719005878, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.0005357532063499093}, {"id": 449, "seek": 251800, "start": 2528.0, "end": 2534.96, "text": " Well, so I guess your answer to that is back propagation is pretty damn useful. So why are", "tokens": [50864, 1042, 11, 370, 286, 2041, 428, 1867, 281, 300, 307, 646, 38377, 307, 1238, 8151, 4420, 13, 407, 983, 366, 51212], "temperature": 0.0, "avg_logprob": -0.09296135719005878, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.0005357532063499093}, {"id": 450, "seek": 251800, "start": 2534.96, "end": 2538.96, "text": " we complaining? I mean, I personally am a big fan of back propagation. I think it's a great", "tokens": [51212, 321, 20740, 30, 286, 914, 11, 286, 5665, 669, 257, 955, 3429, 295, 646, 38377, 13, 286, 519, 309, 311, 257, 869, 51412], "temperature": 0.0, "avg_logprob": -0.09296135719005878, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.0005357532063499093}, {"id": 451, "seek": 251800, "start": 2538.96, "end": 2545.28, "text": " algorithm because it solves an extremely fundamental problem which is finding a neural", "tokens": [51412, 9284, 570, 309, 39890, 364, 4664, 8088, 1154, 597, 307, 5006, 257, 18161, 51728], "temperature": 0.0, "avg_logprob": -0.09296135719005878, "compression_ratio": 1.7701612903225807, "no_speech_prob": 0.0005357532063499093}, {"id": 452, "seek": 254528, "start": 2545.28, "end": 2551.2000000000003, "text": " circuit subject to some constraints. I don't see that problem going away. So that's why I", "tokens": [50364, 9048, 3983, 281, 512, 18491, 13, 286, 500, 380, 536, 300, 1154, 516, 1314, 13, 407, 300, 311, 983, 286, 50660], "temperature": 0.0, "avg_logprob": -0.09682105053430316, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.003426788840442896}, {"id": 453, "seek": 254528, "start": 2552.5600000000004, "end": 2556.48, "text": " really, I think it's pretty unlikely that we'll have anything which is going to be", "tokens": [50728, 534, 11, 286, 519, 309, 311, 1238, 17518, 300, 321, 603, 362, 1340, 597, 307, 516, 281, 312, 50924], "temperature": 0.0, "avg_logprob": -0.09682105053430316, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.003426788840442896}, {"id": 454, "seek": 254528, "start": 2557.28, "end": 2561.2000000000003, "text": " dramatically different. It could happen. But I wouldn't bet on it right now.", "tokens": [50964, 17548, 819, 13, 467, 727, 1051, 13, 583, 286, 2759, 380, 778, 322, 309, 558, 586, 13, 51160], "temperature": 0.0, "avg_logprob": -0.09682105053430316, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.003426788840442896}, {"id": 455, "seek": 254528, "start": 2563.2000000000003, "end": 2571.0400000000004, "text": " So let me ask a sort of big picture question. Do you think neural networks can be made to reason?", "tokens": [51260, 407, 718, 385, 1029, 257, 1333, 295, 955, 3036, 1168, 13, 1144, 291, 519, 18161, 9590, 393, 312, 1027, 281, 1778, 30, 51652], "temperature": 0.0, "avg_logprob": -0.09682105053430316, "compression_ratio": 1.4956896551724137, "no_speech_prob": 0.003426788840442896}, {"id": 456, "seek": 257104, "start": 2571.6, "end": 2576.56, "text": " Why not? Well, if you look, for example, at AlphaGo or AlphaZero,", "tokens": [50392, 1545, 406, 30, 1042, 11, 498, 291, 574, 11, 337, 1365, 11, 412, 20588, 12104, 420, 20588, 57, 2032, 11, 50640], "temperature": 0.0, "avg_logprob": -0.13556719362066033, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.07459539920091629}, {"id": 457, "seek": 257104, "start": 2578.16, "end": 2584.32, "text": " the neural network of AlphaZero plays Go, which we all agree is a game that requires reasoning,", "tokens": [50720, 264, 18161, 3209, 295, 20588, 57, 2032, 5749, 1037, 11, 597, 321, 439, 3986, 307, 257, 1216, 300, 7029, 21577, 11, 51028], "temperature": 0.0, "avg_logprob": -0.13556719362066033, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.07459539920091629}, {"id": 458, "seek": 257104, "start": 2585.12, "end": 2591.04, "text": " better than 99.9% of all humans. Just the neural network without the search, just the neural network", "tokens": [51068, 1101, 813, 11803, 13, 24, 4, 295, 439, 6255, 13, 1449, 264, 18161, 3209, 1553, 264, 3164, 11, 445, 264, 18161, 3209, 51364], "temperature": 0.0, "avg_logprob": -0.13556719362066033, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.07459539920091629}, {"id": 459, "seek": 257104, "start": 2591.04, "end": 2596.32, "text": " itself. Doesn't that give us an existence proof that neural networks can reason?", "tokens": [51364, 2564, 13, 12955, 380, 300, 976, 505, 364, 9123, 8177, 300, 18161, 9590, 393, 1778, 30, 51628], "temperature": 0.0, "avg_logprob": -0.13556719362066033, "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.07459539920091629}, {"id": 460, "seek": 259632, "start": 2596.56, "end": 2601.92, "text": " To push back and disagree a little bit, we all agree that Go is reasoning.", "tokens": [50376, 1407, 2944, 646, 293, 14091, 257, 707, 857, 11, 321, 439, 3986, 300, 1037, 307, 21577, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1487158681010152, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.0007553769974038005}, {"id": 461, "seek": 259632, "start": 2603.2000000000003, "end": 2608.6400000000003, "text": " I think I agree. I don't think it's a trivial. So obviously, reasoning like intelligence is", "tokens": [50708, 286, 519, 286, 3986, 13, 286, 500, 380, 519, 309, 311, 257, 26703, 13, 407, 2745, 11, 21577, 411, 7599, 307, 50980], "temperature": 0.0, "avg_logprob": -0.1487158681010152, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.0007553769974038005}, {"id": 462, "seek": 259632, "start": 2609.92, "end": 2616.6400000000003, "text": " a loose gray area term a little bit. Maybe you disagree with that. But yes, I think it has some", "tokens": [51044, 257, 9612, 10855, 1859, 1433, 257, 707, 857, 13, 2704, 291, 14091, 365, 300, 13, 583, 2086, 11, 286, 519, 309, 575, 512, 51380], "temperature": 0.0, "avg_logprob": -0.1487158681010152, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.0007553769974038005}, {"id": 463, "seek": 259632, "start": 2616.6400000000003, "end": 2624.56, "text": " of the same elements of reasoning. Reasoning is almost akin to search. There's a sequential element", "tokens": [51380, 295, 264, 912, 4959, 295, 21577, 13, 39693, 278, 307, 1920, 47540, 281, 3164, 13, 821, 311, 257, 42881, 4478, 51776], "temperature": 0.0, "avg_logprob": -0.1487158681010152, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.0007553769974038005}, {"id": 464, "seek": 262456, "start": 2625.2, "end": 2634.16, "text": " of stepwise consideration of possibilities and sort of building on top of those possibilities", "tokens": [50396, 295, 1823, 3711, 12381, 295, 12178, 293, 1333, 295, 2390, 322, 1192, 295, 729, 12178, 50844], "temperature": 0.0, "avg_logprob": -0.10487909878001493, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0008425550768151879}, {"id": 465, "seek": 262456, "start": 2634.16, "end": 2639.7599999999998, "text": " in a sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of", "tokens": [50844, 294, 257, 42881, 9060, 1826, 291, 8881, 412, 512, 11269, 13, 407, 1338, 11, 286, 2041, 2433, 1037, 307, 733, 295, 51124], "temperature": 0.0, "avg_logprob": -0.10487909878001493, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0008425550768151879}, {"id": 466, "seek": 262456, "start": 2639.7599999999998, "end": 2644.64, "text": " like that. And when you have a single neural network doing that without search, that's kind of like", "tokens": [51124, 411, 300, 13, 400, 562, 291, 362, 257, 2167, 18161, 3209, 884, 300, 1553, 3164, 11, 300, 311, 733, 295, 411, 51368], "temperature": 0.0, "avg_logprob": -0.10487909878001493, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0008425550768151879}, {"id": 467, "seek": 262456, "start": 2644.64, "end": 2650.08, "text": " that. So there's an existence proof in a particular constrained environment that a process akin to", "tokens": [51368, 300, 13, 407, 456, 311, 364, 9123, 8177, 294, 257, 1729, 38901, 2823, 300, 257, 1399, 47540, 281, 51640], "temperature": 0.0, "avg_logprob": -0.10487909878001493, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0008425550768151879}, {"id": 468, "seek": 265008, "start": 2650.88, "end": 2658.72, "text": " what many people call reasoning exists. But more general kind of reasoning. So off the board.", "tokens": [50404, 437, 867, 561, 818, 21577, 8198, 13, 583, 544, 2674, 733, 295, 21577, 13, 407, 766, 264, 3150, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13205507474067885, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.008574258536100388}, {"id": 469, "seek": 265008, "start": 2658.72, "end": 2662.72, "text": " There is one other existence proof. Oh boy, which one? Us humans?", "tokens": [50796, 821, 307, 472, 661, 9123, 8177, 13, 876, 3237, 11, 597, 472, 30, 4958, 6255, 30, 50996], "temperature": 0.0, "avg_logprob": -0.13205507474067885, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.008574258536100388}, {"id": 470, "seek": 265008, "start": 2662.72, "end": 2672.24, "text": " Yes. Okay. All right. So do you think the architecture that will allow neural", "tokens": [50996, 1079, 13, 1033, 13, 1057, 558, 13, 407, 360, 291, 519, 264, 9482, 300, 486, 2089, 18161, 51472], "temperature": 0.0, "avg_logprob": -0.13205507474067885, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.008574258536100388}, {"id": 471, "seek": 265008, "start": 2672.24, "end": 2677.92, "text": " networks to reason will look similar to the neural network architectures we have today?", "tokens": [51472, 9590, 281, 1778, 486, 574, 2531, 281, 264, 18161, 3209, 6331, 1303, 321, 362, 965, 30, 51756], "temperature": 0.0, "avg_logprob": -0.13205507474067885, "compression_ratio": 1.570048309178744, "no_speech_prob": 0.008574258536100388}, {"id": 472, "seek": 267792, "start": 2678.7200000000003, "end": 2683.36, "text": " I think it will. I think, well, I don't want to make two overly definitive statements.", "tokens": [50404, 286, 519, 309, 486, 13, 286, 519, 11, 731, 11, 286, 500, 380, 528, 281, 652, 732, 24324, 28152, 12363, 13, 50636], "temperature": 0.0, "avg_logprob": -0.10029532488654641, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0031229143496602774}, {"id": 473, "seek": 267792, "start": 2683.92, "end": 2689.44, "text": " I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs", "tokens": [50664, 286, 519, 309, 311, 2138, 1944, 300, 264, 18161, 9590, 300, 486, 5258, 264, 21577, 22397, 82, 50940], "temperature": 0.0, "avg_logprob": -0.10029532488654641, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0031229143496602774}, {"id": 474, "seek": 267792, "start": 2689.44, "end": 2694.48, "text": " of the future will be very similar to the architecture that exists today, maybe a little", "tokens": [50940, 295, 264, 2027, 486, 312, 588, 2531, 281, 264, 9482, 300, 8198, 965, 11, 1310, 257, 707, 51192], "temperature": 0.0, "avg_logprob": -0.10029532488654641, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0031229143496602774}, {"id": 475, "seek": 267792, "start": 2694.48, "end": 2702.8, "text": " bit more recurrent, maybe a little bit deeper. But these neural nets are so insanely powerful.", "tokens": [51192, 857, 544, 18680, 1753, 11, 1310, 257, 707, 857, 7731, 13, 583, 613, 18161, 36170, 366, 370, 40965, 4005, 13, 51608], "temperature": 0.0, "avg_logprob": -0.10029532488654641, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0031229143496602774}, {"id": 476, "seek": 270280, "start": 2702.88, "end": 2708.4, "text": " Why wouldn't they be able to learn to reason? Humans can reason. So why can't neural networks?", "tokens": [50368, 1545, 2759, 380, 436, 312, 1075, 281, 1466, 281, 1778, 30, 35809, 393, 1778, 13, 407, 983, 393, 380, 18161, 9590, 30, 50644], "temperature": 0.0, "avg_logprob": -0.07169208526611329, "compression_ratio": 1.8359375, "no_speech_prob": 0.00490447087213397}, {"id": 477, "seek": 270280, "start": 2709.2000000000003, "end": 2714.48, "text": " So do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning?", "tokens": [50684, 407, 360, 291, 519, 264, 733, 295, 1507, 321, 600, 1612, 18161, 9590, 360, 307, 257, 733, 295, 445, 5336, 21577, 30, 50948], "temperature": 0.0, "avg_logprob": -0.07169208526611329, "compression_ratio": 1.8359375, "no_speech_prob": 0.00490447087213397}, {"id": 478, "seek": 270280, "start": 2714.48, "end": 2718.4, "text": " So it's not a fundamentally different process. Again, this is stuff we don't nobody knows the", "tokens": [50948, 407, 309, 311, 406, 257, 17879, 819, 1399, 13, 3764, 11, 341, 307, 1507, 321, 500, 380, 5079, 3255, 264, 51144], "temperature": 0.0, "avg_logprob": -0.07169208526611329, "compression_ratio": 1.8359375, "no_speech_prob": 0.00490447087213397}, {"id": 479, "seek": 270280, "start": 2718.4, "end": 2724.2400000000002, "text": " answer to. So when it comes to our neural networks, I would think which I would say", "tokens": [51144, 1867, 281, 13, 407, 562, 309, 1487, 281, 527, 18161, 9590, 11, 286, 576, 519, 597, 286, 576, 584, 51436], "temperature": 0.0, "avg_logprob": -0.07169208526611329, "compression_ratio": 1.8359375, "no_speech_prob": 0.00490447087213397}, {"id": 480, "seek": 270280, "start": 2724.2400000000002, "end": 2730.7200000000003, "text": " is that neural networks are capable of reasoning. But if you train a neural network on a task which", "tokens": [51436, 307, 300, 18161, 9590, 366, 8189, 295, 21577, 13, 583, 498, 291, 3847, 257, 18161, 3209, 322, 257, 5633, 597, 51760], "temperature": 0.0, "avg_logprob": -0.07169208526611329, "compression_ratio": 1.8359375, "no_speech_prob": 0.00490447087213397}, {"id": 481, "seek": 273072, "start": 2730.72, "end": 2735.52, "text": " doesn't require reasoning, it's not going to reason. This is a well-known effect where the", "tokens": [50364, 1177, 380, 3651, 21577, 11, 309, 311, 406, 516, 281, 1778, 13, 639, 307, 257, 731, 12, 6861, 1802, 689, 264, 50604], "temperature": 0.0, "avg_logprob": -0.11083488322016019, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.00150057184509933}, {"id": 482, "seek": 273072, "start": 2735.52, "end": 2743.04, "text": " neural network will solve exactly the problem that you pose in front of it in the easiest way possible.", "tokens": [50604, 18161, 3209, 486, 5039, 2293, 264, 1154, 300, 291, 10774, 294, 1868, 295, 309, 294, 264, 12889, 636, 1944, 13, 50980], "temperature": 0.0, "avg_logprob": -0.11083488322016019, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.00150057184509933}, {"id": 483, "seek": 273072, "start": 2744.3999999999996, "end": 2753.12, "text": " Right. That takes us to one of the brilliant ways you describe neural networks, which is", "tokens": [51048, 1779, 13, 663, 2516, 505, 281, 472, 295, 264, 10248, 2098, 291, 6786, 18161, 9590, 11, 597, 307, 51484], "temperature": 0.0, "avg_logprob": -0.11083488322016019, "compression_ratio": 1.554945054945055, "no_speech_prob": 0.00150057184509933}, {"id": 484, "seek": 275312, "start": 2754.08, "end": 2757.04, "text": " you've referred to neural networks as the search for small circuits", "tokens": [50412, 291, 600, 10839, 281, 18161, 9590, 382, 264, 3164, 337, 1359, 26354, 50560], "temperature": 0.0, "avg_logprob": -0.09260889361886417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03406433388590813}, {"id": 485, "seek": 275312, "start": 2757.92, "end": 2763.04, "text": " and maybe general intelligence as the search for small programs,", "tokens": [50604, 293, 1310, 2674, 7599, 382, 264, 3164, 337, 1359, 4268, 11, 50860], "temperature": 0.0, "avg_logprob": -0.09260889361886417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03406433388590813}, {"id": 486, "seek": 275312, "start": 2764.4, "end": 2768.64, "text": " which I found is a metaphor very compelling. Can you elaborate on that difference?", "tokens": [50928, 597, 286, 1352, 307, 257, 19157, 588, 20050, 13, 1664, 291, 20945, 322, 300, 2649, 30, 51140], "temperature": 0.0, "avg_logprob": -0.09260889361886417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03406433388590813}, {"id": 487, "seek": 275312, "start": 2769.2, "end": 2777.52, "text": " Yeah. So the thing which I said precisely was that if you can find the shortest program that", "tokens": [51168, 865, 13, 407, 264, 551, 597, 286, 848, 13402, 390, 300, 498, 291, 393, 915, 264, 31875, 1461, 300, 51584], "temperature": 0.0, "avg_logprob": -0.09260889361886417, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.03406433388590813}, {"id": 488, "seek": 277752, "start": 2777.52, "end": 2783.52, "text": " outputs the data in your disposal, then you will be able to use it to make the best prediction", "tokens": [50364, 23930, 264, 1412, 294, 428, 26400, 11, 550, 291, 486, 312, 1075, 281, 764, 309, 281, 652, 264, 1151, 17630, 50664], "temperature": 0.0, "avg_logprob": -0.09390968612477749, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.01708652824163437}, {"id": 489, "seek": 277752, "start": 2783.52, "end": 2789.68, "text": " possible. And that's a theoretical statement which can be proved mathematically. Now,", "tokens": [50664, 1944, 13, 400, 300, 311, 257, 20864, 5629, 597, 393, 312, 14617, 44003, 13, 823, 11, 50972], "temperature": 0.0, "avg_logprob": -0.09390968612477749, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.01708652824163437}, {"id": 490, "seek": 277752, "start": 2789.68, "end": 2794.8, "text": " you can also prove mathematically that it is that finding the shortest program which generates", "tokens": [50972, 291, 393, 611, 7081, 44003, 300, 309, 307, 300, 5006, 264, 31875, 1461, 597, 23815, 51228], "temperature": 0.0, "avg_logprob": -0.09390968612477749, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.01708652824163437}, {"id": 491, "seek": 277752, "start": 2794.8, "end": 2801.6, "text": " some data is not a computable operation. No finite amount of compute can do this.", "tokens": [51228, 512, 1412, 307, 406, 257, 2807, 712, 6916, 13, 883, 19362, 2372, 295, 14722, 393, 360, 341, 13, 51568], "temperature": 0.0, "avg_logprob": -0.09390968612477749, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.01708652824163437}, {"id": 492, "seek": 280160, "start": 2802.56, "end": 2808.72, "text": " So then with neural networks, neural networks are the next best thing that actually works in", "tokens": [50412, 407, 550, 365, 18161, 9590, 11, 18161, 9590, 366, 264, 958, 1151, 551, 300, 767, 1985, 294, 50720], "temperature": 0.0, "avg_logprob": -0.12078528708599984, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.020323725417256355}, {"id": 493, "seek": 280160, "start": 2808.72, "end": 2814.56, "text": " practice. We are not able to find the best, the shortest program which generates our data,", "tokens": [50720, 3124, 13, 492, 366, 406, 1075, 281, 915, 264, 1151, 11, 264, 31875, 1461, 597, 23815, 527, 1412, 11, 51012], "temperature": 0.0, "avg_logprob": -0.12078528708599984, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.020323725417256355}, {"id": 494, "seek": 280160, "start": 2815.6, "end": 2820.88, "text": " but we are able to find a small, but now that statement should be amended,", "tokens": [51064, 457, 321, 366, 1075, 281, 915, 257, 1359, 11, 457, 586, 300, 5629, 820, 312, 43641, 11, 51328], "temperature": 0.0, "avg_logprob": -0.12078528708599984, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.020323725417256355}, {"id": 495, "seek": 280160, "start": 2821.44, "end": 2824.48, "text": " even a large circuit which fits our data in some way.", "tokens": [51356, 754, 257, 2416, 9048, 597, 9001, 527, 1412, 294, 512, 636, 13, 51508], "temperature": 0.0, "avg_logprob": -0.12078528708599984, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.020323725417256355}, {"id": 496, "seek": 280160, "start": 2825.12, "end": 2829.8399999999997, "text": " Well, I think what you meant by the small circuit is the smallest needed circuit.", "tokens": [51540, 1042, 11, 286, 519, 437, 291, 4140, 538, 264, 1359, 9048, 307, 264, 16998, 2978, 9048, 13, 51776], "temperature": 0.0, "avg_logprob": -0.12078528708599984, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.020323725417256355}, {"id": 497, "seek": 282984, "start": 2829.92, "end": 2834.6400000000003, "text": " Well, the thing which I would change now, back then I really haven't fully internalized", "tokens": [50368, 1042, 11, 264, 551, 597, 286, 576, 1319, 586, 11, 646, 550, 286, 534, 2378, 380, 4498, 6920, 1602, 50604], "temperature": 0.0, "avg_logprob": -0.10904469313444914, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0015236141625791788}, {"id": 498, "seek": 282984, "start": 2834.6400000000003, "end": 2839.6000000000004, "text": " the overparameterized results. The things we know about overparameterized neural", "tokens": [50604, 264, 670, 2181, 335, 2398, 1602, 3542, 13, 440, 721, 321, 458, 466, 670, 2181, 335, 2398, 1602, 18161, 50852], "temperature": 0.0, "avg_logprob": -0.10904469313444914, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0015236141625791788}, {"id": 499, "seek": 282984, "start": 2839.6000000000004, "end": 2846.6400000000003, "text": " nets, now I would phrase it as a large circuit whose weights contain a small amount of information,", "tokens": [50852, 36170, 11, 586, 286, 576, 9535, 309, 382, 257, 2416, 9048, 6104, 17443, 5304, 257, 1359, 2372, 295, 1589, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10904469313444914, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0015236141625791788}, {"id": 500, "seek": 282984, "start": 2847.6000000000004, "end": 2851.6800000000003, "text": " which I think is what's going on. If you imagine the training process of a neural network as you", "tokens": [51252, 597, 286, 519, 307, 437, 311, 516, 322, 13, 759, 291, 3811, 264, 3097, 1399, 295, 257, 18161, 3209, 382, 291, 51456], "temperature": 0.0, "avg_logprob": -0.10904469313444914, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0015236141625791788}, {"id": 501, "seek": 282984, "start": 2851.6800000000003, "end": 2859.6800000000003, "text": " slowly transmit entropy from the data set to the parameters, then somehow the amount of information", "tokens": [51456, 5692, 17831, 30867, 490, 264, 1412, 992, 281, 264, 9834, 11, 550, 6063, 264, 2372, 295, 1589, 51856], "temperature": 0.0, "avg_logprob": -0.10904469313444914, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0015236141625791788}, {"id": 502, "seek": 285968, "start": 2859.68, "end": 2864.56, "text": " in the weights ends up being not very large, which would explain whether generalized so well.", "tokens": [50364, 294, 264, 17443, 5314, 493, 885, 406, 588, 2416, 11, 597, 576, 2903, 1968, 44498, 370, 731, 13, 50608], "temperature": 0.0, "avg_logprob": -0.149610975955395, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.002630413044244051}, {"id": 503, "seek": 285968, "start": 2865.2, "end": 2871.8399999999997, "text": " So that's the large circuit might be one that's helpful for the generalization.", "tokens": [50640, 407, 300, 311, 264, 2416, 9048, 1062, 312, 472, 300, 311, 4961, 337, 264, 2674, 2144, 13, 50972], "temperature": 0.0, "avg_logprob": -0.149610975955395, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.002630413044244051}, {"id": 504, "seek": 285968, "start": 2871.8399999999997, "end": 2872.64, "text": " Yeah, something like this.", "tokens": [50972, 865, 11, 746, 411, 341, 13, 51012], "temperature": 0.0, "avg_logprob": -0.149610975955395, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.002630413044244051}, {"id": 505, "seek": 285968, "start": 2874.72, "end": 2882.3199999999997, "text": " But do you see it important to be able to try to learn something like programs?", "tokens": [51116, 583, 360, 291, 536, 309, 1021, 281, 312, 1075, 281, 853, 281, 1466, 746, 411, 4268, 30, 51496], "temperature": 0.0, "avg_logprob": -0.149610975955395, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.002630413044244051}, {"id": 506, "seek": 285968, "start": 2882.3199999999997, "end": 2888.96, "text": " I mean, if we can, definitely. I think the answer is kind of yes, if we can do it.", "tokens": [51496, 286, 914, 11, 498, 321, 393, 11, 2138, 13, 286, 519, 264, 1867, 307, 733, 295, 2086, 11, 498, 321, 393, 360, 309, 13, 51828], "temperature": 0.0, "avg_logprob": -0.149610975955395, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.002630413044244051}, {"id": 507, "seek": 288896, "start": 2888.96, "end": 2893.76, "text": " We should do things that we can do it. It's the reason we are pushing on deep learning.", "tokens": [50364, 492, 820, 360, 721, 300, 321, 393, 360, 309, 13, 467, 311, 264, 1778, 321, 366, 7380, 322, 2452, 2539, 13, 50604], "temperature": 0.0, "avg_logprob": -0.11392422822805551, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.008520780131220818}, {"id": 508, "seek": 288896, "start": 2895.2, "end": 2900.0, "text": " The fundamental reason, the root cause is that we are able to train them.", "tokens": [50676, 440, 8088, 1778, 11, 264, 5593, 3082, 307, 300, 321, 366, 1075, 281, 3847, 552, 13, 50916], "temperature": 0.0, "avg_logprob": -0.11392422822805551, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.008520780131220818}, {"id": 509, "seek": 288896, "start": 2901.36, "end": 2906.7200000000003, "text": " So in other words, training comes first. We've got our pillar, which is the training pillar.", "tokens": [50984, 407, 294, 661, 2283, 11, 3097, 1487, 700, 13, 492, 600, 658, 527, 27592, 11, 597, 307, 264, 3097, 27592, 13, 51252], "temperature": 0.0, "avg_logprob": -0.11392422822805551, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.008520780131220818}, {"id": 510, "seek": 288896, "start": 2907.44, "end": 2911.04, "text": " And now we are trying to contort our neural networks around the training pillar. We got", "tokens": [51288, 400, 586, 321, 366, 1382, 281, 660, 477, 527, 18161, 9590, 926, 264, 3097, 27592, 13, 492, 658, 51468], "temperature": 0.0, "avg_logprob": -0.11392422822805551, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.008520780131220818}, {"id": 511, "seek": 288896, "start": 2911.04, "end": 2916.8, "text": " to stay trainable. This is an invariant we cannot violate. And so", "tokens": [51468, 281, 1754, 3847, 712, 13, 639, 307, 364, 33270, 394, 321, 2644, 37478, 13, 400, 370, 51756], "temperature": 0.0, "avg_logprob": -0.11392422822805551, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.008520780131220818}, {"id": 512, "seek": 291680, "start": 2917.04, "end": 2921.52, "text": " being trainable means starting from scratch, knowing nothing, you can actually pretty quickly", "tokens": [50376, 885, 3847, 712, 1355, 2891, 490, 8459, 11, 5276, 1825, 11, 291, 393, 767, 1238, 2661, 50600], "temperature": 0.0, "avg_logprob": -0.1723315092894408, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.00039195644785650074}, {"id": 513, "seek": 291680, "start": 2921.52, "end": 2927.52, "text": " converge towards knowing a lot or even slowly. But it means that given the resources at your", "tokens": [50600, 41881, 3030, 5276, 257, 688, 420, 754, 5692, 13, 583, 309, 1355, 300, 2212, 264, 3593, 412, 428, 50900], "temperature": 0.0, "avg_logprob": -0.1723315092894408, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.00039195644785650074}, {"id": 514, "seek": 291680, "start": 2927.52, "end": 2934.2400000000002, "text": " disposal, you can train the neural net and get it to achieve useful performance.", "tokens": [50900, 26400, 11, 291, 393, 3847, 264, 18161, 2533, 293, 483, 309, 281, 4584, 4420, 3389, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1723315092894408, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.00039195644785650074}, {"id": 515, "seek": 291680, "start": 2934.2400000000002, "end": 2937.92, "text": " Yeah, that's a pillar we can't move away from. That's right. Because if you can, whereas if you", "tokens": [51236, 865, 11, 300, 311, 257, 27592, 321, 393, 380, 1286, 1314, 490, 13, 663, 311, 558, 13, 1436, 498, 291, 393, 11, 9735, 498, 291, 51420], "temperature": 0.0, "avg_logprob": -0.1723315092894408, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.00039195644785650074}, {"id": 516, "seek": 291680, "start": 2937.92, "end": 2943.04, "text": " say, Hey, let's find the shortest program, we can't do that. So it doesn't matter how useful", "tokens": [51420, 584, 11, 1911, 11, 718, 311, 915, 264, 31875, 1461, 11, 321, 393, 380, 360, 300, 13, 407, 309, 1177, 380, 1871, 577, 4420, 51676], "temperature": 0.0, "avg_logprob": -0.1723315092894408, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.00039195644785650074}, {"id": 517, "seek": 294304, "start": 2944.0, "end": 2947.6, "text": " that would be. We can do it. So we want.", "tokens": [50412, 300, 576, 312, 13, 492, 393, 360, 309, 13, 407, 321, 528, 13, 50592], "temperature": 0.0, "avg_logprob": -0.30694604194027253, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.000698540301527828}, {"id": 518, "seek": 294304, "start": 2947.6, "end": 2951.52, "text": " So do you think you kind of mentioned that the neural networks are good at finding small", "tokens": [50592, 407, 360, 291, 519, 291, 733, 295, 2835, 300, 264, 18161, 9590, 366, 665, 412, 5006, 1359, 50788], "temperature": 0.0, "avg_logprob": -0.30694604194027253, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.000698540301527828}, {"id": 519, "seek": 294304, "start": 2951.52, "end": 2958.56, "text": " circuits or large circuits? Do you think then the matter of finding small programs is just the data?", "tokens": [50788, 26354, 420, 2416, 26354, 30, 1144, 291, 519, 550, 264, 1871, 295, 5006, 1359, 4268, 307, 445, 264, 1412, 30, 51140], "temperature": 0.0, "avg_logprob": -0.30694604194027253, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.000698540301527828}, {"id": 520, "seek": 294304, "start": 2958.56, "end": 2959.04, "text": " No.", "tokens": [51140, 883, 13, 51164], "temperature": 0.0, "avg_logprob": -0.30694604194027253, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.000698540301527828}, {"id": 521, "seek": 294304, "start": 2959.04, "end": 2968.24, "text": " So the, sorry, not the size or the quality, the type of data sort of ask giving it programs.", "tokens": [51164, 407, 264, 11, 2597, 11, 406, 264, 2744, 420, 264, 3125, 11, 264, 2010, 295, 1412, 1333, 295, 1029, 2902, 309, 4268, 13, 51624], "temperature": 0.0, "avg_logprob": -0.30694604194027253, "compression_ratio": 1.626865671641791, "no_speech_prob": 0.000698540301527828}, {"id": 522, "seek": 296824, "start": 2968.8799999999997, "end": 2974.8799999999997, "text": " Well, I think the thing is that right now, finding there are no good precedents of people", "tokens": [50396, 1042, 11, 286, 519, 264, 551, 307, 300, 558, 586, 11, 5006, 456, 366, 572, 665, 16969, 791, 295, 561, 50696], "temperature": 0.0, "avg_logprob": -0.13532545301649307, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.011149919591844082}, {"id": 523, "seek": 296824, "start": 2974.8799999999997, "end": 2981.9199999999996, "text": " successfully finding programs really well. And so the way you'd find programs is you'd train a", "tokens": [50696, 10727, 5006, 4268, 534, 731, 13, 400, 370, 264, 636, 291, 1116, 915, 4268, 307, 291, 1116, 3847, 257, 51048], "temperature": 0.0, "avg_logprob": -0.13532545301649307, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.011149919591844082}, {"id": 524, "seek": 296824, "start": 2981.9199999999996, "end": 2987.4399999999996, "text": " deep neural network to do it basically. But which is the right way to go about it.", "tokens": [51048, 2452, 18161, 3209, 281, 360, 309, 1936, 13, 583, 597, 307, 264, 558, 636, 281, 352, 466, 309, 13, 51324], "temperature": 0.0, "avg_logprob": -0.13532545301649307, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.011149919591844082}, {"id": 525, "seek": 296824, "start": 2988.0, "end": 2994.0, "text": " But there's not good illustrations that hasn't been done yet. But in principle, it should be possible.", "tokens": [51352, 583, 456, 311, 406, 665, 34540, 300, 6132, 380, 668, 1096, 1939, 13, 583, 294, 8665, 11, 309, 820, 312, 1944, 13, 51652], "temperature": 0.0, "avg_logprob": -0.13532545301649307, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.011149919591844082}, {"id": 526, "seek": 299400, "start": 2994.08, "end": 3001.2, "text": " Can you elaborate a little bit? What's your insight in principle? And put another way,", "tokens": [50368, 1664, 291, 20945, 257, 707, 857, 30, 708, 311, 428, 11269, 294, 8665, 30, 400, 829, 1071, 636, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1675477175368476, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.002047864720225334}, {"id": 527, "seek": 299400, "start": 3001.2, "end": 3007.52, "text": " you don't see why it's not possible. Well, it's kind of like more, it's more a statement of,", "tokens": [50724, 291, 500, 380, 536, 983, 309, 311, 406, 1944, 13, 1042, 11, 309, 311, 733, 295, 411, 544, 11, 309, 311, 544, 257, 5629, 295, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1675477175368476, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.002047864720225334}, {"id": 528, "seek": 299400, "start": 3009.36, "end": 3013.44, "text": " I think that it's, I think that it's unwise to bet against deep learning. And", "tokens": [51132, 286, 519, 300, 309, 311, 11, 286, 519, 300, 309, 311, 517, 3711, 281, 778, 1970, 2452, 2539, 13, 400, 51336], "temperature": 0.0, "avg_logprob": -0.1675477175368476, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.002047864720225334}, {"id": 529, "seek": 299400, "start": 3014.96, "end": 3018.88, "text": " if it's a fun, if it's a cognitive function that humans seem to be able to do, then", "tokens": [51412, 498, 309, 311, 257, 1019, 11, 498, 309, 311, 257, 15605, 2445, 300, 6255, 1643, 281, 312, 1075, 281, 360, 11, 550, 51608], "temperature": 0.0, "avg_logprob": -0.1675477175368476, "compression_ratio": 1.6084905660377358, "no_speech_prob": 0.002047864720225334}, {"id": 530, "seek": 301888, "start": 3019.2000000000003, "end": 3024.32, "text": " it doesn't take too long for some deep neural net to pop up that can do it too.", "tokens": [50380, 309, 1177, 380, 747, 886, 938, 337, 512, 2452, 18161, 2533, 281, 1665, 493, 300, 393, 360, 309, 886, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15226571900503977, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.00251004402525723}, {"id": 531, "seek": 301888, "start": 3025.6800000000003, "end": 3032.88, "text": " Yeah, I'm there with you. I can, I've stopped betting against neural networks at this point", "tokens": [50704, 865, 11, 286, 478, 456, 365, 291, 13, 286, 393, 11, 286, 600, 5936, 34246, 1970, 18161, 9590, 412, 341, 935, 51064], "temperature": 0.0, "avg_logprob": -0.15226571900503977, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.00251004402525723}, {"id": 532, "seek": 301888, "start": 3032.88, "end": 3038.32, "text": " because they continue to surprise us. What about long term memory? Can neural networks have long", "tokens": [51064, 570, 436, 2354, 281, 6365, 505, 13, 708, 466, 938, 1433, 4675, 30, 1664, 18161, 9590, 362, 938, 51336], "temperature": 0.0, "avg_logprob": -0.15226571900503977, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.00251004402525723}, {"id": 533, "seek": 301888, "start": 3038.32, "end": 3045.36, "text": " term memory or something like knowledge basis? So being able to aggregate important information", "tokens": [51336, 1433, 4675, 420, 746, 411, 3601, 5143, 30, 407, 885, 1075, 281, 26118, 1021, 1589, 51688], "temperature": 0.0, "avg_logprob": -0.15226571900503977, "compression_ratio": 1.6177777777777778, "no_speech_prob": 0.00251004402525723}, {"id": 534, "seek": 304536, "start": 3045.44, "end": 3053.2000000000003, "text": " over long periods of time, that would then serve as useful sort of representations of", "tokens": [50368, 670, 938, 13804, 295, 565, 11, 300, 576, 550, 4596, 382, 4420, 1333, 295, 33358, 295, 50756], "temperature": 0.0, "avg_logprob": -0.16063719782336006, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.005465469788759947}, {"id": 535, "seek": 304536, "start": 3054.0, "end": 3060.48, "text": " state that you can make decisions by so have a long term context based on what you make in the", "tokens": [50796, 1785, 300, 291, 393, 652, 5327, 538, 370, 362, 257, 938, 1433, 4319, 2361, 322, 437, 291, 652, 294, 264, 51120], "temperature": 0.0, "avg_logprob": -0.16063719782336006, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.005465469788759947}, {"id": 536, "seek": 304536, "start": 3060.48, "end": 3067.6, "text": " decision. So in some sense, the parameters already do that. The parameters are an aggregation of the", "tokens": [51120, 3537, 13, 407, 294, 512, 2020, 11, 264, 9834, 1217, 360, 300, 13, 440, 9834, 366, 364, 16743, 399, 295, 264, 51476], "temperature": 0.0, "avg_logprob": -0.16063719782336006, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.005465469788759947}, {"id": 537, "seek": 304536, "start": 3067.6, "end": 3072.08, "text": " day of the neural of the entirety of the neural net experience. And so they count as the long", "tokens": [51476, 786, 295, 264, 18161, 295, 264, 31557, 295, 264, 18161, 2533, 1752, 13, 400, 370, 436, 1207, 382, 264, 938, 51700], "temperature": 0.0, "avg_logprob": -0.16063719782336006, "compression_ratio": 1.7772511848341233, "no_speech_prob": 0.005465469788759947}, {"id": 538, "seek": 307208, "start": 3072.56, "end": 3079.04, "text": " form, long term knowledge. And people have trained various neural nets to act as knowledge basis", "tokens": [50388, 1254, 11, 938, 1433, 3601, 13, 400, 561, 362, 8895, 3683, 18161, 36170, 281, 605, 382, 3601, 5143, 50712], "temperature": 0.0, "avg_logprob": -0.13642305913178818, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.003122833091765642}, {"id": 539, "seek": 307208, "start": 3079.04, "end": 3083.2, "text": " and, you know, investigated with invest, people have investigated language models as knowledge", "tokens": [50712, 293, 11, 291, 458, 11, 30070, 365, 1963, 11, 561, 362, 30070, 2856, 5245, 382, 3601, 50920], "temperature": 0.0, "avg_logprob": -0.13642305913178818, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.003122833091765642}, {"id": 540, "seek": 307208, "start": 3083.2, "end": 3089.7599999999998, "text": " basis. So there is work, there is work there. Yeah, but in some sense, do you think in every sense,", "tokens": [50920, 5143, 13, 407, 456, 307, 589, 11, 456, 307, 589, 456, 13, 865, 11, 457, 294, 512, 2020, 11, 360, 291, 519, 294, 633, 2020, 11, 51248], "temperature": 0.0, "avg_logprob": -0.13642305913178818, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.003122833091765642}, {"id": 541, "seek": 307208, "start": 3089.7599999999998, "end": 3097.2, "text": " do you think there's a, it's all just a matter of coming up with a better mechanism of forgetting", "tokens": [51248, 360, 291, 519, 456, 311, 257, 11, 309, 311, 439, 445, 257, 1871, 295, 1348, 493, 365, 257, 1101, 7513, 295, 25428, 51620], "temperature": 0.0, "avg_logprob": -0.13642305913178818, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.003122833091765642}, {"id": 542, "seek": 309720, "start": 3097.2, "end": 3102.3999999999996, "text": " the useless stuff and remembering the useful stuff? Because right now, I mean, there's not been", "tokens": [50364, 264, 14115, 1507, 293, 20719, 264, 4420, 1507, 30, 1436, 558, 586, 11, 286, 914, 11, 456, 311, 406, 668, 50624], "temperature": 0.0, "avg_logprob": -0.12201792663998073, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.045990876853466034}, {"id": 543, "seek": 309720, "start": 3102.3999999999996, "end": 3107.9199999999996, "text": " mechanisms that do remember really long term information. What do you mean by that precisely?", "tokens": [50624, 15902, 300, 360, 1604, 534, 938, 1433, 1589, 13, 708, 360, 291, 914, 538, 300, 13402, 30, 50900], "temperature": 0.0, "avg_logprob": -0.12201792663998073, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.045990876853466034}, {"id": 544, "seek": 309720, "start": 3108.8799999999997, "end": 3112.0, "text": " Precisely. I like the word precisely. So", "tokens": [50948, 48746, 736, 13, 286, 411, 264, 1349, 13402, 13, 407, 51104], "temperature": 0.0, "avg_logprob": -0.12201792663998073, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.045990876853466034}, {"id": 545, "seek": 309720, "start": 3114.72, "end": 3119.8399999999997, "text": " I'm thinking of the kind of compression of information the knowledge basis represent.", "tokens": [51240, 286, 478, 1953, 295, 264, 733, 295, 19355, 295, 1589, 264, 3601, 5143, 2906, 13, 51496], "temperature": 0.0, "avg_logprob": -0.12201792663998073, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.045990876853466034}, {"id": 546, "seek": 311984, "start": 3120.4, "end": 3127.1200000000003, "text": " Sort of creating a, now I apologize for my sort of human centric thinking about", "tokens": [50392, 26149, 295, 4084, 257, 11, 586, 286, 12328, 337, 452, 1333, 295, 1952, 1489, 1341, 1953, 466, 50728], "temperature": 0.0, "avg_logprob": -0.13702679918957994, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01770607940852642}, {"id": 547, "seek": 311984, "start": 3127.6800000000003, "end": 3133.44, "text": " what knowledge is because neural networks aren't interpretable necessarily with the kind of", "tokens": [50756, 437, 3601, 307, 570, 18161, 9590, 3212, 380, 7302, 712, 4725, 365, 264, 733, 295, 51044], "temperature": 0.0, "avg_logprob": -0.13702679918957994, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01770607940852642}, {"id": 548, "seek": 311984, "start": 3133.44, "end": 3139.6800000000003, "text": " knowledge they have discovered. But a good example for me is knowledge basis being able to build up", "tokens": [51044, 3601, 436, 362, 6941, 13, 583, 257, 665, 1365, 337, 385, 307, 3601, 5143, 885, 1075, 281, 1322, 493, 51356], "temperature": 0.0, "avg_logprob": -0.13702679918957994, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01770607940852642}, {"id": 549, "seek": 311984, "start": 3139.6800000000003, "end": 3145.84, "text": " over time something like the knowledge that Wikipedia represents. It's a really compressed,", "tokens": [51356, 670, 565, 746, 411, 264, 3601, 300, 28999, 8855, 13, 467, 311, 257, 534, 30353, 11, 51664], "temperature": 0.0, "avg_logprob": -0.13702679918957994, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.01770607940852642}, {"id": 550, "seek": 314584, "start": 3145.84, "end": 3153.6000000000004, "text": " structured knowledge base. Obviously, not the actual Wikipedia or the language,", "tokens": [50364, 18519, 3601, 3096, 13, 7580, 11, 406, 264, 3539, 28999, 420, 264, 2856, 11, 50752], "temperature": 0.0, "avg_logprob": -0.15476087423471305, "compression_ratio": 1.8408163265306123, "no_speech_prob": 0.0022870716638863087}, {"id": 551, "seek": 314584, "start": 3154.1600000000003, "end": 3159.2000000000003, "text": " but like a semantic web, the dream that semantic web represented. So it's a really nice compressed", "tokens": [50780, 457, 411, 257, 47982, 3670, 11, 264, 3055, 300, 47982, 3670, 10379, 13, 407, 309, 311, 257, 534, 1481, 30353, 51032], "temperature": 0.0, "avg_logprob": -0.15476087423471305, "compression_ratio": 1.8408163265306123, "no_speech_prob": 0.0022870716638863087}, {"id": 552, "seek": 314584, "start": 3159.2000000000003, "end": 3166.32, "text": " knowledge base or something akin to that in the noninterpretable sense as neural networks would", "tokens": [51032, 3601, 3096, 420, 746, 47540, 281, 300, 294, 264, 2107, 41935, 712, 2020, 382, 18161, 9590, 576, 51388], "temperature": 0.0, "avg_logprob": -0.15476087423471305, "compression_ratio": 1.8408163265306123, "no_speech_prob": 0.0022870716638863087}, {"id": 553, "seek": 314584, "start": 3166.32, "end": 3170.08, "text": " have. Well, the neural networks would be noninterpretable if you look at their rates, but", "tokens": [51388, 362, 13, 1042, 11, 264, 18161, 9590, 576, 312, 2107, 41935, 712, 498, 291, 574, 412, 641, 6846, 11, 457, 51576], "temperature": 0.0, "avg_logprob": -0.15476087423471305, "compression_ratio": 1.8408163265306123, "no_speech_prob": 0.0022870716638863087}, {"id": 554, "seek": 314584, "start": 3170.08, "end": 3173.6800000000003, "text": " their outputs should be very interpretable. Okay, so yeah, how do you, how do you make", "tokens": [51576, 641, 23930, 820, 312, 588, 7302, 712, 13, 1033, 11, 370, 1338, 11, 577, 360, 291, 11, 577, 360, 291, 652, 51756], "temperature": 0.0, "avg_logprob": -0.15476087423471305, "compression_ratio": 1.8408163265306123, "no_speech_prob": 0.0022870716638863087}, {"id": 555, "seek": 317368, "start": 3174.48, "end": 3177.3599999999997, "text": " very smart neural networks like language models interpretable?", "tokens": [50404, 588, 4069, 18161, 9590, 411, 2856, 5245, 7302, 712, 30, 50548], "temperature": 0.0, "avg_logprob": -0.13928756174051538, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.006486756727099419}, {"id": 556, "seek": 317368, "start": 3178.0, "end": 3182.0, "text": " Well, you ask them to generate some text and the text will generally be interpretable.", "tokens": [50580, 1042, 11, 291, 1029, 552, 281, 8460, 512, 2487, 293, 264, 2487, 486, 5101, 312, 7302, 712, 13, 50780], "temperature": 0.0, "avg_logprob": -0.13928756174051538, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.006486756727099419}, {"id": 557, "seek": 317368, "start": 3182.0, "end": 3187.8399999999997, "text": " Do you find that the epitome of interpretability, like can you do better? Like can you, because", "tokens": [50780, 1144, 291, 915, 300, 264, 2388, 270, 423, 295, 7302, 2310, 11, 411, 393, 291, 360, 1101, 30, 1743, 393, 291, 11, 570, 51072], "temperature": 0.0, "avg_logprob": -0.13928756174051538, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.006486756727099419}, {"id": 558, "seek": 317368, "start": 3187.8399999999997, "end": 3193.44, "text": " you can't, okay, I'd like to know what does it know and what doesn't know. I would like the neural", "tokens": [51072, 291, 393, 380, 11, 1392, 11, 286, 1116, 411, 281, 458, 437, 775, 309, 458, 293, 437, 1177, 380, 458, 13, 286, 576, 411, 264, 18161, 51352], "temperature": 0.0, "avg_logprob": -0.13928756174051538, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.006486756727099419}, {"id": 559, "seek": 317368, "start": 3193.44, "end": 3199.2, "text": " network to come up with examples where it's completely dumb and examples where it's completely", "tokens": [51352, 3209, 281, 808, 493, 365, 5110, 689, 309, 311, 2584, 10316, 293, 5110, 689, 309, 311, 2584, 51640], "temperature": 0.0, "avg_logprob": -0.13928756174051538, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.006486756727099419}, {"id": 560, "seek": 319920, "start": 3199.2, "end": 3204.96, "text": " brilliant. And the only way I know how to do that now is to generate a lot of examples and use my", "tokens": [50364, 10248, 13, 400, 264, 787, 636, 286, 458, 577, 281, 360, 300, 586, 307, 281, 8460, 257, 688, 295, 5110, 293, 764, 452, 50652], "temperature": 0.0, "avg_logprob": -0.14346242355088057, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.03207144886255264}, {"id": 561, "seek": 319920, "start": 3204.96, "end": 3211.04, "text": " human judgment. But it would be nice if the neural network had some self-awareness about", "tokens": [50652, 1952, 12216, 13, 583, 309, 576, 312, 1481, 498, 264, 18161, 3209, 632, 512, 2698, 12, 17074, 1287, 466, 50956], "temperature": 0.0, "avg_logprob": -0.14346242355088057, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.03207144886255264}, {"id": 562, "seek": 319920, "start": 3211.6, "end": 3216.8799999999997, "text": " 100%. I'm a big believer in self-awareness. And I think that I think, I think", "tokens": [50984, 2319, 6856, 286, 478, 257, 955, 23892, 294, 2698, 12, 17074, 1287, 13, 400, 286, 519, 300, 286, 519, 11, 286, 519, 51248], "temperature": 0.0, "avg_logprob": -0.14346242355088057, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.03207144886255264}, {"id": 563, "seek": 319920, "start": 3218.72, "end": 3223.6, "text": " neural net self-awareness will allow for things like the capabilities, like the ones you describe,", "tokens": [51340, 18161, 2533, 2698, 12, 17074, 1287, 486, 2089, 337, 721, 411, 264, 10862, 11, 411, 264, 2306, 291, 6786, 11, 51584], "temperature": 0.0, "avg_logprob": -0.14346242355088057, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.03207144886255264}, {"id": 564, "seek": 319920, "start": 3223.6, "end": 3228.24, "text": " like for them to know what they know and what they don't know. And for them to know where to", "tokens": [51584, 411, 337, 552, 281, 458, 437, 436, 458, 293, 437, 436, 500, 380, 458, 13, 400, 337, 552, 281, 458, 689, 281, 51816], "temperature": 0.0, "avg_logprob": -0.14346242355088057, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.03207144886255264}, {"id": 565, "seek": 322824, "start": 3228.24, "end": 3232.16, "text": " invest, to increase their skills most optimally. And to your question of interpretability,", "tokens": [50364, 1963, 11, 281, 3488, 641, 3942, 881, 5028, 379, 13, 400, 281, 428, 1168, 295, 7302, 2310, 11, 50560], "temperature": 0.0, "avg_logprob": -0.12329776867015942, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.005625859368592501}, {"id": 566, "seek": 322824, "start": 3232.16, "end": 3236.3199999999997, "text": " there are actually two answers to that question. One answer is, you know, we have the neural net,", "tokens": [50560, 456, 366, 767, 732, 6338, 281, 300, 1168, 13, 1485, 1867, 307, 11, 291, 458, 11, 321, 362, 264, 18161, 2533, 11, 50768], "temperature": 0.0, "avg_logprob": -0.12329776867015942, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.005625859368592501}, {"id": 567, "seek": 322824, "start": 3236.3199999999997, "end": 3240.56, "text": " so we can analyze the neurons and we can try to understand what the different neurons and", "tokens": [50768, 370, 321, 393, 12477, 264, 22027, 293, 321, 393, 853, 281, 1223, 437, 264, 819, 22027, 293, 50980], "temperature": 0.0, "avg_logprob": -0.12329776867015942, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.005625859368592501}, {"id": 568, "seek": 322824, "start": 3240.56, "end": 3244.9599999999996, "text": " different layers mean. And you can actually do that. And OpenAI has done some work on that.", "tokens": [50980, 819, 7914, 914, 13, 400, 291, 393, 767, 360, 300, 13, 400, 7238, 48698, 575, 1096, 512, 589, 322, 300, 13, 51200], "temperature": 0.0, "avg_logprob": -0.12329776867015942, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.005625859368592501}, {"id": 569, "seek": 322824, "start": 3245.7599999999998, "end": 3251.3599999999997, "text": " But there is a different answer, which is that I would say that's the human centric answer where", "tokens": [51240, 583, 456, 307, 257, 819, 1867, 11, 597, 307, 300, 286, 576, 584, 300, 311, 264, 1952, 1489, 1341, 1867, 689, 51520], "temperature": 0.0, "avg_logprob": -0.12329776867015942, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.005625859368592501}, {"id": 570, "seek": 325136, "start": 3251.36, "end": 3258.08, "text": " you say, you know, you look at a human being, you can't read. How do you know what a human", "tokens": [50364, 291, 584, 11, 291, 458, 11, 291, 574, 412, 257, 1952, 885, 11, 291, 393, 380, 1401, 13, 1012, 360, 291, 458, 437, 257, 1952, 50700], "temperature": 0.0, "avg_logprob": -0.10493545532226563, "compression_ratio": 2.060344827586207, "no_speech_prob": 0.08870235830545425}, {"id": 571, "seek": 325136, "start": 3258.08, "end": 3260.8, "text": " being is thinking? You ask them, you say, Hey, what do you think about this? What do you think", "tokens": [50700, 885, 307, 1953, 30, 509, 1029, 552, 11, 291, 584, 11, 1911, 11, 437, 360, 291, 519, 466, 341, 30, 708, 360, 291, 519, 50836], "temperature": 0.0, "avg_logprob": -0.10493545532226563, "compression_ratio": 2.060344827586207, "no_speech_prob": 0.08870235830545425}, {"id": 572, "seek": 325136, "start": 3260.8, "end": 3266.32, "text": " about that? And you get some answers. The answers you get are sticky. In the sense, you already", "tokens": [50836, 466, 300, 30, 400, 291, 483, 512, 6338, 13, 440, 6338, 291, 483, 366, 14470, 13, 682, 264, 2020, 11, 291, 1217, 51112], "temperature": 0.0, "avg_logprob": -0.10493545532226563, "compression_ratio": 2.060344827586207, "no_speech_prob": 0.08870235830545425}, {"id": 573, "seek": 325136, "start": 3266.32, "end": 3273.36, "text": " have a mental model. You already have a mental model of that human being. You already have an", "tokens": [51112, 362, 257, 4973, 2316, 13, 509, 1217, 362, 257, 4973, 2316, 295, 300, 1952, 885, 13, 509, 1217, 362, 364, 51464], "temperature": 0.0, "avg_logprob": -0.10493545532226563, "compression_ratio": 2.060344827586207, "no_speech_prob": 0.08870235830545425}, {"id": 574, "seek": 325136, "start": 3273.36, "end": 3280.2400000000002, "text": " understanding of like a big conception of what it of that human being, how they think, what they know,", "tokens": [51464, 3701, 295, 411, 257, 955, 30698, 295, 437, 309, 295, 300, 1952, 885, 11, 577, 436, 519, 11, 437, 436, 458, 11, 51808], "temperature": 0.0, "avg_logprob": -0.10493545532226563, "compression_ratio": 2.060344827586207, "no_speech_prob": 0.08870235830545425}, {"id": 575, "seek": 328024, "start": 3280.24, "end": 3286.8799999999997, "text": " how they see the world, and then everything you ask, you're adding on to that. And that stickiness", "tokens": [50364, 577, 436, 536, 264, 1002, 11, 293, 550, 1203, 291, 1029, 11, 291, 434, 5127, 322, 281, 300, 13, 400, 300, 2897, 1324, 50696], "temperature": 0.0, "avg_logprob": -0.12442155854891887, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0022513524163514376}, {"id": 576, "seek": 328024, "start": 3287.52, "end": 3293.6, "text": " seems to be, that's one of the really interesting qualities of the human being is that information", "tokens": [50728, 2544, 281, 312, 11, 300, 311, 472, 295, 264, 534, 1880, 16477, 295, 264, 1952, 885, 307, 300, 1589, 51032], "temperature": 0.0, "avg_logprob": -0.12442155854891887, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0022513524163514376}, {"id": 577, "seek": 328024, "start": 3293.6, "end": 3299.52, "text": " is sticky. You don't, you seem to remember the useful stuff, aggregate it well, and forget most", "tokens": [51032, 307, 14470, 13, 509, 500, 380, 11, 291, 1643, 281, 1604, 264, 4420, 1507, 11, 26118, 309, 731, 11, 293, 2870, 881, 51328], "temperature": 0.0, "avg_logprob": -0.12442155854891887, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0022513524163514376}, {"id": 578, "seek": 328024, "start": 3299.52, "end": 3305.04, "text": " of the information that's not useful, that process. But that's also pretty similar to the", "tokens": [51328, 295, 264, 1589, 300, 311, 406, 4420, 11, 300, 1399, 13, 583, 300, 311, 611, 1238, 2531, 281, 264, 51604], "temperature": 0.0, "avg_logprob": -0.12442155854891887, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0022513524163514376}, {"id": 579, "seek": 328024, "start": 3305.04, "end": 3309.6, "text": " process that neural networks do is just that neural networks are much crappier at this time.", "tokens": [51604, 1399, 300, 18161, 9590, 360, 307, 445, 300, 18161, 9590, 366, 709, 2094, 427, 811, 412, 341, 565, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12442155854891887, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0022513524163514376}, {"id": 580, "seek": 331024, "start": 3310.4799999999996, "end": 3315.6, "text": " It doesn't seem to be fundamentally that different. But just stick on reasoning for a little longer.", "tokens": [50376, 467, 1177, 380, 1643, 281, 312, 17879, 300, 819, 13, 583, 445, 2897, 322, 21577, 337, 257, 707, 2854, 13, 50632], "temperature": 0.0, "avg_logprob": -0.15891544944361635, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.0007208003080449998}, {"id": 581, "seek": 331024, "start": 3317.12, "end": 3324.56, "text": " He said, why not? Why can't I reason? What's a good impressive feat benchmark to you of reasoning", "tokens": [50708, 634, 848, 11, 983, 406, 30, 1545, 393, 380, 286, 1778, 30, 708, 311, 257, 665, 8992, 15425, 18927, 281, 291, 295, 21577, 51080], "temperature": 0.0, "avg_logprob": -0.15891544944361635, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.0007208003080449998}, {"id": 582, "seek": 331024, "start": 3327.3599999999997, "end": 3331.6, "text": " that you'll be impressed by if neural networks were able to do? Is that something you already have", "tokens": [51220, 300, 291, 603, 312, 11679, 538, 498, 18161, 9590, 645, 1075, 281, 360, 30, 1119, 300, 746, 291, 1217, 362, 51432], "temperature": 0.0, "avg_logprob": -0.15891544944361635, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.0007208003080449998}, {"id": 583, "seek": 331024, "start": 3331.6, "end": 3338.64, "text": " in mind? Well, I think writing, writing really good code. I think proving really hard theorems,", "tokens": [51432, 294, 1575, 30, 1042, 11, 286, 519, 3579, 11, 3579, 534, 665, 3089, 13, 286, 519, 27221, 534, 1152, 10299, 2592, 11, 51784], "temperature": 0.0, "avg_logprob": -0.15891544944361635, "compression_ratio": 1.6040816326530611, "no_speech_prob": 0.0007208003080449998}, {"id": 584, "seek": 333864, "start": 3339.3599999999997, "end": 3342.7999999999997, "text": " solving open ended problems without of the box solutions.", "tokens": [50400, 12606, 1269, 4590, 2740, 1553, 295, 264, 2424, 6547, 13, 50572], "temperature": 0.0, "avg_logprob": -0.13172741986195977, "compression_ratio": 1.6968503937007875, "no_speech_prob": 0.0010481190402060747}, {"id": 585, "seek": 333864, "start": 3345.8399999999997, "end": 3351.6, "text": " And sort of theorem type mathematical problems. Yeah, I think those ones are a very natural", "tokens": [50724, 400, 1333, 295, 20904, 2010, 18894, 2740, 13, 865, 11, 286, 519, 729, 2306, 366, 257, 588, 3303, 51012], "temperature": 0.0, "avg_logprob": -0.13172741986195977, "compression_ratio": 1.6968503937007875, "no_speech_prob": 0.0010481190402060747}, {"id": 586, "seek": 333864, "start": 3351.6, "end": 3355.68, "text": " example as well. You know, if you can prove an unproven theorem, then it's hard to argue", "tokens": [51012, 1365, 382, 731, 13, 509, 458, 11, 498, 291, 393, 7081, 364, 517, 4318, 553, 20904, 11, 550, 309, 311, 1152, 281, 9695, 51216], "temperature": 0.0, "avg_logprob": -0.13172741986195977, "compression_ratio": 1.6968503937007875, "no_speech_prob": 0.0010481190402060747}, {"id": 587, "seek": 333864, "start": 3355.68, "end": 3360.8799999999997, "text": " don't reason. And so by the way, and this comes back to the point about the hard results, you know,", "tokens": [51216, 500, 380, 1778, 13, 400, 370, 538, 264, 636, 11, 293, 341, 1487, 646, 281, 264, 935, 466, 264, 1152, 3542, 11, 291, 458, 11, 51476], "temperature": 0.0, "avg_logprob": -0.13172741986195977, "compression_ratio": 1.6968503937007875, "no_speech_prob": 0.0010481190402060747}, {"id": 588, "seek": 333864, "start": 3360.8799999999997, "end": 3366.0, "text": " if you got a hard, if you have machine learning, deep learning as a field is very fortunate,", "tokens": [51476, 498, 291, 658, 257, 1152, 11, 498, 291, 362, 3479, 2539, 11, 2452, 2539, 382, 257, 2519, 307, 588, 14096, 11, 51732], "temperature": 0.0, "avg_logprob": -0.13172741986195977, "compression_ratio": 1.6968503937007875, "no_speech_prob": 0.0010481190402060747}, {"id": 589, "seek": 336600, "start": 3366.0, "end": 3372.16, "text": " because we have the ability to sometimes produce these unambiguous results. And when they happen,", "tokens": [50364, 570, 321, 362, 264, 3485, 281, 2171, 5258, 613, 517, 2173, 30525, 3542, 13, 400, 562, 436, 1051, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12472300402886044, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.004197920206934214}, {"id": 590, "seek": 336600, "start": 3372.16, "end": 3377.04, "text": " the debate changes, the conversation changes, it's a converse, you have the ability to produce", "tokens": [50672, 264, 7958, 2962, 11, 264, 3761, 2962, 11, 309, 311, 257, 416, 4308, 11, 291, 362, 264, 3485, 281, 5258, 50916], "temperature": 0.0, "avg_logprob": -0.12472300402886044, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.004197920206934214}, {"id": 591, "seek": 336600, "start": 3377.04, "end": 3382.08, "text": " conversation changing results conversation. And then of course, just like you said, people kind", "tokens": [50916, 3761, 4473, 3542, 3761, 13, 400, 550, 295, 1164, 11, 445, 411, 291, 848, 11, 561, 733, 51168], "temperature": 0.0, "avg_logprob": -0.12472300402886044, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.004197920206934214}, {"id": 592, "seek": 336600, "start": 3382.08, "end": 3386.24, "text": " of take that for granted, say that wasn't actually a hard problem. Well, I mean, at some point,", "tokens": [51168, 295, 747, 300, 337, 12344, 11, 584, 300, 2067, 380, 767, 257, 1152, 1154, 13, 1042, 11, 286, 914, 11, 412, 512, 935, 11, 51376], "temperature": 0.0, "avg_logprob": -0.12472300402886044, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.004197920206934214}, {"id": 593, "seek": 336600, "start": 3386.24, "end": 3392.88, "text": " we'll probably run out of hard problems. Yeah, that whole mortality thing is kind of kind of a", "tokens": [51376, 321, 603, 1391, 1190, 484, 295, 1152, 2740, 13, 865, 11, 300, 1379, 23330, 551, 307, 733, 295, 733, 295, 257, 51708], "temperature": 0.0, "avg_logprob": -0.12472300402886044, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.004197920206934214}, {"id": 594, "seek": 339288, "start": 3392.88, "end": 3398.32, "text": " sticky problem that we haven't quite figured out. Maybe we'll solve that one. I think one of the", "tokens": [50364, 14470, 1154, 300, 321, 2378, 380, 1596, 8932, 484, 13, 2704, 321, 603, 5039, 300, 472, 13, 286, 519, 472, 295, 264, 50636], "temperature": 0.0, "avg_logprob": -0.11668141399111066, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.009266411885619164}, {"id": 595, "seek": 339288, "start": 3398.32, "end": 3402.88, "text": " fascinating things in your entire body of work, but also the work at OpenAI recently,", "tokens": [50636, 10343, 721, 294, 428, 2302, 1772, 295, 589, 11, 457, 611, 264, 589, 412, 7238, 48698, 3938, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11668141399111066, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.009266411885619164}, {"id": 596, "seek": 339288, "start": 3402.88, "end": 3408.48, "text": " one of the conversation changers has been in the world of language models. Can you briefly kind of", "tokens": [50864, 472, 295, 264, 3761, 1534, 433, 575, 668, 294, 264, 1002, 295, 2856, 5245, 13, 1664, 291, 10515, 733, 295, 51144], "temperature": 0.0, "avg_logprob": -0.11668141399111066, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.009266411885619164}, {"id": 597, "seek": 339288, "start": 3409.28, "end": 3413.76, "text": " try to describe the recent history of using neural networks in the domain of language and text?", "tokens": [51184, 853, 281, 6786, 264, 5162, 2503, 295, 1228, 18161, 9590, 294, 264, 9274, 295, 2856, 293, 2487, 30, 51408], "temperature": 0.0, "avg_logprob": -0.11668141399111066, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.009266411885619164}, {"id": 598, "seek": 339288, "start": 3414.48, "end": 3420.1600000000003, "text": " Well, there's been lots of history. I think I think the Elman network was was a small,", "tokens": [51444, 1042, 11, 456, 311, 668, 3195, 295, 2503, 13, 286, 519, 286, 519, 264, 2699, 1601, 3209, 390, 390, 257, 1359, 11, 51728], "temperature": 0.0, "avg_logprob": -0.11668141399111066, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.009266411885619164}, {"id": 599, "seek": 342016, "start": 3420.24, "end": 3425.2, "text": " tiny recurrent neural network applied to language back in the 80s. So the history is really,", "tokens": [50368, 5870, 18680, 1753, 18161, 3209, 6456, 281, 2856, 646, 294, 264, 4688, 82, 13, 407, 264, 2503, 307, 534, 11, 50616], "temperature": 0.0, "avg_logprob": -0.11453959263792825, "compression_ratio": 1.87109375, "no_speech_prob": 0.0037056314758956432}, {"id": 600, "seek": 342016, "start": 3426.3999999999996, "end": 3433.2799999999997, "text": " you know, fairly long, at least. And the thing that started the thing that changed the trajectory", "tokens": [50676, 291, 458, 11, 6457, 938, 11, 412, 1935, 13, 400, 264, 551, 300, 1409, 264, 551, 300, 3105, 264, 21512, 51020], "temperature": 0.0, "avg_logprob": -0.11453959263792825, "compression_ratio": 1.87109375, "no_speech_prob": 0.0037056314758956432}, {"id": 601, "seek": 342016, "start": 3433.2799999999997, "end": 3437.92, "text": " of neural networks and language is the thing that changed the trajectory of all deep learning and", "tokens": [51020, 295, 18161, 9590, 293, 2856, 307, 264, 551, 300, 3105, 264, 21512, 295, 439, 2452, 2539, 293, 51252], "temperature": 0.0, "avg_logprob": -0.11453959263792825, "compression_ratio": 1.87109375, "no_speech_prob": 0.0037056314758956432}, {"id": 602, "seek": 342016, "start": 3437.92, "end": 3443.68, "text": " that's data and compute. So suddenly you move from small language models, which learn a little bit.", "tokens": [51252, 300, 311, 1412, 293, 14722, 13, 407, 5800, 291, 1286, 490, 1359, 2856, 5245, 11, 597, 1466, 257, 707, 857, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11453959263792825, "compression_ratio": 1.87109375, "no_speech_prob": 0.0037056314758956432}, {"id": 603, "seek": 342016, "start": 3444.24, "end": 3448.96, "text": " And with language models, in particular, you can, there's a very clear explanation for why", "tokens": [51568, 400, 365, 2856, 5245, 11, 294, 1729, 11, 291, 393, 11, 456, 311, 257, 588, 1850, 10835, 337, 983, 51804], "temperature": 0.0, "avg_logprob": -0.11453959263792825, "compression_ratio": 1.87109375, "no_speech_prob": 0.0037056314758956432}, {"id": 604, "seek": 344896, "start": 3449.04, "end": 3453.2, "text": " they need to be large to be good, because they're trying to predict the next word.", "tokens": [50368, 436, 643, 281, 312, 2416, 281, 312, 665, 11, 570, 436, 434, 1382, 281, 6069, 264, 958, 1349, 13, 50576], "temperature": 0.0, "avg_logprob": -0.13175621399512658, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.007780799642205238}, {"id": 605, "seek": 344896, "start": 3454.48, "end": 3461.44, "text": " So when you don't know anything, you'll notice very, very broad strokes, surface level patterns,", "tokens": [50640, 407, 562, 291, 500, 380, 458, 1340, 11, 291, 603, 3449, 588, 11, 588, 4152, 24493, 11, 3753, 1496, 8294, 11, 50988], "temperature": 0.0, "avg_logprob": -0.13175621399512658, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.007780799642205238}, {"id": 606, "seek": 344896, "start": 3461.44, "end": 3466.4, "text": " like sometimes there are characters and there is space between those characters,", "tokens": [50988, 411, 2171, 456, 366, 4342, 293, 456, 307, 1901, 1296, 729, 4342, 11, 51236], "temperature": 0.0, "avg_logprob": -0.13175621399512658, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.007780799642205238}, {"id": 607, "seek": 344896, "start": 3466.4, "end": 3470.56, "text": " you'll notice this pattern. And you'll notice that sometimes there is a comma and then the", "tokens": [51236, 291, 603, 3449, 341, 5102, 13, 400, 291, 603, 3449, 300, 2171, 456, 307, 257, 22117, 293, 550, 264, 51444], "temperature": 0.0, "avg_logprob": -0.13175621399512658, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.007780799642205238}, {"id": 608, "seek": 344896, "start": 3470.56, "end": 3474.88, "text": " next character is a capital letter, you'll notice that pattern. Eventually, you may start to notice", "tokens": [51444, 958, 2517, 307, 257, 4238, 5063, 11, 291, 603, 3449, 300, 5102, 13, 17586, 11, 291, 815, 722, 281, 3449, 51660], "temperature": 0.0, "avg_logprob": -0.13175621399512658, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.007780799642205238}, {"id": 609, "seek": 347488, "start": 3474.88, "end": 3479.28, "text": " that there are certain words occur often, you may notice that spellings are a thing,", "tokens": [50364, 300, 456, 366, 1629, 2283, 5160, 2049, 11, 291, 815, 3449, 300, 9827, 1109, 366, 257, 551, 11, 50584], "temperature": 0.0, "avg_logprob": -0.11498974247982627, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0022159831132739782}, {"id": 610, "seek": 347488, "start": 3479.28, "end": 3484.96, "text": " you may notice syntax. And when you get really good at all these, you start to notice the semantics,", "tokens": [50584, 291, 815, 3449, 28431, 13, 400, 562, 291, 483, 534, 665, 412, 439, 613, 11, 291, 722, 281, 3449, 264, 4361, 45298, 11, 50868], "temperature": 0.0, "avg_logprob": -0.11498974247982627, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0022159831132739782}, {"id": 611, "seek": 347488, "start": 3485.76, "end": 3490.48, "text": " you start to notice the facts. But for that to happen, the language model needs to be larger.", "tokens": [50908, 291, 722, 281, 3449, 264, 9130, 13, 583, 337, 300, 281, 1051, 11, 264, 2856, 2316, 2203, 281, 312, 4833, 13, 51144], "temperature": 0.0, "avg_logprob": -0.11498974247982627, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0022159831132739782}, {"id": 612, "seek": 347488, "start": 3491.36, "end": 3496.32, "text": " So that's, let's linger on that, because that's where you and Noam Chomsky disagree.", "tokens": [51188, 407, 300, 311, 11, 718, 311, 45657, 322, 300, 11, 570, 300, 311, 689, 291, 293, 883, 335, 761, 4785, 4133, 14091, 13, 51436], "temperature": 0.0, "avg_logprob": -0.11498974247982627, "compression_ratio": 1.7009345794392523, "no_speech_prob": 0.0022159831132739782}, {"id": 613, "seek": 349632, "start": 3496.32, "end": 3505.6000000000004, "text": " So you think we're actually taking incremental steps, a sort of larger network, larger compute", "tokens": [50364, 407, 291, 519, 321, 434, 767, 1940, 35759, 4439, 11, 257, 1333, 295, 4833, 3209, 11, 4833, 14722, 50828], "temperature": 0.0, "avg_logprob": -0.15058381855487823, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.008438412100076675}, {"id": 614, "seek": 349632, "start": 3505.6000000000004, "end": 3514.6400000000003, "text": " will be able to get to the semantics, be able to understand language without what Noam likes to", "tokens": [50828, 486, 312, 1075, 281, 483, 281, 264, 4361, 45298, 11, 312, 1075, 281, 1223, 2856, 1553, 437, 883, 335, 5902, 281, 51280], "temperature": 0.0, "avg_logprob": -0.15058381855487823, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.008438412100076675}, {"id": 615, "seek": 349632, "start": 3514.6400000000003, "end": 3522.0, "text": " sort of think of as a fundamental understandings of the structure of language, like imposing", "tokens": [51280, 1333, 295, 519, 295, 382, 257, 8088, 1223, 1109, 295, 264, 3877, 295, 2856, 11, 411, 40288, 51648], "temperature": 0.0, "avg_logprob": -0.15058381855487823, "compression_ratio": 1.6079545454545454, "no_speech_prob": 0.008438412100076675}, {"id": 616, "seek": 352200, "start": 3522.08, "end": 3527.92, "text": " your theory of language onto the learning mechanism. So you're saying the learning,", "tokens": [50368, 428, 5261, 295, 2856, 3911, 264, 2539, 7513, 13, 407, 291, 434, 1566, 264, 2539, 11, 50660], "temperature": 0.0, "avg_logprob": -0.11058367215670072, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.027060896158218384}, {"id": 617, "seek": 352200, "start": 3527.92, "end": 3532.4, "text": " you can learn from raw data, the mechanism that underlies language?", "tokens": [50660, 291, 393, 1466, 490, 8936, 1412, 11, 264, 7513, 300, 833, 24119, 2856, 30, 50884], "temperature": 0.0, "avg_logprob": -0.11058367215670072, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.027060896158218384}, {"id": 618, "seek": 352200, "start": 3533.36, "end": 3538.48, "text": " Well, I think it's pretty likely. But I also want to say that I don't really", "tokens": [50932, 1042, 11, 286, 519, 309, 311, 1238, 3700, 13, 583, 286, 611, 528, 281, 584, 300, 286, 500, 380, 534, 51188], "temperature": 0.0, "avg_logprob": -0.11058367215670072, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.027060896158218384}, {"id": 619, "seek": 352200, "start": 3540.4, "end": 3547.28, "text": " know precisely what Chomsky means when he talks about him. You said something about imposing", "tokens": [51284, 458, 13402, 437, 761, 4785, 4133, 1355, 562, 415, 6686, 466, 796, 13, 509, 848, 746, 466, 40288, 51628], "temperature": 0.0, "avg_logprob": -0.11058367215670072, "compression_ratio": 1.5507246376811594, "no_speech_prob": 0.027060896158218384}, {"id": 620, "seek": 354728, "start": 3547.28, "end": 3552.8, "text": " your structure on language. I'm not 100% sure what he means. But empirically, it seems that when", "tokens": [50364, 428, 3877, 322, 2856, 13, 286, 478, 406, 2319, 4, 988, 437, 415, 1355, 13, 583, 25790, 984, 11, 309, 2544, 300, 562, 50640], "temperature": 0.0, "avg_logprob": -0.10082698079337062, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.043265201151371}, {"id": 621, "seek": 354728, "start": 3552.8, "end": 3556.5600000000004, "text": " you inspect those larger language models, they exhibit signs of understanding the semantics,", "tokens": [50640, 291, 15018, 729, 4833, 2856, 5245, 11, 436, 20487, 7880, 295, 3701, 264, 4361, 45298, 11, 50828], "temperature": 0.0, "avg_logprob": -0.10082698079337062, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.043265201151371}, {"id": 622, "seek": 354728, "start": 3556.5600000000004, "end": 3560.4, "text": " whereas the smaller language models do not. We've seen that a few years ago when we", "tokens": [50828, 9735, 264, 4356, 2856, 5245, 360, 406, 13, 492, 600, 1612, 300, 257, 1326, 924, 2057, 562, 321, 51020], "temperature": 0.0, "avg_logprob": -0.10082698079337062, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.043265201151371}, {"id": 623, "seek": 354728, "start": 3560.4, "end": 3566.0, "text": " did work on the sentiment neuron, we trained a small, you know, smallish LSTM to predict the", "tokens": [51020, 630, 589, 322, 264, 16149, 34090, 11, 321, 8895, 257, 1359, 11, 291, 458, 11, 1359, 742, 441, 6840, 44, 281, 6069, 264, 51300], "temperature": 0.0, "avg_logprob": -0.10082698079337062, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.043265201151371}, {"id": 624, "seek": 354728, "start": 3566.0, "end": 3571.92, "text": " next character in Amazon reviews. And we noticed that when you increase the size of the LSTM from", "tokens": [51300, 958, 2517, 294, 6795, 10229, 13, 400, 321, 5694, 300, 562, 291, 3488, 264, 2744, 295, 264, 441, 6840, 44, 490, 51596], "temperature": 0.0, "avg_logprob": -0.10082698079337062, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.043265201151371}, {"id": 625, "seek": 357192, "start": 3571.92, "end": 3577.76, "text": " 500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent the sentiment", "tokens": [50364, 5923, 441, 6840, 44, 5438, 281, 1017, 11, 1360, 441, 6840, 44, 5438, 11, 550, 472, 295, 264, 22027, 3719, 281, 2906, 264, 16149, 50656], "temperature": 0.0, "avg_logprob": -0.12098424802950727, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.017972085624933243}, {"id": 626, "seek": 357192, "start": 3578.48, "end": 3584.7200000000003, "text": " of the article, of sorry, of their view. Now, why is that sentiment is a pretty semantic", "tokens": [50692, 295, 264, 7222, 11, 295, 2597, 11, 295, 641, 1910, 13, 823, 11, 983, 307, 300, 16149, 307, 257, 1238, 47982, 51004], "temperature": 0.0, "avg_logprob": -0.12098424802950727, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.017972085624933243}, {"id": 627, "seek": 357192, "start": 3584.7200000000003, "end": 3588.64, "text": " attribute? It's not a syntactic attribute. And for people who might not know, I don't know if", "tokens": [51004, 19667, 30, 467, 311, 406, 257, 23980, 19892, 19667, 13, 400, 337, 561, 567, 1062, 406, 458, 11, 286, 500, 380, 458, 498, 51200], "temperature": 0.0, "avg_logprob": -0.12098424802950727, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.017972085624933243}, {"id": 628, "seek": 357192, "start": 3588.64, "end": 3592.4, "text": " that's a standard term, but sentiment is whether it's a positive or negative review. That's right.", "tokens": [51200, 300, 311, 257, 3832, 1433, 11, 457, 16149, 307, 1968, 309, 311, 257, 3353, 420, 3671, 3131, 13, 663, 311, 558, 13, 51388], "temperature": 0.0, "avg_logprob": -0.12098424802950727, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.017972085624933243}, {"id": 629, "seek": 357192, "start": 3592.4, "end": 3597.6800000000003, "text": " Like, is the person happy with something or is the person unhappy with something? And so here we had", "tokens": [51388, 1743, 11, 307, 264, 954, 2055, 365, 746, 420, 307, 264, 954, 22172, 365, 746, 30, 400, 370, 510, 321, 632, 51652], "temperature": 0.0, "avg_logprob": -0.12098424802950727, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.017972085624933243}, {"id": 630, "seek": 359768, "start": 3597.68, "end": 3602.7999999999997, "text": " very clear evidence that a small neural net does not capture sentiment while a large neural net", "tokens": [50364, 588, 1850, 4467, 300, 257, 1359, 18161, 2533, 775, 406, 7983, 16149, 1339, 257, 2416, 18161, 2533, 50620], "temperature": 0.0, "avg_logprob": -0.0980092297081186, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.007116299122571945}, {"id": 631, "seek": 359768, "start": 3602.7999999999997, "end": 3608.72, "text": " does. And why is that? Well, our theory is that at some point, you run out of syntax to models,", "tokens": [50620, 775, 13, 400, 983, 307, 300, 30, 1042, 11, 527, 5261, 307, 300, 412, 512, 935, 11, 291, 1190, 484, 295, 28431, 281, 5245, 11, 50916], "temperature": 0.0, "avg_logprob": -0.0980092297081186, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.007116299122571945}, {"id": 632, "seek": 359768, "start": 3608.72, "end": 3615.7599999999998, "text": " you start to gotta focus on something else. And with size, you quickly run out of syntax to model,", "tokens": [50916, 291, 722, 281, 3428, 1879, 322, 746, 1646, 13, 400, 365, 2744, 11, 291, 2661, 1190, 484, 295, 28431, 281, 2316, 11, 51268], "temperature": 0.0, "avg_logprob": -0.0980092297081186, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.007116299122571945}, {"id": 633, "seek": 359768, "start": 3615.7599999999998, "end": 3620.56, "text": " and then you really start to focus on the semantics is would be the idea. That's right. And so I don't", "tokens": [51268, 293, 550, 291, 534, 722, 281, 1879, 322, 264, 4361, 45298, 307, 576, 312, 264, 1558, 13, 663, 311, 558, 13, 400, 370, 286, 500, 380, 51508], "temperature": 0.0, "avg_logprob": -0.0980092297081186, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.007116299122571945}, {"id": 634, "seek": 359768, "start": 3620.56, "end": 3624.72, "text": " want to imply that our models have complete semantic understanding, because that's not true.", "tokens": [51508, 528, 281, 33616, 300, 527, 5245, 362, 3566, 47982, 3701, 11, 570, 300, 311, 406, 2074, 13, 51716], "temperature": 0.0, "avg_logprob": -0.0980092297081186, "compression_ratio": 1.7802197802197801, "no_speech_prob": 0.007116299122571945}, {"id": 635, "seek": 362472, "start": 3625.2799999999997, "end": 3630.72, "text": " But they definitely are showing signs of semantic understanding, partial semantic understanding,", "tokens": [50392, 583, 436, 2138, 366, 4099, 7880, 295, 47982, 3701, 11, 14641, 47982, 3701, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11775053872002496, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0029806571546941996}, {"id": 636, "seek": 362472, "start": 3630.72, "end": 3638.0, "text": " but the smaller models do not show that those signs. Can you take a step back and say, what is GPT2,", "tokens": [50664, 457, 264, 4356, 5245, 360, 406, 855, 300, 729, 7880, 13, 1664, 291, 747, 257, 1823, 646, 293, 584, 11, 437, 307, 26039, 51, 17, 11, 51028], "temperature": 0.0, "avg_logprob": -0.11775053872002496, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0029806571546941996}, {"id": 637, "seek": 362472, "start": 3638.0, "end": 3643.04, "text": " which is one of the big language models that was the conversation changer in the past couple of", "tokens": [51028, 597, 307, 472, 295, 264, 955, 2856, 5245, 300, 390, 264, 3761, 22822, 294, 264, 1791, 1916, 295, 51280], "temperature": 0.0, "avg_logprob": -0.11775053872002496, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0029806571546941996}, {"id": 638, "seek": 362472, "start": 3643.04, "end": 3651.3599999999997, "text": " years? Yes. So GPT2 is a transformer with one and a half billion parameters that was trained on", "tokens": [51280, 924, 30, 1079, 13, 407, 26039, 51, 17, 307, 257, 31782, 365, 472, 293, 257, 1922, 5218, 9834, 300, 390, 8895, 322, 51696], "temperature": 0.0, "avg_logprob": -0.11775053872002496, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.0029806571546941996}, {"id": 639, "seek": 365136, "start": 3651.44, "end": 3660.08, "text": " up on about 40 billion tokens of text, which were obtained from web pages that were linked to", "tokens": [50368, 493, 322, 466, 3356, 5218, 22667, 295, 2487, 11, 597, 645, 14879, 490, 3670, 7183, 300, 645, 9408, 281, 50800], "temperature": 0.0, "avg_logprob": -0.14954442093052814, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.002888669027015567}, {"id": 640, "seek": 365136, "start": 3660.08, "end": 3664.7200000000003, "text": " from Reddit articles with more than three uploads. And what's the transformer? The transformer,", "tokens": [50800, 490, 32210, 11290, 365, 544, 813, 1045, 48611, 13, 400, 437, 311, 264, 31782, 30, 440, 31782, 11, 51032], "temperature": 0.0, "avg_logprob": -0.14954442093052814, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.002888669027015567}, {"id": 641, "seek": 365136, "start": 3664.7200000000003, "end": 3668.96, "text": " it's the most important advance in neural network architectures in recent history.", "tokens": [51032, 309, 311, 264, 881, 1021, 7295, 294, 18161, 3209, 6331, 1303, 294, 5162, 2503, 13, 51244], "temperature": 0.0, "avg_logprob": -0.14954442093052814, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.002888669027015567}, {"id": 642, "seek": 365136, "start": 3669.6800000000003, "end": 3674.08, "text": " What is attention maybe to because I think that's an interesting idea, not necessarily sort of", "tokens": [51280, 708, 307, 3202, 1310, 281, 570, 286, 519, 300, 311, 364, 1880, 1558, 11, 406, 4725, 1333, 295, 51500], "temperature": 0.0, "avg_logprob": -0.14954442093052814, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.002888669027015567}, {"id": 643, "seek": 365136, "start": 3674.08, "end": 3680.48, "text": " technically speaking, but the idea of attention versus maybe what recurrent neural networks", "tokens": [51500, 12120, 4124, 11, 457, 264, 1558, 295, 3202, 5717, 1310, 437, 18680, 1753, 18161, 9590, 51820], "temperature": 0.0, "avg_logprob": -0.14954442093052814, "compression_ratio": 1.6630434782608696, "no_speech_prob": 0.002888669027015567}, {"id": 644, "seek": 368048, "start": 3680.48, "end": 3685.76, "text": " represent. Yeah. So the thing is, the transformer is a combination of multiple ideas simultaneously", "tokens": [50364, 2906, 13, 865, 13, 407, 264, 551, 307, 11, 264, 31782, 307, 257, 6562, 295, 3866, 3487, 16561, 50628], "temperature": 0.0, "avg_logprob": -0.0897814432779948, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0018366200383752584}, {"id": 645, "seek": 368048, "start": 3685.76, "end": 3691.76, "text": " of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key.", "tokens": [50628, 295, 597, 3202, 307, 472, 13, 1144, 291, 519, 3202, 307, 264, 2141, 30, 883, 11, 309, 311, 257, 2141, 11, 457, 309, 311, 406, 264, 2141, 13, 50928], "temperature": 0.0, "avg_logprob": -0.0897814432779948, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0018366200383752584}, {"id": 646, "seek": 368048, "start": 3692.32, "end": 3697.68, "text": " The transformer is successful because it is the simultaneous combination of multiple ideas. And", "tokens": [50956, 440, 31782, 307, 4406, 570, 309, 307, 264, 46218, 6562, 295, 3866, 3487, 13, 400, 51224], "temperature": 0.0, "avg_logprob": -0.0897814432779948, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0018366200383752584}, {"id": 647, "seek": 368048, "start": 3697.68, "end": 3703.04, "text": " if you were to remove either idea, it would be much less successful. So the transformer uses a", "tokens": [51224, 498, 291, 645, 281, 4159, 2139, 1558, 11, 309, 576, 312, 709, 1570, 4406, 13, 407, 264, 31782, 4960, 257, 51492], "temperature": 0.0, "avg_logprob": -0.0897814432779948, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0018366200383752584}, {"id": 648, "seek": 368048, "start": 3703.04, "end": 3707.28, "text": " lot of attention, but attention exists for a few years. So that can't be the main innovation.", "tokens": [51492, 688, 295, 3202, 11, 457, 3202, 8198, 337, 257, 1326, 924, 13, 407, 300, 393, 380, 312, 264, 2135, 8504, 13, 51704], "temperature": 0.0, "avg_logprob": -0.0897814432779948, "compression_ratio": 1.8980392156862744, "no_speech_prob": 0.0018366200383752584}, {"id": 649, "seek": 370728, "start": 3708.2400000000002, "end": 3714.88, "text": " The transformer is designed in such a way that it runs really fast on the GPU.", "tokens": [50412, 440, 31782, 307, 4761, 294, 1270, 257, 636, 300, 309, 6676, 534, 2370, 322, 264, 18407, 13, 50744], "temperature": 0.0, "avg_logprob": -0.08454382556608353, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010806865757331252}, {"id": 650, "seek": 370728, "start": 3716.0, "end": 3720.2400000000002, "text": " And that makes a huge amount of difference. This is one thing. The second thing is that", "tokens": [50800, 400, 300, 1669, 257, 2603, 2372, 295, 2649, 13, 639, 307, 472, 551, 13, 440, 1150, 551, 307, 300, 51012], "temperature": 0.0, "avg_logprob": -0.08454382556608353, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010806865757331252}, {"id": 651, "seek": 370728, "start": 3720.2400000000002, "end": 3726.32, "text": " transformer is not recurrent. And that is really important too, because it is more shallow and", "tokens": [51012, 31782, 307, 406, 18680, 1753, 13, 400, 300, 307, 534, 1021, 886, 11, 570, 309, 307, 544, 20488, 293, 51316], "temperature": 0.0, "avg_logprob": -0.08454382556608353, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010806865757331252}, {"id": 652, "seek": 370728, "start": 3726.32, "end": 3732.4, "text": " therefore much easier to optimize. So in other words, uses attention. It is, it is a really great", "tokens": [51316, 4412, 709, 3571, 281, 19719, 13, 407, 294, 661, 2283, 11, 4960, 3202, 13, 467, 307, 11, 309, 307, 257, 534, 869, 51620], "temperature": 0.0, "avg_logprob": -0.08454382556608353, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.0010806865757331252}, {"id": 653, "seek": 373240, "start": 3732.48, "end": 3737.76, "text": " fit to the GPU. And it is not recurrent. So therefore, less deep and easier to optimize.", "tokens": [50368, 3318, 281, 264, 18407, 13, 400, 309, 307, 406, 18680, 1753, 13, 407, 4412, 11, 1570, 2452, 293, 3571, 281, 19719, 13, 50632], "temperature": 0.0, "avg_logprob": -0.08638616510339685, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0042631481774151325}, {"id": 654, "seek": 373240, "start": 3737.76, "end": 3742.7200000000003, "text": " And the combination of those factors make it successful. So now it makes it makes great", "tokens": [50632, 400, 264, 6562, 295, 729, 6771, 652, 309, 4406, 13, 407, 586, 309, 1669, 309, 1669, 869, 50880], "temperature": 0.0, "avg_logprob": -0.08638616510339685, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0042631481774151325}, {"id": 655, "seek": 373240, "start": 3742.7200000000003, "end": 3747.44, "text": " use of your GPU. It allows you to achieve better results for the same amount of compute.", "tokens": [50880, 764, 295, 428, 18407, 13, 467, 4045, 291, 281, 4584, 1101, 3542, 337, 264, 912, 2372, 295, 14722, 13, 51116], "temperature": 0.0, "avg_logprob": -0.08638616510339685, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0042631481774151325}, {"id": 656, "seek": 373240, "start": 3748.56, "end": 3753.52, "text": " And that's why it's successful. Were you surprised how well transformers worked?", "tokens": [51172, 400, 300, 311, 983, 309, 311, 4406, 13, 12448, 291, 6100, 577, 731, 4088, 433, 2732, 30, 51420], "temperature": 0.0, "avg_logprob": -0.08638616510339685, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0042631481774151325}, {"id": 657, "seek": 373240, "start": 3754.1600000000003, "end": 3761.28, "text": " And GPT2 worked? So you worked on language. You've had a lot of great ideas before transformers came", "tokens": [51452, 400, 26039, 51, 17, 2732, 30, 407, 291, 2732, 322, 2856, 13, 509, 600, 632, 257, 688, 295, 869, 3487, 949, 4088, 433, 1361, 51808], "temperature": 0.0, "avg_logprob": -0.08638616510339685, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.0042631481774151325}, {"id": 658, "seek": 376128, "start": 3761.28, "end": 3766.32, "text": " about in language. So you got to see the whole set of revolutions before and after. Were you", "tokens": [50364, 466, 294, 2856, 13, 407, 291, 658, 281, 536, 264, 1379, 992, 295, 3698, 15892, 949, 293, 934, 13, 12448, 291, 50616], "temperature": 0.0, "avg_logprob": -0.12727151811122894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.024721352383494377}, {"id": 659, "seek": 376128, "start": 3766.32, "end": 3772.2400000000002, "text": " surprised? Yeah, a little. A little? Yeah. I mean, it's hard. It's hard to remember because", "tokens": [50616, 6100, 30, 865, 11, 257, 707, 13, 316, 707, 30, 865, 13, 286, 914, 11, 309, 311, 1152, 13, 467, 311, 1152, 281, 1604, 570, 50912], "temperature": 0.0, "avg_logprob": -0.12727151811122894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.024721352383494377}, {"id": 660, "seek": 376128, "start": 3772.8, "end": 3777.6800000000003, "text": " you adapt really quickly. But it definitely was surprising. It definitely was. In fact, I'll,", "tokens": [50940, 291, 6231, 534, 2661, 13, 583, 309, 2138, 390, 8830, 13, 467, 2138, 390, 13, 682, 1186, 11, 286, 603, 11, 51184], "temperature": 0.0, "avg_logprob": -0.12727151811122894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.024721352383494377}, {"id": 661, "seek": 376128, "start": 3777.6800000000003, "end": 3783.36, "text": " you know what, I'll, I'll retract my statement. It was, it was pretty amazing. It was just amazing", "tokens": [51184, 291, 458, 437, 11, 286, 603, 11, 286, 603, 41107, 452, 5629, 13, 467, 390, 11, 309, 390, 1238, 2243, 13, 467, 390, 445, 2243, 51468], "temperature": 0.0, "avg_logprob": -0.12727151811122894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.024721352383494377}, {"id": 662, "seek": 376128, "start": 3783.36, "end": 3788.1600000000003, "text": " to see, generate this text of this. And you know, you got to keep in mind that we've seen,", "tokens": [51468, 281, 536, 11, 8460, 341, 2487, 295, 341, 13, 400, 291, 458, 11, 291, 658, 281, 1066, 294, 1575, 300, 321, 600, 1612, 11, 51708], "temperature": 0.0, "avg_logprob": -0.12727151811122894, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.024721352383494377}, {"id": 663, "seek": 378816, "start": 3788.16, "end": 3792.48, "text": " at that time, we've seen all this progress in GANs, in improving the, you know, the samples", "tokens": [50364, 412, 300, 565, 11, 321, 600, 1612, 439, 341, 4205, 294, 460, 1770, 82, 11, 294, 11470, 264, 11, 291, 458, 11, 264, 10938, 50580], "temperature": 0.0, "avg_logprob": -0.09414336794898623, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.002017486607655883}, {"id": 664, "seek": 378816, "start": 3792.48, "end": 3797.12, "text": " produced by GANs were just amazing. You have these realistic faces, but text hasn't really moved that", "tokens": [50580, 7126, 538, 460, 1770, 82, 645, 445, 2243, 13, 509, 362, 613, 12465, 8475, 11, 457, 2487, 6132, 380, 534, 4259, 300, 50812], "temperature": 0.0, "avg_logprob": -0.09414336794898623, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.002017486607655883}, {"id": 665, "seek": 378816, "start": 3797.12, "end": 3804.08, "text": " much. And suddenly we moved from, you know, whatever GANs were in 2015, to the best, most", "tokens": [50812, 709, 13, 400, 5800, 321, 4259, 490, 11, 291, 458, 11, 2035, 460, 1770, 82, 645, 294, 7546, 11, 281, 264, 1151, 11, 881, 51160], "temperature": 0.0, "avg_logprob": -0.09414336794898623, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.002017486607655883}, {"id": 666, "seek": 378816, "start": 3804.08, "end": 3809.12, "text": " amazing GANs in one step. And I was really stunning. Even though theory predicted, yeah,", "tokens": [51160, 2243, 460, 1770, 82, 294, 472, 1823, 13, 400, 286, 390, 534, 18550, 13, 2754, 1673, 5261, 19147, 11, 1338, 11, 51412], "temperature": 0.0, "avg_logprob": -0.09414336794898623, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.002017486607655883}, {"id": 667, "seek": 378816, "start": 3809.12, "end": 3812.7999999999997, "text": " you train a big language model, of course, you should get this, but then to see it with your own", "tokens": [51412, 291, 3847, 257, 955, 2856, 2316, 11, 295, 1164, 11, 291, 820, 483, 341, 11, 457, 550, 281, 536, 309, 365, 428, 1065, 51596], "temperature": 0.0, "avg_logprob": -0.09414336794898623, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.002017486607655883}, {"id": 668, "seek": 381280, "start": 3812.8, "end": 3819.84, "text": " eyes. It's something else. And yet we adapt really quickly. And now there's a sort of", "tokens": [50364, 2575, 13, 467, 311, 746, 1646, 13, 400, 1939, 321, 6231, 534, 2661, 13, 400, 586, 456, 311, 257, 1333, 295, 50716], "temperature": 0.0, "avg_logprob": -0.15592109764015283, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04082132503390312}, {"id": 669, "seek": 381280, "start": 3821.52, "end": 3829.2000000000003, "text": " some cognitive scientist, right? Articles saying that GPT2 models don't truly understand language.", "tokens": [50800, 512, 15605, 12662, 11, 558, 30, 5735, 5350, 1566, 300, 26039, 51, 17, 5245, 500, 380, 4908, 1223, 2856, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15592109764015283, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04082132503390312}, {"id": 670, "seek": 381280, "start": 3829.2000000000003, "end": 3834.88, "text": " So we adapt quickly to how amazing the fact that they're able to model the language so well is.", "tokens": [51184, 407, 321, 6231, 2661, 281, 577, 2243, 264, 1186, 300, 436, 434, 1075, 281, 2316, 264, 2856, 370, 731, 307, 13, 51468], "temperature": 0.0, "avg_logprob": -0.15592109764015283, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04082132503390312}, {"id": 671, "seek": 381280, "start": 3835.6000000000004, "end": 3841.44, "text": " So what do you think is the bar? For what? For impressing us that it.", "tokens": [51504, 407, 437, 360, 291, 519, 307, 264, 2159, 30, 1171, 437, 30, 1171, 6729, 278, 505, 300, 309, 13, 51796], "temperature": 0.0, "avg_logprob": -0.15592109764015283, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.04082132503390312}, {"id": 672, "seek": 384144, "start": 3842.32, "end": 3846.08, "text": " I don't know. Do you think that bar will continuously be moved?", "tokens": [50408, 286, 500, 380, 458, 13, 1144, 291, 519, 300, 2159, 486, 15684, 312, 4259, 30, 50596], "temperature": 0.0, "avg_logprob": -0.10001758228648792, "compression_ratio": 1.72, "no_speech_prob": 0.003759866114705801}, {"id": 673, "seek": 384144, "start": 3846.08, "end": 3852.16, "text": " Definitely. I think when you start to see really dramatic economic impact, that's when I think", "tokens": [50596, 12151, 13, 286, 519, 562, 291, 722, 281, 536, 534, 12023, 4836, 2712, 11, 300, 311, 562, 286, 519, 50900], "temperature": 0.0, "avg_logprob": -0.10001758228648792, "compression_ratio": 1.72, "no_speech_prob": 0.003759866114705801}, {"id": 674, "seek": 384144, "start": 3852.16, "end": 3856.8, "text": " that's in some sense the next barrier. Because right now, if you think about the work in AI,", "tokens": [50900, 300, 311, 294, 512, 2020, 264, 958, 13357, 13, 1436, 558, 586, 11, 498, 291, 519, 466, 264, 589, 294, 7318, 11, 51132], "temperature": 0.0, "avg_logprob": -0.10001758228648792, "compression_ratio": 1.72, "no_speech_prob": 0.003759866114705801}, {"id": 675, "seek": 384144, "start": 3856.8, "end": 3861.68, "text": " it's really confusing. It's really hard to know what to make of all these advances.", "tokens": [51132, 309, 311, 534, 13181, 13, 467, 311, 534, 1152, 281, 458, 437, 281, 652, 295, 439, 613, 25297, 13, 51376], "temperature": 0.0, "avg_logprob": -0.10001758228648792, "compression_ratio": 1.72, "no_speech_prob": 0.003759866114705801}, {"id": 676, "seek": 384144, "start": 3862.4, "end": 3867.44, "text": " It's kind of like, okay, you got an advance and now you can do more things and you got another", "tokens": [51412, 467, 311, 733, 295, 411, 11, 1392, 11, 291, 658, 364, 7295, 293, 586, 291, 393, 360, 544, 721, 293, 291, 658, 1071, 51664], "temperature": 0.0, "avg_logprob": -0.10001758228648792, "compression_ratio": 1.72, "no_speech_prob": 0.003759866114705801}, {"id": 677, "seek": 386744, "start": 3868.16, "end": 3875.36, "text": " improvement and you got another cool demo. At some point, I think people who are outside of AI,", "tokens": [50400, 10444, 293, 291, 658, 1071, 1627, 10723, 13, 1711, 512, 935, 11, 286, 519, 561, 567, 366, 2380, 295, 7318, 11, 50760], "temperature": 0.0, "avg_logprob": -0.07510984355005725, "compression_ratio": 1.6633333333333333, "no_speech_prob": 0.006287221796810627}, {"id": 678, "seek": 386744, "start": 3876.16, "end": 3878.64, "text": " they can no longer distinguish this progress anymore.", "tokens": [50800, 436, 393, 572, 2854, 20206, 341, 4205, 3602, 13, 50924], "temperature": 0.0, "avg_logprob": -0.07510984355005725, "compression_ratio": 1.6633333333333333, "no_speech_prob": 0.006287221796810627}, {"id": 679, "seek": 386744, "start": 3878.64, "end": 3882.16, "text": " So we were talking offline about translating Russian to English and", "tokens": [50924, 407, 321, 645, 1417, 21857, 466, 35030, 7220, 281, 3669, 293, 51100], "temperature": 0.0, "avg_logprob": -0.07510984355005725, "compression_ratio": 1.6633333333333333, "no_speech_prob": 0.006287221796810627}, {"id": 680, "seek": 386744, "start": 3882.16, "end": 3886.4, "text": " how there's a lot of brilliant work in Russian that the rest of the world doesn't know about.", "tokens": [51100, 577, 456, 311, 257, 688, 295, 10248, 589, 294, 7220, 300, 264, 1472, 295, 264, 1002, 1177, 380, 458, 466, 13, 51312], "temperature": 0.0, "avg_logprob": -0.07510984355005725, "compression_ratio": 1.6633333333333333, "no_speech_prob": 0.006287221796810627}, {"id": 681, "seek": 386744, "start": 3886.4, "end": 3892.08, "text": " That's true for Chinese. That's true for a lot of scientists and just artistic work in general.", "tokens": [51312, 663, 311, 2074, 337, 4649, 13, 663, 311, 2074, 337, 257, 688, 295, 7708, 293, 445, 17090, 589, 294, 2674, 13, 51596], "temperature": 0.0, "avg_logprob": -0.07510984355005725, "compression_ratio": 1.6633333333333333, "no_speech_prob": 0.006287221796810627}, {"id": 682, "seek": 386744, "start": 3892.08, "end": 3896.96, "text": " Do you think translation is the place where we're going to see sort of economic big impact?", "tokens": [51596, 1144, 291, 519, 12853, 307, 264, 1081, 689, 321, 434, 516, 281, 536, 1333, 295, 4836, 955, 2712, 30, 51840], "temperature": 0.0, "avg_logprob": -0.07510984355005725, "compression_ratio": 1.6633333333333333, "no_speech_prob": 0.006287221796810627}, {"id": 683, "seek": 389696, "start": 3897.28, "end": 3902.2400000000002, "text": " I don't know. I think there is a huge number of applications. First of all, I want to point out", "tokens": [50380, 286, 500, 380, 458, 13, 286, 519, 456, 307, 257, 2603, 1230, 295, 5821, 13, 2386, 295, 439, 11, 286, 528, 281, 935, 484, 50628], "temperature": 0.0, "avg_logprob": -0.1407905978921019, "compression_ratio": 1.6540284360189574, "no_speech_prob": 0.0010460811899974942}, {"id": 684, "seek": 389696, "start": 3902.2400000000002, "end": 3907.6, "text": " that translation already today is huge. I think billions of people interact with", "tokens": [50628, 300, 12853, 1217, 965, 307, 2603, 13, 286, 519, 17375, 295, 561, 4648, 365, 50896], "temperature": 0.0, "avg_logprob": -0.1407905978921019, "compression_ratio": 1.6540284360189574, "no_speech_prob": 0.0010460811899974942}, {"id": 685, "seek": 389696, "start": 3908.8, "end": 3912.96, "text": " big chunks of the internet primarily through translation. So translation is already huge", "tokens": [50956, 955, 24004, 295, 264, 4705, 10029, 807, 12853, 13, 407, 12853, 307, 1217, 2603, 51164], "temperature": 0.0, "avg_logprob": -0.1407905978921019, "compression_ratio": 1.6540284360189574, "no_speech_prob": 0.0010460811899974942}, {"id": 686, "seek": 389696, "start": 3912.96, "end": 3919.76, "text": " and it's hugely positive too. I think self-driving is going to be hugely impactful.", "tokens": [51164, 293, 309, 311, 27417, 3353, 886, 13, 286, 519, 2698, 12, 47094, 307, 516, 281, 312, 27417, 30842, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1407905978921019, "compression_ratio": 1.6540284360189574, "no_speech_prob": 0.0010460811899974942}, {"id": 687, "seek": 391976, "start": 3920.0800000000004, "end": 3926.88, "text": " And it's unknown exactly when it happens, but again, I would not bet against deep learning.", "tokens": [50380, 400, 309, 311, 9841, 2293, 562, 309, 2314, 11, 457, 797, 11, 286, 576, 406, 778, 1970, 2452, 2539, 13, 50720], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 688, "seek": 391976, "start": 3927.84, "end": 3931.2000000000003, "text": " So there's deep learning in general, but you think deep learning for self-driving?", "tokens": [50768, 407, 456, 311, 2452, 2539, 294, 2674, 11, 457, 291, 519, 2452, 2539, 337, 2698, 12, 47094, 30, 50936], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 689, "seek": 391976, "start": 3931.76, "end": 3935.0400000000004, "text": " Yes, deep learning for self-driving. But I was talking about sort of language models.", "tokens": [50964, 1079, 11, 2452, 2539, 337, 2698, 12, 47094, 13, 583, 286, 390, 1417, 466, 1333, 295, 2856, 5245, 13, 51128], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 690, "seek": 391976, "start": 3936.88, "end": 3938.0, "text": " Be your duff a little bit.", "tokens": [51220, 879, 428, 274, 1245, 257, 707, 857, 13, 51276], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 691, "seek": 391976, "start": 3938.0, "end": 3941.0400000000004, "text": " Just to check. You're not seeing a connection between driving and language.", "tokens": [51276, 1449, 281, 1520, 13, 509, 434, 406, 2577, 257, 4984, 1296, 4840, 293, 2856, 13, 51428], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 692, "seek": 391976, "start": 3941.0400000000004, "end": 3944.0800000000004, "text": " No, no. Okay. I'd rather both use neural nets.", "tokens": [51428, 883, 11, 572, 13, 1033, 13, 286, 1116, 2831, 1293, 764, 18161, 36170, 13, 51580], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 693, "seek": 391976, "start": 3944.0800000000004, "end": 3947.6000000000004, "text": " That would be a poetic connection. I think there might be some, like you said,", "tokens": [51580, 663, 576, 312, 257, 41080, 4984, 13, 286, 519, 456, 1062, 312, 512, 11, 411, 291, 848, 11, 51756], "temperature": 0.0, "avg_logprob": -0.23027728497982025, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0912005752325058}, {"id": 694, "seek": 394760, "start": 3947.6, "end": 3954.3199999999997, "text": " there might be some kind of unification towards a kind of multitask transformers", "tokens": [50364, 456, 1062, 312, 512, 733, 295, 517, 3774, 3030, 257, 733, 295, 42338, 3863, 4088, 433, 50700], "temperature": 0.0, "avg_logprob": -0.14303006623920642, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.004537779372185469}, {"id": 695, "seek": 394760, "start": 3954.3199999999997, "end": 3960.72, "text": " that can take on both language and vision tasks. That'd be an interesting unification.", "tokens": [50700, 300, 393, 747, 322, 1293, 2856, 293, 5201, 9608, 13, 663, 1116, 312, 364, 1880, 517, 3774, 13, 51020], "temperature": 0.0, "avg_logprob": -0.14303006623920642, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.004537779372185469}, {"id": 696, "seek": 394760, "start": 3961.44, "end": 3963.6, "text": " Let's see. What can I ask about GPT two more?", "tokens": [51056, 961, 311, 536, 13, 708, 393, 286, 1029, 466, 26039, 51, 732, 544, 30, 51164], "temperature": 0.0, "avg_logprob": -0.14303006623920642, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.004537779372185469}, {"id": 697, "seek": 394760, "start": 3964.88, "end": 3969.8399999999997, "text": " It's simple. It's not much to ask. You take a transform, you make it bigger,", "tokens": [51228, 467, 311, 2199, 13, 467, 311, 406, 709, 281, 1029, 13, 509, 747, 257, 4088, 11, 291, 652, 309, 3801, 11, 51476], "temperature": 0.0, "avg_logprob": -0.14303006623920642, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.004537779372185469}, {"id": 698, "seek": 394760, "start": 3969.8399999999997, "end": 3972.56, "text": " give it more data, and suddenly it does all those amazing things.", "tokens": [51476, 976, 309, 544, 1412, 11, 293, 5800, 309, 775, 439, 729, 2243, 721, 13, 51612], "temperature": 0.0, "avg_logprob": -0.14303006623920642, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.004537779372185469}, {"id": 699, "seek": 394760, "start": 3972.56, "end": 3976.7999999999997, "text": " Yeah. One of the beautiful things is that GPT, the transformers are fundamentally", "tokens": [51612, 865, 13, 1485, 295, 264, 2238, 721, 307, 300, 26039, 51, 11, 264, 4088, 433, 366, 17879, 51824], "temperature": 0.0, "avg_logprob": -0.14303006623920642, "compression_ratio": 1.6466165413533835, "no_speech_prob": 0.004537779372185469}, {"id": 700, "seek": 397680, "start": 3976.8, "end": 3985.92, "text": " simple to explain, to train. Do you think bigger will continue to show better results in language?", "tokens": [50364, 2199, 281, 2903, 11, 281, 3847, 13, 1144, 291, 519, 3801, 486, 2354, 281, 855, 1101, 3542, 294, 2856, 30, 50820], "temperature": 0.0, "avg_logprob": -0.15456159548325973, "compression_ratio": 1.5112107623318385, "no_speech_prob": 0.0010982136009261012}, {"id": 701, "seek": 397680, "start": 3986.96, "end": 3987.44, "text": " Probably.", "tokens": [50872, 9210, 13, 50896], "temperature": 0.0, "avg_logprob": -0.15456159548325973, "compression_ratio": 1.5112107623318385, "no_speech_prob": 0.0010982136009261012}, {"id": 702, "seek": 397680, "start": 3988.2400000000002, "end": 3991.36, "text": " Sort of like what are the next steps with GPT two? Do you think?", "tokens": [50936, 26149, 295, 411, 437, 366, 264, 958, 4439, 365, 26039, 51, 732, 30, 1144, 291, 519, 30, 51092], "temperature": 0.0, "avg_logprob": -0.15456159548325973, "compression_ratio": 1.5112107623318385, "no_speech_prob": 0.0010982136009261012}, {"id": 703, "seek": 397680, "start": 3991.36, "end": 3998.0, "text": " I mean, I think for sure seeing what larger versions can do is one direction. Also,", "tokens": [51092, 286, 914, 11, 286, 519, 337, 988, 2577, 437, 4833, 9606, 393, 360, 307, 472, 3513, 13, 2743, 11, 51424], "temperature": 0.0, "avg_logprob": -0.15456159548325973, "compression_ratio": 1.5112107623318385, "no_speech_prob": 0.0010982136009261012}, {"id": 704, "seek": 397680, "start": 3999.76, "end": 4002.6400000000003, "text": " I mean, there are many questions. There's one question which I'm curious about,", "tokens": [51512, 286, 914, 11, 456, 366, 867, 1651, 13, 821, 311, 472, 1168, 597, 286, 478, 6369, 466, 11, 51656], "temperature": 0.0, "avg_logprob": -0.15456159548325973, "compression_ratio": 1.5112107623318385, "no_speech_prob": 0.0010982136009261012}, {"id": 705, "seek": 400264, "start": 4002.64, "end": 4006.48, "text": " and that's the following. Right now, GPT two, so we feed it all this data from the", "tokens": [50364, 293, 300, 311, 264, 3480, 13, 1779, 586, 11, 26039, 51, 732, 11, 370, 321, 3154, 309, 439, 341, 1412, 490, 264, 50556], "temperature": 0.0, "avg_logprob": -0.13157786798039708, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006471867207437754}, {"id": 706, "seek": 400264, "start": 4006.48, "end": 4010.4, "text": " internet, which means that it needs to memorize all those random facts about everything in the", "tokens": [50556, 4705, 11, 597, 1355, 300, 309, 2203, 281, 27478, 439, 729, 4974, 9130, 466, 1203, 294, 264, 50752], "temperature": 0.0, "avg_logprob": -0.13157786798039708, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006471867207437754}, {"id": 707, "seek": 400264, "start": 4010.4, "end": 4019.68, "text": " internet. It would be nice if the model could somehow use its own intelligence to decide", "tokens": [50752, 4705, 13, 467, 576, 312, 1481, 498, 264, 2316, 727, 6063, 764, 1080, 1065, 7599, 281, 4536, 51216], "temperature": 0.0, "avg_logprob": -0.13157786798039708, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006471867207437754}, {"id": 708, "seek": 400264, "start": 4019.68, "end": 4024.24, "text": " what data it wants to start, accept, and what data it wants to reject, just like people.", "tokens": [51216, 437, 1412, 309, 2738, 281, 722, 11, 3241, 11, 293, 437, 1412, 309, 2738, 281, 8248, 11, 445, 411, 561, 13, 51444], "temperature": 0.0, "avg_logprob": -0.13157786798039708, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006471867207437754}, {"id": 709, "seek": 400264, "start": 4024.24, "end": 4029.7599999999998, "text": " People don't learn all data indiscriminately. We are super selective about what we learn,", "tokens": [51444, 3432, 500, 380, 1466, 439, 1412, 1016, 5606, 16796, 1592, 13, 492, 366, 1687, 33930, 466, 437, 321, 1466, 11, 51720], "temperature": 0.0, "avg_logprob": -0.13157786798039708, "compression_ratio": 1.6729323308270676, "no_speech_prob": 0.006471867207437754}, {"id": 710, "seek": 402976, "start": 4029.76, "end": 4033.0400000000004, "text": " and I think this kind of active learning I think would be very nice to have.", "tokens": [50364, 293, 286, 519, 341, 733, 295, 4967, 2539, 286, 519, 576, 312, 588, 1481, 281, 362, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1266863985759456, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0021145527716726065}, {"id": 711, "seek": 402976, "start": 4034.1600000000003, "end": 4041.0400000000004, "text": " Yeah. Listen, I love active learning. So let me ask, does the selection of data,", "tokens": [50584, 865, 13, 7501, 11, 286, 959, 4967, 2539, 13, 407, 718, 385, 1029, 11, 775, 264, 9450, 295, 1412, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1266863985759456, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0021145527716726065}, {"id": 712, "seek": 402976, "start": 4041.0400000000004, "end": 4045.92, "text": " can you just elaborate that a little bit more? Do you think the selection of data is,", "tokens": [50928, 393, 291, 445, 20945, 300, 257, 707, 857, 544, 30, 1144, 291, 519, 264, 9450, 295, 1412, 307, 11, 51172], "temperature": 0.0, "avg_logprob": -0.1266863985759456, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0021145527716726065}, {"id": 713, "seek": 402976, "start": 4048.0, "end": 4053.6800000000003, "text": " like I have this kind of sense that the optimization of how you select data,", "tokens": [51276, 411, 286, 362, 341, 733, 295, 2020, 300, 264, 19618, 295, 577, 291, 3048, 1412, 11, 51560], "temperature": 0.0, "avg_logprob": -0.1266863985759456, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0021145527716726065}, {"id": 714, "seek": 405368, "start": 4053.7599999999998, "end": 4059.6, "text": " so the active learning process is going to be a place for a lot of breakthroughs", "tokens": [50368, 370, 264, 4967, 2539, 1399, 307, 516, 281, 312, 257, 1081, 337, 257, 688, 295, 22397, 82, 50660], "temperature": 0.0, "avg_logprob": -0.08134210305135758, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.033570509403944016}, {"id": 715, "seek": 405368, "start": 4060.72, "end": 4064.96, "text": " even in the near future, because there hasn't been many breakthroughs there that are public.", "tokens": [50716, 754, 294, 264, 2651, 2027, 11, 570, 456, 6132, 380, 668, 867, 22397, 82, 456, 300, 366, 1908, 13, 50928], "temperature": 0.0, "avg_logprob": -0.08134210305135758, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.033570509403944016}, {"id": 716, "seek": 405368, "start": 4064.96, "end": 4069.2, "text": " I feel like there might be private breakthroughs that companies keep to themselves,", "tokens": [50928, 286, 841, 411, 456, 1062, 312, 4551, 22397, 82, 300, 3431, 1066, 281, 2969, 11, 51140], "temperature": 0.0, "avg_logprob": -0.08134210305135758, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.033570509403944016}, {"id": 717, "seek": 405368, "start": 4069.2, "end": 4072.8799999999997, "text": " because the fundamental problem has to be solved if you want to solve self-driving,", "tokens": [51140, 570, 264, 8088, 1154, 575, 281, 312, 13041, 498, 291, 528, 281, 5039, 2698, 12, 47094, 11, 51324], "temperature": 0.0, "avg_logprob": -0.08134210305135758, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.033570509403944016}, {"id": 718, "seek": 405368, "start": 4072.8799999999997, "end": 4077.7599999999998, "text": " if you want to solve a particular task. What do you think about the space in general?", "tokens": [51324, 498, 291, 528, 281, 5039, 257, 1729, 5633, 13, 708, 360, 291, 519, 466, 264, 1901, 294, 2674, 30, 51568], "temperature": 0.0, "avg_logprob": -0.08134210305135758, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.033570509403944016}, {"id": 719, "seek": 405368, "start": 4077.7599999999998, "end": 4082.3999999999996, "text": " Yeah, so I think that for something like active learning, or in fact for any kind of capability,", "tokens": [51568, 865, 11, 370, 286, 519, 300, 337, 746, 411, 4967, 2539, 11, 420, 294, 1186, 337, 604, 733, 295, 13759, 11, 51800], "temperature": 0.0, "avg_logprob": -0.08134210305135758, "compression_ratio": 1.8131487889273357, "no_speech_prob": 0.033570509403944016}, {"id": 720, "seek": 408240, "start": 4082.4, "end": 4087.6800000000003, "text": " like active learning, the thing that it really needs is a problem. It needs a problem that requires it.", "tokens": [50364, 411, 4967, 2539, 11, 264, 551, 300, 309, 534, 2203, 307, 257, 1154, 13, 467, 2203, 257, 1154, 300, 7029, 309, 13, 50628], "temperature": 0.0, "avg_logprob": -0.07709249443964127, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.001547976047731936}, {"id": 721, "seek": 408240, "start": 4089.28, "end": 4092.88, "text": " It's very hard to do research about the capability if you don't have a task,", "tokens": [50708, 467, 311, 588, 1152, 281, 360, 2132, 466, 264, 13759, 498, 291, 500, 380, 362, 257, 5633, 11, 50888], "temperature": 0.0, "avg_logprob": -0.07709249443964127, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.001547976047731936}, {"id": 722, "seek": 408240, "start": 4092.88, "end": 4096.64, "text": " because then what's going to happen is you will come up with an artificial task,", "tokens": [50888, 570, 550, 437, 311, 516, 281, 1051, 307, 291, 486, 808, 493, 365, 364, 11677, 5633, 11, 51076], "temperature": 0.0, "avg_logprob": -0.07709249443964127, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.001547976047731936}, {"id": 723, "seek": 408240, "start": 4096.64, "end": 4099.52, "text": " get good results, but not really convince anyone.", "tokens": [51076, 483, 665, 3542, 11, 457, 406, 534, 13447, 2878, 13, 51220], "temperature": 0.0, "avg_logprob": -0.07709249443964127, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.001547976047731936}, {"id": 724, "seek": 408240, "start": 4100.56, "end": 4107.52, "text": " Right. We're now past the stage where getting a result on MNIST,", "tokens": [51272, 1779, 13, 492, 434, 586, 1791, 264, 3233, 689, 1242, 257, 1874, 322, 376, 45, 19756, 11, 51620], "temperature": 0.0, "avg_logprob": -0.07709249443964127, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.001547976047731936}, {"id": 725, "seek": 408240, "start": 4107.52, "end": 4110.72, "text": " some clever formulation of MNIST will convince people.", "tokens": [51620, 512, 13494, 37642, 295, 376, 45, 19756, 486, 13447, 561, 13, 51780], "temperature": 0.0, "avg_logprob": -0.07709249443964127, "compression_ratio": 1.6325757575757576, "no_speech_prob": 0.001547976047731936}, {"id": 726, "seek": 411072, "start": 4110.72, "end": 4115.280000000001, "text": " That's right. In fact, you could quite easily come up with a simple active learning scheme on MNIST", "tokens": [50364, 663, 311, 558, 13, 682, 1186, 11, 291, 727, 1596, 3612, 808, 493, 365, 257, 2199, 4967, 2539, 12232, 322, 376, 45, 19756, 50592], "temperature": 0.0, "avg_logprob": -0.11992523696396377, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00039166369242593646}, {"id": 727, "seek": 411072, "start": 4115.280000000001, "end": 4124.88, "text": " and get a 10x speed up, but then so what? I think that active learning will naturally arise", "tokens": [50592, 293, 483, 257, 1266, 87, 3073, 493, 11, 457, 550, 370, 437, 30, 286, 519, 300, 4967, 2539, 486, 8195, 20288, 51072], "temperature": 0.0, "avg_logprob": -0.11992523696396377, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00039166369242593646}, {"id": 728, "seek": 411072, "start": 4125.4400000000005, "end": 4131.6, "text": " as problems that require it to pop up. That's my take on it.", "tokens": [51100, 382, 2740, 300, 3651, 309, 281, 1665, 493, 13, 663, 311, 452, 747, 322, 309, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11992523696396377, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00039166369242593646}, {"id": 729, "seek": 411072, "start": 4132.72, "end": 4138.8, "text": " There's another interesting thing that OpenAS brought up with GPT2, which is when you create a", "tokens": [51464, 821, 311, 1071, 1880, 551, 300, 7238, 3160, 3038, 493, 365, 26039, 51, 17, 11, 597, 307, 562, 291, 1884, 257, 51768], "temperature": 0.0, "avg_logprob": -0.11992523696396377, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00039166369242593646}, {"id": 730, "seek": 413880, "start": 4138.8, "end": 4144.56, "text": " powerful artificial intelligence system, and it was unclear what kind of detrimental,", "tokens": [50364, 4005, 11677, 7599, 1185, 11, 293, 309, 390, 25636, 437, 733, 295, 45694, 11, 50652], "temperature": 0.0, "avg_logprob": -0.12500046599995007, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0039438591338694096}, {"id": 731, "seek": 413880, "start": 4144.56, "end": 4151.360000000001, "text": " once you release GPT2, what kind of detrimental effect it'll have. Because if you have a model", "tokens": [50652, 1564, 291, 4374, 26039, 51, 17, 11, 437, 733, 295, 45694, 1802, 309, 603, 362, 13, 1436, 498, 291, 362, 257, 2316, 50992], "temperature": 0.0, "avg_logprob": -0.12500046599995007, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0039438591338694096}, {"id": 732, "seek": 413880, "start": 4151.360000000001, "end": 4158.320000000001, "text": " that can generate pretty realistic text, you can start to imagine that it would be used by bots in", "tokens": [50992, 300, 393, 8460, 1238, 12465, 2487, 11, 291, 393, 722, 281, 3811, 300, 309, 576, 312, 1143, 538, 35410, 294, 51340], "temperature": 0.0, "avg_logprob": -0.12500046599995007, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0039438591338694096}, {"id": 733, "seek": 413880, "start": 4159.360000000001, "end": 4164.400000000001, "text": " some way that we can't even imagine. There's this nervousness about what it's possible to do.", "tokens": [51392, 512, 636, 300, 321, 393, 380, 754, 3811, 13, 821, 311, 341, 6296, 1287, 466, 437, 309, 311, 1944, 281, 360, 13, 51644], "temperature": 0.0, "avg_logprob": -0.12500046599995007, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.0039438591338694096}, {"id": 734, "seek": 416440, "start": 4164.96, "end": 4170.0, "text": " You did a really brave and I think profound thing, which just started a conversation about this.", "tokens": [50392, 509, 630, 257, 534, 12653, 293, 286, 519, 14382, 551, 11, 597, 445, 1409, 257, 3761, 466, 341, 13, 50644], "temperature": 0.0, "avg_logprob": -0.16400163118229355, "compression_ratio": 1.55, "no_speech_prob": 0.0005702216294594109}, {"id": 735, "seek": 416440, "start": 4170.0, "end": 4178.4, "text": " How do we release powerful artificial intelligence models to the public? If we do it all, how do we", "tokens": [50644, 1012, 360, 321, 4374, 4005, 11677, 7599, 5245, 281, 264, 1908, 30, 759, 321, 360, 309, 439, 11, 577, 360, 321, 51064], "temperature": 0.0, "avg_logprob": -0.16400163118229355, "compression_ratio": 1.55, "no_speech_prob": 0.0005702216294594109}, {"id": 736, "seek": 416440, "start": 4178.4, "end": 4185.44, "text": " privately discuss with other even competitors about how we manage the use of the systems and", "tokens": [51064, 31919, 2248, 365, 661, 754, 18333, 466, 577, 321, 3067, 264, 764, 295, 264, 3652, 293, 51416], "temperature": 0.0, "avg_logprob": -0.16400163118229355, "compression_ratio": 1.55, "no_speech_prob": 0.0005702216294594109}, {"id": 737, "seek": 416440, "start": 4185.44, "end": 4190.5599999999995, "text": " so on? From this whole experience, you've released a report on it, but in general,", "tokens": [51416, 370, 322, 30, 3358, 341, 1379, 1752, 11, 291, 600, 4736, 257, 2275, 322, 309, 11, 457, 294, 2674, 11, 51672], "temperature": 0.0, "avg_logprob": -0.16400163118229355, "compression_ratio": 1.55, "no_speech_prob": 0.0005702216294594109}, {"id": 738, "seek": 419056, "start": 4190.56, "end": 4196.4800000000005, "text": " are there any insights that you've gathered from just thinking about this, about how you release", "tokens": [50364, 366, 456, 604, 14310, 300, 291, 600, 13032, 490, 445, 1953, 466, 341, 11, 466, 577, 291, 4374, 50660], "temperature": 0.0, "avg_logprob": -0.0767917737856016, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.004191103857010603}, {"id": 739, "seek": 419056, "start": 4196.4800000000005, "end": 4203.76, "text": " models like this? I think that my take on this is that the field of AI has been in a state of", "tokens": [50660, 5245, 411, 341, 30, 286, 519, 300, 452, 747, 322, 341, 307, 300, 264, 2519, 295, 7318, 575, 668, 294, 257, 1785, 295, 51024], "temperature": 0.0, "avg_logprob": -0.0767917737856016, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.004191103857010603}, {"id": 740, "seek": 419056, "start": 4203.76, "end": 4210.240000000001, "text": " childhood, and now it's exiting that state and it's entering a state of maturity. What that means", "tokens": [51024, 9278, 11, 293, 586, 309, 311, 48868, 300, 1785, 293, 309, 311, 11104, 257, 1785, 295, 28874, 13, 708, 300, 1355, 51348], "temperature": 0.0, "avg_logprob": -0.0767917737856016, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.004191103857010603}, {"id": 741, "seek": 419056, "start": 4210.240000000001, "end": 4215.6, "text": " is that AI is very successful and also very impactful, and its impact is not only large,", "tokens": [51348, 307, 300, 7318, 307, 588, 4406, 293, 611, 588, 30842, 11, 293, 1080, 2712, 307, 406, 787, 2416, 11, 51616], "temperature": 0.0, "avg_logprob": -0.0767917737856016, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.004191103857010603}, {"id": 742, "seek": 421560, "start": 4215.68, "end": 4223.84, "text": " but it's also growing. For that reason, it seems wise to start thinking about the impact of our", "tokens": [50368, 457, 309, 311, 611, 4194, 13, 1171, 300, 1778, 11, 309, 2544, 10829, 281, 722, 1953, 466, 264, 2712, 295, 527, 50776], "temperature": 0.0, "avg_logprob": -0.11321453518337674, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.024306325241923332}, {"id": 743, "seek": 421560, "start": 4223.84, "end": 4228.56, "text": " systems before releasing them maybe a little bit too soon rather than a little bit too late.", "tokens": [50776, 3652, 949, 16327, 552, 1310, 257, 707, 857, 886, 2321, 2831, 813, 257, 707, 857, 886, 3469, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11321453518337674, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.024306325241923332}, {"id": 744, "seek": 421560, "start": 4229.76, "end": 4234.240000000001, "text": " With the case of GPT-2, like I mentioned earlier, the results really were stunning,", "tokens": [51072, 2022, 264, 1389, 295, 26039, 51, 12, 17, 11, 411, 286, 2835, 3071, 11, 264, 3542, 534, 645, 18550, 11, 51296], "temperature": 0.0, "avg_logprob": -0.11321453518337674, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.024306325241923332}, {"id": 745, "seek": 421560, "start": 4235.04, "end": 4241.4400000000005, "text": " and it seemed plausible. It didn't seem certain. It seemed plausible that something like GPT-2", "tokens": [51336, 293, 309, 6576, 39925, 13, 467, 994, 380, 1643, 1629, 13, 467, 6576, 39925, 300, 746, 411, 26039, 51, 12, 17, 51656], "temperature": 0.0, "avg_logprob": -0.11321453518337674, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.024306325241923332}, {"id": 746, "seek": 424144, "start": 4241.44, "end": 4248.799999999999, "text": " could easily use to reduce the cost of disinformation. There was a question of what's", "tokens": [50364, 727, 3612, 764, 281, 5407, 264, 2063, 295, 717, 20941, 13, 821, 390, 257, 1168, 295, 437, 311, 50732], "temperature": 0.0, "avg_logprob": -0.12040355572333702, "compression_ratio": 1.8185483870967742, "no_speech_prob": 0.007205522153526545}, {"id": 747, "seek": 424144, "start": 4248.799999999999, "end": 4252.879999999999, "text": " the best way to release it, and a staged release seemed logical. A small model was released,", "tokens": [50732, 264, 1151, 636, 281, 4374, 309, 11, 293, 257, 45178, 4374, 6576, 14978, 13, 316, 1359, 2316, 390, 4736, 11, 50936], "temperature": 0.0, "avg_logprob": -0.12040355572333702, "compression_ratio": 1.8185483870967742, "no_speech_prob": 0.007205522153526545}, {"id": 748, "seek": 424144, "start": 4253.679999999999, "end": 4259.839999999999, "text": " and there was time to see the... Many people use these models in lots of cool ways. There've been", "tokens": [50976, 293, 456, 390, 565, 281, 536, 264, 485, 5126, 561, 764, 613, 5245, 294, 3195, 295, 1627, 2098, 13, 821, 600, 668, 51284], "temperature": 0.0, "avg_logprob": -0.12040355572333702, "compression_ratio": 1.8185483870967742, "no_speech_prob": 0.007205522153526545}, {"id": 749, "seek": 424144, "start": 4259.839999999999, "end": 4266.08, "text": " lots of really cool applications. There haven't been any negative applications we know of,", "tokens": [51284, 3195, 295, 534, 1627, 5821, 13, 821, 2378, 380, 668, 604, 3671, 5821, 321, 458, 295, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12040355572333702, "compression_ratio": 1.8185483870967742, "no_speech_prob": 0.007205522153526545}, {"id": 750, "seek": 424144, "start": 4266.08, "end": 4269.839999999999, "text": " and so eventually it was released, but also other people replicated similar models.", "tokens": [51596, 293, 370, 4728, 309, 390, 4736, 11, 457, 611, 661, 561, 46365, 2531, 5245, 13, 51784], "temperature": 0.0, "avg_logprob": -0.12040355572333702, "compression_ratio": 1.8185483870967742, "no_speech_prob": 0.007205522153526545}, {"id": 751, "seek": 426984, "start": 4269.84, "end": 4277.68, "text": " That's an interesting question though that we know of. In your view, staged release is at least", "tokens": [50364, 663, 311, 364, 1880, 1168, 1673, 300, 321, 458, 295, 13, 682, 428, 1910, 11, 45178, 4374, 307, 412, 1935, 50756], "temperature": 0.0, "avg_logprob": -0.10123530301180753, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.0025904658250510693}, {"id": 752, "seek": 426984, "start": 4277.68, "end": 4285.360000000001, "text": " part of the answer to the question of how do we... What do we do once we create a system like this?", "tokens": [50756, 644, 295, 264, 1867, 281, 264, 1168, 295, 577, 360, 321, 485, 708, 360, 321, 360, 1564, 321, 1884, 257, 1185, 411, 341, 30, 51140], "temperature": 0.0, "avg_logprob": -0.10123530301180753, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.0025904658250510693}, {"id": 753, "seek": 426984, "start": 4285.92, "end": 4291.84, "text": " It's part of the answer, yes. Is there any other insights? Say you don't want to release the model", "tokens": [51168, 467, 311, 644, 295, 264, 1867, 11, 2086, 13, 1119, 456, 604, 661, 14310, 30, 6463, 291, 500, 380, 528, 281, 4374, 264, 2316, 51464], "temperature": 0.0, "avg_logprob": -0.10123530301180753, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.0025904658250510693}, {"id": 754, "seek": 426984, "start": 4291.84, "end": 4297.4400000000005, "text": " at all because it's useful to you for whatever the business is. Well, there are plenty of people", "tokens": [51464, 412, 439, 570, 309, 311, 4420, 281, 291, 337, 2035, 264, 1606, 307, 13, 1042, 11, 456, 366, 7140, 295, 561, 51744], "temperature": 0.0, "avg_logprob": -0.10123530301180753, "compression_ratio": 1.6567796610169492, "no_speech_prob": 0.0025904658250510693}, {"id": 755, "seek": 429744, "start": 4297.44, "end": 4303.919999999999, "text": " who don't release models already. Right, of course, but is there some moral ethical responsibility", "tokens": [50364, 567, 500, 380, 4374, 5245, 1217, 13, 1779, 11, 295, 1164, 11, 457, 307, 456, 512, 9723, 18890, 6357, 50688], "temperature": 0.0, "avg_logprob": -0.10605443607677113, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.005058849696069956}, {"id": 756, "seek": 429744, "start": 4304.48, "end": 4311.839999999999, "text": " when you have a very powerful model to communicate? Just as you said, when you had GPT-2, it was", "tokens": [50716, 562, 291, 362, 257, 588, 4005, 2316, 281, 7890, 30, 1449, 382, 291, 848, 11, 562, 291, 632, 26039, 51, 12, 17, 11, 309, 390, 51084], "temperature": 0.0, "avg_logprob": -0.10605443607677113, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.005058849696069956}, {"id": 757, "seek": 429744, "start": 4311.839999999999, "end": 4315.2, "text": " unclear how much it could be used for misinformation. It's an open question,", "tokens": [51084, 25636, 577, 709, 309, 727, 312, 1143, 337, 34238, 13, 467, 311, 364, 1269, 1168, 11, 51252], "temperature": 0.0, "avg_logprob": -0.10605443607677113, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.005058849696069956}, {"id": 758, "seek": 429744, "start": 4316.24, "end": 4320.879999999999, "text": " and getting an answer to that might require that you talk to other really smart people that are", "tokens": [51304, 293, 1242, 364, 1867, 281, 300, 1062, 3651, 300, 291, 751, 281, 661, 534, 4069, 561, 300, 366, 51536], "temperature": 0.0, "avg_logprob": -0.10605443607677113, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.005058849696069956}, {"id": 759, "seek": 432088, "start": 4321.6, "end": 4328.72, "text": " outside of your particular group. Have you... Please tell me there's some optimistic pathway for", "tokens": [50400, 2380, 295, 428, 1729, 1594, 13, 3560, 291, 485, 2555, 980, 385, 456, 311, 512, 19397, 18590, 337, 50756], "temperature": 0.0, "avg_logprob": -0.16535733805762398, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.044631216675043106}, {"id": 760, "seek": 432088, "start": 4329.28, "end": 4332.32, "text": " people across the world to collaborate on these kinds of cases,", "tokens": [50784, 561, 2108, 264, 1002, 281, 18338, 322, 613, 3685, 295, 3331, 11, 50936], "temperature": 0.0, "avg_logprob": -0.16535733805762398, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.044631216675043106}, {"id": 761, "seek": 432088, "start": 4334.56, "end": 4339.4400000000005, "text": " or is it still really difficult from one company to talk to another company?", "tokens": [51048, 420, 307, 309, 920, 534, 2252, 490, 472, 2237, 281, 751, 281, 1071, 2237, 30, 51292], "temperature": 0.0, "avg_logprob": -0.16535733805762398, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.044631216675043106}, {"id": 762, "seek": 432088, "start": 4339.4400000000005, "end": 4346.72, "text": " So it's definitely possible. It's definitely possible to discuss these kind of models with", "tokens": [51292, 407, 309, 311, 2138, 1944, 13, 467, 311, 2138, 1944, 281, 2248, 613, 733, 295, 5245, 365, 51656], "temperature": 0.0, "avg_logprob": -0.16535733805762398, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.044631216675043106}, {"id": 763, "seek": 434672, "start": 4346.72, "end": 4351.280000000001, "text": " colleagues elsewhere and to get their take on what to do.", "tokens": [50364, 7734, 14517, 293, 281, 483, 641, 747, 322, 437, 281, 360, 13, 50592], "temperature": 0.0, "avg_logprob": -0.13092997044692806, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.030620139092206955}, {"id": 764, "seek": 434672, "start": 4352.08, "end": 4354.0, "text": " How hard is it though? I mean...", "tokens": [50632, 1012, 1152, 307, 309, 1673, 30, 286, 914, 485, 50728], "temperature": 0.0, "avg_logprob": -0.13092997044692806, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.030620139092206955}, {"id": 765, "seek": 434672, "start": 4356.320000000001, "end": 4361.92, "text": " Do you see that happening? I think that's a place where it's important to gradually build trust", "tokens": [50844, 1144, 291, 536, 300, 2737, 30, 286, 519, 300, 311, 257, 1081, 689, 309, 311, 1021, 281, 13145, 1322, 3361, 51124], "temperature": 0.0, "avg_logprob": -0.13092997044692806, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.030620139092206955}, {"id": 766, "seek": 434672, "start": 4361.92, "end": 4367.84, "text": " between companies, because ultimately, all the AI developers are building technology,", "tokens": [51124, 1296, 3431, 11, 570, 6284, 11, 439, 264, 7318, 8849, 366, 2390, 2899, 11, 51420], "temperature": 0.0, "avg_logprob": -0.13092997044692806, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.030620139092206955}, {"id": 767, "seek": 434672, "start": 4367.84, "end": 4371.4400000000005, "text": " which is going to be increasingly more powerful, and so it's...", "tokens": [51420, 597, 307, 516, 281, 312, 12980, 544, 4005, 11, 293, 370, 309, 311, 485, 51600], "temperature": 0.0, "avg_logprob": -0.13092997044692806, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.030620139092206955}, {"id": 768, "seek": 437144, "start": 4372.16, "end": 4377.04, "text": " The way to think about it is that ultimately we're only together.", "tokens": [50400, 440, 636, 281, 519, 466, 309, 307, 300, 6284, 321, 434, 787, 1214, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1944830316892812, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.001895629451610148}, {"id": 769, "seek": 437144, "start": 4378.639999999999, "end": 4386.4, "text": " Yeah, it's... I tend to believe in the better angels of our nature, but I do hope that", "tokens": [50724, 865, 11, 309, 311, 485, 286, 3928, 281, 1697, 294, 264, 1101, 18175, 295, 527, 3687, 11, 457, 286, 360, 1454, 300, 51112], "temperature": 0.0, "avg_logprob": -0.1944830316892812, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.001895629451610148}, {"id": 770, "seek": 437144, "start": 4389.04, "end": 4392.719999999999, "text": " when you build a really powerful AI system in a particular domain,", "tokens": [51244, 562, 291, 1322, 257, 534, 4005, 7318, 1185, 294, 257, 1729, 9274, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1944830316892812, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.001895629451610148}, {"id": 771, "seek": 437144, "start": 4392.719999999999, "end": 4398.16, "text": " that you also think about the potential negative consequences of... Yeah.", "tokens": [51428, 300, 291, 611, 519, 466, 264, 3995, 3671, 10098, 295, 485, 865, 13, 51700], "temperature": 0.0, "avg_logprob": -0.1944830316892812, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.001895629451610148}, {"id": 772, "seek": 440144, "start": 4402.24, "end": 4407.36, "text": " It's an interesting and scary possibility that there will be a race for AI development that", "tokens": [50404, 467, 311, 364, 1880, 293, 6958, 7959, 300, 456, 486, 312, 257, 4569, 337, 7318, 3250, 300, 50660], "temperature": 0.0, "avg_logprob": -0.10037290324335513, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0013455867301672697}, {"id": 773, "seek": 440144, "start": 4407.36, "end": 4414.4, "text": " would push people to close that development and not share ideas with others. I don't love this.", "tokens": [50660, 576, 2944, 561, 281, 1998, 300, 3250, 293, 406, 2073, 3487, 365, 2357, 13, 286, 500, 380, 959, 341, 13, 51012], "temperature": 0.0, "avg_logprob": -0.10037290324335513, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0013455867301672697}, {"id": 774, "seek": 440144, "start": 4414.4, "end": 4419.44, "text": " I've been in a pure academic for 10 years. I really like sharing ideas and it's fun, it's exciting.", "tokens": [51012, 286, 600, 668, 294, 257, 6075, 7778, 337, 1266, 924, 13, 286, 534, 411, 5414, 3487, 293, 309, 311, 1019, 11, 309, 311, 4670, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10037290324335513, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0013455867301672697}, {"id": 775, "seek": 440144, "start": 4421.44, "end": 4425.44, "text": " What do you think it takes to... Let's talk about AGI a little bit. What do you think it takes to", "tokens": [51364, 708, 360, 291, 519, 309, 2516, 281, 485, 961, 311, 751, 466, 316, 26252, 257, 707, 857, 13, 708, 360, 291, 519, 309, 2516, 281, 51564], "temperature": 0.0, "avg_logprob": -0.10037290324335513, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0013455867301672697}, {"id": 776, "seek": 440144, "start": 4425.44, "end": 4430.08, "text": " build a system of human level intelligence? We talked about reasoning, we talked about", "tokens": [51564, 1322, 257, 1185, 295, 1952, 1496, 7599, 30, 492, 2825, 466, 21577, 11, 321, 2825, 466, 51796], "temperature": 0.0, "avg_logprob": -0.10037290324335513, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0013455867301672697}, {"id": 777, "seek": 443008, "start": 4430.08, "end": 4435.76, "text": " long-term memory, but in general, what does it take, do you think? Well, I can't be sure,", "tokens": [50364, 938, 12, 7039, 4675, 11, 457, 294, 2674, 11, 437, 775, 309, 747, 11, 360, 291, 519, 30, 1042, 11, 286, 393, 380, 312, 988, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1162184306553432, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.0013043482322245836}, {"id": 778, "seek": 443008, "start": 4437.36, "end": 4445.5199999999995, "text": " but I think the deep learning plus maybe another small idea. Do you think self-play will be involved?", "tokens": [50728, 457, 286, 519, 264, 2452, 2539, 1804, 1310, 1071, 1359, 1558, 13, 1144, 291, 519, 2698, 12, 2858, 486, 312, 3288, 30, 51136], "temperature": 0.0, "avg_logprob": -0.1162184306553432, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.0013043482322245836}, {"id": 779, "seek": 443008, "start": 4445.5199999999995, "end": 4451.04, "text": " Sort of like you've spoken about the powerful mechanism of self-play where systems learn by", "tokens": [51136, 26149, 295, 411, 291, 600, 10759, 466, 264, 4005, 7513, 295, 2698, 12, 2858, 689, 3652, 1466, 538, 51412], "temperature": 0.0, "avg_logprob": -0.1162184306553432, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.0013043482322245836}, {"id": 780, "seek": 443008, "start": 4452.48, "end": 4459.36, "text": " sort of exploring the world in a competitive setting against other entities that are similarly", "tokens": [51484, 1333, 295, 12736, 264, 1002, 294, 257, 10043, 3287, 1970, 661, 16667, 300, 366, 14138, 51828], "temperature": 0.0, "avg_logprob": -0.1162184306553432, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.0013043482322245836}, {"id": 781, "seek": 445936, "start": 4459.36, "end": 4464.48, "text": " skilled as them and so incrementally improving this way? Do you think self-play will be a component", "tokens": [50364, 19690, 382, 552, 293, 370, 26200, 379, 11470, 341, 636, 30, 1144, 291, 519, 2698, 12, 2858, 486, 312, 257, 6542, 50620], "temperature": 0.0, "avg_logprob": -0.12816837165929093, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003022627206519246}, {"id": 782, "seek": 445936, "start": 4464.48, "end": 4471.2, "text": " of building an AGI system? Yeah, so what I would say to build AGI, I think it's going to be", "tokens": [50620, 295, 2390, 364, 316, 26252, 1185, 30, 865, 11, 370, 437, 286, 576, 584, 281, 1322, 316, 26252, 11, 286, 519, 309, 311, 516, 281, 312, 50956], "temperature": 0.0, "avg_logprob": -0.12816837165929093, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003022627206519246}, {"id": 783, "seek": 445936, "start": 4472.48, "end": 4479.679999999999, "text": " deep learning plus some ideas and I think self-play will be one of those ideas. I think that that is a", "tokens": [51020, 2452, 2539, 1804, 512, 3487, 293, 286, 519, 2698, 12, 2858, 486, 312, 472, 295, 729, 3487, 13, 286, 519, 300, 300, 307, 257, 51380], "temperature": 0.0, "avg_logprob": -0.12816837165929093, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003022627206519246}, {"id": 784, "seek": 447968, "start": 4479.68, "end": 4490.320000000001, "text": " very... Self-play has this amazing property that it can surprise us in truly novel ways. For example,", "tokens": [50364, 588, 485, 16348, 12, 2858, 575, 341, 2243, 4707, 300, 309, 393, 6365, 505, 294, 4908, 7613, 2098, 13, 1171, 1365, 11, 50896], "temperature": 0.0, "avg_logprob": -0.19157551464281583, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.043017223477363586}, {"id": 785, "seek": 447968, "start": 4492.08, "end": 4499.280000000001, "text": " like we, I mean, pretty much every self-play system, both are Dota bot. I don't know if", "tokens": [50984, 411, 321, 11, 286, 914, 11, 1238, 709, 633, 2698, 12, 2858, 1185, 11, 1293, 366, 413, 5377, 10592, 13, 286, 500, 380, 458, 498, 51344], "temperature": 0.0, "avg_logprob": -0.19157551464281583, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.043017223477363586}, {"id": 786, "seek": 447968, "start": 4499.280000000001, "end": 4505.200000000001, "text": " OpenAI had a release about multi-agents where you had two little agents who were playing hide and", "tokens": [51344, 7238, 48698, 632, 257, 4374, 466, 4825, 12, 559, 791, 689, 291, 632, 732, 707, 12554, 567, 645, 2433, 6479, 293, 51640], "temperature": 0.0, "avg_logprob": -0.19157551464281583, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.043017223477363586}, {"id": 787, "seek": 450520, "start": 4506.08, "end": 4511.5199999999995, "text": " seek and of course also AlphaZero. They all produce surprising behaviors. They all produce", "tokens": [50408, 8075, 293, 295, 1164, 611, 20588, 57, 2032, 13, 814, 439, 5258, 8830, 15501, 13, 814, 439, 5258, 50680], "temperature": 0.0, "avg_logprob": -0.17075516627385065, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.11456715315580368}, {"id": 788, "seek": 450520, "start": 4511.5199999999995, "end": 4517.28, "text": " behaviors that we didn't expect. They are creative solutions to problems and that seems like an", "tokens": [50680, 15501, 300, 321, 994, 380, 2066, 13, 814, 366, 5880, 6547, 281, 2740, 293, 300, 2544, 411, 364, 50968], "temperature": 0.0, "avg_logprob": -0.17075516627385065, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.11456715315580368}, {"id": 789, "seek": 450520, "start": 4517.28, "end": 4523.84, "text": " important part of AGI that our systems don't exhibit routinely right now. And so that's why I", "tokens": [50968, 1021, 644, 295, 316, 26252, 300, 527, 3652, 500, 380, 20487, 40443, 558, 586, 13, 400, 370, 300, 311, 983, 286, 51296], "temperature": 0.0, "avg_logprob": -0.17075516627385065, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.11456715315580368}, {"id": 790, "seek": 450520, "start": 4523.84, "end": 4527.5199999999995, "text": " like this area, I like this direction because of its ability to surprises.", "tokens": [51296, 411, 341, 1859, 11, 286, 411, 341, 3513, 570, 295, 1080, 3485, 281, 22655, 13, 51480], "temperature": 0.0, "avg_logprob": -0.17075516627385065, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.11456715315580368}, {"id": 791, "seek": 450520, "start": 4527.5199999999995, "end": 4532.5599999999995, "text": " To surprises and an AGI system would surprise us fundamentally. Yes, but and to be precise,", "tokens": [51480, 1407, 22655, 293, 364, 316, 26252, 1185, 576, 6365, 505, 17879, 13, 1079, 11, 457, 293, 281, 312, 13600, 11, 51732], "temperature": 0.0, "avg_logprob": -0.17075516627385065, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.11456715315580368}, {"id": 792, "seek": 453256, "start": 4533.280000000001, "end": 4538.64, "text": " not just a random surprise, but to find a surprising solution to a problem that's also useful.", "tokens": [50400, 406, 445, 257, 4974, 6365, 11, 457, 281, 915, 257, 8830, 3827, 281, 257, 1154, 300, 311, 611, 4420, 13, 50668], "temperature": 0.0, "avg_logprob": -0.11323432127634685, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.0009108256199397147}, {"id": 793, "seek": 453256, "start": 4539.92, "end": 4546.96, "text": " Now, a lot of the self-play mechanisms have been used in the game context or at least in the", "tokens": [50732, 823, 11, 257, 688, 295, 264, 2698, 12, 2858, 15902, 362, 668, 1143, 294, 264, 1216, 4319, 420, 412, 1935, 294, 264, 51084], "temperature": 0.0, "avg_logprob": -0.11323432127634685, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.0009108256199397147}, {"id": 794, "seek": 453256, "start": 4546.96, "end": 4557.04, "text": " simulation context. How far along the path to AGI do you think will be done in simulation? How much", "tokens": [51084, 16575, 4319, 13, 1012, 1400, 2051, 264, 3100, 281, 316, 26252, 360, 291, 519, 486, 312, 1096, 294, 16575, 30, 1012, 709, 51588], "temperature": 0.0, "avg_logprob": -0.11323432127634685, "compression_ratio": 1.5026178010471205, "no_speech_prob": 0.0009108256199397147}, {"id": 795, "seek": 455704, "start": 4557.04, "end": 4564.96, "text": " faith promise do you have in simulation versus having to have a system that operates in the real", "tokens": [50364, 4522, 6228, 360, 291, 362, 294, 16575, 5717, 1419, 281, 362, 257, 1185, 300, 22577, 294, 264, 957, 50760], "temperature": 0.0, "avg_logprob": -0.11507203502039756, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.07915305346250534}, {"id": 796, "seek": 455704, "start": 4564.96, "end": 4571.68, "text": " world, whether it's the real world of digital real-world data or real-world like actual physical", "tokens": [50760, 1002, 11, 1968, 309, 311, 264, 957, 1002, 295, 4562, 957, 12, 13217, 1412, 420, 957, 12, 13217, 411, 3539, 4001, 51096], "temperature": 0.0, "avg_logprob": -0.11507203502039756, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.07915305346250534}, {"id": 797, "seek": 455704, "start": 4571.68, "end": 4577.36, "text": " world with robotics? I don't think it's in either war. I think simulation is a tool and it helps.", "tokens": [51096, 1002, 365, 34145, 30, 286, 500, 380, 519, 309, 311, 294, 2139, 1516, 13, 286, 519, 16575, 307, 257, 2290, 293, 309, 3665, 13, 51380], "temperature": 0.0, "avg_logprob": -0.11507203502039756, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.07915305346250534}, {"id": 798, "seek": 455704, "start": 4577.36, "end": 4583.84, "text": " It has certain strengths and certain weaknesses and we should use it. Yeah, but okay, I understand", "tokens": [51380, 467, 575, 1629, 16986, 293, 1629, 24381, 293, 321, 820, 764, 309, 13, 865, 11, 457, 1392, 11, 286, 1223, 51704], "temperature": 0.0, "avg_logprob": -0.11507203502039756, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.07915305346250534}, {"id": 799, "seek": 458384, "start": 4584.400000000001, "end": 4594.32, "text": " that that's true. But one of the criticisms of self-play, one of the criticisms of reinforcement", "tokens": [50392, 300, 300, 311, 2074, 13, 583, 472, 295, 264, 48519, 295, 2698, 12, 2858, 11, 472, 295, 264, 48519, 295, 29280, 50888], "temperature": 0.0, "avg_logprob": -0.15209512483505977, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0033758014906197786}, {"id": 800, "seek": 458384, "start": 4594.32, "end": 4602.8, "text": " learning is one of the its current power, its current results while amazing have been demonstrated", "tokens": [50888, 2539, 307, 472, 295, 264, 1080, 2190, 1347, 11, 1080, 2190, 3542, 1339, 2243, 362, 668, 18772, 51312], "temperature": 0.0, "avg_logprob": -0.15209512483505977, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0033758014906197786}, {"id": 801, "seek": 458384, "start": 4602.8, "end": 4607.2, "text": " in a simulated environments or very constrained physical environments. Do you think it's possible", "tokens": [51312, 294, 257, 41713, 12388, 420, 588, 38901, 4001, 12388, 13, 1144, 291, 519, 309, 311, 1944, 51532], "temperature": 0.0, "avg_logprob": -0.15209512483505977, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0033758014906197786}, {"id": 802, "seek": 458384, "start": 4607.2, "end": 4613.28, "text": " to escape them? Escape the simulated environments and be able to learn in non-simulated environments?", "tokens": [51532, 281, 7615, 552, 30, 42960, 264, 41713, 12388, 293, 312, 1075, 281, 1466, 294, 2107, 12, 30937, 6987, 12388, 30, 51836], "temperature": 0.0, "avg_logprob": -0.15209512483505977, "compression_ratio": 1.880952380952381, "no_speech_prob": 0.0033758014906197786}, {"id": 803, "seek": 461328, "start": 4613.28, "end": 4621.04, "text": " Or do you think it's possible to also just simulate in a photorealistic and physics realistic way", "tokens": [50364, 1610, 360, 291, 519, 309, 311, 1944, 281, 611, 445, 27817, 294, 257, 2409, 418, 304, 3142, 293, 10649, 12465, 636, 50752], "temperature": 0.0, "avg_logprob": -0.08750594534525057, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0008158406708389521}, {"id": 804, "seek": 461328, "start": 4621.04, "end": 4626.08, "text": " the real world in a way that we can solve real problems with self-play in simulation?", "tokens": [50752, 264, 957, 1002, 294, 257, 636, 300, 321, 393, 5039, 957, 2740, 365, 2698, 12, 2858, 294, 16575, 30, 51004], "temperature": 0.0, "avg_logprob": -0.08750594534525057, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0008158406708389521}, {"id": 805, "seek": 461328, "start": 4627.44, "end": 4632.48, "text": " I think that transfer from simulation to the real world is definitely possible and has been", "tokens": [51072, 286, 519, 300, 5003, 490, 16575, 281, 264, 957, 1002, 307, 2138, 1944, 293, 575, 668, 51324], "temperature": 0.0, "avg_logprob": -0.08750594534525057, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0008158406708389521}, {"id": 806, "seek": 461328, "start": 4632.48, "end": 4637.599999999999, "text": " exhibited many times by many different groups. It's been especially successful in vision.", "tokens": [51324, 49446, 867, 1413, 538, 867, 819, 3935, 13, 467, 311, 668, 2318, 4406, 294, 5201, 13, 51580], "temperature": 0.0, "avg_logprob": -0.08750594534525057, "compression_ratio": 1.6898148148148149, "no_speech_prob": 0.0008158406708389521}, {"id": 807, "seek": 463760, "start": 4638.56, "end": 4644.240000000001, "text": " Also OpenAI in the summer has demonstrated a robot hand which was trained entirely in simulation", "tokens": [50412, 2743, 7238, 48698, 294, 264, 4266, 575, 18772, 257, 7881, 1011, 597, 390, 8895, 7696, 294, 16575, 50696], "temperature": 0.0, "avg_logprob": -0.17650418762766987, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.1065078005194664}, {"id": 808, "seek": 463760, "start": 4645.120000000001, "end": 4648.160000000001, "text": " in a certain way that allowed for sim to real transfer to occur.", "tokens": [50740, 294, 257, 1629, 636, 300, 4350, 337, 1034, 281, 957, 5003, 281, 5160, 13, 50892], "temperature": 0.0, "avg_logprob": -0.17650418762766987, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.1065078005194664}, {"id": 809, "seek": 463760, "start": 4649.68, "end": 4654.4800000000005, "text": " Is this for the Rubik's Cube? Yes, that's right. I wasn't aware that was trained in simulation.", "tokens": [50968, 1119, 341, 337, 264, 10518, 1035, 311, 33003, 30, 1079, 11, 300, 311, 558, 13, 286, 2067, 380, 3650, 300, 390, 8895, 294, 16575, 13, 51208], "temperature": 0.0, "avg_logprob": -0.17650418762766987, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.1065078005194664}, {"id": 810, "seek": 463760, "start": 4654.4800000000005, "end": 4660.0, "text": " It was trained in simulation entirely. Really? So it wasn't in the physical that the hand wasn't", "tokens": [51208, 467, 390, 8895, 294, 16575, 7696, 13, 4083, 30, 407, 309, 2067, 380, 294, 264, 4001, 300, 264, 1011, 2067, 380, 51484], "temperature": 0.0, "avg_logprob": -0.17650418762766987, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.1065078005194664}, {"id": 811, "seek": 463760, "start": 4660.0, "end": 4666.240000000001, "text": " trained? No, 100% of the training was done in simulation and the policy that was learned in", "tokens": [51484, 8895, 30, 883, 11, 2319, 4, 295, 264, 3097, 390, 1096, 294, 16575, 293, 264, 3897, 300, 390, 3264, 294, 51796], "temperature": 0.0, "avg_logprob": -0.17650418762766987, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.1065078005194664}, {"id": 812, "seek": 466624, "start": 4666.24, "end": 4671.36, "text": " simulation was trained to be very adaptive. So adaptive that when you transfer it, it could very", "tokens": [50364, 16575, 390, 8895, 281, 312, 588, 27912, 13, 407, 27912, 300, 562, 291, 5003, 309, 11, 309, 727, 588, 50620], "temperature": 0.0, "avg_logprob": -0.12194511916611221, "compression_ratio": 1.821917808219178, "no_speech_prob": 0.022472122684121132}, {"id": 813, "seek": 466624, "start": 4671.36, "end": 4677.76, "text": " quickly adapt to the physical world. So the kind of perturbations with the giraffe or whatever the", "tokens": [50620, 2661, 6231, 281, 264, 4001, 1002, 13, 407, 264, 733, 295, 40468, 763, 365, 264, 49897, 420, 2035, 264, 50940], "temperature": 0.0, "avg_logprob": -0.12194511916611221, "compression_ratio": 1.821917808219178, "no_speech_prob": 0.022472122684121132}, {"id": 814, "seek": 466624, "start": 4677.76, "end": 4685.2, "text": " heck it was, were those part of the simulation? Well, the simulation was generally, so the simulation", "tokens": [50940, 12872, 309, 390, 11, 645, 729, 644, 295, 264, 16575, 30, 1042, 11, 264, 16575, 390, 5101, 11, 370, 264, 16575, 51312], "temperature": 0.0, "avg_logprob": -0.12194511916611221, "compression_ratio": 1.821917808219178, "no_speech_prob": 0.022472122684121132}, {"id": 815, "seek": 466624, "start": 4685.2, "end": 4690.16, "text": " was trained to be robust to many different things, but not the kind of perturbations we've had in the", "tokens": [51312, 390, 8895, 281, 312, 13956, 281, 867, 819, 721, 11, 457, 406, 264, 733, 295, 40468, 763, 321, 600, 632, 294, 264, 51560], "temperature": 0.0, "avg_logprob": -0.12194511916611221, "compression_ratio": 1.821917808219178, "no_speech_prob": 0.022472122684121132}, {"id": 816, "seek": 469016, "start": 4690.16, "end": 4696.96, "text": " video. So it's never been trained with a glove. It's never been trained with a stuffed giraffe.", "tokens": [50364, 960, 13, 407, 309, 311, 1128, 668, 8895, 365, 257, 26928, 13, 467, 311, 1128, 668, 8895, 365, 257, 24092, 49897, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1360855536027388, "compression_ratio": 1.7862903225806452, "no_speech_prob": 0.06744619458913803}, {"id": 817, "seek": 469016, "start": 4696.96, "end": 4700.8, "text": " So in theory, these are novel perturbations? Correct. It's not in theory in practice.", "tokens": [50704, 407, 294, 5261, 11, 613, 366, 7613, 40468, 763, 30, 12753, 13, 467, 311, 406, 294, 5261, 294, 3124, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1360855536027388, "compression_ratio": 1.7862903225806452, "no_speech_prob": 0.06744619458913803}, {"id": 818, "seek": 469016, "start": 4701.68, "end": 4708.32, "text": " That those are novel perturbations? Well, that's okay. That's a clean, small-scale,", "tokens": [50940, 663, 729, 366, 7613, 40468, 763, 30, 1042, 11, 300, 311, 1392, 13, 663, 311, 257, 2541, 11, 1359, 12, 20033, 11, 51272], "temperature": 0.0, "avg_logprob": -0.1360855536027388, "compression_ratio": 1.7862903225806452, "no_speech_prob": 0.06744619458913803}, {"id": 819, "seek": 469016, "start": 4708.32, "end": 4712.0, "text": " but clean example of a transfer from the simulated world to the physical world.", "tokens": [51272, 457, 2541, 1365, 295, 257, 5003, 490, 264, 41713, 1002, 281, 264, 4001, 1002, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1360855536027388, "compression_ratio": 1.7862903225806452, "no_speech_prob": 0.06744619458913803}, {"id": 820, "seek": 469016, "start": 4712.0, "end": 4716.96, "text": " Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in", "tokens": [51456, 865, 11, 293, 286, 486, 611, 584, 300, 286, 2066, 264, 5003, 10862, 295, 2452, 2539, 281, 3488, 294, 51704], "temperature": 0.0, "avg_logprob": -0.1360855536027388, "compression_ratio": 1.7862903225806452, "no_speech_prob": 0.06744619458913803}, {"id": 821, "seek": 471696, "start": 4716.96, "end": 4722.0, "text": " general and the better the transfer capabilities are, the more useful simulation will become", "tokens": [50364, 2674, 293, 264, 1101, 264, 5003, 10862, 366, 11, 264, 544, 4420, 16575, 486, 1813, 50616], "temperature": 0.0, "avg_logprob": -0.1333830091688368, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.002285598311573267}, {"id": 822, "seek": 471696, "start": 4723.52, "end": 4727.76, "text": " because then you could take, you could experience something in simulation", "tokens": [50692, 570, 550, 291, 727, 747, 11, 291, 727, 1752, 746, 294, 16575, 50904], "temperature": 0.0, "avg_logprob": -0.1333830091688368, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.002285598311573267}, {"id": 823, "seek": 471696, "start": 4728.4, "end": 4732.72, "text": " and then learn a moral of the story which you could then carry with you to the real world, right?", "tokens": [50936, 293, 550, 1466, 257, 9723, 295, 264, 1657, 597, 291, 727, 550, 3985, 365, 291, 281, 264, 957, 1002, 11, 558, 30, 51152], "temperature": 0.0, "avg_logprob": -0.1333830091688368, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.002285598311573267}, {"id": 824, "seek": 471696, "start": 4733.36, "end": 4741.04, "text": " As humans do all the time and they play computer games. So let me ask sort of an embodied question,", "tokens": [51184, 1018, 6255, 360, 439, 264, 565, 293, 436, 862, 3820, 2813, 13, 407, 718, 385, 1029, 1333, 295, 364, 42046, 1168, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1333830091688368, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.002285598311573267}, {"id": 825, "seek": 474104, "start": 4741.76, "end": 4748.4, "text": " staying on AGI for a sec. Do you think AGI system, we need to have a body? We need to have some of", "tokens": [50400, 7939, 322, 316, 26252, 337, 257, 907, 13, 1144, 291, 519, 316, 26252, 1185, 11, 321, 643, 281, 362, 257, 1772, 30, 492, 643, 281, 362, 512, 295, 50732], "temperature": 0.0, "avg_logprob": -0.08657506306966146, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.2533872425556183}, {"id": 826, "seek": 474104, "start": 4748.4, "end": 4756.56, "text": " those human elements of self-awareness, consciousness, sort of fear of mortality, sort of self-preservation", "tokens": [50732, 729, 1952, 4959, 295, 2698, 12, 17074, 1287, 11, 10081, 11, 1333, 295, 4240, 295, 23330, 11, 1333, 295, 2698, 12, 14508, 6864, 51140], "temperature": 0.0, "avg_logprob": -0.08657506306966146, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.2533872425556183}, {"id": 827, "seek": 474104, "start": 4756.56, "end": 4762.32, "text": " in the physical space which comes with having a body? I think having a body will be useful.", "tokens": [51140, 294, 264, 4001, 1901, 597, 1487, 365, 1419, 257, 1772, 30, 286, 519, 1419, 257, 1772, 486, 312, 4420, 13, 51428], "temperature": 0.0, "avg_logprob": -0.08657506306966146, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.2533872425556183}, {"id": 828, "seek": 474104, "start": 4762.32, "end": 4766.72, "text": " I don't think it's necessary, but I think it's very useful to have a body for sure because you can", "tokens": [51428, 286, 500, 380, 519, 309, 311, 4818, 11, 457, 286, 519, 309, 311, 588, 4420, 281, 362, 257, 1772, 337, 988, 570, 291, 393, 51648], "temperature": 0.0, "avg_logprob": -0.08657506306966146, "compression_ratio": 1.7186147186147187, "no_speech_prob": 0.2533872425556183}, {"id": 829, "seek": 476672, "start": 4767.2, "end": 4771.6, "text": " learn a whole new, you can learn things which cannot be learned without a body,", "tokens": [50388, 1466, 257, 1379, 777, 11, 291, 393, 1466, 721, 597, 2644, 312, 3264, 1553, 257, 1772, 11, 50608], "temperature": 0.0, "avg_logprob": -0.11949326506758158, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.028382835909724236}, {"id": 830, "seek": 476672, "start": 4772.400000000001, "end": 4776.0, "text": " but at the same time, I think that you can, if you don't have a body, you could", "tokens": [50648, 457, 412, 264, 912, 565, 11, 286, 519, 300, 291, 393, 11, 498, 291, 500, 380, 362, 257, 1772, 11, 291, 727, 50828], "temperature": 0.0, "avg_logprob": -0.11949326506758158, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.028382835909724236}, {"id": 831, "seek": 476672, "start": 4776.0, "end": 4780.88, "text": " compensate for it and still succeed. You think so? Yes. Well, there is evidence for this. For", "tokens": [50828, 29458, 337, 309, 293, 920, 7754, 13, 509, 519, 370, 30, 1079, 13, 1042, 11, 456, 307, 4467, 337, 341, 13, 1171, 51072], "temperature": 0.0, "avg_logprob": -0.11949326506758158, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.028382835909724236}, {"id": 832, "seek": 476672, "start": 4780.88, "end": 4786.72, "text": " example, there are many people who were born deaf and blind and they were able to compensate for", "tokens": [51072, 1365, 11, 456, 366, 867, 561, 567, 645, 4232, 15559, 293, 6865, 293, 436, 645, 1075, 281, 29458, 337, 51364], "temperature": 0.0, "avg_logprob": -0.11949326506758158, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.028382835909724236}, {"id": 833, "seek": 476672, "start": 4786.72, "end": 4792.56, "text": " the lack of modalities. I'm thinking about Helen Keller specifically. So even if you're not able", "tokens": [51364, 264, 5011, 295, 1072, 16110, 13, 286, 478, 1953, 466, 26294, 48352, 4682, 13, 407, 754, 498, 291, 434, 406, 1075, 51656], "temperature": 0.0, "avg_logprob": -0.11949326506758158, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.028382835909724236}, {"id": 834, "seek": 479256, "start": 4792.56, "end": 4798.320000000001, "text": " to physically interact with the world and if you're not able to, I actually was getting at,", "tokens": [50364, 281, 9762, 4648, 365, 264, 1002, 293, 498, 291, 434, 406, 1075, 281, 11, 286, 767, 390, 1242, 412, 11, 50652], "temperature": 0.0, "avg_logprob": -0.11341847194714492, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.004980630706995726}, {"id": 835, "seek": 479256, "start": 4799.52, "end": 4805.280000000001, "text": " maybe let me ask on the more particular, I'm not sure if it's connected to having a body or not,", "tokens": [50712, 1310, 718, 385, 1029, 322, 264, 544, 1729, 11, 286, 478, 406, 988, 498, 309, 311, 4582, 281, 1419, 257, 1772, 420, 406, 11, 51000], "temperature": 0.0, "avg_logprob": -0.11341847194714492, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.004980630706995726}, {"id": 836, "seek": 479256, "start": 4805.280000000001, "end": 4811.200000000001, "text": " but the idea of consciousness and a more constrained version of that is self-awareness.", "tokens": [51000, 457, 264, 1558, 295, 10081, 293, 257, 544, 38901, 3037, 295, 300, 307, 2698, 12, 17074, 1287, 13, 51296], "temperature": 0.0, "avg_logprob": -0.11341847194714492, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.004980630706995726}, {"id": 837, "seek": 479256, "start": 4811.200000000001, "end": 4817.76, "text": " Do you think an AGI system should have consciousness? We can't define whatever the", "tokens": [51296, 1144, 291, 519, 364, 316, 26252, 1185, 820, 362, 10081, 30, 492, 393, 380, 6964, 2035, 264, 51624], "temperature": 0.0, "avg_logprob": -0.11341847194714492, "compression_ratio": 1.5541125541125542, "no_speech_prob": 0.004980630706995726}, {"id": 838, "seek": 481776, "start": 4817.76, "end": 4822.88, "text": " heck you think consciousness is. Yeah. Hard question to answer, given how hard it is to define it.", "tokens": [50364, 12872, 291, 519, 10081, 307, 13, 865, 13, 11817, 1168, 281, 1867, 11, 2212, 577, 1152, 309, 307, 281, 6964, 309, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10567376745028759, "compression_ratio": 1.916083916083916, "no_speech_prob": 0.04955291375517845}, {"id": 839, "seek": 481776, "start": 4824.64, "end": 4828.24, "text": " Do you think it's useful to think about? I mean, it's definitely interesting,", "tokens": [50708, 1144, 291, 519, 309, 311, 4420, 281, 519, 466, 30, 286, 914, 11, 309, 311, 2138, 1880, 11, 50888], "temperature": 0.0, "avg_logprob": -0.10567376745028759, "compression_ratio": 1.916083916083916, "no_speech_prob": 0.04955291375517845}, {"id": 840, "seek": 481776, "start": 4828.24, "end": 4832.88, "text": " it's fascinating. I think it's definitely possible that our systems will be conscious.", "tokens": [50888, 309, 311, 10343, 13, 286, 519, 309, 311, 2138, 1944, 300, 527, 3652, 486, 312, 6648, 13, 51120], "temperature": 0.0, "avg_logprob": -0.10567376745028759, "compression_ratio": 1.916083916083916, "no_speech_prob": 0.04955291375517845}, {"id": 841, "seek": 481776, "start": 4833.76, "end": 4837.68, "text": " Do you think that's an emergent thing that just comes from, do you think consciousness could emerge", "tokens": [51164, 1144, 291, 519, 300, 311, 364, 4345, 6930, 551, 300, 445, 1487, 490, 11, 360, 291, 519, 10081, 727, 21511, 51360], "temperature": 0.0, "avg_logprob": -0.10567376745028759, "compression_ratio": 1.916083916083916, "no_speech_prob": 0.04955291375517845}, {"id": 842, "seek": 481776, "start": 4837.68, "end": 4842.400000000001, "text": " from the representation that's stored within your networks? So like that it naturally just", "tokens": [51360, 490, 264, 10290, 300, 311, 12187, 1951, 428, 9590, 30, 407, 411, 300, 309, 8195, 445, 51596], "temperature": 0.0, "avg_logprob": -0.10567376745028759, "compression_ratio": 1.916083916083916, "no_speech_prob": 0.04955291375517845}, {"id": 843, "seek": 481776, "start": 4842.400000000001, "end": 4847.04, "text": " emerges when you become more and more, you're able to represent more and more over the world?", "tokens": [51596, 38965, 562, 291, 1813, 544, 293, 544, 11, 291, 434, 1075, 281, 2906, 544, 293, 544, 670, 264, 1002, 30, 51828], "temperature": 0.0, "avg_logprob": -0.10567376745028759, "compression_ratio": 1.916083916083916, "no_speech_prob": 0.04955291375517845}, {"id": 844, "seek": 484704, "start": 4847.04, "end": 4852.72, "text": " Well, I'd say I'd make the following argument, which is humans are conscious,", "tokens": [50364, 1042, 11, 286, 1116, 584, 286, 1116, 652, 264, 3480, 6770, 11, 597, 307, 6255, 366, 6648, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1277181307474772, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.000502698530908674}, {"id": 845, "seek": 484704, "start": 4853.68, "end": 4856.96, "text": " and if you believe that artificial neural nets are sufficiently", "tokens": [50696, 293, 498, 291, 1697, 300, 11677, 18161, 36170, 366, 31868, 50860], "temperature": 0.0, "avg_logprob": -0.1277181307474772, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.000502698530908674}, {"id": 846, "seek": 484704, "start": 4857.84, "end": 4863.2, "text": " similar to the brain, then there should at least exist artificial neural nets you should be conscious", "tokens": [50904, 2531, 281, 264, 3567, 11, 550, 456, 820, 412, 1935, 2514, 11677, 18161, 36170, 291, 820, 312, 6648, 51172], "temperature": 0.0, "avg_logprob": -0.1277181307474772, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.000502698530908674}, {"id": 847, "seek": 484704, "start": 4863.2, "end": 4871.5199999999995, "text": " to. You're leaning on that existence proof pretty heavily. Okay. But that's the best answer I can", "tokens": [51172, 281, 13, 509, 434, 23390, 322, 300, 9123, 8177, 1238, 10950, 13, 1033, 13, 583, 300, 311, 264, 1151, 1867, 286, 393, 51588], "temperature": 0.0, "avg_logprob": -0.1277181307474772, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.000502698530908674}, {"id": 848, "seek": 487152, "start": 4871.6, "end": 4879.120000000001, "text": " give. No, I know. I know. There's still an open question if there's not some magic in the brain", "tokens": [50368, 976, 13, 883, 11, 286, 458, 13, 286, 458, 13, 821, 311, 920, 364, 1269, 1168, 498, 456, 311, 406, 512, 5585, 294, 264, 3567, 50744], "temperature": 0.0, "avg_logprob": -0.1300484819232293, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.10959384590387344}, {"id": 849, "seek": 487152, "start": 4879.120000000001, "end": 4887.040000000001, "text": " that we're not. I mean, I don't mean a non-materialistic magic, but that the brain might be a lot", "tokens": [50744, 300, 321, 434, 406, 13, 286, 914, 11, 286, 500, 380, 914, 257, 2107, 12, 76, 40364, 3142, 5585, 11, 457, 300, 264, 3567, 1062, 312, 257, 688, 51140], "temperature": 0.0, "avg_logprob": -0.1300484819232293, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.10959384590387344}, {"id": 850, "seek": 487152, "start": 4887.040000000001, "end": 4891.4400000000005, "text": " more complicated and interesting than we give it credit for. If that's the case, then it should show", "tokens": [51140, 544, 6179, 293, 1880, 813, 321, 976, 309, 5397, 337, 13, 759, 300, 311, 264, 1389, 11, 550, 309, 820, 855, 51360], "temperature": 0.0, "avg_logprob": -0.1300484819232293, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.10959384590387344}, {"id": 851, "seek": 487152, "start": 4891.4400000000005, "end": 4897.280000000001, "text": " up and at some point, we will find out that we can't continue to make progress. But I think", "tokens": [51360, 493, 293, 412, 512, 935, 11, 321, 486, 915, 484, 300, 321, 393, 380, 2354, 281, 652, 4205, 13, 583, 286, 519, 51652], "temperature": 0.0, "avg_logprob": -0.1300484819232293, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.10959384590387344}, {"id": 852, "seek": 489728, "start": 4897.28, "end": 4901.92, "text": " it's unlikely. So we talk about consciousness, but let me talk about another poorly defined", "tokens": [50364, 309, 311, 17518, 13, 407, 321, 751, 466, 10081, 11, 457, 718, 385, 751, 466, 1071, 22271, 7642, 50596], "temperature": 0.0, "avg_logprob": -0.0700331631828757, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007449896540492773}, {"id": 853, "seek": 489728, "start": 4901.92, "end": 4908.24, "text": " concept of intelligence. Again, we've talked about reasoning. We've talked about memory. What do", "tokens": [50596, 3410, 295, 7599, 13, 3764, 11, 321, 600, 2825, 466, 21577, 13, 492, 600, 2825, 466, 4675, 13, 708, 360, 50912], "temperature": 0.0, "avg_logprob": -0.0700331631828757, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007449896540492773}, {"id": 854, "seek": 489728, "start": 4908.24, "end": 4914.48, "text": " you think is a good test of intelligence for you? Are you impressed by the test that Alan Turing", "tokens": [50912, 291, 519, 307, 257, 665, 1500, 295, 7599, 337, 291, 30, 2014, 291, 11679, 538, 264, 1500, 300, 16442, 314, 1345, 51224], "temperature": 0.0, "avg_logprob": -0.0700331631828757, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007449896540492773}, {"id": 855, "seek": 489728, "start": 4914.48, "end": 4921.2, "text": " formulated with the imitation game with natural language? Is there something in your mind that", "tokens": [51224, 48936, 365, 264, 47624, 1216, 365, 3303, 2856, 30, 1119, 456, 746, 294, 428, 1575, 300, 51560], "temperature": 0.0, "avg_logprob": -0.0700331631828757, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.007449896540492773}, {"id": 856, "seek": 492120, "start": 4921.28, "end": 4929.92, "text": " you would be deeply impressed by if a system was able to do? I mean, lots of things. There's a", "tokens": [50368, 291, 576, 312, 8760, 11679, 538, 498, 257, 1185, 390, 1075, 281, 360, 30, 286, 914, 11, 3195, 295, 721, 13, 821, 311, 257, 50800], "temperature": 0.0, "avg_logprob": -0.1265876433428596, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006185324862599373}, {"id": 857, "seek": 492120, "start": 4929.92, "end": 4936.8, "text": " certain frontier of capabilities today. And there exist things outside of that frontier,", "tokens": [50800, 1629, 35853, 295, 10862, 965, 13, 400, 456, 2514, 721, 2380, 295, 300, 35853, 11, 51144], "temperature": 0.0, "avg_logprob": -0.1265876433428596, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006185324862599373}, {"id": 858, "seek": 492120, "start": 4936.8, "end": 4943.2, "text": " and I would be impressed by any such thing. For example, I would be impressed by a deep learning", "tokens": [51144, 293, 286, 576, 312, 11679, 538, 604, 1270, 551, 13, 1171, 1365, 11, 286, 576, 312, 11679, 538, 257, 2452, 2539, 51464], "temperature": 0.0, "avg_logprob": -0.1265876433428596, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006185324862599373}, {"id": 859, "seek": 492120, "start": 4943.2, "end": 4950.08, "text": " system which solves a very pedestrian task like machine translation or computer vision task or", "tokens": [51464, 1185, 597, 39890, 257, 588, 33947, 5633, 411, 3479, 12853, 420, 3820, 5201, 5633, 420, 51808], "temperature": 0.0, "avg_logprob": -0.1265876433428596, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.006185324862599373}, {"id": 860, "seek": 495008, "start": 4950.08, "end": 4957.6, "text": " something which never makes a mistake a human wouldn't make under any circumstances. I think", "tokens": [50364, 746, 597, 1128, 1669, 257, 6146, 257, 1952, 2759, 380, 652, 833, 604, 9121, 13, 286, 519, 50740], "temperature": 0.0, "avg_logprob": -0.1343964270825656, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.0024654066655784845}, {"id": 861, "seek": 495008, "start": 4957.6, "end": 4961.68, "text": " that is something which have not yet been demonstrated and I would find it very impressive.", "tokens": [50740, 300, 307, 746, 597, 362, 406, 1939, 668, 18772, 293, 286, 576, 915, 309, 588, 8992, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1343964270825656, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.0024654066655784845}, {"id": 862, "seek": 495008, "start": 4962.64, "end": 4966.48, "text": " Yes, so right now, they make mistakes in different, they might be more accurate than human beings,", "tokens": [50992, 1079, 11, 370, 558, 586, 11, 436, 652, 8038, 294, 819, 11, 436, 1062, 312, 544, 8559, 813, 1952, 8958, 11, 51184], "temperature": 0.0, "avg_logprob": -0.1343964270825656, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.0024654066655784845}, {"id": 863, "seek": 495008, "start": 4966.48, "end": 4973.36, "text": " but they still make a different set of mistakes. So I would guess that a lot of the skepticism", "tokens": [51184, 457, 436, 920, 652, 257, 819, 992, 295, 8038, 13, 407, 286, 576, 2041, 300, 257, 688, 295, 264, 19128, 26356, 51528], "temperature": 0.0, "avg_logprob": -0.1343964270825656, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.0024654066655784845}, {"id": 864, "seek": 495008, "start": 4973.36, "end": 4977.28, "text": " that some people have about deep learning is when they look at their mistakes and they say,", "tokens": [51528, 300, 512, 561, 362, 466, 2452, 2539, 307, 562, 436, 574, 412, 641, 8038, 293, 436, 584, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1343964270825656, "compression_ratio": 1.7669172932330828, "no_speech_prob": 0.0024654066655784845}, {"id": 865, "seek": 497728, "start": 4977.28, "end": 4982.16, "text": " well, those mistakes, they make no sense. If you understood the concept, you wouldn't make that", "tokens": [50364, 731, 11, 729, 8038, 11, 436, 652, 572, 2020, 13, 759, 291, 7320, 264, 3410, 11, 291, 2759, 380, 652, 300, 50608], "temperature": 0.0, "avg_logprob": -0.1197352409362793, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.011137375608086586}, {"id": 866, "seek": 497728, "start": 4982.16, "end": 4991.679999999999, "text": " mistake. And I think that changing that would inspire me. That would be, yes, this is progress.", "tokens": [50608, 6146, 13, 400, 286, 519, 300, 4473, 300, 576, 15638, 385, 13, 663, 576, 312, 11, 2086, 11, 341, 307, 4205, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1197352409362793, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.011137375608086586}, {"id": 867, "seek": 497728, "start": 4992.4, "end": 4998.639999999999, "text": " Yeah, that's a really nice way to put it. But I also just don't like that human instinct to", "tokens": [51120, 865, 11, 300, 311, 257, 534, 1481, 636, 281, 829, 309, 13, 583, 286, 611, 445, 500, 380, 411, 300, 1952, 16556, 281, 51432], "temperature": 0.0, "avg_logprob": -0.1197352409362793, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.011137375608086586}, {"id": 868, "seek": 497728, "start": 4999.679999999999, "end": 5004.32, "text": " criticize a model is not intelligent. That's the same instinct as we do when we criticize", "tokens": [51484, 31010, 257, 2316, 307, 406, 13232, 13, 663, 311, 264, 912, 16556, 382, 321, 360, 562, 321, 31010, 51716], "temperature": 0.0, "avg_logprob": -0.1197352409362793, "compression_ratio": 1.6431718061674008, "no_speech_prob": 0.011137375608086586}, {"id": 869, "seek": 500432, "start": 5004.32, "end": 5014.24, "text": " any group of creatures as the other. Because it's very possible that GPT2 is much smarter", "tokens": [50364, 604, 1594, 295, 12281, 382, 264, 661, 13, 1436, 309, 311, 588, 1944, 300, 26039, 51, 17, 307, 709, 20294, 50860], "temperature": 0.0, "avg_logprob": -0.11062655391463314, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0014534206129610538}, {"id": 870, "seek": 500432, "start": 5014.24, "end": 5019.28, "text": " than human beings at many things. That's definitely true. It has a lot more breadth of knowledge.", "tokens": [50860, 813, 1952, 8958, 412, 867, 721, 13, 663, 311, 2138, 2074, 13, 467, 575, 257, 688, 544, 35862, 295, 3601, 13, 51112], "temperature": 0.0, "avg_logprob": -0.11062655391463314, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0014534206129610538}, {"id": 871, "seek": 500432, "start": 5019.28, "end": 5024.639999999999, "text": " Yes, breadth of knowledge and even perhaps depth on certain topics.", "tokens": [51112, 1079, 11, 35862, 295, 3601, 293, 754, 4317, 7161, 322, 1629, 8378, 13, 51380], "temperature": 0.0, "avg_logprob": -0.11062655391463314, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0014534206129610538}, {"id": 872, "seek": 500432, "start": 5026.0, "end": 5030.24, "text": " It's kind of hard to judge what depth means, but there's definitely a sense in which", "tokens": [51448, 467, 311, 733, 295, 1152, 281, 6995, 437, 7161, 1355, 11, 457, 456, 311, 2138, 257, 2020, 294, 597, 51660], "temperature": 0.0, "avg_logprob": -0.11062655391463314, "compression_ratio": 1.588785046728972, "no_speech_prob": 0.0014534206129610538}, {"id": 873, "seek": 503024, "start": 5031.12, "end": 5033.92, "text": " humans don't make mistakes that these models do.", "tokens": [50408, 6255, 500, 380, 652, 8038, 300, 613, 5245, 360, 13, 50548], "temperature": 0.0, "avg_logprob": -0.10120799382527669, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.00124464207328856}, {"id": 874, "seek": 503024, "start": 5034.48, "end": 5039.28, "text": " Yes, the same is applied to autonomous vehicles. The same is probably going to continue being", "tokens": [50576, 1079, 11, 264, 912, 307, 6456, 281, 23797, 8948, 13, 440, 912, 307, 1391, 516, 281, 2354, 885, 50816], "temperature": 0.0, "avg_logprob": -0.10120799382527669, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.00124464207328856}, {"id": 875, "seek": 503024, "start": 5039.28, "end": 5047.36, "text": " applied to a lot of artificial intelligence systems. In the 21st century, the process of", "tokens": [50816, 6456, 281, 257, 688, 295, 11677, 7599, 3652, 13, 682, 264, 5080, 372, 4901, 11, 264, 1399, 295, 51220], "temperature": 0.0, "avg_logprob": -0.10120799382527669, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.00124464207328856}, {"id": 876, "seek": 503024, "start": 5047.36, "end": 5055.28, "text": " analyzing the progress of AI is the search for one case where the system fails in a big way", "tokens": [51220, 23663, 264, 4205, 295, 7318, 307, 264, 3164, 337, 472, 1389, 689, 264, 1185, 18199, 294, 257, 955, 636, 51616], "temperature": 0.0, "avg_logprob": -0.10120799382527669, "compression_ratio": 1.5603864734299517, "no_speech_prob": 0.00124464207328856}, {"id": 877, "seek": 505528, "start": 5055.28, "end": 5061.5199999999995, "text": " where humans would not. And then many people writing articles about it. And then broadly,", "tokens": [50364, 689, 6255, 576, 406, 13, 400, 550, 867, 561, 3579, 11290, 466, 309, 13, 400, 550, 19511, 11, 50676], "temperature": 0.0, "avg_logprob": -0.10553135062163731, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.09386026859283447}, {"id": 878, "seek": 505528, "start": 5062.719999999999, "end": 5068.719999999999, "text": " the public generally gets convinced that the system is not intelligent. And we pacify ourselves", "tokens": [50736, 264, 1908, 5101, 2170, 12561, 300, 264, 1185, 307, 406, 13232, 13, 400, 321, 15165, 2505, 4175, 51036], "temperature": 0.0, "avg_logprob": -0.10553135062163731, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.09386026859283447}, {"id": 879, "seek": 505528, "start": 5068.719999999999, "end": 5073.5199999999995, "text": " by thinking it's not intelligent because of this one anecdotal case. And this seems to continue", "tokens": [51036, 538, 1953, 309, 311, 406, 13232, 570, 295, 341, 472, 26652, 38180, 1389, 13, 400, 341, 2544, 281, 2354, 51276], "temperature": 0.0, "avg_logprob": -0.10553135062163731, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.09386026859283447}, {"id": 880, "seek": 505528, "start": 5073.5199999999995, "end": 5078.4, "text": " happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also", "tokens": [51276, 2737, 13, 865, 11, 286, 914, 11, 456, 307, 3494, 281, 300, 13, 5780, 286, 478, 988, 300, 7140, 295, 561, 366, 611, 51520], "temperature": 0.0, "avg_logprob": -0.10553135062163731, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.09386026859283447}, {"id": 881, "seek": 505528, "start": 5078.4, "end": 5082.16, "text": " extremely impressed by the systems that exist today. But I think this connects to the earlier", "tokens": [51520, 4664, 11679, 538, 264, 3652, 300, 2514, 965, 13, 583, 286, 519, 341, 16967, 281, 264, 3071, 51708], "temperature": 0.0, "avg_logprob": -0.10553135062163731, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.09386026859283447}, {"id": 882, "seek": 508216, "start": 5082.16, "end": 5089.5199999999995, "text": " point we discussed that it's just confusing to judge progress in AI. And you have a new robot", "tokens": [50364, 935, 321, 7152, 300, 309, 311, 445, 13181, 281, 6995, 4205, 294, 7318, 13, 400, 291, 362, 257, 777, 7881, 50732], "temperature": 0.0, "avg_logprob": -0.11043745471585181, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.004899202845990658}, {"id": 883, "seek": 508216, "start": 5089.5199999999995, "end": 5095.44, "text": " demonstrating something. How impressed should you be? And I think that people will start to be", "tokens": [50732, 29889, 746, 13, 1012, 11679, 820, 291, 312, 30, 400, 286, 519, 300, 561, 486, 722, 281, 312, 51028], "temperature": 0.0, "avg_logprob": -0.11043745471585181, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.004899202845990658}, {"id": 884, "seek": 508216, "start": 5095.44, "end": 5101.5199999999995, "text": " impressed once AI starts to really move the needle on the GDP. So you're one of the people that", "tokens": [51028, 11679, 1564, 7318, 3719, 281, 534, 1286, 264, 11037, 322, 264, 19599, 13, 407, 291, 434, 472, 295, 264, 561, 300, 51332], "temperature": 0.0, "avg_logprob": -0.11043745471585181, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.004899202845990658}, {"id": 885, "seek": 508216, "start": 5101.5199999999995, "end": 5108.0, "text": " might be able to create an AI system here, not you, but you and open AI. If you do create an", "tokens": [51332, 1062, 312, 1075, 281, 1884, 364, 7318, 1185, 510, 11, 406, 291, 11, 457, 291, 293, 1269, 7318, 13, 759, 291, 360, 1884, 364, 51656], "temperature": 0.0, "avg_logprob": -0.11043745471585181, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.004899202845990658}, {"id": 886, "seek": 510800, "start": 5108.0, "end": 5116.0, "text": " AI system, and you get to spend sort of the evening with it, him, her, what would you talk about,", "tokens": [50364, 7318, 1185, 11, 293, 291, 483, 281, 3496, 1333, 295, 264, 5634, 365, 309, 11, 796, 11, 720, 11, 437, 576, 291, 751, 466, 11, 50764], "temperature": 0.0, "avg_logprob": -0.17227444504246567, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.0694575309753418}, {"id": 887, "seek": 510800, "start": 5116.0, "end": 5120.8, "text": " do you think? The very first time? First time? Well, the first time I would just", "tokens": [50764, 360, 291, 519, 30, 440, 588, 700, 565, 30, 2386, 565, 30, 1042, 11, 264, 700, 565, 286, 576, 445, 51004], "temperature": 0.0, "avg_logprob": -0.17227444504246567, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.0694575309753418}, {"id": 888, "seek": 510800, "start": 5121.76, "end": 5126.32, "text": " would just ask all kinds of questions and try to make it to get it to make a mistake. And that would", "tokens": [51052, 576, 445, 1029, 439, 3685, 295, 1651, 293, 853, 281, 652, 309, 281, 483, 309, 281, 652, 257, 6146, 13, 400, 300, 576, 51280], "temperature": 0.0, "avg_logprob": -0.17227444504246567, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.0694575309753418}, {"id": 889, "seek": 510800, "start": 5126.32, "end": 5133.92, "text": " be amazed that it doesn't make mistakes and just keep keep asking broad. Okay, what kind of questions", "tokens": [51280, 312, 20507, 300, 309, 1177, 380, 652, 8038, 293, 445, 1066, 1066, 3365, 4152, 13, 1033, 11, 437, 733, 295, 1651, 51660], "temperature": 0.0, "avg_logprob": -0.17227444504246567, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.0694575309753418}, {"id": 890, "seek": 513392, "start": 5133.92, "end": 5141.12, "text": " do you think? Would they be factual or would they be personal, emotional, psychological? What do you", "tokens": [50364, 360, 291, 519, 30, 6068, 436, 312, 48029, 420, 576, 436, 312, 2973, 11, 6863, 11, 14346, 30, 708, 360, 291, 50724], "temperature": 0.0, "avg_logprob": -0.08803815841674804, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.01405435148626566}, {"id": 891, "seek": 513392, "start": 5141.12, "end": 5151.52, "text": " think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself", "tokens": [50724, 519, 30, 1057, 295, 264, 3673, 13, 6068, 291, 1029, 337, 5192, 30, 12151, 13, 286, 914, 11, 983, 576, 286, 4948, 2059, 51244], "temperature": 0.0, "avg_logprob": -0.08803815841674804, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.01405435148626566}, {"id": 892, "seek": 513392, "start": 5151.52, "end": 5157.4400000000005, "text": " talking to a system like this? Now, again, let me emphasize the fact that you truly are one of the", "tokens": [51244, 1417, 281, 257, 1185, 411, 341, 30, 823, 11, 797, 11, 718, 385, 16078, 264, 1186, 300, 291, 4908, 366, 472, 295, 264, 51540], "temperature": 0.0, "avg_logprob": -0.08803815841674804, "compression_ratio": 1.544502617801047, "no_speech_prob": 0.01405435148626566}, {"id": 893, "seek": 515744, "start": 5157.44, "end": 5165.12, "text": " people that might be in the room where this happens. So let me ask a sort of a profound question", "tokens": [50364, 561, 300, 1062, 312, 294, 264, 1808, 689, 341, 2314, 13, 407, 718, 385, 1029, 257, 1333, 295, 257, 14382, 1168, 50748], "temperature": 0.0, "avg_logprob": -0.09975754484838369, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.09258035570383072}, {"id": 894, "seek": 515744, "start": 5165.12, "end": 5171.5199999999995, "text": " about, I've just talked to a Stalin historian. I've been talking to a lot of people who are", "tokens": [50748, 466, 11, 286, 600, 445, 2825, 281, 257, 32126, 25139, 13, 286, 600, 668, 1417, 281, 257, 688, 295, 561, 567, 366, 51068], "temperature": 0.0, "avg_logprob": -0.09975754484838369, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.09258035570383072}, {"id": 895, "seek": 515744, "start": 5171.5199999999995, "end": 5178.5599999999995, "text": " studying power. Abraham Lincoln said, nearly all men can stand adversity. But if you want to test", "tokens": [51068, 7601, 1347, 13, 17782, 15993, 848, 11, 6217, 439, 1706, 393, 1463, 40018, 13, 583, 498, 291, 528, 281, 1500, 51420], "temperature": 0.0, "avg_logprob": -0.09975754484838369, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.09258035570383072}, {"id": 896, "seek": 515744, "start": 5178.5599999999995, "end": 5186.4, "text": " a man's character, give him power. I would say the power of the 21st century, maybe the 22nd,", "tokens": [51420, 257, 587, 311, 2517, 11, 976, 796, 1347, 13, 286, 576, 584, 264, 1347, 295, 264, 5080, 372, 4901, 11, 1310, 264, 5853, 273, 11, 51812], "temperature": 0.0, "avg_logprob": -0.09975754484838369, "compression_ratio": 1.5261044176706828, "no_speech_prob": 0.09258035570383072}, {"id": 897, "seek": 518640, "start": 5186.4, "end": 5192.639999999999, "text": " but hopefully the 21st would be the creation of an AGI system and the people who have control,", "tokens": [50364, 457, 4696, 264, 5080, 372, 576, 312, 264, 8016, 295, 364, 316, 26252, 1185, 293, 264, 561, 567, 362, 1969, 11, 50676], "temperature": 0.0, "avg_logprob": -0.10008162260055542, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00041073892498388886}, {"id": 898, "seek": 518640, "start": 5193.36, "end": 5200.0, "text": " direct possession and control the AGI system. So what do you think after spending that evening", "tokens": [50712, 2047, 20935, 293, 1969, 264, 316, 26252, 1185, 13, 407, 437, 360, 291, 519, 934, 6434, 300, 5634, 51044], "temperature": 0.0, "avg_logprob": -0.10008162260055542, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00041073892498388886}, {"id": 899, "seek": 518640, "start": 5201.12, "end": 5205.12, "text": " having a discussion with the AGI system? What do you think you would do?", "tokens": [51100, 1419, 257, 5017, 365, 264, 316, 26252, 1185, 30, 708, 360, 291, 519, 291, 576, 360, 30, 51300], "temperature": 0.0, "avg_logprob": -0.10008162260055542, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.00041073892498388886}, {"id": 900, "seek": 520512, "start": 5205.5199999999995, "end": 5216.24, "text": " Well, the ideal world I'd like to imagine is one where humanity are like the board,", "tokens": [50384, 1042, 11, 264, 7157, 1002, 286, 1116, 411, 281, 3811, 307, 472, 689, 10243, 366, 411, 264, 3150, 11, 50920], "temperature": 0.0, "avg_logprob": -0.19815830957321895, "compression_ratio": 1.4876543209876543, "no_speech_prob": 0.07152410596609116}, {"id": 901, "seek": 520512, "start": 5216.96, "end": 5223.76, "text": " the board members of a company where the AGI is the CEO. So it would be,", "tokens": [50956, 264, 3150, 2679, 295, 257, 2237, 689, 264, 316, 26252, 307, 264, 9282, 13, 407, 309, 576, 312, 11, 51296], "temperature": 0.0, "avg_logprob": -0.19815830957321895, "compression_ratio": 1.4876543209876543, "no_speech_prob": 0.07152410596609116}, {"id": 902, "seek": 520512, "start": 5225.5199999999995, "end": 5229.84, "text": " I would like the picture of which I would imagine is you have some kind of different", "tokens": [51384, 286, 576, 411, 264, 3036, 295, 597, 286, 576, 3811, 307, 291, 362, 512, 733, 295, 819, 51600], "temperature": 0.0, "avg_logprob": -0.19815830957321895, "compression_ratio": 1.4876543209876543, "no_speech_prob": 0.07152410596609116}, {"id": 903, "seek": 522984, "start": 5230.72, "end": 5237.52, "text": " entities, different countries or cities, and the people that leave their vote for what the AGI", "tokens": [50408, 16667, 11, 819, 3517, 420, 6486, 11, 293, 264, 561, 300, 1856, 641, 4740, 337, 437, 264, 316, 26252, 50748], "temperature": 0.0, "avg_logprob": -0.1475833753744761, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00830908864736557}, {"id": 904, "seek": 522984, "start": 5237.52, "end": 5242.16, "text": " that represents them should do and the AGI that represents them goes and does it. I think a picture", "tokens": [50748, 300, 8855, 552, 820, 360, 293, 264, 316, 26252, 300, 8855, 552, 1709, 293, 775, 309, 13, 286, 519, 257, 3036, 50980], "temperature": 0.0, "avg_logprob": -0.1475833753744761, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00830908864736557}, {"id": 905, "seek": 522984, "start": 5242.16, "end": 5248.56, "text": " like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city,", "tokens": [50980, 411, 300, 11, 286, 915, 588, 23842, 13, 509, 727, 362, 3866, 316, 26252, 11, 291, 576, 362, 364, 316, 26252, 337, 257, 2307, 11, 51300], "temperature": 0.0, "avg_logprob": -0.1475833753744761, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00830908864736557}, {"id": 906, "seek": 522984, "start": 5248.56, "end": 5255.92, "text": " for a country, and it would be trying to in effect take the democratic process to the next level.", "tokens": [51300, 337, 257, 1941, 11, 293, 309, 576, 312, 1382, 281, 294, 1802, 747, 264, 15337, 1399, 281, 264, 958, 1496, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1475833753744761, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00830908864736557}, {"id": 907, "seek": 525592, "start": 5255.92, "end": 5260.08, "text": " And the board can almost fire the CEO? Essentially, press the reset button,", "tokens": [50364, 400, 264, 3150, 393, 1920, 2610, 264, 9282, 30, 23596, 11, 1886, 264, 14322, 2960, 11, 50572], "temperature": 0.0, "avg_logprob": -0.18062274330540706, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.0674232468008995}, {"id": 908, "seek": 525592, "start": 5260.08, "end": 5267.2, "text": " say, re-randomize the parameters. But let me sort of, that's actually, okay, that's a beautiful", "tokens": [50572, 584, 11, 319, 12, 3699, 298, 1125, 264, 9834, 13, 583, 718, 385, 1333, 295, 11, 300, 311, 767, 11, 1392, 11, 300, 311, 257, 2238, 50928], "temperature": 0.0, "avg_logprob": -0.18062274330540706, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.0674232468008995}, {"id": 909, "seek": 525592, "start": 5267.2, "end": 5274.56, "text": " vision, I think, as long as it's possible to press the reset button. Do you think it will always be", "tokens": [50928, 5201, 11, 286, 519, 11, 382, 938, 382, 309, 311, 1944, 281, 1886, 264, 14322, 2960, 13, 1144, 291, 519, 309, 486, 1009, 312, 51296], "temperature": 0.0, "avg_logprob": -0.18062274330540706, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.0674232468008995}, {"id": 910, "seek": 525592, "start": 5274.56, "end": 5280.08, "text": " possible to press the reset button? So I think that it's definitely really possible to build.", "tokens": [51296, 1944, 281, 1886, 264, 14322, 2960, 30, 407, 286, 519, 300, 309, 311, 2138, 534, 1944, 281, 1322, 13, 51572], "temperature": 0.0, "avg_logprob": -0.18062274330540706, "compression_ratio": 1.7548076923076923, "no_speech_prob": 0.0674232468008995}, {"id": 911, "seek": 528008, "start": 5280.88, "end": 5291.6, "text": " So you're talking, so the question that I really understand from you is, will humans or humans", "tokens": [50404, 407, 291, 434, 1417, 11, 370, 264, 1168, 300, 286, 534, 1223, 490, 291, 307, 11, 486, 6255, 420, 6255, 50940], "temperature": 0.0, "avg_logprob": -0.16789955411638532, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.02123311720788479}, {"id": 912, "seek": 528008, "start": 5291.6, "end": 5296.72, "text": " people have control over the AI systems that they build? Yes. And my answer is, it's definitely", "tokens": [50940, 561, 362, 1969, 670, 264, 7318, 3652, 300, 436, 1322, 30, 1079, 13, 400, 452, 1867, 307, 11, 309, 311, 2138, 51196], "temperature": 0.0, "avg_logprob": -0.16789955411638532, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.02123311720788479}, {"id": 913, "seek": 528008, "start": 5296.72, "end": 5302.72, "text": " possible to build AI systems which will want to be controlled by their humans. Wow, that's part of", "tokens": [51196, 1944, 281, 1322, 7318, 3652, 597, 486, 528, 281, 312, 10164, 538, 641, 6255, 13, 3153, 11, 300, 311, 644, 295, 51496], "temperature": 0.0, "avg_logprob": -0.16789955411638532, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.02123311720788479}, {"id": 914, "seek": 530272, "start": 5302.72, "end": 5312.08, "text": " their, so it's not that just they can't help but be controlled, but that's the one of the", "tokens": [50364, 641, 11, 370, 309, 311, 406, 300, 445, 436, 393, 380, 854, 457, 312, 10164, 11, 457, 300, 311, 264, 472, 295, 264, 50832], "temperature": 0.0, "avg_logprob": -0.1400793117025624, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.12226902693510056}, {"id": 915, "seek": 530272, "start": 5312.08, "end": 5317.360000000001, "text": " objectives of their existence is to be controlled. In the same way that human parents", "tokens": [50832, 15961, 295, 641, 9123, 307, 281, 312, 10164, 13, 682, 264, 912, 636, 300, 1952, 3152, 51096], "temperature": 0.0, "avg_logprob": -0.1400793117025624, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.12226902693510056}, {"id": 916, "seek": 530272, "start": 5319.76, "end": 5325.280000000001, "text": " generally want to help their children, they want their children to succeed. It's not a burden for", "tokens": [51216, 5101, 528, 281, 854, 641, 2227, 11, 436, 528, 641, 2227, 281, 7754, 13, 467, 311, 406, 257, 12578, 337, 51492], "temperature": 0.0, "avg_logprob": -0.1400793117025624, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.12226902693510056}, {"id": 917, "seek": 530272, "start": 5325.280000000001, "end": 5331.6, "text": " them. They are excited to help the children to feed them and to dress them and to take care of them.", "tokens": [51492, 552, 13, 814, 366, 2919, 281, 854, 264, 2227, 281, 3154, 552, 293, 281, 5231, 552, 293, 281, 747, 1127, 295, 552, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1400793117025624, "compression_ratio": 1.8514851485148516, "no_speech_prob": 0.12226902693510056}, {"id": 918, "seek": 533160, "start": 5332.56, "end": 5339.360000000001, "text": " And I believe with high conviction that the same will be possible for an AGI. It will be possible", "tokens": [50412, 400, 286, 1697, 365, 1090, 24837, 300, 264, 912, 486, 312, 1944, 337, 364, 316, 26252, 13, 467, 486, 312, 1944, 50752], "temperature": 0.0, "avg_logprob": -0.0912719023855109, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0011327528627589345}, {"id": 919, "seek": 533160, "start": 5339.360000000001, "end": 5345.360000000001, "text": " to program an AGI to design it in such a way that it will have a similar deep drive that it will be", "tokens": [50752, 281, 1461, 364, 316, 26252, 281, 1715, 309, 294, 1270, 257, 636, 300, 309, 486, 362, 257, 2531, 2452, 3332, 300, 309, 486, 312, 51052], "temperature": 0.0, "avg_logprob": -0.0912719023855109, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0011327528627589345}, {"id": 920, "seek": 533160, "start": 5345.360000000001, "end": 5352.8, "text": " delighted to fulfill and the drive will be to help humans flourish. But let me take a step back", "tokens": [51052, 18783, 281, 13875, 293, 264, 3332, 486, 312, 281, 854, 6255, 38311, 13, 583, 718, 385, 747, 257, 1823, 646, 51424], "temperature": 0.0, "avg_logprob": -0.0912719023855109, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0011327528627589345}, {"id": 921, "seek": 533160, "start": 5352.8, "end": 5356.88, "text": " to that moment where you create the AGI system. I think this is a really crucial moment.", "tokens": [51424, 281, 300, 1623, 689, 291, 1884, 264, 316, 26252, 1185, 13, 286, 519, 341, 307, 257, 534, 11462, 1623, 13, 51628], "temperature": 0.0, "avg_logprob": -0.0912719023855109, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0011327528627589345}, {"id": 922, "seek": 535688, "start": 5357.76, "end": 5367.36, "text": " And between that moment and the Democratic Board members with the AGI at the head,", "tokens": [50408, 400, 1296, 300, 1623, 293, 264, 14928, 10008, 2679, 365, 264, 316, 26252, 412, 264, 1378, 11, 50888], "temperature": 0.0, "avg_logprob": -0.13745038803309612, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.01048231776803732}, {"id": 923, "seek": 535688, "start": 5368.8, "end": 5376.16, "text": " there has to be a relinquishing of power. So as George Washington, despite all the bad things he", "tokens": [50960, 456, 575, 281, 312, 257, 1039, 37384, 3807, 295, 1347, 13, 407, 382, 7136, 6149, 11, 7228, 439, 264, 1578, 721, 415, 51328], "temperature": 0.0, "avg_logprob": -0.13745038803309612, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.01048231776803732}, {"id": 924, "seek": 535688, "start": 5376.16, "end": 5380.96, "text": " did, one of the big things he did is he relinquished power. He first of all didn't want to be president", "tokens": [51328, 630, 11, 472, 295, 264, 955, 721, 415, 630, 307, 415, 1039, 37384, 4729, 1347, 13, 634, 700, 295, 439, 994, 380, 528, 281, 312, 3868, 51568], "temperature": 0.0, "avg_logprob": -0.13745038803309612, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.01048231776803732}, {"id": 925, "seek": 538096, "start": 5381.92, "end": 5387.76, "text": " and even when he became president, he didn't keep just serving as most dictators do for indefinitely.", "tokens": [50412, 293, 754, 562, 415, 3062, 3868, 11, 415, 994, 380, 1066, 445, 8148, 382, 881, 34708, 360, 337, 24162, 10925, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1320148325980978, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0838032141327858}, {"id": 926, "seek": 538096, "start": 5389.12, "end": 5397.12, "text": " Do you see yourself being able to relinquish control over an AGI system, given how much", "tokens": [50772, 1144, 291, 536, 1803, 885, 1075, 281, 1039, 37384, 742, 1969, 670, 364, 316, 26252, 1185, 11, 2212, 577, 709, 51172], "temperature": 0.0, "avg_logprob": -0.1320148325980978, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0838032141327858}, {"id": 927, "seek": 538096, "start": 5397.12, "end": 5403.04, "text": " power you can have over the world, at first financial, just make a lot of money and then", "tokens": [51172, 1347, 291, 393, 362, 670, 264, 1002, 11, 412, 700, 4669, 11, 445, 652, 257, 688, 295, 1460, 293, 550, 51468], "temperature": 0.0, "avg_logprob": -0.1320148325980978, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0838032141327858}, {"id": 928, "seek": 538096, "start": 5403.04, "end": 5409.28, "text": " control by having possession as a AGI system? I'd find it trivial to do that. I'd find it", "tokens": [51468, 1969, 538, 1419, 20935, 382, 257, 316, 26252, 1185, 30, 286, 1116, 915, 309, 26703, 281, 360, 300, 13, 286, 1116, 915, 309, 51780], "temperature": 0.0, "avg_logprob": -0.1320148325980978, "compression_ratio": 1.5793991416309012, "no_speech_prob": 0.0838032141327858}, {"id": 929, "seek": 540928, "start": 5409.36, "end": 5414.32, "text": " trivial to relinquish this kind of power. I mean, you know, the kind of scenario you are", "tokens": [50368, 26703, 281, 1039, 37384, 742, 341, 733, 295, 1347, 13, 286, 914, 11, 291, 458, 11, 264, 733, 295, 9005, 291, 366, 50616], "temperature": 0.0, "avg_logprob": -0.121261891630507, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.008056938648223877}, {"id": 930, "seek": 540928, "start": 5414.32, "end": 5421.2, "text": " describing sounds terrifying to me. That's all. I would absolutely not want to be in that position.", "tokens": [50616, 16141, 3263, 18106, 281, 385, 13, 663, 311, 439, 13, 286, 576, 3122, 406, 528, 281, 312, 294, 300, 2535, 13, 50960], "temperature": 0.0, "avg_logprob": -0.121261891630507, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.008056938648223877}, {"id": 931, "seek": 540928, "start": 5422.4, "end": 5428.639999999999, "text": " Do you think you represent the majority or the minority of people in the AGI community?", "tokens": [51020, 1144, 291, 519, 291, 2906, 264, 6286, 420, 264, 16166, 295, 561, 294, 264, 316, 26252, 1768, 30, 51332], "temperature": 0.0, "avg_logprob": -0.121261891630507, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.008056938648223877}, {"id": 932, "seek": 540928, "start": 5429.28, "end": 5435.599999999999, "text": " Well, I mean, it's an open question, an important one. Are most people good is another way to ask it.", "tokens": [51364, 1042, 11, 286, 914, 11, 309, 311, 364, 1269, 1168, 11, 364, 1021, 472, 13, 2014, 881, 561, 665, 307, 1071, 636, 281, 1029, 309, 13, 51680], "temperature": 0.0, "avg_logprob": -0.121261891630507, "compression_ratio": 1.5815899581589958, "no_speech_prob": 0.008056938648223877}, {"id": 933, "seek": 543560, "start": 5436.4800000000005, "end": 5439.6, "text": " So I don't know if most people are good. But", "tokens": [50408, 407, 286, 500, 380, 458, 498, 881, 561, 366, 665, 13, 583, 50564], "temperature": 0.0, "avg_logprob": -0.12811930100996416, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0013038975885137916}, {"id": 934, "seek": 543560, "start": 5442.160000000001, "end": 5445.76, "text": " I think that when it really counts, people can be better than we think.", "tokens": [50692, 286, 519, 300, 562, 309, 534, 14893, 11, 561, 393, 312, 1101, 813, 321, 519, 13, 50872], "temperature": 0.0, "avg_logprob": -0.12811930100996416, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0013038975885137916}, {"id": 935, "seek": 543560, "start": 5446.88, "end": 5452.56, "text": " That's beautifully put. Yeah. Are there specific mechanisms you can think of of aligning AGI", "tokens": [50928, 663, 311, 16525, 829, 13, 865, 13, 2014, 456, 2685, 15902, 291, 393, 519, 295, 295, 419, 9676, 316, 26252, 51212], "temperature": 0.0, "avg_logprob": -0.12811930100996416, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0013038975885137916}, {"id": 936, "seek": 543560, "start": 5452.56, "end": 5458.320000000001, "text": " in values to human values? Is that do you think about these problems of continued alignment", "tokens": [51212, 294, 4190, 281, 1952, 4190, 30, 1119, 300, 360, 291, 519, 466, 613, 2740, 295, 7014, 18515, 51500], "temperature": 0.0, "avg_logprob": -0.12811930100996416, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0013038975885137916}, {"id": 937, "seek": 543560, "start": 5458.320000000001, "end": 5463.4400000000005, "text": " as we develop the systems? Yeah, definitely. In some sense,", "tokens": [51500, 382, 321, 1499, 264, 3652, 30, 865, 11, 2138, 13, 682, 512, 2020, 11, 51756], "temperature": 0.0, "avg_logprob": -0.12811930100996416, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0013038975885137916}, {"id": 938, "seek": 546344, "start": 5464.4, "end": 5469.12, "text": " the kind of question which you are asking is so if you have to translate the question to today's", "tokens": [50412, 264, 733, 295, 1168, 597, 291, 366, 3365, 307, 370, 498, 291, 362, 281, 13799, 264, 1168, 281, 965, 311, 50648], "temperature": 0.0, "avg_logprob": -0.2063624064127604, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0013652516063302755}, {"id": 939, "seek": 546344, "start": 5469.12, "end": 5478.4, "text": " terms, it would be a question about how to get an RL agent that's optimizing a value function", "tokens": [50648, 2115, 11, 309, 576, 312, 257, 1168, 466, 577, 281, 483, 364, 497, 43, 9461, 300, 311, 40425, 257, 2158, 2445, 51112], "temperature": 0.0, "avg_logprob": -0.2063624064127604, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0013652516063302755}, {"id": 940, "seek": 546344, "start": 5478.4, "end": 5483.919999999999, "text": " which itself is learned. And if you look at humans, humans are like that because the reward", "tokens": [51112, 597, 2564, 307, 3264, 13, 400, 498, 291, 574, 412, 6255, 11, 6255, 366, 411, 300, 570, 264, 7782, 51388], "temperature": 0.0, "avg_logprob": -0.2063624064127604, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0013652516063302755}, {"id": 941, "seek": 546344, "start": 5483.919999999999, "end": 5489.919999999999, "text": " function, the value function of humans is not external, it is internal. That's right. And", "tokens": [51388, 2445, 11, 264, 2158, 2445, 295, 6255, 307, 406, 8320, 11, 309, 307, 6920, 13, 663, 311, 558, 13, 400, 51688], "temperature": 0.0, "avg_logprob": -0.2063624064127604, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.0013652516063302755}, {"id": 942, "seek": 548992, "start": 5490.08, "end": 5498.08, "text": " and there are definite ideas of how to train a value function, basically an objective,", "tokens": [50372, 293, 456, 366, 25131, 3487, 295, 577, 281, 3847, 257, 2158, 2445, 11, 1936, 364, 10024, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1317736562093099, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.001063492032699287}, {"id": 943, "seek": 548992, "start": 5498.96, "end": 5504.24, "text": " an as objective as possible perception system that will be trained separately", "tokens": [50816, 364, 382, 10024, 382, 1944, 12860, 1185, 300, 486, 312, 8895, 14759, 51080], "temperature": 0.0, "avg_logprob": -0.1317736562093099, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.001063492032699287}, {"id": 944, "seek": 548992, "start": 5505.68, "end": 5513.12, "text": " to recognize, to internalize human judgments on different situations. And then that component", "tokens": [51152, 281, 5521, 11, 281, 6920, 1125, 1952, 40337, 322, 819, 6851, 13, 400, 550, 300, 6542, 51524], "temperature": 0.0, "avg_logprob": -0.1317736562093099, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.001063492032699287}, {"id": 945, "seek": 548992, "start": 5513.12, "end": 5518.88, "text": " wouldn't be integrated as the value as the base value function for some more capable RL system.", "tokens": [51524, 2759, 380, 312, 10919, 382, 264, 2158, 382, 264, 3096, 2158, 2445, 337, 512, 544, 8189, 497, 43, 1185, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1317736562093099, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.001063492032699287}, {"id": 946, "seek": 551888, "start": 5518.88, "end": 5522.96, "text": " You could imagine a process like this. I'm not saying this is the process. I'm saying this is", "tokens": [50364, 509, 727, 3811, 257, 1399, 411, 341, 13, 286, 478, 406, 1566, 341, 307, 264, 1399, 13, 286, 478, 1566, 341, 307, 50568], "temperature": 0.0, "avg_logprob": -0.09792811942823006, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.0027985030319541693}, {"id": 947, "seek": 551888, "start": 5522.96, "end": 5531.12, "text": " an example of the kind of thing you could do. So on that topic of the objective functions of", "tokens": [50568, 364, 1365, 295, 264, 733, 295, 551, 291, 727, 360, 13, 407, 322, 300, 4829, 295, 264, 10024, 6828, 295, 50976], "temperature": 0.0, "avg_logprob": -0.09792811942823006, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.0027985030319541693}, {"id": 948, "seek": 551888, "start": 5531.12, "end": 5536.72, "text": " human existence, what do you think is the objective function that's implicit in human", "tokens": [50976, 1952, 9123, 11, 437, 360, 291, 519, 307, 264, 10024, 2445, 300, 311, 26947, 294, 1952, 51256], "temperature": 0.0, "avg_logprob": -0.09792811942823006, "compression_ratio": 1.7215189873417722, "no_speech_prob": 0.0027985030319541693}, {"id": 949, "seek": 553672, "start": 5536.72, "end": 5552.4800000000005, "text": " existence? What's the meaning of life? Oh, I think the question is wrong in some way. I think that", "tokens": [50364, 9123, 30, 708, 311, 264, 3620, 295, 993, 30, 876, 11, 286, 519, 264, 1168, 307, 2085, 294, 512, 636, 13, 286, 519, 300, 51152], "temperature": 0.0, "avg_logprob": -0.13560935613271352, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.06163816526532173}, {"id": 950, "seek": 553672, "start": 5552.4800000000005, "end": 5556.64, "text": " the question implies that there is an objective answer which is an external answer, you know,", "tokens": [51152, 264, 1168, 18779, 300, 456, 307, 364, 10024, 1867, 597, 307, 364, 8320, 1867, 11, 291, 458, 11, 51360], "temperature": 0.0, "avg_logprob": -0.13560935613271352, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.06163816526532173}, {"id": 951, "seek": 553672, "start": 5556.64, "end": 5564.320000000001, "text": " your meaning of life is X. I think what's going on is that we exist and that's amazing. And we", "tokens": [51360, 428, 3620, 295, 993, 307, 1783, 13, 286, 519, 437, 311, 516, 322, 307, 300, 321, 2514, 293, 300, 311, 2243, 13, 400, 321, 51744], "temperature": 0.0, "avg_logprob": -0.13560935613271352, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.06163816526532173}, {"id": 952, "seek": 556432, "start": 5564.4, "end": 5568.96, "text": " should try to make the most of it and try to maximize our own value and enjoyment of", "tokens": [50368, 820, 853, 281, 652, 264, 881, 295, 309, 293, 853, 281, 19874, 527, 1065, 2158, 293, 32013, 295, 50596], "temperature": 0.0, "avg_logprob": -0.092385128811673, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0016478887991979718}, {"id": 953, "seek": 556432, "start": 5569.759999999999, "end": 5576.16, "text": " a very short time while we do exist. It's funny because action does require an objective function.", "tokens": [50636, 257, 588, 2099, 565, 1339, 321, 360, 2514, 13, 467, 311, 4074, 570, 3069, 775, 3651, 364, 10024, 2445, 13, 50956], "temperature": 0.0, "avg_logprob": -0.092385128811673, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0016478887991979718}, {"id": 954, "seek": 556432, "start": 5576.16, "end": 5581.84, "text": " It's definitely theirs in some form, but it's difficult to make it explicit and maybe impossible", "tokens": [50956, 467, 311, 2138, 22760, 294, 512, 1254, 11, 457, 309, 311, 2252, 281, 652, 309, 13691, 293, 1310, 6243, 51240], "temperature": 0.0, "avg_logprob": -0.092385128811673, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0016478887991979718}, {"id": 955, "seek": 556432, "start": 5581.84, "end": 5587.12, "text": " to make it explicit. I guess is what you're getting at. And that's an interesting fact of an RL", "tokens": [51240, 281, 652, 309, 13691, 13, 286, 2041, 307, 437, 291, 434, 1242, 412, 13, 400, 300, 311, 364, 1880, 1186, 295, 364, 497, 43, 51504], "temperature": 0.0, "avg_logprob": -0.092385128811673, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0016478887991979718}, {"id": 956, "seek": 556432, "start": 5587.12, "end": 5593.36, "text": " environment. Well, what I was making a slightly different point is that humans want things and", "tokens": [51504, 2823, 13, 1042, 11, 437, 286, 390, 1455, 257, 4748, 819, 935, 307, 300, 6255, 528, 721, 293, 51816], "temperature": 0.0, "avg_logprob": -0.092385128811673, "compression_ratio": 1.6761565836298933, "no_speech_prob": 0.0016478887991979718}, {"id": 957, "seek": 559336, "start": 5593.36, "end": 5599.759999999999, "text": " their wants create the drives that cause them to, you know, our wants are our objective functions,", "tokens": [50364, 641, 2738, 1884, 264, 11754, 300, 3082, 552, 281, 11, 291, 458, 11, 527, 2738, 366, 527, 10024, 6828, 11, 50684], "temperature": 0.0, "avg_logprob": -0.1545010353755025, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0009084254852496088}, {"id": 958, "seek": 559336, "start": 5599.759999999999, "end": 5604.16, "text": " our individual objective functions. We can later decide that we want to change,", "tokens": [50684, 527, 2609, 10024, 6828, 13, 492, 393, 1780, 4536, 300, 321, 528, 281, 1319, 11, 50904], "temperature": 0.0, "avg_logprob": -0.1545010353755025, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0009084254852496088}, {"id": 959, "seek": 559336, "start": 5604.16, "end": 5607.12, "text": " that what we wanted before is no longer good and we want something else.", "tokens": [50904, 300, 437, 321, 1415, 949, 307, 572, 2854, 665, 293, 321, 528, 746, 1646, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1545010353755025, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0009084254852496088}, {"id": 960, "seek": 559336, "start": 5607.12, "end": 5612.0, "text": " Yeah, but they're so dynamic, there's got to be some underlying sort of Freud,", "tokens": [51052, 865, 11, 457, 436, 434, 370, 8546, 11, 456, 311, 658, 281, 312, 512, 14217, 1333, 295, 41590, 11, 51296], "temperature": 0.0, "avg_logprob": -0.1545010353755025, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0009084254852496088}, {"id": 961, "seek": 559336, "start": 5612.0, "end": 5617.839999999999, "text": " there's things like sexual stuff, there's people who think it's the fear of death and there's also", "tokens": [51296, 456, 311, 721, 411, 6701, 1507, 11, 456, 311, 561, 567, 519, 309, 311, 264, 4240, 295, 2966, 293, 456, 311, 611, 51588], "temperature": 0.0, "avg_logprob": -0.1545010353755025, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.0009084254852496088}, {"id": 962, "seek": 561784, "start": 5618.8, "end": 5622.72, "text": " the desire for knowledge and, you know, all these kinds of things, procreation,", "tokens": [50412, 264, 7516, 337, 3601, 293, 11, 291, 458, 11, 439, 613, 3685, 295, 721, 11, 9510, 265, 399, 11, 50608], "temperature": 0.0, "avg_logprob": -0.14408704439798992, "compression_ratio": 1.751700680272109, "no_speech_prob": 0.005907469429075718}, {"id": 963, "seek": 561784, "start": 5623.76, "end": 5628.0, "text": " the sort of all the evolutionary arguments, it seems to be, there might be some kind of", "tokens": [50660, 264, 1333, 295, 439, 264, 27567, 12869, 11, 309, 2544, 281, 312, 11, 456, 1062, 312, 512, 733, 295, 50872], "temperature": 0.0, "avg_logprob": -0.14408704439798992, "compression_ratio": 1.751700680272109, "no_speech_prob": 0.005907469429075718}, {"id": 964, "seek": 561784, "start": 5628.0, "end": 5634.88, "text": " fundamental objective function from which everything else emerges. But it seems like", "tokens": [50872, 8088, 10024, 2445, 490, 597, 1203, 1646, 38965, 13, 583, 309, 2544, 411, 51216], "temperature": 0.0, "avg_logprob": -0.14408704439798992, "compression_ratio": 1.751700680272109, "no_speech_prob": 0.005907469429075718}, {"id": 965, "seek": 561784, "start": 5634.88, "end": 5638.16, "text": " it's very difficult to make it explicit. I think that probably is an evolutionary", "tokens": [51216, 309, 311, 588, 2252, 281, 652, 309, 13691, 13, 286, 519, 300, 1391, 307, 364, 27567, 51380], "temperature": 0.0, "avg_logprob": -0.14408704439798992, "compression_ratio": 1.751700680272109, "no_speech_prob": 0.005907469429075718}, {"id": 966, "seek": 561784, "start": 5638.16, "end": 5641.6, "text": " objective function, which is to survive and procreate and make your students succeed.", "tokens": [51380, 10024, 2445, 11, 597, 307, 281, 7867, 293, 9510, 265, 473, 293, 652, 428, 1731, 7754, 13, 51552], "temperature": 0.0, "avg_logprob": -0.14408704439798992, "compression_ratio": 1.751700680272109, "no_speech_prob": 0.005907469429075718}, {"id": 967, "seek": 561784, "start": 5642.4800000000005, "end": 5647.360000000001, "text": " That would be my guess, but it doesn't give an answer to the question of what's the meaning of", "tokens": [51596, 663, 576, 312, 452, 2041, 11, 457, 309, 1177, 380, 976, 364, 1867, 281, 264, 1168, 295, 437, 311, 264, 3620, 295, 51840], "temperature": 0.0, "avg_logprob": -0.14408704439798992, "compression_ratio": 1.751700680272109, "no_speech_prob": 0.005907469429075718}, {"id": 968, "seek": 564736, "start": 5647.36, "end": 5654.5599999999995, "text": " life. I think you can see how humans are part of this big process, this ancient process we are,", "tokens": [50364, 993, 13, 286, 519, 291, 393, 536, 577, 6255, 366, 644, 295, 341, 955, 1399, 11, 341, 7832, 1399, 321, 366, 11, 50724], "temperature": 0.0, "avg_logprob": -0.0955656247261243, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.014929580502212048}, {"id": 969, "seek": 564736, "start": 5656.639999999999, "end": 5664.16, "text": " we exist on a small planet. And that's it. So given that we exist, try to make the most of it", "tokens": [50828, 321, 2514, 322, 257, 1359, 5054, 13, 400, 300, 311, 309, 13, 407, 2212, 300, 321, 2514, 11, 853, 281, 652, 264, 881, 295, 309, 51204], "temperature": 0.0, "avg_logprob": -0.0955656247261243, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.014929580502212048}, {"id": 970, "seek": 564736, "start": 5664.16, "end": 5670.96, "text": " and try to enjoy more and suffer less as much as we can. Let me ask two silly questions about life.", "tokens": [51204, 293, 853, 281, 2103, 544, 293, 9753, 1570, 382, 709, 382, 321, 393, 13, 961, 385, 1029, 732, 11774, 1651, 466, 993, 13, 51544], "temperature": 0.0, "avg_logprob": -0.0955656247261243, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.014929580502212048}, {"id": 971, "seek": 567096, "start": 5671.68, "end": 5679.36, "text": " One, do you have regrets? Moments that if you went back, you would do differently. And two,", "tokens": [50400, 1485, 11, 360, 291, 362, 31214, 30, 5576, 791, 300, 498, 291, 1437, 646, 11, 291, 576, 360, 7614, 13, 400, 732, 11, 50784], "temperature": 0.0, "avg_logprob": -0.1595221194592151, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.013204364106059074}, {"id": 972, "seek": 567096, "start": 5680.0, "end": 5683.36, "text": " are there moments that you're especially proud of that made you truly happy?", "tokens": [50816, 366, 456, 6065, 300, 291, 434, 2318, 4570, 295, 300, 1027, 291, 4908, 2055, 30, 50984], "temperature": 0.0, "avg_logprob": -0.1595221194592151, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.013204364106059074}, {"id": 973, "seek": 567096, "start": 5684.72, "end": 5692.08, "text": " So I can answer both questions. Of course, there's a huge number of choices and decisions that have", "tokens": [51052, 407, 286, 393, 1867, 1293, 1651, 13, 2720, 1164, 11, 456, 311, 257, 2603, 1230, 295, 7994, 293, 5327, 300, 362, 51420], "temperature": 0.0, "avg_logprob": -0.1595221194592151, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.013204364106059074}, {"id": 974, "seek": 567096, "start": 5692.08, "end": 5696.88, "text": " made that with the benefit of hindsight, I wouldn't have made them. And I do experience some regret,", "tokens": [51420, 1027, 300, 365, 264, 5121, 295, 44357, 11, 286, 2759, 380, 362, 1027, 552, 13, 400, 286, 360, 1752, 512, 10879, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1595221194592151, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.013204364106059074}, {"id": 975, "seek": 569688, "start": 5696.88, "end": 5701.68, "text": " but I tried to take solace in the knowledge that at the time I did the best I could.", "tokens": [50364, 457, 286, 3031, 281, 747, 1404, 617, 294, 264, 3601, 300, 412, 264, 565, 286, 630, 264, 1151, 286, 727, 13, 50604], "temperature": 0.0, "avg_logprob": -0.11424939822306675, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.001984846778213978}, {"id": 976, "seek": 569688, "start": 5702.72, "end": 5707.36, "text": " And in terms of things that I'm proud of, I'm very fortunate to have done things I'm proud of", "tokens": [50656, 400, 294, 2115, 295, 721, 300, 286, 478, 4570, 295, 11, 286, 478, 588, 14096, 281, 362, 1096, 721, 286, 478, 4570, 295, 50888], "temperature": 0.0, "avg_logprob": -0.11424939822306675, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.001984846778213978}, {"id": 977, "seek": 569688, "start": 5708.64, "end": 5713.2, "text": " and they made me happy for some time, but I don't think that that is the source of happiness.", "tokens": [50952, 293, 436, 1027, 385, 2055, 337, 512, 565, 11, 457, 286, 500, 380, 519, 300, 300, 307, 264, 4009, 295, 8324, 13, 51180], "temperature": 0.0, "avg_logprob": -0.11424939822306675, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.001984846778213978}, {"id": 978, "seek": 569688, "start": 5714.4800000000005, "end": 5719.28, "text": " So your academic accomplishments, all the papers, you're one of the most cited people in the world.", "tokens": [51244, 407, 428, 7778, 25943, 11, 439, 264, 10577, 11, 291, 434, 472, 295, 264, 881, 30134, 561, 294, 264, 1002, 13, 51484], "temperature": 0.0, "avg_logprob": -0.11424939822306675, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.001984846778213978}, {"id": 979, "seek": 569688, "start": 5719.84, "end": 5723.4400000000005, "text": " All the breakthroughs I mentioned in computer vision and language and so on", "tokens": [51512, 1057, 264, 22397, 82, 286, 2835, 294, 3820, 5201, 293, 2856, 293, 370, 322, 51692], "temperature": 0.0, "avg_logprob": -0.11424939822306675, "compression_ratio": 1.703422053231939, "no_speech_prob": 0.001984846778213978}, {"id": 980, "seek": 572344, "start": 5723.44, "end": 5729.44, "text": " is what is the source of happiness and pride for you?", "tokens": [50364, 307, 437, 307, 264, 4009, 295, 8324, 293, 10936, 337, 291, 30, 50664], "temperature": 0.0, "avg_logprob": -0.16191632768749137, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.020259007811546326}, {"id": 981, "seek": 572344, "start": 5729.44, "end": 5733.919999999999, "text": " I mean, all those things are a source of pride for sure. I'm very grateful for having done all", "tokens": [50664, 286, 914, 11, 439, 729, 721, 366, 257, 4009, 295, 10936, 337, 988, 13, 286, 478, 588, 7941, 337, 1419, 1096, 439, 50888], "temperature": 0.0, "avg_logprob": -0.16191632768749137, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.020259007811546326}, {"id": 982, "seek": 572344, "start": 5733.919999999999, "end": 5740.16, "text": " those things. And it was very fun to do them. But happiness comes, you know, you can, happiness,", "tokens": [50888, 729, 721, 13, 400, 309, 390, 588, 1019, 281, 360, 552, 13, 583, 8324, 1487, 11, 291, 458, 11, 291, 393, 11, 8324, 11, 51200], "temperature": 0.0, "avg_logprob": -0.16191632768749137, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.020259007811546326}, {"id": 983, "seek": 572344, "start": 5740.16, "end": 5746.16, "text": " well, my current view is that happiness comes from our to a very large degree from the way we", "tokens": [51200, 731, 11, 452, 2190, 1910, 307, 300, 8324, 1487, 490, 527, 281, 257, 588, 2416, 4314, 490, 264, 636, 321, 51500], "temperature": 0.0, "avg_logprob": -0.16191632768749137, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.020259007811546326}, {"id": 984, "seek": 572344, "start": 5746.16, "end": 5751.599999999999, "text": " look at things. You know, you can have a simple meal and be quite happy as a result or you can", "tokens": [51500, 574, 412, 721, 13, 509, 458, 11, 291, 393, 362, 257, 2199, 6791, 293, 312, 1596, 2055, 382, 257, 1874, 420, 291, 393, 51772], "temperature": 0.0, "avg_logprob": -0.16191632768749137, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.020259007811546326}, {"id": 985, "seek": 575160, "start": 5751.6, "end": 5757.360000000001, "text": " talk to someone and be happy as a result as well. Or conversely, you can have a meal and be", "tokens": [50364, 751, 281, 1580, 293, 312, 2055, 382, 257, 1874, 382, 731, 13, 1610, 2615, 736, 11, 291, 393, 362, 257, 6791, 293, 312, 50652], "temperature": 0.0, "avg_logprob": -0.08782473754882812, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002279942389577627}, {"id": 986, "seek": 575160, "start": 5757.360000000001, "end": 5762.240000000001, "text": " disappointed that the meal wasn't a better meal. So I think a lot of happiness comes from that.", "tokens": [50652, 13856, 300, 264, 6791, 2067, 380, 257, 1101, 6791, 13, 407, 286, 519, 257, 688, 295, 8324, 1487, 490, 300, 13, 50896], "temperature": 0.0, "avg_logprob": -0.08782473754882812, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002279942389577627}, {"id": 987, "seek": 575160, "start": 5762.240000000001, "end": 5768.0, "text": " But I'm not sure. I don't want to be too confident. Being humble in the face of the uncertainty seems", "tokens": [50896, 583, 286, 478, 406, 988, 13, 286, 500, 380, 528, 281, 312, 886, 6679, 13, 8891, 16735, 294, 264, 1851, 295, 264, 15697, 2544, 51184], "temperature": 0.0, "avg_logprob": -0.08782473754882812, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002279942389577627}, {"id": 988, "seek": 575160, "start": 5768.0, "end": 5773.84, "text": " to be also a part of this whole happiness thing. Well, I don't think there's a better way to end", "tokens": [51184, 281, 312, 611, 257, 644, 295, 341, 1379, 8324, 551, 13, 1042, 11, 286, 500, 380, 519, 456, 311, 257, 1101, 636, 281, 917, 51476], "temperature": 0.0, "avg_logprob": -0.08782473754882812, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002279942389577627}, {"id": 989, "seek": 575160, "start": 5773.84, "end": 5779.84, "text": " it than meaning of life and discussions of happiness. So Ilya, thank you so much. You've", "tokens": [51476, 309, 813, 3620, 295, 993, 293, 11088, 295, 8324, 13, 407, 286, 45106, 11, 1309, 291, 370, 709, 13, 509, 600, 51776], "temperature": 0.0, "avg_logprob": -0.08782473754882812, "compression_ratio": 1.690391459074733, "no_speech_prob": 0.002279942389577627}, {"id": 990, "seek": 577984, "start": 5779.84, "end": 5785.6, "text": " given me a few incredible ideas. You've given the world many incredible ideas. I really appreciate it.", "tokens": [50364, 2212, 385, 257, 1326, 4651, 3487, 13, 509, 600, 2212, 264, 1002, 867, 4651, 3487, 13, 286, 534, 4449, 309, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14339417102290133, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.125395730137825}, {"id": 991, "seek": 577984, "start": 5785.6, "end": 5789.360000000001, "text": " And thanks for talking today. Yeah, thanks for stopping by. I really enjoyed it.", "tokens": [50652, 400, 3231, 337, 1417, 965, 13, 865, 11, 3231, 337, 12767, 538, 13, 286, 534, 4626, 309, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14339417102290133, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.125395730137825}, {"id": 992, "seek": 577984, "start": 5790.400000000001, "end": 5794.08, "text": " Thanks for listening to this conversation with Ilya Setskever. And thank you to our", "tokens": [50892, 2561, 337, 4764, 281, 341, 3761, 365, 286, 45106, 318, 1385, 330, 331, 13, 400, 1309, 291, 281, 527, 51076], "temperature": 0.0, "avg_logprob": -0.14339417102290133, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.125395730137825}, {"id": 993, "seek": 577984, "start": 5794.08, "end": 5799.4400000000005, "text": " presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App", "tokens": [51076, 15578, 16198, 11, 27016, 3132, 13, 2555, 1949, 7231, 264, 7367, 538, 32529, 27016, 3132, 51344], "temperature": 0.0, "avg_logprob": -0.14339417102290133, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.125395730137825}, {"id": 994, "seek": 577984, "start": 5799.4400000000005, "end": 5805.28, "text": " and using the code Lex podcast. If you enjoy this podcast, subscribe on YouTube,", "tokens": [51344, 293, 1228, 264, 3089, 24086, 7367, 13, 759, 291, 2103, 341, 7367, 11, 3022, 322, 3088, 11, 51636], "temperature": 0.0, "avg_logprob": -0.14339417102290133, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.125395730137825}, {"id": 995, "seek": 580528, "start": 5805.28, "end": 5810.4, "text": " review it with five stars and Apple podcast, support on Patreon or simply connect with me", "tokens": [50364, 3131, 309, 365, 1732, 6105, 293, 6373, 7367, 11, 1406, 322, 15692, 420, 2935, 1745, 365, 385, 50620], "temperature": 0.0, "avg_logprob": -0.1375004231244668, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.02515406161546707}, {"id": 996, "seek": 580528, "start": 5810.4, "end": 5818.24, "text": " on Twitter at Lex Friedman. And now let me leave you with some words from Alan Turing on machine", "tokens": [50620, 322, 5794, 412, 24086, 17605, 1601, 13, 400, 586, 718, 385, 1856, 291, 365, 512, 2283, 490, 16442, 314, 1345, 322, 3479, 51012], "temperature": 0.0, "avg_logprob": -0.1375004231244668, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.02515406161546707}, {"id": 997, "seek": 580528, "start": 5818.24, "end": 5824.96, "text": " learning. Instead of trying to produce a program to simulate the adult mind, why not rather try to", "tokens": [51012, 2539, 13, 7156, 295, 1382, 281, 5258, 257, 1461, 281, 27817, 264, 5075, 1575, 11, 983, 406, 2831, 853, 281, 51348], "temperature": 0.0, "avg_logprob": -0.1375004231244668, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.02515406161546707}, {"id": 998, "seek": 580528, "start": 5824.96, "end": 5831.28, "text": " produce one which simulates the child? If this were then subjected to an appropriate course of", "tokens": [51348, 5258, 472, 597, 1034, 26192, 264, 1440, 30, 759, 341, 645, 550, 32153, 281, 364, 6854, 1164, 295, 51664], "temperature": 0.0, "avg_logprob": -0.1375004231244668, "compression_ratio": 1.5637860082304527, "no_speech_prob": 0.02515406161546707}, {"id": 999, "seek": 583128, "start": 5831.28, "end": 5847.2, "text": " education, one would obtain the adult brain. Thank you for listening and hope to see you next time.", "tokens": [50364, 3309, 11, 472, 576, 12701, 264, 5075, 3567, 13, 1044, 291, 337, 4764, 293, 1454, 281, 536, 291, 958, 565, 13, 51160], "temperature": 0.0, "avg_logprob": -0.19895833730697632, "compression_ratio": 1.1511627906976745, "no_speech_prob": 0.022606728598475456}], "language": "en"}