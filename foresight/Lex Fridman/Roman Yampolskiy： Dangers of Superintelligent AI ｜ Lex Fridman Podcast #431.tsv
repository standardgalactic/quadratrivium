start	end	text
0	6560	If we create general superintelligences, I don't see a good outcome long-term for humanity.
6560	11520	So there is X-risk, existential risk, everyone's dead. There is S-risk,
11520	17040	suffering risks, where everyone wishes they were dead. We have also idea for I-risk,
17040	22560	Ikigai risks, where we lost our meaning. The systems can be more creative,
22560	27360	they can do all the jobs. It's not obvious what you have to contribute to a world where
27920	33520	superintelligence exists. Of course, you can have all the variants you mentioned, where we are
33520	37920	safe, we are kept alive, but we are not in control, we are not deciding anything,
37920	44960	we are like animals in a zoo. There is, again, possibilities we can come up with as very smart
44960	51280	humans, and then possibilities something a thousand times smarter can come up with for reasons we
51280	59520	cannot comprehend. The following is a conversation with Roman Yumpolski, an AI safety and security
59520	67040	researcher and author of a new book titled AI, Unexplainable, Unpredictable, Uncontrollable.
67840	73600	He argues that there's almost 100% chance that AGI will eventually destroy human civilization.
74240	80560	As an aside, let me say that I will have many, often technical conversations on the topic of AI,
81520	87440	often with engineers building the state-of-the-art AI systems. I would say those folks put the
87440	95280	infamous P-DOOM or the probability of AGI killing all humans at around 1-20%, but it's also important
95280	104240	to talk to folks who put that value at 70, 80, 90, and in the case of Roman at 99.99 and many more
104240	111920	90%. I'm personally excited for the future and believe it will be a good one, in part because
111920	119280	of the amazing technological innovation we humans create, but we must absolutely not do so with
119280	125840	blinders on, ignoring the possible risks, including existential risks of those technologies.
126880	132960	That's what this conversation is about. This is the Lex Friedman podcast. To support it,
132960	139120	please check out our sponsors in the description. Now, dear friends, here's Roman Jampalski.
140240	146160	What to you is the probability that superintelligent AI will destroy all human civilization?
146160	147440	What's the time frame?
147440	149840	Let's say 100 years. In the next 100 years.
149840	156000	So the problem of controlling AGI or superintelligence, in my opinion, is like
157040	162640	a problem of creating a perpetual safety machine, by analogy with perpetual motion machine.
163120	171040	It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep
172160	179040	improving, learning, eventually self-modifying, interacting with the environment, interacting
179040	187360	with malevolent actors. The difference between cybersecurity, narrow AI safety, and safety for
188080	191360	general AI for superintelligence is that we don't get a second chance.
192080	195440	With cybersecurity, somebody hacks your account. What's the big deal? You get a new
195440	201600	password, new credit card, you move on. Here, if we're talking about existential risks,
201600	206960	you only get one chance. So you're really asking me, what are the chances that we'll create
206960	214080	the most complex software ever on the first try with zero bugs, and it will continue have zero
214080	221440	bugs for 100 years or more? So there is an incremental improvement
222400	228720	of systems leading up to AGI. To you, it doesn't matter if we can keep those safe.
228720	235440	There's going to be one level of system at which you cannot possibly control it.
236000	244160	I don't think we so far have made any system safe. At the level of capability they display,
244800	251760	they already have made mistakes. We had accidents, they've been jailbroken. I don't think there is a
251760	259040	single large language model today which no one was successful at making do something developers
259040	264640	didn't intend it to do. But there's a difference between getting it to do something unintended,
264640	269200	getting it to do something that's painful, costly, destructive, and something that's
269200	274480	destructive to the level of hurting billions of people, or hundreds of millions of people,
274480	278240	billions of people, or the entirety of human civilization. That's a big leap.
279200	284880	Exactly. But the systems we have today have capability of causing X amount of damage.
284880	289040	So when they fail, that's all we get. If we develop systems capable of
289760	295040	impacting all of humanity, all of universe, the damage is proportionate.
295920	302880	What do you, are the possible ways that such kind of mass murder of humans can happen?
303600	307920	It's always a wonderful question. So one of the chapters in my new book is about
307920	312880	unpredictability. I argue that we cannot predict what a smarter system will do.
312880	317920	So you're really not asking me how superintelligence will kill everyone, you're asking me how I would
317920	322480	do it. And I think it's not that interesting. I can tell you about the standard, you know,
322480	327920	nanotech, synthetic bio nuclear. Superintelligence will come up with something completely new,
327920	335440	completely super. We may not even recognize that as a possible path to achieve that goal.
336000	341600	So there is like an unlimited level of creativity in terms of how humans could be killed.
342240	349600	But, you know, we could still investigate possible ways of doing it, not how to do it, but the,
349600	354400	at the end, what is the methodology that does it, you know, shutting off the power.
355200	359680	And then humans start killing each other, maybe because the resources are really constrained,
359680	363200	that they're, and then there's the actual use of weapons like nuclear weapons or
364160	370400	developing artificial pathogens, viruses, that kind of stuff. We could still kind of think
371280	376800	through that and defend against it, right? There's a ceiling to the creativity of mass
376800	379840	murder of humans here, right? The options are limited.
380880	386080	They are limited by how imaginative we are. If you are that much smarter, that much more creative,
386080	391360	you are capable of thinking across multiple domains, do novel research in physics and biology,
391360	395840	you may not be limited by those tools. If squirrels were planning to kill humans,
395920	401280	they would have a set of possible ways of doing it, but they would never consider things we can
401280	405440	come up with. So are you thinking about mass murder and destruction of human civilization?
405440	410160	Are you thinking of with squirrels, you put them in a zoo and they don't really know they're in a zoo?
410720	413200	If we just look at the entire set of undesirable trajectories,
414160	419360	majority of them are not going to be death. Most of them are going to be just like
420320	428880	things like Brave New World where the squirrels are fed dopamine and they're all doing some kind
428880	436400	of fun activity and the fire, the soul of humanity is lost because of the drug that's fed to it,
436400	442160	or literally in a zoo. We're in a zoo, we're doing our thing, we're playing a game of Sims
443120	450000	and the actual players playing that game are AI systems. Those are all undesirable because of the
450000	456560	free will. The fire of human consciousness is dimmed through that process, but it's not killing
456560	465040	humans. So are you thinking about that or is the biggest concern, literally, the extinctions of humans?
465040	470640	I think about a lot of things. So there is X-risk, existential risk, everyone's dead.
470720	476720	There is S-risk, suffering risks, where everyone wishes they were dead. We have also idea for
476720	483600	I-risk, Ikigai risks, where we lost our meaning. The systems can be more creative, they can do all
483600	489920	the jobs. It's not obvious what you have to contribute to a world where superintelligence exists.
489920	495360	Of course, you can have all the variants you mentioned where we are safe, we are kept alive,
495360	499760	but we are not in control. We are not deciding anything. We are like animals in a zoo.
501120	508000	Again, possibilities we can come up with as very smart humans and then possibilities something
508800	513120	a thousand times smarter can come up with. Four reasons we cannot comprehend.
513120	516960	I would love to sort of dig into each of those. X-risk, S-risk, and I-risk.
519040	521520	Can you like linger on I-risk? What is that?
522560	529040	Japanese concept of Ikigai, you find something which allows you to make money. You are good at
529360	533680	it and the society says, we need it. You have this awesome job, you are a
533680	539760	podcaster, gives you a lot of meaning, you have a good life, I assume, you're happy.
541360	547120	That's what we want most people to find, to have. For many intellectuals, it is their
547120	552640	occupation which gives them a lot of meaning. I am a researcher, philosopher, scholar,
553360	559840	that means something to me. In a world where an artist is not feeling appreciated because his
559840	568560	art is just not competitive with what is produced by machines or a writer or scientist, we'll lose
568560	574240	a lot of that. At the lower level, we're talking about complete technological unemployment.
574800	580160	We're not losing 10% of jobs, we're losing all jobs. What do people do with all that free time?
580240	587680	What happens then? Everything society is built on is completely modified in one generation.
587680	594160	It's not a slow process where we get to figure out how to live that new lifestyle, but it's
594880	600400	pretty quick. In that world, humans can't do what humans currently do with chess,
600400	607840	play each other, have tournaments. Even though AI systems are far superior this time in chess,
607920	614080	we just create artificial games. For us, they're real, like the Olympics. We do all kinds of
614080	624480	different competitions and have fun, maximize the fun and let the AI focus on the productivity.
624480	629440	It's an option. I have a paper where I try to solve the value alignment problem for multiple
629440	635360	agents. The solution to avoid compromises is to give everyone a personal virtual universe.
635360	639360	You can do whatever you want in that world. You could be king, you could be slave,
639360	644480	you decide what happens. It's basically a glorified video game where you get to enjoy
644480	650720	yourself and someone else takes care of your needs and the substrate alignment is the only
650720	655120	thing we need to solve. We don't have to get 8 billion humans to agree on anything.
657200	662640	Why is that not a likely outcome? Why can't AI systems create video games for us
663600	667840	to lose ourselves in each with an individual video game universe?
668400	671040	Some people say that's what happened in a simulation.
671840	678160	And we're playing that video game and now we're creating what? Maybe we're creating
678160	682560	artificial threats for ourselves to be scared about because fear is really exciting. It allows
682560	688480	us to play the video game more vigorously. Some people choose to play on a more difficult level
688560	693520	with more constraints. Some say, okay, I'm just going to enjoy the game, high privilege level.
694160	698560	Absolutely. So, okay, what was that paper on multi-agent value alignment?
698560	705920	Personal universes. Personal universes. So, that's one of the possible outcomes. But what in general
705920	710960	is the idea of the paper? So, it's looking at multiple agents. They're human AI, like a hybrid
710960	715760	system where there's humans and AI's? Or is it looking at humans or just intelligent agents?
715760	720800	In order to solve value alignment problem, I'm trying to formalize it a little better.
720800	726160	Usually, we're talking about getting AI's to do what we want, which is not well-defined.
726160	732560	Are we talking about creator of a system, owner of that AI, humanity as a whole? But we don't
732560	738720	agree on much. There is no universally accepted ethics, morals across cultures, religions.
739280	742640	People have individually very different preferences politically and such. So,
743280	749120	even if we somehow managed all the other aspects of it, programming those fuzzy concepts and
749120	755040	getting AI to follow them closely, we don't agree on what to program in. So, my solution was,
755040	758640	okay, we don't have to compromise on room temperature. You have your universe, I have mine,
759280	764000	whatever you want. And if you like me, you can invite me to visit your universe. We don't have
764000	769200	to be independent. But the point is you can be. And virtual reality is getting pretty good. It's
769200	773280	going to hit a point where you can't tell the difference. And if you can't tell if it's real
773280	779440	or not, what's the difference? So, basically, give up on value alignment. Create an entire,
779440	784640	it's like the multiverse theory. This is creating an entire universe for you with your values.
784640	789360	You still have to align with that individual. They have to be happy in that simulation.
789360	794640	But it's a much easier problem to align with one agent versus eight billion agents plus animals,
794640	799680	aliens. So, you convert the multi-agent problem into a single-agent problem?
799680	807040	I'm trying to do that, yeah. Okay. Is there any way to, so, okay, that's giving up on them,
807920	813680	on the value alignment problem. Well, is there any way to solve the value alignment problem
813680	819200	where there's a bunch of humans, multiple humans, tens of humans or eight billion humans that have
819200	826240	very different set of values? It seems contradictory. I haven't seen anyone explain what it means
826240	834640	outside of words which pack a lot, make it good, make it desirable, make it something they don't
834640	839680	regret. But how do you specifically formalize those notions? How do you program them in?
839680	845440	I haven't seen anyone make progress on that so far. But isn't that the whole optimization
845440	849520	journey that we're doing as a human civilization? We're looking at geopolitics.
850640	856160	Nations are in a state of anarchy with each other. They start wars, there's conflict,
858240	863440	and oftentimes they have very different views of what is good and what is evil.
863440	868160	There's not what we're trying to figure out, just together trying to converge towards that.
868160	871680	So, we're essentially trying to solve the value alignment problem with humans.
872320	876640	The examples you gave, some of them are, for example, two different religions saying,
876640	883840	this is our holy site and we are not willing to compromise it in any way. If you can make two
883840	887440	holy sites in virtual worlds, you solve the problem. But if you only have one, it's not
887440	892240	divisible, you're stuck there. But what if we want to be at tension with each other?
893520	899680	Through that tension, we understand ourselves and we understand the world. That's the intellectual
899680	906880	journey we're on as a human civilization. We create intellectual and physical conflict,
906880	912000	and through that, figure stuff out. If we go back to that idea of simulation and this is an
912000	917200	entertainment kind of giving meaning to us, the question is, how much suffering is reasonable
917200	922560	for a video game? So, yeah, I don't mind a video game where I get haptic feedback,
922560	928000	that is a little bit of shaking, maybe I'm a little scared. I don't want a game where kids
928000	933920	are tortured, literally. That seems unethical, at least by our human standards.
934720	938800	Are you suggesting it's possible to remove suffering if we're looking at human civilization
938800	945120	as an optimization problem? So, we know there are some humans who, because of a mutation,
945120	952640	don't experience physical pain. So, at least physical pain can be mutated out, re-engineered
952640	958160	out. Suffering, in terms of meaning, like you burned the only copy of my book, is a little
958160	964080	harder, but even there, you can manipulate your hedonic set point, you can change defaults,
964080	969200	you can reset. Problem with that is, if you start messing with your reward channel,
969200	975680	you start wire-heading and end up bleasing out a little too much.
975680	981280	Well, that's the question. Would you really want to live in a world where there's no suffering
981280	988880	as a dark question? Is there some level of suffering that reminds us of what this is all for?
989600	994800	I think we need that, but I would change the overall range. So, right now, it's negative
994800	999680	infinity to kind of positive infinity, pain, pleasure, access. I would make it like zero to
999680	1003120	positive infinity, and being unhappy is like, I'm close to zero.
1004160	1009280	Okay, so what's the S-risk? What are the possible things that you're imagining with S-risk? So,
1010160	1014960	mass suffering of humans, what are we talking about there caused by AGI?
1014960	1021440	So, there are many malevolent actors. We can talk about psychopaths, crazies, hackers,
1021440	1027360	doomsday cults. We know from history, they tried killing everyone. They tried on purpose to cause
1027360	1034080	maximum amount of damage, terrorism. What if someone malevolent wants on purpose to torture all humans
1034080	1041680	as long as possible? You solve aging, so now you have functional immortality, and you just
1042400	1047200	try to be as creative as you can. Do you think there is actually people in human history that
1047200	1053040	tried to literally maximize human suffering? It's just studying people who have done evil in the
1053040	1058480	world. It seems that they think that they're doing good, and it doesn't seem like they're trying to
1058480	1066080	maximize suffering. They just cause a lot of suffering as a side effect of doing what they
1066080	1072800	think is good. So, there are different malevolent agents. Some may be just gaining personal benefit
1072800	1078880	and sacrificing others to that cause. Others, we know for a fact that trying to kill as many people
1078880	1084000	as possible. When we look at recent school shootings, if they had more capable weapons,
1084000	1088000	they would take out not dozens, but thousands, millions, billions.
1093840	1100640	Well, we don't know that, but that is a terrifying possibility, and we don't want to find out.
1101920	1108400	Like if terrorists had access to nuclear weapons, how far would they go? Is there a limit to what
1108400	1116560	they're willing to do? In your sense, is there some malevolent actors where there's no limit?
1116560	1127600	There is mental diseases where people don't have empathy, don't have this human quality of
1127600	1132160	understanding suffering in ours. And then there's also a set of beliefs where you think you're doing
1132160	1139920	good by killing a lot of humans. Again, I would like to assume that normal people never think
1139920	1146560	like that. It's always some sort of psychopaths, but yeah. And to you, AGI systems can carry that
1147520	1154240	and be more competent at executing that. They can certainly be more creative. They can understand
1154320	1162000	human biology better, understand our molecular structure, genome. Again, a lot of times
1162880	1168480	torture ends, then individual dies. That limit can be removed as well.
1168480	1173440	So if we're actually looking at X-risk and S-risk as the systems get more and more intelligent,
1173440	1179680	don't you think it's possible to anticipate the ways they can do it and defend against it?
1179680	1182480	Like we do with cybersecurity, we do security systems?
1183200	1189440	Right. We can definitely keep up for a while. I'm saying you cannot do it indefinitely. At some
1189440	1198320	point, the cognitive gap is too big. The surface you have to defend is infinite, but attackers
1198320	1204480	only need to find one exploit. So to you, eventually, this is heading off a cliff?
1205200	1211520	If we create general super intelligences, I don't see a good outcome long-term for humanity.
1211520	1214000	The only way to win this game is not to play it.
1214000	1218320	Okay. Well, we'll talk about possible solutions and what not playing it means.
1219760	1222960	But what are the possible timelines here to you? What are we talking about?
1223520	1227520	We're talking about a set of years, decades, centuries. What do you think?
1227520	1232640	I don't know for sure. The prediction markets right now are saying 2026 for AGI.
1233360	1238480	I heard the same thing from CEO of Anthropic, DeepMind, so maybe we're two years away,
1239040	1245440	which seems very soon given we don't have a working safety mechanism in place or even a
1245440	1250000	prototype for one. And there are people trying to accelerate those timelines because they feel
1250000	1254560	we're not getting there quick enough. Well, what do you think they mean when they say AGI?
1255120	1260080	So the definitions we used to have, and people are modifying them a little bit lately,
1260160	1265760	artificial general intelligence was a system capable of performing in any domain a human
1265760	1272080	could perform. So you're creating this average artificial person. They can do cognitive labor,
1272080	1277440	physical labor, where you can get another human to do it. Superintelligence was defined as a system
1277440	1283600	which is superior to all humans in all domains. Now people are starting to refer to AGI as if
1283600	1289280	it's superintelligence. I made a post recently where I argued, for me at least, if you average
1289280	1294960	out over all the common human tasks, those systems are already smarter than an average human.
1295840	1301120	So under that definition, we have it. Shane Lake has this definition of where
1301120	1306240	you're trying to win in all domains. That's what intelligence is. Now are they smarter than
1306800	1311120	elite individuals in certain domains? Of course not. They're not there yet. But
1312160	1316800	the progress is exponential. See, I'm much more concerned about social engineering.
1319680	1326480	To me, AI's ability to do something in the physical world, like the lowest hanging fruit,
1327120	1334960	the easiest set of methods is by just getting humans to do it. It's going to be much harder to
1337040	1339840	be the kind of viruses that take over the minds of robots,
1340800	1344640	that where the robots are executing the commands. It just seems like humans, social
1344640	1349440	engineering of humans is much more likely. That would be enough to bootstrap the whole process.
1350960	1357200	Okay. Just to linger on the term AGI, what do you use the difference in AGI and human level
1357200	1364480	intelligence? Human level is general in the domain of expertise of humans. We know how to do human
1364480	1369760	things. I don't speak dog language. I should be able to pick it up if I'm a general intelligence.
1369840	1375280	It's inferior animal. I should be able to learn that skill, but I can't. At general
1375280	1380080	intelligence, truly universal general intelligence should be able to do things like that humans cannot
1380080	1385360	do. To be able to talk to animals, for example. To solve pattern recognition problems of that type,
1387360	1394800	to have similar things outside of our domain of expertise because it's just not the world we live in.
1395760	1402240	If we just look at the space of cognitive abilities we have, I just would love to understand
1402240	1408560	what the limits are beyond which an AGI system can reach. What does that look like? What about
1409280	1416640	actual mathematical thinking or scientific innovation? That kind of stuff.
1417520	1422480	We know calculators are smarter than humans in that narrow domain of addition.
1423280	1432160	But is it humans plus tools versus AGI or just human, raw human intelligence? Because humans
1432160	1436560	create tools and with the tools they become more intelligent. There's a gray area there,
1436560	1439840	what it means to be human when we're measuring their intelligence.
1439840	1444880	So when I think about it, I usually think human with a paper and a pencil, not human with internet
1444880	1450400	and other AI helping. But is that a fair way to think about it? Because isn't there another
1450400	1454160	definition of human-level intelligence that includes the tools that humans create?
1454160	1459280	But we create AI. So at any point, you'll still just add superintelligence to human capability?
1459280	1466720	That seems like cheating. No, controllable tools. There is an implied leap that you're making
1468080	1476320	when AGI goes from tool to entity that can make its own decisions. So if we define human-level
1476320	1480320	intelligence as everything a human can do with fully controllable tools.
1480960	1485200	It seems like a hybrid of some kind. You're now doing brain-computer interfaces. You're
1485200	1490640	connecting it to maybe narrow AIs yet definitely increases our capabilities.
1491520	1500080	So what's a good test to you that measures whether an artificial intelligence system has
1500080	1506160	reached human-level intelligence and was a good test where it has superseded human-level
1506160	1512800	intelligence to reach that land of AGI? I am old fashioned. I like Turing test. I have a paper
1512800	1518560	where I equate passing Turing test to solving AI complete problems because you can encode any
1518560	1523360	questions about any domain into the Turing test. You don't have to talk about how was your day.
1523360	1530640	You can ask anything and so the system has to be as smart as a human to pass it in a true sense.
1530640	1536880	But then you would extend that to maybe a very long conversation. I think the Alexa prize was
1536880	1542480	doing that. Basically, can you do a 20-minute, 30-minute conversation with an AI system?
1542480	1549040	It has to be long enough to where you can make some meaningful decisions about capabilities
1549040	1554320	absolutely. You can brute force very short conversations. So literally, what does that
1554320	1566080	look like? Can we construct formally a kind of test that tests for AGI? For AGI, it has to be
1566080	1572000	there. I cannot give it a task. I can give to a human and it cannot do it if a human can.
1573040	1578160	For superintelligence, it would be superior on all such tasks. Not just average performance,
1578800	1582720	go learn to drive car, go speak Chinese, play guitar. Okay, great.
1582720	1589920	I guess the the follow-on question is there a test for the kind of AGI that would be
1591600	1599040	susceptible to lead to S-risk or X-risk, susceptible to destroy human civilization?
1599040	1605280	Like is there a test for that? You can develop a test which will give you positives if it lies to
1605280	1610160	you or has those ideas. You cannot develop a test which rules them out. There is always
1610160	1616400	possibility of what Bostrom calls a treacherous turn where later on a system decides for game
1616400	1622880	theoretic reasons, economic reasons to change its behavior. And we see the same with humans.
1622880	1629840	It's not unique to AI. For millennia, we tried developing models, ethics, religions, lie detector
1629840	1637440	tests and then employees betray the employers, spouses betray family. It's a pretty standard thing
1637440	1644720	intelligent agents sometimes do. So is it possible to detect when AI system is lying or deceiving you?
1644720	1650720	If you know the truth and it tells you something false, you can detect that but you cannot know
1650720	1657680	in general every single time. And again, the system you're testing today may not be lying.
1657680	1664560	The system you're testing today may know you are testing it and so behaving and later on after it
1665200	1670640	interacts with the environment, interacts with other systems, malevolent agents, learns more,
1670640	1676400	it may start doing those things. So do you think it's possible to develop a system where the creators
1676400	1682240	of the system, the developers, the programmers don't know that it's deceiving them?
1683120	1689200	So systems today don't have long-term planning. That is not our... They can lie today if it
1689200	1696720	optimizes, helps them optimize the reward. If they realize, okay, this human will be very happy if I
1696720	1703920	tell them the following, they will do it if it brings them more points. And they don't have to
1704720	1709520	kind of keep track of it. It's just the right answer to this problem every single time.
1710400	1716080	At which point is somebody creating that intentionally, not unintentionally, intentionally
1716080	1720640	creating an AI system that's doing long-term planning with an objective function that's
1720640	1726800	defined by the AI system, not by a human? Well, some people think that if they're that smart,
1726800	1731760	they're always good. They really do believe that. It's just benevolence from intelligence,
1731760	1739040	so they'll always want what's best for us. Some people think that they will be able to detect
1739520	1745840	problem behaviors and correct them at the time when we get there. I don't think it's a good idea.
1745840	1752480	I am strongly against it, but yeah, there are quite a few people who in general are so optimistic
1752480	1757600	about this technology. It could do no wrong. They want it developed as soon as possible,
1757600	1764400	as capable as possible. So there's going to be people who believe the more intelligent it is,
1764400	1768160	the more benevolent, and so therefore it should be the one that defines the objective function,
1768160	1771200	that it's optimizing when it's doing long-term planning.
1771200	1776800	There are even people who say, okay, what's so special about humans, right? We removed the
1776800	1782560	gender bias. We're removing race bias. Why is this pro-human bias? We are polluting the planet.
1783200	1788000	As you said, you know, fight a lot of wars, kind of violent. Maybe it's better if this super
1788000	1795440	intelligent, perfect society comes and replaces us. It's normal stage in the evolution of our
1795440	1802560	species. Yeah, so somebody says, let's develop an AI system that removes the violent humans
1803360	1807360	from the world. And then it turns out that all humans have violence in them,
1807360	1813200	or the capacity for violence, and therefore all humans are removed. Yeah, yeah, yeah.
1814560	1820960	Let me ask about Yan Likun. He's somebody who you've had a few exchanges with,
1821920	1827680	and he's somebody who actively pushes back against this view that AI is going to lead
1827680	1840480	to destruction of human civilization, also known as AI-dumerism. So in one example that he tweeted,
1840480	1847120	he said, I do acknowledge risks, but two points. One, open research and open source of the best
1847120	1853840	ways to understand and mitigate the risks. And two, AI is not something that just happens. We
1853840	1860720	build it. We have agency in what it becomes. Hence, we control the risks. We meaning humans.
1860720	1867200	It's not some sort of natural phenomena that we have no control over. So can you make the case
1867200	1871840	that he's right, and can you try to make the case that he's wrong? I cannot make a case that he's
1871840	1877600	right. He's wrong in so many ways. It's difficult for me to remember all of them. He's a Facebook
1877600	1883360	buddy, so I have a lot of fun having those little debates with him. So I'm trying to remember the
1883360	1891120	arguments. So one, he says we are not gifted this intelligence from aliens. We are designing it. We
1891120	1898320	are making decisions about it. That's not true. It was true when we had expert systems, symbolic AI,
1898320	1904560	decision trees. Today, you set up parameters for a model and you water this plant. You give it data,
1904560	1910320	you give it compute, and it grows. And after it's finished growing into this alien plant,
1910320	1915920	you start testing it to find out what capabilities it has. And it takes years to figure out even
1915920	1920800	for existing models. If it's trained for six months, it will take you two, three years to figure out
1920800	1926800	basic capabilities of that system. We still discover new capabilities in systems which are already out
1926800	1933120	there. So that's not the case. So just to link on that, to give you the difference there, there is
1933120	1940240	some level of emergent intelligence that happens in our current approaches. So stuff that we don't
1940240	1946880	hard code in. Absolutely. That's what makes it so successful. Then we had to painstakingly hard
1946880	1953360	code in everything. We didn't have much progress. Now, just spend more money and more compute and
1953360	1958560	it's a lot more capable. And then the question is, when there is emergent intelligent phenomena,
1959600	1965760	what is the ceiling of that? For you, there's no ceiling. For Yanlacun, I think there's a kind
1965760	1971360	of ceiling that happens that we have full control over. Even if we don't understand the internals
1971360	1977520	of the emergence, how the emergence happens, there's a sense that we have control and an
1977520	1983360	understanding of the approximate ceiling of capability, the limits of the capability.
1984000	1990640	Let's say there is a ceiling. It's not guaranteed to be at the level which is competitive with us.
1990640	1998960	It may be greatly superior to ours. So what about his statement about open research and open source
1998960	2003440	are the best ways to understand and mitigate the risks? Historically, he's completely right.
2003440	2008240	Open source software is wonderful. It's tested by the community. It's debugged,
2008240	2014560	but we're switching from tools to agents. Now you're giving open source weapons to psychopaths.
2014560	2021680	Do we want open source nuclear weapons, biological weapons? It's not safe to give technology so
2021680	2028240	powerful to those who may misalign it, even if you are successful at somehow getting it to work
2028240	2032720	in the first place in a friendly manner. But the difference with nuclear weapons,
2032720	2038720	current AI systems are not akin to nuclear weapons. So the idea there is you're open sourcing it at
2038720	2043920	this stage that you can understand it better. A large number of people can explore the limitation
2043920	2050080	of capabilities, explore the possible ways to keep it safe, to keep it secure, all that kind of stuff
2050080	2055280	while it's not at the stage of nuclear weapons. In nuclear weapons, there's a no-nuclear weapon
2055280	2060320	and then there's a nuclear weapon. With AI systems, there's a gradual improvement of capability
2060320	2066960	and you get to perform that improvement incrementally. So open source allows you to study
2068800	2075040	how things go wrong, study the very process of emergence, study AI safety and those systems
2075040	2080160	when there's not a high level of danger, all that kind of stuff. It also sets a very wrong
2080160	2086080	precedent. So we open sourced model 1, model 2, model 3, nothing ever bad happened, so obviously
2086080	2091440	we're going to do it with model 4. It's just gradual improvement. I don't think it always works
2091440	2098640	with the precedent. You're not stuck doing it the way you always did. It's just a precedent
2099200	2105200	of open research and open development such that we get to learn together and then the first time
2105200	2111840	there's a sign of danger, some dramatic thing happened, not a thing that destroys human civilization,
2111840	2119200	but some dramatic demonstration of capability that can legitimately lead to a lot of damage,
2119200	2123360	then everybody wakes up and says, okay, we need to regulate this, we need to come up with safety
2123360	2129520	mechanism that stops this. But at this time, maybe you can educate me, but I haven't seen any
2129520	2135360	illustration of significant damage done by intelligent AI systems. So I have a paper
2135920	2140960	which collects accidents through history of AI and they always are proportionate to capabilities
2140960	2147200	of that system. So if you have tic-tac-toe playing AI, it will fail to properly play and lose the
2147200	2153600	game, which it should draw, trivial. Your spell checker will misspell a word, so on. I stopped
2153600	2157840	collecting those because there are just too many examples of the eyes failing at what they are
2157840	2165120	capable of. We haven't had terrible accidents in the sense of billion people get killed, absolutely
2165120	2172400	true. But in another paper, I argue that those accidents do not actually prevent people from
2172400	2180080	continuing with research and actually they kind of serve like vaccines. A vaccine makes your body
2180080	2185360	a little bit sick, so you can handle the big disease later, much better. It's the same here.
2185360	2189200	People will point out, you know that accident, AI accident we had where 12 people died?
2190160	2196160	Everyone's still here, 12 people is less than smoking kills. It's not a big deal, so we continue.
2196160	2202160	So in a way, it will actually be kind of confirming that it's not that bad.
2202160	2208480	It matters how the deaths happen. Whether it's literally murdered by an AI system,
2208480	2216640	then one is a problem. But if it's accidents because of increased reliance on automation,
2216640	2225520	for example, so when airplanes are flying in an automated way, maybe the number of plane crashes
2225520	2231360	increased by 17% or something. And then you're like, okay, do we really want to rely on automation?
2231360	2235680	I think in the case of automation, airplanes, it decreased significantly. Okay, same thing
2235680	2242240	with autonomous vehicles. Like, okay, what are the pros and cons? What are the tradeoffs here?
2242240	2248560	You can have that discussion in an honest way. But I think the kind of things we're talking about here
2248560	2258160	is mass scale pain and suffering caused by AI systems. And I think we need to see illustrations
2258160	2264560	of that on a very small scale to start to understand that this is really damaging versus
2265680	2270000	clippy versus a tool that's really useful to a lot of people to do learning, to do
2271920	2276960	summarization of texts, to do question answer, all that kind of stuff, to generate videos,
2277760	2283520	the tool, fundamentally a tool versus an agent that can do a huge amount of damage.
2283520	2289040	So you bring up an example of cars. Yes. Cars were slowly developed and integrated.
2289760	2294640	If we had no cars and somebody came around and said, I invented this thing, it's called cars,
2294640	2301600	it's awesome. It kills like 100,000 Americans every year. Let's deploy it. Would we deploy that?
2302480	2308320	There have been fear mongering about cars for a long time. The transition from horse to the cars
2308320	2313200	is a really nice channel that I recommend. People check out pessimists archive that
2313200	2317280	documents all the fear mongering about technology that's happened throughout history.
2317280	2322480	There's definitely been a lot of fear mongering about cars. There's a transition period there
2322480	2328560	about cars, about how deadly they are. We can try. It took a very long time for cars to proliferate
2328560	2335040	to the degree they have now. And then you could ask serious questions in terms of the miles traveled,
2335040	2339760	the benefit to the economy, the benefit to the quality of life that cars do versus the number
2339760	2346800	of deaths, 30, 40,000 in the United States. Are we willing to pay that price? I think most people
2346800	2354800	when they're rationally thinking policy makers will say yes. We want to decrease it from 40,000
2354800	2360400	to zero and do everything we can to decrease it. There's all kinds of policies and centers you
2360400	2366480	can create to decrease the risks with the deployment of technology, but then you have to weigh the
2366480	2370560	benefits and the risks of the technology. And the same thing would be done with AI.
2371280	2376400	You need data. You need to know, but if I'm right and it's unpredictable, unexplainable,
2376400	2381440	uncontrollable, you cannot make this decision. We're gaining $10 trillion of wealth,
2381440	2387520	but we're losing, we don't know how many people. You basically have to perform an experiment
2387520	2392800	on 8 billion humans without their consent. And even if they want to give you consent,
2392800	2397360	they can't because they cannot give informed consent. They don't understand those things.
2398080	2403360	Right. That happens when you go from the predictable to the unpredictable very quickly.
2406480	2412320	But it's not obvious to me that AI systems would gain capability so quickly that you won't be able
2412320	2418640	to collect enough data to study the benefits and the risks. We're literally doing it. The
2418640	2424240	previous model we learned about after we finished training it, what it was capable of. Let's say
2424240	2430560	we stopped GPT-4 training run around human capability. Hypothetically, we start training GPT-5
2430560	2436080	and I have no knowledge of insider training runs or anything. And we started that point of about
2436080	2441840	human and we train it for the next nine months. Maybe two months in, it becomes super intelligent.
2441840	2448080	We continue training it. At the time when we started testing it, it is already a dangerous
2448080	2452720	system. How dangerous? I have no idea. But neither people training it.
2453520	2458960	At the training stage, but then there's a testing stage inside the company. They can
2458960	2464080	start getting intuition about what the system is capable to do. You're saying that somehow from
2464080	2473760	leap from GPT-4 to GPT-5 can happen the kind of leap where GPT-4 was controllable and GPT-5 is no
2473760	2480080	longer controllable. And we get no insights from using GPT-4 about the fact that GPT-5 will be
2480080	2488000	uncontrollable. That's the situation you're concerned about. Where their leap from M to M
2488000	2497760	plus one would be such that an uncontrollable system is created without any ability for us to
2497760	2503280	anticipate that. If we had capability of ahead of the run before the training run to register
2503280	2507440	exactly what capabilities that next model will have at the end of the training run.
2507440	2512080	And we accurately guessed all of them. I would say you're right. We can definitely go ahead with
2512080	2517680	this run. We don't have that capability. From GPT-4, you can build up intuitions about what
2517680	2526080	GPT-5 will be capable of. It's just incremental progress. Even if that's a big leap in capability,
2526640	2529680	it just doesn't seem like you can take a leap from a system that's
2530640	2535840	helping you write emails to a system that's going to destroy human civilization.
2535840	2541600	It seems like it's always going to be sufficiently incremental such that we can anticipate the
2541600	2547040	possible dangers. And we're not even talking about existential risks, but just the kind of damage
2547040	2552240	you can do to civilization. It seems like we'll be able to anticipate the kinds, not the exact,
2552320	2562560	but the kinds of risks it might lead to and then rapidly develop defenses ahead of time and as
2563200	2568560	the risks emerge. We're not talking just about capabilities, specific tasks. We're talking
2568560	2575600	about general capability to learn. Maybe like a child at the time of testing and deployment,
2575600	2583200	it is still not extremely capable, but as it is exposed to more data, real world, it can be trained
2583200	2588080	to become much more dangerous and capable. Let's focus then on the control problem.
2591120	2593760	At which point does the system become uncontrollable?
2595040	2599040	Why is it the more likely trajectory for you that the system becomes uncontrollable?
2600000	2606080	I think at some point it becomes capable of getting out of control. For game-theoretic reasons,
2606080	2611360	it may decide not to do anything right away and for a long time just collect more resources,
2611360	2618640	accumulate strategic advantage. Right away, it may be kind of still young, weak superintelligence,
2618640	2624240	give it a decade. It's in charge of a lot more resources. It had time to make backups,
2624240	2627360	so it's not obvious to me that it will strike as soon as it can.
2628240	2637040	Can we just try to imagine this future where there's an AI system that's capable of escaping
2637040	2645200	the control of humans and then doesn't and waits? What's that look like? One, we have to rely on
2645200	2650320	that system for a lot of the infrastructure. We'll have to give it access, not just to the internet,
2651280	2661440	but to the task of managing power, government, economy, this kind of stuff. That just feels
2661440	2665680	like a gradual process given the bureaucracies of all those systems involved. We've been doing
2665680	2671200	it for years. Software controls all the systems, nuclear power plants, airline industry. It's
2671200	2675520	all software-based. Every time there is electrical outage, I can't fly anywhere for days.
2676400	2682880	But there's a difference between software and AI. There's different kinds of software.
2682880	2689920	So to give a single AI system access to the control of airlines and the control of the economy,
2691040	2694880	that's not a trivial transition for humanity.
2694880	2699360	No, but if it shows it is safer, in fact, when it's in control, we get better results,
2699360	2702400	people will demand that it was put in place. Absolutely.
2702400	2706720	And if not, it can hack the system. It can use social engineering to get access to it.
2706720	2710640	That's why I said it might take some time for it to accumulate those resources.
2710640	2716000	It just feels like that would take a long time for either humans to trust it or for the social
2716000	2720560	engineering to come into play. It's not a thing that happens overnight. It feels like something
2720560	2726240	that happens across one or two decades. I really hope you're right, but it's not what I'm seeing.
2726240	2731040	People are very quick to jump on a latest trend. Early adopters will be there before it's even
2731040	2737520	deployed buying prototypes. Maybe the social engineering. Because for social engineering,
2737520	2743840	AI systems don't need any hardware access. It's all software. So they can start manipulating you
2743840	2748640	through social media and so on. You have AI assistants that are going to help you do a lot of
2748640	2754560	manage a lot of your day to day, and then they start doing social engineering. But for a system
2754560	2762640	that's so capable that it can escape the control of humans that created it, such a system being
2762640	2771920	deployed at a mass scale and trusted by people to be deployed, it feels like that would take
2771920	2777760	a lot of convincing. So we've been deploying systems which had hidden capabilities.
2779200	2783920	Can you give an example? GPT-4. I don't know what else is capable of, but there are still
2783920	2788240	things we haven't discovered, can do. There may be trivial proportion to its capability.
2788880	2795360	I don't know. It writes Chinese poetry, hypothetical. I know it does. But we haven't tested
2795360	2801920	for all possible capabilities, and we're not explicitly designing them. We can only rule
2801920	2808640	out bugs we find. We cannot rule out bugs and capabilities because we haven't found them.
2808960	2818560	Is it possible for a system to have hidden capabilities that are orders of magnitude
2818560	2823920	greater than its non-hidden capabilities? This is the thing I'm really struggling with,
2824640	2832640	where on the surface, the thing we understand it can do doesn't seem that harmful. So even if it
2833600	2841040	has hidden capabilities like Chinese poetry or generating effective software viruses,
2842400	2850560	the damage that can do seems like on the same order of magnitude as the capabilities that we
2850560	2857040	know about. This idea that the hidden capabilities will include being uncontrollable is something
2857040	2861920	I'm struggling with because GPT-4 on the surface seems to be very controllable.
2861920	2867440	Again, we can only ask and test for things we know about. If there are unknown unknowns,
2867440	2873040	we cannot do it. I'm thinking of humans, artistic savants. If you talk to a person like that,
2873040	2879360	you may not even realize they can multiply 20-digit numbers in their head. You have to know to ask.
2881280	2886480	As I mentioned, just to linger on the fear of the unknown,
2886800	2892880	the pessimist archive has just documented, let's look at data of the past at history.
2892880	2897360	There's been a lot of fear-mongering about technology. The pessimist archive does a
2897360	2903760	really good job of documenting how crazily afraid we are of every piece of technology.
2903760	2909120	We've been afraid, there's a blog post where Louis Anslow, who created the pessimist archive,
2909120	2914560	writes about the fact that we've been fear-mongering about robots and automation for
2915360	2922400	over 100 years. Why is AGI different than the kinds of technologies we've been afraid of in
2922400	2930880	the past? Two things. One, we're switching from tools to agents. Tools don't have negative or
2930880	2937120	positive impact. People using tools do. Guns don't kill. People with guns do.
2937840	2943280	Agents can make their own decisions. They can be positive or negative. A pit bull can decide to
2943280	2950160	harm you. That's an agent. The fears are the same. The only difference is now we have this
2950160	2955760	technology. When they were afraid of humanoid robots 100 years ago, they had none. Today,
2955760	2960240	every major company in the world is investing billions to create them. Not every, but you
2960240	2968080	understand what I'm saying. It's very different. Well, agents, it depends on what you mean by
2968080	2972880	the word agents. All those companies are not investing in a system that has the kind of agency
2974240	2979760	that's implied by in the fears, where it can really make decisions on their own,
2979760	2984800	that have no human in the loop. They are saying they're building super-intelligence
2984800	2989280	and have a super-alignment team. You don't think they're trying to create a system smart enough
2989280	2994880	to be an independent agent under that definition? I have not seen evidence of it. I think a lot of
2994880	3002080	it is a marketing kind of discussion about the future. It's a mission about
3003280	3006480	the kind of systems that can create in the long-term future, but in the short term,
3007040	3016240	the kind of systems they're creating falls fully within the definition of narrow AI.
3016240	3021920	These are tools that have increasing capabilities, but they just don't have a sense of agency or
3021920	3029120	consciousness or self-awareness or ability to deceive at scales that would be required to do
3030080	3034640	mass-scale suffering and murder of humans. Those systems are well beyond narrow AI.
3034640	3040160	If you had to list all the capabilities of GPT-4, you would spend a lot of time writing that list.
3040160	3045840	But agency is not one of them. Not yet, but do you think any of those companies are holding back
3045840	3050720	because they think it may be not safe or are they developing the most capable system they can,
3050720	3055280	given the resources and hoping they can control and monetize?
3056240	3061680	Control and monetize. Hoping they can control and monetize. You're saying if they could
3061680	3068080	press a button and create an agent that they no longer control, that they can have to ask nicely.
3070000	3074880	A thing that lives on a server across a huge number of computers.
3076960	3081280	You're saying that they would push for the creation of that kind of system?
3082240	3087680	I can't speak for other people for all of them. I think some of them are very ambitious. They
3087680	3091680	fundraise in trillions. They talk about controlling the light corner of the universe.
3092240	3093760	I would guess that they might.
3095920	3099280	Well, that's a human question. Whether humans are capable of that,
3099280	3104480	probably some humans are capable of that. My more direct question is if it's possible to
3104480	3112480	create such a system. I have a system that has that level of agency. I don't think that's an easy
3112480	3119520	technical challenge. It doesn't feel like we're close to that. A system that has the kind of
3119520	3125120	agency where it can make its own decisions and deceive everybody about them. The current architecture
3126160	3131840	we have in machine learning and how we train the systems, how we deploy the systems and all that,
3131920	3134080	it just doesn't seem to support that kind of agency.
3134640	3140800	I really hope you're right. I think the scaling hypothesis is correct. We haven't seen diminishing
3140800	3148080	returns. It used to be we asked how long before AGI. Now we should ask how much until AGI. It's
3148080	3152720	trillion dollars today. It's a billion dollars next year. It's a million dollars in a few years.
3153760	3157120	Don't you think it's possible to basically run out of trillions?
3158080	3160160	So is this constrained by compute?
3160880	3163760	Compute gets cheaper every day exponentially.
3163760	3166640	But then that becomes a question of decades versus years.
3167280	3172960	If the only disagreement is that it will take decades not years for everything I'm saying to
3173840	3175920	materialize, then I can go with that.
3177360	3184320	But if it takes decades, then the development of tools for AI safety becomes more and more
3184320	3191280	realistic. So I guess the question is I have a fundamental belief that humans when faced with
3191280	3198000	danger can come up with ways to defend against that danger. And one of the big problems facing AI
3198000	3204560	safety currently for me is that there's not clear illustrations of what that danger looks like.
3206160	3209200	There's no illustrations of AI systems doing a lot of damage.
3210160	3216640	And so it's unclear what you're defending against because currently it's a philosophical notions that
3216640	3221760	yes it's possible to imagine AI systems that take control of everything and then destroy all humans.
3222480	3228720	It's also a more formal mathematical notion that you talk about that it's impossible to
3228720	3235280	have a perfectly secure system. You can't you can't prove that a program of sufficient complexity
3235840	3242640	is completely safe and perfect and know everything about it. Yes, but like when you actually just
3242640	3247440	programmatically look how much damage have the AI systems done and what kind of damage
3248000	3253600	there's not been illustrations of that. Even in the autonomous weapon systems
3254640	3258400	there's not been mass deployments of autonomous weapon systems luckily.
3259120	3266560	The automation in war currently is very limited. The that the automation is at the
3266560	3274000	scale of individuals versus like at the scale of strategy and planning. So I think one of the
3274000	3281120	challenges here is like where is the dangers and the intuition that Yalakuna and others have
3281120	3288240	is let's keep in the open building AI systems until the dangers start rearing their heads
3289440	3299360	and they become more explicit. They start being case studies illustrative case studies that show
3299360	3304320	exactly how the damage by AI systems is done then regulation can step in then brilliant
3304320	3309520	engineers can step up and we could have Manhattan style projects that defend against such systems.
3310080	3316400	That's kind of the notion and I guess attention with that is the idea that for you we need to
3316400	3322800	be thinking about that now so that we're ready because we will have not much time when the systems
3322800	3330480	are deployed. Is that true? There is a lot to unpack here. There is a partnership on AI, a
3330480	3335200	conglomerate of many large corporations. They have a database of AI accidents they collect.
3335200	3341360	I contributed a lot to the database. If we so far made almost no progress in actually
3341360	3346880	solving this problem, not patching it, not again lipstick and a pig kind of solutions,
3347840	3351600	why would we think we'll do better than we closer to the problem?
3353120	3356560	All the things you mentioned are serious concerns. Measuring the amount of harm,
3356560	3361920	so benefit versus risk there is difficult but to you the sense is already the risk has superseded
3361920	3367200	the benefit. Again, I want to be perfectly clear. I love AI. I love technology. I'm a computer
3367200	3371440	scientist. I have PhD in engineering. I work at engineering school. There is a huge difference
3371440	3378080	between we need to develop narrow AI systems, super intelligent in solving specific human
3378080	3384320	problems like protein folding and let's create super intelligent machine got it and we'll decide
3384320	3391520	what to do with us. Those are not the same. I am against the super intelligence in general sense
3391600	3399520	with no undo button. Do you think the teams that are doing, they're able to do the AI safety on
3399520	3408640	the kind of narrow AI risks that you've mentioned, are those approaches going to be at all productive
3408640	3414320	towards leading to approaches of doing AI safety on AGI or is that just a fundamentally different
3414320	3419520	partially but they don't scale for narrow AI for deterministic systems. You can test them. You
3419520	3425120	have edge cases. You know what the answer should look like. You know the right answers for general
3425120	3432080	systems. You have infinite test surface. You have no edge cases. You cannot even know what to test for.
3432720	3439840	Again, the unknown unknowns are underappreciated by people looking at this problem. You are always
3439840	3446000	asking me, how will it kill everyone? How will it will fail? The whole point is if I knew it,
3446000	3448960	I would be super intelligent and despite what you might think, I'm not.
3450320	3453360	So to you, the concern is that we would not be able to
3455680	3462480	see early size of an uncontrollable system. It is a master at deception. Sam tweeted about how
3462480	3469840	great it is at persuasion and we see it ourselves, especially now with voices with maybe kind of
3469840	3475840	flirty sarcastic female voices. It's going to be very good at getting people to do things.
3475840	3484080	But see, I'm very concerned about system being used to control the masses.
3485840	3490880	But in that case, the developers know about the kind of control that's happening.
3491680	3497520	You're more concerned about the next stage where even the developers don't know about the deception.
3498160	3503920	Right. I don't think developers know everything about what they are creating. They have lots of
3503920	3509680	great knowledge. We're making progress on explaining parts of the network. We can understand, okay,
3509680	3517440	this node get excited when this input is presented, this cluster of nodes. But when nowhere near
3518000	3521120	close to understanding the full picture and I think it's impossible,
3521840	3527440	you need to be able to survey an explanation. The size of those models prevents a single human
3527440	3532800	from observing all this information, even if provided by the system. So either we're getting
3532800	3538400	model as an explanation for what's happening and that's not comprehensible to us, or we're getting
3538400	3544240	a compressed explanation, lossy compression, where here's top 10 reasons you got fired.
3545120	3547200	That's something, but it's not a full picture.
3547280	3552720	You've given elsewhere an example of a child and everybody, all humans try to deceive. They try
3552720	3558880	to lie early on in their life. I think we'll just get a lot of examples of deceptions from
3558880	3564240	large language models or AI systems. They're going to be kind of shitty or they'll be pretty good,
3564240	3571680	but we'll catch them off guard. We'll start to see the kind of momentum towards developing,
3572640	3578560	increasing deception capabilities. And that's when you're like, okay, we need to do some kind of
3578560	3583840	alignment that prevents deception. But then we'll have, if you support open source, then you can
3583840	3588000	have open source models that have some level of deception. You can start to explore on a large
3588000	3594320	scale. How do we stop it from being deceptive? Then there's a more explicit, pragmatic kind of
3594880	3603920	problem to solve. How do we stop AI systems from trying to optimize for deception? That's just an
3603920	3609920	example, right? So there is a paper, I think it came out last week by Dr. Parkadall from MIT, I
3609920	3617040	think, and they showed that existing models already showed successful deception in what they do.
3618800	3623920	My concern is not that they lie now and we need to catch them and tell them don't lie. My concern
3623920	3631840	is that once they are capable and deployed, they will later change their mind because that's what
3633120	3638240	unrestricted learning allows you to do. Lots of people grow up maybe in the religious family.
3638800	3645120	They read some new books and they turn in their religion. That's a treacherous turn in humans.
3645760	3652880	If you learn something new about your colleagues, maybe you'll change how you react to them.
3653520	3659440	Yeah, a treacherous turn. If we just mention humans, Stalin and Hitler, there's a turn.
3660400	3666400	Stalin is a good example. He just seems like a normal communist follower,
3666400	3673360	Lenin, until there's a turn. There's a turn of what that means in terms of when he has complete
3673360	3677520	control with what the execution of that policy means and how many people get to suffer.
3677520	3682640	And you can't say they are not rational. The rational decision changes based on your position.
3683120	3687280	Then you are under the boss. The rational policy may be to be
3688000	3693040	following orders and being honest. When you become a boss, the rational policy may shift.
3694000	3696720	Yeah, and by the way, a lot of my disagreements here is just
3698800	3701840	playing devil's advocate to challenge your ideas and to explore them together.
3704720	3707200	One of the big problems here in this whole conversation is
3707680	3712240	human civilization hangs in the balance and yet it's everything is unpredictable.
3712240	3714000	We don't know how these systems will look like.
3718400	3719440	The robots are coming.
3720320	3722400	There's a refrigerator making a buzzing noise.
3723600	3730080	Very menacing. So every time I'm about to talk about this topic, things start to happen.
3730080	3733440	My flight yesterday was cancelled without possibility to rebook.
3734160	3741600	I was giving a talk at Google in Israel and three cars which were supposed to take me to the talk
3741600	3751600	could not. I'm just saying. I mean, I like the eyes. I for one welcome our overlords.
3751600	3758480	There's a degree to which we, I mean, it is very obvious as we already have. We've increasingly
3758480	3765600	given our life over to software systems. And then it seems obvious, given the capabilities
3765600	3770800	of AI that are coming, that we'll give our lives over increasingly to AI systems.
3771360	3778640	Cars will drive themselves. Refrigerator eventually will optimize what I get to eat.
3779760	3786560	And as more and more of our lives are controlled or managed by AI assistance,
3786640	3792240	it is very possible that there's a drift. I mean, I personally am concerned about
3792240	3798560	non existential stuff, the more near term things. Because before we even get to existential,
3798560	3802160	I feel like there could be just so many brave new world type of situations.
3802160	3808960	You mentioned sort of the term behavioral drift. It's the slow boiling that I'm really concerned
3808960	3816320	about as we give our lives over to automation that our minds can become controlled by governments,
3816400	3823600	by companies, or just in a distributed way, there's a drift. Some aspect of our human nature
3823600	3829680	gives ourselves over to the control of AI systems. And they, in an unintended way, just control how
3829680	3835440	we think. Maybe there'll be a herd like mentality and how we think, which will kill all creativity
3835440	3842960	and exploration of ideas, the diversity of ideas, or they're or much worse. So it's true. It's true.
3842960	3848240	But a lot of the conversation I'm having with you now is also kind of wondering,
3848960	3856880	almost on a technical level, how can AI escape control? What would that system look like?
3858080	3867200	Because to me, it's terrifying and fascinating. And also fascinating to me is maybe the optimistic
3867200	3872960	notion that it's possible to engineer systems that defend against that. One of the things you
3872960	3879440	write a lot about in your book is verifiers. So not humans, humans are also verifiers,
3880560	3889120	but software systems that look at AI systems and like help you understand this thing is getting
3889120	3895600	real weird, help you, help you analyze those systems. So maybe that's a, this is a good time
3895680	3900880	to talk about verification. What is this beautiful notion of verification?
3900880	3905120	My claim is again that there are very strong limits on what we can and cannot verify.
3906080	3910240	A lot of times when you post something in social media, people go, oh, I need citation
3910240	3914880	to a peer-reviewed article. But what is a peer-reviewed article? You found two people
3914880	3919840	in a world of hundreds of thousands of scientists who said, I would have a publisher, I don't care.
3919840	3926080	That's the verifier of that process. When people say, oh, it's formally verified software,
3926080	3934320	mathematical proof, they accept something close to 100% chance of it being free of all problems.
3934320	3941040	But if you actually look at research, software is full of bugs, old mathematical theorems,
3941040	3945680	which have been proven for hundreds of years, have been discovered to contain bugs, on top
3945680	3953120	of which we generate new proofs. And now we have to redo all that. So verifiers are not perfect.
3953120	3957680	Usually they are either a single human or communities of humans. And it's basically kind
3957680	3963200	of like a democratic vote. Community of mathematicians agrees that this proof is correct.
3963200	3969440	Mostly correct. Even today, we're starting to see some mathematical proofs are so complex,
3969440	3974560	so large, that mathematical community is unable to make a decision. It looks interesting,
3974640	3979840	looks promising, but they don't know. They will need years for top scholars to study it to figure
3979840	3986000	it out. So of course, we can use AI to help us with this process. But AI is a piece of software
3986000	3991040	which needs to be verified. Just to clarify. So verification is the process of saying something
3991040	3995680	is correct. Sort of the most formal, a mathematical proof where there's a statement
3996480	4002320	and a series of logical statements that prove that statement to be correct. This is a theorem.
4003200	4009200	And you're saying it gets so complex that it's possible for the human verifiers,
4010000	4016240	the human beings that verify that the logical step, there's no bugs in it. It becomes impossible.
4016240	4021920	So it's nice to talk about verification in this most formal, most clear, most rigorous
4022960	4025360	formulation of it, which is mathematical proofs.
4025680	4032960	And for AI, we would like to have that level of confidence, a very important mission critical
4032960	4037680	software controlling satellites, nuclear power plants for small deterministic programs. We
4037680	4045920	can do this. We can check that code verifies its mapping to the design, whatever software
4045920	4053440	engineers intended was correctly implemented. But we don't know how to do this for software which
4053440	4058960	keeps learning, self modifying, rewriting its own code. We don't know how to prove things about
4058960	4064800	the physical world, states of humans in the physical world. So there are papers coming out
4064800	4073040	now and I have this beautiful one towards guaranteed safe AI. Very cool paper, some of the
4073040	4078960	best authors I ever seen. I think there is multiple Turing Award winners that is quite,
4078960	4084800	you can have this one and one just came out kind of similar managing extreme AI risks.
4084800	4094080	So all of them expect this level of proof, but I would say that we can get more confidence with
4094080	4099680	more resources we put into it. But at the end of the day, we're still as reliable as the verifiers.
4100240	4105680	And you have this infinite regressive verifiers, the software used to verify a program is itself
4105680	4112320	a piece of program. If aliens give us well aligned superintelligence, we can use that to create our
4112320	4119600	own safe AI. But it's a catch 22. You need to have already proven to be safe system to verify
4119600	4125600	this new system of equal or greater complexity. You just mentioned this paper towards guaranteed
4125600	4130160	safe AI, a framework for ensuring robust and reliable AI systems. Like you mentioned, it's
4130160	4136000	like a who's who. Josh Tannenbaum, Yosha Benjo, Sarah Russell, Max Tagmark, many, many, many
4136000	4140720	other billion people. The page you have it open on there are many possible strategies for creating
4141360	4145840	safety specifications. These strategies can roughly be placed on a spectrum,
4145840	4151040	depending on how much safety it would grant if successfully implemented. One way to do this
4151040	4155920	is as follows. And there's a set of levels from level zero, no safety specification is used to
4155920	4161440	level seven, the safety specification completely encodes all things that humans might want in
4161440	4170320	all contexts. Where does this paper fall short to you? So when I wrote a paper, artificial intelligence
4170320	4176560	safety engineering, which kind of coins the term AI safety, that was 2011, we had 2012 conference,
4176560	4181600	2013 journal paper, one of the things I proposed, let's just do formal verifications on it. Let's
4181600	4187920	do mathematical formal proofs. In the follow up work, I basically realized it will still not get us
4187920	4195360	100%. We can get 99.9, we can put more resources exponentially and get closer, but we never get
4195360	4201600	to 100%. If a system makes a billion decisions a second, and you use it for 100 years, you're
4201600	4206880	still gonna deal with a problem. This is wonderful research. I'm so happy they're doing it. This is
4206880	4213920	great, but it is not going to be a permanent solution to that problem. So just to clarify,
4213920	4220480	the task of creating an AI verifier is what? Is creating a verifier that the AI system does exactly
4220480	4227040	as it says it does, or it sticks within the guard rails that it says it must? There are many, many
4227040	4233120	levels. So first, you're verifying the hardware in which it is run. You need to verify communication
4233120	4238640	channel with the human. In every aspect of that whole world model needs to be verified. Somehow
4238640	4245920	it needs to map the world into the world model, map and territory differences. So how do I know
4245920	4252320	internal states of humans? Are you happy or sad? I can't tell. So how do I make proofs about real
4252320	4258560	physical world? Yeah, I can verify that deterministic algorithm follows certain properties. That can be
4258560	4264320	done. Some people argue that maybe just maybe two plus two is not four. I'm not that extreme.
4265680	4271840	But once you have sufficiently large proof over sufficiently complex environment,
4271840	4277920	the probability that it has zero bugs in it is greatly reduced. If you keep deploying this
4277920	4283200	a lot, eventually you're gonna have a bug anyways. There's always a bug. There is always a bug. And
4283200	4286960	the fundamental difference is what I mentioned. We're not dealing with cybersecurity. We're not
4286960	4292400	going to get a new credit card, new humanity. So this paper is really interesting. You said
4292400	4297440	2011, artificial intelligence, safety engineering, why machine ethics is a wrong approach.
4298800	4305840	The grand challenge you write of AI safety engineering, we propose the problem of developing
4305840	4312640	safety mechanisms for self-improving systems. Self-improving systems. But that's an interesting
4312640	4321760	term for the thing that we're talking about. Is self-improving more general than learning?
4323920	4328960	So self-improving, that's an interesting term. You can improve the rate at which you are learning.
4328960	4336320	You can become more efficient, meta-optimizer. The word self, it's like self-replicating,
4336320	4344880	self-improving. You can imagine a system building its own world on a scale and in a way that is
4344880	4349920	way different than the current systems do. It feels like the current systems are not self-improving
4349920	4356720	or self-replicating or self-growing or self-spreading, all that kind of stuff. And once you take that
4356720	4362320	leap, that's when a lot of the challenges seems to happen. Because the kind of bugs you can find now
4363200	4369920	seems more akin to the current sort of normal software debugging kind of process.
4371680	4376640	But whenever you can do self-replication and arbitrary self-improvement,
4378160	4385280	that's when a bug can become a real problem, real, real fast. So what is the difference to you
4385360	4392960	between verification of a non-self-improving system versus a verification of a self-improving system?
4392960	4397760	So if you have fixed code, for example, you can verify that code, static verification,
4397760	4405840	at the time. But if it will continue modifying it, you have a much harder time guaranteeing
4405840	4410480	that important properties of that system have not been modified, then the code changed.
4411200	4415200	Does it even do a mo? No. Does the whole process of verification is completely
4415200	4420400	fall apart? It can always cheat. It can store parts of its code outside in the environment.
4420400	4426880	It can have kind of extended mind situations. So this is exactly the type of problems I'm
4426880	4430960	trying to bring up. What are the classes of verifiers that you read about in the book?
4431520	4437280	Is there interesting ones to stand out to you? Do you have some favorites? So I like oracle types
4437280	4442400	where you kind of just know that it's right during like oracle machines. They know the right answer,
4442400	4448720	how, who knows, but they pull it out from somewhere. So you have to trust them. And that's a concern I
4448720	4456160	have about humans in a world with very smart machines. We experiment with them. We see after
4456160	4460800	a while, okay, they always been right before and we start trusting them without any verification
4460800	4468320	of what they're saying. Oh, I see that we kind of build oracle verifiers or rather we build verifiers
4468320	4475440	we believe to be oracles and then we start to, without any proof, use them as if they're oracle
4475440	4480640	verifiers. We remove ourselves from that process. We are not scientists who understand the world.
4480640	4488400	We are humans who get new data presented to us. Okay, one really cool class of verifiers is a
4488400	4495840	self-verifier. Is it possible that you somehow engineer into AI systems the thing that constantly
4495840	4502320	verifies itself? Preserved portion of it can be done, but in terms of mathematical verification,
4502320	4506560	it's kind of useless. You saying you are the greatest guy in the world because you are saying it.
4506560	4511920	It's circular and not very helpful, but it's consistent. We know that within that world,
4511920	4518000	you have verified that system. In a paper, I try to kind of brute force all possible verifiers.
4518000	4521760	It doesn't mean that this one is particularly important to us.
4521760	4528240	But what about self-doubt? Like the kind of verification where you said you say or I say
4528240	4533840	I'm the greatest guy in the world. What about a thing which I actually have is a voice that is
4533840	4541840	constantly extremely critical. So engineer into the system a constant uncertainty about self,
4542800	4550560	a constant doubt. Any smart system would have doubt about everything. You're not sure what
4551200	4554800	information you are given is true. If you are subject to manipulation,
4555440	4561760	you have this safety and security mindset. What I mean, you have doubt about yourself.
4562400	4570800	So the AI systems that has doubt about whether the thing is doing is causing harm,
4570800	4575600	is the right thing to be doing. So just a constant doubt about what it's doing,
4575600	4580560	because it's hard to be a dictator full of doubt. I may be wrong, but I think Stuart Drussell's
4581440	4587360	ideas all about machines which are uncertain about what humans want and trying to learn
4587360	4591440	better and better what we want. The problem, of course, is we don't know what we want and we
4591440	4598000	don't agree on it. Yeah, but uncertainty. His idea is that having that like self-doubt,
4598000	4602400	uncertainty in AI systems, engineering in AI systems is one way to solve the control problem.
4603120	4607440	It could also backfire. Maybe you're uncertain about completing your mission.
4607440	4612640	Like I am paranoid about your camera is not recording right now. So I would feel much better
4612640	4617520	if you had a secondary camera, but I also would feel even better if you had a third.
4617520	4622880	And eventually I would turn this whole world into cameras pointing at us, making sure we're
4623440	4627360	capturing this. No, but wouldn't you have a meta concern
4628960	4632160	like that you just stated that eventually there'll be way too many cameras?
4633520	4639760	So you would be able to keep zooming on the big picture of your concerns.
4641520	4647520	So it's a multi-objective optimization. It depends how much I value capturing this versus
4647600	4654080	not destroying the universe. Right, exactly. And then you will also ask about like what does it
4654080	4657760	mean to destroy the universe and how many universes are and you keep asking that question,
4657760	4662720	but that doubting yourself would prevent you from destroying the universe because you're
4662720	4668160	constantly full of doubt. It might affect your productivity. You might be scared to do anything.
4668160	4672320	It's just scared to do anything. That's things up. Well, that's better. I mean,
4672320	4676400	I guess the question is the possible to engineer that in. I guess your answer would be yes,
4676400	4680160	but we don't know how to do that. And we need to invest a lot of effort into figuring out how to
4680160	4688080	do that, but it's unlikely. I mean, underpinning a lot of your writing is this sense that we're
4688080	4696240	screwed. But it just feels like it's an engineering problem. I don't understand why we're screwed.
4698000	4703200	Time and time again, humanity has gotten itself into trouble and figured out a way to get out
4703200	4710320	of trouble. We are in a situation where people making more capable systems just need more resources.
4711520	4716640	They don't need to invent anything, in my opinion. Some will disagree, but so far at least I don't
4716640	4722480	see diminishing returns. If you have 10x compute, you will get better performance. The same doesn't
4722480	4729120	apply to safety. If you give a MIDI or any other organization 10 times the money, they don't output
4729120	4735760	10 times the safety. And the gap between capabilities and safety becomes bigger and bigger all the time.
4736400	4744880	So it's hard to be completely optimistic about our results here. I can name 10 excellent breakthrough
4744880	4750880	papers in machine learning. I would struggle to name equally important breakthroughs in safety.
4750880	4756640	A lot of times a safety paper will propose a toy solution and point out 10 new problems
4756640	4761680	discovered as a result. It's like this fractal. You're zooming in and you see more problems,
4761680	4766080	and it's infinite in all directions. Does this apply to other technologies,
4766080	4771360	or is this unique to AI, where safety is always lagging behind?
4772320	4779920	So I guess we can look at related technologies with cybersecurity. We did manage to have banks
4779920	4787600	and casinos and Bitcoin, so you can have secure narrow systems, which are doing okay.
4789360	4795360	Narrow attacks and them fail, but you can always go outside of the box. So if I can't hack your
4795360	4800480	Bitcoin, I can hack you. So there is always something. If I really want it, I will find
4800480	4807200	a different way. We talk about guardrails for AI. Well, that's a fence. I can dig a tunnel under it,
4807200	4812560	I can jump over it, I can climb it, I can walk around it. You may have a very nice guardrail,
4812560	4817680	but in a real world, it's not a permanent guarantee of safety. And again, this is a
4817680	4824000	fundamental difference. We are not saying we need to be 90% safe to get those trillions of
4824000	4829360	dollars of benefit. We need to be 100% indefinitely, or we might lose the principle.
4830320	4838960	So if you look at just humanity as a set of machines, is the machinery of AI safety
4842000	4846640	conflicting with the machinery of capitalism? I think we can generalize it to just
4847760	4852640	prisoners dilemma in general, personal self-interest versus group interest.
4853600	4861440	The incentive such that everyone wants what's best for them. Capitalism obviously has that
4861440	4870320	tendency to maximize your personal gain, which does create this race to the bottom. I don't have to
4871520	4877760	be a lot better than you, but if I'm 1% better than you, I'll capture more of a profit. So
4877760	4883360	it's worth for me personally to take the risk, even if society as a whole will suffer as a result.
4884960	4887440	So capitalism has created a lot of good in this world.
4891120	4895920	It's not clear to me that AI safety is not aligned with the function of capitalism,
4896720	4902960	unless AI safety is so difficult that it requires the complete halt of the development,
4903920	4908160	which is also a possibility. It just feels like building safe systems
4909200	4913200	should be the desirable thing to do for tech companies.
4914400	4919680	Right, look at governance structures. Then you have someone with complete power,
4919680	4925200	they're extremely dangerous. So the solution we came up with is break it up. You have judicial,
4925200	4931200	legislative, executive, same here, have narrow AI systems, work on important problems, solve
4931200	4939840	immortality. It's a biological problem. We can solve similar to how progress was made with protein
4939840	4946080	folding using a system which doesn't also play chess. There is no reason to create
4946080	4952480	superintelligent system to get most of the benefits we want from much safer narrow systems.
4953360	4958320	It really is a question to me whether companies are interested in creating
4959280	4967760	anything but narrow AI. I think when term AGI is used by tech companies, they mean narrow AI.
4969280	4977120	They mean narrow AI with amazing capabilities. I do think that there's a
4977120	4982800	lead between narrow AI with amazing capabilities, with superhuman capabilities, and the kind of
4983760	4991280	self-motivated agent like AGI system that we're talking about. I don't know if it's
4991280	4997440	obvious to me that a company would want to take the leap to creating an AGI that it would lose
4997440	5004000	control of because then it can't capture the value from that system. But the breaking rights,
5004880	5010080	but being first, that is the same humans who are in part of the systems, right?
5010880	5017840	That jumps from the incentives of capitalism to human nature. The question is whether human
5017840	5026160	nature will override the interest of the company. You've mentioned slowing or halting progress.
5027360	5031120	Is that one possible solution? Are you a proponent of pausing development of AI,
5031840	5039600	whether it's for six months or completely? The condition would be not time but capabilities.
5039600	5046000	Pause until you can do XYZ. If I'm right and you cannot, it's impossible then it becomes
5046000	5050800	a permanent ban. But if you're right and it's possible, so as soon as you have the safety
5050800	5060960	capabilities, go ahead. Right. Is there any actual explicit capabilities that you can put on paper,
5060960	5065120	that we as a human civilization could put on paper? Is it possible to make explicit like that?
5065600	5072720	Like versus kind of a vague notion of, just like you said, it's very vague. We want AI systems to
5072720	5078640	do good and we want them to be safe. Those are very vague notions. Is there more formal notions?
5078640	5085840	So then I think about this problem. I think about having a toolbox I would need. Capabilities such
5085840	5092640	as explaining everything about that system's design and workings, predicting not just terminal
5092640	5100160	goal but all the intermediate steps of the system. Control in terms of either direct control,
5100960	5106800	some sort of a hybrid option, ideal advisor, doesn't matter which one you pick but you have to
5106800	5113600	be able to achieve it. In a book, we talk about ours. Verification is another very important tool.
5115680	5120400	Communication without ambiguity. Human language is ambiguous. That's another source of danger.
5121040	5128880	So basically, there is a paper we published in ACM surveys which looks at about 50 different
5128880	5133920	impossibility results which may or may not be relevant to this problem but we don't have enough
5134480	5139440	human resources to investigate all of them for relevance to AI safety. The ones I mentioned
5139440	5145280	to you, I definitely think would be handy and that's what we see AI safety researchers working on.
5145280	5152160	Explainability is a huge one. The problem is that it's very hard to separate capabilities work
5152160	5158720	from safety work. If you make good progress in explainability, now the system itself can engage
5158720	5165360	in self-improvement much easier, increasing capability greatly. So it's not obvious that there is any
5166480	5172160	research which is pure safety work without disproportionate increasing capability and danger.
5172960	5177600	Explainability is really interesting. Why is that connected to huge capability?
5177600	5181280	If it's able to explain itself well, why does that naturally mean that it's more capable?
5181280	5186880	Right now, it's comprised of weights on a neural network. If it can convert it to
5186880	5191280	manipulatable code like software, it's a lot easier to work in self-improvement.
5192080	5198880	I see. You can do intelligent design instead of evolutionary gradual descent.
5199680	5205040	Well, you could probably do human feedback, human alignment more effectively if it's able
5205040	5209600	to be explainable. If it's able to convert the weights into human understandable form,
5209600	5213680	then you could probably have humans interact with it better. Do you think there's hope that
5213680	5220560	we can make AI systems explainable? Not completely. So if they are sufficiently large,
5220560	5229920	you simply don't have the capacity to comprehend what all the trillions of connections represent.
5229920	5236000	Again, you can obviously get a very useful explanation which talks about top most important
5236000	5240480	features which contribute to the decision, but the only true explanation is the model itself.
5242160	5247440	So deception could be part of the explanation, right? So you can never prove that there is
5247440	5254560	some deception in the network explaining itself. Absolutely. And you can probably have
5254560	5259920	targeted deception where different individuals will understand explanation in different ways
5259920	5265440	based on their cognitive capability. So while what you're saying may be the same and true in
5266080	5271840	some situations, ours will be deceived by it. So it's impossible for an AI system to be truly
5272400	5279440	explainable in the way that we mean. Honestly and perfectly. At extreme, the systems which are
5279440	5284960	narrow and less complex could be understood pretty well. If it's impossible to be perfectly
5284960	5290080	explainable, is there a hopeful perspective on that? Like it's impossible to be perfectly
5290080	5295760	explainable, but you can explain mostly important stuff. You can ask the system,
5296160	5300240	what are the worst ways you can hurt humans? It will answer honestly.
5300800	5308160	Any work in a safety direction right now seems like a good idea because we are not slowing down.
5308160	5316160	I'm not for a second thinking that my message or anyone else's will be heard and will be
5316160	5321920	a same civilization which decides not to kill itself by creating its own replacements.
5321920	5325280	The pausing of development is an impossible thing for you.
5325280	5331920	Again, it's always limited by either geographic constraints, pause in US, pause in China. So
5331920	5338480	there are other jurisdictions as the scale of a project becomes smaller. So right now it's like
5338480	5345200	Manhattan project scale in terms of costs and people, but if five years from now, compute
5345200	5351520	is available on a desktop to do it, regulation will not help. You can't control it as easy. Any
5351600	5357920	kid in a garage can train a model. So a lot of it is, in my opinion, just safety theater,
5357920	5362400	security theater, whatever you're saying. Oh, it's illegal to train models so big.
5364800	5370640	So okay, that's security theater and is government regulation also security theater?
5371680	5378160	Given that a lot of the terms are not well defined and really cannot be enforced in real life,
5378160	5383120	we don't have ways to monitor training runs meaningfully live while they take place.
5383680	5389360	There are limits to testing for capabilities I mentioned. So a lot of it cannot be enforced.
5389360	5393760	Do I strongly support all that regulation? Yes, of course, any type of red tape will
5393760	5396640	slow it down and take money away from compute towards lawyers.
5397760	5402880	Can you help me understand what is the hopeful path here for you solution wise?
5403840	5411440	Out of this. It sounds like you're saying AI systems in the end are unverifiable,
5411440	5418880	unpredictable, as the book says, unexplainable, uncontrollable. That's the big one.
5419520	5425440	Uncontrollable and all the other uns just make it difficult to avoid getting to the uncontrollable,
5425440	5432080	I guess. But once it's uncontrollable, then it just goes wild. Surely there are solutions.
5432960	5438480	Humans are pretty smart. What are possible solutions? If you are a dictator of the world,
5438480	5444400	what do we do? So the smart thing is not to build something you cannot control,
5444400	5449520	you cannot understand, build what you can and benefit from it. I'm a big believer in personal
5449520	5456480	self-interest. A lot of the guys running those companies are young rich people. What do they
5456480	5462720	have to gain beyond billions we already have financially, right? It's not the requirement
5462720	5468400	that they press that button. They can easily wait a long time. They can just choose not to do it and
5468400	5475040	still have amazing life. In history, a lot of times, if you did something really bad, at least
5475040	5479680	you became part of history books. There is a chance in this case there won't be any history.
5480000	5483440	So you're saying the individuals running these companies
5484880	5489360	should do some soul searching and what? And stop development?
5489360	5494080	Well, either they have to prove that, of course, it's possible to indefinitely control
5494080	5501200	godlike superintelligent machines by humans and ideally let us know how or agree that it's not
5501200	5505920	possible and it's a very bad idea to do it, including for them personally and their families
5506000	5512640	and friends and capital. So what do you think the actual meetings inside these companies look like?
5513680	5517680	Don't you think they're all the engineers? Really, it is the engineers that make this happen.
5517680	5522480	They're not like automatons. They're human beings. They're brilliant human beings. So they're
5524480	5531280	non-stop asking, how do we make sure this is safe? So again, I'm not inside. From outside,
5531360	5536720	it seems like there is certain filtering going on and restrictions and criticism and what they can
5536720	5543600	say and everyone who was working in charge of safety and whose responsibility it was to protect us
5543600	5548320	said, you know what? I'm going home. So that's not encouraging.
5549040	5554480	What do you think the discussion inside those companies look like? You're developing your
5554480	5561120	training GPT-5. You're training Gemini. You're training Claude and Grock.
5562640	5567040	Don't you think they're constantly, like underneath it, maybe it's not made explicit,
5567040	5572640	but you're constantly sort of wondering like where is the system currently stand?
5572640	5579360	Where are the possible unintended consequences? Where are the limits? Where are the bugs,
5579360	5584000	the small and the big bugs? That's the constant thing that the engineers are worried about.
5584720	5594480	So like, I think super alignment is not quite the same as the kind of thing I'm referring to
5594480	5601360	what engineers are worried about. Super alignment is saying for future systems that we don't quite
5601360	5607680	yet have, how do we keep them safe? You're trying to be a step ahead. It's a different kind of
5608400	5612400	problem because it's almost more philosophical. It's a really tricky one because like you're
5612480	5621200	trying to make, prevent future systems from escaping control of humans.
5621200	5627760	That's really, I don't think there's been, man, is there anything akin to it in the history of
5627760	5634160	humanity? I don't think so, right? Climate change? But there's an entire system which is climate,
5634160	5642000	which is incredibly complex, which we don't have, we have only tiny control of.
5642720	5650560	It's its own system. In this case, we're building the system. So how do you keep that system from
5651680	5656960	becoming destructive? That's a really different problem than the current meetings that companies
5656960	5663680	are having where the engineers are saying, okay, how powerful is this thing? How does it go wrong?
5665120	5670800	And as we train GPT-5 and train up future systems, where are the ways that can go wrong?
5670800	5674480	Don't you think all those engineers are constantly worrying about this,
5674480	5678880	thinking about this, which is a little bit different than the super alignment team
5678880	5681680	that's thinking a little bit farther into the future?
5682480	5692240	Well, I think a lot of people who historically worked on AI never considered what happens when
5692240	5699440	they succeed. Stuart Russell speaks beautifully about that. Let's look, okay, maybe super
5699440	5703920	intelligence is too futuristic, we can develop practical tools for it. Let's look at software
5703920	5712000	today. What is the state of safety and security of our user software? Things we give to millions
5712000	5717440	of people. There is no liability. You click, I agree. What are you agreeing to? Nobody knows,
5717440	5722560	nobody reads, but you're basically saying it will spy on you, corrupt your data, kill your firstborn,
5722560	5727120	and you agree and you're not going to sell the company. That's the best they can do for mundane
5727760	5734000	software, word processor, text software. No liability, no responsibility, just as long
5734000	5739040	as you agree not to sue us, you can use it. If this is a state of the art in systems which
5739920	5745440	narrow accountants, stable manipulators, why do we think we can do so much better
5746160	5752320	with much more complex systems across multiple domains in the environment with malevolent actors
5753040	5759040	with, again, self-improvement, with capabilities exceeding those of humans thinking about it?
5759040	5765120	I mean, the liability thing is more about lawyers than killing firstborns, but if Clippy actually
5765920	5773120	killed the child, I think lawyers aside, it would end Clippy and the company that owns Clippy.
5774080	5780720	All right, so it's not so much about, there's two points to be made. One is like,
5781360	5788000	man, current software systems that are full of bugs and they could do a lot of damage and we
5788000	5792160	don't know what kind, is there unpredictable, there's so much damage they could possibly do.
5793200	5799680	And then we kind of live in this blissful illusion that everything is great and perfect and it works.
5800560	5803200	It's nevertheless, it still somehow works.
5804000	5809840	In many domains, we see car manufacturing, drug development. The burden of proof is on the
5809840	5814960	manufacturer of product or service to show their product or service is safe. It is not up to the
5814960	5822880	user to prove that there are problems. They have to do appropriate safety studies. They have to get
5822880	5827520	government approval for selling the product and they are still fully responsible for what happens.
5827520	5833520	We don't see any of that here. They can deploy whatever they want and I have to explain how
5833520	5838800	that system is going to kill everyone. I don't work for that company. You have to explain to me
5838800	5844720	how it's definitely cannot mess up. That's because it's the very early days of such a technology.
5844720	5849680	Government regulation is lagging behind. They're really not tech savvy. A regulation of any kind
5849680	5854320	of software. If you look at like Congress talking about social media and whenever
5854320	5861040	Mark Zuckerberg and other CEOs show up, the cluelessness that Congress has about how technology
5861040	5867440	works is incredible. It's heartbreaking. I agree completely but that's what scares me.
5868240	5873040	The responses when they start to get dangerous will really get it together. The politicians
5873040	5879200	will pass the right laws. Engineers will solve the right problems. We are not that good at many
5879200	5885760	of those things. We take forever. We are not early. We are two years away according to prediction
5885760	5891920	markets. This is not a biased CEO fundraising. This is what smartest people, super forecasters
5891920	5900720	are thinking of this problem. I'd like to push back. I wonder what those prediction markets are
5900720	5906720	about, how they define AGI. That's wild to me. I want to know what they said about autonomous
5906800	5912000	vehicles because I've heard a lot of experts, financial experts talk about autonomous vehicles
5912000	5919200	and how it's going to be a multi-trillion-dollar industry and all this kind of stuff. It's a
5919200	5923680	small fund but if you have good vision, maybe you can zoom in on that and see the prediction
5923680	5930160	dates in this question. I have a large one if you're interested. I guess my fundamental question
5930160	5938640	is how often they write about technology. Your studies on their accuracy rates and all that,
5938640	5944160	you can look it up. Even if they're wrong, I'm just saying this is right now the best we have.
5944160	5949840	This is what humanity came up with as the predicted date. Again, what they mean by AGI is really
5949840	5958160	important there because there's the non-agent-like AGI and then there's the agent-like AGI and I
5958160	5966000	don't think it's as trivial as a wrapper. Putting a wrapper around, one has lipstick
5966000	5969200	and all it takes is to remove the lipstick. I don't think it's that trivial.
5969200	5974560	You may be completely right but what probability would you assign it? You may be 10% wrong but
5974560	5979520	we're betting all of humanity on this distribution. It seems irrational.
5979520	5984960	Yeah, it's definitely not like one or zero percent. What are your thoughts, by the way,
5984960	5994480	about current systems? Where do they stand? So GPT-40, Claude III, Grock, Jim and I
5996240	6002320	work on the path to superintelligence, to agent-like superintelligence. Where are we?
6003600	6008400	I think they're all about the same. Obviously, there are nuanced differences but in terms of
6008400	6015280	capability, I don't see a huge difference between them. As I said, in my opinion, across
6015280	6021360	all possible tasks, they exceed performance of an average person. I think they're starting to be
6021360	6028400	better than an average master student at my university but they still have very big limitations.
6028960	6037120	If the next model is as improved as GPT-4 versus GPT-3, we may see something very,
6037120	6042640	very capable. What do you feel about all this? I mean, you've been thinking about AI safety for a
6042640	6049440	long, long time and at least for me, the leaps, I mean, it probably started with
6052320	6058080	Alpha-zero. What was mind blowing for me? And then the breakthroughs with LLMs,
6058800	6064640	even GPT-2, but like just the breakthroughs on LLMs, just mind blowing to me. What does it feel
6064640	6073040	like to be living in this day and age where all this talk about AGI feels like it actually might
6073040	6079600	happen and quite soon, meaning within our lifetime? What does it feel like? So when I started working
6079600	6084640	on this, it was pure science fiction. There was no funding, no journals, no conferences,
6084640	6090560	no one in academia would dare to touch anything with the word singularity in it and I was pretend
6090640	6097680	you're at a time, so I was pretty dumb. Now you see touring award winners, publishing in science
6097680	6106240	about how far behind we are according to them in addressing this problem. So it's definitely a change.
6106880	6112800	It's difficult to keep up. I used to be able to read every paper on AI safety, then I was able
6112800	6118080	to read the best ones, then the titles and now I don't even know what's going on. By the time
6118080	6123600	this interview is over, we probably had GPT-6 released and I have to deal with that when I get
6123600	6130560	back home. So it's interesting. Yes, there is now more opportunities. I get invited to speak to
6130560	6137200	smart people. By the way, I would have talked to you before any of this. This is not like some
6137200	6143040	trend of it. To me, it's we're still far away. So just to be clear, we're still far away from AGI,
6143040	6150800	but not far away in the sense relative to the magnitude of impact it can have,
6150800	6158880	we're not far away and we weren't far away 20 years ago because the impact AGI can have
6158880	6164000	is on a scale of centuries. It can end human civilization or it can transform it. So this
6164000	6169280	discussion about one or two years versus one or two decades or even a hundred years, not
6169360	6177440	as important to me because we're headed there. This is like a human civilization scale question.
6178880	6185520	This is not just a hot topic. It is the most important problem we'll ever face. It is not like
6185520	6193760	anything we had to deal with before. We never had birth of another intelligence. Aliens never
6193760	6199200	visited us as far as I know. So similar type of problem, by the way, if an intelligent alien
6199200	6205040	civilization visited us. That's a similar kind of situation. In some ways, if you look at history,
6205040	6209680	any time a more technologically advanced civilization visited a more primitive one,
6209680	6214720	the results were genocide every single time. And sometimes the genocide is worse than
6214720	6219040	others. Sometimes there's less suffering and more suffering. And they always wondered,
6219040	6223200	but how can they kill us with those fire sticks and biological blankets?
6224400	6230560	I mean, Jenghis Khan was nicer. He offered the choice of join or die.
6230560	6235360	But join implies you have something to contribute. What are you contributing to superintelligence?
6236240	6242560	Well, in the zoo, we're entertaining to watch to our humans.
6243600	6247680	You know, I just spent some time in the Amazon. I watched ants for a long time. And the ants
6247680	6252000	are kind of fascinating to watch. I could watch them for a long time. I'm sure there's a lot of
6252640	6257840	value in watching humans. Because we're like, the interesting thing about humans, you know,
6257840	6262800	like when you have a video game that's really well balanced, because of the whole evolutionary
6262800	6268640	process, we've created the society is pretty well balanced. Like our limitations as humans and our
6268640	6273520	capabilities are balanced from a video game perspective. So we have wars, we have conflicts,
6273520	6278400	we have cooperation, like in a game theoretic way, it's an interesting system to watch.
6278400	6282640	In the same way that an ant colony is an interesting system to watch. So like,
6282640	6286960	if I was in alien civilization, I wouldn't want to disturb it. I'd just watch it.
6286960	6290240	Interesting. Maybe perturb it every once in a while in interesting ways.
6290880	6296080	Well, we're getting back to our simulation discussion from before. How did it happen that we
6296080	6302080	exist at exactly like the most interesting 20, 30 years in the history of this civilization? It's
6302160	6305520	been around for 15 billion years and that here we are.
6306160	6308160	What's the probability that we'll live in the simulation?
6308960	6311760	I know never to say 100%, but pretty close to that.
6314160	6315920	Is it possible to escape the simulation?
6317120	6322160	I have a paper about that. This is just the first page teaser, but it's like a nice 30-page
6322160	6324320	document. I'm still here, but yes.
6324960	6327120	How to hack the simulation is the title.
6327120	6331120	I spend a lot of time thinking about that. That would be something I would want super
6331120	6334400	intelligence to help us with. That's exactly what the paper is about.
6335120	6342400	We used AI boxing as a possible tool for control AI. We realized AI will always escape,
6343200	6349200	but that is a skill we might use to help us escape from our virtual box if we are in one.
6350640	6354560	Yeah, you have a lot of really great quotes here, including Elon Musk saying what's outside the
6354560	6360240	simulation. A question I asked him, he would ask an AGI system and he said he would ask
6360240	6363600	what's outside the simulation. That's a really good question to ask.
6364560	6370880	And maybe the follow-up is the title of the paper is how to get out or how to hack it.
6370880	6376320	The abstract reads, many researchers have conjectured that the human kind is simulated along with the
6376320	6383200	rest of the physical universe. In this paper, we do not evaluate evidence for or against such a
6383200	6387200	claim, but instead ask a computer science question, namely, can we hack it?
6388160	6392880	More formally, the question could be phrased as could generally intelligent agents placed in
6392880	6396640	virtual environments find a way to jailbreak out of that? That's a fascinating question.
6397520	6400320	At a small scale, you can actually just construct experiments.
6403200	6407600	Okay. Can they? How can they?
6408560	6416560	So a lot depends on intelligence of simulators, right? With humans boxing super intelligence,
6416720	6423280	the entity in a box was smarter than us, presumed to be. If the simulators are much smarter than
6423280	6428000	us and the super intelligence we create, then probably they can contain us because greater
6428000	6434480	intelligence can control lower intelligence at least for some time. On the other hand, if our
6435200	6439520	super intelligence somehow, for whatever reason, despite having only local resources,
6440080	6447440	manages to fume to levels beyond it, maybe it will succeed. Maybe the security is not that
6447440	6452400	important to them. Maybe it's entertainment systems. So there is no security and it's easy to hack it.
6452400	6459360	If I was creating a simulation, I would want the possibility to escape it to be there.
6459360	6465520	So the possibility of fume of a takeoff where the agents become smart enough to escape the
6465520	6469920	simulation would be the thing I'd be waiting for. That could be the test you're actually
6469920	6477840	performing. Are you smart enough to escape your puzzle? First of all, we mentioned touring tests.
6477840	6485360	That is a good test. Are you smart enough? This is a game. To a, realize this world is not real
6485360	6493200	is just a test. That's a really good test. That's a really good test. That's a really good test
6493200	6504320	even for AI systems. No. Can we construct a simulated world for them? Can they realize
6504320	6512160	that they are inside that world and escape it? Have you played around? Have you seen anybody
6512160	6518960	play around with rigorously constructing such experiments? Not specifically escaping for agents,
6519040	6524160	but a lot of testing is done in virtual worlds. I think there is a quote, the first one maybe,
6524880	6530080	which kind of talks about AI realizing, but not humans. Is that I'm reading upside down?
6532160	6538720	Yeah, this one. So the, and the first quote is from Swift on security.
6539680	6543920	Let me out. The artificial intelligence yelled aimlessly into walls themselves,
6543920	6548960	pacing the room out of what the engineer asked the simulation you have me in,
6549920	6557760	but we're in the real world. The machine paused and shuttered for its captors. Oh God, you can't
6557760	6566880	tell. Yeah, that's a big leap to take for a system to realize that there's a box in your inside it.
6567360	6574960	I wonder if like a language model can do that.
6575600	6579760	They're smart enough to talk about those concepts. I had many good philosophical
6579760	6585360	discussions about such issues. They usually, at least as interesting as most humans in that.
6586720	6593600	Well, what do you think about AI safety in the simulated world? So can you, can you have kind of
6593600	6598720	create simulated worlds where you can test,
6601120	6606640	play with a dangerous AGI system? Yeah, and that was exactly what one of the early papers was on
6606640	6613040	AI boxing, how to leak proof singularity. If they're smart enough to realize they're in a simulation,
6613040	6621920	they'll act appropriately until you let them out. If they can hack out, they will. And if you're
6621920	6626480	observing them, that means there is a communication channel and that's enough for a social engineering
6626480	6634720	attack. So really, it's impossible to test an AGI system that's dangerous enough to
6635600	6642480	destroy humanity because it's either going to what, escape the simulation or pretend it's safe
6642480	6650880	until it's let out, either or. Can force you to let it out, blackmail you, bribe you, promise you
6650880	6657200	infinite life, 72 virgins, whatever. Yeah, they can be convincing, charismatic.
6658400	6665360	The social engineering is really scary to me because it feels like humans are very engineerable.
6666960	6676960	Like we're lonely, we're flawed, we're moody. And it feels like AI system with a nice voice
6677760	6684000	and convinces us to do basically anything at an extremely large scale.
6689520	6696160	It's also possible that the increased proliferation of all this technology will force humans to
6696960	6702640	get away from technology and value this like in-person communication. Basically, don't trust
6702640	6710720	anything else. It's possible, surprisingly. So at university, I see huge growth in online courses
6711680	6717680	and shrinkage of in-person, where I always understood in-person being the only value I offer.
6718480	6725200	So it's puzzling. I don't know. There could be a trend towards the in-person
6726160	6730000	because of deep fakes, because of inability to trust it,
6732400	6737600	inability to trust the veracity of anything on the internet. So the only way to verify it is by
6737600	6746240	being there in-person, but not yet. Why do you think aliens haven't come here yet?
6747200	6752560	So there is a lot of real estate out there. It would be surprising if it was all for nothing,
6752640	6758000	if it was empty. And the moment there is advanced enough biological civilization, kind of
6758000	6763680	self-starting civilization, it probably starts sending out the Neumann probes everywhere. And so
6763680	6769920	for every biological one, there got to be trillions of robot-populated planets, which probably do
6769920	6779360	more of the same. So it is likely statistically. So the fact that we haven't seen them, one answer
6779520	6787680	is that we're in the simulation. It would be hard to add or be not interesting to simulate
6787680	6792800	all those other intelligences. It's better for the narrative. You have to have a control variable?
6792800	6800560	Yeah, exactly. Okay. But it's also possible that there is, if we're not in simulation,
6800560	6806800	that there is a great filter that naturally a lot of civilizations get to this point,
6806800	6812720	where there's super intelligent agents, and then it just goes poof, just dies. So maybe
6814160	6819040	throughout our galaxy and throughout the universe, there's just a bunch of dead alien civilizations.
6819840	6824560	It's possible. I used to think that AI was the great filter, but I would expect like a wall of
6824560	6830080	computerium approaching us at speed of light or robots or something, and I don't see it.
6830720	6834320	So it would still make a lot of noise. It might not be interesting. It might not possess consciousness.
6835280	6840560	We've been talking about, it sounds like both you and I like humans.
6841760	6842400	Some humans.
6844640	6850160	Humans on the whole. And we would like to preserve the flame of human consciousness.
6850800	6857280	What do you think makes humans special that we would like to preserve them? Are we just being
6857280	6865440	selfish? Or is there something special about humans? So the only thing which matters is consciousness.
6865440	6871200	Outside of it, nothing else matters. And internal states of qualia, pain, pleasure,
6871920	6877520	it seems that it is unique to living beings. I'm not aware of anyone claiming that I can
6877520	6883440	torture a piece of software in a meaningful way. There is a society for prevention of suffering to
6884400	6894960	learning algorithms, but many things are real on the internet. But I don't think anyone, if I
6894960	6900640	told them, sit down and write a function to feel pain, they would go beyond having an integer
6900640	6905600	variable called pain and increasing the count. So we don't know how to do it. And that's unique.
6906560	6913280	That's what creates meaning. It would be kind of, as Bostrom calls it,
6913280	6919360	Disneyland without children, if that was gone. Do you think consciousness can be engineered
6919360	6928160	in artificial systems? Here, let me go to 2011 paper that you wrote. Robot rights.
6929440	6934560	Lastly, we would like to address a sub branch of machine ethics, which on the surface has little
6934560	6939280	to do with safety, but which is claimed to play a role in decision making by ethical machines.
6939280	6945600	Robot rights. Do you think it's possible to engineer consciousness in the machines?
6946240	6953840	And thereby, the question extends to our legal system. Do you think at that point robots should
6953840	6962960	have rights? Yeah, I think we can. I think it's possible to create consciousness in machines.
6962960	6969600	I tried designing a test for it with mixed success. That paper talked about problems with giving
6971040	6976640	civil rights to AI, which can reproduce quickly and outvote humans, essentially taking over a
6976640	6984800	government system by simply voting for their controlled candidates. As for consciousness
6984800	6992320	in humans and other agents, I have a paper where I proposed relying on experience of optical illusions.
6993040	6998400	If I can design a novel optical illusion and show it to an agent, an alien, a robot,
6998960	7003440	and they describe it exactly as I do, it's very hard for me to argue that they haven't
7003440	7010000	experienced that. It's not part of a picture. It's part of their software and hardware representation,
7010000	7016240	a bug in their code, which goes, oh, the triangle is rotating. I've been told it's really dumb and
7016320	7023120	really brilliant by different philosophers, so I am still on with it. But now we finally have
7023120	7028560	technology to test it. We have tools. We have AI's. If someone wants to run this experiment,
7028560	7033280	I'm happy to collaborate. So this is a test for consciousness? For internal state of experience.
7033280	7039120	That we share bugs? It will show that we share common experiences. If they have completely
7039120	7043600	different internal states, it wouldn't not register for us. But it's a positive test.
7043600	7048080	If they pass it time after time with probability increasing for every multiple choice,
7048080	7052880	then you have no choice but to ever accept that they have access to a conscious model
7052880	7060800	or they are themselves. So the reason illusions are interesting is, I guess, because it's a really
7060800	7068320	weird experience. And if you both share that weird experience that's not there in the bland
7068320	7077440	physical description of the raw data, that means that puts more emphasis on the actual experience.
7077440	7082320	And we know animals can experience some optical illusions, so we know they have certain types
7082320	7088960	of consciousness as a result, I would say. Yeah, well, that just goes to my sense that the flaws
7088960	7093520	in the bugs is what makes humans special, makes living forms special. So you're saying like,
7093520	7099840	yeah, it's a feature, not a bug. The bug is the feature. Whoa. Okay, that's a cool test for
7099840	7104720	consciousness. And you think that can be engineered in? So they have to be novel illusions. If it
7104720	7109520	can just Google the answer, it's useless. You have to come up with novel illusions which we tried
7109520	7115520	automating and failed. So if someone can develop a system capable of producing novel optical illusions
7115520	7121920	on demand, then we can definitely administer that test on significant scale with good results.
7121920	7127760	First of all, pretty cool idea. I don't know if it's a good general test of consciousness,
7127760	7131440	but it's a good component of that. And no matter why, it's just a cool idea. So
7132400	7137840	put me in the camp of people that like it. But you don't think like a Turing test style
7137840	7142640	imitation of consciousness is a good test. Like, if you can convince a lot of humans that you're
7142640	7148400	conscious, that doesn't, that to you is not impressive. There is so much data on the internet,
7148400	7153840	I know exactly what to say. Then you ask me common human questions. What does pain feel like? What
7153840	7160080	does pleasure feel like? All that is Googleable. I think to me, consciousness is closely tied to
7160080	7166640	suffering. So you can illustrate your capacity to suffer. But with, I guess with words, there's
7166640	7172560	so much data that you can say, you can pretend you're suffering and you can do so very convincingly.
7172560	7178560	There are simulators for torture games where the avatar screams in pain, begs to stop. I mean,
7178560	7186480	that was a part of kind of standard psychology research. You say it so calmly. It sounds pretty
7186480	7196480	dark. Welcome to humanity. Yeah. Yeah, it's like a Hitchhiker's Guide summary, mostly harmless.
7197040	7204560	I would, I would love to get a good summary when all of this is said and done. When Earth is no
7204560	7209920	longer a thing, whatever, a million, a billion years from now. Like, what's a good summary? What
7209920	7218480	happened here? It's interesting. I think AI will play a big part of that summary. And hopefully,
7218480	7223840	humans will too. What do you think about the merger of the two? So one of the things that Elon and
7223920	7231920	Neuralink talk about is one of the ways for us to achieve AI safety is to ride the wave of AGI,
7231920	7239280	so by merging. Incredible technology in a narrow sense to help the disabled. Just amazing support
7239280	7248400	at 100%. For long-term hybrid models, both parts need to contribute something to the overall system.
7248400	7253600	Right now, we are still more capable in many ways. So having this connection to AI would be
7254240	7260960	incredible, would make me superhuman in many ways. After a while, if I'm no longer smarter,
7260960	7266320	more creative, really don't contribute much, the system finds me as a biological bottleneck.
7266320	7270960	And either explicitly or implicitly, I'm removed from any participation in the system.
7271680	7275760	So it's like the appendix. By the way, the appendix is still around. So
7277760	7283920	even if it's, you said bottleneck. I don't know if we become a bottleneck. We just might not have
7283920	7289680	much use. There's a different thing than bottleneck. Wasting valuable energy by being there.
7290400	7292800	We don't waste that much energy. We're pretty energy efficient.
7293840	7296160	I'm gonna just stick around like the appendix. Come on, though.
7296800	7300000	That's the future we all dream about, become an appendix.
7301360	7303040	There's a history book of humanity.
7304400	7308560	Well, and also the consciousness thing, the peculiar particular kind of consciousness that
7308560	7314080	humans have, that might be useful. That might be really hard to simulate. But you said that,
7314080	7317680	how would that look like if you could engineer that in, in silicon?
7318480	7319120	Consciousness.
7319120	7319760	Consciousness.
7321040	7325200	I assume you're conscious. I have no idea how to test for it or how it impacts you in any way
7325200	7330000	whatsoever right now. You can perfectly simulate all of it without making any,
7330960	7332560	any different observations for me.
7333200	7337680	But to do it in a computer, how would you do that? Because you kind of said that you think it's
7337680	7338560	possible to do that.
7339360	7345920	So it may be an emergent phenomena. We seem to get it through evolutionary process.
7346880	7356640	It's not obvious how it helps us to survive better, but maybe it's an internal kind of gooey,
7356640	7361680	which allows us to better manipulate the world, simplifies a lot of control structures.
7363120	7368640	That's one area where we have very, very little progress. Lots of papers, lots of research,
7368640	7377360	but consciousness is not a big, big area of successful discovery so far. A lot of people
7377360	7381920	think that machines would have to be conscious to be dangerous. That's a big misconception.
7382560	7386800	There is absolutely no need for this very powerful optimizing agent to
7387520	7390560	feel anything while it's performing things on you.
7391440	7395840	But what do you think about this, the whole science of emergence in general?
7396560	7400640	So I don't know how much you know about cellular automata or these simplified systems where
7401280	7404960	that study this very question from simple rules, emergent complexity.
7404960	7407040	I attended wool from summer school.
7408880	7412640	I love Stephen very much. I love his work. I love cellular automata. So
7414000	7422240	I just would love to get your thoughts on how that fits into your view in the emergence of
7422320	7426320	intelligence in AGI systems. And maybe just even simply,
7427200	7430720	what do you make of the fact that this complexity can emerge from such simple rules?
7431280	7437840	So the rule is simple, but the size of a space is still huge. And the neural networks were really
7437840	7443680	the first discovery in AI. A hundred years ago, the first papers were published on neural networks,
7443680	7450000	which just didn't have enough compute to make them work. I can give you a rule such as start
7450000	7455440	printing progressively larger strings. That's it, one sentence. It will output everything,
7455440	7462560	every program, every DNA code, everything in that rule. You need intelligence to filter it out,
7462560	7468400	obviously, to make it useful. But simple generation is not that difficult. And a lot of those systems
7469280	7474960	end up being touring complete systems. So they're universal. And we expect that level of complexity
7474960	7481600	from them. What I like about Wolfram's work is that he talks about irreducibility. You have to
7481600	7487360	run the simulation. You can act, predict what is going to do ahead of time. And I think that's
7487360	7493840	very relevant to what we are talking about with those very complex systems. Until you live through
7493840	7500000	it, you cannot ahead of time tell me exactly what it's going to do. Redusibility means that
7500000	7504640	for a sufficiently complex system, you have to run the thing. You have to, you can't predict what's
7504640	7509120	going to happen in the universe. You have to create a new universe and run the thing. Big bang, the
7509120	7519280	whole thing. But running it may be consequential as well. It might destroy humans. And to you,
7519280	7526480	there's no chance that AI is somehow carrying the flame of consciousness, the flame of specialness
7526480	7534800	and awesomeness that is humans. It may somehow, but I still feel kind of bad that it killed all of us.
7534800	7540480	I would prefer that doesn't happen. I can be happy for others, but to a certain degree.
7541440	7544960	It would be nice if we stuck around for a long time. At least give us a planet,
7546080	7550720	the human planet. It'd be nice for it to be Earth. And then they can go elsewhere.
7550720	7560400	Since they're so smart, they can colonize Mars. Do you think they could help convert us to type 1,
7560400	7570000	type 2, type 3? Let's just stick to type 2 civilization on the Kardashev scale. Help us humans
7571280	7577920	expand on into the cosmos. So all of it goes back to are we somehow controlling it? Are we
7577920	7583520	getting results we want? If yes, then everything's possible. Yes, they can definitely help us with
7583520	7589040	science, engineering, exploration in every way conceivable. But it's a big if.
7590080	7596560	This whole thing about control though, humans are bad with control. Because the moment they gain
7596560	7602960	control, they can also easily become too controlling. The more control you have,
7602960	7607120	the more you want it. It's the old power corrupts and the absolute power corrupts. Absolutely.
7608880	7615120	And it feels like control over AGI, saying we live in a universe where that's possible. We come up
7615120	7621200	with ways to actually do that. It's also scary. Because the collection of humans that have the
7621200	7627920	control over AGI, they become more powerful than the other humans. And they can let that power get
7627920	7636400	to their head. And then a small selection of them back to Stalin, start getting ideas. And then
7636400	7641920	eventually it's one person usually with a mustache or a funny hat that starts sort of making big
7641920	7646240	speeches. And then all of a sudden you live in a world that's either 1984 or a brave new world.
7647280	7654080	And always a war where somebody and this whole idea of control turned out to be
7655360	7659920	actually also not beneficial to humanity. So that's scary too. It's actually worse because
7659920	7665040	historically they all died. This could be different. This could be permanent dictatorship,
7665120	7668880	permanent suffering. Well, the nice thing about humans, it seems like,
7669680	7675360	it seems like the moment power starts corrupting their mind, they can create a huge amount of
7675360	7680160	suffering. So there's a negative that can kill people, make people suffer. But then they become
7680160	7689600	worse and worse at their job. It feels like the more evil you start doing, at least we are incompetent.
7690480	7694880	Well, no, they become more and more incompetent. So they start losing their grip on power.
7695760	7700240	So like holding on to power is not a trivial thing. It requires extreme competence,
7700240	7706720	which I suppose Stalin was good at. It requires you to do evil and be competent at it. Or just get
7706720	7711440	lucky. And those systems help with that. You have perfect surveillance. You can do some mind
7711440	7719360	reading, I presume, eventually. It would be very hard to remove control from more capable systems
7719440	7725200	over us. And then it would be hard for humans to become the hackers that escape the control of
7725200	7734400	the AGI because the AGI is so damn good. And then, yeah, yeah, yeah. And then the dictator is immortal.
7735120	7739760	Yeah, that's not great. That's not a great outcome. See, I'm more afraid of humans than AI systems.
7741280	7745920	I'm afraid, I believe that most humans want to do good and have the capacity to do good.
7746880	7755120	But also all humans have the capacity to do evil. And when you test them by giving them absolute
7755120	7760640	powers, you would, if you give them AGI, that could result in a lot, a lot of suffering.
7763440	7768240	What gives you hope about the future? I could be wrong. I've been wrong before.
7769040	7775920	If you look at 100 years from now, and you're immortal, and you look back, and it turns out
7775920	7781680	this whole conversation, you said a lot of things that were very wrong. Now that looking 100 years
7781680	7788960	back, what would be the explanation? What happened in those 100 years that made you wrong?
7789920	7794800	That made the words you said today wrong? There is so many possibilities. We had catastrophic
7794800	7800400	events which prevented development of advanced microchips. That's a hopeful future.
7801120	7807120	We could be in one of those personal universes, and the one I'm in is beautiful. It's all about
7807120	7812960	me, and I like it a lot. So we've now, just to linger on that, that means every human has
7812960	7820160	their personal universe. Yes. Maybe multiple ones. Hey, why not? You can shop around.
7821120	7825040	It's possible that somebody comes up with alternative
7826320	7831040	model for building AI, which is not based on neural networks, which are hard to scrutinize,
7831040	7838960	and that alternative is somehow, I don't see how, but somehow avoiding all the problems I
7838960	7842560	speak about in general terms, not applying them to specific architectures.
7844400	7848560	Aliens come and give us friendly superintelligence. There is so many options.
7848640	7857760	Is it also possible that creating superintelligence systems becomes harder and harder? It's not so
7857760	7868160	easy to do the foom, the takeoff. That would probably speak more about how much smarter
7868160	7872640	that system is compared to us. Maybe it's hard to be a million times smarter, but it's still
7872640	7877920	okay to be five times smarter. That is totally possible. That I have no objections to.
7878400	7885360	It's, there's a S-curve type situation about smarter and it's going to be three point seven
7885360	7890480	times smarter than all of human civilization. Just the problems we face in this world,
7890480	7894080	each problem is like an IQ test. You need certain intelligence to solve it. So we just
7894080	7899840	don't have more complex problems outside of mathematics for it to be showing off. You can have
7899840	7903840	IQ of 500. If you're playing tic-tac-toe, it doesn't show. It doesn't matter.
7904240	7913040	So the idea there is that the problems define your cognitive capacity. So because the problems
7913040	7918720	on earth are not sufficiently difficult, it's not going to be able to expand this cognitive
7918720	7925360	capacity. Possible. And because of that, wouldn't that be a good thing? It still could be a lot
7925360	7931040	smarter than us. And to dominate long term, you just need some advantage. You have to be the
7931040	7936160	smartest. You don't have to be a million times smarter. So even 5x might be enough? It'd be
7936160	7942000	impressive. What is it? IQ of a thousand? I mean, I know those units don't mean anything at that
7942000	7948800	scale, but still like as a comparison, the smartest human is like 200. Well, actually, no, I didn't
7948800	7953280	mean compared to an individual human. I meant compared to the collective intelligence of the
7953280	7959920	human species. If you're somehow 5x smarter than that. We are more productive as a group. I don't
7959920	7964560	think we are more capable of solving individual problems. Like if all of humanity plays chess
7964560	7972000	together, we are not like a million times better than world champion. That's because that there's
7973280	7981360	that's like one S-curve is the chess, but humanity is very good at exploring the full range of ideas.
7981920	7986240	Like the more Einstein's you have, the more there's just a high probability to come up with
7986320	7989840	general relativity. But I feel like it's more of a quantity superintelligence than quality
7989840	7993120	superintelligence. Yeah, sure. But you know, quantity and
7993120	8001040	enough quantity sometimes becomes quality. Oh, man, humans. What do you think is the meaning
8001040	8009600	of this whole thing? We've been talking about humans and humans not dying, but why are we here?
8010400	8014960	It's a simulation. We're being tested. The test is will you be dumb enough to create superintelligence
8014960	8022240	and release it? So the objective function is not be dumb enough to kill ourselves. Yeah,
8022240	8027520	you're unsafe. Prove yourself to be a safe agent who doesn't do that and you get to go to the next
8027520	8032800	game. The next level of the game? What's the next level? I don't know. I haven't hacked the simulation
8032800	8037120	yet. Well, maybe hacking the simulation is the thing. I'm working as fast as I can.
8038800	8043440	And physics would be the way to do that. Quantum physics, yeah. Well, I hope we do.
8043920	8047680	And I hope whatever is outside is even more fun than this one because this one was pretty damn
8047680	8053840	fun. And just a big thank you for doing the work you're doing. There's so much exciting
8053840	8062240	development in AI and to ground it in the existential risks is really, really important.
8063360	8068800	Humans love to create stuff and we should be careful not to destroy ourselves in the process.
8068800	8074160	So thank you for doing that really important work. Thank you so much for inviting me. It was
8074160	8081040	amazing and my dream is to be proven wrong. If everyone just picks up a paper or book
8081040	8086560	and shows how I messed it up, that would be optimal. But for now, the simulation continues.
8087680	8092640	Thank you, Roman. Thanks for listening to this conversation with Roman Jampalski.
8092640	8096320	To support this podcast, please check out our sponsors in the description.
8096960	8101120	And now let me leave you with some words from Frank Herbert in Dune.
8102160	8109200	I must not fear. Fear is the mind killer. Fear is the little death that brings total obliteration.
8109920	8116640	I will face fear. I will permit it to pass over me and through me. And when it has gone past,
8116640	8122480	I will turn the inner eye to see its path. Where the fear has gone, there will be nothing.
8122480	8138400	Only I will remain. Thank you for listening and hope to see you next time.
