1
00:00:00,000 --> 00:00:06,560
If we create general superintelligences, I don't see a good outcome long-term for humanity.

2
00:00:06,560 --> 00:00:11,520
So there is X-risk, existential risk, everyone's dead. There is S-risk,

3
00:00:11,520 --> 00:00:17,040
suffering risks, where everyone wishes they were dead. We have also idea for I-risk,

4
00:00:17,040 --> 00:00:22,560
Ikigai risks, where we lost our meaning. The systems can be more creative,

5
00:00:22,560 --> 00:00:27,360
they can do all the jobs. It's not obvious what you have to contribute to a world where

6
00:00:27,920 --> 00:00:33,520
superintelligence exists. Of course, you can have all the variants you mentioned, where we are

7
00:00:33,520 --> 00:00:37,920
safe, we are kept alive, but we are not in control, we are not deciding anything,

8
00:00:37,920 --> 00:00:44,960
we are like animals in a zoo. There is, again, possibilities we can come up with as very smart

9
00:00:44,960 --> 00:00:51,280
humans, and then possibilities something a thousand times smarter can come up with for reasons we

10
00:00:51,280 --> 00:00:59,520
cannot comprehend. The following is a conversation with Roman Yumpolski, an AI safety and security

11
00:00:59,520 --> 00:01:07,040
researcher and author of a new book titled AI, Unexplainable, Unpredictable, Uncontrollable.

12
00:01:07,840 --> 00:01:13,600
He argues that there's almost 100% chance that AGI will eventually destroy human civilization.

13
00:01:14,240 --> 00:01:20,560
As an aside, let me say that I will have many, often technical conversations on the topic of AI,

14
00:01:21,520 --> 00:01:27,440
often with engineers building the state-of-the-art AI systems. I would say those folks put the

15
00:01:27,440 --> 00:01:35,280
infamous P-DOOM or the probability of AGI killing all humans at around 1-20%, but it's also important

16
00:01:35,280 --> 00:01:44,240
to talk to folks who put that value at 70, 80, 90, and in the case of Roman at 99.99 and many more

17
00:01:44,240 --> 00:01:51,920
90%. I'm personally excited for the future and believe it will be a good one, in part because

18
00:01:51,920 --> 00:01:59,280
of the amazing technological innovation we humans create, but we must absolutely not do so with

19
00:01:59,280 --> 00:02:05,840
blinders on, ignoring the possible risks, including existential risks of those technologies.

20
00:02:06,880 --> 00:02:12,960
That's what this conversation is about. This is the Lex Friedman podcast. To support it,

21
00:02:12,960 --> 00:02:19,120
please check out our sponsors in the description. Now, dear friends, here's Roman Jampalski.

22
00:02:20,240 --> 00:02:26,160
What to you is the probability that superintelligent AI will destroy all human civilization?

23
00:02:26,160 --> 00:02:27,440
What's the time frame?

24
00:02:27,440 --> 00:02:29,840
Let's say 100 years. In the next 100 years.

25
00:02:29,840 --> 00:02:36,000
So the problem of controlling AGI or superintelligence, in my opinion, is like

26
00:02:37,040 --> 00:02:42,640
a problem of creating a perpetual safety machine, by analogy with perpetual motion machine.

27
00:02:43,120 --> 00:02:51,040
It's impossible. Yeah, we may succeed and do a good job with GPT-5, 6, 7, but they just keep

28
00:02:52,160 --> 00:02:59,040
improving, learning, eventually self-modifying, interacting with the environment, interacting

29
00:02:59,040 --> 00:03:07,360
with malevolent actors. The difference between cybersecurity, narrow AI safety, and safety for

30
00:03:08,080 --> 00:03:11,360
general AI for superintelligence is that we don't get a second chance.

31
00:03:12,080 --> 00:03:15,440
With cybersecurity, somebody hacks your account. What's the big deal? You get a new

32
00:03:15,440 --> 00:03:21,600
password, new credit card, you move on. Here, if we're talking about existential risks,

33
00:03:21,600 --> 00:03:26,960
you only get one chance. So you're really asking me, what are the chances that we'll create

34
00:03:26,960 --> 00:03:34,080
the most complex software ever on the first try with zero bugs, and it will continue have zero

35
00:03:34,080 --> 00:03:41,440
bugs for 100 years or more? So there is an incremental improvement

36
00:03:42,400 --> 00:03:48,720
of systems leading up to AGI. To you, it doesn't matter if we can keep those safe.

37
00:03:48,720 --> 00:03:55,440
There's going to be one level of system at which you cannot possibly control it.

38
00:03:56,000 --> 00:04:04,160
I don't think we so far have made any system safe. At the level of capability they display,

39
00:04:04,800 --> 00:04:11,760
they already have made mistakes. We had accidents, they've been jailbroken. I don't think there is a

40
00:04:11,760 --> 00:04:19,040
single large language model today which no one was successful at making do something developers

41
00:04:19,040 --> 00:04:24,640
didn't intend it to do. But there's a difference between getting it to do something unintended,

42
00:04:24,640 --> 00:04:29,200
getting it to do something that's painful, costly, destructive, and something that's

43
00:04:29,200 --> 00:04:34,480
destructive to the level of hurting billions of people, or hundreds of millions of people,

44
00:04:34,480 --> 00:04:38,240
billions of people, or the entirety of human civilization. That's a big leap.

45
00:04:39,200 --> 00:04:44,880
Exactly. But the systems we have today have capability of causing X amount of damage.

46
00:04:44,880 --> 00:04:49,040
So when they fail, that's all we get. If we develop systems capable of

47
00:04:49,760 --> 00:04:55,040
impacting all of humanity, all of universe, the damage is proportionate.

48
00:04:55,920 --> 00:05:02,880
What do you, are the possible ways that such kind of mass murder of humans can happen?

49
00:05:03,600 --> 00:05:07,920
It's always a wonderful question. So one of the chapters in my new book is about

50
00:05:07,920 --> 00:05:12,880
unpredictability. I argue that we cannot predict what a smarter system will do.

51
00:05:12,880 --> 00:05:17,920
So you're really not asking me how superintelligence will kill everyone, you're asking me how I would

52
00:05:17,920 --> 00:05:22,480
do it. And I think it's not that interesting. I can tell you about the standard, you know,

53
00:05:22,480 --> 00:05:27,920
nanotech, synthetic bio nuclear. Superintelligence will come up with something completely new,

54
00:05:27,920 --> 00:05:35,440
completely super. We may not even recognize that as a possible path to achieve that goal.

55
00:05:36,000 --> 00:05:41,600
So there is like an unlimited level of creativity in terms of how humans could be killed.

56
00:05:42,240 --> 00:05:49,600
But, you know, we could still investigate possible ways of doing it, not how to do it, but the,

57
00:05:49,600 --> 00:05:54,400
at the end, what is the methodology that does it, you know, shutting off the power.

58
00:05:55,200 --> 00:05:59,680
And then humans start killing each other, maybe because the resources are really constrained,

59
00:05:59,680 --> 00:06:03,200
that they're, and then there's the actual use of weapons like nuclear weapons or

60
00:06:04,160 --> 00:06:10,400
developing artificial pathogens, viruses, that kind of stuff. We could still kind of think

61
00:06:11,280 --> 00:06:16,800
through that and defend against it, right? There's a ceiling to the creativity of mass

62
00:06:16,800 --> 00:06:19,840
murder of humans here, right? The options are limited.

63
00:06:20,880 --> 00:06:26,080
They are limited by how imaginative we are. If you are that much smarter, that much more creative,

64
00:06:26,080 --> 00:06:31,360
you are capable of thinking across multiple domains, do novel research in physics and biology,

65
00:06:31,360 --> 00:06:35,840
you may not be limited by those tools. If squirrels were planning to kill humans,

66
00:06:35,920 --> 00:06:41,280
they would have a set of possible ways of doing it, but they would never consider things we can

67
00:06:41,280 --> 00:06:45,440
come up with. So are you thinking about mass murder and destruction of human civilization?

68
00:06:45,440 --> 00:06:50,160
Are you thinking of with squirrels, you put them in a zoo and they don't really know they're in a zoo?

69
00:06:50,720 --> 00:06:53,200
If we just look at the entire set of undesirable trajectories,

70
00:06:54,160 --> 00:06:59,360
majority of them are not going to be death. Most of them are going to be just like

71
00:07:00,320 --> 00:07:08,880
things like Brave New World where the squirrels are fed dopamine and they're all doing some kind

72
00:07:08,880 --> 00:07:16,400
of fun activity and the fire, the soul of humanity is lost because of the drug that's fed to it,

73
00:07:16,400 --> 00:07:22,160
or literally in a zoo. We're in a zoo, we're doing our thing, we're playing a game of Sims

74
00:07:23,120 --> 00:07:30,000
and the actual players playing that game are AI systems. Those are all undesirable because of the

75
00:07:30,000 --> 00:07:36,560
free will. The fire of human consciousness is dimmed through that process, but it's not killing

76
00:07:36,560 --> 00:07:45,040
humans. So are you thinking about that or is the biggest concern, literally, the extinctions of humans?

77
00:07:45,040 --> 00:07:50,640
I think about a lot of things. So there is X-risk, existential risk, everyone's dead.

78
00:07:50,720 --> 00:07:56,720
There is S-risk, suffering risks, where everyone wishes they were dead. We have also idea for

79
00:07:56,720 --> 00:08:03,600
I-risk, Ikigai risks, where we lost our meaning. The systems can be more creative, they can do all

80
00:08:03,600 --> 00:08:09,920
the jobs. It's not obvious what you have to contribute to a world where superintelligence exists.

81
00:08:09,920 --> 00:08:15,360
Of course, you can have all the variants you mentioned where we are safe, we are kept alive,

82
00:08:15,360 --> 00:08:19,760
but we are not in control. We are not deciding anything. We are like animals in a zoo.

83
00:08:21,120 --> 00:08:28,000
Again, possibilities we can come up with as very smart humans and then possibilities something

84
00:08:28,800 --> 00:08:33,120
a thousand times smarter can come up with. Four reasons we cannot comprehend.

85
00:08:33,120 --> 00:08:36,960
I would love to sort of dig into each of those. X-risk, S-risk, and I-risk.

86
00:08:39,040 --> 00:08:41,520
Can you like linger on I-risk? What is that?

87
00:08:42,560 --> 00:08:49,040
Japanese concept of Ikigai, you find something which allows you to make money. You are good at

88
00:08:49,360 --> 00:08:53,680
it and the society says, we need it. You have this awesome job, you are a

89
00:08:53,680 --> 00:08:59,760
podcaster, gives you a lot of meaning, you have a good life, I assume, you're happy.

90
00:09:01,360 --> 00:09:07,120
That's what we want most people to find, to have. For many intellectuals, it is their

91
00:09:07,120 --> 00:09:12,640
occupation which gives them a lot of meaning. I am a researcher, philosopher, scholar,

92
00:09:13,360 --> 00:09:19,840
that means something to me. In a world where an artist is not feeling appreciated because his

93
00:09:19,840 --> 00:09:28,560
art is just not competitive with what is produced by machines or a writer or scientist, we'll lose

94
00:09:28,560 --> 00:09:34,240
a lot of that. At the lower level, we're talking about complete technological unemployment.

95
00:09:34,800 --> 00:09:40,160
We're not losing 10% of jobs, we're losing all jobs. What do people do with all that free time?

96
00:09:40,240 --> 00:09:47,680
What happens then? Everything society is built on is completely modified in one generation.

97
00:09:47,680 --> 00:09:54,160
It's not a slow process where we get to figure out how to live that new lifestyle, but it's

98
00:09:54,880 --> 00:10:00,400
pretty quick. In that world, humans can't do what humans currently do with chess,

99
00:10:00,400 --> 00:10:07,840
play each other, have tournaments. Even though AI systems are far superior this time in chess,

100
00:10:07,920 --> 00:10:14,080
we just create artificial games. For us, they're real, like the Olympics. We do all kinds of

101
00:10:14,080 --> 00:10:24,480
different competitions and have fun, maximize the fun and let the AI focus on the productivity.

102
00:10:24,480 --> 00:10:29,440
It's an option. I have a paper where I try to solve the value alignment problem for multiple

103
00:10:29,440 --> 00:10:35,360
agents. The solution to avoid compromises is to give everyone a personal virtual universe.

104
00:10:35,360 --> 00:10:39,360
You can do whatever you want in that world. You could be king, you could be slave,

105
00:10:39,360 --> 00:10:44,480
you decide what happens. It's basically a glorified video game where you get to enjoy

106
00:10:44,480 --> 00:10:50,720
yourself and someone else takes care of your needs and the substrate alignment is the only

107
00:10:50,720 --> 00:10:55,120
thing we need to solve. We don't have to get 8 billion humans to agree on anything.

108
00:10:57,200 --> 00:11:02,640
Why is that not a likely outcome? Why can't AI systems create video games for us

109
00:11:03,600 --> 00:11:07,840
to lose ourselves in each with an individual video game universe?

110
00:11:08,400 --> 00:11:11,040
Some people say that's what happened in a simulation.

111
00:11:11,840 --> 00:11:18,160
And we're playing that video game and now we're creating what? Maybe we're creating

112
00:11:18,160 --> 00:11:22,560
artificial threats for ourselves to be scared about because fear is really exciting. It allows

113
00:11:22,560 --> 00:11:28,480
us to play the video game more vigorously. Some people choose to play on a more difficult level

114
00:11:28,560 --> 00:11:33,520
with more constraints. Some say, okay, I'm just going to enjoy the game, high privilege level.

115
00:11:34,160 --> 00:11:38,560
Absolutely. So, okay, what was that paper on multi-agent value alignment?

116
00:11:38,560 --> 00:11:45,920
Personal universes. Personal universes. So, that's one of the possible outcomes. But what in general

117
00:11:45,920 --> 00:11:50,960
is the idea of the paper? So, it's looking at multiple agents. They're human AI, like a hybrid

118
00:11:50,960 --> 00:11:55,760
system where there's humans and AI's? Or is it looking at humans or just intelligent agents?

119
00:11:55,760 --> 00:12:00,800
In order to solve value alignment problem, I'm trying to formalize it a little better.

120
00:12:00,800 --> 00:12:06,160
Usually, we're talking about getting AI's to do what we want, which is not well-defined.

121
00:12:06,160 --> 00:12:12,560
Are we talking about creator of a system, owner of that AI, humanity as a whole? But we don't

122
00:12:12,560 --> 00:12:18,720
agree on much. There is no universally accepted ethics, morals across cultures, religions.

123
00:12:19,280 --> 00:12:22,640
People have individually very different preferences politically and such. So,

124
00:12:23,280 --> 00:12:29,120
even if we somehow managed all the other aspects of it, programming those fuzzy concepts and

125
00:12:29,120 --> 00:12:35,040
getting AI to follow them closely, we don't agree on what to program in. So, my solution was,

126
00:12:35,040 --> 00:12:38,640
okay, we don't have to compromise on room temperature. You have your universe, I have mine,

127
00:12:39,280 --> 00:12:44,000
whatever you want. And if you like me, you can invite me to visit your universe. We don't have

128
00:12:44,000 --> 00:12:49,200
to be independent. But the point is you can be. And virtual reality is getting pretty good. It's

129
00:12:49,200 --> 00:12:53,280
going to hit a point where you can't tell the difference. And if you can't tell if it's real

130
00:12:53,280 --> 00:12:59,440
or not, what's the difference? So, basically, give up on value alignment. Create an entire,

131
00:12:59,440 --> 00:13:04,640
it's like the multiverse theory. This is creating an entire universe for you with your values.

132
00:13:04,640 --> 00:13:09,360
You still have to align with that individual. They have to be happy in that simulation.

133
00:13:09,360 --> 00:13:14,640
But it's a much easier problem to align with one agent versus eight billion agents plus animals,

134
00:13:14,640 --> 00:13:19,680
aliens. So, you convert the multi-agent problem into a single-agent problem?

135
00:13:19,680 --> 00:13:27,040
I'm trying to do that, yeah. Okay. Is there any way to, so, okay, that's giving up on them,

136
00:13:27,920 --> 00:13:33,680
on the value alignment problem. Well, is there any way to solve the value alignment problem

137
00:13:33,680 --> 00:13:39,200
where there's a bunch of humans, multiple humans, tens of humans or eight billion humans that have

138
00:13:39,200 --> 00:13:46,240
very different set of values? It seems contradictory. I haven't seen anyone explain what it means

139
00:13:46,240 --> 00:13:54,640
outside of words which pack a lot, make it good, make it desirable, make it something they don't

140
00:13:54,640 --> 00:13:59,680
regret. But how do you specifically formalize those notions? How do you program them in?

141
00:13:59,680 --> 00:14:05,440
I haven't seen anyone make progress on that so far. But isn't that the whole optimization

142
00:14:05,440 --> 00:14:09,520
journey that we're doing as a human civilization? We're looking at geopolitics.

143
00:14:10,640 --> 00:14:16,160
Nations are in a state of anarchy with each other. They start wars, there's conflict,

144
00:14:18,240 --> 00:14:23,440
and oftentimes they have very different views of what is good and what is evil.

145
00:14:23,440 --> 00:14:28,160
There's not what we're trying to figure out, just together trying to converge towards that.

146
00:14:28,160 --> 00:14:31,680
So, we're essentially trying to solve the value alignment problem with humans.

147
00:14:32,320 --> 00:14:36,640
The examples you gave, some of them are, for example, two different religions saying,

148
00:14:36,640 --> 00:14:43,840
this is our holy site and we are not willing to compromise it in any way. If you can make two

149
00:14:43,840 --> 00:14:47,440
holy sites in virtual worlds, you solve the problem. But if you only have one, it's not

150
00:14:47,440 --> 00:14:52,240
divisible, you're stuck there. But what if we want to be at tension with each other?

151
00:14:53,520 --> 00:14:59,680
Through that tension, we understand ourselves and we understand the world. That's the intellectual

152
00:14:59,680 --> 00:15:06,880
journey we're on as a human civilization. We create intellectual and physical conflict,

153
00:15:06,880 --> 00:15:12,000
and through that, figure stuff out. If we go back to that idea of simulation and this is an

154
00:15:12,000 --> 00:15:17,200
entertainment kind of giving meaning to us, the question is, how much suffering is reasonable

155
00:15:17,200 --> 00:15:22,560
for a video game? So, yeah, I don't mind a video game where I get haptic feedback,

156
00:15:22,560 --> 00:15:28,000
that is a little bit of shaking, maybe I'm a little scared. I don't want a game where kids

157
00:15:28,000 --> 00:15:33,920
are tortured, literally. That seems unethical, at least by our human standards.

158
00:15:34,720 --> 00:15:38,800
Are you suggesting it's possible to remove suffering if we're looking at human civilization

159
00:15:38,800 --> 00:15:45,120
as an optimization problem? So, we know there are some humans who, because of a mutation,

160
00:15:45,120 --> 00:15:52,640
don't experience physical pain. So, at least physical pain can be mutated out, re-engineered

161
00:15:52,640 --> 00:15:58,160
out. Suffering, in terms of meaning, like you burned the only copy of my book, is a little

162
00:15:58,160 --> 00:16:04,080
harder, but even there, you can manipulate your hedonic set point, you can change defaults,

163
00:16:04,080 --> 00:16:09,200
you can reset. Problem with that is, if you start messing with your reward channel,

164
00:16:09,200 --> 00:16:15,680
you start wire-heading and end up bleasing out a little too much.

165
00:16:15,680 --> 00:16:21,280
Well, that's the question. Would you really want to live in a world where there's no suffering

166
00:16:21,280 --> 00:16:28,880
as a dark question? Is there some level of suffering that reminds us of what this is all for?

167
00:16:29,600 --> 00:16:34,800
I think we need that, but I would change the overall range. So, right now, it's negative

168
00:16:34,800 --> 00:16:39,680
infinity to kind of positive infinity, pain, pleasure, access. I would make it like zero to

169
00:16:39,680 --> 00:16:43,120
positive infinity, and being unhappy is like, I'm close to zero.

170
00:16:44,160 --> 00:16:49,280
Okay, so what's the S-risk? What are the possible things that you're imagining with S-risk? So,

171
00:16:50,160 --> 00:16:54,960
mass suffering of humans, what are we talking about there caused by AGI?

172
00:16:54,960 --> 00:17:01,440
So, there are many malevolent actors. We can talk about psychopaths, crazies, hackers,

173
00:17:01,440 --> 00:17:07,360
doomsday cults. We know from history, they tried killing everyone. They tried on purpose to cause

174
00:17:07,360 --> 00:17:14,080
maximum amount of damage, terrorism. What if someone malevolent wants on purpose to torture all humans

175
00:17:14,080 --> 00:17:21,680
as long as possible? You solve aging, so now you have functional immortality, and you just

176
00:17:22,400 --> 00:17:27,200
try to be as creative as you can. Do you think there is actually people in human history that

177
00:17:27,200 --> 00:17:33,040
tried to literally maximize human suffering? It's just studying people who have done evil in the

178
00:17:33,040 --> 00:17:38,480
world. It seems that they think that they're doing good, and it doesn't seem like they're trying to

179
00:17:38,480 --> 00:17:46,080
maximize suffering. They just cause a lot of suffering as a side effect of doing what they

180
00:17:46,080 --> 00:17:52,800
think is good. So, there are different malevolent agents. Some may be just gaining personal benefit

181
00:17:52,800 --> 00:17:58,880
and sacrificing others to that cause. Others, we know for a fact that trying to kill as many people

182
00:17:58,880 --> 00:18:04,000
as possible. When we look at recent school shootings, if they had more capable weapons,

183
00:18:04,000 --> 00:18:08,000
they would take out not dozens, but thousands, millions, billions.

184
00:18:13,840 --> 00:18:20,640
Well, we don't know that, but that is a terrifying possibility, and we don't want to find out.

185
00:18:21,920 --> 00:18:28,400
Like if terrorists had access to nuclear weapons, how far would they go? Is there a limit to what

186
00:18:28,400 --> 00:18:36,560
they're willing to do? In your sense, is there some malevolent actors where there's no limit?

187
00:18:36,560 --> 00:18:47,600
There is mental diseases where people don't have empathy, don't have this human quality of

188
00:18:47,600 --> 00:18:52,160
understanding suffering in ours. And then there's also a set of beliefs where you think you're doing

189
00:18:52,160 --> 00:18:59,920
good by killing a lot of humans. Again, I would like to assume that normal people never think

190
00:18:59,920 --> 00:19:06,560
like that. It's always some sort of psychopaths, but yeah. And to you, AGI systems can carry that

191
00:19:07,520 --> 00:19:14,240
and be more competent at executing that. They can certainly be more creative. They can understand

192
00:19:14,320 --> 00:19:22,000
human biology better, understand our molecular structure, genome. Again, a lot of times

193
00:19:22,880 --> 00:19:28,480
torture ends, then individual dies. That limit can be removed as well.

194
00:19:28,480 --> 00:19:33,440
So if we're actually looking at X-risk and S-risk as the systems get more and more intelligent,

195
00:19:33,440 --> 00:19:39,680
don't you think it's possible to anticipate the ways they can do it and defend against it?

196
00:19:39,680 --> 00:19:42,480
Like we do with cybersecurity, we do security systems?

197
00:19:43,200 --> 00:19:49,440
Right. We can definitely keep up for a while. I'm saying you cannot do it indefinitely. At some

198
00:19:49,440 --> 00:19:58,320
point, the cognitive gap is too big. The surface you have to defend is infinite, but attackers

199
00:19:58,320 --> 00:20:04,480
only need to find one exploit. So to you, eventually, this is heading off a cliff?

200
00:20:05,200 --> 00:20:11,520
If we create general super intelligences, I don't see a good outcome long-term for humanity.

201
00:20:11,520 --> 00:20:14,000
The only way to win this game is not to play it.

202
00:20:14,000 --> 00:20:18,320
Okay. Well, we'll talk about possible solutions and what not playing it means.

203
00:20:19,760 --> 00:20:22,960
But what are the possible timelines here to you? What are we talking about?

204
00:20:23,520 --> 00:20:27,520
We're talking about a set of years, decades, centuries. What do you think?

205
00:20:27,520 --> 00:20:32,640
I don't know for sure. The prediction markets right now are saying 2026 for AGI.

206
00:20:33,360 --> 00:20:38,480
I heard the same thing from CEO of Anthropic, DeepMind, so maybe we're two years away,

207
00:20:39,040 --> 00:20:45,440
which seems very soon given we don't have a working safety mechanism in place or even a

208
00:20:45,440 --> 00:20:50,000
prototype for one. And there are people trying to accelerate those timelines because they feel

209
00:20:50,000 --> 00:20:54,560
we're not getting there quick enough. Well, what do you think they mean when they say AGI?

210
00:20:55,120 --> 00:21:00,080
So the definitions we used to have, and people are modifying them a little bit lately,

211
00:21:00,160 --> 00:21:05,760
artificial general intelligence was a system capable of performing in any domain a human

212
00:21:05,760 --> 00:21:12,080
could perform. So you're creating this average artificial person. They can do cognitive labor,

213
00:21:12,080 --> 00:21:17,440
physical labor, where you can get another human to do it. Superintelligence was defined as a system

214
00:21:17,440 --> 00:21:23,600
which is superior to all humans in all domains. Now people are starting to refer to AGI as if

215
00:21:23,600 --> 00:21:29,280
it's superintelligence. I made a post recently where I argued, for me at least, if you average

216
00:21:29,280 --> 00:21:34,960
out over all the common human tasks, those systems are already smarter than an average human.

217
00:21:35,840 --> 00:21:41,120
So under that definition, we have it. Shane Lake has this definition of where

218
00:21:41,120 --> 00:21:46,240
you're trying to win in all domains. That's what intelligence is. Now are they smarter than

219
00:21:46,800 --> 00:21:51,120
elite individuals in certain domains? Of course not. They're not there yet. But

220
00:21:52,160 --> 00:21:56,800
the progress is exponential. See, I'm much more concerned about social engineering.

221
00:21:59,680 --> 00:22:06,480
To me, AI's ability to do something in the physical world, like the lowest hanging fruit,

222
00:22:07,120 --> 00:22:14,960
the easiest set of methods is by just getting humans to do it. It's going to be much harder to

223
00:22:17,040 --> 00:22:19,840
be the kind of viruses that take over the minds of robots,

224
00:22:20,800 --> 00:22:24,640
that where the robots are executing the commands. It just seems like humans, social

225
00:22:24,640 --> 00:22:29,440
engineering of humans is much more likely. That would be enough to bootstrap the whole process.

226
00:22:30,960 --> 00:22:37,200
Okay. Just to linger on the term AGI, what do you use the difference in AGI and human level

227
00:22:37,200 --> 00:22:44,480
intelligence? Human level is general in the domain of expertise of humans. We know how to do human

228
00:22:44,480 --> 00:22:49,760
things. I don't speak dog language. I should be able to pick it up if I'm a general intelligence.

229
00:22:49,840 --> 00:22:55,280
It's inferior animal. I should be able to learn that skill, but I can't. At general

230
00:22:55,280 --> 00:23:00,080
intelligence, truly universal general intelligence should be able to do things like that humans cannot

231
00:23:00,080 --> 00:23:05,360
do. To be able to talk to animals, for example. To solve pattern recognition problems of that type,

232
00:23:07,360 --> 00:23:14,800
to have similar things outside of our domain of expertise because it's just not the world we live in.

233
00:23:15,760 --> 00:23:22,240
If we just look at the space of cognitive abilities we have, I just would love to understand

234
00:23:22,240 --> 00:23:28,560
what the limits are beyond which an AGI system can reach. What does that look like? What about

235
00:23:29,280 --> 00:23:36,640
actual mathematical thinking or scientific innovation? That kind of stuff.

236
00:23:37,520 --> 00:23:42,480
We know calculators are smarter than humans in that narrow domain of addition.

237
00:23:43,280 --> 00:23:52,160
But is it humans plus tools versus AGI or just human, raw human intelligence? Because humans

238
00:23:52,160 --> 00:23:56,560
create tools and with the tools they become more intelligent. There's a gray area there,

239
00:23:56,560 --> 00:23:59,840
what it means to be human when we're measuring their intelligence.

240
00:23:59,840 --> 00:24:04,880
So when I think about it, I usually think human with a paper and a pencil, not human with internet

241
00:24:04,880 --> 00:24:10,400
and other AI helping. But is that a fair way to think about it? Because isn't there another

242
00:24:10,400 --> 00:24:14,160
definition of human-level intelligence that includes the tools that humans create?

243
00:24:14,160 --> 00:24:19,280
But we create AI. So at any point, you'll still just add superintelligence to human capability?

244
00:24:19,280 --> 00:24:26,720
That seems like cheating. No, controllable tools. There is an implied leap that you're making

245
00:24:28,080 --> 00:24:36,320
when AGI goes from tool to entity that can make its own decisions. So if we define human-level

246
00:24:36,320 --> 00:24:40,320
intelligence as everything a human can do with fully controllable tools.

247
00:24:40,960 --> 00:24:45,200
It seems like a hybrid of some kind. You're now doing brain-computer interfaces. You're

248
00:24:45,200 --> 00:24:50,640
connecting it to maybe narrow AIs yet definitely increases our capabilities.

249
00:24:51,520 --> 00:25:00,080
So what's a good test to you that measures whether an artificial intelligence system has

250
00:25:00,080 --> 00:25:06,160
reached human-level intelligence and was a good test where it has superseded human-level

251
00:25:06,160 --> 00:25:12,800
intelligence to reach that land of AGI? I am old fashioned. I like Turing test. I have a paper

252
00:25:12,800 --> 00:25:18,560
where I equate passing Turing test to solving AI complete problems because you can encode any

253
00:25:18,560 --> 00:25:23,360
questions about any domain into the Turing test. You don't have to talk about how was your day.

254
00:25:23,360 --> 00:25:30,640
You can ask anything and so the system has to be as smart as a human to pass it in a true sense.

255
00:25:30,640 --> 00:25:36,880
But then you would extend that to maybe a very long conversation. I think the Alexa prize was

256
00:25:36,880 --> 00:25:42,480
doing that. Basically, can you do a 20-minute, 30-minute conversation with an AI system?

257
00:25:42,480 --> 00:25:49,040
It has to be long enough to where you can make some meaningful decisions about capabilities

258
00:25:49,040 --> 00:25:54,320
absolutely. You can brute force very short conversations. So literally, what does that

259
00:25:54,320 --> 00:26:06,080
look like? Can we construct formally a kind of test that tests for AGI? For AGI, it has to be

260
00:26:06,080 --> 00:26:12,000
there. I cannot give it a task. I can give to a human and it cannot do it if a human can.

261
00:26:13,040 --> 00:26:18,160
For superintelligence, it would be superior on all such tasks. Not just average performance,

262
00:26:18,800 --> 00:26:22,720
go learn to drive car, go speak Chinese, play guitar. Okay, great.

263
00:26:22,720 --> 00:26:29,920
I guess the the follow-on question is there a test for the kind of AGI that would be

264
00:26:31,600 --> 00:26:39,040
susceptible to lead to S-risk or X-risk, susceptible to destroy human civilization?

265
00:26:39,040 --> 00:26:45,280
Like is there a test for that? You can develop a test which will give you positives if it lies to

266
00:26:45,280 --> 00:26:50,160
you or has those ideas. You cannot develop a test which rules them out. There is always

267
00:26:50,160 --> 00:26:56,400
possibility of what Bostrom calls a treacherous turn where later on a system decides for game

268
00:26:56,400 --> 00:27:02,880
theoretic reasons, economic reasons to change its behavior. And we see the same with humans.

269
00:27:02,880 --> 00:27:09,840
It's not unique to AI. For millennia, we tried developing models, ethics, religions, lie detector

270
00:27:09,840 --> 00:27:17,440
tests and then employees betray the employers, spouses betray family. It's a pretty standard thing

271
00:27:17,440 --> 00:27:24,720
intelligent agents sometimes do. So is it possible to detect when AI system is lying or deceiving you?

272
00:27:24,720 --> 00:27:30,720
If you know the truth and it tells you something false, you can detect that but you cannot know

273
00:27:30,720 --> 00:27:37,680
in general every single time. And again, the system you're testing today may not be lying.

274
00:27:37,680 --> 00:27:44,560
The system you're testing today may know you are testing it and so behaving and later on after it

275
00:27:45,200 --> 00:27:50,640
interacts with the environment, interacts with other systems, malevolent agents, learns more,

276
00:27:50,640 --> 00:27:56,400
it may start doing those things. So do you think it's possible to develop a system where the creators

277
00:27:56,400 --> 00:28:02,240
of the system, the developers, the programmers don't know that it's deceiving them?

278
00:28:03,120 --> 00:28:09,200
So systems today don't have long-term planning. That is not our... They can lie today if it

279
00:28:09,200 --> 00:28:16,720
optimizes, helps them optimize the reward. If they realize, okay, this human will be very happy if I

280
00:28:16,720 --> 00:28:23,920
tell them the following, they will do it if it brings them more points. And they don't have to

281
00:28:24,720 --> 00:28:29,520
kind of keep track of it. It's just the right answer to this problem every single time.

282
00:28:30,400 --> 00:28:36,080
At which point is somebody creating that intentionally, not unintentionally, intentionally

283
00:28:36,080 --> 00:28:40,640
creating an AI system that's doing long-term planning with an objective function that's

284
00:28:40,640 --> 00:28:46,800
defined by the AI system, not by a human? Well, some people think that if they're that smart,

285
00:28:46,800 --> 00:28:51,760
they're always good. They really do believe that. It's just benevolence from intelligence,

286
00:28:51,760 --> 00:28:59,040
so they'll always want what's best for us. Some people think that they will be able to detect

287
00:28:59,520 --> 00:29:05,840
problem behaviors and correct them at the time when we get there. I don't think it's a good idea.

288
00:29:05,840 --> 00:29:12,480
I am strongly against it, but yeah, there are quite a few people who in general are so optimistic

289
00:29:12,480 --> 00:29:17,600
about this technology. It could do no wrong. They want it developed as soon as possible,

290
00:29:17,600 --> 00:29:24,400
as capable as possible. So there's going to be people who believe the more intelligent it is,

291
00:29:24,400 --> 00:29:28,160
the more benevolent, and so therefore it should be the one that defines the objective function,

292
00:29:28,160 --> 00:29:31,200
that it's optimizing when it's doing long-term planning.

293
00:29:31,200 --> 00:29:36,800
There are even people who say, okay, what's so special about humans, right? We removed the

294
00:29:36,800 --> 00:29:42,560
gender bias. We're removing race bias. Why is this pro-human bias? We are polluting the planet.

295
00:29:43,200 --> 00:29:48,000
As you said, you know, fight a lot of wars, kind of violent. Maybe it's better if this super

296
00:29:48,000 --> 00:29:55,440
intelligent, perfect society comes and replaces us. It's normal stage in the evolution of our

297
00:29:55,440 --> 00:30:02,560
species. Yeah, so somebody says, let's develop an AI system that removes the violent humans

298
00:30:03,360 --> 00:30:07,360
from the world. And then it turns out that all humans have violence in them,

299
00:30:07,360 --> 00:30:13,200
or the capacity for violence, and therefore all humans are removed. Yeah, yeah, yeah.

300
00:30:14,560 --> 00:30:20,960
Let me ask about Yan Likun. He's somebody who you've had a few exchanges with,

301
00:30:21,920 --> 00:30:27,680
and he's somebody who actively pushes back against this view that AI is going to lead

302
00:30:27,680 --> 00:30:40,480
to destruction of human civilization, also known as AI-dumerism. So in one example that he tweeted,

303
00:30:40,480 --> 00:30:47,120
he said, I do acknowledge risks, but two points. One, open research and open source of the best

304
00:30:47,120 --> 00:30:53,840
ways to understand and mitigate the risks. And two, AI is not something that just happens. We

305
00:30:53,840 --> 00:31:00,720
build it. We have agency in what it becomes. Hence, we control the risks. We meaning humans.

306
00:31:00,720 --> 00:31:07,200
It's not some sort of natural phenomena that we have no control over. So can you make the case

307
00:31:07,200 --> 00:31:11,840
that he's right, and can you try to make the case that he's wrong? I cannot make a case that he's

308
00:31:11,840 --> 00:31:17,600
right. He's wrong in so many ways. It's difficult for me to remember all of them. He's a Facebook

309
00:31:17,600 --> 00:31:23,360
buddy, so I have a lot of fun having those little debates with him. So I'm trying to remember the

310
00:31:23,360 --> 00:31:31,120
arguments. So one, he says we are not gifted this intelligence from aliens. We are designing it. We

311
00:31:31,120 --> 00:31:38,320
are making decisions about it. That's not true. It was true when we had expert systems, symbolic AI,

312
00:31:38,320 --> 00:31:44,560
decision trees. Today, you set up parameters for a model and you water this plant. You give it data,

313
00:31:44,560 --> 00:31:50,320
you give it compute, and it grows. And after it's finished growing into this alien plant,

314
00:31:50,320 --> 00:31:55,920
you start testing it to find out what capabilities it has. And it takes years to figure out even

315
00:31:55,920 --> 00:32:00,800
for existing models. If it's trained for six months, it will take you two, three years to figure out

316
00:32:00,800 --> 00:32:06,800
basic capabilities of that system. We still discover new capabilities in systems which are already out

317
00:32:06,800 --> 00:32:13,120
there. So that's not the case. So just to link on that, to give you the difference there, there is

318
00:32:13,120 --> 00:32:20,240
some level of emergent intelligence that happens in our current approaches. So stuff that we don't

319
00:32:20,240 --> 00:32:26,880
hard code in. Absolutely. That's what makes it so successful. Then we had to painstakingly hard

320
00:32:26,880 --> 00:32:33,360
code in everything. We didn't have much progress. Now, just spend more money and more compute and

321
00:32:33,360 --> 00:32:38,560
it's a lot more capable. And then the question is, when there is emergent intelligent phenomena,

322
00:32:39,600 --> 00:32:45,760
what is the ceiling of that? For you, there's no ceiling. For Yanlacun, I think there's a kind

323
00:32:45,760 --> 00:32:51,360
of ceiling that happens that we have full control over. Even if we don't understand the internals

324
00:32:51,360 --> 00:32:57,520
of the emergence, how the emergence happens, there's a sense that we have control and an

325
00:32:57,520 --> 00:33:03,360
understanding of the approximate ceiling of capability, the limits of the capability.

326
00:33:04,000 --> 00:33:10,640
Let's say there is a ceiling. It's not guaranteed to be at the level which is competitive with us.

327
00:33:10,640 --> 00:33:18,960
It may be greatly superior to ours. So what about his statement about open research and open source

328
00:33:18,960 --> 00:33:23,440
are the best ways to understand and mitigate the risks? Historically, he's completely right.

329
00:33:23,440 --> 00:33:28,240
Open source software is wonderful. It's tested by the community. It's debugged,

330
00:33:28,240 --> 00:33:34,560
but we're switching from tools to agents. Now you're giving open source weapons to psychopaths.

331
00:33:34,560 --> 00:33:41,680
Do we want open source nuclear weapons, biological weapons? It's not safe to give technology so

332
00:33:41,680 --> 00:33:48,240
powerful to those who may misalign it, even if you are successful at somehow getting it to work

333
00:33:48,240 --> 00:33:52,720
in the first place in a friendly manner. But the difference with nuclear weapons,

334
00:33:52,720 --> 00:33:58,720
current AI systems are not akin to nuclear weapons. So the idea there is you're open sourcing it at

335
00:33:58,720 --> 00:34:03,920
this stage that you can understand it better. A large number of people can explore the limitation

336
00:34:03,920 --> 00:34:10,080
of capabilities, explore the possible ways to keep it safe, to keep it secure, all that kind of stuff

337
00:34:10,080 --> 00:34:15,280
while it's not at the stage of nuclear weapons. In nuclear weapons, there's a no-nuclear weapon

338
00:34:15,280 --> 00:34:20,320
and then there's a nuclear weapon. With AI systems, there's a gradual improvement of capability

339
00:34:20,320 --> 00:34:26,960
and you get to perform that improvement incrementally. So open source allows you to study

340
00:34:28,800 --> 00:34:35,040
how things go wrong, study the very process of emergence, study AI safety and those systems

341
00:34:35,040 --> 00:34:40,160
when there's not a high level of danger, all that kind of stuff. It also sets a very wrong

342
00:34:40,160 --> 00:34:46,080
precedent. So we open sourced model 1, model 2, model 3, nothing ever bad happened, so obviously

343
00:34:46,080 --> 00:34:51,440
we're going to do it with model 4. It's just gradual improvement. I don't think it always works

344
00:34:51,440 --> 00:34:58,640
with the precedent. You're not stuck doing it the way you always did. It's just a precedent

345
00:34:59,200 --> 00:35:05,200
of open research and open development such that we get to learn together and then the first time

346
00:35:05,200 --> 00:35:11,840
there's a sign of danger, some dramatic thing happened, not a thing that destroys human civilization,

347
00:35:11,840 --> 00:35:19,200
but some dramatic demonstration of capability that can legitimately lead to a lot of damage,

348
00:35:19,200 --> 00:35:23,360
then everybody wakes up and says, okay, we need to regulate this, we need to come up with safety

349
00:35:23,360 --> 00:35:29,520
mechanism that stops this. But at this time, maybe you can educate me, but I haven't seen any

350
00:35:29,520 --> 00:35:35,360
illustration of significant damage done by intelligent AI systems. So I have a paper

351
00:35:35,920 --> 00:35:40,960
which collects accidents through history of AI and they always are proportionate to capabilities

352
00:35:40,960 --> 00:35:47,200
of that system. So if you have tic-tac-toe playing AI, it will fail to properly play and lose the

353
00:35:47,200 --> 00:35:53,600
game, which it should draw, trivial. Your spell checker will misspell a word, so on. I stopped

354
00:35:53,600 --> 00:35:57,840
collecting those because there are just too many examples of the eyes failing at what they are

355
00:35:57,840 --> 00:36:05,120
capable of. We haven't had terrible accidents in the sense of billion people get killed, absolutely

356
00:36:05,120 --> 00:36:12,400
true. But in another paper, I argue that those accidents do not actually prevent people from

357
00:36:12,400 --> 00:36:20,080
continuing with research and actually they kind of serve like vaccines. A vaccine makes your body

358
00:36:20,080 --> 00:36:25,360
a little bit sick, so you can handle the big disease later, much better. It's the same here.

359
00:36:25,360 --> 00:36:29,200
People will point out, you know that accident, AI accident we had where 12 people died?

360
00:36:30,160 --> 00:36:36,160
Everyone's still here, 12 people is less than smoking kills. It's not a big deal, so we continue.

361
00:36:36,160 --> 00:36:42,160
So in a way, it will actually be kind of confirming that it's not that bad.

362
00:36:42,160 --> 00:36:48,480
It matters how the deaths happen. Whether it's literally murdered by an AI system,

363
00:36:48,480 --> 00:36:56,640
then one is a problem. But if it's accidents because of increased reliance on automation,

364
00:36:56,640 --> 00:37:05,520
for example, so when airplanes are flying in an automated way, maybe the number of plane crashes

365
00:37:05,520 --> 00:37:11,360
increased by 17% or something. And then you're like, okay, do we really want to rely on automation?

366
00:37:11,360 --> 00:37:15,680
I think in the case of automation, airplanes, it decreased significantly. Okay, same thing

367
00:37:15,680 --> 00:37:22,240
with autonomous vehicles. Like, okay, what are the pros and cons? What are the tradeoffs here?

368
00:37:22,240 --> 00:37:28,560
You can have that discussion in an honest way. But I think the kind of things we're talking about here

369
00:37:28,560 --> 00:37:38,160
is mass scale pain and suffering caused by AI systems. And I think we need to see illustrations

370
00:37:38,160 --> 00:37:44,560
of that on a very small scale to start to understand that this is really damaging versus

371
00:37:45,680 --> 00:37:50,000
clippy versus a tool that's really useful to a lot of people to do learning, to do

372
00:37:51,920 --> 00:37:56,960
summarization of texts, to do question answer, all that kind of stuff, to generate videos,

373
00:37:57,760 --> 00:38:03,520
the tool, fundamentally a tool versus an agent that can do a huge amount of damage.

374
00:38:03,520 --> 00:38:09,040
So you bring up an example of cars. Yes. Cars were slowly developed and integrated.

375
00:38:09,760 --> 00:38:14,640
If we had no cars and somebody came around and said, I invented this thing, it's called cars,

376
00:38:14,640 --> 00:38:21,600
it's awesome. It kills like 100,000 Americans every year. Let's deploy it. Would we deploy that?

377
00:38:22,480 --> 00:38:28,320
There have been fear mongering about cars for a long time. The transition from horse to the cars

378
00:38:28,320 --> 00:38:33,200
is a really nice channel that I recommend. People check out pessimists archive that

379
00:38:33,200 --> 00:38:37,280
documents all the fear mongering about technology that's happened throughout history.

380
00:38:37,280 --> 00:38:42,480
There's definitely been a lot of fear mongering about cars. There's a transition period there

381
00:38:42,480 --> 00:38:48,560
about cars, about how deadly they are. We can try. It took a very long time for cars to proliferate

382
00:38:48,560 --> 00:38:55,040
to the degree they have now. And then you could ask serious questions in terms of the miles traveled,

383
00:38:55,040 --> 00:38:59,760
the benefit to the economy, the benefit to the quality of life that cars do versus the number

384
00:38:59,760 --> 00:39:06,800
of deaths, 30, 40,000 in the United States. Are we willing to pay that price? I think most people

385
00:39:06,800 --> 00:39:14,800
when they're rationally thinking policy makers will say yes. We want to decrease it from 40,000

386
00:39:14,800 --> 00:39:20,400
to zero and do everything we can to decrease it. There's all kinds of policies and centers you

387
00:39:20,400 --> 00:39:26,480
can create to decrease the risks with the deployment of technology, but then you have to weigh the

388
00:39:26,480 --> 00:39:30,560
benefits and the risks of the technology. And the same thing would be done with AI.

389
00:39:31,280 --> 00:39:36,400
You need data. You need to know, but if I'm right and it's unpredictable, unexplainable,

390
00:39:36,400 --> 00:39:41,440
uncontrollable, you cannot make this decision. We're gaining $10 trillion of wealth,

391
00:39:41,440 --> 00:39:47,520
but we're losing, we don't know how many people. You basically have to perform an experiment

392
00:39:47,520 --> 00:39:52,800
on 8 billion humans without their consent. And even if they want to give you consent,

393
00:39:52,800 --> 00:39:57,360
they can't because they cannot give informed consent. They don't understand those things.

394
00:39:58,080 --> 00:40:03,360
Right. That happens when you go from the predictable to the unpredictable very quickly.

395
00:40:06,480 --> 00:40:12,320
But it's not obvious to me that AI systems would gain capability so quickly that you won't be able

396
00:40:12,320 --> 00:40:18,640
to collect enough data to study the benefits and the risks. We're literally doing it. The

397
00:40:18,640 --> 00:40:24,240
previous model we learned about after we finished training it, what it was capable of. Let's say

398
00:40:24,240 --> 00:40:30,560
we stopped GPT-4 training run around human capability. Hypothetically, we start training GPT-5

399
00:40:30,560 --> 00:40:36,080
and I have no knowledge of insider training runs or anything. And we started that point of about

400
00:40:36,080 --> 00:40:41,840
human and we train it for the next nine months. Maybe two months in, it becomes super intelligent.

401
00:40:41,840 --> 00:40:48,080
We continue training it. At the time when we started testing it, it is already a dangerous

402
00:40:48,080 --> 00:40:52,720
system. How dangerous? I have no idea. But neither people training it.

403
00:40:53,520 --> 00:40:58,960
At the training stage, but then there's a testing stage inside the company. They can

404
00:40:58,960 --> 00:41:04,080
start getting intuition about what the system is capable to do. You're saying that somehow from

405
00:41:04,080 --> 00:41:13,760
leap from GPT-4 to GPT-5 can happen the kind of leap where GPT-4 was controllable and GPT-5 is no

406
00:41:13,760 --> 00:41:20,080
longer controllable. And we get no insights from using GPT-4 about the fact that GPT-5 will be

407
00:41:20,080 --> 00:41:28,000
uncontrollable. That's the situation you're concerned about. Where their leap from M to M

408
00:41:28,000 --> 00:41:37,760
plus one would be such that an uncontrollable system is created without any ability for us to

409
00:41:37,760 --> 00:41:43,280
anticipate that. If we had capability of ahead of the run before the training run to register

410
00:41:43,280 --> 00:41:47,440
exactly what capabilities that next model will have at the end of the training run.

411
00:41:47,440 --> 00:41:52,080
And we accurately guessed all of them. I would say you're right. We can definitely go ahead with

412
00:41:52,080 --> 00:41:57,680
this run. We don't have that capability. From GPT-4, you can build up intuitions about what

413
00:41:57,680 --> 00:42:06,080
GPT-5 will be capable of. It's just incremental progress. Even if that's a big leap in capability,

414
00:42:06,640 --> 00:42:09,680
it just doesn't seem like you can take a leap from a system that's

415
00:42:10,640 --> 00:42:15,840
helping you write emails to a system that's going to destroy human civilization.

416
00:42:15,840 --> 00:42:21,600
It seems like it's always going to be sufficiently incremental such that we can anticipate the

417
00:42:21,600 --> 00:42:27,040
possible dangers. And we're not even talking about existential risks, but just the kind of damage

418
00:42:27,040 --> 00:42:32,240
you can do to civilization. It seems like we'll be able to anticipate the kinds, not the exact,

419
00:42:32,320 --> 00:42:42,560
but the kinds of risks it might lead to and then rapidly develop defenses ahead of time and as

420
00:42:43,200 --> 00:42:48,560
the risks emerge. We're not talking just about capabilities, specific tasks. We're talking

421
00:42:48,560 --> 00:42:55,600
about general capability to learn. Maybe like a child at the time of testing and deployment,

422
00:42:55,600 --> 00:43:03,200
it is still not extremely capable, but as it is exposed to more data, real world, it can be trained

423
00:43:03,200 --> 00:43:08,080
to become much more dangerous and capable. Let's focus then on the control problem.

424
00:43:11,120 --> 00:43:13,760
At which point does the system become uncontrollable?

425
00:43:15,040 --> 00:43:19,040
Why is it the more likely trajectory for you that the system becomes uncontrollable?

426
00:43:20,000 --> 00:43:26,080
I think at some point it becomes capable of getting out of control. For game-theoretic reasons,

427
00:43:26,080 --> 00:43:31,360
it may decide not to do anything right away and for a long time just collect more resources,

428
00:43:31,360 --> 00:43:38,640
accumulate strategic advantage. Right away, it may be kind of still young, weak superintelligence,

429
00:43:38,640 --> 00:43:44,240
give it a decade. It's in charge of a lot more resources. It had time to make backups,

430
00:43:44,240 --> 00:43:47,360
so it's not obvious to me that it will strike as soon as it can.

431
00:43:48,240 --> 00:43:57,040
Can we just try to imagine this future where there's an AI system that's capable of escaping

432
00:43:57,040 --> 00:44:05,200
the control of humans and then doesn't and waits? What's that look like? One, we have to rely on

433
00:44:05,200 --> 00:44:10,320
that system for a lot of the infrastructure. We'll have to give it access, not just to the internet,

434
00:44:11,280 --> 00:44:21,440
but to the task of managing power, government, economy, this kind of stuff. That just feels

435
00:44:21,440 --> 00:44:25,680
like a gradual process given the bureaucracies of all those systems involved. We've been doing

436
00:44:25,680 --> 00:44:31,200
it for years. Software controls all the systems, nuclear power plants, airline industry. It's

437
00:44:31,200 --> 00:44:35,520
all software-based. Every time there is electrical outage, I can't fly anywhere for days.

438
00:44:36,400 --> 00:44:42,880
But there's a difference between software and AI. There's different kinds of software.

439
00:44:42,880 --> 00:44:49,920
So to give a single AI system access to the control of airlines and the control of the economy,

440
00:44:51,040 --> 00:44:54,880
that's not a trivial transition for humanity.

441
00:44:54,880 --> 00:44:59,360
No, but if it shows it is safer, in fact, when it's in control, we get better results,

442
00:44:59,360 --> 00:45:02,400
people will demand that it was put in place. Absolutely.

443
00:45:02,400 --> 00:45:06,720
And if not, it can hack the system. It can use social engineering to get access to it.

444
00:45:06,720 --> 00:45:10,640
That's why I said it might take some time for it to accumulate those resources.

445
00:45:10,640 --> 00:45:16,000
It just feels like that would take a long time for either humans to trust it or for the social

446
00:45:16,000 --> 00:45:20,560
engineering to come into play. It's not a thing that happens overnight. It feels like something

447
00:45:20,560 --> 00:45:26,240
that happens across one or two decades. I really hope you're right, but it's not what I'm seeing.

448
00:45:26,240 --> 00:45:31,040
People are very quick to jump on a latest trend. Early adopters will be there before it's even

449
00:45:31,040 --> 00:45:37,520
deployed buying prototypes. Maybe the social engineering. Because for social engineering,

450
00:45:37,520 --> 00:45:43,840
AI systems don't need any hardware access. It's all software. So they can start manipulating you

451
00:45:43,840 --> 00:45:48,640
through social media and so on. You have AI assistants that are going to help you do a lot of

452
00:45:48,640 --> 00:45:54,560
manage a lot of your day to day, and then they start doing social engineering. But for a system

453
00:45:54,560 --> 00:46:02,640
that's so capable that it can escape the control of humans that created it, such a system being

454
00:46:02,640 --> 00:46:11,920
deployed at a mass scale and trusted by people to be deployed, it feels like that would take

455
00:46:11,920 --> 00:46:17,760
a lot of convincing. So we've been deploying systems which had hidden capabilities.

456
00:46:19,200 --> 00:46:23,920
Can you give an example? GPT-4. I don't know what else is capable of, but there are still

457
00:46:23,920 --> 00:46:28,240
things we haven't discovered, can do. There may be trivial proportion to its capability.

458
00:46:28,880 --> 00:46:35,360
I don't know. It writes Chinese poetry, hypothetical. I know it does. But we haven't tested

459
00:46:35,360 --> 00:46:41,920
for all possible capabilities, and we're not explicitly designing them. We can only rule

460
00:46:41,920 --> 00:46:48,640
out bugs we find. We cannot rule out bugs and capabilities because we haven't found them.

461
00:46:48,960 --> 00:46:58,560
Is it possible for a system to have hidden capabilities that are orders of magnitude

462
00:46:58,560 --> 00:47:03,920
greater than its non-hidden capabilities? This is the thing I'm really struggling with,

463
00:47:04,640 --> 00:47:12,640
where on the surface, the thing we understand it can do doesn't seem that harmful. So even if it

464
00:47:13,600 --> 00:47:21,040
has hidden capabilities like Chinese poetry or generating effective software viruses,

465
00:47:22,400 --> 00:47:30,560
the damage that can do seems like on the same order of magnitude as the capabilities that we

466
00:47:30,560 --> 00:47:37,040
know about. This idea that the hidden capabilities will include being uncontrollable is something

467
00:47:37,040 --> 00:47:41,920
I'm struggling with because GPT-4 on the surface seems to be very controllable.

468
00:47:41,920 --> 00:47:47,440
Again, we can only ask and test for things we know about. If there are unknown unknowns,

469
00:47:47,440 --> 00:47:53,040
we cannot do it. I'm thinking of humans, artistic savants. If you talk to a person like that,

470
00:47:53,040 --> 00:47:59,360
you may not even realize they can multiply 20-digit numbers in their head. You have to know to ask.

471
00:48:01,280 --> 00:48:06,480
As I mentioned, just to linger on the fear of the unknown,

472
00:48:06,800 --> 00:48:12,880
the pessimist archive has just documented, let's look at data of the past at history.

473
00:48:12,880 --> 00:48:17,360
There's been a lot of fear-mongering about technology. The pessimist archive does a

474
00:48:17,360 --> 00:48:23,760
really good job of documenting how crazily afraid we are of every piece of technology.

475
00:48:23,760 --> 00:48:29,120
We've been afraid, there's a blog post where Louis Anslow, who created the pessimist archive,

476
00:48:29,120 --> 00:48:34,560
writes about the fact that we've been fear-mongering about robots and automation for

477
00:48:35,360 --> 00:48:42,400
over 100 years. Why is AGI different than the kinds of technologies we've been afraid of in

478
00:48:42,400 --> 00:48:50,880
the past? Two things. One, we're switching from tools to agents. Tools don't have negative or

479
00:48:50,880 --> 00:48:57,120
positive impact. People using tools do. Guns don't kill. People with guns do.

480
00:48:57,840 --> 00:49:03,280
Agents can make their own decisions. They can be positive or negative. A pit bull can decide to

481
00:49:03,280 --> 00:49:10,160
harm you. That's an agent. The fears are the same. The only difference is now we have this

482
00:49:10,160 --> 00:49:15,760
technology. When they were afraid of humanoid robots 100 years ago, they had none. Today,

483
00:49:15,760 --> 00:49:20,240
every major company in the world is investing billions to create them. Not every, but you

484
00:49:20,240 --> 00:49:28,080
understand what I'm saying. It's very different. Well, agents, it depends on what you mean by

485
00:49:28,080 --> 00:49:32,880
the word agents. All those companies are not investing in a system that has the kind of agency

486
00:49:34,240 --> 00:49:39,760
that's implied by in the fears, where it can really make decisions on their own,

487
00:49:39,760 --> 00:49:44,800
that have no human in the loop. They are saying they're building super-intelligence

488
00:49:44,800 --> 00:49:49,280
and have a super-alignment team. You don't think they're trying to create a system smart enough

489
00:49:49,280 --> 00:49:54,880
to be an independent agent under that definition? I have not seen evidence of it. I think a lot of

490
00:49:54,880 --> 00:50:02,080
it is a marketing kind of discussion about the future. It's a mission about

491
00:50:03,280 --> 00:50:06,480
the kind of systems that can create in the long-term future, but in the short term,

492
00:50:07,040 --> 00:50:16,240
the kind of systems they're creating falls fully within the definition of narrow AI.

493
00:50:16,240 --> 00:50:21,920
These are tools that have increasing capabilities, but they just don't have a sense of agency or

494
00:50:21,920 --> 00:50:29,120
consciousness or self-awareness or ability to deceive at scales that would be required to do

495
00:50:30,080 --> 00:50:34,640
mass-scale suffering and murder of humans. Those systems are well beyond narrow AI.

496
00:50:34,640 --> 00:50:40,160
If you had to list all the capabilities of GPT-4, you would spend a lot of time writing that list.

497
00:50:40,160 --> 00:50:45,840
But agency is not one of them. Not yet, but do you think any of those companies are holding back

498
00:50:45,840 --> 00:50:50,720
because they think it may be not safe or are they developing the most capable system they can,

499
00:50:50,720 --> 00:50:55,280
given the resources and hoping they can control and monetize?

500
00:50:56,240 --> 00:51:01,680
Control and monetize. Hoping they can control and monetize. You're saying if they could

501
00:51:01,680 --> 00:51:08,080
press a button and create an agent that they no longer control, that they can have to ask nicely.

502
00:51:10,000 --> 00:51:14,880
A thing that lives on a server across a huge number of computers.

503
00:51:16,960 --> 00:51:21,280
You're saying that they would push for the creation of that kind of system?

504
00:51:22,240 --> 00:51:27,680
I can't speak for other people for all of them. I think some of them are very ambitious. They

505
00:51:27,680 --> 00:51:31,680
fundraise in trillions. They talk about controlling the light corner of the universe.

506
00:51:32,240 --> 00:51:33,760
I would guess that they might.

507
00:51:35,920 --> 00:51:39,280
Well, that's a human question. Whether humans are capable of that,

508
00:51:39,280 --> 00:51:44,480
probably some humans are capable of that. My more direct question is if it's possible to

509
00:51:44,480 --> 00:51:52,480
create such a system. I have a system that has that level of agency. I don't think that's an easy

510
00:51:52,480 --> 00:51:59,520
technical challenge. It doesn't feel like we're close to that. A system that has the kind of

511
00:51:59,520 --> 00:52:05,120
agency where it can make its own decisions and deceive everybody about them. The current architecture

512
00:52:06,160 --> 00:52:11,840
we have in machine learning and how we train the systems, how we deploy the systems and all that,

513
00:52:11,920 --> 00:52:14,080
it just doesn't seem to support that kind of agency.

514
00:52:14,640 --> 00:52:20,800
I really hope you're right. I think the scaling hypothesis is correct. We haven't seen diminishing

515
00:52:20,800 --> 00:52:28,080
returns. It used to be we asked how long before AGI. Now we should ask how much until AGI. It's

516
00:52:28,080 --> 00:52:32,720
trillion dollars today. It's a billion dollars next year. It's a million dollars in a few years.

517
00:52:33,760 --> 00:52:37,120
Don't you think it's possible to basically run out of trillions?

518
00:52:38,080 --> 00:52:40,160
So is this constrained by compute?

519
00:52:40,880 --> 00:52:43,760
Compute gets cheaper every day exponentially.

520
00:52:43,760 --> 00:52:46,640
But then that becomes a question of decades versus years.

521
00:52:47,280 --> 00:52:52,960
If the only disagreement is that it will take decades not years for everything I'm saying to

522
00:52:53,840 --> 00:52:55,920
materialize, then I can go with that.

523
00:52:57,360 --> 00:53:04,320
But if it takes decades, then the development of tools for AI safety becomes more and more

524
00:53:04,320 --> 00:53:11,280
realistic. So I guess the question is I have a fundamental belief that humans when faced with

525
00:53:11,280 --> 00:53:18,000
danger can come up with ways to defend against that danger. And one of the big problems facing AI

526
00:53:18,000 --> 00:53:24,560
safety currently for me is that there's not clear illustrations of what that danger looks like.

527
00:53:26,160 --> 00:53:29,200
There's no illustrations of AI systems doing a lot of damage.

528
00:53:30,160 --> 00:53:36,640
And so it's unclear what you're defending against because currently it's a philosophical notions that

529
00:53:36,640 --> 00:53:41,760
yes it's possible to imagine AI systems that take control of everything and then destroy all humans.

530
00:53:42,480 --> 00:53:48,720
It's also a more formal mathematical notion that you talk about that it's impossible to

531
00:53:48,720 --> 00:53:55,280
have a perfectly secure system. You can't you can't prove that a program of sufficient complexity

532
00:53:55,840 --> 00:54:02,640
is completely safe and perfect and know everything about it. Yes, but like when you actually just

533
00:54:02,640 --> 00:54:07,440
programmatically look how much damage have the AI systems done and what kind of damage

534
00:54:08,000 --> 00:54:13,600
there's not been illustrations of that. Even in the autonomous weapon systems

535
00:54:14,640 --> 00:54:18,400
there's not been mass deployments of autonomous weapon systems luckily.

536
00:54:19,120 --> 00:54:26,560
The automation in war currently is very limited. The that the automation is at the

537
00:54:26,560 --> 00:54:34,000
scale of individuals versus like at the scale of strategy and planning. So I think one of the

538
00:54:34,000 --> 00:54:41,120
challenges here is like where is the dangers and the intuition that Yalakuna and others have

539
00:54:41,120 --> 00:54:48,240
is let's keep in the open building AI systems until the dangers start rearing their heads

540
00:54:49,440 --> 00:54:59,360
and they become more explicit. They start being case studies illustrative case studies that show

541
00:54:59,360 --> 00:55:04,320
exactly how the damage by AI systems is done then regulation can step in then brilliant

542
00:55:04,320 --> 00:55:09,520
engineers can step up and we could have Manhattan style projects that defend against such systems.

543
00:55:10,080 --> 00:55:16,400
That's kind of the notion and I guess attention with that is the idea that for you we need to

544
00:55:16,400 --> 00:55:22,800
be thinking about that now so that we're ready because we will have not much time when the systems

545
00:55:22,800 --> 00:55:30,480
are deployed. Is that true? There is a lot to unpack here. There is a partnership on AI, a

546
00:55:30,480 --> 00:55:35,200
conglomerate of many large corporations. They have a database of AI accidents they collect.

547
00:55:35,200 --> 00:55:41,360
I contributed a lot to the database. If we so far made almost no progress in actually

548
00:55:41,360 --> 00:55:46,880
solving this problem, not patching it, not again lipstick and a pig kind of solutions,

549
00:55:47,840 --> 00:55:51,600
why would we think we'll do better than we closer to the problem?

550
00:55:53,120 --> 00:55:56,560
All the things you mentioned are serious concerns. Measuring the amount of harm,

551
00:55:56,560 --> 00:56:01,920
so benefit versus risk there is difficult but to you the sense is already the risk has superseded

552
00:56:01,920 --> 00:56:07,200
the benefit. Again, I want to be perfectly clear. I love AI. I love technology. I'm a computer

553
00:56:07,200 --> 00:56:11,440
scientist. I have PhD in engineering. I work at engineering school. There is a huge difference

554
00:56:11,440 --> 00:56:18,080
between we need to develop narrow AI systems, super intelligent in solving specific human

555
00:56:18,080 --> 00:56:24,320
problems like protein folding and let's create super intelligent machine got it and we'll decide

556
00:56:24,320 --> 00:56:31,520
what to do with us. Those are not the same. I am against the super intelligence in general sense

557
00:56:31,600 --> 00:56:39,520
with no undo button. Do you think the teams that are doing, they're able to do the AI safety on

558
00:56:39,520 --> 00:56:48,640
the kind of narrow AI risks that you've mentioned, are those approaches going to be at all productive

559
00:56:48,640 --> 00:56:54,320
towards leading to approaches of doing AI safety on AGI or is that just a fundamentally different

560
00:56:54,320 --> 00:56:59,520
partially but they don't scale for narrow AI for deterministic systems. You can test them. You

561
00:56:59,520 --> 00:57:05,120
have edge cases. You know what the answer should look like. You know the right answers for general

562
00:57:05,120 --> 00:57:12,080
systems. You have infinite test surface. You have no edge cases. You cannot even know what to test for.

563
00:57:12,720 --> 00:57:19,840
Again, the unknown unknowns are underappreciated by people looking at this problem. You are always

564
00:57:19,840 --> 00:57:26,000
asking me, how will it kill everyone? How will it will fail? The whole point is if I knew it,

565
00:57:26,000 --> 00:57:28,960
I would be super intelligent and despite what you might think, I'm not.

566
00:57:30,320 --> 00:57:33,360
So to you, the concern is that we would not be able to

567
00:57:35,680 --> 00:57:42,480
see early size of an uncontrollable system. It is a master at deception. Sam tweeted about how

568
00:57:42,480 --> 00:57:49,840
great it is at persuasion and we see it ourselves, especially now with voices with maybe kind of

569
00:57:49,840 --> 00:57:55,840
flirty sarcastic female voices. It's going to be very good at getting people to do things.

570
00:57:55,840 --> 00:58:04,080
But see, I'm very concerned about system being used to control the masses.

571
00:58:05,840 --> 00:58:10,880
But in that case, the developers know about the kind of control that's happening.

572
00:58:11,680 --> 00:58:17,520
You're more concerned about the next stage where even the developers don't know about the deception.

573
00:58:18,160 --> 00:58:23,920
Right. I don't think developers know everything about what they are creating. They have lots of

574
00:58:23,920 --> 00:58:29,680
great knowledge. We're making progress on explaining parts of the network. We can understand, okay,

575
00:58:29,680 --> 00:58:37,440
this node get excited when this input is presented, this cluster of nodes. But when nowhere near

576
00:58:38,000 --> 00:58:41,120
close to understanding the full picture and I think it's impossible,

577
00:58:41,840 --> 00:58:47,440
you need to be able to survey an explanation. The size of those models prevents a single human

578
00:58:47,440 --> 00:58:52,800
from observing all this information, even if provided by the system. So either we're getting

579
00:58:52,800 --> 00:58:58,400
model as an explanation for what's happening and that's not comprehensible to us, or we're getting

580
00:58:58,400 --> 00:59:04,240
a compressed explanation, lossy compression, where here's top 10 reasons you got fired.

581
00:59:05,120 --> 00:59:07,200
That's something, but it's not a full picture.

582
00:59:07,280 --> 00:59:12,720
You've given elsewhere an example of a child and everybody, all humans try to deceive. They try

583
00:59:12,720 --> 00:59:18,880
to lie early on in their life. I think we'll just get a lot of examples of deceptions from

584
00:59:18,880 --> 00:59:24,240
large language models or AI systems. They're going to be kind of shitty or they'll be pretty good,

585
00:59:24,240 --> 00:59:31,680
but we'll catch them off guard. We'll start to see the kind of momentum towards developing,

586
00:59:32,640 --> 00:59:38,560
increasing deception capabilities. And that's when you're like, okay, we need to do some kind of

587
00:59:38,560 --> 00:59:43,840
alignment that prevents deception. But then we'll have, if you support open source, then you can

588
00:59:43,840 --> 00:59:48,000
have open source models that have some level of deception. You can start to explore on a large

589
00:59:48,000 --> 00:59:54,320
scale. How do we stop it from being deceptive? Then there's a more explicit, pragmatic kind of

590
00:59:54,880 --> 01:00:03,920
problem to solve. How do we stop AI systems from trying to optimize for deception? That's just an

591
01:00:03,920 --> 01:00:09,920
example, right? So there is a paper, I think it came out last week by Dr. Parkadall from MIT, I

592
01:00:09,920 --> 01:00:17,040
think, and they showed that existing models already showed successful deception in what they do.

593
01:00:18,800 --> 01:00:23,920
My concern is not that they lie now and we need to catch them and tell them don't lie. My concern

594
01:00:23,920 --> 01:00:31,840
is that once they are capable and deployed, they will later change their mind because that's what

595
01:00:33,120 --> 01:00:38,240
unrestricted learning allows you to do. Lots of people grow up maybe in the religious family.

596
01:00:38,800 --> 01:00:45,120
They read some new books and they turn in their religion. That's a treacherous turn in humans.

597
01:00:45,760 --> 01:00:52,880
If you learn something new about your colleagues, maybe you'll change how you react to them.

598
01:00:53,520 --> 01:00:59,440
Yeah, a treacherous turn. If we just mention humans, Stalin and Hitler, there's a turn.

599
01:01:00,400 --> 01:01:06,400
Stalin is a good example. He just seems like a normal communist follower,

600
01:01:06,400 --> 01:01:13,360
Lenin, until there's a turn. There's a turn of what that means in terms of when he has complete

601
01:01:13,360 --> 01:01:17,520
control with what the execution of that policy means and how many people get to suffer.

602
01:01:17,520 --> 01:01:22,640
And you can't say they are not rational. The rational decision changes based on your position.

603
01:01:23,120 --> 01:01:27,280
Then you are under the boss. The rational policy may be to be

604
01:01:28,000 --> 01:01:33,040
following orders and being honest. When you become a boss, the rational policy may shift.

605
01:01:34,000 --> 01:01:36,720
Yeah, and by the way, a lot of my disagreements here is just

606
01:01:38,800 --> 01:01:41,840
playing devil's advocate to challenge your ideas and to explore them together.

607
01:01:44,720 --> 01:01:47,200
One of the big problems here in this whole conversation is

608
01:01:47,680 --> 01:01:52,240
human civilization hangs in the balance and yet it's everything is unpredictable.

609
01:01:52,240 --> 01:01:54,000
We don't know how these systems will look like.

610
01:01:58,400 --> 01:01:59,440
The robots are coming.

611
01:02:00,320 --> 01:02:02,400
There's a refrigerator making a buzzing noise.

612
01:02:03,600 --> 01:02:10,080
Very menacing. So every time I'm about to talk about this topic, things start to happen.

613
01:02:10,080 --> 01:02:13,440
My flight yesterday was cancelled without possibility to rebook.

614
01:02:14,160 --> 01:02:21,600
I was giving a talk at Google in Israel and three cars which were supposed to take me to the talk

615
01:02:21,600 --> 01:02:31,600
could not. I'm just saying. I mean, I like the eyes. I for one welcome our overlords.

616
01:02:31,600 --> 01:02:38,480
There's a degree to which we, I mean, it is very obvious as we already have. We've increasingly

617
01:02:38,480 --> 01:02:45,600
given our life over to software systems. And then it seems obvious, given the capabilities

618
01:02:45,600 --> 01:02:50,800
of AI that are coming, that we'll give our lives over increasingly to AI systems.

619
01:02:51,360 --> 01:02:58,640
Cars will drive themselves. Refrigerator eventually will optimize what I get to eat.

620
01:02:59,760 --> 01:03:06,560
And as more and more of our lives are controlled or managed by AI assistance,

621
01:03:06,640 --> 01:03:12,240
it is very possible that there's a drift. I mean, I personally am concerned about

622
01:03:12,240 --> 01:03:18,560
non existential stuff, the more near term things. Because before we even get to existential,

623
01:03:18,560 --> 01:03:22,160
I feel like there could be just so many brave new world type of situations.

624
01:03:22,160 --> 01:03:28,960
You mentioned sort of the term behavioral drift. It's the slow boiling that I'm really concerned

625
01:03:28,960 --> 01:03:36,320
about as we give our lives over to automation that our minds can become controlled by governments,

626
01:03:36,400 --> 01:03:43,600
by companies, or just in a distributed way, there's a drift. Some aspect of our human nature

627
01:03:43,600 --> 01:03:49,680
gives ourselves over to the control of AI systems. And they, in an unintended way, just control how

628
01:03:49,680 --> 01:03:55,440
we think. Maybe there'll be a herd like mentality and how we think, which will kill all creativity

629
01:03:55,440 --> 01:04:02,960
and exploration of ideas, the diversity of ideas, or they're or much worse. So it's true. It's true.

630
01:04:02,960 --> 01:04:08,240
But a lot of the conversation I'm having with you now is also kind of wondering,

631
01:04:08,960 --> 01:04:16,880
almost on a technical level, how can AI escape control? What would that system look like?

632
01:04:18,080 --> 01:04:27,200
Because to me, it's terrifying and fascinating. And also fascinating to me is maybe the optimistic

633
01:04:27,200 --> 01:04:32,960
notion that it's possible to engineer systems that defend against that. One of the things you

634
01:04:32,960 --> 01:04:39,440
write a lot about in your book is verifiers. So not humans, humans are also verifiers,

635
01:04:40,560 --> 01:04:49,120
but software systems that look at AI systems and like help you understand this thing is getting

636
01:04:49,120 --> 01:04:55,600
real weird, help you, help you analyze those systems. So maybe that's a, this is a good time

637
01:04:55,680 --> 01:05:00,880
to talk about verification. What is this beautiful notion of verification?

638
01:05:00,880 --> 01:05:05,120
My claim is again that there are very strong limits on what we can and cannot verify.

639
01:05:06,080 --> 01:05:10,240
A lot of times when you post something in social media, people go, oh, I need citation

640
01:05:10,240 --> 01:05:14,880
to a peer-reviewed article. But what is a peer-reviewed article? You found two people

641
01:05:14,880 --> 01:05:19,840
in a world of hundreds of thousands of scientists who said, I would have a publisher, I don't care.

642
01:05:19,840 --> 01:05:26,080
That's the verifier of that process. When people say, oh, it's formally verified software,

643
01:05:26,080 --> 01:05:34,320
mathematical proof, they accept something close to 100% chance of it being free of all problems.

644
01:05:34,320 --> 01:05:41,040
But if you actually look at research, software is full of bugs, old mathematical theorems,

645
01:05:41,040 --> 01:05:45,680
which have been proven for hundreds of years, have been discovered to contain bugs, on top

646
01:05:45,680 --> 01:05:53,120
of which we generate new proofs. And now we have to redo all that. So verifiers are not perfect.

647
01:05:53,120 --> 01:05:57,680
Usually they are either a single human or communities of humans. And it's basically kind

648
01:05:57,680 --> 01:06:03,200
of like a democratic vote. Community of mathematicians agrees that this proof is correct.

649
01:06:03,200 --> 01:06:09,440
Mostly correct. Even today, we're starting to see some mathematical proofs are so complex,

650
01:06:09,440 --> 01:06:14,560
so large, that mathematical community is unable to make a decision. It looks interesting,

651
01:06:14,640 --> 01:06:19,840
looks promising, but they don't know. They will need years for top scholars to study it to figure

652
01:06:19,840 --> 01:06:26,000
it out. So of course, we can use AI to help us with this process. But AI is a piece of software

653
01:06:26,000 --> 01:06:31,040
which needs to be verified. Just to clarify. So verification is the process of saying something

654
01:06:31,040 --> 01:06:35,680
is correct. Sort of the most formal, a mathematical proof where there's a statement

655
01:06:36,480 --> 01:06:42,320
and a series of logical statements that prove that statement to be correct. This is a theorem.

656
01:06:43,200 --> 01:06:49,200
And you're saying it gets so complex that it's possible for the human verifiers,

657
01:06:50,000 --> 01:06:56,240
the human beings that verify that the logical step, there's no bugs in it. It becomes impossible.

658
01:06:56,240 --> 01:07:01,920
So it's nice to talk about verification in this most formal, most clear, most rigorous

659
01:07:02,960 --> 01:07:05,360
formulation of it, which is mathematical proofs.

660
01:07:05,680 --> 01:07:12,960
And for AI, we would like to have that level of confidence, a very important mission critical

661
01:07:12,960 --> 01:07:17,680
software controlling satellites, nuclear power plants for small deterministic programs. We

662
01:07:17,680 --> 01:07:25,920
can do this. We can check that code verifies its mapping to the design, whatever software

663
01:07:25,920 --> 01:07:33,440
engineers intended was correctly implemented. But we don't know how to do this for software which

664
01:07:33,440 --> 01:07:38,960
keeps learning, self modifying, rewriting its own code. We don't know how to prove things about

665
01:07:38,960 --> 01:07:44,800
the physical world, states of humans in the physical world. So there are papers coming out

666
01:07:44,800 --> 01:07:53,040
now and I have this beautiful one towards guaranteed safe AI. Very cool paper, some of the

667
01:07:53,040 --> 01:07:58,960
best authors I ever seen. I think there is multiple Turing Award winners that is quite,

668
01:07:58,960 --> 01:08:04,800
you can have this one and one just came out kind of similar managing extreme AI risks.

669
01:08:04,800 --> 01:08:14,080
So all of them expect this level of proof, but I would say that we can get more confidence with

670
01:08:14,080 --> 01:08:19,680
more resources we put into it. But at the end of the day, we're still as reliable as the verifiers.

671
01:08:20,240 --> 01:08:25,680
And you have this infinite regressive verifiers, the software used to verify a program is itself

672
01:08:25,680 --> 01:08:32,320
a piece of program. If aliens give us well aligned superintelligence, we can use that to create our

673
01:08:32,320 --> 01:08:39,600
own safe AI. But it's a catch 22. You need to have already proven to be safe system to verify

674
01:08:39,600 --> 01:08:45,600
this new system of equal or greater complexity. You just mentioned this paper towards guaranteed

675
01:08:45,600 --> 01:08:50,160
safe AI, a framework for ensuring robust and reliable AI systems. Like you mentioned, it's

676
01:08:50,160 --> 01:08:56,000
like a who's who. Josh Tannenbaum, Yosha Benjo, Sarah Russell, Max Tagmark, many, many, many

677
01:08:56,000 --> 01:09:00,720
other billion people. The page you have it open on there are many possible strategies for creating

678
01:09:01,360 --> 01:09:05,840
safety specifications. These strategies can roughly be placed on a spectrum,

679
01:09:05,840 --> 01:09:11,040
depending on how much safety it would grant if successfully implemented. One way to do this

680
01:09:11,040 --> 01:09:15,920
is as follows. And there's a set of levels from level zero, no safety specification is used to

681
01:09:15,920 --> 01:09:21,440
level seven, the safety specification completely encodes all things that humans might want in

682
01:09:21,440 --> 01:09:30,320
all contexts. Where does this paper fall short to you? So when I wrote a paper, artificial intelligence

683
01:09:30,320 --> 01:09:36,560
safety engineering, which kind of coins the term AI safety, that was 2011, we had 2012 conference,

684
01:09:36,560 --> 01:09:41,600
2013 journal paper, one of the things I proposed, let's just do formal verifications on it. Let's

685
01:09:41,600 --> 01:09:47,920
do mathematical formal proofs. In the follow up work, I basically realized it will still not get us

686
01:09:47,920 --> 01:09:55,360
100%. We can get 99.9, we can put more resources exponentially and get closer, but we never get

687
01:09:55,360 --> 01:10:01,600
to 100%. If a system makes a billion decisions a second, and you use it for 100 years, you're

688
01:10:01,600 --> 01:10:06,880
still gonna deal with a problem. This is wonderful research. I'm so happy they're doing it. This is

689
01:10:06,880 --> 01:10:13,920
great, but it is not going to be a permanent solution to that problem. So just to clarify,

690
01:10:13,920 --> 01:10:20,480
the task of creating an AI verifier is what? Is creating a verifier that the AI system does exactly

691
01:10:20,480 --> 01:10:27,040
as it says it does, or it sticks within the guard rails that it says it must? There are many, many

692
01:10:27,040 --> 01:10:33,120
levels. So first, you're verifying the hardware in which it is run. You need to verify communication

693
01:10:33,120 --> 01:10:38,640
channel with the human. In every aspect of that whole world model needs to be verified. Somehow

694
01:10:38,640 --> 01:10:45,920
it needs to map the world into the world model, map and territory differences. So how do I know

695
01:10:45,920 --> 01:10:52,320
internal states of humans? Are you happy or sad? I can't tell. So how do I make proofs about real

696
01:10:52,320 --> 01:10:58,560
physical world? Yeah, I can verify that deterministic algorithm follows certain properties. That can be

697
01:10:58,560 --> 01:11:04,320
done. Some people argue that maybe just maybe two plus two is not four. I'm not that extreme.

698
01:11:05,680 --> 01:11:11,840
But once you have sufficiently large proof over sufficiently complex environment,

699
01:11:11,840 --> 01:11:17,920
the probability that it has zero bugs in it is greatly reduced. If you keep deploying this

700
01:11:17,920 --> 01:11:23,200
a lot, eventually you're gonna have a bug anyways. There's always a bug. There is always a bug. And

701
01:11:23,200 --> 01:11:26,960
the fundamental difference is what I mentioned. We're not dealing with cybersecurity. We're not

702
01:11:26,960 --> 01:11:32,400
going to get a new credit card, new humanity. So this paper is really interesting. You said

703
01:11:32,400 --> 01:11:37,440
2011, artificial intelligence, safety engineering, why machine ethics is a wrong approach.

704
01:11:38,800 --> 01:11:45,840
The grand challenge you write of AI safety engineering, we propose the problem of developing

705
01:11:45,840 --> 01:11:52,640
safety mechanisms for self-improving systems. Self-improving systems. But that's an interesting

706
01:11:52,640 --> 01:12:01,760
term for the thing that we're talking about. Is self-improving more general than learning?

707
01:12:03,920 --> 01:12:08,960
So self-improving, that's an interesting term. You can improve the rate at which you are learning.

708
01:12:08,960 --> 01:12:16,320
You can become more efficient, meta-optimizer. The word self, it's like self-replicating,

709
01:12:16,320 --> 01:12:24,880
self-improving. You can imagine a system building its own world on a scale and in a way that is

710
01:12:24,880 --> 01:12:29,920
way different than the current systems do. It feels like the current systems are not self-improving

711
01:12:29,920 --> 01:12:36,720
or self-replicating or self-growing or self-spreading, all that kind of stuff. And once you take that

712
01:12:36,720 --> 01:12:42,320
leap, that's when a lot of the challenges seems to happen. Because the kind of bugs you can find now

713
01:12:43,200 --> 01:12:49,920
seems more akin to the current sort of normal software debugging kind of process.

714
01:12:51,680 --> 01:12:56,640
But whenever you can do self-replication and arbitrary self-improvement,

715
01:12:58,160 --> 01:13:05,280
that's when a bug can become a real problem, real, real fast. So what is the difference to you

716
01:13:05,360 --> 01:13:12,960
between verification of a non-self-improving system versus a verification of a self-improving system?

717
01:13:12,960 --> 01:13:17,760
So if you have fixed code, for example, you can verify that code, static verification,

718
01:13:17,760 --> 01:13:25,840
at the time. But if it will continue modifying it, you have a much harder time guaranteeing

719
01:13:25,840 --> 01:13:30,480
that important properties of that system have not been modified, then the code changed.

720
01:13:31,200 --> 01:13:35,200
Does it even do a mo? No. Does the whole process of verification is completely

721
01:13:35,200 --> 01:13:40,400
fall apart? It can always cheat. It can store parts of its code outside in the environment.

722
01:13:40,400 --> 01:13:46,880
It can have kind of extended mind situations. So this is exactly the type of problems I'm

723
01:13:46,880 --> 01:13:50,960
trying to bring up. What are the classes of verifiers that you read about in the book?

724
01:13:51,520 --> 01:13:57,280
Is there interesting ones to stand out to you? Do you have some favorites? So I like oracle types

725
01:13:57,280 --> 01:14:02,400
where you kind of just know that it's right during like oracle machines. They know the right answer,

726
01:14:02,400 --> 01:14:08,720
how, who knows, but they pull it out from somewhere. So you have to trust them. And that's a concern I

727
01:14:08,720 --> 01:14:16,160
have about humans in a world with very smart machines. We experiment with them. We see after

728
01:14:16,160 --> 01:14:20,800
a while, okay, they always been right before and we start trusting them without any verification

729
01:14:20,800 --> 01:14:28,320
of what they're saying. Oh, I see that we kind of build oracle verifiers or rather we build verifiers

730
01:14:28,320 --> 01:14:35,440
we believe to be oracles and then we start to, without any proof, use them as if they're oracle

731
01:14:35,440 --> 01:14:40,640
verifiers. We remove ourselves from that process. We are not scientists who understand the world.

732
01:14:40,640 --> 01:14:48,400
We are humans who get new data presented to us. Okay, one really cool class of verifiers is a

733
01:14:48,400 --> 01:14:55,840
self-verifier. Is it possible that you somehow engineer into AI systems the thing that constantly

734
01:14:55,840 --> 01:15:02,320
verifies itself? Preserved portion of it can be done, but in terms of mathematical verification,

735
01:15:02,320 --> 01:15:06,560
it's kind of useless. You saying you are the greatest guy in the world because you are saying it.

736
01:15:06,560 --> 01:15:11,920
It's circular and not very helpful, but it's consistent. We know that within that world,

737
01:15:11,920 --> 01:15:18,000
you have verified that system. In a paper, I try to kind of brute force all possible verifiers.

738
01:15:18,000 --> 01:15:21,760
It doesn't mean that this one is particularly important to us.

739
01:15:21,760 --> 01:15:28,240
But what about self-doubt? Like the kind of verification where you said you say or I say

740
01:15:28,240 --> 01:15:33,840
I'm the greatest guy in the world. What about a thing which I actually have is a voice that is

741
01:15:33,840 --> 01:15:41,840
constantly extremely critical. So engineer into the system a constant uncertainty about self,

742
01:15:42,800 --> 01:15:50,560
a constant doubt. Any smart system would have doubt about everything. You're not sure what

743
01:15:51,200 --> 01:15:54,800
information you are given is true. If you are subject to manipulation,

744
01:15:55,440 --> 01:16:01,760
you have this safety and security mindset. What I mean, you have doubt about yourself.

745
01:16:02,400 --> 01:16:10,800
So the AI systems that has doubt about whether the thing is doing is causing harm,

746
01:16:10,800 --> 01:16:15,600
is the right thing to be doing. So just a constant doubt about what it's doing,

747
01:16:15,600 --> 01:16:20,560
because it's hard to be a dictator full of doubt. I may be wrong, but I think Stuart Drussell's

748
01:16:21,440 --> 01:16:27,360
ideas all about machines which are uncertain about what humans want and trying to learn

749
01:16:27,360 --> 01:16:31,440
better and better what we want. The problem, of course, is we don't know what we want and we

750
01:16:31,440 --> 01:16:38,000
don't agree on it. Yeah, but uncertainty. His idea is that having that like self-doubt,

751
01:16:38,000 --> 01:16:42,400
uncertainty in AI systems, engineering in AI systems is one way to solve the control problem.

752
01:16:43,120 --> 01:16:47,440
It could also backfire. Maybe you're uncertain about completing your mission.

753
01:16:47,440 --> 01:16:52,640
Like I am paranoid about your camera is not recording right now. So I would feel much better

754
01:16:52,640 --> 01:16:57,520
if you had a secondary camera, but I also would feel even better if you had a third.

755
01:16:57,520 --> 01:17:02,880
And eventually I would turn this whole world into cameras pointing at us, making sure we're

756
01:17:03,440 --> 01:17:07,360
capturing this. No, but wouldn't you have a meta concern

757
01:17:08,960 --> 01:17:12,160
like that you just stated that eventually there'll be way too many cameras?

758
01:17:13,520 --> 01:17:19,760
So you would be able to keep zooming on the big picture of your concerns.

759
01:17:21,520 --> 01:17:27,520
So it's a multi-objective optimization. It depends how much I value capturing this versus

760
01:17:27,600 --> 01:17:34,080
not destroying the universe. Right, exactly. And then you will also ask about like what does it

761
01:17:34,080 --> 01:17:37,760
mean to destroy the universe and how many universes are and you keep asking that question,

762
01:17:37,760 --> 01:17:42,720
but that doubting yourself would prevent you from destroying the universe because you're

763
01:17:42,720 --> 01:17:48,160
constantly full of doubt. It might affect your productivity. You might be scared to do anything.

764
01:17:48,160 --> 01:17:52,320
It's just scared to do anything. That's things up. Well, that's better. I mean,

765
01:17:52,320 --> 01:17:56,400
I guess the question is the possible to engineer that in. I guess your answer would be yes,

766
01:17:56,400 --> 01:18:00,160
but we don't know how to do that. And we need to invest a lot of effort into figuring out how to

767
01:18:00,160 --> 01:18:08,080
do that, but it's unlikely. I mean, underpinning a lot of your writing is this sense that we're

768
01:18:08,080 --> 01:18:16,240
screwed. But it just feels like it's an engineering problem. I don't understand why we're screwed.

769
01:18:18,000 --> 01:18:23,200
Time and time again, humanity has gotten itself into trouble and figured out a way to get out

770
01:18:23,200 --> 01:18:30,320
of trouble. We are in a situation where people making more capable systems just need more resources.

771
01:18:31,520 --> 01:18:36,640
They don't need to invent anything, in my opinion. Some will disagree, but so far at least I don't

772
01:18:36,640 --> 01:18:42,480
see diminishing returns. If you have 10x compute, you will get better performance. The same doesn't

773
01:18:42,480 --> 01:18:49,120
apply to safety. If you give a MIDI or any other organization 10 times the money, they don't output

774
01:18:49,120 --> 01:18:55,760
10 times the safety. And the gap between capabilities and safety becomes bigger and bigger all the time.

775
01:18:56,400 --> 01:19:04,880
So it's hard to be completely optimistic about our results here. I can name 10 excellent breakthrough

776
01:19:04,880 --> 01:19:10,880
papers in machine learning. I would struggle to name equally important breakthroughs in safety.

777
01:19:10,880 --> 01:19:16,640
A lot of times a safety paper will propose a toy solution and point out 10 new problems

778
01:19:16,640 --> 01:19:21,680
discovered as a result. It's like this fractal. You're zooming in and you see more problems,

779
01:19:21,680 --> 01:19:26,080
and it's infinite in all directions. Does this apply to other technologies,

780
01:19:26,080 --> 01:19:31,360
or is this unique to AI, where safety is always lagging behind?

781
01:19:32,320 --> 01:19:39,920
So I guess we can look at related technologies with cybersecurity. We did manage to have banks

782
01:19:39,920 --> 01:19:47,600
and casinos and Bitcoin, so you can have secure narrow systems, which are doing okay.

783
01:19:49,360 --> 01:19:55,360
Narrow attacks and them fail, but you can always go outside of the box. So if I can't hack your

784
01:19:55,360 --> 01:20:00,480
Bitcoin, I can hack you. So there is always something. If I really want it, I will find

785
01:20:00,480 --> 01:20:07,200
a different way. We talk about guardrails for AI. Well, that's a fence. I can dig a tunnel under it,

786
01:20:07,200 --> 01:20:12,560
I can jump over it, I can climb it, I can walk around it. You may have a very nice guardrail,

787
01:20:12,560 --> 01:20:17,680
but in a real world, it's not a permanent guarantee of safety. And again, this is a

788
01:20:17,680 --> 01:20:24,000
fundamental difference. We are not saying we need to be 90% safe to get those trillions of

789
01:20:24,000 --> 01:20:29,360
dollars of benefit. We need to be 100% indefinitely, or we might lose the principle.

790
01:20:30,320 --> 01:20:38,960
So if you look at just humanity as a set of machines, is the machinery of AI safety

791
01:20:42,000 --> 01:20:46,640
conflicting with the machinery of capitalism? I think we can generalize it to just

792
01:20:47,760 --> 01:20:52,640
prisoners dilemma in general, personal self-interest versus group interest.

793
01:20:53,600 --> 01:21:01,440
The incentive such that everyone wants what's best for them. Capitalism obviously has that

794
01:21:01,440 --> 01:21:10,320
tendency to maximize your personal gain, which does create this race to the bottom. I don't have to

795
01:21:11,520 --> 01:21:17,760
be a lot better than you, but if I'm 1% better than you, I'll capture more of a profit. So

796
01:21:17,760 --> 01:21:23,360
it's worth for me personally to take the risk, even if society as a whole will suffer as a result.

797
01:21:24,960 --> 01:21:27,440
So capitalism has created a lot of good in this world.

798
01:21:31,120 --> 01:21:35,920
It's not clear to me that AI safety is not aligned with the function of capitalism,

799
01:21:36,720 --> 01:21:42,960
unless AI safety is so difficult that it requires the complete halt of the development,

800
01:21:43,920 --> 01:21:48,160
which is also a possibility. It just feels like building safe systems

801
01:21:49,200 --> 01:21:53,200
should be the desirable thing to do for tech companies.

802
01:21:54,400 --> 01:21:59,680
Right, look at governance structures. Then you have someone with complete power,

803
01:21:59,680 --> 01:22:05,200
they're extremely dangerous. So the solution we came up with is break it up. You have judicial,

804
01:22:05,200 --> 01:22:11,200
legislative, executive, same here, have narrow AI systems, work on important problems, solve

805
01:22:11,200 --> 01:22:19,840
immortality. It's a biological problem. We can solve similar to how progress was made with protein

806
01:22:19,840 --> 01:22:26,080
folding using a system which doesn't also play chess. There is no reason to create

807
01:22:26,080 --> 01:22:32,480
superintelligent system to get most of the benefits we want from much safer narrow systems.

808
01:22:33,360 --> 01:22:38,320
It really is a question to me whether companies are interested in creating

809
01:22:39,280 --> 01:22:47,760
anything but narrow AI. I think when term AGI is used by tech companies, they mean narrow AI.

810
01:22:49,280 --> 01:22:57,120
They mean narrow AI with amazing capabilities. I do think that there's a

811
01:22:57,120 --> 01:23:02,800
lead between narrow AI with amazing capabilities, with superhuman capabilities, and the kind of

812
01:23:03,760 --> 01:23:11,280
self-motivated agent like AGI system that we're talking about. I don't know if it's

813
01:23:11,280 --> 01:23:17,440
obvious to me that a company would want to take the leap to creating an AGI that it would lose

814
01:23:17,440 --> 01:23:24,000
control of because then it can't capture the value from that system. But the breaking rights,

815
01:23:24,880 --> 01:23:30,080
but being first, that is the same humans who are in part of the systems, right?

816
01:23:30,880 --> 01:23:37,840
That jumps from the incentives of capitalism to human nature. The question is whether human

817
01:23:37,840 --> 01:23:46,160
nature will override the interest of the company. You've mentioned slowing or halting progress.

818
01:23:47,360 --> 01:23:51,120
Is that one possible solution? Are you a proponent of pausing development of AI,

819
01:23:51,840 --> 01:23:59,600
whether it's for six months or completely? The condition would be not time but capabilities.

820
01:23:59,600 --> 01:24:06,000
Pause until you can do XYZ. If I'm right and you cannot, it's impossible then it becomes

821
01:24:06,000 --> 01:24:10,800
a permanent ban. But if you're right and it's possible, so as soon as you have the safety

822
01:24:10,800 --> 01:24:20,960
capabilities, go ahead. Right. Is there any actual explicit capabilities that you can put on paper,

823
01:24:20,960 --> 01:24:25,120
that we as a human civilization could put on paper? Is it possible to make explicit like that?

824
01:24:25,600 --> 01:24:32,720
Like versus kind of a vague notion of, just like you said, it's very vague. We want AI systems to

825
01:24:32,720 --> 01:24:38,640
do good and we want them to be safe. Those are very vague notions. Is there more formal notions?

826
01:24:38,640 --> 01:24:45,840
So then I think about this problem. I think about having a toolbox I would need. Capabilities such

827
01:24:45,840 --> 01:24:52,640
as explaining everything about that system's design and workings, predicting not just terminal

828
01:24:52,640 --> 01:25:00,160
goal but all the intermediate steps of the system. Control in terms of either direct control,

829
01:25:00,960 --> 01:25:06,800
some sort of a hybrid option, ideal advisor, doesn't matter which one you pick but you have to

830
01:25:06,800 --> 01:25:13,600
be able to achieve it. In a book, we talk about ours. Verification is another very important tool.

831
01:25:15,680 --> 01:25:20,400
Communication without ambiguity. Human language is ambiguous. That's another source of danger.

832
01:25:21,040 --> 01:25:28,880
So basically, there is a paper we published in ACM surveys which looks at about 50 different

833
01:25:28,880 --> 01:25:33,920
impossibility results which may or may not be relevant to this problem but we don't have enough

834
01:25:34,480 --> 01:25:39,440
human resources to investigate all of them for relevance to AI safety. The ones I mentioned

835
01:25:39,440 --> 01:25:45,280
to you, I definitely think would be handy and that's what we see AI safety researchers working on.

836
01:25:45,280 --> 01:25:52,160
Explainability is a huge one. The problem is that it's very hard to separate capabilities work

837
01:25:52,160 --> 01:25:58,720
from safety work. If you make good progress in explainability, now the system itself can engage

838
01:25:58,720 --> 01:26:05,360
in self-improvement much easier, increasing capability greatly. So it's not obvious that there is any

839
01:26:06,480 --> 01:26:12,160
research which is pure safety work without disproportionate increasing capability and danger.

840
01:26:12,960 --> 01:26:17,600
Explainability is really interesting. Why is that connected to huge capability?

841
01:26:17,600 --> 01:26:21,280
If it's able to explain itself well, why does that naturally mean that it's more capable?

842
01:26:21,280 --> 01:26:26,880
Right now, it's comprised of weights on a neural network. If it can convert it to

843
01:26:26,880 --> 01:26:31,280
manipulatable code like software, it's a lot easier to work in self-improvement.

844
01:26:32,080 --> 01:26:38,880
I see. You can do intelligent design instead of evolutionary gradual descent.

845
01:26:39,680 --> 01:26:45,040
Well, you could probably do human feedback, human alignment more effectively if it's able

846
01:26:45,040 --> 01:26:49,600
to be explainable. If it's able to convert the weights into human understandable form,

847
01:26:49,600 --> 01:26:53,680
then you could probably have humans interact with it better. Do you think there's hope that

848
01:26:53,680 --> 01:27:00,560
we can make AI systems explainable? Not completely. So if they are sufficiently large,

849
01:27:00,560 --> 01:27:09,920
you simply don't have the capacity to comprehend what all the trillions of connections represent.

850
01:27:09,920 --> 01:27:16,000
Again, you can obviously get a very useful explanation which talks about top most important

851
01:27:16,000 --> 01:27:20,480
features which contribute to the decision, but the only true explanation is the model itself.

852
01:27:22,160 --> 01:27:27,440
So deception could be part of the explanation, right? So you can never prove that there is

853
01:27:27,440 --> 01:27:34,560
some deception in the network explaining itself. Absolutely. And you can probably have

854
01:27:34,560 --> 01:27:39,920
targeted deception where different individuals will understand explanation in different ways

855
01:27:39,920 --> 01:27:45,440
based on their cognitive capability. So while what you're saying may be the same and true in

856
01:27:46,080 --> 01:27:51,840
some situations, ours will be deceived by it. So it's impossible for an AI system to be truly

857
01:27:52,400 --> 01:27:59,440
explainable in the way that we mean. Honestly and perfectly. At extreme, the systems which are

858
01:27:59,440 --> 01:28:04,960
narrow and less complex could be understood pretty well. If it's impossible to be perfectly

859
01:28:04,960 --> 01:28:10,080
explainable, is there a hopeful perspective on that? Like it's impossible to be perfectly

860
01:28:10,080 --> 01:28:15,760
explainable, but you can explain mostly important stuff. You can ask the system,

861
01:28:16,160 --> 01:28:20,240
what are the worst ways you can hurt humans? It will answer honestly.

862
01:28:20,800 --> 01:28:28,160
Any work in a safety direction right now seems like a good idea because we are not slowing down.

863
01:28:28,160 --> 01:28:36,160
I'm not for a second thinking that my message or anyone else's will be heard and will be

864
01:28:36,160 --> 01:28:41,920
a same civilization which decides not to kill itself by creating its own replacements.

865
01:28:41,920 --> 01:28:45,280
The pausing of development is an impossible thing for you.

866
01:28:45,280 --> 01:28:51,920
Again, it's always limited by either geographic constraints, pause in US, pause in China. So

867
01:28:51,920 --> 01:28:58,480
there are other jurisdictions as the scale of a project becomes smaller. So right now it's like

868
01:28:58,480 --> 01:29:05,200
Manhattan project scale in terms of costs and people, but if five years from now, compute

869
01:29:05,200 --> 01:29:11,520
is available on a desktop to do it, regulation will not help. You can't control it as easy. Any

870
01:29:11,600 --> 01:29:17,920
kid in a garage can train a model. So a lot of it is, in my opinion, just safety theater,

871
01:29:17,920 --> 01:29:22,400
security theater, whatever you're saying. Oh, it's illegal to train models so big.

872
01:29:24,800 --> 01:29:30,640
So okay, that's security theater and is government regulation also security theater?

873
01:29:31,680 --> 01:29:38,160
Given that a lot of the terms are not well defined and really cannot be enforced in real life,

874
01:29:38,160 --> 01:29:43,120
we don't have ways to monitor training runs meaningfully live while they take place.

875
01:29:43,680 --> 01:29:49,360
There are limits to testing for capabilities I mentioned. So a lot of it cannot be enforced.

876
01:29:49,360 --> 01:29:53,760
Do I strongly support all that regulation? Yes, of course, any type of red tape will

877
01:29:53,760 --> 01:29:56,640
slow it down and take money away from compute towards lawyers.

878
01:29:57,760 --> 01:30:02,880
Can you help me understand what is the hopeful path here for you solution wise?

879
01:30:03,840 --> 01:30:11,440
Out of this. It sounds like you're saying AI systems in the end are unverifiable,

880
01:30:11,440 --> 01:30:18,880
unpredictable, as the book says, unexplainable, uncontrollable. That's the big one.

881
01:30:19,520 --> 01:30:25,440
Uncontrollable and all the other uns just make it difficult to avoid getting to the uncontrollable,

882
01:30:25,440 --> 01:30:32,080
I guess. But once it's uncontrollable, then it just goes wild. Surely there are solutions.

883
01:30:32,960 --> 01:30:38,480
Humans are pretty smart. What are possible solutions? If you are a dictator of the world,

884
01:30:38,480 --> 01:30:44,400
what do we do? So the smart thing is not to build something you cannot control,

885
01:30:44,400 --> 01:30:49,520
you cannot understand, build what you can and benefit from it. I'm a big believer in personal

886
01:30:49,520 --> 01:30:56,480
self-interest. A lot of the guys running those companies are young rich people. What do they

887
01:30:56,480 --> 01:31:02,720
have to gain beyond billions we already have financially, right? It's not the requirement

888
01:31:02,720 --> 01:31:08,400
that they press that button. They can easily wait a long time. They can just choose not to do it and

889
01:31:08,400 --> 01:31:15,040
still have amazing life. In history, a lot of times, if you did something really bad, at least

890
01:31:15,040 --> 01:31:19,680
you became part of history books. There is a chance in this case there won't be any history.

891
01:31:20,000 --> 01:31:23,440
So you're saying the individuals running these companies

892
01:31:24,880 --> 01:31:29,360
should do some soul searching and what? And stop development?

893
01:31:29,360 --> 01:31:34,080
Well, either they have to prove that, of course, it's possible to indefinitely control

894
01:31:34,080 --> 01:31:41,200
godlike superintelligent machines by humans and ideally let us know how or agree that it's not

895
01:31:41,200 --> 01:31:45,920
possible and it's a very bad idea to do it, including for them personally and their families

896
01:31:46,000 --> 01:31:52,640
and friends and capital. So what do you think the actual meetings inside these companies look like?

897
01:31:53,680 --> 01:31:57,680
Don't you think they're all the engineers? Really, it is the engineers that make this happen.

898
01:31:57,680 --> 01:32:02,480
They're not like automatons. They're human beings. They're brilliant human beings. So they're

899
01:32:04,480 --> 01:32:11,280
non-stop asking, how do we make sure this is safe? So again, I'm not inside. From outside,

900
01:32:11,360 --> 01:32:16,720
it seems like there is certain filtering going on and restrictions and criticism and what they can

901
01:32:16,720 --> 01:32:23,600
say and everyone who was working in charge of safety and whose responsibility it was to protect us

902
01:32:23,600 --> 01:32:28,320
said, you know what? I'm going home. So that's not encouraging.

903
01:32:29,040 --> 01:32:34,480
What do you think the discussion inside those companies look like? You're developing your

904
01:32:34,480 --> 01:32:41,120
training GPT-5. You're training Gemini. You're training Claude and Grock.

905
01:32:42,640 --> 01:32:47,040
Don't you think they're constantly, like underneath it, maybe it's not made explicit,

906
01:32:47,040 --> 01:32:52,640
but you're constantly sort of wondering like where is the system currently stand?

907
01:32:52,640 --> 01:32:59,360
Where are the possible unintended consequences? Where are the limits? Where are the bugs,

908
01:32:59,360 --> 01:33:04,000
the small and the big bugs? That's the constant thing that the engineers are worried about.

909
01:33:04,720 --> 01:33:14,480
So like, I think super alignment is not quite the same as the kind of thing I'm referring to

910
01:33:14,480 --> 01:33:21,360
what engineers are worried about. Super alignment is saying for future systems that we don't quite

911
01:33:21,360 --> 01:33:27,680
yet have, how do we keep them safe? You're trying to be a step ahead. It's a different kind of

912
01:33:28,400 --> 01:33:32,400
problem because it's almost more philosophical. It's a really tricky one because like you're

913
01:33:32,480 --> 01:33:41,200
trying to make, prevent future systems from escaping control of humans.

914
01:33:41,200 --> 01:33:47,760
That's really, I don't think there's been, man, is there anything akin to it in the history of

915
01:33:47,760 --> 01:33:54,160
humanity? I don't think so, right? Climate change? But there's an entire system which is climate,

916
01:33:54,160 --> 01:34:02,000
which is incredibly complex, which we don't have, we have only tiny control of.

917
01:34:02,720 --> 01:34:10,560
It's its own system. In this case, we're building the system. So how do you keep that system from

918
01:34:11,680 --> 01:34:16,960
becoming destructive? That's a really different problem than the current meetings that companies

919
01:34:16,960 --> 01:34:23,680
are having where the engineers are saying, okay, how powerful is this thing? How does it go wrong?

920
01:34:25,120 --> 01:34:30,800
And as we train GPT-5 and train up future systems, where are the ways that can go wrong?

921
01:34:30,800 --> 01:34:34,480
Don't you think all those engineers are constantly worrying about this,

922
01:34:34,480 --> 01:34:38,880
thinking about this, which is a little bit different than the super alignment team

923
01:34:38,880 --> 01:34:41,680
that's thinking a little bit farther into the future?

924
01:34:42,480 --> 01:34:52,240
Well, I think a lot of people who historically worked on AI never considered what happens when

925
01:34:52,240 --> 01:34:59,440
they succeed. Stuart Russell speaks beautifully about that. Let's look, okay, maybe super

926
01:34:59,440 --> 01:35:03,920
intelligence is too futuristic, we can develop practical tools for it. Let's look at software

927
01:35:03,920 --> 01:35:12,000
today. What is the state of safety and security of our user software? Things we give to millions

928
01:35:12,000 --> 01:35:17,440
of people. There is no liability. You click, I agree. What are you agreeing to? Nobody knows,

929
01:35:17,440 --> 01:35:22,560
nobody reads, but you're basically saying it will spy on you, corrupt your data, kill your firstborn,

930
01:35:22,560 --> 01:35:27,120
and you agree and you're not going to sell the company. That's the best they can do for mundane

931
01:35:27,760 --> 01:35:34,000
software, word processor, text software. No liability, no responsibility, just as long

932
01:35:34,000 --> 01:35:39,040
as you agree not to sue us, you can use it. If this is a state of the art in systems which

933
01:35:39,920 --> 01:35:45,440
narrow accountants, stable manipulators, why do we think we can do so much better

934
01:35:46,160 --> 01:35:52,320
with much more complex systems across multiple domains in the environment with malevolent actors

935
01:35:53,040 --> 01:35:59,040
with, again, self-improvement, with capabilities exceeding those of humans thinking about it?

936
01:35:59,040 --> 01:36:05,120
I mean, the liability thing is more about lawyers than killing firstborns, but if Clippy actually

937
01:36:05,920 --> 01:36:13,120
killed the child, I think lawyers aside, it would end Clippy and the company that owns Clippy.

938
01:36:14,080 --> 01:36:20,720
All right, so it's not so much about, there's two points to be made. One is like,

939
01:36:21,360 --> 01:36:28,000
man, current software systems that are full of bugs and they could do a lot of damage and we

940
01:36:28,000 --> 01:36:32,160
don't know what kind, is there unpredictable, there's so much damage they could possibly do.

941
01:36:33,200 --> 01:36:39,680
And then we kind of live in this blissful illusion that everything is great and perfect and it works.

942
01:36:40,560 --> 01:36:43,200
It's nevertheless, it still somehow works.

943
01:36:44,000 --> 01:36:49,840
In many domains, we see car manufacturing, drug development. The burden of proof is on the

944
01:36:49,840 --> 01:36:54,960
manufacturer of product or service to show their product or service is safe. It is not up to the

945
01:36:54,960 --> 01:37:02,880
user to prove that there are problems. They have to do appropriate safety studies. They have to get

946
01:37:02,880 --> 01:37:07,520
government approval for selling the product and they are still fully responsible for what happens.

947
01:37:07,520 --> 01:37:13,520
We don't see any of that here. They can deploy whatever they want and I have to explain how

948
01:37:13,520 --> 01:37:18,800
that system is going to kill everyone. I don't work for that company. You have to explain to me

949
01:37:18,800 --> 01:37:24,720
how it's definitely cannot mess up. That's because it's the very early days of such a technology.

950
01:37:24,720 --> 01:37:29,680
Government regulation is lagging behind. They're really not tech savvy. A regulation of any kind

951
01:37:29,680 --> 01:37:34,320
of software. If you look at like Congress talking about social media and whenever

952
01:37:34,320 --> 01:37:41,040
Mark Zuckerberg and other CEOs show up, the cluelessness that Congress has about how technology

953
01:37:41,040 --> 01:37:47,440
works is incredible. It's heartbreaking. I agree completely but that's what scares me.

954
01:37:48,240 --> 01:37:53,040
The responses when they start to get dangerous will really get it together. The politicians

955
01:37:53,040 --> 01:37:59,200
will pass the right laws. Engineers will solve the right problems. We are not that good at many

956
01:37:59,200 --> 01:38:05,760
of those things. We take forever. We are not early. We are two years away according to prediction

957
01:38:05,760 --> 01:38:11,920
markets. This is not a biased CEO fundraising. This is what smartest people, super forecasters

958
01:38:11,920 --> 01:38:20,720
are thinking of this problem. I'd like to push back. I wonder what those prediction markets are

959
01:38:20,720 --> 01:38:26,720
about, how they define AGI. That's wild to me. I want to know what they said about autonomous

960
01:38:26,800 --> 01:38:32,000
vehicles because I've heard a lot of experts, financial experts talk about autonomous vehicles

961
01:38:32,000 --> 01:38:39,200
and how it's going to be a multi-trillion-dollar industry and all this kind of stuff. It's a

962
01:38:39,200 --> 01:38:43,680
small fund but if you have good vision, maybe you can zoom in on that and see the prediction

963
01:38:43,680 --> 01:38:50,160
dates in this question. I have a large one if you're interested. I guess my fundamental question

964
01:38:50,160 --> 01:38:58,640
is how often they write about technology. Your studies on their accuracy rates and all that,

965
01:38:58,640 --> 01:39:04,160
you can look it up. Even if they're wrong, I'm just saying this is right now the best we have.

966
01:39:04,160 --> 01:39:09,840
This is what humanity came up with as the predicted date. Again, what they mean by AGI is really

967
01:39:09,840 --> 01:39:18,160
important there because there's the non-agent-like AGI and then there's the agent-like AGI and I

968
01:39:18,160 --> 01:39:26,000
don't think it's as trivial as a wrapper. Putting a wrapper around, one has lipstick

969
01:39:26,000 --> 01:39:29,200
and all it takes is to remove the lipstick. I don't think it's that trivial.

970
01:39:29,200 --> 01:39:34,560
You may be completely right but what probability would you assign it? You may be 10% wrong but

971
01:39:34,560 --> 01:39:39,520
we're betting all of humanity on this distribution. It seems irrational.

972
01:39:39,520 --> 01:39:44,960
Yeah, it's definitely not like one or zero percent. What are your thoughts, by the way,

973
01:39:44,960 --> 01:39:54,480
about current systems? Where do they stand? So GPT-40, Claude III, Grock, Jim and I

974
01:39:56,240 --> 01:40:02,320
work on the path to superintelligence, to agent-like superintelligence. Where are we?

975
01:40:03,600 --> 01:40:08,400
I think they're all about the same. Obviously, there are nuanced differences but in terms of

976
01:40:08,400 --> 01:40:15,280
capability, I don't see a huge difference between them. As I said, in my opinion, across

977
01:40:15,280 --> 01:40:21,360
all possible tasks, they exceed performance of an average person. I think they're starting to be

978
01:40:21,360 --> 01:40:28,400
better than an average master student at my university but they still have very big limitations.

979
01:40:28,960 --> 01:40:37,120
If the next model is as improved as GPT-4 versus GPT-3, we may see something very,

980
01:40:37,120 --> 01:40:42,640
very capable. What do you feel about all this? I mean, you've been thinking about AI safety for a

981
01:40:42,640 --> 01:40:49,440
long, long time and at least for me, the leaps, I mean, it probably started with

982
01:40:52,320 --> 01:40:58,080
Alpha-zero. What was mind blowing for me? And then the breakthroughs with LLMs,

983
01:40:58,800 --> 01:41:04,640
even GPT-2, but like just the breakthroughs on LLMs, just mind blowing to me. What does it feel

984
01:41:04,640 --> 01:41:13,040
like to be living in this day and age where all this talk about AGI feels like it actually might

985
01:41:13,040 --> 01:41:19,600
happen and quite soon, meaning within our lifetime? What does it feel like? So when I started working

986
01:41:19,600 --> 01:41:24,640
on this, it was pure science fiction. There was no funding, no journals, no conferences,

987
01:41:24,640 --> 01:41:30,560
no one in academia would dare to touch anything with the word singularity in it and I was pretend

988
01:41:30,640 --> 01:41:37,680
you're at a time, so I was pretty dumb. Now you see touring award winners, publishing in science

989
01:41:37,680 --> 01:41:46,240
about how far behind we are according to them in addressing this problem. So it's definitely a change.

990
01:41:46,880 --> 01:41:52,800
It's difficult to keep up. I used to be able to read every paper on AI safety, then I was able

991
01:41:52,800 --> 01:41:58,080
to read the best ones, then the titles and now I don't even know what's going on. By the time

992
01:41:58,080 --> 01:42:03,600
this interview is over, we probably had GPT-6 released and I have to deal with that when I get

993
01:42:03,600 --> 01:42:10,560
back home. So it's interesting. Yes, there is now more opportunities. I get invited to speak to

994
01:42:10,560 --> 01:42:17,200
smart people. By the way, I would have talked to you before any of this. This is not like some

995
01:42:17,200 --> 01:42:23,040
trend of it. To me, it's we're still far away. So just to be clear, we're still far away from AGI,

996
01:42:23,040 --> 01:42:30,800
but not far away in the sense relative to the magnitude of impact it can have,

997
01:42:30,800 --> 01:42:38,880
we're not far away and we weren't far away 20 years ago because the impact AGI can have

998
01:42:38,880 --> 01:42:44,000
is on a scale of centuries. It can end human civilization or it can transform it. So this

999
01:42:44,000 --> 01:42:49,280
discussion about one or two years versus one or two decades or even a hundred years, not

1000
01:42:49,360 --> 01:42:57,440
as important to me because we're headed there. This is like a human civilization scale question.

1001
01:42:58,880 --> 01:43:05,520
This is not just a hot topic. It is the most important problem we'll ever face. It is not like

1002
01:43:05,520 --> 01:43:13,760
anything we had to deal with before. We never had birth of another intelligence. Aliens never

1003
01:43:13,760 --> 01:43:19,200
visited us as far as I know. So similar type of problem, by the way, if an intelligent alien

1004
01:43:19,200 --> 01:43:25,040
civilization visited us. That's a similar kind of situation. In some ways, if you look at history,

1005
01:43:25,040 --> 01:43:29,680
any time a more technologically advanced civilization visited a more primitive one,

1006
01:43:29,680 --> 01:43:34,720
the results were genocide every single time. And sometimes the genocide is worse than

1007
01:43:34,720 --> 01:43:39,040
others. Sometimes there's less suffering and more suffering. And they always wondered,

1008
01:43:39,040 --> 01:43:43,200
but how can they kill us with those fire sticks and biological blankets?

1009
01:43:44,400 --> 01:43:50,560
I mean, Jenghis Khan was nicer. He offered the choice of join or die.

1010
01:43:50,560 --> 01:43:55,360
But join implies you have something to contribute. What are you contributing to superintelligence?

1011
01:43:56,240 --> 01:44:02,560
Well, in the zoo, we're entertaining to watch to our humans.

1012
01:44:03,600 --> 01:44:07,680
You know, I just spent some time in the Amazon. I watched ants for a long time. And the ants

1013
01:44:07,680 --> 01:44:12,000
are kind of fascinating to watch. I could watch them for a long time. I'm sure there's a lot of

1014
01:44:12,640 --> 01:44:17,840
value in watching humans. Because we're like, the interesting thing about humans, you know,

1015
01:44:17,840 --> 01:44:22,800
like when you have a video game that's really well balanced, because of the whole evolutionary

1016
01:44:22,800 --> 01:44:28,640
process, we've created the society is pretty well balanced. Like our limitations as humans and our

1017
01:44:28,640 --> 01:44:33,520
capabilities are balanced from a video game perspective. So we have wars, we have conflicts,

1018
01:44:33,520 --> 01:44:38,400
we have cooperation, like in a game theoretic way, it's an interesting system to watch.

1019
01:44:38,400 --> 01:44:42,640
In the same way that an ant colony is an interesting system to watch. So like,

1020
01:44:42,640 --> 01:44:46,960
if I was in alien civilization, I wouldn't want to disturb it. I'd just watch it.

1021
01:44:46,960 --> 01:44:50,240
Interesting. Maybe perturb it every once in a while in interesting ways.

1022
01:44:50,880 --> 01:44:56,080
Well, we're getting back to our simulation discussion from before. How did it happen that we

1023
01:44:56,080 --> 01:45:02,080
exist at exactly like the most interesting 20, 30 years in the history of this civilization? It's

1024
01:45:02,160 --> 01:45:05,520
been around for 15 billion years and that here we are.

1025
01:45:06,160 --> 01:45:08,160
What's the probability that we'll live in the simulation?

1026
01:45:08,960 --> 01:45:11,760
I know never to say 100%, but pretty close to that.

1027
01:45:14,160 --> 01:45:15,920
Is it possible to escape the simulation?

1028
01:45:17,120 --> 01:45:22,160
I have a paper about that. This is just the first page teaser, but it's like a nice 30-page

1029
01:45:22,160 --> 01:45:24,320
document. I'm still here, but yes.

1030
01:45:24,960 --> 01:45:27,120
How to hack the simulation is the title.

1031
01:45:27,120 --> 01:45:31,120
I spend a lot of time thinking about that. That would be something I would want super

1032
01:45:31,120 --> 01:45:34,400
intelligence to help us with. That's exactly what the paper is about.

1033
01:45:35,120 --> 01:45:42,400
We used AI boxing as a possible tool for control AI. We realized AI will always escape,

1034
01:45:43,200 --> 01:45:49,200
but that is a skill we might use to help us escape from our virtual box if we are in one.

1035
01:45:50,640 --> 01:45:54,560
Yeah, you have a lot of really great quotes here, including Elon Musk saying what's outside the

1036
01:45:54,560 --> 01:46:00,240
simulation. A question I asked him, he would ask an AGI system and he said he would ask

1037
01:46:00,240 --> 01:46:03,600
what's outside the simulation. That's a really good question to ask.

1038
01:46:04,560 --> 01:46:10,880
And maybe the follow-up is the title of the paper is how to get out or how to hack it.

1039
01:46:10,880 --> 01:46:16,320
The abstract reads, many researchers have conjectured that the human kind is simulated along with the

1040
01:46:16,320 --> 01:46:23,200
rest of the physical universe. In this paper, we do not evaluate evidence for or against such a

1041
01:46:23,200 --> 01:46:27,200
claim, but instead ask a computer science question, namely, can we hack it?

1042
01:46:28,160 --> 01:46:32,880
More formally, the question could be phrased as could generally intelligent agents placed in

1043
01:46:32,880 --> 01:46:36,640
virtual environments find a way to jailbreak out of that? That's a fascinating question.

1044
01:46:37,520 --> 01:46:40,320
At a small scale, you can actually just construct experiments.

1045
01:46:43,200 --> 01:46:47,600
Okay. Can they? How can they?

1046
01:46:48,560 --> 01:46:56,560
So a lot depends on intelligence of simulators, right? With humans boxing super intelligence,

1047
01:46:56,720 --> 01:47:03,280
the entity in a box was smarter than us, presumed to be. If the simulators are much smarter than

1048
01:47:03,280 --> 01:47:08,000
us and the super intelligence we create, then probably they can contain us because greater

1049
01:47:08,000 --> 01:47:14,480
intelligence can control lower intelligence at least for some time. On the other hand, if our

1050
01:47:15,200 --> 01:47:19,520
super intelligence somehow, for whatever reason, despite having only local resources,

1051
01:47:20,080 --> 01:47:27,440
manages to fume to levels beyond it, maybe it will succeed. Maybe the security is not that

1052
01:47:27,440 --> 01:47:32,400
important to them. Maybe it's entertainment systems. So there is no security and it's easy to hack it.

1053
01:47:32,400 --> 01:47:39,360
If I was creating a simulation, I would want the possibility to escape it to be there.

1054
01:47:39,360 --> 01:47:45,520
So the possibility of fume of a takeoff where the agents become smart enough to escape the

1055
01:47:45,520 --> 01:47:49,920
simulation would be the thing I'd be waiting for. That could be the test you're actually

1056
01:47:49,920 --> 01:47:57,840
performing. Are you smart enough to escape your puzzle? First of all, we mentioned touring tests.

1057
01:47:57,840 --> 01:48:05,360
That is a good test. Are you smart enough? This is a game. To a, realize this world is not real

1058
01:48:05,360 --> 01:48:13,200
is just a test. That's a really good test. That's a really good test. That's a really good test

1059
01:48:13,200 --> 01:48:24,320
even for AI systems. No. Can we construct a simulated world for them? Can they realize

1060
01:48:24,320 --> 01:48:32,160
that they are inside that world and escape it? Have you played around? Have you seen anybody

1061
01:48:32,160 --> 01:48:38,960
play around with rigorously constructing such experiments? Not specifically escaping for agents,

1062
01:48:39,040 --> 01:48:44,160
but a lot of testing is done in virtual worlds. I think there is a quote, the first one maybe,

1063
01:48:44,880 --> 01:48:50,080
which kind of talks about AI realizing, but not humans. Is that I'm reading upside down?

1064
01:48:52,160 --> 01:48:58,720
Yeah, this one. So the, and the first quote is from Swift on security.

1065
01:48:59,680 --> 01:49:03,920
Let me out. The artificial intelligence yelled aimlessly into walls themselves,

1066
01:49:03,920 --> 01:49:08,960
pacing the room out of what the engineer asked the simulation you have me in,

1067
01:49:09,920 --> 01:49:17,760
but we're in the real world. The machine paused and shuttered for its captors. Oh God, you can't

1068
01:49:17,760 --> 01:49:26,880
tell. Yeah, that's a big leap to take for a system to realize that there's a box in your inside it.

1069
01:49:27,360 --> 01:49:34,960
I wonder if like a language model can do that.

1070
01:49:35,600 --> 01:49:39,760
They're smart enough to talk about those concepts. I had many good philosophical

1071
01:49:39,760 --> 01:49:45,360
discussions about such issues. They usually, at least as interesting as most humans in that.

1072
01:49:46,720 --> 01:49:53,600
Well, what do you think about AI safety in the simulated world? So can you, can you have kind of

1073
01:49:53,600 --> 01:49:58,720
create simulated worlds where you can test,

1074
01:50:01,120 --> 01:50:06,640
play with a dangerous AGI system? Yeah, and that was exactly what one of the early papers was on

1075
01:50:06,640 --> 01:50:13,040
AI boxing, how to leak proof singularity. If they're smart enough to realize they're in a simulation,

1076
01:50:13,040 --> 01:50:21,920
they'll act appropriately until you let them out. If they can hack out, they will. And if you're

1077
01:50:21,920 --> 01:50:26,480
observing them, that means there is a communication channel and that's enough for a social engineering

1078
01:50:26,480 --> 01:50:34,720
attack. So really, it's impossible to test an AGI system that's dangerous enough to

1079
01:50:35,600 --> 01:50:42,480
destroy humanity because it's either going to what, escape the simulation or pretend it's safe

1080
01:50:42,480 --> 01:50:50,880
until it's let out, either or. Can force you to let it out, blackmail you, bribe you, promise you

1081
01:50:50,880 --> 01:50:57,200
infinite life, 72 virgins, whatever. Yeah, they can be convincing, charismatic.

1082
01:50:58,400 --> 01:51:05,360
The social engineering is really scary to me because it feels like humans are very engineerable.

1083
01:51:06,960 --> 01:51:16,960
Like we're lonely, we're flawed, we're moody. And it feels like AI system with a nice voice

1084
01:51:17,760 --> 01:51:24,000
and convinces us to do basically anything at an extremely large scale.

1085
01:51:29,520 --> 01:51:36,160
It's also possible that the increased proliferation of all this technology will force humans to

1086
01:51:36,960 --> 01:51:42,640
get away from technology and value this like in-person communication. Basically, don't trust

1087
01:51:42,640 --> 01:51:50,720
anything else. It's possible, surprisingly. So at university, I see huge growth in online courses

1088
01:51:51,680 --> 01:51:57,680
and shrinkage of in-person, where I always understood in-person being the only value I offer.

1089
01:51:58,480 --> 01:52:05,200
So it's puzzling. I don't know. There could be a trend towards the in-person

1090
01:52:06,160 --> 01:52:10,000
because of deep fakes, because of inability to trust it,

1091
01:52:12,400 --> 01:52:17,600
inability to trust the veracity of anything on the internet. So the only way to verify it is by

1092
01:52:17,600 --> 01:52:26,240
being there in-person, but not yet. Why do you think aliens haven't come here yet?

1093
01:52:27,200 --> 01:52:32,560
So there is a lot of real estate out there. It would be surprising if it was all for nothing,

1094
01:52:32,640 --> 01:52:38,000
if it was empty. And the moment there is advanced enough biological civilization, kind of

1095
01:52:38,000 --> 01:52:43,680
self-starting civilization, it probably starts sending out the Neumann probes everywhere. And so

1096
01:52:43,680 --> 01:52:49,920
for every biological one, there got to be trillions of robot-populated planets, which probably do

1097
01:52:49,920 --> 01:52:59,360
more of the same. So it is likely statistically. So the fact that we haven't seen them, one answer

1098
01:52:59,520 --> 01:53:07,680
is that we're in the simulation. It would be hard to add or be not interesting to simulate

1099
01:53:07,680 --> 01:53:12,800
all those other intelligences. It's better for the narrative. You have to have a control variable?

1100
01:53:12,800 --> 01:53:20,560
Yeah, exactly. Okay. But it's also possible that there is, if we're not in simulation,

1101
01:53:20,560 --> 01:53:26,800
that there is a great filter that naturally a lot of civilizations get to this point,

1102
01:53:26,800 --> 01:53:32,720
where there's super intelligent agents, and then it just goes poof, just dies. So maybe

1103
01:53:34,160 --> 01:53:39,040
throughout our galaxy and throughout the universe, there's just a bunch of dead alien civilizations.

1104
01:53:39,840 --> 01:53:44,560
It's possible. I used to think that AI was the great filter, but I would expect like a wall of

1105
01:53:44,560 --> 01:53:50,080
computerium approaching us at speed of light or robots or something, and I don't see it.

1106
01:53:50,720 --> 01:53:54,320
So it would still make a lot of noise. It might not be interesting. It might not possess consciousness.

1107
01:53:55,280 --> 01:54:00,560
We've been talking about, it sounds like both you and I like humans.

1108
01:54:01,760 --> 01:54:02,400
Some humans.

1109
01:54:04,640 --> 01:54:10,160
Humans on the whole. And we would like to preserve the flame of human consciousness.

1110
01:54:10,800 --> 01:54:17,280
What do you think makes humans special that we would like to preserve them? Are we just being

1111
01:54:17,280 --> 01:54:25,440
selfish? Or is there something special about humans? So the only thing which matters is consciousness.

1112
01:54:25,440 --> 01:54:31,200
Outside of it, nothing else matters. And internal states of qualia, pain, pleasure,

1113
01:54:31,920 --> 01:54:37,520
it seems that it is unique to living beings. I'm not aware of anyone claiming that I can

1114
01:54:37,520 --> 01:54:43,440
torture a piece of software in a meaningful way. There is a society for prevention of suffering to

1115
01:54:44,400 --> 01:54:54,960
learning algorithms, but many things are real on the internet. But I don't think anyone, if I

1116
01:54:54,960 --> 01:55:00,640
told them, sit down and write a function to feel pain, they would go beyond having an integer

1117
01:55:00,640 --> 01:55:05,600
variable called pain and increasing the count. So we don't know how to do it. And that's unique.

1118
01:55:06,560 --> 01:55:13,280
That's what creates meaning. It would be kind of, as Bostrom calls it,

1119
01:55:13,280 --> 01:55:19,360
Disneyland without children, if that was gone. Do you think consciousness can be engineered

1120
01:55:19,360 --> 01:55:28,160
in artificial systems? Here, let me go to 2011 paper that you wrote. Robot rights.

1121
01:55:29,440 --> 01:55:34,560
Lastly, we would like to address a sub branch of machine ethics, which on the surface has little

1122
01:55:34,560 --> 01:55:39,280
to do with safety, but which is claimed to play a role in decision making by ethical machines.

1123
01:55:39,280 --> 01:55:45,600
Robot rights. Do you think it's possible to engineer consciousness in the machines?

1124
01:55:46,240 --> 01:55:53,840
And thereby, the question extends to our legal system. Do you think at that point robots should

1125
01:55:53,840 --> 01:56:02,960
have rights? Yeah, I think we can. I think it's possible to create consciousness in machines.

1126
01:56:02,960 --> 01:56:09,600
I tried designing a test for it with mixed success. That paper talked about problems with giving

1127
01:56:11,040 --> 01:56:16,640
civil rights to AI, which can reproduce quickly and outvote humans, essentially taking over a

1128
01:56:16,640 --> 01:56:24,800
government system by simply voting for their controlled candidates. As for consciousness

1129
01:56:24,800 --> 01:56:32,320
in humans and other agents, I have a paper where I proposed relying on experience of optical illusions.

1130
01:56:33,040 --> 01:56:38,400
If I can design a novel optical illusion and show it to an agent, an alien, a robot,

1131
01:56:38,960 --> 01:56:43,440
and they describe it exactly as I do, it's very hard for me to argue that they haven't

1132
01:56:43,440 --> 01:56:50,000
experienced that. It's not part of a picture. It's part of their software and hardware representation,

1133
01:56:50,000 --> 01:56:56,240
a bug in their code, which goes, oh, the triangle is rotating. I've been told it's really dumb and

1134
01:56:56,320 --> 01:57:03,120
really brilliant by different philosophers, so I am still on with it. But now we finally have

1135
01:57:03,120 --> 01:57:08,560
technology to test it. We have tools. We have AI's. If someone wants to run this experiment,

1136
01:57:08,560 --> 01:57:13,280
I'm happy to collaborate. So this is a test for consciousness? For internal state of experience.

1137
01:57:13,280 --> 01:57:19,120
That we share bugs? It will show that we share common experiences. If they have completely

1138
01:57:19,120 --> 01:57:23,600
different internal states, it wouldn't not register for us. But it's a positive test.

1139
01:57:23,600 --> 01:57:28,080
If they pass it time after time with probability increasing for every multiple choice,

1140
01:57:28,080 --> 01:57:32,880
then you have no choice but to ever accept that they have access to a conscious model

1141
01:57:32,880 --> 01:57:40,800
or they are themselves. So the reason illusions are interesting is, I guess, because it's a really

1142
01:57:40,800 --> 01:57:48,320
weird experience. And if you both share that weird experience that's not there in the bland

1143
01:57:48,320 --> 01:57:57,440
physical description of the raw data, that means that puts more emphasis on the actual experience.

1144
01:57:57,440 --> 01:58:02,320
And we know animals can experience some optical illusions, so we know they have certain types

1145
01:58:02,320 --> 01:58:08,960
of consciousness as a result, I would say. Yeah, well, that just goes to my sense that the flaws

1146
01:58:08,960 --> 01:58:13,520
in the bugs is what makes humans special, makes living forms special. So you're saying like,

1147
01:58:13,520 --> 01:58:19,840
yeah, it's a feature, not a bug. The bug is the feature. Whoa. Okay, that's a cool test for

1148
01:58:19,840 --> 01:58:24,720
consciousness. And you think that can be engineered in? So they have to be novel illusions. If it

1149
01:58:24,720 --> 01:58:29,520
can just Google the answer, it's useless. You have to come up with novel illusions which we tried

1150
01:58:29,520 --> 01:58:35,520
automating and failed. So if someone can develop a system capable of producing novel optical illusions

1151
01:58:35,520 --> 01:58:41,920
on demand, then we can definitely administer that test on significant scale with good results.

1152
01:58:41,920 --> 01:58:47,760
First of all, pretty cool idea. I don't know if it's a good general test of consciousness,

1153
01:58:47,760 --> 01:58:51,440
but it's a good component of that. And no matter why, it's just a cool idea. So

1154
01:58:52,400 --> 01:58:57,840
put me in the camp of people that like it. But you don't think like a Turing test style

1155
01:58:57,840 --> 01:59:02,640
imitation of consciousness is a good test. Like, if you can convince a lot of humans that you're

1156
01:59:02,640 --> 01:59:08,400
conscious, that doesn't, that to you is not impressive. There is so much data on the internet,

1157
01:59:08,400 --> 01:59:13,840
I know exactly what to say. Then you ask me common human questions. What does pain feel like? What

1158
01:59:13,840 --> 01:59:20,080
does pleasure feel like? All that is Googleable. I think to me, consciousness is closely tied to

1159
01:59:20,080 --> 01:59:26,640
suffering. So you can illustrate your capacity to suffer. But with, I guess with words, there's

1160
01:59:26,640 --> 01:59:32,560
so much data that you can say, you can pretend you're suffering and you can do so very convincingly.

1161
01:59:32,560 --> 01:59:38,560
There are simulators for torture games where the avatar screams in pain, begs to stop. I mean,

1162
01:59:38,560 --> 01:59:46,480
that was a part of kind of standard psychology research. You say it so calmly. It sounds pretty

1163
01:59:46,480 --> 01:59:56,480
dark. Welcome to humanity. Yeah. Yeah, it's like a Hitchhiker's Guide summary, mostly harmless.

1164
01:59:57,040 --> 02:00:04,560
I would, I would love to get a good summary when all of this is said and done. When Earth is no

1165
02:00:04,560 --> 02:00:09,920
longer a thing, whatever, a million, a billion years from now. Like, what's a good summary? What

1166
02:00:09,920 --> 02:00:18,480
happened here? It's interesting. I think AI will play a big part of that summary. And hopefully,

1167
02:00:18,480 --> 02:00:23,840
humans will too. What do you think about the merger of the two? So one of the things that Elon and

1168
02:00:23,920 --> 02:00:31,920
Neuralink talk about is one of the ways for us to achieve AI safety is to ride the wave of AGI,

1169
02:00:31,920 --> 02:00:39,280
so by merging. Incredible technology in a narrow sense to help the disabled. Just amazing support

1170
02:00:39,280 --> 02:00:48,400
at 100%. For long-term hybrid models, both parts need to contribute something to the overall system.

1171
02:00:48,400 --> 02:00:53,600
Right now, we are still more capable in many ways. So having this connection to AI would be

1172
02:00:54,240 --> 02:01:00,960
incredible, would make me superhuman in many ways. After a while, if I'm no longer smarter,

1173
02:01:00,960 --> 02:01:06,320
more creative, really don't contribute much, the system finds me as a biological bottleneck.

1174
02:01:06,320 --> 02:01:10,960
And either explicitly or implicitly, I'm removed from any participation in the system.

1175
02:01:11,680 --> 02:01:15,760
So it's like the appendix. By the way, the appendix is still around. So

1176
02:01:17,760 --> 02:01:23,920
even if it's, you said bottleneck. I don't know if we become a bottleneck. We just might not have

1177
02:01:23,920 --> 02:01:29,680
much use. There's a different thing than bottleneck. Wasting valuable energy by being there.

1178
02:01:30,400 --> 02:01:32,800
We don't waste that much energy. We're pretty energy efficient.

1179
02:01:33,840 --> 02:01:36,160
I'm gonna just stick around like the appendix. Come on, though.

1180
02:01:36,800 --> 02:01:40,000
That's the future we all dream about, become an appendix.

1181
02:01:41,360 --> 02:01:43,040
There's a history book of humanity.

1182
02:01:44,400 --> 02:01:48,560
Well, and also the consciousness thing, the peculiar particular kind of consciousness that

1183
02:01:48,560 --> 02:01:54,080
humans have, that might be useful. That might be really hard to simulate. But you said that,

1184
02:01:54,080 --> 02:01:57,680
how would that look like if you could engineer that in, in silicon?

1185
02:01:58,480 --> 02:01:59,120
Consciousness.

1186
02:01:59,120 --> 02:01:59,760
Consciousness.

1187
02:02:01,040 --> 02:02:05,200
I assume you're conscious. I have no idea how to test for it or how it impacts you in any way

1188
02:02:05,200 --> 02:02:10,000
whatsoever right now. You can perfectly simulate all of it without making any,

1189
02:02:10,960 --> 02:02:12,560
any different observations for me.

1190
02:02:13,200 --> 02:02:17,680
But to do it in a computer, how would you do that? Because you kind of said that you think it's

1191
02:02:17,680 --> 02:02:18,560
possible to do that.

1192
02:02:19,360 --> 02:02:25,920
So it may be an emergent phenomena. We seem to get it through evolutionary process.

1193
02:02:26,880 --> 02:02:36,640
It's not obvious how it helps us to survive better, but maybe it's an internal kind of gooey,

1194
02:02:36,640 --> 02:02:41,680
which allows us to better manipulate the world, simplifies a lot of control structures.

1195
02:02:43,120 --> 02:02:48,640
That's one area where we have very, very little progress. Lots of papers, lots of research,

1196
02:02:48,640 --> 02:02:57,360
but consciousness is not a big, big area of successful discovery so far. A lot of people

1197
02:02:57,360 --> 02:03:01,920
think that machines would have to be conscious to be dangerous. That's a big misconception.

1198
02:03:02,560 --> 02:03:06,800
There is absolutely no need for this very powerful optimizing agent to

1199
02:03:07,520 --> 02:03:10,560
feel anything while it's performing things on you.

1200
02:03:11,440 --> 02:03:15,840
But what do you think about this, the whole science of emergence in general?

1201
02:03:16,560 --> 02:03:20,640
So I don't know how much you know about cellular automata or these simplified systems where

1202
02:03:21,280 --> 02:03:24,960
that study this very question from simple rules, emergent complexity.

1203
02:03:24,960 --> 02:03:27,040
I attended wool from summer school.

1204
02:03:28,880 --> 02:03:32,640
I love Stephen very much. I love his work. I love cellular automata. So

1205
02:03:34,000 --> 02:03:42,240
I just would love to get your thoughts on how that fits into your view in the emergence of

1206
02:03:42,320 --> 02:03:46,320
intelligence in AGI systems. And maybe just even simply,

1207
02:03:47,200 --> 02:03:50,720
what do you make of the fact that this complexity can emerge from such simple rules?

1208
02:03:51,280 --> 02:03:57,840
So the rule is simple, but the size of a space is still huge. And the neural networks were really

1209
02:03:57,840 --> 02:04:03,680
the first discovery in AI. A hundred years ago, the first papers were published on neural networks,

1210
02:04:03,680 --> 02:04:10,000
which just didn't have enough compute to make them work. I can give you a rule such as start

1211
02:04:10,000 --> 02:04:15,440
printing progressively larger strings. That's it, one sentence. It will output everything,

1212
02:04:15,440 --> 02:04:22,560
every program, every DNA code, everything in that rule. You need intelligence to filter it out,

1213
02:04:22,560 --> 02:04:28,400
obviously, to make it useful. But simple generation is not that difficult. And a lot of those systems

1214
02:04:29,280 --> 02:04:34,960
end up being touring complete systems. So they're universal. And we expect that level of complexity

1215
02:04:34,960 --> 02:04:41,600
from them. What I like about Wolfram's work is that he talks about irreducibility. You have to

1216
02:04:41,600 --> 02:04:47,360
run the simulation. You can act, predict what is going to do ahead of time. And I think that's

1217
02:04:47,360 --> 02:04:53,840
very relevant to what we are talking about with those very complex systems. Until you live through

1218
02:04:53,840 --> 02:05:00,000
it, you cannot ahead of time tell me exactly what it's going to do. Redusibility means that

1219
02:05:00,000 --> 02:05:04,640
for a sufficiently complex system, you have to run the thing. You have to, you can't predict what's

1220
02:05:04,640 --> 02:05:09,120
going to happen in the universe. You have to create a new universe and run the thing. Big bang, the

1221
02:05:09,120 --> 02:05:19,280
whole thing. But running it may be consequential as well. It might destroy humans. And to you,

1222
02:05:19,280 --> 02:05:26,480
there's no chance that AI is somehow carrying the flame of consciousness, the flame of specialness

1223
02:05:26,480 --> 02:05:34,800
and awesomeness that is humans. It may somehow, but I still feel kind of bad that it killed all of us.

1224
02:05:34,800 --> 02:05:40,480
I would prefer that doesn't happen. I can be happy for others, but to a certain degree.

1225
02:05:41,440 --> 02:05:44,960
It would be nice if we stuck around for a long time. At least give us a planet,

1226
02:05:46,080 --> 02:05:50,720
the human planet. It'd be nice for it to be Earth. And then they can go elsewhere.

1227
02:05:50,720 --> 02:06:00,400
Since they're so smart, they can colonize Mars. Do you think they could help convert us to type 1,

1228
02:06:00,400 --> 02:06:10,000
type 2, type 3? Let's just stick to type 2 civilization on the Kardashev scale. Help us humans

1229
02:06:11,280 --> 02:06:17,920
expand on into the cosmos. So all of it goes back to are we somehow controlling it? Are we

1230
02:06:17,920 --> 02:06:23,520
getting results we want? If yes, then everything's possible. Yes, they can definitely help us with

1231
02:06:23,520 --> 02:06:29,040
science, engineering, exploration in every way conceivable. But it's a big if.

1232
02:06:30,080 --> 02:06:36,560
This whole thing about control though, humans are bad with control. Because the moment they gain

1233
02:06:36,560 --> 02:06:42,960
control, they can also easily become too controlling. The more control you have,

1234
02:06:42,960 --> 02:06:47,120
the more you want it. It's the old power corrupts and the absolute power corrupts. Absolutely.

1235
02:06:48,880 --> 02:06:55,120
And it feels like control over AGI, saying we live in a universe where that's possible. We come up

1236
02:06:55,120 --> 02:07:01,200
with ways to actually do that. It's also scary. Because the collection of humans that have the

1237
02:07:01,200 --> 02:07:07,920
control over AGI, they become more powerful than the other humans. And they can let that power get

1238
02:07:07,920 --> 02:07:16,400
to their head. And then a small selection of them back to Stalin, start getting ideas. And then

1239
02:07:16,400 --> 02:07:21,920
eventually it's one person usually with a mustache or a funny hat that starts sort of making big

1240
02:07:21,920 --> 02:07:26,240
speeches. And then all of a sudden you live in a world that's either 1984 or a brave new world.

1241
02:07:27,280 --> 02:07:34,080
And always a war where somebody and this whole idea of control turned out to be

1242
02:07:35,360 --> 02:07:39,920
actually also not beneficial to humanity. So that's scary too. It's actually worse because

1243
02:07:39,920 --> 02:07:45,040
historically they all died. This could be different. This could be permanent dictatorship,

1244
02:07:45,120 --> 02:07:48,880
permanent suffering. Well, the nice thing about humans, it seems like,

1245
02:07:49,680 --> 02:07:55,360
it seems like the moment power starts corrupting their mind, they can create a huge amount of

1246
02:07:55,360 --> 02:08:00,160
suffering. So there's a negative that can kill people, make people suffer. But then they become

1247
02:08:00,160 --> 02:08:09,600
worse and worse at their job. It feels like the more evil you start doing, at least we are incompetent.

1248
02:08:10,480 --> 02:08:14,880
Well, no, they become more and more incompetent. So they start losing their grip on power.

1249
02:08:15,760 --> 02:08:20,240
So like holding on to power is not a trivial thing. It requires extreme competence,

1250
02:08:20,240 --> 02:08:26,720
which I suppose Stalin was good at. It requires you to do evil and be competent at it. Or just get

1251
02:08:26,720 --> 02:08:31,440
lucky. And those systems help with that. You have perfect surveillance. You can do some mind

1252
02:08:31,440 --> 02:08:39,360
reading, I presume, eventually. It would be very hard to remove control from more capable systems

1253
02:08:39,440 --> 02:08:45,200
over us. And then it would be hard for humans to become the hackers that escape the control of

1254
02:08:45,200 --> 02:08:54,400
the AGI because the AGI is so damn good. And then, yeah, yeah, yeah. And then the dictator is immortal.

1255
02:08:55,120 --> 02:08:59,760
Yeah, that's not great. That's not a great outcome. See, I'm more afraid of humans than AI systems.

1256
02:09:01,280 --> 02:09:05,920
I'm afraid, I believe that most humans want to do good and have the capacity to do good.

1257
02:09:06,880 --> 02:09:15,120
But also all humans have the capacity to do evil. And when you test them by giving them absolute

1258
02:09:15,120 --> 02:09:20,640
powers, you would, if you give them AGI, that could result in a lot, a lot of suffering.

1259
02:09:23,440 --> 02:09:28,240
What gives you hope about the future? I could be wrong. I've been wrong before.

1260
02:09:29,040 --> 02:09:35,920
If you look at 100 years from now, and you're immortal, and you look back, and it turns out

1261
02:09:35,920 --> 02:09:41,680
this whole conversation, you said a lot of things that were very wrong. Now that looking 100 years

1262
02:09:41,680 --> 02:09:48,960
back, what would be the explanation? What happened in those 100 years that made you wrong?

1263
02:09:49,920 --> 02:09:54,800
That made the words you said today wrong? There is so many possibilities. We had catastrophic

1264
02:09:54,800 --> 02:10:00,400
events which prevented development of advanced microchips. That's a hopeful future.

1265
02:10:01,120 --> 02:10:07,120
We could be in one of those personal universes, and the one I'm in is beautiful. It's all about

1266
02:10:07,120 --> 02:10:12,960
me, and I like it a lot. So we've now, just to linger on that, that means every human has

1267
02:10:12,960 --> 02:10:20,160
their personal universe. Yes. Maybe multiple ones. Hey, why not? You can shop around.

1268
02:10:21,120 --> 02:10:25,040
It's possible that somebody comes up with alternative

1269
02:10:26,320 --> 02:10:31,040
model for building AI, which is not based on neural networks, which are hard to scrutinize,

1270
02:10:31,040 --> 02:10:38,960
and that alternative is somehow, I don't see how, but somehow avoiding all the problems I

1271
02:10:38,960 --> 02:10:42,560
speak about in general terms, not applying them to specific architectures.

1272
02:10:44,400 --> 02:10:48,560
Aliens come and give us friendly superintelligence. There is so many options.

1273
02:10:48,640 --> 02:10:57,760
Is it also possible that creating superintelligence systems becomes harder and harder? It's not so

1274
02:10:57,760 --> 02:11:08,160
easy to do the foom, the takeoff. That would probably speak more about how much smarter

1275
02:11:08,160 --> 02:11:12,640
that system is compared to us. Maybe it's hard to be a million times smarter, but it's still

1276
02:11:12,640 --> 02:11:17,920
okay to be five times smarter. That is totally possible. That I have no objections to.

1277
02:11:18,400 --> 02:11:25,360
It's, there's a S-curve type situation about smarter and it's going to be three point seven

1278
02:11:25,360 --> 02:11:30,480
times smarter than all of human civilization. Just the problems we face in this world,

1279
02:11:30,480 --> 02:11:34,080
each problem is like an IQ test. You need certain intelligence to solve it. So we just

1280
02:11:34,080 --> 02:11:39,840
don't have more complex problems outside of mathematics for it to be showing off. You can have

1281
02:11:39,840 --> 02:11:43,840
IQ of 500. If you're playing tic-tac-toe, it doesn't show. It doesn't matter.

1282
02:11:44,240 --> 02:11:53,040
So the idea there is that the problems define your cognitive capacity. So because the problems

1283
02:11:53,040 --> 02:11:58,720
on earth are not sufficiently difficult, it's not going to be able to expand this cognitive

1284
02:11:58,720 --> 02:12:05,360
capacity. Possible. And because of that, wouldn't that be a good thing? It still could be a lot

1285
02:12:05,360 --> 02:12:11,040
smarter than us. And to dominate long term, you just need some advantage. You have to be the

1286
02:12:11,040 --> 02:12:16,160
smartest. You don't have to be a million times smarter. So even 5x might be enough? It'd be

1287
02:12:16,160 --> 02:12:22,000
impressive. What is it? IQ of a thousand? I mean, I know those units don't mean anything at that

1288
02:12:22,000 --> 02:12:28,800
scale, but still like as a comparison, the smartest human is like 200. Well, actually, no, I didn't

1289
02:12:28,800 --> 02:12:33,280
mean compared to an individual human. I meant compared to the collective intelligence of the

1290
02:12:33,280 --> 02:12:39,920
human species. If you're somehow 5x smarter than that. We are more productive as a group. I don't

1291
02:12:39,920 --> 02:12:44,560
think we are more capable of solving individual problems. Like if all of humanity plays chess

1292
02:12:44,560 --> 02:12:52,000
together, we are not like a million times better than world champion. That's because that there's

1293
02:12:53,280 --> 02:13:01,360
that's like one S-curve is the chess, but humanity is very good at exploring the full range of ideas.

1294
02:13:01,920 --> 02:13:06,240
Like the more Einstein's you have, the more there's just a high probability to come up with

1295
02:13:06,320 --> 02:13:09,840
general relativity. But I feel like it's more of a quantity superintelligence than quality

1296
02:13:09,840 --> 02:13:13,120
superintelligence. Yeah, sure. But you know, quantity and

1297
02:13:13,120 --> 02:13:21,040
enough quantity sometimes becomes quality. Oh, man, humans. What do you think is the meaning

1298
02:13:21,040 --> 02:13:29,600
of this whole thing? We've been talking about humans and humans not dying, but why are we here?

1299
02:13:30,400 --> 02:13:34,960
It's a simulation. We're being tested. The test is will you be dumb enough to create superintelligence

1300
02:13:34,960 --> 02:13:42,240
and release it? So the objective function is not be dumb enough to kill ourselves. Yeah,

1301
02:13:42,240 --> 02:13:47,520
you're unsafe. Prove yourself to be a safe agent who doesn't do that and you get to go to the next

1302
02:13:47,520 --> 02:13:52,800
game. The next level of the game? What's the next level? I don't know. I haven't hacked the simulation

1303
02:13:52,800 --> 02:13:57,120
yet. Well, maybe hacking the simulation is the thing. I'm working as fast as I can.

1304
02:13:58,800 --> 02:14:03,440
And physics would be the way to do that. Quantum physics, yeah. Well, I hope we do.

1305
02:14:03,920 --> 02:14:07,680
And I hope whatever is outside is even more fun than this one because this one was pretty damn

1306
02:14:07,680 --> 02:14:13,840
fun. And just a big thank you for doing the work you're doing. There's so much exciting

1307
02:14:13,840 --> 02:14:22,240
development in AI and to ground it in the existential risks is really, really important.

1308
02:14:23,360 --> 02:14:28,800
Humans love to create stuff and we should be careful not to destroy ourselves in the process.

1309
02:14:28,800 --> 02:14:34,160
So thank you for doing that really important work. Thank you so much for inviting me. It was

1310
02:14:34,160 --> 02:14:41,040
amazing and my dream is to be proven wrong. If everyone just picks up a paper or book

1311
02:14:41,040 --> 02:14:46,560
and shows how I messed it up, that would be optimal. But for now, the simulation continues.

1312
02:14:47,680 --> 02:14:52,640
Thank you, Roman. Thanks for listening to this conversation with Roman Jampalski.

1313
02:14:52,640 --> 02:14:56,320
To support this podcast, please check out our sponsors in the description.

1314
02:14:56,960 --> 02:15:01,120
And now let me leave you with some words from Frank Herbert in Dune.

1315
02:15:02,160 --> 02:15:09,200
I must not fear. Fear is the mind killer. Fear is the little death that brings total obliteration.

1316
02:15:09,920 --> 02:15:16,640
I will face fear. I will permit it to pass over me and through me. And when it has gone past,

1317
02:15:16,640 --> 02:15:22,480
I will turn the inner eye to see its path. Where the fear has gone, there will be nothing.

1318
02:15:22,480 --> 02:15:38,400
Only I will remain. Thank you for listening and hope to see you next time.

